{
  "owner": {
    "id": "syfyufei",
    "display_name": "Yufei Sun (Adrian)",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/58319029?u=bf502ce9cc90cb5205c96d7b546f4290552b49f2&v=4",
    "url": "https://github.com/syfyufei",
    "bio": "I am an egalitarian and a comparative political scientist. Currently I am a Ph.D. candidate at Department of Political Science, Tsinghua University.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 5,
      "total_commands": 32,
      "total_skills": 0,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "syfyufei/LLM-Research-Marketplace",
      "url": "https://github.com/syfyufei/LLM-Research-Marketplace",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-12-19T07:40:32Z",
        "created_at": "2025-12-03T10:32:13Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1811
        },
        {
          "path": ".gitattributes",
          "type": "blob",
          "size": 66
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 205
        },
        {
          "path": "AGENTS.md",
          "type": "blob",
          "size": 3333
        },
        {
          "path": "ARCHITECTURE.md",
          "type": "blob",
          "size": 6444
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 8455
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 9026
        },
        {
          "path": "TEST_PLAN.md",
          "type": "blob",
          "size": 6498
        },
        {
          "path": "TRUTH_VERIFICATION_COMPLETION_SUMMARY.md",
          "type": "blob",
          "size": 13369
        },
        {
          "path": "TRUTH_VERIFICATION_TEST_PLAN.md",
          "type": "blob",
          "size": 12329
        },
        {
          "path": "USAGE.md",
          "type": "blob",
          "size": 5449
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/project-management.md",
          "type": "blob",
          "size": 2628
        },
        {
          "path": "docs/research-memory.md",
          "type": "blob",
          "size": 2391
        },
        {
          "path": "docs/skill-squared.md",
          "type": "blob",
          "size": 2656
        },
        {
          "path": "install.sh",
          "type": "blob",
          "size": 1782
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 426
        },
        {
          "path": "plugins/academic-figures/README.md",
          "type": "blob",
          "size": 1412
        },
        {
          "path": "plugins/academic-figures/academic-figures.md",
          "type": "blob",
          "size": 2197
        },
        {
          "path": "plugins/academic-figures/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/commands/bar.md",
          "type": "blob",
          "size": 5209
        },
        {
          "path": "plugins/academic-figures/commands/dotwhisker.md",
          "type": "blob",
          "size": 5258
        },
        {
          "path": "plugins/academic-figures/commands/line.md",
          "type": "blob",
          "size": 3870
        },
        {
          "path": "plugins/academic-figures/commands/map.md",
          "type": "blob",
          "size": 3586
        },
        {
          "path": "plugins/academic-figures/commands/scatter.md",
          "type": "blob",
          "size": 5762
        },
        {
          "path": "plugins/academic-figures/commands/setup.md",
          "type": "blob",
          "size": 2964
        },
        {
          "path": "plugins/academic-figures/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/config/config.json",
          "type": "blob",
          "size": 1084
        },
        {
          "path": "plugins/academic-figures/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/docs/academic-figures.md",
          "type": "blob",
          "size": 4699
        },
        {
          "path": "plugins/academic-figures/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/templates/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/academic-figures/templates/python/common.py.template",
          "type": "blob",
          "size": 3697
        },
        {
          "path": "plugins/project-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 649
        },
        {
          "path": "plugins/project-management/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/commands/create.md",
          "type": "blob",
          "size": 2602
        },
        {
          "path": "plugins/project-management/commands/restructure.md",
          "type": "blob",
          "size": 2361
        },
        {
          "path": "plugins/project-management/commands/status.md",
          "type": "blob",
          "size": 2308
        },
        {
          "path": "plugins/project-management/commands/validate.md",
          "type": "blob",
          "size": 2305
        },
        {
          "path": "plugins/project-management/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/config/config.json",
          "type": "blob",
          "size": 784
        },
        {
          "path": "plugins/project-management/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/docs/project-management.md",
          "type": "blob",
          "size": 2628
        },
        {
          "path": "plugins/project-management/project-management.md",
          "type": "blob",
          "size": 6076
        },
        {
          "path": "plugins/project-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/templates/project",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/project-management/templates/project/README.md.template",
          "type": "blob",
          "size": 909
        },
        {
          "path": "plugins/project-management/templates/project/gitignore.template",
          "type": "blob",
          "size": 404
        },
        {
          "path": "plugins/project-management/templates/project/project-config.json.template",
          "type": "blob",
          "size": 472
        },
        {
          "path": "plugins/project-management/templates/project/project.yml.template",
          "type": "blob",
          "size": 585
        },
        {
          "path": "plugins/research-memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/research-memory/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/research-memory/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 775
        },
        {
          "path": "plugins/research-memory/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/research-memory/commands/bootstrap.md",
          "type": "blob",
          "size": 492
        },
        {
          "path": "plugins/research-memory/commands/checkpoint.md",
          "type": "blob",
          "size": 1403
        },
        {
          "path": "plugins/research-memory/commands/focus.md",
          "type": "blob",
          "size": 2897
        },
        {
          "path": "plugins/research-memory/commands/insights.md",
          "type": "blob",
          "size": 1415
        },
        {
          "path": "plugins/research-memory/commands/query.md",
          "type": "blob",
          "size": 487
        },
        {
          "path": "plugins/research-memory/commands/remember.md",
          "type": "blob",
          "size": 592
        },
        {
          "path": "plugins/research-memory/commands/review.md",
          "type": "blob",
          "size": 1261
        },
        {
          "path": "plugins/research-memory/commands/status.md",
          "type": "blob",
          "size": 687
        },
        {
          "path": "plugins/research-memory/commands/summary.md",
          "type": "blob",
          "size": 834
        },
        {
          "path": "plugins/research-memory/commands/timeline.md",
          "type": "blob",
          "size": 1027
        },
        {
          "path": "plugins/research-memory/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/research-memory/config/config.json",
          "type": "blob",
          "size": 327
        },
        {
          "path": "plugins/research-memory/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/research-memory/docs/research-memory.md",
          "type": "blob",
          "size": 2391
        },
        {
          "path": "plugins/research-memory/research-memory.md",
          "type": "blob",
          "size": 10375
        },
        {
          "path": "plugins/skill-squared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 605
        },
        {
          "path": "plugins/skill-squared/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/commands/command.md",
          "type": "blob",
          "size": 1523
        },
        {
          "path": "plugins/skill-squared/commands/create.md",
          "type": "blob",
          "size": 1983
        },
        {
          "path": "plugins/skill-squared/commands/sync.md",
          "type": "blob",
          "size": 1713
        },
        {
          "path": "plugins/skill-squared/commands/validate.md",
          "type": "blob",
          "size": 1558
        },
        {
          "path": "plugins/skill-squared/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/config/config.json",
          "type": "blob",
          "size": 615
        },
        {
          "path": "plugins/skill-squared/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/docs/skill-squared.md",
          "type": "blob",
          "size": 2656
        },
        {
          "path": "plugins/skill-squared/skill-squared.md",
          "type": "blob",
          "size": 4994
        },
        {
          "path": "plugins/skill-squared/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/templates/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/skill-squared/templates/skill/CLAUDE.md.template",
          "type": "blob",
          "size": 1718
        },
        {
          "path": "plugins/skill-squared/templates/skill/README.md.template",
          "type": "blob",
          "size": 1555
        },
        {
          "path": "plugins/skill-squared/templates/skill/command.md.template",
          "type": "blob",
          "size": 557
        },
        {
          "path": "plugins/skill-squared/templates/skill/install.sh.template",
          "type": "blob",
          "size": 1308
        },
        {
          "path": "plugins/skill-squared/templates/skill/marketplace.json.template",
          "type": "blob",
          "size": 462
        },
        {
          "path": "plugins/skill-squared/templates/skill/plugin.json.template",
          "type": "blob",
          "size": 321
        },
        {
          "path": "plugins/skill-squared/templates/skill/skill.md.template",
          "type": "blob",
          "size": 1205
        },
        {
          "path": "plugins/truth-verification",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 513
        },
        {
          "path": "plugins/truth-verification/README.md",
          "type": "blob",
          "size": 6834
        },
        {
          "path": "plugins/truth-verification/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/commands/audit.md",
          "type": "blob",
          "size": 10826
        },
        {
          "path": "plugins/truth-verification/commands/cite-check.md",
          "type": "blob",
          "size": 7837
        },
        {
          "path": "plugins/truth-verification/commands/init.md",
          "type": "blob",
          "size": 3603
        },
        {
          "path": "plugins/truth-verification/commands/register.md",
          "type": "blob",
          "size": 4783
        },
        {
          "path": "plugins/truth-verification/commands/reproduce.md",
          "type": "blob",
          "size": 9946
        },
        {
          "path": "plugins/truth-verification/commands/status.md",
          "type": "blob",
          "size": 9497
        },
        {
          "path": "plugins/truth-verification/commands/track.md",
          "type": "blob",
          "size": 7617
        },
        {
          "path": "plugins/truth-verification/commands/verify.md",
          "type": "blob",
          "size": 5728
        },
        {
          "path": "plugins/truth-verification/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/config/config.json",
          "type": "blob",
          "size": 3429
        },
        {
          "path": "plugins/truth-verification/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/docs/truth-verification.md",
          "type": "blob",
          "size": 15516
        },
        {
          "path": "plugins/truth-verification/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/templates/truth",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/truth-verification/templates/truth/audit-report.md.template",
          "type": "blob",
          "size": 5574
        },
        {
          "path": "plugins/truth-verification/templates/truth/manifest.json.template",
          "type": "blob",
          "size": 4087
        },
        {
          "path": "plugins/truth-verification/truth-verification.md",
          "type": "blob",
          "size": 7033
        }
      ],
      "marketplace": {
        "name": "LLM-Research-Marketplace",
        "version": "4.0.0",
        "description": "Curated marketplace of Claude Code skills for AI/LLM research and development",
        "owner_info": {
          "name": "Adrian",
          "email": "syfyufei@gmail.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "research-memory",
            "description": "A skill for academic research memory management.",
            "source": "./plugins/research-memory",
            "category": null,
            "version": "0.2.0",
            "author": {
              "name": "Adrian",
              "email": "syfyufei@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add syfyufei/LLM-Research-Marketplace",
              "/plugin install research-memory@LLM-Research-Marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-19T07:40:32Z",
              "created_at": "2025-12-03T10:32:13Z",
              "license": null
            },
            "commands": [
              {
                "name": "/bootstrap",
                "description": "Restore project context and generate work plan",
                "path": "plugins/research-memory/commands/bootstrap.md",
                "frontmatter": {
                  "description": "Restore project context and generate work plan"
                },
                "content": "Use the research-memory skill to bootstrap the project context.\n\nCall the `research_memory_bootstrap` tool to:\n- Load the project overview from `memory/project-overview.md`\n- Show recent development log entries from `memory/devlog.md`\n- List current TODOs from `memory/todos.md`\n- Generate a suggested work plan for today\n\nPresent the information in a clear, organized format and suggest what the user should work on next."
              },
              {
                "name": "/checkpoint",
                "description": "Create a named checkpoint of current state",
                "path": "plugins/research-memory/commands/checkpoint.md",
                "frontmatter": {
                  "description": "Create a named checkpoint of current state"
                },
                "content": "Use the research-memory skill to create a checkpoint of the current project state.\n\nAsk the user for a checkpoint name/description if not provided.\n\nA checkpoint captures:\n- Current timestamp\n- Checkpoint name/tag\n- Project state summary\n- Recent work summary\n- Active TODOs at this point\n- Key metrics snapshot\n\nLog the checkpoint to `memory/checkpoints.md` in this format:\n\n```markdown\n## Checkpoint: [NAME] (YYYY-MM-DD HH:MM)\n\n**Purpose**: [User's description]\n\n**Project State**:\n- Phase: [Current phase]\n- Progress: [Brief summary]\n- Open TODOs: X items\n- Recent experiments: Y\n\n**Key Metrics**:\n- Total sessions: N\n- Experiments run: M\n- Decisions made: P\n\n**Notable Context**:\n- [Important note 1]\n- [Important note 2]\n\n---\n```\n\nAfter creating the checkpoint, confirm:\n- Where it was saved\n- How to reference it later\n- Suggest using it as a comparison point\n\nUse cases:\n- Before major changes: `/research-memory:checkpoint \"Before refactor\"`\n- Milestones: `/research-memory:checkpoint \"Paper draft v1 complete\"`\n- Experiments: `/research-memory:checkpoint \"Baseline model finalized\"`\n- Reviews: `/research-memory:checkpoint \"End of sprint 3\"`\n\nSupport optional arguments:\n- `--tag milestone` : Mark as a milestone checkpoint\n- `--tag experiment` : Mark as an experiment checkpoint\n- `--tag backup` : Mark as a safety backup point"
              },
              {
                "name": "/focus",
                "description": "Get focused daily work plan",
                "path": "plugins/research-memory/commands/focus.md",
                "frontmatter": {
                  "description": "Get focused daily work plan"
                },
                "content": "Use the research-memory skill to generate a focused daily work plan.\n\nRead from memory files and analyze:\n- Open TODOs from `memory/todos.md`\n- Recent work context from `memory/devlog.md`\n- Current project phase and blockers\n- Dependencies between tasks\n\nGenerate a focused daily plan with:\n\n## ‚òÄÔ∏è Today's Focus ([DATE])\n\n### ‚ö° Top Priority (2-3 hours)\n[Single most important task today]\n- **Task**: [Description]\n- **Why now**: [Urgency/blocking reason]\n- **Blocks**: [What depends on this]\n- **Context**: [Relevant background from recent work]\n- **Success criteria**: [How to know it's done]\n\n### üìå Secondary Tasks (1-2 hours each)\n[2-3 important but not urgent tasks]\n- **Task 1**: [Description]\n  ‚Üí Est: X hours | Impact: High/Medium\n\n- **Task 2**: [Description]\n  ‚Üí Est: X hours | Impact: High/Medium\n\n### üéØ Stretch Goals (if time permits)\n[1-2 nice-to-have tasks for extra time]\n- [Task that can be done if ahead of schedule]\n- [Quick win or preparation for tomorrow]\n\n### ‚è∞ Time Budget Check\n- Top Priority: 2-3 hours\n- Secondary Tasks: 2-4 hours\n- Stretch Goals: 1-2 hours\n- **Total: 5-9 hours**\n\n### üí° Focus Recommendation\n[Specific advice on how to approach today's work]\n- Recommended order: [Task sequence]\n- Deep work needed: [Which tasks need focus]\n- Quick wins: [Tasks that can be done quickly]\n- Blockers to resolve: [What's in the way]\n\n### üö´ Not Today\n[Tasks that are important but should wait]\n- [Task]: Wait until [dependency] is resolved\n- [Task]: Schedule for [later time]\n\n---\n\n## Selection Criteria\n\nWhen selecting tasks, prioritize based on:\n1. **Blockers**: Tasks that are blocking other work\n2. **Urgency**: Deadlines or time-sensitive items\n3. **Dependencies**: Prerequisites for planned work\n4. **Energy level**: Match task difficulty to typical daily energy curve\n5. **Momentum**: Build on recent progress\n\n## Time Management\n\nSupport optional arguments:\n- `--hours N` : Plan for N hours of work (default: 6-8)\n- `--energy high|normal|low` : Adjust for energy level\n- `--mode deep|varied|quick` : Focus style preference\n\nExamples:\n```bash\n/research-memory:focus\n/research-memory:focus --hours 4\n/research-memory:focus --energy low --mode quick\n```\n\n## Focus Tips\n\nInclude practical advice:\n- **Deep work blocks**: Suggest 90-120 min focused sessions\n- **Break strategy**: Recommend when to take breaks\n- **Context switching**: Minimize by grouping similar tasks\n- **Energy management**: Hard tasks when fresh, routine tasks when tired\n\n## Integration with Other Commands\n\nSuggest complementary commands:\n- \"Run `/research-memory:status` to see current state\"\n- \"Use `/research-memory:remember` at end of day to log progress\"\n- \"Check `/research-memory:insights` if feeling stuck\"\n\nBe specific, actionable, and realistic. Help the user make actual progress today, not just plan theoretically."
              },
              {
                "name": "/insights",
                "description": "Get AI-powered insights and suggestions",
                "path": "plugins/research-memory/commands/insights.md",
                "frontmatter": {
                  "description": "Get AI-powered insights and suggestions"
                },
                "content": "Use the research-memory skill to analyze the project and provide intelligent insights.\n\nRead all memory files and analyze patterns, gaps, and opportunities.\n\nProvide insights in these categories:\n\n## üîç Pattern Analysis\n- **Work patterns**: When are you most productive?\n- **Research phases**: Which phase takes longest?\n- **Decision velocity**: How quickly do decisions get made?\n- **Iteration speed**: Time between experiments\n\n## ‚ö†Ô∏è Potential Issues\n- **Forgotten TODOs**: Items open for >30 days\n- **Stale decisions**: Decisions that may need revisiting\n- **Repeated experiments**: Similar experiments with different results\n- **Documentation gaps**: Missing context or rationale\n\n## üí° Suggestions\n- **Next steps**: Based on current progress and patterns\n- **Quick wins**: Easy tasks that could be completed soon\n- **Research directions**: Promising areas to explore\n- **Efficiency tips**: Ways to speed up common tasks\n\n## üìä Project Health Metrics\n- **Momentum**: Are things progressing well?\n- **Clarity**: Are goals and next steps clear?\n- **Documentation**: Is the project well-documented?\n- **Risk factors**: What could slow down progress?\n\n## üéØ Recommendations\n- [Prioritized list of 3-5 actionable recommendations]\n\nBe constructive and specific. Focus on actionable insights that can immediately improve the research workflow."
              },
              {
                "name": "/query",
                "description": "Query research memory history",
                "path": "plugins/research-memory/commands/query.md",
                "frontmatter": {
                  "description": "Query research memory history"
                },
                "content": "Use the research-memory skill to search through project history.\n\nAsk the user what they want to search for if not specified.\n\nCall the `research_memory_query_history` tool with the search query to find relevant information from:\n- Development logs (`memory/devlog.md`)\n- Decision records (`memory/decisions.md`)\n- Experiment data (`memory/experiments.csv`)\n\nPresent the search results with context, timestamps, and relevant excerpts."
              },
              {
                "name": "/remember",
                "description": "Remember current work session",
                "path": "plugins/research-memory/commands/remember.md",
                "frontmatter": {
                  "description": "Remember current work session"
                },
                "content": "Use the research-memory skill to remember the current work session.\n\nAnalyze the conversation history and extract:\n- Session goal and main objective\n- Summary of changes made\n- Experiments conducted (hypothesis, dataset, model, metrics, notes)\n- Key decisions made (decision, rationale, alternatives considered)\n- New TODOs and completed items\n- Research phases (data_preprocess, modeling, robustness, etc.)\n\nThen call the `research_memory_log_session` tool with the structured information.\n\nConfirm what was logged and where it was saved."
              },
              {
                "name": "/review",
                "description": "Review work in a specific time period",
                "path": "plugins/research-memory/commands/review.md",
                "frontmatter": {
                  "description": "Review work in a specific time period"
                },
                "content": "Use the research-memory skill to review work done in a specific time period.\n\nAsk the user for the time period if not specified in the command arguments.\n\nRead from memory files and extract all activities within the specified period:\n- Development log entries\n- Experiments conducted\n- Decisions made\n- TODOs created and completed\n\nPresent as a structured review:\n\n## Period Review: [START_DATE] to [END_DATE]\n\n### Summary\n- Total sessions: X\n- Experiments completed: Y\n- Decisions made: Z\n- TODOs completed: N of M\n\n### Detailed Activities\n\n#### Week of [DATE]\n- [Activity 1 with context]\n- [Activity 2 with context]\n\n#### Week of [DATE]\n- [Activity 1 with context]\n- [Activity 2 with context]\n\n### Key Achievements\n- [Notable accomplishment 1]\n- [Notable accomplishment 2]\n\n### Challenges & Blockers\n- [Issue 1 and how it was resolved]\n- [Issue 2 and current status]\n\n### Productivity Insights\n- Most productive days: [DAYS]\n- Primary focus area: [PHASE]\n- Velocity: X activities per week\n\nSupport arguments:\n- `--last-week` : Review last 7 days\n- `--last-month` : Review last 30 days\n- `--from YYYY-MM-DD --to YYYY-MM-DD` : Custom range\n- `--format report|weekly|monthly` : Different report styles"
              },
              {
                "name": "/status",
                "description": "Quick project status overview",
                "path": "plugins/research-memory/commands/status.md",
                "frontmatter": {
                  "description": "Quick project status overview"
                },
                "content": "Use the research-memory skill to show a quick project status overview.\n\nCall the `research_memory_bootstrap` tool and present a concise summary including:\n- **Project name and main objective**\n- **Last updated**: When was the last session logged\n- **Recent progress**: Brief 1-2 line summary of latest work\n- **Current phase**: Which research phase (data_preprocess, modeling, robustness, etc.)\n- **Active TODOs**: Number of open vs completed items\n- **Quick stats**:\n  - Total sessions logged\n  - Total experiments conducted\n  - Total decisions recorded\n\nPresent this in a compact, dashboard-style format that can be quickly scanned."
              },
              {
                "name": "/summary",
                "description": "Generate comprehensive project summary",
                "path": "plugins/research-memory/commands/summary.md",
                "frontmatter": {
                  "description": "Generate comprehensive project summary"
                },
                "content": "Use the research-memory skill to generate a comprehensive project summary.\n\nRead from memory files and create a structured summary:\n\n## Project Overview\n- Research questions and hypotheses\n- Dataset information\n- Timeline (start date to present)\n\n## Key Findings\n- Summarize all successful experiments\n- Highlight significant results with metrics\n- Key insights discovered\n\n## Important Decisions\n- List major decisions made\n- Rationale for each decision\n- Impact on project direction\n\n## Progress Summary\n- What phases are complete\n- What's currently in progress\n- What's planned next\n\n## Open Questions / TODOs\n- Outstanding issues\n- Next steps\n\nFormat this as a polished report suitable for:\n- Team updates\n- Advisor meetings\n- Research documentation\n- Progress reports"
              },
              {
                "name": "/timeline",
                "description": "Visualize project timeline",
                "path": "plugins/research-memory/commands/timeline.md",
                "frontmatter": {
                  "description": "Visualize project timeline"
                },
                "content": "Use the research-memory skill to create a visual project timeline.\n\nRead `memory/devlog.md`, `memory/experiments.csv`, and `memory/decisions.md` to extract:\n- All dated entries\n- Experiments with timestamps\n- Decision points\n\nPresent as a chronological timeline:\n\n```\n2025-11-15 | [DATA] Data cleaning completed - removed 150 outliers\n2025-11-18 | [MODEL] Baseline model R¬≤=0.72\n2025-11-20 | [DECISION] Chose log transformation over Box-Cox\n2025-11-22 | [ROBUST] Heteroscedasticity test passed\n2025-11-25 | [EXPERIMENT] Added control variables, R¬≤‚Üí0.78\n2025-12-01 | [WRITING] Started draft of results section\n```\n\nUse emoji indicators for different activities:\n- üìä Data processing\n- üî¨ Experiments\n- ü§î Decisions\n- ‚úÖ Completed milestones\n- üìù Writing\n\nSupport optional arguments:\n- `--last 7d` : Show last 7 days\n- `--last 30d` : Show last 30 days\n- `--from YYYY-MM-DD --to YYYY-MM-DD` : Custom date range\n\nIf no arguments provided, show last 14 days by default."
              }
            ],
            "skills": []
          },
          {
            "name": "project-management",
            "description": "A skill for standardized project scaffolding, restructuring, and validation.",
            "source": "./plugins/project-management",
            "category": null,
            "version": "0.1.0",
            "author": {
              "name": "Adrian",
              "email": "syfyufei@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add syfyufei/LLM-Research-Marketplace",
              "/plugin install project-management@LLM-Research-Marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-19T07:40:32Z",
              "created_at": "2025-12-03T10:32:13Z",
              "license": null
            },
            "commands": [
              {
                "name": "/create",
                "description": "Create a standardized research project workspace with templates, Git init, and metadata.",
                "path": "plugins/project-management/commands/create.md",
                "frontmatter": {
                  "description": "Create a standardized research project workspace with templates, Git init, and metadata."
                },
                "content": "## Goal\nGuide the user through `/project-management:create` to spin up a new project directory following the standard blueprint.\n\n## Preparation\n1. Collect parameters (ask concise follow-up questions if missing):\n   - `project_name` (kebab-case, required)\n   - `root` directory (default current workspace)\n   - `git_init` (y/n, default yes)\n   - `author_name` (default from config)\n   - `description`\n   - `force` overwrite flag if directory already exists\n2. Validate project name against regex `^[a-z][a-z0-9-]*[a-z0-9]$`. Reject uppercase, underscores, or trailing hyphens.\n3. Determine `PROJECT_PATH=\"$ROOT/$project_name\"` and confirm with the user before creating anything when `force` not provided.\n\n## Execution Steps\n1. **Create directories**\n   ```bash\n   PROJECT_PATH=\"<abs path>\"\n   mkdir -p \"$PROJECT_PATH\"/{claude-code,data/raw,data/cleaned,codes,paper/figures,paper/manuscripts,pre/poster,pre/slides}\n   ```\n   Mention any pre-existing directories that were reused.\n2. **Render templates** using values:\n   - `creation_date=$(date +%Y-%m-%d)`\n   - `year=$(date +%Y)`\n   Copy from `../templates/project/*.template`:\n   ```bash\n   env PROJECT_NAME=\"...\" AUTHOR_NAME=\"...\" DESCRIPTION=\"...\" CREATION_DATE=\"$creation_date\" YEAR=\"$year\" \\\n     envsubst < ../templates/project/README.md.template > \"$PROJECT_PATH/README.md\"\n   ```\n   Repeat for `.gitignore` and `project.yml`. Also write `.project-config.json` matching the template in `skills/project-management.md`.\n3. **Git initialization**\n   ```bash\n   if [ \"$git_init\" = \"true\" ]; then\n     (cd \"$PROJECT_PATH\" && git init && git add . && git commit -m \"Initial commit: $project_name\")\n   fi\n   ```\n   If Git commands fail, capture stderr and report as warning, not fatal.\n4. **Summary**\n   Respond with:\n   - Absolute project path\n   - Directories created\n   - Files generated\n   - Git initialization status\n   - Suggested next steps (e.g., ‚Äúopen README.md and update problem statement‚Äù)\n\n## Error Handling\n- Directory already exists ‚Üí offer `force` or alternative name.\n- Missing template or permission issue ‚Üí stop and show exact path plus remediation idea.\n- Invalid project name ‚Üí repeat regex hint.\n\n## Example Reply\n```\nCreated /Users/adrian/research/causal-impact\n- Directories: claude-code, data/raw, data/cleaned, ...\n- Files: README.md, .gitignore, project.yml, .project-config.json\nGit repo initialized with initial commit.\nNext: document hypotheses inside README.md and add datasets under data/raw/.\n```"
              },
              {
                "name": "/restructure",
                "description": "Restructure an existing project into the standard directory/file blueprint with backups.",
                "path": "plugins/project-management/commands/restructure.md",
                "frontmatter": {
                  "description": "Restructure an existing project into the standard directory/file blueprint with backups."
                },
                "content": "## Goal\nUpgrade an existing project to the canonical layout while preserving data by creating a timestamped backup and normalizing directories.\n\n## Preparation\n1. Gather `root` (project path), `backup` (default true), `remove_nonstandard` (default true), and `force` flags.\n2. Verify the directory exists via `ls \"$root\"`; abort with actionable error if missing.\n3. Load required directories/files from `../config/config.json -> standard_structure`.\n\n## Execution\n1. **Backup (if enabled)**\n   ```bash\n   if [ \"$backup\" = \"true\" ]; then\n     ts=$(date +%Y%m%d_%H%M%S)\n     backup_dir=\"$(dirname \"$root\")/$(basename \"$root\")_backup_$ts\"\n     cp -R \"$root\" \"$backup_dir\"\n   fi\n   ```\n   Report backup path.\n2. **Create missing directories**\n   Loop through required list and run `mkdir -p \"$root/$dir\"` while tracking which ones were added.\n3. **Suggest file moves**\n   - Scan top-level items (excluding `.git`, `.claude*`).\n   - For loose `.py`/`.ipynb`, suggest moving into `codes/`.\n   - For figures (`*.png`, `*.svg`, `*.pdf`), suggest `paper/figures/`.\n   Confirm before moving unless `force=true`.\n4. **Remove non-standard directories** when permitted:\n   ```bash\n   if [ \"$remove_nonstandard\" = \"true\" ]; then\n     # list directories not in required set + allowed hidden ones\n   fi\n   ```\n   Always log deletions; skip if uncertain.\n5. **Refresh templates**\n   Re-render README, `.gitignore`, `project.yml`, `.project-config.json` using `update_existing=true` semantics (overwrite while keeping backups). Mention fields updated (e.g., restructure date).\n6. **Report**\n   Summarize backup, created dirs, removed dirs, files moved, and manual follow-ups.\n\n## Error Handling\n- If copy fails due to permissions, warn and confirm whether to continue without backup.\n- If user declines destructive step, skip and note outstanding work.\n- Keep backups until user deletes them.\n\n## Example Reply\n```\nRestructured ~/projects/ablation-study\n- Backup: ~/projects/ablation-study_backup_20250214_103255\n- Created dirs: paper/manuscripts, pre/poster\n- Moved scripts -> codes/, PNGs -> paper/figures/\n- Removed non-standard dir: old-notes/\nTemplates refreshed with restructure date 2025-02-14.\nNext: run /project-management:validate to confirm compliance.\n```"
              },
              {
                "name": "/status",
                "description": "Generate a comprehensive status report for a research project, combining structure stats, validation score, and Git telemetry.",
                "path": "plugins/project-management/commands/status.md",
                "frontmatter": {
                  "description": "Generate a comprehensive status report for a research project, combining structure stats, validation score, and Git telemetry."
                },
                "content": "## Goal\nProduce `/project-management:status` output so the user can see readiness at a glance.\n\n## Preparation\n1. Ask for `root` path (default current).\n2. Determine whether to include Git data (`include_git`, default true) and per-directory file stats (`include_file_stats`, default true).\n\n## Execution\n1. Confirm project exists. Collect `project_name=$(basename \"$root\")` and `last_modified` via `stat -f \"%Sm\"`.\n2. Traverse directory tree (skip hidden) using `find` or `python - <<'PY' ...` to compute:\n   - Total directory count\n   - Total file count\n   - Total size in MB (rounded to 0.1)\n   - Optional breakdown: for each top-level directory, record file count + size.\n3. Invoke `/project-management:validate` internally (or re-run the validation logic inline) to reuse compliance score and key issues.\n4. If `.git` exists and `include_git=true`, gather:\n   ```bash\n   commits=$(git -C \"$root\" rev-list --count HEAD 2>/dev/null || echo 0)\n   branches=$(git -C \"$root\" branch --list | wc -l | tr -d ' ')\n   last_commit=$(git -C \"$root\" log -1 --format=%ci 2>/dev/null || echo \"n/a\")\n   ```\n   Handle detached HEAD gracefully.\n5. Render a human-readable report:\n   - Header: project name, path, last modified\n   - Structure stats table\n   - Compliance score with emoji indicator (>=90 ‚úÖ, 75-89 üëç, 60-74 ‚ö†Ô∏è, <60 ‚ùå)\n   - File breakdown (only if requested)\n   - Git stats or warning if repo missing\n   - Recommendations (e.g., ‚ÄúScore < 70 ‚Üí run /project-management:restructure‚Äù)\n\n## Error Handling\n- Missing directory ‚Üí respond with actionable message.\n- `git` not installed or repo missing ‚Üí show ‚ÄúGit data unavailable‚Äù.\n- Traversal permission issues ‚Üí skip and mention path.\n\n## Example Reply\n```\nStatus: /Users/adrian/projects/ablation-study (updated 2025-02-18 09:42)\nStructure: 23 dirs, 178 files, 412.5‚ÄØMB, compliance score 88/100 (üëç)\nBreakdown:\n- data/: 94 files, 320.4‚ÄØMB\n- codes/: 28 files, 12.6‚ÄØMB\nGit: 42 commits across 3 branches, last commit 2025-02-17 22:10 UTC\nInsights:\n- Missing directory pre/poster. Create before conference prep.\n- Consider validating again after migrating notebooks under codes/.\n```"
              },
              {
                "name": "/validate",
                "description": "Validate a project directory, compute compliance score, and optionally auto-fix issues.",
                "path": "plugins/project-management/commands/validate.md",
                "frontmatter": {
                  "description": "Validate a project directory, compute compliance score, and optionally auto-fix issues."
                },
                "content": "## Goal\nRun `/project-management:validate` to ensure a project matches the standard blueprint and produce a scored report.\n\n## Preparation\n1. Ask for `root` path (default current).\n2. Determine whether to `fix_issues` (y/n) and if `strict_mode` should flag extra directories/files.\n3. Load scoring weights from `../config/config.json -> validation.scoring_weights`.\n\n## Execution Steps\n1. **Directory/File Scan**\n   ```bash\n   root=\"<abs path>\"\n   required_dirs=(claude-code data/raw data/cleaned codes paper/figures paper/manuscripts pre/poster pre/slides)\n   required_files=(README.md .gitignore project.yml)\n   ```\n   - Capture missing and extra items (ignore `.git`, `.claude`, `.claude-plugin`, `__pycache__`).\n2. **Score Calculation**\n   - `dir_score = (required_dirs - missing_dirs)/required_dirs * 40`\n   - `file_score = (required_files - missing_files)/required_files * 35`\n   - `content_score = 15` if key files exist and non-empty else 0.\n   - `git_score = 10` when `.git` exists and has commits (`git rev-list --count HEAD > 0`).\n   - Total score = round sum to int.\n3. **Issue List**\n   - Missing directories/files.\n   - Strict mode: list extras.\n   - Empty or tiny files (<10 bytes) flagged for rewrite.\n4. **Auto-Fix (optional)**\n   When `fix_issues=true`, create missing directories and render templates with `missing_only=true` so existing files remain untouched. Document each fix.\n5. **Report**\n   Respond with:\n   - Score out of 100 + qualitative tier (>=90 excellent, >=75 good, >=60 fair, else poor).\n   - Table/bullets of missing vs. extra.\n   - Fixes applied or pending.\n   - Recommendations (e.g., ‚ÄúInitialize Git before sharing‚Äù).\n\n## Error Handling\n- Root missing ‚Üí abort with helpful tip.\n- If Git commands fail, treat as warning and continue scoring without Git points.\n- When template rendering fails, include failing file path and leave instructions for manual creation.\n\n## Example Reply\n```\n/Users/adrian/projects/causal-impact scored 82/100 (good)\nMissing: pre/poster, README.md\nExtras (strict): old-data/\nFixes: created pre/poster, generated README.md from template.\nRecommendation: move raw datasets under data/raw and remove old-data after backup.\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "skill-squared",
            "description": "A skill for skill creation, extension, sync, and validation.",
            "source": "./plugins/skill-squared",
            "category": null,
            "version": "0.1.0",
            "author": {
              "name": "Adrian",
              "email": "syfyufei@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add syfyufei/LLM-Research-Marketplace",
              "/plugin install skill-squared@LLM-Research-Marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-19T07:40:32Z",
              "created_at": "2025-12-03T10:32:13Z",
              "license": null
            },
            "commands": [
              {
                "name": "/command",
                "description": "Add a new slash command to an existing skill and update plugin metadata.",
                "path": "plugins/skill-squared/commands/command.md",
                "frontmatter": {
                  "description": "Add a new slash command to an existing skill and update plugin metadata."
                },
                "content": "## Goal\nExecute `/skill-squared:command` so the user gains a new `.claude/commands/<name>.md` in their skill repo and the plugin manifest references it.\n\n## Steps\n1. Collect:\n   - `skill_dir` (absolute path to skill root)\n   - `command_name` (kebab-case)\n   - `command_description`\n   - Optional `command_instructions`\n2. Validate:\n   - `skill_dir` contains `.claude-plugin/plugin.json`\n   - Command name not already present in `.claude/commands/`\n   - Plugin JSON is valid.\n3. Ensure `.claude/commands/` folder exists: `mkdir -p \"$skill_dir/.claude/commands\"`.\n4. Render command file using `../templates/skill/command.md.template`. Variables:\n   - `command_name`\n   - `command_title` (title case)\n   - `command_description`\n   - `command_instructions` (fallback to default sentence when empty)\n5. Append command path to plugin manifest:\n   ```bash\n   jq '.commands += [\"./.claude/commands/'\"$command_name\"'.md\"]' \\\n     \"$skill_dir/.claude-plugin/plugin.json\" > /tmp/plugin.json && \\\n   mv /tmp/plugin.json \"$skill_dir/.claude-plugin/plugin.json\"\n   ```\n   If the `commands` array is missing, initialize it.\n6. Respond with success message including the relative command path and reminder to reinstall the skill.\n\n## Error Handling\n- Directory missing ‚Üí explain expected structure.\n- Invalid command name ‚Üí show kebab-case example.\n- JSON update failure ‚Üí keep created command file but warn user to manually register it."
              },
              {
                "name": "/create",
                "description": "Create a brand new Claude Code skill repository with templates, metadata, and executable install script.",
                "path": "plugins/skill-squared/commands/create.md",
                "frontmatter": {
                  "description": "Create a brand new Claude Code skill repository with templates, metadata, and executable install script."
                },
                "content": "## Goal\nUse `/skill-squared:create` to scaffold a complete skill project from scratch.\n\n## Workflow\n1. Gather required inputs:\n   - `skill_name` (kebab-case)\n   - `skill_description`\n   - `author_name`\n   - `author_email`\n   - `github_user`\n   - Optional `target_dir` (defaults to current)\n   - Optional `version` (default `0.1.0`)\n2. Validate:\n   - Name matches `^[a-z0-9]+(-[a-z0-9]+)*$`\n   - Email contains `@`\n   - Target directory is writable and destination folder does not already exist (prompt for overwrite only if user insists).\n3. Create directory skeleton inside `$TARGET/$skill_name`:\n   ```\n   .claude-plugin/\n   .claude/commands/\n   skills/\n   handlers/\n   templates/skill/\n   templates/command/\n   config/\n   docs/\n   ```\n4. Render templates from `../templates/skill/*.template` using env substitution:\n   ```bash\n   export SKILL_NAME=...\n   export SKILL_DESCRIPTION=...\n   export AUTHOR_NAME=...\n   export AUTHOR_EMAIL=...\n   export GITHUB_USER=...\n   export VERSION=...\n   export CREATED_DATE=$(date +%Y-%m-%d)\n   envsubst < ../templates/skill/plugin.json.template > \"$SKILL_PATH/.claude-plugin/plugin.json\"\n   # repeat for marketplace.json, skill.md, README.md, install.sh, CLAUDE.md\n   ```\n5. Copy `.gitignore` and `LICENSE` contents (use MIT license text with current year + author).\n6. Make install script executable: `chmod +x \"$SKILL_PATH/install.sh\"`.\n7. Output summary table:\n   - Root path\n   - Directories created\n   - Files rendered\n   - Reminder to run `./install.sh` from inside the new skill.\n\n## Error Handling\n- If directory exists, stop unless the user explicitly confirms `force`.\n- Missing templates ‚Üí point to `../templates/skill/` and suggest running `/skill-squared:sync` to refresh.\n- Template render failure ‚Üí include variable name causing the issue and reference docs/skill-squared.md for valid placeholders."
              },
              {
                "name": "/sync",
                "description": "Synchronize a standalone skill repository into the marketplace with optional dry-run and backups.",
                "path": "plugins/skill-squared/commands/sync.md",
                "frontmatter": {
                  "description": "Synchronize a standalone skill repository into the marketplace with optional dry-run and backups."
                },
                "content": "## Goal\nUse `/skill-squared:sync` to copy the skill markdown + command files from a source repo into this marketplace safely.\n\n## Inputs\n- `source_dir` (standalone skill root)\n- `target_dir` (usually this marketplace root)\n- `skill_name` (auto-detect from plugin.json if omitted)\n- `dry_run` (bool, default false)\n- `backup` (bool, default true)\n\n## Procedure\n1. Validate both directories exist and contain the expected `.claude-plugin/plugin.json` files.\n2. Determine `skill_name`:\n   - If provided, trust it.\n   - Else parse `.claude-plugin/plugin.json` for `\"name\"`.\n3. Load sync config from `../config/config.json -> sync.files_to_sync` (defaults: `skills/{skill_name}.md` + `.claude/commands/`).\n4. Build file list; show preview to the user and confirm unless `dry_run=false` with explicit approval.\n5. For each file:\n   - Ensure target subdirectory exists (e.g., `.claude/commands/skill-squared/` inside the marketplace).\n   - If backup enabled and target file already exists, copy it to `<file>.backup.<timestamp>.md`.\n   - Copy file when not `dry_run`; otherwise, log `[DRY RUN]` entry.\n6. After copying, verify file sizes match (`stat -f \"%z\"`) and display summary counts: copied, skipped, backups.\n7. Recommend running `/skill-squared:validate` on the marketplace copy to confirm success.\n\n## Error Handling\n- Missing source/target path ‚Üí fail fast and show `ls` command to help locate.\n- Dry-run flagged issues (e.g., missing commands dir) should block execution until resolved.\n- If timestamp indicates target newer than source, warn and request confirmation before overwriting."
              },
              {
                "name": "/validate",
                "description": "Validate a skill repository for required files, JSON structure, frontmatter, and executable permissions.",
                "path": "plugins/skill-squared/commands/validate.md",
                "frontmatter": {
                  "description": "Validate a skill repository for required files, JSON structure, frontmatter, and executable permissions."
                },
                "content": "## Goal\nRun `/skill-squared:validate` on a skill directory (standalone or marketplace) to ensure it meets publishing requirements.\n\n## Inputs\n- `skill_dir`\n- Optional `skill_name`\n- `strict` flag (optional) for additional linting\n\n## Steps\n1. Confirm directory exists. If `skill_name` omitted, auto-detect by reading `.claude-plugin/plugin.json` or the lone file inside `skills/`.\n2. Load validation config from `../config/config.json -> validation`.\n3. **Required files**: iterate through configured patterns (expand `{skill_name}`) and ensure each exists. Record missing items as errors.\n4. **JSON validation**: run `jq empty` (or `python -m json.tool`) on `.claude-plugin/plugin.json` and `.claude-plugin/marketplace.json`.\n5. **Frontmatter checks**:\n   - Parse YAML blocks (between `---` lines) for `skills/{skill}.md`.\n   - Iterate `.claude/commands/*.md`; ensure `description` present.\n6. **Executable files**: verify `install.sh` has executable bit (`[ -x install.sh ]`).\n7. Compile findings into categories:\n   - `errors`\n   - `warnings`\n   - `info`\n   Provide actionable suggestions for each.\n8. Return pass/fail message plus summary table (e.g., ‚Äú3 errors, 1 warning‚Äù). Encourage rerunning after fixes.\n\n## Error Handling\n- If YAML/frontmatter missing, highlight file and provide snippet example.\n- If JSON parse fails, show `jq` stderr to guide fixes.\n- When permissions insufficient, suggest `chmod +x install.sh`."
              }
            ],
            "skills": []
          },
          {
            "name": "truth-verification",
            "description": "Ensure research integrity by tracking data provenance, verifying authenticity, and preventing fabricated claims.",
            "source": "./plugins/truth-verification",
            "category": null,
            "version": "1.0.0",
            "author": {
              "name": "Adrian",
              "email": "syfyufei@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add syfyufei/LLM-Research-Marketplace",
              "/plugin install truth-verification@LLM-Research-Marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-19T07:40:32Z",
              "created_at": "2025-12-03T10:32:13Z",
              "license": null
            },
            "commands": [
              {
                "name": "/audit",
                "description": null,
                "path": "plugins/truth-verification/commands/audit.md",
                "frontmatter": null,
                "content": "# truth-verification:audit\n\n**Goal**: Generate comprehensive audit reports documenting research methodology, data provenance, and analysis lineage for publication, compliance, and reproducibility verification.\n\n**When to use**: Before publication, before sharing research, for compliance audits, or to create reproducible research archives.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists with complete tracking\n2. Register all data files\n3. Track all script executions\n4. Run `/truth-verification:verify` to confirm data integrity\n5. Run `/truth-verification:reproduce` to validate reproducibility\n6. Optional: Run `/truth-verification:cite-check` to validate citations\n\n---\n\n## Execution\n\n### Generate default audit report (Markdown):\n```bash\n/truth-verification:audit\n```\n\n### Generate with timeline:\n```bash\n/truth-verification:audit --include-timeline\n```\n\n### Generate JSON report for systems:\n```bash\n/truth-verification:audit --format json --output audit-report.json\n```\n\n### Generate HTML report for web viewing:\n```bash\n/truth-verification:audit --format html --output audit-report.html\n```\n\n### Generate with full details:\n```bash\n/truth-verification:audit --detail-level full --include-timeline --include-anomalies\n```\n\n### Generate publication-ready report:\n```bash\n/truth-verification:audit --format markdown \\\n  --include-timeline \\\n  --include-recommendations \\\n  --output publication-audit.md\n```\n\n### Generate with anomaly detection:\n```bash\n/truth-verification:audit --include-anomalies --anomaly-sensitivity high\n```\n\n---\n\n## What Happens\n\n1. **Data Collection**:\n   - Loads complete manifest with all tracking data\n   - Collects all registration, execution, and verification logs\n   - Gathers timeline information from timestamps\n   - Extracts dependency graph and reproducibility metrics\n\n2. **Analysis**:\n   - Validates all data sources and their provenance\n   - Traces complete lineage from inputs ‚Üí scripts ‚Üí outputs\n   - Detects anomalies:\n     - Files modified after registration\n     - Missing scripts or data\n     - Broken dependency chains\n     - Orphaned data\n     - Unusual patterns (very large jumps in file size, unexpected modifications)\n   - Evaluates reproducibility\n   - Checks for policy violations\n\n3. **Timeline Generation** (optional):\n   - Creates chronological record of:\n     - When data was registered\n     - When scripts were executed\n     - When verifications occurred\n     - When modifications were detected\n     - All significant events with timestamps\n\n4. **Anomaly Detection** (optional):\n   - Identifies suspicious patterns:\n     - Data modified after analysis claimed to be complete\n     - Scripts with very long/short execution times\n     - Unusual modifications to input data\n     - Files added to manifest much later than creation\n     - Gaps in tracking\n   - Provides risk assessments\n\n5. **Report Generation**:\n   - Creates formatted report in selected format\n   - Includes summary statistics\n   - Lists all data sources with provenance\n   - Documents all analysis steps\n   - Shows complete dependency chains\n   - Provides reproducibility assessment\n   - Lists issues and recommendations\n   - Includes timeline (if requested)\n   - Includes anomaly analysis (if requested)\n\n6. **Manifest Update**:\n   - Records audit generation in manifest\n   - Adds audit metadata and timestamp\n   - Updates audit_status field\n\n---\n\n## Output Examples\n\n### Markdown Audit Report (Sample):\n```\n# Research Audit Report\n\n## Executive Summary\n- Project: test-project\n- Generated: 2025-12-15T15:00:00Z\n- Status: AUDIT COMPLETE ‚úì\n- Reproducibility Score: 95/100 (Excellent)\n- Issues Found: 0 critical, 0 warnings\n\n## Data Inventory\n\n### Registered Data Sources (5 files)\n1. data/raw/dataset.csv\n   - Registered: 2025-12-15T10:00:00Z\n   - Hash: a7b3f8d9e2c1b4f6a8e9c2d5f7a8b3c1\n   - Size: 2.5 MB\n   - Status: ‚úì Verified\n   - Integrity: ‚úì Unchanged since registration\n\n2. data/raw/metrics.csv\n   - Registered: 2025-12-15T10:05:00Z\n   - Hash: b8c4a9e3d1f2c5a8b1d4e7f9a2c5d8e1\n   - Size: 1.2 MB\n   - Status: ‚úì Verified\n   - Integrity: ‚úì Unchanged since registration\n\n... (more data sources)\n\n## Analysis Scripts (3 tracked)\n1. codes/preprocessing.py\n   - First executed: 2025-12-15T11:00:00Z (45 seconds)\n   - Executions: 3 total\n   - Inputs: data/raw/dataset.csv, data/raw/metrics.csv\n   - Outputs: data/cleaned/merged.csv\n   - Status: ‚úì All executions complete\n\n2. codes/analysis.py\n   - First executed: 2025-12-15T12:30:00Z (120 seconds)\n   - Executions: 2 total\n   - Inputs: data/cleaned/merged.csv\n   - Outputs: paper/figures/results.png, paper/data/analysis.json\n   - Status: ‚úì All executions complete\n\n... (more scripts)\n\n## Dependency Chains\n### Chain 1: Primary Analysis Pipeline\n  data/raw/dataset.csv (2.5 MB)\n    ‚Üí preprocessing.py (45 sec)\n    ‚Üí data/cleaned/merged.csv (2.3 MB) ‚úì\n    ‚Üí analysis.py (120 sec)\n    ‚Üí paper/figures/results.png (456 KB) ‚úì\n    ‚Üí paper/manuscript.md (references results)\n\nStatus: ‚úì COMPLETE AND TRACEABLE\n\n### Chain 2: Metrics Processing\n  data/raw/metrics.csv (1.2 MB)\n    ‚Üí merge.py (30 sec)\n    ‚Üí data/cleaned/metrics.csv (1.1 MB) ‚úì\n\nStatus: ‚úì COMPLETE AND TRACEABLE\n\n## Timeline of Events\n\n2025-12-15T10:00:00Z - Registered data/raw/dataset.csv (2.5 MB)\n2025-12-15T10:05:00Z - Registered data/raw/metrics.csv (1.2 MB)\n2025-12-15T10:30:00Z - Registered codes/preprocessing.py\n2025-12-15T11:00:00Z - Executed preprocessing.py (45 seconds)\n2025-12-15T11:05:00Z - Generated data/cleaned/merged.csv (2.3 MB)\n2025-12-15T12:30:00Z - Executed analysis.py (120 seconds)\n2025-12-15T12:32:00Z - Generated paper/figures/results.png (456 KB)\n2025-12-15T14:00:00Z - Verified all data files (‚úì all unchanged)\n2025-12-15T14:30:00Z - Validated reproducibility score: 95/100\n\n## Reproducibility Assessment\nScore: 95/100 (EXCELLENT)\n- Input Integrity: 40/40 ‚úì\n- Script Traceability: 30/30 ‚úì\n- Output Attribution: 20/20 ‚úì\n- No Orphaned Data: 5/10 (5 extra files marked as backup)\n\nRecommendation: READY FOR PUBLICATION\n\n## Anomalies Detected\nNone - All systems normal.\n\n## Compliance Summary\n‚úì All data sources registered with cryptographic hashes\n‚úì All analysis steps traced with execution metadata\n‚úì All outputs attributed to generating scripts\n‚úì No data modifications after analysis completion\n‚úì Complete dependency graph with no broken links\n‚úì Reproducibility score exceeds 90 (excellent range)\n\n## Recommendations\n1. Archive this audit report with the manuscript\n2. Commit .truth/ directory to version control\n3. Share audit report with collaborators\n4. Consider publishing with full reproducibility support\n\n---\nGenerated by truth-verification v0.4.0\nAudit Report Version: 1.0.0\n```\n\n---\n\n## Report Formats\n\n### Markdown Format\n- Human-readable\n- Suitable for documentation\n- Can be included in papers\n- Good for version control\n- Default format\n\n### JSON Format\n- Machine-parseable\n- Suitable for tools and systems\n- Complete structured data\n- Enables automated analysis\n- Good for dashboards\n\n### HTML Format\n- Web-viewable\n- Interactive navigation\n- Formatted for presentation\n- Shareable via email or web\n- Includes styling and navigation\n\n---\n\n## Anomaly Sensitivity Levels\n\n**Low**: Only critical issues (missing files, broken chains)\n**Medium**: Critical + significant issues (modified data, missing metadata)\n**High**: All issues including minor inconsistencies\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"No data to audit\" | Manifest is empty | Register files and track scripts first |\n| \"Invalid manifest\" | Manifest corrupted | Restore from backup or reinitialize |\n| \"Cannot generate report\" | Disk space or permission issue | Check disk and permissions |\n| \"Invalid format\" | Unsupported report format | Use json, markdown, or html |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Audit report generated in selected format\n2. Report includes all sections:\n   - Executive summary\n   - Data inventory\n   - Analysis scripts\n   - Dependency chains\n   - Timeline (if requested)\n   - Anomalies (if requested)\n   - Recommendations\n3. Report file created at specified output path\n4. Manifest updated with audit metadata\n\n---\n\n## Advanced Scenarios\n\n### Generate with full forensics:\n```bash\n/truth-verification:audit --detail-level forensic \\\n  --include-timeline \\\n  --include-anomalies \\\n  --anomaly-sensitivity high \\\n  --format markdown\n```\n\nCreates detailed analysis suitable for investigations.\n\n### Compare audits over time:\n```bash\n/truth-verification:audit --compare-with-previous-audit\n```\n\nShows what changed since last audit.\n\n### Generate for specific data subset:\n```bash\n/truth-verification:audit --include-only \"data/raw/*,paper/*\"\n```\n\nOnly audits specific directories.\n\n### Generate with external reviewer summary:\n```bash\n/truth-verification:audit --format markdown \\\n  --executive-summary-only \\\n  --output executive-audit-summary.md\n```\n\nCreates concise summary for external reviewers.\n\n---\n\n## Publication Integration\n\n**Complete publication workflow**:\n```bash\n# 1. Initialize and track all work\n/truth-verification:init\n/truth-verification:register --recursive --dir data/\n/truth-verification:track --script codes/...\n/truth-verification:verify\n\n# 2. Validate reproducibility and citations\n/truth-verification:reproduce --generate-report\n/truth-verification:cite-check --paper paper/manuscript.md --strict-mode\n\n# 3. Generate final audit\n/truth-verification:audit --include-timeline --format markdown --output audit-trail.md\n\n# 4. Publish with complete reproducibility documentation\n# Include audit-trail.md with submission\n# Archive .truth/ with code repository\n# Share reproducibility badge or DOI\n```\n\n---\n\n## Compliance and Regulatory\n\nUse audit reports for:\n- **Research Ethics**: Document data handling and modification tracking\n- **Data Privacy**: Track access and modifications\n- **Regulatory Compliance**: Create audit trails for compliance audits\n- **Fraud Detection**: Anomaly detection helps identify suspicious patterns\n- **Institutional Policies**: Demonstrate compliance with research standards\n\n---\n\n## Next Steps\n\nAfter audit generation:\n\n1. **Review report**: Check for issues and recommendations\n2. **Address anomalies**: Investigate any detected anomalies\n3. **Archive report**: Save audit report with manuscript/code\n4. **Share with team**: Provide audit report to collaborators\n5. **Publish**: Submit with full reproducibility documentation\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Register data files\n- `/truth-verification:track` - Record script execution\n- `/truth-verification:verify` - Check data integrity\n- `/truth-verification:reproduce` - Validate reproducibility\n- `/truth-verification:cite-check` - Validate citations\n"
              },
              {
                "name": "/cite-check",
                "description": null,
                "path": "plugins/truth-verification/commands/cite-check.md",
                "frontmatter": null,
                "content": "# truth-verification:cite-check\n\n**Goal**: Validate that all data references in papers and manuscripts correspond to registered data sources, ensuring no fabricated citations.\n\n**When to use**: Before finalizing papers, ensure every data claim references actual registered files.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists with registered data files\n2. Have paper/manuscript files to validate (Markdown, LaTeX, or plain text)\n3. Ensure data files are registered in manifest (run `/truth-verification:register` if needed)\n4. Optional: Define citation patterns for your domain\n\n---\n\n## Execution\n\n### Check citations in a paper file:\n```bash\n/truth-verification:cite-check --paper paper/manuscript.md\n```\n\n### Check all papers in a directory:\n```bash\n/truth-verification:cite-check --directory paper/manuscripts\n```\n\n### Strict mode (fail on unresolved references):\n```bash\n/truth-verification:cite-check --paper paper/manuscript.md --strict-mode\n```\n\n### Include orphaned data (data not cited anywhere):\n```bash\n/truth-verification:cite-check --paper paper/manuscript.md --report-orphaned-data\n```\n\n### Custom citation patterns:\n```bash\n/truth-verification:cite-check --paper paper/manuscript.md \\\n  --patterns \"data/.*\\.csv,file:data/[a-zA-Z0-9_-]+\\.(csv|json)\"\n```\n\n### Generate detailed report:\n```bash\n/truth-verification:cite-check --paper paper/manuscript.md \\\n  --report-format detailed --output citation-report.txt\n```\n\n---\n\n## What Happens\n\n1. **Paper Scanning**:\n   - Reads paper file (supports .md, .tex, .txt formats)\n   - Extracts all potential data references using citation patterns\n   - Handles different reference formats:\n     - File paths: `data/raw/dataset.csv`\n     - Labeled references: `file:data/raw/dataset.csv`\n     - URL references: `https://example.com/data.csv`\n     - Line citations: `data/raw/dataset.csv:line42`\n\n2. **Reference Validation**:\n   - Checks each referenced file exists in manifest\n   - Verifies file hashes if available\n   - Identifies missing or unregistered files\n   - Flags external URLs (may be outside manifest scope)\n\n3. **Orphaned Data Detection** (optional):\n   - Scans manifest for registered files\n   - Checks if each file is referenced in papers\n   - Reports data files never cited anywhere\n   - Suggests either removing data or citing it\n\n4. **Citation Index Creation**:\n   - Builds index of all citations and their locations\n   - Records line numbers and context\n   - Groups citations by file\n\n5. **Report Generation**:\n   - Summary: Total files referenced, verified, unresolved\n   - Detailed list of all citations\n   - Issues found (missing refs, unregistered files)\n   - Recommendations for fixing issues\n\n6. **Manifest Update** (optional):\n   - Adds citations array to manifest (if `--update-manifest`)\n   - Records verified citations with timestamps\n   - Tracks which data appears in which papers\n\n---\n\n## Output Examples\n\n### All citations verified:\n```\n‚úì Citation Validation Complete\n  Paper: paper/manuscript.md\n  Total references: 12\n  Verified: 12\n  Unregistered: 0\n  External: 0\n  Status: ALL CITATIONS VERIFIED ‚úì\n\nVerified Citations:\n  ‚úì data/raw/dataset.csv (line 23)\n  ‚úì data/raw/metrics.csv (line 45)\n  ‚úì data/cleaned/results.csv (line 78)\n  ... (9 more verified)\n```\n\n### Issues found:\n```\n‚ö† Citation Validation Complete (with issues)\n  Paper: paper/manuscript.md\n  Total references: 12\n  Verified: 10\n  Unregistered: 2\n  External: 0\n\nUnregistered References:\n  ‚ùå data/raw/missing_dataset.csv (line 89)\n     Status: FILE NOT IN MANIFEST\n     Suggestion: Register file with /truth-verification:register\n\n  ‚ùå results/analysis_output.csv (line 102)\n     Status: FILE NOT IN MANIFEST\n     Suggestion: Register file or correct citation\n\nOrphaned Data (in manifest but not cited):\n  ‚ö† data/raw/old_backup.csv - Never cited in papers\n     Suggestion: Remove if obsolete, or add citation if needed\n```\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"Paper file not found\" | File path doesn't exist | Verify paper file location |\n| \"No citations found\" | Paper has no data references | Check citation patterns match your format |\n| \"Invalid manifest\" | `.truth/manifest.json` corrupted | Restore from backup or reinitialize |\n| \"Cannot read paper file\" | Permission denied or corrupted file | Check file permissions |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Command returns citation validation summary\n2. All referenced files exist in manifest\n3. If unresolved references found, displays suggestions for fixing them\n4. Orphaned data identified (if `--report-orphaned-data`)\n5. Citation index created in manifest (if `--update-manifest`)\n\n---\n\n## Citation Patterns\n\nSupported formats:\n- **File paths**: `data/raw/dataset.csv`\n- **Labeled refs**: `data:raw/dataset.csv` or `file:data/raw/dataset.csv`\n- **Line citations**: `data/raw/dataset.csv:42` (reference specific line)\n- **Markdown links**: `[Dataset](data/raw/dataset.csv)`\n- **LaTeX citations**: `\\cite{data/raw/dataset.csv}`\n- **URLs**: `https://example.com/data.csv`\n\nDefault patterns defined in `config/config.json`:\n```json\n\"citation_patterns\": [\n  \"file:path/to/file\",\n  \"data/[a-zA-Z0-9_-]+\\\\.csv\",\n  \"codes/[a-zA-Z0-9_-]+\\\\.py\",\n  \"paper/figures/[a-zA-Z0-9_-]+\\\\.(png|jpg|pdf)\"\n]\n```\n\n---\n\n## Strict Mode\n\nIn strict mode (`--strict-mode`):\n- Command fails if any references unresolved\n- Returns non-zero exit code\n- Suitable for CI/CD pipelines and publication automation\n- Forces all citations to be verified before proceeding\n\n```bash\n/truth-verification:cite-check --paper manuscript.md --strict-mode\n# Returns error if any citations unverified\n```\n\n---\n\n## Advanced Scenarios\n\n### Check multiple papers:\n```bash\n/truth-verification:cite-check --directory paper/manuscripts --recursive\n```\n\nValidates all papers in directory and subdirectories.\n\n### Cross-reference papers (data used in multiple papers):\n```bash\n/truth-verification:cite-check --directory paper/manuscripts \\\n  --cross-reference --report-data-usage\n```\n\nShows which data files appear in which papers.\n\n### Audit trail (when data was cited):\n```bash\n/truth-verification:cite-check --paper manuscript.md \\\n  --include-audit-trail\n```\n\nRecords when each citation was first verified.\n\n### Fix citations automatically:\n```bash\n/truth-verification:cite-check --paper manuscript.md \\\n  --auto-fix-paths\n```\n\nAttempts to find correctly spelled file paths and suggest corrections.\n\n---\n\n## Integration with verification workflow\n\n**Complete validation workflow**:\n```bash\n# 1. Initialize\n/truth-verification:init\n\n# 2. Register all data\n/truth-verification:register --recursive --dir data/\n\n# 3. Track script execution\n/truth-verification:track --script codes/analysis.py ...\n\n# 4. Verify data integrity\n/truth-verification:verify\n\n# 5. Check paper citations\n/truth-verification:cite-check --paper paper/manuscript.md --strict-mode\n\n# 6. Generate reproducibility report\n/truth-verification:reproduce --generate-report\n\n# 7. Publish with confidence\n```\n\n---\n\n## Next Steps\n\nAfter validating citations:\n\n1. **Fix unverified references**: Register missing files or correct citations\n2. **Remove orphaned data**: Delete unused files or add citations\n3. **Check reproducibility**: `/truth-verification:reproduce` to validate complete chain\n4. **Generate audit**: `/truth-verification:audit` before publication\n5. **Archive manifest**: Commit `.truth/` to version control with manuscript\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Register data files\n- `/truth-verification:track` - Record script execution\n- `/truth-verification:reproduce` - Validate reproducibility (Phase 3)\n- `/truth-verification:audit` - Generate comprehensive audit reports (Phase 4)\n- `/truth-verification:verify` - Check data integrity\n"
              },
              {
                "name": "/init",
                "description": null,
                "path": "plugins/truth-verification/commands/init.md",
                "frontmatter": null,
                "content": "# truth-verification:init\n\n**Goal**: Initialize the `.truth/` directory infrastructure and create an empty manifest file for tracking data provenance and analysis workflows.\n\n**When to use**: Run this command at the start of any research project or when setting up truth-verification for an existing project.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure you are in your project's root directory\n2. Have write permissions for creating the `.truth/` directory\n3. Verify the project doesn't already have a `.truth/` directory (unless using `--force`)\n\n---\n\n## Execution\n\n### Basic usage:\n```bash\n/truth-verification:init\n```\n\nCreates `.truth/` directory with default manifest structure.\n\n### Force overwrite existing manifest:\n```bash\n/truth-verification:init --force\n```\n\nRecreates manifest even if `.truth/` already exists (backs up old manifest as `.truth/manifest.backup.json`).\n\n### Custom manifest location:\n```bash\n/truth-verification:init --manifest-dir .verification\n```\n\nUses `.verification/` instead of `.truth/` (useful for teams with naming preferences).\n\n---\n\n## What Happens\n\n1. **Creates directory structure**:\n   ```\n   .truth/\n   ‚îú‚îÄ‚îÄ manifest.json           # Central registry\n   ‚îú‚îÄ‚îÄ hashes/                 # Hash records (populated by register)\n   ‚îú‚îÄ‚îÄ logs/                   # Execution logs\n   ‚îî‚îÄ‚îÄ reports/                # Generated reports\n   ```\n\n2. **Creates empty manifest.json** with template structure:\n   ```json\n   {\n     \"version\": \"1.0.0\",\n     \"initialized_at\": \"2025-12-15T10:30:00Z\",\n     \"project_root\": \"/path/to/project\",\n     \"data_sources\": [],\n     \"analysis_scripts\": [],\n     \"results\": [],\n     \"dependencies\": []\n   }\n   ```\n\n3. **Adds `.truth/` to .gitignore** (creates .gitignore if needed):\n   - Prevents accidental commits of manifest changes during local work\n   - Optional: Use `--include-gitignore` to NOT add to .gitignore (for shared teams)\n\n4. **Validates setup**:\n   - Confirms all directories created\n   - Verifies manifest.json is valid JSON\n   - Tests write permissions\n   - Shows success message or errors\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"Permission denied\" | No write access to project dir | Check directory permissions: `chmod u+w .` |\n| \".truth directory already exists\" | Init already run | Use `--force` to overwrite, or use different `--manifest-dir` |\n| \"Not in a project root\" | Current dir isn't a valid project | Verify you're in project root (`ls project.yml` or `ls .claude-plugin/`) |\n| \"Invalid manifest JSON created\" | File system issue | Contact support; manifest template is hard-coded and should never fail |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Directory `.truth/` exists with subdirectories (manifest, hashes, logs, reports)\n2. File `.truth/manifest.json` contains valid JSON with empty data_sources array\n3. `.gitignore` includes `.truth/` line (unless `--include-gitignore=false`)\n4. Command output shows: `‚úì Initialized .truth/ directory at /path/to/project/.truth`\n\n---\n\n## Next Steps\n\nAfter initialization:\n\n1. **Register existing data**: `/truth-verification:register --recursive --dir data/raw`\n2. **Check status**: `/truth-verification:status`\n3. **Track analysis**: `/truth-verification:track --script codes/analysis.py ...` (after running scripts)\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Add data files to manifest after init\n- `/truth-verification:status` - View current verification state\n- `/truth-verification:verify` - Check if any registered files have been modified\n"
              },
              {
                "name": "/register",
                "description": null,
                "path": "plugins/truth-verification/commands/register.md",
                "frontmatter": null,
                "content": "# truth-verification:register\n\n**Goal**: Register data source files with SHA256 hashing to establish baseline integrity and enable change detection.\n\n**When to use**: Before starting data analysis, register all raw data files. Register outputs after completion to create dependencies for reproducibility tracking.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists (run `/truth-verification:init` if needed)\n2. Identify all data files to register\n3. Have read access to all files being registered\n4. For large files (>100MB), allow extra time for hashing\n\n---\n\n## Execution\n\n### Register a single file:\n```bash\n/truth-verification:register --file data/raw/dataset.csv\n```\n\n### Register all files in a directory (recursive):\n```bash\n/truth-verification:register --recursive --dir data/raw\n```\n\n### Register with pattern filtering:\n```bash\n/truth-verification:register --recursive --dir data/ --include \"*.csv\" --exclude \"*-temp*\"\n```\n\n### Register and add metadata:\n```bash\n/truth-verification:register --file data/raw/dataset.csv --source \"downloaded from https://example.com/data\" --description \"Q4 2025 sales records\"\n```\n\n### Dry run (preview without committing):\n```bash\n/truth-verification:register --recursive --dir data/ --dry-run\n```\n\nShows which files would be registered without modifying manifest.\n\n---\n\n## What Happens\n\n1. **File Discovery**:\n   - Locates all files matching criteria\n   - Skips symbolic links and directories (unless `--follow-symlinks`)\n   - Skips binary files by default (unless `--include-binary`)\n\n2. **Hash Calculation**:\n   - Computes SHA256 for each file\n   - Uses streaming for files >10MB to minimize memory use\n   - Stores hash at `.truth/hashes/{filename}.sha256`\n\n3. **Metadata Collection**:\n   - File size, modification time, permissions\n   - Optional: source URL, description, author\n   - Optional: file tags for organization (e.g., \"raw\", \"external\", \"validated\")\n\n4. **Manifest Update**:\n   - Adds entry to `.truth/manifest.json` under `data_sources`\n   - Creates entry structure:\n   ```json\n   {\n     \"path\": \"data/raw/dataset.csv\",\n     \"hash\": \"a7b3f...\",\n     \"size_bytes\": 1048576,\n     \"registered_at\": \"2025-12-15T10:30:00Z\",\n     \"source\": \"original source info\",\n     \"tags\": [\"raw\", \"external\"]\n   }\n   ```\n\n5. **Logging**:\n   - Records registration in `.truth/logs/register.log`\n   - Shows summary: \"Registered 15 files, 2 skipped, total 487 MB\"\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"File not found: ...\" | Path doesn't exist | Verify file path with `ls -la` |\n| \"Permission denied\" | Can't read file | Check read permissions: `chmod u+r filename` |\n| \"Invalid manifest\" | `.truth/manifest.json` corrupted | Restore with `/truth-verification:init --force` |\n| \"File already registered\" | File exists in manifest | Use `--update` to re-hash and update entry |\n| \"Hash mismatch during re-registration\" | File was modified since last registration | Use `--force` to overwrite hash, or investigate the modification |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Output shows count of registered files: `‚úì Registered 15 files`\n2. New entries appear in `.truth/manifest.json` under `data_sources`\n3. Hash files created in `.truth/hashes/` directory\n4. Log entry in `.truth/logs/register.log` shows timestamp and file list\n\n---\n\n## Advanced Options\n\n### Register with backup:\n```bash\n/truth-verification:register --file data/raw/dataset.csv --backup-original\n```\n\nCreates `.truth/backups/dataset.csv.original` for future recovery if needed.\n\n### Register with external source tracking:\n```bash\n/truth-verification:register --file data/raw/download.zip --source \"AWS S3: s3://bucket/archive.zip\" --source-hash \"s3:md5:abcd1234...\"\n```\n\nEnables verification that original source hasn't changed (if source supports hash queries).\n\n### Register outputs as results:\n```bash\n/truth-verification:register --file data/cleaned/results.csv --tag \"output\" --parent-script codes/analysis.py\n```\n\nLinks result file to generating script for dependency tracking.\n\n---\n\n## Next Steps\n\nAfter registration:\n\n1. **Proceed with analysis**: Run your data processing scripts\n2. **Track execution**: `/truth-verification:track --script codes/analysis.py --inputs data/raw/dataset.csv --outputs data/cleaned/results.csv`\n3. **Verify later**: `/truth-verification:verify --file data/raw/dataset.csv` to check if data changed\n\n---\n\n## Related Commands\n\n- `/truth-verification:init` - Initialize manifest before registering\n- `/truth-verification:verify` - Check if registered files have been modified\n- `/truth-verification:track` - Record how registered files are used in scripts\n- `/truth-verification:reproduce` - Validate complete dependency chains\n"
              },
              {
                "name": "/reproduce",
                "description": null,
                "path": "plugins/truth-verification/commands/reproduce.md",
                "frontmatter": null,
                "content": "# truth-verification:reproduce\n\n**Goal**: Validate that research results are reproducible by verifying the complete dependency chain from source data through analysis scripts to final outputs.\n\n**When to use**: Before publishing or sharing research, ensure all data and scripts form a complete reproducible chain.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists with registered files\n2. Register data sources: `/truth-verification:register --recursive --dir data/`\n3. Track script execution: `/truth-verification:track --script codes/...`\n4. Optional: Run `/truth-verification:verify` to ensure current data integrity\n\n---\n\n## Execution\n\n### Check reproducibility of complete project:\n```bash\n/truth-verification:reproduce\n```\n\n### Generate detailed reproducibility report:\n```bash\n/truth-verification:reproduce --generate-report --output reproduce-report.md\n```\n\n### Show only reproducibility score:\n```bash\n/truth-verification:reproduce --score-only\n```\n\n### Check reproducibility for specific output:\n```bash\n/truth-verification:reproduce --target data/cleaned/results.csv\n```\n\n### Trace backwards from output to all inputs:\n```bash\n/truth-verification:reproduce --trace-back data/cleaned/results.csv\n```\n\n### Export report as JSON for tools:\n```bash\n/truth-verification:reproduce --generate-report --format json --output reproduce.json\n```\n\n---\n\n## What Happens\n\n1. **Dependency Graph Analysis**:\n   - Loads dependency graph from manifest\n   - Identifies all data ‚Üí script ‚Üí result chains\n   - Detects cycles or broken links\n   - Validates graph structure integrity\n\n2. **Data Validation**:\n   - Verifies all input files exist and match registered hashes\n   - Checks all intermediate files are registered\n   - Validates output files exist and are accessible\n   - Reports missing or modified files\n\n3. **Script Traceability**:\n   - Confirms all scripts are registered and accessible\n   - Verifies script hashes haven't changed\n   - Validates execution parameters are recorded\n   - Reports any scripts with missing metadata\n\n4. **Reproducibility Scoring**:\n   - Calculates score based on configuration weights:\n     - Input integrity (40 points): All inputs verified\n     - Script traceability (30 points): All scripts recorded\n     - Output attribution (20 points): All outputs linked to scripts\n     - No orphaned data (10 points): No unregistered files\n   - Produces score 0-100\n   - Rates as: Poor (<60), Fair (60-75), Good (75-90), Excellent (>90)\n\n5. **Dependency Tracing**:\n   - Traces complete lineage for each output\n   - Shows which inputs contributed to which outputs\n   - Identifies redundant files or unused data\n   - Detects missing intermediate steps\n\n6. **Report Generation**:\n   - Summary: Overall reproducibility score and rating\n   - Detailed chain breakdown: Which data ‚Üí which script ‚Üí which output\n   - Issues found: Missing files, broken chains, unregistered data\n   - Recommendations: What needs to be added for reproducibility\n   - Timeline: When each step was executed\n\n7. **Manifest Update**:\n   - Records reproducibility score in manifest\n   - Updates `reproducibility_status` field\n   - Adds timestamp of reproducibility check\n\n---\n\n## Output Examples\n\n### Excellent reproducibility:\n```\n‚úì Reproducibility Validation Complete\n  Status: EXCELLENT (95/100)\n\nSummary:\n  Input integrity: ‚úì 100% (5/5 files verified)\n  Script traceability: ‚úì 100% (3/3 scripts recorded)\n  Output attribution: ‚úì 100% (8/8 outputs linked)\n  No orphaned data: ‚úì 100% (0 orphaned files)\n\nReproducibility Score: 95/100 - EXCELLENT\n  Can be reproduced with high confidence.\n  Ready for publication.\n\nDependency Chains (3 found):\n  ‚úì Chain 1: data/raw/raw1.csv\n              ‚Üí codes/preprocessing.py\n              ‚Üí data/cleaned/merged.csv\n              ‚Üí codes/analysis.py\n              ‚Üí paper/figures/results.png (8 steps, all traceable)\n\n  ‚úì Chain 2: data/raw/metrics.csv\n              ‚Üí codes/merge.py\n              ‚Üí data/cleaned/metrics.csv (2 steps, all traceable)\n\n  ‚úì Chain 3: data/cleaned/merged.csv + codes/statistical.py\n              ‚Üí paper/manuscripts/results.md (complete)\n```\n\n### Issues found:\n```\n‚ö† Reproducibility Validation Complete (with issues)\n  Status: FAIR (68/100)\n\nSummary:\n  Input integrity: ‚ö† 80% (4/5 files verified, 1 MODIFIED)\n  Script traceability: ‚úì 100% (3/3 scripts recorded)\n  Output attribution: ‚ö† 75% (6/8 outputs linked)\n  No orphaned data: ‚ùå 0% (2 orphaned files detected)\n\nReproducibility Score: 68/100 - FAIR\n  Can be reproduced but some issues exist.\n  Recommend fixing before publication.\n\nIssues Detected:\n  ‚ùå Input file modified: data/raw/dataset.csv\n     Expected hash: a7b3f8d9e2c1...\n     Current hash:  c9d5b1a4e2f3...\n     Impact: Results from this file may not be reproducible\n\n  ‚ùå Output not attributed: results/preliminary.csv\n     Not linked to any script in manifest\n     Suggestion: Track the script that generated it\n\n  ‚ö† Orphaned data: data/raw/backup_2025-12-01.csv\n     Registered but never used in any script\n     Suggestion: Remove or cite in paper\n\nRecommendations:\n  1. Investigate modified input: data/raw/dataset.csv\n  2. Re-register modified file or restore from backup\n  3. Track the script that generated results/preliminary.csv\n  4. Remove orphaned file: data/raw/backup_2025-12-01.csv\n  5. Re-run /reproduce to validate fixes\n```\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"No dependencies found\" | No scripts tracked yet | Run `/truth-verification:track` for each script |\n| \"Input file missing\" | Referenced file no longer exists | Restore from backup or remove from manifest |\n| \"Broken dependency chain\" | Script or output file missing | Register missing files or re-track scripts |\n| \"Invalid manifest\" | `.truth/manifest.json` corrupted | Restore from backup or reinitialize |\n\n---\n\n## Reproducibility Score Breakdown\n\n**Input Integrity (40 points)**:\n- 40 pts: All registered input files exist and match hashes\n- 30 pts: 75%+ of input files verified\n- 20 pts: 50%+ of input files verified\n- 10 pts: <50% of input files verified\n- 0 pts: No input files verified or verified = false\n\n**Script Traceability (30 points)**:\n- 30 pts: All scripts recorded with parameters and timestamps\n- 20 pts: 75%+ of scripts have metadata\n- 10 pts: 50%+ of scripts have metadata\n- 5 pts: <50% of scripts have metadata\n- 0 pts: No scripts tracked\n\n**Output Attribution (20 points)**:\n- 20 pts: All outputs linked to generating scripts\n- 15 pts: 75%+ of outputs attributed\n- 10 pts: 50%+ of outputs attributed\n- 5 pts: 25%+ of outputs attributed\n- 0 pts: No outputs attributed\n\n**No Orphaned Data (10 points)**:\n- 10 pts: All registered data files used in scripts\n- 5 pts: 50%+ of data files used\n- 0 pts: <50% of data files used or many orphaned files\n\n---\n\n## Reproducibility Levels\n\n| Score | Level | Meaning | Action |\n|-------|-------|---------|--------|\n| 90-100 | Excellent | Fully reproducible, ready for publication | Publish with confidence |\n| 75-89 | Good | Mostly reproducible, minor issues | Address recommendations before publication |\n| 60-74 | Fair | Partially reproducible, significant issues | Fix issues before sharing |\n| <60 | Poor | Barely reproducible, major gaps | Major work needed to ensure reproducibility |\n\n---\n\n## Advanced Scenarios\n\n### Trace specific output backwards:\n```bash\n/truth-verification:reproduce --trace-back data/cleaned/results.csv\n```\n\nShows complete lineage: which inputs ‚Üí which scripts ‚Üí this output.\n\n### Check reproducibility for publication:\n```bash\n/truth-verification:reproduce --generate-report \\\n  --include-recommendations \\\n  --format markdown \\\n  --output publication-audit.md\n```\n\nGenerates publication-ready documentation.\n\n### Compare reproducibility over time:\n```bash\n/truth-verification:reproduce --compare-with-date \"2025-12-01T00:00:00Z\"\n```\n\nShows how reproducibility score changed since specific date.\n\n### Validate for specific paper:\n```bash\n/truth-verification:reproduce --target paper/manuscripts/main.md \\\n  --include-cited-data-only\n```\n\nOnly validates data cited in the specified paper.\n\n---\n\n## Reproducibility Workflow\n\n**Before Publication**:\n```bash\n# 1. Register all data\n/truth-verification:register --recursive --dir data/\n\n# 2. Track all scripts\n/truth-verification:track --script codes/preprocessing.py ...\n/truth-verification:track --script codes/analysis.py ...\n/truth-verification:track --script codes/visualization.py ...\n\n# 3. Verify data integrity\n/truth-verification:verify\n\n# 4. Check reproducibility\n/truth-verification:reproduce --generate-report\n\n# 5. Fix any issues found\n# ... investigate and fix issues ...\n\n# 6. Re-validate\n/truth-verification:reproduce --score-only\n\n# 7. Publish when score > 90\n```\n\n---\n\n## Integration with other commands\n\n**Complete publication workflow**:\n```bash\n/truth-verification:init\n/truth-verification:register --recursive --dir data/\n/truth-verification:track --script codes/...\n/truth-verification:verify\n/truth-verification:reproduce --generate-report  # This command\n/truth-verification:cite-check --paper paper/manuscript.md --strict-mode\n/truth-verification:audit --include-timeline\n# Ready for publication!\n```\n\n---\n\n## Next Steps\n\nAfter validation:\n\n1. **If score > 90**: Proceed to citation validation and publication\n2. **If score 75-90**: Address recommendations before publishing\n3. **If score < 75**: Fix critical issues before sharing\n4. **Generate audit**: `/truth-verification:audit` for final report\n5. **Archive**: Commit `.truth/` with manuscript in version control\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Register data files\n- `/truth-verification:track` - Record script execution\n- `/truth-verification:verify` - Check data integrity\n- `/truth-verification:cite-check` - Validate citations (Phase 3)\n- `/truth-verification:audit` - Generate audit reports (Phase 4)\n"
              },
              {
                "name": "/status",
                "description": null,
                "path": "plugins/truth-verification/commands/status.md",
                "frontmatter": null,
                "content": "# truth-verification:status\n\n**Goal**: Display a comprehensive verification status dashboard showing project integrity, statistics, and recommendations for research continuity.\n\n**When to use**: Run before major milestones (paper submission, milestone checkpoints) or as a quick health check (daily/weekly).\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists (run `/truth-verification:init` if needed)\n2. Register files to track (run `/truth-verification:register` if manifest is empty)\n3. Optional: Run `/truth-verification:verify` first for current integrity status\n\n---\n\n## Execution\n\n### Quick status overview:\n```bash\n/truth-verification:status\n```\n\nShows dashboard with essential statistics.\n\n### Detailed status with recommendations:\n```bash\n/truth-verification:status --detail-level full\n```\n\nIncludes detailed statistics, issue analysis, and actionable recommendations.\n\n### Export status to file:\n```bash\n/truth-verification:status --output status-report.txt\n```\n\n### Generate JSON status for tools/scripts:\n```bash\n/truth-verification:status --format json --output status.json\n```\n\n### Focus on specific category:\n```bash\n/truth-verification:status --category data\n```\n\nShows only data source statistics.\n\n---\n\n## What Happens\n\n1. **Manifest Analysis**:\n   - Reads `.truth/manifest.json`\n   - Parses all data sources, scripts, results, and dependencies\n   - Calculates comprehensive statistics\n\n2. **Status Checks**:\n   - Verifies .truth/ directory is accessible\n   - Checks for recent verification logs\n   - Detects unregistered files in monitored directories\n   - Computes dependency graph completeness\n\n3. **Statistics Computation**:\n   - Counts registered items by category\n   - Calculates total data size\n   - Measures time since last activities\n   - Computes integrity and reproducibility metrics\n\n4. **Dashboard Rendering**:\n   - Displays human-readable status summary\n   - Shows color-coded indicators (‚úì green, ‚ö† yellow, ‚ùå red)\n   - Lists recommendations and next actions\n\n---\n\n## Sample Output\n\n```\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n                    TRUTH-VERIFICATION STATUS\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nPROJECT: test-verification\nROOT: /path/to/test-verification\nSTATUS: ‚úì HEALTHY\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDATA SOURCES\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Registered files:     15\n  Total size:          487 MB\n  Integrity status:     ‚úì All verified (as of 2 hours ago)\n  Last verification:    2025-12-15T15:00:00Z\n  Files modified:       0\n  Files missing:        0\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nANALYSIS SCRIPTS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Tracked scripts:      3\n  Total executions:     12\n  Last execution:       2025-12-15T14:30:00Z (analysis.py)\n  Execution success:    100%\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRESULTS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Generated results:    8\n  Total output size:    156 MB\n  All results traced:   ‚úì Yes\n  Reproducibility:      95/100 (Excellent)\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDEPENDENCIES\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Total dependencies:   18\n  Complete chains:      ‚úì All intact\n  Broken links:         0\n  Orphaned data:        0 files\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nINTEGRATION STATUS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  research-memory:      ‚úì Integrated\n  project-management:   ‚úì Integrated\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRECOMMENDATIONS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚úì All systems nominal. Ready for paper submission.\n‚úì Consider archiving .truth/ with manuscript for reproducibility.\n```\n\n---\n\n## Status Categories\n\n### Data Sources\n- Number of registered files\n- Total data volume\n- Integrity verification status\n- Time since last verification\n- Any missing or modified files\n\n### Analysis Scripts\n- Number of tracked scripts\n- Execution history\n- Execution success rate\n- Latest execution details\n\n### Results\n- Number of generated results\n- Output volume\n- Traceability to source scripts\n- Reproducibility scoring\n\n### Dependencies\n- Total number of tracked dependencies\n- Completeness of dependency graph\n- Detection of orphaned or unused files\n- Validation of chains\n\n### Integration Status\n- research-memory connection status\n- project-management validation status\n- Any integration warnings or issues\n\n---\n\n## Interpretation Guide\n\n### ‚úì HEALTHY Status\nAll checks pass:\n- All data files integrity verified\n- All dependencies complete\n- No orphaned files\n- High reproducibility score (>85)\n\n‚Üí Safe to proceed with analysis or publication.\n\n### ‚ö† CAUTION Status\nSome issues detected:\n- Some data files not recently verified\n- Some minor dependency issues\n- Low-medium reproducibility score (60-85)\n\n‚Üí Investigate recommendations before major decisions.\n\n### ‚ùå CRITICAL Status\nSerious issues detected:\n- Missing data files\n- Broken dependency chains\n- Data integrity violations\n- Very low reproducibility score (<60)\n\n‚Üí Must resolve before proceeding with analysis or publication.\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"Manifest not found\" | `.truth/manifest.json` doesn't exist | Run `/truth-verification:init` first |\n| \"Manifest corrupted\" | JSON parsing failed | Restore from backup or reinitialize |\n| \"No data registered\" | Manifest has empty data_sources array | Register files: `/truth-verification:register ...` |\n| \"Permission denied\" | Can't read manifest or logs | Check file permissions in `.truth/` |\n\n---\n\n## Advanced Options\n\n### Show status for time period:\n```bash\n/truth-verification:status --since \"2025-12-15T00:00:00Z\"\n```\n\nShows activities and changes since specified time.\n\n### Compare with previous status:\n```bash\n/truth-verification:status --compare-with-previous-day\n```\n\nHighlights what changed in past 24 hours.\n\n### Generate status with historical trend:\n```bash\n/truth-verification:status --include-historical-trend\n```\n\nShows how integrity has changed over time.\n\n---\n\n## Common Status Checks\n\n**Before Paper Submission**:\n```bash\n/truth-verification:status --detail-level full\n```\n\nEnsure reproducibility score is excellent (>90).\n\n**Weekly Health Check**:\n```bash\n/truth-verification:status\n```\n\nQuick overview to catch issues early.\n\n**Collaboration Checkpoint**:\n```bash\n/truth-verification:status --format json\n```\n\nShare structured status with team members.\n\n---\n\n## Next Steps\n\nBased on status output:\n\n**If HEALTHY**:\n- Proceed with analysis or publication\n- Consider archiving `.truth/` with results\n\n**If CAUTION**:\n- Run `/truth-verification:verify` to get detailed results\n- Address recommendations listed\n- Retry `/truth-verification:status` to confirm resolution\n\n**If CRITICAL**:\n- Immediately run `/truth-verification:verify --fail-fast`\n- Investigate issues with `/truth-verification:audit`\n- Restore data from backup if needed\n\n---\n\n## Related Commands\n\n- `/truth-verification:verify` - Detailed integrity checking\n- `/truth-verification:audit` - Comprehensive audit report\n- `/truth-verification:reproduce` - Check reproducibility in detail\n- `/truth-verification:init` - Initialize if status fails\n"
              },
              {
                "name": "/track",
                "description": null,
                "path": "plugins/truth-verification/commands/track.md",
                "frontmatter": null,
                "content": "# truth-verification:track\n\n**Goal**: Track the execution of analysis scripts and record data dependencies to enable reproducibility verification and analysis traceability.\n\n**When to use**: After running analysis scripts, track their execution to document data lineage and enable reproducibility validation.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists (run `/truth-verification:init` if needed)\n2. Register input data files first (run `/truth-verification:register` for all source data)\n3. Have the analysis script file available\n4. Know which files are inputs and outputs for the script\n5. Optional: Note any parameters used in the script execution\n\n---\n\n## Execution\n\n### Track script execution with inputs and outputs:\n```bash\n/truth-verification:track --script codes/analysis.py --inputs data/raw/dataset.csv --outputs data/cleaned/results.csv\n```\n\n### Track with multiple inputs:\n```bash\n/truth-verification:track --script codes/preprocessing.py --inputs data/raw/raw1.csv,data/raw/raw2.csv --outputs data/cleaned/merged.csv\n```\n\n### Track with execution parameters:\n```bash\n/truth-verification:track --script codes/analysis.py \\\n  --inputs data/raw/dataset.csv \\\n  --outputs data/cleaned/results.csv \\\n  --parameters \"threshold=0.75,method=zscore\"\n```\n\n### Track with execution duration:\n```bash\n/truth-verification:track --script codes/analysis.py \\\n  --inputs data/raw/dataset.csv \\\n  --outputs data/cleaned/results.csv \\\n  --execution-duration 45\n```\n\n### Auto-register outputs if not yet registered:\n```bash\n/truth-verification:track --script codes/analysis.py \\\n  --inputs data/raw/dataset.csv \\\n  --outputs data/cleaned/results.csv \\\n  --auto-register-outputs\n```\n\n---\n\n## What Happens\n\n1. **Script Validation**:\n   - Verifies script file exists and is readable\n   - Extracts script hash (SHA256) for tracking\n   - Validates script format based on extension\n\n2. **Input Verification**:\n   - Checks all input files are registered in manifest\n   - Verifies file hashes match registered values\n   - Warns if input files have been modified since registration\n\n3. **Output Handling**:\n   - Checks if output files exist\n   - Registers outputs if not already registered (with `--auto-register-outputs`)\n   - Records output file paths and hashes\n\n4. **Dependency Graph Creation**:\n   - Creates edges in dependency graph:\n     - Input files ‚Üí Script (relationship: \"input\")\n     - Script ‚Üí Output files (relationship: \"produces\")\n   - Links previous script outputs to current script inputs\n   - Builds complete lineage chain\n\n5. **Manifest Update**:\n   - Adds entry to `analysis_scripts` array\n   - Adds entries to `dependencies` array\n   - Records execution metadata in manifest\n   - Updates `last_updated` timestamp\n\n6. **Research Memory Integration** (if enabled):\n   - Logs execution event to `memory/devlog.md` (if using research-memory skill)\n   - Records: script name, inputs, outputs, execution time\n   - Tags entry with `#data-tracking`\n\n7. **Logging**:\n   - Records event in `.truth/logs/track.log`\n   - Shows summary of tracked dependencies\n\n---\n\n## Output Examples\n\n### Successful tracking:\n```\n‚úì Tracked script execution\n  Script: codes/analysis.py\n  Inputs: data/raw/dataset.csv (verified ‚úì)\n  Outputs: data/cleaned/results.csv (registered & hashed)\n  Dependencies: 2 edges added\n  Duration: 45 seconds\n  Status: Complete\n```\n\n### With warnings:\n```\n‚ö† Tracked script execution (with warnings)\n  Script: codes/analysis.py\n  Inputs: data/raw/dataset.csv (MODIFIED since registration!)\n    Expected hash: a7b3f8d9...\n    Current hash:  c9d5b1a4...\n  Outputs: data/cleaned/results.csv (registered & hashed)\n  Dependencies: 2 edges added\n\n  Warning: Input data modified! Reproducibility may be affected.\n```\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"Script file not found\" | Script path doesn't exist | Verify script file path |\n| \"Input file not registered\" | Input file not in manifest | Run `/truth-verification:register --file <input>` first |\n| \"Output file not found\" | Output file doesn't exist | Run script first to generate output, or use `--dry-run` to preview |\n| \"Hash mismatch on input\" | Input file was modified | Investigate modification or re-register with `--update` |\n| \"Invalid manifest\" | `.truth/manifest.json` corrupted | Restore from backup or reinitialize |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Command returns summary with all dependencies listed\n2. Manifest contains new entries:\n   - `analysis_scripts`: New script entry with execution metadata\n   - `dependencies`: New edges linking inputs ‚Üí script ‚Üí outputs\n3. `.truth/logs/track.log` contains execution record\n4. If research-memory enabled: New entry in `memory/devlog.md`\n\n---\n\n## Dependency Graph Structure\n\nThe manifest's `dependencies` array tracks relationships:\n\n```json\n{\n  \"id\": \"dep_001\",\n  \"type\": \"data_to_script\",\n  \"from\": \"data/raw/dataset.csv\",\n  \"to\": \"codes/analysis.py\",\n  \"relationship\": \"input\",\n  \"created_at\": \"2025-12-15T14:30:00Z\"\n}\n```\n\nGraph types:\n- **data_to_script**: Data file feeds into a script\n- **script_to_result**: Script produces a result\n- **result_to_script**: Previous result feeds into current script\n\n---\n\n## Advanced Scenarios\n\n### Track with dry-run (preview dependencies):\n```bash\n/truth-verification:track --script codes/analysis.py \\\n  --inputs data/raw/dataset.csv \\\n  --outputs data/cleaned/results.csv \\\n  --dry-run\n```\n\nShows what dependencies would be created without modifying manifest.\n\n### Track with external input (outside project):\n```bash\n/truth-verification:track --script codes/download.py \\\n  --external-inputs \"https://api.example.com/data\" \\\n  --outputs data/raw/downloaded.csv\n```\n\nRecords external data sources that scripts depend on.\n\n### Track with full lineage history:\n```bash\n/truth-verification:track --script codes/pipeline.py \\\n  --inputs data/raw/raw.csv \\\n  --outputs data/final/results.csv \\\n  --include-intermediate results/step1.csv,results/step2.csv\n```\n\nDocuments all intermediate processing steps for complete traceability.\n\n### Check script before tracking:\n```bash\n/truth-verification:track --script codes/analysis.py \\\n  --validate-inputs \\\n  --validate-outputs\n```\n\nValidates all inputs are registered and all outputs exist before tracking.\n\n---\n\n## Integration with research-memory\n\nWhen research-memory is installed, tracking automatically logs:\n\n**In memory/devlog.md**:\n```markdown\n## 2025-12-15 14:30 - Data Analysis Pipeline #data-tracking\nRan analysis script with:\n- Input: data/raw/dataset.csv (verified ‚úì)\n- Output: data/cleaned/results.csv (456 KB)\n- Duration: 45 seconds\n- Status: Complete\n\nDependency graph updated with 2 edges.\n```\n\nThis creates an audit trail in your research memory.\n\n---\n\n## Next Steps\n\nAfter tracking script execution:\n\n1. **Verify outputs**: `/truth-verification:verify --file data/cleaned/results.csv`\n2. **Track next step**: If outputs feed into another script, track that too\n3. **Check reproducibility**: `/truth-verification:reproduce` (Phase 3) to validate complete chain\n4. **Generate audit**: `/truth-verification:audit` (Phase 4) to create full report\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Register data files and outputs\n- `/truth-verification:verify` - Check data integrity\n- `/truth-verification:reproduce` - Validate reproducibility of complete chains (Phase 3)\n- `/truth-verification:audit` - Generate comprehensive audit reports (Phase 4)\n- `/research-memory:remember` - Log analysis work to memory (when research-memory installed)\n"
              },
              {
                "name": "/verify",
                "description": null,
                "path": "plugins/truth-verification/commands/verify.md",
                "frontmatter": null,
                "content": "# truth-verification:verify\n\n**Goal**: Verify the integrity of registered data files by comparing their current SHA256 hashes against stored baseline values. Detect any unauthorized or accidental modifications.\n\n**When to use**: Verify data regularly (weekly or after major analysis steps) to detect corruption or changes.\n\n---\n\n## Preparation\n\nBefore running this command:\n\n1. Ensure `.truth/manifest.json` exists with registered files\n2. Ensure all registered files still exist on disk\n3. Have read permissions for all files\n4. Allocate time proportional to total data size (large datasets may take minutes)\n\n---\n\n## Execution\n\n### Verify all registered files:\n```bash\n/truth-verification:verify\n```\n\n### Verify specific file:\n```bash\n/truth-verification:verify --file data/raw/dataset.csv\n```\n\n### Verify files in a directory:\n```bash\n/truth-verification:verify --directory data/raw\n```\n\n### Verify with detailed report:\n```bash\n/truth-verification:verify --report-format detailed\n```\n\n### Verify and generate JSON report:\n```bash\n/truth-verification:verify --report-format json --output-file verify-report.json\n```\n\n### Stop on first error (fail-fast mode):\n```bash\n/truth-verification:verify --fail-fast\n```\n\nStops checking after first file integrity failure.\n\n---\n\n## What Happens\n\n1. **Hash Calculation**:\n   - Recomputes SHA256 for each registered file\n   - Uses streaming to minimize memory usage\n   - Shows progress for large files\n\n2. **Comparison**:\n   - Compares current hash against registered baseline\n   - Marks as VERIFIED if hashes match\n   - Marks as MODIFIED if hashes differ\n   - Marks as MISSING if file no longer exists\n\n3. **Status Recording**:\n   - Updates `last_verified` timestamp in manifest for each file\n   - Records verification results in `.truth/logs/verify.log`\n   - Updates overall `integrity_status` in manifest\n\n4. **Report Generation**:\n   - Displays summary: \"15 files verified, 2 modified, 0 missing\"\n   - Lists all files with issues in detail\n   - Suggests next actions if issues found\n\n---\n\n## Output Examples\n\n### Success (all files verified):\n```\n‚úì Verification Complete\n  Files verified: 15\n  Files modified: 0\n  Files missing: 0\n  Status: ALL DATA INTEGRITY VERIFIED ‚úì\n  Last verification: 2025-12-15T15:30:00Z\n```\n\n### Issues found:\n```\n‚ö† Verification Complete\n  Files verified: 15\n  Files modified: 2\n  Files missing: 0\n  Status: INTEGRITY ISSUES DETECTED\n\nModified Files:\n  ‚ùå data/raw/dataset.csv\n     Expected hash: a7b3f8d9e2c1...\n     Current hash:  c9d5b1a4e2f3...\n     Last verified: 2025-12-15T10:30:00Z\n     Time since change: 5 hours\n\n  ‚ùå data/cleaned/results.csv\n     Expected hash: b8c4a9e3d1f2...\n     Current hash:  d1e6c2b5f4a9...\n     Last verified: 2025-12-15T14:30:00Z\n     Time since change: 1 hour\n```\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| \"File not found: ...\" | Registered file no longer exists | Use `--mark-missing` to update manifest, then investigate deletion |\n| \"Permission denied\" | Can't read a file | Check file permissions: `ls -l filename` |\n| \"Invalid manifest\" | `.truth/manifest.json` corrupted | Restore backup: `git checkout .truth/manifest.json` |\n| \"Hash computation timeout\" | Very large file or slow disk | Use `--timeout-minutes 30` to extend limit |\n| \"No files to verify\" | Manifest has empty data_sources array | Register files first: `/truth-verification:register ...` |\n\n---\n\n## Success Indicators\n\nAfter successful execution:\n\n1. Command returns \"ALL DATA INTEGRITY VERIFIED ‚úì\" or lists specific issues\n2. Manifest's `integrity_status` field updated with verification timestamp\n3. All files shown as \"verified\" (or specific issues listed)\n4. `.truth/logs/verify.log` contains verification record\n\n---\n\n## Interpretation of Results\n\n### VERIFIED\nFile's current hash matches registered baseline exactly. Data is unchanged.\n\n### MODIFIED\nFile's hash differs from baseline. This can happen due to:\n- **Legitimate**: Analysis scripts updated the file (expected)\n- **Accidental**: File was edited manually without updating hash\n- **Concerning**: Unauthorized modification or data corruption\n\n### MISSING\nFile was registered but no longer exists on disk. This can happen due to:\n- **Accidental**: File was deleted\n- **Expected**: Data cleanup or restructuring\n- **Concerning**: Data loss or backup failure\n\n---\n\n## Advanced Scenarios\n\n### Update modified files that changed legitimately:\n```bash\n/truth-verification:verify --auto-update-modified --backup-old-hashes\n```\n\nRecalculates hashes for files that changed, and backs up old hashes.\n\n### Check specific time range:\n```bash\n/truth-verification:verify --modified-since \"2025-12-15T12:00:00Z\"\n```\n\nOnly reports files modified since specified timestamp.\n\n### Compare against previous verification:\n```bash\n/truth-verification:verify --compare-with-previous\n```\n\nShows detailed diff of what changed since last verification.\n\n---\n\n## Next Steps\n\nIf all files verify:\n- Continue with analysis confident data is unchanged\n- Run `/truth-verification:status` to view overall health\n- Commit `.truth/manifest.json` to version control\n\nIf issues found:\n- Investigate modified files: `git log -p -- data/raw/dataset.csv`\n- If modification was intentional, re-register: `/truth-verification:register --file data/raw/dataset.csv --update`\n- If modification was accidental, restore from backup or git\n\n---\n\n## Related Commands\n\n- `/truth-verification:register` - Register new files to establish baseline\n- `/truth-verification:status` - View verification status and recommendations\n- `/truth-verification:reproduce` - Check if results are reproducible given current data\n- `/truth-verification:audit` - Generate comprehensive audit report\n"
              }
            ],
            "skills": []
          },
          {
            "name": "academic-figures",
            "description": "Generate publication-ready academic figures with grayscale styling and Chinese font support.",
            "source": "./plugins/academic-figures",
            "category": null,
            "version": "0.1.0",
            "author": {
              "name": "Adrian",
              "email": "syfyufei@gmail.com"
            },
            "install_commands": [
              "/plugin marketplace add syfyufei/LLM-Research-Marketplace",
              "/plugin install academic-figures@LLM-Research-Marketplace"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2025-12-19T07:40:32Z",
              "created_at": "2025-12-03T10:32:13Z",
              "license": null
            },
            "commands": [
              {
                "name": "/bar",
                "description": "Generate a bar chart with error bars and grayscale styling for academic publications",
                "path": "plugins/academic-figures/commands/bar.md",
                "frontmatter": {
                  "description": "Generate a bar chart with error bars and grayscale styling for academic publications"
                },
                "content": "## Goal\n\nGenerate a publication-ready bar chart with optional error bars, grayscale styling, and Chinese font support.\n\n## Preparation\n\nCollect the following parameters from the user:\n\n| Parameter | Required | Default | Description |\n|-----------|----------|---------|-------------|\n| `data` | Yes | - | Data source: CSV file path |\n| `x` | Yes | - | Column name for categories (X-axis) |\n| `y` | Yes | - | Column name for values |\n| `error` | No | - | Column name for error values (for error bars) |\n| `group` | No | - | Column name for grouping (creates grouped bars) |\n| `title` | No | - | Chart title |\n| `xlabel` | No | - | X-axis label |\n| `ylabel` | No | - | Y-axis label |\n| `orientation` | No | `vertical` | Bar orientation: vertical or horizontal |\n| `show_values` | No | `true` | Show value labels on bars |\n| `output` | No | `paper/figures/bar_chart` | Output file path |\n| `width` | No | 6 | Figure width in inches |\n| `height` | No | 4 | Figure height in inches |\n\n## Execution Steps\n\n1. **Load and prepare data**\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom pathlib import Path\n\ndata = pd.read_csv('{{data}}')\n```\n\n2. **Configure academic style**\n\n```python\nplt.rcParams['font.sans-serif'] = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\nCOLORS = ['#000000', '#4D4D4D', '#7F7F7F', '#B2B2B2', '#D9D9D9']\nHATCHES = ['', '///', '...', 'xxx', '\\\\\\\\\\\\']\n\nmpl.rcParams.update({\n    'font.size': 11,\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'axes.grid': True,\n    'grid.alpha': 0.3,\n    'legend.frameon': False,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n```\n\n3. **Generate bar chart**\n\n```python\nfig, ax = plt.subplots(figsize=({{width}}, {{height}}))\n\nx_col = '{{x}}'\ny_col = '{{y}}'\nerror_col = '{{error}}' if '{{error}}' else None\ngroup_col = '{{group}}' if '{{group}}' else None\norientation = '{{orientation}}' or 'vertical'\nshow_values = '{{show_values}}' != 'false'\n\ncategories = data[x_col].unique()\nx_pos = np.arange(len(categories))\n\nif group_col:\n    # Grouped bar chart\n    groups = data[group_col].unique()\n    n_groups = len(groups)\n    width = 0.8 / n_groups\n\n    for i, group in enumerate(groups):\n        subset = data[data[group_col] == group]\n        values = [subset[subset[x_col] == cat][y_col].values[0] if len(subset[subset[x_col] == cat]) > 0 else 0\n                  for cat in categories]\n        errors = None\n        if error_col:\n            errors = [subset[subset[x_col] == cat][error_col].values[0] if len(subset[subset[x_col] == cat]) > 0 else 0\n                      for cat in categories]\n\n        offset = (i - n_groups/2 + 0.5) * width\n        bars = ax.bar(x_pos + offset, values, width,\n                      label=group,\n                      color=COLORS[i % len(COLORS)],\n                      edgecolor='black', linewidth=0.8,\n                      hatch=HATCHES[i % len(HATCHES)] if i > 0 else '',\n                      yerr=errors, capsize=4,\n                      error_kw={'linewidth': 1, 'color': 'black'})\nelse:\n    # Simple bar chart\n    values = data.groupby(x_col)[y_col].mean().reindex(categories).values\n    errors = None\n    if error_col:\n        errors = data.groupby(x_col)[error_col].mean().reindex(categories).values\n\n    bars = ax.bar(x_pos, values,\n                  color=COLORS[2],\n                  edgecolor='black', linewidth=0.8,\n                  yerr=errors, capsize=4,\n                  error_kw={'linewidth': 1, 'color': 'black'})\n\n    # Add value labels above bars (above error bars if present)\n    if show_values:\n        for bar, val, err in zip(bars, values, errors if errors is not None else [0]*len(values)):\n            height = val + (err if err else 0) + (max(values) * 0.03)\n            ax.text(bar.get_x() + bar.get_width()/2, height,\n                    f'{val:.0f}%' if val < 100 else f'{val:.0f}',\n                    ha='center', va='bottom', fontsize=9)\n\n# Configure axes\nax.set_xticks(x_pos)\nax.set_xticklabels(categories, rotation=15 if len(str(categories[0])) > 4 else 0, ha='right' if len(str(categories[0])) > 4 else 'center')\nax.set_xlabel('{{xlabel}}' or '')\nax.set_ylabel('{{ylabel}}' or y_col)\n\nif '{{title}}':\n    ax.set_title('{{title}}')\n\nif group_col:\n    ax.legend()\n\n# Adjust y-axis to fit labels\nif show_values:\n    ymax = ax.get_ylim()[1]\n    ax.set_ylim(0, ymax * 1.1)\n\nplt.tight_layout()\n```\n\n4. **Save output**\n\n```python\noutput_path = Path('{{output}}' or 'paper/figures/bar_chart')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nfor fmt in ['pdf', 'png']:\n    fig.savefig(f\"{output_path}.{fmt}\")\n    print(f\"Saved: {output_path}.{fmt}\")\n\nplt.close()\n```\n\n## Example Usage\n\n```\n/academic-figures:bar survey.csv --x category --y support_rate --error std_error --title \"ÊîøÁ≠ñÊîØÊåÅÁéá\" --ylabel \"ÊîØÊåÅÁéá (%)\"\n```\n\n## Output\n\n- Grayscale bar chart with optional error bars\n- Value labels positioned above error bars (not overlapping)\n- PDF and PNG formats"
              },
              {
                "name": "/dotwhisker",
                "description": "Generate a dot-whisker coefficient plot for regression results",
                "path": "plugins/academic-figures/commands/dotwhisker.md",
                "frontmatter": {
                  "description": "Generate a dot-whisker coefficient plot for regression results"
                },
                "content": "## Goal\n\nGenerate a publication-ready dot-whisker plot (coefficient plot) showing regression estimates with confidence intervals, following AJPS style guidelines.\n\n## Preparation\n\nCollect the following parameters from the user:\n\n| Parameter | Required | Default | Description |\n|-----------|----------|---------|-------------|\n| `data` | Yes | - | CSV with columns: term, estimate, std.error (or conf.low, conf.high) |\n| `model` | No | - | Column name for model identifier (for multi-model comparison) |\n| `ci_level` | No | 0.95 | Confidence interval level |\n| `reference` | No | 0 | Reference line position |\n| `reorder` | No | `false` | Reorder variables by estimate size |\n| `title` | No | - | Chart title |\n| `xlabel` | No | `Coefficient Estimate` | X-axis label |\n| `output` | No | `paper/figures/dotwhisker` | Output file path |\n| `width` | No | 6 | Figure width in inches |\n| `height` | No | 5 | Figure height in inches |\n\n## Data Format\n\nInput CSV should have these columns:\n- `term`: Variable names\n- `estimate`: Point estimates\n- `std.error`: Standard errors (will compute CI as estimate +/- 1.96*SE)\n- OR `conf.low`, `conf.high`: Pre-computed confidence interval bounds\n- `model` (optional): Model identifier for multi-model plots\n\n## Execution Steps\n\n1. **Load and prepare data**\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom pathlib import Path\nfrom scipy import stats\n\ndata = pd.read_csv('{{data}}')\n\n# Compute confidence intervals if not provided\nci_level = float('{{ci_level}}' or 0.95)\nz_score = stats.norm.ppf((1 + ci_level) / 2)\n\nif 'conf.low' not in data.columns:\n    data['conf.low'] = data['estimate'] - z_score * data['std.error']\n    data['conf.high'] = data['estimate'] + z_score * data['std.error']\n```\n\n2. **Configure academic style**\n\n```python\nplt.rcParams['font.sans-serif'] = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\nmpl.rcParams.update({\n    'font.size': 11,\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'axes.grid': True,\n    'grid.alpha': 0.3,\n    'legend.frameon': False,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n\nCOLORS = ['#000000', '#7F7F7F']\nMARKERS = ['o', 's', '^', 'D']\n```\n\n3. **Generate dot-whisker plot**\n\n```python\nfig, ax = plt.subplots(figsize=({{width}}, {{height}}))\n\nmodel_col = '{{model}}' if '{{model}}' else None\nreference = float('{{reference}}' or 0)\nreorder = '{{reorder}}' == 'true'\n\nif reorder:\n    data = data.sort_values('estimate', ascending=True)\n\nvariables = data['term'].unique()\ny_pos = np.arange(len(variables))\n\nif model_col and model_col in data.columns:\n    # Multi-model comparison\n    models = data[model_col].unique()\n    n_models = len(models)\n    offset_step = 0.3 / n_models\n\n    for i, model in enumerate(models):\n        subset = data[data[model_col] == model]\n        offset = (i - n_models/2 + 0.5) * offset_step\n\n        # Map variable positions\n        var_to_pos = {var: pos for pos, var in enumerate(variables)}\n        positions = [var_to_pos[var] + offset for var in subset['term']]\n\n        # Draw whiskers (confidence intervals)\n        for j, (_, row) in enumerate(subset.iterrows()):\n            pos = var_to_pos[row['term']] + offset\n            ax.plot([row['conf.low'], row['conf.high']], [pos, pos],\n                    color=COLORS[i % len(COLORS)], linewidth=1.5)\n\n        # Draw points\n        ax.scatter(subset['estimate'], positions,\n                   color=COLORS[i % len(COLORS)],\n                   s=50, marker=MARKERS[i % len(MARKERS)],\n                   label=model, zorder=5)\nelse:\n    # Single model\n    for i, (_, row) in enumerate(data.iterrows()):\n        ax.plot([row['conf.low'], row['conf.high']], [i, i],\n                color='black', linewidth=1.5, solid_capstyle='round')\n\n    ax.scatter(data['estimate'], y_pos, color='black', s=60, zorder=5)\n\n# Reference line\nax.axvline(x=reference, color='gray', linestyle='--', linewidth=1, alpha=0.7)\n\n# Configure axes\nax.set_yticks(y_pos)\nax.set_yticklabels(variables)\nax.set_xlabel('{{xlabel}}' or f'Á≥ªÊï∞‰º∞ËÆ°ÂÄº ({int(ci_level*100)}% ÁΩÆ‰ø°Âå∫Èó¥)')\n\nif '{{title}}':\n    ax.set_title('{{title}}')\n\nax.invert_yaxis()  # Highest variable at top\n\nif model_col:\n    ax.legend(loc='lower right')\n\nplt.tight_layout()\n```\n\n4. **Save output**\n\n```python\noutput_path = Path('{{output}}' or 'paper/figures/dotwhisker')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nfor fmt in ['pdf', 'png']:\n    fig.savefig(f\"{output_path}.{fmt}\")\n    print(f\"Saved: {output_path}.{fmt}\")\n\nplt.close()\n```\n\n## Example Usage\n\n```\n/academic-figures:dotwhisker regression_results.csv --title \"Â∑•ËµÑÂÜ≥ÂÆöÂõ†Á¥†ÂàÜÊûê\" --xlabel \"Á≥ªÊï∞‰º∞ËÆ°ÂÄº (95% ÁΩÆ‰ø°Âå∫Èó¥)\"\n```\n\n## Data Example\n\n```csv\nterm,estimate,std.error\nÊïôËÇ≤Âπ¥Èôê,0.082,0.009\nÂ∑•‰ΩúÁªèÈ™å,0.035,0.004\nÊÄßÂà´ (Â•≥ÊÄß),-0.152,0.023\nÂüéÂ∏ÇÊà∑Á±ç,0.124,0.019\n```\n\n## Output\n\n- Clean dot-whisker plot with reference line at zero\n- Confidence intervals displayed as horizontal lines\n- Point estimates as solid dots\n- No overlapping annotations"
              },
              {
                "name": "/line",
                "description": "Generate a line chart with grayscale styling for academic publications",
                "path": "plugins/academic-figures/commands/line.md",
                "frontmatter": {
                  "description": "Generate a line chart with grayscale styling for academic publications"
                },
                "content": "## Goal\n\nGenerate a publication-ready line chart with grayscale styling and Chinese font support.\n\n## Preparation\n\nCollect the following parameters from the user:\n\n| Parameter | Required | Default | Description |\n|-----------|----------|---------|-------------|\n| `data` | Yes | - | Data source: CSV file path, or inline data |\n| `x` | Yes | - | Column name for X-axis |\n| `y` | Yes | - | Column name(s) for Y-axis (comma-separated for multiple lines) |\n| `group` | No | - | Column name for grouping/coloring lines |\n| `title` | No | - | Chart title |\n| `xlabel` | No | X column name | X-axis label |\n| `ylabel` | No | Y column name | Y-axis label |\n| `output` | No | `paper/figures/line_chart` | Output file path (without extension) |\n| `format` | No | `pdf,png` | Output formats |\n| `width` | No | 6 | Figure width in inches |\n| `height` | No | 4 | Figure height in inches |\n\n## Execution Steps\n\n1. **Load data**\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom pathlib import Path\n\n# Load data\ndata = pd.read_csv('{{data}}')  # or pd.read_excel() for .xlsx\n```\n\n2. **Configure academic style**\n\n```python\n# Font configuration\nplt.rcParams['font.sans-serif'] = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\n# Academic style\nCOLORS = ['#000000', '#4D4D4D', '#7F7F7F', '#B2B2B2', '#D9D9D9']\nLINESTYLES = ['-', '--', ':', '-.']\nMARKERS = ['o', 's', '^', 'D', 'v']\n\nmpl.rcParams.update({\n    'font.size': 11,\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'axes.grid': True,\n    'grid.alpha': 0.3,\n    'grid.linewidth': 0.5,\n    'grid.linestyle': '--',\n    'legend.frameon': False,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n```\n\n3. **Generate line chart**\n\n```python\nfig, ax = plt.subplots(figsize=({{width}}, {{height}}))\n\nx_col = '{{x}}'\ny_cols = '{{y}}'.split(',')  # Support multiple Y columns\ngroup_col = '{{group}}' if '{{group}}' else None\n\nif group_col:\n    # Grouped lines\n    groups = data[group_col].unique()\n    for i, group in enumerate(groups):\n        subset = data[data[group_col] == group]\n        ax.plot(subset[x_col], subset[y_cols[0]],\n                color=COLORS[i % len(COLORS)],\n                linestyle=LINESTYLES[i % len(LINESTYLES)],\n                marker=MARKERS[i % len(MARKERS)],\n                markersize=6, linewidth=1.5,\n                label=group)\nelse:\n    # Multiple Y columns as separate lines\n    for i, y_col in enumerate(y_cols):\n        ax.plot(data[x_col], data[y_col.strip()],\n                color=COLORS[i % len(COLORS)],\n                linestyle=LINESTYLES[i % len(LINESTYLES)],\n                marker=MARKERS[i % len(MARKERS)],\n                markersize=6, linewidth=1.5,\n                label=y_col.strip())\n\n# Labels and title\nax.set_xlabel('{{xlabel}}' or x_col)\nax.set_ylabel('{{ylabel}}' or y_cols[0])\nif '{{title}}':\n    ax.set_title('{{title}}')\n\nax.legend()\nplt.tight_layout()\n```\n\n4. **Save output**\n\n```python\noutput_path = Path('{{output}}' or 'paper/figures/line_chart')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nformats = '{{format}}'.split(',') if '{{format}}' else ['pdf', 'png']\nfor fmt in formats:\n    fig.savefig(f\"{output_path}.{fmt.strip()}\")\n    print(f\"Saved: {output_path}.{fmt.strip()}\")\n\nplt.close()\n```\n\n## Example Usage\n\nUser request: \"Create a line chart showing GDP growth over years for regions A, B, C\"\n\n```\n/academic-figures:line data.csv --x year --y gdp --group region --title \"GDPÂ¢ûÈïøË∂ãÂäøÊØîËæÉ\"\n```\n\n## Output\n\n- PDF file (vector format, best for publications)\n- PNG file (raster format, for preview)\n\nBoth saved to `paper/figures/` or specified output path."
              },
              {
                "name": "/map",
                "description": "Generate a choropleth map with grayscale fill for academic publications",
                "path": "plugins/academic-figures/commands/map.md",
                "frontmatter": {
                  "description": "Generate a choropleth map with grayscale fill for academic publications"
                },
                "content": "## Goal\n\nGenerate a publication-ready choropleth map with grayscale fills suitable for academic journals.\n\n## Preparation\n\nCollect the following parameters from the user:\n\n| Parameter | Required | Default | Description |\n|-----------|----------|---------|-------------|\n| `shapefile` | Yes | - | Path to shapefile (.shp) or GeoJSON file |\n| `data` | No | - | CSV with attribute data to join |\n| `join_key` | No | - | Column name to join shapefile with data |\n| `fill_var` | No | - | Column name for fill values (choropleth) |\n| `title` | No | - | Map title |\n| `legend_title` | No | - | Legend title |\n| `output` | No | `paper/figures/map` | Output file path |\n| `width` | No | 8 | Figure width in inches |\n| `height` | No | 6 | Figure height in inches |\n\n## Execution Steps\n\n1. **Load geographic data**\n\n```python\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom pathlib import Path\n\n# Load shapefile or GeoJSON\ngdf = gpd.read_file('{{shapefile}}')\n\n# Optionally join with attribute data\nif '{{data}}':\n    attr_data = pd.read_csv('{{data}}')\n    join_key = '{{join_key}}'\n    gdf = gdf.merge(attr_data, on=join_key, how='left')\n```\n\n2. **Configure academic style**\n\n```python\nplt.rcParams['font.sans-serif'] = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\nmpl.rcParams.update({\n    'font.size': 11,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n\n# Grayscale colormap\nfrom matplotlib.colors import LinearSegmentedColormap\ngrayscale_cmap = LinearSegmentedColormap.from_list('grayscale',\n    ['#FFFFFF', '#D9D9D9', '#B2B2B2', '#7F7F7F', '#4D4D4D', '#000000'])\n```\n\n3. **Generate map**\n\n```python\nfig, ax = plt.subplots(figsize=({{width}}, {{height}}))\n\nfill_var = '{{fill_var}}' if '{{fill_var}}' else None\n\nif fill_var and fill_var in gdf.columns:\n    # Choropleth map\n    gdf.plot(column=fill_var,\n             ax=ax,\n             cmap=grayscale_cmap,\n             edgecolor='black',\n             linewidth=0.5,\n             legend=True,\n             legend_kwds={\n                 'label': '{{legend_title}}' or fill_var,\n                 'orientation': 'horizontal',\n                 'shrink': 0.6,\n                 'pad': 0.05\n             })\nelse:\n    # Simple boundary map\n    gdf.plot(ax=ax,\n             facecolor='white',\n             edgecolor='black',\n             linewidth=0.8)\n\n# Remove axes for cleaner look\nax.set_axis_off()\n\nif '{{title}}':\n    ax.set_title('{{title}}', fontsize=12, fontweight='bold', pad=10)\n\nplt.tight_layout()\n```\n\n4. **Save output**\n\n```python\noutput_path = Path('{{output}}' or 'paper/figures/map')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nfor fmt in ['pdf', 'png']:\n    fig.savefig(f\"{output_path}.{fmt}\")\n    print(f\"Saved: {output_path}.{fmt}\")\n\nplt.close()\n```\n\n## Requirements\n\nThis command requires `geopandas` which can be installed with:\n```bash\npip install geopandas\n```\n\n## Example Usage\n\n```\n/academic-figures:map china_provinces.shp --data gdp_data.csv --join_key province --fill_var gdp_per_capita --title \"‰∏≠ÂõΩÂêÑÁúÅ‰∫∫ÂùáGDP\"\n```\n\n## Data Sources\n\nCommon sources for shapefiles:\n- Natural Earth: https://www.naturalearthdata.com/\n- GADM: https://gadm.org/\n- ‰∏≠ÂõΩÂú∞Âõæ: https://datav.aliyun.com/portal/school/atlas/area_selector\n\n## Output\n\n- Choropleth map with grayscale fill gradient\n- Clean boundary lines\n- Horizontal legend below map\n- PDF and PNG formats"
              },
              {
                "name": "/scatter",
                "description": "Generate a scatter plot with optional trend line for academic publications",
                "path": "plugins/academic-figures/commands/scatter.md",
                "frontmatter": {
                  "description": "Generate a scatter plot with optional trend line for academic publications"
                },
                "content": "## Goal\n\nGenerate a publication-ready scatter plot with grayscale styling, optional trend lines, and support for grouping and faceting.\n\n## Preparation\n\nCollect the following parameters from the user:\n\n| Parameter | Required | Default | Description |\n|-----------|----------|---------|-------------|\n| `data` | Yes | - | Data source: CSV file path |\n| `x` | Yes | - | Column name for X-axis |\n| `y` | Yes | - | Column name for Y-axis |\n| `group` | No | - | Column name for grouping (different markers) |\n| `facet` | No | - | Column name for faceting (multiple panels) |\n| `fit_line` | No | `none` | Trend line: none, linear, loess |\n| `title` | No | - | Chart title |\n| `xlabel` | No | X column name | X-axis label |\n| `ylabel` | No | Y column name | Y-axis label |\n| `output` | No | `paper/figures/scatter` | Output file path |\n| `width` | No | 6 | Figure width in inches |\n| `height` | No | 5 | Figure height in inches |\n\n## Execution Steps\n\n1. **Load data**\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom pathlib import Path\n\ndata = pd.read_csv('{{data}}')\n```\n\n2. **Configure academic style**\n\n```python\nplt.rcParams['font.sans-serif'] = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\nCOLORS = ['#000000', '#4D4D4D', '#7F7F7F', '#B2B2B2']\nMARKERS = ['o', 's', '^', 'D', 'v']\n\nmpl.rcParams.update({\n    'font.size': 11,\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'axes.grid': True,\n    'grid.alpha': 0.3,\n    'legend.frameon': False,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n```\n\n3. **Generate scatter plot**\n\n```python\nx_col = '{{x}}'\ny_col = '{{y}}'\ngroup_col = '{{group}}' if '{{group}}' else None\nfacet_col = '{{facet}}' if '{{facet}}' else None\nfit_line = '{{fit_line}}' or 'none'\n\nif facet_col:\n    # Faceted scatter plot\n    facets = data[facet_col].unique()\n    n_facets = len(facets)\n    fig, axes = plt.subplots(1, n_facets, figsize=({{width}} * n_facets / 2, {{height}}), sharey=True)\n    if n_facets == 1:\n        axes = [axes]\n\n    for ax, facet in zip(axes, facets):\n        subset = data[data[facet_col] == facet]\n\n        ax.scatter(subset[x_col], subset[y_col],\n                   color='black', s=30, alpha=0.6,\n                   edgecolors='black', linewidths=0.5)\n\n        if fit_line == 'linear':\n            z = np.polyfit(subset[x_col], subset[y_col], 1)\n            p = np.poly1d(z)\n            x_line = np.linspace(subset[x_col].min(), subset[x_col].max(), 100)\n            ax.plot(x_line, p(x_line), color='black', linestyle='--', linewidth=1)\n\n        ax.set_title(facet)\n        ax.set_xlabel('{{xlabel}}' or x_col if ax == axes[len(axes)//2] else '')\n\n    axes[0].set_ylabel('{{ylabel}}' or y_col)\n\n    if '{{title}}':\n        fig.suptitle('{{title}}', y=1.02, fontweight='bold')\n\nelse:\n    # Single scatter plot\n    fig, ax = plt.subplots(figsize=({{width}}, {{height}}))\n\n    if group_col:\n        groups = data[group_col].unique()\n        for i, group in enumerate(groups):\n            subset = data[data[group_col] == group]\n            ax.scatter(subset[x_col], subset[y_col],\n                       marker=MARKERS[i % len(MARKERS)],\n                       color=COLORS[i % len(COLORS)],\n                       s=40, alpha=0.7, label=group,\n                       edgecolors='black', linewidths=0.5)\n    else:\n        ax.scatter(data[x_col], data[y_col],\n                   color='black', s=40, alpha=0.6,\n                   edgecolors='black', linewidths=0.5)\n\n    # Add trend line\n    if fit_line == 'linear':\n        z = np.polyfit(data[x_col], data[y_col], 1)\n        p = np.poly1d(z)\n        x_line = np.linspace(data[x_col].min(), data[x_col].max(), 100)\n        ax.plot(x_line, p(x_line), color='black', linestyle='--', linewidth=1.5,\n                label='Ë∂ãÂäøÁ∫ø' if group_col else None)\n    elif fit_line == 'loess':\n        try:\n            from scipy.ndimage import uniform_filter1d\n            # Simple moving average as LOESS approximation\n            sorted_idx = np.argsort(data[x_col])\n            x_sorted = data[x_col].values[sorted_idx]\n            y_sorted = data[y_col].values[sorted_idx]\n            y_smooth = uniform_filter1d(y_sorted, size=max(5, len(y_sorted)//10))\n            ax.plot(x_sorted, y_smooth, color='black', linestyle='--', linewidth=1.5,\n                    label='Ë∂ãÂäøÁ∫ø' if group_col else None)\n        except ImportError:\n            print(\"LOESS requires scipy. Using linear fit instead.\")\n            z = np.polyfit(data[x_col], data[y_col], 1)\n            p = np.poly1d(z)\n            x_line = np.linspace(data[x_col].min(), data[x_col].max(), 100)\n            ax.plot(x_line, p(x_line), color='black', linestyle='--', linewidth=1.5)\n\n    ax.set_xlabel('{{xlabel}}' or x_col)\n    ax.set_ylabel('{{ylabel}}' or y_col)\n\n    if '{{title}}':\n        ax.set_title('{{title}}')\n\n    if group_col or (fit_line != 'none' and group_col):\n        ax.legend()\n\nplt.tight_layout()\n```\n\n4. **Save output**\n\n```python\noutput_path = Path('{{output}}' or 'paper/figures/scatter')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nfor fmt in ['pdf', 'png']:\n    fig.savefig(f\"{output_path}.{fmt}\")\n    print(f\"Saved: {output_path}.{fmt}\")\n\nplt.close()\n```\n\n## Example Usage\n\n```\n/academic-figures:scatter data.csv --x education --y income --group region --fit_line linear --title \"ÊïôËÇ≤‰∏éÊî∂ÂÖ•ÁöÑÂÖ≥Á≥ª\"\n```\n\n## Output\n\n- Scatter plot with grayscale styling\n- Optional trend line (linear or LOESS)\n- Grouped by marker shape and color\n- Faceted panels if specified"
              },
              {
                "name": "/setup",
                "description": "Check environment and configure fonts for academic figure generation",
                "path": "plugins/academic-figures/commands/setup.md",
                "frontmatter": {
                  "description": "Check environment and configure fonts for academic figure generation"
                },
                "content": "## Goal\n\nVerify the user's Python environment has required packages installed and detect available Chinese fonts for academic figure generation.\n\n## Execution Steps\n\n1. **Check Python and required packages**\n\nRun the following Python script to check dependencies:\n\n```python\nimport sys\nprint(f\"Python version: {sys.version}\")\n\nrequired = ['matplotlib', 'numpy', 'pandas']\noptional = ['seaborn', 'geopandas', 'scipy']\n\nmissing_required = []\nmissing_optional = []\n\nfor pkg in required:\n    try:\n        __import__(pkg)\n        print(f\"[OK] {pkg}\")\n    except ImportError:\n        missing_required.append(pkg)\n        print(f\"[MISSING] {pkg}\")\n\nfor pkg in optional:\n    try:\n        __import__(pkg)\n        print(f\"[OK] {pkg} (optional)\")\n    except ImportError:\n        missing_optional.append(pkg)\n        print(f\"[MISSING] {pkg} (optional)\")\n\nif missing_required:\n    print(f\"\\nInstall missing required packages: pip install {' '.join(missing_required)}\")\n```\n\n2. **Detect available Chinese fonts**\n\n```python\nfrom matplotlib import font_manager\n\n# Chinese font candidates by platform\nfont_candidates = [\n    'STFangsong', 'SimSun', 'SimHei',  # Windows/Mac\n    'PingFang SC', 'Heiti SC', 'STHeiti',  # macOS\n    'Noto Sans CJK SC', 'WenQuanYi Micro Hei',  # Linux\n    'Microsoft YaHei', 'FangSong'\n]\n\navailable_fonts = set(f.name for f in font_manager.fontManager.ttflist)\nchinese_fonts = [f for f in font_candidates if f in available_fonts]\n\nprint(\"\\nAvailable Chinese fonts:\")\nfor f in chinese_fonts:\n    print(f\"  - {f}\")\n\nif chinese_fonts:\n    print(f\"\\nRecommended font: {chinese_fonts[0]}\")\nelse:\n    print(\"\\nWARNING: No Chinese fonts found. Chinese characters may not display correctly.\")\n    print(\"Consider installing: Noto Sans CJK or Source Han Sans\")\n```\n\n3. **Test figure generation**\n\n```python\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n# Configure Chinese font\nchinese_fonts = ['STFangsong', 'SimSun', 'PingFang SC', 'SimHei']\nplt.rcParams['font.sans-serif'] = chinese_fonts\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['axes.unicode_minus'] = False\n\n# Test plot\nfig, ax = plt.subplots(figsize=(4, 3))\nax.bar(['Á±ªÂà´A', 'Á±ªÂà´B', 'Á±ªÂà´C'], [3, 7, 5], color=['black', 'gray', 'lightgray'])\nax.set_title('‰∏≠ÊñáÂ≠ó‰ΩìÊµãËØï')\nax.set_ylabel('Êï∞ÂÄº')\nplt.tight_layout()\nplt.savefig('font_test.png', dpi=150)\nplt.close()\nprint(\"\\nTest figure saved to: font_test.png\")\nprint(\"Please verify Chinese characters display correctly.\")\n```\n\n## Output\n\nReport the following to the user:\n- Python version and package status\n- Available Chinese fonts\n- Test figure location for verification\n\n## Error Handling\n\n- If matplotlib is missing: Provide `pip install matplotlib numpy pandas` command\n- If no Chinese fonts found: Suggest installing Noto Sans CJK fonts\n- If font test fails: Check matplotlib cache with `matplotlib.get_cachedir()`"
              }
            ],
            "skills": []
          }
        ]
      }
    }
  ]
}