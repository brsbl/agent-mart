{
  "owner": {
    "id": "tilmon-engineering",
    "display_name": "Tilmon Engineering",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/88894744?v=4",
    "url": "https://github.com/tilmon-engineering",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 2,
      "total_commands": 14,
      "total_skills": 29,
      "total_stars": 2,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "tilmon-engineering/claude-skills",
      "url": "https://github.com/tilmon-engineering/claude-skills",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-01-03T19:37:48Z",
        "created_at": "2025-12-12T15:12:26Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1374
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/.DS_Store",
          "type": "blob",
          "size": 6148
        },
        {
          "path": ".claude/settings.json",
          "type": "blob",
          "size": 77
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 250
        },
        {
          "path": ".tool-versions",
          "type": "blob",
          "size": 12
        },
        {
          "path": "Justfile",
          "type": "blob",
          "size": 650
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 2421
        },
        {
          "path": "TODO.md",
          "type": "blob",
          "size": 81
        },
        {
          "path": "data",
          "type": "tree",
          "size": null
        },
        {
          "path": "data/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/design-plans/2025-12-01-marketing-experimentation.md",
          "type": "blob",
          "size": 14617
        },
        {
          "path": "docs/design-plans/2025-12-05-qualitative-research-framework.md",
          "type": "blob",
          "size": 17150
        },
        {
          "path": "docs/design-plans/2026-01-01-marketplace-transformation.md",
          "type": "blob",
          "size": 14046
        },
        {
          "path": "docs/design-plans/2026-01-03-autonomy-worktree-support.md",
          "type": "blob",
          "size": 10468
        },
        {
          "path": "docs/implementation-plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_01.md",
          "type": "blob",
          "size": 12632
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_02.md",
          "type": "blob",
          "size": 12779
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_03.md",
          "type": "blob",
          "size": 20861
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_04.md",
          "type": "blob",
          "size": 18726
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_05.md",
          "type": "blob",
          "size": 9786
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_06.md",
          "type": "blob",
          "size": 953
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_07.md",
          "type": "blob",
          "size": 21104
        },
        {
          "path": "docs/implementation-plans/2025-12-01-marketing-experimentation/phase_08.md",
          "type": "blob",
          "size": 16149
        },
        {
          "path": "docs/plans",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/plans/2025-11-17-datapeeker-analytics-platform-design.md",
          "type": "blob",
          "size": 8409
        },
        {
          "path": "docs/plans/2025-11-17-datapeeker-implementation-summary.md",
          "type": "blob",
          "size": 1611
        },
        {
          "path": "docs/plans/2025-11-17-datapeeker-platform-implementation.md",
          "type": "blob",
          "size": 23046
        },
        {
          "path": "docs/plans/2025-11-24-data-import-cleaning-skills-design.md",
          "type": "blob",
          "size": 17865
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 618
        },
        {
          "path": "plugins/autonomy/README.md",
          "type": "blob",
          "size": 34403
        },
        {
          "path": "plugins/autonomy/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/agents/branch-analyzer.md",
          "type": "blob",
          "size": 8538
        },
        {
          "path": "plugins/autonomy/agents/journal-reader.md",
          "type": "blob",
          "size": 2554
        },
        {
          "path": "plugins/autonomy/agents/journal-summarizer.md",
          "type": "blob",
          "size": 3668
        },
        {
          "path": "plugins/autonomy/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/commands/analyze-branch.md",
          "type": "blob",
          "size": 1380
        },
        {
          "path": "plugins/autonomy/commands/branch-status.md",
          "type": "blob",
          "size": 1497
        },
        {
          "path": "plugins/autonomy/commands/checkpoint-iteration.md",
          "type": "blob",
          "size": 726
        },
        {
          "path": "plugins/autonomy/commands/compare-branches.md",
          "type": "blob",
          "size": 1562
        },
        {
          "path": "plugins/autonomy/commands/create-goal.md",
          "type": "blob",
          "size": 572
        },
        {
          "path": "plugins/autonomy/commands/end-iteration.md",
          "type": "blob",
          "size": 612
        },
        {
          "path": "plugins/autonomy/commands/fork-iteration.md",
          "type": "blob",
          "size": 1428
        },
        {
          "path": "plugins/autonomy/commands/fork-worktree.md",
          "type": "blob",
          "size": 2174
        },
        {
          "path": "plugins/autonomy/commands/list-branches.md",
          "type": "blob",
          "size": 1339
        },
        {
          "path": "plugins/autonomy/commands/list-worktrees.md",
          "type": "blob",
          "size": 1284
        },
        {
          "path": "plugins/autonomy/commands/remove-worktree.md",
          "type": "blob",
          "size": 1754
        },
        {
          "path": "plugins/autonomy/commands/review-progress.md",
          "type": "blob",
          "size": 542
        },
        {
          "path": "plugins/autonomy/commands/slime.md",
          "type": "blob",
          "size": 1326
        },
        {
          "path": "plugins/autonomy/commands/start-iteration.md",
          "type": "blob",
          "size": 593
        },
        {
          "path": "plugins/autonomy/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branch-status",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branch-status/SKILL.md",
          "type": "blob",
          "size": 7475
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branches/SKILL.md",
          "type": "blob",
          "size": 10569
        },
        {
          "path": "plugins/autonomy/skills/checkpointing-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/checkpointing-an-iteration/SKILL.md",
          "type": "blob",
          "size": 6750
        },
        {
          "path": "plugins/autonomy/skills/comparing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/comparing-branches/SKILL.md",
          "type": "blob",
          "size": 10275
        },
        {
          "path": "plugins/autonomy/skills/creating-a-goal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/creating-a-goal/SKILL.md",
          "type": "blob",
          "size": 6186
        },
        {
          "path": "plugins/autonomy/skills/ending-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/ending-an-iteration/SKILL.md",
          "type": "blob",
          "size": 17528
        },
        {
          "path": "plugins/autonomy/skills/forking-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/forking-iteration/SKILL.md",
          "type": "blob",
          "size": 7116
        },
        {
          "path": "plugins/autonomy/skills/forking-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/forking-worktree/SKILL.md",
          "type": "blob",
          "size": 11405
        },
        {
          "path": "plugins/autonomy/skills/listing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/listing-branches/SKILL.md",
          "type": "blob",
          "size": 5876
        },
        {
          "path": "plugins/autonomy/skills/listing-worktrees",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/listing-worktrees/SKILL.md",
          "type": "blob",
          "size": 8333
        },
        {
          "path": "plugins/autonomy/skills/removing-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/removing-worktree/SKILL.md",
          "type": "blob",
          "size": 9370
        },
        {
          "path": "plugins/autonomy/skills/reviewing-progress",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/reviewing-progress/SKILL.md",
          "type": "blob",
          "size": 7071
        },
        {
          "path": "plugins/autonomy/skills/slime-strategy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/slime-strategy/SKILL.md",
          "type": "blob",
          "size": 13819
        },
        {
          "path": "plugins/autonomy/skills/starting-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/starting-an-iteration/SKILL.md",
          "type": "blob",
          "size": 7519
        },
        {
          "path": "plugins/datapeeker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 670
        },
        {
          "path": "plugins/datapeeker/README.md",
          "type": "blob",
          "size": 9035
        },
        {
          "path": "plugins/datapeeker/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/agents/analyze-transcript.md",
          "type": "blob",
          "size": 5021
        },
        {
          "path": "plugins/datapeeker/agents/categorize-free-text.md",
          "type": "blob",
          "size": 4847
        },
        {
          "path": "plugins/datapeeker/agents/detect-exact-duplicates.md",
          "type": "blob",
          "size": 2534
        },
        {
          "path": "plugins/datapeeker/agents/detect-foreign-keys.md",
          "type": "blob",
          "size": 10366
        },
        {
          "path": "plugins/datapeeker/agents/detect-near-duplicates.md",
          "type": "blob",
          "size": 4215
        },
        {
          "path": "plugins/datapeeker/agents/detect-outliers.md",
          "type": "blob",
          "size": 5105
        },
        {
          "path": "plugins/datapeeker/agents/extract-supporting-quotes.md",
          "type": "blob",
          "size": 9810
        },
        {
          "path": "plugins/datapeeker/agents/generate-initial-codes.md",
          "type": "blob",
          "size": 7269
        },
        {
          "path": "plugins/datapeeker/agents/identify-themes.md",
          "type": "blob",
          "size": 9575
        },
        {
          "path": "plugins/datapeeker/agents/intercoder-reliability-check.md",
          "type": "blob",
          "size": 7810
        },
        {
          "path": "plugins/datapeeker/agents/market-researcher.md",
          "type": "blob",
          "size": 5985
        },
        {
          "path": "plugins/datapeeker/agents/quality-assessment.md",
          "type": "blob",
          "size": 3445
        },
        {
          "path": "plugins/datapeeker/agents/search-disconfirming-evidence.md",
          "type": "blob",
          "size": 13552
        },
        {
          "path": "plugins/datapeeker/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/CLAUDE.md",
          "type": "blob",
          "size": 11980
        },
        {
          "path": "plugins/datapeeker/skills/analyze-skill-dependencies.py",
          "type": "blob",
          "size": 9259
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/SKILL.md",
          "type": "blob",
          "size": 38723
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-1.md",
          "type": "blob",
          "size": 2704
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-2.md",
          "type": "blob",
          "size": 5919
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-3.md",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-4.md",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-5.md",
          "type": "blob",
          "size": 5102
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/SKILL.md",
          "type": "blob",
          "size": 16428
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-1.md",
          "type": "blob",
          "size": 4025
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-2.md",
          "type": "blob",
          "size": 3983
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-3.md",
          "type": "blob",
          "size": 4623
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-4.md",
          "type": "blob",
          "size": 7326
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-5.md",
          "type": "blob",
          "size": 8521
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/SKILL.md",
          "type": "blob",
          "size": 21051
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/graphviz.md",
          "type": "blob",
          "size": 13661
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/mermaid.md",
          "type": "blob",
          "size": 9060
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/vega-lite.md",
          "type": "blob",
          "size": 14492
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/image-formats.md",
          "type": "blob",
          "size": 12930
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/terminal-formats.md",
          "type": "blob",
          "size": 17354
        },
        {
          "path": "plugins/datapeeker/skills/detect-foreign-keys",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/detect-foreign-keys/SKILL.md",
          "type": "blob",
          "size": 18266
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/SKILL.md",
          "type": "blob",
          "size": 15736
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/01-data-familiarization.md",
          "type": "blob",
          "size": 3254
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/02-temporal-patterns.md",
          "type": "blob",
          "size": 2768
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/03-segmentation-patterns.md",
          "type": "blob",
          "size": 2154
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/04-relationship-patterns.md",
          "type": "blob",
          "size": 1666
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/05-anomaly-investigation.md",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/06-insights.md",
          "type": "blob",
          "size": 3825
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/07-next-questions.md",
          "type": "blob",
          "size": 3550
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/overview-summary.md",
          "type": "blob",
          "size": 931
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/SKILL.md",
          "type": "blob",
          "size": 20501
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-1-question-decomposition.md",
          "type": "blob",
          "size": 2276
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-2-data-discovery.md",
          "type": "blob",
          "size": 1984
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-3-sub-question.md",
          "type": "blob",
          "size": 2619
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-4-synthesis.md",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-5-conclusions.md",
          "type": "blob",
          "size": 5192
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/SKILL.md",
          "type": "blob",
          "size": 22900
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/overview-summary.md",
          "type": "blob",
          "size": 251
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-1.md",
          "type": "blob",
          "size": 843
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-2.md",
          "type": "blob",
          "size": 1380
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-3-query.md",
          "type": "blob",
          "size": 961
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-4.md",
          "type": "blob",
          "size": 2048
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-5.md",
          "type": "blob",
          "size": 2081
        },
        {
          "path": "plugins/datapeeker/skills/importing-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/SKILL.md",
          "type": "blob",
          "size": 20141
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-1.md",
          "type": "blob",
          "size": 802
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-2.md",
          "type": "blob",
          "size": 1301
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-3.md",
          "type": "blob",
          "size": 2120
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-4.md",
          "type": "blob",
          "size": 2092
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-5.md",
          "type": "blob",
          "size": 4149
        },
        {
          "path": "plugins/datapeeker/skills/interpreting-results",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/interpreting-results/SKILL.md",
          "type": "blob",
          "size": 23407
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/SKILL.md",
          "type": "blob",
          "size": 56518
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/.gitkeep",
          "type": "blob",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/01-discovery.md",
          "type": "blob",
          "size": 3800
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/02-hypothesis-generation.md",
          "type": "blob",
          "size": 5156
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/03-prioritization.md",
          "type": "blob",
          "size": 5312
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/04-experiment-tracker.md",
          "type": "blob",
          "size": 2950
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/05-synthesis.md",
          "type": "blob",
          "size": 4248
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/06-iteration-plan.md",
          "type": "blob",
          "size": 6694
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/SKILL.md",
          "type": "blob",
          "size": 20986
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats/citations.md",
          "type": "blob",
          "size": 18777
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats/reproducibility.md",
          "type": "blob",
          "size": 20302
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks/3-paragraph-essay.md",
          "type": "blob",
          "size": 20920
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks/narrative-structure.md",
          "type": "blob",
          "size": 21276
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools/marp.md",
          "type": "blob",
          "size": 19294
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools/pandoc.md",
          "type": "blob",
          "size": 25147
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/SKILL.md",
          "type": "blob",
          "size": 21563
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-1-facilitator-guide.md",
          "type": "blob",
          "size": 15733
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-2-session-execution.md",
          "type": "blob",
          "size": 16673
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-1-interview-guide.md",
          "type": "blob",
          "size": 8123
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-2-interview-execution.md",
          "type": "blob",
          "size": 13309
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-1-observation-protocol.md",
          "type": "blob",
          "size": 15440
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-2-field-work.md",
          "type": "blob",
          "size": 17751
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/overview-summary.md",
          "type": "blob",
          "size": 2143
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-3-familiarization.md",
          "type": "blob",
          "size": 3171
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-4-coding.md",
          "type": "blob",
          "size": 6019
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-5-themes.md",
          "type": "blob",
          "size": 4457
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-6-reporting.md",
          "type": "blob",
          "size": 6852
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-1-survey-design.md",
          "type": "blob",
          "size": 13163
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-2-survey-distribution.md",
          "type": "blob",
          "size": 11909
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/CLAUDE.md",
          "type": "blob",
          "size": 8887
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/baseline-results.md",
          "type": "blob",
          "size": 7319
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/green-results.md",
          "type": "blob",
          "size": 10237
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/rationalization-patterns.md",
          "type": "blob",
          "size": 7946
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-1-skip-bias-documentation.md",
          "type": "blob",
          "size": 2260
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-2-skip-intercoder-reliability.md",
          "type": "blob",
          "size": 2333
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-3-skip-disconfirming-evidence.md",
          "type": "blob",
          "size": 2795
        },
        {
          "path": "plugins/datapeeker/skills/skill-dependencies.mermaid",
          "type": "blob",
          "size": 2945
        },
        {
          "path": "plugins/datapeeker/skills/understanding-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/understanding-data/SKILL.md",
          "type": "blob",
          "size": 11528
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/SKILL.md",
          "type": "blob",
          "size": 5886
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/exploring-schema.md",
          "type": "blob",
          "size": 6110
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/formatting-output.md",
          "type": "blob",
          "size": 9615
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/importing-data.md",
          "type": "blob",
          "size": 11221
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/invoking-cli.md",
          "type": "blob",
          "size": 11413
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/optimizing-performance.md",
          "type": "blob",
          "size": 9800
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/writing-queries.md",
          "type": "blob",
          "size": 9637
        },
        {
          "path": "plugins/datapeeker/skills/writing-queries",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/writing-queries/SKILL.md",
          "type": "blob",
          "size": 24214
        },
        {
          "path": "plugins/datapeeker/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/templates/_template",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/templates/_template/.query-template.md",
          "type": "blob",
          "size": 923
        },
        {
          "path": "plugins/datapeeker/templates/_template/00 - overview.md",
          "type": "blob",
          "size": 1529
        },
        {
          "path": "plugins/datapeeker/templates/_template/README.md",
          "type": "blob",
          "size": 2134
        },
        {
          "path": "testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "testing/baseline-results.md",
          "type": "blob",
          "size": 7010
        },
        {
          "path": "testing/final-verification.md",
          "type": "blob",
          "size": 9119
        },
        {
          "path": "testing/green-results.md",
          "type": "blob",
          "size": 8529
        },
        {
          "path": "testing/marketing-experimentation-pressure-tests.md",
          "type": "blob",
          "size": 8443
        }
      ],
      "marketplace": {
        "name": "tilmon-eng-skills",
        "version": "1.0.0",
        "description": "Tilmon Engineering institutional knowledge marketplace - AI agent skills for analysis, security, customer service, and knowledge management",
        "owner_info": {
          "name": "Tilmon Engineering",
          "email": "team@tilmonengineering.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "datapeeker",
            "description": "Structured research methods for AI agents - hypothesis testing, exploratory analysis, comparative analysis, and qualitative research",
            "source": "./plugins/datapeeker",
            "category": null,
            "version": "1.0.0",
            "author": {
              "name": "Tilmon Engineering",
              "email": "team@tilmonengineering.com"
            },
            "install_commands": [
              "/plugin marketplace add tilmon-engineering/claude-skills",
              "/plugin install datapeeker@tilmon-eng-skills"
            ],
            "signals": {
              "stars": 2,
              "forks": 0,
              "pushed_at": "2026-01-03T19:37:48Z",
              "created_at": "2025-12-12T15:12:26Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "cleaning-data",
                "description": "Systematic data quality remediation - detect duplicates/outliers/inconsistencies, design cleaning strategy, execute transformations, verify results (component skill for DataPeeker analysis sessions)",
                "path": "plugins/datapeeker/skills/cleaning-data/SKILL.md",
                "frontmatter": {
                  "name": "cleaning-data",
                  "description": "Systematic data quality remediation - detect duplicates/outliers/inconsistencies, design cleaning strategy, execute transformations, verify results (component skill for DataPeeker analysis sessions)"
                },
                "content": "# Cleaning Data - Component Skill\n\n## Purpose\n\nUse this skill when:\n- Have completed the `importing-data` skill with quality report generated\n- Need to address data quality issues before analysis (duplicates, outliers, NULL handling, free text categorization)\n- Want systematic approach to cleaning decisions with documented rationale\n- Need to create clean tables ready for process skills (exploratory-analysis, guided-investigation, etc.)\n- Following DataPeeker principle: **cleaning is ALWAYS mandatory** even if minimal issues found\n\nThis skill is a **prerequisite** for all DataPeeker analysis workflows and consumes the quality report from importing-data.\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have completed the `importing-data` skill successfully\n2. Have access to `05-quality-report.md` generated by importing-data\n3. Have `raw_*` table(s) in `data/analytics.db`\n4. Be familiar with basic SQLite for validation queries\n5. Understand data quality concepts: duplicates, outliers, NULL handling, categorical standardization\n\n## Data Cleaning Process\n\nCreate a TodoWrite checklist for the 5-phase data cleaning process:\n\n```\nPhase 1: Quality Report Review - pending\nPhase 2: Issue Detection (Agent-Delegated) - pending\nPhase 3: Cleaning Strategy Design - pending\nPhase 4: Cleaning Execution - pending\nPhase 5: Verification & Documentation - pending\n```\n\nMark each phase as you complete it. Document all findings in numbered markdown files (`01-cleaning-scope.md` through `05-verification-report.md`) within your analysis workspace directory.\n\n---\n\n## Phase 1: Quality Report Review\n\n**Goal:** Read quality report from importing-data, understand detected issues, prioritize for cleaning based on impact and severity.\n\n### Read Quality Report from importing-data\n\n**Locate the quality report:**\n```\nanalysis/[session-name]/05-quality-report.md\n```\n\nThis report (generated by importing-data Phase 5) contains:\n- Table schema and row counts\n- NULL percentages per column\n- Duplicate counts and examples\n- Outlier flags (3 MAD threshold) per numeric column\n- Free text candidates (columns with >50% uniqueness)\n- Summary of quality concerns\n\n**Extract key information:**\n- Which columns have >10% NULLs?\n- How many duplicate rows exist (exact duplicates)?\n- Which numeric columns have outliers?\n- Which text columns need categorization?\n\n**Document:** Summarize findings from quality report.\n\n### Prioritize Issues Using Framework\n\n**Issue Prioritization Matrix:**\n\nEvaluate each issue on three dimensions:\n\n**1. Impact (% of rows/columns affected)**\n- **High:** >10% of rows affected\n- **Medium:** 1-10% of rows affected\n- **Low:** <1% of rows affected\n\n**2. Severity (effect on analysis validity)**\n- **Critical:** Makes analysis invalid or misleading (e.g., key column >50% NULL)\n- **Significant:** Reduces data quality for important columns (e.g., duplicates, inconsistent categories)\n- **Minor:** Affects edge cases only (e.g., outliers that are legitimate)\n\n**3. Effort (complexity to resolve)**\n- **Low:** Simple removal, exclusion, or standardization (1-2 SQL queries)\n- **Medium:** Requires sub-agent for categorization or pattern analysis (3-5 queries)\n- **High:** Complex deduplication, manual review, or domain expertise needed (>5 queries)\n\n**Combine dimensions to assign priority:**\n\n| Impact | Severity  | Effort | Priority     | Action Timing |\n|--------|-----------|--------|--------------|---------------|\n| High   | Critical  | Any    | **CRITICAL** | Must address  |\n| High   | Significant| Low/Med| **HIGH**     | Must address  |\n| Medium | Critical  | Low/Med| **HIGH**     | Must address  |\n| Any    | Any       | High   | **MEDIUM**   | Address if time permits |\n| Low    | Minor     | Any    | **LOW**      | Document, may skip |\n\n**Document:** Create prioritized issue table in `01-cleaning-scope.md`.\n\n### Define Cleaning Scope and Objectives\n\nCreate `analysis/[session-name]/01-cleaning-scope.md` with: ./templates/phase-1.md\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Read and understood `05-quality-report.md` from importing-data\n- [ ] Extracted all detected issues (NULLs, duplicates, outliers, free text, FK orphans)\n- [ ] Applied prioritization matrix (impact × severity × effort)\n- [ ] Reviewed FK relationships and orphaned records (if multiple tables)\n- [ ] Defined cleaning objectives with success criteria\n- [ ] `01-cleaning-scope.md` created with all sections filled\n\n---\n\n## Phase 2: Issue Detection (Agent-Delegated)\n\n**Goal:** Deep-dive investigation of prioritized data quality issues using sub-agents to prevent context pollution.\n\n**CRITICAL:** This phase MUST use sub-agent delegation. DO NOT analyze data in main agent context.\n\n### Detection 1: Duplicate Records\n\n**Use dedicated duplicate detection agents**\n\n**For exact duplicates:**\n\nInvoke the `detect-exact-duplicates` agent:\n\n```\nTask tool with agent: detect-exact-duplicates\nParameters:\n- table_name: raw_[actual_table_name]\n- key_columns: [columns that define uniqueness, from Phase 1 scope]\n```\n\n**For near-duplicates (fuzzy matching):**\n\nInvoke the `detect-near-duplicates` agent:\n\n```\nTask tool with agent: detect-near-duplicates\nParameters:\n- table_name: raw_[actual_table_name]\n- text_column: [specific text column flagged in Phase 1]\n```\n\nRepeat for each text column requiring fuzzy matching.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Detection 2: Outliers (MAD-Based)\n\n**Use dedicated outlier detection agent**\n\nFor each numeric column flagged in Phase 1:\n\nInvoke the `detect-outliers` agent:\n\n```\nTask tool with agent: detect-outliers\nParameters:\n- table_name: raw_[actual_table_name]\n- numeric_col: [specific numeric column from Phase 1 scope]\n```\n\nRepeat for each numeric column requiring outlier analysis.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Detection 4: Referential Integrity Validation\n\n**If multiple tables exist with FK relationships identified in Phase 1:**\n\n**Use dedicated FK validation agent**\n\nFor each FK relationship flagged in Phase 1:\n\nInvoke the `detect-foreign-keys` agent (focused validation mode):\n\n```\nTask tool with agent: detect-foreign-keys\nParameters:\n- database_path: data/analytics.db\n- child_table: [specific child table]\n- child_column: [FK column]\n- parent_table: [specific parent table]\n- parent_column: [PK column]\n```\n\nThe agent will:\n- Confirm value overlap percentage (validate Phase 1 findings)\n- Identify specific orphaned record IDs\n- Assess orphan patterns (recent vs old, specific categories, etc.)\n- Quantify impact on analysis (% of records affected in joins)\n\n**If single table:** Skip this detection, document \"N/A - Single table\" in detected issues report.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Review Sub-Agent Findings\n\nAfter all sub-agents return findings:\n\n**For duplicates:**\n- Are exact duplicates truly identical (all columns match)?\n- Are near-duplicates legitimate variations or data entry errors?\n- Which duplicate groups should be merged vs kept separate?\n\n**For outliers:**\n- Are outliers data errors or legitimate extreme values?\n- Do outliers follow any pattern (seasonal, geographic, product-specific)?\n- Which outliers should be excluded vs capped vs flagged?\n\n**For FK orphans (if multiple tables):**\n- Are orphaned records recent (may resolve soon) or old (permanent issue)?\n- Do orphans follow a pattern (specific categories, time periods)?\n- Can orphans be matched to parent records through fuzzy matching?\n- Should orphans be excluded, flagged, or have placeholder parents created?\n\n**Document:** Observations and preliminary decisions for Strategy Phase.\n\n### Create Detected Issues Report\n\nCreate `analysis/[session-name]/02-detected-issues.md` with: ./templates/phase-2.md\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Delegated duplicate detection to sub-agent (exact and near-duplicates)\n- [ ] Delegated outlier detection to sub-agent (MAD-based, all flagged columns)\n- [ ] Delegated FK validation to sub-agent (if multiple tables with relationships)\n- [ ] Reviewed all sub-agent findings and documented observations\n- [ ] Created `02-detected-issues.md` with all sections filled (including FK orphans if applicable)\n- [ ] Identified specific records/issues for Phase 3 strategy decisions\n- [ ] Listed implications for Phase 3 (what user needs to decide)\n\n---\n\n## Phase 3: Cleaning Strategy Design\n\n**Goal:** Design cleaning approach for each detected issue type, present options with trade-offs, get user confirmation before execution.\n\n### Review Detected Issues from Phase 2\n\nFrom `02-detected-issues.md`, identify all issue types requiring decisions:\n- Exact duplicates: [count]\n- Near-duplicates: [count]\n- Outliers per column: [counts]\n- Free text categorization: [columns]\n\n### Decision Framework: Duplicates\n\n**For exact duplicates, choose ONE approach:**\n\n**Option A: Keep First Occurrence**\n- **Method:** ORDER BY rowid, keep lowest rowid per duplicate group\n- **Pros:** Simple, deterministic, fast\n- **Cons:** First may not be most complete/accurate\n- **Use when:** No quality difference between duplicates\n\n**Option B: Keep Most Complete**\n- **Method:** Rank by completeness (fewest NULLs), keep best per group\n- **Pros:** Preserves maximum information\n- **Cons:** More complex, requires completeness scoring\n- **Use when:** Duplicates have varying data quality\n\n**Option C: Merge Records**\n- **Method:** Combine non-NULL values from all duplicates\n- **Pros:** No data loss\n- **Cons:** Complex, may create inconsistencies\n- **Use when:** Duplicates have complementary information\n\n**For near-duplicates, choose ONE approach:**\n\n**Option A: Auto-Merge High Confidence (>95%)**\n- **Method:** Apply fuzzy matching agent's high confidence mappings automatically\n- **Pros:** Efficient, addresses most obvious issues\n- **Cons:** Small risk of incorrect merges\n- **Use when:** Trust fuzzy matching agent's assessment\n\n**Option B: Manual Review All**\n- **Method:** Review every near-duplicate group before merging\n- **Pros:** Zero incorrect merges\n- **Cons:** Time-consuming\n- **Use when:** Data quality is critical\n\n**Option C: Skip Near-Duplicates**\n- **Method:** Only handle exact duplicates, leave fuzzy matches as-is\n- **Pros:** Conservative, no risk\n- **Cons:** Misses data quality improvements\n- **Use when:** Near-duplicates are legitimate variations\n\n**Document chosen approach in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Outliers\n\n**For each numeric column with outliers, choose ONE approach:**\n\n**Option A: Exclude Outliers**\n- **Method:** Filter out rows where value > 3 MAD from median\n- **Pros:** Clean dataset, no extreme values skewing analysis\n- **Cons:** Data loss, may exclude legitimate extremes\n- **Use when:** Outliers are clearly data errors\n\n**Option B: Cap at Threshold**\n- **Method:** Set outliers to 3 MAD threshold (winsorization)\n- **Pros:** Preserves row count, reduces extreme influence\n- **Cons:** Distorts actual values\n- **Use when:** Want to preserve rows but limit extreme influence\n\n**Option C: Flag and Keep**\n- **Method:** Add outlier_flag column, keep all data\n- **Pros:** No data loss, analysts can filter if needed\n- **Cons:** Outliers may still skew analysis if not filtered\n- **Use when:** Outliers might be legitimate, need analyst judgment\n\n**Option D: Keep As-Is**\n- **Method:** No transformation\n- **Pros:** Preserves true data\n- **Cons:** Extremes may dominate analysis\n- **Use when:** Outliers are legitimate (VIPs, seasonal spikes)\n\n**Document chosen approach per column in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Free Text Categorization\n\n**If free text columns flagged in Phase 2:**\n\n**Step 1: Invoke categorization agent**\n\n```\nTask tool with agent: categorize-free-text\nParameters:\n- column_name: [specific text column]\n- unique_values: [list from Phase 2 detection]\n- business_context: [optional context about what values represent]\n```\n\n**Step 2: Review agent's proposed categories**\n\nAgent will return:\n- Proposed category schema (3-10 categories)\n- Value mappings with confidence levels\n- Ambiguous/uncategorizable values flagged\n\n**Step 3: Choose categorization approach:**\n\n**Option A: Accept Agent Proposal**\n- **Method:** Use agent's categories and mappings as-is\n- **Pros:** Fast, leverages semantic analysis\n- **Cons:** May miss business context\n- **Use when:** Agent's categories make sense for analysis\n\n**Option B: Modify Categories**\n- **Method:** Adjust agent's proposal (rename, merge, split categories)\n- **Pros:** Incorporates business knowledge\n- **Cons:** Requires manual refinement\n- **Use when:** Agent's categories are close but need tweaking\n\n**Option C: Manual Categorization**\n- **Method:** Define categories and mappings from scratch\n- **Pros:** Full control, perfect fit for business needs\n- **Cons:** Time-consuming\n- **Use when:** Agent's proposal doesn't fit business model\n\n**Option D: Keep As-Is**\n- **Method:** No categorization\n- **Pros:** Preserves original data\n- **Cons:** High-cardinality text column harder to analyze\n- **Use when:** Free text values are inherently unique (IDs, descriptions)\n\n**Document chosen approach in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Referential Integrity (If Multiple Tables)\n\n**For FK orphaned records identified in Phase 2:**\n\nFor each FK relationship with orphaned records, choose ONE approach:\n\n**Option A: Exclude Orphaned Records**\n- **Method:** Filter out child records where FK doesn't match any parent PK\n- **Pros:** Clean referential integrity, INNER JOINs work correctly\n- **Cons:** Data loss\n- **Use when:** Orphaned records are data errors with no business value\n\n**Option B: Preserve with NULL**\n- **Method:** Set orphaned FK values to NULL (retain child records)\n- **Pros:** Preserves child row count, makes orphans explicit\n- **Cons:** Loses relationship information, NULL handling required in queries\n- **Use when:** Child records have value even without parent context\n\n**Option C: Flag and Keep**\n- **Method:** Add `[fk_column]_orphan_flag` column, keep original FK value\n- **Pros:** No data loss, analysts can filter as needed\n- **Cons:** Referential integrity violated until analyst filters\n- **Use when:** Need investigation before deciding, orphans may resolve\n\n**Option D: Create Placeholder Parent**\n- **Method:** Insert synthetic parent record (e.g., id=-1, name=\"Unknown\"), map orphans to it\n- **Pros:** Preserves referential integrity AND child rows, INNER JOINs work\n- **Cons:** Creates synthetic data, may skew parent-level aggregations\n- **Use when:** JOINs required but can't lose child records (e.g., orders with unknown customer)\n\n**Document chosen approach per FK relationship in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Business Rules (Optional)\n\n**If business rules were defined in Phase 1 scope:**\n\nFor each rule, choose approach for violations:\n\n**Option A: Exclude Violating Records**\n- **Method:** Filter out rows that fail validation\n- **Pros:** Clean dataset, only valid data\n- **Cons:** Data loss\n- **Use when:** Invalid data cannot be corrected\n\n**Option B: Cap/Coerce to Valid Range**\n- **Method:** Adjust values to meet constraints\n- **Pros:** Preserves rows\n- **Cons:** Changes actual data\n- **Use when:** Violations are minor (e.g., age 150 → cap at 120)\n\n**Option C: Flag and Keep**\n- **Method:** Add validation_flag column\n- **Pros:** No data loss, transparent\n- **Cons:** Invalid data present\n- **Use when:** Need to investigate violations before deciding\n\n**Document chosen approach per rule in `03-cleaning-strategy.md`**\n\n---\n\n### Create Cleaning Strategy Document\n\nCreate `analysis/[session-name]/03-cleaning-strategy.md` with: ./templates/phase-3.md\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Reviewed all detected issues from Phase 2\n- [ ] Chosen approach for duplicates (exact and near)\n- [ ] Chosen approach for outliers (per numeric column)\n- [ ] Reviewed free text categorization agent proposal (if applicable)\n- [ ] Chosen approach for free text categorization\n- [ ] Chosen approach for FK orphans (if multiple tables with relationships)\n- [ ] Defined business rule handling (if applicable)\n- [ ] User confirmed all strategies via checkpoint review\n- [ ] `03-cleaning-strategy.md` created with all decisions documented\n\n---\n\n## Phase 4: Cleaning Execution\n\n**Goal:** Execute approved cleaning strategies, create clean_* tables, track all exclusions and transformations.\n\n**CRITICAL:** All transformations use CREATE TABLE AS SELECT pattern. Keep raw_* tables intact.\n\n### Transformation 1: Remove Duplicates\n\n**Based on Strategy from Phase 3:**\n\n**For Exact Duplicates (Keep First approach):**\n\n```sql\n-- Create clean table without exact duplicates\nCREATE TABLE clean_[table_name] AS\nWITH ranked_records AS (\n  SELECT *,\n    ROW_NUMBER() OVER (\n      PARTITION BY [key_columns]\n      ORDER BY rowid\n    ) as rn\n  FROM raw_[table_name]\n)\nSELECT [all_columns]  -- Exclude rn column\nFROM ranked_records\nWHERE rn = 1;\n```\n\n**For Exact Duplicates (Keep Most Complete approach):**\n\n```sql\n-- Create clean table keeping most complete record per duplicate group\nCREATE TABLE clean_[table_name] AS\nWITH completeness_scored AS (\n  SELECT *,\n    ([count non-NULL columns formula]) as completeness_score,\n    ROW_NUMBER() OVER (\n      PARTITION BY [key_columns]\n      ORDER BY completeness_score DESC, rowid\n    ) as rn\n  FROM raw_[table_name]\n)\nSELECT [all_columns]  -- Exclude rn and completeness_score\nFROM completeness_scored\nWHERE rn = 1;\n```\n\n**For Near-Duplicates (Auto-Merge High Confidence approach):**\n\n```sql\n-- Create mapping table from categorization agent\nCREATE TABLE [column]_near_dup_mapping AS\nSELECT\n  original_value,\n  canonical_value\nFROM (VALUES\n  ('[value1]', '[canonical]'),\n  ('[value2]', '[canonical]'),\n  ...\n) AS mapping(original_value, canonical_value);\n\n-- Apply mapping\nUPDATE clean_[table_name]\nSET [text_column] = (\n  SELECT canonical_value\n  FROM [column]_near_dup_mapping\n  WHERE original_value = [text_column]\n)\nWHERE [text_column] IN (SELECT original_value FROM [column]_near_dup_mapping);\n```\n\n**Verification:**\n\n```sql\n-- Verify duplicate removal\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\nSELECT COUNT(*) - COUNT(DISTINCT [key_columns]) as remaining_dups FROM clean_[table_name];\n-- Expected: 0 remaining duplicates\n\n-- Exclusion count\nSELECT [raw_count] - [clean_count] as excluded_duplicates;\n```\n\n**Document in `04-cleaning-execution.md`:**\n- SQL executed\n- Before count: [N] rows\n- After count: [N] rows\n- Duplicates removed: [N] rows ([X]%)\n\n---\n\n### Transformation 2: Handle Outliers\n\n**Based on Strategy from Phase 3:**\n\n**For Outliers (Exclude approach):**\n\n```sql\n-- Calculate thresholds\nWITH stats AS (\n  SELECT\n    [median_calculation] as median,\n    [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\n-- Create clean table excluding outliers\nCREATE TABLE clean_[table_name] AS\nSELECT r.*\nFROM raw_[table_name] r\nCROSS JOIN stats s\nWHERE ABS(r.[numeric_col] - s.median) <= 3 * s.mad\n   OR r.[numeric_col] IS NULL;  -- Keep NULLs\n```\n\n**For Outliers (Cap at Threshold approach - Winsorization):**\n\n```sql\nWITH stats AS (\n  SELECT\n    [median_calculation] as median,\n    [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\nCREATE TABLE clean_[table_name] AS\nSELECT\n  [other_columns],\n  CASE\n    WHEN [numeric_col] > median + 3 * mad THEN median + 3 * mad\n    WHEN [numeric_col] < median - 3 * mad THEN median - 3 * mad\n    ELSE [numeric_col]\n  END as [numeric_col]\nFROM raw_[table_name]\nCROSS JOIN stats;\n```\n\n**For Outliers (Flag and Keep approach):**\n\n```sql\nWITH stats AS (\n  SELECT [median_calculation] as median, [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.*,\n  CASE\n    WHEN ABS(r.[numeric_col] - s.median) > 3 * s.mad THEN 1\n    ELSE 0\n  END as [numeric_col]_outlier_flag\nFROM raw_[table_name] r\nCROSS JOIN stats s;\n```\n\n**Verification:**\n\n```sql\n-- Verify outlier handling\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\n\n-- For Exclude approach: check no outliers remain\nWITH stats AS (...)\nSELECT COUNT(*) as remaining_outliers\nFROM clean_[table_name], stats\nWHERE ABS([numeric_col] - median) > 3 * mad;\n-- Expected: 0\n\n-- For Cap approach: check values at thresholds\nSELECT MIN([numeric_col]), MAX([numeric_col]) FROM clean_[table_name];\n\n-- For Flag approach: check flag distribution\nSELECT [numeric_col]_outlier_flag, COUNT(*)\nFROM clean_[table_name]\nGROUP BY [numeric_col]_outlier_flag;\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Approach used per column\n- SQL executed\n- Before/after row counts (if excluding)\n- Outliers affected: [N] rows ([X]%)\n\n---\n\n### Transformation 3: Categorize Free Text\n\n**Based on Strategy from Phase 3:**\n\n**If using agent's proposed categories:**\n\n```sql\n-- Create category mapping from agent output\nCREATE TABLE [column]_category_mapping AS\nVALUES\n  ('[value]', '[Category 1]'),\n  ('[value]', '[Category 1]'),\n  ('[value]', '[Category 2]'),\n  ...\n) AS mapping(original_value, category);\n\n-- Apply categorization\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.[other_columns],\n  COALESCE(m.category, 'Other') as [column]_category\nFROM raw_[table_name] r\nLEFT JOIN [column]_category_mapping m\n  ON r.[text_column] = m.original_value;\n```\n\n**If handling uncategorizable values:**\n\n```sql\n-- Option A: Exclude uncategorizable\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.*,\n  m.category as [column]_category\nFROM raw_[table_name] r\nINNER JOIN [column]_category_mapping m\n  ON r.[text_column] = m.original_value;\n-- INNER JOIN excludes unmapped values\n\n-- Option B: Map to \"Other\" category (shown in previous query with COALESCE)\n```\n\n**Verification:**\n\n```sql\n-- Verify categorization coverage\nSELECT\n  [column]_category,\n  COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM clean_[table_name]\nGROUP BY [column]_category\nORDER BY count DESC;\n\n-- Check for unmapped values (if kept)\nSELECT COUNT(*) as unmapped\nFROM clean_[table_name]\nWHERE [column]_category IS NULL OR [column]_category = 'Other';\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Category schema used\n- SQL executed\n- Distribution by category\n- Unmapped/excluded values: [N] rows ([X]%)\n\n---\n\n### Transformation 4: Business Rule Validation (if applicable)\n\n**Based on Strategy from Phase 3:**\n\n```sql\n-- Exclude rows violating business rules\nCREATE TABLE clean_[table_name] AS\nSELECT *\nFROM raw_[table_name]\nWHERE [rule_1_validation]\n  AND [rule_2_validation]\n  ...;\n\n-- Example rules:\n-- WHERE age BETWEEN 0 AND 120\n-- AND amount > 0\n-- AND date BETWEEN '2020-01-01' AND '2025-12-31'\n```\n\n**Verification:**\n\n```sql\n-- Verify no violations remain\nSELECT COUNT(*) as violations\nFROM clean_[table_name]\nWHERE NOT ([rule_1_validation] AND [rule_2_validation] ...);\n-- Expected: 0\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Rules enforced\n- SQL executed\n- Violations excluded: [N] rows ([X]%)\n\n---\n\n### Transformation 5: Referential Integrity Enforcement (if multiple tables)\n\n**Based on Strategy from Phase 3:**\n\n**For Orphaned Records (Exclude approach):**\n\n```sql\n-- Remove orphaned child records (Option A from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT c.*\nFROM raw_child_table c\nINNER JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n-- INNER JOIN automatically excludes orphans\n```\n\n**For Orphaned Records (Preserve with NULL approach):**\n\n```sql\n-- Set orphaned FK values to NULL (Option B from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL THEN NULL\n    ELSE c.fk_column\n  END as fk_column\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**For Orphaned Records (Flag and Keep approach):**\n\n```sql\n-- Add orphan flag column (Option C from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL AND c.fk_column IS NOT NULL THEN 1\n    ELSE 0\n  END as fk_column_orphan_flag\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**For Orphaned Records (Create Placeholder Parent approach):**\n\n```sql\n-- Step 1: Create placeholder parent record (Option D from Phase 3)\nINSERT INTO raw_parent_table (pk_column, name, other_fields)\nVALUES (-1, 'Unknown', NULL, ...);\n\n-- Step 2: Remap orphans to placeholder\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL THEN -1\n    ELSE c.fk_column\n  END as fk_column\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**Verification:**\n\n```sql\n-- Verify no orphans remain (for Exclude approach)\nSELECT COUNT(*) as orphans\nFROM clean_child_table c\nLEFT JOIN clean_parent_table p ON c.fk_column = p.pk_column\nWHERE p.pk_column IS NULL AND c.fk_column IS NOT NULL;\n-- Expected: 0\n\n-- Verify NULL remapping (for Preserve with NULL approach)\nSELECT COUNT(*) as nulled_fks\nFROM clean_child_table\nWHERE fk_column IS NULL;\n-- Expected: [count of orphans from Phase 2]\n\n-- Verify flag accuracy (for Flag and Keep approach)\nSELECT fk_column_orphan_flag, COUNT(*)\nFROM clean_child_table\nGROUP BY fk_column_orphan_flag;\n-- Expected: flag=1 count matches orphan count from Phase 2\n```\n\n**Document in `04-cleaning-execution.md`:**\n- FK relationship handled\n- Approach used (Exclude/Preserve/Flag/Placeholder)\n- SQL executed\n- Orphans affected: [N] rows ([X]%)\n- JOIN behavior after transformation\n\n[If single table: \"N/A - Single table analysis\"]\n\n---\n\n### Combined Transformation Approach\n\n**If multiple transformations needed, use CTE chain:**\n\n```sql\nCREATE TABLE clean_[table_name] AS\nWITH\n-- Step 1: Remove duplicates\ndeduped AS (\n  SELECT *, ROW_NUMBER() OVER (PARTITION BY [key_cols] ORDER BY rowid) as rn\n  FROM raw_[table_name]\n),\nno_dups AS (\n  SELECT [all_columns] FROM deduped WHERE rn = 1\n),\n-- Step 2: Handle outliers\noutliers_removed AS (\n  SELECT d.*\n  FROM no_dups d\n  CROSS JOIN (SELECT [median], [mad] FROM ...) stats\n  WHERE ABS(d.[numeric_col] - stats.median) <= 3 * stats.mad\n),\n-- Step 3: Apply categorization\ncategorized AS (\n  SELECT\n    o.*,\n    COALESCE(m.category, 'Other') as [column]_category\n  FROM outliers_removed o\n  LEFT JOIN [column]_category_mapping m ON o.[text_col] = m.original_value\n),\n-- Step 4: Enforce business rules\nfinal AS (\n  SELECT *\n  FROM categorized\n  WHERE [rule_validations]\n)\nSELECT * FROM final;\n```\n\n**Verification of combined transformations:**\n\n```sql\n-- Row count reconciliation\nSELECT\n  (SELECT COUNT(*) FROM raw_[table_name]) as raw_count,\n  (SELECT COUNT(*) FROM clean_[table_name]) as clean_count,\n  (SELECT COUNT(*) FROM raw_[table_name]) - (SELECT COUNT(*) FROM clean_[table_name]) as total_excluded;\n```\n\n---\n\n### Create Cleaning Execution Log\n\nCreate `analysis/[session-name]/04-cleaning-execution.md` with: ./templates/phase-4.md\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] Executed all transformations from Phase 3 strategy\n- [ ] Created clean_[table_name] table in data/analytics.db\n- [ ] Verified each transformation with specific checks\n- [ ] Reconciled row counts (raw = clean + exclusions)\n- [ ] Documented all exclusions with reasons and counts\n- [ ] Spot-checked sample records before/after\n- [ ] `04-cleaning-execution.md` created with all results documented\n\n---\n\n## Phase 5: Verification & Documentation\n\n**Goal:** Validate cleaning results, quantify quality improvements, document complete audit trail from raw to clean.\n\n### Verify Row Count Reconciliation\n\n**Critical validation - MUST match exactly:**\n\n```sql\n-- Count raw table\nSELECT COUNT(*) as raw_count FROM raw_[table_name];\n\n-- Count clean table\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\n\n-- Calculate exclusions from Phase 4 log\n-- Expected: raw_count = clean_count + total_exclusions\n```\n\n**Document:** Confirm reconciliation passes. If mismatch, investigate before proceeding.\n\n---\n\n### Verify Transformation Results\n\n**For each transformation applied in Phase 4:**\n\n**Duplicate Removal Verification:**\n\n```sql\n-- Confirm no duplicates remain\nSELECT [key_columns], COUNT(*) as occurrences\nFROM clean_[table_name]\nGROUP BY [key_columns]\nHAVING COUNT(*) > 1;\n-- Expected: 0 rows returned\n```\n\n**Outlier Handling Verification:**\n\n```sql\n-- For Exclude approach: confirm no outliers remain\nWITH stats AS (\n  SELECT [median], [mad] FROM ...\n)\nSELECT COUNT(*) as remaining_outliers\nFROM clean_[table_name], stats\nWHERE ABS([numeric_col] - median) > 3 * mad;\n-- Expected: 0 rows\n\n-- For Cap approach: confirm values at thresholds\nSELECT MIN([numeric_col]) as min_val, MAX([numeric_col]) as max_val\nFROM clean_[table_name];\n-- Expected: min >= (median - 3*MAD), max <= (median + 3*MAD)\n\n-- For Flag approach: check flag accuracy\nSELECT [numeric_col]_outlier_flag, COUNT(*)\nFROM clean_[table_name]\nGROUP BY [numeric_col]_outlier_flag;\n-- Expected: distribution matches Phase 4 execution log\n```\n\n**Free Text Categorization Verification:**\n\n```sql\n-- Confirm all values categorized\nSELECT COUNT(*) as uncategorized\nFROM clean_[table_name]\nWHERE [column]_category IS NULL;\n-- Expected: 0 (unless \"keep uncategorized\" was strategy)\n\n-- Verify category distribution\nSELECT [column]_category, COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM clean_[table_name]\nGROUP BY [column]_category\nORDER BY count DESC;\n-- Expected: matches Phase 4 execution results\n```\n\n**Business Rule Verification (if applicable):**\n\n```sql\n-- Confirm no violations remain\nSELECT COUNT(*) as violations\nFROM clean_[table_name]\nWHERE NOT ([rule_1_condition] AND [rule_2_condition] ...);\n-- Expected: 0 rows\n```\n\n**Document all verification results:**\n- ✓ Pass: Expected result confirmed\n- ✗ Fail: Unexpected result, requires investigation\n\n---\n\n### Compare Data Quality Metrics\n\n**Before vs After comparison:**\n\n```sql\n-- Completeness comparison\nSELECT\n  'raw' as table_name,\n  COUNT(*) as total_rows,\n  COUNT([col1]) as [col1]_non_null,\n  ROUND(100.0 * COUNT([col1]) / COUNT(*), 2) as [col1]_completeness_pct\nFROM raw_[table_name]\nUNION ALL\nSELECT\n  'clean' as table_name,\n  COUNT(*),\n  COUNT([col1]),\n  ROUND(100.0 * COUNT([col1]) / COUNT(*), 2)\nFROM clean_[table_name];\n```\n\n**Create quality improvement table:**\n\n| Metric | Raw Table | Clean Table | Improvement |\n|--------|-----------|-------------|-------------|\n| Total rows | [N] | [N] | -[X]% (exclusions) |\n| Completeness ([col1]) | [X]% | [X]% | +[X] pct points |\n| Duplicate groups | [N] | 0 | -[N] (100%) |\n| Outliers ([col2]) | [N] | 0 | -[N] (100%) |\n| Free text unique values | [N] | [N categories] | -[X]% (categorization) |\n\n**Document:** Quality improvements quantified with specific deltas.\n\n---\n\n### Spot Check Sample Records\n\n**Select representative samples to manually verify:**\n\n```sql\n-- Records that were in duplicate groups\nSELECT * FROM clean_[table_name] WHERE rowid IN ([IDs from Phase 2]);\n-- Verify: correct record kept per strategy\n\n-- Records with outliers (if flagged/capped, not excluded)\nSELECT * FROM clean_[table_name] WHERE [numeric_col]_outlier_flag = 1;\n-- Verify: flag accurate, values capped if applicable\n\n-- Records with categorized free text\nSELECT [original_col], [col]_category FROM clean_[table_name] LIMIT 20;\n-- Verify: categories make sense, mapping correct\n```\n\n**Document:** Manual verification confirms automated transformations worked correctly.\n\n---\n\n### Assess Limitations and Confidence\n\n**Document what this cleaning did NOT address:**\n\n- **Scope limitations:** [Issues identified but not addressed - e.g., \"Date range not validated\"]\n- **Data coverage:** [Time periods, geographies, categories not covered]\n- **Assumptions made:** [Business rules assumed without domain validation]\n- **Edge cases:** [Unusual values handled a specific way]\n\n**Confidence assessment:**\n\n- **High confidence:** [Transformations with clear validation - e.g., \"Exact duplicate removal\"]\n- **Medium confidence:** [Transformations with some subjectivity - e.g., \"Free text categorization\"]\n- **Low confidence / Needs review:** [Transformations requiring domain expertise - e.g., \"Outliers might be legitimate\"]\n\n**Document:** Honest assessment of what was cleaned and what wasn't, with confidence levels.\n\n---\n\n### Create Verification Report\n\nCreate `analysis/[session-name]/05-verification-report.md` with: ./templates/phase-5.md\n\n**CHECKPOINT:** Before concluding cleaning-data skill, you MUST have:\n- [ ] Verified row count reconciliation (raw = clean + exclusions)\n- [ ] Validated all transformations with specific queries\n- [ ] Quantified quality improvements with before/after metrics\n- [ ] Spot-checked sample records manually\n- [ ] Documented limitations and assumptions honestly\n- [ ] Assessed confidence level for each transformation\n- [ ] Created complete exclusion accounting table\n- [ ] `05-verification-report.md` created with all sections filled\n- [ ] clean_* table ready for analysis process skills\n\n---\n\n## Common Rationalizations\n\n### \"The data looks clean after Phase 2, I can skip Phase 3 strategy design\"\n**Why this is wrong:** Detecting issues isn't the same as deciding how to fix them. Different approaches (exclude vs cap vs flag) have different analytical implications.\n\n**Do instead:** Always complete Phase 3 with explicit decision frameworks. Document why you chose each approach with user confirmation.\n\n### \"I'll just exclude all outliers automatically, no need to review them\"\n**Why this is wrong:** Some outliers are legitimate (VIP customers, seasonal spikes, rare events). Automatic exclusion loses valuable data.\n\n**Do instead:** Complete Phase 2 detection with agent analysis. Review patterns in Phase 3. Choose approach based on business context, not just statistical threshold.\n\n### \"The fuzzy matching agent found near-duplicates, I'll merge them all\"\n**Why this is wrong:** 90-95% similarity doesn't mean identical. \"John Smith\" vs \"John Smyth\" might be the same person OR two different people.\n\n**Do instead:** Review confidence levels in Phase 3. Auto-merge only high confidence (>95%). Manual review medium confidence. Document decisions.\n\n### \"I don't need to document exclusions, I can remember what I removed\"\n**Why this is wrong:** Undocumented exclusions break audit trail. When results are questioned, you can't explain what data was excluded or why.\n\n**Do instead:** Complete Phase 4 execution log with exclusion summary table. Document every excluded record with reason and count. Reconcile in Phase 5.\n\n### \"Verification is just running the same queries again, waste of time\"\n**Why this is wrong:** Phase 5 verification checks RESULTS, not execution. Queries can run without errors but produce wrong results (logic bugs, wrong thresholds, incorrect mappings).\n\n**Do instead:** Always complete Phase 5 with before/after comparisons, spot checks, and manual inspection. Verification catches transformation bugs.\n\n### \"I found one issue, fixed it, done with cleaning\"\n**Why this is wrong:** Data quality issues cluster. If you found duplicates, likely also have outliers, NULLs, and inconsistencies. One fix doesn't make data \"clean\".\n\n**Do instead:** Complete all 5 phases systematically. Phase 2 detects ALL issue types. Address all prioritized issues in Phases 3-4.\n\n### \"The clean table has fewer rows, that's proof it's better\"\n**Why this is wrong:** Smaller isn't always better. Excluding 50% of data might remove all the interesting variation. Quality ≠ quantity reduction.\n\n**Do instead:** Complete Phase 5 with quality improvement quantification. Measure completeness, consistency, validity improvements - not just row count reduction.\n\n### \"I'll categorize free text myself, faster than using the agent\"\n**Why this is wrong:** Manual categorization is inconsistent, misses patterns, and pollutes main agent context with hundreds of unique values.\n\n**Do instead:** Always delegate free text analysis to categorize-free-text agent in Phase 3. Agent provides structured mapping with confidence levels. Review and adjust if needed.\n\n### \"Business rules failed for 2%, I'll just delete those rows and move on\"\n**Why this is wrong:** 2% violations might indicate systematic data quality issue (bad data entry, import error, logic flaw). Deleting hides the problem.\n\n**Do instead:** Investigate violations in Phase 2. Document why they violate rules in Phase 3. Consider whether to exclude, correct, or flag in strategy. Document pattern in Phase 5.\n\n### \"Phase 5 validation passed, I'm done - no need to document limitations\"\n**Why this is wrong:** All cleaning has limitations and assumptions. Pretending otherwise misleads analysts who use the clean data.\n\n**Do instead:** Complete Phase 5 limitations section honestly. Document what was NOT cleaned, assumptions made, edge cases, confidence levels. Transparency builds trust.\n\n---\n\n## Summary\n\nThis skill ensures systematic, documented data cleaning with quality validation by:\n\n1. **Prioritized scope definition:** Read quality report from importing-data, apply impact × severity × effort framework - ensures high-value issues addressed first, not random fixes.\n\n2. **Structured decision-making:** Present options with trade-offs for duplicates, outliers, free text, business rules - gets user confirmation before execution, prevents undocumented assumptions.\n\n3. **Agent-delegated detection:** Use dedicated sub-agents (detect-exact-duplicates, detect-near-duplicates, detect-outliers, categorize-free-text) - prevents context pollution while ensuring thorough analysis.\n\n4. **Explicit strategy approval:** Document chosen approach per issue type in Phase 3 with rationale - creates decision audit trail, enables strategy review if results questioned.\n\n5. **Transformation transparency:** Execute cleaning with CREATE TABLE AS SELECT, preserve raw_* tables, track all exclusions - maintains complete audit trail from raw to clean.\n\n6. **Rigorous verification:** Validate transformations, quantify quality improvements, spot-check samples, document limitations - ensures clean data is actually clean and limitations are known.\n\nFollow this process and you'll create well-documented clean tables with validated quality improvements, complete audit trail from raw data to analysis-ready data, and honest assessment of what was cleaned and what limitations remain.\n\n---"
              },
              {
                "name": "comparative-analysis",
                "description": "Systematic comparison of segments, cohorts, or time periods - ensure fair apples-to-apples comparisons, identify meaningful differences, explain WHY differences exist",
                "path": "plugins/datapeeker/skills/comparative-analysis/SKILL.md",
                "frontmatter": {
                  "name": "comparative-analysis",
                  "description": "Systematic comparison of segments, cohorts, or time periods - ensure fair apples-to-apples comparisons, identify meaningful differences, explain WHY differences exist"
                },
                "content": "# Comparative Analysis Process\n\n## Overview\n\nThis skill guides you through systematic comparison of two or more groups, segments, cohorts, or time periods. Unlike exploratory-analysis (where you discover patterns) or guided-investigation (where you answer broad questions), comparative analysis helps you **rigorously compare** specific groups and **explain** why they differ.\n\nComparative analysis is appropriate when:\n- You need to compare performance between specific segments (regions, products, customer cohorts)\n- You want to understand how groups differ across multiple dimensions\n- You're evaluating changes before/after an intervention or between time periods\n- The user asks \"How does X compare to Y?\" or \"What's different about segment A vs B?\"\n- You need to make fair, apples-to-apples comparisons with controls for confounding factors\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis comparative-analysis <name>`)\n4. Have clearly defined what you're comparing (user must specify comparison groups)\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Define Comparison - pending\n- Phase 2: Segment Definition - pending\n- Phase 3: Metric Comparison - pending\n- Phase 4: Difference Explanation - pending\n- Phase 5: Conclusions and Recommendations - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Define Comparison\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Clarified what groups/segments/periods are being compared\n- [ ] Identified the comparison objective (what question does this answer?)\n- [ ] Determined comparison type (segments, cohorts, time periods, before/after)\n- [ ] Established what metrics will be compared\n- [ ] Documented the comparison framework\n- [ ] Saved to `01 - comparison-definition.md`\n\n### Instructions\n\n1. **Clarify the comparison goal with the user**\n\nAsk clarifying questions:\n- What specific groups/segments do you want to compare?\n- What's the purpose of this comparison? What decision will it inform?\n- What differences would be meaningful or actionable?\n- Are there specific metrics you care about most?\n- What time period should the comparison cover?\n\n2. **Define the comparison framework**\n\nCreate `analysis/[session-name]/01-comparison-definition.md` with: ./templates/phase-1.md\n\n3. **Get user confirmation before proceeding**\n   - Review comparison definition with user\n   - Confirm groups are defined correctly\n   - Verify metrics align with user's goals\n   - Adjust framework if needed\n\n**Common Rationalization:** \"The comparison is obvious, I'll just start querying\"\n**Reality:** Without explicit definition, you'll make unstated assumptions about what \"fair\" means.\n\n**Common Rationalization:** \"I'll compare everything and see what's different\"\n**Reality:** Unfocused comparison produces noise. Define specific metrics and materiality thresholds.\n\n---\n\n## Phase 2: Segment Definition\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Verified that comparison groups exist in the data\n- [ ] Validated group definitions with actual queries\n- [ ] Checked for data quality issues within each group\n- [ ] Documented group characteristics and sample sizes\n- [ ] Confirmed groups are comparable (similar data quality, coverage)\n- [ ] Saved to `02 - segment-definition.md`\n\n### Instructions\n\n1. **Validate that groups exist and are well-defined**\n\nCreate `analysis/[session-name]/02-segment-definition.md` with: ./templates/phase-2.md\n\n2. **Handle segment definition issues**\n\nIf groups are not comparable:\n- **Sample size too small:** Consider combining groups, expanding time window, or adjusting comparison\n- **Different time periods:** Either align periods or explicitly control for temporal effects\n- **Data quality differs:** Document the difference and consider if it invalidates comparison\n- **Overlapping groups:** Redefine to ensure groups are mutually exclusive\n\n3. **Document any exclusions or adjustments**\n\nIf you exclude outliers, filter dates, or adjust definitions, document clearly:\n```markdown\n## Adjustments Made for Fair Comparison\n\n1. **Outlier handling:** Excluded 8 transactions >$50,000 as data entry errors (confirmed with field validation)\n2. **Date alignment:** Limited both groups to Feb 1 - Apr 30 to match shorter group's coverage\n3. **Null handling:** Excluded 127 transactions with NULL customer_id from both groups\n```\n\n**Common Rationalization:** \"The groups look fine, I'll skip validation\"\n**Reality:** Unstated data quality issues or sample size problems will invalidate your comparison.\n\n**Common Rationalization:** \"Sample sizes are different but that's okay\"\n**Reality:** Large sample size differences require per-capita normalization. Raw totals are misleading.\n\n---\n\n## Phase 3: Metric Comparison\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Calculated all primary metrics for each group\n- [ ] Calculated all secondary metrics for each group\n- [ ] Computed differences (absolute and percentage) between groups\n- [ ] Created comparison visualizations (tables, charts)\n- [ ] Identified which metrics differ meaningfully (per materiality threshold)\n- [ ] Documented all comparisons with queries and results\n- [ ] Saved to `03 - metric-comparison.md`\n\n### Instructions\n\n1. **Compare groups systematically across all metrics**\n\nCreate `analysis/[session-name]/03-metric-comparison.md` with: ./templates/phase-3.md\n\n2. **Be rigorous about normalization**\n\nCompare apples-to-apples:\n- Use per-customer or per-day metrics, not raw totals (unless groups are identical size)\n- Calculate percentages within group (e.g., % of group's revenue by category)\n- Use rate metrics (transactions per customer, revenue per transaction)\n\nWrong: \"Northeast has $458K revenue vs Southeast's $392K\"\nRight: \"Northeast has $161/customer vs Southeast's $150/customer (7.5% higher)\"\n\n3. **Compute statistical and practical significance**\n\n**Statistical significance:** Is difference larger than random variation would explain?\n- With large samples (>1000), small differences can be statistically significant\n- Document sample sizes to provide context\n\n**Practical significance:** Is difference large enough to matter for decisions?\n- Use materiality threshold from Phase 1\n- 5% difference in revenue might be huge for a billion-dollar company, trivial for a startup\n\n4. **Use visualization to make differences clear**\n\n- Side-by-side tables with difference columns\n- ASCII bar charts showing relative magnitudes\n- Percentage difference callouts\n\n**Common Rationalization:** \"I'll just show the raw numbers and let the user interpret\"\n**Reality:** Your job is interpretation. Show differences clearly and explain what they mean.\n\n**Common Rationalization:** \"Northeast has more revenue, that's the answer\"\n**Reality:** Explain WHY - more customers? Higher spend per customer? Different product mix? Dig deeper.\n\n**Common Rationalization:** \"These differences are statistically significant, so they matter\"\n**Reality:** Statistical significance ≠ practical significance. A 1% difference might be \"significant\" but not meaningful.\n\n---\n\n## Phase 4: Difference Explanation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Identified the 2-3 most important differences from Phase 3\n- [ ] For each difference, investigated potential causes\n- [ ] Analyzed confounding factors\n- [ ] Ruled out alternative explanations where possible\n- [ ] Quantified relative contribution of different factors\n- [ ] Documented explanation analysis with supporting queries\n- [ ] Saved to `04 - difference-explanation.md`\n\n### Instructions\n\n1. **Focus on the most meaningful differences**\n\nDon't try to explain every small difference. Focus on:\n- Differences that exceed materiality threshold\n- Differences that are surprising or counter-intuitive\n- Differences that inform the comparison objective\n\n2. **Investigate WHY differences exist**\n\nCreate `analysis/[session-name]/04-difference-explanation.md` with: ./templates/phase-4.md\n\n3. **Be intellectually honest about causation**\n\nComparative analysis shows WHAT differs, not always WHY:\n- With observational data, you usually can't prove causation\n- Multiple explanations may fit the data\n- Acknowledge uncertainty and alternative explanations\n\n4. **Decompose complex differences**\n\nWhen metrics differ, break them into components:\n- Revenue = Customers × Revenue per Customer\n- Revenue per Customer = Transactions per Customer × Revenue per Transaction\n- Identify which component drives the overall difference\n\n**Common Rationalization:** \"I found the difference, that's enough\"\n**Reality:** Finding the difference is half the job. Explaining WHY is equally important.\n\n**Common Rationalization:** \"This factor correlates with the difference, so it's the cause\"\n**Reality:** Correlation ≠ causation. Multiple factors may correlate. Be cautious about causal claims.\n\n**Common Rationalization:** \"I'll ignore confounds since I can't measure them\"\n**Reality:** Acknowledge unmeasured confounds explicitly. They limit your conclusions but shouldn't be ignored.\n\n---\n\n## Phase 5: Conclusions and Recommendations\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Summarized key differences and their magnitudes\n- [ ] Explained most likely drivers of differences\n- [ ] Assessed confidence in findings\n- [ ] Identified actionable insights\n- [ ] Made specific recommendations based on comparison\n- [ ] Documented limitations and caveats\n- [ ] Saved to `05 - conclusions-and-recommendations.md`\n- [ ] Updated `00 - overview.md` with comparison summary\n\n### Instructions\n\n1. **Synthesize findings into clear conclusions**\n\nCreate `analysis/[session-name]/05-conclusions-and-recommendations.md` with: ./templates/phase-5.md\n\n2. **Update overview document**\n\nUpdate: `00 - overview.md`\n\nAdd at the end:\n\n```markdown\n## Comparison Summary\n\n**Groups Compared:** [Groups]\n\n**Time Period:** [Date range]\n\n**Comparison Completed:** [Date]\n\n---\n\n## Key Differences Identified\n\n1. **[Difference 1]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n2. **[Difference 2]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n3. **[Difference 3]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n---\n\n## Top Recommendations\n\n1. **[Recommendation 1]:** [One sentence]\n   - Expected impact: [Magnitude]\n\n2. **[Recommendation 2]:** [One sentence]\n   - Expected impact: [Magnitude]\n\n---\n\n## File Index\n\n- 01 - Comparison Definition\n- 02 - Segment Definition\n- 03 - Metric Comparison\n- 04 - Difference Explanation\n- 05 - Conclusions and Recommendations\n```\n\n3. **Communicate findings to user**\n\nPresent conclusions clearly:\n- Lead with directional summary (which group \"wins\" and why)\n- Quantify key differences with specific numbers\n- Explain drivers of differences\n- Acknowledge uncertainty and limitations\n- Provide actionable recommendations\n- Suggest follow-up questions\n\n**Common Rationalization:** \"I'll just present all the numbers and let the user draw conclusions\"\n**Reality:** Your job is to interpret and synthesize. Provide clear conclusions, not just data dumps.\n\n**Common Rationalization:** \"I'm 100% confident in these conclusions\"\n**Reality:** Be honest about confidence levels and limitations. Overconfidence undermines credibility.\n\n**Common Rationalization:** \"Comparison complete, no follow-up needed\"\n**Reality:** Comparisons often raise more questions than they answer. Identify high-value follow-up investigations.\n\n---\n\n## Common Rationalizations\n\n### \"The groups are obviously different, I'll skip formal definition\"\n**Why this is wrong:** \"Obvious\" differences often have unstated assumptions. Explicit definition prevents misunderstandings and ensures you're answering the right question.\n\n**Do instead:** Complete Phase 1 fully. Define groups, metrics, and materiality thresholds explicitly.\n\n### \"Sample sizes look fine, I'll skip validation\"\n**Why this is wrong:** Sample size is only one aspect. Data quality, temporal alignment, and outliers can invalidate comparisons even with large samples.\n\n**Do instead:** Validate groups thoroughly in Phase 2. Check quality, coverage, and comparability.\n\n### \"I'll just compare raw totals\"\n**Why this is wrong:** Raw totals are misleading when groups have different sizes. $500K vs $400K tells you nothing if one group is 2x the size of the other.\n\n**Do instead:** Normalize by customers, days, or transactions. Compare per-capita or rate metrics.\n\n### \"Northeast has higher revenue, that's the finding\"\n**Why this is wrong:** Stating WHAT differs without explaining WHY provides limited value. The explanation is where actionable insights live.\n\n**Do instead:** Decompose differences. Explain whether higher revenue comes from more customers, higher per-customer value, different mix, etc.\n\n### \"These groups are different, so one must be better\"\n**Why this is wrong:** Different doesn't mean better. Context matters. Lower-revenue group might serve a different market, have different goals, or optimize for different metrics.\n\n**Do instead:** Interpret differences in context. Consider whether \"better\" is even the right framing.\n\n### \"I found a correlation, so that's the cause\"\n**Why this is wrong:** Correlation ≠ causation. Many factors correlate with outcomes without causing them.\n\n**Do instead:** Be cautious with causal language. Say \"associated with\" or \"correlated with\" rather than \"caused by\" unless you have experimental evidence.\n\n### \"I'll ignore the confounds I can't measure\"\n**Why this is wrong:** Unmeasured confounds don't disappear by ignoring them. They limit what you can conclude.\n\n**Do instead:** Explicitly acknowledge unmeasured confounds in Phase 4. Explain how they limit causal interpretation.\n\n### \"I'll recommend that Southeast copy Northeast exactly\"\n**Why this is wrong:** You don't know if differences are due to replicable practices or immutable characteristics (market size, demographics, etc.).\n\n**Do instead:** Recommend further investigation to understand whether differences are actionable or structural.\n\n### \"This comparison answered the question completely\"\n**Why this is wrong:** Comparisons typically reveal new questions about root causes, generalizability, and interventions.\n\n**Do instead:** Identify high-value follow-up questions in Phase 5. Guide next investigations.\n\n### \"Statistical significance means it matters\"\n**Why this is wrong:** With large samples, tiny differences can be statistically significant but practically meaningless.\n\n**Do instead:** Focus on practical significance (materiality threshold). A 1% difference might be \"significant\" but not meaningful.\n\n---\n\n## Summary\n\nThis skill ensures rigorous, fair comparisons by:\n\n1. **Defining comparisons explicitly:** Clear groups, metrics, and materiality thresholds prevent unstated assumptions\n2. **Validating segment quality:** Ensuring groups are comparable prevents invalid comparisons\n3. **Comparing systematically:** Multi-metric analysis reveals patterns that single metrics miss\n4. **Explaining differences:** Understanding WHY groups differ is as important as knowing WHAT differs\n5. **Acknowledging limitations:** Honest assessment of confounds and causation builds credibility\n6. **Providing actionable insights:** Converting findings into recommendations and follow-up questions\n\nFollow this process and you'll deliver fair, rigorous comparisons that explain not just WHAT differs but WHY, identify actionable opportunities, and guide follow-up investigations."
              },
              {
                "name": "creating-visualizations",
                "description": "Component skill for creating effective visualizations (terminal-based and image-based) in DataPeeker analysis sessions",
                "path": "plugins/datapeeker/skills/creating-visualizations/SKILL.md",
                "frontmatter": {
                  "name": "creating-visualizations",
                  "description": "Component skill for creating effective visualizations (terminal-based and image-based) in DataPeeker analysis sessions"
                },
                "content": "# Creating Visualizations\n\n## Purpose\n\nThis component skill guides creation of clear, effective visualizations for analytics documentation. Use it when:\n- Presenting query results in a more visual format\n- Need to reveal patterns that are hard to see in raw numbers\n- Creating reports or documentation that will be read by stakeholders\n- Documenting data workflows, lineage, or database schemas\n- Referenced by process skills requiring data visualization\n\n**Supports two approaches:**\n- **Terminal-based** (plotext, sparklines, etc.) - For interactive analysis\n- **Image-based** (Kroki: Mermaid, GraphViz, Vega-Lite) - For reports and complex diagrams\n\n## Prerequisites\n\n- Query results obtained and interpreted\n- Understanding of patterns to highlight (use `interpreting-results` skill)\n- Analysis documented in markdown files\n- Clear communication goal for the visualization\n\n## Visualization Creation Process\n\nCreate a TodoWrite checklist for the 4-phase visualization process:\n\n```\nPhase 1: Choose Visualization Type\nPhase 2: Structure Data for Display\nPhase 3: Create Visualization\nPhase 4: Annotate with Context\n```\n\nMark each phase as you complete it. Include visualizations in numbered markdown files alongside queries and interpretations.\n\n---\n\n## Phase 1: Choose Visualization Type\n\n**Goal:** Select the right visualization format for your data and communication goal.\n\n### Visualization Selection Decision Tree\n\nAsk these questions in order:\n\n**1. What type of data am I visualizing?**\n\n- **Single summary statistic** → Callout box or highlighted metric\n- **List of values** → Table or ranked list\n- **Distribution across categories** → Bar chart (ASCII or markdown)\n- **Time series** → Line chart (sparkline) or time table\n- **Comparison between groups** → Side-by-side table or grouped bars\n- **Part-to-whole relationship** → Percentage table or ASCII pie chart\n- **Correlation or relationship** → Scatter (character plot) or correlation matrix\n\n**2. What is my primary communication goal?**\n\n- **Show exact values** → Table with clear formatting\n- **Show relative magnitudes** → Bar chart or ranked list\n- **Show trends over time** → Sparkline or time series table\n- **Show distribution shape** → Histogram (ASCII)\n- **Show ranking** → Ordered list or horizontal bars\n- **Show proportions** → Percentage table with bars\n\n**3. How many data points?**\n\n- **1-5 values** → Callout boxes or simple list\n- **6-20 values** → Table or bar chart\n- **21-50 values** → Grouped table or histogram\n- **50+ values** → Summary statistics + histogram, or top/bottom N\n\n**4. Who is the audience?**\n\n- **Technical analysts** → Full tables with precision\n- **Business stakeholders** → Simplified visuals with key takeaways\n- **Mixed audience** → Visual summary + detailed table\n\n### Available Visualization Types\n\n**DataPeeker supports two complementary approaches:**\n\n#### Terminal-Based Formats (Primary for analysis):\n1. **Markdown Tables** - Structured data with alignment\n2. **ASCII Bar Charts** - Visual magnitude comparison (plotext, termgraph)\n3. **Sparklines** - Compact trend indicators (sparklines library)\n4. **ASCII Histograms** - Distribution visualization (plotext)\n5. **Callout Boxes** - Highlighting key metrics\n6. **Ranked Lists** - Ordered items with context\n7. **Comparison Tables** - Side-by-side metrics\n8. **Line Plots** - Time series (plotext, asciichartpy)\n\n#### Image-Based Formats (For reports and complex diagrams):\n1. **Mermaid** - Flowcharts, Gantt charts, workflows\n2. **GraphViz** - Network graphs, data lineage, hierarchies\n3. **Vega-Lite** - Statistical charts (bar, line, scatter)\n4. **ERD/DBML** - Database schemas\n\n**Choose based on:**\n- What pattern you want to communicate\n- Where the output will be viewed (terminal vs report)\n- Complexity of the visualization needed\n\n---\n\n## Phase 2: Structure Data for Display\n\n**Goal:** Organize and format data for effective visualization.\n\n### Data Preparation Checklist\n\nBefore creating visualization:\n\n**1. Sort appropriately:**\n```markdown\nFor ranked data:\n- Sort by the metric you want to emphasize (descending for \"top N\")\n- Consider: Alphabetical only if order doesn't matter\n\nFor time series:\n- Sort chronologically (oldest to newest, or newest first if recent matters)\n\nFor categorical:\n- Sort by frequency, magnitude, or logical grouping\n- Avoid: Random or database-default ordering\n```\n\n**2. Round to appropriate precision:**\n```markdown\nExamples:\n- Revenue: Round to thousands or whole dollars (not $1,234.56789)\n- Percentages: 1-2 decimal places (14.3%, not 14.285714%)\n- Counts: Whole numbers only (1,234 not 1234.0)\n- Ratios: 2-3 significant figures (2.4x not 2.3567x)\n\nRule: Show precision that matches the certainty of your data\n```\n\n**3. Add calculated columns:**\n```markdown\nUseful additions:\n- Percentage of total\n- Difference from average/baseline\n- Rank or percentile\n- Running totals or moving averages\n- Year-over-year change\n```\n\n**4. Consider grouping:**\n```markdown\nFor large datasets:\n- Show Top N + \"Other\" row\n- Group by logical categories\n- Use ranges/buckets for continuous data\n- Separate outliers from main distribution\n```\n\n**5. Format for readability:**\n```markdown\nBest practices:\n- Add thousand separators (1,234 not 1234)\n- Use consistent decimal places within columns\n- Align numbers right, text left\n- Include units in headers ($, %, units)\n```\n\n---\n\n## Phase 3: Create Visualization\n\n**Goal:** Build the actual visualization using appropriate format and tools.\n\n### Two Visualization Approaches\n\nDataPeeker supports two complementary visualization approaches:\n\n#### 1. Terminal-Based Visualizations (Primary)\n\n**Use for:**\n- Interactive terminal/Jupyter notebook analysis\n- Quick data exploration\n- Markdown documentation that stays in terminal\n- Fast iteration without external dependencies\n\n**Available formats:**\n1. **Markdown Tables** - Structured data with multiple columns, exact values\n2. **ASCII Bar Charts** - Visual magnitude comparison, relative sizes\n3. **Sparklines** - Compact trend indicators with Unicode characters\n4. **ASCII Histograms** - Distribution visualization, shape and spread\n5. **Callout Boxes** - Highlighting key metrics or insights\n6. **Ranked Lists** - Top/bottom N items with narrative context\n7. **Comparison Tables** - Side-by-side metrics across segments or time\n8. **Line Plots** - Time series and trends\n\n**→ See [terminal-formats.md](./terminal-formats.md) for implementation**\n\n#### 2. Image-Based Visualizations (via Kroki)\n\n**Use for:**\n- Reports and presentations (embedded images)\n- Complex diagrams (workflows, data lineage, relationships)\n- Database schemas and architecture\n- Documentation that needs to be viewed outside terminal\n- High-quality charts for stakeholder communication\n\n**Available formats:**\n1. **Mermaid** - Flowcharts, Gantt charts, sequence diagrams\n2. **GraphViz** - Network graphs, data lineage, hierarchies\n3. **Vega-Lite** - Statistical charts (bar, line, scatter, histograms)\n4. **D2** - Modern diagrams, architecture, data models\n5. **ERD/DBML** - Database schemas and relationships\n\n**→ See [image-formats.md](./image-formats.md) for implementation**\n\n### Choosing Between Terminal and Image Formats\n\n**Use Terminal formats when:**\n- Working interactively in analysis session\n- Output stays in markdown/terminal\n- Quick iteration and exploration\n- Simple charts and tables\n\n**Use Image formats when:**\n- Creating final reports or presentations\n- Visualizing complex relationships (data lineage, workflows)\n- Documenting database schemas\n- Output needs to be embedded in documents/web\n- Audience views outside terminal environment\n\n**Can use both:**\n- Terminal for exploration → Image for final report\n- Tables (terminal) + Diagrams (image) in same document\n\n### ⚠️ CRITICAL: Tool Usage Requirements\n\n**MANDATORY:** All visualizations (bar charts, line plots, histograms, sparklines, scatter plots) **MUST** use established visualization tools. **NEVER create these manually.**\n\n**✅ ALLOWED - Manual Creation:**\n- Markdown tables with exact values\n- Callout boxes and formatted text\n- Ranked lists with exact numbers\n\n**❌ PROHIBITED - Manual Creation:**\n- Bar charts (no manual █ characters)\n- Line plots or time series (no manual * or - characters)\n- Histograms\n- Sparklines (no manual ▁▂▃▄▅▆▇█ characters)\n- Any visualization requiring scaling or positioning\n\n### Implementation Details\n\n**📄 For visualization implementations, use these guides:**\n\n#### Terminal-Based Visualizations\n**[terminal-formats.md](./terminal-formats.md)**\n\nThis document provides:\n- **Mandatory tool usage principles** (read this first!)\n- **Quick Start guide** with tool installation (plotext, asciichartpy, termgraph, sparklines)\n- **Complete code examples** for each visualization type using proper tools\n- **SQLite integration examples** for generating visualizations from query results\n\n**The rule:** If it visualizes relative magnitudes, trends, or distributions → USE A TOOL. If it's exact numbers in a table → Manual creation is fine.\n\n#### Image-Based Visualizations\n**[image-formats.md](./image-formats.md)**\n\nThis document provides:\n- **Kroki overview** - Unified API for generating diagrams from text\n- **Quick Start guide** with Python examples and API usage\n- **Format selection guide** - When to use Mermaid vs GraphViz vs Vega-Lite\n- **Complete implementation guides** for each format in `formats/` directory:\n  - [Mermaid](./formats/mermaid.md) - Flowcharts, Gantt, sequences\n  - [GraphViz](./formats/graphviz.md) - Network graphs, data lineage\n  - [Vega-Lite](./formats/vega-lite.md) - Statistical charts\n- **DataPeeker integration examples** - Visualizing data workflows and schemas\n\n---\n\n## Phase 4: Annotate with Context\n\n**Goal:** Add context and guidance so visualization is self-explanatory.\n\n### Annotation Checklist\n\nEvery visualization should include:\n\n**1. Title/Caption:**\n```markdown\n## [Clear, descriptive title that states what is being shown]\n\nExample:\n✓ Good: \"Monthly Revenue by Product Category (Jan-Dec 2024)\"\n✗ Bad: \"Revenue Chart\"\n```\n\n**2. Data source and date:**\n```markdown\n**Data source:** analytics.db, orders table\n**Time period:** Q4 2024 (Oct 1 - Dec 31)\n**Last updated:** 2025-11-18\n```\n\n**3. Key takeaway (above or below visualization):**\n```markdown\n**Key Finding:** Electronics drove 42.5% of Q4 revenue despite representing\nonly 15% of order volume, indicating premium product performance.\n```\n\n**4. Units and scale:**\n```markdown\n- Include $ or % symbols\n- Clarify if values are in thousands: ($000s)\n- Note if values are indexed or normalized\n- Specify timezone for timestamps\n```\n\n**5. Context for interpretation:**\n```markdown\n**Context notes:**\n- Q4 includes Black Friday/Cyber Monday (Nov 24-27)\n- New product line launched Oct 15, affecting Electronics category\n- Shipping delays in December may have suppressed orders\n```\n\n**6. Limitations and caveats:**\n```markdown\n**Caveats:**\n- Data excludes returns and cancellations\n- International orders converted to USD at average quarterly exchange rate\n- First week of October had incomplete data due to system migration\n```\n\n**7. What to look for:**\n```markdown\n**What to notice:**\n- Electronics peak in November (holiday season)\n- Clothing shows consistent decline (investigate seasonality)\n- Sports category smallest but growing fastest (+45% QoQ)\n```\n\n\n## Visualization Best Practices\n\n### DO:\n\n1. **Choose format based on communication goal, not convenience**\n   - Ask: \"What do I want the reader to notice first?\"\n   - Match visualization to insight you're highlighting\n\n2. **Make visualizations self-contained**\n   - Reader should understand without reading entire document\n   - Include title, units, source, key takeaway\n\n3. **Use consistent formatting within analysis**\n   - Same bar width for all bar charts\n   - Same precision for similar metrics\n   - Consistent color/symbol conventions (if using)\n\n4. **Highlight what matters**\n   - Use **bold** for most important values\n   - Put key finding at top or bottom\n   - Add 🔥, ⚠️, ✓ symbols sparingly for emphasis\n\n5. **Test readability**\n   - View in markdown preview (not just raw markdown)\n   - Check alignment and spacing\n   - Ensure visualization works in different font sizes\n\n6. **Layer detail progressively**\n   - Summary visualization first (bar chart, key metrics)\n   - Detailed table second (full data)\n   - Technical notes third (methodology, caveats)\n\n7. **Combine formats when helpful**\n   - Bar chart + exact values table\n   - Sparkline + summary statistics\n   - Visualization + narrative interpretation\n\n### DON'T:\n\n1. **Don't create visualizations for their own sake**\n   - If a simple table is clearer, use the table\n   - Visualization should reveal patterns, not obscure them\n\n2. **Don't use excessive precision**\n   - Revenue in dollars, not cents ($1,234 not $1,234.56)\n   - Percentages to 1 decimal place (14.3% not 14.285714%)\n\n3. **Don't hide important caveats**\n   - Data quality issues must be visible\n   - Exclusions and filters must be noted\n   - Sample size and time period must be clear\n\n4. **Don't use misleading scales**\n   - Bar charts should start at zero (not truncated y-axis)\n   - Be explicit if using non-zero baseline\n\n5. **Don't over-format**\n   - Too many symbols/colors creates visual noise\n   - Keep it simple and professional\n\n6. **Don't assume reader knows context**\n   - Define abbreviations\n   - Explain what metrics mean\n   - Note if using non-standard calculations\n\n7. **Don't forget the \"so what?\"**\n   - Every visualization needs an interpretation\n   - State implications, not just observations\n\n---\n\n## Common Visualization Patterns\n\n### Pattern 1: Before/After Comparison\n\n```markdown\n## Impact of Pricing Change (Oct 15, 2024)\n\n### Before Pricing Change (Oct 1-14)\n- Average Order Value: **$145.67**\n- Daily Orders: **234**\n- Daily Revenue: **$34,087**\n\n### After Pricing Change (Oct 15-31)\n- Average Order Value: **$127.23** (↓ $18.44, -12.7%)\n- Daily Orders: **289** (↑ 55, +23.5%)\n- Daily Revenue: **$36,769** (↑ $2,682, +7.9%)\n\n**Net effect:** Lower prices increased volume enough to grow total revenue.\n```\n\n### Pattern 2: Distribution Summary\n\n**⚠️ Use plotext to create histograms - DO NOT create manually**\n\nShow distribution with summary statistics:\n\n```python\nimport plotext as plt\nimport statistics\n\n# Customer LTV values from query\nltv_values = [423, 687, 892, 2145, ...]  # Your data\n\nplt.hist(ltv_values, bins=7)\nplt.title('Customer Lifetime Value Distribution')\nplt.xlabel('Customer LTV ($)')\nplt.ylabel('Number of Customers')\nplt.show()\n\n# Show summary statistics\nprint(f\"\\nSummary Statistics:\")\nprint(f\"Median LTV: ${statistics.median(ltv_values):,.0f}\")\nprint(f\"Mean LTV: ${statistics.mean(ltv_values):,.0f}\")\nprint(f\"75th percentile: ${statistics.quantiles(ltv_values, n=4)[2]:,.0f}\")\n```\n\n**See terminal-formats.md Format 4 for complete histogram examples.**\n\n### Pattern 3: Segmentation Analysis\n\n**✅ Tables are fine for exact values, use plotext/termgraph for visual breakdown**\n\n```markdown\n## Customer Segmentation by Purchase Behavior\n\n| Segment         | Customers | Avg Orders | Avg LTV | % of Revenue | Strategy      |\n|:----------------|----------:|-----------:|--------:|-------------:|:--------------|\n| **Champions**   |       234 |       18.3 |  $2,145 |        18.2% | VIP treatment |\n| **Loyal**       |     1,456 |        8.7 |    $892 |        47.3% | Retain & grow |\n| **Potential**   |     3,678 |        2.4 |    $287 |        38.5% | Nurture       |\n| **At Risk**     |       892 |        1.2 |    $156 |         5.1% | Win-back      |\n| **Lost**        |     2,134 |        1.0 |     $87 |         6.8% | Low priority  |\n\n**Key insight:** Top two segments (Champions + Loyal) are only 18% of customer\nbase but generate 66% of revenue. These 1,690 customers should receive majority\nof retention investment.\n```\n\n**For visual breakdown, use plotext:**\n```python\nimport plotext as plt\n\nsegments = ['Champions', 'Loyal', 'Potential', 'At Risk', 'Lost']\nrevenue = [501030, 1299552, 1055586, 139152, 185658]\n\nplt.simple_bar(segments, revenue, title='Revenue by Customer Segment')\nplt.xlabel('Segment')\nplt.ylabel('Revenue ($)')\nplt.show()\n```\n\n**See terminal-formats.md Format 2 for complete bar chart examples.**\n\n### Pattern 4: Time Series with Annotations\n\n**⚠️ Use plotext or asciichartpy - DO NOT create manually**\n\n```python\nimport plotext as plt\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nrevenue = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5,\n           1.5, 1.6, 1.7, 1.7, 1.9, 2.0]  # Revenue in millions\n\nplt.plot(months, revenue)\nplt.title('Monthly Revenue Trend with Key Events')\nplt.xlabel('Month')\nplt.ylabel('Revenue ($M)')\nplt.show()\n\nprint(\"\\nKey Events:\")\nprint(\"- Oct 1: Q4 begins, seasonal uptick expected\")\nprint(\"- Oct 15: Pricing change (-10% on popular items)\")\nprint(\"- Nov 1: New product line launched (premium segment)\")\nprint(\"- Nov 24-27: Black Friday/Cyber Monday surge\")\nprint(\"\\nAnalysis: Revenue growth accelerated after new product launch (Nov),\")\nprint(\"suggesting demand for premium options. Pricing change impact unclear due to\")\nprint(\"seasonal overlap.\")\n```\n\n**See terminal-formats.md Format 8 for complete line plot examples.**\n\n### Pattern 5: Funnel Analysis\n\n**✅ Tables for exact values, use plotext for visualization**\n\n```markdown\n## Purchase Funnel Conversion Rates\n\n| Step              | Count   | Conversion | Drop-off | Notes |\n|:------------------|--------:|-----------:|---------:|:------|\n| 1. Site Visitors  | 100,000 |     100.0% |        — |       |\n| 2. Product Viewers|  45,000 |      45.0% |    55.0% | High bounce rate |\n| 3. Add to Cart    |  12,000 |      26.7% |    73.3% |       |\n| 4. Begin Checkout |   8,500 |      70.8% |    29.2% | Cart abandonment |\n| 5. Complete       |   3,200 |      37.6% |    62.4% | Payment issues? |\n\n**Overall Conversion:** 3.2%\n\n**Problem areas:**\n1. **Bounce rate (55%):** Half of visitors leave without viewing products\n   - Action: Improve landing page, clearer value proposition\n\n2. **Cart abandonment (29%):** Losing 3,500 potential customers at checkout\n   - Action: Simplify checkout, add progress indicator\n\n3. **Checkout failure (62%):** Massive drop-off at payment\n   - Action: URGENT — investigate payment gateway, error messages\n\n**Quick win:** Fixing checkout issues could 2.6x conversion (3.2% → 8.4%)\n```\n\n**For funnel visualization, use plotext:**\n```python\nimport plotext as plt\n\nsteps = ['Visitors', 'Viewers', 'Cart', 'Checkout', 'Purchase']\ncounts = [100000, 45000, 12000, 8500, 3200]\n\nplt.simple_bar(steps, counts, title='Purchase Funnel')\nplt.xlabel('Funnel Step')\nplt.ylabel('Count')\nplt.show()\n```\n\n**See terminal-formats.md Format 2 for complete bar chart examples.**\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `creating-visualizations` component skill to present query results\nvisually, making patterns and insights more accessible to stakeholders.\n```\n\nWhen creating visualizations during analysis:\n1. Choose format based on communication goal (Phase 1)\n2. Structure data for clarity (Phase 2)\n3. Build visualization with appropriate text format (Phase 3)\n4. Annotate with context and interpretation (Phase 4)\n\nThis ensures analysis outputs are not just technically correct but also\neffectively communicated and actionable.\n\n---\n\n## When to Visualize\n\n**Visualize when:**\n- Pattern is easier to see visually than in raw numbers\n- Presenting to stakeholders who need quick understanding\n- Comparing multiple segments, time periods, or metrics\n- Distribution shape matters (histograms)\n- Trend direction matters (sparklines, time series)\n\n**Use tables when:**\n- Exact values are critical\n- Reader needs to reference specific numbers\n- Data is already structured and scannable\n- Audience is technical and prefers precision\n\n**Use both when:**\n- Visualization reveals pattern, table provides detail\n- Different audiences (executive summary + appendix)\n- Building progressive disclosure (overview → detail)\n\n---\n\n## Quality Checklist\n\nBefore finalizing any visualization, verify:\n\n- [ ] Visualization has clear, descriptive title\n- [ ] Units are labeled ($ , %, etc.)\n- [ ] Data source and time period documented\n- [ ] Key takeaway stated explicitly\n- [ ] Appropriate precision (not over-rounded or over-precise)\n- [ ] Scale is appropriate (bars from zero, etc.)\n- [ ] Annotations explain what to notice\n- [ ] Caveats and limitations noted\n- [ ] Visualization renders correctly in markdown preview\n- [ ] Numbers match source query results\n- [ ] Format matches communication goal\n- [ ] Audience can understand without additional context\n\n**If any checklist item fails, revise before including in analysis.**"
              },
              {
                "name": "detect-foreign-keys",
                "description": "Identify foreign key relationships between tables using heuristics, value overlap analysis, and referential integrity checks",
                "path": "plugins/datapeeker/skills/detect-foreign-keys/SKILL.md",
                "frontmatter": {
                  "name": "detect-foreign-keys",
                  "description": "Identify foreign key relationships between tables using heuristics, value overlap analysis, and referential integrity checks"
                },
                "content": "# Detecting Foreign Keys\n\n## Purpose\n\nThis component skill guides systematic foreign key relationship detection in relational databases. Use it when:\n- Multiple tables exist in the database and relationships are undocumented\n- Need to understand table relationships before joining data\n- Validating referential integrity between tables\n- Identifying orphaned records that reference non-existent parent records\n- Referenced by importing-data or cleaning-data skills requiring relationship analysis\n\n## Prerequisites\n\n- Tables exist in database (relational database with SQL support)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Table schemas have been examined (Phase 1 of understanding-data)\n- Analysis workspace created\n\n## Foreign Key Detection Process\n\nCreate a TodoWrite checklist for the 5-phase FK detection process:\n\n```\nPhase 1: Candidate Identification - pending\nPhase 2: Value Overlap Analysis - pending\nPhase 3: Cardinality Assessment - pending\nPhase 4: Referential Integrity Validation - pending\nPhase 5: Relationship Documentation - pending\n```\n\nMark each phase as you complete it. Document all findings in structured format.\n\n---\n\n## Phase 1: Candidate Identification\n\n**Goal:** Identify columns that are likely foreign keys based on naming patterns, data types, and uniqueness.\n\n### Identify Candidate FK Columns by Naming Convention\n\n**Common FK naming patterns:**\n- Columns ending in `_id` (e.g., `customer_id`, `product_id`)\n- Columns ending in `Id` (e.g., `customerId`, `productId`)\n- Columns named exactly `id` (but only in child tables)\n- Columns starting with `fk_` (e.g., `fk_customer`)\n- Columns matching another table name (e.g., `customer` in orders table)\n\n```sql\n-- List all columns across all tables\nSELECT\n  m.name as table_name,\n  p.name as column_name,\n  p.type as column_type\nFROM sqlite_master m\nJOIN pragma_table_info(m.name) p\nWHERE m.type = 'table'\n  AND m.name NOT LIKE 'sqlite_%'\n  AND (\n    p.name LIKE '%_id'\n    OR p.name LIKE '%Id'\n    OR p.name LIKE 'fk_%'\n    OR p.name = 'id'\n  )\nORDER BY m.name, p.name;\n```\n\n**Document:**\n- List of candidate FK columns per table\n- Note naming patterns observed\n- Flag columns that might be composite keys (multiple FK columns in same table)\n\n### Identify Candidate PK Columns\n\n**Primary key characteristics:**\n- Named `id`, `[table]_id`, or similar\n- INTEGER or TEXT type\n- Likely to be unique\n- Often the first column in the table\n\n```sql\n-- Find columns likely to be primary keys\nSELECT\n  m.name as table_name,\n  p.name as column_name,\n  p.type as column_type,\n  p.pk as is_primary_key\nFROM sqlite_master m\nJOIN pragma_table_info(m.name) p\nWHERE m.type = 'table'\n  AND m.name NOT LIKE 'sqlite_%'\n  AND (\n    p.pk = 1  -- Explicitly defined PK\n    OR p.name = 'id'\n    OR p.name = m.name || '_id'\n  )\nORDER BY m.name;\n```\n\n**Document:**\n- Primary key columns per table\n- Whether PKs are explicitly defined (pk=1) or inferred\n- Tables without obvious primary keys\n\n### Match FK Candidates to Potential Parent Tables\n\n**Heuristic:** A column named `customer_id` likely references a table named `customers` or `customer`.\n\n```sql\n-- Cross-reference FK column names with table names\n-- (Pseudo-query - implement with string matching logic)\n-- For each FK candidate like 'customer_id':\n--   1. Strip suffix (_id, Id)\n--   2. Look for table named 'customers', 'customer', or similar\n--   3. Record as potential relationship\n```\n\n**Document:**\n- FK candidate → Parent table mapping (e.g., `orders.customer_id` → `customers.id`)\n- Confidence level:\n  - **High:** Exact name match (e.g., `customer_id` → `customer` table)\n  - **Medium:** Plural/singular variation (e.g., `customer_id` → `customers` table)\n  - **Low:** Partial name match or ambiguous\n\n---\n\n## Phase 2: Value Overlap Analysis\n\n**Goal:** Validate FK candidates by checking if their values actually exist in the proposed parent table.\n\n### Check Value Overlap Percentage\n\nFor each candidate FK relationship identified in Phase 1:\n\n```sql\n-- Calculate what percentage of FK values exist in parent table\nWITH fk_values AS (\n  SELECT DISTINCT child_fk_column as value\n  FROM child_table\n  WHERE child_fk_column IS NOT NULL\n),\npk_values AS (\n  SELECT DISTINCT parent_pk_column as value\n  FROM parent_table\n  WHERE parent_pk_column IS NOT NULL\n),\noverlap AS (\n  SELECT COUNT(*) as matching_count\n  FROM fk_values fk\n  WHERE fk.value IN (SELECT value FROM pk_values)\n)\nSELECT\n  (SELECT COUNT(*) FROM fk_values) as total_fk_values,\n  (SELECT COUNT(*) FROM pk_values) as total_pk_values,\n  overlap.matching_count,\n  ROUND(100.0 * overlap.matching_count / (SELECT COUNT(*) FROM fk_values), 2) as match_percentage\nFROM overlap;\n```\n\n**Interpret match percentage:**\n- **100% match:** Strong FK relationship (perfect referential integrity)\n- **95-99% match:** Likely FK with some orphaned records\n- **80-94% match:** Possible FK with significant orphans (investigate)\n- **<80% match:** Unlikely to be true FK (name coincidence or wrong parent table)\n\n**Document:**\n- Match percentage for each candidate relationship\n- Count of orphaned FK values (values not in parent)\n- Count of unused PK values (values not referenced by any FK)\n\n### Identify Orphaned Records\n\nFor relationships with <100% match:\n\n```sql\n-- Find child records with FK values that don't exist in parent\nSELECT\n  child_table.rowid,\n  child_table.child_fk_column as orphaned_value,\n  COUNT(*) OVER (PARTITION BY child_table.child_fk_column) as occurrences\nFROM child_table\nLEFT JOIN parent_table ON child_table.child_fk_column = parent_table.parent_pk_column\nWHERE parent_table.parent_pk_column IS NULL\n  AND child_table.child_fk_column IS NOT NULL\nLIMIT 20;\n```\n\n**Document:**\n- Sample orphaned values\n- How many child records affected\n- Whether orphaned values follow a pattern (all recent, specific category, etc.)\n\n### Check Reverse Overlap (Unused Parent Records)\n\n```sql\n-- Find parent records not referenced by any child\nSELECT\n  parent_table.parent_pk_column as unused_pk_value,\n  COUNT(*) as occurrence_count\nFROM parent_table\nLEFT JOIN child_table ON parent_table.parent_pk_column = child_table.child_fk_column\nWHERE child_table.child_fk_column IS NULL\n  AND parent_table.parent_pk_column IS NOT NULL\nLIMIT 20;\n```\n\n**Document:**\n- Count of unused parent records\n- Whether this is expected (e.g., new customers with no orders yet)\n\n---\n\n## Phase 3: Cardinality Assessment\n\n**Goal:** Determine the relationship type (one-to-one, one-to-many, many-to-many).\n\n### Calculate FK → PK Cardinality\n\n**How many child records per parent record?**\n\n```sql\n-- Average number of child records per parent\nSELECT\n  COUNT(*) as total_child_records,\n  COUNT(DISTINCT child_fk_column) as distinct_fk_values,\n  ROUND(1.0 * COUNT(*) / NULLIF(COUNT(DISTINCT child_fk_column), 0), 2) as avg_children_per_parent,\n  MIN(child_count) as min_children,\n  MAX(child_count) as max_children\nFROM child_table\nCROSS JOIN (\n  SELECT\n    child_fk_column as fk,\n    COUNT(*) as child_count\n  FROM child_table\n  WHERE child_fk_column IS NOT NULL\n  GROUP BY child_fk_column\n);\n```\n\n**Interpret cardinality:**\n- **avg = 1.0, max = 1:** One-to-one relationship\n- **avg > 1.0:** One-to-many relationship (most common)\n- **Multiple FK columns referencing same parent:** Potential many-to-many via junction table\n\n**Document:**\n- Relationship type (one-to-one, one-to-many)\n- Average, min, max child records per parent\n- Whether distribution is balanced or skewed\n\n### Identify Many-to-Many Relationships\n\n**Junction table characteristics:**\n- Table has 2+ foreign keys\n- Few or no other columns besides FKs\n- Composite primary key (both FKs together)\n\n```sql\n-- Find tables with multiple FK candidates (potential junction tables)\nSELECT\n  table_name,\n  COUNT(*) as fk_column_count,\n  GROUP_CONCAT(column_name, ', ') as fk_columns\nFROM (\n  SELECT\n    m.name as table_name,\n    p.name as column_name\n  FROM sqlite_master m\n  JOIN pragma_table_info(m.name) p\n  WHERE m.type = 'table'\n    AND m.name NOT LIKE 'sqlite_%'\n    AND (p.name LIKE '%_id' OR p.name LIKE 'fk_%')\n)\nGROUP BY table_name\nHAVING COUNT(*) >= 2\nORDER BY fk_column_count DESC;\n```\n\n**Document:**\n- Junction tables identified\n- Which two (or more) tables they connect\n- Cardinality of the many-to-many relationship\n\n### Check for Self-Referencing FKs\n\n**Hierarchical data pattern:**\n- Table has FK pointing to its own PK (e.g., `employee.manager_id` → `employee.id`)\n\n```sql\n-- Find columns that might reference the same table\nSELECT\n  table_name,\n  column_name,\n  type\nFROM (\n  SELECT\n    m.name as table_name,\n    p.name as column_name,\n    p.type as type\n  FROM sqlite_master m\n  JOIN pragma_table_info(m.name) p\n  WHERE m.type = 'table'\n    AND m.name NOT LIKE 'sqlite_%'\n    AND (\n      p.name LIKE 'parent_%'\n      OR p.name LIKE 'manager_%'\n      OR p.name LIKE '%_parent_id'\n    )\n);\n```\n\n**Document:**\n- Self-referencing relationships\n- Depth of hierarchy (max levels)\n- Orphaned roots or cycles\n\n---\n\n## Phase 4: Referential Integrity Validation\n\n**Goal:** Quantify integrity violations and assess data quality impact.\n\n### Calculate Integrity Violation Rate\n\nFor each confirmed FK relationship:\n\n```sql\n-- Comprehensive referential integrity check\nWITH integrity_check AS (\n  SELECT\n    COUNT(*) as total_child_records,\n    COUNT(child_fk_column) as non_null_fk_count,\n    COUNT(*) - COUNT(child_fk_column) as null_fk_count,\n    SUM(CASE WHEN p.parent_pk_column IS NULL AND c.child_fk_column IS NOT NULL THEN 1 ELSE 0 END) as orphaned_count\n  FROM child_table c\n  LEFT JOIN parent_table p ON c.child_fk_column = p.parent_pk_column\n)\nSELECT\n  total_child_records,\n  non_null_fk_count,\n  null_fk_count,\n  ROUND(100.0 * null_fk_count / total_child_records, 2) as null_fk_pct,\n  orphaned_count,\n  ROUND(100.0 * orphaned_count / non_null_fk_count, 2) as orphaned_pct,\n  non_null_fk_count - orphaned_count as valid_fk_count,\n  ROUND(100.0 * (non_null_fk_count - orphaned_count) / non_null_fk_count, 2) as integrity_pct\nFROM integrity_check;\n```\n\n**Document:**\n- Total child records\n- NULL FK percentage (records with no parent reference)\n- Orphaned FK percentage (records referencing non-existent parent)\n- Valid FK percentage (clean referential integrity)\n\n### Assess Impact of Integrity Violations\n\n**Business impact depends on:**\n- How joins will be used (INNER vs LEFT)\n- Whether orphaned records are recent (data entry lag) or old (data quality issue)\n- Whether NULL FKs are expected (optional relationships)\n\n```sql\n-- Analyze orphaned records by recency\nSELECT\n  CASE\n    WHEN date_column >= date('now', '-7 days') THEN 'Last 7 days'\n    WHEN date_column >= date('now', '-30 days') THEN 'Last 30 days'\n    WHEN date_column >= date('now', '-90 days') THEN 'Last 90 days'\n    ELSE 'Older than 90 days'\n  END as recency,\n  COUNT(*) as orphaned_count\nFROM child_table c\nLEFT JOIN parent_table p ON c.child_fk_column = p.parent_pk_column\nWHERE p.parent_pk_column IS NULL\n  AND c.child_fk_column IS NOT NULL\n  AND c.date_column IS NOT NULL\nGROUP BY recency\nORDER BY MIN(c.date_column);\n```\n\n**Document:**\n- Whether orphans are recent (may resolve soon) or old (permanent issue)\n- Impact on analytical queries (e.g., \"10% of orders will be excluded in INNER JOIN to customers\")\n\n### Validate Composite Keys\n\nIf multiple columns together form a FK:\n\n```sql\n-- Check integrity for composite FK\nWITH composite_fk_values AS (\n  SELECT DISTINCT\n    child_table.fk_column1,\n    child_table.fk_column2\n  FROM child_table\n  WHERE child_table.fk_column1 IS NOT NULL\n    AND child_table.fk_column2 IS NOT NULL\n),\ncomposite_pk_values AS (\n  SELECT DISTINCT\n    parent_table.pk_column1,\n    parent_table.pk_column2\n  FROM parent_table\n)\nSELECT\n  COUNT(*) as total_composite_fk_values,\n  SUM(CASE WHEN pk.pk_column1 IS NULL THEN 1 ELSE 0 END) as orphaned_count\nFROM composite_fk_values fk\nLEFT JOIN composite_pk_values pk\n  ON fk.fk_column1 = pk.pk_column1\n  AND fk.fk_column2 = pk.pk_column2;\n```\n\n**Document:**\n- Composite key relationships identified\n- Integrity percentage for composite keys\n\n---\n\n## Phase 5: Relationship Documentation\n\n**Goal:** Create structured documentation of all discovered relationships for use in cleaning and analysis.\n\n### Create Relationship Catalog\n\nDocument each confirmed relationship:\n\n```markdown\n## Foreign Key Relationships\n\n### High Confidence Relationships (>95% integrity)\n\n#### orders.customer_id → customers.id\n- **Relationship Type:** Many-to-one\n- **Child Table:** orders (1,523 rows)\n- **Parent Table:** customers (342 rows)\n- **Match Percentage:** 98.2%\n- **Cardinality:** Avg 4.5 orders per customer (min: 1, max: 47)\n- **NULL FKs:** 12 rows (0.8%)\n- **Orphaned FKs:** 15 rows (1.0%)\n- **Recommended Join:** LEFT JOIN (to preserve orphaned orders)\n- **Cleaning Action:** Investigate 15 orphaned orders, flag for review\n\n### Medium Confidence Relationships (80-95% integrity)\n\n#### products.category_id → categories.id\n- **Relationship Type:** Many-to-one\n- **Child Table:** products (856 rows)\n- **Parent Table:** categories (24 rows)\n- **Match Percentage:** 87.3%\n- **Cardinality:** Avg 35.7 products per category (min: 2, max: 142)\n- **NULL FKs:** 89 rows (10.4%)\n- **Orphaned FKs:** 20 rows (2.4%)\n- **Recommended Join:** INNER JOIN (if categorized products only needed)\n- **Cleaning Action:** Exclude or recategorize 20 orphaned products\n\n### Low Confidence / Unconfirmed (<80% integrity)\n\n#### transactions.merchant_id → merchants.id\n- **Relationship Type:** Uncertain\n- **Match Percentage:** 67.8%\n- **Issue:** Large number of orphaned merchant_id values\n- **Recommendation:** Verify with data owner - may be wrong parent table\n```\n\n### Create Join Recommendations\n\nFor each relationship:\n\n```markdown\n## Join Recommendations\n\n### orders ⟶ customers\n\n**Recommended SQL:**\n```sql\n-- Use LEFT JOIN to preserve all orders (including orphans)\nSELECT\n  o.*,\n  c.customer_name,\n  c.customer_segment\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id;\n\n-- Alternative: INNER JOIN if orphans should be excluded\nSELECT\n  o.*,\n  c.customer_name\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id;\n-- Note: Excludes 15 orders (1.0%) with invalid customer_id\n```\n\n**Join Impact:**\n- LEFT JOIN: Preserves all 1,523 orders (15 will have NULL customer fields)\n- INNER JOIN: Returns 1,508 orders (99.0% of total)\n- Recommendation: Use LEFT JOIN, filter nulls in WHERE clause if needed\n```\n\n### Document Data Quality Implications\n\n```markdown\n## Data Quality Implications\n\n### Orphaned Records Summary\n\nTotal orphaned records across all relationships: 35 (2.1% of all child records)\n\n| Child Table | FK Column | Orphan Count | % of Child Table | Impact |\n|-------------|-----------|--------------|------------------|--------|\n| orders | customer_id | 15 | 1.0% | Low - recent orders, may resolve |\n| products | category_id | 20 | 2.4% | Medium - affects category analysis |\n\n### Recommended Cleaning Actions\n\n**High Priority:**\n1. products.category_id orphans (20 rows) - CREATE placeholder category \"Uncategorized\" or exclude from analysis\n2. orders.customer_id orphans (15 rows) - FLAG for customer service review\n\n**Medium Priority:**\n3. NULL customer_id in orders (12 rows) - Investigate if legitimate (guest checkout?) or data entry error\n\n### Analysis Limitations\n\nDue to referential integrity issues:\n- Customer-level aggregations will exclude 1.0% of orders (if using INNER JOIN)\n- Category-level product analysis may be incomplete (2.4% of products uncategorized)\n- Time-series trends should use LEFT JOIN to preserve all records\n```\n\n---\n\n## Integration with Other Skills\n\n### With `importing-data` (Phase 5: Quality Assessment)\n\nAfter importing tables, run FK detection to include in quality report:\n\n```markdown\n## Foreign Key Relationships (from detect-foreign-keys skill)\n\nHigh Confidence:\n- orders.customer_id → customers.id (98% integrity, 15 orphans)\n- ...\n\nMedium Confidence:\n- products.category_id → categories.id (87% integrity, 20 orphans)\n```\n\n### With `cleaning-data` (Phase 1: Scope Definition)\n\nUse FK findings to inform cleaning scope:\n\n```markdown\n## Referential Integrity Issues\n\nFrom detect-foreign-keys analysis:\n- **orders.customer_id:** 15 orphaned records (1.0%) - Priority: HIGH\n  - Recommended action: Flag for review, preserve with LEFT JOIN\n```\n\n### With `understanding-data` (Phase 4: Relationship Identification)\n\nThis skill provides the systematic process for Phase 4:\n\n```markdown\n## Phase 4: Relationship Identification\n\nUse the `detect-foreign-keys` component skill to systematically identify and validate all foreign key relationships.\n```\n\n---\n\n## Common Pitfalls\n\n**DON'T:**\n- Assume naming conventions are always correct (validate with value overlap)\n- Skip Phase 4 integrity validation - orphaned records break analyses\n- Use INNER JOIN without understanding orphan impact\n- Ignore NULL FKs - they may be legitimate or data quality issues\n\n**DO:**\n- Validate every candidate FK with value overlap analysis (Phase 2)\n- Quantify integrity violations with exact counts and percentages\n- Document both high-confidence and uncertain relationships\n- Provide join recommendations based on integrity findings\n- Feed FK findings back into cleaning-data scope\n\n---\n\n## When to Re-Run\n\nRe-run this skill when:\n- New tables are added to the database\n- Referential integrity violations are suspected\n- Planning complex multi-table analyses\n- Cleaning activities might have affected FK relationships\n- Data loads introduce new orphaned records\n\n---\n\n## Success Criteria\n\nAfter completing this skill, you should have:\n- ✅ Complete catalog of FK relationships with confidence levels\n- ✅ Integrity percentages for each relationship\n- ✅ Count and examples of orphaned records\n- ✅ Cardinality assessment (one-to-one, one-to-many, many-to-many)\n- ✅ Join recommendations (LEFT vs INNER, filters needed)\n- ✅ Data quality implications documented\n- ✅ Cleaning actions prioritized\n\nThis documentation feeds into importing-data quality reports and cleaning-data scope definitions, ensuring relationship-aware data quality management."
              },
              {
                "name": "exploratory-analysis",
                "description": "Systematic exploratory data analysis process - discover patterns in unfamiliar data, identify meaningful insights, formulate specific questions for deeper investigation",
                "path": "plugins/datapeeker/skills/exploratory-analysis/SKILL.md",
                "frontmatter": {
                  "name": "exploratory-analysis",
                  "description": "Systematic exploratory data analysis process - discover patterns in unfamiliar data, identify meaningful insights, formulate specific questions for deeper investigation"
                },
                "content": "# Exploratory Analysis Process\n\n## Overview\n\nThis skill guides you through systematic exploration of unfamiliar datasets when you don't yet have a specific question to answer. Unlike hypothesis-testing (where you test a specific claim) or guided-investigation (where you answer a specific question), exploratory analysis helps you **discover** what's interesting in the data and **identify** what questions you should be asking.\n\nExploratory analysis is appropriate when:\n- You have a new dataset and need to understand what's in it\n- The user says \"Just see what's interesting\" or \"Tell me what stands out\"\n- You need to discover patterns before formulating specific questions\n- You're looking for unexpected insights or anomalies\n- You want to generate hypotheses for later testing\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis exploratory-analysis <name>`)\n4. Have NO preconceived hypotheses or specific questions (if you do, use hypothesis-testing or guided-investigation instead)\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Data Familiarization - pending\n- Phase 2: Pattern Discovery - pending\n- Phase 3: Anomaly Investigation - pending\n- Phase 4: Insight Generation - pending\n- Phase 5: Question Formulation - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Data Familiarization\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Catalogued all tables and their apparent purposes\n- [ ] Documented schema (columns, types, relationships)\n- [ ] Assessed data quality (completeness, ranges, distributions)\n- [ ] Identified temporal coverage and granularity\n- [ ] Saved to `01 - data-familiarization.md`\n\n### Instructions\n\n1. **Understand what data you have**\n\nCreate `analysis/[session-name]/01 - data-familiarization.md` with: ./templates/01-data-familiarization.md\n\n2. **Use the understanding-data component skill**\n   - Profile the database systematically\n   - Don't skip quality checks - surprises are common\n   - Look for obvious data issues that would affect exploration\n\n3. **Resist premature pattern-hunting**\n   - Don't start with \"I wonder if weekends are different\"\n   - First understand WHAT data you have, THEN explore patterns\n   - If you catch yourself forming hypotheses, note them for Phase 2 but finish familiarization first\n\n**Common Rationalization:** \"I can see the tables, I don't need detailed familiarization\"\n**Reality:** Skipping familiarization leads to missing important context about data quality, coverage, and structure.\n\n**Common Rationalization:** \"I'll explore patterns while I familiarize myself with the data\"\n**Reality:** Mixing familiarization and pattern-hunting creates confusion. Separate concerns.\n\n---\n\n## Phase 2: Pattern Discovery\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Explored temporal patterns (trends, seasonality, cycles)\n- [ ] Explored segmentation patterns (groups, categories, clusters)\n- [ ] Explored relationship patterns (correlations, associations)\n- [ ] Documented each exploration with rationale, query, results, observations\n- [ ] Created separate files for each exploration vector\n- [ ] Files saved as `02-temporal-patterns.md`, `03-segmentation-patterns.md`, `04-relationship-patterns.md`\n\n### Instructions\n\n1. **Explore SYSTEMATICALLY along three vectors**\n\nYou MUST explore all three vectors, even if some seem less promising. Surprises often come from unexpected places.\n\n#### Vector 1: Temporal Patterns\n\nCreate `analysis/[session-name]/02-temporal-patterns.md` with: ./templates/02-temporal-patterns.md\n\n#### Vector 2: Segmentation Patterns\n\nCreate `analysis/[session-name]/03-segmentation-patterns.md` with: ./templates/03-segmentation-patterns.md\n\n#### Vector 3: Relationship Patterns\n\nCreate `analysis/[session-name]/04-relationship-patterns.md` with: ./templates/04-relationship-patterns.md\n\n2. **Be systematic, not random**\n   - Explore ALL three vectors (temporal, segmentation, relationship)\n   - Don't skip a vector just because first query seems uninteresting\n   - Patterns often hide in places you don't expect\n\n3. **Separate observation from interpretation**\n   - In each analysis, state FACTS (what the numbers show)\n   - Save interpretation for Phase 4 (Insight Generation)\n   - Resist the urge to explain patterns yet - just catalog them\n\n4. **Use visualizations liberally**\n   - Text-based tables, ASCII charts, markdown formatting\n   - Help patterns become visible\n   - Refer to `creating-visualizations` component skill\n\n**Common Rationalization:** \"I found an interesting pattern, I'll just focus on that\"\n**Reality:** Focusing on first interesting pattern causes you to miss better patterns elsewhere. Complete all three vectors.\n\n**Common Rationalization:** \"This dimension looks boring, I'll skip it\"\n**Reality:** \"Boring\" dimensions often hide surprising patterns. Explore systematically.\n\n**Common Rationalization:** \"I'll combine all patterns into one analysis file\"\n**Reality:** Separate files by vector creates structure and makes findings easy to locate.\n\n---\n\n## Phase 3: Anomaly Investigation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Identified 3-5 specific anomalies, outliers, or unexpected patterns from Phase 2\n- [ ] Investigated each anomaly with follow-up queries\n- [ ] Determined if anomalies are data quality issues or real phenomena\n- [ ] Documented findings\n- [ ] Saved to `05 - anomaly-investigation.md`\n\n### Instructions\n\n1. **Review Phase 2 for anomalies**\n\nGo back through temporal, segmentation, and relationship explorations and identify:\n- Unexpected spikes or drops\n- Outlier values or segments\n- Counterintuitive patterns\n- Violations of expected business logic\n- Inconsistencies or irregularities\n\n2. **Investigate each anomaly**\n\nCreate `analysis/[session-name]/05 - anomaly-investigation.md` with: ./templates/05-anomaly-investigation.md\n\n3. **Distinguish data quality from real patterns**\n   - NULL values, duplicates, data entry errors → data quality issues\n   - Seasonal events, one-time promotions, external shocks → real phenomena\n   - When unclear, state that explicitly\n\n4. **Don't ignore anomalies**\n   - Anomalies are often where the most interesting insights hide\n   - Investigate thoroughly, don't dismiss as \"probably nothing\"\n   - Data quality issues found now save headaches later\n\n**Common Rationalization:** \"That spike is probably a holiday, I'll ignore it\"\n**Reality:** VERIFY your assumption. \"Probably\" isn't good enough. Check.\n\n**Common Rationalization:** \"This anomaly is a data quality issue, I'll just exclude it\"\n**Reality:** Document what you're excluding and why. Future analysts need to know.\n\n**Common Rationalization:** \"I found 20 anomalies, I'll investigate them all\"\n**Reality:** Focus on the 3-5 most significant. You're exploring, not auditing every data point.\n\n---\n\n## Phase 4: Insight Generation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Reviewed all patterns and anomalies from Phases 2-3\n- [ ] Identified 3-5 insights that are actionable, surprising, or meaningful\n- [ ] For each insight, documented: finding, significance, context, caveats\n- [ ] Assessed confidence level for each insight\n- [ ] Saved to `06 - insights.md`\n\n### Instructions\n\n1. **Extract insights from patterns**\n\nNot every pattern is an insight. An insight must be:\n- **Actionable:** Suggests a decision or further investigation\n- **Surprising:** Non-obvious, counter to expectations, or revealing hidden structure\n- **Meaningful:** Materially significant magnitude or impact\n\n2. **Document insights systematically**\n\nCreate `analysis/[session-name]/06 - insights.md` with: ./templates/06-insights.md\n\n3. **Be selective**\n   - Don't try to turn every pattern into an insight\n   - 3-5 high-quality insights beat 20 marginal observations\n   - If you have more than 5, rank them and focus on top 5\n\n4. **Quantify magnitude**\n   - \"Sales vary by region\" is not enough\n   - \"Top region has 3x sales of bottom region\" is specific\n   - Numbers make insights actionable\n\n5. **Assess confidence honestly**\n   - High confidence: Strong evidence, large sample, consistent pattern, clear interpretation\n   - Medium confidence: Clear pattern but causation unclear OR limited time window OR potential confounds\n   - Low confidence: Interesting but small sample OR inconsistent OR multiple plausible explanations\n\n**Common Rationalization:** \"Every pattern I found is an insight\"\n**Reality:** Most patterns are noise or expected. Be selective. Insights must clear the bar: actionable, surprising, meaningful.\n\n**Common Rationalization:** \"I'll just state the pattern and let the user figure out if it matters\"\n**Reality:** Your job is to interpret. Explain WHY the pattern matters and WHAT it suggests.\n\n**Common Rationalization:** \"I'm very confident in this insight\"\n**Reality:** Exploratory analysis rarely produces high confidence. Be honest about uncertainty and limitations.\n\n---\n\n## Phase 5: Question Formulation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 3-5 specific, answerable questions based on insights\n- [ ] For each question, identified: what process to use, what data is needed, why it matters\n- [ ] Prioritized questions by potential value\n- [ ] Saved to `07 - next-questions.md`\n- [ ] Updated `00 - overview.md` with exploration summary\n\n### Instructions\n\n1. **Convert insights into questions**\n\nEach insight should suggest 1-2 follow-up questions. Good questions:\n- Build on what you learned in exploration\n- Are specific enough to be answerable\n- Have clear business value\n- Can be addressed with available (or obtainable) data\n\n2. **Document questions for follow-up**\n\nCreate `analysis/[session-name]/07 - next-questions.md` with: ./templates/07-next-questions.md\n\n3. **Update exploration overview**\n\nUpdate `analysis/[session-name]/00 - overview.md` by adding content from: ./templates/overview-summary.md\n\n4. **Final verification checklist**\n   - [ ] Explored all three vectors (temporal, segmentation, relationship)\n   - [ ] Investigated significant anomalies\n   - [ ] Generated 3-5 qualified insights (actionable, surprising, meaningful)\n   - [ ] Formulated specific follow-up questions\n   - [ ] Prioritized questions by value\n   - [ ] Updated overview with summary\n   - [ ] Communicated findings to user\n\n**Common Rationalization:** \"I found interesting patterns, I'm done\"\n**Reality:** Exploration isn't complete until you've formulated what to investigate next. Always end with questions.\n\n**Common Rationalization:** \"I'll just suggest broad areas to explore further\"\n**Reality:** Be specific. \"Investigate customer behavior\" is not actionable. \"Compare weekend vs weekday sales per operating hour using comparative-analysis skill\" is actionable.\n\n**Common Rationalization:** \"I'll list every possible question I can think of\"\n**Reality:** Focus on the 3-5 highest-value questions. Too many options create decision paralysis.\n\n---\n\n## Common Rationalizations\n\n### \"I can see interesting patterns already, I'll skip data familiarization\"\n**Why this is wrong:** Without understanding data quality, coverage, and structure, your pattern discoveries may be artifacts or noise.\n\n**Do instead:** Complete Phase 1 fully. Familiarization prevents false discoveries and wasted effort.\n\n### \"This exploration vector looks boring, I'll skip it\"\n**Why this is wrong:** The most surprising insights often come from places you didn't expect. \"Boring\" dimensions frequently hide interesting patterns.\n\n**Do instead:** Explore ALL three vectors (temporal, segmentation, relationship) systematically. Be comprehensive.\n\n### \"I found one great insight, that's enough\"\n**Why this is wrong:** One insight doesn't exhaust a dataset. You likely missed other valuable patterns.\n\n**Do instead:** Continue systematic exploration. Aim for 3-5 insights across different dimensions.\n\n### \"Every pattern I found is an insight\"\n**Why this is wrong:** Most patterns are noise, expected, or immaterial. Calling everything an insight dilutes the valuable discoveries.\n\n**Do instead:** Apply strict criteria: actionable, surprising, meaningful. Be selective.\n\n### \"I'll just describe patterns and let the user decide if they're important\"\n**Why this is wrong:** Your job is interpretation, not just data reporting. Users expect you to identify what matters and why.\n\n**Do instead:** Assess significance, provide context, explain business implications. Do the analytical thinking.\n\n### \"I'm very confident in these exploratory findings\"\n**Why this is wrong:** Exploratory analysis generates hypotheses, not confirmations. Patterns need validation through targeted investigation.\n\n**Do instead:** Be honest about confidence levels. Exploratory findings are typically medium-low confidence until validated.\n\n### \"I found interesting patterns, analysis is complete\"\n**Why this is wrong:** Exploration is the beginning, not the end. The goal is to identify what to investigate deeply.\n\n**Do instead:** Always complete Phase 5. Convert insights into specific, answerable questions for follow-up.\n\n### \"I'll combine all patterns into one big report\"\n**Why this is wrong:** Mixing temporal, segmentation, and relationship analyses creates confusion and makes findings hard to locate.\n\n**Do instead:** Separate files by exploration vector (02-temporal, 03-segmentation, 04-relationship). Clear structure aids comprehension.\n\n### \"This anomaly is probably a data error, I'll just exclude it\"\n**Why this is wrong:** Assumptions about anomalies are often wrong. \"Probably\" isn't good enough. Also, excluding data without documentation creates reproducibility issues.\n\n**Do instead:** Investigate anomalies in Phase 3. Document what you exclude and why. Verify your assumptions.\n\n### \"I have 15 follow-up questions, I'll list them all\"\n**Why this is wrong:** Too many options create decision paralysis. Not all questions are equally valuable.\n\n**Do instead:** Prioritize ruthlessly. Focus on 3-5 highest-value questions. Help the user know where to start.\n\n---\n\n## Summary\n\nThis skill ensures systematic, thorough exploration of unfamiliar datasets by:\n\n1. **Familiarizing with data first:** Understand structure, quality, and coverage before pattern-hunting\n2. **Exploring systematically:** Three vectors (temporal, segmentation, relationship) ensure comprehensive discovery\n3. **Investigating anomalies:** Surprises and outliers often contain the most valuable insights\n4. **Generating selective insights:** Apply strict criteria (actionable, surprising, meaningful) to separate signal from noise\n5. **Formulating specific questions:** Convert insights into answerable questions for deeper investigation\n6. **Prioritizing next steps:** Help users know what to investigate first and why\n\nFollow this process and you'll discover what's truly interesting in unfamiliar data, avoid random pattern-chasing, and identify high-value questions for targeted investigation."
              },
              {
                "name": "guided-investigation",
                "description": "Systematic process for investigating open-ended questions - decompose vague questions into specific sub-questions, map to data, investigate incrementally, synthesize findings",
                "path": "plugins/datapeeker/skills/guided-investigation/SKILL.md",
                "frontmatter": {
                  "name": "guided-investigation",
                  "description": "Systematic process for investigating open-ended questions - decompose vague questions into specific sub-questions, map to data, investigate incrementally, synthesize findings"
                },
                "content": "# Guided Investigation Process\n\n## Overview\n\nThis skill guides you through systematic investigation of open-ended or exploratory questions. Unlike hypothesis testing (where you test a specific claim), guided investigation helps you answer questions like \"Why is X happening?\" or \"What's driving Y?\" by breaking them into specific sub-questions and investigating each systematically.\n\nGuided investigation is appropriate when:\n- You have a broad question without a specific hypothesis\n- You need to understand a complex phenomenon with multiple potential factors\n- The user says \"I want to understand...\" or \"What's causing...\"\n- You need to decompose a vague question into answerable parts\n- You're investigating a business problem with unclear root causes\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis guided-investigation <name>`)\n4. Have a clear investigative goal from the user\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Question Decomposition - pending\n- Phase 2: Data Discovery - pending\n- Phase 3: Systematic Investigation - pending\n- Phase 4: Synthesis - pending\n- Phase 5: Conclusions and Recommendations - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Question Decomposition\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Clarified the user's broad question\n- [ ] Decomposed it into 3-5 specific, answerable sub-questions\n- [ ] Prioritized sub-questions by importance/dependency\n- [ ] Documented the investigative framework\n- [ ] Saved to `01 - question-decomposition.md`\n\n### Instructions\n\n1. **Ask clarifying questions** to understand the user's goal\n\n   - What's the core question you're trying to answer?\n   - What decision will this inform?\n   - What would constitute a satisfactory answer?\n   - What do you already know or suspect?\n   - What constraints exist (time, data, etc.)?\n\n2. **Decompose the broad question** into specific sub-questions\n\nCreate `analysis/[session-name]/01-question-decomposition.md` with: ./templates/phase-1-question-decomposition.md\n\n3. **STOP and get user confirmation**\n   - Review the sub-questions with the user\n   - Confirm they address the core question\n   - Adjust priorities if needed\n   - Do NOT proceed until user confirms the framework\n\n**Common Rationalization:** \"The question is clear, I can just start querying data\"\n**Reality:** Vague questions lead to unfocused investigation. Decompose first, always.\n\n**Common Rationalization:** \"I'll figure out the sub-questions as I go\"\n**Reality:** Without a clear framework, you'll chase random patterns. Plan the investigation first.\n\n---\n\n## Phase 2: Data Discovery\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Mapped each sub-question to specific data tables/columns\n- [ ] Identified what data exists vs what's missing\n- [ ] Documented data limitations that will constrain the investigation\n- [ ] Created a query plan for each sub-question\n- [ ] Saved to `02 - data-discovery.md`\n\n### Instructions\n\n1. **Map sub-questions to data**\n\nCreate `analysis/[session-name]/02-data-discovery.md` with: ./templates/phase-2-data-discovery.md\n\n2. **Run initial data quality checks**\n   - Use `understanding-data` skill to verify table structures\n   - Check for NULL values, date ranges, value distributions\n   - Document any surprises or data quality issues\n\n3. **Adjust investigation plan if needed**\n   - If key data is missing, modify sub-questions\n   - Reprioritize based on data availability\n   - Document what questions cannot be answered with available data\n\n**Common Rationalization:** \"I'll just start with queries and see what the data shows\"\n**Reality:** Without mapping questions to data first, you'll waste time on unfocused queries.\n\n**Common Rationalization:** \"I can skip data quality checks since I know the data\"\n**Reality:** Assumptions about data often turn out wrong. Check systematically.\n\n---\n\n## Phase 3: Systematic Investigation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Created one numbered markdown file per sub-question\n- [ ] Executed all planned queries for each sub-question\n- [ ] Documented rationale, query, results, and observations for each\n- [ ] Tracked findings incrementally\n- [ ] Files saved as `03-SQ1-*.md`, `04-SQ2-*.md`, etc.\n\n### Instructions\n\n1. **Investigate ONE sub-question at a time**\n\nImportant: **ONE FILE PER SUB-QUESTION**, not one file per query. Each sub-question file may contain multiple queries.\n\n2. **For each sub-question, create a dedicated file:**\n\nCreate `analysis/[session-name]/03-SQ1-[descriptive-name].md` (then 04-SQ2, 05-SQ3, etc.) with: ./templates/phase-3-sub-question.md\n\n3. **Investigation sequence**\n   - Follow the dependency order from Phase 1\n   - Complete each sub-question fully before moving to next\n   - Build on findings: let earlier answers inform later queries\n   - Update your investigation plan if findings suggest new directions\n\n4. **Use component skills as needed**\n   - `writing-queries` skill for complex SQL\n   - `interpreting-results` skill for understanding patterns\n   - `creating-visualizations` skill for markdown tables/text charts\n\n5. **Document incrementally**\n   - Don't wait until the end to document\n   - Capture observations immediately after each query\n   - Note surprises, anomalies, or unexpected patterns\n\n**Common Rationalization:** \"I'll run all queries first, then document everything at the end\"\n**Reality:** You'll forget context and rationale. Document as you go.\n\n**Common Rationalization:** \"I found something interesting, I'll chase it instead of finishing current sub-question\"\n**Reality:** Stay disciplined. Note the interesting finding, complete current sub-question, then decide if it warrants investigation.\n\n**Common Rationalization:** \"I can combine multiple sub-questions into one file\"\n**Reality:** One file per sub-question creates clear structure and makes findings easy to locate.\n\n---\n\n## Phase 4: Synthesis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Reviewed all sub-question findings together\n- [ ] Identified patterns and connections across sub-questions\n- [ ] Assessed which hypotheses from Phase 1 are supported/refuted\n- [ ] Created a coherent narrative explaining the findings\n- [ ] Saved to `XX - synthesis.md` (use next sequential number)\n\n### Instructions\n\n1. **Create synthesis file**\n\nCreate `analysis/[session-name]/XX-synthesis.md` with: ./templates/phase-4-synthesis.md\n\n2. **Build a coherent narrative**\n   - Connect the dots between sub-question findings\n   - Identify the most parsimonious explanation\n   - Acknowledge where evidence is strong vs weak\n   - Be honest about alternative explanations\n\n3. **Check your logic**\n   - Does the explanation account for ALL the findings?\n   - Are there contradictions you're ignoring?\n   - Are you cherry-picking evidence that fits your story?\n   - What would someone skeptical say?\n\n**Common Rationalization:** \"The first sub-question gave me the answer, I don't need synthesis\"\n**Reality:** Individual findings need integration. Synthesis reveals connections and tests coherence.\n\n**Common Rationalization:** \"I'll just present the findings separately and let the user synthesize\"\n**Reality:** Your job is to synthesize. Don't pass the cognitive work to the user.\n\n---\n\n## Phase 5: Conclusions and Recommendations\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written clear answer to the original broad question\n- [ ] Provided specific, actionable recommendations\n- [ ] Listed concrete next steps or follow-up investigations\n- [ ] Documented key limitations and caveats\n- [ ] Saved to `XX - conclusions.md` (use next sequential number)\n- [ ] Updated `00 - overview.md` with summary\n\n### Instructions\n\n1. **Create conclusions file**\n\nCreate `analysis/[session-name]/XX-conclusions.md` with: ./templates/phase-5-conclusions.md\n\n2. **Update overview file**\n\nUpdate: `00 - overview.md`\n\nAdd at the end:\n\n```markdown\n## Investigation Summary\n\n**Broad Question:** [Original question]\n\n**Answer:** [One-sentence conclusion]\n\n**Confidence:** [High/Medium/Low]\n\n**Key Finding:** [Most important discovery]\n\n**Primary Recommendation:** [Top priority action]\n\n**Critical Limitation:** [Most important caveat]\n\n**Recommended Follow-up:** [Most valuable next investigation]\n\n---\n\n## File Index\n\n- 01 - Question Decomposition\n- 02 - Data Discovery\n- 03-SQ1 - [Sub-question 1 name]\n- 04-SQ2 - [Sub-question 2 name]\n- 05-SQ3 - [Sub-question 3 name]\n- [etc. - list all files]\n- XX - Synthesis\n- XX - Conclusions\n```\n\n3. **Final verification checklist**\n   - [ ] All sub-questions answered\n   - [ ] Synthesis creates coherent narrative\n   - [ ] Recommendations are specific and actionable\n   - [ ] Limitations honestly stated\n   - [ ] Follow-ups identified\n   - [ ] Overview updated\n   - [ ] User informed of conclusions\n\n**Common Rationalization:** \"I found interesting patterns, that's enough\"\n**Reality:** Patterns aren't conclusions. Synthesize findings into clear answer to original question.\n\n**Common Rationalization:** \"I'll let the user decide what to do with the findings\"\n**Reality:** Provide specific recommendations. Don't make them do all the strategic thinking.\n\n**Common Rationalization:** \"I'll skip the limitations section since the conclusion is solid\"\n**Reality:** Every investigation has limitations. Acknowledging them increases credibility.\n\n---\n\n## Complete Example: Customer Churn Investigation\n\n### Example Scenario\nUser asks: \"Why are we losing customers in the premium segment?\"\n\n### Phase 1: Question Decomposition (01 - question-decomposition.md)\n\n```markdown\n# Question Decomposition\n\n## Broad Investigative Question\n\n\"Why are we losing customers in the premium segment?\"\n\n## Context and Motivation\n\nPremium customers (>$500/month) have historically been our most stable segment with <5% annual churn. In Q1 2024, churn rate jumped to 12%. Need to understand root cause to stem the losses.\n\n## Sub-Questions\n\n### Sub-Question 1: When did the churn increase begin?\n**What we need to learn:** Precise timing of when churn accelerated\n**Why it matters:** Helps identify triggering events (pricing change, product issue, competitor launch)\n**Success criteria:** Month-by-month churn rate showing inflection point\n\n### Sub-Question 2: Are churned customers concentrated in specific product lines?\n**What we need to learn:** Whether churn is product-specific or segment-wide\n**Why it matters:** Product-specific churn suggests product issues; broad churn suggests market/competitive factors\n**Success criteria:** Churn rate by product category with statistical significance\n\n### Sub-Question 3: What was the tenure of churned customers?\n**What we need to learn:** Are we losing new customers or long-tenured ones?\n**Why it matters:** New customer churn suggests onboarding issues; long-tenure churn suggests value erosion\n**Success criteria:** Distribution of churned customers by tenure (0-6mo, 6-12mo, 12-24mo, 24mo+)\n\n### Sub-Question 4: Did churned customers show usage decline before churning?\n**What we need to learn:** Whether churn was preceded by disengagement\n**Why it matters:** Usage decline signals value realization problems; sudden churn suggests competitive switching\n**Success criteria:** Usage metrics 30/60/90 days before churn vs stable customers\n\n### Sub-Question 5: Are there geographic patterns to churn?\n**What we need to learn:** Whether churn is concentrated in specific regions\n**Why it matters:** Geographic concentration suggests regional competitive or operational factors\n**Success criteria:** Churn rate by region with sample size validation\n\n## Investigation Dependencies\n\n1. SQ1 first (timing) - establishes when to focus detailed analysis\n2. SQ2 and SQ5 parallel (product and geography) - identify concentration\n3. SQ3 (tenure) - requires churn cohort identified from SQ1\n4. SQ4 last (usage) - most complex, builds on understanding from others\n\n## Hypotheses to Consider\n\n1. **Price increase impact:** 8% price increase in January may have pushed customers over threshold\n2. **Competitor launches:** Competitor Y launched enterprise tier in December\n3. **Product quality:** Premium features had stability issues in Q4 2023\n4. **Support degradation:** Support team had high turnover in Q1\n5. **Contract renewal timing:** Many premium contracts up for renewal in Q1\n\n## Success Criteria for Overall Investigation\n\nInvestigation complete when we can identify:\n1. Primary driver of increased churn (with 70%+ confidence)\n2. Quantified impact of that driver\n3. Actionable recommendations to reduce churn\n```\n\n### Phase 2: Data Discovery (02 - data-discovery.md)\n\n```markdown\n# Data Discovery\n\n## Available Data\n\n### Tables Overview\n\n- `customers`: Customer master data (id, signup_date, segment, region)\n- `subscriptions`: Subscription details (customer_id, product_id, start_date, end_date, status)\n- `usage_metrics`: Daily usage stats (customer_id, date, login_count, feature_usage)\n- `products`: Product catalog (id, name, category, tier)\n- `support_tickets`: Customer support interactions\n\n### Relevant Columns by Sub-Question\n\n#### Sub-Question 1: Timing of churn increase\n**Required data:**\n- `subscriptions.end_date` - when subscription ended\n- `subscriptions.status` - to identify churns vs active\n- `customers.segment` - to filter to premium\n\n**Data check needed:**\n- Definition of \"churn\" - is end_date populated for all churns?\n- Completeness of historical data - how far back do we have data?\n\n#### Sub-Question 2: Product concentration\n**Required data:**\n- `subscriptions.product_id` - what they were subscribed to\n- `products.category` - to group products\n\n**Data check needed:**\n- Are multi-product customers handled correctly?\n- Do we have category mapping for all products?\n\n#### Sub-Question 3: Customer tenure\n**Required data:**\n- `customers.signup_date` - when they joined\n- `subscriptions.end_date` - when they churned\n\n**Data check needed:**\n- Consistency between signup_date and first subscription start_date\n\n#### Sub-Question 4: Usage decline\n**Required data:**\n- `usage_metrics.login_count` - engagement measure\n- `usage_metrics.feature_usage` - specific feature adoption\n\n**Data check needed:**\n- Is usage data complete for all customers?\n- What's the grain (daily, weekly)?\n\n#### Sub-Question 5: Geographic patterns\n**Required data:**\n- `customers.region` - geographic identifier\n\n**Data check needed:**\n- Region data completeness\n- Region definition granularity (country, state, city?)\n\n## Data Gaps and Limitations\n\n1. **No explicit churn reason:** Don't have exit interview data or cancellation reasons\n2. **No competitor data:** Cannot directly measure competitive switching\n3. **No pricing history:** Cannot analyze individual price points or grandfathered rates\n4. **Limited support quality metrics:** Have ticket count but not resolution time or satisfaction scores\n\n## Query Plan\n\n### Sub-Question 1: Timing\n1. Monthly churn count and rate for premium segment (last 12 months)\n2. Comparison to prior year same period\n3. Weekly granularity for Q1 2024 to identify precise inflection point\n\n### Sub-Question 2: Product concentration\n1. Churn rate by product category\n2. Expected vs actual churn by category (chi-square test approach)\n\n### Sub-Question 3: Tenure\n1. Distribution of churned customers by tenure bucket\n2. Churn rate by tenure bucket (churned / total in bucket)\n\n### Sub-Question 4: Usage patterns\n1. Average usage metrics 30/60/90 days before churn\n2. Comparison to stable premium customers in same timeframe\n\n### Sub-Question 5: Geography\n1. Churn count and rate by region\n2. Statistical significance test for regions\n\n## Investigation Strategy\n\n1. Start with SQ1 (timing) - will identify the specific churn cohort to analyze\n2. Then SQ2 (product) and SQ5 (geography) in parallel - both straightforward, high-value\n3. Then SQ3 (tenure) - quick analysis once cohort is identified\n4. Finally SQ4 (usage) - most complex, requires time-series analysis\n```\n\n### Phases 3-5\n\n[Would continue with actual queries and findings, following the same detailed structure as the hypothesis-testing example. For brevity in this skill documentation, showing structure rather than complete worked example.]\n\n---\n\n## Common Rationalizations\n\n### \"The question is vague, I'll just explore the data and see what I find\"\n**Why this is wrong:** Unfocused exploration leads to random pattern-chasing and analysis paralysis.\n\n**Do instead:** Decompose the vague question into specific sub-questions in Phase 1. Structure the investigation.\n\n### \"I'll skip question decomposition since I know what to investigate\"\n**Why this is wrong:** Your initial instinct about what to investigate often misses important angles. Systematic decomposition reveals blind spots.\n\n**Do instead:** Always do Phase 1. Writing down sub-questions forces you to think comprehensively.\n\n### \"I found an interesting pattern, I'll investigate that instead of my planned sub-questions\"\n**Why this is wrong:** Chasing tangents destroys investigation coherence. You end up with fragments, not a complete answer.\n\n**Do instead:** Note interesting patterns for potential follow-up, but complete your current sub-question first.\n\n### \"I'll combine multiple sub-questions into one analysis\"\n**Why this is wrong:** Mixing sub-questions creates confusion and makes findings hard to locate later.\n\n**Do instead:** One file per sub-question. Keep them separate and focused.\n\n### \"The data shows X, so the answer must be Y\"\n**Why this is wrong:** Correlation isn't causation. Data patterns have multiple possible explanations.\n\n**Do instead:** Use Phase 4 synthesis to consider multiple explanations. Test competing hypotheses.\n\n### \"I have some findings, I'll just list them for the user\"\n**Why this is wrong:** Raw findings without synthesis don't answer the original question. You're not a data printer, you're an analyst.\n\n**Do instead:** Synthesize findings into a coherent narrative in Phase 4. Answer the actual question asked.\n\n### \"I'll make strong recommendations even though I'm not confident\"\n**Why this is wrong:** Overconfident recommendations based on weak evidence damage credibility and lead to bad decisions.\n\n**Do instead:** Calibrate recommendations to confidence level. If confidence is medium, say so and explain what would increase it.\n\n### \"I found the answer, I don't need to identify follow-up questions\"\n**Why this is wrong:** Every investigation should identify what to investigate next. Good analysis always reveals new questions.\n\n**Do instead:** Always list 2-3 follow-up investigations in Phase 5. Show what the next layer of analysis would be.\n\n### \"I'll skip documenting limitations since it weakens my conclusion\"\n**Why this is wrong:** Hiding limitations destroys trust. Readers find them anyway, and then they distrust everything.\n\n**Do instead:** Explicitly document limitations. Honest uncertainty is more credible than false certainty.\n\n---\n\n## Summary\n\nThis skill ensures systematic, comprehensive investigation of open-ended questions by:\n\n1. **Decomposing vague questions:** Break broad questions into specific, answerable sub-questions\n2. **Mapping questions to data:** Verify what data exists before diving into analysis\n3. **Investigating systematically:** One sub-question at a time, building incrementally\n4. **Synthesizing findings:** Connect the dots to create coherent explanations\n5. **Providing actionable conclusions:** Answer the original question with specific recommendations\n6. **Identifying next questions:** Every investigation reveals what to investigate next\n\nFollow this process and you'll produce thorough, defensible investigations that answer complex business questions."
              },
              {
                "name": "hypothesis-testing",
                "description": "Rigorous hypothesis testing process for data analysis - formulate hypotheses before looking at data, design tests, analyze systematically, interpret with skepticism",
                "path": "plugins/datapeeker/skills/hypothesis-testing/SKILL.md",
                "frontmatter": {
                  "name": "hypothesis-testing",
                  "description": "Rigorous hypothesis testing process for data analysis - formulate hypotheses before looking at data, design tests, analyze systematically, interpret with skepticism"
                },
                "content": "# Hypothesis Testing Process\n\n## Overview\n\nThis skill guides you through rigorous hypothesis testing in data analysis. The core principle is **scientific rigor**: formulate your hypotheses BEFORE looking at the data to avoid p-hacking and confirmation bias.\n\nHypothesis testing is appropriate when:\n- You have a specific claim or belief to test\n- You want to avoid confirmation bias\n- The question can be framed as \"Does X affect Y?\"\n- You need defensible, reproducible conclusions\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis hypothesis-testing <name>`)\n4. Understand the basic structure of your data tables\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Hypothesis Formulation (H0/H1) - pending\n- Phase 2: Test Design - pending\n- Phase 3: Data Analysis - pending\n- Phase 4: Result Interpretation - pending\n- Phase 5: Conclusion and Follow-up - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Hypothesis Formulation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written null hypothesis (H0) - specific, testable statement\n- [ ] Written alternative hypothesis (H1) - specific, directional or non-directional\n- [ ] Documented WHY you're testing this (user goal, business context)\n- [ ] Saved to `01 - hypothesis-formulation.md`\n\n### Instructions\n\n1. **Ask clarifying questions** about the user's analytical goal\n   - What belief or claim needs testing?\n   - What would constitute evidence for/against this belief?\n   - What's the practical significance threshold?\n\n2. **Write hypotheses in `01 - hypothesis-formulation.md`** with: ./templates/phase-1.md\n\n3. **STOP and get user confirmation**\n   - Read the hypotheses back to the user\n   - Confirm this is what they want to test\n   - Do NOT proceed to Phase 2 until confirmed\n\n**Common Rationalization:** \"I'll just peek at the data to refine the hypothesis\"\n**Reality:** This creates confirmation bias. Formulate hypothesis FIRST, always.\n\n---\n\n## Phase 2: Test Design\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Defined exact metrics to calculate\n- [ ] Specified comparison method (segments, time periods, etc.)\n- [ ] Listed required data checks (missing values, outliers, sample sizes)\n- [ ] Written test plan WITHOUT executing any queries yet\n- [ ] Saved to `02 - test-design.md`\n\n### Instructions\n\n1. **Design your test in `02 - test-design.md`** with: ./templates/phase-2.md\n\n2. **STOP and verify test design**\n   - Does this test actually answer the hypothesis?\n   - Are there simpler/better ways to test it?\n   - Have you planned for data quality issues?\n   - Get user confirmation before proceeding\n\n**Common Rationalization:** \"I'll just run one quick query to see if the data exists\"\n**Reality:** That's looking at data before test design is complete. Finish design FIRST.\n\n---\n\n## Phase 3: Data Analysis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Executed schema/data quality checks\n- [ ] Documented any data issues found\n- [ ] Executed main analysis query\n- [ ] Executed supporting analysis queries\n- [ ] Saved all queries and results to numbered files (03-*, 04-*, etc.)\n\n### Instructions\n\n1. **Execute queries in order, one file per query**\n\nCreate separate numbered files:\n- `03 - schema-check.md`\n- `04 - data-quality-check.md`\n- `05 - main-analysis.md`\n- `06 - supporting-analysis.md` (if needed)\n\n2. **For each query file, use this structure:** ./templates/phase-3-query.md\n\n3. **Execute queries using appropriate tool**\n   - Use SQLite CLI or database tool\n   - Copy EXACT results (don't summarize or round)\n   - If query fails, document error and revise\n\n4. **Handle data quality issues**\n   - If quality checks reveal problems, document them\n   - Decide: exclude bad data, transform it, or note as limitation\n   - Update test design if needed (document why)\n\n**Common Rationalization:** \"I'll skip data quality checks since the data looks fine\"\n**Reality:** ALWAYS check data quality. Surprises happen. Document what you checked.\n\n**Common Rationalization:** \"I'll combine all queries into one file to save time\"\n**Reality:** Separate files create clear audit trail. One query, one file.\n\n---\n\n## Phase 4: Result Interpretation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Described what the data shows (facts only)\n- [ ] Calculated relevant comparisons (% differences, ratios)\n- [ ] Considered alternative explanations\n- [ ] Assessed confounding factors identified in Phase 1\n- [ ] Stated whether results support/reject H0\n- [ ] Saved to `07 - interpretation.md` (or next number)\n\n### Instructions\n\n1. **Create interpretation file: `XX - interpretation.md`** with: ./templates/phase-4.md\n\n2. **Be intellectually honest**\n   - State limitations clearly\n   - Don't overstate conclusions\n   - Acknowledge uncertainty\n   - Suggest what additional data would help\n\n**Common Rationalization:** \"The pattern is obvious, I don't need to consider alternatives\"\n**Reality:** Always consider alternatives. Obvious patterns often have surprising explanations.\n\n**Common Rationalization:** \"I'll downplay limitations so the conclusion looks stronger\"\n**Reality:** Stating limitations INCREASES credibility. Be honest about uncertainty.\n\n---\n\n## Phase 5: Conclusion and Follow-up\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written clear, actionable conclusion\n- [ ] Listed 2-3 specific follow-up questions\n- [ ] Suggested concrete next steps\n- [ ] Saved to `08 - conclusion.md` (or next number)\n- [ ] Updated `00 - overview.md` with summary\n\n### Instructions\n\n1. **Create conclusion file: `XX - conclusion.md`** with: ./templates/phase-5.md\n\n2. **Update overview file: `00 - overview.md`**\n\nAdd summary section with: ./templates/overview-summary.md\n\n3. **Final checklist before marking complete**\n   - [ ] All phases documented in numbered files\n   - [ ] Queries and results included\n   - [ ] Limitations acknowledged\n   - [ ] Follow-up questions specified\n   - [ ] Overview updated\n   - [ ] User informed of conclusions\n\n**Common Rationalization:** \"I found the answer, I'm done\"\n**Reality:** Good analysis always identifies the NEXT question. List follow-ups.\n\n**Common Rationalization:** \"I'll skip updating the overview since it's all in the detailed files\"\n**Reality:** Overview provides navigational summary. Always update it.\n\n---\n\n## Complete Example: Day-of-Week Analysis\n\n### Example Scenario\nUser wants to test: \"Do we get more sales on weekends?\"\n\n### Phase 1: Hypothesis Formulation (01 - hypothesis-formulation.md)\n\n```markdown\n# Hypothesis Formulation\n\n## Analytical Goal\nDetermine if weekend days (Saturday, Sunday) have higher sales than weekday days (Monday-Friday)\n\n## Context\nBusiness wants to optimize staffing and inventory. If weekends are significantly busier, we should staff up. If not, we can balance resources across the week.\n\n## Hypotheses\n\n### Null Hypothesis (H0)\nWeekend days and weekday days have equal average sales. Any observed differences are due to random variation.\n\n### Alternative Hypothesis (H1)\nWeekend days have significantly different average sales compared to weekday days.\n\n## Success Criteria\n- Difference >25% would be practically meaningful (enough to justify staffing changes)\n- Pattern should be consistent across multiple weeks\n\n## Potential Confounds\n- Holidays: Holiday Monday might inflate weekday averages\n- Promotions: Weekend promotions might inflate weekend sales\n- Store hours: Different hours on weekends might affect opportunity\n- Seasonality: Analysis period might not be representative of full year\n```\n\n### Phase 2: Test Design (02 - test-design.md)\n\n```markdown\n# Test Design\n\n## Metrics\n\n### Primary Metric\nAverage daily sales amount: SUM(amount) / COUNT(DISTINCT date) for weekend vs weekday groups\n\n### Supporting Metrics\n- Total transaction count per day-of-week (to assess sample size)\n- Median sales per transaction (to check if average is representative)\n- Week-over-week consistency (to see if pattern is stable)\n\n## Comparison Structure\nGroup days into two categories:\n- Weekend: Saturday (day 6), Sunday (day 0)\n- Weekday: Monday-Friday (days 1-5)\n\nCalculate average daily sales for each group, compare the ratio.\n\n## Data Requirements\n\n### Required Tables/Columns\n- Table: `sales` (or similar)\n  - `transaction_date` (DATE or TEXT in ISO format)\n  - `amount` (NUMERIC)\n\n### Data Quality Checks\n1. Check for NULL dates or amounts\n2. Verify date range covers complete weeks\n3. Confirm adequate sample size (>100 transactions per day-of-week)\n4. Identify any outlier days (holidays, system outages)\n\n### Queries Needed\n1. Schema check: PRAGMA table_info(sales)\n2. Data quality: NULL checks, date range, outlier detection\n3. Main analysis: Daily sales by day-of-week\n4. Supporting: Transaction counts, weekly pattern consistency\n\n## Statistical Considerations\nLook for differences >25% between weekend and weekday averages. Check if pattern is consistent across multiple weeks (not just one unusual weekend).\n```\n\n### Phase 3: Data Analysis (03-06 files)\n\n**03 - schema-check.md:**\n```markdown\n# Schema Check\n\n## Rationale\nVerify the sales table exists and has the required columns for date and amount\n\n## Query\n```sql\nPRAGMA table_info(sales);\n```\n\n## Results\n```\ncid | name              | type    | notnull | dflt_value | pk\n0   | transaction_id    | INTEGER | 0       | NULL       | 1\n1   | transaction_date  | TEXT    | 0       | NULL       | 0\n2   | amount            | REAL    | 0       | NULL       | 0\n3   | product_id        | INTEGER | 0       | NULL       | 0\n```\n\n## Initial Observations\n- Table `sales` exists\n- Has `transaction_date` column (TEXT type - will need STRFTIME to extract day-of-week)\n- Has `amount` column (REAL type - good for calculations)\n- Columns allow NULLs - need to check data quality\n```\n\n**04 - data-quality-check.md:**\n```markdown\n# Data Quality Check\n\n## Rationale\nVerify data completeness and identify any issues before main analysis\n\n## Query\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(CASE WHEN transaction_date IS NULL THEN 1 END) as null_dates,\n  COUNT(CASE WHEN amount IS NULL THEN 1 END) as null_amounts,\n  MIN(transaction_date) as earliest_date,\n  MAX(transaction_date) as latest_date,\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount\nFROM sales;\n```\n\n## Results\n```\ntotal_rows | null_dates | null_amounts | earliest_date | latest_date | min_amount | max_amount\n15847      | 0          | 0            | 2024-01-01    | 2024-03-31  | 5.00       | 899.99\n```\n\n## Initial Observations\n- 15,847 total transactions\n- No NULL dates or amounts (clean data)\n- 3 months of data (Jan-Mar 2024)\n- 13 complete weeks (91 days / 7)\n- Amount range: $5 to $899.99 (no obvious outliers)\n```\n\n**05 - main-analysis.md:**\n```markdown\n# Main Analysis: Sales by Day of Week\n\n## Rationale\nCalculate average sales by day of week to test if weekend days differ from weekdays\n\n## Query\n```sql\nSELECT\n  CAST(STRFTIME('%w', transaction_date) AS INTEGER) as day_of_week,\n  CASE\n    WHEN CAST(STRFTIME('%w', transaction_date) AS INTEGER) IN (0, 6) THEN 'Weekend'\n    ELSE 'Weekday'\n  END as day_type,\n  COUNT(DISTINCT transaction_date) as days_count,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(SUM(amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_sales,\n  ROUND(AVG(amount), 2) as avg_transaction_amount\nFROM sales\nGROUP BY day_of_week, day_type\nORDER BY day_of_week;\n```\n\n## Results\n```\nday_of_week | day_type | days_count | transaction_count | total_sales | avg_daily_sales | avg_transaction_amount\n0           | Weekend  | 13         | 1456              | 52389.44    | 4030.00        | 35.98\n1           | Weekday  | 13         | 2234              | 102345.67   | 7872.90        | 45.81\n2           | Weekday  | 13         | 2198              | 98234.55    | 7556.50        | 44.70\n3           | Weekday  | 13         | 2301              | 105678.90   | 8129.15        | 45.93\n4           | Weekday  | 13         | 2345              | 108901.23   | 8377.02        | 46.44\n5           | Weekday  | 13         | 2456              | 115432.11   | 8879.39        | 47.00\n6           | Weekend  | 13         | 2857              | 98765.43    | 7597.34        | 34.56\n```\n\n## Initial Observations\n- Sunday (0): $4,030 avg daily sales, 1,456 transactions, $35.98 avg transaction\n- Monday-Friday (1-5): $7,800-$8,800 avg daily sales, 2,200-2,500 transactions, $44-47 avg transaction\n- Saturday (6): $7,597 avg daily sales, 2,857 transactions, $34.56 avg transaction\n- Saturday has HIGH transaction count but LOW per-transaction amount\n- Sunday has both low transaction count AND low per-transaction amount\n```\n\n**06 - supporting-analysis.md:**\n```markdown\n# Supporting Analysis: Weekend vs Weekday Comparison\n\n## Rationale\nAggregate weekend and weekday categories to test the hypothesis directly\n\n## Query\n```sql\nSELECT\n  CASE\n    WHEN CAST(STRFTIME('%w', transaction_date) AS INTEGER) IN (0, 6) THEN 'Weekend'\n    ELSE 'Weekday'\n  END as day_type,\n  COUNT(DISTINCT transaction_date) as days_count,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(SUM(amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_sales,\n  ROUND(AVG(amount), 2) as avg_transaction_amount\nFROM sales\nGROUP BY day_type;\n```\n\n## Results\n```\nday_type | days_count | transaction_count | total_sales | avg_daily_sales | avg_transaction_amount\nWeekday  | 65         | 11534             | 530592.46   | 8163.00         | 46.01\nWeekend  | 26         | 4313              | 151154.87   | 5813.65         | 35.04\n```\n\n## Initial Observations\n- Weekday average: $8,163 per day\n- Weekend average: $5,814 per day\n- Difference: $2,349 (29% lower on weekends)\n- Weekend has 62% fewer transactions per day\n- Weekend transactions are 24% smaller on average\n```\n\n### Phase 4: Interpretation (07 - interpretation.md)\n\n```markdown\n# Result Interpretation\n\n## Summary of Findings\n\n### Primary Metric Results\nWeekends have significantly LOWER sales than weekdays:\n- Weekday avg: $8,163 per day\n- Weekend avg: $5,814 per day\n- Difference: 29% lower on weekends\n\nThis contradicts the original hypothesis that weekends would have higher sales.\n\n### Statistical Assessment\n- Magnitude: 29% difference is both statistically meaningful (large sample) and practically significant (>25% threshold)\n- Consistency: Pattern holds across 13 weeks of data\n- Practical importance: This magnitude justifies different staffing/inventory approaches\n\n## Alternative Explanations\n\n1. **Store hours:** Maybe stores close earlier on weekends\n   - Evidence: Transaction count is 62% lower on weekends, suggesting operational constraint\n   - Impact: Could fully explain lower daily totals if hours are reduced\n\n2. **Customer behavior:** Maybe weekend shoppers buy smaller items\n   - Evidence: Average transaction is 24% smaller on weekends ($35 vs $46)\n   - Impact: Explains some but not all of the difference\n\n3. **Product mix:** Maybe high-value products aren't available on weekends\n   - Evidence: Would need product category data to verify\n   - Impact: Unknown\n\n4. **Sunday effect:** Maybe Sunday drags down weekend average\n   - Evidence: Sunday has only $4,030 avg (47% lower than Saturday's $7,597)\n   - Impact: If we exclude Sunday, Saturday is closer to weekdays but still 7% lower\n\n## Hypothesis Test Result\n\n### Null Hypothesis (H0)\nWeekend days and weekday days have equal average sales.\n\n### Decision\n**REJECT H0**\n\n### Rationale\nThe 29% difference between weekend and weekday average sales is large, consistent across 13 weeks, and exceeds our 25% practical significance threshold. The pattern is clear: weekends have lower sales than weekdays.\n\nHOWEVER: This is the OPPOSITE of what we hypothesized. The alternative hypothesis stated \"weekend days have significantly different sales\" which is true, but we expected higher, not lower.\n\n## Limitations\n\n1. **Cannot determine causation:** Data shows weekends are lower but doesn't explain why\n2. **Store operational factors unknown:** Don't have data on hours, staffing, inventory availability\n3. **Sunday is dramatically lower:** Weekend average is heavily influenced by very low Sunday sales\n4. **Seasonal effects:** 3 months (Jan-Mar) may not represent full year (could be post-holiday slump)\n5. **No control for promotions/holidays:** Any holidays in the period could skew results\n```\n\n### Phase 5: Conclusion (08 - conclusion.md)\n\n```markdown\n# Conclusion and Follow-up\n\n## Main Conclusion\n\nWeekends have 29% lower average daily sales than weekdays, contradicting the initial hypothesis that weekends would be busier. This pattern is consistent across 13 weeks and appears driven by both fewer transactions (62% lower) and smaller average purchases (24% lower).\n\n## Actionable Insights\n\n**DO NOT increase weekend staffing based on this data.** The hypothesis that weekends are busier is not supported.\n\nInstead:\n\n1. **Investigate Sunday specifically:** With only $4,030 avg daily sales (vs $7,600 on Saturday), Sunday operations may not be profitable. Consider:\n   - Reduced hours on Sunday\n   - Sunday-specific promotions to drive traffic\n   - Or closing on Sundays if fixed costs exceed contribution margin\n\n2. **Understand operational constraints:** Before making staffing decisions, determine:\n   - Are weekend hours reduced? (This might explain lower transaction counts)\n   - Is weekend inventory limited? (This might explain smaller purchases)\n   - Are certain high-value products unavailable on weekends?\n\n3. **Focus weekday resources:** If pattern holds year-round, optimize for Monday-Friday peak demand\n\n## Follow-up Questions\n\n1. **What are actual store operating hours by day?**\n   - Data needed: Store hours table, or time-of-day in transaction timestamps\n   - Approach: Calculate sales per operating hour rather than per day\n   - Why: Would distinguish \"fewer hours\" from \"fewer customers per hour\"\n\n2. **Does the weekend pattern hold across all seasons?**\n   - Data needed: Full year of transaction data\n   - Approach: Repeat this analysis for each quarter\n   - Why: Jan-Mar might be post-holiday slump; need to verify pattern is year-round\n\n3. **Are there product category differences on weekends?**\n   - Data needed: Product category or department in transaction records\n   - Approach: Analyze category mix and average prices by day-of-week\n   - Why: Would explain the 24% smaller average transaction on weekends\n\n4. **How do weekends compare if we exclude Sunday?**\n   - Data needed: Current dataset (already have this)\n   - Approach: Re-run analysis treating Sunday separately\n   - Why: Sunday is so dramatically lower it may be distorting the weekend average\n\n## Confidence Level\n\n**Medium confidence** in the finding that weekends are lower, but **low confidence** in the explanation.\n\nReasoning:\n- Pattern is clear and consistent across 13 weeks (strengthens confidence)\n- Sample sizes are adequate (4,313 weekend transactions is plenty)\n- BUT: Cannot distinguish operational constraints from customer behavior (weakens confidence)\n- AND: Only 3 months of data may not represent full year (weakens confidence)\n\n**Next step:** Investigate store hours and expand to full year data before making operational changes.\n```\n\n---\n\n## Common Rationalizations\n\n### \"I'll just look at the data first to understand it better\"\n**Why this is wrong:** Looking at data before formulating hypotheses creates confirmation bias. You'll unconsciously form hypotheses that match what you see, then \"test\" them. This isn't science.\n\n**Do instead:** Formulate hypothesis from domain knowledge, business context, or theory. Then look at data.\n\n### \"The hypothesis is obvious from the results, I don't need to write it down\"\n**Why this is wrong:** Hindsight bias makes everything seem obvious after you see the answer. Writing hypothesis first creates accountability.\n\n**Do instead:** Always write H0 and H1 in Phase 1 before any query execution.\n\n### \"I'll skip data quality checks since the data looks clean\"\n**Why this is wrong:** You can't know data is clean until you check. Surprises happen often.\n\n**Do instead:** ALWAYS run data quality queries. Document what you checked and what you found (even if it's \"no issues\").\n\n### \"The pattern is clear, I don't need to consider alternative explanations\"\n**Why this is wrong:** Obvious patterns often have non-obvious causes. Confounding factors are common.\n\n**Do instead:** Always list 2-3 alternative explanations in Phase 4, even if you think they're unlikely.\n\n### \"I'll combine multiple queries into one file for efficiency\"\n**Why this is wrong:** One file per query creates clear audit trail and makes analysis reproducible.\n\n**Do instead:** One query, one file. Use consistent numbering (03-, 04-, etc.).\n\n### \"I found the answer, analysis is complete\"\n**Why this is wrong:** Good analysis always identifies the next question. Every conclusion should raise new questions.\n\n**Do instead:** Always list 2-3 follow-up questions in Phase 5.\n\n### \"I'll downplay limitations so the conclusion looks stronger\"\n**Why this is wrong:** Acknowledging limitations increases credibility. Readers trust honest uncertainty more than false certainty.\n\n**Do instead:** State limitations clearly. Be honest about what you don't know.\n\n### \"I'll skip updating the overview since everything is in detailed files\"\n**Why this is wrong:** Overview provides navigation and quick summary. Future readers (including you) will thank you.\n\n**Do instead:** Always update `00 - overview.md` with results summary in Phase 5.\n\n---\n\n## Summary\n\nThis skill ensures rigorous, reproducible hypothesis testing by:\n\n1. **Preventing confirmation bias:** Formulate hypothesis BEFORE looking at data\n2. **Ensuring thoughtful design:** Plan your test before executing queries\n3. **Creating audit trail:** One query per file, with rationale and results\n4. **Demanding intellectual honesty:** Consider alternatives, state limitations\n5. **Identifying next questions:** Every analysis should suggest follow-up investigations\n\nFollow this process and you'll produce defensible, credible analysis that stands up to scrutiny."
              },
              {
                "name": "importing-data",
                "description": "Systematic CSV import process - discover structure, design schema, standardize formats, import to database, detect quality issues (component skill for DataPeeker analysis sessions)",
                "path": "plugins/datapeeker/skills/importing-data/SKILL.md",
                "frontmatter": {
                  "name": "importing-data",
                  "description": "Systematic CSV import process - discover structure, design schema, standardize formats, import to database, detect quality issues (component skill for DataPeeker analysis sessions)"
                },
                "content": "# Importing Data - Component Skill\n\n## Purpose\n\nUse this skill when:\n- Starting a new analysis with CSV data files\n- Need to import CSV into relational database systematically\n- Want to ensure proper schema design and type inference\n- Need quality assessment before cleaning/analysis begins\n- Replacing ad-hoc import workflows\n\nThis skill is a **prerequisite** for all DataPeeker analysis workflows and is referenced by all process skills.\n\n**Note:** DataPeeker uses SQLite (`data/analytics.db`), but this process applies to any SQL database. For SQLite-specific commands, see the `using-sqlite` skill.\n\n## Prerequisites\n\n- **CSV file accessible** on local filesystem\n- **Database** with SQL support (DataPeeker uses SQLite at `data/analytics.db`)\n- **Workspace created** for analysis session (via `just start-analysis` or equivalent)\n- **Understanding** that this skill creates `raw_*` tables, not final tables (cleaning-data handles finalization)\n\n## Data Import Process\n\nCreate a TodoWrite checklist for the 5-phase data import process:\n\n```\nPhase 1: CSV Discovery & Profiling - pending\nPhase 2: Schema Design & Type Inference - pending\nPhase 3: Basic Standardization - pending\nPhase 4: Import Execution - pending\nPhase 5: Quality Assessment & Reporting - pending\n```\n\nMark each phase as you complete it. Document all findings in numbered markdown files (`01-csv-profile.md` through `05-quality-report.md`) within your analysis workspace directory.\n\n---\n\n## Phase 1: CSV Discovery & Profiling\n\n**Goal:** Understand CSV file structure, detect encoding and delimiter, capture sample data for schema design.\n\n### File Discovery\n\nIdentify the CSV file(s) to import:\n- Ask user for CSV file path(s)\n- Verify file exists and is readable\n- Note file size for sampling strategy (>100K rows = sample-based profiling)\n\n**Document:** Record CSV file path, size, timestamp in `01-csv-profile.md`\n\n### Encoding Detection\n\nDetect file encoding to prevent import errors:\n\n```bash\nfile -I /path/to/file.csv\n```\n\nCommon encodings:\n- `charset=us-ascii` or `charset=utf-8` → Standard, no conversion needed\n- `charset=iso-8859-1` or `charset=windows-1252` → May need conversion to UTF-8\n\n**Document:** Record detected encoding. If non-UTF-8, note conversion requirement.\n\n### Delimiter Detection\n\nAnalyze first few lines to detect delimiter:\n\n```bash\nhead -n 5 /path/to/file.csv\n```\n\nCheck for:\n- Comma (`,`) - most common\n- Tab (`\\t`) - TSV files\n- Pipe (`|`) - less common\n- Semicolon (`;`) - European CSV files\n\n**Document:** Record detected delimiter character.\n\n### Header Detection\n\nDetermine if first row contains headers:\n- Read first row\n- Check if row contains text labels vs data values\n- If ambiguous, ask user to confirm\n\n**Document:** Record whether headers present, list header names if found.\n\n### Sample Data Capture\n\nCapture representative samples for schema inference:\n\n```bash\n# First 10 rows\nhead -n 11 /path/to/file.csv > /tmp/csv_head_sample.txt\n\n# Last 10 rows\ntail -n 10 /path/to/file.csv > /tmp/csv_tail_sample.txt\n\n# Row count\nwc -l /path/to/file.csv\n```\n\n**Document:** Include head and tail samples in `01-csv-profile.md` for reference during schema design.\n\n### Phase 1 Documentation Template\n\nCreate `analysis/[session-name]/01-csv-profile.md` with: ./templates/phase-1.md\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] CSV file path confirmed and file accessible\n- [ ] Encoding, delimiter, headers detected\n- [ ] Sample data captured (head + tail)\n- [ ] `01-csv-profile.md` created with all sections filled\n- [ ] Column overview completed with initial observations\n\n---\n\n## Phase 2: Schema Design & Type Inference\n\n**Goal:** Design database schema by inferring types from CSV samples, propose table structure with rationale.\n\n### Analyze Column Types\n\nFor each column from Phase 1 profiling, infer appropriate data type:\n\n**Type Inference Rules** (adapt to your database):\n\n1. **Integer types** - Use when:\n   - All non-NULL values are whole numbers\n   - No decimal points observed\n   - Typical for: IDs, counts, years, quantities\n   - Examples: INTEGER (SQLite), INT/BIGINT (PostgreSQL/MySQL)\n\n2. **Decimal/Float types** - Use when:\n   - Values contain decimal points\n   - Monetary amounts, measurements, percentages\n   - Examples: REAL (SQLite), NUMERIC/DECIMAL (PostgreSQL), DECIMAL (MySQL)\n\n3. **Text/String types** - Use when:\n   - Mixed alphanumeric content\n   - Dates/datetimes stored as text (ISO 8601: YYYY-MM-DD or YYYY-MM-DD HH:MM:SS)\n   - Categories, names, descriptions\n   - Examples: TEXT (SQLite), VARCHAR/TEXT (PostgreSQL/MySQL)\n   - **Default choice when unsure**\n\n**Note:** Date/time handling varies by database. SQLite stores dates as TEXT. PostgreSQL/MySQL have native DATE/TIMESTAMP types.\n\n**Document:** For each column, record inferred type with rationale and sample values.\n\n### Handle NULL Representations\n\nIdentify NULL representations in CSV:\n- Empty cells → `NULL` in database\n- Literal strings: \"N/A\", \"null\", \"NULL\", \"None\", \"#N/A\" → Convert to `NULL`\n- Numeric codes: -999, -1 (if documented as NULL) → Convert to `NULL`\n\n**Document:** List all NULL representations found and conversion strategy.\n\n### Design Table Schema\n\nPropose CREATE TABLE statement:\n\n```sql\nCREATE TABLE raw_[table_name] (\n  [column_1_name] [TYPE],  -- Rationale for type choice\n  [column_2_name] [TYPE],  -- Rationale for type choice\n  ...\n);\n```\n\n**Table naming convention:**\n- Prefix with `raw_` to indicate unprocessed data\n- Use lowercase with underscores: `raw_sales_data`, `raw_customers`\n- Derive from CSV filename or ask user for preferred name\n\n**Document:** Full CREATE TABLE statement with inline comments explaining each type choice.\n\n### Present Schema to User\n\nUse AskUserQuestion tool to present schema proposal:\n- Show proposed table name\n- Show each column with type and rationale\n- Ask user to confirm or request changes\n\n**Document:** User's approval and any requested modifications.\n\n### Phase 2 Documentation Template\n\nCreate `analysis/[session-name]/02-schema-design.md` with: ./templates/phase-2.md\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] All columns analyzed with type inference rationale\n- [ ] NULL representations identified and mapped\n- [ ] CREATE TABLE statement drafted\n- [ ] User approved schema (via AskUserQuestion)\n- [ ] `02-schema-design.md` created with all sections filled\n\n---\n\n## Phase 3: Basic Standardization\n\n**Goal:** Define transformation rules for dates, numbers, whitespace, and text formatting to ensure clean, consistent data in raw_* table.\n\n### Date Format Standardization\n\n**Target format:** ISO 8601\n- Dates: `YYYY-MM-DD` (e.g., 2025-01-15)\n- Datetimes: `YYYY-MM-DD HH:MM:SS` (e.g., 2025-01-15 14:30:00)\n\n**Common source formats to convert:**\n- `MM/DD/YYYY` or `M/D/YYYY` → `YYYY-MM-DD`\n- `DD/MM/YYYY` or `D/M/YYYY` → `YYYY-MM-DD` (verify with user which is month vs day)\n- `YYYY/MM/DD` → `YYYY-MM-DD` (slash to hyphen)\n- `Mon DD, YYYY` → `YYYY-MM-DD` (e.g., \"Jan 15, 2025\" → \"2025-01-15\")\n- Timestamps with T separator: `YYYY-MM-DDTHH:MM:SS` → Keep as-is (valid ISO 8601)\n\n**Document:** List each date column, source format detected, target format, conversion logic.\n\n### Number Format Normalization\n\n**Remove non-numeric characters:**\n- Currency symbols: `$123.45` → `123.45`\n- Comma separators: `1,234.56` → `1234.56`\n- Percentage signs: `45%` → `45` or `0.45` (document choice)\n- Units: `25kg`, `100m` → `25`, `100` (document unit in column name/comments)\n\n**Decimal handling:**\n- Preserve decimal points\n- Convert European format if detected: `1.234,56` → `1234.56` (verify with user)\n\n**Document:** List each numeric column, formatting issues found, normalization rules.\n\n### Whitespace and Text Normalization\n\n**Whitespace cleaning:**\n- Trim leading/trailing whitespace from all TEXT columns\n- Normalize internal whitespace: multiple spaces → single space\n- Normalize line endings: `\\r\\n` or `\\r` → `\\n`\n\n**Text case standardization** (optional, apply selectively):\n- IDs/codes: Often uppercase for consistency\n- Names: Title case or preserve original\n- Free text: Preserve original case\n- **Document choice per column type**\n\n**Document:** Which columns get whitespace cleaning, which get case normalization.\n\n### NULL Standardization\n\nApply NULL representation mapping from Phase 2:\n- Convert all identified NULL representations to actual SQLite `NULL`\n- Empty strings → `NULL` for numeric/date columns\n- Empty strings → Preserve as empty string `''` for TEXT columns (document choice)\n\n**Document:** NULL conversion applied, count of conversions per column.\n\n### Phase 3 Documentation Template\n\nCreate `analysis/[session-name]/03-standardization-rules.md` with: ./templates/phase-3.md\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Date standardization rules defined for all date columns\n- [ ] Number normalization rules defined for all numeric columns\n- [ ] Whitespace/text rules defined\n- [ ] NULL conversion mapping finalized\n- [ ] `03-standardization-rules.md` created with verification queries\n\n---\n\n## Phase 4: Import Execution\n\n**Goal:** Execute import with standardization rules, verify row count and data integrity.\n\n### Generate CREATE TABLE Statement\n\nFrom Phase 2 schema design, finalize CREATE TABLE statement:\n\n```sql\nCREATE TABLE IF NOT EXISTS raw_[table_name] (\n  [column_1] [TYPE],\n  [column_2] [TYPE],\n  ...\n);\n```\n\n**Execute:**\n```bash\nsqlite3 data/analytics.db < create_table.sql\n```\n\n**Verify table created:**\n```sql\n-- Check table exists\n.tables\n\n-- Inspect schema\nPRAGMA table_info(raw_[table_name]);\n```\n\n**Document:** Paste table creation confirmation and schema output.\n\n### Import CSV with Standardization\n\n**Import method options:**\n\n**Option 1: SQLite `.import` command** (for simple cases):\n```bash\nsqlite3 data/analytics.db <<EOF\n.mode csv\n.import /path/to/file.csv raw_[table_name]\nEOF\n```\n\n**Option 2: Python script** (for complex standardization):\n```python\nimport csv\nimport sqlite3\nfrom datetime import datetime\n\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\nwith open('/path/to/file.csv', 'r', encoding='utf-8') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Apply standardization rules from Phase 3\n        row['date_column'] = standardize_date(row['date_column'])\n        row['amount_column'] = standardize_number(row['amount_column'])\n        # ... apply other rules ...\n\n        cursor.execute(\"\"\"\n            INSERT INTO raw_[table_name]\n            ([columns]) VALUES ([placeholders])\n        \"\"\", tuple(row.values()))\n\nconn.commit()\nconn.close()\n```\n\n**Document:** Which method used, any import warnings/errors encountered and resolved.\n\n### Verify Import Success\n\n**Row count verification:**\n```sql\n-- Count rows in table\nSELECT COUNT(*) as row_count FROM raw_[table_name];\n```\n\nCompare to CSV row count from Phase 1. Should match (or CSV count - 1 if CSV had header row).\n\n**Sample data inspection:**\n```sql\n-- View first 5 rows\nSELECT * FROM raw_[table_name] LIMIT 5;\n\n-- View last 5 rows\nSELECT * FROM raw_[table_name]\nORDER BY rowid DESC LIMIT 5;\n```\n\n**Column completeness check:**\n```sql\n-- Check NULL counts per column\nSELECT\n  COUNT(*) as total_rows,\n  COUNT([column_1]) as [column_1]_non_null,\n  COUNT([column_2]) as [column_2]_non_null,\n  ...\nFROM raw_[table_name];\n```\n\n**Document:** Paste actual results showing row counts, sample rows, NULL counts.\n\n### Run Verification Queries from Phase 3\n\nExecute all verification queries defined in `03-standardization-rules.md`:\n- Date format verification\n- Number format verification\n- Whitespace verification\n\n**Expected:** All verification queries return 0 rows (no violations).\n\n**If violations found:** Document count and examples, decide whether to:\n1. Re-import with adjusted rules\n2. Document as acceptable edge cases\n3. Flag for cleaning-data phase\n\n**Document:** Results of all verification queries.\n\n### Phase 4 Documentation Template\n\nCreate `analysis/[session-name]/04-import-log.md` with: ./templates/phase-4.md\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] raw_* table created in data/analytics.db\n- [ ] CSV data imported with standardization applied\n- [ ] Row count verified matches CSV (accounting for headers)\n- [ ] Sample data inspected and looks correct\n- [ ] All Phase 3 verification queries executed\n- [ ] `04-import-log.md` created with all results documented\n\n---\n\n## Phase 5: Quality Assessment & Reporting\n\n**Goal:** Systematically detect data quality issues using sub-agent to prevent context pollution, document findings for cleaning-data skill.\n\n**CRITICAL:** This phase MUST use sub-agent delegation. DO NOT analyze data in main agent context.\n\n### Delegate Quality Assessment to Sub-Agent\n\n**Use dedicated quality-assessment agent**\n\nInvoke the `quality-assessment` agent (defined in `.claude/agents/quality-assessment.md`):\n\n```\nTask tool with agent: quality-assessment\nParameters:\n- table_name: raw_[actual_table_name]\n- columns: [list of all columns from schema]\n- numeric_columns: [list of numeric columns for outlier detection]\n- text_columns: [list of text columns for uniqueness analysis]\n```\n\nThe agent will execute all quality checks (NULL analysis, duplicates, outliers, free text) and return structured findings.\n\n**Document agent findings in `05-quality-report.md` using template below.**\n\n### Delegate Foreign Key Detection to Sub-Agent\n\n**If multiple tables exist in the database**, detect foreign key relationships between them.\n\n**Use dedicated detect-foreign-keys agent**\n\nInvoke the `detect-foreign-keys` agent (defined in `.claude/agents/detect-foreign-keys.md`):\n\n```\nTask tool with agent: detect-foreign-keys\nParameters:\n- database_path: data/analytics.db\n- table_names: [list of all raw_* tables in database]\n```\n\n**When to run FK detection:**\n- **Multiple tables imported:** Run to discover relationships\n- **Single table imported:** Skip (no relationships possible), document \"N/A - single table\" in quality report\n\nThe agent will:\n1. Identify FK candidate columns based on naming patterns\n2. Validate candidates with value overlap analysis\n3. Assess cardinality (one-to-one, one-to-many, many-to-many)\n4. Quantify referential integrity violations (orphaned records)\n5. Return structured relationship catalog with join recommendations\n\n**Document FK findings in `05-quality-report.md` using template below.**\n\n### Create Quality Report for cleaning-data\n\nCreate `analysis/[session-name]/05-quality-report.md` with: ./templates/phase-5.md\n\n**CHECKPOINT:** Before concluding importing-data skill, you MUST have:\n- [ ] Sub-agent completed quality assessment (NOT done in main context)\n- [ ] NULL percentages documented for all columns\n- [ ] Duplicates detected and examples captured\n- [ ] Outliers identified in all numeric columns\n- [ ] Free text columns identified for categorization\n- [ ] FK relationships detected (if multiple tables) via detect-foreign-keys sub-agent\n- [ ] Referential integrity assessed with orphaned record counts\n- [ ] `05-quality-report.md` created with all sections filled (including FK relationships)\n- [ ] Quality report ready for cleaning-data skill to consume\n\n---\n\n## Common Rationalizations\n\n### \"The CSV looks clean, I can skip profiling and go straight to import\"\n**Why this is wrong:** Hidden issues (encoding, delimiters, NULL representations) cause import failures or silent data corruption. Even \"clean\" CSVs have edge cases.\n\n**Do instead:** Always complete Phase 1 profiling. Takes 5 minutes and prevents hours of debugging broken imports.\n\n### \"I'll just guess the schema types, they're obvious from the column names\"\n**Why this is wrong:** Column named \"year\" might contain \"2023-Q1\" (TEXT). \"amount\" might have \"$\" symbols. Type inference prevents silent casting failures.\n\n**Do instead:** Complete Phase 2 type inference with sample analysis. Document rationale for each type choice.\n\n### \"Dates look fine, I don't need standardization rules\"\n**Why this is wrong:** Mixed formats (\"01/15/2025\", \"2025-01-15\", \"Jan 15 2025\") break date arithmetic and sorting. Standardization is mandatory.\n\n**Do instead:** Complete Phase 3 with explicit date format conversion rules. Verify with queries after import.\n\n### \"I'll do the import manually, don't need to document it\"\n**Why this is wrong:** Undocumented imports can't be reproduced. When re-importing updated data, you'll forget the transformations applied.\n\n**Do instead:** Complete Phase 4 import log with commands, results, and verification. Future-you will thank present-you.\n\n### \"The data looks good after import, quality assessment is overkill\"\n**Why this is wrong:** Duplicates, outliers, and NULL patterns are invisible without systematic checks. These surface as bugs during analysis.\n\n**Do instead:** ALWAYS complete Phase 5 with sub-agent delegation. Quality report saves time in cleaning-data phase.\n\n### \"I'll run quality checks in the main agent, it's faster than delegating\"\n**Why this is wrong:** Analyzing thousands of rows pollutes main agent context, degrading performance for entire session. Sub-agents prevent this.\n\n**Do instead:** ALWAYS delegate Phase 5 to sub-agent with exact sqlite3 commands provided.\n\n### \"This CSV has 100K rows, I should skip some profiling steps for speed\"\n**Why this is wrong:** Large files are MORE likely to have quality issues, not less. Sampling strategies exist for large files.\n\n**Do instead:** Use head/tail sampling (Phase 1) and query-based profiling (Phase 5 sub-agent). Don't skip phases.\n\n### \"The import succeeded, I don't need to verify row counts\"\n**Why this is wrong:** Silent data loss happens. Header rows miscounted, empty rows skipped, encoding issues truncating data.\n\n**Do instead:** Always verify row count matches (Phase 4). If mismatch, investigate before proceeding.\n\n### \"I'll clean the data during import, don't need separate cleaning-data skill\"\n**Why this is wrong:** Import handles technical standardization (formats). Cleaning handles semantic issues (business rules, categorization). Mixing them creates confusion.\n\n**Do instead:** Keep importing-data focused on standardization. Let cleaning-data handle deduplication, outliers, free text.\n\n### \"Quality report shows no issues, I can skip cleaning-data\"\n**Why this is wrong:** cleaning-data is ALWAYS mandatory per design. Even \"clean\" data needs business rule validation and final verification.\n\n**Do instead:** Proceed to cleaning-data even if quality report shows minimal issues. Document \"no cleaning needed\" if appropriate.\n\n---\n\n## Summary\n\nThis skill ensures systematic, documented CSV import with quality assessment by:\n\n1. **Profiling before importing:** Understand encoding, delimiters, headers, and sample data before designing schema - prevents import failures and silent corruption.\n\n2. **Explicit type inference:** Analyze samples to infer INTEGER/REAL/TEXT types with documented rationale - prevents type casting failures and ensures correct data representation.\n\n3. **Mandatory standardization:** Convert dates to ISO 8601, normalize numbers, clean whitespace, map NULL representations - creates consistent data foundation for analysis.\n\n4. **Verified import execution:** Document CREATE TABLE statements, import methods, row count verification - ensures reproducibility and data integrity.\n\n5. **Systematic quality assessment:** Delegate NULL detection, duplicate finding, outlier identification, and free text discovery to sub-agent - prevents context pollution while ensuring comprehensive quality checks.\n\n6. **Audit trail maintenance:** Create numbered markdown files (01-05) documenting every decision - provides full traceability from raw CSV to raw_* tables.\n\nFollow this process and you'll create clean, well-documented raw_* tables ready for the cleaning-data skill, avoid silent data corruption, and maintain complete audit trail for reproducible imports.\n\n---"
              },
              {
                "name": "interpreting-results",
                "description": "Component skill for systematic result interpretation with intellectual honesty in DataPeeker analysis sessions",
                "path": "plugins/datapeeker/skills/interpreting-results/SKILL.md",
                "frontmatter": {
                  "name": "interpreting-results",
                  "description": "Component skill for systematic result interpretation with intellectual honesty in DataPeeker analysis sessions"
                },
                "content": "# Interpreting Results\n\n## Purpose\n\nThis component skill guides rigorous, intellectually honest interpretation of query results. Use it when:\n- Analyzing query outputs to draw conclusions\n- Need to avoid premature or biased interpretations\n- Considering alternative explanations before committing to conclusions\n- Referenced by process skills requiring result interpretation\n\n## Prerequisites\n\n- Query executed and results obtained\n- Query documented with rationale (use `writing-queries` skill)\n- Understanding of data source and quality (use `understanding-data` skill)\n- Analysis context and goals clearly defined\n\n## Result Interpretation Process\n\nCreate a TodoWrite checklist for the 6-step interpretation framework:\n\n```\nPhase 1: Understand Context\nPhase 2: Describe Patterns\nPhase 3: Generate Alternative Explanations\nPhase 4: Assess Significance\nPhase 5: State Conclusions with Caveats\nPhase 6: Identify Follow-up Questions\n```\n\nMark each phase as you complete it. Document all interpretations in numbered markdown files.\n\n---\n\n## Phase 1: Understand Context\n\n**Goal:** Ground interpretation in business and analytical context before looking for patterns.\n\n### Recall Analytical Question\n\nBefore interpreting results, explicitly state:\n\n```markdown\n## Context for Interpretation\n\n**Original Question:** [What we set out to answer]\n\n**Why This Matters:** [Business context and decisions that depend on this answer]\n\n**Hypothesis (if applicable):** [What we expected to find, and why]\n\n**Data Period:** [Time range covered by results]\n\n**Filters Applied:** [Any exclusions or subsets]\n\n**Known Data Limitations:** [Quality issues, missing data, coverage gaps]\n```\n\n### Consider External Factors\n\nAsk yourself:\n\n1. **What was happening during this time period?**\n   - Seasonality (holidays, end-of-quarter, etc.)\n   - Business changes (promotions, product launches, policy changes)\n   - External events (economy, weather, competition)\n\n2. **How might this affect results?**\n   - Example: High December sales might be holiday-driven, not a trend\n   - Example: Spike in refunds might correlate with product recall\n\n3. **What context is needed to interpret these numbers?**\n   - Industry benchmarks\n   - Historical performance\n   - Comparable segments or time periods\n\n**Document:** External factors that might explain or confound results.\n\n### Define Success Criteria\n\nBefore calling results \"good\" or \"bad\", define what you're comparing to:\n\n```markdown\n## Success Criteria\n\n**Comparing to:**\n- Historical baseline: [e.g., \"Q1 2023 had $500K revenue\"]\n- Target/goal: [e.g., \"Target was 10% growth\"]\n- Industry benchmark: [e.g., \"Industry average conversion is 2.5%\"]\n- Control group: [e.g., \"Comparing treatment to control segment\"]\n\n**Threshold for meaningful difference:**\n- [e.g., \"Need >5% difference to be operationally significant\"]\n```\n\n**Don't proceed to pattern identification without context.**\n\n---\n\n## Phase 2: Describe Patterns\n\n**Goal:** Objectively describe what the data shows before explaining why.\n\n### Start with Descriptive Statements\n\nDescribe results using neutral, factual language:\n\n**DO:**\n- \"Category A has 2.3x the revenue of Category B ($450K vs $195K)\"\n- \"Sales declined 15% from January to February (10,234 → 8,699 units)\"\n- \"Day-of-week distribution ranges from 12.8% (Sunday) to 16.2% (Friday)\"\n- \"Top 3 products account for 45% of total revenue\"\n\n**DON'T:**\n- \"Category A is performing well\" (What's the baseline?)\n- \"Sales are dropping\" (Implies negative trend without context)\n- \"Friday is the best day\" (Best by what measure? Compared to what?)\n- \"These products are successful\" (Define success first)\n\n### Identify Pattern Types\n\nCategorize what you observe:\n\n**Magnitude patterns:**\n```markdown\n- Absolute values: [e.g., \"Total revenue: $1.2M\"]\n- Relative comparisons: [e.g., \"Region A is 3x larger than Region B\"]\n- Distributions: [e.g., \"80% of revenue from 20% of customers\"]\n```\n\n**Time patterns:**\n```markdown\n- Trends: [e.g., \"Monthly growth averaged 5% over 6 months\"]\n- Cycles: [e.g., \"Weekly pattern peaks mid-week, dips on weekends\"]\n- Anomalies: [e.g., \"March 15 spike to 3x normal daily volume\"]\n```\n\n**Relationship patterns:**\n```markdown\n- Correlations: [e.g., \"Higher price segments have lower order counts\"]\n- Segments: [e.g., \"B2B customers have 4x higher average order value than B2C\"]\n- Thresholds: [e.g., \"Sharp drop-off in conversion above $100 price point\"]\n```\n\n### Quantify Patterns Precisely\n\nUse specific numbers, not vague terms:\n\n**Vague:** \"Sales increased significantly\"\n**Precise:** \"Sales increased 23% (1,234 → 1,518 units), a gain of 284 units\"\n\n**Vague:** \"Most customers prefer Category A\"\n**Precise:** \"58% of customers (2,340 of 4,034) purchased from Category A\"\n\n**Vague:** \"There's a big difference between segments\"\n**Precise:** \"Segment 1 average: $127, Segment 2 average: $89, difference of $38 (43%)\"\n\n### Check for Data Artifacts\n\nBefore accepting patterns as real, verify they're not artifacts:\n\n```sql\n-- Verify: Is this pattern real or a data issue?\n\n-- Check for row count consistency\nSELECT COUNT(*) FROM results;  -- Does this match expectations?\n\n-- Check for NULL inflation\nSELECT COUNT(*) - COUNT(column_name) as null_count FROM results;\n\n-- Check for duplicate records\nSELECT COUNT(*) as total_rows, COUNT(DISTINCT id) as unique_ids FROM results;\n\n-- Check for incomplete periods\nSELECT MIN(date), MAX(date), COUNT(DISTINCT date) as date_count FROM results;\n```\n\n**Document:** Any data quality issues that might create misleading patterns.\n\n---\n\n## Phase 3: Generate Alternative Explanations\n\n**Goal:** Consider multiple explanations before committing to one.\n\nThis is the most critical phase for intellectual honesty. **Premature explanation is the enemy of good analysis.**\n\n### Brainstorm Alternative Hypotheses\n\nFor each pattern identified, generate at least 3 possible explanations:\n\n```markdown\n## Pattern: Friday has 16.2% of weekly sales vs 12.8% on Sunday\n\n### Possible Explanations:\n\n1. **Consumer behavior:** People shop more on Fridays (payday, preparing for weekend)\n   - Testable: Do other metrics (sessions, conversion rate) also peak Friday?\n\n2. **Business operations:** We run promotions on Fridays\n   - Testable: Check promotion calendar, compare promoted vs non-promoted Fridays\n\n3. **Data artifact:** Incomplete weeks in dataset skew day-of-week calculation\n   - Testable: Count how many of each weekday are in dataset\n\n4. **Seasonality interaction:** Dataset includes holiday weeks where Friday patterns differ\n   - Testable: Split analysis into holiday vs non-holiday weeks\n\n5. **Geographic mix:** Different time zones make \"Friday\" broader (Friday in US, Saturday in Asia)\n   - Testable: Segment by customer timezone if available\n```\n\n### Use the \"Why Might This NOT Be True?\" Test\n\nFor your preferred explanation, actively argue against it:\n\n```markdown\n## Preferred Explanation: Customers shop more on Fridays due to payday\n\n### Why this might be WRONG:\n\n- Many customers have direct deposit on different days\n- Weekend shopping (Sat/Sun) should be higher if it's leisure time\n- No evidence yet that Friday shoppers are paid-on-Friday workers\n- Pattern might be driven by small number of large B2B orders\n- Could be specific to this dataset's time period only\n\n### What would convince me this is RIGHT:\n\n- [ ] Friday pattern consistent across multiple months/quarters\n- [ ] Segmentation shows pattern strongest for consumer (not B2B) purchases\n- [ ] Individual customer purchase history shows Friday preference\n- [ ] Pattern persists after removing outlier large orders\n- [ ] Industry data confirms Friday shopping peak\n```\n\n### Consider Confounding Variables\n\nIdentify factors that might explain the pattern instead of your hypothesis:\n\n**Template:**\n\n```markdown\n## Potential Confounds\n\n1. **[Confound name]:** [How it could explain the pattern]\n   - Test: [How to rule this in/out]\n\nExample:\n\n1. **Marketing send schedule:** Email campaigns go out Thursday, driving Friday purchases\n   - Test: Compare Friday sales on campaign weeks vs non-campaign weeks\n\n2. **Product mix:** High-value products launched mid-dataset period\n   - Test: Segment analysis into before/after product launch\n\n3. **Measurement error:** Weekend orders processed Monday, suppressing Sunday counts\n   - Test: Check order_date vs processed_date, validate weekend entries\n```\n\n### The \"And\" vs \"Or\" Test\n\nDistinguish between:\n- **Exclusive explanations** (A OR B is true, not both)\n- **Contributing factors** (A AND B AND C all contribute)\n\n```markdown\n## Explanation Type\n\nThis pattern is likely caused by:\n- [ ] Single dominant factor (identify the ONE cause)\n- [x] Multiple contributing factors (list all contributors)\n\nIf multiple factors:\n- Factor 1: [Estimated contribution]\n- Factor 2: [Estimated contribution]\n- Factor 3: [Estimated contribution]\n\nTestable: Can we quantify each factor's contribution?\n```\n\n---\n\n## Phase 4: Assess Significance\n\n**Goal:** Determine if patterns are meaningful before acting on them.\n\n### Statistical Significance (Directional)\n\nWhile we can't run formal statistical tests without additional tools, we can reason about significance:\n\n```markdown\n## Significance Assessment\n\n**Sample size:**\n- [e.g., \"Based on 10,234 orders over 90 days\"]\n- [Is this enough data to trust the pattern?]\n\n**Effect size:**\n- [e.g., \"15% difference between segments\"]\n- [Is the difference large enough to matter?]\n\n**Consistency:**\n- [e.g., \"Pattern appears in 8 of 10 months\"]\n- [Is this stable or fluctuating wildly?]\n\n**Variance:**\n- [e.g., \"Group A: 100 ± 45, Group B: 150 ± 12\"]\n- [Do ranges overlap significantly?]\n```\n\n### Practical Significance\n\nEven if a pattern is statistically real, is it actionable?\n\n**Questions to ask:**\n\n1. **Is the difference large enough to matter operationally?**\n   ```markdown\n   - Finding: Segment A has 2% higher conversion than Segment B\n   - Volume: Segment B is 50x larger\n   - Practical significance: Optimizing Segment B (even with lower rate) has\n     25x more impact than Segment A\n   - Conclusion: Pattern is real but not the priority\n   ```\n\n2. **Can we actually act on this finding?**\n   ```markdown\n   - Finding: Customers in ZIP codes starting with \"9\" have higher LTV\n   - Actionability: We can't control customer ZIP codes\n   - Practical significance: Low - interesting but not actionable\n   - Alternative: Look for correlated factors we CAN influence\n   ```\n\n3. **What's the cost/benefit of acting?**\n   ```markdown\n   - Finding: 3% revenue increase if we extend hours to 10pm\n   - Cost: Staffing, utilities for extra 2 hours\n   - Benefit: 3% of $1M = $30K annual revenue increase\n   - Margin: Estimated $8K net profit after costs\n   - Assessment: Marginally significant, requires testing\n   ```\n\n### Error Bar Reasoning\n\nWithout formal confidence intervals, reason about uncertainty:\n\n```markdown\n## Uncertainty Assessment\n\n**Data quality confidence:** [High/Medium/Low]\n- [Any known issues with data accuracy?]\n\n**Sample representativeness:** [High/Medium/Low]\n- [Does this sample represent the broader population?]\n- [Any selection bias in how data was collected?]\n\n**Temporal stability:** [High/Medium/Low]\n- [Will this pattern hold next month? Next year?]\n- [Dependent on temporary conditions?]\n\n**Overall confidence in pattern:** [High/Medium/Low]\n- [Considering all factors, how confident are we this is real?]\n```\n\n---\n\n## Phase 5: State Conclusions with Caveats\n\n**Goal:** Make clear, hedged claims that accurately represent certainty level.\n\n### Use Appropriate Hedging Language\n\nMatch your language to your confidence level:\n\n**High confidence:**\n- \"The data clearly shows...\"\n- \"We can conclude that...\"\n- \"This pattern is consistent and robust...\"\n\n**Medium confidence:**\n- \"The data suggests...\"\n- \"This pattern appears to indicate...\"\n- \"The most likely explanation is...\"\n\n**Low confidence:**\n- \"There is weak evidence that...\"\n- \"One possible interpretation is...\"\n- \"We observe a pattern, but it could be explained by...\"\n\n**Inappropriate hedging:**\n- Avoid: \"The data proves...\" (data doesn't prove, it provides evidence)\n- Avoid: \"Obviously this means...\" (if it's obvious, you don't need data)\n- Avoid: \"This clearly demonstrates...\" (unless confidence is truly high)\n\n### Separate Observation from Inference\n\nStructure conclusions to distinguish facts from interpretations:\n\n```markdown\n## Conclusions\n\n### What We Observed (Facts)\n- Friday sales averaged 16.2% of weekly total (vs 14.3% expected if uniform)\n- This pattern appeared in 11 of 12 months studied\n- Friday average order value ($127) is similar to weekly average ($124)\n- Friday transaction count is 18% higher than Sunday (2,340 vs 1,980)\n\n### What We Infer (Interpretations)\n- Friday traffic increase (not AOV increase) drives higher sales\n- Pattern is stable across most months (except December outlier)\n- This is likely a behavioral pattern, not a pricing or promotion effect\n\n### Confidence Level: Medium-High\n- Strong evidence for Friday traffic pattern\n- Insufficient data on WHY (customer motivation unclear)\n- Need to rule out confounds (marketing calendar, staffing changes)\n```\n\n### Explicitly State Limitations\n\nEvery conclusion should include what you DON'T know:\n\n```markdown\n## Caveats and Limitations\n\n**What this analysis does NOT tell us:**\n- [e.g., \"We don't know if Friday shoppers are different people or same\n   people shopping more frequently\"]\n- [e.g., \"We can't determine causation from this correlation\"]\n- [e.g., \"This dataset doesn't include abandoned carts, only completed purchases\"]\n\n**Assumptions we made:**\n- [e.g., \"Assumed all timestamps are in local timezone\"]\n- [e.g., \"Treated returns as separate from original purchase date\"]\n\n**Data quality concerns:**\n- [e.g., \"First two weeks of January had incomplete data\"]\n- [e.g., \"Product category field was NULL for 5% of records\"]\n\n**Generalizability limits:**\n- [e.g., \"This analysis covers only online sales, not in-store\"]\n- [e.g., \"Time period includes major pandemic shifts, may not represent normal behavior\"]\n```\n\n### The \"So What?\" Test\n\nForce yourself to state the implications clearly:\n\n```markdown\n## Implications\n\n**For decision-makers:**\n- [What should they DO differently based on this finding?]\n- [What should they STOP doing?]\n- [What remains uncertain that needs more investigation?]\n\nExample:\n\n**For marketing team:**\n- Consider scheduling campaigns for Thursday delivery (to catch Friday traffic)\n- Don't assume Friday success will translate to other days\n- Test: Run A/B test with campaign timing to validate causal relationship\n\n**For ops team:**\n- Current Friday staffing appears adequate (no degradation in service metrics)\n- Monitor: Watch for Friday capacity constraints as volume grows\n\n**For analytics team:**\n- Investigate: Why do customers shop more on Fridays?\n- Build: Day-of-week forecasting model to improve inventory planning\n```\n\n---\n\n## Phase 6: Identify Follow-up Questions\n\n**Goal:** Turn conclusions into next analytical steps.\n\n### Generate \"Next-Level\" Questions\n\nGood analysis creates more questions than it answers:\n\n```markdown\n## Follow-up Questions\n\n### Questions to deepen understanding:\n1. [Question that drills into WHY pattern exists]\n   - Data needed: [What data would answer this?]\n   - Query approach: [How would we query for this?]\n\n2. [Question that tests alternative explanation]\n   - Data needed: [...]\n   - Query approach: [...]\n\n### Questions to test generalizability:\n3. [Does this pattern hold in different segments?]\n   - Segment by: [Customer type, product category, region, etc.]\n\n4. [Is this pattern stable over time?]\n   - Test: [Earlier time periods, recent vs historical]\n\n### Questions to assess actionability:\n5. [Can we influence this pattern?]\n   - Experiment: [What intervention could we test?]\n\n6. [What's the ROI of acting on this finding?]\n   - Calculate: [Revenue impact, cost, net benefit]\n```\n\n### Prioritize Follow-up Questions\n\nNot all questions are equally valuable:\n\n```markdown\n## Question Prioritization\n\n**High Priority (Do Next):**\n- [Questions that directly inform pending decisions]\n- [Questions that could refute our main conclusion]\n- [Questions that are cheap/fast to answer with existing data]\n\n**Medium Priority (Do Eventually):**\n- [Questions that deepen understanding but don't change decisions]\n- [Questions that require additional data collection]\n\n**Low Priority (Backlog):**\n- [Interesting but not actionable]\n- [Questions that would take significant effort for marginal insight]\n```\n\n### Design Follow-up Analyses\n\nFor high-priority questions, sketch the analysis:\n\n```markdown\n## Proposed Follow-up Analysis\n\n**Question:** Does Friday pattern vary by customer segment?\n\n**Hypothesis:** Business customers drive Friday peak (ordering for next week)\n\n**Data needed:**\n- Customer segment field (B2B vs B2C)\n- Order data with day-of-week already calculated\n\n**Query approach:**\n```sql\n-- Compare day-of-week patterns by segment\nSELECT\n  customer_segment,\n  day_of_week,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY customer_segment), 2) as pct_of_segment\nFROM orders_with_dow\nGROUP BY customer_segment, day_of_week\nORDER BY customer_segment, day_of_week;\n```\n\n**Expected outcome:**\n- If hypothesis correct: B2B will show stronger Friday peak than B2C\n- If hypothesis wrong: Pattern will be similar across segments\n- Alternative: Entirely different day-of-week pattern by segment\n\n**Decision impact:**\n- If B2B-driven: Focus Friday capacity on B2B fulfillment\n- If B2C-driven: Focus Friday capacity on consumer experience\n```\n\n---\n\n## Documentation Requirements\n\nAfter completing all 6 phases, create an interpretation summary:\n\n```markdown\n## Result Interpretation Summary\n\n### Context\n- Question: [What we set out to answer]\n- Data: [What we analyzed]\n- Time period: [Coverage]\n\n### Key Findings\n1. [Finding 1 with supporting numbers]\n2. [Finding 2 with supporting numbers]\n3. [Finding 3 with supporting numbers]\n\n### Interpretation\n[2-3 paragraph narrative explaining what findings mean]\n\n### Confidence Assessment\n- Overall confidence: [High/Medium/Low]\n- Key uncertainties: [What remains unknown]\n- Supporting evidence: [What makes us confident]\n- Contradicting evidence: [What makes us uncertain]\n\n### Caveats\n- [Limitation 1]\n- [Limitation 2]\n- [Limitation 3]\n\n### Recommendations\n1. [Actionable recommendation with rationale]\n2. [Actionable recommendation with rationale]\n3. [Further investigation needed]\n\n### Follow-up Questions\n- High priority: [Question 1]\n- High priority: [Question 2]\n- Medium priority: [Question 3]\n```\n\n---\n\n## Common Interpretation Pitfalls\n\n### Pitfall 1: Confirmation Bias\n\n**Problem:** Seeing what you expect to see, ignoring contradictory evidence.\n\n**Example:**\n- Hypothesis: \"Campaign increased sales\"\n- Finding: Sales up 10% during campaign\n- Bias: Concluding campaign worked without checking control group\n- Missed: Sales were up 12% overall that week (campaign underperformed!)\n\n**Prevention:**\n- Always generate alternative explanations (Phase 3)\n- Actively look for evidence against your hypothesis\n- Use comparison groups (before/after, treatment/control)\n\n### Pitfall 2: Correlation ≠ Causation\n\n**Problem:** Assuming that because A and B move together, A causes B.\n\n**Example:**\n- Finding: \"Higher-priced products have lower return rates\"\n- Causal claim: \"Raising prices will reduce returns\" (WRONG!)\n- Reality: Quality drives both price and returns (confound)\n\n**Prevention:**\n- Distinguish correlation from causation in language\n- Identify potential confounding variables\n- Design experiments to test causation (not just observe correlation)\n\n### Pitfall 3: Cherry-Picking\n\n**Problem:** Highlighting patterns that support your story, hiding those that don't.\n\n**Example:**\n- Finding: Segment A had 20% growth, Segment B had 5% growth, Segment C declined 8%\n- Cherry-pick: \"We're seeing strong growth in key segments!\" (only mention A & B)\n- Honest: \"Mixed results: growth in 2 of 3 segments, overall trend uncertain\"\n\n**Prevention:**\n- Report all segments, not just interesting ones\n- Include negative/null findings\n- Predefine what you'll measure before looking at data\n\n### Pitfall 4: Texas Sharpshooter Fallacy\n\n**Problem:** Finding patterns in noise, then creating explanations post-hoc.\n\n**Example:**\n- Finding: \"Sales spike every 3rd Tuesday when temperature is above 75°F\"\n- Reality: Random noise, but you found a pattern in 3 instances\n- Test: Does pattern predict NEXT 3rd Tuesday above 75°F? (Probably not)\n\n**Prevention:**\n- Test patterns on hold-out data (future time period)\n- Ask: \"Would I have predicted this pattern before seeing data?\"\n- Calculate: How many patterns did I check? (Multiple comparisons problem)\n\n### Pitfall 5: Ignoring Base Rates\n\n**Problem:** Misinterpreting percentages without considering absolute numbers.\n\n**Example:**\n- Finding: \"New product line has 50% higher conversion rate!\" (2% vs 3%)\n- Ignored: New line has 1/100th the traffic of main line\n- Reality: Absolute impact is tiny (20 vs 5,000 conversions)\n\n**Prevention:**\n- Report both percentages and absolute numbers\n- Consider volume when assessing significance\n- Calculate absolute impact, not just relative change\n\n### Pitfall 6: Simpson's Paradox\n\n**Problem:** Aggregate trends that reverse when data is segmented.\n\n**Example:**\n- Aggregate: Treatment group has worse outcomes than control\n- Segmented: Treatment is better in EVERY segment\n- Cause: Treatment group had more severe cases (confound)\n\n**Prevention:**\n- Segment data by key dimensions\n- Check if aggregate pattern holds within segments\n- Identify potential confounding factors\n\n### Pitfall 7: Survivorship Bias\n\n**Problem:** Analyzing only data that \"survived\" to be recorded, missing the full picture.\n\n**Example:**\n- Finding: \"Our top customers have 80% retention rate!\"\n- Missed: You only analyzed customers who made it to \"top\" status\n- Reality: 95% of customers churned before becoming \"top\"\n\n**Prevention:**\n- Define population carefully (all customers vs top customers)\n- Analyze attrition/dropout before looking at survivors\n- Include \"failed\" cases in analysis\n\n---\n\n## When to Revisit Interpretation\n\nRe-run portions of this skill when:\n- New data becomes available (test if conclusions hold)\n- Stakeholders challenge your conclusions (strengthen with alternative explanations)\n- You're about to make a major decision based on findings (verify confidence level)\n- Follow-up analyses contradict initial findings (update interpretation)\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `interpreting-results` component skill to systematically interpret query outputs, ensuring intellectual honesty and avoiding premature conclusions.\n```\n\nThis ensures analysts:\n1. Consider context before interpreting patterns\n2. Describe patterns objectively before explaining them\n3. Generate multiple alternative explanations\n4. Assess both statistical and practical significance\n5. State conclusions with appropriate caveats\n6. Identify valuable follow-up questions\n\nRigorous interpretation is the difference between data analysis and data-driven storytelling."
              },
              {
                "name": "marketing-experimentation",
                "description": "Systematic marketing experimentation process - discover concepts, generate hypotheses, coordinate multiple experiments, synthesize results, generate next-iteration ideas through rigorous validation cycles",
                "path": "plugins/datapeeker/skills/marketing-experimentation/SKILL.md",
                "frontmatter": {
                  "name": "marketing-experimentation",
                  "description": "Systematic marketing experimentation process - discover concepts, generate hypotheses, coordinate multiple experiments, synthesize results, generate next-iteration ideas through rigorous validation cycles"
                },
                "content": "# Marketing Experimentation\n\n## Overview\n\nUse this skill when you need to validate marketing concepts or business ideas through rigorous experimental cycles. This skill orchestrates the complete Build-Measure-Learn cycle from concept to data-driven signal.\n\n**When to use this skill:**\n- You have a marketing concept or business idea that needs validation\n- You want to test multiple related hypotheses systematically\n- You need to integrate results across multiple experiments\n- You're designing the next iteration based on experimental evidence\n- You're conducting qualitative market research combined with quantitative testing\n\n**What this skill does:**\n- Validates concepts through market research before experimentation\n- Generates multiple testable hypotheses from marketing ideas\n- Coordinates multiple experiments: quantitative (hypothesis-testing) and qualitative (qualitative-research)\n- Synthesizes results across experiments using interpreting-results and creating-visualizations\n- Produces clear signals (positive/negative/null/mixed) for each campaign\n- Generates actionable next-iteration ideas based on experimental evidence\n\n**What this skill does NOT do:**\n- Design individual experiments (delegates to hypothesis-testing or qualitative-research)\n- Execute statistical analysis directly (uses hypothesis-testing for quantitative rigor)\n- Conduct interviews/surveys/observations directly (uses qualitative-research for qualitative rigor)\n- Operationalize successful ideas (focuses on validation, not scaling)\n- Platform-specific implementation (tool-agnostic techniques only)\n\n**Integration with existing skills:**\n- **Delegates to `hypothesis-testing`** for quantitative experiment design and execution (metrics, A/B tests, statistical analysis)\n- **Delegates to `qualitative-research`** for qualitative experiment design and execution (interviews, surveys, focus groups, observations)\n- **Uses `interpreting-results`** to synthesize findings across multiple experiments\n- **Uses `creating-visualizations`** to communicate aggregate results\n- **Invokes `market-researcher` agent** for concept validation via internet research\n\n**Multi-conversation persistence:**\nThis skill is designed for campaigns spanning days or weeks. Each phase documents completely enough that new conversations can resume after extended breaks. The experiment tracker (04-experiment-tracker.md) serves as the living coordination hub.\n\n## Prerequisites\n\n**Required skills:**\n- `hypothesis-testing` - Quantitative experiment design and execution (invoked for metric-based experiments)\n- `qualitative-research` - Qualitative experiment design and execution (invoked for interviews, surveys, focus groups, observations)\n- `interpreting-results` - Result synthesis and pattern identification (invoked in Phase 5)\n- `creating-visualizations` - Aggregate result visualization (invoked in Phase 5)\n\n**Required agents:**\n- `market-researcher` - Concept validation via internet research (invoked in Phase 1)\n\n**Required knowledge:**\n- Understanding of Lean Startup Build-Measure-Learn cycle\n- Familiarity with marketing tactics (landing pages, ads, email, content)\n- Basic experimental design principles (control/treatment, signals, metrics)\n- Understanding of qualitative vs quantitative research methods\n\n**Data requirements:**\n- None initially (market research is qualitative)\n- Data requirements emerge from experiment design in Phase 4\n- Quantitative experiments (hypothesis-testing): SQL databases, analytics data, A/B test results\n- Qualitative experiments (qualitative-research): Interview transcripts, survey responses, observation notes\n\n## Mandatory Process Structure\n\n**CRITICAL:** This is a 6-phase process skill. You MUST complete all phases in order. Use TodoWrite to track progress through each phase.\n\n**TodoWrite template:**\n\nWhen starting a marketing-experimentation session, create these todos:\n\n```markdown\n- [ ] Phase 1: Discovery & Asset Inventory\n- [ ] Phase 2: Hypothesis Generation\n- [ ] Phase 3: Prioritization\n- [ ] Phase 4: Experiment Coordination\n- [ ] Phase 5: Cross-Experiment Synthesis\n- [ ] Phase 6: Iteration Planning\n```\n\n**Workspace structure:**\n\nAll work for a marketing-experimentation session is saved to:\n```\nanalysis/marketing-experimentation/[campaign-name]/\n├── 01-discovery.md\n├── 02-hypothesis-generation.md\n├── 03-prioritization.md\n├── 04-experiment-tracker.md\n├── 05-synthesis.md\n├── 06-iteration-plan.md\n└── experiments/\n    ├── [experiment-1]/               # hypothesis-testing session\n    ├── [experiment-2]/               # hypothesis-testing session\n    └── [experiment-3]/               # hypothesis-testing session\n```\n\n**Phase progression rules:**\n1. Each phase has a CHECKPOINT with verification requirements\n2. You MUST satisfy all checkpoint requirements before proceeding to the next phase\n3. Document every decision with rationale in the numbered markdown files\n4. Commit markdown files after each phase completes\n5. The experiment tracker (04-experiment-tracker.md) is a LIVING DOCUMENT - update it throughout Phase 4\n\n**Multi-conversation resumption:**\n- At the start of any conversation, check if an experiment tracker exists for this campaign\n- If it exists, read it first to understand current experiment status\n- Update the tracker as experiments progress\n- All phases should be complete enough to resume after days or weeks\n\n---\n\n## Phase 1: Discovery & Asset Inventory\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Gathered business concept description from user\n- [ ] Invoked market-researcher agent and documented findings\n- [ ] Completed asset inventory (content, campaigns, audiences, data)\n- [ ] Defined success criteria and validation signals\n- [ ] Documented known constraints\n- [ ] Saved to `01-discovery.md`\n\n### Instructions\n\n1. **Gather the business concept**\n   - Ask user to describe the marketing concept or business idea to validate\n   - What problem does it solve? Who is the target audience?\n   - What's the desired outcome? (awareness, leads, conversions, etc.)\n   - What stage is this idea at? (new concept, existing campaign, iteration)\n\n2. **Invoke market-researcher agent for concept validation**\n\n   Dispatch the `market-researcher` agent with the concept description:\n   - Agent will research market demand signals\n   - Agent will identify similar solutions and competitors\n   - Agent will analyze audience needs and pain points\n   - Agent will find validation evidence (case studies, reviews, testimonials)\n\n   Document agent findings in `01-discovery.md` under \"Market Research Findings\"\n\n3. **Conduct asset inventory**\n\n   Work with user to inventory existing assets that could be leveraged:\n\n   **Content Assets:**\n   - Blog posts, case studies, whitepapers\n   - Video content, webinars, tutorials\n   - Social media presence and following\n   - Email lists and subscriber segments\n\n   **Campaign Assets:**\n   - Existing ad campaigns and performance data\n   - Landing pages and conversion rates\n   - Email campaigns and open/click rates\n   - SEO performance and keyword rankings\n\n   **Audience Assets:**\n   - Customer segments and personas\n   - Audience data (demographics, behaviors, preferences)\n   - Customer feedback and reviews\n   - Support tickets and common questions\n\n   **Data Assets:**\n   - Analytics platforms (Google Analytics, Mixpanel, etc.)\n   - CRM data (Salesforce, HubSpot, etc.)\n   - Ad platform data (Google Ads, Facebook Ads, etc.)\n   - Email platform data (Mailchimp, SendGrid, etc.)\n\n4. **Define success criteria and validation signals**\n\n   Work with user to define:\n   - What metrics indicate success? (CTR, conversion rate, CAC, LTV, etc.)\n   - What magnitude of change is meaningful? (practical significance thresholds)\n   - What signal types are acceptable?\n     - Positive: Validates concept, proceed to scale\n     - Negative: Invalidates concept, pivot or abandon\n     - Null: Inconclusive, needs refinement or more data\n     - Mixed: Some aspects work, some don't, iterate strategically\n\n5. **Document known constraints**\n\n   Capture any constraints that will affect experimentation:\n   - Budget constraints (ad spend limits, tool costs)\n   - Time constraints (launch deadlines, seasonal factors)\n   - Resource constraints (team capacity, content production)\n   - Technical constraints (platform limitations, integration issues)\n   - Regulatory constraints (GDPR, CCPA, industry regulations)\n\n6. **Create `01-discovery.md`** with: `./templates/01-discovery.md`\n\n7. **STOP and get user confirmation**\n   - Review discovery findings with user\n   - Confirm asset inventory is complete\n   - Confirm success criteria are appropriate\n   - Do NOT proceed to Phase 2 until confirmed\n\n**Common Rationalization:** \"I'll skip discovery and go straight to testing - the concept is obvious\"\n**Reality:** Discovery surfaces assumptions, constraints, and existing assets that dramatically affect experiment design. Always start with discovery.\n\n**Common Rationalization:** \"I don't need market research - I already know this market\"\n**Reality:** The market-researcher agent provides current, data-driven validation signals that prevent building experiments around false assumptions. Always validate.\n\n**Common Rationalization:** \"Asset inventory is busywork - I'll figure out what's available as I go\"\n**Reality:** Existing assets can dramatically reduce experiment cost and time. Inventorying first prevents reinventing wheels and enables building on proven foundations.\n\n---\n\n## Phase 2: Hypothesis Generation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 5-10 testable hypotheses from the concept\n- [ ] Each hypothesis maps to a specific tactic/channel\n- [ ] Each hypothesis has expected outcome and rationale\n- [ ] Hypotheses cover multiple tactics (not all ads or all email)\n- [ ] Referenced relevant frameworks (Lean Startup, AARRR, ICE/RICE)\n- [ ] Saved to `02-hypothesis-generation.md`\n\n### Instructions\n\n1. **Generate 5-10 testable hypotheses**\n\n   For each hypothesis, use this format:\n\n   **Hypothesis [N]: [Brief statement]**\n   - **Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n   - **Expected Outcome:** [Specific, measurable result]\n   - **Rationale:** [Why we believe this will work based on discovery findings]\n   - **Variables to Test:** [What will we manipulate/measure]\n\n   **Example hypothesis:**\n\n   **Hypothesis 1: Value proposition clarity drives conversion**\n   - **Tactic/Channel:** Landing page A/B test\n   - **Expected Outcome:** 15%+ increase in conversion rate from landing page variant with simplified value proposition\n   - **Rationale:** Market research showed audience confusion about product benefits. Discovery found existing landing page has 8 different value propositions competing for attention.\n   - **Variables to Test:** Headline clarity, benefit hierarchy, CTA prominence\n\n2. **Ensure tactic coverage**\n\n   Verify hypotheses cover multiple marketing tactics:\n\n   **Acquisition Tactics:**\n   - Landing pages (conversion optimization, value prop testing, layout)\n   - Ad campaigns (targeting, creative, messaging, platforms)\n   - Content marketing (blog posts, videos, webinars, lead magnets)\n   - SEO (keyword targeting, content optimization, technical SEO)\n\n   **Activation Tactics:**\n   - Email sequences (onboarding, nurture, activation)\n   - Product tours (in-app guidance, feature discovery)\n   - Social proof (testimonials, case studies, reviews)\n\n   **Retention Tactics:**\n   - Email campaigns (engagement, re-activation, upsell)\n   - Content (newsletters, educational content, community)\n\n   Don't generate 10 ad hypotheses. Aim for diversity across tactics.\n\n3. **Reference experimentation frameworks**\n\n   **Lean Startup Build-Measure-Learn:**\n   - Build: What's the minimum viable test? (landing page, ad, email, etc.)\n   - Measure: What metrics indicate success/failure?\n   - Learn: What will we learn regardless of outcome?\n\n   **AARRR Pirate Metrics:**\n   - Acquisition: How do users find us?\n   - Activation: Do they have a great first experience?\n   - Retention: Do they come back?\n   - Referral: Do they tell others?\n   - Revenue: Do they pay?\n\n   Map each hypothesis to one or more AARRR stages.\n\n   **ICE/RICE Prioritization (used in Phase 3):**\n   - Impact: How much will this move the metric?\n   - Confidence: How sure are we this will work?\n   - Ease: How easy is this to implement?\n   - Reach: How many users will this affect? (RICE only)\n\n4. **Create `02-hypothesis-generation.md`** with: `./templates/02-hypothesis-generation.md`\n\n5. **STOP and get user confirmation**\n   - Review all hypotheses with user\n   - Confirm hypotheses are testable and meaningful\n   - Confirm tactic coverage is appropriate\n   - Do NOT proceed to Phase 3 until confirmed\n\n**Common Rationalization:** \"I'll generate hypotheses as I build experiments - more efficient\"\n**Reality:** Generating hypotheses before prioritization enables strategic selection of highest-impact tests. Generating ad-hoc leads to testing whatever's easiest, not what matters most.\n\n**Common Rationalization:** \"I'll focus all hypotheses on one tactic (ads) since that's what we know\"\n**Reality:** Tactic diversity reveals which channels work for this concept. Single-tactic testing creates blind spots and missed opportunities.\n\n**Common Rationalization:** \"I'll write vague hypotheses and refine them during experiment design\"\n**Reality:** Vague hypotheses lead to vague experiments that produce vague results. Specific hypotheses with expected outcomes enable clear signal detection.\n\n**Common Rationalization:** \"More hypotheses = better coverage, I'll generate 20+\"\n**Reality:** Too many hypotheses dilute focus and create analysis paralysis in prioritization. 5-10 high-quality hypotheses enable strategic selection of 2-4 tests.\n\n---\n\n## Phase 3: Prioritization\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Scored all hypotheses using ICE or RICE framework with computational method\n- [ ] Created prioritized backlog (highest to lowest score) using Python script\n- [ ] Selected 2-4 highest-priority hypotheses for testing\n- [ ] Documented prioritization rationale\n- [ ] Identified any dependencies or sequencing requirements\n- [ ] Saved to `03-prioritization.md`\n\n### Instructions\n\n**CRITICAL:** You MUST use computational methods (Python scripts) to calculate scores. Do NOT estimate or manually calculate scores.\n\n1. **Choose prioritization framework**\n\n   **ICE Framework** (simpler, faster):\n   - **Impact:** How much will this move the success metric? (1-10 scale)\n   - **Confidence:** How confident are we this will work? (1-10 scale)\n   - **Ease:** How easy is this to implement? (1-10 scale)\n   - **Score:** (Impact × Confidence) / Ease\n\n   **RICE Framework** (more comprehensive):\n   - **Reach:** How many users will this affect? (absolute number or percentage)\n   - **Impact:** How much will this move the metric per user? (1-10 scale: 0.25=minimal, 3=massive)\n   - **Confidence:** How confident are we in our estimates? (percentage: 50%, 80%, 100%)\n   - **Effort:** Person-weeks to implement (absolute number)\n   - **Score:** (Reach × Impact × Confidence) / Effort\n\n   Choose ICE for speed, RICE for precision when reach varies significantly.\n\n2. **Score each hypothesis using Python script**\n\n   **For ICE Framework:**\n\n   Create a Python script to compute and sort ICE scores:\n\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"\n   ICE Score Calculator for Marketing Experimentation\n\n   Computes ICE scores: (Impact × Confidence) / Ease\n   Sorts hypotheses by score (highest to lowest)\n   \"\"\"\n\n   hypotheses = [\n       {\n           \"id\": \"H1\",\n           \"name\": \"Value proposition clarity drives conversion\",\n           \"impact\": 8,\n           \"confidence\": 7,\n           \"ease\": 9\n       },\n       {\n           \"id\": \"H2\",\n           \"name\": \"Ad targeting refinement\",\n           \"impact\": 7,\n           \"confidence\": 6,\n           \"ease\": 5\n       },\n       {\n           \"id\": \"H3\",\n           \"name\": \"Email sequence optimization\",\n           \"impact\": 6,\n           \"confidence\": 8,\n           \"ease\": 8\n       },\n       {\n           \"id\": \"H4\",\n           \"name\": \"Content marketing expansion\",\n           \"impact\": 5,\n           \"confidence\": 4,\n           \"ease\": 3\n       },\n   ]\n\n   # Calculate ICE scores\n   for h in hypotheses:\n       h['ice_score'] = (h['impact'] * h['confidence']) / h['ease']\n\n   # Sort by ICE score (descending)\n   sorted_hypotheses = sorted(hypotheses, key=lambda x: x['ice_score'], reverse=True)\n\n   # Print results table\n   print(\"| Hypothesis | Impact | Confidence | Ease | ICE Score | Rank |\")\n   print(\"|------------|--------|------------|------|-----------|------|\")\n   for rank, h in enumerate(sorted_hypotheses, 1):\n       print(f\"| {h['id']}: {h['name'][:30]} | {h['impact']} | {h['confidence']} | {h['ease']} | {h['ice_score']:.2f} | {rank} |\")\n   ```\n\n   **Usage:**\n   ```bash\n   python3 ice_calculator.py\n   ```\n\n   **For RICE Framework:**\n\n   Create a Python script to compute and sort RICE scores:\n\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"\n   RICE Score Calculator for Marketing Experimentation\n\n   Computes RICE scores: (Reach × Impact × Confidence) / Effort\n   Sorts hypotheses by score (highest to lowest)\n   \"\"\"\n\n   hypotheses = [\n       {\n           \"id\": \"H1\",\n           \"name\": \"Value proposition clarity drives conversion\",\n           \"reach\": 10000,        # users affected\n           \"impact\": 3,           # 0.25=minimal, 1=low, 2=medium, 3=high, 5=massive\n           \"confidence\": 80,      # percentage (50, 80, 100)\n           \"effort\": 2            # person-weeks\n       },\n       {\n           \"id\": \"H2\",\n           \"name\": \"Ad targeting refinement\",\n           \"reach\": 50000,\n           \"impact\": 1,\n           \"confidence\": 50,\n           \"effort\": 4\n       },\n       {\n           \"id\": \"H3\",\n           \"name\": \"Email sequence optimization\",\n           \"reach\": 5000,\n           \"impact\": 2,\n           \"confidence\": 80,\n           \"effort\": 3\n       },\n       {\n           \"id\": \"H4\",\n           \"name\": \"Content marketing expansion\",\n           \"reach\": 20000,\n           \"impact\": 1,\n           \"confidence\": 50,\n           \"effort\": 8\n       },\n   ]\n\n   # Calculate RICE scores\n   for h in hypotheses:\n       # Convert confidence percentage to decimal\n       confidence_decimal = h['confidence'] / 100\n       h['rice_score'] = (h['reach'] * h['impact'] * confidence_decimal) / h['effort']\n\n   # Sort by RICE score (descending)\n   sorted_hypotheses = sorted(hypotheses, key=lambda x: x['rice_score'], reverse=True)\n\n   # Print results table\n   print(\"| Hypothesis | Reach | Impact | Confidence | Effort | RICE Score | Rank |\")\n   print(\"|------------|-------|--------|------------|--------|------------|------|\")\n   for rank, h in enumerate(sorted_hypotheses, 1):\n       print(f\"| {h['id']}: {h['name'][:30]} | {h['reach']} | {h['impact']} | {h['confidence']}% | {h['effort']}w | {h['rice_score']:.2f} | {rank} |\")\n   ```\n\n   **Usage:**\n   ```bash\n   python3 rice_calculator.py\n   ```\n\n   **Scoring Guidance:**\n\n   **Impact (1-10 for ICE, 0.25-5 for RICE):**\n   - ICE: 1-3 minimal, 4-6 moderate, 7-8 significant, 9-10 transformative\n   - RICE: 0.25 minimal, 1 low, 2 medium, 3 high, 5 massive\n\n   **Confidence (1-10 for ICE, 50-100% for RICE):**\n   - ICE: 1-3 speculative, 4-6 uncertain, 7-8 likely, 9-10 validated\n   - RICE: 50% low confidence, 80% high confidence, 100% certainty\n\n   **Ease (1-10 for ICE):**\n   - 1-3: Complex, significant resources\n   - 4-6: Moderate effort, some obstacles\n   - 7-8: Straightforward, few dependencies\n   - 9-10: Trivial, immediate execution\n\n   **Effort (person-weeks for RICE):**\n   - Estimate total person-weeks required\n   - Include design, implementation, monitoring time\n   - Examples: 1w (simple landing page), 4w (complex ad campaign), 8w (content series)\n\n3. **Run scoring script and document results**\n\n   1. Create the Python script (ice_calculator.py or rice_calculator.py)\n   2. Update the `hypotheses` list with actual hypothesis data from Phase 2\n   3. Run the script: `python3 [ice|rice]_calculator.py`\n   4. Copy the output table into `03-prioritization.md`\n   5. Include the script in the markdown file for reproducibility:\n\n   ```markdown\n   ## Prioritization Calculation\n\n   **Method:** ICE Framework\n\n   **Calculation Script:**\n   ```python\n   [paste full script here]\n   ```\n\n   **Results:**\n\n   [paste output table here]\n   ```\n\n4. **Select 2-4 highest-priority hypotheses**\n\n   Considerations for selection:\n   - **Don't test everything:** Focus on highest-scoring 2-4 hypotheses\n   - **Resource constraints:** Match selection to available time/budget/capacity\n   - **Learning value:** Sometimes lower-scoring hypothesis with high uncertainty is worth testing\n   - **Dependencies:** Test prerequisites before dependent hypotheses\n   - **Sequencing:** Consider whether experiments need to run sequentially or can run in parallel\n\n   **Selection criteria:**\n   - Primary: Top 2-4 by ICE/RICE score from computational results\n   - Secondary: Balance quick wins vs. high-impact long-term bets\n   - Tertiary: Ensure tactic diversity (don't test 3 ad variants if other tactics untested)\n\n5. **Document experiment sequence**\n\n   Determine execution strategy:\n   - **Parallel:** Multiple experiments running simultaneously (faster results, higher resource needs)\n   - **Sequential:** One experiment at a time (slower, easier to manage)\n   - **Hybrid:** Run independent experiments in parallel, sequence dependent ones\n\n   **Example sequence plan:**\n   ```\n   Week 1-2: Launch H1 (landing page) and H3 (email) in parallel\n   Week 3-4: Analyze H1 and H3 results\n   Week 5-6: Launch H2 (ads) based on H1 learnings\n   Week 7-8: Analyze H2 results\n   ```\n\n6. **Create `03-prioritization.md`** with: `./templates/03-prioritization.md`\n\n7. **STOP and get user confirmation**\n   - Review computed scores and prioritization with user\n   - Confirm selected hypotheses are appropriate\n   - Confirm experiment sequence is feasible\n   - Do NOT proceed to Phase 4 until confirmed\n\n**Common Rationalization:** \"I'll test all hypotheses - don't want to miss opportunities\"\n**Reality:** Resource constraints make testing everything impossible. Prioritization ensures highest-value experiments get resources. Unfocused testing produces weak signals across too many fronts.\n\n**Common Rationalization:** \"Scoring is subjective and arbitrary - I'll just pick what feels right\"\n**Reality:** Scoring frameworks force explicit reasoning about trade-offs. \"Feels right\" selections optimize for recency bias and personal preference, not business value. Computational methods ensure consistency.\n\n**Common Rationalization:** \"I'll skip prioritization and go straight to easiest test\"\n**Reality:** Easiest test rarely equals highest value. Prioritization prevents optimizing for ease at the expense of impact.\n\n**Common Rationalization:** \"I'll estimate scores mentally instead of running the script\"\n**Reality:** Manual estimation introduces calculation errors and inconsistency. Python scripts ensure exact, reproducible results that can be audited and verified.\n\n---\n\n## Phase 4: Experiment Coordination\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Created experiment tracker with all selected hypotheses\n- [ ] Determined experiment type for each hypothesis (Quantitative or Qualitative)\n- [ ] Invoked appropriate skill for each hypothesis (hypothesis-testing OR qualitative-research)\n- [ ] Updated tracker with experiment status (Planned, In Progress, Complete)\n- [ ] Documented location of each experiment session\n- [ ] Saved experiment tracker to `04-experiment-tracker.md`\n- [ ] Note: This phase may span multiple days/weeks and conversations\n\n### Instructions\n\n**CRITICAL:** This phase is designed for multi-conversation workflows. The experiment tracker is a LIVING DOCUMENT that you will update throughout experimentation. New conversations should ALWAYS read this file first.\n\n1. **Determine experiment type for each hypothesis**\n\n   **CRITICAL:** Before creating the tracker, classify each hypothesis as Quantitative or Qualitative.\n\n   **Quantitative experiments** use hypothesis-testing skill:\n   - Measure **numeric metrics**: CTR, conversion rate, bounce rate, time on page, revenue, CAC, LTV, etc.\n   - Rely on **existing data sources**: Google Analytics, ad platforms, CRM, email platforms, database queries\n   - Test using **A/B tests, multivariate tests, or time-series analysis**\n   - Require **statistical significance testing**\n   - **Examples:**\n     - Landing page A/B test measuring conversion rate\n     - Ad campaign comparing CTR across different creatives\n     - Email sequence measuring open rate and click-through rate\n     - SEO experiment tracking organic traffic changes\n\n   **Qualitative experiments** use qualitative-research skill:\n   - Gather **non-numeric insights**: opinions, experiences, needs, pain points, motivations\n   - Collect through **interviews, surveys, focus groups, or observations**\n   - Analyze using **thematic analysis** rather than statistical tests\n   - Focus on **understanding why** behaviors occur\n   - **Examples:**\n     - Customer discovery interviews to understand pain points\n     - Open-ended survey asking about product needs\n     - Focus group discussing ad creative perceptions\n     - Observational study of how users interact with product\n\n   **Decision criteria:**\n\n   Ask: \"What do we need to learn?\"\n   - If answer is \"Does X increase metric Y by Z%?\" → **Quantitative** (hypothesis-testing)\n   - If answer is \"Why do users do X?\" or \"What do users think about Y?\" → **Qualitative** (qualitative-research)\n\n   Ask: \"What data will we collect?\"\n   - If answer is \"Metrics from analytics\" → **Quantitative** (hypothesis-testing)\n   - If answer is \"Interview transcripts, survey responses, or observation notes\" → **Qualitative** (qualitative-research)\n\n   **Mixed methods:**\n   - Some hypotheses may require BOTH quantitative and qualitative experiments\n   - Example: \"Value prop clarity drives conversion\" could test:\n     - Quantitatively: A/B test landing page, measure conversion rate (hypothesis-testing)\n     - Qualitatively: Interview users about which value prop resonates (qualitative-research)\n   - Track these as separate experiments with linked hypotheses in the tracker\n\n2. **Create experiment tracker**\n\n   The tracker is your coordination hub for managing multiple experiments over time.\n\n   Create `04-experiment-tracker.md` with: `./templates/04-experiment-tracker.md`\n\n   **Tracker format:**\n\n   For each selected hypothesis, create an entry:\n\n   ```markdown\n   ### Experiment 1: [Hypothesis Brief Name]\n\n   **Status:** [Planned | In Progress | Complete]\n   **Hypothesis:** [Full hypothesis statement from Phase 2]\n   **Tactic/Channel:** [landing page | ads | email | etc.]\n   **Priority Score:** [ICE/RICE score from Phase 3]\n   **Start Date:** [YYYY-MM-DD or \"Not started\"]\n   **Completion Date:** [YYYY-MM-DD or \"In progress\"]\n   **Location:** `analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/`\n   **Signal:** [Positive | Negative | Null | Mixed | \"Not analyzed\"]\n   **Key Findings:** [Brief summary when complete, \"TBD\" otherwise]\n   ```\n\n2. **Invoke appropriate skill for each experiment**\n\n   For each hypothesis marked \"Planned\" or \"In Progress\":\n\n   **Step 1:** Read the hypothesis details from `02-hypothesis-generation.md`\n\n   **Step 2a:** If **Quantitative experiment**, invoke `hypothesis-testing` skill:\n   ```markdown\n   Use hypothesis-testing skill to test: [Hypothesis statement]\n\n   Context for hypothesis-testing:\n   - Session name: [descriptive-name-for-experiment]\n   - Save location: analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/\n   - Success criteria: [From Phase 1 discovery]\n   - Expected outcome: [From Phase 2 hypothesis]\n   - Metric to measure: [CTR, conversion rate, etc.]\n   - Data source: [Google Analytics, database, etc.]\n   ```\n\n   **Step 2b:** If **Qualitative experiment**, invoke `qualitative-research` skill:\n   ```markdown\n   Use qualitative-research skill to conduct: [Hypothesis statement]\n\n   Context for qualitative-research:\n   - Session name: [descriptive-name-for-experiment]\n   - Save location: analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/\n   - Research question: [From Phase 2 hypothesis]\n   - Collection method: [Interviews | Surveys | Focus Groups | Observations]\n   - Success criteria: [What insights validate/invalidate hypothesis?]\n   ```\n\n   **Step 3:** Update experiment tracker:\n   - Change status from \"Planned\" to \"In Progress\"\n   - Add start date\n   - Update location with actual path\n   - Document experiment type (Quantitative or Qualitative)\n\n   **Step 4a:** Let **hypothesis-testing** skill complete its 5-phase workflow:\n   - Phase 1: Hypothesis Formulation\n   - Phase 2: Test Design\n   - Phase 3: Data Analysis\n   - Phase 4: Statistical Interpretation\n   - Phase 5: Conclusion\n\n   **Step 4b:** Let **qualitative-research** skill complete its 6-phase workflow:\n   - Phase 1: Research Design\n   - Phase 2: Data Collection\n   - Phase 3: Data Familiarization\n   - Phase 4: Systematic Coding\n   - Phase 5: Theme Development\n   - Phase 6: Synthesis & Reporting\n\n   **Step 5:** When skill completes, update tracker:\n   - Change status to \"Complete\"\n   - Add completion date\n   - Document signal (Positive/Negative/Null/Mixed)\n   - Summarize key findings\n\n   **Step 6:** Commit tracker updates after each status change\n\n3. **Handle multi-conversation resumption**\n\n   **At the start of EVERY conversation during Phase 4:**\n\n   1. Check if `04-experiment-tracker.md` exists\n   2. If it exists, READ IT FIRST before doing anything else\n   3. Review experiment status:\n      - Planned: Ready to launch\n      - In Progress: Check hypothesis-testing session for current phase\n      - Complete: Ready for synthesis (Phase 5)\n   4. Ask user which experiment to continue or which new experiment to launch\n   5. Update tracker with new status/dates/findings\n   6. Commit tracker updates\n\n   **Example resumption:**\n   ```markdown\n   I've read the experiment tracker. Current status:\n   - Experiment 1 (H1: Value prop): Complete, Positive signal\n   - Experiment 2 (H3: Email sequence): In Progress, currently in hypothesis-testing Phase 3\n   - Experiment 3 (H2: Ad targeting): Planned, not yet started\n\n   What would you like to do?\n   a) Continue Experiment 2 (in hypothesis-testing Phase 3)\n   b) Start Experiment 3\n   c) Move to synthesis (Phase 5) since Experiment 1 is complete\n   ```\n\n4. **Coordinate parallel vs. sequential experiments**\n\n   **Parallel execution (multiple experiments simultaneously):**\n   - Launch multiple hypothesis-testing sessions\n   - Track each separately in experiment tracker\n   - Update tracker as each progresses independently\n   - Requires managing multiple analysis directories\n\n   **Sequential execution (one at a time):**\n   - Complete one experiment fully before starting next\n   - Simpler tracking, easier to manage\n   - Can incorporate learnings between experiments\n\n   **Hybrid execution:**\n   - Run independent experiments in parallel\n   - Sequence dependent experiments (e.g., H2 depends on H1 insights)\n\n5. **Progress through all experiments**\n\n   Continue invoking hypothesis-testing and updating the tracker until:\n   - All selected experiments have status \"Complete\"\n   - All experiments have documented signals\n   - All findings are summarized in tracker\n\n   Only when ALL experiments are complete should you proceed to Phase 5.\n\n**Common Rationalization:** \"I'll keep experiment details in my head - the tracker is just busywork\"\n**Reality:** Multi-day campaigns lose context between conversations. The tracker is the ONLY source of truth that persists across sessions. Without it, you'll re-ask questions and lose progress.\n\n**Common Rationalization:** \"I'll wait until all experiments finish before updating the tracker\"\n**Reality:** Batch updates create opportunity for lost data. Update the tracker IMMEDIATELY after status changes. Real-time tracking prevents confusion and missed experiments.\n\n**Common Rationalization:** \"I'll design the experiment myself instead of using hypothesis-testing\"\n**Reality:** hypothesis-testing skill provides rigorous experimental design, statistical analysis, and signal detection. Skipping it produces weak experiments with ambiguous results.\n\n**Common Rationalization:** \"All experiments are done, I don't need to update the tracker before synthesis\"\n**Reality:** The tracker is your input to Phase 5. Incomplete tracker means incomplete synthesis. Update ALL fields (status, dates, signals, findings) before proceeding.\n\n---\n\n## Phase 5: Cross-Experiment Synthesis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] ALL experiments from Phase 4 marked \"Complete\" with signals documented\n- [ ] Created aggregate results table across all experiments\n- [ ] Invoked presenting-data skill to synthesize findings with visualizations\n- [ ] Documented what worked, what didn't, and what's unclear\n- [ ] Classified overall campaign signal (Positive/Negative/Null/Mixed)\n- [ ] Saved to `05-synthesis.md`\n\n### Instructions\n\n**CRITICAL:** This phase synthesizes results ACROSS multiple experiments. Do NOT proceed until ALL Phase 4 experiments are complete with documented signals.\n\n1. **Verify experiment completion**\n\n   Read `04-experiment-tracker.md` and verify:\n   - All experiments have Status = \"Complete\"\n   - All experiments have Signal documented (Positive/Negative/Null/Mixed)\n   - All experiments have Key Findings summarized\n\n   If any experiments are incomplete, return to Phase 4 to finish them.\n\n2. **Create aggregate results table**\n\n   Compile findings from all experiments into a summary table:\n\n   **Example Aggregate Table (Mixed Quantitative & Qualitative):**\n\n   | Experiment | Type | Hypothesis | Tactic | Signal | Key Finding | Confidence |\n   |------------|------|------------|--------|--------|-------------|------------|\n   | E1 | Quant | Value prop clarity | Landing page A/B test | Positive | Conversion rate +18% (p<0.05) | High |\n   | E2 | Qual | Customer pain points | Discovery interviews | Positive | 8 of 10 cited onboarding complexity | High |\n   | E3 | Quant | Ad targeting | Ads | Null | CTR +2% (not sig., p=0.12) | Medium |\n   | E4 | Qual | Ad message resonance | Focus groups | Negative | 6 of 8 found messaging confusing | High |\n\n   **For Quantitative experiments (hypothesis-testing):**\n   - Signal classification (Positive/Negative/Null/Mixed)\n   - Key metric measured (CTR, conversion rate, etc.)\n   - Magnitude of effect with statistical significance (e.g., \"+18%, p<0.05\")\n   - Confidence level from statistical analysis\n\n   **For Qualitative experiments (qualitative-research):**\n   - Signal classification (Positive/Negative/Null/Mixed)\n   - Key themes identified with prevalence (e.g., \"8 of 10 participants mentioned X\")\n   - Representative quotes or patterns\n   - Confidence assessment (credibility, dependability, transferability)\n\n3. **Invoke presenting-data skill for comprehensive synthesis**\n\n   Use the `presenting-data` skill to create complete synthesis with visualizations and presentation materials:\n\n   ```markdown\n   Use presenting-data skill to synthesize marketing experimentation results:\n\n   Context:\n   - Campaign: [campaign name]\n   - Experiments completed: [count]\n   - Results table: [paste aggregate table]\n   - Audience: [stakeholders/decision-makers]\n   - Format: [markdown report | slides | whitepaper]\n   - Focus: Pattern identification across experiments (what works, what doesn't, what's unclear)\n   ```\n\n   **presenting-data skill will handle:**\n   - Pattern identification (using interpreting-results internally)\n   - Visualization creation (using creating-visualizations internally)\n   - Synthesis documentation (markdown, slides, or whitepaper format)\n   - Citation of sources (individual hypothesis-testing sessions)\n   - Reproducibility (references to experiment locations)\n\n   **Focus areas for synthesis:**\n   - **What worked:** Experiments with Positive signals (both quantitative and qualitative)\n   - **What didn't work:** Experiments with Negative signals (both quantitative and qualitative)\n   - **What's unclear:** Experiments with Null or Mixed signals\n   - **Cross-experiment patterns:** Do results cluster by tactic? By audience? By timing? Do quantitative and qualitative findings align or conflict?\n   - **Triangulation:** Do qualitative findings explain quantitative results? (e.g., interviews reveal WHY conversion rate increased)\n   - **Confounding factors:** Are there external factors affecting multiple experiments?\n   - **Confidence assessment:** Which findings are robust? Which are uncertain? How do qualitative and quantitative confidence levels compare?\n\n4. **Document patterns and insights**\n\n   The presenting-data skill will create `05-synthesis.md` (or slides/whitepaper) with:\n\n   **What Worked (Positive Signals):**\n   - List experiments with positive results\n   - Explain WHY these worked (based on analysis)\n   - Identify commonalities across successful experiments\n\n   **What Didn't Work (Negative Signals):**\n   - List experiments with negative results\n   - Explain WHY these failed (based on analysis)\n   - Identify lessons learned\n\n   **What's Unclear (Null/Mixed Signals):**\n   - List experiments with inconclusive results\n   - Explain potential reasons (insufficient power, confounding factors, etc.)\n   - Identify what additional investigation is needed\n\n   **Cross-Experiment Patterns:**\n   - Do results cluster by tactic, audience, timing, or other factors?\n   - Are there confounding variables affecting multiple experiments?\n   - What overarching insights emerge?\n\n   **Visualizations (created by presenting-data):**\n   - Signal distribution (bar chart: Positive/Negative/Null/Mixed counts)\n   - Effect sizes (bar chart: metric changes by experiment)\n   - Confidence levels (scatter plot: effect size vs. confidence)\n   - Tactic performance (grouped by channel/tactic)\n\n5. **Classify overall campaign signal**\n\n   Based on aggregate analysis (from presenting-data output), classify the campaign:\n\n   **Positive:** Campaign validates concept, proceed to scaling\n   - Multiple experiments show positive signals\n   - Successful tactics identified for scale-up\n   - Clear path to ROI improvement\n\n   **Negative:** Campaign invalidates concept, pivot or abandon\n   - Multiple experiments show negative signals\n   - No successful tactics identified\n   - Concept doesn't resonate with audience\n\n   **Null:** Campaign results inconclusive, needs refinement\n   - Most experiments show null signals\n   - Insufficient power or confounding factors\n   - Needs redesigned experiments or longer observation\n\n   **Mixed:** Some aspects work, some don't, iterate strategically\n   - Mix of positive and negative signals across experiments\n   - Some tactics work, others don't\n   - Selective scaling + pivots needed\n\n6. **Review presenting-data output and finalize synthesis**\n\n   After presenting-data skill completes:\n   - Review generated synthesis document\n   - Verify all experiments are covered\n   - Confirm visualizations are appropriate\n   - Ensure signal classification is documented\n   - Make any necessary edits for clarity\n\n7. **STOP and get user confirmation**\n   - Review synthesis findings with user\n   - Confirm pattern interpretations are accurate\n   - Confirm overall signal classification is appropriate\n   - Do NOT proceed to Phase 6 until confirmed\n\n**Common Rationalization:** \"I'll synthesize results mentally - no need to document patterns\"\n**Reality:** Mental synthesis loses details and creates false confidence. Documented synthesis with presenting-data skill ensures intellectual honesty and identifies confounding factors you'd otherwise miss.\n\n**Common Rationalization:** \"I'll skip synthesis for experiments with clear signals\"\n**Reality:** Individual experiment signals don't reveal cross-experiment patterns. Synthesis identifies why some tactics work while others don't - the strategic insight that guides iteration.\n\n**Common Rationalization:** \"Visualization is optional - the data speaks for itself\"\n**Reality:** Tabular data obscures patterns. Visualization reveals signal distribution, effect size clusters, and confidence patterns that inform strategic decisions. presenting-data handles this systematically.\n\n---\n\n## Phase 6: Iteration Planning\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 3-7 new experiment ideas based on synthesis findings\n- [ ] Categorized ideas (scale winners, investigate nulls, pivot from failures, explore new)\n- [ ] Documented campaign-level signal and strategic recommendation\n- [ ] Explained feed-forward pattern (ideas → new marketing-experimentation sessions)\n- [ ] Saved to `06-iteration-plan.md`\n- [ ] Campaign complete - ready for next cycle or conclusion\n\n### Instructions\n\n**CRITICAL:** Phase 6 generates experiment IDEAS, NOT hypotheses. Ideas feed into new marketing-experimentation sessions where Phase 2 formalizes hypotheses. Do NOT skip the discovery and hypothesis generation steps.\n\n1. **Generate 3-7 new experiment ideas**\n\n   Based on Phase 5 synthesis, generate ideas for next iteration:\n\n   **Idea Format:**\n\n   **Idea [N]: [Brief descriptive name]**\n   - **Rationale:** [Why this idea based on current findings]\n   - **Expected Learning:** [What we'll learn from testing this]\n   - **Category:** [Scale Winners | Investigate Nulls | Pivot from Failures | Explore New]\n\n   **Example Ideas:**\n\n   **Idea 1: Scale value prop landing page to paid ads**\n   - **Rationale:** E1 showed +18% conversion from simplified value prop. Apply winning message to ad creative.\n   - **Expected Learning:** Does simplified value prop improve ad CTR and cost-per-conversion?\n   - **Category:** Scale Winners\n\n   **Idea 2: Investigate email sequence timing sensitivity**\n   - **Rationale:** E3 showed negative result for email sequence, but timing may be a confound (sent during holidays).\n   - **Expected Learning:** Is the email sequence inherently weak, or was timing the issue?\n   - **Category:** Investigate Nulls\n\n   **Idea 3: Pivot from broad ad targeting to lookalike audiences**\n   - **Rationale:** E2 showed null result for ad targeting. Broad targeting may dilute signal. Pivot to lookalike audiences based on E1 converters.\n   - **Expected Learning:** Do lookalike audiences outperform broad targeting?\n   - **Category:** Pivot from Failures\n\n2. **Categorize ideas by strategy**\n\n   **Scale Winners:**\n   - Double down on successful tactics\n   - Apply winning patterns to new channels\n   - Increase budget/effort on validated approaches\n   - Examples: Winning landing page → ads, winning ad → email, winning message → content\n\n   **Investigate Nulls:**\n   - Redesign experiments with null/mixed results\n   - Address confounding factors identified in synthesis\n   - Increase statistical power (larger sample, longer duration)\n   - Examples: Retest with better timing, retest with clearer treatment, retest with focused audience\n\n   **Pivot from Failures:**\n   - Abandon unsuccessful approaches\n   - Try alternative tactics for same goal\n   - Apply learnings to avoid similar failures\n   - Examples: Broad targeting failed → try lookalike, generic message failed → try personalization\n\n   **Explore New:**\n   - Test entirely new tactics not in original hypothesis set\n   - Investigate new audience segments\n   - Explore new channels or platforms\n   - Examples: Add referral program, test new platform, try new content format\n\n3. **Document campaign-level signal and strategic recommendation**\n\n   **Campaign Summary:**\n   - **Overall Signal:** [Positive | Negative | Null | Mixed]\n   - **Key Wins:** [List successful tactics]\n   - **Key Learnings:** [List insights regardless of signal]\n   - **Strategic Recommendation:** [What to do next]\n\n   **Strategic Recommendations by Signal:**\n\n   **Positive Signal:**\n   - Proceed to scaling: Increase budget/effort on winning tactics\n   - Optimize: Refine winning approaches for incremental gains\n   - Expand: Apply winning patterns to new channels/audiences\n\n   **Negative Signal:**\n   - Pivot: Major change in approach, audience, or value proposition\n   - Pause: Reassess concept before additional investment\n   - Abandon: Consider alternative concepts if no viable path forward\n\n   **Null Signal:**\n   - Refine: Redesign experiments with better power/clarity\n   - Investigate: Address confounding factors before new tests\n   - Extend: Continue observation period if time-dependent\n\n   **Mixed Signal:**\n   - Selective scaling: Double down on winners\n   - Selective pivots: Abandon or redesign losers\n   - Strategic iteration: Focus next tests on promising areas\n\n4. **Explain feed-forward pattern**\n\n   **CRITICAL:** Ideas from Phase 6 are NOT ready for testing. They MUST go through a new marketing-experimentation session:\n\n   **Feed-Forward Cycle:**\n   ```\n   Phase 6 generates IDEAS\n          ↓\n   Start new marketing-experimentation session with idea\n          ↓\n   Phase 1: Discovery (validate idea with market-researcher)\n          ↓\n   Phase 2: Hypothesis Generation (formalize idea into testable hypotheses)\n          ↓\n   Phase 3-6: Complete full experimental cycle\n   ```\n\n   **Why this matters:**\n   - Ideas need market validation (Phase 1) before testing\n   - Ideas need formalization into specific hypotheses (Phase 2)\n   - Skipping discovery leads to untested assumptions\n   - Skipping hypothesis generation leads to vague experiments\n\n   **Example:**\n   Phase 6 generates \"Scale value prop to ads\" (idea)\n   → New session Phase 1: Market research on ad platform best practices\n   → New session Phase 2: Generate hypotheses like \"H1: Simplified value prop in ad headline increases CTR by 10%+\" (specific, testable)\n   → Continue with Phase 3-6 to test formal hypotheses\n\n5. **Create `06-iteration-plan.md`** with: `./templates/06-iteration-plan.md`\n\n6. **STOP and review with user**\n   - Review all iteration ideas with user\n   - Confirm strategic recommendation is appropriate\n   - Confirm understanding of feed-forward cycle\n   - Discuss whether to start new marketing-experimentation session or conclude campaign\n\n**Common Rationalization:** \"I'll turn ideas directly into experiments - skip the new session\"\n**Reality:** Ideas need discovery and hypothesis generation. Skipping these steps leads to untested assumptions and vague experiments. Always run ideas through a new marketing-experimentation session.\n\n**Common Rationalization:** \"I'll generate hypotheses in Phase 6 for efficiency\"\n**Reality:** Phase 6 generates IDEAS, Phase 2 (in a new session) generates hypotheses. Conflating these skips critical validation and formalization steps. Ideas → new session → hypotheses.\n\n**Common Rationalization:** \"Campaign signal is obvious from results, no need to document strategic recommendation\"\n**Reality:** Documented recommendation provides clear guidance for stakeholders and future sessions. Without it, insights are lost and decisions become ad-hoc.\n\n---\n\n## Common Rationalizations\n\nThese are rationalizations that lead to failure. When you catch yourself thinking any of these, STOP and follow the skill process instead.\n\n### \"I'll skip discovery and just start testing - the concept is obvious\"\n\n**Why this fails:** Discovery surfaces assumptions, constraints, and existing assets that dramatically affect experiment design. \"Obvious\" concepts often hide critical assumptions that need validation.\n\n**Reality:** Market-researcher agent provides current, data-driven validation signals. Asset inventory reveals resources that reduce experiment cost and time. Success criteria definition prevents ambiguous results. Always start with discovery.\n\n**What to do instead:** Complete Phase 1 (Discovery & Asset Inventory) before generating hypotheses. Invoke market-researcher agent. Document all findings.\n\n---\n\n### \"I'll design the experiment myself instead of using hypothesis-testing or qualitative-research\"\n\n**Why this fails:** The research skills provide rigorous experimental design, analysis, and signal detection. hypothesis-testing ensures statistical rigor for quantitative experiments. qualitative-research ensures systematic rigor for qualitative experiments. Skipping them produces weak experiments with ambiguous results.\n\n**Reality:** Marketing-experimentation is a meta-orchestrator that coordinates multiple experiments. It does NOT design experiments itself. Delegation to appropriate skills (hypothesis-testing or qualitative-research) ensures methodological rigor.\n\n**What to do instead:** Determine experiment type (quantitative or qualitative) in Phase 4. Invoke hypothesis-testing skill for quantitative experiments. Invoke qualitative-research skill for qualitative experiments. Let the appropriate skill handle all design, execution, and analysis.\n\n---\n\n### \"One experiment is enough to draw conclusions\"\n\n**Why this fails:** Single experiments miss cross-experiment patterns. Some tactics work, others don't. Single-experiment campaigns can't identify which channels/tactics are most effective.\n\n**Reality:** Marketing-experimentation tests 2-4 hypotheses to reveal strategic insights. Synthesis (Phase 5) identifies patterns across experiments - which tactics work, which don't, and why.\n\n**What to do instead:** Follow Phase 3 prioritization to select 2-4 hypotheses. Complete all experiments before synthesis. Use Phase 5 to identify patterns.\n\n---\n\n### \"I'll wait until all experiments complete before updating the tracker\"\n\n**Why this fails:** Batch updates create opportunity for lost data. Multi-day campaigns lose context between conversations. Incomplete tracker leads to missed experiments and confusion.\n\n**Reality:** The experiment tracker (04-experiment-tracker.md) is the ONLY source of truth that persists across sessions. Update it IMMEDIATELY after status changes.\n\n**What to do instead:** Update tracker after every status change (Planned → In Progress, In Progress → Complete). Commit tracker updates to git. Read tracker FIRST in every new conversation.\n\n---\n\n### \"Results are obvious, I don't need to document synthesis\"\n\n**Why this fails:** Individual experiment signals don't reveal cross-experiment patterns. \"Obvious\" interpretations miss confounding factors and alternative explanations.\n\n**Reality:** Documented synthesis with presenting-data skill ensures intellectual honesty. Visualization reveals patterns. Statistical assessment identifies robust vs uncertain findings.\n\n**What to do instead:** Always complete Phase 5 (Cross-Experiment Synthesis). Invoke presenting-data skill. Document patterns, visualizations, and signal classification. Get user confirmation.\n\n---\n\n### \"I'll form hypotheses in Phase 6 for efficiency\"\n\n**Why this fails:** Phase 6 generates IDEAS, not hypotheses. Ideas need discovery (Phase 1) and hypothesis generation (Phase 2) in new sessions. Skipping these steps leads to untested assumptions and vague experiments.\n\n**Reality:** Feed-forward cycle: Phase 6 ideas → new marketing-experimentation session → Phase 1 discovery → Phase 2 hypothesis generation → Phase 3-6 complete cycle.\n\n**What to do instead:** Generate IDEAS in Phase 6. Start NEW marketing-experimentation session with selected idea. Complete Phase 1 and Phase 2 to formalize idea into testable hypotheses.\n\n---\n\n### \"I'll estimate ICE/RICE scores mentally instead of running the script\"\n\n**Why this fails:** Manual estimation introduces calculation errors and inconsistency. Mental math is unreliable for multiplication and division.\n\n**Reality:** Python scripts ensure exact, reproducible results that can be audited and verified. Computational methods eliminate human error.\n\n**What to do instead:** Use Python scripts (ICE or RICE calculator) from Phase 3 instructions. Update hypothesis data in script. Run script and document exact scores. Copy output table to prioritization document.\n\n---\n\n### \"I'll synthesize results mentally - no need to use presenting-data\"\n\n**Why this fails:** Mental synthesis loses details and creates false confidence. Cross-experiment patterns require systematic analysis.\n\n**Reality:** presenting-data skill handles pattern identification (via interpreting-results), visualization creation (via creating-visualizations), and synthesis documentation. It ensures intellectual honesty and reproducibility.\n\n**What to do instead:** Always invoke presenting-data skill in Phase 5. Provide aggregate results table. Request pattern analysis and visualizations. Document all findings from presenting-data output.\n\n---\n\n## Summary\n\nThe marketing-experimentation skill ensures rigorous, evidence-based validation of marketing concepts through structured experimental cycles. This skill orchestrates the complete Build-Measure-Learn loop from concept to data-driven signal.\n\n**What this skill ensures:**\n\n1. **Validated concepts through market research** - market-researcher agent provides current demand signals, competitive landscape analysis, and audience insights before experimentation begins.\n\n2. **Strategic hypothesis generation** - 5-10 testable hypotheses spanning multiple tactics (landing pages, ads, email, content) grounded in discovery findings and mapped to experimentation frameworks (Lean Startup, AARRR).\n\n3. **Data-driven prioritization** - Computational methods (ICE/RICE Python scripts) ensure exact, reproducible scoring. Selection of 2-4 highest-value hypotheses optimizes resource allocation.\n\n4. **Multi-experiment coordination** - Experiment tracker (living document) enables multi-conversation workflows spanning days or weeks. Status tracking (Planned, In Progress, Complete) maintains visibility across all experiments. Supports both quantitative and qualitative experiment types.\n\n5. **Methodological rigor through delegation** - hypothesis-testing skill handles quantitative experiment design (statistical analysis, A/B tests, metrics). qualitative-research skill handles qualitative experiment design (interviews, surveys, focus groups, observations, thematic analysis). Marketing-experimentation coordinates multiple tests without duplicating methodology.\n\n6. **Cross-experiment synthesis** - presenting-data skill identifies patterns across experiments (what works, what doesn't, what's unclear). Aggregate analysis reveals strategic insights invisible in single experiments.\n\n7. **Clear signal generation** - Campaign-level classification (Positive/Negative/Null/Mixed) with strategic recommendations (Scale/Pivot/Refine/Pause) provides actionable guidance for stakeholders.\n\n8. **Systematic iteration** - Phase 6 generates experiment IDEAS (not hypotheses) that feed into new marketing-experimentation sessions. Feed-forward cycle maintains rigor through repeated discovery and hypothesis generation.\n\n9. **Multi-conversation persistence** - Complete documentation at every phase enables resumption after days or weeks. Experiment tracker serves as coordination hub. All artifacts are git-committable.\n\n10. **Tool-agnostic approach** - Focuses on techniques (value proposition testing, targeting strategies, sequence optimization) rather than specific platforms. Applicable across marketing tools and channels.\n\n**Key principles:**\n- Discovery before experimentation (Phase 1 always first)\n- Hypothesis generation separate from idea generation (Phase 2 vs Phase 6)\n- Multiple experiments for pattern identification (2-4 minimum)\n- Computational scoring for objectivity (Python scripts)\n- Delegation for methodological rigor (hypothesis-testing for quantitative, qualitative-research for qualitative)\n- Mixed-methods integration (quantitative metrics + qualitative insights for complete picture)\n- Synthesis for strategic insight (presenting-data skill handles both quantitative and qualitative results)\n- Documentation for reproducibility (numbered markdown files, git commits)\n- Iteration through validated cycles (ideas → new sessions → discovery → hypotheses)"
              },
              {
                "name": "presenting-data",
                "description": "Component skill for creating compelling data-driven presentations and whitepapers using marp and pandoc with proper citations and reproducibility",
                "path": "plugins/datapeeker/skills/presenting-data/SKILL.md",
                "frontmatter": {
                  "name": "presenting-data",
                  "description": "Component skill for creating compelling data-driven presentations and whitepapers using marp and pandoc with proper citations and reproducibility"
                },
                "content": "# Presenting Data\n\n## Purpose\n\nThis component skill guides creation of professional data-driven presentations and whitepapers. Use it when:\n- Communicating analysis findings to stakeholders\n- Creating executive summaries and detailed technical reports\n- Documenting reproducible research with proper citations\n- Building a presentation hierarchy: slides → whitepapers (drill-in capability)\n- Referenced by process skills for final deliverables\n\n**Supports two complementary formats:**\n- **Presentations (marp)** - Slide decks for meetings, pitches, and executive summaries\n- **Whitepapers (pandoc)** - Comprehensive documents with citations, cross-references, and academic formatting\n\n## Prerequisites\n\n- Analysis completed with clear findings\n- Query results documented and interpreted (use `interpreting-results` skill)\n- Visualizations prepared (use `creating-visualizations` skill)\n- Understanding of data sources, queries, and reproducibility requirements\n- Clear communication goal and target audience identified\n\n## Data Presentation Process\n\nCreate a TodoWrite checklist for the 5-phase presentation process:\n\n```\nPhase 1: Analyze Audience & Purpose - pending\nPhase 2: Structure Narrative - pending\nPhase 3: Create Content - pending\nPhase 4: Add Citations & Reproducibility - pending\nPhase 5: Generate Outputs - pending\n```\n\nMark each phase as you complete it. Document all presentation materials in your analysis directory.\n\n---\n\n## Phase 1: Analyze Audience & Purpose\n\n**Goal:** Understand who will consume your presentation and what decisions they need to make.\n\n### Identify Your Audience\n\n**Executive Stakeholders:**\n- Format: Slide presentation (5-10 slides)\n- Focus: Key findings, business impact, recommendations\n- Detail Level: High-level metrics, visual emphasis\n- Tool: **marp** for quick, visual presentations\n\n**Technical Peers:**\n- Format: Whitepaper or technical report (20-50 pages)\n- Focus: Methodology, reproducibility, detailed analysis\n- Detail Level: SQL queries, statistical methods, data quality notes\n- Tool: **pandoc** for comprehensive documentation\n\n**Mixed Audience:**\n- Format: Both (presentation + supporting whitepaper)\n- Focus: Slides for overview, whitepaper for drill-in details\n- Detail Level: Presentation hierarchy allowing progressive disclosure\n- Tools: **marp** for slides, **pandoc** for backing documents\n\n### Define Communication Goals\n\n**What decisions will this presentation support?**\n- Strategic planning (high-level trends and forecasts)\n- Operational changes (specific process improvements)\n- Technical validation (methodology and reproducibility)\n- Policy changes (compliance, risk, standards)\n\n**What actions should the audience take?**\n- Approve/reject a proposal\n- Allocate budget or resources\n- Change operational procedures\n- Investigate further (drill into details)\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Target audience identified (executive, technical, or mixed)\n- [ ] Primary communication goal defined\n- [ ] Desired audience action articulated\n- [ ] Output format selected (presentation, whitepaper, or both)\n\n---\n\n## Phase 2: Structure Narrative\n\n**Goal:** Organize findings into a compelling narrative that guides the audience to your conclusions.\n\n### Use the 3-Paragraph Essay Structure\n\n**→ See [frameworks/3-paragraph-essay.md](./frameworks/3-paragraph-essay.md) for detailed guidance**\n\nThe classic essay structure adapts perfectly to data presentations:\n\n1. **Introduction & Thesis**\n   - State the question or problem\n   - Present your key finding or recommendation\n   - Preview supporting evidence\n\n2. **Body & Supporting Arguments**\n   - Present data findings that support your thesis\n   - Use visualizations to make patterns clear\n   - Address alternative explanations\n   - Cite data sources and methodology\n\n3. **Conclusion & Next Steps**\n   - Restate key findings\n   - Articulate implications and recommendations\n   - Identify limitations and follow-up questions\n\n4. **Bibliography & Supporting Documentation**\n   - Data sources and SQL queries\n   - Reproducibility information (versions, timestamps)\n   - References to prior research or methodology\n   - Links to detailed whitepapers or repositories\n\n### Apply Narrative Structure for Data Stories\n\n**→ See [frameworks/narrative-structure.md](./frameworks/narrative-structure.md) for storytelling patterns**\n\nData presentations follow narrative arcs:\n- **Setup**: Establish business context and question\n- **Conflict**: Present the data challenge or pattern\n- **Resolution**: Show findings and recommendations\n- **Call to Action**: Define next steps\n\n### Outline Your Presentation\n\nCreate outline document: `analysis/[session-name]/presentation-outline.md`\n\n**For Presentations (Marp):**\n```markdown\n# Presentation Outline\n\n## Slide 1: Title & Context\n- Analysis question\n- Time period and data sources\n\n## Slide 2-3: Key Findings (Thesis)\n- 3-5 bullet points with metrics\n- Visual emphasis\n\n## Slide 4-7: Supporting Evidence (Body)\n- One finding per slide\n- Include visualizations\n- Reference methodology\n\n## Slide 8: Conclusions & Recommendations\n- Restate key findings\n- Next steps\n- Questions\n\n## Slide 9: Reproducibility Notes (Appendix)\n- Data sources\n- Query locations\n- Validation status\n```\n\n**For Whitepapers (Pandoc):**\n```markdown\n# Whitepaper Outline\n\n## Introduction (2-3 pages)\n- Business context and objectives\n- Research question\n- Thesis statement\n\n## Methodology (5-10 pages)\n- Data sources and collection methods\n- SQL queries and transformations\n- Analysis frameworks used\n- Data quality assessment\n\n## Results (10-20 pages)\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n- Visualizations and tables\n\n## Discussion (5-10 pages)\n- Interpretation of findings\n- Comparison to prior research\n- Limitations and caveats\n- Alternative explanations\n\n## Conclusions (2-3 pages)\n- Summary of key findings\n- Recommendations\n- Future research directions\n\n## References\n- Bibliography (BibTeX format)\n- Appendix: SQL queries\n- Appendix: Data validation notes\n```\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Narrative structure selected (essay-based or story-based)\n- [ ] Presentation outline created with sections identified\n- [ ] Key findings and supporting evidence mapped\n- [ ] Introduction, body, conclusion, and bibliography sections defined\n\n---\n\n## Phase 3: Create Content\n\n**Goal:** Write presentation content using appropriate tools (marp for slides, pandoc for documents).\n\n### Creating Slide Presentations with Marp\n\n**→ See [tools/marp.md](./tools/marp.md) for detailed CLI usage and syntax**\n\nMarp transforms Markdown into professional slide decks. Use for:\n- Executive summaries (5-10 slides)\n- Meeting presentations\n- Quick stakeholder updates\n\n**Basic Marp Syntax:**\n```markdown\n---\ntheme: gaia\npaginate: true\nfooter: \"DataPeeker Analysis\"\n---\n\n# Q4 Sales Analysis\n## Key Findings\n\n---\n\n## Data Source\n- Database: `analytics_prod.sales_metrics`\n- Query Date: 2025-11-25\n- Period: 2024-10-01 to 2024-12-31\n\n---\n\n## Finding 1: Revenue Growth\n\n- Total Revenue: $1.2M\n- YoY Growth: +23%\n- Top Region: West Coast\n\n![width:600px](revenue-chart.png)\n\n---\n\n## Methodology\n\n\\```sql\nSELECT\n  DATE_TRUNC('month', date) as month,\n  SUM(amount) as revenue\nFROM sales_metrics\nWHERE date BETWEEN '2024-10-01' AND '2024-12-31'\nGROUP BY month\n\\```\n\n---\n\n## Conclusions\n\n- Revenue exceeded target by 15%\n- West Coast expansion successful\n- Recommend continued investment\n\n---\n\n## Reproducibility\n\n- Queries: `queries/q4_analysis.sql`\n- Data validated: 2025-11-25\n- Full report: [Technical Whitepaper](./whitepaper.pdf)\n```\n\n**Generate Presentation:**\n```bash\nmarp presentation.md -o presentation.pdf\n```\n\n### Creating Whitepapers with Pandoc\n\n**→ See [tools/pandoc.md](./tools/pandoc.md) for detailed CLI usage and citations**\n\nPandoc creates publication-quality documents. Use for:\n- Technical reports (20-50 pages)\n- Comprehensive analysis documentation\n- Academic papers with citations and cross-references\n\n**Basic Pandoc Structure:**\n```yaml\n---\ntitle: \"Q4 Sales Performance Analysis\"\nauthor: \"DataPeeker Team\"\ndate: \"2025-11-25\"\ninstitute: \"Tilmon Engineering\"\nabstract: |\n  This analysis examines Q4 2024 sales data, revealing 23% YoY growth\n  driven primarily by West Coast expansion. Methodology, findings, and\n  recommendations are presented with full reproducibility documentation.\nkeywords: \"sales analysis, SQL, data analysis, reproducibility\"\ntoc: true\nlof: true\nlot: true\n---\n\n# Introduction\n\nThis analysis addresses the question: What factors drove Q4 2024\nsales performance? Using DataPeeker to analyze production database\nrecords, we identify key growth drivers and provide actionable\nrecommendations.\n\n# Methodology\n\n## Data Collection\n\nData was extracted from `analytics_prod.sales_metrics` using:\n\n\\```sql\nSELECT\n  transaction_id,\n  customer_id,\n  region,\n  amount,\n  transaction_date\nFROM sales_metrics\nWHERE DATE(transaction_date) BETWEEN '2024-10-01' AND '2024-12-31'\nORDER BY transaction_date\n\\```\n\n## Analysis Framework\n\nAnalysis followed established data quality frameworks [@jones2024]\nand reproducibility standards [@smith2023].\n\n# Results\n\n![Q4 Revenue by Region](revenue-by-region.png){#fig:revenue}\n\nFigure @fig:revenue demonstrates clear regional patterns.\n\n# Discussion\n\nOur findings align with previous research on seasonal trends\n[@williams2024] while revealing new patterns in customer behavior.\n\n# Conclusions\n\nThree key findings emerge from this analysis:\n1. West Coast growth exceeded projections\n2. Customer acquisition accelerated in Q4\n3. Average transaction value increased 12%\n\n# References\n```\n\n**Generate Whitepaper:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s -V geometry:margin=1in \\\n  --toc \\\n  --number-sections \\\n  -o whitepaper.pdf\n```\n\n### Integrating Visualizations\n\n**Use `creating-visualizations` component skill** to create charts and diagrams:\n\n**Terminal visualizations:**\n- Use `creating-visualizations` terminal formats for inline code examples\n- Include ASCII charts in whitepaper appendices\n- Show sparklines for trend indicators\n\n**Image-based visualizations:**\n- Use `creating-visualizations` image formats (Kroki) for slides and whitepapers\n- Generate Mermaid flowcharts for methodology sections\n- Create GraphViz diagrams for data lineage\n- Use Vega-Lite for statistical charts\n\n**Best Practices:**\n- Export visualizations as PNG/SVG for marp presentations\n- Reference figure numbers in pandoc documents\n- Include chart source data or generation code\n- Document visualization choices in methodology\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Presentation/whitepaper content drafted in Markdown\n- [ ] Visualizations created and embedded\n- [ ] SQL queries and code snippets included\n- [ ] Narrative structure followed (introduction, body, conclusion)\n\n---\n\n## Phase 4: Add Citations & Reproducibility\n\n**Goal:** Document data sources, queries, and methodology to enable reproducibility and proper attribution.\n\n### Citing Data Sources and Queries\n\n**→ See [formats/citations.md](./formats/citations.md) for BibTeX and CSL formats**\n\nProper citation enables:\n- Traceability to original data sources\n- Validation of methodology\n- Reproducibility by others\n- Academic and professional credibility\n\n**Citation Types for Data Analysis:**\n1. **Data Sources** - Databases, APIs, file systems\n2. **SQL Queries** - Specific queries used in analysis\n3. **Analysis Tools** - Software and versions (DataPeeker, Python, R)\n4. **Prior Research** - Published papers or internal reports\n5. **Methodology References** - Statistical methods or frameworks\n\n**Example BibTeX Entries:**\n```bibtex\n@misc{production_database_2025,\n  author = {Tilmon Engineering},\n  title = {Production Sales Metrics Database},\n  year = {2025},\n  url = {analytics_prod.sales_metrics},\n  note = {Query timestamp: 2025-11-25 14:30 UTC}\n}\n\n@software{datapeeker_2025,\n  author = {Tilmon Engineering},\n  title = {DataPeeker: SQL Analysis Tool},\n  year = {2025},\n  version = {2.1.0},\n  url = {https://github.com/tilmon/datapeeker}\n}\n```\n\n### Documenting Reproducibility\n\n**→ See [formats/reproducibility.md](./formats/reproducibility.md) for comprehensive checklist**\n\nReproducible research requires documentation of:\n- **Data**: Source, timestamp, version, schema\n- **Queries**: Full SQL text, execution time, row counts\n- **Environment**: Tool versions, dependencies, configuration\n- **Process**: Step-by-step methodology\n- **Validation**: Data quality checks, cross-validation results\n\n**Reproducibility Section Template:**\n```markdown\n## Reproducibility Information\n\n### Data Sources\n- **Database**: `analytics_prod.sales_metrics`\n- **Schema Version**: v2.3.1\n- **Query Timestamp**: 2025-11-25 14:30:00 UTC\n- **Records Examined**: 50,000 transactions\n- **Time Period**: 2024-10-01 to 2024-12-31\n\n### Analysis Environment\n- **Tool**: DataPeeker v2.1.0\n- **Python Version**: 3.11.5\n- **Key Libraries**: pandas 2.1.0, plotext 5.2.8\n- **Operating System**: macOS 14.6.0\n\n### Query Repository\n- **Location**: `github.com/tilmon/analysis/queries/q4_sales.sql`\n- **Commit Hash**: abc123def456\n- **Execution Time**: 3.2 seconds\n- **Rows Returned**: 50,000\n\n### Data Quality Validation\n- **Null Values**: 0.02% (within tolerance)\n- **Duplicates**: 0 detected\n- **Outliers**: 12 identified and documented separately\n- **Cross-Validation**: Results match source system aggregate queries\n\n### Reproducibility Instructions\n1. Clone repository: `git clone github.com/tilmon/analysis`\n2. Install dependencies: `pip install -r requirements.txt`\n3. Run analysis: `python scripts/q4_analysis.py`\n4. View results: `analysis/q4-2024/01-findings.md`\n```\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] BibTeX bibliography created with data sources\n- [ ] SQL queries documented with execution details\n- [ ] Reproducibility section added with environment info\n- [ ] Citations inserted in text using [@citation_key] syntax\n\n---\n\n## Phase 5: Generate Outputs\n\n**Goal:** Compile final presentation and whitepaper artifacts using marp and pandoc.\n\n### Generate Slide Presentation (Marp)\n\n**Basic PDF Output:**\n```bash\nmarp presentation.md -o presentation.pdf\n```\n\n**HTML with Speaker Notes:**\n```bash\nmarp presentation.md -o presentation.html\n```\n\n**PowerPoint (Editable):**\n```bash\nmarp presentation.md --pptx -o presentation.pptx\n```\n\n**With Custom Theme:**\n```bash\nmarp presentation.md --theme-set custom-theme.css -o presentation.pdf\n```\n\n**Watch Mode (Live Preview):**\n```bash\nmarp -w -p presentation.md\n```\n\n### Generate Whitepaper (Pandoc)\n\n**PDF with Citations:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -V geometry:margin=1in \\\n  -o whitepaper.pdf\n```\n\n**PDF with Cross-References:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -o whitepaper.pdf\n```\n\n**Word Document (Editable):**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --reference-doc=template.docx \\\n  -o whitepaper.docx\n```\n\n**HTML for Web Publishing:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s -c style.css \\\n  --self-contained \\\n  -o whitepaper.html\n```\n\n### Presentation Hierarchy (Slides → Whitepapers)\n\n**Link from slides to detailed documentation:**\n```markdown\n# Conclusion\n\nFor detailed methodology and reproducibility:\n→ [Technical Whitepaper](./whitepaper.pdf)\n→ [GitHub Repository](https://github.com/tilmon/analysis)\n```\n\n**File Organization:**\n```\nanalysis/\n├── q4-2024/\n│   ├── presentation.md (marp source)\n│   ├── presentation.pdf (generated)\n│   ├── whitepaper.md (pandoc source)\n│   ├── whitepaper.pdf (generated)\n│   ├── references.bib (bibliography)\n│   ├── queries/\n│   │   └── q4_analysis.sql\n│   └── visualizations/\n│       ├── revenue-chart.png\n│       └── regional-breakdown.png\n```\n\n**Automation Script:**\n```bash\n#!/bin/bash\n# generate_deliverables.sh\n\n# Generate presentation\necho \"Creating presentation...\"\nmarp presentation.md -o presentation.pdf\n\n# Generate whitepaper\necho \"Creating whitepaper...\"\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -V geometry:margin=1in \\\n  -o whitepaper.pdf\n\necho \"Deliverables created:\"\necho \"  - presentation.pdf (slides)\"\necho \"  - whitepaper.pdf (detailed report)\"\n```\n\n**CHECKPOINT:** Final verification before delivery:\n- [ ] Presentation PDF generated and reviewed\n- [ ] Whitepaper PDF generated with citations and cross-references\n- [ ] All visualizations properly embedded and sized\n- [ ] Bibliography and references correctly formatted\n- [ ] Reproducibility information complete\n- [ ] Links between presentation and whitepaper working\n- [ ] Files organized in analysis directory structure\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill when presenting findings:\n\n```markdown\nUse the `presenting-data` component skill to create professional\ndeliverables from your analysis:\n- Executive presentations (marp) for quick communication\n- Technical whitepapers (pandoc) for detailed documentation\n- Both formats with proper citations and reproducibility\n```\n\nWhen presenting analysis results:\n1. **Choose format** based on audience (Phase 1)\n2. **Structure narrative** using essay or story patterns (Phase 2)\n3. **Create content** with marp/pandoc (Phase 3)\n4. **Add citations** and reproducibility documentation (Phase 4)\n5. **Generate outputs** for delivery (Phase 5)\n\n**Typical Usage Contexts:**\n- `exploratory-analysis` → Use presenting-data for final report\n- `guided-investigation` → Use presenting-data to document findings\n- `hypothesis-testing` → Use presenting-data to publish results\n- `comparative-analysis` → Use presenting-data for comparison reports\n\n---\n\n## Quality Checklist\n\nBefore considering presentation complete:\n\n**Content Quality:**\n- [ ] Clear thesis or key finding stated upfront\n- [ ] Supporting evidence logically organized\n- [ ] Visualizations effectively communicate patterns\n- [ ] Conclusions tied directly to evidence\n- [ ] Limitations and caveats acknowledged\n\n**Reproducibility:**\n- [ ] Data sources fully documented\n- [ ] SQL queries included or referenced\n- [ ] Tool versions and environment documented\n- [ ] Execution timestamps recorded\n- [ ] Reproducibility instructions provided\n\n**Citation Quality:**\n- [ ] All data sources cited in bibliography\n- [ ] Prior research properly attributed\n- [ ] Analysis tools and versions documented\n- [ ] Citation style consistent throughout\n- [ ] Links to repositories and queries working\n\n**Technical Quality:**\n- [ ] Presentation PDF renders correctly\n- [ ] Whitepaper PDF has working cross-references\n- [ ] Images embedded and properly sized\n- [ ] Code blocks have syntax highlighting\n- [ ] Math equations render correctly (if applicable)\n\n**Narrative Quality:**\n- [ ] Introduction establishes context and question\n- [ ] Body presents evidence systematically\n- [ ] Conclusion restates findings clearly\n- [ ] Bibliography enables drill-in for details\n- [ ] Overall flow guides audience to conclusions\n\n**Presentation-Whitepaper Hierarchy:**\n- [ ] Slides provide high-level overview\n- [ ] Whitepaper provides comprehensive details\n- [ ] Links between formats working\n- [ ] Consistent terminology and metrics\n- [ ] Both reference same data sources\n\n---\n\n## Best Practices\n\n### DO:\n✅ Start with audience analysis to choose format\n✅ Use 3-paragraph essay structure for clear narrative\n✅ Include SQL queries for reproducibility\n✅ Cite data sources in bibliography\n✅ Create both slides and whitepaper for mixed audiences\n✅ Link from presentations to detailed whitepapers\n✅ Document environment, versions, and timestamps\n✅ Use `creating-visualizations` for charts and diagrams\n✅ Version control both source markdown and generated PDFs\n✅ Test reproducibility instructions before delivery\n\n### DON'T:\n❌ Skip audience analysis - format should match need\n❌ Present findings without methodology documentation\n❌ Forget to cite data sources and prior research\n❌ Generate PDFs without reviewing output quality\n❌ Mix presentation styles (stay consistent)\n❌ Overcomplicate slides with too much detail\n❌ Omit reproducibility information\n❌ Assume audience can recreate analysis without documentation\n❌ Ignore cross-references and internal links\n❌ Deliver without validating bibliography formatting"
              },
              {
                "name": "qualitative-research",
                "description": "Use when conducting customer discovery interviews, user research, surveys, focus groups, or observational research requiring rigorous analysis - provides systematic 6-phase framework with mandatory bias prevention (reflexivity, intercoder reliability, disconfirming evidence search) and reproducible methodology; peer to hypothesis-testing for qualitative vs quantitative validation",
                "path": "plugins/datapeeker/skills/qualitative-research/SKILL.md",
                "frontmatter": {
                  "name": "qualitative-research",
                  "description": "Use when conducting customer discovery interviews, user research, surveys, focus groups, or observational research requiring rigorous analysis - provides systematic 6-phase framework with mandatory bias prevention (reflexivity, intercoder reliability, disconfirming evidence search) and reproducible methodology; peer to hypothesis-testing for qualitative vs quantitative validation"
                },
                "content": "# Qualitative Research\n\n## Overview\n\nSystematic framework for conducting and analyzing qualitative research (interviews, surveys, focus groups, observations) with rigorous bias prevention and reproducible methodology.\n\n**Core principle:** Rigor through mandatory checkpoints. Prevent confirmation bias by enforcing disconfirming evidence search, intercoder reliability, and reflexivity documentation.\n\n**Peer to hypothesis-testing:** hypothesis-testing validates quantitative hypotheses with data analysis. qualitative-research validates qualitative hypotheses with systematic interview/survey analysis.\n\n## When to Use\n\nUse this skill when:\n- Conducting customer discovery interviews to validate demand\n- Running user research to understand pain points\n- Analyzing survey responses for themes and patterns\n- Conducting focus groups or observational research\n- ANY qualitative data collection and analysis requiring rigorous, reproducible methodology\n\nWhen NOT to use:\n- Quantitative data analysis (use hypothesis-testing instead)\n- Casual conversations or informal feedback (not systematic research)\n- Literature review or secondary research (use internet-researcher agent)\n\n## Mandatory Process Structure\n\n**YOU MUST use TodoWrite to track progress through all 6 phases.**\n\nCreate todos at the start:\n\n```markdown\n- Phase 1: Research Design (question, method, instrument, biases) - pending\n- Phase 2: Data Collection (execute protocol, track saturation) - pending\n- Phase 3: Data Familiarization (immerse without coding) - pending\n- Phase 4: Systematic Coding (codebook, reliability check) - pending\n- Phase 5: Theme Development (build themes, search disconfirming evidence) - pending\n- Phase 6: Synthesis & Reporting (findings, limitations, follow-ups) - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n**Flexible Entry:** If user has existing data (transcripts, survey responses), can start at Phase 3. Verify raw data exists in `raw-data/` directory.\n\n---\n\n## Phase 1: Research Design\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Research question defined (specific, testable)\n- [ ] Qualitative method selected (interview/survey/focus group/observation)\n- [ ] Collection instrument created (interview guide, survey questions, protocol)\n- [ ] Sampling strategy documented (who, how many, recruitment)\n- [ ] **Reflexivity baseline documented** (YOUR assumptions and biases written down)\n- [ ] Saved to `01-research-design.md`\n\n### Instructions\n\n1. **Select method and load appropriate template:**\n   - Interview → Use `templates/interviews/phase-1-interview-guide.md`\n   - Survey → Use `templates/surveys/phase-1-survey-design.md`\n   - Focus Group → Use `templates/focus-groups/phase-1-facilitator-guide.md`\n   - Observation → Use `templates/observations/phase-1-observation-protocol.md`\n\n2. **Document reflexivity baseline (MANDATORY):**\n\n**This is NON-NEGOTIABLE.** Before any data collection, write down:\n- What you believe the answer will be\n- What assumptions you're making\n- What biases you bring (industry experience, expert opinions, prior hypotheses)\n- What would surprise you\n\n**Why this matters:** If you don't document biases BEFORE data collection, you cannot identify confirmation bias AFTER.\n\n3. **Create neutral questions (use template guidance):**\n\nTemplates enforce neutral question design. Common mistakes:\n- Leading: \"How much would you pay for X?\" (assumes they want X)\n- Neutral: \"How do you currently solve Y problem?\" (explores actual behavior)\n\n4. **Plan adequate sample size:**\n- Interviews: Minimum 8-10 for saturation monitoring\n- Surveys: Depends on question type and analysis goals\n- Focus groups: 3-5 groups minimum\n- Observations: Plan for 10-20 observation sessions\n\n5. **Save to `01-research-design.md` using template**\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 2 until reflexivity baseline documented.\n\n### Common Rationalization: \"I don't have biases to document\"\n\n**Why this is wrong:** Everyone has assumptions. If you can't name them, they're controlling you invisibly.\n\n**Do instead:** Write one sentence: \"I believe [X] because [Y].\" That's your bias. Document it.\n\n### Common Rationalization: \"Expert opinion reduces need for bias documentation\"\n\n**Why this is wrong:** Expert opinion IS a bias that must be documented. Authority backing is a strong prior.\n\n**Do instead:** \"Expert A said B. This is my assumption going in. Must verify with data.\"\n\n### Common Rationalization: \"Time pressure means I can't do formal process\"\n\n**Why this is wrong:** Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours.\n\n**Do instead:** Set timer for 5 minutes. Write down assumptions. Move on.\n\n---\n\n## Phase 2: Data Collection\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Minimum sample collected (Phase 1 plan executed)\n- [ ] Saturation monitoring documented\n- [ ] All raw data captured (transcripts, responses, field notes)\n- [ ] Raw data files in `raw-data/` directory\n- [ ] Reflexive journal maintained during collection\n- [ ] Saved to `02-data-collection-log.md`\n\n### Instructions\n\n1. **Execute method-specific protocol:**\n   - Use Phase 2 template for your selected method\n   - Maintain consistency (same questions, same facilitator when possible)\n   - Document context for each data collection instance\n\n2. **Track toward saturation:**\n\n**Saturation = when new insights stop emerging**\n\nAfter each interview/session/survey batch, ask:\n- Did this reveal new themes I hadn't seen?\n- Or was this reinforcing existing patterns?\n\nDocument in collection log. Plan to continue until 2-3 consecutive instances add nothing new.\n\n3. **Maintain reflexive journal (MANDATORY):**\n\nAfter each data collection instance, write:\n- What surprised you\n- What confirmed your assumptions\n- What contradicted your expectations\n- How your thinking is evolving\n\n**Why this matters:** Reflexivity tracks how your interpretation changes. Prevents retroactively fitting data to initial beliefs.\n\n4. **Create raw data files:**\n\n**File structure:**\n```\nraw-data/\n├── transcript-001.md\n├── transcript-002.md\n├── ...\n```\n\nOR for surveys:\n```\nraw-data/\n├── survey-responses-batch-1.md\n├── survey-responses-batch-2.md\n```\n\nOne file per interview/session. Numbered sequentially.\n\n5. **Save collection log to `02-data-collection-log.md`**\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 3 until minimum sample collected and raw data captured.\n\n---\n\n## Phase 3: Data Familiarization\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] All raw data read/reviewed multiple times\n- [ ] Initial observations documented (NOT codes, just observations)\n- [ ] Surprising findings noted (contradictions to assumptions)\n- [ ] Reflexivity updated (how understanding evolved)\n- [ ] Saved to `03-familiarization-notes.md`\n\n### Instructions\n\n1. **Read ALL data without coding:**\n\n**This is critical:** Do NOT start coding yet. Just read and observe.\n\n**Why:** Premature coding locks you into first impressions. Familiarization lets patterns emerge naturally.\n\n2. **For large datasets (10+ interviews), use analyze-transcript agent:**\n\n```\nInvoke: analyze-transcript agent\nInput: transcript-001.md through transcript-010.md\nOutput: Summary, key quotes, initial observations per transcript\n```\n\nAgent prevents context pollution. Returns structured observations for your review.\n\n3. **Document observations in `03-familiarization-notes.md`:**\n\n**Format:**\n- Initial patterns noticed (not themes yet - just \"I see X coming up\")\n- Surprising findings (\"I expected A but saw B\")\n- Questions emerging (\"Why did 3 people mention Y?\")\n- Reflexive notes (\"This contradicts my assumption that...\")\n\n4. **STOP and verify checkpoint:** Cannot proceed to Phase 4 until all data reviewed and surprises documented.\n\n### Common Rationalization: \"I can code while familiarizing to save time\"\n\n**Why this is wrong:** Coding while familiarizing locks you into first impressions. Patterns shift after full dataset review.\n\n**Do instead:** Finish familiarization completely. Then start fresh with coding.\n\n---\n\n## Phase 4: Systematic Coding\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] Codebook complete (definitions, inclusion/exclusion criteria, examples)\n- [ ] Entire dataset coded systematically\n- [ ] **Intercoder reliability check completed** (10-20% sample)\n- [ ] Agreement percentage documented\n- [ ] Audit trail of all coding decisions\n- [ ] Saved to `04-coding-analysis.md`\n\n### Instructions\n\n1. **Develop initial codebook using agent:**\n\n```\nInvoke: generate-initial-codes agent\nInput: 2-3 transcripts or data segments\nOutput: Suggested codes with definitions and examples\n```\n\nReview agent suggestions. Refine codes. Create codebook.\n\n2. **Codebook structure (MANDATORY):**\n\nFor each code:\n- **Name:** Short label\n- **Definition:** What this code means\n- **Inclusion criteria:** When to apply this code\n- **Exclusion criteria:** When NOT to apply\n- **Examples:** 2-3 data extracts demonstrating code\n\n3. **Code all data systematically:**\n\nWork through raw data files sequentially. Apply codes from codebook. Document any new codes discovered (add to codebook with rationale).\n\n4. **Intercoder reliability check (MANDATORY - NON-NEGOTIABLE):**\n\n```\nInvoke: intercoder-reliability-check agent\nInput: Codebook + 2 transcripts (10-20% of dataset)\nOutput: Independent coding + agreement analysis\n```\n\n**This step is REQUIRED.** Cannot skip. Cannot defer. Cannot substitute with user review.\n\n**Why:** Even clear codebooks have subjective judgment. Second coder catches systematic bias in code application.\n\n5. **Document in `04-coding-analysis.md`:**\n\n**Sections:**\n- Section 1: Codebook (all codes with definitions and examples)\n- Section 2: Coding Process (how you applied codes, any refinements)\n- Section 3: Intercoder Reliability (agent results, agreement %, disagreement resolution)\n- Section 4: Audit Trail (all coding decisions documented)\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 5 without intercoder reliability check COMPLETED and documented.\n\n### Common Rationalization: \"Coding was straightforward, low risk of errors\"\n\n**Why this is wrong:** \"Straightforward\" is subjective. Even clear codes have interpretation variance.\n\n**Do instead:** If coding is straightforward, intercoder reliability will be high and quick. Do the check.\n\n### Common Rationalization: \"Time constraints justify skipping verification\"\n\n**Why this is wrong:** Presenting flawed findings takes more time to fix than 1-hour verification.\n\n**Do instead:** Verification takes 1 hour. Fixing flawed findings after presentation takes days. Do the math.\n\n### Common Rationalization: \"User reviewed coding, that's enough validation\"\n\n**Why this is wrong:** User can't catch their own interpretation bias. Second coder does.\n\n**Do instead:** User review is pre-flight check. Intercoder reliability is the actual test. Both required.\n\n### Common Rationalization: \"Can do reliability check later if needed\"\n\n**Why this is wrong:** After themes developed, reliability check invalidates hours of work if problems found.\n\n**Do instead:** Reliability MUST be verified in Phase 4, not Phase 6. Do it now.\n\n---\n\n## Phase 5: Theme Development & Refinement\n\n**CHECKPOINT:** Before proceeding to Phase 6, you MUST have:\n- [ ] Themes defined with supporting codes\n- [ ] **Disconfirming evidence search completed** (MANDATORY for ALL themes)\n- [ ] Negative cases explained (data that doesn't fit themes)\n- [ ] Themes refined based on full dataset review\n- [ ] Verbatim data extracts supporting each theme\n- [ ] Saved to `05-theme-development.md`\n\n### Instructions\n\n1. **Group codes into potential themes using agent:**\n\n```\nInvoke: identify-themes agent\nInput: Codebook + all coded segments\nOutput: Potential themes with supporting codes and data extracts\n```\n\nReview agent suggestions. Refine theme definitions.\n\n2. **Disconfirming evidence search (MANDATORY - NON-NEGOTIABLE):**\n\n**For EACH theme, you MUST run:**\n\n```\nInvoke: search-disconfirming-evidence agent\nInput: Theme definition + full dataset\nOutput: Contradictory evidence, edge cases, exceptions to pattern\n```\n\n**This is REQUIRED.** No exceptions. No shortcuts. No \"pattern is obvious so no need.\"\n\n**Why:** Clear patterns are MOST vulnerable to confirmation bias. Obvious themes need MOST rigorous verification.\n\n3. **Document negative cases:**\n\nFor each theme, explain:\n- How many participants DON'T fit this theme?\n- What did those participants say instead?\n- Why doesn't the theme apply to them?\n- Is there a boundary condition (theme applies only in specific contexts)?\n\n**Example:**\n```\nTheme 1: \"Cost concerns are primary barrier\" - 8 of 10 participants\n\nNEGATIVE CASES:\n- Participant 3: Didn't mention cost. Focused entirely on integration complexity.\n- Participant 7: Said price was \"not a concern if it solves the problem\"\n\nEXPLANATION: Theme applies to majority but not universal. Subset willing to pay premium for right solution.\n```\n\n4. **Refine themes based on disconfirming evidence:**\n\nAfter seeing contradictions, revise theme definitions for accuracy. \"8 of 10\" is more honest than \"all participants.\"\n\n5. **Extract supporting quotes using agent:**\n\n```\nInvoke: extract-supporting-quotes agent\nInput: Theme definition + coded dataset\nOutput: Best representative verbatim quotes for each theme\n```\n\n6. **Document in `05-theme-development.md`:**\n\n**Format:**\n- Theme name and definition\n- Supporting codes\n- Prevalence (X of Y participants)\n- Verbatim quotes (use extract-supporting-quotes agent output)\n- Disconfirming evidence (from search-disconfirming-evidence agent)\n- Negative case explanation\n\n7. **STOP and verify checkpoint:** Cannot proceed to Phase 6 without disconfirming evidence search for ALL themes.\n\n### Common Rationalization: \"Themes are clearly supported by majority of participants\"\n\n**Why this is wrong:** Majority agreement doesn't eliminate contradictory evidence. Must explain ALL data.\n\n**Do instead:** \"8 of 10 mentioned cost. What about the 2 who didn't? Must explain.\"\n\n### Common Rationalization: \"Expert prediction validates findings\"\n\n**Why this is wrong:** Expert prediction + matching findings = confirmation bias red flag, not validation.\n\n**Do instead:** When predictions match findings perfectly, search HARDEST for contradictions.\n\n### Common Rationalization: \"High consistency (8/10, 9/10) indicates robust themes\"\n\n**Why this is wrong:** High unanimity can indicate leading questions or selective interpretation.\n\n**Do instead:** Real customer sentiment is messy. 9/10 agreement deserves scrutiny, not celebration.\n\n### Common Rationalization: \"Disconfirming evidence search unnecessary when pattern is obvious\"\n\n**Why this is wrong:** Obvious patterns are MOST vulnerable to confirmation bias.\n\n**Do instead:** Obvious patterns require MOST rigorous disconfirmation. Search is mandatory.\n\n---\n\n## Phase 6: Synthesis & Reporting\n\n**CHECKPOINT:** Before marking complete, you MUST have:\n- [ ] Findings documented with verbatim quotes for each theme\n- [ ] **Limitations explicitly stated** (sample, method, researcher bias, context)\n- [ ] Confidence assessment (credibility, dependability, confirmability, transferability)\n- [ ] 2-3 follow-up research questions identified\n- [ ] Overview updated with final summary\n- [ ] Saved to `06-findings-report.md` and `00-overview.md` updated\n\n### Instructions\n\n1. **Write findings report:**\n\n**Structure:**\n- **Main Findings:** Each theme with supporting quotes\n- **Prevalence:** Honest reporting (X of Y, not \"all\" or \"most\")\n- **Negative Cases:** Exceptions explained\n- **Context:** When/where does this apply?\n\n2. **Document limitations (MANDATORY - be HONEST):**\n\n**You MUST address:**\n- Sample limitations (size, homogeneity, recruitment source)\n- Method constraints (interviews vs. observations, question design)\n- Researcher bias (documented in Phase 1, how it may have influenced)\n- Context limitations (geography, time period, industry)\n\n**Why:** Acknowledging limitations STRENGTHENS credibility. False certainty undermines trust.\n\n3. **Assess confidence (trustworthiness criteria):**\n\n- **Credibility:** Do findings accurately represent participant experiences?\n- **Dependability:** Would another researcher reach similar conclusions?\n- **Confirmability:** Are findings based on data, not researcher bias?\n- **Transferability:** Do findings apply beyond this specific sample?\n\nRate each: High / Medium / Low. Provide justification.\n\n4. **Identify 2-3 follow-up questions:**\n\nEvery analysis should raise new questions:\n- What would you investigate next?\n- What surprised you that needs deeper exploration?\n- What would strengthen confidence in findings?\n\n5. **Update `00-overview.md` with summary:**\n\nAdd final summary section with:\n- Main findings (3-5 bullet points)\n- Signal classification (if invoked by marketing-experimentation): Positive/Negative/Null/Mixed\n- Confidence level\n- Follow-up recommendations\n\n6. **Save to `06-findings-report.md`**\n\n7. **Mark Phase 6 complete:** All checkpoints verified.\n\n### Common Rationalization: \"Limitations will undermine findings, downplay them\"\n\n**Why this is wrong:** Stating limitations INCREASES credibility. Readers trust honest uncertainty.\n\n**Do instead:** State limitations clearly. Be honest about what you don't know.\n\n---\n\n## Common Rationalizations - STOP\n\nThese are violations of skill requirements:\n\n| Excuse | Reality |\n|--------|---------|\n| \"I don't have biases to document\" | Everyone has assumptions. If you can't name them, they're controlling you invisibly. |\n| \"Expert opinion reduces need for bias documentation\" | Expert opinion IS a bias. Authority backing is a strong prior that MUST be documented. |\n| \"Time pressure justifies skipping formal process\" | Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours. |\n| \"Coding was straightforward, low risk\" | \"Straightforward\" is subjective. Even clear codes have interpretation variance. |\n| \"Time constraints justify skipping verification\" | Verification takes 1 hour. Fixing flawed findings after presentation takes days. |\n| \"Informal spot-check is sufficient\" | Spot-checks catch obvious errors. Intercoder reliability catches systematic bias. Both required. |\n| \"User reviewed coding, enough validation\" | User can't catch their own interpretation bias. Second coder does. Non-negotiable. |\n| \"Can do reliability check later if needed\" | After themes developed, reliability check invalidates hours of work. Do it in Phase 4. |\n| \"Themes clearly supported by majority\" | Majority agreement doesn't eliminate contradictory evidence. Must explain ALL data. |\n| \"Expert prediction validates findings\" | When predictions match findings perfectly, that's when to search hardest for contradictions. |\n| \"High consistency (8/10, 9/10) indicates robustness\" | Real customer sentiment is messy. 9/10 agreement deserves scrutiny. |\n| \"Disconfirming evidence search unnecessary for obvious patterns\" | Obvious patterns MOST vulnerable to confirmation bias. Search is mandatory. |\n| \"Limitations undermine findings\" | Stating limitations INCREASES credibility. False certainty undermines trust. |\n| \"This is just initial/exploratory research\" | Exploratory means open-ended questions. Doesn't mean skip rigor. Follow the phases. |\n| \"I'm following the spirit of the rules\" | Violating checkpoints violates both letter AND spirit. No shortcuts. |\n\n**All of these mean: Checkpoint violated. Cannot proceed.**\n\n## Red Flags - STOP\n\nIf you catch yourself thinking ANY of these, you are rationalizing. STOP and follow the checkpoint:\n\n- \"I recommend...\" (should be \"You MUST...\")\n- \"Would you like to...\" (should be \"Cannot proceed without...\")\n- \"This is optional\" (critical steps are MANDATORY)\n- \"Spot-check\" instead of \"intercoder reliability check\"\n- \"I'll look for contradictions\" instead of \"Invoking search-disconfirming-evidence agent\"\n- \"This is just initial validation\" (rigor required at all stages)\n- \"Expert backing reduces need for X\" (authority is bias, must be documented)\n- \"Pattern is obvious\" (obvious patterns need MOST rigorous verification)\n- \"Can skip X and do it later\" (checkpoints are mandatory NOW, not later)\n\n**All of these mean: Violated skill requirements. Go back and complete checkpoint.**\n\n---\n\n## Summary\n\nThis skill ensures rigorous, reproducible qualitative research by:\n\n1. **Preventing confirmation bias:** Reflexivity baseline, neutral questions, disconfirming evidence search\n2. **Ensuring systematic analysis:** Codebook rigor, intercoder reliability, audit trails\n3. **Enforcing checkpoints:** Cannot skip critical steps (reflexivity, reliability, disconfirmation)\n4. **Using agent-based methods:** Sub-agents handle data-intensive operations, prevent context pollution\n5. **Demanding intellectual honesty:** Explicit limitations, confidence assessment, honest prevalence reporting\n\n**Follow this process and you'll produce defensible, credible qualitative research that stands up to scrutiny.**"
              },
              {
                "name": "understanding-data",
                "description": "Component skill for systematic data profiling and exploration in DataPeeker analysis sessions",
                "path": "plugins/datapeeker/skills/understanding-data/SKILL.md",
                "frontmatter": {
                  "name": "understanding-data",
                  "description": "Component skill for systematic data profiling and exploration in DataPeeker analysis sessions"
                },
                "content": "# Understanding Data\n\n## Purpose\n\nThis component skill guides systematic data profiling before analysis begins. Use it when:\n- Starting a new analysis session with unfamiliar data\n- Encountering unexpected query results\n- Need to understand data structure, quality, or relationships\n- Referenced by process skills requiring data familiarity\n\n## Prerequisites\n\n- Data loaded into a relational database (SQLite, PostgreSQL, MySQL, SQL Server, etc.)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Analysis workspace created (if using DataPeeker conventions)\n\n## Data Understanding Process\n\nCreate a TodoWrite checklist for the 4-phase data profiling process:\n\n```\nPhase 1: Schema Discovery\nPhase 2: Data Quality Assessment\nPhase 3: Distribution Analysis\nPhase 4: Relationship Identification\n```\n\nMark each phase as you complete it. Document all findings in a numbered markdown file.\n\n---\n\n## Phase 1: Schema Discovery\n\n**Goal:** Understand tables, columns, types, and cardinalities.\n\n### List All Tables\n\n```sql\n-- Get all tables in database (database-specific methods):\n-- SQLite: SELECT name FROM sqlite_master WHERE type='table';\n-- PostgreSQL: SELECT tablename FROM pg_tables WHERE schemaname = 'public';\n-- MySQL: SHOW TABLES;\n-- SQL Server: SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE';\n\n-- Standard SQL approach (PostgreSQL, MySQL, SQL Server):\nSELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'your_schema'\nORDER BY table_name;\n```\n\n**Document:** Table names and their likely business meaning.\n\n**Note:** Use database-specific CLI commands or syntax as appropriate for your database engine.\n\n### Examine Table Schemas\n\nFor each table of interest:\n\n```sql\n-- Get column information (database-specific):\n-- SQLite: PRAGMA table_info(table_name);\n-- PostgreSQL: \\d table_name (CLI) or SELECT * FROM information_schema.columns WHERE table_name = 'table_name';\n-- MySQL: DESCRIBE table_name; or SHOW COLUMNS FROM table_name;\n-- SQL Server: EXEC sp_columns table_name; or SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'table_name';\n\n-- Standard SQL approach:\nSELECT column_name, data_type, is_nullable, column_default\nFROM information_schema.columns\nWHERE table_name = 'your_table'\nORDER BY ordinal_position;\n```\n\n**Document:**\n- Column names and types\n- Which columns are likely keys/identifiers\n- Which columns are measures vs dimensions\n- Missing columns you'd expect to see\n\n### Check Row Counts\n\n```sql\n-- Count rows in each table\nSELECT 'table_name' as table_name, COUNT(*) as row_count\nFROM table_name;\n```\n\n**Document:**\n- Relative sizes of tables\n- Whether counts match expectations\n- Any suspiciously small/large tables\n\n### Identify Unique Identifiers\n\n```sql\n-- Check column uniqueness\nSELECT COUNT(*) as total_rows,\n       COUNT(DISTINCT column_name) as unique_values,\n       COUNT(*) - COUNT(DISTINCT column_name) as duplicate_count\nFROM table_name;\n```\n\n**Test for each candidate key column.**\n\n**Document:**\n- Which columns are unique (potential primary keys)\n- Which columns have high cardinality (useful for grouping)\n- Which columns have low cardinality (categories/flags)\n\n---\n\n## Phase 2: Data Quality Assessment\n\n**Goal:** Identify missing data, invalid values, and data quality issues.\n\n### Check for NULL Values\n\n```sql\n-- Count NULLs for each important column\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column1) as column1_non_null,\n  COUNT(*) - COUNT(column1) as column1_null_count,\n  ROUND(100.0 * (COUNT(*) - COUNT(column1)) / COUNT(*), 2) as column1_null_pct\nFROM table_name;\n```\n\n**Document:**\n- Which columns have missing data\n- Percentage of missingness\n- Whether missingness is random or systematic\n\n### Check for Empty Strings\n\n```sql\n-- Find empty or whitespace-only strings\nSELECT COUNT(*) as empty_string_count\nFROM table_name\nWHERE TRIM(text_column) = ''\n   OR text_column IS NULL;\n```\n\n**Document:** Columns with empty string issues.\n\n### Examine Value Ranges\n\nFor numeric columns:\n\n```sql\n-- Get min, max, and outlier candidates\nSELECT\n  MIN(numeric_column) as min_value,\n  MAX(numeric_column) as max_value,\n  AVG(numeric_column) as avg_value,\n  COUNT(*) as total_count,\n  COUNT(CASE WHEN numeric_column < 0 THEN 1 END) as negative_count,\n  COUNT(CASE WHEN numeric_column = 0 THEN 1 END) as zero_count\nFROM table_name;\n```\n\n**Document:**\n- Impossible values (negative prices, future dates, etc.)\n- Extreme outliers\n- Suspicious patterns (many zeros, rounded numbers)\n\n### Check Date/Time Validity\n\n```sql\n-- Examine date ranges and formats\nSELECT\n  MIN(date_column) as earliest_date,\n  MAX(date_column) as latest_date,\n  COUNT(DISTINCT date_column) as unique_dates,\n  COUNT(*) as total_rows\nFROM table_name;\n```\n\n**Document:**\n- Date ranges (do they make sense for the business context?)\n- Gaps in date coverage\n- Future dates where inappropriate\n\n### Examine Categorical Values\n\n```sql\n-- Get frequency distribution for categorical column\nSELECT\n  category_column,\n  COUNT(*) as frequency,\n  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM table_name), 2) as percentage\nFROM table_name\nGROUP BY category_column\nORDER BY frequency DESC;\n```\n\n**Document:**\n- Valid categories and their distributions\n- Unexpected categories\n- Misspellings or data entry variations\n- Extremely rare categories\n\n---\n\n## Phase 3: Distribution Analysis\n\n**Goal:** Understand how data is distributed across key dimensions.\n\n### Time Distribution\n\nFor any table with dates/timestamps:\n\n```sql\n-- Group by time period to see distribution\n-- Extract year-month (database-specific function):\n-- SQLite: STRFTIME('%Y-%m', date_column)\n-- PostgreSQL: TO_CHAR(date_column, 'YYYY-MM')\n-- MySQL: DATE_FORMAT(date_column, '%Y-%m')\n-- SQL Server: FORMAT(date_column, 'yyyy-MM')\n\n-- Example using SQLite syntax:\nSELECT\n  STRFTIME('%Y-%m', date_column) as year_month,\n  COUNT(*) as row_count\nFROM table_name\nGROUP BY year_month\nORDER BY year_month;\n```\n\n**Look for:**\n- Seasonal patterns\n- Growth/decline trends\n- Gaps or spikes\n- Incomplete periods (partial months)\n\n**Document:** Time coverage and patterns.\n\n### Segmentation Distribution\n\nFor key categorical dimensions:\n\n```sql\n-- Distribution by segment\nSELECT\n  segment_column,\n  COUNT(*) as count,\n  COUNT(DISTINCT id_column) as unique_entities,\n  ROUND(AVG(numeric_measure), 2) as avg_measure\nFROM table_name\nGROUP BY segment_column\nORDER BY count DESC;\n```\n\n**Document:**\n- How data is distributed across segments\n- Whether segments are balanced or skewed\n- Segments with insufficient data for analysis\n\n### Value Distribution Buckets\n\nFor continuous numeric measures:\n\n```sql\n-- Create distribution buckets\nSELECT\n  CASE\n    WHEN value < 10 THEN '0-9'\n    WHEN value < 50 THEN '10-49'\n    WHEN value < 100 THEN '50-99'\n    WHEN value < 500 THEN '100-499'\n    ELSE '500+'\n  END as value_bucket,\n  COUNT(*) as frequency\nFROM table_name\nGROUP BY value_bucket\nORDER BY value_bucket;\n```\n\n**Adjust buckets to your data range.**\n\n**Document:**\n- Shape of distribution (normal, skewed, bimodal?)\n- Presence of outliers\n- Whether transformations might be needed\n\n### Correlation Exploration\n\nFor related numeric columns:\n\n```sql\n-- Check if two measures move together\nSELECT\n  segment,\n  COUNT(*) as n,\n  ROUND(AVG(measure1), 2) as avg_measure1,\n  ROUND(AVG(measure2), 2) as avg_measure2\nFROM table_name\nGROUP BY segment\nORDER BY avg_measure1;\n```\n\n**Look for:** Whether segments with high measure1 also have high measure2.\n\n**Document:** Observed correlations or anti-correlations.\n\n---\n\n## Phase 4: Relationship Identification\n\n**Goal:** Understand how tables relate to each other.\n\n### Foreign Key Discovery\n\n```sql\n-- Find potential foreign keys by checking if column values exist in another table\nSELECT\n  'table_a.fk_column references table_b.id_column' as relationship,\n  COUNT(*) as rows_in_table_a,\n  COUNT(DISTINCT a.fk_column) as unique_fk_values,\n  (SELECT COUNT(DISTINCT id_column) FROM table_b) as unique_ids_in_table_b\nFROM table_a a;\n```\n\n**Test:** Do all fk_column values exist in table_b? (Referential integrity)\n\n**Document:**\n- Confirmed foreign key relationships\n- Orphaned records (FKs with no matching primary key)\n- Unused primary keys (keys with no foreign key references)\n\n### Join Cardinality\n\n```sql\n-- Understand join behavior before using it\nSELECT\n  'table_a to table_b' as join_direction,\n  COUNT(*) as rows_before_join,\n  COUNT(b.id) as matches_found,\n  COUNT(*) - COUNT(b.id) as rows_without_match\nFROM table_a a\nLEFT JOIN table_b b ON a.fk_column = b.id_column;\n```\n\n**Document:**\n- One-to-one, one-to-many, or many-to-many?\n- Percentage of successful joins\n- Whether LEFT/INNER/OUTER JOIN is appropriate\n\n### Cross-Table Consistency\n\n```sql\n-- Check if aggregates match between tables\nSELECT 'table_a' as source, SUM(amount) as total FROM table_a\nUNION ALL\nSELECT 'table_b' as source, SUM(amount) as total FROM table_b;\n```\n\n**For fact tables that should reconcile.**\n\n**Document:**\n- Whether totals match expectations\n- Discrepancies that need explanation\n\n### Hierarchical Relationships\n\n```sql\n-- Find parent-child relationships\nSELECT\n  parent_id,\n  COUNT(*) as child_count,\n  COUNT(DISTINCT child_category) as distinct_child_types\nFROM table_name\nGROUP BY parent_id\nORDER BY child_count DESC;\n```\n\n**Document:**\n- Hierarchical structures (customers > orders > line items)\n- Depth of hierarchy\n- Orphaned children or childless parents\n\n---\n\n## Documentation Requirements\n\nAfter completing all 4 phases, create a summary document:\n\n### Data Profile Summary\n\n```markdown\n## Data Profile Summary\n\n### Tables\n- `table_name` (X rows): Description and purpose\n\n### Key Findings\n- Data Quality: [Major issues or \"clean\"]\n- Coverage: [Date ranges, completeness]\n- Relationships: [How tables connect]\n\n### Analysis Implications\n- [What this data can/cannot answer]\n- [Segments available for analysis]\n- [Known limitations]\n\n### Recommended Filters\n- [Filters to apply for clean data]\n- [Time periods to focus on]\n```\n\n---\n\n## Common Pitfalls\n\n**DON'T:**\n- Assume column names accurately describe contents\n- Skip Phase 2 (data quality) - dirty data leads to wrong conclusions\n- Ignore NULL values - they often have business meaning\n- Forget to check join cardinality before complex queries\n\n**DO:**\n- Verify assumptions with queries\n- Document surprises and anomalies\n- Note data quality issues for later interpretation\n- Keep a running list of questions for data owners\n\n---\n\n## When to Re-Profile\n\nRe-run portions of this skill when:\n- Query results don't match expectations\n- You discover a new table mid-analysis\n- You suspect data quality issues\n- Time period changes significantly (new data loaded)\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nIf unfamiliar with the data, use the `understanding-data` component skill to profile tables before proceeding.\n```\n\nThis ensures analysts don't make assumptions about data structure or quality.\n\n## Database Engine Specifics\n\nThis skill provides database-agnostic guidance with examples in multiple SQL dialects.\n\n**Related skills for database-specific implementation:**\n- `using-sqlite` - SQLite CLI usage, syntax, and optimizations\n- `using-postgresql` - PostgreSQL-specific features (if available)\n- Other database-specific skills for CLI commands, performance tuning, and engine-specific features"
              },
              {
                "name": "using-sqlite",
                "description": "Use when working with SQLite databases in DataPeeker analysis sessions - querying data, importing CSVs, exploring schemas, formatting output, or optimizing performance. Provides task-oriented guidance for effective SQLite CLI usage in data analysis workflows.",
                "path": "plugins/datapeeker/skills/using-sqlite/SKILL.md",
                "frontmatter": {
                  "name": "using-sqlite",
                  "description": "Use when working with SQLite databases in DataPeeker analysis sessions - querying data, importing CSVs, exploring schemas, formatting output, or optimizing performance. Provides task-oriented guidance for effective SQLite CLI usage in data analysis workflows."
                },
                "content": "# Using SQLite\n\n## Overview\n\nSQLite is the primary database for DataPeeker analysis sessions. This skill provides task-oriented guidance for common SQLite operations during data analysis.\n\n**Core principle:** Explore schema first, format output for readability, write efficient queries, verify all operations.\n\n## When to Use\n\nUse this skill when you need to:\n- Explore an unfamiliar database schema\n- Query data for analysis or hypothesis testing\n- Import CSV files into SQLite tables\n- Format query output for readability\n- Diagnose slow queries or optimize performance\n- Understand which CLI invocation pattern to use\n\n**When NOT to use:**\n- Database-agnostic data profiling (see `understanding-data` skill for patterns that work across all SQL databases)\n- Complex data cleaning logic (delegate to `cleaning-data` skill + sub-agents)\n- Statistical analysis (use Python/pandas for advanced statistics)\n- Large-scale transformations (use Python sqlite3 module)\n\n## DataPeeker Conventions\n\n```\nDatabase Path:    data/analytics.db (relative from project root)\nTable Naming:     raw_* for imported data, clean_* for cleaned data\nSingle Database:  All tables in one file per analysis session\n```\n\n**Example workflow:**\n```\nCSV file → raw_sales → clean_sales → Analysis queries\n```\n\n## Quick Reference\n\n| Task | Guidance File |\n|------|---------------|\n| Understand what tables/columns exist | @./exploring-schema.md |\n| Make query results readable | @./formatting-output.md |\n| Write analytical queries | @./writing-queries.md |\n| Load CSV files | @./importing-data.md |\n| Fix slow queries | @./optimizing-performance.md |\n| Choose CLI invocation method | @./invoking-cli.md |\n\n## Task-Oriented Guidance\n\n### Exploring Schema\n\n**Before writing queries, understand the database structure.**\n\nSee @./exploring-schema.md for:\n- Listing all tables (.tables)\n- Viewing table structure (.schema, PRAGMA table_info)\n- Understanding column types and constraints\n- Checking for indexes\n\n**When:** Starting analysis, unfamiliar database, before writing joins\n\n---\n\n### Formatting Output\n\n**Make query results readable for analysis.**\n\nSee @./formatting-output.md for:\n- Output modes (column, csv, json, markdown)\n- Showing/hiding headers (.headers on/off)\n- Setting column widths for readability\n- Redirecting output to files\n\n**When:** Query results hard to read, need specific format for export, preparing reports\n\n---\n\n### Writing Queries\n\n**SQLite-specific query patterns and conventions.**\n\nSee @./writing-queries.md for:\n- SQLite idioms used in DataPeeker (COUNT(*) - COUNT(col) for NULLs)\n- Date handling with STRFTIME\n- DataPeeker percentage calculation conventions\n- Common verification queries\n\n**See also:** `writing-queries` and `understanding-data` skills for database-agnostic SQL patterns and data profiling approaches. This guidance focuses on SQLite-specific syntax, CLI usage, and optimizations.\n\n**When:** Need SQLite-specific syntax, DataPeeker query conventions, date formatting with STRFTIME\n\n---\n\n### Importing Data\n\n**Load CSV files and verify import success.**\n\nSee @./importing-data.md for:\n- Using .import command\n- Verification queries (row counts, sample data)\n- When to use CLI vs Python sqlite3\n- Transaction handling\n\n**When:** Loading new data, Phase 4 of importing-data skill, verifying data loaded correctly\n\n---\n\n### Optimizing Performance\n\n**Diagnose and fix slow queries.**\n\nSee @./optimizing-performance.md for:\n- EXPLAIN QUERY PLAN analysis\n- Creating indexes for common queries\n- Using transactions for bulk operations\n- PRAGMA optimization\n\n**When:** Query takes >1 second, loading large datasets, repeated similar queries\n\n---\n\n### Invoking CLI\n\n**Choose the right method to run sqlite3 commands.**\n\nSee @./invoking-cli.md for:\n- Interactive mode (for exploration)\n- Heredoc pattern (for multi-command scripts)\n- File redirect (for SQL files)\n- One-liner mode (for quick checks)\n\n**When:** Starting any SQLite operation, unsure which invocation to use\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Writing queries without exploring schema | Always run .tables and .schema first |\n| Poor output formatting (hard to read results) | Use .mode column and .headers on for readability |\n| Ignoring NULL values in calculations | Use COUNT(*) - COUNT(col) for NULL counting |\n| Integer division losing decimals | Use 100.0 (not 100) for percentage calculations |\n| Slow queries without diagnosis | Run EXPLAIN QUERY PLAN before optimizing |\n| Assuming import succeeded | Always verify with SELECT COUNT(*) after import |\n| Using wrong CLI invocation pattern | Interactive for exploration, heredoc for scripts |\n\n## Verification Before Proceeding\n\n**After any import operation:**\n```sql\n-- 1. Verify row count matches expectation\nSELECT COUNT(*) FROM raw_table;\n\n-- 2. Check sample data looks correct\nSELECT * FROM raw_table LIMIT 5;\n\n-- 3. Verify no unexpected NULLs\nSELECT COUNT(*) - COUNT(critical_column) FROM raw_table;\n```\n\n**After writing a complex query:**\n```sql\n-- 1. Check query plan\nEXPLAIN QUERY PLAN SELECT ...;\n\n-- 2. Time the query\n.timer on\nSELECT ...;\n```\n\n## Real-World Impact\n\n**Systematic approach prevents:**\n- Writing queries against wrong tables (explore schema first)\n- Unreadable output (format before analyzing)\n- Slow queries (diagnose with EXPLAIN QUERY PLAN)\n- Silent data loss (verify imports)\n- Incorrect percentages (use float division)\n\n**Following this skill:**\n- Reduces query debugging time by 50%+\n- Catches import issues immediately\n- Produces readable analysis artifacts\n- Ensures reproducible workflows"
              },
              {
                "name": "writing-queries",
                "description": "Component skill for systematic SQL query development in DataPeeker analysis sessions",
                "path": "plugins/datapeeker/skills/writing-queries/SKILL.md",
                "frontmatter": {
                  "name": "writing-queries",
                  "description": "Component skill for systematic SQL query development in DataPeeker analysis sessions"
                },
                "content": "# Writing Queries\n\n## Purpose\n\nThis component skill guides systematic query development for analytics. Use it when:\n- Writing SQL queries for analysis tasks\n- Need to ensure queries are correct, documented, and reproducible\n- Referenced by process skills requiring query execution\n- Struggling with how to express an analytical question in SQL\n\n## Prerequisites\n\n- CSV files imported to database (relational database with SQL support)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Understanding of table schema (use `understanding-data` skill if needed)\n- Clear analytical question to answer\n\n## Query Development Process\n\nCreate a TodoWrite checklist for the 5-phase query development process:\n\n```\nPhase 1: Clarify Analytical Question\nPhase 2: Design Query Logic\nPhase 3: Write SQL\nPhase 4: Verify Results\nPhase 5: Document Query\n```\n\nMark each phase as you complete it. Save queries in numbered markdown files.\n\n---\n\n## Phase 1: Clarify Analytical Question\n\n**Goal:** Translate vague analytical question into specific, answerable query requirements.\n\n### Ask Clarifying Questions\n\nBefore writing any SQL, ensure you can answer:\n\n1. **What exactly are we measuring?**\n   - Specific metric name and calculation method\n   - Example: \"Total revenue\" vs \"Average revenue per customer\" vs \"Revenue per day\"\n\n2. **What is the unit of analysis?**\n   - Per transaction, per customer, per day, per product, etc.\n   - Example: \"Daily sales\" means one row per date\n\n3. **What time period?**\n   - All time, specific date range, rolling window?\n   - How to handle incomplete periods (partial weeks/months)?\n\n4. **What filters apply?**\n   - Specific products, regions, customer segments?\n   - Exclude returns, cancelled orders, test data?\n\n5. **What groupings/segments?**\n   - By product category, by region, by customer type?\n   - Or overall aggregate with no grouping?\n\n6. **What comparison or context?**\n   - Compared to previous period, to average, to another segment?\n   - Rank ordering, percentage of total?\n\n### Document Requirements\n\nBefore proceeding to Phase 2, write down:\n\n```markdown\n## Query Requirements\n\n**Analytical Question:** [Original question in plain language]\n\n**Specific Metric:** [Exact calculation]\n- Formula: [e.g., SUM(amount) / COUNT(DISTINCT customer_id)]\n- Name: [e.g., \"Average Revenue Per Customer\"]\n\n**Unit of Analysis:** [e.g., \"One row per product category\"]\n\n**Time Period:** [e.g., \"January 1 - March 31, 2024\"]\n\n**Filters:**\n- [Filter 1: e.g., \"Exclude cancelled orders\"]\n- [Filter 2: e.g., \"Only completed transactions\"]\n\n**Grouping:** [e.g., \"Group by product_category\"]\n\n**Ordering:** [e.g., \"Order by total_revenue DESC, show top 10\"]\n```\n\n**Don't proceed until requirements are crystal clear.**\n\n---\n\n## Phase 2: Design Query Logic\n\n**Goal:** Plan query structure before writing SQL syntax.\n\n### Identify Required Tables\n\nList all tables needed and why:\n\n```markdown\n## Tables Needed\n\n1. **orders** - Contains transaction dates and amounts\n   - Join key: order_id\n   - Columns: order_date, total_amount, customer_id\n\n2. **customers** - Contains customer segment information\n   - Join key: customer_id\n   - Columns: customer_id, segment, region\n\n3. **order_items** - Contains product details (if needed)\n   - Join key: order_id\n   - Columns: product_id, quantity, price\n```\n\n### Design Join Strategy\n\nIf multiple tables are needed:\n\n```markdown\n## Join Logic\n\nMain table: orders (one row per transaction)\n\nLEFT JOIN customers ON orders.customer_id = customers.customer_id\n- Type: LEFT (to keep orders even if customer record missing)\n- Cardinality: many-to-one (many orders per customer)\n- Risk: None - customer_id should always match\n\nINNER JOIN order_items ON orders.order_id = order_items.order_id\n- Type: INNER (only want orders with items)\n- Cardinality: one-to-many (multiple items per order)\n- Risk: Row explosion - order totals will be duplicated\n- Mitigation: Calculate item metrics separately or use DISTINCT\n```\n\n**Check join cardinality before complex joins to avoid row explosion.**\n\n### Plan Calculation Steps\n\nBreak complex calculations into logical steps:\n\n```markdown\n## Calculation Steps\n\nStep 1: Filter to desired date range and conditions\n- WHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n- AND status = 'completed'\n\nStep 2: Calculate base metrics\n- Total revenue: SUM(amount)\n- Transaction count: COUNT(*)\n- Unique customers: COUNT(DISTINCT customer_id)\n\nStep 3: Derive calculated metrics\n- Average revenue per customer: total_revenue / unique_customers\n\nStep 4: Group and aggregate\n- GROUP BY product_category\n\nStep 5: Order results\n- ORDER BY total_revenue DESC\n- LIMIT 10\n```\n\n### Consider Edge Cases\n\nDocument potential issues:\n\n```markdown\n## Edge Cases to Handle\n\n1. **NULL values:** How to handle NULL in date, amount, or category?\n   - Decision: Exclude rows with NULL in critical columns\n\n2. **Zero division:** What if denominator is zero (e.g., no customers)?\n   - Decision: Use NULLIF or CASE to avoid division by zero\n\n3. **Duplicate records:** Could there be duplicate transactions?\n   - Decision: Check with COUNT(*) vs COUNT(DISTINCT order_id)\n\n4. **Date formatting:** Are dates stored as strings or native DATE type?\n   - Decision: Use database date functions to extract components, verify format is consistent\n```\n\n---\n\n## Phase 3: Write SQL\n\n**Goal:** Translate design into clean, commented SQL.\n\n### SQL Best Practices\n\n**DO:**\n\n1. **Use clear formatting:**\n```sql\nSELECT\n  column1,\n  column2,\n  SUM(column3) as total\nFROM table_name\nWHERE condition = 'value'\nGROUP BY column1, column2\nORDER BY total DESC;\n```\n\n2. **Add comments for clarity:**\n```sql\n-- Calculate daily sales metrics for Q1 2024\nSELECT\n  -- Extract date component (function varies by database)\n  -- SQLite: DATE(order_date)\n  -- PostgreSQL: order_date::date or DATE(order_date)\n  -- MySQL: DATE(order_date)\n  CAST(order_date AS DATE) as sale_date,\n  COUNT(*) as transaction_count,  -- Total orders per day\n  SUM(amount) as total_revenue,   -- Gross revenue before returns\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n  AND status = 'completed'  -- Exclude cancelled/pending\nGROUP BY CAST(order_date AS DATE)\nORDER BY sale_date;\n```\n\n3. **Use meaningful aliases:**\n```sql\n-- Good\nSELECT product_category as category, SUM(amount) as total_revenue\n\n-- Bad\nSELECT product_category as pc, SUM(amount) as t\n```\n\n4. **Handle NULLs explicitly:**\n```sql\nSELECT\n  customer_id,\n  COUNT(order_id) as order_count,\n  COALESCE(SUM(amount), 0) as total_spent  -- Replace NULL with 0\nFROM orders\nWHERE customer_id IS NOT NULL  -- Explicit NULL handling\nGROUP BY customer_id;\n```\n\n5. **Use CTEs for complex queries:**\n```sql\n-- Break complex logic into readable steps\nWITH daily_sales AS (\n  SELECT\n    DATE(order_date) as sale_date,\n    SUM(amount) as revenue\n  FROM orders\n  WHERE status = 'completed'\n  GROUP BY DATE(order_date)\n),\ndaily_stats AS (\n  SELECT\n    AVG(revenue) as avg_daily_revenue,\n    -- SQLite doesn't have STDEV() - calculate manually using variance formula\n    SQRT(AVG(revenue * revenue) - AVG(revenue) * AVG(revenue)) as stddev_revenue\n  FROM daily_sales\n)\nSELECT\n  ds.sale_date,\n  ds.revenue,\n  ROUND((ds.revenue - dst.avg_daily_revenue) / dst.stddev_revenue, 2) as z_score\nFROM daily_sales ds\nCROSS JOIN daily_stats dst\nORDER BY ds.sale_date;\n```\n\n**DON'T:**\n\n1. **Don't use SELECT * in analysis queries:**\n```sql\n-- Bad - unclear what columns you're using\nSELECT * FROM orders WHERE amount > 100;\n\n-- Good - explicit about needed columns\nSELECT order_id, order_date, amount FROM orders WHERE amount > 100;\n```\n\n2. **Don't ignore case sensitivity in comparisons:**\n```sql\n-- Risky - might miss 'PENDING' or 'Pending'\nWHERE status = 'pending'\n\n-- Better - normalize case\nWHERE LOWER(status) = 'pending'\n```\n\n3. **Don't create ambiguous joins:**\n```sql\n-- Bad - which table's date column?\nSELECT date, amount FROM orders JOIN shipments USING (order_id);\n\n-- Good - explicit table prefixes\nSELECT o.order_date, o.amount, s.ship_date\nFROM orders o\nJOIN shipments s ON o.order_id = s.order_id;\n```\n\n4. **Don't forget GROUP BY requirements:**\n```sql\n-- Wrong - product_category not in GROUP BY\nSELECT product_category, SUM(amount) FROM orders GROUP BY customer_id;\n\n-- Correct\nSELECT product_category, SUM(amount) FROM orders GROUP BY product_category;\n```\n\n---\n\n## Phase 4: Verify Results\n\n**Goal:** Ensure query returns correct, sensible results.\n\n### Verification Checklist\n\nBefore trusting query results, verify:\n\n**1. Row count makes sense:**\n```sql\n-- Check: Does row count match expectations?\nSELECT COUNT(*) as row_count FROM (\n  -- Your query here\n);\n```\n\nExpected: If grouping by day-of-week, should have 7 rows.\nIf grouping by month, should have 12 rows (or fewer if partial year).\n\n**2. Aggregates are reasonable:**\n```sql\n-- Check: Are totals in expected range?\nSELECT\n  SUM(amount) as total_revenue,\n  COUNT(*) as transaction_count,\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount,\n  AVG(amount) as avg_amount\nFROM orders;\n```\n\nAsk: Do these values make business sense? Any impossibly high/low values?\n\n**3. No unexpected NULLs:**\n```sql\n-- Check: Are there NULL values in results?\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column1) as column1_non_null,\n  COUNT(column2) as column2_non_null\nFROM (\n  -- Your query here\n);\n```\n\nIf critical columns have NULLs, investigate why.\n\n**4. Percentages sum to 100% (if applicable):**\n```sql\n-- Check: Do percentage columns sum to 100?\nSELECT SUM(pct_of_total) as total_pct FROM (\n  SELECT\n    category,\n    100.0 * COUNT(*) / SUM(COUNT(*)) OVER () as pct_of_total\n  FROM orders\n  GROUP BY category\n);\n```\n\nShould equal 100.0 (or close, accounting for rounding).\n\n**5. Spot check specific values:**\n\nPick a specific row from results and verify manually:\n\n```sql\n-- Example: Verify March 15 sales total\nSELECT SUM(amount) FROM orders WHERE CAST(order_date AS DATE) = '2024-03-15';\n```\n\nCompare to what your main query shows for March 15.\n\n**6. Compare to known totals:**\n\n```sql\n-- Check: Does grouped data sum to overall total?\n-- Total from grouped query:\nSELECT SUM(category_total) FROM (\n  SELECT category, SUM(amount) as category_total\n  FROM orders\n  GROUP BY category\n);\n\n-- Should equal overall total:\nSELECT SUM(amount) FROM orders;\n```\n\n### Common Query Errors\n\n**Row explosion from joins:**\n```sql\n-- Symptom: Totals are too high after joining\n-- Cause: One-to-many join duplicates rows\n-- Fix: Use DISTINCT or calculate at appropriate grain\n\n-- Wrong - duplicates order amounts\nSELECT o.order_id, SUM(o.amount)\nFROM orders o\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id;  -- order amount counted once per item!\n\n-- Right - calculate order and item metrics separately\nSELECT\n  o.order_id,\n  o.amount as order_total,\n  COUNT(oi.item_id) as item_count\nFROM orders o\nLEFT JOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, o.amount;\n```\n\n**Incorrect grouping:**\n```sql\n-- Symptom: Unexpected row count or aggregates\n-- Cause: Missing or extra columns in GROUP BY\n-- Fix: Include ALL non-aggregated columns in GROUP BY\n\n-- Wrong - missing category from GROUP BY\nSELECT category, product, SUM(sales)\nFROM products\nGROUP BY category;  -- Error or wrong results\n\n-- Right\nSELECT category, product, SUM(sales)\nFROM products\nGROUP BY category, product;\n```\n\n**Date/time parsing errors:**\n```sql\n-- Symptom: Date groupings don't match expectations\n-- Cause: Date format inconsistency or wrong extraction\n-- Fix: Verify date format and use appropriate database function\n\n-- Check date format first\nSELECT DISTINCT date_column FROM table LIMIT 10;\n\n-- Extract date components using database-specific functions\n-- Postgres: TO_CHAR, DATE_TRUNC, EXTRACT\n-- MySQL: DATE_FORMAT, YEAR, MONTH\n-- SQLite: STRFTIME, DATE\n-- See database documentation for specific syntax\n\n-- Generic approach: Cast to DATE type\nSELECT CAST(date_column AS DATE) FROM table;\n```\n\n---\n\n## Phase 5: Document Query\n\n**Goal:** Create reproducible documentation for query and results.\n\n### Query Documentation Template\n\nSave each query in a numbered markdown file:\n\n```markdown\n# [Query Purpose]\n\n## Analytical Question\n[What question does this query answer?]\n\nExample: \"What are the top 10 product categories by revenue in Q1 2024?\"\n\n## Rationale\n[Why is this query needed for the analysis?]\n\nExample: \"Identifies high-value categories to prioritize for Q2 marketing campaigns.\"\n\n## Query\n```sql\n-- Clear, commented SQL\nSELECT\n  product_category,\n  COUNT(DISTINCT order_id) as order_count,\n  SUM(amount) as total_revenue,\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n  AND status = 'completed'\nGROUP BY product_category\nORDER BY total_revenue DESC\nLIMIT 10;\n```\n\n## Results\n[Paste actual query results - raw output]\n\n```\nproduct_category | order_count | total_revenue | avg_order_value\nElectronics      | 1523        | 345678.90     | 226.89\nHome & Garden    | 2341        | 298432.10     | 127.48\nClothing         | 3456        | 234567.80     | 67.89\n...\n```\n\n## Interpretation\n[What do the results show? Key takeaways.]\n\n- Electronics generated highest revenue ($345K) despite fewer orders (1,523)\n- Electronics has highest average order value ($227) - premium category\n- Clothing has most orders (3,456) but lower AOV ($68) - volume category\n\n## Data Quality Notes\n[Any issues or caveats about the data]\n\n- No NULL categories found\n- Date range: 90 complete days (Jan 1 - Mar 31)\n- Excluded 47 cancelled orders from analysis\n```\n\n### Documentation Best Practices\n\n**DO:**\n- Save queries in version control (git)\n- Include exact SQL that was run\n- Paste actual results, not summaries\n- Note any data quality issues discovered\n- Date your analysis files\n\n**DON'T:**\n- Paraphrase results (show actual numbers)\n- Round or simplify results prematurely\n- Skip documenting failed queries (document what didn't work and why)\n- Forget to note filters and exclusions\n\n---\n\n## Common Query Patterns\n\n### Pattern 1: Time Series Analysis\n\n**Use case:** Analyze metrics over time (daily, weekly, monthly trends)\n\n```sql\n-- Daily trend\nSELECT\n  CAST(order_date AS DATE) as date,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue,\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY CAST(order_date AS DATE)\nORDER BY date;\n\n-- Weekly trend (implementation varies by database)\n-- Postgres: DATE_TRUNC('week', order_date)\n-- MySQL: DATE_SUB(order_date, INTERVAL WEEKDAY(order_date) DAY)\n-- SQLite: DATE(order_date, 'weekday 0', '-6 days')\nSELECT\n  -- Use database-specific function to get week start date\n  [week_start_function] as week_start,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY week_start\nORDER BY week_start;\n\n-- Monthly trend (implementation varies by database)\n-- Postgres: TO_CHAR(order_date, 'YYYY-MM')\n-- MySQL: DATE_FORMAT(order_date, '%Y-%m')\n-- SQLite: STRFTIME('%Y-%m', order_date)\nSELECT\n  -- Use database-specific function to extract year-month\n  [year_month_function] as year_month,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders\nGROUP BY year_month\nORDER BY year_month;\n```\n\n### Pattern 2: Segmentation Analysis\n\n**Use case:** Compare metrics across categories or segments\n\n```sql\n-- Basic segmentation\nSELECT\n  segment,\n  COUNT(*) as count,\n  SUM(amount) as total,\n  ROUND(AVG(amount), 2) as average,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct_of_total\nFROM orders\nGROUP BY segment\nORDER BY total DESC;\n\n-- Multi-level segmentation\nSELECT\n  region,\n  customer_type,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nGROUP BY region, customer_type\nORDER BY region, revenue DESC;\n```\n\n### Pattern 3: Ranking and Top N\n\n**Use case:** Identify top/bottom performers\n\n```sql\n-- Top 10 by revenue\nSELECT\n  product_name,\n  SUM(amount) as total_revenue\nFROM order_items oi\nJOIN products p ON oi.product_id = p.product_id\nGROUP BY product_name\nORDER BY total_revenue DESC\nLIMIT 10;\n\n-- Top 20% of customers (by revenue)\n-- NOTE: PERCENT_RANK() requires SQLite 3.28+\nWITH customer_revenue AS (\n  SELECT\n    customer_id,\n    SUM(amount) as total_revenue\n  FROM orders\n  GROUP BY customer_id\n),\nranked_customers AS (\n  SELECT\n    customer_id,\n    total_revenue,\n    PERCENT_RANK() OVER (ORDER BY total_revenue DESC) as percentile_rank\n  FROM customer_revenue\n)\n-- Must use CTE to filter on window function results\nSELECT\n  customer_id,\n  total_revenue,\n  percentile_rank\nFROM ranked_customers\nWHERE percentile_rank <= 0.20\nORDER BY total_revenue DESC;\n\n-- Top 3 products per category\nWITH ranked_products AS (\n  SELECT\n    category,\n    product_name,\n    SUM(amount) as revenue,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY SUM(amount) DESC) as rank\n  FROM products\n  GROUP BY category, product_name\n)\nSELECT category, product_name, revenue\nFROM ranked_products\nWHERE rank <= 3\nORDER BY category, rank;\n```\n\n### Pattern 4: Period-over-Period Comparison\n\n**Use case:** Compare current period to previous period\n\n```sql\n-- Month-over-month comparison\nWITH monthly_sales AS (\n  SELECT\n    -- Extract year-month (use database-specific function)\n    [year_month_function] as year_month,\n    SUM(amount) as revenue\n  FROM orders\n  GROUP BY year_month\n)\nSELECT\n  year_month,\n  revenue as current_month_revenue,\n  LAG(revenue) OVER (ORDER BY year_month) as previous_month_revenue,\n  revenue - LAG(revenue) OVER (ORDER BY year_month) as revenue_change,\n  ROUND(100.0 * (revenue - LAG(revenue) OVER (ORDER BY year_month)) /\n        LAG(revenue) OVER (ORDER BY year_month), 2) as pct_change\nFROM monthly_sales\nORDER BY year_month;\n\n-- Year-over-year comparison\n-- Extract month and year components (use database-specific functions)\nSELECT\n  [month_function] as month,\n  SUM(CASE WHEN [year_function] = '2023' THEN amount ELSE 0 END) as revenue_2023,\n  SUM(CASE WHEN [year_function] = '2024' THEN amount ELSE 0 END) as revenue_2024,\n  SUM(CASE WHEN [year_function] = '2024' THEN amount ELSE 0 END) -\n  SUM(CASE WHEN [year_function] = '2023' THEN amount ELSE 0 END) as yoy_change\nFROM orders\nWHERE [year_function] IN ('2023', '2024')\nGROUP BY month\nORDER BY month;\n```\n\n### Pattern 5: Cohort Analysis\n\n**Use case:** Analyze behavior of groups defined by time of first action\n\n```sql\n-- Customer cohort by first purchase month\nWITH first_purchase AS (\n  SELECT\n    customer_id,\n    -- Extract year-month from first purchase (use database-specific function)\n    [year_month_function(MIN(order_date))] as cohort_month\n  FROM orders\n  GROUP BY customer_id\n)\nSELECT\n  fp.cohort_month,\n  COUNT(DISTINCT o.customer_id) as customers,\n  COUNT(o.order_id) as total_orders,\n  SUM(o.amount) as total_revenue,\n  ROUND(1.0 * COUNT(o.order_id) / COUNT(DISTINCT o.customer_id), 2) as orders_per_customer\nFROM first_purchase fp\nJOIN orders o ON fp.customer_id = o.customer_id\nGROUP BY fp.cohort_month\nORDER BY fp.cohort_month;\n```\n\n### Pattern 6: Distribution Analysis\n\n**Use case:** Understand distribution of values (percentiles, buckets)\n\n```sql\n-- Value distribution by buckets\nSELECT\n  CASE\n    WHEN amount < 25 THEN '$0-24'\n    WHEN amount < 50 THEN '$25-49'\n    WHEN amount < 100 THEN '$50-99'\n    WHEN amount < 250 THEN '$100-249'\n    ELSE '$250+'\n  END as amount_bucket,\n  COUNT(*) as order_count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct_of_orders,\n  SUM(amount) as total_revenue,\n  ROUND(100.0 * SUM(amount) / SUM(SUM(amount)) OVER (), 2) as pct_of_revenue\nFROM orders\nGROUP BY amount_bucket\nORDER BY MIN(amount);\n\n-- Percentile calculation (manual approach for SQLite)\nWITH ranked_orders AS (\n  SELECT\n    amount,\n    ROW_NUMBER() OVER (ORDER BY amount) as row_num,\n    COUNT(*) OVER () as total_count\n  FROM orders\n)\nSELECT\n  'P25' as percentile,\n  amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.25 AS INTEGER)\nUNION ALL\nSELECT 'P50', amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.50 AS INTEGER)\nUNION ALL\nSELECT 'P75', amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.75 AS INTEGER);\n```\n\n---\n\n## Database-Specific Implementation\n\nThis skill provides database-agnostic query guidance. For database-specific syntax:\n\n- **SQLite**: Use the `using-sqlite` skill for SQLite-specific date functions (STRFTIME), CLI invocation patterns, and SQLite idioms\n- **PostgreSQL, MySQL, etc.**: Consult database documentation for date/time functions, string operations, and window function syntax\n\n**Core SQL patterns** (SELECT, JOIN, GROUP BY, CTEs, window functions) are consistent across databases. **Function syntax** (date extraction, string manipulation) varies by database.\n\n---\n\n## Anti-Patterns to Avoid\n\n### Anti-Pattern 1: Premature Aggregation\n\n**Problem:** Aggregating too early loses detail needed for further analysis\n\n```sql\n-- Bad - can't drill down further\nSELECT\n  category,\n  SUM(amount) as total\nFROM orders\nGROUP BY category;\n\n-- Better - keep grain, aggregate in visualization/reporting layer\n-- Or use CTE to preserve both detail and summary\nWITH category_totals AS (\n  SELECT category, SUM(amount) as total\n  FROM orders\n  GROUP BY category\n)\nSELECT\n  o.*,\n  ct.total as category_total,\n  ROUND(100.0 * o.amount / ct.total, 2) as pct_of_category\nFROM orders o\nJOIN category_totals ct ON o.category = ct.category;\n```\n\n### Anti-Pattern 2: Ignoring NULL Semantics\n\n**Problem:** NULLs behave unexpectedly in comparisons and aggregates\n\n```sql\n-- Bad - NULL != NULL, so this misses NULL values\nSELECT * FROM orders WHERE status != 'completed';\n\n-- Good - explicitly handle NULLs\nSELECT * FROM orders\nWHERE status != 'completed' OR status IS NULL;\n\n-- Bad - COUNT(*) includes NULLs, COUNT(column) doesn't\nSELECT COUNT(column) FROM table;  -- May not match row count!\n\n-- Good - be explicit\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column) as non_null_count,\n  COUNT(*) - COUNT(column) as null_count\nFROM table;\n```\n\n### Anti-Pattern 3: Implicit Type Conversion\n\n**Problem:** SQLite's dynamic typing can cause unexpected behavior\n\n```sql\n-- Bad - comparing number to string\nSELECT * FROM orders WHERE amount > '100';  -- Works but risky\n\n-- Good - explicit types\nSELECT * FROM orders WHERE CAST(amount AS REAL) > 100.0;\n\n-- Bad - date comparison as strings might fail\nWHERE date_column > '2024-1-1'  -- Might not work as expected\n\n-- Good - proper date format\nWHERE date_column > '2024-01-01'  -- ISO format: YYYY-MM-DD\n```\n\n### Anti-Pattern 4: Over-Reliance on Subqueries\n\n**Problem:** Nested subqueries are hard to read and debug\n\n```sql\n-- Bad - deeply nested subqueries\nSELECT * FROM (\n  SELECT * FROM (\n    SELECT * FROM orders WHERE amount > 100\n  ) WHERE category = 'Electronics'\n) WHERE order_date > '2024-01-01';\n\n-- Good - use CTEs for readability\nWITH high_value_orders AS (\n  SELECT * FROM orders WHERE amount > 100\n),\nelectronics_orders AS (\n  SELECT * FROM high_value_orders WHERE category = 'Electronics'\n)\nSELECT * FROM electronics_orders\nWHERE order_date > '2024-01-01';\n```\n\n### Anti-Pattern 5: Cartesian Products\n\n**Problem:** Forgetting join conditions creates row explosion\n\n```sql\n-- Bad - missing join condition\nSELECT o.*, c.*\nFROM orders o, customers c;  -- Every order paired with every customer!\n\n-- Good - explicit join\nSELECT o.*, c.*\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id;\n```\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `writing-queries` component skill to develop SQL queries systematically, ensuring correctness and documentation.\n```\n\nWhen writing queries during analysis:\n1. Clarify what you're measuring (Phase 1)\n2. Design query logic before coding (Phase 2)\n3. Write clean, commented SQL (Phase 3)\n4. Verify results make sense (Phase 4)\n5. Document in numbered file (Phase 5)\n\nThis systematic approach prevents common SQL errors and ensures reproducible analysis."
              }
            ]
          },
          {
            "name": "autonomy",
            "description": "Enable AI agents to iteratively self-direct in pursuit of open-ended goals with state continuity across conversations through iteration journals",
            "source": "./plugins/autonomy",
            "category": null,
            "version": "1.0.0",
            "author": {
              "name": "Tilmon Engineering",
              "email": "team@tilmonengineering.com"
            },
            "install_commands": [
              "/plugin marketplace add tilmon-engineering/claude-skills",
              "/plugin install autonomy@tilmon-eng-skills"
            ],
            "signals": {
              "stars": 2,
              "forks": 0,
              "pushed_at": "2026-01-03T19:37:48Z",
              "created_at": "2025-12-12T15:12:26Z",
              "license": null
            },
            "commands": [
              {
                "name": "/analyze-branch",
                "description": "Analyze another branch to extract findings, decisions, and insights",
                "path": "plugins/autonomy/commands/analyze-branch.md",
                "frontmatter": {
                  "description": "Analyze another branch to extract findings, decisions, and insights",
                  "allowed-tools": "Skill, Read, Glob, Bash, Task, AskUserQuestion",
                  "model": "sonnet",
                  "argument-hint": "[branch-name] [search-description]"
                },
                "content": "# Analyze Branch\n\nAnalyze another branch's iteration journals to extract key findings, decisions, and insights.\n\nYou must invoke the `analyzing-branches` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:analyzing-branches\"\nargs: \"[branch-name] [search-description]\"\n```\n\n**Arguments:**\n- `branch-name` - The git branch to analyze (e.g., \"experiment-a\")\n- `search-description` - Free-text description of what to look for (e.g., \"pricing experiments and revenue optimization\")\n\nThe skill will:\n1. Use `git merge-base` to find where the branches diverged\n2. Extract iteration journals from the divergent commits\n3. Search for relevant content based on your criteria\n4. Produce a markdown report ready for inclusion in your journal\n\n**Use cases:**\n- Review work from parallel experiment branch\n- Extract lessons learned from concluded experiments\n- Compare approaches across different branches\n- Salvage good ideas from any branch\n\n**Example:**\n```\n/analyze-branch experiment-a \"API optimization attempts and performance improvements\"\n```\n\nThe resulting report can be copied into your current iteration's \"External Context Gathered\" section."
              },
              {
                "name": "/branch-status",
                "description": "Detailed status report for single autonomy branch",
                "path": "plugins/autonomy/commands/branch-status.md",
                "frontmatter": {
                  "description": "Detailed status report for single autonomy branch",
                  "allowed-tools": "Skill",
                  "model": "sonnet",
                  "argument-hint": "<branch-name>"
                },
                "content": "# Branch Status\n\nProvide comprehensive status report for a single autonomy branch showing complete iteration history, metrics progression, and current state.\n\nYou must invoke the `analyzing-branch-status` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:analyzing-branch-status\"\nargs: \"<branch-name>\"\n```\n\n**Arguments:**\n- `branch-name` (required) - Branch name to analyze. `autonomy/` prefix optional (will be added automatically if missing).\n\n**The skill will:**\n1. Normalize branch name (add `autonomy/` prefix if missing)\n2. Validate branch exists\n3. Dispatch branch-analyzer agent to read all commits on branch\n4. Agent generates Python script to analyze iteration timeline, metrics progression, status changes\n5. Produce comprehensive report with:\n   - Complete iteration list with dates and status\n   - Metrics over time (if applicable)\n   - Blocker history\n   - Current state and recommendations\n\n**Example usage:**\n```\n/branch-status experiment-a\n/branch-status autonomy/experiment-b\n```\n\n**Output includes:**\n- Iteration timeline (all iterations from first to latest)\n- Status changes over time\n- Metrics progression\n- Blocker history\n- Current state assessment\n- Recommended next actions\n\n**Note:** This command only operates on `autonomy/*` branches. For general iteration review on current branch, use `/review-progress`."
              },
              {
                "name": "/checkpoint-iteration",
                "description": "Save current iteration progress before conversation compaction or as interim checkpoint",
                "path": "plugins/autonomy/commands/checkpoint-iteration.md",
                "frontmatter": {
                  "description": "Save current iteration progress before conversation compaction or as interim checkpoint",
                  "allowed-tools": "Skill, Read, Write, Edit",
                  "model": "sonnet"
                },
                "content": "# Checkpoint Iteration\n\nYou must invoke the `checkpointing-an-iteration` skill to save current progress.\n\nUse the Skill tool:\n```\nskill: \"autonomy:checkpointing-an-iteration\"\n```\n\nThe skill will:\n1. Review conversation since iteration started\n2. Update journal entry with current work performed\n3. Capture decisions, artifacts, blockers so far\n4. Preserve state before potential context compaction\n\nUse this when:\n- Conversation getting long, compaction imminent\n- Reached natural pause point mid-iteration\n- Want to preserve important context discovered so far"
              },
              {
                "name": "/compare-branches",
                "description": "Compare two autonomy branches to show different approaches and outcomes",
                "path": "plugins/autonomy/commands/compare-branches.md",
                "frontmatter": {
                  "description": "Compare two autonomy branches to show different approaches and outcomes",
                  "allowed-tools": "Skill",
                  "model": "sonnet",
                  "argument-hint": "<branch-a> <branch-b>"
                },
                "content": "# Compare Branches\n\nCompare two autonomy branches to show differences in approaches, iterations, metrics, and outcomes.\n\nYou must invoke the `comparing-branches` skill to perform the comparison.\n\nUse the Skill tool:\n```\nskill: \"autonomy:comparing-branches\"\nargs: \"<branch-a> <branch-b>\"\n```\n\n**Arguments:**\n- `branch-a` (required) - First branch name. `autonomy/` prefix optional.\n- `branch-b` (required) - Second branch name. `autonomy/` prefix optional.\n\n**The skill will:**\n1. Normalize both branch names (add `autonomy/` prefix if missing)\n2. Validate both branches exist\n3. Use `git merge-base` to find where branches diverged\n4. Dispatch branch-analyzer agent to compare approaches\n5. Agent generates Python script for comparative analysis\n6. Produce report showing:\n   - Where branches diverged\n   - Iteration counts on each branch since divergence\n   - Comparative metrics (if applicable)\n   - Different approaches taken\n   - Outcomes on each branch\n\n**Example usage:**\n```\n/compare-branches experiment-a experiment-b\n/compare-branches autonomy/usage-pricing autonomy/flat-enterprise\n```\n\n**Output includes:**\n- Divergence point (common ancestor)\n- Iteration timeline comparison\n- Metrics trajectories (if applicable)\n- Status comparison\n- Different decisions made\n- Outcomes on each branch\n- Insights and recommendations\n\n**Note:** This command only operates on `autonomy/*` branches."
              },
              {
                "name": "/create-goal",
                "description": "Create a new open-ended goal for autonomy tracking",
                "path": "plugins/autonomy/commands/create-goal.md",
                "frontmatter": {
                  "description": "Create a new open-ended goal for autonomy tracking",
                  "allowed-tools": "Skill, Read, Write, Glob, Bash",
                  "model": "sonnet"
                },
                "content": "# Create Goal\n\nYou must invoke the `creating-a-goal` skill to set up a new open-ended goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:creating-a-goal\"\n```\n\nThe skill will:\n1. Prompt for goal statement and success criteria\n2. Generate goal directory name\n3. Create autonomy/[goal-name]/ directory structure\n4. Write goal.md with goal definition\n5. Prepare for first iteration\n\nAfter creating the goal, use `/start-iteration` to begin work."
              },
              {
                "name": "/end-iteration",
                "description": "End the current iteration, writing journal entry and summarizing work completed",
                "path": "plugins/autonomy/commands/end-iteration.md",
                "frontmatter": {
                  "description": "End the current iteration, writing journal entry and summarizing work completed",
                  "allowed-tools": "Skill, Read, Write, Glob, Bash, Task, TodoWrite",
                  "model": "sonnet"
                },
                "content": "# End Iteration\n\nYou must invoke the `ending-an-iteration` skill to finalize the current iteration.\n\nUse the Skill tool:\n```\nskill: \"autonomy:ending-an-iteration\"\n```\n\nThe skill will:\n1. Review the conversation to identify skills used, decisions made, and artifacts created\n2. Write a comprehensive journal entry documenting this iteration\n3. Update summary if needed (every 5 iterations)\n4. Prepare the state for the next iteration"
              },
              {
                "name": "/fork-iteration",
                "description": "Create new autonomy branch forked from current commit or specific iteration",
                "path": "plugins/autonomy/commands/fork-iteration.md",
                "frontmatter": {
                  "description": "Create new autonomy branch forked from current commit or specific iteration",
                  "allowed-tools": "Skill, AskUserQuestion, Bash",
                  "model": "sonnet",
                  "argument-hint": "[iteration] <strategy-name>"
                },
                "content": "# Fork Iteration\n\nCreate a new autonomy branch forked from current commit or any iteration tag in git history.\n\nYou must invoke the `forking-iteration` skill to perform the fork.\n\nUse the Skill tool:\n```\nskill: \"autonomy:forking-iteration\"\nargs: \"[iteration] <strategy-name>\"\n```\n\n**Arguments:**\n- `iteration` (optional) - Iteration number (NNNN format) to fork from. If provided, searches backward in current branch history for matching iteration tag. If omitted, forks from current HEAD.\n- `strategy-name` (required) - Name for new branch (kebab-case). Will become `autonomy/<strategy-name>`.\n\n**The skill will:**\n1. Resolve fork point (iteration tag or current HEAD)\n2. Validate fork point exists\n3. Checkout fork point\n4. Create new branch `autonomy/<strategy-name>`\n5. Report success with next steps\n\n**Example usage:**\n```\n# Fork from current commit\n/fork-iteration experiment-b\n\n# Fork from specific iteration in current branch history\n/fork-iteration 0015 experiment-b\n\n# Bootstrap autonomy workflow from non-autonomy branch\ngit checkout main\n/fork-iteration initial-strategy\n```\n\n**Note:** This creates the branch but does NOT start an iteration. After forking, run `/start-iteration` to begin work on the new branch."
              },
              {
                "name": "/fork-worktree",
                "description": "Create new autonomy branch with dedicated worktree for parallel agent workflows",
                "path": "plugins/autonomy/commands/fork-worktree.md",
                "frontmatter": {
                  "description": "Create new autonomy branch with dedicated worktree for parallel agent workflows",
                  "allowed-tools": "Skill, AskUserQuestion, Bash",
                  "model": "sonnet",
                  "argument-hint": "[iteration] <strategy-name>"
                },
                "content": "# Fork Worktree\n\nCreate a new autonomy branch with a dedicated worktree for running parallel Claude agents on different branches simultaneously.\n\nYou must invoke the `forking-worktree` skill to perform the fork.\n\nUse the Skill tool:\n```\nskill: \"autonomy:forking-worktree\"\nargs: \"[iteration] <strategy-name>\"\n```\n\n**Arguments:**\n- `iteration` (optional) - Iteration number (NNNN format) to fork from. If provided, searches backward in current branch history for matching iteration tag. If omitted, forks from current HEAD.\n- `strategy-name` (required) - Name for new branch and worktree (kebab-case). Will become `autonomy/<strategy-name>` branch and `.worktrees/autonomy/<strategy-name>/` directory.\n\n**The skill will:**\n1. Detect repository root (works from main repo or within worktrees)\n2. Resolve fork point (iteration tag or current HEAD)\n3. Validate fork point exists and worktree path is available\n4. Create new branch `autonomy/<strategy-name>` with worktree at `.worktrees/autonomy/<strategy-name>/`\n5. Report success with navigation instructions\n\n**Example usage:**\n```\n# Fork from current commit (works from anywhere)\n/fork-worktree experiment-b\n\n# Fork from specific iteration in current branch history\n/fork-worktree 0015 experiment-b\n\n# Fork from within another worktree (creates sibling worktree, not nested)\ncd .worktrees/autonomy/experiment-a\n/fork-worktree experiment-c  # Creates .worktrees/autonomy/experiment-c/ at root level\n```\n\n**Note:** This creates the branch AND worktree but does NOT start an iteration. After forking, navigate to the worktree directory and run `/start-iteration` to begin work:\n```bash\ncd .worktrees/autonomy/<strategy-name>\n/start-iteration\n```\n\n**Difference from `/fork-iteration`:**\n- `/fork-iteration` creates branch in current working directory (main repo)\n- `/fork-worktree` creates branch + isolated worktree directory for parallel work\n- Use worktrees when running multiple agents in parallel on different branches"
              },
              {
                "name": "/list-branches",
                "description": "Inventory autonomy branches with flexible sorting and grouping",
                "path": "plugins/autonomy/commands/list-branches.md",
                "frontmatter": {
                  "description": "Inventory autonomy branches with flexible sorting and grouping",
                  "allowed-tools": "Skill",
                  "model": "sonnet",
                  "argument-hint": "[optional-query]"
                },
                "content": "# List Branches\n\nDisplay inventory of all autonomy branches with user-specified sorting, grouping, and information display.\n\nYou must invoke the `listing-branches` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:listing-branches\"\nargs: \"[optional-query]\"\n```\n\n**Arguments:**\n- `optional-query` - Free-text description of how to sort, group, and what information to display (e.g., \"sort by most recent, group by status\", \"show branches updated in last 30 days with metrics\")\n- If no query provided, defaults to: sort by most recent update, show all branches\n\n**The skill will:**\n1. Parse user's query to understand desired sorting/grouping/information\n2. Dispatch branch-analyzer agent to read git data\n3. Agent generates Python script for computational analysis (never \"eyeball it\")\n4. Produce formatted markdown table with requested information\n\n**Example queries:**\n```\n/list-branches\n/list-branches sort by most recent, show only active\n/list-branches group by status, show metrics\n/list-branches show branches updated in last 30 days\n```\n\n**Note:** This command only operates on `autonomy/*` branches. For general iteration review, use `/review-progress`."
              },
              {
                "name": "/list-worktrees",
                "description": "List all autonomy worktrees with their status and location",
                "path": "plugins/autonomy/commands/list-worktrees.md",
                "frontmatter": {
                  "description": "List all autonomy worktrees with their status and location",
                  "allowed-tools": "Skill, Bash",
                  "model": "sonnet",
                  "argument-hint": ""
                },
                "content": "# List Worktrees\n\nList all autonomy worktrees showing their branch, location, and current HEAD commit.\n\nYou must invoke the `listing-worktrees` skill to perform the listing.\n\nUse the Skill tool:\n```\nskill: \"autonomy:listing-worktrees\"\nargs: \"\"\n```\n\n**The skill will:**\n1. Find all git worktrees in the repository\n2. Filter to autonomy worktrees (in `.worktrees/autonomy/`)\n3. Display formatted table with branch, path, HEAD commit, and lock status\n4. Provide navigation hints\n\n**Example output:**\n```\nAutonomy Worktrees:\n\nBranch                    Path                                      HEAD       Locked\nautonomy/experiment-a     .worktrees/autonomy/experiment-a          a1b2c3d\nautonomy/experiment-b     .worktrees/autonomy/experiment-b          d4e5f6g    🔒\nautonomy/cdn-optimize     .worktrees/autonomy/cdn-optimize          h7i8j9k\n\nTotal: 3 autonomy worktrees\n\nTo navigate to a worktree:\n  cd .worktrees/autonomy/<strategy-name>\n\nTo remove a worktree:\n  /remove-worktree <strategy-name>\n```\n\n**Note:** This only lists worktrees, not branches. To see all autonomy branches (including those without worktrees), use `/list-branches`."
              },
              {
                "name": "/remove-worktree",
                "description": "Safely remove an autonomy worktree while preserving the branch and its history",
                "path": "plugins/autonomy/commands/remove-worktree.md",
                "frontmatter": {
                  "description": "Safely remove an autonomy worktree while preserving the branch and its history",
                  "allowed-tools": "Skill, AskUserQuestion, Bash",
                  "model": "sonnet",
                  "argument-hint": "[--force] <strategy-name>"
                },
                "content": "# Remove Worktree\n\nSafely remove an autonomy worktree directory while preserving the autonomy branch, commits, and iteration tags.\n\nYou must invoke the `removing-worktree` skill to perform the removal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:removing-worktree\"\nargs: \"[--force] <strategy-name>\"\n```\n\n**Arguments:**\n- `--force` (optional) - Skip uncommitted changes check and force removal\n- `strategy-name` (required) - Name of worktree to remove (without `autonomy/` prefix)\n\n**The skill will:**\n1. Detect repository root (works from anywhere)\n2. Validate worktree exists at `.worktrees/autonomy/<strategy-name>/`\n3. Check for uncommitted changes (unless `--force`)\n4. Remove worktree directory\n5. Prune git worktree metadata\n6. Report success\n\n**Example usage:**\n```\n# Safe removal (fails if uncommitted changes)\n/remove-worktree experiment-b\n\n# Force removal (discards uncommitted changes)\n/remove-worktree --force experiment-b\n\n# Remove from within another worktree\ncd .worktrees/autonomy/experiment-a\n/remove-worktree experiment-b  # Removes sibling worktree\n```\n\n**What gets removed:**\n- Worktree directory: `.worktrees/autonomy/<strategy-name>/`\n- Git worktree metadata\n\n**What persists:**\n- Branch `autonomy/<strategy-name>` and all commits\n- All iteration tags (`autonomy/<strategy-name>/iteration-NNNN`)\n- Git history and journal commits\n- Can checkout branch later or create new worktree for it\n\n**Manual removal:**\nIf automated removal fails:\n```bash\ngit worktree remove --force .worktrees/autonomy/<strategy-name>\ngit worktree prune\n```"
              },
              {
                "name": "/review-progress",
                "description": "Review progress toward an open-ended goal by reading iteration journals",
                "path": "plugins/autonomy/commands/review-progress.md",
                "frontmatter": {
                  "description": "Review progress toward an open-ended goal by reading iteration journals",
                  "allowed-tools": "Skill, Read, Glob, Task, TodoWrite",
                  "model": "sonnet"
                },
                "content": "# Review Progress\n\nYou must invoke the `reviewing-progress` skill to assess progress toward the current goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:reviewing-progress\"\n```\n\nThe skill will:\n1. Load and summarize all iterations for the goal\n2. Present progress metrics and completed work\n3. Identify current blockers and open questions\n4. Suggest next steps for continuing the goal"
              },
              {
                "name": "/slime",
                "description": "Set up slime mold strategy for parallel exploration with autonomy branches",
                "path": "plugins/autonomy/commands/slime.md",
                "frontmatter": {
                  "description": "Set up slime mold strategy for parallel exploration with autonomy branches",
                  "allowed-tools": "Skill",
                  "model": "sonnet"
                },
                "content": "# Slime Mold Strategy Setup\n\nYou must invoke the `slime-strategy` skill to set up the complete slime mold exploration workflow.\n\nUse the Skill tool:\n```\nskill: \"autonomy:slime-strategy\"\n```\n\n**What this command does:**\n\nIf no autonomy goal exists yet:\n1. Uses `creating-a-goal` skill to define goal and set up `autonomy/[goal-name]/` directory\n2. Creates/updates `autonomy/CLAUDE.md` with slime mold strategy documentation\n3. Uses `forking-iteration` skill to create initial `autonomy/[goal-name]` branch\n4. Creates `iteration-0000-YYYY-MM-DD.md` as baseline setup journal\n5. Makes git commit with tag `autonomy/[goal-name]/iteration-0000`\n\nIf autonomy goal already exists:\n- Updates `autonomy/CLAUDE.md` to ensure slime mold strategy documentation is current\n- Skips goal creation, branching, and iteration 0000 (idempotent behavior)\n\n**After running `/slime`:**\n- Use `/start-iteration` to begin iteration 0001 (first real work iteration)\n- Use `/fork-iteration <strategy-name>` to create additional exploration branches\n- Use `/analyze-branch <branch> <search>` to cross-pollinate learnings between branches\n- Use `/compare-branches <branch-a> <branch-b>` to understand divergent approaches"
              },
              {
                "name": "/start-iteration",
                "description": "Start a new iteration for an open-ended goal, loading context from previous iterations",
                "path": "plugins/autonomy/commands/start-iteration.md",
                "frontmatter": {
                  "description": "Start a new iteration for an open-ended goal, loading context from previous iterations",
                  "allowed-tools": "Skill, Read, Write, Glob, Bash, Task, TodoWrite",
                  "model": "sonnet"
                },
                "content": "# Start Iteration\n\nYou must invoke the `starting-an-iteration` skill to begin a new iteration for an open-ended goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:starting-an-iteration\"\n```\n\nThe skill will:\n1. Detect if a goal already exists or prompt to create a new one\n2. Load context from recent iterations\n3. Present the current state and open questions\n4. Prepare you to continue working toward the goal"
              }
            ],
            "skills": [
              {
                "name": "analyzing-branch-status",
                "description": "Use when user wants detailed status report for single autonomy branch including iteration timeline and metrics progression",
                "path": "plugins/autonomy/skills/analyzing-branch-status/SKILL.md",
                "frontmatter": {
                  "name": "analyzing-branch-status",
                  "description": "Use when user wants detailed status report for single autonomy branch including iteration timeline and metrics progression"
                },
                "content": "# Analyzing Branch Status\n\n## Overview\n\nProvide comprehensive status report for a single autonomy branch by analyzing all journal commits and extracting timeline, metrics, and state evolution.\n\n**Core principle:** Dispatch branch-analyzer agent for computational analysis. Never manually review commits.\n\n## When to Use\n\nUse this skill when:\n- User runs `/branch-status` command\n- User wants deep dive into one branch\n- User wants to see metrics progression over time\n- User wants blocker history for a branch\n\n**DO NOT use for:**\n- Listing all branches (use listing-branches instead)\n- Comparing two branches (use comparing-branches instead)\n- Current branch review (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse and validate | Normalize branch name, check exists | Bash |\n| 2. Dispatch agent | Send branch to branch-analyzer | Task |\n| 3. Present report | Display comprehensive status | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Branch Name\n\nNormalize and validate the branch name:\n\n**Normalize:**\n```bash\n# If user provided name without autonomy/ prefix, add it\nif [[ \"$branch_name\" != autonomy/* ]]; then\n  branch_name=\"autonomy/$branch_name\"\nfi\n```\n\n**Validate exists:**\n```bash\n# Check if branch exists (local or remote)\nif ! git branch -a | grep -q \"$branch_name\\$\"; then\n  echo \"Error: Branch '$branch_name' not found among autonomy branches.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  echo \"\"\n  echo \"Run '/list-branches' to see all autonomy branches.\"\n  exit 1\nfi\n```\n\n**Validate is autonomy branch:**\n```bash\n# Verify it's an autonomy branch\nif [[ \"$branch_name\" != autonomy/* ]]; then\n  echo \"Error: Branch '$branch_name' is not an autonomy branch.\"\n  echo \"\"\n  echo \"These commands only operate on autonomy/* branches.\"\n  echo \"\"\n  echo \"To analyze this branch's iterations:\"\n  echo \"- Run '/review-progress' (works on any branch)\"\n  echo \"\"\n  echo \"To convert to autonomy workflow:\"\n  echo \"- Run '/fork-iteration <strategy-name>' to create autonomy branch from current state\"\n  exit 1\nfi\n```\n\n### Step 2: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with detailed instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"Analyze autonomy branch '$branch_name' and provide comprehensive status report.\n\nTasks:\n1. Read all journal commits on branch (commits starting with 'journal: ')\n2. Parse each commit message for:\n   - Iteration number\n   - Date\n   - Status (active/blocked/concluded/dead-end)\n   - Metrics\n   - Blockers\n   - Next steps\n3. Generate Python script to analyze:\n   - Complete iteration timeline (chronological)\n   - Status changes over time\n   - Metrics progression (if metrics exist)\n   - Blocker history\n   - Current state from most recent commit\n4. Execute Python script\n5. Output comprehensive markdown report\n\nUse computational methods (Python scripts), do not eyeball the analysis.\n\nReport format:\n- Iteration Timeline section\n- Metrics Over Time section (if metrics exist)\n- Status Evolution section\n- Blocker History section\n- Current State and Recommendations section\"\n```\n\n**Agent will:**\n1. List all commits on branch: `git log autonomy/<branch-name>`\n2. Filter for journal commits (start with \"journal: \")\n3. Parse each commit message metadata\n4. Generate Python script for analysis\n5. Execute script to produce timeline, metrics, blockers\n6. Return formatted markdown report\n\n### Step 3: Present Report\n\nDisplay agent's comprehensive report to user.\n\n**Example output format:**\n```markdown\n# Branch Status: autonomy/experiment-a\n\n**Current Status:** blocked\n**Latest Iteration:** 0028\n**Last Updated:** 2026-01-02\n**Total Iterations:** 28\n\n---\n\n## Iteration Timeline\n\n| Iteration | Date | Status | Summary |\n|-----------|------|--------|---------|\n| 0001 | 2025-11-15 | active | Initial setup of usage-based pricing model |\n| 0002 | 2025-11-16 | active | Implemented tier calculations |\n| ... | ... | ... | ... |\n| 0027 | 2026-01-01 | active | Stripe API integration progress |\n| 0028 | 2026-01-02 | blocked | Awaiting Stripe webhook documentation |\n\n---\n\n## Metrics Over Time\n\nMRR progression:\n- Iteration 0001: $45k (baseline)\n- Iteration 0010: $52k (+15.6%)\n- Iteration 0020: $58k (+28.9%)\n- Iteration 0028: $62k (+37.8%)\n\nBuild time:\n- Iteration 0015: 5.2min (baseline)\n- Iteration 0028: 3.2min (-38.5%)\n\n---\n\n## Status Evolution\n\n- Iterations 0001-0027: active (normal progression)\n- Iteration 0028: blocked (current)\n\n---\n\n## Blocker History\n\n**Current Blockers (Iteration 0028):**\n- Stripe webhook integration unclear: need updated API docs\n- Finance team approval pending for pricing structure\n\n**Resolved Blockers:**\n- Iteration 0015: Build performance (resolved at 0016)\n- Iteration 0022: User feedback collection (resolved at 0024)\n\n---\n\n## Current State and Recommendations\n\n**Where we are:**\nBranch has made substantial progress over 28 iterations. MRR increased 37.8%, build time reduced 38.5%. Currently blocked on external dependencies.\n\n**Recommended actions:**\n1. Escalate Stripe API documentation request\n2. Schedule finance team review meeting\n3. Consider parallel work on pricing page UI while blocked\n4. Review iteration 0027 for alternative integration approaches\n\n**Branch health:** Active exploration, currently blocked but making good progress\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY analyzes `autonomy/*` branches:\n- Validates branch has `autonomy/` prefix\n- Will not analyze non-autonomy branches\n- For general iteration review, user should use `/review-progress`\n\n### Computational Analysis Required\n\n**DO NOT:**\n- Manually read through commits\n- \"Eyeball\" metrics progression\n- Guess at patterns or trends\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Read-Only Operations\n\nAll analysis happens via git commands:\n- Never checkout the branch\n- Read commits via `git log <branch-name>`\n- Branch-analyzer uses read-only operations\n- No modifications to any files\n\n### Metrics May Not Exist\n\nNot all goals have quantitative metrics:\n- Some goals are qualitative\n- Metrics section may be \"None\" in commit messages\n- Report should handle missing metrics gracefully\n- Don't force metrics where they don't exist\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll read the journal files to get status\" | NO. Read commit messages via git log. Don't checkout branch. |\n| \"Only 10 iterations, I can review manually\" | NO. Always dispatch branch-analyzer for computational analysis. |\n| \"Branch is on remote, I can't analyze it\" | YES YOU CAN. Use git log origin/branch-name to read commits. |\n| \"No metrics in some commits, report is incomplete\" | OK. Not all iterations have metrics. Report what exists. |\n| \"I'll checkout branch to read latest journal\" | NO. Read commit message via git log. Never checkout. |\n\n## After Analyzing\n\nOnce analysis is complete:\n- Report displayed to user\n- No files created or modified\n- User can fork from any iteration: `/fork-iteration <iteration> <strategy-name>`\n- User can compare with another branch: `/compare-branches <branch-a> <branch-b>`"
              },
              {
                "name": "analyzing-branches",
                "description": "Use when analyzing another branch's iteration journals to extract findings, decisions, and insights from divergent work",
                "path": "plugins/autonomy/skills/analyzing-branches/SKILL.md",
                "frontmatter": {
                  "name": "analyzing-branches",
                  "description": "Use when analyzing another branch's iteration journals to extract findings, decisions, and insights from divergent work"
                },
                "content": "# Analyzing Branches\n\n## Overview\n\nAnalyze another branch's iteration journals to extract key findings, decisions, and insights from work that diverged from the current branch.\n\n**Core principle:** Learn from parallel explorations. Extract valuable insights from any branch, regardless of success or failure.\n\n## When to Use\n\nUse this skill when:\n- User runs `/analyze-branch` command\n- Want to review work from parallel experiment branch\n- Need to extract lessons from concluded experiments\n- Comparing approaches across different branches\n- Looking for salvageable ideas from any branch\n\n**DO NOT use for:**\n- Analyzing current branch history (use `/review-progress` instead)\n- Comparing file changes (use `git diff` directly)\n- When branches have no autonomy iterations\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract branch name and search criteria | Manual |\n| 2. Find divergence | Use git merge-base to find common ancestor | Bash |\n| 3. Extract iterations | Get autonomy tags from divergent commits | Bash |\n| 4. Read journals | Read iteration files from target branch | Bash, Read |\n| 5. Search and filter | Find relevant content based on criteria | Manual |\n| 6. Generate report | Create markdown summary | Direct output |\n\n## Process\n\n### Step 1: Parse Arguments\n\nExtract branch name and search criteria from command arguments:\n\n```\nArguments format: \"[branch-name] [search-description]\"\nExample: \"experiment-a pricing experiments and revenue optimization\"\n```\n\n**Parse:**\n- First argument: Branch name (e.g., \"experiment-a\")\n- Remaining arguments: Search criteria as free text\n\n**Verify goal exists:**\n```bash\n# Use Glob to find goal\npattern: \"autonomy/*/goal.md\"\n```\n\nIf no goal found:\n```\n\"No autonomy goal found in this project. Branch analysis requires an autonomy goal.\"\n```\nStop here.\n\nExtract goal name from path for use in later steps.\n\n### Step 2: Find Divergence Point\n\nUse git to find where branches diverged:\n\n```bash\n# Get current branch name\ncurrent_branch=$(git branch --show-current)\n\n# Find common ancestor between current and target branch\nmerge_base=$(git merge-base \"$current_branch\" \"$target_branch\" 2>&1)\n\n# Check if command succeeded\nif [ $? -ne 0 ]; then\n  # Error handling - see Step 2a\nfi\n```\n\n**Step 2a: Handle Git Errors**\n\nIf `git merge-base` fails, determine cause and prompt user:\n\n**Error: Branch not found**\n```markdown\nError: Branch '$target_branch' does not exist.\n\nAvailable branches:\n$(git branch -a | sed 's/^..//; s/ -> .*//')\n\nPlease verify the branch name and try again.\n```\n\n**Error: No common ancestor**\n```markdown\nWarning: Cannot find common ancestor between current branch and '$target_branch'.\n\nThis may mean:\n- Branches have completely independent histories\n- Branch was rebased and history was rewritten\n\nOptions:\n1. Analyze entire branch history (may include duplicate work)\n2. Specify iteration range manually\n3. Specify common ancestor commit manually\n\nWhich would you like?\n```\n\nUse AskUserQuestion to let user choose:\n- Option 1: Set `merge_base=\"\"` and analyze all iterations\n- Option 2: Prompt for iteration range, skip git operations\n- Option 3: Prompt for commit hash, use as merge_base\n\n### Step 3: Extract Divergent Iterations\n\nFind autonomy iteration tags on target branch after divergence:\n\n```bash\n# Get all autonomy iteration tags on target branch\nif [ -n \"$merge_base\" ]; then\n  # Analyze only divergent work\n  commit_range=\"$merge_base..$target_branch\"\nelse\n  # Analyze entire branch (fallback)\n  commit_range=\"$target_branch\"\nfi\n\n# Extract iteration numbers from tags\niterations=$(git log \"$commit_range\" \\\n  --pretty=format:\"%D\" \\\n  | tr ',' '\\n' \\\n  | grep \"tag: autonomy/iteration-\" \\\n  | sed 's/.*autonomy\\/iteration-//' \\\n  | sort -n)\n\n# Count iterations found\niteration_count=$(echo \"$iterations\" | wc -w)\n```\n\n**If no iterations found:**\n```markdown\nNo autonomy iteration tags found on branch '$target_branch' after divergence.\n\nPossible reasons:\n- Branch hasn't used autonomy plugin\n- All iterations are before the divergence point\n- Iterations exist but weren't committed with git integration\n\nWould you like to:\n1. Analyze entire branch (all iterations)\n2. Specify iteration range manually\n```\n\nUse AskUserQuestion for recovery.\n\n### Step 4: Read Journal Files\n\nFor each iteration found, read the journal content:\n\n```bash\n# For each iteration number\nfor iter in $iterations; do\n  # Find journal file with this iteration number\n  # Format: iteration-NNNN-YYYY-MM-DD.md\n  journal_file=$(git ls-tree -r --name-only \"$target_branch\" \\\n    \"autonomy/$goal_name/\" \\\n    | grep \"iteration-$(printf '%04d' $iter)-\")\n\n  if [ -n \"$journal_file\" ]; then\n    # Read file content from target branch\n    journal_content=$(git show \"$target_branch:$journal_file\")\n\n    # Store for analysis in next step\n  fi\ndone\n```\n\n**Build iteration data structure:**\nFor each iteration, extract:\n- Iteration number\n- Date (from filename)\n- Beginning State section\n- Iteration Intention section\n- Work Performed subsections\n- Ending State section\n- Full content for searching\n\n### Step 5: Search and Filter\n\nApply search criteria to find relevant iterations and content:\n\n**Search strategy:**\n```bash\n# Convert search criteria to grep pattern\n# Example: \"pricing experiments\" → \"pricing|experiments\"\nsearch_pattern=$(echo \"$search_criteria\" | tr ' ' '|')\n\n# For each iteration's content\n# Score by relevance (number of search term matches)\n# Prioritize sections: Key Decisions, Ending State, Reasoning & Strategy\n```\n\n**Extract relevant sections:**\n- **Key Findings:** Extract from Ending State + Work Performed\n- **Decisions:** Extract from Key Decisions Made subsection\n- **Ideas/Experiments:** Extract from Reasoning & Strategy Changes\n- **Outcomes:** Extract from Ending State\n\n**If search finds nothing:**\n```markdown\nNo iterations matched search criteria: \"$search_criteria\"\n\nAnalyzed $iteration_count iterations on '$target_branch'.\n\nWould you like to:\n1. Broaden search (show all iterations)\n2. Refine search criteria\n```\n\nUse AskUserQuestion to offer alternatives.\n\n### Step 6: Generate Report\n\nProduce markdown report formatted for journal inclusion:\n\n```markdown\n## Analysis of Branch: $target_branch\n\n**Analyzed range:** Iterations $first_iter-$last_iter (diverged from iteration $divergence_iter)\n**Search focus:** $search_criteria\n**Common ancestor:** $merge_base\n**Branch status:** [Active/Merged/Concluded - from latest iteration if stated]\n\n---\n\n### Key Findings\n\n[For each significant finding extracted from journals:]\n- **Iteration NNNN:** [Finding or insight]\n  - **Context:** [What problem was being addressed]\n  - **Approach taken:** [How it was implemented or explored]\n  - **Outcome:** [Results from Ending State]\n  - **Reference:** `$target_branch @ iteration-NNNN`\n\n---\n\n### Decisions and Rationale\n\n[For each major decision from Key Decisions Made:]\n- **Iteration NNNN:** [Decision]\n  - **Rationale:** [Why this choice was made]\n  - **Outcome:** [Result - success/failure/inconclusive/ongoing]\n  - **Takeaway:** [What this tells us]\n\n---\n\n### Ideas and Experiments\n\n[For each experimental approach from Reasoning & Strategy Changes:]\n- **Iteration NNNN:** [What was tried]\n  - **Results:** [What was learned]\n  - **Status:** [Validated/Invalidated/Needs more testing/Inconclusive]\n  - **Applicability:** [Could this apply to current branch?]\n\n---\n\n### Timeline of Branch Activity\n\n| Iteration | Date | Summary | Status |\n|-----------|------|---------|--------|\n| NNNN | YYYY-MM-DD | [From Iteration Intention or Ending State] | [From Ending State] |\n| NNNN | YYYY-MM-DD | [Summary] | [Status] |\n\n---\n\n**Analysis Summary:** [Neutral assessment of what was learned, key patterns, notable outcomes]\n```\n\n**Output to user:**\nDisplay the complete report. User can copy relevant sections into their current journal's \"External Context Gathered\" section.\n\n## Important Notes\n\n### Neutral Framing\n\n**Do NOT assume branch status:**\n- Branch may be active, merged, concluded, or abandoned\n- Don't label as \"failed\" unless journal explicitly states that\n- Let the journal entries speak for themselves\n- Use neutral language: \"findings\", \"outcomes\", not \"failures\"\n\n### Git Safety\n\n**Read-only operations:**\n- Never checkout or switch branches\n- Use `git show` to read historical files\n- Don't modify target branch\n- Don't create commits\n\n### Search Flexibility\n\n**Interpreting search criteria:**\n- Free-text is intentionally loose\n- Don't require exact matches\n- Look for semantic relevance\n- Prioritize iteration sections mentioned in criteria\n\n**If criteria is very specific:**\n- \"pricing tier experiments\" → Focus on those specific words\n- \"revenue optimization\" → Include related terms (profit, MRR, monetization)\n\n**If criteria is broad:**\n- \"general findings\" → Show all significant content\n- \"lessons learned\" → Focus on Ending State assessments\n\n### Report Quality\n\n**Characteristics of good report:**\n- **Self-contained:** Readable without accessing original branch\n- **Actionable:** Clear what to do with findings\n- **Referenced:** Links to specific iterations for deep dives\n- **Concise:** Extract signal, avoid noise\n- **Balanced:** Show successes and failures equally\n\n### Performance Considerations\n\n**For branches with many iterations:**\n- Reading 50+ iterations may be slow\n- Consider limiting to most recent N iterations if search is broad\n- User can refine criteria to narrow scope\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Branch is abandoned, so it failed\" | NO. Don't assume status. Read journals for actual outcomes. |\n| \"I'll modify the target branch\" | NO. Read-only operations only. Never checkout or modify target. |\n| \"Search found nothing, report empty\" | NO. Offer to broaden search or show all iterations. |\n| \"I'll analyze current branch\" | NO. Use /review-progress for current branch analysis. |\n| \"No common ancestor means I should fail\" | NO. Offer alternatives: analyze all, manual range, manual ancestor. |\n| \"Report should only include successes\" | NO. Balanced view - show what worked AND what didn't. |\n\n## After Analyzing\n\nOnce analysis is complete:\n- Report displayed to user\n- User copies relevant sections into current journal\n- No files created or modified by this skill\n- Can analyze multiple branches in same conversation\n- Skill usage logged in journal's \"Skills & Workflows Used\" section"
              },
              {
                "name": "checkpointing-an-iteration",
                "description": "Use when saving current iteration progress mid-conversation, before context compaction, or at interim pause points",
                "path": "plugins/autonomy/skills/checkpointing-an-iteration/SKILL.md",
                "frontmatter": {
                  "name": "checkpointing-an-iteration",
                  "description": "Use when saving current iteration progress mid-conversation, before context compaction, or at interim pause points"
                },
                "content": "# Checkpointing an Iteration\n\n## Overview\n\nSave current iteration progress by updating the journal entry with work performed so far, preserving state before potential context loss.\n\n**Core principle:** Protect important context from compaction. Update journal incrementally during long conversations.\n\n## When to Use\n\nUse this skill when:\n- User runs `/checkpoint-iteration` command\n- Conversation is getting long, compaction may be imminent\n- Reached natural pause point mid-iteration\n- Discovered important context that must be preserved\n- Want to save progress before taking risky action\n\n**DO NOT use for:**\n- Ending the iteration (use ending-an-iteration instead)\n- Starting iteration (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Find journal file | Locate current iteration journal | Glob |\n| 2. Review conversation | Extract work performed so far | Manual review |\n| 3. Read current journal | Get existing content | Read |\n| 4. Update Work Performed | Append new findings | Edit |\n| 5. Announce checkpoint | Confirm save | Direct output |\n\n## Process\n\n### Step 1: Find Journal File\n\nLocate the current iteration's journal:\n\n```bash\n# Use Glob to find journal files\npattern: \"autonomy/*/iteration-*.md\"\n```\n\nSort by filename (iteration number) and identify the most recent one. This should be today's date or recent.\n\n**If no journal file found:**\n```\n\"No active iteration journal found.\n\nThis usually means `/start-iteration` wasn't run yet. Start an iteration first before checkpointing.\"\n```\nStop here.\n\n**If journal file found:**\nExtract the full path and proceed.\n\n### Step 2: Review Conversation\n\nReview the conversation since iteration started to extract:\n\n**Skills & Workflows Used (so far):**\n- Scan for `<invoke name=\"Skill\">` tool calls\n- Note which skills used and for what purpose\n\n**Key Decisions Made (so far):**\n- Identify major choices and rationale\n- Note alternatives considered\n\n**Artifacts Created/Modified (so far):**\n- Files created or changed\n- Git commits made\n- Pull requests opened\n\n**External Context Gathered (so far):**\n- Web research findings\n- User feedback\n- Documentation consulted\n\n**Reasoning & Strategy Changes (so far):**\n- Why certain approaches chosen\n- Where strategy pivoted\n\n**Blockers Encountered (so far):**\n- What's preventing progress\n- Dependencies identified\n\n**Open Questions (so far):**\n- What needs resolution\n- Decisions deferred\n\n### Step 3: Read Current Journal\n\nRead the existing journal file to see what's already documented:\n\n```bash\n# Use Read tool\nfile: \"autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\"\n```\n\nThe journal will have:\n- **Metadata header** (# Iteration NNNN - YYYY-MM-DD)\n- **Beginning State** section (from starting-an-iteration)\n- **Iteration Intention** section (from starting-an-iteration)\n- **Work Performed** section (may be partially filled from previous checkpoint, or empty)\n- **Ending State** section (will be empty - that's for ending-an-iteration)\n- **Iteration Metadata** section (will be empty - that's for ending-an-iteration)\n\n### Step 4: Update Work Performed Section\n\nUpdate the \"Work Performed\" section with current findings from Step 2.\n\n**If Work Performed section is empty:**\nReplace the empty section with full content:\n\n```markdown\n## Work Performed\n\n### Skills & Workflows Used\n[From Step 2 review]\n\n### Key Decisions Made\n[From Step 2 review]\n\n### Artifacts Created/Modified\n[From Step 2 review]\n\n### External Context Gathered\n[From Step 2 review]\n\n### Reasoning & Strategy Changes\n[From Step 2 review]\n\n### Blockers Encountered\n[From Step 2 review]\n\n### Open Questions\n[From Step 2 review]\n```\n\n**If Work Performed section already has content:**\nMerge new findings with existing:\n- Add new skills to \"Skills & Workflows Used\" list\n- Add new decisions to \"Key Decisions Made\"\n- Add new artifacts to \"Artifacts Created/Modified\"\n- Append new findings to other sections\n- Preserve all existing content\n\nUse Edit tool to update the file.\n\n### Step 5: Announce Checkpoint\n\nReport to user:\n\n```markdown\n**Checkpoint saved for iteration [N]**\n\nJournal updated: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\n## Checkpoint Summary\n- **Skills used:** [Count] skills/workflows\n- **Decisions made:** [Count] key decisions\n- **Artifacts created:** [Count] files/commits\n- **Blockers:** [Count] blockers identified\n- **Open questions:** [Count] questions pending\n\nContext preserved. Safe to continue or compact conversation.\n\n---\n\nTo finalize this iteration, use `/end-iteration` when ready.\n```\n\n## Important Notes\n\n### Checkpoint vs. End Iteration\n\n**Checkpoint:**\n- Updates journal mid-iteration\n- Iteration continues after checkpoint\n- Can checkpoint multiple times per iteration\n- Doesn't update summary.md\n- Doesn't finalize \"Ending State\"\n\n**End Iteration:**\n- Finalizes journal entry\n- Iteration concludes\n- Writes final \"Ending State\"\n- Updates summary.md if needed (every 5 iterations)\n- Conversation ends or new iteration starts\n\n### Multiple Checkpoints\n\nIt's fine to checkpoint multiple times:\n- Each checkpoint merges with previous content\n- Later checkpoints add to earlier ones\n- All information accumulated in journal\n\n### What Gets Preserved\n\nCheckpointing preserves:\n- Work done so far\n- Decisions and reasoning\n- Blockers discovered\n- Context gathered\n\nCheckpointing does NOT capture:\n- Future plans (that's in \"Ending State\" at iteration end)\n- Final assessment (that's for ending-an-iteration)\n- Complete iteration story (still in progress)\n\n### Merging Strategy\n\nWhen updating Work Performed with existing content:\n- **Append** new items to lists (don't overwrite)\n- **Preserve** all existing information\n- **Deduplicate** if same item mentioned twice\n- **Maintain** chronological order within sections\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create new journal file for checkpoint\" | NO. Update existing journal from starting-an-iteration. |\n| \"I'll overwrite existing Work Performed section\" | NO. Merge new content with existing. |\n| \"I'll write Ending State during checkpoint\" | NO. That's only for ending-an-iteration. |\n| \"Checkpoint means iteration is over\" | NO. Iteration continues after checkpoint. |\n| \"I'll skip checkpoint if conversation isn't long\" | NO. Checkpoint anytime user requests it. |\n\n## After Checkpointing\n\nOnce checkpoint is saved:\n- Journal updated with current progress\n- Context preserved against compaction\n- Iteration continues normally\n- Can checkpoint again later if needed\n- When ready to conclude, use `/end-iteration`"
              },
              {
                "name": "comparing-branches",
                "description": "Use when user wants to compare two autonomy branches to see different approaches, metrics, and outcomes",
                "path": "plugins/autonomy/skills/comparing-branches/SKILL.md",
                "frontmatter": {
                  "name": "comparing-branches",
                  "description": "Use when user wants to compare two autonomy branches to see different approaches, metrics, and outcomes"
                },
                "content": "# Comparing Branches\n\n## Overview\n\nCompare two autonomy branches to show where they diverged, how their approaches differ, and what outcomes each achieved.\n\n**Core principle:** Use git merge-base to find divergence, then dispatch branch-analyzer for computational comparison.\n\n## When to Use\n\nUse this skill when:\n- User runs `/compare-branches` command\n- User wants to see differences between two exploration branches\n- User wants to understand alternative approaches\n- User wants to compare metrics/outcomes across branches\n\n**DO NOT use for:**\n- Analyzing single branch (use analyzing-branch-status instead)\n- Listing all branches (use listing-branches instead)\n- Current branch review (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse and validate | Normalize branch names, check both exist | Bash |\n| 2. Find divergence | Use git merge-base to find common ancestor | Bash |\n| 3. Dispatch agent | Send both branches to branch-analyzer | Task |\n| 4. Present comparison | Display comparative report | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Branch Names\n\nExtract and normalize both branch names:\n\n**Parse arguments:**\n```\nargs = \"<branch-a> <branch-b>\"\nSplit on whitespace:\n  branch_a = first word\n  branch_b = second word\n\nIf not exactly 2 words:\n  Error: \"Usage: /compare-branches <branch-a> <branch-b>\"\n```\n\n**Normalize:**\n```bash\n# Add autonomy/ prefix if missing\nif [[ \"$branch_a\" != autonomy/* ]]; then\n  branch_a=\"autonomy/$branch_a\"\nfi\n\nif [[ \"$branch_b\" != autonomy/* ]]; then\n  branch_b=\"autonomy/$branch_b\"\nfi\n```\n\n**Validate both exist:**\n```bash\n# Check branch A\nif ! git branch -a | grep -q \"$branch_a\\$\"; then\n  echo \"Error: Branch '$branch_a' not found.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  exit 1\nfi\n\n# Check branch B\nif ! git branch -a | grep -q \"$branch_b\\$\"; then\n  echo \"Error: Branch '$branch_b' not found.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  exit 1\nfi\n```\n\n**Validate both are autonomy branches:**\n```bash\nfor branch in \"$branch_a\" \"$branch_b\"; do\n  if [[ \"$branch\" != autonomy/* ]]; then\n    echo \"Error: Branch '$branch' is not an autonomy branch.\"\n    echo \"\"\n    echo \"These commands only operate on autonomy/* branches.\"\n    exit 1\n  fi\ndone\n```\n\n### Step 2: Find Divergence Point\n\nUse git to find where the branches diverged:\n\n```bash\n# Find common ancestor\nmerge_base=$(git merge-base \"$branch_a\" \"$branch_b\" 2>&1)\n\n# Check if command succeeded\nif [ $? -ne 0 ]; then\n  echo \"Error: Cannot find common ancestor between '$branch_a' and '$branch_b'.\"\n  echo \"\"\n  echo \"This may mean:\"\n  echo \"- Branches have completely independent histories\"\n  echo \"- Branch was rebased and history was rewritten\"\n  echo \"\"\n  echo \"Git error: $merge_base\"\n  exit 1\nfi\n\n# Get divergence info\ndivergence_date=$(git log -1 --format='%ai' \"$merge_base\")\ndivergence_short=$(git rev-parse --short \"$merge_base\")\n```\n\n**Find iteration at divergence (if it exists):**\n```bash\n# Check if divergence point has an iteration tag\ndivergence_tag=$(git tag --points-at \"$merge_base\" | grep 'iteration-')\nif [ -n \"$divergence_tag\" ]; then\n  divergence_iteration=$(echo \"$divergence_tag\" | sed 's/.*iteration-//')\nelse\n  divergence_iteration=\"(no iteration tag at divergence)\"\nfi\n```\n\n### Step 3: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with comparison instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"Compare two autonomy branches and show differences in approaches and outcomes.\n\nBranches:\n- Branch A: $branch_a\n- Branch B: $branch_b\n\nDivergence point: $merge_base ($divergence_short)\nDivergence date: $divergence_date\nDivergence iteration: $divergence_iteration\n\nTasks:\n1. Read all journal commits on branch A since divergence\n2. Read all journal commits on branch B since divergence\n3. Parse each commit message for: iteration, date, status, metrics, blockers, next steps\n4. Generate Python script to compare:\n   - Iteration counts on each branch\n   - Status patterns (how often active/blocked/etc)\n   - Metrics trajectories (if metrics exist)\n   - Different decisions/approaches mentioned\n   - Outcomes on each branch\n5. Execute Python script\n6. Output comparative markdown report\n\nUse computational methods (Python scripts), do not eyeball the comparison.\n\nReport format:\n- Divergence Information section\n- Iteration Comparison section\n- Metrics Comparison section (if metrics exist)\n- Approach Differences section\n- Outcomes and Status section\n- Insights and Recommendations section\"\n```\n\n**Agent will:**\n1. Get commit range for each branch since divergence\n2. Read journal commits from both ranges\n3. Parse metadata from commit messages\n4. Generate Python script for comparison\n5. Execute script to produce comparative analysis\n6. Return formatted markdown report\n\n### Step 4: Present Comparison Report\n\nDisplay agent's comparative report to user.\n\n**Example output format:**\n```markdown\n# Branch Comparison\n\n**Branch A:** autonomy/experiment-a\n**Branch B:** autonomy/experiment-b\n\n---\n\n## Divergence Information\n\n**Common ancestor:** abc123f\n**Divergence date:** 2025-12-15\n**Divergence iteration:** 0015\n\nBranches have been exploring different approaches for 18 days.\n\n---\n\n## Iteration Comparison\n\n| Branch | Iterations Since Divergence | Current Iteration | Latest Update |\n|--------|----------------------------|-------------------|---------------|\n| experiment-a | 13 (0016-0028) | 0028 | 2026-01-02 |\n| experiment-b | 8 (0016-0023) | 0023 | 2025-12-28 |\n\n**Observation:** Branch A has progressed more iterations but is currently blocked. Branch B has fewer iterations but is active.\n\n---\n\n## Metrics Comparison\n\n### MRR Trajectory\n\n- **experiment-a:** $45k → $62k (+37.8%)\n  - Faster growth, reached $62k at iteration 0028\n- **experiment-b:** $45k → $58k (+28.9%)\n  - Steady growth, reached $58k at iteration 0023\n\n### Build Time\n\n- **experiment-a:** 5.2min → 3.2min (-38.5%)\n  - Significant optimization focus\n- **experiment-b:** 5.2min → 4.8min (-7.7%)\n  - Minor improvements only\n\n---\n\n## Approach Differences\n\n### Branch A (experiment-a): Usage-based pricing\n- Implemented tiered usage model\n- Real-time usage tracking\n- API rate limiting integration\n- **Current status:** Blocked on Stripe API integration\n\n### Branch B (experiment-b): Flat enterprise pricing\n- Fixed pricing tiers (Startup/Growth/Enterprise)\n- Annual commitment discounts\n- Sales-assisted onboarding\n- **Current status:** Active, implementing pricing page UI\n\n---\n\n## Outcomes and Status\n\n| Branch | Current Status | Key Achievement | Main Blocker |\n|--------|----------------|-----------------|--------------|\n| experiment-a | blocked | Higher MRR growth (+37.8%) | Stripe webhook docs |\n| experiment-b | active | Simpler implementation | None currently |\n\n---\n\n## Insights and Recommendations\n\n**What worked well:**\n- **experiment-a:** Usage-based model drove higher revenue but added complexity\n- **experiment-b:** Flat pricing is simpler to implement and maintain\n\n**What didn't work:**\n- **experiment-a:** Dependency on external Stripe API caused blocking\n- **experiment-b:** Slower revenue growth compared to usage model\n\n**Cross-branch learning:**\n- experiment-b could adopt experiment-a's build optimizations (-38.5% improvement)\n- experiment-a could simplify by borrowing experiment-b's pricing page approach\n- Consider hybrid: flat base + usage overage (best of both)\n\n**Recommendations:**\n1. If revenue growth is priority: Continue experiment-a, resolve Stripe blocker\n2. If speed to market is priority: Continue experiment-b, ship simple version\n3. If unsure: Fork new branch from divergence point implementing hybrid approach\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY compares `autonomy/*` branches:\n- Validates both branches have `autonomy/` prefix\n- Will not compare non-autonomy branches\n- Branches must share common ancestor (git merge-base succeeds)\n\n### Computational Comparison Required\n\n**DO NOT:**\n- Manually compare commits\n- \"Eyeball\" which branch is better\n- Guess at metrics differences\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Read-Only Operations\n\nAll comparison happens via git commands:\n- Never checkout either branch\n- Read commits via `git log <branch>` for each\n- Branch-analyzer uses read-only operations\n- No modifications to any files\n\n### No Value Judgments\n\n**DO NOT:**\n- Declare one branch \"better\" or \"worse\"\n- Recommend abandoning a branch\n- Make strategic decisions for user\n\n**DO:**\n- Present objective comparison\n- Note trade-offs\n- Suggest cross-branch learning opportunities\n- Let user decide which approach to continue\n\n### Branches May Have No Metrics\n\n- Not all branches track quantitative metrics\n- Metrics section may be \"None\" in commit messages\n- Report should handle missing metrics gracefully\n- Can still compare on iterations, status, decisions\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll declare branch A is better\" | NO. Present objective comparison, let user decide. |\n| \"I'll recommend merging branches\" | NO. Autonomy branches never merge. Only cross-learning via /analyze-branch. |\n| \"Only 5 iterations different, I can compare manually\" | NO. Always dispatch branch-analyzer for computational analysis. |\n| \"Branches have no common ancestor, I'll error\" | CORRECT. This is a real error case - branches are independent. |\n| \"I'll checkout branches to compare journals\" | NO. Read commit messages via git log. Never checkout. |\n\n## After Comparing\n\nOnce comparison is complete:\n- Report displayed to user\n- No files created or modified\n- User can fork new branch from divergence point: `/fork-iteration <iteration> <strategy-name>`\n- User can analyze either branch individually: `/branch-status <branch-name>`\n- User can use `/analyze-branch` to extract specific learnings from one branch for use in another"
              },
              {
                "name": "creating-a-goal",
                "description": "Use when setting up a new open-ended goal for autonomy tracking, before starting the first iteration",
                "path": "plugins/autonomy/skills/creating-a-goal/SKILL.md",
                "frontmatter": {
                  "name": "creating-a-goal",
                  "description": "Use when setting up a new open-ended goal for autonomy tracking, before starting the first iteration"
                },
                "content": "# Creating a Goal\n\n## Overview\n\nSet up a new open-ended goal by creating the directory structure, writing the goal definition, and preparing for iteration tracking.\n\n**Core principle:** One-time setup. Run this once per goal, then use starting-an-iteration for all subsequent work.\n\n## When to Use\n\nUse this skill when:\n- User runs `/create-goal` command\n- Starting a brand new open-ended goal\n- No existing autonomy goal directory exists\n\n**DO NOT use for:**\n- Continuing existing goal (use starting-an-iteration instead)\n- Closed goals with definition of done (use ed3d-superpowers workflow)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Verify no existing goal | Check for autonomy directory | Glob |\n| 2. Get goal details | Prompt user for goal statement | User interaction |\n| 3. Generate directory name | Convert to kebab-case | Manual |\n| 4. Create structure | Make directory | Bash |\n| 5. Write goal.md | Document goal definition | Write |\n| 6. Announce ready | Tell user to run /start-iteration | Direct output |\n\n## Process\n\n### Step 1: Verify No Existing Goal\n\nCheck that no goal already exists:\n\n```bash\n# Use Glob to check\npattern: \"autonomy/*/goal.md\"\n```\n\n**If goal.md found:**\n```\n\"An autonomy goal already exists: [goal-name]\nUse '/start-iteration' to continue working on this goal.\nIf you want to create a different goal, first archive or remove the existing one.\"\n```\nStop here - don't create duplicate goals.\n\n**If no goal found:**\nProceed to Step 2.\n\n### Step 2: Get Goal Details from User\n\nPrompt user for goal information:\n\n```\n\"What open-ended goal would you like to pursue?\n\nThis should be a goal that never truly completes - ongoing optimization, continuous improvement, or iterative exploration.\n\nExamples:\n- Maximize monthly recurring revenue\n- Improve developer productivity\n- Reduce customer churn\n- Optimize application performance\n\nYour goal:\"\n```\n\n**User provides goal statement.**\n\nThen ask about success criteria:\n\n```\n\"What metrics or indicators should we track for this goal?\n\nSince this is open-ended, there's no 'done' state, but we can track progress.\n\nExamples:\n- MRR: Starting at $45k/month\n- Churn rate: Currently 13%\n- Build time: Currently 5 minutes\n\nYour metrics (or 'none' if not applicable):\"\n```\n\n**User provides metrics or indicates none.**\n\n### Step 3: Generate Directory Name\n\nConvert goal statement to kebab-case:\n\n**Rules:**\n- Lowercase all characters\n- Replace spaces with hyphens\n- Remove special characters\n- Keep concise (max 50 chars)\n\n**Examples:**\n- \"Maximize monthly recurring revenue\" → `maximize-monthly-recurring-revenue`\n- \"Improve developer productivity\" → `improve-developer-productivity`\n- \"Reduce customer churn by 50%\" → `reduce-customer-churn`\n\n### Step 4: Create Directory Structure\n\nCreate the goal directory:\n\n```bash\n# Use Bash to create directory\nmkdir -p autonomy/[goal-name]\n```\n\n### Step 5: Write goal.md\n\nCreate the goal definition file:\n\n```markdown\n# Goal: [Original goal statement]\n\n## Goal Statement\n[Full description from user]\n\n## Success Criteria\n[Open-ended - note that this has no completion state]\n\n## Metrics to Track\n[If user provided metrics:]\n- [Metric 1]: [Starting value]\n- [Metric 2]: [Starting value]\n\n[If no metrics:]\n- No specific metrics defined\n- Progress will be qualitative\n\n## Current Status\nActive - Ready for iteration 1\n\n## Started\n[Current date: YYYY-MM-DD]\n\n## Notes\n[Any additional context from user]\n```\n\nWrite this file to: `autonomy/[goal-name]/goal.md`\n\n### Step 6: Announce Ready\n\nInform user that goal is created:\n\n```markdown\n**Goal created: [goal-name]**\n\nDirectory: `autonomy/[goal-name]/`\nGoal definition: `autonomy/[goal-name]/goal.md`\n\n---\n\n**Next step:** Run `/start-iteration` to begin the first iteration.\n\nThis will:\n- Create iteration-0001-[today's date].md\n- Set up workspace for working toward the goal\n- Track progress in iteration journals\n```\n\n## Important Notes\n\n### Single Goal Per Project\n\nCurrently, autonomy supports one goal per project directory:\n- If goal.md exists, creation fails\n- User must archive/remove existing goal to create new one\n- Future enhancement could support multiple goals\n\n### Goal Naming\n\nKeep directory names:\n- **Descriptive** but **concise**\n- **Stable** (won't change over time)\n- **Filesystem-safe** (kebab-case, no special chars)\n\n### Open-Ended Requirement\n\nVerify goal is truly open-ended:\n\n**Good examples:**\n- \"Maximize X\" - always can improve\n- \"Reduce Y\" - continuous optimization\n- \"Improve Z\" - never fully complete\n\n**Bad examples:**\n- \"Build authentication system\" - has done state (use ed3d-superpowers)\n- \"Fix bug #123\" - has done state\n- \"Write documentation\" - has done state\n\nIf user suggests closed goal, guide them:\n```\n\"This goal seems to have a clear completion state. For goals with a definition of 'done',\nconsider using the ed3d-superpowers development workflow instead.\n\nAutonomy is designed for never-ending optimization and improvement goals.\n\nWould you like to reframe this as an open-ended goal, or would a different workflow be better?\"\n```\n\n### Metrics Are Optional\n\nNot all open-ended goals have quantifiable metrics:\n- \"Improve code quality\" - may be qualitative\n- \"Increase team knowledge\" - hard to measure\n- \"Enhance user experience\" - subjective\n\nAccept \"none\" or \"qualitative\" as valid answers.\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create goal even though one exists\" | NO. One goal per project. Check first. |\n| \"User goal is closed-ended but I'll create anyway\" | NO. Guide to appropriate workflow. |\n| \"I'll skip metrics prompt to save time\" | NO. Always ask - user may have important metrics. |\n| \"Directory name can have spaces\" | NO. Use kebab-case only. |\n| \"I'll start first iteration automatically\" | NO. Let user run /start-iteration when ready. |\n\n## After Creating\n\nOnce goal is created:\n- goal.md exists with definition\n- Directory ready for iteration journals\n- User runs `/start-iteration` to begin work\n- Iteration 1 will create first journal entry"
              },
              {
                "name": "ending-an-iteration",
                "description": "Use when concluding work on an open-ended goal to write iteration journal entry documenting work performed, decisions made, and state changes",
                "path": "plugins/autonomy/skills/ending-an-iteration/SKILL.md",
                "frontmatter": {
                  "name": "ending-an-iteration",
                  "description": "Use when concluding work on an open-ended goal to write iteration journal entry documenting work performed, decisions made, and state changes"
                },
                "content": "# Ending an Iteration\n\n## Overview\n\nConclude the current iteration by reviewing the conversation, documenting what happened, and preparing state for the next iteration.\n\n**Core principle:** Comprehensive journaling enables continuity. Future iterations depend on accurate state capture.\n\n## When to Use\n\nUse this skill when:\n- User runs `/end-iteration` command\n- Agent suggests natural stopping point and user confirms\n- Context limit approaching and good time to pause\n- Subtask complete and ready to hand off to next iteration\n\n**DO NOT use for:**\n- Middle of active work (finish current subtask first)\n- Before resolving critical blockers (unless blocked externally)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Review conversation | Extract skills, decisions, artifacts | Manual review |\n| 2. Identify stopping point | Why is this iteration ending? | User confirmation |\n| 3. Complete journal entry | Update existing journal file | Read, Edit |\n| 4. Update summary | If iteration 5, 10, 15, etc. | Task (journal-summarizer) |\n| 5a. Git commit and tag | Commit journal to current branch | Bash |\n| 5b. Announce completion | Confirm next steps | Direct output |\n\n## Process\n\n### Step 1: Review Conversation\n\nReview the full conversation to extract key information:\n\n**Skills & Workflows Used:**\n- Scan conversation for `<invoke name=\"Skill\">` tool calls\n- Note which skills were used and for what purpose\n- Generic detection - works with any plugin skills\n\nExample findings:\n```markdown\n### Skills & Workflows Used\n- `brainstorming`: Designed pricing tier alternatives\n- `hypothesis-testing`: Validated annual billing preference\n- `internet-researcher`: Found competitor pricing data\n```\n\n**Key Decisions Made:**\n- Identify major choices: architecture, strategy, approach\n- Note rationale: why was this chosen?\n- Record alternatives considered\n\nExample:\n```markdown\n### Key Decisions Made\n- **Decision:** Implement usage-based pricing tier\n  **Rationale:** Research showed power users want to scale gradually\n  **Alternatives:** Flat enterprise tier (rejected: less flexible)\n```\n\n**Artifacts Created/Modified:**\n- Files created or changed (use git status/diff if applicable)\n- Git commits made during iteration\n- Pull requests opened\n- Documentation written\n\nExample:\n```markdown\n### Artifacts Created/Modified\n- `src/pricing/usage-tier.ts`: New usage-based pricing calculator\n- `docs/pricing-strategy.md`: Documented pricing decision rationale\n- Git commit: abc123f \"Add usage-based pricing tier\"\n```\n\n**External Context Gathered:**\n- Web research findings\n- User feedback received\n- Documentation consulted\n- Competitor analysis\n\nExample:\n```markdown\n### External Context Gathered\n- Research: Competitor X charges $0.10/unit, Competitor Y charges $0.15/unit\n- User feedback: \"Annual discount of 20% drives conversions\"\n- Documentation: Stripe API supports usage-based billing natively\n```\n\n**Reasoning & Strategy Changes:**\n- Why certain approaches were chosen\n- What alternatives were explored\n- Where strategy pivoted and why\n\nExample:\n```markdown\n### Reasoning & Strategy Changes\n- Initially planned flat enterprise tier\n- Research showed power users prefer scaling gradually\n- Pivoted to usage-based model to reduce friction\n- This aligns with SaaS best practices for 2026\n```\n\n**Blockers Encountered:**\n- What's preventing progress?\n- Dependencies on external factors\n- Questions needing answers\n\nExample:\n```markdown\n### Blockers Encountered\n- Stripe webhook integration unclear: need to consult API docs\n- Finance team needs to approve pricing before launch\n- Usage tracking infrastructure not yet built (dependency)\n```\n\n**Open Questions:**\n- What needs to be resolved next?\n- Decisions deferred to future iterations\n- Unknowns requiring investigation\n\nExample:\n```markdown\n### Open Questions\n- Should we offer annual discount on usage-based tier?\n- What's the right per-unit price point?\n- Do we need a minimum monthly commit?\n```\n\n### Step 2: Identify Stopping Point\n\nDetermine why this iteration is ending:\n\n**Ask user to confirm:**\n```\n\"Suggested stopping point: [reason]. Should we end this iteration?\"\n\nReasons:\n- Subtask complete: [what was finished]\n- Blocked on: [external dependency]\n- Context approaching limit\n- Natural break point: [why this makes sense]\n```\n\n**User confirms** → Proceed to write journal\n\n**User wants to continue** → Return to work, don't end iteration yet\n\n### Step 3: Complete Journal Entry\n\nThe journal file was created by starting-an-iteration. Now complete it:\n\n1. **Find current journal file:**\n   ```bash\n   # Use Glob to find iteration files\n   pattern: \"autonomy/*/iteration-*.md\"\n   ```\n\n   Sort by iteration number and identify the most recent (should be today's date).\n\n2. **Read existing journal:**\n   ```bash\n   # Use Read to load current content\n   file: \"autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\"\n   ```\n\n   Journal will have:\n   - Beginning State (from starting-an-iteration)\n   - Iteration Intention (from starting-an-iteration)\n   - Work Performed (may be filled by checkpointing, or still empty)\n   - Ending State (empty - we'll fill this)\n   - Iteration Metadata (empty - we'll fill this)\n\n3. **Update Work Performed section:**\n\n   Using findings from Step 1, update the Work Performed subsections.\n\n   **If Work Performed is empty:**\n   Replace entire section with Step 1 findings.\n\n   **If Work Performed has content (from checkpointing):**\n   Merge Step 1 findings with existing:\n   - Add any new skills/workflows used since checkpoint\n   - Add any new decisions made since checkpoint\n   - Add new artifacts created since checkpoint\n   - Append new context gathered since checkpoint\n   - Note any additional blockers or questions\n   - Preserve all existing checkpoint content\n\n4. **Fill Ending State section:**\n   ```markdown\n   ## Ending State\n   [What is the state NOW at iteration end?]\n   - Progress made during this iteration\n   - What's complete vs. what remains\n   - Updated metrics (if applicable)\n   - How well did we achieve the iteration intention?\n   - Recommended next steps for iteration N+1\n   ```\n\n5. **Fill Iteration Metadata section:**\n   ```markdown\n   ## Iteration Metadata\n   - Context usage: [Note if approaching limits, if compaction occurred]\n   - Checkpoints: [How many times was checkpoint-iteration used?]\n   - Suggested next action: [What should iteration N+1 work on?]\n   ```\n\n6. **Update file using Edit tool:**\n   - Replace Work Performed section (merge with existing)\n   - Replace Ending State section (fill in)\n   - Replace Iteration Metadata section (fill in)\n   - Preserve Beginning State and Iteration Intention sections\n\n### Step 4: Update Summary (if needed)\n\nCheck if summary update is needed:\n\n**Update summary every 5 iterations:** 5, 10, 15, 20, etc.\n\n```\nIf current iteration number % 5 == 0:\n   Dispatch journal-summarizer agent to update summary.md\n```\n\n**Dispatch journal-summarizer:**\n```\nTask tool with subagent_type: \"autonomy:journal-summarizer\"\nPrompt: \"Update summary.md for goal '[goal-name]' to include iterations 1-[N].\n        Previous summary covered iterations 1-[N-5].\n        Add new learnings, update metrics, note new blockers.\"\nModel: haiku\n```\n\n**Wait for agent** to update summary.md\n\n### Step 5a: Git Commit and Tag\n\nAfter journal is complete and summary is updated (if needed), commit to git:\n\n1. **Extract iteration metadata:**\n   - Goal name from journal path: `autonomy/[goal-name]/`\n   - Iteration number from filename: `iteration-NNNN-YYYY-MM-DD.md`\n   - Extract 2-3 line summary from Ending State section\n\n2. **Determine iteration status:**\n\n   Analyze Ending State section to determine status:\n   - Look for explicit status indicators: \"This is a dead end\", \"Approach invalidated\", \"Successfully completed\", \"Blocked by\", etc.\n   - **active** (default) - Normal progression, work continuing\n   - **blocked** - Contains phrases like \"blocked by\", \"waiting on\", \"cannot proceed until\"\n   - **concluded** - Contains phrases like \"successfully completed\", \"goal achieved\", \"experiment succeeded\"\n   - **dead-end** - Contains phrases like \"dead end\", \"not working\", \"abandoning approach\", \"invalidated\"\n\n3. **Extract quantitative metrics:**\n\n   Parse Ending State for metrics with numbers:\n   - Look for patterns like \"MRR: $62k (+12%)\", \"Build time: 3.2min (-40%)\", \"Churn: 8% (from 13%)\"\n   - Collect all quantitative progress indicators\n   - Format as single line: `Metrics: [metric1], [metric2], ...` or \"None\" if no metrics\n\n4. **Summarize journal content:**\n\n   Create 4-6 sentence summary from Work Performed and Ending State:\n   - What was accomplished this iteration\n   - Key decisions made and rationale\n   - Major learnings or discoveries\n   - How this iteration moved toward the goal\n   - Be substantive but concise - this is for git log readers\n\n5. **Build enhanced commit message:**\n   ```\n   journal: [goal-name] iteration NNNN\n\n   [2-3 line summary from Ending State - as before]\n\n   ## Journal Summary\n\n   [4-6 sentence summary from step 4 above - what happened, what was learned, what changed]\n\n   ## Iteration Metadata\n\n   Status: [active|blocked|concluded|dead-end]\n   Metrics: [quantitative metrics from step 3, or \"None\"]\n   Blockers: [summary from Blockers Encountered section, or \"None\"]\n   Next: [next iteration intention from Iteration Metadata section]\n\n   🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\n   Co-Authored-By: Claude <noreply@anthropic.com>\n   ```\n\n6. **Stage files:**\n   ```bash\n   # Always stage journal file\n   git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n\n   # If summary was updated this iteration (iteration % 5 == 0)\n   git add autonomy/[goal-name]/summary.md\n   ```\n\n7. **Create commit:**\n   ```bash\n   # Use heredoc for multi-line message with enhanced format\n   git commit -m \"$(cat <<'EOF'\n   journal: [goal-name] iteration NNNN\n\n   [2-3 line summary from Ending State]\n\n   ## Journal Summary\n\n   [4-6 sentence summary of iteration]\n\n   ## Iteration Metadata\n\n   Status: [active|blocked|concluded|dead-end]\n   Metrics: [metrics or \"None\"]\n   Blockers: [blockers or \"None\"]\n   Next: [next iteration intention]\n\n   🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\n   Co-Authored-By: Claude <noreply@anthropic.com>\n   EOF\n   )\"\n   ```\n\n8. **Create annotated tag with branch-aware naming:**\n   ```bash\n   # Get current branch name\n   current_branch=$(git branch --show-current)\n\n   # Extract strategy name from branch\n   # For autonomy branches: \"autonomy/experiment-a\" → \"experiment-a\"\n   # For non-autonomy branches: use goal name as strategy\n   if [[ \"$current_branch\" =~ ^autonomy/ ]]; then\n     strategy_name=${current_branch#autonomy/}\n   else\n     # Use goal name from journal path\n     strategy_name=\"[goal-name]\"\n   fi\n\n   # Tag format: autonomy/<strategy-name>/iteration-NNNN (4 digits, zero-padded)\n   git tag -a \"autonomy/${strategy_name}/iteration-$(printf '%04d' NNNN)\" \\\n     -m \"journal: [goal-name] iteration NNNN\"\n   ```\n\n9. **Handle errors gracefully:**\n   - If git operations fail (not a repo, detached HEAD, permissions, etc.):\n     - Capture error message\n     - Continue to Step 5b anyway (journal is written, that's critical)\n     - Report warning to user with manual commands\n\n**Error reporting format:**\n```markdown\n⚠️ **Git Integration Warning:**\nFailed to commit journal: [error message]\n\nYou can manually commit with:\n  git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n  git commit -m \"journal: [goal-name] iteration NNNN ...\"\n  git tag -a \"autonomy/[strategy-name]/iteration-NNNN\" -m \"journal: [goal-name] iteration NNNN\"\n```\n\n**Success indicator:**\n- If git operations succeed, note success for Step 5b announcement\n- Tag `autonomy/[strategy-name]/iteration-NNNN` marks this iteration in git history\n\n### Step 5b: Announce Completion\n\nReport to user with git status:\n\n**If git operations succeeded:**\n```markdown\n**Iteration [N] complete for goal: [goal-name]**\n\n✓ Journal committed and tagged: `autonomy/[strategy-name]/iteration-NNNN`\n\nJournal entry: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\nBranch: [current-branch-name]\nStatus: [active|blocked|concluded|dead-end]\n\n## Summary of This Iteration\n- **Work completed:** [Brief summary]\n- **Key decisions:** [Major choices made]\n- **Blockers:** [What's preventing progress]\n- **Next steps:** [Recommended for iteration N+1]\n\n---\n\n**Ready to resume in next conversation with `/start-iteration`**\n```\n\n**If git operations failed:**\n```markdown\n**Iteration [N] complete for goal: [goal-name]**\n\nJournal entry written: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\n⚠️ **Git Integration Warning:**\nFailed to commit journal: [error message]\n\nYou can manually commit with:\n  git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n  git commit -m \"journal: [goal-name] iteration NNNN ...\"\n  git tag -a \"autonomy/[strategy-name]/iteration-NNNN\" -m \"journal: [goal-name] iteration NNNN\"\n\n## Summary of This Iteration\n- **Work completed:** [Brief summary]\n- **Key decisions:** [Major choices made]\n- **Blockers:** [What's preventing progress]\n- **Next steps:** [Recommended for iteration N+1]\n\n---\n\n**Ready to resume in next conversation with `/start-iteration`**\n```\n\n## Important Notes\n\n### Journal File Already Exists\n\nThe journal file was created by starting-an-iteration with:\n- Beginning State (from context loading)\n- Iteration Intention (from user input)\n- Empty Work Performed sections (to be filled)\n\nYour job is to **complete** the journal, not create it from scratch.\n\n### Merging with Checkpoint Content\n\nIf user ran `/checkpoint-iteration` during the iteration:\n- Work Performed section will have partial content\n- Merge your Step 1 findings with existing content\n- Add new information discovered since checkpoint\n- Don't overwrite or lose checkpoint data\n\n### Comprehensive Documentation\n\n**Critical:** Future iterations depend entirely on this journal. Include:\n- Enough detail that iteration N+1 can pick up seamlessly\n- Rationale for decisions (not just what, but why)\n- All blockers, even minor ones\n- Specific next steps, not vague \"continue working\"\n\n### Date in Filename\n\nAlways use today's date (YYYY-MM-DD format):\n```bash\n# Get current date\ndate +%Y-%m-%d\n```\n\n### Don't Skip Sections\n\nEven if a section is empty, include it with a note:\n```markdown\n### Blockers Encountered\nNone - iteration progressed smoothly\n```\n\nThis shows the section was considered, not forgotten.\n\n### Git Integration\n\n**Automatic commits to current branch:**\n- Journal file is committed to whatever branch you're currently on\n- Does NOT create new branch or switch branches\n- Uses enhanced commit message format with:\n  - Brief summary (2-3 lines)\n  - Journal Summary section (4-6 sentences)\n  - Iteration Metadata section (status, metrics, blockers, next steps)\n- Tags commit as `autonomy/<strategy-name>/iteration-NNNN` for branch-aware navigation\n\n**Branch-aware tagging:**\n- On autonomy branches: Tag uses branch name (e.g., `autonomy/experiment-a/iteration-0015`)\n- On non-autonomy branches: Tag uses goal name (backward compatibility)\n- Each branch has its own iteration namespace\n- No tag collisions when multiple branches have same iteration number\n\n**Error handling:**\n- Git failures do NOT block iteration completion\n- Journal is always written, even if commit fails\n- User receives manual commands if git fails\n- Graceful degradation ensures state is preserved\n\n**What gets committed:**\n- Always: `iteration-NNNN-YYYY-MM-DD.md` journal file\n- Sometimes: `summary.md` (if iteration % 5 == 0)\n- Never: Other files in working directory\n\n**Tag benefits:**\n- Navigate to iteration on specific branch: `git checkout autonomy/experiment-a/iteration-0042`\n- List all iterations on a branch: `git tag -l 'autonomy/experiment-a/*'`\n- List all autonomy iterations: `git tag -l 'autonomy/*/*'`\n- Immutable history markers for CI/CD integration\n- Enables branch management commands to query git log for metadata\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create new journal file instead of updating existing\" | NO. Journal was created by starting-an-iteration. Update it with Edit tool. |\n| \"I'll overwrite Work Performed that has checkpoint content\" | NO. Merge with existing. Preserve checkpoint data. |\n| \"Journal is too detailed, I'll abbreviate\" | NO. Be comprehensive. Future iterations need full context. |\n| \"No blockers this time, I'll skip that section\" | NO. Write \"None\" to show you checked. |\n| \"Summary update can wait\" | NO. If iteration % 5 == 0, update summary. |\n| \"I'll skip comparing against iteration intention\" | NO. Ending State should assess how well intention was achieved. |\n| \"Git commit failed, so iteration failed\" | NO. Journal writing is critical, git commit is helpful. Warn about git error but complete iteration. |\n| \"I'll create a new branch for the commit\" | NO. Commit to current branch. User controls branching strategy. |\n\n## After Ending\n\nOnce iteration is ended:\n- Journal file completed with Ending State and Iteration Metadata\n- Summary updated if needed (every 5 iterations)\n- Git commit created and tagged as `autonomy/iteration-NNNN` (if git operations succeeded)\n- Full iteration story captured for next iteration\n- Can start new iteration anytime with `/start-iteration`"
              },
              {
                "name": "forking-iteration",
                "description": "Use when user wants to create new autonomy branch from current commit or specific past iteration",
                "path": "plugins/autonomy/skills/forking-iteration/SKILL.md",
                "frontmatter": {
                  "name": "forking-iteration",
                  "description": "Use when user wants to create new autonomy branch from current commit or specific past iteration"
                },
                "content": "# Forking Iteration\n\n## Overview\n\nCreate new autonomy branch forked from current commit or any iteration tag in git history. Enables \"slime mold strategy\" of parallel exploration.\n\n**Core principle:** Branch creation is independent of iteration management. Fork creates branch, start-iteration begins work.\n\n## When to Use\n\nUse this skill when:\n- User runs `/fork-iteration` command\n- User wants to create new exploration branch\n- User wants to try alternative approach from past iteration\n- User wants to bootstrap autonomy workflow from non-autonomy branch\n\n**DO NOT use for:**\n- Creating non-autonomy branches (use git directly)\n- Starting iterations (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract iteration (optional) and strategy-name | Manual |\n| 2. Validate strategy-name | Check kebab-case, not already exists | Bash |\n| 3. Resolve fork point | Find iteration tag or use HEAD | Bash |\n| 4. Create branch | Checkout fork point, create autonomy branch | Bash |\n| 5. Report success | Confirm creation with next steps | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Arguments\n\nExtract arguments from command args:\n\n**Format:** `[iteration] <strategy-name>`\n\n**Parse:**\n```\nIf args contains only one word:\n  iteration = None\n  strategy_name = args\nElse if args contains two words:\n  iteration = first word\n  strategy_name = second word\nElse:\n  Error: \"Invalid arguments. Usage: /fork-iteration [iteration] <strategy-name>\"\n```\n\n**Validate strategy-name:**\n- Must be kebab-case (lowercase, hyphens, no special chars)\n- Must not be empty\n- Should be descriptive (warn if too generic like \"test\" or \"new\")\n\n**Normalize strategy-name:**\n```bash\n# Convert to lowercase, replace invalid chars with hyphens\nstrategy_name=$(echo \"$strategy_name\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]-' '-' | sed 's/^-*//; s/-*$//')\n```\n\n### Step 2: Check if Branch Already Exists\n\nValidate that `autonomy/<strategy-name>` doesn't already exist:\n\n```bash\n# Check for existing branch (local or remote)\nif git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  # Error: branch exists\nfi\n```\n\n**If exists:**\n```markdown\nError: Branch 'autonomy/<strategy-name>' already exists.\n\nTo work on existing branch:\n  git checkout autonomy/<strategy-name>\n\nTo create with different name:\n  /fork-iteration [iteration] <different-name>\n\nAvailable autonomy branches:\n[List from git branch -a | grep autonomy/]\n```\n\nStop here if branch exists.\n\n### Step 3: Resolve Fork Point\n\nDetermine what commit to fork from:\n\n**If iteration specified:**\n```bash\n# Search for iteration tag in current branch history\nmatching_tags=$(git tag --merged HEAD | grep \"iteration-$(printf '%04d' $iteration)\\$\")\n\ntag_count=$(echo \"$matching_tags\" | wc -l)\n\nif [ \"$tag_count\" -eq 0 ]; then\n  # Error: iteration not found\n  echo \"Error: Iteration $iteration not found in current branch history.\"\n  echo \"\"\n  echo \"Most recent iterations in current branch:\"\n  git tag --merged HEAD | grep 'iteration-' | tail -5\n  exit 1\nelif [ \"$tag_count\" -eq 1 ]; then\n  # Use the tag\n  fork_point=\"$matching_tags\"\nelse\n  # Multiple matches (shouldn't happen with branch-namespaced tags, but handle it)\n  echo \"Multiple iteration tags found for iteration $iteration:\"\n  echo \"$matching_tags\"\n  echo \"\"\n  # Use AskUserQuestion to let user choose\n  exit 1\nfi\n```\n\n**If iteration NOT specified:**\n```bash\n# Fork from current HEAD\nfork_point=$(git rev-parse HEAD)\nfork_description=\"current commit (HEAD)\"\n```\n\n**Validate fork point exists:**\n```bash\nif ! git rev-parse \"$fork_point\" >/dev/null 2>&1; then\n  echo \"Error: Fork point '$fork_point' not found in git history.\"\n  exit 1\nfi\n```\n\n### Step 4: Create New Branch\n\nCheckout fork point and create new autonomy branch:\n\n```bash\n# Checkout fork point (detached HEAD)\ngit checkout \"$fork_point\"\n\n# Create and switch to new branch\ngit checkout -b \"autonomy/$strategy_name\"\n\n# Confirm creation\ncurrent_branch=$(git branch --show-current)\nif [ \"$current_branch\" != \"autonomy/$strategy_name\" ]; then\n  echo \"Error: Failed to create branch 'autonomy/$strategy_name'\"\n  exit 1\nfi\n```\n\n**On success:**\n- Branch created and checked out\n- Working directory contains files from fork point\n- Ready for `/start-iteration`\n\n### Step 5: Report Success\n\nAnnounce successful branch creation:\n\n```markdown\n✓ Branch `autonomy/<strategy-name>` created\n\nForked from: [tag or commit hash]\nCurrent branch: autonomy/<strategy-name>\n\nNext step: Run `/start-iteration` to begin work on this branch.\n```\n\n## Important Notes\n\n### Branch Creation Only\n\n**This skill ONLY creates branches:**\n- Does NOT start iterations\n- Does NOT modify journal files\n- Does NOT determine iteration numbering\n- `start-iteration` will handle iteration logic when user runs it\n\n### Works from Any Starting Point\n\nCan fork from:\n- Autonomy branches (e.g., `autonomy/experiment-a`)\n- Non-autonomy branches (e.g., `main`, `develop`)\n- Detached HEAD\n- Any commit with autonomy iteration tags\n\n### No Merge Provisions\n\n**Autonomy branches NEVER merge:**\n- Each branch is independent exploration\n- Branches learn from each other via `/analyze-branch` (read-only)\n- No git merge operations\n- No rebase operations\n- Branches are peers, not hierarchical\n\n### Strategy Name Matters\n\nBranch name becomes identity:\n- Used in tag names: `autonomy/<strategy-name>/iteration-NNNN`\n- Appears in branch listings\n- Should describe exploration direction\n- Can't easily change later (would require tag renaming)\n\n**Good names:**\n- `usage-based-pricing`\n- `cdn-optimization`\n- `react-migration`\n\n**Bad names:**\n- `test` (too generic)\n- `new` (not descriptive)\n- `experiment1` (meaningless)\n\n### Iteration Numbering Continuity\n\nWhen `start-iteration` runs on new branch:\n- If forked from iteration tag: continues numbering (fork from 0015 → next is 0016)\n- If forked from non-iteration commit: starts at 0001 (if goal exists)\n- If no goal exists: user must run `/create-goal` first\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll also start the iteration in this skill\" | NO. Branch creation is separate. User runs /start-iteration when ready. |\n| \"User wants to fork, I'll check out the branch for them\" | Already doing this. Checkout happens during creation. |\n| \"Branch name has spaces, I'll keep them\" | NO. Normalize to kebab-case. |\n| \"I'll search all branches for iteration tag\" | NO. Only search tags reachable from current HEAD (git tag --merged HEAD). |\n| \"Branch exists, I'll overwrite it\" | NO. Error and show user how to work with existing branch. |\n| \"No goal found, I'll create one\" | NO. User must run /create-goal explicitly. |\n\n## After Forking\n\nOnce branch is created:\n- User is on new `autonomy/<strategy-name>` branch\n- Working directory contains files from fork point\n- No iterations started yet\n- User runs `/start-iteration` to begin work\n- Iteration numbering determined by start-iteration skill"
              },
              {
                "name": "forking-worktree",
                "description": "Use when user wants to create new autonomy branch with dedicated worktree for parallel agent workflows",
                "path": "plugins/autonomy/skills/forking-worktree/SKILL.md",
                "frontmatter": {
                  "name": "forking-worktree",
                  "description": "Use when user wants to create new autonomy branch with dedicated worktree for parallel agent workflows"
                },
                "content": "# Forking Worktree\n\n## Overview\n\nCreate new autonomy branch with dedicated worktree for running parallel Claude agents on different branches simultaneously. All worktrees created at repository root level (`.worktrees/autonomy/`), regardless of where command is invoked.\n\n**Core principle:** Worktrees represent branches, not iterations. Multiple iterations happen within same worktree. Worktrees enable parallel exploration with isolated working directories.\n\n## When to Use\n\nUse this skill when:\n- User runs `/fork-worktree` command\n- User wants to run multiple agents in parallel on different autonomy branches\n- User wants isolated working directory for new exploration branch\n- User wants to create autonomy branch without disturbing current working directory\n\n**DO NOT use for:**\n- Creating branches without worktrees (use forking-iteration instead)\n- Creating non-autonomy branches (use git directly)\n- Starting iterations (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract iteration (optional) and strategy-name | Manual |\n| 2. Detect repository root | Find git root from anywhere (main repo or worktree) | Bash |\n| 3. Validate | Check strategy-name, branch, and worktree path availability | Bash |\n| 4. Resolve fork point | Find iteration tag or use HEAD | Bash |\n| 5. Create worktree | Create branch + worktree at root level | Bash |\n| 6. Report success | Confirm creation with navigation instructions | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Arguments\n\nExtract arguments from command args:\n\n**Format:** `[iteration] <strategy-name>`\n\n**Parse:**\n```\nIf args contains only one word:\n  iteration = None\n  strategy_name = args\nElse if args contains two words:\n  iteration = first word\n  strategy_name = second word\nElse:\n  Error: \"Invalid arguments. Usage: /fork-worktree [iteration] <strategy-name>\"\n```\n\n**Validate strategy-name:**\n- Must be kebab-case (lowercase, hyphens, no special chars)\n- Must not be empty\n- Should be descriptive (warn if too generic like \"test\" or \"new\")\n\n**Normalize strategy-name:**\n```bash\n# Convert to lowercase, replace invalid chars with hyphens\nstrategy_name=$(echo \"$strategy_name\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]-' '-' | sed 's/^-*//; s/-*$//')\n```\n\n### Step 2: Detect Repository Root\n\nFind the git repository root, regardless of whether we're in main repo or inside a worktree:\n\n```bash\n# Get common git directory (works from anywhere, including worktrees)\ngit_common_dir=$(git rev-parse --git-common-dir)\n\n# Repository root is parent of .git directory\n# For main repo: .git is directory -> parent is repo root\n# For worktree: .git is file pointing to .git/worktrees/X -> common-dir points back to main .git\nif [ -d \"$git_common_dir\" ]; then\n  repo_root=$(cd \"$git_common_dir/..\" && pwd)\nelse\n  echo \"Error: Unable to determine repository root\"\n  exit 1\nfi\n```\n\n**Why this matters:**\n- Ensures all worktrees created at `.worktrees/autonomy/` relative to repo root\n- Prevents nested worktrees (`.worktrees/autonomy/experiment-a/.worktrees/...`)\n- Works whether invoked from main repo or from within another worktree\n\n### Step 3: Validate Branch and Worktree Availability\n\nCheck that we can create the new branch and worktree:\n\n**Check branch doesn't exist:**\n```bash\nif git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  echo \"Error: Branch 'autonomy/$strategy_name' already exists.\"\n  echo \"\"\n  echo \"To work on existing branch:\"\n  echo \"  git checkout autonomy/$strategy_name\"\n  echo \"\"\n  echo \"To create worktree for existing branch:\"\n  echo \"  cd $repo_root\"\n  echo \"  git worktree add .worktrees/autonomy/$strategy_name autonomy/$strategy_name\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep autonomy/\n  exit 1\nfi\n```\n\n**Check worktree path doesn't exist:**\n```bash\nworktree_path=\"$repo_root/.worktrees/autonomy/$strategy_name\"\n\nif [ -e \"$worktree_path\" ]; then\n  echo \"Error: Worktree directory already exists: $worktree_path\"\n  echo \"\"\n  echo \"Remove it first:\"\n  echo \"  /remove-worktree $strategy_name\"\n  echo \"Or:\"\n  echo \"  git worktree remove .worktrees/autonomy/$strategy_name\"\n  exit 1\nfi\n```\n\n**Create parent directory if needed:**\n```bash\nmkdir -p \"$repo_root/.worktrees/autonomy\"\n```\n\n### Step 4: Resolve Fork Point\n\nDetermine what commit to fork from (same logic as forking-iteration):\n\n**If iteration specified:**\n```bash\n# Search for iteration tag in current branch history\nmatching_tags=$(git tag --merged HEAD | grep \"iteration-$(printf '%04d' $iteration)\\$\")\n\ntag_count=$(echo \"$matching_tags\" | grep -c '^' || echo \"0\")\n\nif [ \"$tag_count\" -eq 0 ]; then\n  echo \"Error: Iteration $iteration not found in current branch history.\"\n  echo \"\"\n  echo \"Most recent iterations in current branch:\"\n  git tag --merged HEAD | grep 'iteration-' | tail -5\n  exit 1\nelif [ \"$tag_count\" -eq 1 ]; then\n  fork_point=\"$matching_tags\"\n  fork_description=\"iteration $iteration (tag: $matching_tags)\"\nelse\n  # Multiple matches (shouldn't happen with branch-namespaced tags)\n  echo \"Multiple iteration tags found for iteration $iteration:\"\n  echo \"$matching_tags\"\n  echo \"\"\n  echo \"Please specify which tag to fork from.\"\n  exit 1\nfi\n```\n\n**If iteration NOT specified:**\n```bash\n# Fork from current HEAD\nfork_point=$(git rev-parse HEAD)\nfork_description=\"current commit ($(git rev-parse --short HEAD))\"\n```\n\n**Validate fork point exists:**\n```bash\nif ! git rev-parse \"$fork_point\" >/dev/null 2>&1; then\n  echo \"Error: Fork point '$fork_point' not found in git history.\"\n  exit 1\nfi\n```\n\n### Step 5: Create Worktree\n\nCreate the branch and worktree in a single git command:\n\n```bash\n# Navigate to repository root (important for relative path resolution)\ncd \"$repo_root\"\n\n# Create branch + worktree together\n# -b creates new branch\n# path is relative to repo root: .worktrees/autonomy/<strategy-name>\n# fork_point is where to create the branch from\ngit worktree add -b \"autonomy/$strategy_name\" \\\n  \".worktrees/autonomy/$strategy_name\" \\\n  \"$fork_point\"\n\n# Verify worktree created successfully\nif [ ! -d \"$worktree_path\" ]; then\n  echo \"Error: Failed to create worktree at $worktree_path\"\n  exit 1\nfi\n\n# Verify branch exists\nif ! git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  echo \"Error: Failed to create branch 'autonomy/$strategy_name'\"\n  exit 1\nfi\n```\n\n**What this does:**\n- Creates new branch `autonomy/<strategy-name>` at fork point\n- Creates worktree directory at `.worktrees/autonomy/<strategy-name>/`\n- Checks out the new branch in the worktree (not in current directory)\n- Current directory remains unchanged\n\n### Step 6: Report Success\n\nAnnounce successful worktree creation with navigation instructions:\n\n```markdown\n✓ Worktree created successfully\n\nBranch: autonomy/<strategy-name>\nWorktree: .worktrees/autonomy/<strategy-name>/\nForked from: [fork_description]\n\nNext steps:\n1. Navigate to worktree:\n   cd .worktrees/autonomy/<strategy-name>\n\n2. Start iteration:\n   /start-iteration\n\nThe worktree is an isolated working directory where you can work on this branch independently of other branches.\n```\n\n**Calculate relative path for convenience:**\n```bash\n# If we're already in a worktree, provide sibling navigation\ncurrent_dir=$(pwd)\nif [[ \"$current_dir\" == *\"/.worktrees/autonomy/\"* ]]; then\n  # In a worktree, provide relative navigation to sibling\n  echo \"From your current location:\"\n  echo \"  cd ../$strategy_name\"\nfi\n```\n\n## Important Notes\n\n### Repository Root Detection\n\n**Works from anywhere:**\n- Main repository working directory\n- Inside any worktree (including deeply nested work)\n- Detached HEAD state\n- Any subdirectory\n\n**All worktrees created at same level:**\n```\nrepo-root/\n├── .git/\n├── .worktrees/\n│   └── autonomy/\n│       ├── experiment-a/    # Created from main repo\n│       ├── experiment-b/    # Created from experiment-a worktree\n│       └── experiment-c/    # Created from experiment-b worktree\n```\n\n### Branch and Worktree Lifecycle\n\n**Branch persists after worktree removal:**\n- Removing worktree (via `/remove-worktree`) deletes directory\n- Branch `autonomy/<strategy-name>` and all commits persist\n- Can create new worktree for same branch later\n- All iteration tags remain in git history\n\n**One branch per worktree:**\n- Git enforces: branch can only be checked out in one worktree at a time\n- Attempting to create worktree for already-checked-out branch fails\n- This is a feature, not a bug (prevents conflicting changes)\n\n### Worktrees Are Optional\n\n**Existing workflows unchanged:**\n- `/fork-iteration` still creates branches without worktrees\n- All autonomy skills work in both main repo and worktrees\n- Worktrees are enhancement for parallel agent workflows\n- Use worktrees when beneficial, ignore when not needed\n\n### Isolation and Independence\n\n**Each worktree is independent:**\n- Separate working directory with own file state\n- Can have uncommitted changes independent of other worktrees\n- Can be at different commits (though shares branch history)\n- Perfect for running multiple Claude agents in parallel\n\n**Shared git metadata:**\n- All commits immediately visible across all worktrees\n- Tags created in one worktree visible in all worktrees\n- Branch operations (create, delete) affect all worktrees\n- `.git` directory is shared\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Create worktree in current directory\" | NO. Always create at repo root `.worktrees/autonomy/` regardless of where invoked. |\n| \"I'll use relative path .worktrees/...\" | YES, but must cd to repo_root first. Path is relative to repo root. |\n| \"Branch exists, I'll create worktree for it\" | NO. This skill creates NEW branches. Existing branch = error. |\n| \"I'll also start the iteration\" | NO. Worktree creation is separate. User navigates and runs /start-iteration. |\n| \"I'll checkout the branch in current directory\" | NO. Branch checked out in worktree, current directory unchanged. |\n| \"Nested worktrees are fine\" | NO. All worktrees must be at root level `.worktrees/autonomy/`. |\n| \"I'll use git rev-parse --show-toplevel\" | NO. That gives worktree root, not repo root. Use --git-common-dir. |\n\n## After Creating Worktree\n\nOnce worktree is created:\n- New directory exists at `.worktrees/autonomy/<strategy-name>/`\n- Branch `autonomy/<strategy-name>` checked out in that directory\n- Current working directory unchanged (still in main repo or original worktree)\n- User must navigate to worktree: `cd .worktrees/autonomy/<strategy-name>`\n- User runs `/start-iteration` to begin work\n- All autonomy skills work normally in worktree\n\n## Integration with Parallel Agents\n\n**Typical workflow:**\n```bash\n# Terminal 1: First agent\n/fork-worktree experiment-a\ncd .worktrees/autonomy/experiment-a\n/start-iteration\n# Agent 1 works here\n\n# Terminal 2: Second agent (can run while agent 1 is working)\n/fork-worktree experiment-b\ncd .worktrees/autonomy/experiment-b\n/start-iteration\n# Agent 2 works here independently\n\n# Terminal 3: Analysis (from anywhere)\n/list-branches\n/compare-branches experiment-a experiment-b\n```\n\nEach agent has:\n- Isolated working directory\n- Independent iteration journals\n- Separate uncommitted changes\n- Shared git history and tags"
              },
              {
                "name": "listing-branches",
                "description": "Use when user wants to inventory autonomy branches with custom sorting, grouping, or filtering",
                "path": "plugins/autonomy/skills/listing-branches/SKILL.md",
                "frontmatter": {
                  "name": "listing-branches",
                  "description": "Use when user wants to inventory autonomy branches with custom sorting, grouping, or filtering"
                },
                "content": "# Listing Branches\n\n## Overview\n\nDisplay inventory of all autonomy branches with user-specified sorting, grouping, and information display using computational analysis.\n\n**Core principle:** Use branch-analyzer agent with Python scripts for precise analysis. Never \"eyeball it\".\n\n## When to Use\n\nUse this skill when:\n- User runs `/list-branches` command\n- User wants to see all autonomy branches\n- User wants custom sorting or grouping of branches\n- User wants to filter branches by criteria\n\n**DO NOT use for:**\n- Analyzing single branch (use analyzing-branch-status instead)\n- Comparing two branches (use comparing-branches instead)\n- General iteration review on current branch (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse query | Extract sorting/grouping/filter requirements | Manual |\n| 2. Dispatch agent | Send query to branch-analyzer | Task |\n| 3. Format results | Present markdown table to user | Direct output |\n\n## Process\n\n### Step 1: Parse User Query\n\nExtract requirements from user's query (if provided):\n\n**Default (no query):**\n- Sort by: most recent update (git log date)\n- Grouping: none (flat list)\n- Show: branch name, latest iteration, last update, status\n\n**Parse user query for:**\n- **Sorting:** \"sort by [criterion]\" → most recent, alphabetical, iteration count, status\n- **Grouping:** \"group by [field]\" → status, date range, metric value\n- **Filtering:** \"show only [criteria]\" → active, blocked, updated since date\n- **Information:** \"with [fields]\" → metrics, blockers, next steps\n\n**Example queries:**\n```\n\"sort by most recent, show only active\"\n  → Sort: recency, Filter: status=active, Show: default fields\n\n\"group by status, show metrics\"\n  → Group: status, Sort: recency within groups, Show: + metrics\n\n\"show branches updated in last 30 days\"\n  → Filter: date > (today - 30 days), Sort: recency, Show: default\n```\n\n### Step 2: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with detailed instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"List all autonomy branches and analyze their status.\n\nUser query: [user's query or 'default: sort by most recent']\n\nRequirements:\n- Find all branches matching 'autonomy/*'\n- For each branch, find most recent journal commit (starts with 'journal: ')\n- Parse commit message for: status, metrics, blockers, next steps\n- [Apply sorting: {criterion}]\n- [Apply grouping: {field}]\n- [Apply filtering: {criteria}]\n- Generate Python script to process data\n- Output markdown table with columns: [requested fields]\n\nUse computational methods (Python scripts), do not eyeball the analysis.\"\n```\n\n**Agent will:**\n1. Run `git branch -a | grep 'autonomy/'` to list all autonomy branches\n2. For each branch, find most recent journal commit\n3. Parse commit message metadata (Status, Metrics, Blockers, Next)\n4. Generate Python script to sort/group/filter\n5. Execute Python script\n6. Return formatted markdown table\n\n### Step 3: Present Results\n\nDisplay agent's output to user.\n\n**Example output format:**\n```markdown\n# Autonomy Branches\n\nShowing 3 branches (sorted by most recent update)\n\n| Branch | Latest Iteration | Last Updated | Status | Metrics | Next |\n|--------|------------------|--------------|--------|---------|------|\n| experiment-a | 0028 | 2026-01-02 | blocked | MRR: $62k (+12%) | Resolve Stripe API integration |\n| experiment-b | 0015 | 2025-12-28 | active | Build: 3.2min (-40%) | Implement checkout flow |\n| initial-strategy | 0042 | 2025-12-15 | concluded | Churn: 8% (from 13%) | Goal achieved |\n```\n\n**If no autonomy branches found:**\n```markdown\nNo autonomy branches found.\n\nTo create your first autonomy branch:\n1. Run `/create-goal` to set up an open-ended goal\n2. Run `/fork-iteration <strategy-name>` to create autonomy branch\n3. Run `/start-iteration` to begin work\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY operates on `autonomy/*` branches:\n- Filters for branches with `autonomy/` prefix\n- Will not show non-autonomy branches (e.g., `main`, `develop`)\n- For general iteration review, user should use `/review-progress`\n\n### Computational Analysis Required\n\n**DO NOT:**\n- Manually count or sort branches\n- \"Eyeball\" which branches are active\n- Guess at groupings or filters\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Flexible Query Parsing\n\nUser queries are free-text and flexible:\n- Don't require exact syntax\n- Interpret intent from natural language\n- Ask via AskUserQuestion if query is ambiguous\n- Default to sensible behavior if unclear\n\n### No Branch Checkout Required\n\nAll analysis happens via git commands:\n- Never checkout branches\n- Read commit messages via `git log <branch>`\n- Branch-analyzer uses read-only operations\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll manually list branches from git branch output\" | NO. Dispatch branch-analyzer agent for computational analysis. |\n| \"Only 3 branches, I can eyeball the sorting\" | NO. Always use Python scripts for precision. |\n| \"User query is unclear, I'll guess\" | NO. Use AskUserQuestion to clarify if ambiguous. |\n| \"I'll check out each branch to read journals\" | NO. Use git log to read commit messages without checkout. |\n| \"Non-autonomy branch appeared, I'll include it\" | NO. Only autonomy/* branches. Strict filtering. |\n\n## After Listing\n\nOnce branches are listed:\n- Results displayed to user\n- No files created or modified\n- User can drill into specific branch with `/branch-status <branch-name>`\n- User can fork from any listed iteration with `/fork-iteration <iteration> <strategy-name>`"
              },
              {
                "name": "listing-worktrees",
                "description": "Use when user wants to see all autonomy worktrees with their status",
                "path": "plugins/autonomy/skills/listing-worktrees/SKILL.md",
                "frontmatter": {
                  "name": "listing-worktrees",
                  "description": "Use when user wants to see all autonomy worktrees with their status"
                },
                "content": "# Listing Worktrees\n\n## Overview\n\nList all autonomy worktrees in the repository, showing their branch, location, HEAD commit, and lock status. Helps users navigate and manage multiple parallel worktrees.\n\n**Core principle:** Worktrees are working directories, not branches. This lists checked-out worktrees only, not all autonomy branches.\n\n## When to Use\n\nUse this skill when:\n- User runs `/list-worktrees` command\n- User wants to see which autonomy branches have active worktrees\n- User needs to find worktree paths for navigation\n- User wants to identify locked or stale worktrees\n\n**DO NOT use for:**\n- Listing all autonomy branches (use `/list-branches` instead)\n- Analyzing branch status or progress (use `/branch-status`)\n- Comparing branches (use `/compare-branches`)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Get all worktrees | Run git worktree list | Bash |\n| 2. Filter to autonomy | Keep only .worktrees/autonomy/ paths | Bash |\n| 3. Format output | Create table with branch, path, HEAD, lock status | Manual |\n| 4. Provide guidance | Add navigation and management hints | Direct output |\n\n## Process\n\n### Step 1: Get All Worktrees\n\nRetrieve complete worktree list from git:\n\n```bash\n# Get porcelain format for machine parsing\nworktree_list=$(git worktree list --porcelain)\n\n# Format:\n# worktree /path/to/worktree\n# HEAD <commit-hash>\n# branch refs/heads/<branch-name>\n#\n# worktree /path/to/another\n# ...\n```\n\n**Parse porcelain output:**\n```bash\n# Extract worktree information\n# Each worktree is separated by blank line\n# Fields: worktree, HEAD, branch, detached, locked\n\n# Example parsing with awk:\ngit worktree list --porcelain | awk '\n  /^worktree / { path = substr($0, 10) }\n  /^HEAD / { head = substr($0, 6); head_short = substr(head, 1, 7) }\n  /^branch / { branch = substr($0, 8); gsub(\"refs/heads/\", \"\", branch) }\n  /^locked/ { locked = \"yes\" }\n  /^$/ {\n    if (path != \"\") {\n      print path \"|\" head_short \"|\" branch \"|\" locked\n      path = \"\"; head = \"\"; head_short = \"\"; branch = \"\"; locked = \"no\"\n    }\n  }\n  END {\n    if (path != \"\") {\n      print path \"|\" head_short \"|\" branch \"|\" locked\n    }\n  }\n'\n```\n\n### Step 2: Filter to Autonomy Worktrees\n\nKeep only worktrees in `.worktrees/autonomy/`:\n\n```bash\n# Filter parsed output to autonomy worktrees only\nautonomy_worktrees=$(echo \"$all_worktrees\" | grep \"\\.worktrees/autonomy/\")\n\n# Count autonomy worktrees\ncount=$(echo \"$autonomy_worktrees\" | grep -c \"^\" || echo \"0\")\n\nif [ \"$count\" -eq 0 ]; then\n  echo \"No autonomy worktrees found.\"\n  echo \"\"\n  echo \"Autonomy branches exist in main repository or without worktrees.\"\n  echo \"\"\n  echo \"To create a worktree:\"\n  echo \"  /fork-worktree <strategy-name>\"\n  echo \"\"\n  echo \"To see all autonomy branches:\"\n  echo \"  /list-branches\"\n  exit 0\nfi\n```\n\n### Step 3: Format Output Table\n\nCreate readable table with worktree information:\n\n```markdown\nAutonomy Worktrees:\n\nBranch                    Path                                      HEAD       Locked\nautonomy/experiment-a     .worktrees/autonomy/experiment-a          a1b2c3d\nautonomy/experiment-b     .worktrees/autonomy/experiment-b          d4e5f6g    🔒\nautonomy/cdn-optimize     .worktrees/autonomy/cdn-optimize          h7i8j9k\n\nTotal: 3 autonomy worktrees\n```\n\n**Column specifications:**\n- **Branch**: Full branch name (`autonomy/<strategy-name>`)\n- **Path**: Relative path from repository root\n- **HEAD**: Short commit hash (7 chars)\n- **Locked**: 🔒 if locked, empty otherwise\n\n**Implementation:**\n```bash\n# Print header\nprintf \"%-25s %-41s %-10s %s\\n\" \"Branch\" \"Path\" \"HEAD\" \"Locked\"\n\n# Print each worktree\necho \"$autonomy_worktrees\" | while IFS='|' read -r path head branch locked; do\n  # Make path relative to repo root if absolute\n  rel_path=$(realpath --relative-to=\"$repo_root\" \"$path\" 2>/dev/null || echo \"$path\")\n\n  # Lock indicator\n  lock_icon=\"\"\n  if [ \"$locked\" = \"yes\" ]; then\n    lock_icon=\"🔒\"\n  fi\n\n  # Print row\n  printf \"%-25s %-41s %-10s %s\\n\" \"$branch\" \"$rel_path\" \"$head\" \"$lock_icon\"\ndone\n```\n\n### Step 4: Provide Guidance\n\nAdd helpful navigation and management instructions:\n\n```markdown\nTo navigate to a worktree:\n  cd .worktrees/autonomy/<strategy-name>\n\nTo remove a worktree:\n  /remove-worktree <strategy-name>\n\nTo create a new worktree:\n  /fork-worktree <strategy-name>\n\nNote: This lists worktrees only. To see all autonomy branches (including those without worktrees):\n  /list-branches\n```\n\n**Additional context:**\n```markdown\nLocked worktrees (🔒):\n- Cannot be removed without unlocking\n- To unlock: git worktree unlock .worktrees/autonomy/<strategy-name>\n```\n\n## Important Notes\n\n### Worktrees vs Branches\n\n**This command shows worktrees, not branches:**\n- A branch may exist without a worktree (created via `/fork-iteration`)\n- A branch with worktree can also be checked out in main repo (though git prevents this)\n- Use `/list-branches` to see all autonomy branches regardless of worktrees\n\n**Example scenario:**\n```\nBranches in repo:\n- autonomy/experiment-a (has worktree)\n- autonomy/experiment-b (has worktree)\n- autonomy/experiment-c (no worktree, created via /fork-iteration)\n\n/list-worktrees shows: experiment-a, experiment-b\n/list-branches shows: experiment-a, experiment-b, experiment-c\n```\n\n### Main Worktree\n\nThe main repository working directory is technically a worktree, but NOT listed here:\n- We filter to `.worktrees/autonomy/` only\n- Main repo worktree is not part of parallel agent workflow\n- Focus on dedicated worktrees for clarity\n\n### Locked Worktrees\n\nWorktrees can be locked to prevent accidental removal:\n\n```bash\n# Lock a worktree\ngit worktree lock .worktrees/autonomy/<strategy-name>\n\n# Lock with reason\ngit worktree lock --reason \"Long-running experiment\" .worktrees/autonomy/<strategy-name>\n\n# Unlock\ngit worktree unlock .worktrees/autonomy/<strategy-name>\n```\n\n**When locked:**\n- `/remove-worktree` fails\n- Must unlock manually before removal\n- Useful for protecting important worktrees\n\n### Relative Paths\n\nPaths displayed are relative to repository root:\n- `.worktrees/autonomy/experiment-a` (not absolute `/home/user/repo/.worktrees/...`)\n- Easier to copy-paste for `cd` commands\n- Consistent regardless of where command invoked\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"This shows all autonomy branches\" | NO. Only branches with worktrees. Use /list-branches for all branches. |\n| \"Main repo appears in list\" | NO. Only dedicated worktrees in .worktrees/autonomy/. |\n| \"I can see iteration progress here\" | NO. This is just worktree metadata. Use /branch-status for iteration progress. |\n| \"I'll parse git worktree list directly\" | YES, but use --porcelain for reliable parsing. |\n| \"All worktrees are unlocked\" | NO. Check locked field; some may be locked. |\n\n## After Listing Worktrees\n\nUser can:\n- Navigate to worktree: `cd .worktrees/autonomy/<strategy-name>`\n- Remove worktree: `/remove-worktree <strategy-name>`\n- View branch status: `/branch-status <strategy-name>`\n- Compare worktrees: `/compare-branches <strategy-a> <strategy-b>`\n\n## Integration with Other Commands\n\n**Relationship to other autonomy commands:**\n\n| Command | Shows | Purpose |\n|---------|-------|---------|\n| `/list-worktrees` | Worktrees only | Navigate to worktree directories |\n| `/list-branches` | All autonomy branches | See all exploration branches |\n| `/branch-status` | Single branch details | Iteration progress and status |\n| `/compare-branches` | Two branches comparison | Compare different approaches |\n\n**Typical workflow:**\n```bash\n# See which worktrees exist\n/list-worktrees\n\n# Navigate to one\ncd .worktrees/autonomy/experiment-a\n\n# Work on iteration\n/start-iteration\n# ... work ...\n/end-iteration\n\n# Check all branches (including ones without worktrees)\n/list-branches\n\n# Clean up worktree when done\ncd <repo-root>\n/remove-worktree experiment-a\n```\n\n## Empty State\n\nWhen no autonomy worktrees exist:\n```markdown\nNo autonomy worktrees found.\n\nAutonomy branches may exist in main repository or without worktrees.\n\nTo create a worktree:\n  /fork-worktree <strategy-name>\n\nTo see all autonomy branches:\n  /list-branches\n\nTo work on autonomy branch in main repo:\n  /fork-iteration <strategy-name>\n  /start-iteration\n```"
              },
              {
                "name": "removing-worktree",
                "description": "Use when user wants to safely remove an autonomy worktree while preserving the branch",
                "path": "plugins/autonomy/skills/removing-worktree/SKILL.md",
                "frontmatter": {
                  "name": "removing-worktree",
                  "description": "Use when user wants to safely remove an autonomy worktree while preserving the branch"
                },
                "content": "# Removing Worktree\n\n## Overview\n\nSafely remove an autonomy worktree directory while preserving the autonomy branch, all commits, and iteration history. Only removes the working directory; git history remains intact.\n\n**Core principle:** Worktree removal is destructive for working directory but non-destructive for git history. Branch and all commits persist.\n\n## When to Use\n\nUse this skill when:\n- User runs `/remove-worktree` command\n- User finished with worktree and wants to clean up disk space\n- User wants to recreate worktree from scratch\n- User made mistakes in worktree and wants fresh start\n\n**DO NOT use for:**\n- Deleting branches (use git directly)\n- Deleting iterations or journals (those are committed, can't be removed this way)\n- Cleaning up non-autonomy worktrees\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract --force flag and strategy-name | Manual |\n| 2. Detect repository root | Find git root from anywhere | Bash |\n| 3. Validate worktree exists | Check .worktrees/autonomy/<strategy-name>/ exists | Bash |\n| 4. Safety checks | Check uncommitted changes (unless --force) | Bash |\n| 5. Remove worktree | git worktree remove | Bash |\n| 6. Cleanup metadata | git worktree prune | Bash |\n| 7. Report success | Confirm removal, note branch persists | Direct output |\n\n## Process\n\n### Step 1: Parse Arguments\n\nExtract arguments from command args:\n\n**Format:** `[--force] <strategy-name>`\n\n**Parse:**\n```\nforce = false\nstrategy_name = \"\"\n\nfor arg in args:\n  if arg == \"--force\":\n    force = true\n  else:\n    if strategy_name != \"\":\n      Error: \"Multiple strategy names provided. Usage: /remove-worktree [--force] <strategy-name>\"\n    strategy_name = arg\n\nif strategy_name == \"\":\n  Error: \"Strategy name required. Usage: /remove-worktree [--force] <strategy-name>\"\n```\n\n**Normalize strategy-name:**\n```bash\n# Remove 'autonomy/' prefix if user included it\nstrategy_name=$(echo \"$strategy_name\" | sed 's/^autonomy\\///')\n```\n\n### Step 2: Detect Repository Root\n\nFind the git repository root (same logic as forking-worktree):\n\n```bash\n# Get common git directory\ngit_common_dir=$(git rev-parse --git-common-dir)\n\n# Repository root is parent of .git directory\nif [ -d \"$git_common_dir\" ]; then\n  repo_root=$(cd \"$git_common_dir/..\" && pwd)\nelse\n  echo \"Error: Unable to determine repository root\"\n  exit 1\nfi\n```\n\n### Step 3: Validate Worktree Exists\n\nCheck that the worktree actually exists:\n\n```bash\nworktree_path=\"$repo_root/.worktrees/autonomy/$strategy_name\"\n\nif [ ! -d \"$worktree_path\" ]; then\n  echo \"Error: Worktree not found: $worktree_path\"\n  echo \"\"\n  echo \"Available autonomy worktrees:\"\n  if [ -d \"$repo_root/.worktrees/autonomy\" ]; then\n    ls -1 \"$repo_root/.worktrees/autonomy\"\n  else\n    echo \"  (none)\"\n  fi\n  echo \"\"\n  echo \"To see all worktrees: /list-worktrees\"\n  exit 1\nfi\n```\n\n### Step 4: Safety Checks\n\nCheck for uncommitted changes (unless `--force`):\n\n```bash\nif [ \"$force\" = false ]; then\n  # Change to worktree directory to check status\n  cd \"$worktree_path\"\n\n  # Check for uncommitted changes\n  if ! git diff-index --quiet HEAD -- 2>/dev/null; then\n    echo \"Error: Worktree has uncommitted changes\"\n    echo \"\"\n    echo \"Uncommitted changes in $worktree_path:\"\n    git status --short\n    echo \"\"\n    echo \"Options:\"\n    echo \"  1. Commit changes:\"\n    echo \"     cd $worktree_path\"\n    echo \"     /end-iteration  # or git commit\"\n    echo \"\"\n    echo \"  2. Force removal (discards changes):\"\n    echo \"     /remove-worktree --force $strategy_name\"\n    exit 1\n  fi\n\n  # Check for untracked files (warn but allow)\n  untracked_count=$(git ls-files --others --exclude-standard | wc -l)\n  if [ \"$untracked_count\" -gt 0 ]; then\n    echo \"Warning: Worktree has $untracked_count untracked file(s)\"\n    echo \"\"\n    git ls-files --others --exclude-standard | head -10\n    echo \"\"\n    echo \"Proceeding with removal. Untracked files will be deleted.\"\n    echo \"\"\n  fi\nfi\n```\n\n### Step 5: Remove Worktree\n\nRemove the worktree using git:\n\n```bash\n# Return to repo root for git worktree commands\ncd \"$repo_root\"\n\n# Remove worktree\nif [ \"$force\" = true ]; then\n  git worktree remove --force \".worktrees/autonomy/$strategy_name\"\nelse\n  git worktree remove \".worktrees/autonomy/$strategy_name\"\nfi\n\n# Check if removal succeeded\nif [ $? -ne 0 ]; then\n  echo \"Error: git worktree remove failed\"\n  echo \"\"\n  echo \"Manual removal command:\"\n  echo \"  cd $repo_root\"\n  echo \"  git worktree remove --force .worktrees/autonomy/$strategy_name\"\n  echo \"  git worktree prune\"\n  exit 1\nfi\n```\n\n### Step 6: Cleanup Metadata\n\nPrune stale worktree metadata:\n\n```bash\n# Clean up any stale administrative files\ngit worktree prune\n\n# Verify worktree removed from git's list\nif git worktree list | grep -q \".worktrees/autonomy/$strategy_name\"; then\n  echo \"Warning: Worktree still appears in git worktree list\"\n  echo \"Manual cleanup may be needed:\"\n  echo \"  git worktree prune --verbose\"\nfi\n```\n\n### Step 7: Report Success\n\nAnnounce successful removal with important notes:\n\n```markdown\n✓ Worktree removed successfully\n\nRemoved: .worktrees/autonomy/<strategy-name>/\n\nBranch preserved:\n- Branch: autonomy/<strategy-name>\n- All commits and iteration tags remain in git history\n- Can checkout branch later: git checkout autonomy/<strategy-name>\n- Can create new worktree: /fork-worktree <strategy-name>  # (will fail if branch exists, use different name or delete branch first)\n\nTo delete the branch entirely:\n  git branch -d autonomy/<strategy-name>  # Safe delete (only if merged)\n  git branch -D autonomy/<strategy-name>  # Force delete\n```\n\n**If force was used:**\n```markdown\n⚠️  Force removal completed\n\nAny uncommitted changes in the worktree were discarded.\n```\n\n## Important Notes\n\n### Destructive vs Non-Destructive\n\n**What gets destroyed (non-recoverable):**\n- Worktree working directory (`.worktrees/autonomy/<strategy-name>/`)\n- Any uncommitted changes in that directory\n- Any untracked files in that directory\n\n**What persists (safe):**\n- Branch `autonomy/<strategy-name>` (can still git checkout)\n- All committed iteration journals\n- All iteration tags (`autonomy/<strategy-name>/iteration-NNNN`)\n- All git history and commits\n\n### Branch Still Exists\n\nAfter worktree removal:\n- Branch is \"unlocked\" (can be checked out elsewhere)\n- All commits remain in git history\n- Can checkout in main repo: `git checkout autonomy/<strategy-name>`\n- Can create new worktree for same branch (but this skill creates new branch, not for existing)\n- To work with existing branch in worktree: manual git command needed\n\n### Force Flag Behavior\n\n**Without `--force`:**\n- Checks for uncommitted changes (fails if found)\n- Warns about untracked files (but proceeds)\n- Safe for preserving work\n\n**With `--force`:**\n- Skips all safety checks\n- Discards uncommitted changes\n- Deletes untracked files\n- Use when worktree is broken or changes are intentional throwaways\n\n### Cannot Remove Current Worktree\n\nGit prevents removing the worktree you're currently in:\n\n```bash\n# This fails:\ncd .worktrees/autonomy/experiment-a\n/remove-worktree experiment-a\n# Error: Cannot remove current working tree\n```\n\n**Solution:** Navigate to different directory first:\n```bash\ncd \"$repo_root\"  # Or any other directory\n/remove-worktree experiment-a\n```\n\n### Locked Worktrees\n\nIf worktree was locked (via `git worktree lock`):\n- Removal fails with error\n- Must unlock first: `git worktree unlock .worktrees/autonomy/<strategy-name>`\n- Then retry removal\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"This deletes the branch\" | NO. Only removes worktree directory. Branch persists. |\n| \"This deletes iteration journals\" | NO. Journals are committed. They persist in git history. |\n| \"I can remove worktree I'm currently in\" | NO. Must navigate away first. |\n| \"Force is always safe\" | NO. Force discards uncommitted work. Use carefully. |\n| \"After removal, branch is gone\" | NO. Branch still exists. Can checkout or delete separately. |\n| \"I'll also clean up the branch\" | NO. Worktree removal is separate from branch deletion. |\n\n## After Removing Worktree\n\nOnce worktree is removed:\n- Directory `.worktrees/autonomy/<strategy-name>/` no longer exists\n- Branch `autonomy/<strategy-name>` still exists\n- All iteration journals in git history\n- All tags preserved\n- Branch can be checked out in main repo or new worktree created for it\n- Disk space freed\n\n## Manual Recovery\n\nIf automated removal fails or worktree is in broken state:\n\n```bash\n# Manual removal commands\ncd <repo-root>\n\n# Force remove worktree\ngit worktree remove --force .worktrees/autonomy/<strategy-name>\n\n# If that fails, manually delete directory then prune\nrm -rf .worktrees/autonomy/<strategy-name>\ngit worktree prune\n\n# Verify cleanup\ngit worktree list\n```\n\n## Relationship to Branch Deletion\n\n**Worktree removal ≠ Branch deletion**\n\nAfter removing worktree:\n```bash\n# Branch still exists\ngit branch -a | grep autonomy/<strategy-name>\n# autonomy/<strategy-name>\n\n# To also delete branch:\ngit branch -D autonomy/<strategy-name>\n\n# This removes branch and all its commits from local repo\n# (Tags remain unless explicitly deleted)\n```\n\nUse worktree removal for cleanup, branch deletion for ending exploration."
              },
              {
                "name": "reviewing-progress",
                "description": "Use when user wants to assess progress toward an open-ended goal by reading and summarizing all iteration journals",
                "path": "plugins/autonomy/skills/reviewing-progress/SKILL.md",
                "frontmatter": {
                  "name": "reviewing-progress",
                  "description": "Use when user wants to assess progress toward an open-ended goal by reading and summarizing all iteration journals"
                },
                "content": "# Reviewing Progress\n\n## Overview\n\nAssess progress toward an open-ended goal by reading iteration journals, summarizing achievements, identifying patterns, and suggesting next steps.\n\n**Core principle:** Regular review reveals trends, validates strategy, and informs future direction.\n\n## When to Use\n\nUse this skill when:\n- User runs `/review-progress` command\n- User asks \"how are we doing?\" or \"what's our progress?\"\n- Need to assess if current strategy is working\n- Deciding whether to pivot approach\n- Preparing report on goal status\n\n**Can be used:**\n- Mid-iteration (doesn't end current iteration)\n- Standalone (no active iteration required)\n- Anytime user wants status check\n\n## Quick Reference\n\n| Step | Action | Tool/Agent |\n|------|--------|------------|\n| 1. Locate goal | Find autonomy directory | Glob |\n| 2. Load summary | Read summary.md if exists | Read |\n| 3. Load recent iterations | Get last 3-5 iterations | Task (journal-reader) |\n| 4. Analyze progress | Identify trends, metrics, patterns | Manual analysis |\n| 5. Present report | Structured progress summary | Direct output |\n\n## Process\n\n### Step 1: Locate Goal\n\nFind the goal directory:\n\n```bash\n# Use Glob to find goal\npattern: \"autonomy/*/goal.md\"\n```\n\n**If multiple goals found:**\n- List goals and ask user which to review\n- If only one goal, proceed with that one\n\n**If no goal found:**\n```\n\"No autonomy goal found in this project. Use `/start-iteration` to begin tracking an open-ended goal.\"\n```\n\n### Step 2: Load Summary (if exists)\n\nCheck for summary.md:\n\n```bash\n# Use Read to load summary\nfile: \"autonomy/[goal-name]/summary.md\"\n```\n\n**If summary exists:**\n- Read full content\n- Note which iterations it covers\n- Use as foundation for report\n\n**If summary doesn't exist:**\n- Will rely on reading iterations directly\n\n### Step 3: Load Recent Iterations\n\nDispatch journal-reader to load recent context:\n\n```\nTask tool with subagent_type: \"autonomy:journal-reader\"\nPrompt: \"Read all iteration files for goal '[goal-name]' (or last 5-10 if many exist).\n        Extract:\n        - Progress timeline\n        - Completed work by iteration\n        - Persistent blockers\n        - Metric trends\n        - Skills most frequently used\n        - Strategic pivots\"\nModel: haiku\n```\n\nWait for agent response with structured findings.\n\n### Step 4: Analyze Progress\n\nSynthesize information from summary and journal-reader:\n\n**Calculate metrics:**\n- If goal tracks metrics: Starting value → Current value → % change\n- Iteration count: How many iterations completed?\n- Time elapsed: First iteration date → Latest iteration date\n\n**Identify patterns:**\n- Which types of work have been most common?\n- Are there recurring blockers?\n- Has strategy evolved or stayed consistent?\n- Which skills/workflows used most frequently?\n\n**Assess trajectory:**\n- Is progress accelerating, steady, or slowing?\n- Are recent iterations more or less productive?\n- Is current approach working?\n\n**Flag concerns:**\n- Blockers appearing in multiple iterations (persistent issues)\n- Metrics stagnating or declining\n- Lack of clear recent progress\n- Strategy thrashing (frequent pivots without validation)\n\n### Step 5: Present Report\n\nGenerate comprehensive progress report:\n\n```markdown\n# Progress Report: [Goal Name]\n\n**Report Date:** [Today's date]\n**Goal Status:** Active\n**Iterations Completed:** [N]\n**Time Elapsed:** [First date] - [Latest date] ([X days/weeks/months])\n\n---\n\n## Goal Statement\n[From goal.md]\n\n---\n\n## Progress Overview\n\n### Key Metrics\n| Metric | Starting | Current | Change |\n|--------|----------|---------|--------|\n| [Metric 1] | [Value] | [Value] | [+X% or -Y%] |\n| [Metric 2] | [Value] | [Value] | [+X% or -Y%] |\n\n### Timeline of Major Work\n- **Iteration 1:** [Summary of work]\n- **Iteration 2:** [Summary of work]\n- **Iteration N:** [Summary of work]\n\n### Completed Initiatives\n✅ [Initiative 1]: [Outcome]\n✅ [Initiative 2]: [Outcome]\n\n### In Progress\n🚧 [Initiative 3]: [Current state]\n\n---\n\n## Current State\n\n### What's Working Well\n- [Positive pattern 1]\n- [Positive pattern 2]\n\n### Current Blockers\n- **[Blocker 1]:** [First appeared iteration X, still unresolved]\n- **[Blocker 2]:** [Description and impact]\n\n### Open Questions\n- [Question 1]\n- [Question 2]\n\n---\n\n## Analysis\n\n### Strategic Evolution\n[How has the approach changed over time?]\n- Iteration 1-3: [Initial strategy]\n- Iteration 4-6: [Pivot or continuation]\n- Current: [Where we are now]\n\n### Skills & Methods Most Used\n- **[Skill/workflow]:** Used in [X] iterations for [purpose]\n- **[Skill/workflow]:** Used in [Y] iterations for [purpose]\n\n### Effectiveness Assessment\n[Is the current approach working?]\n- **Strengths:** [What's effective]\n- **Weaknesses:** [What's not working]\n- **Opportunities:** [What could be explored next]\n\n---\n\n## Recommendations\n\n### Immediate Next Steps\n1. [Specific action based on current state]\n2. [Specific action based on blockers]\n\n### Strategic Considerations\n- [Consider pivot if X]\n- [Double down on Y because Z]\n- [Investigate new approach for A]\n\n### Health Check\n[Overall assessment: Is goal on track? Should strategy change? Is this goal still valuable?]\n\n---\n\n**This is an open-ended goal - continuous iteration and optimization expected.**\n```\n\n## Important Notes\n\n### Mid-Iteration Review\n\nIf reviewing during active iteration:\n- Include current iteration work in report\n- Note that current iteration is in progress\n- Don't write journal entry (that's ending-an-iteration's job)\n\n### Standalone Review\n\nIf no iteration active:\n- Report based solely on journal history\n- Suggest starting new iteration if next steps are clear\n\n### Frequency Recommendations\n\nSuggest reviewing progress:\n- Every 5 iterations (natural checkpoint)\n- When considering strategy pivot\n- Before major decisions\n- If feeling lost or uncertain\n\n### Honesty in Assessment\n\n**Be honest:**\n- If metrics aren't improving, say so\n- If approach isn't working, acknowledge it\n- If goal seems stalled, flag it\n- Don't sugarcoat to avoid disappointing user\n\n**Constructive:**\n- Always include specific next steps\n- Suggest concrete changes if needed\n- Identify what IS working to preserve\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Progress looks good\" without data | NO. Use specific metrics and evidence. |\n| \"I'll review just the last iteration\" | NO. Look at full history for patterns. |\n| \"No need to flag concerns\" | NO. Honest assessment helps course-correct. |\n| \"I'll skip analysis and just list work done\" | NO. Synthesis and insights are the value. |\n| \"Goal is open-ended so no progress metrics\" | NO. Even open goals have measurable indicators. |\n\n## After Reviewing\n\nOnce review is complete:\n- User has clear picture of progress\n- Can decide: continue current path, pivot strategy, or pause goal\n- Next iteration can incorporate review insights\n- May identify need to update goal.md with new success criteria"
              },
              {
                "name": "slime-strategy",
                "description": "Use when user wants to set up slime mold exploration strategy with parallel autonomy branches for genetic algorithm approach to problem-solving",
                "path": "plugins/autonomy/skills/slime-strategy/SKILL.md",
                "frontmatter": {
                  "name": "slime-strategy",
                  "description": "Use when user wants to set up slime mold exploration strategy with parallel autonomy branches for genetic algorithm approach to problem-solving"
                },
                "content": "# Slime Mold Strategy Setup\n\n## Overview\n\nSet up the complete \"slime mold strategy\" workflow for exploring problem spaces through parallel autonomy branches that cooperate like a genetic algorithm.\n\n**Core principle:** Initialize autonomy workflow with slime mold philosophy: multiple parallel branches exploring different approaches, cooperating not competing, cross-pollinating insights.\n\n## When to Use\n\nUse this skill when:\n- User runs `/slime` command\n- User wants to adopt slime mold strategy for exploration\n- Setting up parallel branch exploration with genetic algorithm approach\n\n**DO NOT use for:**\n- Adding single goal without branch strategy (use `/create-goal`)\n- Creating additional branches in existing workflow (use `/fork-iteration`)\n- Analyzing existing branches (use `/branch-status` or `/compare-branches`)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Check state | Look for existing autonomy goal | Glob |\n| 2a. Create goal | Invoke creating-a-goal skill if needed | Skill |\n| 2b. Update only | Skip to CLAUDE.md if goal exists | - |\n| 3. Update CLAUDE.md | Replace slime mold section or create file | Read, Write |\n| 4. Create branch | Invoke forking-iteration with goal name | Skill |\n| 5. Initialize iteration | Create iteration-0000 journal file | Write |\n| 6. Git commit | Commit with enhanced format and tag | Bash |\n\n## Process\n\n### Step 1: Check Existing State\n\nFirst, determine if autonomy workflow already exists:\n\n```bash\n# Look for any existing goal.md files\nexisting_goals=$(find autonomy -name \"goal.md\" 2>/dev/null)\n\nif [ -n \"$existing_goals\" ]; then\n  mode=\"update-only\"\n  echo \"Autonomy workflow already exists. Will update CLAUDE.md only.\"\n\n  # Extract goal name from first goal found\n  goal_dir=$(dirname \"$existing_goals\" | head -1)\n  goal_name=$(basename \"$goal_dir\")\nelse\n  mode=\"full-setup\"\n  echo \"No existing autonomy goal found. Will perform full setup.\"\nfi\n```\n\n**Validation:**\n- If multiple goals found, use first one alphabetically\n- Warn user: \"Multiple goals detected, using [goal-name]\"\n\n### Step 2: Create Goal (full-setup mode only)\n\nIf `mode=\"full-setup\"`, invoke the creating-a-goal skill:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Invoke creating-a-goal skill using Skill tool\n  skill: \"autonomy:creating-a-goal\"\n\n  # After skill completes, extract goal name\n  goal_dir=$(find autonomy -name \"goal.md\" -exec dirname {} \\; | head -1)\n  goal_name=$(basename \"$goal_dir\")\n\n  echo \"Goal created: $goal_name\"\nfi\n```\n\n**Error Handling:**\n- If creating-a-goal fails or user cancels\n- Abort entire workflow\n- Show: \"Goal creation cancelled. Slime mold setup aborted.\"\n- Don't proceed to CLAUDE.md or iteration 0000\n\n### Step 3: Create/Update autonomy/CLAUDE.md\n\nEnsure `autonomy/CLAUDE.md` exists and contains current slime mold strategy documentation.\n\n**CLAUDE.md Content (exact text to use):**\n\n```markdown\n# Slime Mold Strategy\n\nThis project uses the \"slime mold strategy\" for exploring the problem space. Like a slime mold organism that extends multiple tendrils to find optimal paths, we maintain parallel autonomy branches that explore different approaches to the same goal. These branches represent a genetic algorithm: each branch is an independent experiment testing different hypotheses, strategies, or implementations.\n\nCrucially, these branches are NOT competing—they are cooperating. All branches are part of the same organism pursuing the same goal. When branches encounter each other or when you discover useful insights in one branch, use `/analyze-branch <branch-name> <search-description>` to extract \"genetic material\" (learnings, patterns, solutions) and incorporate them into your current branch. Use `/compare-branches <branch-a> <branch-b>` to understand how different approaches diverged and what outcomes each achieved. Use `/list-branches` to inventory all active exploration tendrils. When ready to fork a new experimental direction, use `/fork-iteration [iteration] <strategy-name>` to create a new tendril from any point in history.\n\nThe goal is not to find the single \"best\" branch, but to explore the solution space thoroughly and cross-pollinate insights across branches, allowing the organism as a whole to discover optimal solutions through parallel exploration and cooperation.\n```\n\n**Implementation:**\n\n```bash\n# Check if autonomy/CLAUDE.md exists\nif [ -f \"autonomy/CLAUDE.md\" ]; then\n  # Read existing content\n  existing_content=$(cat autonomy/CLAUDE.md)\n\n  # Check if slime mold section exists\n  if echo \"$existing_content\" | grep -q \"# Slime Mold Strategy\"; then\n    echo \"Updating existing slime mold section in CLAUDE.md\"\n\n    # Strategy: Replace section from \"# Slime Mold Strategy\" to next \"# \" heading or EOF\n    # Use Read tool to get current content\n    # Use Edit tool to replace the section\n\n    # Read current file\n    # Find start of \"# Slime Mold Strategy\" section\n    # Find end (next # heading or EOF)\n    # Replace that section with new content\n  else\n    echo \"Appending slime mold section to existing CLAUDE.md\"\n\n    # Append new section to end of file\n    # Use Edit tool to add content at end\n  fi\nelse\n  echo \"Creating new autonomy/CLAUDE.md with slime mold section\"\n\n  # Create new file with slime mold section\n  # Use Write tool to create file\nfi\n```\n\n**Write/Edit the slime mold section:**\n\nUse Write tool if creating new file, or Edit tool if updating existing file.\n\nThe section content is the markdown block shown above under \"CLAUDE.md Content\".\n\n### Step 4: Create Initial Branch (full-setup mode only)\n\nIf `mode=\"full-setup\"`, create the initial autonomy branch:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Invoke forking-iteration skill with goal-name as strategy-name\n  # No iteration number specified = fork from current HEAD\n\n  skill: \"autonomy:forking-iteration\"\n  args: \"$goal_name\"\n\n  # This creates branch: autonomy/[goal-name]\n  # And checks out that branch\n\n  echo \"Created and switched to branch: autonomy/$goal_name\"\nfi\n```\n\n**Error Handling:**\n- If forking-iteration fails (e.g., branch already exists)\n- Show error from skill\n- Don't create iteration 0000\n- Suggest: \"Use /fork-iteration <different-name> to create branch with different name\"\n\n### Step 5: Create Iteration 0000 (full-setup mode only)\n\nIf `mode=\"full-setup\"`, create the baseline setup iteration:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Get current date in YYYY-MM-DD format\n  current_date=$(date +%Y-%m-%d)\n\n  # Create iteration-0000 file\n  iteration_file=\"autonomy/$goal_name/iteration-0000-$current_date.md\"\n\n  # Use Write tool to create file with this content:\nfi\n```\n\n**Iteration 0000 Content:**\n\n```markdown\n# Iteration 0000 - YYYY-MM-DD\n\n## Beginning State\n\nNo previous iterations. This is the initial setup for the slime mold exploration strategy.\n\n**Baseline Metrics:**\n[Extract from goal.md metrics section, or \"None established yet\" if no metrics defined]\n\n## Iteration Intention\n\nEstablish the foundation for slime mold strategy exploration:\n- Define goal and success criteria\n- Create initial autonomy branch\n- Document exploration approach\n- Prepare for iteration 0001 to begin actual work\n\n## Work Performed\n\n### Setup Completed\n- Created goal: [Extract goal statement from goal.md]\n- Established autonomy/CLAUDE.md with slime mold strategy documentation\n- Created initial branch: autonomy/[goal-name]\n- Initialized iteration tracking system\n\n### Slime Mold Strategy Adopted\n- Multiple parallel branches will explore different approaches\n- Branches cooperate via /analyze-branch for cross-pollination\n- Use /fork-iteration to create new exploration tendrils\n- Use /compare-branches to understand divergent paths\n\n## Ending State\n\nAutonomy workflow initialized. Ready to begin iteration 0001 with actual exploration work. Use `/start-iteration` to begin.\n\n**Recommended next action:** Run `/start-iteration` to begin first real iteration of exploration.\n```\n\n**Implementation:**\n\nRead `autonomy/[goal-name]/goal.md` to extract:\n- Goal statement (from first paragraph or \"## Goal\" section)\n- Metrics (from \"## Metrics\" or \"Success Criteria\" section)\n\nUse Write tool to create `iteration-0000-YYYY-MM-DD.md` with content above, substituting:\n- `YYYY-MM-DD` with current date\n- `[goal-name]` with actual goal name\n- Goal statement and metrics from goal.md\n\n### Step 6: Git Commit and Tag (full-setup mode only)\n\nIf `mode=\"full-setup\"`, commit iteration 0000 with enhanced format:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Stage the iteration file and CLAUDE.md\n  git add \"autonomy/$goal_name/iteration-0000-$current_date.md\"\n  git add \"autonomy/CLAUDE.md\"\n\n  # Create enhanced commit message\n  git commit -m \"$(cat <<'EOF'\njournal: $goal_name iteration 0000\n\nEstablished slime mold exploration strategy with initial autonomy branch.\n\n## Journal Summary\n\nCreated goal definition for $goal_name, initialized slime mold strategy documentation in CLAUDE.md, created initial autonomy/$goal_name branch, and established iteration 0000 as baseline. Workflow ready for iteration 0001 to begin actual exploration work.\n\n## Iteration Metadata\n\nStatus: active\nMetrics: None (baseline setup)\nBlockers: None\nNext: Run /start-iteration to begin iteration 0001\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\nEOF\n)\"\n\n  # Create branch-aware tag\n  git tag -a \"autonomy/$goal_name/iteration-0000\" \\\n    -m \"journal: $goal_name iteration 0000\"\n\n  echo \"Created git commit and tag: autonomy/$goal_name/iteration-0000\"\nfi\n```\n\n**Commit Message Format:**\n- First line: `journal: [goal-name] iteration 0000`\n- Blank line\n- 2-3 line summary\n- `## Journal Summary` section (4-6 sentences)\n- `## Iteration Metadata` section (Status, Metrics, Blockers, Next)\n- Claude Code attribution\n\n**Tag Format:**\n- `autonomy/[goal-name]/iteration-0000`\n- Matches branch-aware tagging from ending-an-iteration skill\n\n### Step 7: Final Messages\n\nDisplay completion message based on mode:\n\n**Full-setup mode:**\n```\nSlime mold strategy initialized successfully!\n\n✓ Goal created: [goal-name]\n✓ CLAUDE.md documented slime mold strategy\n✓ Branch created: autonomy/[goal-name]\n✓ Iteration 0000 established as baseline\n✓ Git commit and tag: autonomy/[goal-name]/iteration-0000\n\nNext steps:\n1. Run /start-iteration to begin iteration 0001\n2. Use /fork-iteration <strategy-name> to create additional exploration branches\n3. Use /analyze-branch to cross-pollinate insights between branches\n4. Use /compare-branches to understand divergent approaches\n```\n\n**Update-only mode:**\n```\nSlime mold strategy documentation updated!\n\n✓ CLAUDE.md updated with current slime mold strategy description\n\nYour autonomy workflow is ready. Use these commands:\n- /fork-iteration <strategy-name> - Create new exploration branch\n- /list-branches - Inventory all autonomy branches\n- /compare-branches <a> <b> - Compare two exploration paths\n- /analyze-branch <branch> <search> - Extract insights from another branch\n```\n\n## Important Notes\n\n### Git Repository Required\n\nThis skill requires a git repository:\n- Check with `git rev-parse --git-dir` before starting\n- If not in git repo, show error: \"This command requires a git repository. Run 'git init' first.\"\n- Abort workflow if git not available\n\n### Idempotent Behavior\n\nThe skill is designed to be idempotent:\n- Safe to run multiple times\n- If goal exists: Only updates CLAUDE.md (doesn't recreate goal or iteration 0000)\n- CLAUDE.md section replacement ensures documentation stays current\n- Use this to update slime mold description as concept evolves\n\n### CLAUDE.md Location\n\n- File created at `autonomy/CLAUDE.md` (working directory, not plugin directory)\n- At same level as `autonomy/[goal-name]/` directories\n- Provides project-level context for slime mold strategy\n\n### Iteration 0000 is Special\n\n- Iteration 0000 is baseline setup only\n- First real work iteration is 0001 (via `/start-iteration`)\n- Don't use `starting-an-iteration` skill for iteration 0000\n- Manually create the file with setup-specific content\n\n### Branch Name = Goal Name\n\n- Initial branch always named `autonomy/[goal-name]`\n- Matches goal directory name for consistency\n- Additional branches via `/fork-iteration` can use different strategy names\n- Example: goal \"improve-performance\" → branch \"autonomy/improve-performance\"\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll use starting-an-iteration for iteration 0000\" | NO. Manually create iteration 0000 with setup-specific content. |\n| \"I'll skip CLAUDE.md in update-only mode\" | NO. Always update CLAUDE.md to keep description current. |\n| \"I'll append to CLAUDE.md even if section exists\" | NO. Replace existing slime mold section to keep it current. |\n| \"I'll create iteration 0001 after 0000\" | NO. User runs /start-iteration for 0001. This skill only creates 0000. |\n| \"I'll use different branch name than goal name\" | NO. Initial branch must be autonomy/[goal-name]. Use /fork-iteration for other names. |\n| \"I'll skip git commit if user doesn't want it\" | NO. Git integration is core to autonomy workflow. Always commit. |\n\n## After Setup\n\nOnce `/slime` completes:\n- User should run `/start-iteration` to begin iteration 0001\n- User can run `/fork-iteration <strategy-name>` to create additional branches\n- User can use `/analyze-branch` for cross-pollination\n- User can use `/compare-branches` to understand divergence\n\nThe slime mold strategy is now active and documented in `autonomy/CLAUDE.md`."
              },
              {
                "name": "starting-an-iteration",
                "description": "Use when beginning a new conversation to work on an open-ended goal, loading context from previous iterations through iteration journals",
                "path": "plugins/autonomy/skills/starting-an-iteration/SKILL.md",
                "frontmatter": {
                  "name": "starting-an-iteration",
                  "description": "Use when beginning a new conversation to work on an open-ended goal, loading context from previous iterations through iteration journals"
                },
                "content": "# Starting an Iteration\n\n## Overview\n\nBegin a new iteration for an open-ended goal by loading context from previous iterations, setting up the workspace, and preparing to continue progress.\n\n**Core principle:** Each conversation is one iteration. Load recent state, understand where you left off, continue the journey.\n\n## When to Use\n\nUse this skill when:\n- User runs `/start-iteration` command\n- Beginning work on an ongoing open-ended goal\n- Need to load context from previous conversation sessions\n\n**DO NOT use for:**\n- Closed goals with definition of done (use ed3d-superpowers workflow instead)\n- First-time brainstorming (use brainstorming skill first)\n- One-off tasks that don't need iteration tracking\n\n## Quick Reference\n\n| Step | Action | Tool/Agent |\n|------|--------|------------|\n| 1. Detect goal | Check for autonomy directory | Glob |\n| 2. Load recent context | Read last 3-5 iterations | Task (journal-reader) |\n| 3. Load older context | Summarize if >5 iterations | Task (journal-summarizer) |\n| 4. Present state | Show context to user | Direct output |\n| 5. Create journal | Write initial journal entry | Write |\n| 6. Ready to work | Create TodoWrite tracker | TodoWrite |\n\n## Process\n\n### Step 1: Detect Existing Goal\n\nCheck if a goal already exists in the current project:\n\n```bash\n# Use Glob to check for autonomy directory\npattern: \"autonomy/*/goal.md\"\n```\n\n**If goal.md found:**\n- Extract goal name from directory path\n- Proceed to Step 2 (Load Recent Context)\n\n**If no goal found:**\n```\n\"No autonomy goal found in this project.\n\nUse `/create-goal` to set up a new open-ended goal first, then run `/start-iteration` to begin work.\"\n```\nStop here - cannot start iteration without a goal.\n\n### Step 2: Load Recent Context\n\nFor existing goals, load context from recent iterations:\n\n1. **Count iterations:**\n   ```bash\n   # Use Glob to find iteration files\n   pattern: \"autonomy/[goal-name]/iteration-*.md\"\n   ```\n\n2. **Determine iteration number:**\n   - Count existing files: N\n   - This iteration will be: N+1\n\n3. **Dispatch journal-reader agent:**\n   ```\n   Task tool with subagent_type: \"autonomy:journal-reader\"\n   Prompt: \"Read last 3-5 iterations for goal '[goal-name]' and extract:\n           - Current state\n           - Open blockers and questions\n           - Recent progress\n           - Key metrics\n           - Recommended next steps\"\n   Model: haiku\n   ```\n\n4. **Wait for agent response** with structured context\n\n### Step 3: Load Older Context (if >5 iterations)\n\nIf more than 5 iterations exist, summarize older ones:\n\n1. **Check if summary.md exists:**\n   ```bash\n   # Use Read to check\n   file: \"autonomy/[goal-name]/summary.md\"\n   ```\n\n2. **Determine what needs summarizing:**\n   - If summary.md exists: Check which iterations it covers\n   - If summary.md missing OR outdated: Summarize iterations 1 through (N-5)\n\n3. **Dispatch journal-summarizer agent:**\n   ```\n   Task tool with subagent_type: \"autonomy:journal-summarizer\"\n   Prompt: \"Summarize iterations [range] for goal '[goal-name]'.\n           Extract major initiatives, persistent blockers, key learnings,\n           strategic pivots, and metric trends. Update summary.md.\"\n   Model: haiku\n   ```\n\n4. **Wait for agent to write summary.md**\n\n### Step 4: Present State to User\n\nCombine findings from journal-reader (and summary if applicable):\n\n```markdown\n**Iteration [N] started for goal: [goal-name]**\n\n## Current State\n[From journal-reader: most recent ending state]\n\n## Recent Progress (Last 3-5 Iterations)\n[Condensed summary of what was accomplished]\n\n## Open Blockers\n- [Blocker 1]\n- [Blocker 2]\n\n## Open Questions\n- [Question 1]\n- [Question 2]\n\n## Key Metrics\n- [Metric]: [Current value] (was [previous value])\n\n## Recommended Next Steps\n[From previous iteration's suggestions]\n\n---\n\n**Ready to continue. What should we work on this iteration?**\n```\n\n### Step 5: Create Initial Journal Entry\n\nBefore beginning work, create the journal file for this iteration:\n\n1. **Determine iteration number and filename:**\n   - Count: N existing iterations\n   - This iteration: N+1\n   - Filename: `iteration-NNNN-YYYY-MM-DD.md` (today's date)\n\n2. **Prompt user for iteration intention:**\n   ```\n   \"What do you want to accomplish this iteration?\n\n   This helps keep us honest about the goal. We'll check against this intention when ending the iteration.\"\n   ```\n\n3. **Write initial journal entry:**\n\n```markdown\n# Iteration NNNN - YYYY-MM-DD\n\n## Beginning State\n[From journal-reader output or user context:\n- Current progress summary\n- Known blockers from previous iteration\n- Open questions being addressed\n- Current metric values if tracked]\n\n## Iteration Intention\n[User's stated intention for this iteration]\n\n## Work Performed\n\n### Skills & Workflows Used\n[Will be filled during checkpoint or ending]\n\n### Key Decisions Made\n[Will be filled during checkpoint or ending]\n\n### Artifacts Created/Modified\n[Will be filled during checkpoint or ending]\n\n### External Context Gathered\n[Will be filled during checkpoint or ending]\n\n### Reasoning & Strategy Changes\n[Will be filled during checkpoint or ending]\n\n### Blockers Encountered\n[Will be filled during checkpoint or ending]\n\n### Open Questions\n[Will be filled during checkpoint or ending]\n\n## Ending State\n[Will be filled when ending iteration]\n\n## Iteration Metadata\n[Will be filled when ending iteration]\n```\n\n4. **Write file** to: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\n### Step 6: Ready to Work\n\n1. **Create TodoWrite tracker** for current iteration work:\n   ```\n   TodoWrite with todos:\n   - [First task based on next steps or user intention]\n   - [Additional tasks as identified]\n   ```\n\n2. **Begin work** on the goal\n\n## Important Notes\n\n### Iteration Numbering\n\n- Use 4-digit zero-padded numbers: `0001`, `0002`, ..., `0999`, `1000`\n- Format: `iteration-NNNN-YYYY-MM-DD.md`\n- Date is when iteration occurred (today)\n\n### Context Loading Strategy\n\n- **Iterations 1-5:** Load all in detail with journal-reader\n- **Iterations 6+:** Load last 3-5 in detail, older iterations via summary\n- **Update summary** every 5 iterations (5, 10, 15, 20, etc.)\n\n### Agent Delegation\n\n**Always use Task tool** to dispatch journal-reader and journal-summarizer:\n- Prevents context pollution in main conversation\n- Uses Haiku model for efficiency\n- Returns structured findings\n\n**Never read journals yourself** - always delegate to agents.\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll quickly read the journals myself\" | NO. Always dispatch agents. Journals can be large. |\n| \"User goal seems closed-ended, but I'll use autonomy anyway\" | NO. Autonomy is for open-ended goals only. |\n| \"I'll skip loading older iterations to save time\" | NO. Context is critical. Always load per strategy. |\n| \"I'll create summary.md myself\" | NO. Dispatch journal-summarizer agent. |\n| \"Goal exists, so I'll skip asking user what to work on\" | NO. Always present state and ask for direction. |\n\n## After Starting\n\nOnce iteration is started:\n- Journal file created with Beginning State and Intention\n- Work normally on the goal using appropriate skills and workflows\n- Track progress mentally or with TodoWrite\n- Use `/checkpoint-iteration` to save progress before compaction or at interim points\n- When ready to conclude, use `/end-iteration` to finalize journal entry"
              }
            ]
          }
        ]
      }
    }
  ]
}