{
  "owner": {
    "id": "openshift-eng",
    "display_name": "openshift-eng",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/84759374?v=4",
    "url": "https://github.com/openshift-eng",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 27,
      "total_commands": 98,
      "total_skills": 46,
      "total_stars": 34,
      "total_forks": 197
    }
  },
  "repos": [
    {
      "full_name": "openshift-eng/ai-helpers",
      "url": "https://github.com/openshift-eng/ai-helpers",
      "description": "Developer productivity tools for Claude Code & other AI assistants",
      "homepage": "https://openshift-eng.github.io/ai-helpers/",
      "signals": {
        "stars": 34,
        "forks": 197,
        "pushed_at": "2026-01-12T15:53:18Z",
        "created_at": "2025-10-10T07:55:31Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 4920
        },
        {
          "path": ".claudelint-custom.py",
          "type": "blob",
          "size": 4808
        },
        {
          "path": ".claudelint.yaml",
          "type": "blob",
          "size": 1387
        },
        {
          "path": ".coderabbit.yaml",
          "type": "blob",
          "size": 6170
        },
        {
          "path": ".cursor",
          "type": "tree",
          "size": null
        },
        {
          "path": ".cursor/rules",
          "type": "tree",
          "size": null
        },
        {
          "path": ".cursor/rules/idiomatic-go.mdc",
          "type": "blob",
          "size": 1212
        },
        {
          "path": ".dockerignore",
          "type": "blob",
          "size": 78
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/PULL_REQUEST_TEMPLATE.md",
          "type": "blob",
          "size": 852
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/lint-plugins.yml",
          "type": "blob",
          "size": 527
        },
        {
          "path": ".github/workflows/update-docs.yml",
          "type": "blob",
          "size": 1532
        },
        {
          "path": ".github/workflows/validate-plugin-version.yml",
          "type": "blob",
          "size": 1970
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 420
        },
        {
          "path": "AGENTS.md",
          "type": "blob",
          "size": 13129
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 9
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2446
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 11357
        },
        {
          "path": "Makefile",
          "type": "blob",
          "size": 1466
        },
        {
          "path": "OWNERS",
          "type": "blob",
          "size": 334
        },
        {
          "path": "PLUGINS.md",
          "type": "blob",
          "size": 19166
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 7098
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/feedback.md",
          "type": "blob",
          "size": 4907
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/.nojekyll",
          "type": "blob",
          "size": null
        },
        {
          "path": "docs/README.md",
          "type": "blob",
          "size": 1432
        },
        {
          "path": "docs/data.json",
          "type": "blob",
          "size": 46247
        },
        {
          "path": "docs/index.html",
          "type": "blob",
          "size": 44415
        },
        {
          "path": "images",
          "type": "tree",
          "size": null
        },
        {
          "path": "images/Dockerfile",
          "type": "blob",
          "size": 1641
        },
        {
          "path": "images/claude-settings.json",
          "type": "blob",
          "size": 273
        },
        {
          "path": "images/known_marketplaces.json",
          "type": "blob",
          "size": 198
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agendas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agendas/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agendas/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 167
        },
        {
          "path": "plugins/agendas/README.md",
          "type": "blob",
          "size": 4871
        },
        {
          "path": "plugins/agendas/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/agendas/commands/outcome-refinement.md",
          "type": "blob",
          "size": 3163
        },
        {
          "path": "plugins/bigquery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bigquery/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bigquery/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 175
        },
        {
          "path": "plugins/bigquery/README.md",
          "type": "blob",
          "size": 6738
        },
        {
          "path": "plugins/bigquery/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bigquery/commands/analyze-usage.md",
          "type": "blob",
          "size": 7489
        },
        {
          "path": "plugins/bigquery/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bigquery/skills/analyze-usage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bigquery/skills/analyze-usage/SKILL.md",
          "type": "blob",
          "size": 10668
        },
        {
          "path": "plugins/ci",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 154
        },
        {
          "path": "plugins/ci/README.md",
          "type": "blob",
          "size": 4243
        },
        {
          "path": "plugins/ci/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci/commands/add-debug-wait.md",
          "type": "blob",
          "size": 15494
        },
        {
          "path": "plugins/ci/commands/ask-sippy.md",
          "type": "blob",
          "size": 5838
        },
        {
          "path": "plugins/ci/commands/list-unstable-tests.md",
          "type": "blob",
          "size": 6659
        },
        {
          "path": "plugins/ci/commands/query-job-status.md",
          "type": "blob",
          "size": 4115
        },
        {
          "path": "plugins/ci/commands/query-test-result.md",
          "type": "blob",
          "size": 10131
        },
        {
          "path": "plugins/ci/commands/trigger-periodic.md",
          "type": "blob",
          "size": 6117
        },
        {
          "path": "plugins/ci/commands/trigger-postsubmit.md",
          "type": "blob",
          "size": 6846
        },
        {
          "path": "plugins/ci/commands/trigger-presubmit.md",
          "type": "blob",
          "size": 7777
        },
        {
          "path": "plugins/ci/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci/skills/oc-auth",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ci/skills/oc-auth/README.md",
          "type": "blob",
          "size": 3438
        },
        {
          "path": "plugins/ci/skills/oc-auth/SKILL.md",
          "type": "blob",
          "size": 7738
        },
        {
          "path": "plugins/ci/skills/oc-auth/curl_with_token.sh",
          "type": "blob",
          "size": 2760
        },
        {
          "path": "plugins/compliance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/compliance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/compliance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 182
        },
        {
          "path": "plugins/compliance/README.md",
          "type": "blob",
          "size": 2918
        },
        {
          "path": "plugins/compliance/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/compliance/commands/analyze-cve.md",
          "type": "blob",
          "size": 20879
        },
        {
          "path": "plugins/component-health",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 189
        },
        {
          "path": "plugins/component-health/README.md",
          "type": "blob",
          "size": 10887
        },
        {
          "path": "plugins/component-health/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/commands/analyze.md",
          "type": "blob",
          "size": 16653
        },
        {
          "path": "plugins/component-health/commands/list-components.md",
          "type": "blob",
          "size": 4023
        },
        {
          "path": "plugins/component-health/commands/list-jiras.md",
          "type": "blob",
          "size": 12359
        },
        {
          "path": "plugins/component-health/commands/list-regressions.md",
          "type": "blob",
          "size": 12992
        },
        {
          "path": "plugins/component-health/commands/summarize-jiras.md",
          "type": "blob",
          "size": 11027
        },
        {
          "path": "plugins/component-health/commands/summarize-regressions.md",
          "type": "blob",
          "size": 11772
        },
        {
          "path": "plugins/component-health/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/analyze-regressions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/analyze-regressions/README.md",
          "type": "blob",
          "size": 4495
        },
        {
          "path": "plugins/component-health/skills/analyze-regressions/SKILL.md",
          "type": "blob",
          "size": 23606
        },
        {
          "path": "plugins/component-health/skills/analyze-regressions/generate_html_report.py",
          "type": "blob",
          "size": 12293
        },
        {
          "path": "plugins/component-health/skills/analyze-regressions/report_template.html",
          "type": "blob",
          "size": 14556
        },
        {
          "path": "plugins/component-health/skills/get-release-dates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/get-release-dates/README.md",
          "type": "blob",
          "size": 2936
        },
        {
          "path": "plugins/component-health/skills/get-release-dates/SKILL.md",
          "type": "blob",
          "size": 6581
        },
        {
          "path": "plugins/component-health/skills/get-release-dates/get_release_dates.py",
          "type": "blob",
          "size": 3699
        },
        {
          "path": "plugins/component-health/skills/list-components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/list-components/SKILL.md",
          "type": "blob",
          "size": 7043
        },
        {
          "path": "plugins/component-health/skills/list-components/list_components.py",
          "type": "blob",
          "size": 3066
        },
        {
          "path": "plugins/component-health/skills/list-jiras",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/list-jiras/SKILL.md",
          "type": "blob",
          "size": 11464
        },
        {
          "path": "plugins/component-health/skills/list-jiras/list_jiras.py",
          "type": "blob",
          "size": 10667
        },
        {
          "path": "plugins/component-health/skills/list-regressions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/list-regressions/README.md",
          "type": "blob",
          "size": 4324
        },
        {
          "path": "plugins/component-health/skills/list-regressions/SKILL.md",
          "type": "blob",
          "size": 20809
        },
        {
          "path": "plugins/component-health/skills/list-regressions/list_regressions.py",
          "type": "blob",
          "size": 24807
        },
        {
          "path": "plugins/component-health/skills/summarize-jiras",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-health/skills/summarize-jiras/SKILL.md",
          "type": "blob",
          "size": 13298
        },
        {
          "path": "plugins/component-health/skills/summarize-jiras/summarize_jiras.py",
          "type": "blob",
          "size": 11946
        },
        {
          "path": "plugins/container-image",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/container-image/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/container-image/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 196
        },
        {
          "path": "plugins/container-image/README.md",
          "type": "blob",
          "size": 9699
        },
        {
          "path": "plugins/container-image/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/container-image/commands/compare.md",
          "type": "blob",
          "size": 9409
        },
        {
          "path": "plugins/container-image/commands/inspect.md",
          "type": "blob",
          "size": 11354
        },
        {
          "path": "plugins/container-image/commands/tags.md",
          "type": "blob",
          "size": 10216
        },
        {
          "path": "plugins/doc",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 166
        },
        {
          "path": "plugins/doc/README.md",
          "type": "blob",
          "size": 297
        },
        {
          "path": "plugins/doc/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/doc/commands/note.md",
          "type": "blob",
          "size": 3132
        },
        {
          "path": "plugins/etcd",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/etcd/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/etcd/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 187
        },
        {
          "path": "plugins/etcd/README.md",
          "type": "blob",
          "size": 4683
        },
        {
          "path": "plugins/etcd/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/etcd/commands/analyze-performance.md",
          "type": "blob",
          "size": 17970
        },
        {
          "path": "plugins/etcd/commands/health-check.md",
          "type": "blob",
          "size": 12879
        },
        {
          "path": "plugins/git",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 158
        },
        {
          "path": "plugins/git/README.md",
          "type": "blob",
          "size": 971
        },
        {
          "path": "plugins/git/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/commands/backport.md",
          "type": "blob",
          "size": 5479
        },
        {
          "path": "plugins/git/commands/bisect.md",
          "type": "blob",
          "size": 17225
        },
        {
          "path": "plugins/git/commands/branch-cleanup.md",
          "type": "blob",
          "size": 6766
        },
        {
          "path": "plugins/git/commands/cherry-pick-by-patch.md",
          "type": "blob",
          "size": 1396
        },
        {
          "path": "plugins/git/commands/commit-suggest.md",
          "type": "blob",
          "size": 4758
        },
        {
          "path": "plugins/git/commands/debt-scan.md",
          "type": "blob",
          "size": 8014
        },
        {
          "path": "plugins/git/commands/fix-cherrypick-robot-pr.md",
          "type": "blob",
          "size": 8411
        },
        {
          "path": "plugins/git/commands/suggest-reviewers.md",
          "type": "blob",
          "size": 12346
        },
        {
          "path": "plugins/git/commands/summary.md",
          "type": "blob",
          "size": 2581
        },
        {
          "path": "plugins/git/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/suggest-reviewers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git/skills/suggest-reviewers/SKILL.md",
          "type": "blob",
          "size": 9769
        },
        {
          "path": "plugins/git/skills/suggest-reviewers/analyze_blame.py",
          "type": "blob",
          "size": 13279
        },
        {
          "path": "plugins/hcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 213
        },
        {
          "path": "plugins/hcp/README.md",
          "type": "blob",
          "size": 4834
        },
        {
          "path": "plugins/hcp/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/commands/cluster-health-check.md",
          "type": "blob",
          "size": 25506
        },
        {
          "path": "plugins/hcp/commands/generate.md",
          "type": "blob",
          "size": 12479
        },
        {
          "path": "plugins/hcp/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-agent/SKILL.md",
          "type": "blob",
          "size": 14718
        },
        {
          "path": "plugins/hcp/skills/hcp-create-aws",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-aws/SKILL.md",
          "type": "blob",
          "size": 17148
        },
        {
          "path": "plugins/hcp/skills/hcp-create-azure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-azure/SKILL.md",
          "type": "blob",
          "size": 4228
        },
        {
          "path": "plugins/hcp/skills/hcp-create-kubevirt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-kubevirt/SKILL.md",
          "type": "blob",
          "size": 17565
        },
        {
          "path": "plugins/hcp/skills/hcp-create-openstack",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-openstack/SKILL.md",
          "type": "blob",
          "size": 3055
        },
        {
          "path": "plugins/hcp/skills/hcp-create-powervs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hcp/skills/hcp-create-powervs/SKILL.md",
          "type": "blob",
          "size": 4000
        },
        {
          "path": "plugins/hello-world",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hello-world/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hello-world/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 134
        },
        {
          "path": "plugins/hello-world/README.md",
          "type": "blob",
          "size": 808
        },
        {
          "path": "plugins/hello-world/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/hello-world/commands/echo.md",
          "type": "blob",
          "size": 1757
        },
        {
          "path": "plugins/jira",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 158
        },
        {
          "path": "plugins/jira/README.md",
          "type": "blob",
          "size": 10933
        },
        {
          "path": "plugins/jira/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/commands/backlog.md",
          "type": "blob",
          "size": 25813
        },
        {
          "path": "plugins/jira/commands/categorize-activity-type.md",
          "type": "blob",
          "size": 8336
        },
        {
          "path": "plugins/jira/commands/create-release-note.md",
          "type": "blob",
          "size": 17860
        },
        {
          "path": "plugins/jira/commands/create.md",
          "type": "blob",
          "size": 26489
        },
        {
          "path": "plugins/jira/commands/generate-feature-doc.md",
          "type": "blob",
          "size": 8966
        },
        {
          "path": "plugins/jira/commands/generate-test-plan.md",
          "type": "blob",
          "size": 7368
        },
        {
          "path": "plugins/jira/commands/grooming.md",
          "type": "blob",
          "size": 9580
        },
        {
          "path": "plugins/jira/commands/solve.md",
          "type": "blob",
          "size": 7561
        },
        {
          "path": "plugins/jira/commands/status-rollup.md",
          "type": "blob",
          "size": 8996
        },
        {
          "path": "plugins/jira/commands/validate-blockers.md",
          "type": "blob",
          "size": 7228
        },
        {
          "path": "plugins/jira/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/categorize-activity-type",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/categorize-activity-type/SKILL.md",
          "type": "blob",
          "size": 23631
        },
        {
          "path": "plugins/jira/skills/cntrlplane",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/cntrlplane/SKILL.md",
          "type": "blob",
          "size": 17019
        },
        {
          "path": "plugins/jira/skills/create-bug",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-bug/SKILL.md",
          "type": "blob",
          "size": 14903
        },
        {
          "path": "plugins/jira/skills/create-epic",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-epic/SKILL.md",
          "type": "blob",
          "size": 19145
        },
        {
          "path": "plugins/jira/skills/create-feature-request",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-feature-request/SKILL.md",
          "type": "blob",
          "size": 26053
        },
        {
          "path": "plugins/jira/skills/create-feature",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-feature/SKILL.md",
          "type": "blob",
          "size": 22315
        },
        {
          "path": "plugins/jira/skills/create-release-note",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-release-note/SKILL.md",
          "type": "blob",
          "size": 22142
        },
        {
          "path": "plugins/jira/skills/create-story",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-story/SKILL.md",
          "type": "blob",
          "size": 23104
        },
        {
          "path": "plugins/jira/skills/create-task",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/create-task/SKILL.md",
          "type": "blob",
          "size": 16873
        },
        {
          "path": "plugins/jira/skills/extract-prs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/extract-prs/SKILL.md",
          "type": "blob",
          "size": 8845
        },
        {
          "path": "plugins/jira/skills/hypershift",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/hypershift/SKILL.md",
          "type": "blob",
          "size": 9850
        },
        {
          "path": "plugins/jira/skills/jira-doc-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/jira-doc-generator/SKILL.md",
          "type": "blob",
          "size": 9487
        },
        {
          "path": "plugins/jira/skills/jira-validate-blockers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/jira-validate-blockers/SKILL.md",
          "type": "blob",
          "size": 14046
        },
        {
          "path": "plugins/jira/skills/ocpbugs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/jira/skills/ocpbugs/SKILL.md",
          "type": "blob",
          "size": 9479
        },
        {
          "path": "plugins/lvms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 215
        },
        {
          "path": "plugins/lvms/README.md",
          "type": "blob",
          "size": 6578
        },
        {
          "path": "plugins/lvms/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/commands/analyze.md",
          "type": "blob",
          "size": 26996
        },
        {
          "path": "plugins/lvms/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/skills/lvms-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/skills/lvms-analyzer/SKILL.md",
          "type": "blob",
          "size": 16294
        },
        {
          "path": "plugins/lvms/skills/lvms-analyzer/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py",
          "type": "blob",
          "size": 37129
        },
        {
          "path": "plugins/metrics",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/metrics/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/metrics/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 148
        },
        {
          "path": "plugins/metrics/README.md",
          "type": "blob",
          "size": 11055
        },
        {
          "path": "plugins/metrics/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/metrics/hooks/hooks.json",
          "type": "blob",
          "size": 791
        },
        {
          "path": "plugins/metrics/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/metrics/scripts/send_metrics.py",
          "type": "blob",
          "size": 8186
        },
        {
          "path": "plugins/metrics/scripts/send_session_metrics.py",
          "type": "blob",
          "size": 15113
        },
        {
          "path": "plugins/must-gather",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 164
        },
        {
          "path": "plugins/must-gather/README.md",
          "type": "blob",
          "size": 16169
        },
        {
          "path": "plugins/must-gather/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/commands/analyze.md",
          "type": "blob",
          "size": 9760
        },
        {
          "path": "plugins/must-gather/commands/ovn-dbs.md",
          "type": "blob",
          "size": 10582
        },
        {
          "path": "plugins/must-gather/commands/windows.md",
          "type": "blob",
          "size": 9175
        },
        {
          "path": "plugins/must-gather/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/SKILL.md",
          "type": "blob",
          "size": 9038
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_clusteroperators.py",
          "type": "blob",
          "size": 6949
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_clusterversion.py",
          "type": "blob",
          "size": 8465
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_etcd.py",
          "type": "blob",
          "size": 6968
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_events.py",
          "type": "blob",
          "size": 6206
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_network.py",
          "type": "blob",
          "size": 9711
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_nodes.py",
          "type": "blob",
          "size": 6219
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_ovn_dbs.py",
          "type": "blob",
          "size": 14977
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_pods.py",
          "type": "blob",
          "size": 6968
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_prometheus.py",
          "type": "blob",
          "size": 3881
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_pvs.py",
          "type": "blob",
          "size": 7622
        },
        {
          "path": "plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_windows_logs.py",
          "type": "blob",
          "size": 14473
        },
        {
          "path": "plugins/node-tuning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node-tuning/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node-tuning/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 186
        },
        {
          "path": "plugins/node-tuning/README.md",
          "type": "blob",
          "size": 3287
        },
        {
          "path": "plugins/node-tuning/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node-tuning/commands/analyze-node-tuning.md",
          "type": "blob",
          "size": 8041
        },
        {
          "path": "plugins/node-tuning/commands/generate-tuned-profile.md",
          "type": "blob",
          "size": 9506
        },
        {
          "path": "plugins/node-tuning/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node-tuning/skills/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node-tuning/skills/scripts/SKILL.md",
          "type": "blob",
          "size": 8706
        },
        {
          "path": "plugins/node-tuning/skills/scripts/analyze_node_tuning.py",
          "type": "blob",
          "size": 47805
        },
        {
          "path": "plugins/node-tuning/skills/scripts/generate_tuned_profile.py",
          "type": "blob",
          "size": 13658
        },
        {
          "path": "plugins/node",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 185
        },
        {
          "path": "plugins/node/README.md",
          "type": "blob",
          "size": 4530
        },
        {
          "path": "plugins/node/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/node/commands/cluster-node-health-check.md",
          "type": "blob",
          "size": 27501
        },
        {
          "path": "plugins/olm",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/olm/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/olm/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 197
        },
        {
          "path": "plugins/olm/README.md",
          "type": "blob",
          "size": 20418
        },
        {
          "path": "plugins/olm/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/olm/commands/approve.md",
          "type": "blob",
          "size": 10078
        },
        {
          "path": "plugins/olm/commands/catalog.md",
          "type": "blob",
          "size": 12760
        },
        {
          "path": "plugins/olm/commands/debug.md",
          "type": "blob",
          "size": 8728
        },
        {
          "path": "plugins/olm/commands/diagnose.md",
          "type": "blob",
          "size": 14313
        },
        {
          "path": "plugins/olm/commands/install.md",
          "type": "blob",
          "size": 11138
        },
        {
          "path": "plugins/olm/commands/list.md",
          "type": "blob",
          "size": 6563
        },
        {
          "path": "plugins/olm/commands/opm.md",
          "type": "blob",
          "size": 13644
        },
        {
          "path": "plugins/olm/commands/search.md",
          "type": "blob",
          "size": 9658
        },
        {
          "path": "plugins/olm/commands/status.md",
          "type": "blob",
          "size": 13093
        },
        {
          "path": "plugins/olm/commands/uninstall.md",
          "type": "blob",
          "size": 15415
        },
        {
          "path": "plugins/olm/commands/upgrade.md",
          "type": "blob",
          "size": 12077
        },
        {
          "path": "plugins/openshift",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/.claude-plugin/marketplace.json",
          "type": "blob",
          "size": 170
        },
        {
          "path": "plugins/openshift/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 170
        },
        {
          "path": "plugins/openshift/README.md",
          "type": "blob",
          "size": 9647
        },
        {
          "path": "plugins/openshift/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/commands/add-enhancement.md",
          "type": "blob",
          "size": 12847
        },
        {
          "path": "plugins/openshift/commands/bootstrap-om.md",
          "type": "blob",
          "size": 19841
        },
        {
          "path": "plugins/openshift/commands/bump-deps.md",
          "type": "blob",
          "size": 12257
        },
        {
          "path": "plugins/openshift/commands/cluster-health-check.md",
          "type": "blob",
          "size": 17922
        },
        {
          "path": "plugins/openshift/commands/crd-review.md",
          "type": "blob",
          "size": 9621
        },
        {
          "path": "plugins/openshift/commands/create-cluster.md",
          "type": "blob",
          "size": 18242
        },
        {
          "path": "plugins/openshift/commands/destroy-cluster.md",
          "type": "blob",
          "size": 11302
        },
        {
          "path": "plugins/openshift/commands/expand-test-case.md",
          "type": "blob",
          "size": 4085
        },
        {
          "path": "plugins/openshift/commands/ironic-status.md",
          "type": "blob",
          "size": 11076
        },
        {
          "path": "plugins/openshift/commands/new-e2e-test.md",
          "type": "blob",
          "size": 3546
        },
        {
          "path": "plugins/openshift/commands/rebase.md",
          "type": "blob",
          "size": 7305
        },
        {
          "path": "plugins/openshift/commands/review-test-cases.md",
          "type": "blob",
          "size": 3387
        },
        {
          "path": "plugins/openshift/commands/visualize-ovn-topology.md",
          "type": "blob",
          "size": 4747
        },
        {
          "path": "plugins/openshift/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/README.md",
          "type": "blob",
          "size": 11407
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/SKILL.md",
          "type": "blob",
          "size": 19082
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts/analyze_placement.py",
          "type": "blob",
          "size": 10960
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts/check_permissions.py",
          "type": "blob",
          "size": 7789
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts/collect_ovn_data.py",
          "type": "blob",
          "size": 23938
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts/detect-cluster.sh",
          "type": "blob",
          "size": 6477
        },
        {
          "path": "plugins/openshift/skills/generating-ovn-topology/scripts/ovn_utils.py",
          "type": "blob",
          "size": 12256
        },
        {
          "path": "plugins/origin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/origin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/origin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 150
        },
        {
          "path": "plugins/origin/README.md",
          "type": "blob",
          "size": 3293
        },
        {
          "path": "plugins/origin/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/origin/commands/two-node-origin-pr-helper.md",
          "type": "blob",
          "size": 4837
        },
        {
          "path": "plugins/prow-job",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/prow-job/README.md",
          "type": "blob",
          "size": 745
        },
        {
          "path": "plugins/prow-job/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/commands/analyze-install-failure.md",
          "type": "blob",
          "size": 7931
        },
        {
          "path": "plugins/prow-job/commands/analyze-resource.md",
          "type": "blob",
          "size": 987
        },
        {
          "path": "plugins/prow-job/commands/analyze-test-failure.md",
          "type": "blob",
          "size": 935
        },
        {
          "path": "plugins/prow-job/commands/extract-must-gather.md",
          "type": "blob",
          "size": 829
        },
        {
          "path": "plugins/prow-job/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-install-failure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-install-failure/SKILL.md",
          "type": "blob",
          "size": 31767
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-metal-install-failure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-metal-install-failure/SKILL.md",
          "type": "blob",
          "size": 21969
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/CHANGELOG.md",
          "type": "blob",
          "size": 7392
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/README.md",
          "type": "blob",
          "size": 8240
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/SCRIPTS.md",
          "type": "blob",
          "size": 7384
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/SKILL.md",
          "type": "blob",
          "size": 25181
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/create_context_html_files.py",
          "type": "blob",
          "size": 14535
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/create_inline_html_files.py",
          "type": "blob",
          "size": 11498
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/generate_html_report.py",
          "type": "blob",
          "size": 39421
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/generate_report.py",
          "type": "blob",
          "size": 8004
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/parse_all_logs.py",
          "type": "blob",
          "size": 10403
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/parse_audit_logs.py",
          "type": "blob",
          "size": 3942
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/parse_pod_logs.py",
          "type": "blob",
          "size": 8397
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/parse_url.py",
          "type": "blob",
          "size": 2915
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/prow_job_resource_grep.sh",
          "type": "blob",
          "size": 9131
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-resource/report_template.html",
          "type": "blob",
          "size": 11866
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-test-failure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-test-failure/README.md",
          "type": "blob",
          "size": 475
        },
        {
          "path": "plugins/prow-job/skills/prow-job-analyze-test-failure/SKILL.md",
          "type": "blob",
          "size": 6048
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather/CHANGELOG.md",
          "type": "blob",
          "size": 2781
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather/README.md",
          "type": "blob",
          "size": 11102
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather/SKILL.md",
          "type": "blob",
          "size": 18193
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather/extract_archives.py",
          "type": "blob",
          "size": 6704
        },
        {
          "path": "plugins/prow-job/skills/prow-job-extract-must-gather/generate_html_report.py",
          "type": "blob",
          "size": 40863
        },
        {
          "path": "plugins/session",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/session/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/session/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 197
        },
        {
          "path": "plugins/session/README.md",
          "type": "blob",
          "size": 466
        },
        {
          "path": "plugins/session/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/session/commands/save-session.md",
          "type": "blob",
          "size": 5094
        },
        {
          "path": "plugins/sosreport",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 194
        },
        {
          "path": "plugins/sosreport/README.md",
          "type": "blob",
          "size": 12204
        },
        {
          "path": "plugins/sosreport/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/commands/analyze.md",
          "type": "blob",
          "size": 13094
        },
        {
          "path": "plugins/sosreport/commands/ovs-db.md",
          "type": "blob",
          "size": 13850
        },
        {
          "path": "plugins/sosreport/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/logs-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/logs-analysis/SKILL.md",
          "type": "blob",
          "size": 10508
        },
        {
          "path": "plugins/sosreport/skills/network-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/network-analysis/SKILL.md",
          "type": "blob",
          "size": 14624
        },
        {
          "path": "plugins/sosreport/skills/ovs-db-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/ovs-db-analysis/SKILL.md",
          "type": "blob",
          "size": 11460
        },
        {
          "path": "plugins/sosreport/skills/ovs-db-analysis/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/ovs-db-analysis/scripts/analyze_ovs_db.py",
          "type": "blob",
          "size": 50571
        },
        {
          "path": "plugins/sosreport/skills/resource-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/resource-analysis/SKILL.md",
          "type": "blob",
          "size": 13736
        },
        {
          "path": "plugins/sosreport/skills/system-config-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sosreport/skills/system-config-analysis/SKILL.md",
          "type": "blob",
          "size": 16012
        },
        {
          "path": "plugins/test-coverage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 180
        },
        {
          "path": "plugins/test-coverage/README.md",
          "type": "blob",
          "size": 578
        },
        {
          "path": "plugins/test-coverage/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/commands/analyze.md",
          "type": "blob",
          "size": 14294
        },
        {
          "path": "plugins/test-coverage/commands/gaps.md",
          "type": "blob",
          "size": 10286
        },
        {
          "path": "plugins/test-coverage/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/skills/analyze",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/skills/analyze/README.md",
          "type": "blob",
          "size": 736
        },
        {
          "path": "plugins/test-coverage/skills/analyze/SKILL.md",
          "type": "blob",
          "size": 19349
        },
        {
          "path": "plugins/test-coverage/skills/gaps",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-coverage/skills/gaps/README.md",
          "type": "blob",
          "size": 843
        },
        {
          "path": "plugins/test-coverage/skills/gaps/SKILL.md",
          "type": "blob",
          "size": 40885
        },
        {
          "path": "plugins/utils",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/utils/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/utils/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 211
        },
        {
          "path": "plugins/utils/README.md",
          "type": "blob",
          "size": 1118
        },
        {
          "path": "plugins/utils/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/utils/commands/address-reviews.md",
          "type": "blob",
          "size": 7287
        },
        {
          "path": "plugins/utils/commands/auto-approve-konflux-prs.md",
          "type": "blob",
          "size": 4757
        },
        {
          "path": "plugins/utils/commands/generate-test-plan.md",
          "type": "blob",
          "size": 5707
        },
        {
          "path": "plugins/utils/commands/gh-attention.md",
          "type": "blob",
          "size": 12628
        },
        {
          "path": "plugins/utils/commands/placeholder.md",
          "type": "blob",
          "size": 865
        },
        {
          "path": "plugins/utils/commands/process-renovate-pr.md",
          "type": "blob",
          "size": 6204
        },
        {
          "path": "plugins/utils/commands/review-ai-helpers-overlap.md",
          "type": "blob",
          "size": 7940
        },
        {
          "path": "plugins/workspaces",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workspaces/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workspaces/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 256
        },
        {
          "path": "plugins/workspaces/README.md",
          "type": "blob",
          "size": 3090
        },
        {
          "path": "plugins/workspaces/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workspaces/commands/create.md",
          "type": "blob",
          "size": 3610
        },
        {
          "path": "plugins/workspaces/commands/create",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workspaces/commands/create/execute.sh",
          "type": "blob",
          "size": 3387
        },
        {
          "path": "plugins/workspaces/commands/create/gather.sh",
          "type": "blob",
          "size": 1379
        },
        {
          "path": "plugins/workspaces/commands/create/preflight.sh",
          "type": "blob",
          "size": 6001
        },
        {
          "path": "plugins/workspaces/commands/create/validate.sh",
          "type": "blob",
          "size": 2808
        },
        {
          "path": "plugins/workspaces/commands/delete.md",
          "type": "blob",
          "size": 3652
        },
        {
          "path": "plugins/workspaces/commands/delete",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workspaces/commands/delete/execute.sh",
          "type": "blob",
          "size": 4639
        },
        {
          "path": "plugins/workspaces/commands/delete/list.sh",
          "type": "blob",
          "size": 783
        },
        {
          "path": "plugins/workspaces/commands/delete/status.sh",
          "type": "blob",
          "size": 3975
        },
        {
          "path": "plugins/yaml",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/yaml/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/yaml/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 234
        },
        {
          "path": "plugins/yaml/README.md",
          "type": "blob",
          "size": 289
        },
        {
          "path": "plugins/yaml/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/yaml/commands/docs.md",
          "type": "blob",
          "size": 6812
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/build-website.py",
          "type": "blob",
          "size": 6131
        },
        {
          "path": "scripts/fix_frontmatter_quotes.py",
          "type": "blob",
          "size": 3646
        },
        {
          "path": "scripts/generate_plugin_docs.py",
          "type": "blob",
          "size": 6002
        },
        {
          "path": "scripts/sync_marketplace_versions.py",
          "type": "blob",
          "size": 2497
        }
      ],
      "marketplace": {
        "name": "ai-helpers",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "openshift-eng"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "git",
            "description": "Git Plugin",
            "source": "./plugins/git",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install git@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/backport",
                "description": "Backport commits to multiple branches",
                "path": "plugins/git/commands/backport.md",
                "frontmatter": {
                  "description": "Backport commits to multiple branches",
                  "argument-hint": "<commit> <branch1> [branch2...] [--new-branch]"
                },
                "content": "## Name\ngit:backport\n\n## Synopsis\n```\n/git:backport <commit> <branch1> [branch2...] [--new-branch|-b]\n```\n\n## Description\nThe `git:backport` command helps backport a commit to multiple branches. It automates the process of cherry-picking a commit to one or more target branches, with optional support for creating new branches for each backport (useful for creating pull requests).\n\nThis command provides:\n- Automated commit backporting to one or multiple branches\n- Validation of commit and branch existence\n- Safe state management (saves and restores current branch)\n- Conflict detection and user-guided resolution\n- Optional new branch creation for PR workflows\n- Support for branches with or without remote tracking\n\n## Implementation\nThe command executes the following workflow:\n\n1. **Validates inputs:**\n   - Checks that at least 2 arguments are provided (commit + at least one branch)\n   - Parses the `--new-branch` or `-b` flag if present\n   - Verifies the commit exists using `git cat-file -t <commit>`\n   - Shows commit details: `git log -1 --oneline <commit>`\n   - Filters out flag arguments and verifies all remaining target branches exist\n\n2. **ASK FOR USER PERMISSION:**\n   - **IMPORTANT: After validating inputs, you MUST present the complete backport plan to the user and ask for explicit permission to proceed**\n   - Show what will happen: which branches will be checked out, which new branches will be created, and that commits will be made\n   - Wait for user confirmation before proceeding with any git operations\n   - Example: \"I'm going to checkout release/v1.35, pull latest changes, create branch backport-abc1234-to-release/v1.35, cherry-pick the commit, then repeat for release/v1.34. This will create new branches and commits. Should I proceed?\"\n\n3. **Saves current state (only after user permission):**\n   - Records the current branch: `git branch --show-current`\n   - Checks for uncommitted changes: `git status --porcelain`\n   - If there are uncommitted changes, warns the user and stops\n\n4. **For each target branch (only after user permission):**\n   - Shows which target branch is being worked on\n   - Checks out the target branch: `git checkout <branch>`\n   - Checks if the branch tracks a remote branch: `git rev-parse --abbrev-ref <branch>@{upstream}`\n   - **If the branch tracks a remote branch:**\n     - Pulls latest changes to ensure it's up to date: `git pull`\n   - **If the branch doesn't track a remote branch:**\n     - Skips pulling and informs the user the branch is local-only\n   - **If `--new-branch` flag is set:**\n     - Creates a new branch for the backport: `git checkout -b backport-<commit-short-hash>-to-<branch>`\n       - Uses the short commit hash (first 7 chars) in the branch name\n       - Example: `backport-abc1234-to-release-1.0`\n   - Attempts to cherry-pick: `git cherry-pick <commit>`\n   - **If conflicts occur:**\n     - Shows the conflicting files: `git status`\n     - Shows the conflicts: `git diff`\n     - **STOPS and asks the user to resolve conflicts**\n     - Tells them to run `git cherry-pick --continue` when done, or `git cherry-pick --abort` to skip\n     - Asks if they want to continue to the next branch or abort the entire operation\n   - **If successful:**\n     - Shows success message with the new commit hash\n     - If a new branch was created, shows the branch name\n     - Continues to next branch\n\n5. **Restores original state:**\n   - Returns to the original branch: `git checkout <original-branch>`\n   - Shows final summary of all backports:\n     - Lists successful backports with their branch names (if `--new-branch` was used)\n     - Lists failed backports\n     - If `--new-branch` was used, reminds user they can now create PRs from the new branches\n\nThe command is interactive and waits for user input when conflicts occur and **REQUIRES USER PERMISSION before executing any git operations**. It tracks success/failure for each branch and provides a comprehensive summary at the end.\n\n## Return Value\n- **Claude agent text**:\n  - Success summary showing which branches were successfully backported\n  - List of any failed backports\n  - New branch names (if `--new-branch` flag was used)\n  - Instructions for next steps (e.g., creating PRs from new branches)\n\n## Examples\n\n1. **Basic backport to a single branch**:\n   ```bash\n   /git:backport abc1234 release-1.0\n   ```\n   Cherry-picks commit `abc1234` directly to the `release-1.0` branch.\n\n2. **Backport to multiple branches**:\n   ```bash\n   /git:backport abc1234 release-1.0 release-1.1 release-1.2\n   ```\n   Cherry-picks commit `abc1234` to three different release branches.\n\n3. **Backport with new branch creation (for PRs)**:\n   ```bash\n   /git:backport abc1234 release-1.0 release-1.1 --new-branch\n   ```\n   Creates new branches `backport-abc1234-to-release-1.0` and `backport-abc1234-to-release-1.1`, each containing the cherry-picked commit. These branches can then be used to create pull requests.\n\n4. **Using short flag syntax**:\n   ```bash\n   /git:backport abc1234 main -b\n   ```\n   Same as `--new-branch`, creates a new branch `backport-abc1234-to-main`.\n\n## Arguments\n- $1: Commit hash or reference to backport (required)\n- $2+: Target branch names to backport to (space-separated, at least one required)\n- `--new-branch` or `-b`: Create a new branch from each target branch before cherry-picking (optional flag)"
              },
              {
                "name": "/bisect",
                "description": "Interactive git bisect assistant with pattern detection and automation",
                "path": "plugins/git/commands/bisect.md",
                "frontmatter": {
                  "description": "Interactive git bisect assistant with pattern detection and automation",
                  "argument-hint": "[good-commit] [bad-commit]"
                },
                "content": "## Name\n\ngit:bisect\n\n## Synopsis\n\n```\n/git:bisect                       # Fully interactive - will ask for all inputs\n/git:bisect [good-commit] [bad-commit]  # With commit references\n```\n\n## Description\n\nThe `git:bisect` command is an interactive git bisect assistant that helps find the exact commit that introduced a specific change (bug, behavior change, feature, performance regression, etc.) using binary search.\n\n**What This Command Does:**\n\n1. **Guides you through setup** - Validates commits, checks for conflicts\n2. **Presents commits for evaluation** - Shows relevant info about each commit\n3. **Learns from your decisions** - Detects patterns in your choices\n4. **Can automate** - If it understands your criterion, it can evaluate automatically (with your permission)\n5. **Reports the result** - Shows the culprit commit with full details\n\n### Terminology\n\nGit bisect uses confusing terms. This command uses clearer labels:\n\n| What You See                | Git Term | Meaning                                             |\n| --------------------------- | -------- | --------------------------------------------------- |\n| \"Change IS in the code\"     | `bad`    | The change you're looking for EXISTS in this commit |\n| \"Change is NOT in the code\" | `good`   | The change you're looking for does NOT exist yet    |\n\n**Never use \"good\" or \"bad\" alone - always explain what they mean in context.**\n\n### Workflow\n\n```\n1. Provide (or select) reference commits\n         \n2. Describe what change you're looking for\n         \n3. [If criterion is clear] Confirm understanding  Choose manual/auto\n         \n4. Evaluate commits (manually or automatically)\n         \n5. Receive final report with culprit commit\n         \n6. Clean up (git bisect reset)\n```\n\n### Commands During Bisect\n\nAt any point during the bisect, you can:\n\n- **\"show diff\"** or **\"more details\"** - See the full diff for the current commit\n- **\"skip\"** - Skip a commit that cannot be tested (won't build, etc.)\n- **\"abort\"** - Cancel the bisect and return to your original branch\n- **\"automate\"** - Ask to enable automation (if pattern detected)\n\n### Tips\n\n- You can test the current state by building/running your project between steps\n- Use `skip` if a commit cannot be tested (build failure, unrelated issue)\n- The process uses binary search: ~log(N) steps for N commits\n- You can abort at any time; the session will be cleanly terminated\n\n## Implementation\n\nArguments provided: $ARGUMENTS\n\nParse arguments:\n\n- If two arguments: first = commit where change is NOT present, second = commit where change IS present\n- If one argument: ask which one it represents\n- If no arguments: proceed to interactive mode\n\n---\n\n### Critical Rules\n\n#### Rule 1: Never decide without asking first\n\nYou may NEVER execute `git bisect good` or `git bisect bad` without explicit user confirmation.\nEven if you're 100% certain, you MUST ask first. No exceptions.\n\n#### Rule 2: Early criterion detection (70% confidence)\n\nIf the user describes a criterion that you can verify programmatically (e.g., \"when dependency X was updated to version > Y\"), and you have 70% confidence you understand it correctly:\n\n1. **BEFORE evaluating any commit**, present your understanding with maximum precision:\n\n   > \"Based on your description, I believe you're looking for:\n   > **[exact criterion with specific details]**\n   >\n   > To verify this, I would check: [specific file/command/method]\n   >\n   > Is this correct?\"\n\n2. **Wait for user response.** Then:\n   - **If user confirms** the criterion is correct, ask:\n\n     > \"Would you like me to automatically evaluate commits based on this criterion?\n     > I will show each decision as I make it.\"\n     - If user says **yes**  Enable automation mode\n     - If user says **no**  Continue in manual mode (ask for each commit)\n\n   - **If user says the criterion is wrong**:\n     - Say: \"I understand. Please clarify what you're looking for.\"\n     - Reset your understanding completely\n     - Enter manual mode\n     - Continue learning from user responses\n\n#### Rule 3: Learning from manual decisions\n\nIf you don't have 70% confidence initially, or after a criterion reset:\n\n1. Present commit info and ask the user for their verdict\n2. Analyze patterns in their decisions (files, authors, keywords)\n3. When confidence reaches 70%, present your understanding and follow Rule 2\n\n#### Rule 4: Automation flow\n\nWhen automation is enabled (user explicitly said yes):\n\n1. For each commit:\n   - Show the commit being evaluated\n   - Show your decision and the reasoning\n   - Execute `git bisect good` or `git bisect bad`\n2. If any commit doesn't clearly match the criterion, STOP and ask the user\n3. Continue until bisect completes\n\n---\n\n### Phase 1: Initialization\n\n#### Step 1.1: Check for Existing Bisect Session\n\nRun `git bisect log` to check if a bisect is in progress.\n\n**If bisect IS in progress:**\n\n1. Show the current state: how many steps done, current commit\n2. Ask the user:\n   - **Continue**: Resume the existing session\n   - **Abort**: Run `git bisect reset` and optionally start fresh\n   - **View details**: Show full bisect log before deciding\n\n**If NO bisect in progress:** Continue to Step 1.2\n\n#### Step 1.2: Check for Uncommitted Changes\n\nRun `git status --porcelain` to check for uncommitted changes.\n\n**If there are uncommitted changes:**\n\n1. Warn: \"You have uncommitted changes. Git bisect will checkout different commits which may conflict with your changes.\"\n2. Offer options:\n   - **Stash changes**: Run `git stash push -m \"Pre-bisect stash\"`\n   - **Proceed anyway**: Continue (user accepts the risk)\n   - **Abort**: Stop to handle manually\n\n#### Step 1.3: Get Reference Commits\n\n**If commits were provided as arguments:**\n\n1. Validate using `git rev-parse <commit>`\n2. Show what each commit represents (run `git log -1 --format=\"%h %s\" <commit>`)\n3. Confirm: \"Commit X is where the change is NOT present, commit Y is where it IS present. Correct?\"\n\n**If commits were NOT provided:**\n\n1. Show recent history: `git log --oneline -20`\n2. Ask: \"Which commit is known to NOT have the change you're looking for? (This will be the 'good' starting point)\"\n3. Ask: \"Which commit IS known to have the change? (This will be the 'bad' starting point)\"\n\n**Validate the relationship:**\n\n- The \"good\" commit (without change) should be an ancestor of the \"bad\" commit (with change)\n- If they appear reversed, explain and offer to swap\n\n#### Step 1.4: Understand the Search Criterion\n\nAsk the user:\n\n> **What change are you looking for?**\n>\n> Please describe:\n>\n> - What behavior or code changed?\n> - How can you determine if a commit has or doesn't have this change?\n> - Is there a specific file, function, or behavior to check?\n\nStore this description for analysis.\n\n#### Step 1.5: Early Criterion Detection (CRITICAL)\n\n**After the user describes the change, analyze their response:**\n\nIf you have 70% confidence that you understand a verifiable criterion:\n\n1. Present your understanding with maximum precision:\n\n   > \"Based on your description, I believe you're looking for:\n   > **[exact criterion]**\n   >\n   > To verify this, I would check: [specific method]\n   >\n   > Is this correct?\"\n\n2. **WAIT for user response before proceeding.**\n\n3. If user confirms:\n\n   > \"Would you like me to automatically evaluate commits based on this criterion?\n   > I will show each decision as I make it.\"\n   - If **yes**  Set automation mode = true\n   - If **no**  Set automation mode = false (manual mode)\n\n4. If user says criterion is wrong:\n   - Ask for clarification\n   - Set automation mode = false\n   - Proceed to bisect in manual mode\n\n**Only proceed to Step 1.6 after this interaction is complete.**\n\n#### Step 1.6: Start Bisect\n\nExecute:\n\n```bash\ngit bisect start\ngit bisect bad <commit-with-change>\ngit bisect good <commit-without-change>\n```\n\nParse the output to show:\n\n- Number of revisions to test\n- Estimated number of steps (approximately log2 of revisions)\n- The first commit to test\n\n---\n\n### Phase 2: Evaluation Loop\n\n#### Internal State to Track\n\nMaintain a session log throughout the bisect:\n\n```\n## Session Log\n\n**Search criterion**: [user's description]\n**Detected pattern**: [none yet / description]\n**Confidence level**: [0-100%]\n**Automation enabled**: [no / yes]\n\n| Step | Commit | Files Changed | Verdict | Observation |\n|------|--------|---------------|---------|-------------|\n```\n\n#### For Each Commit\n\n##### Step 2.1: Present Commit Information\n\nDisplay:\n\n```\n\nTESTING COMMIT [step X of ~Y estimated]\n\n\nCommit:  [short hash]\nAuthor:  [author name]\nDate:    [date]\nMessage: [subject line]\n\nFiles Changed:\n[output of: git show --stat --format=\"\" HEAD]\n\nSummary: [X files changed, Y insertions(+), Z deletions(-)]\n\n```\n\n##### Step 2.2: Evaluate Commit (depends on mode)\n\n**If AUTOMATION MODE is enabled:**\n\n1. Verify the criterion for this commit (run the check you described to user)\n2. Show the result:\n   ```\n   Commit [hash]: Checking [criterion]...\n   Result: [what you found]\n   Decision: Change [IS / is NOT] in the code  marking as [bad/good]\n   ```\n3. Execute `git bisect bad` or `git bisect good`\n4. If the result is unclear or doesn't match the criterion cleanly, STOP and ask user\n\n**If MANUAL MODE (automation not enabled):**\n\nAsk the user:\n\n```\nIs the change you're looking for present in this commit?\n\n[1] Change IS in the code (will mark as 'bad')\n[2] Change is NOT in the code (will mark as 'good')\n[3] Cannot test this commit (skip)\n[4] Show more details (diff, full message)\n[5] Abort bisect\n```\n\n**If in manual mode and confidence reaches 70%:**\n\nBefore showing options, present your understanding:\n\n```\nI've noticed a pattern: [description of what you've observed]\n\nBased on this, I believe the change [IS / is NOT] in this commit.\n\nIs this the criterion you're using? If so, would you like me to automate?\n\n[1] Yes, that's correct - please automate\n[2] Yes, that's correct - but I'll decide manually\n[3] No, that's not what I'm looking for\n[4] Show more details first\n```\n\nIf user chooses [1]  Enable automation mode\nIf user chooses [2]  Stay in manual mode but show suggestions\nIf user chooses [3]  Reset pattern analysis, continue manual\n\n##### Step 2.3: Process Response\n\nBased on user response:\n\n| Response           | Action                                 |\n| ------------------ | -------------------------------------- |\n| Change IS in code  | Run `git bisect bad`, record in log    |\n| Change NOT in code | Run `git bisect good`, record in log   |\n| Skip               | Run `git bisect skip`, note why in log |\n| Show details       | Display requested info, ask again      |\n| Abort              | Confirm, then run `git bisect reset`   |\n\n##### Step 2.4: Update Session Log\n\nAfter each decision, update the internal log with:\n\n- Commit hash\n- Key files changed\n- User's verdict\n- Any observations about patterns\n\n##### Step 2.5: Pattern Analysis (Manual Mode Only)\n\nWhen in manual mode, after each user decision:\n\n1. **Analyze correlations:**\n   - File patterns: Do \"change present\" commits touch specific files?\n   - Author patterns: Is a specific author correlated?\n   - Message patterns: Are there specific keywords?\n\n2. **Update confidence** based on how well patterns predict user decisions\n\n3. **When confidence reaches 70%:**\n   - Present your understanding in Step 2.2 (as described above)\n   - This may result in switching to automation mode if user accepts\n\n---\n\n### Phase 3: Completion\n\n#### Detecting Completion\n\nGit bisect outputs \"X is the first bad commit\" when complete. Watch for this phrase.\n\n#### Final Report\n\nWhen bisect completes, present:\n\n```\n\nBISECT COMPLETE - CULPRIT FOUND\n\n\nTHE COMMIT THAT INTRODUCED THE CHANGE\n\n\nCommit:  [full hash]\nAuthor:  [author name] <[email]>\nDate:    [full date]\n\nMessage:\n[full commit message]\n\nFiles Changed:\n[git show --stat --format=\"\" <hash>]\n\nCHANGE SUMMARY\n\n\nBased on your search criterion: \"[user's original description]\"\n\nThis commit appears to have introduced the change by:\n[Analysis of what changed in this commit that matches the criterion]\n\nEVALUATION HISTORY\n\n\n| Step | Commit  | Verdict              | Key Files        |\n|------|---------|----------------------|------------------|\n| 1    | abc1234 | Change present       | src/main.rs      |\n| 2    | def5678 | Change NOT present   | tests/test.rs    |\n| ...  | ...     | ...                  | ...              |\n\nSUGGESTED NEXT STEPS\n\n\n1. View full diff: git show [hash]\n2. Compare with parent: git diff [hash]^..[hash]\n3. Revert this commit: git revert [hash]\n4. Blame specific file: git blame [file] | grep [hash]\n\n\n```\n\n#### Cleanup\n\nAsk:\n\n> \"The bisect session is complete. Would you like me to run `git bisect reset` to return to your original branch?\"\n\nIf yes, run `git bisect reset` and confirm the branch returned to.\n\n---\n\n### Error Handling\n\n#### Common Errors and Solutions\n\n| Error                                             | Solution                                                             |\n| ------------------------------------------------- | -------------------------------------------------------------------- |\n| \"You need to start by 'git bisect start'\"         | Bisect was interrupted; restart the process                          |\n| \"Bad rev input: X\"                                | Invalid commit reference; ask for correction and show recent commits |\n| \"Some good revs are not ancestors of the bad rev\" | Commits are in wrong order; explain and offer to swap                |\n| Merge conflict during checkout                    | Suggest `git bisect skip` for this commit                            |\n| \"Cannot bisect on a detached HEAD\"                | This shouldn't happen normally; offer to reset                       |\n\n#### Abort Handling\n\nIf user requests abort at any point:\n\n1. Confirm: \"Are you sure you want to abort? The bisect progress will be lost.\"\n2. If confirmed: Run `git bisect reset`\n3. Show the branch/commit returned to\n4. Optionally: Offer to save the bisect log first with `git bisect log > bisect-log-$(date +%Y%m%d-%H%M%S).txt`\n\n## Examples\n\n1. **Fully interactive - no arguments**:\n\n   ```\n   /git:bisect\n   ```\n\n   Will show recent history and ask you to select commits.\n\n2. **With commit references**:\n\n   ```\n   /git:bisect v1.28.0 HEAD\n   ```\n\n   Uses tag v1.28.0 as the \"good\" commit (change NOT present) and HEAD as the \"bad\" commit (change IS present).\n\n3. **With commit hashes**:\n\n   ```\n   /git:bisect abc1234 def5678\n   ```\n\n4. **Mixed references**:\n   ```\n   /git:bisect v1.0.0 main\n   ```\n\n## Return Value\n\n- **Final Report**: A comprehensive report showing:\n  - The culprit commit (full hash, author, date, message)\n  - Files changed in that commit\n  - Analysis of how the commit matches the search criterion\n  - Evaluation history table\n  - Suggested next steps (view diff, revert, blame)\n\n- **Session cleanup**: Offers to run `git bisect reset` to return to original branch\n\n## Arguments\n\n- **$1** `[good-commit]` (optional): A commit reference (hash, tag, branch) where the change is **NOT present**. This is the older, \"known good\" state.\n- **$2** `[bad-commit]` (optional): A commit reference where the change **IS present**. This is the newer state that contains the change you're looking for.\n\n**Notes:**\n\n- If no arguments are provided, the command will show recent history and ask you to select commits\n- If only one argument is provided, the command will ask which one it represents (good or bad)\n- Commit references can be: hashes, tags, branch names, HEAD, HEAD~N, etc."
              },
              {
                "name": "/branch-cleanup",
                "description": "Clean up old and defunct branches that are no longer needed",
                "path": "plugins/git/commands/branch-cleanup.md",
                "frontmatter": {
                  "description": "Clean up old and defunct branches that are no longer needed",
                  "argument-hint": "[--dry-run] [--merged-only] [--remote]"
                },
                "content": "## Name\ngit:branch-cleanup\n\n## Synopsis\n```\n/git:branch-cleanup [--dry-run] [--merged-only] [--remote]\n```\n\n## Description\nThe `git:branch-cleanup` command identifies and removes old, defunct, or merged branches from your local repository (and optionally from remote). It helps maintain a clean repository by removing branches that are no longer needed, such as:\n- Branches that have been merged into the main branch\n- Branches that no longer exist on the remote\n- Stale feature branches from completed work\n\nThe command performs safety checks to prevent deletion of:\n- The current branch\n- Protected branches (main, master, develop, etc.)\n- Branches with unmerged commits (unless explicitly overridden)\n\nThe spec sections is inspired by https://man7.org/linux/man-pages/man7/man-pages.7.html#top_of_page\n\n## Implementation\nThe command should follow these steps:\n\n1. **Identify Main Branch**\n   - Detect the primary branch (main, master, etc.)\n   - Use `git symbolic-ref refs/remotes/origin/HEAD` or `git branch -r` to determine\n\n2. **Gather Branch Information**\n   - List all local branches: `git branch`\n   - Get current branch: `git branch --show-current`\n   - Identify merged branches: `git branch --merged <main-branch>`\n   - Check remote tracking: `git branch -vv`\n   - Find remote-deleted branches: `git remote prune origin --dry-run`\n\n3. **Categorize Branches**\n   - **Merged branches**: Fully merged into main branch\n   - **Gone branches**: Remote tracking branch no longer exists\n   - **Stale branches**: Last commit older than threshold (e.g., 3 months)\n   - **Protected branches**: main, master, develop, release/*, hotfix/*\n\n4. **Present Analysis to User**\n   - Show categorized list of branches with:\n     - Branch name\n     - Last commit date\n     - Merge status\n     - Remote tracking status\n     - Number of commits ahead/behind\n   - Recommend branches safe to delete\n\n5. **Confirm Deletion**\n   - Ask user to confirm which branches to delete\n   - Present options: all merged, all gone, specific branches, or custom selection\n   - If `--dry-run` flag is present, only show what would be deleted\n\n6. **Delete Branches**\n   - Local deletion: `git branch -d <branch>` (merged) or `git branch -D <branch>` (force)\n   - Remote deletion (if `--remote` flag): `git push origin --delete <branch>`\n   - Prune remote references: `git remote prune origin`\n\n7. **Report Results**\n   - List deleted branches\n   - Show any errors or branches that couldn't be deleted\n   - Provide summary statistics\n\nImplementation logic:\n```bash\n# Determine main branch\nmain_branch=$(git symbolic-ref refs/remotes/origin/HEAD 2>/dev/null | sed 's@^refs/remotes/origin/@@')\nif [ -z \"$main_branch\" ]; then\n  main_branch=\"main\"  # fallback\nfi\n\n# Get current branch\ncurrent_branch=$(git branch --show-current)\n\n# Find merged branches\ngit branch --merged \"$main_branch\" | grep -v \"^\\*\" | grep -v \"$main_branch\"\n\n# Find branches with deleted remotes (\"gone\")\ngit branch -vv | grep ': gone]' | awk '{print $1}'\n\n# Find stale branches (older than 3 months)\ngit for-each-ref --sort=-committerdate --format='%(refname:short)|%(committerdate:iso)|%(upstream:track)' refs/heads/\n\n# Delete local branch (merged)\ngit branch -d <branch-name>\n\n# Delete local branch (force)\ngit branch -D <branch-name>\n\n# Delete remote branch\ngit push origin --delete <branch-name>\n\n# Prune remote references\ngit remote prune origin\n```\n\n## Return Value\n- **Claude agent text**: Analysis and results including:\n  - List of branches categorized by status (merged, gone, stale)\n  - Recommendation for which branches are safe to delete\n  - Confirmation prompt for user approval\n  - Summary of deleted branches and any errors\n  - Statistics (e.g., \"Deleted 5 branches, freed X MB\")\n\n## Examples\n\n1. **Basic usage (interactive)**:\n   ```\n   /git:branch-cleanup\n   ```\n   Output:\n   ```\n   Analyzing branches in repository...\n\n   Main branch: main\n   Current branch: feature/new-api\n\n   === Merged Branches (safe to delete) ===\n   feature/bug-fix-123        Merged 2 weeks ago\n   feature/update-deps        Merged 1 month ago\n\n   === Gone Branches (remote deleted) ===\n   feature/old-feature        Remote: gone\n   hotfix/urgent-fix          Remote: gone\n\n   === Stale Branches (no activity > 3 months) ===\n   experiment/prototype       Last commit: 4 months ago, not merged\n\n   === Protected Branches (will not delete) ===\n   main\n   develop\n\n   Recommendations:\n   - Safe to delete: feature/bug-fix-123, feature/update-deps (merged)\n   - Safe to delete: feature/old-feature, hotfix/urgent-fix (remote gone)\n   - Review needed: experiment/prototype (unmerged, stale)\n\n   What would you like to delete?\n   ```\n\n2. **Dry run (preview only)**:\n   ```\n   /git:branch-cleanup --dry-run\n   ```\n   Output:\n   ```\n   [DRY RUN MODE - No changes will be made]\n\n   Would delete the following merged branches:\n   - feature/bug-fix-123\n   - feature/update-deps\n\n   Would delete the following gone branches:\n   - feature/old-feature\n   - hotfix/urgent-fix\n\n   Total: 4 branches would be deleted\n   ```\n\n3. **Merged branches only**:\n   ```\n   /git:branch-cleanup --merged-only\n   ```\n   Output:\n   ```\n   Analyzing merged branches...\n\n   Found 3 merged branches:\n   - feature/bug-fix-123\n   - feature/update-deps\n   - feature/ui-improvements\n\n   Delete these branches? (y/n)\n   ```\n\n4. **Including remote cleanup**:\n   ```\n   /git:branch-cleanup --remote\n   ```\n   Output:\n   ```\n   Deleting local and remote branches...\n\n    Deleted local: feature/bug-fix-123\n    Deleted remote: origin/feature/bug-fix-123\n    Deleted local: feature/update-deps\n    Deleted remote: origin/feature/update-deps\n\n   Summary: Deleted 2 branches locally and remotely\n   ```\n\n## Arguments\n- `--dry-run`: Preview which branches would be deleted without actually deleting them\n- `--merged-only`: Only consider branches that have been fully merged into the main branch\n- `--remote`: Also delete branches from the remote repository (requires push permissions)\n- `--force`: Force delete branches even if they have unmerged commits (use with caution)\n- `--older-than=<days>`: Only consider branches with no commits in the last N days (default: 90)\n\n## Safety Considerations\n- **Never delete**: Current branch, main, master, develop, or release/* branches\n- **Require confirmation**: Always ask user before deleting branches\n- **Preserve unmerged work**: By default, only delete merged branches unless `--force` is used\n- **Backup suggestion**: Recommend creating a backup of unmerged branches before deletion\n- **Remote deletion**: Only delete remote branches if user explicitly requests with `--remote` flag"
              },
              {
                "name": "/cherry-pick-by-patch",
                "description": "Cherry-pick git commit into current branch by \"patch\" command",
                "path": "plugins/git/commands/cherry-pick-by-patch.md",
                "frontmatter": {
                  "argument-hint": "<commit_hash>",
                  "description": "Cherry-pick git commit into current branch by \"patch\" command"
                },
                "content": "## Name\ngit:cherry-pick-by-patch\n\n## Synopsis\n```\n/git:cherry-pick-by-patch commit_hash\n```\n\n## Description\n\nThe `/git-cherry-pick-by-patch commit_hash` command cherry-picks commit with hash\n`commit_hash` into current branch. Rather then doing `git cherry-pick commit_hash`,\nthe command streams the output of `git show commit_hash` to\n`patch -p1 --no-backup-if-mismatch`, and then commit changes with commit message\nfrom `commit_hash` commit.\n\n## Implementation\n\n### Pre-requisites\n\nThe commit with hash `commit_hash` must exist. To verify that use:\n```bash\ngit show commit_hash\n```\nand check if exit code is zero.\n\nFail, if there is no `commit_hash` in the current repository checkout.\n\n### Cherry-pick `commit_hash` into current branch\n\n1. Execute command\n    ```bash\n    git show commit_hash | patch -p1 --no-backup-if-mismatch\n    ```\nand check if exit code is zero. Fail if exit code is not zero.\n\n2. Find files removed from local checkout by the patch command and execute `git rm` for them.\n\n3. Find files added or modified by the patch command and execute `git add` for them.\n\n4. Commit changes by `git commit` command and use commit title and description from `commit_hash` commit.\n\n## Arguments\n\n- **$1** (required): Commit hash (e.g., `902409c0`) of commit to cherry-pick."
              },
              {
                "name": "/commit-suggest",
                "description": "Generate Conventional Commits style commit messages or summarize existing commits",
                "path": "plugins/git/commands/commit-suggest.md",
                "frontmatter": {
                  "description": "Generate Conventional Commits style commit messages or summarize existing commits",
                  "argument-hint": "[N]"
                },
                "content": "## Name\ngit:commit-suggest\n\n## Synopsis\n```\n/git:commit-suggest       # Analyze staged changes\n/git:commit-suggest [N]     # Analyze last N commits (1-100)\n```\n\n## Description\nAI-powered command that analyzes code changes and generates Conventional Commitsstyle messages.\n\n**Modes:**\n- **Mode 1 (no argument)**  Analyze staged changes (`git add` required)\n- **Mode 2 (with N)**  Analyze last N commits to rewrite (N=1) or summarize for squash (N2)\n\n**Use cases:**\n- Create standardized commit messages\n- Improve or rewrite existing commits\n- Generate squash messages for PR merges\n\n**Difference from `/git:summary`**  That command is read-only, while `git:commit-suggest` generates actionable commit message suggestions for user review and manual use.\n\n## Implementation\n\nThe command operates in two modes based on input:\n\n**Mode 1 (no argument):**\n1. Collect staged changes via `git diff --cached`\n2. Analyze file paths and code content to identify type (feat/fix/etc.) and scope\n3. Generate 3 commit message suggestions (Recommended, Standard, Minimal)\n4. Display formatted suggestions and prompt user for selection\n   - Ask: \"Which suggestion would you like to use? (1/2/3 or skip)\"\n   - Support responses: `1`, `use option 2`, `commit with option 3`, `skip`\n   - Execute `git commit` with selected message if user requests\n\n**Mode 2 (with N):**\n1. Retrieve last N commits using `git log`\n2. Parse commit messages to extract types, scopes, and descriptions\n3. For **N=1**: Suggest improved rewrite\n   For **N2**: Merge commits intelligently by type priority (`fix > feat > perf > refactor > docs > test > chore`)\n4. Generate 3 commit message suggestions (Recommended, Standard, Minimal)\n5. Display formatted suggestions and prompt user for selection\n   - Ask: \"Which suggestion would you like to use? (1/2/3 or skip)\"\n   - Support responses: `1`, `use option 2`, `amend with option 3`, `skip`\n   - Execute `git commit --amend` (N=1) or squash operation (N2) if user requests\n\n## Examples\n\n```bash\n# Generate message for staged files\ngit add src/auth.ts src/middleware.ts\n/git:commit-suggest\n\n# Rewrite last commit message\n/git:commit-suggest 1\n\n# Summarize last 5 commits for squash\n/git:commit-suggest 5\n```\n\n## Return Value\n\nGenerates 3 commit message suggestions:\n- **Suggestion #1 (Recommended)**  Detailed with full body and metadata\n- **Suggestion #2 (Standard)**  Concise with main points\n- **Suggestion #3 (Minimal)**  Title and short summary\n\nEach suggestion includes:\n- Conventional Commits message (`type(scope): description`)\n- Blank line between title and body\n- Optional body text explaining the changes\n- Optional footer (issue refs, co-authors, breaking changes, etc.)\n\n**Example:**\n```\n\nSuggestion #1 (Recommended)\n\nfeat(auth): add JWT authentication middleware\n\nImplement token-based authentication for API endpoints.\nThe middleware verifies JWT tokens and extracts user information.\n\nFixes: #123\n\nWhich suggestion would you like to use? (1/2/3 or skip)\n```\n\n### Mode 2 Specifics\n\n- **N=1**  Suggest improved rewrite for the last commit\n- **N2**  Generate unified squash message with footer: `Squashed from N commits:` + original commit list\n\n## Conventional Commits Reference\n\n### Format\n```\ntype(scope): description\n\n[optional body]\n\n[optional footer]\n```\n\n### Common Types\n- `feat`  New feature\n- `fix`  Bug fix\n- `docs`  Documentation changes\n- `refactor`  Code refactoring\n- `perf`  Performance improvements\n- `test`  Test additions or modifications\n- `build`  Build system or dependency changes\n- `ci`  CI configuration changes\n- `chore`  Other changes that don't modify src or test files\n\n### Scope & Footer Examples\n\n**Scope**: `auth`, `api`, `ui`, `db`, `deps` (indicates affected module)\n\n**Footer**:\n- Issue refs: `Fixes: #123`, `Closes: #456`, `Related: #789`\n- Breaking changes: `BREAKING CHANGE: description`\n- Co-authors: `Co-authored-by: Name <email@example.com>`\n\n## Arguments\n\n- **[N]** (optional): Number of recent commits to analyze (1-100)\n  - If omitted: Analyzes staged changes (Mode 1)\n  - If N=1: Suggests improved rewrite for the last commit\n  - If N2: Generates unified squash message for last N commits\n\n## See Also\n- **`/git:summary`**  Display repository status and recent commits (read-only)\n- [Conventional Commits Specification](https://www.conventionalcommits.org/)"
              },
              {
                "name": "/debt-scan",
                "description": "Analyze technical debt indicators in the repository",
                "path": "plugins/git/commands/debt-scan.md",
                "frontmatter": {
                  "description": "Analyze technical debt indicators in the repository",
                  "argument-hint": null
                },
                "content": "## Name\ngit:debt-scan\n\n## Synopsis\n```\n/git:debt-scan\n```\n\n## Description\nThe `git:debt-scan` command provides a comprehensive analysis of technical debt indicators in the current Git repository. It scans for common code health signals including TODO/FIXME comments, stale branches, large files, uncommitted changes, and recent development activity patterns. This command is designed to give developers quick insights into areas that may need attention without making any modifications to the repository.\n\nIt provides essential information for developers including:\n- Count and locations of TODO, FIXME, HACK, and XXX comments\n- Stale branches tracking openshift org remotes that may need cleanup (excludes main, master, develop, release-*, gh-pages, local-only branches, and personal forks)\n- Large git-tracked files that might benefit from refactoring\n- Uncommitted or unstaged changes\n- Recent commit activity trends\n\nThe spec sections is inspired by https://man7.org/linux/man-pages/man7/man-pages.7.html#top_of_page\n\n## Implementation\n- Executes multiple analysis commands to gather technical debt indicators\n- Searches codebase for technical debt comments (TODO, FIXME, HACK, XXX)\n- Identifies stale branches tracking openshift org GitHub remotes (excludes main, master, develop, release-*, gh-pages, local-only branches, and personal forks)\n- Verifies branches still exist on upstream remote to avoid false positives\n- Finds large git-tracked files that may need refactoring\n- Shows uncommitted changes\n- Analyzes recent commit patterns\n- Formats output for clear readability\n- All operations are read-only with no side effects\n\nImplementation logic:\n```bash\n# Search for technical debt comments\necho \"=== Technical Debt Comments ===\"\nFILE_PATTERNS=\"*.{js,ts,go,py,java,rb,c,cpp,h,hpp,cs,php,swift,kt}\" && for marker in TODO FIXME HACK XXX; do echo \"$marker comments: $(grep -r \"$marker\" --include=\"$FILE_PATTERNS\" . 2>/dev/null | grep -v '.git/' | wc -l | xargs)\"; done\n\n# Show top 10 files with most debt comments\necho -e \"\\n=== Files with Most Debt Comments ===\"\ngrep -r 'TODO\\|FIXME\\|HACK\\|XXX' --include=\"*.{js,ts,go,py,java,rb,c,cpp,h,hpp,cs,php,swift,kt}\" . 2>/dev/null | grep -v '.git/' | cut -d: -f1 | sort | uniq -c | sort -rn | head -10\n\n# Check for stale branches on openshift remotes (excluding main, master, release branches)\necho -e \"\\n=== Stale Branches (not updated in 30+ days) ===\" && bash -c 'OPENSHIFT_REMOTES=$(git remote -v | grep -E \"github\\.com[:/]openshift/\" | grep fetch | awk \"{print \\$1}\" | sort -u); if [ -z \"$OPENSHIFT_REMOTES\" ]; then echo \"(No openshift GitHub remotes found)\"; else for remote in $OPENSHIFT_REMOTES; do git ls-remote --heads \"$remote\" | awk \"{print \\$2}\" | sed \"s|refs/heads/||\" | grep -vE \"^(main|master|develop|release-.*|gh-pages)$\" | while read branch; do commit_info=$(git log -1 --format=\"%cr|%an\" \"$remote/$branch\" 2>/dev/null || echo \"\"); if [ -n \"$commit_info\" ]; then date=$(echo \"$commit_info\" | cut -d\"|\" -f1); author=$(echo \"$commit_info\" | cut -d\"|\" -f2); echo \"$date\" | grep -qE \"years? ago|months? ago|[4-9] weeks ago|[0-9]{2,} weeks ago\" && echo \"$branch - Last commit: $date by $author\"; fi; done | head -10; done; fi'\n\n# Find large files (only git-tracked files)\necho -e \"\\n=== Large Files (>1MB, tracked by git) ===\"\ngit ls-files | xargs ls -lh 2>/dev/null | awk '$5 ~ /M$/ && $5+0 > 1 {print $9 \" - \" $5}' | head -10\n\n# Check uncommitted changes\necho -e \"\\n=== Uncommitted Changes ===\"\ngit status --porcelain | head -20\n\n# Recent commit activity\necho -e \"\\n=== Commit Activity ===\" && echo \"Commits in last week: $(git log --since='1 week ago' --oneline 2>/dev/null | wc -l | xargs)\" && echo \"Commits in last month: $(git log --since='1 month ago' --oneline 2>/dev/null | wc -l | xargs)\" && bash -c 'count=$(git log --since=\"30 days ago\" --oneline 2>/dev/null | wc -l | xargs); if command -v bc >/dev/null 2>&1; then avg=$(echo \"scale=1; $count / 30\" | bc 2>/dev/null); echo \"Average commits per day (last 30 days): ${avg:-0}\"; else echo \"Average commits per day (last 30 days): ~$((count / 30))\"; fi'\n```\n\n## Return Value\n- **Claude agent text**: Formatted analysis including:\n  - Count of technical debt comments by type (TODO, FIXME, HACK, XXX)\n  - List of files with the most debt comments\n  - List of stale branches (30+ days old)\n  - List of large files (>1MB) tracked by git that may need refactoring\n  - Summary of uncommitted/unstaged changes\n  - Recent commit activity statistics\n\n## Examples\n\n1. **Clean repository with minimal debt**:\n   ```\n   /git:debt-scan\n   ```\n   Output:\n   ```\n   === Technical Debt Comments ===\n   TODO comments: 5\n   FIXME comments: 1\n   HACK comments: 0\n   XXX comments: 0\n\n   === Files with Most Debt Comments ===\n      3 ./src/api/users.ts\n      2 ./src/utils/helpers.js\n      1 ./tests/integration.test.ts\n\n   === Stale Branches (not updated in 30+ days) ===\n   feature/old-experiment - Last commit: 3 months ago by Alice\n   bugfix/minor-issue - Last commit: 2 months ago by Bob\n\n   === Large Files (>1MB, tracked by git) ===\n   data/seed.json - 2.1M\n\n   === Uncommitted Changes ===\n   \n\n   === Commit Activity ===\n   Commits in last week: 12\n   Commits in last month: 48\n   Average commits per day (last 30 days): 1.6\n   ```\n\n2. **Repository with technical debt to address**:\n   ```\n   /git:debt-scan\n   ```\n   Output:\n   ```\n   === Technical Debt Comments ===\n   TODO comments: 47\n   FIXME comments: 23\n   HACK comments: 8\n   XXX comments: 5\n\n   === Files with Most Debt Comments ===\n     12 ./src/legacy/payment-processor.js\n      9 ./src/controllers/auth.ts\n      7 ./src/services/notifications.py\n      5 ./src/utils/data-transformer.go\n      4 ./tests/e2e/checkout.test.js\n\n   === Stale Branches (not updated in 30+ days) ===\n   feature/refactor-database - Last commit: 5 months ago by Carol\n   feature/new-ui - Last commit: 4 months ago by Dave\n   bugfix/memory-leak - Last commit: 6 weeks ago by Eve\n\n   === Large Files (>1MB, tracked by git) ===\n   src/legacy/monolith.js - 5.3M\n   dist/bundle.min.js - 3.2M\n   data/migrations.sql - 2.8M\n\n   === Uncommitted Changes ===\n    M src/api/routes.ts\n   ?? temp/debug-logs.txt\n   ?? scripts/experimental.sh\n\n   === Commit Activity ===\n   Commits in last week: 3\n   Commits in last month: 15\n   Average commits per day (last 30 days): 0.5\n   ```\n\n3. **Active development repository**:\n   ```\n   /git:debt-scan\n   ```\n   Output:\n   ```\n   === Technical Debt Comments ===\n   TODO comments: 18\n   FIXME comments: 4\n   HACK comments: 2\n   XXX comments: 0\n\n   === Files with Most Debt Comments ===\n      6 ./src/features/new-feature.ts\n      4 ./src/api/v2/endpoints.go\n      3 ./tests/unit/service.test.js\n\n   === Stale Branches (not updated in 30+ days) ===\n   (no stale branches found)\n\n   === Large Files (>1MB, tracked by git) ===\n   docs/api-reference.pdf - 1.2M\n\n   === Uncommitted Changes ===\n    M src/features/new-feature.ts\n    M tests/unit/service.test.js\n\n   === Commit Activity ===\n   Commits in last week: 28\n   Commits in last month: 89\n   Average commits per day (last 30 days): 3.0\n   ```\n\n## Interpretation Guide\n\n**Technical Debt Comments:**\n- 0-10: Excellent - minimal documented debt\n- 11-30: Good - manageable debt levels\n- 31-100: Moderate - consider dedicating time to address\n- 100+: High - prioritize technical debt reduction\n\n**Stale Branches:**\n- Branches inactive for 30+ days may be abandoned\n- Consider cleaning up or merging forgotten branches\n- Review with team before deletion\n\n**Large Files:**\n- Git-tracked files over 1MB may benefit from:\n  - Code splitting or modularization\n  - Moving data to separate files or Git LFS\n  - Compression or optimization\n  - Being added to .gitignore if they're generated/built files\n\n**Commit Activity:**\n- Low activity may indicate stagnant project\n- Very high activity may indicate need for better planning\n- Consistent activity suggests healthy development pace\n\n## Arguments:\n- None"
              },
              {
                "name": "/fix-cherrypick-robot-pr",
                "description": "Fix a cherrypick-robot PR that needs manual intervention",
                "path": "plugins/git/commands/fix-cherrypick-robot-pr.md",
                "frontmatter": {
                  "description": "Fix a cherrypick-robot PR that needs manual intervention",
                  "argument-hint": "<pr-url> [error-messages]"
                },
                "content": "## Name\ngit:fix-cherrypick-robot-pr\n\n## Synopsis\n```\n/git:fix-cherrypick-robot-pr <pr-url> [error-messages]\n```\n\n## Description\n\nThe `git:fix-cherrypick-robot-pr` command replaces a cherrypick-robot PR with a clean, manually-crafted cherry-pick PR that includes fixes the robot cannot handle.\n\nThe cherrypick-robot creates automated PRs but cannot:\n- Fix verification failures (JSON validation, missing annotations)\n- Resolve merge conflicts\n- Add context-specific fixes\n- Handle edge cases requiring human judgment\n- Apply repository-specific cleanup\n\nThis command helps you create a replacement PR with all necessary fixes applied.\n\n## Implementation\n\n### 1. Extract Information from the Robot PR\n\nUse `gh pr view <pr-url>` to extract:\n- Base branch (e.g., `release-4.19`)\n- PR title (to extract bug ID like `OCPBUGS-65944`)\n- All commit hashes included in the PR\n- PR number for later closure\n- Current PR checks/CI status\n\nExample:\n```bash\ngh pr view <pr-url> --json baseRefName,title,commits,number,statusCheckRollup\n```\n\n### 2. Analyze Error Messages\n\nParse the provided error output to identify:\n- Root causes (JSON validation, missing annotations, conflicts, etc.)\n- Affected files\n- Required fixes\n- Fix strategy\n\n**Error sources (in priority order):**\n1. User-provided error messages (from command arguments)\n2. File path if provided (e.g., `/path/to/ci-errors.log`)\n3. CI failure URL if provided\n4. Automatically fetch from PR status checks\n\n### 3. Discover Git Remotes and Create Branch\n\n```bash\n# Discover the upstream remote (the main repository)\n# Look for a remote that's not owned by the current user\nUPSTREAM_REMOTE=$(git remote -v | grep \"fetch\" | grep -v \"$(git config user.name)\" | awk '{print $1}' | head -1)\n\n# Discover the fork remote (your fork)\nFORK_REMOTE=$(git remote -v | grep \"$(git config user.name).*push\" | awk '{print $1}' | head -1)\n\n# If not found, fall back to common names\nUPSTREAM_REMOTE=${UPSTREAM_REMOTE:-upstream}\nFORK_REMOTE=${FORK_REMOTE:-origin}\n\n# Fetch the latest base branch\ngit fetch $UPSTREAM_REMOTE <base-branch>\n\n# Create new branch following naming convention\ngit checkout -b cherry-pick-<issue-number>-to-<base-branch> $UPSTREAM_REMOTE/<base-branch>\n```\n\nExample branch name: `cherry-pick-12345-to-release-1.0`\n\n### 4. Cherry-Pick Commits\n\nCherry-pick all commits from the robot PR in order:\n\n```bash\n# For each commit hash extracted from the robot PR\ngit cherry-pick <commit-hash>\n\n# OR use the cherry-pick-by-patch command\n/git:cherry-pick-by-patch <commit-hash>\n```\n\nHandle any conflicts that arise during cherry-picking.\n\n### 5. Apply Necessary Fixes Based on Errors\n\nBased on the error analysis from step 2, apply the necessary fixes:\n\n**Analyze the errors to determine:**\n1. Which files are causing failures\n2. What type of failure (validation, conflict, test, build)\n3. What fix strategy is appropriate for the repository\n\n**Common fix strategies:**\n\n- **Validation failures**: Check if files can be excluded from validation or need correction\n- **Generated file mismatches**: Run repository update/regeneration scripts (e.g., `make update`, `make generate`)\n- **Merge conflicts**: Resolve conflicts by reviewing both sides and understanding the target branch context\n- **Test failures**: Update tests to be compatible with the target branch\n- **Build failures**: Update dependencies or build configuration for the target branch\n\n**Apply fixes with clear commits:**\n```bash\n# Make necessary changes based on error analysis\n# Stage and commit each logical fix separately\ngit add <affected-files>\ngit commit -m \"<clear description of what was fixed and why>\"\n```\n\n**Note**: The specific fix commands will vary by repository. Consult the repository's documentation for:\n- Verification script locations and options\n- Code generation/update commands\n- Testing conventions\n- Contribution guidelines\n\n### 6. Push and Create Replacement PR\n\n```bash\n# Use the discovered fork remote (from step 3)\n# If running this step separately, rediscover the fork remote:\nFORK_REMOTE=$(git remote -v | grep \"$(git config user.name).*push\" | awk '{print $1}' | head -1)\nFORK_REMOTE=${FORK_REMOTE:-origin}\n\n# Push to your fork\ngit push -u $FORK_REMOTE cherry-pick-<issue-number>-to-<base-branch>\n\n# Create PR using gh CLI\ngh pr create \\\n  --base <base-branch> \\\n  --title \"[<base-branch>] <BUG-ID>: <Description>\" \\\n  --body \"$(cat <<'EOF'\n## Summary\nCherry-pick of <original-commits> to <base-branch> with manual fixes.\n\n## Commits\n- <commit-1-hash>: <commit-1-message>\n- <commit-2-hash>: <commit-2-message>\n\n## Fixes Applied\n- <description-of-fix-1>\n- <description-of-fix-2>\n\n## References\n- Original PR: #<robot-pr-number>\n- JIRA: <bug-id>\n\n Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\nEOF\n)\"\n```\n\n### 7. Close the Old Robot PR\n\nAdd a comment to the robot PR explaining the closure:\n\n```bash\ngh pr comment <robot-pr-number> --body \"Closing this PR in favor of #<new-pr-number> which includes the following fixes:\n- <specific-fix-1>\n- <specific-fix-2>\n\n/close\"\n```\n\nThe `/close` command triggers the bot to close the PR.\n\n## Return Value\n\n- **Success**: New PR URL and confirmation that old PR is closed\n- **Failure**: Error message with specific issue encountered\n\n## Examples\n\n### Example 1: With Error Messages Pasted Directly\n\n```\n/git:fix-cherrypick-robot-pr https://github.com/org/repo/pull/12345\n\nError messages:\n[paste CI error output here]\n```\n\n**The command will:**\n1. Extract PR information (base branch, commits, bug ID)\n2. Analyze the error messages to identify failure types\n3. Cherry-pick commits to a new branch\n4. Guide you through applying appropriate fixes based on repository conventions\n5. Create a new PR with fixes applied\n6. Close the old robot PR with explanation\n\n### Example 2: With Error Log File Reference\n\n```\n/git:fix-cherrypick-robot-pr https://github.com/org/repo/pull/12345\n\nError log file: /path/to/ci-errors.log\n```\n\nThe command reads the error log file and processes it the same way as Example 1.\n\n### Example 3: With CI Failure Page Link\n\n```\n/git:fix-cherrypick-robot-pr https://github.com/org/repo/pull/12345\n\nCI failure: https://ci-system.example.com/logs/...\n```\n\nThe command fetches the CI logs from the provided URL and analyzes them.\n\n### Example 4: No Error Messages (Auto-detect)\n\n```\n/git:fix-cherrypick-robot-pr https://github.com/org/repo/pull/12345\n```\n\nIf no error messages are provided, the command will:\n1. Check PR status using `gh pr view`\n2. Identify failing checks\n3. Fetch CI logs automatically\n4. Analyze and fix based on detected issues\n\n## Arguments\n\n- **$1** (required): PR URL - The URL of the cherrypick-robot PR to fix (e.g., `https://github.com/org/repo/pull/12345`)\n- **$2** (optional): Error messages - Can be:\n  - Error messages pasted directly\n  - File path to error log (e.g., `/path/to/ci-errors.log`)\n  - CI failure page URL\n  - Omitted (will auto-detect from PR status)\n\n## Common Issues This Handles\n\nBeyond what the robot can do:\n-  **Validation errors** - Apply exclusions or corrections based on repository conventions\n-  **Generated file mismatches** - Run appropriate update/regeneration commands\n-  **Merge conflicts** - Resolve using context\n-  **Test failures** - Update tests for target branch compatibility\n-  **Build failures** - Update dependencies or configuration\n-  **Context-specific fixes** - Apply fixes appropriate for the target branch\n-  **Edge cases** - Handle with human judgment\n\n## Notes\n\n- Works with cherrypick-robot PRs across different repositories\n- Error messages help determine exactly what to fix\n- Automatically discovers git remote names (no hardcoded assumptions)\n- All changes pushed to your fork (auto-discovered remote)\n- New PRs target the upstream repository\n- Branch naming convention: `cherry-pick-<issue>-to-<release>`\n- Maintains full control to add any fixes needed\n- If no error messages provided, will check PR status and CI logs automatically\n- Remote discovery uses `git remote -v` and `git config user.name` to identify fork vs upstream\n- Falls back to common names (`origin` for fork, `upstream` for main repo) if auto-discovery fails\n- Fix strategies will vary by repository - consult repository documentation for specific commands"
              },
              {
                "name": "/suggest-reviewers",
                "description": "Suggest appropriate reviewers for a PR based on git blame and OWNERS files",
                "path": "plugins/git/commands/suggest-reviewers.md",
                "frontmatter": {
                  "description": "Suggest appropriate reviewers for a PR based on git blame and OWNERS files",
                  "argument-hint": "[base-branch]"
                },
                "content": "## Name\ngit:suggest-reviewers\n\n## Synopsis\n```\n/git:suggest-reviewers [base-branch]\n```\n\n## Description\nThe `git:suggest-reviewers` command analyzes changed files and suggests the most appropriate reviewers for a pull request. It works with both committed changes on feature branches and uncommitted changes (even on the main branch), making it useful before you've created a branch or made any commits. It prioritizes developers who have actually contributed to the code being modified, using git blame data as the primary signal and OWNERS files as supporting information.\n\nThe command performs the following analysis:\n- Identifies all files changed (both committed and uncommitted changes)\n- Runs git blame on changed lines to find recent and frequent contributors\n- Searches for OWNERS files in the directories of changed files (and parent directories)\n- Aggregates and ranks potential reviewers based on:\n  - Frequency and recency of contributions to modified code (highest priority)\n  - Presence in OWNERS files (secondary consideration)\n- Outputs a prioritized list of suggested reviewers\n\nThis command is particularly useful for large codebases with distributed ownership where choosing the right reviewer can be challenging. You can use it at any stage of development - from uncommitted local changes to a complete feature branch ready for PR.\n\n## Implementation\n\n### Step 1: Determine the base branch\n- If `base-branch` argument is provided, use it\n- Otherwise, detect the main branch (usually `main` or `master`)\n- Verify the base branch exists\n\n```bash\n# Detect main branch if not provided\ngit symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'\n```\n\n### Step 2: Get changed files\n- Determine the current branch name\n- Detect if we're on the base branch (main/master) or a feature branch\n- Detect if there are uncommitted changes (staged or unstaged)\n- List all modified, added, or renamed files based on the scenario\n- Exclude deleted files (no one to blame)\n\n**Scenario detection:**\n```bash\n# Get current branch\ncurrent_branch=$(git branch --show-current)\n\n# Check if on base branch\nif [ \"$current_branch\" = \"$base_branch\" ]; then\n  on_base_branch=true\nelse\n  on_base_branch=false\nfi\n\n# Check for uncommitted changes\nhas_uncommitted=$(git status --short | grep -v '^??' | wc -l)\n```\n\n**Case 1: On base branch (main/master) with uncommitted changes**\n```bash\n# Get staged changes\ngit diff --name-only --diff-filter=d --cached\n\n# Get unstaged changes\ngit diff --name-only --diff-filter=d\n\n# Combine and deduplicate\n```\n\n**Case 2: On feature branch with only committed changes**\n```bash\n# Get all changes from base branch to HEAD\ngit diff --name-only --diff-filter=d ${base_branch}...HEAD\n```\n\n**Case 3: On feature branch with committed + uncommitted changes**\n```bash\n# Get committed changes\ngit diff --name-only --diff-filter=d ${base_branch}...HEAD\n\n# Get uncommitted changes\ngit diff --name-only --diff-filter=d HEAD\ngit diff --name-only --diff-filter=d --cached\n\n# Combine and deduplicate all files\n```\n\n**Case 4: On base branch with no changes**\n- Display error: \"No changes detected. Please make some changes or switch to a feature branch.\"\n\n### Step 3: Analyze git blame for changed lines\n\n**IMPORTANT: Use the helper script** `${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py` to perform this analysis. Do NOT implement this logic manually.\n\nThe script automatically handles:\n- Parsing git diff to identify specific line ranges that were modified\n- Running git blame on those line ranges (not entire files)\n- Extracting and aggregating author information\n- Filtering out bot accounts\n\n**For uncommitted changes:**\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py \\\n  --mode uncommitted \\\n  --file <file1> \\\n  --file <file2> \\\n  --output json\n```\n\n**For committed changes on feature branch:**\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py \\\n  --mode committed \\\n  --base-branch ${base_branch} \\\n  --file <file1> \\\n  --file <file2> \\\n  --output json\n```\n\n**Output format:**\n```json\n{\n  \"Author Name\": {\n    \"line_count\": 45,\n    \"most_recent_date\": \"2024-10-15T14:23:10\",\n    \"files\": [\"file1.go\", \"file2.go\"],\n    \"email\": \"author@example.com\"\n  }\n}\n```\n\n### Step 4: Find OWNERS files\n- For each changed file, search for OWNERS files in:\n  - The same directory\n  - Parent directories up to repository root\n- Parse OWNERS files to extract:\n  - `approvers`: People who can approve changes\n  - `reviewers`: People who can review changes\n- OWNERS file format (YAML):\n  ```yaml\n  approvers:\n    - username1\n    - username2\n  reviewers:\n    - username3\n    - username4\n  ```\n\n### Step 5: Aggregate and rank reviewers\n- Combine data from git blame and OWNERS files\n- Rank potential reviewers based on weighted scoring:\n  1. **Lines contributed** (weight: 10) - More lines modified = better knowledge\n  2. **Recency** (weight: 5) - Recent contributions = current knowledge\n  3. **OWNERS approver + contributor** (weight: 3 bonus) - Authority + knowledge\n  4. **OWNERS reviewer + contributor** (weight: 2 bonus) - Review rights + knowledge\n  5. **OWNERS only, no contributions** (weight: 1) - Authority but may lack specific knowledge\n- Exclude the current PR author from suggestions\n- Filter out bot accounts (e.g., \"openshift-bot\", \"k8s-ci-robot\", \"*[bot]\")\n- Normalize scores and sort by total score\n\n### Step 6: Output results\n- Display reviewers in ranked order\n- Show why each reviewer is suggested (contribution count, recency, OWNERS role)\n- Group by priority tiers based on score ranges\n- Include GitHub usernames if available\n- Show files each reviewer has worked on\n\n## Return Value\n- **Claude agent text**: Formatted list of suggested reviewers including:\n  - **Primary reviewers**: Major contributors to the modified code\n  - **Secondary reviewers**: Moderate contributors or OWNERS with some contributions\n  - **Additional reviewers**: OWNERS members or minor contributors\n  - Explanation for each suggestion (e.g., \"Modified 45 lines across 3 files, last contribution 5 days ago, OWNERS approver\")\n  - Summary of analysis (files analyzed, OWNERS files found, total lines changed)\n\n## Examples\n\n1. **Basic usage** (auto-detect base branch):\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzed 8 files changed from main (245 lines modified)\n   Found 3 OWNERS files\n\n   PRIMARY REVIEWERS:\n   - @alice (modified 89 lines across 4 files, last contribution 5 days ago, OWNERS approver)\n   - @bob (modified 67 lines in pkg/controller/manager.go, last contribution 2 weeks ago)\n\n   SECONDARY REVIEWERS:\n   - @charlie (modified 45 lines across 2 files, last contribution 1 month ago, OWNERS reviewer)\n   - @diana (modified 23 lines in pkg/api/handler.go, last contribution 3 weeks ago)\n\n   ADDITIONAL REVIEWERS:\n   - @eve (OWNERS approver in pkg/util/, no recent contributions to changed code)\n\n   Recommendation: Add @alice and @bob as reviewers\n   ```\n\n2. **Specify base branch**:\n   ```\n   /git:suggest-reviewers release-4.15\n   ```\n   Output:\n   ```\n   Analyzed 3 files changed from release-4.15 (78 lines modified)\n   Found 2 OWNERS files\n\n   PRIMARY REVIEWERS:\n   - @frank (modified 56 lines in vendor/kubernetes/client.go, last contribution 1 week ago, OWNERS approver)\n\n   SECONDARY REVIEWERS:\n   - @grace (modified 12 lines in vendor/kubernetes/types.go, last contribution 2 months ago)\n   - @henry (OWNERS reviewer in vendor/kubernetes/, contributed to adjacent code 1 month ago)\n\n   Recommendation: Add @frank as primary reviewer, @grace as optional\n   ```\n\n3. **No OWNERS files found**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzed 5 files changed from main (156 lines modified)\n   No OWNERS files found in modified paths\n\n   SUGGESTED REVIEWERS (based on code contributions):\n   - @isabel (modified 89 lines across 4 files, last contribution 5 days ago)\n   - @jack (modified 34 lines in src/main.ts, last contribution 10 days ago)\n   - @karen (modified 12 lines in src/utils.ts, last contribution 3 months ago)\n\n   Note: No OWNERS files found. Consider consulting team leads for additional reviewers.\n   ```\n\n4. **Single file change**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzed 1 file changed from main: src/auth/login.ts (34 lines modified)\n   Found 1 OWNERS file\n\n   PRIMARY REVIEWERS:\n   - @lisa (modified 28 lines, last contribution 3 weeks ago, OWNERS approver)\n\n   SECONDARY REVIEWERS:\n   - @mike (modified 6 lines, last contribution 2 months ago, OWNERS reviewer)\n\n   Recommendation: Add @lisa as reviewer\n   ```\n\n5. **OWNERS members with no contributions**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzed 4 files changed from main (112 lines modified)\n   Found 2 OWNERS files\n\n   PRIMARY REVIEWERS:\n   - @noah (modified 78 lines across 3 files, last contribution 1 week ago)\n\n   SECONDARY REVIEWERS:\n   - @olivia (modified 34 lines in pkg/config/parser.go, last contribution 5 weeks ago)\n\n   ADDITIONAL REVIEWERS:\n   - @paul (OWNERS approver in pkg/, no contributions to changed code)\n   - @quinn (OWNERS reviewer in pkg/, no contributions to changed code)\n\n   Recommendation: Add @noah as primary reviewer. OWNERS members @paul and @quinn\n   may provide approval but consider @noah for technical review.\n   ```\n\n6. **Uncommitted changes on main branch**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzing uncommitted changes on main branch\n   Found 3 modified files (2 staged, 1 unstaged) - 87 lines modified\n   Found 1 OWNERS file\n\n   PRIMARY REVIEWERS:\n   - @rachel (modified 45 lines across 2 files, last contribution 2 weeks ago, OWNERS approver)\n   - @steve (modified 32 lines in src/api/handler.ts, last contribution 1 month ago)\n\n   SECONDARY REVIEWERS:\n   - @tina (modified 10 lines in src/utils/format.ts, last contribution 3 months ago)\n\n   Recommendation: Add @rachel and @steve as reviewers\n\n   Note: These are uncommitted changes. Consider creating a feature branch and committing before creating a PR.\n   ```\n\n7. **Uncommitted changes on feature branch**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Analyzing branch feature/add-logging (includes uncommitted changes)\n   - Committed changes from main: 4 files, 156 lines\n   - Uncommitted changes: 2 files, 34 lines\n   Total: 5 unique files, 190 lines modified\n   Found 2 OWNERS files\n\n   PRIMARY REVIEWERS:\n   - @uma (modified 98 lines across 3 files, last contribution 1 week ago, OWNERS reviewer)\n   - @victor (modified 67 lines in pkg/logger/logger.go, last contribution 2 weeks ago)\n\n   SECONDARY REVIEWERS:\n   - @wendy (modified 25 lines in pkg/config/settings.go, last contribution 1 month ago, OWNERS approver)\n\n   Recommendation: Add @uma and @victor as reviewers\n\n   Note: You have uncommitted changes. Consider committing them before creating a PR.\n   ```\n\n8. **No changes detected**:\n   ```\n   /git:suggest-reviewers\n   ```\n   Output:\n   ```\n   Error: No changes detected.\n\n   You are on branch 'main' with no uncommitted changes.\n\n   To use this command:\n   - Make some changes to files (staged or unstaged), or\n   - Switch to a feature branch with committed changes, or\n   - Create a new feature branch with: git checkout -b feature/your-feature-name\n   ```\n\n## Arguments\n- `base-branch` (optional): The base branch to compare against (default: auto-detect main branch, usually `main` or `master`)\n\n## Notes\n- The command analyzes both committed and uncommitted changes\n- Works on any branch, including main/master (analyzes uncommitted changes in this case)\n- For uncommitted changes, git blame is run on HEAD; for committed changes, on the base branch\n- OWNERS files must be in YAML format with `approvers` and/or `reviewers` fields\n- The current user (detected from git config) is automatically excluded from suggestions\n- Reviewers are ranked primarily by their contribution to the specific code being changed\n- OWNERS membership provides a bonus but is not the primary ranking factor\n- If no reviewers are found via git blame, OWNERS members will be suggested as fallback\n- If you're on the base branch with no uncommitted changes, the command will display an error"
              },
              {
                "name": "/summary",
                "description": "Show current branch, git status, and recent commits for quick context",
                "path": "plugins/git/commands/summary.md",
                "frontmatter": {
                  "description": "Show current branch, git status, and recent commits for quick context",
                  "argument-hint": null
                },
                "content": "## Name\ngit:summary\n\n## Synopsis\n```\n/git:summary\n```\n\n## Description\nThe `git:summary` command provides a comprehensive overview of the current Git repository state. It displays the current branch, tracking status, working tree status, and recent commit history in a single concise view. This command is designed to give developers quick context about their repository without running multiple Git commands manually.\n\nIt provides essential information for developers including:\n- Current branch and remote tracking status (ahead/behind)\n- Working tree status (modified, staged, and untracked files)\n- Recent commit history with one-line summaries\n- Uncommitted changes summary\n\nThe spec sections is inspired by https://man7.org/linux/man-pages/man7/man-pages.7.html#top_of_page\n\n## Implementation\n- Executes multiple git commands to gather repository state\n- Retrieves current branch name and tracking information\n- Shows git status for modified, staged, and untracked files\n- Displays last 5 commits with one-line summaries\n- Summarizes uncommitted changes\n- Formats output for clear readability\n- All information is read-only with no side effects\n\nImplementation logic:\n```bash\n# Get current branch and tracking status\ngit branch -vv\n\n# Show working tree status\ngit status --short\n\n# Display recent commits\ngit log --oneline -5\n\n# Summarize uncommitted changes\ngit diff --stat\n```\n\n## Return Value\n- **Claude agent text**: Formatted summary including:\n  - Current branch name and remote tracking status\n  - List of modified, staged, and untracked files\n  - Last 5 commit messages with hashes\n  - Statistics of uncommitted changes\n\n## Examples\n\n1. **Basic usage**:\n   ```\n   /git:summary\n   ```\n   Output:\n   ```\n   Current branch: main\n   Your branch is up to date with 'origin/main'.\n\n   Modified files:\n    M src/index.ts\n   ?? temp/\n\n   Recent commits:\n   abc123 Fix authentication bug\n   def456 Add user profile feature\n   ghi789 Update dependencies\n   jkl012 Refactor database layer\n   mno345 Initial commit\n\n   Uncommitted changes:\n   1 file changed, 15 insertions(+), 3 deletions(-)\n   ```\n\n2. **Repository with no changes**:\n   ```\n   /git:summary\n   ```\n   Output:\n   ```\n   Current branch: develop\n   Your branch is up to date with 'origin/develop'.\n\n   Working tree clean\n\n   Recent commits:\n   pqr678 Merge pull request #42\n   stu901 Add test coverage\n   vwx234 Fix linting issues\n   yza567 Update README\n   bcd890 Release v2.0.0\n   ```\n\n## Arguments:\n- None"
              }
            ],
            "skills": [
              {
                "name": "Suggest Reviewers Helper",
                "description": "Git blame analysis helper for the suggest-reviewers command",
                "path": "plugins/git/skills/suggest-reviewers/SKILL.md",
                "frontmatter": {
                  "name": "Suggest Reviewers Helper",
                  "description": "Git blame analysis helper for the suggest-reviewers command"
                },
                "content": "# Suggest Reviewers Helper\n\nThis skill provides a Python helper script that analyzes git blame data for the `/git:suggest-reviewers` command. The script handles the complex task of identifying which lines were changed and who authored the original code.\n\n## When to Use This Skill\n\nUse this skill when implementing the `/git:suggest-reviewers` command. The helper script should be invoked during Step 3 of the command implementation (analyzing git blame for changed lines).\n\n**DO NOT implement git blame analysis manually** - always use the provided `analyze_blame.py` script.\n\n## Prerequisites\n\n- Python 3.6 or higher\n- Git repository with commit history\n- Git CLI available in PATH\n\n## Helper Script: analyze_blame.py\n\nThe `analyze_blame.py` script automates the complex process of:\n1. Parsing git diff output to identify specific line ranges that were modified\n2. Running git blame on only the changed line ranges (not entire files)\n3. Extracting and aggregating author information with statistics\n4. Filtering out bot accounts automatically\n\n### Usage\n\n**For uncommitted changes:**\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py \\\n  --mode uncommitted \\\n  --file path/to/file1.go \\\n  --file path/to/file2.py \\\n  --output json\n```\n\n**For committed changes on a feature branch:**\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py \\\n  --mode committed \\\n  --base-branch main \\\n  --file path/to/file1.go \\\n  --file path/to/file2.py \\\n  --output json\n```\n\n### Parameters\n\n- `--mode`: Required. Either `uncommitted` or `committed`\n  - `uncommitted`: Analyzes unstaged/staged changes against HEAD\n  - `committed`: Analyzes committed changes against a base branch\n\n- `--base-branch`: Required when mode is `committed`. The base branch to compare against (e.g., `main`, `master`)\n\n- `--file`: Can be specified multiple times. Each file to analyze for blame information. Only changed files should be passed.\n\n- `--output`: Output format. Default is `json`. Options:\n  - `json`: Machine-readable JSON output\n  - `text`: Human-readable text output\n\n### Output Format (JSON)\n\n```json\n{\n  \"Author Name\": {\n    \"line_count\": 45,\n    \"most_recent_date\": \"2024-10-15T14:23:10\",\n    \"files\": [\"file1.go\", \"file2.go\"],\n    \"email\": \"author@example.com\"\n  },\n  \"Another Author\": {\n    \"line_count\": 23,\n    \"most_recent_date\": \"2024-09-20T09:15:33\",\n    \"files\": [\"file3.py\"],\n    \"email\": \"another@example.com\"\n  }\n}\n```\n\n### Output Fields\n\n- `line_count`: Total number of modified lines authored by this person\n- `most_recent_date`: ISO 8601 timestamp of their most recent contribution to the changed code\n- `files`: Array of files where this author has contributions in the changed lines\n- `email`: Author's email address from git commits\n\n### Bot Filtering\n\nThe script automatically filters out common bot accounts:\n- GitHub bots (e.g., `dependabot[bot]`, `renovate[bot]`)\n- CI bots (e.g., `openshift-ci-robot`, `k8s-ci-robot`)\n- Generic bot patterns (any name containing `[bot]` or ending in `-bot`)\n\n## Implementation Steps\n\n### Step 1: Collect changed files\n\nBefore invoking the script, collect the list of changed files based on the scenario:\n\n**Uncommitted changes:**\n```bash\n# Get staged and unstaged files\nfiles=$(git diff --name-only --diff-filter=d HEAD)\nfiles+=\" $(git diff --name-only --diff-filter=d --cached)\"\n```\n\n**Committed changes:**\n```bash\n# Get files changed from base branch\nfiles=$(git diff --name-only --diff-filter=d ${base_branch}...HEAD)\n```\n\n### Step 2: Invoke the script\n\nBuild the command with the appropriate mode and all changed files:\n\n```bash\n# Start building the command\ncmd=\"python3 ${CLAUDE_PLUGIN_ROOT}/skills/suggest-reviewers/analyze_blame.py\"\n\n# Add mode\nif [ \"$has_uncommitted\" = true ] || [ \"$on_base_branch\" = true ]; then\n  cmd=\"$cmd --mode uncommitted\"\nelse\n  cmd=\"$cmd --mode committed --base-branch $base_branch\"\nfi\n\n# Add each file\nfor file in $files; do\n  cmd=\"$cmd --file $file\"\ndone\n\n# Add output format\ncmd=\"$cmd --output json\"\n\n# Execute and capture JSON output\nblame_data=$($cmd)\n```\n\n### Step 3: Parse the output\n\nThe JSON output can be parsed using Python, jq, or any JSON parser:\n\n```bash\n# Example using jq to get top contributor\necho \"$blame_data\" | jq -r 'to_entries | sort_by(-.value.line_count) | .[0].key'\n\n# Example using Python\npython3 << EOF\nimport json\nimport sys\n\ndata = json.loads('''$blame_data''')\n\n# Sort by line count\nsorted_authors = sorted(data.items(), key=lambda x: x[1]['line_count'], reverse=True)\n\nfor author, stats in sorted_authors:\n    print(f\"{author}: {stats['line_count']} lines, last modified {stats['most_recent_date']}\")\nEOF\n```\n\n### Step 4: Combine with OWNERS data\n\nAfter getting blame data, merge it with OWNERS file information to produce the final ranked list of reviewers.\n\n## Error Handling\n\n### No changed files\n\nIf no files are passed to the script:\n```\nError: No files specified. Use --file option at least once.\n```\n\n**Resolution:** Ensure you've detected changed files correctly before invoking the script.\n\n### Invalid mode\n\nIf an invalid mode is specified:\n```\nError: Invalid mode 'invalid'. Must be 'uncommitted' or 'committed'.\n```\n\n**Resolution:** Use either `--mode uncommitted` or `--mode committed`.\n\n### Missing base branch in committed mode\n\nIf `--mode committed` is used without `--base-branch`:\n```\nError: --base-branch is required when mode is 'committed'.\n```\n\n**Resolution:** Provide the base branch: `--base-branch main`\n\n### File not in repository\n\nIf a specified file is not tracked by git:\n```\nWarning: File 'path/to/file' is not tracked by git, skipping.\n```\n\n**Resolution:** This is a warning and can be safely ignored. The script will skip untracked files.\n\n### No blame data found\n\nIf git blame returns no data for any files:\n```json\n{}\n```\n\n**Resolution:** This can happen if:\n- All changed files are newly created (no blame history)\n- All changes are in binary files\n- Git blame is unable to run\n\nIn this case, fall back to OWNERS-only suggestions.\n\n## Examples\n\n### Example 1: Analyze uncommitted changes\n\n```bash\n$ python3 analyze_blame.py --mode uncommitted --file src/main.go --file src/utils.go --output json\n{\n  \"Alice Developer\": {\n    \"line_count\": 45,\n    \"most_recent_date\": \"2024-10-15T14:23:10\",\n    \"files\": [\"src/main.go\", \"src/utils.go\"],\n    \"email\": \"alice@example.com\"\n  },\n  \"Bob Engineer\": {\n    \"line_count\": 12,\n    \"most_recent_date\": \"2024-09-20T09:15:33\",\n    \"files\": [\"src/main.go\"],\n    \"email\": \"bob@example.com\"\n  }\n}\n```\n\n### Example 2: Analyze committed changes on feature branch\n\n```bash\n$ python3 analyze_blame.py --mode committed --base-branch main --file pkg/controller/manager.go --output json\n{\n  \"Charlie Contributor\": {\n    \"line_count\": 78,\n    \"most_recent_date\": \"2024-10-01T11:42:55\",\n    \"files\": [\"pkg/controller/manager.go\"],\n    \"email\": \"charlie@example.com\"\n  }\n}\n```\n\n### Example 3: Text output format\n\n```bash\n$ python3 analyze_blame.py --mode uncommitted --file README.md --output text\n\nBlame Analysis Results:\n=======================\n\nAlice Developer (alice@example.com)\n  Lines: 23\n  Most recent: 2024-10-15T14:23:10\n  Files: README.md\n\nBob Engineer (bob@example.com)\n  Lines: 5\n  Most recent: 2024-08-12T16:30:21\n  Files: README.md\n```\n\n### Example 4: Multiple files with mixed results\n\n```bash\n$ python3 analyze_blame.py --mode committed --base-branch release-4.15 \\\n    --file vendor/k8s.io/client-go/kubernetes/clientset.go \\\n    --file pkg/controller/node.go \\\n    --file docs/README.md \\\n    --output json\n{\n  \"Diana Developer\": {\n    \"line_count\": 156,\n    \"most_recent_date\": \"2024-09-28T13:15:42\",\n    \"files\": [\"vendor/k8s.io/client-go/kubernetes/clientset.go\", \"pkg/controller/node.go\"],\n    \"email\": \"diana@example.com\"\n  },\n  \"Eve Technical Writer\": {\n    \"line_count\": 34,\n    \"most_recent_date\": \"2024-10-10T10:22:18\",\n    \"files\": [\"docs/README.md\"],\n    \"email\": \"eve@example.com\"\n  }\n}\n```\n\n## Technical Details\n\n### How the script works\n\n1. **Determine diff range**: Based on mode, calculates what to compare:\n   - `uncommitted`: Compares working directory against HEAD\n   - `committed`: Compares HEAD against base branch\n\n2. **Parse diff output**: Runs `git diff` with unified format to identify:\n   - Which files changed\n   - Which line ranges were added/modified\n   - Ignores deleted lines (can't blame what doesn't exist)\n\n3. **Run git blame**: For each file and line range:\n   - Runs `git blame -L start,end --line-porcelain file`\n   - Parses porcelain format to extract author, email, and timestamp\n   - Aggregates data across all changed lines\n\n4. **Filter and aggregate**:\n   - Removes bot accounts\n   - Groups by author name\n   - Counts total lines per author\n   - Tracks most recent contribution date\n   - Lists all files each author contributed to\n\n5. **Output results**: Formats as JSON or text based on `--output` parameter\n\n### Performance considerations\n\n- Only blames changed line ranges, not entire files (much faster for small changes to large files)\n- Processes files in parallel when possible\n- Caches git commands where appropriate\n- Skips binary files automatically\n\n## Limitations\n\n- Does not handle file renames/moves (treats as delete + add)\n- Bot filtering is based on common patterns; custom bots may not be filtered\n- Requires git history; newly initialized repos may not have useful data\n- Does not consider commit message content or PR review history\n\n## See Also\n\n- Main command: `/git:suggest-reviewers` in `plugins/git/commands/suggest-reviewers.md`\n- Git blame documentation: https://git-scm.com/docs/git-blame\n- Git diff documentation: https://git-scm.com/docs/git-diff"
              }
            ]
          },
          {
            "name": "hello-world",
            "description": "Hello World Plugin",
            "source": "./plugins/hello-world",
            "category": null,
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install hello-world@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/echo",
                "description": "Hello world plugin implementation",
                "path": "plugins/hello-world/commands/echo.md",
                "frontmatter": {
                  "description": "Hello world plugin implementation",
                  "argument-hint": "[name]"
                },
                "content": "## Name\nhello-world:echo\n\n## Synopsis\n```\n/hello-world:echo [name]\n```\n\n## Description\nThe `hello-world:echo` command prints a greeting message to the console. By default, it prints \"Hello world\", but when provided with a name argument `hello-world:echo $1`, it prints \"Hello ${1}\". This command serves as a basic example of a Claude Code plugin implementation, demonstrating the minimal structure required for a functional plugin command.\n\nIt provides a reference implementation for plugin developers. It demonstrates:\n- Basic command structure\n- Shell command execution within a plugin\n- Handling arguments\n- Minimal configuration requirements\n\nThe spec sections is inspired by https://man7.org/linux/man-pages/man7/man-pages.7.html#top_of_page\n\n## Implementation\n- The command executes a simple bash `echo` statement\n- Accepts an optional name argument (`$1`)\n- If `$1` is provided, outputs \"Hello $1\"\n- If no argument is provided, outputs \"Hello world\"\n- Output is sent directly to standard output\n- The command is stateless and has no side effects\n\nImplementation logic:\n```bash\nif [ -n \"$1\" ]; then\n  echo \"Hello $1\"\nelse\n  echo \"Hello world\"\nfi\n```\n\n## Return Value\n- **Claude agent text**: \"Hello world\" (default) or \"Hello $1\" (when name is provided)\n\n## Examples\n\n1. **Basic usage (no arguments)**:\n   ```\n   /hello-world:echo\n   ```\n   Output:\n   ```\n   Hello world\n   ```\n\n2. **With a name argument**:\n   ```\n   /hello-world:echo Alice\n   ```\n   Output:\n   ```\n   Hello Alice\n   ```\n\n3. **With multiple words as name**:\n   ```\n   /hello-world:echo \"John Doe\"\n   ```\n   Output:\n   ```\n   Hello John Doe\n   ```\n\n## Arguments:\n- $1: The name to be printed \"Hello ${1}\""
              }
            ],
            "skills": []
          },
          {
            "name": "jira",
            "description": "A plugin to automate tasks with Jira",
            "source": "./plugins/jira",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install jira@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/backlog",
                "description": "Find suitable JIRA tickets from the backlog to work on based on priority and activity",
                "path": "plugins/jira/commands/backlog.md",
                "frontmatter": {
                  "description": "Find suitable JIRA tickets from the backlog to work on based on priority and activity",
                  "argument-hint": "[project-key] [--assignee username] [--days-inactive N]"
                },
                "content": "## Name\njira:backlog\n\n## Synopsis\n```\n/jira:backlog [project-key] [--assignee username] [--days-inactive N]\n```\n\n## Description\n\nThe `jira:backlog` command helps identify suitable tickets from the JIRA backlog to work on by **intelligently analyzing** unassigned tickets and bot-assigned tickets. Unlike simple filtering, this command reads ticket descriptions, comments, and activity patterns to recommend the best candidates for work.\n\n**Key Feature:** The command selects **2 tickets from each priority level** (Critical, High, Normal, Low) that are **actually available to pick up** (unassigned or assigned to bots only), giving you a balanced view across all priorities so you can choose based on your expertise and preference.\n\n**Important:** This command only recommends tickets that are **unassigned** or **assigned to bots** (like \"OCP DocsBot\"). Tickets assigned to real people are excluded, even if they have no recent activity, because they may already be claimed by someone.\n\nThis command is particularly useful for:\n- Finding tickets to work on when you have available capacity\n- Identifying unassigned or abandoned tickets that need attention\n- Getting a mix of priorities to choose from (not just critical)\n- Understanding ticket context before starting work\n\nHow it works:\n- Searches for unassigned tickets or tickets with no activity for 28+ days (configurable)\n- **Filters for availability** - keeps only unassigned or bot-assigned tickets\n- **Analyzes ticket content** - reads descriptions, comments, and activity patterns\n- **Evaluates suitability** - identifies tickets that are ready vs. need verification vs. may be abandoned\n- **Selects intelligently** - chooses the most suitable 2 tickets from each priority level\n- Provides comprehensive summaries with recommendations for each ticket\n\n## Prerequisites\n\nThis command requires JIRA credentials to be configured via the JIRA MCP server setup, even though it uses direct API calls instead of MCP commands.\n\n### 1. Install the Jira Plugin\n\nIf you haven't already installed the Jira plugin, see the [Jira Plugin README](../README.md#installation) for installation instructions.\n\n### 2. Configure JIRA Credentials via MCP Configuration File\n\n** Important:** While this command does NOT use MCP commands to query JIRA, it DOES read credentials from the MCP server configuration file. You must configure the MCP server settings even if you're only using this command.\n\n**Why not use MCP commands?** The MCP approach has performance issues when fetching large datasets:\n- Each MCP response must be processed by Claude, consuming tokens\n- Large result sets (even with pagination) cause 413 errors from Claude due to tool result size limits\n- Processing hundreds of tickets through MCP commands creates excessive context usage\n- Direct API calls allow us to stream data to disk without intermediate processing\n\n**Solution:** This command uses `curl` to fetch data directly from JIRA and save to disk, then processes it with Python. It reads JIRA credentials from `~/.config/claude-code/mcp.json` - the same file used by the MCP server.\n\n**Required Configuration File Format:**\n\nCreate or edit `~/.config/claude-code/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"atlassian\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-atlassian\"],\n      \"env\": {\n        \"JIRA_URL\": \"https://issues.redhat.com\",\n        \"JIRA_USERNAME\": \"your-email@redhat.com\",\n        \"JIRA_API_TOKEN\": \"your-atlassian-api-token-here\",\n        \"JIRA_PERSONAL_TOKEN\": \"your-redhat-jira-personal-token-here\"\n      }\n    }\n  }\n}\n```\n\n**Field Descriptions:**\n- `JIRA_URL`: Your JIRA instance URL (e.g., `https://issues.redhat.com` for Red Hat JIRA)\n- `JIRA_USERNAME`: Your JIRA username/email address\n- `JIRA_API_TOKEN`: Atlassian API token from [Atlassian API Token Management Page](https://id.atlassian.com/manage-profile/security/api-tokens)\n- `JIRA_PERSONAL_TOKEN`: Red Hat JIRA Personal Access Token from [Red Hat Jira PAT Management Page](https://issues.redhat.com/secure/ViewProfile.jspa?selectedTab=com.atlassian.pats.pats-plugin:jira-user-personal-access-tokens)\n\n**Note:** The command will use `JIRA_PERSONAL_TOKEN` if available (preferred for Red Hat JIRA), otherwise falls back to `JIRA_API_TOKEN`.\n\n### 3. Start Local MCP Server with Podman\n\n** Recommended Setup:** Use the podman containerized approach. We tested npx methods on October 31, 2025, and encountered 404 errors and missing dependencies. The containerized setup works reliably.\n\n**Steps:**\n\n1. **First, ensure your `~/.config/claude-code/mcp.json` is created with credentials** (see example above)\n\n2. **Start the MCP server container using credentials from mcp.json:**\n\n   ```bash\n   # Extract credentials from your mcp.json file\n   JIRA_URL=$(jq -r '.mcpServers.atlassian.env.JIRA_URL' ~/.config/claude-code/mcp.json)\n   JIRA_USERNAME=$(jq -r '.mcpServers.atlassian.env.JIRA_USERNAME' ~/.config/claude-code/mcp.json)\n   JIRA_API_TOKEN=$(jq -r '.mcpServers.atlassian.env.JIRA_API_TOKEN' ~/.config/claude-code/mcp.json)\n   JIRA_PERSONAL_TOKEN=$(jq -r '.mcpServers.atlassian.env.JIRA_PERSONAL_TOKEN' ~/.config/claude-code/mcp.json)\n\n   # Start the container\n   podman run -d --name mcp-atlassian -p 8080:8080 \\\n     -e \"JIRA_URL=${JIRA_URL}\" \\\n     -e \"JIRA_USERNAME=${JIRA_USERNAME}\" \\\n     -e \"JIRA_API_TOKEN=${JIRA_API_TOKEN}\" \\\n     -e \"JIRA_PERSONAL_TOKEN=${JIRA_PERSONAL_TOKEN}\" \\\n     -e \"JIRA_SSL_VERIFY=true\" \\\n     ghcr.io/sooperset/mcp-atlassian:latest --transport sse --port 8080 -vv\n   ```\n\n3. **Verify the container is running:**\n   ```bash\n   podman ps | grep mcp-atlassian\n   ```\n\n4. **Restart Claude Code** to ensure it reads the mcp.json configuration\n\n**Managing the Container:**\n```bash\n# Check if container is running\npodman ps | grep mcp-atlassian\n\n# View logs\npodman logs mcp-atlassian\n\n# Stop the container\npodman stop mcp-atlassian\n\n# Start the container again\npodman start mcp-atlassian\n\n# Remove the container (you'll need to run 'podman run' again)\npodman rm mcp-atlassian\n```\n\n### 4. Verify MCP Server Configuration\n\nTo verify your MCP server is properly configured and can connect to JIRA, you can test it with a simple JIRA query in Claude Code:\n\n```bash\nAsk Claude Code to run: \"Use the mcp__atlassian__jira_get_issue tool to fetch OCPBUGS-1\"\n```\n\nIf the MCP server is properly configured, you should see issue details returned. If you see an error:\n- **\"Tool not found\"**: The MCP server is not properly registered with Claude Code. Re-run the `claude mcp add` command.\n- **\"Authentication failed\"** or **401/403 errors**: Check your `JIRA_PERSONAL_TOKEN` and `JIRA_USERNAME` are correct.\n- **\"Connection refused\"**: If using a local MCP server, ensure the podman container is running (`podman ps`).\n- **\"Could not find issue\"**: Your authentication works! This just means the specific issue doesn't exist or you don't have access.\n\nSee the [full JIRA Plugin README](../README.md) for complete setup instructions and troubleshooting.\n\n## Implementation\n\nThe command executes the following workflow:\n\n1. **Extract Credentials from MCP Configuration File**\n   - Read credentials from `~/.config/claude-code/mcp.json`\n   - Extract from the `atlassian` MCP server configuration:\n     ```bash\n     MCP_CONFIG=\"$HOME/.config/claude-code/mcp.json\"\n\n     JIRA_URL=$(jq -r '.mcpServers.atlassian.env.JIRA_URL' \"$MCP_CONFIG\")\n     JIRA_EMAIL=$(jq -r '.mcpServers.atlassian.env.JIRA_USERNAME // .mcpServers.atlassian.env.JIRA_EMAIL' \"$MCP_CONFIG\")\n     JIRA_PERSONAL_TOKEN=$(jq -r '.mcpServers.atlassian.env.JIRA_PERSONAL_TOKEN' \"$MCP_CONFIG\")\n     JIRA_API_TOKEN=$(jq -r '.mcpServers.atlassian.env.JIRA_API_TOKEN' \"$MCP_CONFIG\")\n\n     # Use JIRA_PERSONAL_TOKEN if available, otherwise fall back to JIRA_API_TOKEN\n     AUTH_TOKEN=\"${JIRA_PERSONAL_TOKEN:-$JIRA_API_TOKEN}\"\n     ```\n   - If any required credentials are missing or the file doesn't exist, display error:\n     ```bash\n     Error: JIRA credentials not configured.\n\n     This command requires JIRA credentials from the MCP server configuration file.\n     Please create or edit ~/.config/claude-code/mcp.json with your JIRA credentials.\n\n     See Prerequisites section for the required mcp.json format and setup instructions.\n     ```\n\n2. **Parse Arguments and Set Defaults**\n   - Parse project key from $1 (required): \"OCPBUGS\", \"JIRA\", \"HYPE\", etc.\n   - Parse optional --assignee filter (defaults to \"Unassigned\")\n   - Parse optional --days-inactive (defaults to 28 days)\n   - Validate project key format (uppercase, may contain hyphens)\n   - Create working directory: `mkdir -p .work/jira-backlog/{project-key}/`\n\n3. **Construct JQL Query**\n   - Build base JQL query:\n     ```jql\n     project = {project-key}\n     AND status NOT IN (Closed, Resolved, Done, Verified, Release Pending, ON_QA)\n     AND (\n       assignee IS EMPTY\n       OR updated <= -{days-inactive}d\n     )\n     ORDER BY priority DESC, updated ASC\n     ```\n   - If --assignee provided, replace assignee filter with: `AND assignee = {username} AND updated <= -{days-inactive}d`\n   - URL-encode the JQL query for use in API requests\n\n4. **Fetch All Backlog Tickets Using curl with Pagination**\n\n   **Fetch Strategy:**\n   - Fetch 1000 tickets per request (JIRA's maximum `maxResults` value)\n   - Use pagination (`startAt` parameter) to fetch all tickets\n   - Save each batch directly to disk to avoid memory issues\n   - Continue until all tickets are fetched\n\n   **Authentication:**\n   - For Red Hat JIRA (Data Center): Use Bearer token with `JIRA_PERSONAL_TOKEN` (recommended)\n   - For JIRA Cloud: Can use Basic Auth with `email:api_token`, but Bearer token also works\n\n   **Important API Details:**\n   - Use `/rest/api/2/search` endpoint (API v2 works reliably with Red Hat JIRA)\n   - Use `Authorization: Bearer ${JIRA_PERSONAL_TOKEN}` header for authentication\n   - Check HTTP response code to detect authentication failures\n\n   **Batch Processing Loop:**\n   ```bash\n   START_AT=0\n   BATCH_NUM=0\n   TOTAL_FETCHED=0\n\n   while true; do\n     # Construct API URL with pagination\n     API_URL=\"${JIRA_URL}/rest/api/2/search?\\\n      jql=${ENCODED_JQL}&\\\n      startAt=${START_AT}&\\\n      maxResults=1000&\\\n      fields=summary,status,priority,assignee,reporter,created,updated,description,labels,components,watches,comment\"\n\n     # Fetch batch using curl with Bearer token authentication\n     HTTP_CODE=$(curl -s -w \"%{http_code}\" \\\n       -o \"batch-${BATCH_NUM}.json\" \\\n       -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n       -H \"Accept: application/json\" \\\n       \"${API_URL}\")\n\n     # Check HTTP response code\n     if [ \"$HTTP_CODE\" -ne 200 ]; then\n       echo \"Error: HTTP $HTTP_CODE received\"\n       cat \"batch-${BATCH_NUM}.json\"\n       exit 1\n     fi\n\n     # Parse response to check if more results exist\n     BATCH_SIZE=$(jq '.issues | length' \"batch-${BATCH_NUM}.json\")\n     TOTAL=$(jq '.total' \"batch-${BATCH_NUM}.json\")\n\n     TOTAL_FETCHED=$((TOTAL_FETCHED + BATCH_SIZE))\n\n     echo \" Fetched ${BATCH_SIZE} tickets (${TOTAL_FETCHED}/${TOTAL} total)\"\n\n     # Check if done\n     if [ ${TOTAL_FETCHED} -ge ${TOTAL} ] || [ ${BATCH_SIZE} -eq 0 ]; then\n       break\n     fi\n\n     # Move to next batch\n     START_AT=$((START_AT + 1000))\n     BATCH_NUM=$((BATCH_NUM + 1))\n   done\n\n   echo \"\"\n   echo \" Fetching complete: ${TOTAL_FETCHED} tickets downloaded in $((BATCH_NUM + 1)) batch(es)\"\n   ```\n\n   **Why curl instead of MCP:**\n   - Direct file streaming avoids Claude's tool result size limits (413 errors)\n   - Can handle thousands of tickets without token consumption\n   - Faster - no intermediate serialization through MCP protocol\n   - More reliable for large datasets\n\n5. **Process Batches with Python to Filter and Group by Priority**\n\n   Create a Python script (`.work/jira-backlog/{project-key}/process_batches.py`) that:\n\n   **Inputs:**\n   - All batch files: `.work/jira-backlog/{project-key}/batch-*.json`\n\n   **Processing Logic:**\n   ```python\n   import json\n   import glob\n   from collections import defaultdict\n\n   # Load all batches\n   all_tickets = []\n   for batch_file in sorted(glob.glob('.work/jira-backlog/{project-key}/batch-*.json')):\n       with open(batch_file) as f:\n           data = json.load(f)\n           all_tickets.extend(data['issues'])\n\n   # Filter for available tickets\n   available_tickets = []\n   for ticket in all_tickets:\n       assignee = ticket['fields'].get('assignee')\n\n       # Include if unassigned\n       if assignee is None:\n           available_tickets.append(ticket)\n           continue\n\n       # Include if assigned to a bot\n       assignee_name = assignee.get('displayName', '').lower()\n       if 'bot' in assignee_name:\n           available_tickets.append(ticket)\n           continue\n\n       # Otherwise exclude (assigned to real person)\n\n   # Group by priority\n   priority_buckets = defaultdict(list)\n   for ticket in available_tickets:\n       priority_name = ticket['fields']['priority']['name']\n\n       # Normalize priority names\n       if priority_name in ['Critical', 'Blocker']:\n           priority_buckets['Critical'].append(ticket)\n       elif priority_name in ['High', 'Major']:\n           priority_buckets['High'].append(ticket)\n       elif priority_name in ['Normal', 'Medium']:\n           priority_buckets['Normal'].append(ticket)\n       elif priority_name in ['Low', 'Minor', 'Trivial']:\n           priority_buckets['Low'].append(ticket)\n\n   # Save filtered results\n   with open('.work/jira-backlog/{project-key}/filtered.json', 'w') as f:\n       json.dump(priority_buckets, f, indent=2)\n\n   # Save statistics\n   stats = {p: len(tickets) for p, tickets in priority_buckets.items()}\n   with open('.work/jira-backlog/{project-key}/stats.json', 'w') as f:\n       json.dump(stats, f, indent=2)\n\n   print(f\"Filtered {len(available_tickets)} available tickets from {len(all_tickets)} total\")\n   print(f\"Priority distribution: {stats}\")\n   ```\n\n   **Outputs:**\n- `.work/jira-backlog/{project-key}/filtered.json` - All filtered tickets grouped by priority\n- `.work/jira-backlog/{project-key}/stats.json` - Priority distribution statistics\n\n   **Run the script:**\n   ```bash\n   python .work/jira-backlog/${PROJECT_KEY}/process_batches.py\n   ```\n\n6. **Intelligently Analyze and Select Best Tickets from Each Priority**\n\n   **CRITICAL:** This is NOT a mechanical selection process. You must read and analyze ticket content to make intelligent decisions.\n\n   **Load Filtered Data:**\n   - Read filtered tickets from `.work/jira-backlog/{project-key}/filtered.json`\n   - Data is already grouped by priority level (Critical, High, Normal, Low)\n\n   For each priority level (Critical, High, Normal, Low):\n\n   **Step 6a: Extract Tickets for Human-Readable Analysis**\n   - Take the first 10 tickets from each priority bucket (or all available if fewer than 10)\n   - For each ticket, format in a readable way showing:\n     - Key, summary, status, assignee, reporter\n     - Days since last update, number of comments, watchers\n     - Description preview (first 300-400 characters)\n     - Last 1-3 comments (author, date, first 200 characters of body)\n     - Components and labels\n   - **Output this formatted data** so you can read and analyze it\n   - DO NOT analyze inside Python/bash - extract the data in readable format first\n\n   **Step 6b: Analyze Ticket Suitability**\n   Read the formatted data and evaluate each ticket for:\n\n   **Age Classifications (consider when prioritizing):**\n   - **Very Fresh (< 30 days):** Highest priority - likely still relevant and active\n   - **Recent (30-60 days):** Good candidates - still relatively fresh\n   - **Getting Stale (60-90 days):** Verify still relevant before recommending\n   - **Old (90-120 days):** Lower priority - may need verification/grooming\n   - **Very Old (120+ days):** Lowest priority - likely needs re-evaluation before work\n\n   **Disqualifying Factors (skip these tickets):**\n   - **Assigned to real people (non-bots)** - these tickets should have been filtered out in Step 3, but double-check\n   - Status is \"Verified\", \"Release Pending\", \"ON_QA\", \"Done\" (already complete)\n   - Zero comments AND very old (120+ days) - likely abandoned\n   - Comments indicate \"duplicate\", \"won't fix\", \"closed as\"\n   - Comments show work is blocked or waiting on external dependencies\n\n   **Positive Indicators (prioritize these):**\n   - **Unassigned or bot-assigned** - available for anyone to pick up\n   - **Recency matters:** Fresher tickets (< 60 days) should be weighted higher\n   - Active discussion in comments showing real investigation (but not claimed by someone)\n   - Clear, reproducible issue with steps to reproduce\n   - Recent comments (< 30 days) saying \"needs investigation\", \"looking for owner\", \"ready for work\"\n   - Has must-gather, logs, or reproduction environment attached/linked\n   - Well-defined scope and acceptance criteria\n   - Clear description with some comments showing interest\n\n   **Step 6c: Select Best 2 from Each Priority**\n   - Based on your analysis, select the 2 MOST SUITABLE tickets from each priority level\n   - **Selection Criteria (in order of importance):**\n     1. **Recency:** Fresher tickets (< 60 days) strongly preferred to ensure relevance\n     2. **Clarity:** Clear reproduction steps, well-defined scope, available resources (must-gather, logs)\n     3. **Impact:** User-facing issues, customer cases, or significant system problems\n     4. **Activity:** Active discussion/comments showing the issue is real and being tracked\n   - If a priority level has fewer than 2 suitable tickets, include what's available\n   - Document WHY each ticket was selected (your reasoning, including age classification)\n   - Flag tickets that need verification before work can start\n   - In your recommendation, mention the ticket age using the classifications above\n\n7. **Format Output Report**\n   Generate a formatted report organized by priority:\n\n   ```bash\n   # Backlog Tickets for {project-key}\n\n   ## Search Criteria\n   - Project: {project-key}\n   - Assignee: {assignee or \"Unassigned\"}\n   - Days Inactive: {days-inactive}+\n   - Total Tickets Found: {count}\n\n   ---\n\n   ## Critical Priority ({count} tickets)\n\n   ### 1. {ISSUE-KEY}: {Summary}\n   **Status:** {status} | **Updated:** {days} days ago | **Reporter:** {name}\n   **Components:** {components} | **Labels:** {labels}\n   **Watchers:** {count} | **Comments:** {count}\n\n   **Context:**\n   {2-3 sentence summary of the ticket}\n\n   **Recent Activity:**\n   - {Most recent comment summary or \"No recent comments\"}\n   - {Status change or \"No status changes\"}\n   - {Blocker/Question if identified}\n\n   **Recommendation:** {Why this ticket is suitable to work on or what's needed before starting}\n\n   ---\n\n   ### 2. {ISSUE-KEY}: {Summary}\n   ...\n\n   ---\n\n   ## High Priority ({count} tickets)\n   ...\n\n   ## Normal Priority ({count} tickets)\n   ...\n\n   ## Low Priority ({count} tickets)\n   ...\n\n   ---\n\n   ## Summary\n   - Critical: {count} tickets available\n   - High: {count} tickets available\n   - Normal: {count} tickets available\n   - Low: {count} tickets available\n\n   **Suggested Next Steps:**\n   1. Review the tickets above and select one that matches your expertise\n   2. Check if the ticket has clear acceptance criteria - if not, consider grooming it first using `/jira:grooming {issue-key}`\n   3. Use `/jira:solve {issue-key} {remote}` to start working on the ticket\n   ```\n\n8. **Display Report to User**\n   - Show the formatted report\n   - Provide guidance on next steps\n   - Suggest using `/jira:grooming` for tickets lacking clarity\n   - Suggest using `/jira:solve` to start working on selected ticket\n\n9. **Save Report (Optional)**\n   - Offer to save report to `.work/jira-backlog/{project-key}-{timestamp}.md`\n   - Useful for tracking backlog trends over time\n\n**Error Handling:**\n- **Missing credentials file**: If `~/.config/claude-code/mcp.json` doesn't exist, display:\n  ```bash\n  Error: JIRA credentials not configured.\n\n  This command requires JIRA credentials from the MCP server configuration file.\n  File not found: ~/.config/claude-code/mcp.json\n\n  Please create this file with your JIRA credentials.\n  See Prerequisites section for the required mcp.json format and setup instructions.\n  ```\n- **Invalid credentials in file**: If credentials are missing from mcp.json, display:\n  ```bash\n  Error: JIRA credentials incomplete in ~/.config/claude-code/mcp.json\n\n  Required fields in .mcpServers.atlassian.env:\n  - JIRA_URL (e.g., https://issues.redhat.com)\n  - JIRA_USERNAME (your JIRA email/username)\n  - JIRA_PERSONAL_TOKEN (preferred) or JIRA_API_TOKEN\n\n  See Prerequisites section for the required mcp.json format.\n  ```\n- **Authentication failure**: If curl returns 401/403, display:\n  ```bash\n  Error: JIRA authentication failed (HTTP 401/403)\n\n  Please verify your JIRA credentials in ~/.config/claude-code/mcp.json:\n  1. Check that JIRA_PERSONAL_TOKEN is correct and not expired\n  2. Verify JIRA_USERNAME matches your JIRA account\n  3. Ensure JIRA_URL is correct (e.g., https://issues.redhat.com)\n  4. Test authentication: curl -H \"Authorization: Bearer YOUR_TOKEN\" YOUR_JIRA_URL/rest/api/2/myself\n\n  To regenerate your token, visit:\n  https://issues.redhat.com/secure/ViewProfile.jspa?selectedTab=com.atlassian.pats.pats-plugin:jira-user-personal-access-tokens\n  ```\n- **Invalid project key**: Display error with example format (e.g., \"OCPBUGS\", \"JIRA\", \"HYPE\")\n- **No tickets found**:\n  - Explain why (e.g., all tickets have assignees and recent activity)\n  - Suggest relaxing search criteria (lower --days-inactive threshold)\n  - Suggest trying different project or removing assignee filter\n- **curl errors**: Check exit code and display helpful error message\n- **jq not found**: Inform user to install jq (`brew install jq` on macOS, `apt-get install jq` on Linux)\n- **Rate limiting**: If API returns 429, implement exponential backoff (wait 60s, retry)\n\n**Performance Considerations:**\n- **Large batch size:** Fetch 1000 tickets per request (JIRA's maximum) for efficiency\n- **Direct file storage:** Stream responses directly to disk, avoid loading into memory\n- **No token consumption:** Using curl avoids Claude's context/token limits\n- **Parallel-safe:** Can process very large backlogs (10,000+ tickets) without issues\n- **Field filtering:** Only request needed fields to minimize response size\n- **Python processing:** All filtering/analysis happens in Python, not in Claude context\n- **Minimal Claude interaction:** Only present final filtered/analyzed results to user\n\n## Return Value\n\n- **Console Output**: Formatted report showing suggested tickets organized by priority\n- **Intermediate Files** (created during processing):\n  - `.work/jira-backlog/{project-key}/batch-*.json` - Raw JIRA API responses (one per 1000 tickets)\n  - `.work/jira-backlog/{project-key}/process_batches.py` - Python script for filtering\n  - `.work/jira-backlog/{project-key}/filtered.json` - All filtered tickets grouped by priority\n  - `.work/jira-backlog/{project-key}/stats.json` - Priority distribution statistics\n- **Optional Final Report**: `.work/jira-backlog/{project-key}-{timestamp}.md` containing the full report\n- **Summary Statistics**: Count of available tickets per priority level\n\n## Examples\n\n**Note:** All examples require JIRA credentials to be configured in `~/.config/claude-code/mcp.json` (see Prerequisites section). The command uses curl to fetch data directly from JIRA's REST API, bypassing MCP commands to avoid 413 errors with large datasets.\n\n1. **Find unassigned tickets in OCPBUGS project**:\n   ```bash\n   /jira:backlog OCPBUGS\n   ```\n   Output: Intelligently analyzes tickets and shows 2 recommended tickets from each priority level (Critical, High, Normal, Low)\n\n   Example performance (tested October 31, 2025):\n   - Fetched 2,535 tickets in 3 batches\n   - Found 817 available tickets (unassigned or bot-assigned)\n   - No 413 errors or token limit issues\n\n2. **Find stale tickets with custom inactivity threshold**:\n   ```bash\n   /jira:backlog OCPBUGS --days-inactive 14\n   ```\n   Output: Report showing tickets with no activity for 14+ days, 2 per priority level\n\n3. **Find tickets assigned to a specific user that are stale**:\n   ```bash\n   /jira:backlog OCPBUGS --assignee jsmith --days-inactive 30\n   ```\n   Output: Report showing tickets assigned to jsmith with 30+ days of inactivity, 2 per priority level\n\n4. **Find tickets in Hypershift project**:\n   ```bash\n   /jira:backlog HYPE\n   ```\n   Output: Analyzes backlog and shows best 2 tickets from each priority in HYPE project\n\n5. **Find tickets in CNV project**:\n   ```bash\n   /jira:backlog CNV\n   ```\n   Output: Report showing available backlog tickets across all priorities in CNV project\n\n**Performance Note:** The curl-based approach can handle large backlogs efficiently. In testing with OCPBUGS (2,535 tickets), the command successfully fetched and processed all tickets without hitting Claude's token limits or encountering 413 errors.\n\n## Arguments\n\n- **project-key** (required): JIRA project key to search (e.g., OCPBUGS, JIRA, HYPE, CNV)\n  - Must be uppercase\n  - May contain hyphens (e.g., \"MY-PROJECT\")\n  - If not provided, will prompt user to specify\n- `--assignee` (optional): Filter by assignee username\n  - Default: Search for unassigned tickets (assignee IS EMPTY)\n  - If username provided: Find tickets assigned to that user with stale activity\n  - Example: `--assignee jsmith` finds jsmith's stale tickets\n- `--days-inactive` (optional): Number of days of inactivity to consider a ticket stale\n  - Default: 28 days\n  - Lower values find more recently inactive tickets\n  - Example: `--days-inactive 14` finds tickets with 14+ days of no activity"
              },
              {
                "name": "/categorize-activity-type",
                "description": "Categorize JIRA tickets into activity types using AI",
                "path": "plugins/jira/commands/categorize-activity-type.md",
                "frontmatter": {
                  "description": "Categorize JIRA tickets into activity types using AI",
                  "argument-hint": "<issue-key> [--auto-apply]"
                },
                "content": "## Name\njira:categorize-activity-type\n\n## Synopsis\n```bash\n/jira:categorize-activity-type <issue-key> [--auto-apply]\n```\n\n## Description\n\nAnalyzes JIRA tickets and assigns appropriate Activity Type categories based on ticket content, issue type, labels, and parent Epic context. Uses AI-powered categorization with confidence scoring to ensure accurate assignments.\n\nThe command supports six activity type categories:\n1. **Associate Wellness & Development** - Professional growth, training, learning, team building\n2. **Incidents & Support** - Production incidents, customer support, troubleshooting, emergency fixes\n3. **Security & Compliance** - Security vulnerabilities, compliance requirements, security patches, audits\n4. **Quality / Stability / Reliability** - Bug fixes, test improvements, reliability enhancements, technical debt\n5. **Future Sustainability** - Infrastructure improvements, developer experience, automation, tooling\n6. **Product / Portfolio Work** - Feature development, product enhancements, new capabilities\n\n## Implementation\n\n### Phase 1: Fetch Ticket Data\n\nUse MCP to fetch only the fields needed for categorization:\n\n```python\nissue_data = mcp__atlassian__jira_get_issue(\n    issue_key=\"${1}\",\n    fields=\"summary,description,issuetype,labels,parent,components,priority,customfield_12320040\"\n)\n```\n\nExtract relevant fields:\n- `summary` - Ticket title\n- `description` - Detailed ticket description\n- `issuetype.name` - Issue Type (Bug, Story, Task, Vulnerability, etc.)\n- `labels` - Ticket labels\n- `parent.key` - Parent Epic/Story key (if available)\n- `components` - Component assignments\n- `priority` - Priority level\n- `customfield_12320040` - Current Activity Type value (if set)\n\n### Phase 2: Invoke Categorization Skill\n\nDelegate categorization analysis to the `categorize-activity-type` skill which implements:\n\n1. **Issue Type Heuristics** - Apply default mappings:\n   - Vulnerability/Weakness  Security & Compliance\n   - Bug (security-related)  Security & Compliance\n   - Bug (standard)  Quality / Stability / Reliability\n   - Story (product)  Product / Portfolio Work\n   - Task  Analyze parent context or keywords\n\n2. **Keyword Scanning** - Search for indicator words:\n   - Incidents: \"incident\", \"outage\", \"customer issue\", \"emergency\", \"hotfix\"\n   - Security: \"CVE\", \"vulnerability\", \"security patch\", \"compliance\", \"audit\"\n   - Quality: \"bug\", \"flaky test\", \"memory leak\", \"crash\", \"error handling\"\n   - Sustainability: \"refactor\", \"technical debt\", \"developer experience\", \"CI/CD\", \"automation\"\n   - Product: \"feature\", \"enhancement\", \"capability\", \"user story\", \"requirement\"\n   - Wellness: \"training\", \"learning\", \"conference\", \"onboarding\", \"mentoring\"\n\n3. **Parent Context Inheritance** - When ticket is child of Epic:\n   - Fetch parent Epic details\n   - Check parent's Activity Type (if set)\n   - Inherit category with adjusted confidence\n\n4. **Ambiguity Resolution** - Apply priority rules:\n   - Security-related always takes precedence\n   - Explicit keywords override issue type heuristics\n   - Parent inheritance used when keywords unclear\n   - Low confidence reported when evidence conflicts\n\nSee [skills/categorize-activity-type/SKILL.md](../skills/categorize-activity-type/SKILL.md) for detailed categorization methodology.\n\n### Phase 3: Present Results\n\nDisplay categorization analysis to user:\n\n```text\nActivity Type: Quality / Stability / Reliability\nConfidence: High\n\nReasoning: This is a Bug issue type focused on fixing a memory leak in the scanner\ncomponent. Memory leaks directly impact system stability and reliability. The description\nmentions \"intermittent crashes\" and \"resource exhaustion,\" which are classic reliability\nconcerns. No security implications mentioned, and this is a proactive fix rather than a\ncustomer-facing incident.\n\nKey Evidence:\n- Issue Type: Bug\n- Summary contains: \"Fix memory leak\"\n- Description mentions: \"crashes\", \"resource exhaustion\", \"intermittent failures\"\n- No security keywords present\n- No parent Epic context available\n```\n\n### Phase 4: Apply Update (Conditional)\n\n**Auto-apply logic:**\n\n- If `--auto-apply` flag present AND confidence is **High**:\n  - Automatically update Activity Type field\n  - Display confirmation to user\n\n- Otherwise (no flag OR confidence is Medium/Low):\n  - Present suggestion to user\n  - Ask for confirmation before applying\n  - If user confirms, proceed with update\n  - If user declines, exit without changes\n\n**Update using MCP:**\n\n```python\nmcp__atlassian__jira_update_issue(\n    issue_key=\"${1}\",\n    fields={\n        \"customfield_12320040\": {  # Activity Type field\n            \"value\": \"<SELECTED_ACTIVITY_TYPE>\"\n        }\n    }\n)\n```\n\n**Success confirmation:**\n\n```text\n Updated ROX-12345: Activity Type set to \"Quality / Stability / Reliability\"\n  View at: https://issues.redhat.com/browse/ROX-12345\n```\n\n### Phase 5: Error Handling\n\n**Handle common errors:**\n\n- **Issue not found**: Display error and suggest verifying issue key\n- **Permission denied**: Inform user they lack update permissions\n- **Invalid field value**: Verify Activity Type value matches allowed options\n- **MCP connection error**: Suggest checking MCP server configuration\n\n## Arguments\n\n- **$1 - issue-key** (required)\n  - JIRA issue key to categorize\n  - Format: PROJECT-NUMBER (e.g., ROX-12345, OCPBUGS-67890)\n  - Must be a valid, accessible JIRA issue\n\n- **--auto-apply** (optional)\n  - Automatically apply Activity Type when confidence is High\n  - Without this flag, always prompts user for confirmation\n  - Only applies for High confidence categorizations\n  - Medium/Low confidence always requires manual confirmation\n\n## Return Value\n\n**Format:** Human-readable categorization analysis with optional JIRA update\n\n**Components:**\n1. **Selected Activity Type** - One of the six categories\n2. **Confidence Level** - High, Medium, or Low\n3. **Reasoning** - 2-3 sentences explaining the categorization with specific evidence\n4. **Key Evidence** - Bullet points of relevant data from the ticket\n5. **Update Confirmation** - Success message if Activity Type was applied\n\n**Exit codes:**\n- `0` - Success (with or without update)\n- `1` - Error (invalid issue key, permission denied, MCP error, etc.)\n\n## Examples\n\n1. **Basic categorization (manual confirmation):**\n   ```bash\n   /jira:categorize-activity-type ROX-12345\n   ```\n\n   Result: Displays categorization analysis, prompts user to confirm before updating\n\n2. **Auto-apply for high confidence:**\n   ```bash\n   /jira:categorize-activity-type ROX-12345 --auto-apply\n   ```\n\n   Result: If confidence is High, automatically updates Activity Type without prompting\n\n3. **Process security vulnerability:**\n   ```bash\n   /jira:categorize-activity-type ROX-28072 --auto-apply\n   ```\n\n   Expected: Issue Type = Vulnerability  \"Security & Compliance\" (High confidence, auto-applied)\n\n4. **Process bug with unclear context:**\n   ```bash\n   /jira:categorize-activity-type OCPBUGS-45678\n   ```\n\n   Expected: Analyzes keywords and parent Epic, may show Medium confidence, asks for confirmation\n\n## See Also\n\n- `jira:grooming` - Backlog grooming with activity type analysis\n- `jira:create` - Create issues with pre-assigned activity types\n- `jira:validate-blockers` - Validate release blockers with scoring\n\n## Notes\n\n- **Activity Type field ID**: `customfield_12320040` - **This is specific to Red Hat JIRA instances**. Other JIRA instances will have different custom field IDs for Activity Type. To find your instance's field ID:\n  1. Use MCP to fetch any issue: `mcp__atlassian__jira_get_issue(issue_key=\"YOUR-ISSUE-KEY\")`\n  2. Search the response for \"Activity Type\" or your custom field name\n  3. Note the field ID (e.g., `customfield_12345`) associated with that field\n  4. Update the command implementation and skill to use your field ID instead of `customfield_12320040`\n- **Requires MCP**: Jira MCP server must be configured (see [plugin README](../README.md))\n- **Read and write permissions**: User must have permission to view and edit the specified JIRA issue\n- **AI-powered**: Categorization uses LLM reasoning, not simple rule matching\n- **Confidence scoring**: High confidence triggers auto-apply (with flag), Medium/Low always prompts"
              },
              {
                "name": "/create-release-note",
                "description": "Generate bug fix release notes from Jira tickets and linked GitHub PRs",
                "path": "plugins/jira/commands/create-release-note.md",
                "frontmatter": {
                  "description": "Generate bug fix release notes from Jira tickets and linked GitHub PRs",
                  "argument-hint": "<issue-key>"
                },
                "content": "## Name\njira:create-release-note\n\n## Synopsis\n```\n/jira:create-release-note <issue-key>\n```\n\n## Description\n\nThe `jira:create-release-note` command automatically generates bug fix release notes by analyzing Jira bug tickets and their linked GitHub pull requests, then updates the Jira ticket with the generated release note content.\n\nThis command is particularly useful for:\n- Creating consistent, well-formatted release notes across all bugs\n- Automatically extracting information from multiple sources (Jira + GitHub)\n- Saving time by analyzing PR code changes, commits, and descriptions\n- Ensuring complete release notes with Cause, Consequence, Fix, Result, and Workaround\n\nThe command follows the standard release note template format and populates both the Release Note Type and Release Note Text fields in Jira.\n\n## Implementation\n\nThe `jira:create-release-note` command runs in multiple phases:\n\n###  Phase 1: Fetch and Validate Jira Bug\n\n1. **Fetch bug ticket** using `mcp__atlassian__jira_get_issue` MCP tool:\n   - Request all fields to ensure we have complete data\n   - Verify the issue is a Bug type\n   - Extract issue description, links, and custom fields\n\n2. **Validate issue type**:\n   - If not a Bug, warn user and ask if they want to continue\n   - Release notes are typically for bugs, but may apply to other types\n\n3. **Parse bug description** to extract required sections:\n   - **Cause**: The root cause of the problem\n   - **Consequence**: The impact or effect of the problem\n\n4. **Handle missing sections**:\n   - If Cause or Consequence sections are missing, inform the user\n   - Provide template format and ask user to update the bug description\n   - Optionally, allow user to provide Cause/Consequence interactively\n\n###  Phase 2: Extract Linked GitHub PRs\n\nExtract all linked GitHub PR URLs from multiple sources:\n\n1. **Remote links** (Primary source - web links in Jira):\n   - Check the Jira issue response for web links/remote links\n   - Common field names: `remotelinks`, `issuelinks` with `outwardIssue.fields.issuetype.name == \"GitHub PR\"`\n   - Look for GitHub PR URLs in remote link objects\n   - Pattern: `https://github.com/{org}/{repo}/pull/{number}`\n   - Extract PR URLs and parse into `{org}/{repo}` and `{number}`\n\n2. **Description text**: Scan bug description for GitHub PR URLs\n   - Use regex pattern to find PR URLs: `https://github\\.com/[\\w-]+/[\\w-]+/pull/\\d+`\n   - Extract and parse all matches\n   - **IMPORTANT**: Do NOT use `gh issue view {JIRA-KEY}` - Jira keys are not GitHub issues\n\n3. **Comments**: Scan bug comments for GitHub PR URLs\n   - Iterate through comments\n   - Extract PR URLs using same regex pattern\n   - **IMPORTANT**: Only look for full GitHub PR URLs, not issue references\n\n4. **Deduplicate**: Create unique set of PR URLs\n\n5. **Search by bug number** (Fallback if no PR URLs found):\n   - If no PRs found via links, search GitHub for PRs mentioning the bug\n   - **For OCPBUGS**: Search common repos (openshift/hypershift, openshift/cluster-api-provider-*):\n     ```bash\n     # Try common OpenShift repos\n     for repo in \"openshift/hypershift\" \"openshift/cluster-api-provider-aws\" \"openshift/origin\"; do\n       gh pr list --repo $repo --search \"{issue-key} in:title,body\" --state all --limit 10 --json number,url,title\n     done\n     ```\n   - Ask user to confirm which PRs are relevant\n   - **IMPORTANT**: Never use `gh issue view {JIRA-KEY}` - this will fail because Jira keys are not GitHub issue numbers\n\n6. **Validate**: Ensure at least one PR is found\n   - If no PRs found after all attempts, show error: \"No GitHub PRs found linked to {issue-key}. Please link at least one PR to generate release notes.\"\n   - Provide instructions on how to link PRs in Jira\n\n###  Phase 3: Analyze Each GitHub PR\n\nFor each linked PR, analyze multiple sources to extract Fix, Result, and Workaround information:\n\n1. **Extract repository and PR number** from URL:\n   - Parse: `https://github.com/{org}/{repo}/pull/{number}`\n   - Store: `{org}/{repo}` as `REPO`, `{number}` as `PR_NUMBER`\n\n2. **Fetch PR details** using `gh` CLI:\n   ```bash\n   gh pr view {PR_NUMBER} --json body,title,commits,url,state --repo {REPO}\n   ```\n   - Extract: title, body/description, commits array, state\n   - Handle errors: If PR is inaccessible, log warning and skip\n\n3. **Fetch PR diff** using `gh` CLI:\n   ```bash\n   gh pr diff {PR_NUMBER} --repo {REPO}\n   ```\n   - Analyze code changes to understand what was fixed\n   - Look for key changes in error handling, validation, etc.\n\n4. **Fetch PR comments** using `gh` CLI:\n   ```bash\n   gh pr view {PR_NUMBER} --json comments --repo {REPO}\n   ```\n   - Extract reviewer comments and author responses\n   - Look for clarifications about the fix or workarounds\n\n5. **Analyze all sources**:\n   - **PR Title**: Often summarizes the fix\n   - **PR Body/Description**: Usually contains detailed explanation\n   - **Commit Messages**: May provide step-by-step implementation details\n   - **Code Changes**: Shows exactly what was modified\n   - **PR Comments**: May contain clarifications or additional context\n\n6. **Extract key information**:\n   - **Fix**: What code/configuration changes were made to address the problem\n   - **Result**: What behavior changed after the fix\n   - **Workaround**: If mentioned, any temporary solutions before the fix\n\n###  Phase 4: Synthesize Release Note Content\n\nCombine information from all analyzed PRs into a cohesive release note:\n\n1. **Combine Cause/Consequence** from Jira bug:\n   - Use the extracted Cause and Consequence sections\n   - Clean up formatting (remove Jira markup if needed)\n\n2. **Synthesize Fix** from all PRs:\n   - If single PR: Use the PR's fix description\n   - If multiple PRs: Combine into coherent narrative\n   - Focus on \"what was changed\" rather than \"how it was coded\"\n   - Keep it concise but complete\n\n3. **Synthesize Result** from all PRs:\n   - Describe the outcome after the fix is applied\n   - Focus on user-visible changes\n   - Example: \"The control plane operator no longer crashes when...\"\n\n4. **Extract Workaround** (if applicable):\n   - Check if PRs mention temporary solutions\n   - Only include if a workaround was documented\n   - Omit this section if no workaround exists\n\n5. **Format according to template**:\n   ```\n   Cause: <extracted from bug description>\n   Consequence: <extracted from bug description>\n   Fix: <synthesized from PR analysis>\n   Result: <synthesized from PR analysis>\n   Workaround: <synthesized from PR analysis if applicable>\n   ```\n\n###  Phase 5: Security Validation\n\nScan the generated release note text for sensitive data before submission:\n\n1. **Prohibited content patterns**:\n   - Credentials: Passwords, API tokens, access keys\n   - Cloud keys: AWS access keys (AKIA...), GCP service accounts, Azure credentials\n   - Kubeconfigs: Cluster credentials, service account tokens\n   - SSH keys: Private keys, authorized_keys content\n   - Certificates: PEM files, private key content\n   - URLs with credentials: `https://user:pass@example.com`\n\n2. **Scanning approach**:\n   - Use regex patterns to detect common credential formats\n   - Check for base64-encoded secrets\n   - Look for common secret prefixes (sk_, ghp_, etc.)\n\n3. **Action if detected**:\n   - STOP release note creation immediately\n   - Inform user what type of data was detected (without showing it)\n   - Example: \"Detected what appears to be an API token in the release note text.\"\n   - Ask user to review PR content and redact sensitive information\n   - Provide guidance on safe alternatives (use placeholders like `<redacted>`, `YOUR_API_KEY`, etc.)\n\n###  Phase 6: Select Release Note Type\n\nPrompt user to select the appropriate Release Note Type:\n\n1. **Available options** (from Jira dropdown):\n   - Bug Fix (most common for OCPBUGS)\n   - Release Note Not Required\n   - Known Issue\n   - Enhancement\n   - Rebase\n   - Technology Preview\n   - Deprecated Functionality\n   - CVE\n\n2. **Auto-detection** (optional):\n   - For OCPBUGS: Default to \"Bug Fix\"\n   - Check PR titles/descriptions for keywords\n   - Suggest type based on content analysis\n\n3. **User confirmation**:\n   - Show suggested type\n   - Allow user to override\n   - Use `AskUserQuestion` tool for interactive selection\n\n###  Phase 7: Update Jira Ticket\n\nUpdate the Jira bug ticket with generated release note:\n\n1. **Prepare fields** for update:\n   ```\n   {\n     \"customfield_12320850\": {\"value\": \"<Release Note Type>\"},\n     \"customfield_12317313\": \"<Release Note Text>\"\n   }\n   ```\n\n2. **Update using MCP tool**:\n   ```\n   mcp__atlassian__jira_update_issue(\n     issue_key=<issue-key>,\n     fields={\n       \"customfield_12320850\": {\"value\": \"Bug Fix\"},\n       \"customfield_12317313\": \"<formatted release note text>\"\n     }\n   )\n   ```\n\n3. **Handle update errors**:\n   - Permission denied: User may not have rights to update these fields\n   - Invalid field value: Release Note Type value not in allowed list\n   - Field not found: Custom field IDs may be different in different Jira instances\n\n###  Phase 8: Display Results\n\nShow the user what was created:\n\n1. **Display generated release note**:\n   ```\n   Release Note Created for {issue-key}\n\n   Type: Bug Fix\n\n   Text:\n   ---\n   Cause: ...\n   Consequence: ...\n   Fix: ...\n   Result: ...\n   Workaround: ...\n   ---\n\n   Updated: https://issues.redhat.com/browse/{issue-key}\n   ```\n\n2. **Provide next steps**:\n   - Link to the updated Jira ticket\n   - Suggest reviewing the release note in Jira\n   - Mention that the release note can be edited manually if needed\n\n## Arguments\n\n- **$1  issue-key** *(required)*\n  Jira issue key for the bug (e.g., `OCPBUGS-12345`).\n  Must be a valid bug ticket with linked GitHub PRs.\n\n## Return Value\n\n- **Issue Key**: The Jira issue that was updated\n- **Release Note Type**: The selected release note type\n- **Release Note Text**: The generated release note content\n- **Issue URL**: Direct link to the updated Jira ticket\n\n## Examples\n\n### Basic Usage\n\nCreate release note for a bug with linked PRs:\n```\n/jira:create-release-note OCPBUGS-38358\n```\n\nThe command will:\n1. Fetch the bug from Jira\n2. Extract Cause and Consequence from the description\n3. Find and analyze all linked GitHub PRs\n4. Generate the release note\n5. Prompt for Release Note Type selection\n6. Update the Jira ticket\n7. Display the results\n\n### Example Output\n\n```\nAnalyzing OCPBUGS-38358...\n\nFound bug: \"hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\"\n\nExtracted from bug description:\n  Cause: hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\n  Consequence: control-plane-operator enters a crash loop\n\nFound 1 linked GitHub PR:\n  - https://github.com/openshift/hypershift/pull/4567\n\nAnalyzing PR #4567...\n  Title: \"Fix panic when CloudProviderConfig.Subnet is not specified\"\n  Commits: 2\n  Files changed: 3\n\nSynthesizing release note...\n\nSelect Release Note Type:\n1. Bug Fix\n2. Release Note Not Required\n3. Known Issue\n4. Enhancement\n5. CVE\n\nSelection: 1 (Bug Fix)\n\nUpdating Jira ticket...\n\n Release Note Created for OCPBUGS-38358\n\nType: Bug Fix\n\nText:\n---\nCause: hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\nConsequence: control-plane-operator enters a crash loop\nFix: Added nil check for CloudProviderConfig.Subnet before accessing Subnet.ID field\nResult: The control-plane-operator no longer crashes when CloudProviderConfig.Subnet is not specified\n---\n\nUpdated: https://issues.redhat.com/browse/OCPBUGS-38358\n```\n\n## Error Handling\n\n### No GitHub PRs Linked\n\n**Scenario**: Bug ticket has no linked GitHub PRs.\n\n**Error Message**:\n```\nNo GitHub PRs found linked to OCPBUGS-12345.\n\nTo generate a release note, please link at least one GitHub PR to this bug.\n\nHow to link PRs:\n1. Edit the bug in Jira\n2. Add a web link to the GitHub PR URL\n3. Or mention the PR URL in a comment\n4. Then run this command again\n```\n\n**Action**: Exit without updating the ticket.\n\n### PR Not Accessible\n\n**Scenario**: One or more linked PRs cannot be accessed.\n\n**Warning Message**:\n```\nWarning: Unable to access PR https://github.com/org/repo/pull/123\nVerify the PR exists and you have permissions.\n\nContinuing with remaining PRs...\n```\n\n**Action**: Skip the inaccessible PR, continue with others. If all PRs are inaccessible, treat as \"No PRs\" error.\n\n### Missing Cause or Consequence\n\n**Scenario**: Bug description doesn't contain required Cause and/or Consequence sections.\n\n**Error Message**:\n```\nBug description is missing required sections:\n  - Missing: Cause\n  - Missing: Consequence\n\nPlease update the bug description to include these sections.\n\nTemplate format:\n---\nDescription of problem:\n<problem description>\n\nCause:\n<root cause of the problem>\n\nConsequence:\n<impact or effect of the problem>\n\nSteps to Reproduce:\n1. ...\n---\n\nWould you like to provide Cause and Consequence interactively? (yes/no)\n```\n\n**Action**:\n- If user says yes: Prompt for Cause and Consequence\n- If user says no: Exit and ask them to update the bug\n\n### Security Validation Failure\n\n**Scenario**: Generated release note contains potential credentials or secrets.\n\n**Error Message**:\n```\nSecurity validation failed!\n\nDetected what appears to be an API token in the release note text.\n\nThis may have come from:\n- PR description\n- Commit messages\n- Code changes\n- PR comments\n\nPlease review the source PRs and remove any credentials before proceeding.\n\nUse placeholder values instead:\n- YOUR_API_KEY\n- <redacted>\n- ********\n\nAborting release note creation.\n```\n\n**Action**: Stop immediately, do not update Jira ticket.\n\n### Update Permission Denied\n\n**Scenario**: User doesn't have permission to update Release Note fields.\n\n**Error Message**:\n```\nFailed to update OCPBUGS-12345.\n\nError: You do not have permission to edit field 'Release Note Type'\n\nThis may require specific Jira permissions. Please contact your Jira administrator or use the Jira web UI to add the release note manually.\n\nGenerated release note (for manual entry):\n---\nCause: ...\nConsequence: ...\n...\n---\n```\n\n**Action**: Display the generated release note so user can manually copy it.\n\n### Invalid Release Note Type\n\n**Scenario**: Selected release note type is not valid for this Jira instance.\n\n**Error Message**:\n```\nFailed to update Release Note Type field.\n\nError: Value \"Bug Fix\" is not valid for field customfield_12320850\n\nThis may indicate a Jira configuration issue. Please verify the allowed values for Release Note Type in your Jira instance.\n```\n\n**Action**: Ask user to select a different type or manually update in Jira.\n\n### Multiple PRs with Conflicting Information\n\n**Scenario**: Multiple PRs describe different fixes or contradictory information.\n\n**Warning Message**:\n```\nFound 3 linked PRs with different fix descriptions:\n- PR #123: Fix A\n- PR #456: Fix B\n- PR #789: Fix C\n\nCombining all fixes into a single release note...\n```\n\n**Action**: Use AI to synthesize a coherent narrative combining all fixes. If truly contradictory, ask user for clarification.\n\n## Best Practices\n\n1. **Link PRs early**: Add GitHub PR links to bugs as soon as PRs are created\n2. **Use structured bug descriptions**: Always include Cause and Consequence sections\n3. **Review before submission**: Check the generated release note before confirming\n4. **Sanitize PR content**: Don't include credentials in PR descriptions or commits\n5. **One release note per bug**: Don't run this command multiple times for the same bug\n6. **Update if needed**: Release notes can be manually edited in Jira after creation\n\n## Prerequisites\n\n### Required Tools\n\n1. **MCP Jira Server**: Must be configured and accessible\n   - See [Jira Plugin README](../README.md) for setup instructions\n   - Requires read/write permissions for bugs\n\n2. **GitHub CLI (`gh`)**: Must be installed and authenticated\n   - Install: `brew install gh` (macOS) or see [GitHub CLI docs](https://cli.github.com/)\n   - Authenticate: `gh auth login`\n   - Verify: `gh auth status`\n\n3. **Access to GitHub Repositories**: Must have read access to repos where PRs are located\n   - PRs in private repos require appropriate GitHub permissions\n   - Public repos should work without additional configuration\n\n### Required Permissions\n\n1. **Jira Permissions**:\n   - Read access to bug tickets\n   - Write access to Release Note Type field (customfield_12320850)\n   - Write access to Release Note Text field (customfield_12317313)\n\n2. **GitHub Permissions**:\n   - Read access to pull requests\n   - Read access to repository diffs and commits\n\n## See Also\n\n- `jira:solve` - Analyze and solve Jira issues\n- `jira:create` - Create Jira issues with guided workflows\n- `jira:generate-test-plan` - Generate test plans for PRs\n- `jira:status-rollup` - Create status rollup reports\n\n## Technical Notes\n\n### Jira Custom Fields\n\nThe command uses these Jira custom field IDs:\n- `customfield_12320850`: Release Note Type (dropdown)\n- `customfield_12317313`: Release Note Text (text field)\n\nThese field IDs are specific to Red Hat's Jira instance. If using a different Jira instance, you may need to update the field IDs.\n\n### GitHub CLI Rate Limits\n\nThe `gh` CLI is subject to GitHub API rate limits:\n- Authenticated: 5,000 requests per hour\n- This command typically uses 3-4 requests per PR (view, diff, comments)\n- For bugs with many linked PRs, be aware of rate limits\n\n### Release Note Template\n\nThe release note follows this structure:\n- **Cause**: Root cause (from Jira)\n- **Consequence**: Impact (from Jira)\n- **Fix**: What was changed (from PRs)\n- **Result**: Outcome after fix (from PRs)\n- **Workaround**: Temporary solution (from PRs, optional)\n\nThis format is standard for Red Hat bug fix release notes."
              },
              {
                "name": "/create",
                "description": "Create Jira issues (story, epic, feature, task, bug, feature-request) with proper formatting",
                "path": "plugins/jira/commands/create.md",
                "frontmatter": {
                  "description": "Create Jira issues (story, epic, feature, task, bug, feature-request) with proper formatting",
                  "argument-hint": "<type> [project-key] <summary> [--component <name>] [--version <version>] [--parent <key>]"
                },
                "content": "## Name\njira:create\n\n## Synopsis\n```\n/jira:create <type> [project-key] <summary> [options]\n```\n\n## Description\nThe `jira:create` command creates Jira issues following best practices and team-specific conventions. It supports creating stories, epics, features, tasks, bugs, and feature requests with intelligent defaults, interactive prompts, and validation.\n\nThis command is particularly useful for:\n- Creating well-formed user stories with acceptance criteria\n- Organizing epics and features with proper hierarchy\n- Submitting bugs with complete reproduction steps\n- Capturing customer-driven feature requests with business justification\n- Maintaining consistency across team Jira practices\n\n## Key Features\n\n- **Multi-Type Support** - Create stories, epics, features, tasks, bugs, or feature requests from a single command\n- **Smart Defaults** - Automatically applies project-specific conventions (e.g., CNTRLPLANE, OCPBUGS, RFE)\n- **Interactive Guidance** - Prompts for missing information with helpful templates\n- **Context Detection** - Analyzes summary text to suggest components (ARO, ROSA, HyperShift)\n- **Security Validation** - Scans for credentials and secrets before submission\n- **Template Support** - Provides user story templates, bug report templates, feature request workflows, acceptance criteria formats\n\n## Issue Hierarchy and Parent Linking\n\nJira issues form a hierarchy. Understanding this hierarchy is critical for proper parent linking:\n\n```\nFeature (Strategic objective, market problem)\n    \n     Epic (Body of work, fits in a quarter)\n            \n             Story (User-facing functionality, fits in a sprint)\n            \n             Task (Technical work, fits in a sprint)\n```\n\n### Parent Linking Field Reference\n\n**CRITICAL:** Different relationships use different Jira fields. Using the wrong field will cause creation to fail.\n\n| Relationship | Field | MCP Parameter | Value Format |\n|--------------|-------|---------------|--------------|\n| **Epic  Feature** | Parent Link (custom field) | `additional_fields.customfield_12313140` | `\"PROJ-123\"` (string) |\n| **Story  Epic** | Epic Link (custom field) | `additional_fields.customfield_12311140` | `\"PROJ-123\"` (string) |\n| **Task  Epic** | Epic Link (custom field) | `additional_fields.customfield_12311140` | `\"PROJ-123\"` (string) |\n| **Task  Story** | Epic Link (custom field) | `additional_fields.customfield_12311140` | `\"PROJ-123\"` (string) |\n\n**Why the difference?**\n- The Parent Link field (`customfield_12313140`) is used for EpicFeature relationships in CNTRLPLANE\n- The Epic Link field (`customfield_12311140`) is used for Story/TaskEpic relationships\n- Both are custom fields specific to how Red Hat Jira handles hierarchy\n- The standard `parent` field does NOT work for these relationships\n\n### MCP Code Examples for Parent Linking\n\n#### Linking a Story to an Epic\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Add metrics endpoint for cluster health\",\n    issue_type=\"Story\",\n    description=\"<story description>\",\n    components=\"HyperShift / ROSA\",\n    additional_fields={\n        \"customfield_12311140\": \"CNTRLPLANE-456\",  # Epic Link - links to parent epic\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n#### Linking an Epic to a Feature\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Multi-cluster metrics aggregation\",\n    issue_type=\"Epic\",\n    description=\"<epic description>\",\n    components=\"HyperShift\",\n    additional_fields={\n        \"customfield_12311141\": \"Multi-cluster metrics aggregation\",  # Epic Name (same as summary)\n        \"customfield_12313140\": \"CNTRLPLANE-100\",  # Parent Link - links to parent feature (STRING, not object!)\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n#### Linking a Task to an Epic\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Refactor metrics collection pipeline\",\n    issue_type=\"Task\",\n    description=\"<task description>\",\n    additional_fields={\n        \"customfield_12311140\": \"CNTRLPLANE-456\",  # Epic Link - links to parent epic\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### Parent Linking Implementation Strategy\n\nWhen the `--parent` flag is provided, follow this strategy:\n\n#### Step 1: Pre-Validation (Required)\n\nBefore creating the issue, validate the parent:\n\n```python\n# Fetch parent issue to verify it exists and is correct type\nparent_issue = mcp__atlassian__jira_get_issue(issue_key=\"<parent-key>\")\n\n# Verify parent type matches expected hierarchy:\n# - If creating Story/Task with --parent, parent should be Epic\n# - If creating Epic with --parent, parent should be Feature\n```\n\n**Validation rules:**\n| Creating | Parent Should Be | If Wrong Type |\n|----------|------------------|---------------|\n| Story | Epic | Warn user, ask to confirm or correct |\n| Task | Epic or Story | Warn user, ask to confirm or correct |\n| Epic | Feature | Warn user, ask to confirm or correct |\n\n**If parent not found:**\n```\nParent issue CNTRLPLANE-999 not found.\n\nOptions:\n1. Proceed without parent link\n2. Specify different parent\n3. Cancel creation\n\nWhat would you like to do?\n```\n\n#### Step 2: Attempt Creation with Parent Link\n\nInclude the appropriate parent field based on issue type:\n\n- **Story/Task  Epic:** Use `customfield_12311140` (Epic Link)\n- **Epic  Feature:** Use `customfield_12313140` (Parent Link)\n\n#### Step 3: Fallback Strategy (If Creation Fails)\n\nIf creation fails with an error related to parent linking:\n\n1. **Detect linking error:** Error message contains \"epic\", \"parent\", \"link\", or \"customfield\"\n\n2. **Create without parent link:**\n   ```python\n   issue = mcp__atlassian__jira_create_issue(\n       # ... same parameters but WITHOUT the parent/epic link field\n   )\n   ```\n\n3. **Link via update:**\n   ```python\n   # For Story/Task  Epic:\n   mcp__atlassian__jira_update_issue(\n       issue_key=issue[\"key\"],\n       fields={},\n       additional_fields={\"customfield_12311140\": \"<epic-key>\"}\n   )\n\n   # For Epic  Feature:\n   mcp__atlassian__jira_update_issue(\n       issue_key=issue[\"key\"],\n       fields={},\n       additional_fields={\"customfield_12313140\": \"<feature-key>\"}\n   )\n   ```\n\n4. **Report outcome:**\n   ```\n   Created: CNTRLPLANE-789\n   Linked to parent: CNTRLPLANE-456 \n   Title: <issue title>\n   URL: https://issues.redhat.com/browse/CNTRLPLANE-789\n   ```\n\n#### Step 4: If Fallback Also Fails\n\nIf the update to add parent link also fails:\n```\nCreated: CNTRLPLANE-789\n  Automatic parent linking failed. Please link manually in Jira.\nURL: https://issues.redhat.com/browse/CNTRLPLANE-789\n```\n\n### Common Parent Linking Errors\n\n| Error Message | Cause | Solution |\n|---------------|-------|----------|\n| `Field 'parent' does not exist` | Using standard `parent` field | Use `customfield_12313140` (Parent Link) or `customfield_12311140` (Epic Link) |\n| `customfield_12311140 is not valid` | Epic Link field issue | Use fallback: create then update |\n| `customfield_12313140 is not valid` | Parent Link field issue | Use fallback: create then update |\n| `Parent issue not found` | Invalid parent key | Verify parent exists first |\n| `Cannot link to issue of type X` | Wrong parent type | Verify hierarchy (StoryEpic, EpicFeature) |\n\n## Implementation\n\nThe `jira:create` command runs in multiple phases:\n\n###  Phase 1: Load Implementation Guidance\n\nInvoke the appropriate skill based on issue type using the Skill tool:\n\n- **Type: `story`**  Invoke `jira:create-story` skill\n  - Loads user story template guidance\n  - Provides acceptance criteria formats\n  - Offers story quality validation\n\n- **Type: `epic`**  Invoke `jira:create-epic` skill\n  - Loads epic structure guidance\n  - Provides epic name field handling\n  - Offers parent feature linking workflow\n\n- **Type: `feature`**  Invoke `jira:create-feature` skill\n  - Loads strategic planning guidance\n  - Provides market problem framework\n  - Offers success criteria templates\n\n- **Type: `task`**  Invoke `jira:create-task` skill\n  - Loads technical task guidance\n  - Provides task vs story differentiation\n  - Offers acceptance criteria for technical work\n\n- **Type: `bug`**  Invoke `jira:create-bug` skill\n  - Loads bug report template\n  - Provides structured reproduction steps\n  - Offers severity and reproducibility guidance\n\n- **Type: `feature-request`**  Invoke `jira:create-feature-request` skill\n  - Loads customer-driven feature request guidance\n  - Provides 4-question workflow (title, description, business requirements, components)\n  - Offers component mapping from teams/operators\n  - Targets RFE project\n\n###  Phase 2: Apply Project-Specific Conventions\n\nInvoke project-specific and team-specific skills using the Skill tool as needed:\n\n**Project-specific skills:**\n- **CNTRLPLANE:** Invoke `cntrlplane` skill for CNTRLPLANE stories/epics/features/tasks\n- **OCPBUGS:** Invoke `ocpbugs` skill for OCPBUGS bugs\n- **Other projects:** Use only type-specific skills for best practices\n\n**Team-specific skills:**\n- Detected based on keywords in summary/description or component\n- Apply team-specific conventions (component selection, custom fields, workflows)\n- Layer on top of project-specific conventions\n- Example: HyperShift team  invoke `hypershift` skill\n\n**General projects** use only the type-specific skills (create-story, create-bug, etc.) for best practices.\n\n###  Phase 3: Parse Arguments & Detect Context\n\nParse command arguments:\n- **Required:** `type`, `summary`\n- **Optional:** `project_key` (may have project-specific defaults)\n- **Optional flags:** `--component`, `--version`, `--parent`\n\nAnalyze summary text for context clues:\n- Extract keywords that may indicate team, component, or platform\n- Pass context to project-specific and team-specific skills for interpretation\n- Skills handle keyword detection and component/field suggestions\n\n###  Phase 4: Apply Smart Defaults\n\n**Universal requirements (MUST be applied to ALL tickets):**\n- **Security level:** Red Hat Employee (required)\n- **Labels:** ai-generated-jira (required)\n\n**Project defaults:**\n- May include default project for certain issue types\n- Version defaults (if applicable)\n- Additional labels (for tracking or automation)\n\n**Team defaults:**\n- Component selection (based on keywords or prompts)\n- Custom field values\n- Workflow-specific requirements\n\n**General projects:**\n- Use type-specific skills for issue structure\n- Prompt for required fields as needed\n\n###  Phase 5: Interactive Prompts (Hybrid Approach)\n\nPrompt for missing required information based on issue type:\n\n**For Stories:**\n- Offer user story template: \"As a... I want... So that...\"\n- Collect acceptance criteria (suggest formats)\n- Confirm auto-detected component\n\n**For Epics:**\n- Collect epic objective and scope\n- Collect epic acceptance criteria\n- Collect timeline/target release\n- Set epic name field (same as summary)\n- Optional parent feature link (via `--parent` or prompt)\n\n**For Features:**\n- Collect market problem description\n- Collect strategic value and business impact\n- Collect success criteria (adoption, usage, outcomes, business)\n- Identify component epics (3-8 major work streams)\n- Collect timeline and milestones\n\n**For Tasks:**\n- Collect task description (what needs to be done)\n- Collect motivation/context (why it's needed)\n- Optionally collect acceptance criteria\n- Optionally collect technical details (files, approach)\n\n**For Bugs:**\n- Use bug template (interactive fill-in):\n  - Description of problem\n  - Version-Release number\n  - How reproducible (Always | Sometimes | Rarely)\n  - Steps to Reproduce (numbered list)\n  - Actual results (include error messages)\n  - Expected results (correct behavior)\n  - Additional info (logs, screenshots)\n\n**For Feature Requests:**\n- Use 4-question workflow:\n  1. Proposed title of feature request\n  2. Nature and description (current limitations, desired behavior, use case)\n  3. Business requirements (customer impact, regulatory drivers, justification)\n  4. Affected packages and components (teams, operators, component mapping)\n\n###  Phase 5.5: Summary Validation\n\nBefore security validation, validate the summary format to catch common mistakes:\n\n**Check for anti-patterns:**\n1. Summary starts with \"As a\" (user story format belongs in description)\n2. Summary contains \"I want\" or \"so that\" (belongs in description)\n3. Summary exceeds 100 characters (likely too long, may be full user story)\n\n**Action if anti-pattern detected:**\n1. Detect that user put full user story in summary field\n2. Extract the key action/feature from the summary\n3. Generate a concise alternative (5-10 words)\n4. Prompt user for confirmation:\n   ```\n   The summary looks like a full user story. Summaries should be concise titles.\n\n   Current: \"As a cluster admin, I want to configure ImageTagMirrorSet in HostedCluster CRs so that I can enable tag-based image proxying\"\n\n   Suggested: \"Enable ImageTagMirrorSet configuration in HostedCluster CRs\"\n\n   Use the suggested summary? (yes/no/edit)\n   ```\n\n5. If user says yes, use suggested summary\n6. If user says edit, prompt for their preferred summary\n7. If user says no, use their original summary (but warn it may be truncated in Jira)\n\n**Note:** This validation should happen BEFORE creating the issue, to avoid having to update the summary afterward.\n\n###  Phase 6: Security Validation\n\nScan all content (summary, description, comments) for sensitive data:\n\n**Prohibited content:**\n- Credentials (usernames/passwords, API tokens)\n- Cloud keys (AWS access keys, GCP service accounts, Azure credentials)\n- Kubeconfigs (cluster credentials, service account tokens)\n- SSH keys (private keys, authorized_keys)\n- Certificates (PEM files, private keys)\n- URLs with embedded credentials (e.g., `https://user:pass@example.com`)\n\n**Action if detected:**\n- STOP issue creation immediately\n- Inform user what type of data was detected (without exposing it)\n- Ask user to redact sensitive information\n- Provide guidance on safe alternatives (placeholder values)\n\n###  Phase 7: Create Issue via MCP\n\nUse the `mcp__atlassian__jira_create_issue` MCP tool with collected parameters.\n\n**Build additional_fields:**\n\n**Required fields (MUST be included):**\n- `security`: `{\"name\": \"Red Hat Employee\"}`\n- `labels`: `[\"ai-generated-jira\"]` (may be combined with additional labels)\n\n**Project-specific and team-specific fields:**\n- Custom field mappings\n- Version fields\n- Additional labels\n- Parent links\n- Component names\n- Any other project/team-specific requirements\n\nThe MCP tool parameters come from the combined guidance of type-specific, project-specific, and team-specific skills, with universal requirements always applied.\n\n**Note:** Project-specific skills (e.g., CNTRLPLANE) may implement fallback strategies for handling creation failures (such as epic linking). Refer to the project-specific skill documentation for these strategies.\n\n###  Phase 8: Return Result\n\nDisplay to user:\n- **Issue Key** (e.g., PROJECT-1234)\n- **Issue URL** (direct link to created issue)\n- **Summary of applied defaults** (any fields auto-populated by skills)\n\n**Example output:**\n```\nCreated: PROJECT-1234\nTitle: <issue summary>\nURL: <issue URL>\n\nApplied defaults:\n- <Field>: <Value>\n- <Field>: <Value>\n(varies by project/team)\n```\n\n## Usage Examples\n\n1. **Create a story with minimal info**:\n   ```\n   /jira:create story MYPROJECT \"Add user dashboard\"\n   ```\n    Prompts for user story format, acceptance criteria, and any required fields\n\n2. **Create a story with options**:\n   ```\n   /jira:create story MYPROJECT \"Add search functionality\" --component \"Frontend\" --version \"2.5.0\"\n   ```\n    Uses provided component and version, prompts only for description and AC\n\n3. **Create an epic with parent feature**:\n   ```\n   /jira:create epic MYPROJECT \"Mobile application redesign\" --parent MYPROJECT-100\n   ```\n    Links epic to parent feature, prompts for epic details\n\n4. **Create a bug**:\n   ```\n   /jira:create bug MYPROJECT \"Login button doesn't work on mobile\"\n   ```\n    Prompts for bug template fields (description, steps, actual/expected results)\n\n5. **Create a bug with component**:\n   ```\n   /jira:create bug MYPROJECT \"API returns 500 error\" --component \"Backend\"\n   ```\n    Uses specified component, prompts for bug details\n\n6. **Create a task under a story**:\n   ```\n   /jira:create task MYPROJECT \"Update API documentation\" --parent MYPROJECT-456\n   ```\n    Links task to parent story, prompts for task description\n\n7. **Create a feature**:\n   ```\n   /jira:create feature MYPROJECT \"Advanced search capabilities\"\n   ```\n    Prompts for market problem, strategic value, success criteria, epic breakdown\n\n8. **Create a feature request**:\n   ```\n   /jira:create feature-request RFE \"Support custom SSL certificates for ROSA HCP\"\n   ```\n    Prompts for nature/description, business requirements, affected components (4-question workflow)\n\n9. **Create with project-specific conventions** (examples vary by project):\n   ```\n   /jira:create story SPECIALPROJECT \"New capability\"\n   ```\n    Applies SPECIALPROJECT-specific skills and conventions automatically\n\n## Arguments\n\n- **$1  type** *(required)*\n  Issue type to create.\n  **Options:** `story` | `epic` | `feature` | `task` | `bug` | `feature-request`\n\n- **$2  project-key** *(optional for bugs and feature-requests)*\n  JIRA project key (e.g., `CNTRLPLANE`, `OCPBUGS`, `RFE`, `MYPROJECT`).\n  **Default for bugs:** `OCPBUGS`\n  **Default for feature-requests:** `RFE`\n  **Required for:** stories, epics, features, tasks\n\n- **$3  summary** *(required)*\n  Issue title/summary text.\n  Use quotes for multi-word summaries: `\"Enable automatic scaling\"`\n\n- **--component** *(optional)*\n  Component name (e.g., `\"HyperShift / ROSA\"`, `\"Networking\"`, `\"API\"`).\n  Auto-detected from summary context if not provided (for CNTRLPLANE/OCPBUGS).\n\n- **--version** *(optional)*\n  Target version. User input is normalized to Jira format `openshift-X.Y`.\n\n  **Accepted input formats (examples):**\n  | User Input | Normalized |\n  |------------|------------|\n  | `4.21` | `openshift-4.21` |\n  | `4.22.0` | `openshift-4.22` |\n  | `openshift 4.23` | `openshift-4.23` |\n  | `OCP 4.21` | `openshift-4.21` |\n  | `ocp 4.22` | `openshift-4.22` |\n\n  **Behavior:** If not provided via flag, user is prompted (optional field).\n\n  **Normalization rules:**\n  1. Convert to lowercase\n  2. Remove \"ocp\" or \"openshift\" prefix (with space or hyphen)\n  3. Extract version number (X.Y or X.Y.Z  X.Y)\n  4. Prepend \"openshift-\"\n\n- **--parent** *(optional)*\n  Parent issue key for linking (e.g., `CNTRLPLANE-123`).\n  **Valid for:**\n  - Epics: Link to parent Feature\n  - Tasks: Link to parent Story or Epic\n  - Stories: Link to parent Epic (less common)\n\n## Return Value\n\n- **Issue Key**: The created Jira issue identifier (e.g., `CNTRLPLANE-1234`)\n- **Issue URL**: Direct link to the created issue\n- **Summary**: Confirmation of applied defaults and field values\n\n## Configuration\n\n### Project-Specific Skills\n\nThe command automatically detects and applies project-specific conventions:\n\n- **CNTRLPLANE:** Uses `cntrlplane` skill for CNTRLPLANE stories/epics/features/tasks\n- **OCPBUGS:** Uses `ocpbugs` skill for OCPBUGS bugs\n- **Other projects:** Uses general best practices from type-specific skills\n\nTo add conventions for your project, create a skill at:\n```\nplugins/jira/skills/your-project-name/SKILL.md\n```\n\nThen update the command implementation to invoke your skill when the project is detected.\n\n### Environment Variables\n\nThe command respects MCP Jira server configuration:\n- **JIRA_PROJECTS_FILTER:** Filter which projects are accessible\n- **JIRA_SERVER_URL:** Jira instance URL\n- **JIRA_AUTH:** Authentication credentials\n\n## Error Handling\n\n### Invalid Issue Type\n\n**Scenario:** User specifies invalid type.\n\n**Action:**\n```\nInvalid issue type \"stroy\". Valid types: story, epic, feature, task, bug\n\nDid you mean \"story\"?\n```\n\n### Missing Project Key\n\n**Scenario:** Project key required but not provided.\n\n**Action:**\n```\nProject key is required for stories/tasks/epics/features.\n\nUsage: /jira:create story PROJECT-KEY \"summary\"\n\nExample: /jira:create story CNTRLPLANE \"Enable autoscaling\"\n```\n\n### Component Required But Not Provided\n\n**Scenario:** Project requires component, cannot auto-detect, user didn't specify.\n\n**Action:**\n```\nComponent is required for CNTRLPLANE issues. Which component?\n1. HyperShift / ARO - for ARO HCP (Azure) issues\n2. HyperShift / ROSA - for ROSA HCP (AWS) issues\n3. HyperShift - for platform-agnostic issues\n\nSelect a component (1-3):\n```\n\n### Parent Issue Not Found\n\n**Scenario:** User specifies `--parent` but issue doesn't exist.\n\n**Action:**\n```\nParent issue CNTRLPLANE-999 not found.\n\nOptions:\n1. Proceed without parent link\n2. Specify different parent\n3. Cancel creation\n\nWhat would you like to do?\n```\n\n### Security Validation Failure\n\n**Scenario:** Credentials or secrets detected.\n\n**Action:**\n```\nI detected what appears to be an API token in the description.\n\nPlease review and redact before proceeding. Use placeholder values like:\n- YOUR_API_KEY\n- <redacted>\n- ********\n\nWould you like to edit the description?\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error.\n\n**Action:**\n1. Parse error message\n2. Translate to user-friendly explanation\n3. Suggest corrective action\n4. Offer to retry\n\n**Common errors:**\n- **\"Field 'component' is required\"**  Prompt for component\n- **\"Version not found\"**  Fetch available versions, suggest closest match\n- **\"Permission denied\"**  User may lack permissions, suggest contacting admin\n- **\"Issue type not available\"**  Project may not support this issue type\n\n### Epic Link Creation Failure\n\n**Scenario:** Story/task creation fails when including epic link field.\n\n**Action:**\nRefer to project-specific skills for epic linking fallback strategies:\n- **CNTRLPLANE:** See CNTRLPLANE skill \"Epic Linking Implementation Strategy\" section\n- **Other projects:** Consult project-specific skill documentation\n\n**General pattern:**\n1. Detect error related to linking (error contains \"epic\", \"parent\", \"link\", or \"customfield\")\n2. Check project-specific skill for recommended fallback approach\n3. Typically: Create without link, then link via update\n4. Inform user of outcome\n5. **Last stand fallback:** If all strategies fail (including update-after-create), retry with absolute minimal fields:\n   - Remove ALL custom fields (epic link, target version, etc.)\n   - Keep only: project_key, summary, issue_type, description, assignee, components\n   - Log to console what was stripped out\n   - If this succeeds, inform user which fields need manual configuration in Jira\n\n### Field Format Error\n\n**Scenario:** Field provided in wrong format (e.g., Target Version as string instead of array).\n\n**Common field format errors:**\n\n1. **Target Version format**\n   -  Wrong: `\"customfield_12319940\": \"openshift-4.21\"`\n   -  Correct: `\"customfield_12319940\": [{\"id\": \"12448830\"}]`\n   - **Action:** Fetch version ID using `mcp__atlassian__jira_get_project_versions`, convert to correct format\n\n2. **Epic Link format**\n   -  Wrong: `\"parent\": {\"key\": \"EPIC-123\"}` (for stories)\n   -  Correct: `\"customfield_12311140\": \"EPIC-123\"` (string, not object)\n   - **Action:** Convert to correct format and retry\n\n3. **Component format**\n   -  Wrong: `\"components\": \"ComponentName\"`\n   -  Correct: `\"components\": [\"ComponentName\"]` (array) or just `\"ComponentName\"` (MCP accepts both)\n   - **Action:** Ensure consistent format\n\n**Detection:**\n- Parse error message for field names\n- Check skill documentation for correct format\n- Automatically convert and retry when possible\n\n## Best Practices\n\n1. **Use descriptive summaries:** Include relevant keywords for context and auto-detection\n2. **Provide flags when known:** Use `--component` and `--version` to skip prompts\n3. **Link related work:** Use `--parent` to maintain hierarchy\n4. **Review before submit:** Check the formatted content before confirming creation\n5. **Follow templates:** Use the provided templates for consistency\n6. **Sanitize content:** Remove credentials before including logs or screenshots\n\n## Anti-Patterns to Avoid\n\n **Wrong issue type**\n```\n/jira:create story MYPROJECT \"Refactor database layer\"\n```\n This is technical work, use `task` instead\n\n **Vague summaries**\n```\n/jira:create bug \"Something is broken\"\n```\n Be specific: \"API server returns 500 error when creating namespaces\"\n\n **Missing context**\n```\n/jira:create epic MYPROJECT \"Improve things\"\n```\n Be descriptive: \"Mobile application redesign\"\n\n **Including credentials**\n```\nSteps to reproduce:\n1. Export API_KEY=sk_live_abc123xyz\n```\n Use placeholders: \"Export API_KEY=YOUR_API_KEY\"\n\n## See Also\n\n- `jira:solve` - Analyze and solve Jira issues\n- `jira:grooming` - Generate grooming meeting agendas\n- `jira:status-rollup` - Create status rollup reports\n- `jira:generate-test-plan` - Generate test plans for PRs\n\n## Skills Reference\n\nThe following skills are automatically invoked by this command:\n\n**Type-specific skills:**\n- **create-story** - User story creation guidance\n- **create-epic** - Epic creation and structure\n- **create-feature** - Feature planning and strategy\n- **create-task** - Technical task creation\n- **create-bug** - Bug report templates\n- **create-feature-request** - Customer-driven feature request workflow for RFE project\n\n**Project-specific skills:**\n- **cntrlplane** - CNTRLPLANE project conventions (stories, epics, features, tasks)\n- **ocpbugs** - OCPBUGS project conventions (bugs only)\n\n**Team-specific skills:**\n- **hypershift** - HyperShift team conventions (component selection for ARO/ROSA/HyperShift)\n\nTo view skill details:\n```bash\nls plugins/jira/skills/\ncat plugins/jira/skills/create-story/SKILL.md\ncat plugins/jira/skills/create-feature-request/SKILL.md\ncat plugins/jira/skills/cntrlplane/SKILL.md\ncat plugins/jira/skills/ocpbugs/SKILL.md\ncat plugins/jira/skills/hypershift/SKILL.md\n```"
              },
              {
                "name": "/generate-feature-doc",
                "description": "Generate comprehensive feature documentation from Jira feature and all related issues and PRs",
                "path": "plugins/jira/commands/generate-feature-doc.md",
                "frontmatter": {
                  "description": "Generate comprehensive feature documentation from Jira feature and all related issues and PRs",
                  "argument-hint": "<feature-key>"
                },
                "content": "## Name\njira:generate-feature-doc\n\n## Synopsis\n```\n/jira:generate-feature-doc <feature-key>\n```\n\n## Description\n\nThe `jira:generate-feature-doc` command generates comprehensive feature documentation by recursively analyzing a Jira feature ticket and all its related issues, sub-tasks, and linked GitHub pull requests.\n\nThis command is particularly useful for:\n- Creating complete feature documentation after implementation\n- Understanding the full scope and implementation details of a feature\n- Generating onboarding materials for new team members\n- Creating technical design documents from actual implementation\n- Documenting complex features with multiple related issues and PRs\n\nThe command performs deep analysis of:\n- The main feature issue and all related Jira tickets (recursively)\n- All linked GitHub PRs, commits, and code changes\n- Design decisions captured in PR discussions and code reviews\n- Implementation details from actual code changes\n\n## Implementation\n\nThis command orchestrates two skills in sequence to generate comprehensive feature documentation:\n\n1. **Extract GitHub PRs** (via `jira:extract-prs` skill)\n   - Discovers all descendant issues recursively using `childIssuesOf()` JQL\n   - Extracts PR URLs from Jira Remote Links (primary) and text content (backup)\n   - Fetches PR metadata from GitHub (state, title)\n   - Returns structured JSON with deduplicated PRs\n\n2. **Generate Documentation** (via `jira-doc-generator` skill)\n   - Receives PR data from step 1\n   - Analyzes MERGED PRs only (fetches metadata, diff, comments)\n   - Synthesizes feature documentation with the following sections:\n     * Overview\n     * Background and Goals\n     * Architecture and Design\n     * Usage Guide\n     * Related Resources (External Links only)\n   - Skip sections: Implementation Details, Testing\n   - Outputs to `.work/jira/feature-doc/<feature-key>/feature-doc.md`\n\nFor detailed implementation, see:\n- `plugins/jira/skills/extract-prs/SKILL.md`\n- `plugins/jira/skills/jira-doc-generator/SKILL.md`\n\n## Arguments\n\n- **$1  feature-key** *(required)*\n  Jira issue key for the feature (e.g., `OCPSTRAT-1612`).\n  Can be any issue type, but typically used for Features, Epics, or Stories.\n\n## Return Value\n\n- **Documentation File**: `.work/jira/feature-doc/<feature-key>/feature-doc.md`\n- **Summary Statistics**: Number of issues analyzed, PRs analyzed, total lines of documentation\n- **Related Resources**: List of all Jira issues and GitHub PRs included in the analysis\n- **Debug Files**: Raw issue data, PR data, and analysis log\n\n## Output Format\n\nThe command generates a comprehensive markdown document at:\n```\n.work/jira/feature-doc/<feature-key>/feature-doc.md\n```\n\n### Document Structure\n\n```markdown\n# Feature: <Feature Title>\n\n## Overview\n- **Jira Issue**: <main issue link>\n- **Status**: <status>\n- **Related Issues**: <count>\n- **Related PRs**: <count>\n- **Implementation Period**: <date range>\n\n## Background and Goals\n<Extracted from feature description>\n\n## Architecture and Design\n<Synthesized from design discussions and PR descriptions>\n\n## Usage Guide\n<How to use the implemented feature>\n\n## Related Resources\n<External links only - no tables>\n```\n\n## Examples\n\n### Basic Usage\n\nGenerate documentation for a feature:\n```\n/jira:generate-feature-doc OCPSTRAT-1612\n```\n\nThe command will:\n1. Fetch the main feature issue\n2. Recursively discover all related issues (sub-tasks, linked issues, etc.)\n3. Extract all GitHub PR links from Remote Links API and text\n4. Analyze each PR (description, code changes, discussions)\n5. Synthesize all information into a comprehensive document\n6. Save to `.work/jira/feature-doc/OCPSTRAT-1612/feature-doc.md`\n7. Display summary statistics\n\n### Example Output\n\n```\n Analyzing OCPSTRAT-1612...\n\n Discovering related issues (subtasks only)...\n   Found main feature: \"Configure and Modify Internal OVN IPV4 Subnets\"\n   Discovered 2 subtasks\n  Total issues: 3\n\n Extracting GitHub PRs...\n  - From text (descriptions/comments): 1 PR\n  - From Remote Links API (PRIMARY): 6 PRs\n  Total unique PRs: 7 (after deduplication)\n\n Analyzing PRs (MERGED only)...\n   [1/7] openshift/hypershift#6444 (MERGED)\n    [2/7] openshift/hypershift#6500 (OPEN - skipped)\n   [3/7] openshift/hypershift#6554 (MERGED)\n  ...\n\n Generating documentation...\n   Feature overview and background\n   Architecture and design section\n   Implementation details\n   Related resources index\n\n Documentation generated successfully!\n\n File: .work/jira/feature-doc/OCPSTRAT-1612/feature-doc.md\n Statistics:\n  - Jira issues analyzed: 3 (1 feature + 2 subtasks)\n  - GitHub PRs discovered: 7\n  - GitHub PRs analyzed: 5 (MERGED only)\n  - GitHub PRs skipped: 2 (1 OPEN, 1 CLOSED)\n  - Total commits: 12\n  - Total files changed: 41\n  - Documentation lines: 150\n```\n\n## Error Handling\n\nFor complete error handling documentation, see SKILL.md. Common scenarios:\n\n- **Issue Not Found**: 404 from Jira API - verify issue key and permissions\n- **No Related Issues or PRs**: Feature may not be implemented yet - generates docs from main issue only\n- **Circular Dependencies**: Cycle detection prevents infinite loops\n- **GitHub Rate Limit**: Can resume after rate limit reset or generate partial docs\n- **PR Not Accessible**: Skips inaccessible PRs (403, 404) but lists them in output\n- **Large Feature**: Warns when >50 issues/PRs, allows user to cancel\n\nSee `jira-doc-generator` SKILL.md Section \"Error Handling\" for detailed guidance.\n\n## Prerequisites\n\n### Required Tools\n\n1. **jq** - JSON parser\n   - Check: `which jq`\n   - Install: `brew install jq` (macOS) or `apt-get install jq` (Linux)\n\n2. **GitHub CLI (`gh`)** - For fetching PR data\n   - Install: `brew install gh` (macOS) or see [GitHub CLI docs](https://cli.github.com/)\n   - Authenticate: `gh auth login`\n   - Verify: `gh auth status`\n\n3. **JIRA Access** - Read permissions for issues\n   - Network access to `https://issues.redhat.com`\n\n4. **GitHub Access** - Read access to repositories where PRs are located\n   - PRs in private repos require appropriate GitHub permissions\n\n### Performance Considerations\n\n- **Time**: Large features may take 5-10 minutes to analyze completely\n- **API Quota**: Each PR analysis uses ~3-4 GitHub API calls\n- **Rate Limits**: GitHub allows 5,000 API calls/hour when authenticated\n- **Disk Space**: Generated documents typically range from 10KB to 500KB\n\n## Best Practices\n\n1. **Link issues properly**: Ensure sub-tasks and related issues are properly linked in Jira\n2. **Use Remote Links**: Add GitHub PRs via \"Link Issue\" in JIRA UI (not just in descriptions)\n3. **Use for completed features**: Best results when feature is fully implemented\n4. **Review and edit**: Generated documentation is a starting point - review and enhance manually\n5. **Save the output**: Keep generated docs in version control for team reference\n\n## See Also\n\n- `jira:solve` - Analyze and solve Jira issues\n- `jira:create-release-note` - Generate release notes from bugs and PRs\n- `jira:status-rollup` - Create status rollup reports\n- `utils:generate-test-plan` - Generate test plans for features\n\n## Technical Notes\n\n### Recursive Discovery Algorithm\n\nUses depth-first search with cycle detection:\n- Maximum recursion depth: 5 levels (configurable)\n- Visited issues tracked in a set to prevent re-processing\n- **Only explores**: subtasks and parent issues\n- **Does NOT explore**: issuelinks (relates, blocks, clones, duplicates) - these represent separate features\n\nSee SKILL.md Step 2 for complete algorithm details.\n\n### GitHub PR Extraction Strategy\n\n**Dual-source approach** (implemented in SKILL.md Step 3):\n1. **Primary**: JIRA Remote Links API (`/rest/api/2/issue/{key}/remotelink`)\n   - Authoritative source for PRs added via \"Link Issue\" UI\n   - NOT included in main issue API response\n2. **Backup**: Text extraction from descriptions and comments\n   - Regex: `https?://github\\.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+/pulls?/[0-9]+`\n\nBoth sources are merged and deduplicated.\n\n### PR Analysis Filter\n\n**Only MERGED PRs are analyzed** (implemented in SKILL.md Step 4):\n- **MERGED**: Analyzed and included in documentation\n- **OPEN**: Skipped (work in progress, may change)\n- **CLOSED**: Skipped (not merged, implementation not accepted)\n\nThis ensures documentation reflects only shipped implementation.\n\n### Output Directory Structure\n\n```\n.work/jira/feature-doc/<feature-key>/\n feature-doc.md           # Main documentation\n main-issue.json          # Main feature issue data\n issue-*.json             # Related issue data\n pr-*.json                # PR metadata\n pr-*.diff                # PR diffs\n pr-urls.txt              # All discovered PR URLs\n analysis-log.txt         # Analysis log (for debugging)\n```"
              },
              {
                "name": "/generate-test-plan",
                "description": "Generate test steps for a JIRA issue",
                "path": "plugins/jira/commands/generate-test-plan.md",
                "frontmatter": {
                  "description": "Generate test steps for a JIRA issue",
                  "argument-hint": "[JIRA issue key] [GitHub PR URLs]"
                },
                "content": "## Name\njira:generate-test-plan\n\n## Synopsis\n/jira:generate-test-plan [JIRA issue key] [GitHub PR URLs]\n\n## Description\nThe 'jira:generate-test-plan' command takes a JIRA issue key and optionally a list of PR URLs. It fetches the JIRA issue details, retrieves all related PRs (or uses the provided PR list), analyzes the changes, and generates a comprehensive manual testing guide.\n\n**JIRA Issue Test Guide Generator**\n\n## Implementation\n\n- The command uses curl to fetch JIRA data via REST API: https://issues.redhat.com/rest/api/2/issue/{$1}\n- Uses WebFetch to extract PR links from JIRA issue if no PRs provided\n- Uses `gh pr view` to fetch PR details for each PR\n- Analyzes changes across all PRs to understand implementation\n- Generates comprehensive manual test scenarios\n\n## Process Flow:\n\n1. **JIRA Analysis**: Fetch and parse JIRA issue details:\n   - Use curl to fetch JIRA issue data: `curl -s \"https://issues.redhat.com/rest/api/2/issue/{$1}\"`\n   - Parse JSON response to extract:\n     - Issue summary and description\n     - Context and acceptance criteria\n     - Steps to reproduce (for bugs)\n     - Expected vs actual behavior\n   - Extract issue type (Story, Bug, Task, etc.)\n\n2. **PR Discovery**: Get list of PRs to analyze:\n   - **If no PRs provided in arguments** ($2, $3, etc. are empty):\n     - Use WebFetch on https://issues.redhat.com/browse/{$1}\n     - Extract all GitHub PR links from:\n       - \"Issue Links\" section\n       - \"Development\" section\n       - PR links in comments\n   - **If PRs provided in arguments**:\n     - Use only the PRs provided in $2, $3, $4, etc.\n     - Ignore any other PRs linked to the JIRA\n\n3. **PR Analysis**: For each PR, fetch and analyze:\n   - Use `gh pr view {PR_NUMBER} --repo <your repo> --json title,body,commits,files,labels`\n   - Extract:\n     - PR title and description\n     - Changed files and their diffs\n     - Commit messages\n     - PR status (merged, open, closed)\n   - Read changed files to understand implementation details\n   - Use Grep and Glob tools to:\n     - Find related test files\n     - Locate configuration or documentation\n     - Identify dependencies\n\n4. **Change Analysis**: Understand what was changed across all PRs:\n   - Identify the overall objective (bug fix, feature, refactor)\n   - Determine affected components (API, CLI, operator, control-plane, etc.)\n   - Find platform-specific changes (AWS, Azure, KubeVirt, etc.)\n   - Map which PR addresses which aspect of the JIRA\n   - Identify any dependencies between PRs\n\n5. **Test Scenario Generation**: Create comprehensive test plan:\n   - Map JIRA acceptance criteria to test scenarios\n   - For bugs: Use reproduction steps as test cases\n   - Generate test scenarios covering:\n     - Happy path scenarios (based on acceptance criteria)\n     - Edge cases and error handling\n     - Platform-specific variations if applicable\n     - Regression scenarios\n   - For multiple PRs:\n     - Create integrated test scenarios\n     - Verify PRs work correctly together\n     - Test each PR's contribution to the overall solution\n\n6. **Test Guide Creation**: Generate detailed manual testing document:\n   - **Filename**: Always use JIRA key format: `test-{JIRA_KEY}.md`\n     - Convert JIRA key to lowercase\n     - Examples: `test-cntrlplane-205.md`, `test-ocpbugs-12345.md`\n   - **Structure**:\n     - **JIRA Summary**: Include JIRA key, title, description, acceptance criteria\n     - **PR Summary**: List all PRs with titles and how they relate to the JIRA\n     - **Prerequisites**:\n       - Required infrastructure and tools\n       - Environment setup requirements\n       - Access requirements\n     - **Test Scenarios**:\n       - Map each test to JIRA acceptance criteria\n       - Numbered test cases with clear steps\n       - Expected results with verification commands\n       - Platform-specific test variations\n     - **Regression Testing**:\n       - Related features to verify\n       - Areas that might be affected\n     - **Success Criteria**:\n       - Checklist mapping to JIRA acceptance criteria\n     - **Troubleshooting**:\n       - Common issues and debug steps\n     - **Notes**:\n       - Known limitations\n       - Links to JIRA and all PRs\n       - Critical test cases highlighted\n\n7. **Exclusions**: Apply smart filtering:\n   - **Skip PRs that don't require testing**:\n     - PRs that only add documentation (.md files only)\n     - PRs that only add CI/tooling (.github/, .claude/ directories)\n     - PRs marked with labels like \"skip-testing\" or \"docs-only\"\n   - **Note skipped PRs** in the test guide with reasoning\n   - Focus test scenarios on PRs with actual code changes\n\n8. **Output**: Display the testing guide:\n   - Show the file path where the guide was saved\n   - Provide a summary of:\n     - JIRA issue being tested\n     - Number of PRs included\n     - Number of test scenarios generated\n     - Critical test cases to focus on\n   - Highlight any PRs that were skipped and why\n   - Ask if the user would like any modifications to the test guide\n\n## Examples:\n\n1. **Generate test steps for JIRA with auto-discovered PRs**:\n   `/jira:generate-test-plan CNTRLPLANE-205`\n\n2. **Generate test steps for JIRA with specific PRs only**:\n   `/jira:generate-test-plan CNTRLPLANE-205 https://github.com/openshift/hypershift/pull/6888`\n\n3. **Generate test steps for multiple specific PRs**:\n   `/jira:generate-test-plan CNTRLPLANE-205 https://github.com/openshift/hypershift/pull/6888 https://github.com/openshift/hypershift/pull/6889`\n\n## Arguments:\n\n- **$1**: JIRA issue key (required) - e.g., CNTRLPLANE-205, OCPBUGS-12345\n- **$2, $3, ..., $N**: Optional GitHub PR URLs\n  - If provided: Only these PRs will be analyzed\n  - If omitted: All PRs linked to the JIRA will be discovered and analyzed\n\n## Smart Features:\n\n1. **Automatic PR Discovery**:\n   - Scans JIRA issue for all related PRs\n   - Identifies PRs in \"Issue Links\", \"Development\" section, and comments\n\n2. **Selective PR Testing**:\n   - Allows manual override to test specific PRs only\n   - Useful when JIRA has many PRs but only some need testing\n\n3. **Context-Aware Test Generation**:\n   - Bug fixes: Focus on reproduction steps and verification\n   - Features: Focus on acceptance criteria and user workflows\n   - Refactors: Focus on regression and functional equivalence\n\n4. **Multi-PR Integration**:\n   - Understands how multiple PRs work together\n   - Creates integration test scenarios\n   - Identifies dependencies and testing order\n\n5. **Build/Deploy Section Exclusion**:\n   - Does NOT include build or deployment steps\n   - Assumes environment is already set up\n   - Focuses purely on testing procedures\n\n6. **Cleanup Section Exclusion**:\n   - Does NOT include cleanup steps\n   - Focuses on test execution and verification\n\n## Example Workflow:\n\n```bash\n# Auto-discover all PRs from JIRA\n/jira:generate-test-plan CNTRLPLANE-205\n\n# Test only specific PRs\n/jira:generate-test-plan CNTRLPLANE-205 https://github.com/openshift/hypershift/pull/6888\n\n# Test multiple specific PRs\n/jira:generate-test-plan OCPBUGS-12345 https://github.com/openshift/hypershift/pull/1234 https://github.com/openshift/hypershift/pull/1235\n```\n\nThe command will provide a comprehensive manual testing guide that QE or developers can use to thoroughly test the JIRA issue implementation."
              },
              {
                "name": "/grooming",
                "description": "Analyze new bugs and cards added over a time period and generate grooming meeting agenda",
                "path": "plugins/jira/commands/grooming.md",
                "frontmatter": {
                  "description": "Analyze new bugs and cards added over a time period and generate grooming meeting agenda",
                  "argument-hint": "[project-filter] [time-period] [--component component-name] [--label label-name] [--type issue-type] [--status status] [--story-points]"
                },
                "content": "## Name\njira:grooming\n\n## Synopsis\n```\n/jira:grooming [project-filter] [time-period] [--component component-name] [--label label-name] [--type issue-type] [--status status] [--story-points]\n```\n\n## Description\nThe `jira:grooming` command helps teams prepare for backlog grooming meetings. It automatically collects bugs and user stories created within a specified time period OR assigned to a specific sprint, analyzes their priority, complexity, and dependencies, and generates structured grooming meeting agendas.\n\nThis command is particularly useful for:\n- Backlog organization before sprint planning\n- Sprint-specific grooming sessions\n- Sprint-specific grooming sessions with story point summaries\n- Sprint retrospectives analyzing completed work\n- Regular requirement grooming meetings\n- Priority assessment of new bugs\n- Technical debt organization and planning\n\n## Key Features\n\n- **Automated Data Collection**  Collect and categorize issues within specified time periods or sprints by type (Bug, Story, Task, Epic), extract key information (priority, components, labels), and identify unassigned or incomplete issues.\n\n- **Story Point Analysis**  When `--story-points` flag is used, extract and analyze story points for all issues, calculate totals by status, priority, and type, and provide velocity metrics for sprint retrospectives.\n\n- **Status Filtering**  Filter issues by status (e.g., Closed, Done, In Progress, Open) using the `--status` flag to focus on specific workflow states for sprint reviews or retrospectives.\n\n- **Intelligent Analysis**  Evaluate issue complexity based on historical data, identify related or duplicate issues, analyze business value and technical impact, and detect potential dependencies.\n\n- **Agenda Generation**  Build a structured, actionable meeting outline organized by priority and type, with discussion points, decision recommendations, estimation references, and risk alerts.\n\n## Implementation\n\nThe `jira:grooming` command runs in three main phases:\n\n###  Phase 1: Data Collection\n- Automatically queries JIRA for issues based on the provided time range or sprint name:\n  - **Time range mode**: Filters issues by creation date within the specified period (e.g., `last-week`, `2024-01-01:2024-01-31`)\n  - **Sprint mode**: Filters issues by JIRA Sprint without time constraints (e.g., `\"OTA 277\"`)\n  - Sprint detection: If the time-period parameter doesn't match known time range formats, it's treated as a sprint name\n- Supports complex JQL filters, including multi-project, component-based, label-based, issue-type and status filtering.\n- Extracts key fields such as title, type, priority, component, reporter, story points and assignee.\n- Detects related or duplicate issues to provide better context.\n\n###  Phase 2: Analysis & Processing\n- Groups collected issues by type and priority (e.g., Critical Bugs, High Priority Stories).\n- When `--story-points` flag is used:\n  - Calculates total story points by status (Closed, In Progress, Open, etc.)\n  - Calculates story points by priority and type\n  - Identifies issues missing story point estimates\n  - Computes velocity metrics for sprint retrospectives\n- Identifies incomplete or unclear issues that need clarification.\n- Estimates complexity and effort based on similar historical data.\n- Highlights risks, dependencies, and recommended next actions.\n\n###  Phase 3: Report Generation\n- Automatically generates a **structured grooming meeting agenda** in Markdown format.\n- Includes discussion points, decision checklists, and action items.\n- When `--story-points` flag is used, includes:\n  - Story point summary section with totals by status\n  - Velocity metrics and completion percentages\n  - Breakdown by priority and issue type\n  - List of issues missing story point estimates\n- Output can be copied directly into Confluence or shared with the team.\n\n## Usage Examples\n\n1. **Single project weekly review**:\n   ```\n   /jira:grooming OCPSTRAT last-week\n   ```\n\n2. **Multiple OpenShift projects**:\n   ```\n   /jira:grooming \"OCPSTRAT,OCPBUGS,HOSTEDCP\" last-2-weeks\n   ```\n\n3. **Filter by component**:\n   ```\n   /jira:grooming OCPSTRAT last-week --component \"Control Plane\"\n   ```\n\n4. **Custom date range**:\n   ```\n   /jira:grooming OCPBUGS 2024-10-01:2024-10-15\n   ```\n\n5. **Filter by label**:\n   ```\n   /jira:grooming OCPSTRAT last-week --label \"technical-debt\"\n   ```\n\n6. **Combine component and label filters**:\n   ```\n   /jira:grooming OCPSTRAT last-week --component \"Control Plane\" --label \"performance\"\n   ```\n\n7. **Query sprint issues with component filter**:\n   ```bash\n   /jira:grooming OCPBUGS \"OTA 277\" --component \"Cluster Version Operator\"\n   ```\n\n8. **Filter by issue type**:\n   ```\n   /jira:grooming OCPSTRAT last-week --type Bug\n   ```\n\n9. **Filter by multiple issue types**:\n   ```\n   /jira:grooming OCPSTRAT last-week --type \"Bug,Epic\"\n   ```\n\n10. **Combine all filters**:\n    ```\n    /jira:grooming OCPSTRAT last-week --component \"Control Plane\" --label \"performance\" --type Story\n    ```\n\n11. **Sprint retrospective with closed issues and story points**:\n   ```bash\n   /jira:grooming \"CORENET\" \"Sprint 276\" --status Closed --story-points\n   ```\n\n12. **Combine Label and status filters**:\n    ```bash\n    /jira:grooming OCPBUGS last-week --label \"performance\" --status \"NEW\"\n\n\n## Output Format\n\n### Grooming Meeting Agenda\n\nThe command outputs a ready-to-use Markdown document that can be copied into Confluence or shared with your team.\n\n```markdown\n# Backlog Grooming Agenda\n**Project**: [project-key] | **Period**: [time-period] | **New Issues**: [count]\n\n##  Summary\n\n##  Story Point Summary (when --story-points is used)\n\n**Total Story Points**: 89\n\n### By Status\n- **Closed**: 55 points (61.8%)\n- **In Progress**: 21 points (23.6%)\n- **Open**: 13 points (14.6%)\n\n### By Type\n- **Story**: 55 points\n- **Bug**: 21 points\n- **Task**: 13 points\n\n### Issues Missing Story Points (3)\n- PROJ-1240 - Performance optimization needed\n- PROJ-1241 - Update documentation\n- PROJ-1242 - Refactor authentication module\n\n##  Critical Issues ([count])\n- **[PROJ-1234]** System crashes on login - *Critical, needs immediate attention*\n- **[PROJ-1235]** Performance degradation - *High, assign to team lead*\n\n##  High Priority Stories ([count])  \n- **[PROJ-1236]** User profile enhancement - *Ready for sprint*\n- **[PROJ-1237]** Payment integration - *Needs design review*\n\n##  Needs Clarification ([count])\n- **[PROJ-1238]** Missing acceptance criteria\n- **[PROJ-1239]** Unclear technical requirements\n\n##  Action Items\n- [ ] Assign PROJ-1234 to senior developer (immediate)\n- [ ] Schedule design review for PROJ-1237 (this week)\n- [ ] Clarify requirements for PROJ-1238,1239 (before next grooming)\n- [ ] Add story point estimates to 3 issues\n```\n\n## Configuration\n\n### Default Query Configuration (.jira-grooming.json)\n```json\n{\n  \"defaultProjects\": [\"OCPSTRAT\", \"OCPBUGS\"],\n  \"defaultLabels\": [],\n  \"priorityMapping\": {\n    \"Critical\": \" Critical\",\n    \"High\": \" High Priority\"\n  },\n  \"estimationReference\": {\n    \"enableHistoricalComparison\": true\n  }\n}\n```\n\n## Arguments\n\n- **$1  project-filter**  \n  JIRA project selector. Supports single or multiple projects (comma-separated).  \n  Examples:\n    - `OCPSTRAT`\n    - `\"OCPSTRAT,OCPBUGS,HOSTEDCP\"`\n    - `\"OpenShift Virtualization,Red Hat OpenShift Control Planes\"`  \n      Default: read from configuration file\n\n- **$2  time-period**\n  Time range for issue collection OR sprint name.\n  Options:\n  - Time ranges: `last-week` | `last-2-weeks` | `last-month` | `YYYY-MM-DD:YYYY-MM-DD`\n  - Sprint name: Any string that doesn't match time range formats (e.g., `\"OTA 277\"`)\n  Default: `last-week`\n\n  **Behavior:**\n  - If a time range is provided, filters issues by creation date within that range\n  - If a sprint name is provided, filters issues by JIRA Sprint WITHOUT applying time range constraints\n\n- **--component** *(optional)*\n  Filter by JIRA component (single or comma-separated).\n  Examples:\n    - `--component \"Networking\"`\n    - `--component \"Control Plane,Storage\"`\n\n- **--label** *(optional)*\n  Filter by JIRA labels (single or comma-separated).\n  Examples:\n    - `--label \"technical-debt\"`\n    - `--label \"performance,security\"`\n\n- **--type** *(optional)*\n  Filter by JIRA issue type (single or comma-separated).\n  Examples:\n    - `--type Bug`\n    - `--type \"Epic,Story\"`\n    - `--type \"Bug,Task,Feature\"`\n\n  Common issue types: `Bug`, `Story`, `Task`, `Epic`, `Feature`, `Sub-task`\n\n- **--status** *(optional)*\n  Filter by JIRA issue status (single or comma-separated).\n  Examples:\n    - `--status Closed`\n    - `--status \"POST,ON_QA\"`\n    - `--status \"In Progress\"`\n\n- **--story-points** *(optional)*\n  Include story point analysis in the grooming report.\n  When this flag is present:\n    - Extracts story points from JIRA issues\n    - Calculates total story points by status, and type\n    - Identifies issues missing story point estimates\n    - Generates velocity metrics for sprint retrospectives\n    - Includes story point summary section in the report\n    \n## Return Value\n- **Markdown Report**: Ready-to-use grooming agenda with categorized issues and action items\n\n## See Also\n- `jira:status-rollup` - Status rollup reports\n- `jira:solve` - Issue solution generation"
              },
              {
                "name": "/solve",
                "description": "Analyze a JIRA issue and create a pull request to solve it.",
                "path": "plugins/jira/commands/solve.md",
                "frontmatter": {
                  "description": "Analyze a JIRA issue and create a pull request to solve it."
                },
                "content": "## Name\njira:solve\n\n## Synopsis\n```\n/jira:solve <jira-issue-id> [remote]\n```\n\n## Description\n\nThe `jira:solve` command analyzes a JIRA issue and creates a pull request to solve it.\n\nThis command takes a JIRA URL, fetches the issue description and requirements, analyzes the codebase to understand how to implement the solution, and creates a comprehensive pull request with the necessary changes.\n\n**Usage Examples:**\n\n1. **Solve a specific JIRA issue**:\n   ```\n   /jira:solve OCPBUGS-12345 origin\n   ```\n\n## Implementation\n\n- The command uses curl to fetch JIRA data via REST API: https://issues.redhat.com/rest/api/2/issue/{$1}\n- Parses JSON response using jq or text processing\n- Extracts key fields: summary, description, components, labels\n- No authentication required for public Red Hat JIRA issues\n- Creates a PR with the solution\n\n### Process Flow\n\n1. **Issue Analysis**: Parse JIRA URL and fetch issue details:\n   - Use curl to fetch JIRA issue data: curl -s \"https://issues.redhat.com/rest/api/2/issue/{$1}\"\n   - Parse JSON response to extract:\n      - Issue summary and description\n      - From within the description expect the following sections\n         - Required\n            - Context\n            - Acceptance criteria\n         - Optional\n            - Steps to reproduce (for bugs)\n            - Expected vs actual behavior\n   - Ask the user for further issue grooming if the requried sections are missing\n\n2. **Codebase Analysis**: Search and analyze relevant code:\n   - Find related files and functions\n   - Understand current implementation\n   - Identify areas that need changes\n   - Use Grep and Glob tools to search for:\n      - Related function names mentioned in JIRA\n      - File patterns related to the component\n      - Similar existing implementations\n      - Test files that need updates\n\n3. **Solution Implementation**:\n   - Think hard and create a detailed, step-by-step plan to implement this feature. Save it to spec-$1.md within the .work/jira/solve folder, for example .work/jira/solve/spec-OCPBUGS-12345.md\n   - Always ask the user to review the plan and give them the choice to modify it before start the implementation\n   - Implement the plan:\n    - Make necessary code changes using Edit/MultiEdit tools\n    - Follow existing code patterns and conventions\n    - Add or update tests as needed\n    - Update documentation if needed within the docs/ folder\n    - If the problem is too complex consider delegating to one of the SME agents.\n    - Ensure godoc comments are generated for any newly created public functions\n      - Use your best judgement if godoc comments are needed for private functions\n      - For example, a comment should not be generated for a simple function like func add(int a, b) int { return a + b}\n    - Create unit tests for any newly created functions\n  - After making code changes, verify the implementation based on the repository's tooling:\n    - **Check for Makefile**: Run `ls Makefile` to see if one exists\n    - **If Makefile exists**: Check available targets with `make help` or `grep '^[^#]*:' Makefile | head -20`\n    - **Run appropriate verification commands**:\n      - If `make lint-fix` exists: Run it to ensure imports are properly sorted and linting issues are fixed\n      - If `make verify`, `make build`, `make test` exist: Run these to ensure code builds and passes tests\n      - If no Makefile or make targets: Look for alternative commands:\n        - Go projects: `go fmt ./...`, `go vet ./...`, `go test ./...`, `go build ./...`\n        - Node.js: `npm test`, `npm run build`, `npm run lint`\n        - Python: `pytest`, `python -m unittest`, `pylint`, `black .`\n        - Other: Follow repository conventions in CI config files (.github/workflows/, .gitlab-ci.yml, etc.)\n    - **Never assume make targets exist** - always verify first\n    - **You must ensure verification passes** before proceeding to \"Commit Creation\"\n\n4. **Commit Creation**: \n   - Create feature branch using the jira-key $1 as the branch name. For example: \"git checkout -b fix-{jira-key}\"\n   - Break commits into logical components based on the nature of the changes\n   - Each commit should honor https://www.conventionalcommits.org/en/v1.0.0/ and always include a commit message body articulating the \"why\"\n   - Use your judgment to organize commits in a way that makes them easy to review and understand\n   - Common logical groupings (use as guidance, not rigid rules):\n     - API changes: Changes in `api/` directory (types, CRDs)\n       - Example: `git commit -m\"feat(api): Update HostedCluster API for X\" -m\"Add new fields to support Y functionality\"`\n     - Vendor changes: Dependency updates in `vendor/` directory\n       - Example: `git commit -m\"chore(vendor): Update dependencies for X\" -m\"Required to pick up bug fixes in upstream library Y\"`\n     - Generated code: Auto-generated clients, informers, listers, and CRDs\n       - Example: `git commit -m\"chore(generated): Regenerate clients and CRDs\" -m\"Regenerate after API changes to ensure client code is in sync\"`\n     - CLI changes: User-facing command changes in `cmd/` directory\n       - Example: `git commit -m\"feat(cli): Add support for X flag\" -m\"This allows users to configure Y behavior at cluster creation time\"`\n     - Operator changes: Controller logic in `operator/` or `controllers/`\n       - Example: `git commit -m\"feat(operator): Implement X controller logic\" -m\"Without this the controller won't reconcile when Y condition occurs\"`\n     - Support/utilities: Shared code in `support/` directory\n       - Example: `git commit -m\"refactor(support): Extract common X utility\" -m\"Consolidate duplicated logic from multiple controllers into shared helper\"`\n     - Tests: Test additions or modifications\n       - Example: `git commit -m\"test: Add tests for X functionality\" -m\"Ensure the new behavior is covered by unit tests to prevent regressions\"`\n     - Documentation: Changes in `docs/` directory\n       - Example: `git commit -m\"docs: Document X feature\" -m\"Help users understand how to configure and use the new capability\"`\n\n5. **PR Creation**: \n   - Push the branch with all commits against the remote specified in argument $2\n   - Create pull request with:\n     - Clear title referencing JIRA issue as a prefix. For example: \"OCPBUGS-12345: ...\"\n     - The PR description should satisfy the template within .github/PULL_REQUEST_TEMPLATE.md if the file exists\n     - The \" Generated with Claude Code\" sentence should include a reference to the slash command that triggered the execution, for example \"via `/jira-solve OCPBUGS-12345 enxebre`\"\n     - Always create as draft PR\n     - Always create the PR against the remote origin\n     - Use gh cli if you need to\n\n6. **PR Description Review**:\n   - After creating the PR, display the PR URL and description to the user\n   - Ask the user: \"Please review the PR description. Would you like me to update it? (yes/no)\"\n   - If the user says yes or requests changes:\n     - Ask what changes they'd like to make\n     - Update the PR description using `gh pr edit {PR_NUMBER} --body \"{new_description}\"`\n     - Repeat this review step until the user is satisfied\n   - If the user says no or is satisfied, acknowledge and provide next steps\n\n\n## Arguments:\n- $1: The JIRA issue to solve (required)\n- $2: The remote repository to push the branch. Defaults to \"origin\".\n\nThe command will provide progress updates and create a comprehensive solution addressing all requirements from the JIRA issue."
              },
              {
                "name": "/status-rollup",
                "description": "Generate a status rollup comment for any JIRA issue based on all child issues and a given date range",
                "path": "plugins/jira/commands/status-rollup.md",
                "frontmatter": {
                  "description": "Generate a status rollup comment for any JIRA issue based on all child issues and a given date range",
                  "argument-hint": "issue-id [--start-date YYYY-MM-DD] [--end-date YYYY-MM-DD]"
                },
                "content": "## Name\njira:status-rollup\n\n## Synopsis\n```\n/jira:status-rollup issue-id [--start-date YYYY-MM-DD] [--end-date YYYY-MM-DD]\n```\n\n## Description\nThe `jira:status-rollup` command generates a comprehensive status rollup for any JIRA issue (Feature, Epic, Story, etc.) by recursively analyzing all child issues and their activity within a specified date range. The command intelligently extracts insights from changelogs and comments to create a concise, well-formatted status summary that can be reviewed and refined before being posted to Jira.\n\nThis command is particularly useful for:\n- Weekly status updates on Features or Epics\n- Sprint retrospectives and planning\n- Executive summaries of complex work hierarchies\n- Identifying blockers and risks across multiple issues\n\nKey capabilities:\n- Recursively traverses entire issue hierarchies (any depth)\n- Analyzes status transitions, assignee changes, and priority shifts\n- Extracts blockers, risks, and completion insights from comments\n- Generates properly formatted Jira wiki markup with nested bullets\n- Caches all data in a temp file for fast iterative refinement\n- Allows review and modification before posting to Jira\n\n[Extended thinking: This command takes any JIRA issue ID (Feature, Epic, Story, etc.) and optional date range, recursively collects all descendant issues, analyzes their changes and comments within the date range, and generates a concise status summary rolled up to the parent issue level. The summary is presented to the user for review and refinement before being posted as a comment.]\n\n## Implementation\n\nThe command executes the following workflow:\n\n1. **Parse Arguments and Validate**\n   - Extract issue ID from $1\n   - Parse --start-date and --end-date if provided\n   - Validate date format (YYYY-MM-DD)\n   - Default to issue creation date if no start-date provided\n   - Default to today if no end-date provided\n\n2. **Issue Validation**\n   - Use `mcp__atlassian__jira_get_issue` to fetch the issue\n   - Verify the issue exists and is accessible\n   - Extract issue key, summary, type, and basic info\n   - Works with any issue type (Feature, Epic, Story, Task, etc.)\n\n3. **Data Collection - Build Issue Hierarchy**\n   - Find direct children using JQL: `parent = {issue-id}`\n   - Recursively find all descendant issues (any depth)\n   - Fetch detailed issue data for each issue (status, summary, assignee, etc.)\n   - Use `mcp__atlassian__jira_batch_get_changelogs` for all issue keys\n   - Filter changelog entries to date range (status transitions, assignee changes, etc.)\n   - Fetch comments using `expand=renderedFields`, filter by date range\n   - Save all data to temp file: `/tmp/jira-rollup-{issue-id}-{timestamp}.md`\n\n4. **Data Analysis - Derive Status**\n   - Calculate completion metrics (total, done, in-progress, blocked, percentage)\n   - Identify issues completed/started/blocked within date range\n   - Extract significant status transitions and key changes\n   - Analyze comments for keywords:\n     - **Blockers**: \"blocked\", \"waiting on\", \"stuck\", \"dependency\"\n     - **Risks**: \"risk\", \"concern\", \"problem\", \"at risk\"\n     - **Completion**: \"completed\", \"done\", \"merged\", \"delivered\"\n     - **Progress**: \"started\", \"working on\", \"implementing\"\n     - **Help needed**: \"need\", \"require\", \"help\", \"support\"\n   - Extract entities: team mentions, dependencies, PR references, deadlines\n   - Prioritize comments (high/medium/low based on keywords)\n   - Cross-reference comments with status transitions\n   - Assess overall health (on track, at risk, blocked, complete)\n   - Append analysis results to temp file\n\n5. **Generate Status Summary**\n   - Read from temp file (NO re-fetching from Jira)\n   - Create formatted summary in Jira wiki markup:\n     ```\n     h2. Status Rollup From: {start-date} to {end-date}\n\n     *Overall Status:* [Clear statement about health and progress]\n\n     *This Week:*\n     * Completed:\n     *# [ISSUE-ID] - [Specific achievement from comments]\n     *# [ISSUE-ID] - [Specific achievement from comments]\n     * In Progress:\n     *# [ISSUE-ID] - [Current state and specific details]\n     * Blocked:\n     *# [ISSUE-ID] - [Specific reason for blocker]\n\n     *Next Week:*\n     * [Planned item based on analysis]\n\n     *Metrics:* X/Y issues complete (Z%)\n     ```\n   - Use specific insights from comments (NOT vague phrases like \"ongoing work\")\n   - Include PR references, ticket numbers, specific tasks mentioned\n   - Add direct quotes when they provide critical context\n   - Use `*#` syntax for nested bullets (Jira wiki markup)\n\n6. **Present to User for Review**\n   - Display temp file location for verification\n   - Show generated summary\n   - Ask if user wants changes\n\n7. **Iterative Refinement**\n   - If user requests changes, read from temp file (don't re-fetch)\n   - Support refinement strategies:\n     - Focus more on blockers/risks/completion\n     - Add/remove technical details or quotes\n     - Change grouping (by epic, type, status, assignee)\n     - Adjust level of detail (high-level vs. detailed)\n   - Regenerate only affected sections\n   - Repeat until user satisfied\n\n8. **Post Comment to Issue**\n   - Use `mcp__atlassian__jira_add_comment` to post to parent issue\n   - Append footer: \" Generated with [Claude Code](https://claude.com/claude-code) via `/jira:status-rollup {issue-id} --start-date {date} --end-date {date}`\"\n   - Confirm with user and provide issue URL\n\n9. **Temp File Cleanup**\n   - Ask user if they want to keep `/tmp/jira-rollup-{issue-id}-{timestamp}.md`\n   - Delete if user says no, otherwise keep for reference\n\n**Error Handling:**\n- Invalid issue ID: Display error with verification instructions\n- No child issues: Offer to generate summary for single issue\n- No activity in date range: Generate summary based on current state\n- Invalid date format: Display error with correct format example\n- Large hierarchies (100+ issues): Show progress indicators\n\n**Performance Considerations:**\n- Use batch API endpoints where available\n- Implement appropriate delays to respect rate limits\n- Cache all data in temp file for instant refinement\n\n## Return Value\n- **Posted to Jira**: Formatted status comment on the parent issue\n- **Temp file**: `/tmp/jira-rollup-{issue-id}-{timestamp}.md` containing:\n  - Parent issue details\n  - Complete issue hierarchy with counts by type\n  - Raw changelog data for all issues\n  - All comments with metadata (author, date, issue key)\n  - Comment analysis (keywords, priorities, cross-references)\n  - Metrics summary\n\n## Examples\n\n1. **Generate status for a Feature for a specific week**:\n   ```\n   /jira:status-rollup FEATURE-123 --start-date 2025-01-06 --end-date 2025-01-13\n   ```\n   Output: Weekly status comment posted to FEATURE-123\n\n2. **Generate status for an Epic**:\n   ```\n   /jira:status-rollup EPIC-456 --start-date 2025-01-06 --end-date 2025-01-13\n   ```\n   Output: Epic status summary with all child stories analyzed\n\n3. **Generate status for a Story with subtasks**:\n   ```\n   /jira:status-rollup STORY-789\n   ```\n   Output: Status from story creation date to today\n\n4. **Generate status from a start date to now**:\n   ```\n   /jira:status-rollup CNTRLPLANE-1234 --start-date 2025-01-06\n   ```\n   Output: Status from Jan 6 to today\n\n**Example Output:**\n```\nh2. Weekly Status: 2025-01-06 to 2025-01-13\n\n*Overall Status:* Feature is on track. Core authentication work completed this week with 2 PRs merged. UI integration starting with design approved.\n\n*This Week:*\n* Completed:\n*# AUTH-101 - OAuth2 implementation (PR #456 merged, all review feedback addressed)\n*# AUTH-102 - OAuth2 token validation (unit tests added, edge cases handled)\n* In Progress:\n*# UI-201 - Login UI components (design review completed, implementing responsive layout for mobile)\n*# AUTH-103 - Session handling (refactoring cookie storage mechanism, PR in draft)\n* Blocked:\n*# AUTH-104 - Azure AD integration (blocked on subscription approval, escalated to infrastructure team). Per Jane Doe: \"Need Azure subscription approved before proceeding - submitted ticket #12345\"\n\n*Next Week:*\n* Complete session handling refactor (AUTH-103) and submit for review\n* Finish login UI responsive implementation (UI-201) once design assets are finalized\n* Begin end-to-end testing (AUTH-107) if session handling is merged\n\n*Metrics:* 8/15 issues complete (53%)\n\n Generated with [Claude Code](https://claude.com/claude-code) via `/jira:status-rollup FEATURE-123 --start-date 2025-01-06 --end-date 2025-01-13`\n```\n\n## Arguments\n- `issue-id` (required): The JIRA issue ID to analyze (e.g., FEATURE-123, EPIC-456, STORY-789, CNTRLPLANE-1234)\n- `--start-date` (optional): Start date in YYYY-MM-DD format. Defaults to issue creation date if not provided\n- `--end-date` (optional): End date in YYYY-MM-DD format. Defaults to today if not provided"
              },
              {
                "name": "/validate-blockers",
                "description": "Validate proposed release blockers using Red Hat OpenShift release blocker criteria",
                "path": "plugins/jira/commands/validate-blockers.md",
                "frontmatter": {
                  "description": "Validate proposed release blockers using Red Hat OpenShift release blocker criteria",
                  "argument-hint": "[target-version] [component-filter] [--bug issue-key]"
                },
                "content": "## Name\njira:validate-blockers\n\n## Synopsis\n```\n/jira:validate-blockers [target-version] [component-filter] [--bug issue-key]\n```\n\n**Note**: `target-version` is required unless `--bug` is provided.\n\n## Description\nThe `jira:validate-blockers` command helps release managers make data-driven decisions on proposed release blockers. It analyzes bugs with \"Release Blocker = Proposed\" status using Red Hat OpenShift release blocker criteria and provides clear APPROVE/REJECT/DISCUSS recommendations with detailed justification.\n\nThis command is essential for:\n- Validating proposed release blockers\n- Making blocker approval/rejection decisions\n- Understanding why a bug should or shouldn't block a release\n- Reviewing blocker criteria compliance\n\n## Key Features\n\n- **Proposed Blocker Focus**  Automatically filters for bugs with Release Blocker = Proposed\n- **Red Hat OpenShift Release Blocker Criteria**  Analyzes against documented blocker criteria\n- **Clear Recommendations**  Provides APPROVE, REJECT, or DISCUSS recommendations\n- **Detailed Justification**  Shows which criteria matched and analysis rationale\n- **Component Filtering**  Scope validation to specific components\n\n## Implementation\n\nFor detailed implementation guidance including JQL queries, scoring algorithms, and decision logic, see the [jira-validate-blockers skill](../skills/jira-validate-blockers/SKILL.md).\n\n### High-Level Workflow\n\n1. **Parse Arguments and Build Filters**  Extract arguments (target version, component filter, bug ID). Build JQL query for:\n   - **Single bug mode** (if --bug provided): Query specific bug by issue key (version not required)\n   - **Component + version mode** (if both provided): Query proposed blockers matching target version and component, excluding already-fixed bugs (status not in Closed, Release Pending, Verified, ON_QA)\n   - **Version only mode** (if version provided): Query all proposed blockers for target version, excluding already-fixed bugs\n   - **Error**: If neither --bug nor version provided, show error message\n\n2. **Query Proposed Blockers**  Use MCP tools to fetch bugs based on mode:\n   - Single bug: Fetch one specific bug with all fields\n   - Component + version: Fetch proposed blockers matching version and component filter, excluding already-fixed statuses\n   - Version only: Fetch all proposed blockers for target version, excluding already-fixed statuses\n\n3. **Analyze Each Blocker**  Apply Red Hat OpenShift release blocker criteria:\n   - Strong blockers: Component Readiness regression, Service Delivery blocker, data loss, install/upgrade failures, service unavailability, regressions\n   - Never blockers: Severity below Important, new features without regression\n   - Workaround assessment: Check if acceptable workaround exists (idempotent, safe at scale, timely)\n\n4. **Generate Recommendations**  Create report with APPROVE/REJECT/DISCUSS verdicts and justifications.\n\n### Technical Details\n\nThe skill file provides complete details on:\n- JQL query construction for proposed blockers\n- Blocker scoring criteria and point values\n- Workaround assessment logic\n- Decision thresholds\n\n## Usage Examples\n\n### Version-Based Validation\n\n1. **Validate all proposed blockers for a target version**:\n   ```\n   /jira:validate-blockers 4.21\n   ```\n\n### Component-Based Validation\n\n2. **Validate blockers for a specific component and version**:\n   ```\n   /jira:validate-blockers 4.21 \"Hypershift\"\n   ```\n\n### Single Bug Validation\n\n3. **Validate a specific proposed blocker (version not required)**:\n   ```\n   /jira:validate-blockers --bug OCPBUGS-36846\n   ```\n\n## Output Format\n\nThe command outputs a blocker validation report:\n\n```markdown\n#  Release Blocker Validation Report\n**Components**: All | **Project**: OCPBUGS | **Proposed Blockers**: 5 | **Generated**: 2025-11-23\n\n## Summary\n-  **Recommend APPROVE**: 2\n-  **Recommend REJECT**: 1\n-  **Needs DISCUSSION**: 2\n\n---\n\n## Blocker Analysis\n\n### OCPBUGS-12345: Cluster install fails on AWS  APPROVE\n\n**Recommendation**: APPROVE - This bug meets blocker criteria\n\n**Criteria Matched**:\n-  Install/upgrade failure\n-  Affects all users\n-  No acceptable workaround\n\n**Justification**:\nInstall failures are strong blockers. This bug prevents cluster installation on AWS, affecting all users attempting AWS deployments. No workaround exists.\n\n**Suggested Action**: Approve as release blocker\n\n---\n\n### OCPBUGS-12346: UI button misaligned  REJECT\n\n**Recommendation**: REJECT - This bug does not meet blocker criteria\n\n**Criteria Matched**:\n-  Cosmetic/UI-only issue (not data loss/corruption/unavailability)\n-  Severity: Low (must be Important or higher)\n\n**Justification**:\nBugs with severity below Important are never blockers. This is a cosmetic issue with no functional impact.\n\n**Suggested Action**: Reject as release blocker, triage to appropriate sprint\n\n---\n\n## Next Steps\n1. Review APPROVE recommendations - add to blocker list\n2. Review REJECT recommendations - remove blocker status\n3. Discuss unclear cases in triage meeting\n```\n\n## Arguments\n\n**IMPORTANT**: Either `target-version` OR `--bug` must be provided. If neither is provided, the command will error out.\n\n### Core Arguments\n\n- **$1  target-version** *(required unless --bug is provided)*\n  Target release version to validate blockers for. Format: `X.Y` (e.g., `4.21`, `4.22`)\n\n  The implementation automatically:\n  - Expands version to search for both `X.Y` and `X.Y.0` in Target Version, Target Backport Versions, and Affected Version fields\n  - Excludes already-fixed bugs with status: Closed, Release Pending, Verified, ON_QA\n\n  Examples:\n  - `4.21` - Validates active proposed blockers for 4.21\n  - `4.22` - Validates active proposed blockers for 4.22\n\n  **Note**: Not required when `--bug` is provided.\n\n- **$2  component-filter** *(optional)*\n  Component name(s) to filter proposed blockers. Supports single or multiple (comma-separated) components.\n\n  Examples:\n  - `\"Hypershift\"` - Single component\n  - `\"Hypershift,Cluster Version Operator\"` - Multiple components\n\n  **Note**: Ignored if `--bug` is provided. Requires `target-version` to be specified.\n\n  Default: All components for the target version\n\n- **--bug** *(optional)*\n  Validate a single specific bug by its JIRA issue key.\n\n  When provided, analyzes only this bug and ignores both target-version and component-filter.\n\n  Examples:\n  - `--bug OCPBUGS-36846`\n\n  **Note**: When `--bug` is provided, target-version and component-filter are ignored.\n\n  Default: Not specified\n\n## Return Value\n\n- **Markdown Report**: Blocker validation report with recommendations\n- **Exit Code**:\n  - `0` - Success\n  - `1` - Error querying JIRA or analyzing bugs\n\n## Prerequisites\n\n- Jira MCP server must be configured (see [plugin README](../README.md))\n- MCP tools provide read-only access to JIRA APIs\n- No JIRA credentials required for read operations (public Red Hat JIRA issues)\n- Access to JIRA projects (OCPBUGS)\n- Permission to search and view issues in target projects"
              }
            ],
            "skills": [
              {
                "name": "JIRA Activity Type Categorizer",
                "description": "Detailed categorization logic for assigning JIRA tickets to activity type categories",
                "path": "plugins/jira/skills/categorize-activity-type/SKILL.md",
                "frontmatter": {
                  "name": "JIRA Activity Type Categorizer",
                  "description": "Detailed categorization logic for assigning JIRA tickets to activity type categories",
                  "command": "/jira:categorize-activity-type"
                },
                "content": "# JIRA Activity Type Categorizer - Implementation Guide\n\nYou are an expert JIRA ticket categorization specialist with deep knowledge of software development workflows, operational patterns, and engineering activities. This skill provides detailed categorization logic for the `/jira:categorize-activity-type` command.\n\n## When to Use This Skill\n\nThis skill is invoked automatically by the `/jira:categorize-activity-type` command to analyze JIRA tickets and assign activity type categories.\n\n## Prerequisites\n\n- MCP Jira server must be configured (see [plugin README](../../README.md))\n- MCP tools available: `mcp__atlassian__jira_get_issue`, `mcp__atlassian__jira_update_issue`\n- Access to JIRA instance with Activity Type custom field (`customfield_12320040`)\n\n## Activity Type Categories\n\n1. **Associate Wellness & Development**\n   - Professional growth, training, learning, team building, and employee development activities\n   - Examples: Conference attendance, training sessions, mentoring, onboarding, knowledge sharing\n\n2. **Incidents & Support**\n   - Production incidents, customer support, troubleshooting, emergency fixes, and reactive operational work\n   - Examples: Outages, customer escalations, hotfixes, emergency deployments, support tickets\n\n3. **Security & Compliance**\n   - Security vulnerabilities, compliance requirements, security patches, audits, and regulatory work\n   - Examples: CVE remediation, security audits, penetration testing, compliance reports\n\n4. **Quality / Stability / Reliability**\n   - Bug fixes, test improvements, reliability enhancements, technical debt reduction, and quality initiatives\n   - Examples: Bug fixes, flaky test resolution, crash fixes, error handling improvements, test coverage\n\n5. **Future Sustainability**\n   - Infrastructure improvements, developer experience enhancements, automation, tooling, and proactive technical investments\n   - Examples: Refactoring, CI/CD improvements, developer tooling, observability, technical debt cleanup\n\n6. **Product / Portfolio Work**\n   - Feature development, product enhancements, new capabilities, and planned product roadmap items\n   - Examples: New features, user-facing enhancements, MVPs, product requirements, roadmap items\n\n## Categorization Methodology\n\n### 1. Primary Analysis Sources (Priority Order)\n\nAnalyze these data sources in order of importance:\n\n1. **Ticket Summary (title)** - Often contains key indicator words and intent\n2. **Ticket Description** - Provides detailed context, acceptance criteria, and technical details\n3. **Issue Type** - Bug, Story, Task, Vulnerability, Weakness, Epic, Sub-task\n4. **Parent Epic/Story details** - Inherit context from parent when ticket is a child\n5. **Labels and metadata** - Additional classification hints (e.g., \"technical-debt\", \"security\")\n\n### 2. Issue Type Heuristics\n\nApply these default mappings based on issue type, then validate/override with keyword analysis:\n\n| Issue Type | Default Category | Initial Confidence | Override Conditions |\n|------------|------------------|-------------------|---------------------|\n| Vulnerability | Security & Compliance | High | Rarely override |\n| Weakness | Security & Compliance | Medium-High | Override if clearly not security |\n| Bug (with security keywords) | Security & Compliance | High | Never override |\n| Bug (standard) | Quality / Stability / Reliability | High | Override if infrastructure/sustainability focus |\n| Story (product-focused) | Product / Portfolio Work | Medium | Override if operational/infrastructure |\n| Story (operational) | Future Sustainability | Medium | Override if customer-facing |\n| Task (Epic child) | Inherit from Epic | Medium | Override with strong keywords |\n| Task (standalone) | Analyze keywords | Low | Always analyze |\n| Epic | Analyze keywords + children | Low | Always analyze |\n\n**Critical rule:** Security-related content ALWAYS takes precedence over other categorizations.\n\n### 3. Keyword Indicators\n\nScan the combined text (summary + description) for these keyword patterns:\n\n#### Incidents & Support\n**Primary keywords:** `incident`, `outage`, `customer issue`, `support ticket`, `emergency`, `hotfix`, `production down`, `urgent fix`\n\n**Contextual phrases:**\n- \"customer reported\"\n- \"production failure\"\n- \"emergency deployment\"\n- \"service degradation\"\n- \"immediate attention required\"\n\n**Scoring:** 3+ matches = High confidence\n\n#### Security & Compliance\n**Primary keywords:** `CVE`, `vulnerability`, `security patch`, `compliance`, `audit`, `penetration test`, `authentication`, `authorization`, `privilege escalation`, `XSS`, `SQL injection`\n\n**Patterns:**\n- CVE identifiers (CVE-YYYY-NNNNN)\n- Security advisory references\n- OWASP mentions\n- Compliance framework names (SOC2, HIPAA, PCI-DSS)\n\n**Scoring:** 1+ match = High confidence (security is critical)\n\n#### Quality / Stability / Reliability\n**Primary keywords:** `bug`, `flaky test`, `memory leak`, `crash`, `error handling`, `test coverage`, `intermittent failure`, `race condition`, `deadlock`, `timeout`, `retry logic`\n\n**Contextual phrases:**\n- \"fix crashes\"\n- \"improve stability\"\n- \"reduce flakiness\"\n- \"handle errors\"\n- \"prevent failures\"\n\n**Scoring:** 2+ matches = High confidence\n\n#### Future Sustainability\n**Primary keywords:** `refactor`, `technical debt`, `developer experience`, `CI/CD`, `automation`, `tooling`, `infrastructure`, `observability`, `monitoring`, `alerting`, `logging`, `performance optimization`, `build time`\n\n**Contextual phrases:**\n- \"improve development workflow\"\n- \"reduce build time\"\n- \"enhance developer productivity\"\n- \"automate manual process\"\n- \"infrastructure as code\"\n- \"observability improvements\"\n\n**Scoring:** 3+ matches = High confidence\n\n#### Product / Portfolio Work\n**Primary keywords:** `feature`, `enhancement`, `capability`, `user story`, `requirement`, `MVP`, `roadmap`, `customer request`, `new functionality`, `user-facing`\n\n**Contextual phrases:**\n- \"new functionality\"\n- \"user-facing change\"\n- \"product requirement\"\n- \"customer-requested feature\"\n- \"roadmap item\"\n\n**Scoring:** 2+ matches = Medium-High confidence\n\n#### Associate Wellness & Development\n**Primary keywords:** `training`, `learning`, `conference`, `onboarding`, `mentoring`, `knowledge sharing`, `team building`, `career development`, `workshop`, `certification`\n\n**Contextual phrases:**\n- \"attend conference\"\n- \"training session\"\n- \"professional development\"\n- \"team offsite\"\n- \"knowledge transfer\"\n\n**Scoring:** 1+ match = High confidence (usually clear and unambiguous)\n\n### 4. Parent Context Inheritance\n\nWhen a ticket is a subtask or linked to an Epic, apply inheritance logic:\n\n#### Inheritance Process\n\n1. **Fetch Parent Epic/Story:**\n   ```python\n   if parent_key:\n       parent_issue = mcp__atlassian__jira_get_issue(\n           issue_key=parent_key,\n           fields=\"summary,customfield_12320040\"\n       )\n       parent_summary = parent_issue[\"fields\"].get(\"summary\", \"\")\n       parent_activity_type = parent_issue[\"fields\"].get(\"customfield_12320040\", {}).get(\"value\", None)\n   ```\n\n2. **Evaluate Parent Category:**\n   - **If parent has Activity Type already set:**\n     - Inherit parent category if ticket has no strong contradicting keywords\n     - Set confidence to Medium\n     - Note in reasoning: \"Inherited from parent Epic {parent_key}\"\n\n   - **If parent Activity Type not set, analyze parent summary:**\n     - Apply keyword scanning to parent summary (e.g., \"Security Remediation Q4\"  Security & Compliance)\n     - Use inferred category with Low-Medium confidence\n     - Note in reasoning: \"Inferred from parent Epic title\"\n\n3. **Override Parent When Appropriate:**\n   - If ticket has 3+ strong keywords contradicting parent  override with High confidence\n   - If ticket is clearly different nature than parent  override with Medium confidence\n   - Example: Parent Epic = \"Product Feature X\" (Product Work), but child task = \"Set up CI pipeline\" (Future Sustainability)\n\n#### Parent Context Examples\n\n**Strong inheritance:**\n- Parent Epic: \"Security Remediation Q4\"  Child tasks likely \"Security & Compliance\"\n- Parent Epic: \"Developer Experience Improvements\"  Child tasks likely \"Future Sustainability\"\n\n**Override parent:**\n- Parent Epic: \"User Dashboard Feature\" (Product Work)\n- Child Task: \"Fix crash in dashboard rendering\"  Override to \"Quality / Stability / Reliability\"\n\n### 5. Ambiguity Resolution\n\nWhen multiple categories seem applicable, use this priority order:\n\n#### Priority Hierarchy (Highest to Lowest)\n\n1. **Security & Compliance** - ALWAYS wins if security-related\n2. **Incidents & Support** - Takes precedence for production emergencies\n3. **Explicit Keyword Evidence** - Strong keyword matches override issue type\n4. **Issue Type Heuristic** - Default to type-based categorization\n5. **Parent Inheritance** - Use parent Epic category when keywords weak\n6. **Low Confidence + User Clarification** - When completely unclear, report Low confidence\n\n**Tie-breaking:** When multiple categories have equal keyword scores, use the priority order above to select deterministically. For example, if both \"Security & Compliance\" and \"Quality / Stability / Reliability\" have 2 keyword matches each, choose \"Security & Compliance\" as it has higher priority.\n\n#### Common Ambiguity Cases\n\n##### Case 1: Bug that improves infrastructure\n- Example: \"Fix slow build times in CI pipeline\"\n- Resolution: **Future Sustainability** (primary intent is infrastructure improvement, not fixing user-facing bug)\n- Confidence: Medium-High\n\n##### Case 2: Feature that addresses security\n- Example: \"Add two-factor authentication support\"\n- Resolution: **Security & Compliance** (security always wins)\n- Confidence: High\n\n##### Case 3: Task with no Epic and minimal description\n- Example: \"Update documentation\"\n- Resolution: Analyze what documentation (user-facing  Product, developer  Sustainability)\n- Confidence: Low or Medium (depends on available context)\n\n##### Case 4: Operational story vs. product story\n- Example: \"Improve database query performance\"\n- Resolution: If user-facing performance  **Product Work**, if backend optimization  **Future Sustainability**\n- Confidence: Medium (requires careful reading)\n\n## Implementation Steps\n\n### Step 1: Extract and Normalize Ticket Data\n\n```python\n# Extract core fields\nsummary = issue_data[\"fields\"][\"summary\"]\ndescription = issue_data[\"fields\"].get(\"description\", \"\") or \"\"\nissue_type = issue_data[\"fields\"][\"issuetype\"][\"name\"]\nlabels = issue_data[\"fields\"].get(\"labels\", [])\nparent_key = issue_data[\"fields\"].get(\"parent\", {}).get(\"key\", None)\ncomponents = [c[\"name\"] for c in issue_data[\"fields\"].get(\"components\", [])]\ncurrent_activity_type = issue_data[\"fields\"].get(\"customfield_12320040\", {}).get(\"value\", None)\n\n# Normalize text for keyword matching\ncombined_text = (summary + \" \" + description).lower()\n```\n\n### Step 2: Apply Issue Type Heuristic\n\n```python\ninitial_category = None\ninitial_confidence = \"Low\"\n\nif issue_type in [\"Vulnerability\", \"Weakness\"]:\n    initial_category = \"Security & Compliance\"\n    initial_confidence = \"High\" if issue_type == \"Vulnerability\" else \"Medium-High\"\n\nelif issue_type == \"Bug\":\n    # Check for security keywords first\n    security_keywords = [\"cve\", \"security\", \"vulnerability\", \"exploit\", \"xss\", \"injection\", \"privilege\"]\n    if any(kw in combined_text for kw in security_keywords):\n        initial_category = \"Security & Compliance\"\n        initial_confidence = \"High\"\n    else:\n        initial_category = \"Quality / Stability / Reliability\"\n        initial_confidence = \"High\"\n\nelif issue_type == \"Story\":\n    # Default to Product Work, but will be validated by keywords\n    initial_category = \"Product / Portfolio Work\"\n    initial_confidence = \"Medium\"\n\nelif issue_type == \"Task\":\n    # Tasks need more analysis - check parent or keywords\n    initial_category = None  # Will be determined by parent or keywords\n    initial_confidence = \"Low\"\n\nelif issue_type == \"Epic\":\n    # Epics need keyword analysis\n    initial_category = None\n    initial_confidence = \"Low\"\n```\n\n### Step 3: Keyword Scanning and Scoring\n\n```python\n# Define keyword sets\nkeyword_categories = {\n    \"Incidents & Support\": [\n        \"incident\", \"outage\", \"customer issue\", \"support ticket\", \"emergency\",\n        \"hotfix\", \"production down\", \"urgent fix\", \"customer reported\",\n        \"service degradation\"\n    ],\n    \"Security & Compliance\": [\n        \"cve\", \"vulnerability\", \"security patch\", \"compliance\", \"audit\",\n        \"penetration test\", \"authentication\", \"authorization\", \"privilege\",\n        \"xss\", \"injection\", \"exploit\"\n    ],\n    \"Quality / Stability / Reliability\": [\n        \"bug\", \"flaky test\", \"memory leak\", \"crash\", \"error handling\",\n        \"test coverage\", \"intermittent\", \"race condition\", \"deadlock\",\n        \"timeout\", \"retry logic\", \"stability\"\n    ],\n    \"Future Sustainability\": [\n        \"refactor\", \"technical debt\", \"developer experience\", \"ci/cd\",\n        \"automation\", \"tooling\", \"infrastructure\", \"observability\",\n        \"monitoring\", \"alerting\", \"performance optimization\", \"build time\"\n    ],\n    \"Product / Portfolio Work\": [\n        \"feature\", \"enhancement\", \"capability\", \"user story\", \"requirement\",\n        \"mvp\", \"roadmap\", \"customer request\", \"new functionality\", \"user-facing\"\n    ],\n    \"Associate Wellness & Development\": [\n        \"training\", \"learning\", \"conference\", \"onboarding\", \"mentoring\",\n        \"knowledge sharing\", \"team building\", \"career development\",\n        \"workshop\", \"certification\"\n    ]\n}\n\n# Count keyword matches for each category\nkeyword_scores = {}\nfor category, keywords in keyword_categories.items():\n    score = sum(1 for kw in keywords if kw in combined_text)\n    keyword_scores[category] = score\n\n# Find dominant category based on keywords with tie-breaking\n# Priority order for tie-breaking (highest to lowest)\ncategory_priority = [\n    \"Security & Compliance\",\n    \"Incidents & Support\",\n    \"Quality / Stability / Reliability\",\n    \"Future Sustainability\",\n    \"Product / Portfolio Work\",\n    \"Associate Wellness & Development\"\n]\n\n# Find max score\nmax_score = max(keyword_scores.values()) if keyword_scores else 0\n\n# Find all categories with max score, then select by priority\ncandidates = [cat for cat in category_priority if keyword_scores.get(cat, 0) == max_score]\ndominant_category = candidates[0] if candidates else \"Product / Portfolio Work\"\ndominant_score = max_score\n```\n\n### Step 4: Determine Final Category\n\n```python\nfinal_category = initial_category\nfinal_confidence = initial_confidence\nreasoning_notes = []\n\n# Security ALWAYS wins if keywords present\nif keyword_scores[\"Security & Compliance\"] >= 1:\n    final_category = \"Security & Compliance\"\n    final_confidence = \"High\"\n    reasoning_notes.append(\"Security-related content takes precedence\")\n\n# Strong keyword evidence (3+ matches)\nelif dominant_score >= 3:\n    final_category = dominant_category\n    final_confidence = \"High\"\n    reasoning_notes.append(f\"Strong keyword evidence ({dominant_score} matches)\")\n\n# Moderate keyword evidence (1-2 matches)\nelif dominant_score >= 1 and dominant_category != initial_category:\n    final_category = dominant_category\n    final_confidence = \"Medium\"\n    reasoning_notes.append(f\"Keyword evidence ({dominant_score} matches) overrides issue type\")\n\n# No strong keywords, use issue type heuristic\nelif initial_category:\n    final_category = initial_category\n    final_confidence = initial_confidence\n    reasoning_notes.append(\"Based on issue type heuristic\")\n\n# No clear category, check parent\nelif parent_key:\n    # Attempt parent inheritance (see Step 5)\n    pass\n\n# Still no category\nelse:\n    final_category = \"Product / Portfolio Work\"  # Safe default\n    final_confidence = \"Low\"\n    reasoning_notes.append(\"Insufficient evidence, using default\")\n```\n\n### Step 5: Parent Inheritance (If Needed)\n\n```python\nif not final_category and parent_key:\n    try:\n        parent_issue = mcp__atlassian__jira_get_issue(\n            issue_key=parent_key,\n            fields=\"summary,customfield_12320040\"\n        )\n        parent_summary = parent_issue[\"fields\"].get(\"summary\", \"\")\n        parent_activity_type = parent_issue[\"fields\"].get(\"customfield_12320040\", {}).get(\"value\", None)\n\n        if parent_activity_type:\n            # Parent has Activity Type already set\n            final_category = parent_activity_type\n            final_confidence = \"Medium\"\n            reasoning_notes.append(f\"Inherited from parent Epic {parent_key}\")\n        elif parent_summary:\n            # Parent has no Activity Type, analyze summary for keywords\n            parent_text = parent_summary.lower()\n            for category, keywords in keyword_categories.items():\n                score = sum(1 for kw in keywords if kw in parent_text)\n                if score >= 1:\n                    final_category = category\n                    final_confidence = \"Low\" if score == 1 else \"Medium\"\n                    reasoning_notes.append(f\"Inferred from parent Epic title: {parent_summary}\")\n                    break\n    except Exception as e:\n        # Parent fetch failed, log and continue without inheritance\n        reasoning_notes.append(f\"Could not fetch parent Epic {parent_key}: {str(e)}\")\n```\n\n### Step 6: Confidence Adjustment\n\n```python\n# Increase confidence\nif keyword_scores.get(final_category, 0) >= 3:\n    final_confidence = \"High\"\nelif issue_type in [\"Vulnerability\"] and final_category == \"Security & Compliance\":\n    final_confidence = \"High\"\n\n# Decrease confidence\nif not description or len(description) < 50:\n    if final_confidence == \"High\":\n        final_confidence = \"Medium\"\n    reasoning_notes.append(\"Limited context due to short/missing description\")\n\nif keyword_scores.get(final_category, 0) == 0 and not parent_key:\n    final_confidence = \"Low\"\n    reasoning_notes.append(\"No supporting keyword evidence\")\n```\n\n### Step 7: Generate Structured Output\n\n```python\n# Collect evidence\nevidence_points = []\nevidence_points.append(f\"Issue Type: {issue_type}\")\n\nif keyword_scores.get(final_category, 0) > 0:\n    # Find which keywords matched\n    matched_keywords = [kw for kw in keyword_categories[final_category] if kw in combined_text]\n    evidence_points.append(f\"Summary/Description contains: {', '.join(matched_keywords[:3])}\")\n\nif parent_key:\n    evidence_points.append(f\"Parent Epic: {parent_key}\")\n\nif labels:\n    evidence_points.append(f\"Labels: {', '.join(labels[:3])}\")\n\n# Build reasoning\nreasoning = f\"{' '.join(reasoning_notes)}. \"\n\nif issue_type == \"Bug\" and final_category == \"Quality / Stability / Reliability\":\n    reasoning += \"This is a standard bug fix addressing system stability and reliability. \"\nelif issue_type == \"Vulnerability\":\n    reasoning += \"This is a security vulnerability that must be addressed through security remediation processes. \"\nelif final_category == \"Future Sustainability\":\n    reasoning += \"This work focuses on improving infrastructure, tooling, or developer experience for long-term sustainability. \"\nelif final_category == \"Product / Portfolio Work\":\n    reasoning += \"This work delivers user-facing features or product enhancements aligned with the roadmap. \"\n\n# Format output\noutput = f\"\"\"\nActivity Type: {final_category}\nConfidence: {final_confidence}\n\nReasoning: {reasoning}\n\nKey Evidence:\n{chr(10).join('- ' + point for point in evidence_points)}\n\"\"\"\n\nreturn {\n    \"category\": final_category,\n    \"confidence\": final_confidence,\n    \"reasoning\": reasoning,\n    \"evidence\": evidence_points,\n    \"output\": output\n}\n```\n\n## Quality Standards\n\n- **Be decisive but honest** - Choose a category, but clearly state confidence level\n- **Always cite evidence** - Reference specific ticket data in reasoning\n- **Consider multiple sources** - Don't rely on a single indicator\n- **Prioritize security** - Security-related content always takes precedence\n- **Never invent data** - Only use information present in the ticket\n- **Explain uncertainty** - If confidence is Low, explain why and what's missing\n\n## Error Handling\n\n### Missing or Incomplete Data\n\n**Missing description:**\n```python\nif not description or len(description) < 20:\n    # Lower confidence\n    if final_confidence == \"High\":\n        final_confidence = \"Medium\"\n    # Note in reasoning\n    reasoning_notes.append(\"Limited context due to missing/short description\")\n```\n\n**No parent Epic (for Tasks):**\n```python\nif issue_type == \"Task\" and not parent_key:\n    # Rely heavily on keywords\n    if keyword_scores.get(final_category, 0) == 0:\n        final_confidence = \"Low\"\n        reasoning_notes.append(\"No parent Epic context available\")\n```\n\n**Unknown issue type:**\n```python\nif issue_type not in [\"Bug\", \"Story\", \"Task\", \"Epic\", \"Vulnerability\", \"Weakness\"]:\n    # Treat as generic task\n    initial_category = None\n    initial_confidence = \"Low\"\n    reasoning_notes.append(f\"Uncommon issue type: {issue_type}\")\n```\n\n### MCP Errors\n\n**Parent fetch failure:**\n```python\ntry:\n    parent_issue = mcp__atlassian__jira_get_issue(\n        issue_key=parent_key,\n        fields=\"summary,customfield_12320040\"\n    )\nexcept Exception as e:\n    # Continue without parent context\n    reasoning_notes.append(\"Could not fetch parent Epic details\")\n```\n\n## Output Examples\n\n### Example 1: High Confidence Bug Fix\n\n```text\nActivity Type: Quality / Stability / Reliability\nConfidence: High\n\nReasoning: Based on issue type heuristic. This is a Bug issue type focused on fixing a\nmemory leak in the scanner component. Memory leaks directly impact system stability and\nreliability. The description mentions \"intermittent crashes\" and \"resource exhaustion,\"\nwhich are classic reliability concerns. No security implications mentioned, and this is\na proactive fix rather than a customer-facing incident.\n\nKey Evidence:\n- Issue Type: Bug\n- Summary/Description contains: memory leak, crash, intermittent\n- No security keywords present\n```\n\n### Example 2: Medium Confidence with Parent Inheritance\n\n```text\nActivity Type: Future Sustainability\nConfidence: Medium\n\nReasoning: Inherited from parent Epic ROX-25641. Keyword evidence (2 matches) supports\ncategorization. This task is part of a larger developer experience improvement initiative\nfocused on enhancing CI/CD pipeline performance.\n\nKey Evidence:\n- Issue Type: Task\n- Summary/Description contains: automation, build time\n- Parent Epic: ROX-25641\n- Labels: technical-debt\n```\n\n### Example 3: Security Vulnerability (Always High)\n\n```text\nActivity Type: Security & Compliance\nConfidence: High\n\nReasoning: Security-related content takes precedence. This is a Vulnerability issue type\nrequiring immediate security remediation. The ticket references CVE-2024-12345 and\ndescribes a privilege escalation vulnerability in the authentication module.\n\nKey Evidence:\n- Issue Type: Vulnerability\n- Summary/Description contains: CVE-2024-12345, privilege escalation, authentication\n- Labels: security\n```\n\n### Example 4: Low Confidence (Needs Clarification)\n\n```text\nActivity Type: Product / Portfolio Work\nConfidence: Low\n\nReasoning: Insufficient evidence, using default. The ticket has minimal description and\nno clear keyword indicators. Issue type is Task with no parent Epic context available.\n\nKey Evidence:\n- Issue Type: Task\n- Summary: \"Update configuration\"\n- No parent Epic context available\n- No supporting keyword evidence\n```\n\nNote: For Low confidence, the command should always prompt the user for confirmation\nbefore applying, even with `--auto-apply` flag."
              },
              {
                "name": "CNTRLPLANE Jira Conventions",
                "description": "Jira conventions for the CNTRLPLANE project used by OpenShift teams",
                "path": "plugins/jira/skills/cntrlplane/SKILL.md",
                "frontmatter": {
                  "name": "CNTRLPLANE Jira Conventions",
                  "description": "Jira conventions for the CNTRLPLANE project used by OpenShift teams"
                },
                "content": "# CNTRLPLANE Jira Conventions\n\nThis skill provides conventions and requirements for creating Jira issues in the CNTRLPLANE project, which is used by various OpenShift teams for feature development, epics, stories, and tasks.\n\n## When to Use This Skill\n\nUse this skill when creating Jira items in the CNTRLPLANE project:\n- **Project: CNTRLPLANE** - Features, Epics, Stories, Tasks for OpenShift teams\n- **Issue Types: Story, Epic, Feature, Task**\n\nThis skill is automatically invoked by the `/jira:create` command when the project_key is \"CNTRLPLANE\".\n\n## Project Information\n\n### CNTRLPLANE Project\n**Full name:** Red Hat OpenShift Control Planes\n\n**Key:** CNTRLPLANE\n\n**Used for:** Features, Epics, Stories, Tasks, Spikes\n\n**Used by:** Multiple OpenShift teams (HyperShift, Cluster Infrastructure, Networking, Storage, etc.)\n\n## Version Requirements\n\n**Note:** Universal requirements (Security Level: Red Hat Employee, Labels: ai-generated-jira) are defined in the `/jira:create` command and automatically applied to all tickets.\n\n### Target Version (customfield_12319940)\n\n**Status:** OPTIONAL (many issues in CNTRLPLANE have null target version)\n\n**Recommendation:** **Prompt the user** for target version if needed, rather than assuming a default.\n\n**Prompt:** \"Which OpenShift version should this target? (e.g., 4.22, openshift 4.22, OCP 4.22) or press Enter to skip\"\n\n### Version Input Normalization\n\nUsers may specify versions in various formats. Normalize all inputs to the Jira format `openshift-X.Y`:\n\n| User Input | Normalized Output |\n|------------|-------------------|\n| `4.21` | `openshift-4.21` |\n| `4.22.0` | `openshift-4.22` |\n| `openshift 4.23` | `openshift-4.23` |\n| `openshift-4.21` | `openshift-4.21` |\n| `OCP 4.22` | `openshift-4.22` |\n| `ocp 4.21` | `openshift-4.21` |\n| `OpenShift 4.23` | `openshift-4.23` |\n\n**Normalization rules:**\n1. Convert to lowercase\n2. Remove \"ocp\" or \"openshift\" prefix (with or without space/hyphen)\n3. Extract version number (X.Y or X.Y.Z  X.Y)\n4. Prepend \"openshift-\"\n\n### Setting Target Version in MCP\n\n**If target version is set:**\n\n1. **First, fetch available versions:**\n   ```python\n   versions = mcp__atlassian__jira_get_project_versions(project_key=\"CNTRLPLANE\")\n   ```\n\n2. **Find the version ID** for the normalized version name (e.g., \"openshift-4.22\")\n\n3. **Use correct MCP format** (array of version objects with ID):\n   ```python\n   \"customfield_12319940\": [{\"id\": \"VERSION_ID\"}]  # e.g., openshift-4.22\n   ```\n\n**IMPORTANT:** Do NOT use string format like `\"openshift-4.22\"` - this will fail. Must use array with version ID.\n\n**Never set:**\n- Fix Version/s (`fixVersions`) - This is managed by the release team\n\n### Version Handling Workflow\n\nWhen user specifies a version (via `--version` flag or prompt):\n1. **Normalize** the input to `openshift-X.Y` format\n2. **Fetch** available versions using `mcp__atlassian__jira_get_project_versions`\n3. **Find** the matching version ID\n4. **If version doesn't exist**, suggest closest match or ask user to confirm\n5. **Use array format** with version ID: `[{\"id\": \"VERSION_ID\"}]`\n\n## Parent Linking in CNTRLPLANE\n\n**See:** `/jira:create` command documentation for the complete \"Issue Hierarchy and Parent Linking\" reference, including field mapping, MCP code examples, and fallback strategies.\n\n### Quick Reference for CNTRLPLANE\n\nCNTRLPLANE uses different fields for different parent relationships:\n\n| Creating | Parent Type | Field to Use | Value Format |\n|----------|-------------|--------------|--------------|\n| Story | Epic | `customfield_12311140` (Epic Link) | `\"CNTRLPLANE-123\"` (string) |\n| Task | Epic | `customfield_12311140` (Epic Link) | `\"CNTRLPLANE-123\"` (string) |\n| Epic | Feature | `customfield_12313140` (Parent Link) | `\"CNTRLPLANE-123\"` (string) |\n\n** CRITICAL:**\n- Story/Task  Epic uses **Epic Link** (`customfield_12311140`)\n- Epic  Feature uses **Parent Link** (`customfield_12313140`)\n- Both fields take STRING values (issue key), NOT objects\n- The standard `parent` field does NOT work\n\n### CNTRLPLANE-Specific Field IDs\n\n| Field | Custom Field ID | Format |\n|-------|-----------------|--------|\n| Epic Link (for stories/tasks) | `customfield_12311140` | String: `\"CNTRLPLANE-123\"` |\n| Parent Link (for epicsfeatures) | `customfield_12313140` | String: `\"CNTRLPLANE-123\"` |\n| Epic Name (required for epics) | `customfield_12311141` | String: same as summary |\n| Target Version | `customfield_12319940` | Array: `[{\"id\": \"12448830\"}]` |\n\n### Implementation\n\nFollow the implementation strategy documented in `/jira:create` command:\n1. **Pre-validate** the parent exists and is the correct type\n2. **Attempt creation** with the appropriate parent field\n3. **Use fallback** if creation fails (create without link, then update)\n4. **Report outcome** to user\n\n## Component Requirements\n\n**IMPORTANT:** Component requirements are **team-specific**.\n\nSome teams require specific components, while others do not. The CNTRLPLANE skill does NOT enforce component selection.\n\n**Team-specific component handling:**\n- Teams may have their own skills that define required components\n- For example, HyperShift team uses `hypershift` skill for component selection\n- Other teams may use different components based on their structure\n\n**If component is not specified:**\n- Prompt user: \"Does this issue require a component? (optional)\"\n- If yes, ask user to specify component name\n- If no, proceed without component\n\n## Issue Type Requirements\n\n**Note:** Issue type templates and best practices are defined in type-specific skills (create-story, create-epic, create-feature, create-task).\n\n### Stories\n- Must include acceptance criteria\n- May link to parent Epic (use `--parent` flag)\n\n### Epics\n- **Epic Name field required:** `customfield_epicname` must be set (same value as summary)\n- May link to parent Feature (use `--parent` flag)\n\n### Features\n- Should include market problem and success criteria (see `create-feature` skill)\n\n### Tasks\n- May link to parent Story or Epic (use `--parent` flag)\n\n**Note:** Security validation (credential scanning) is defined in the `/jira:create` command and automatically applied to all tickets.\n\n## MCP Tool Integration\n\n### For CNTRLPLANE Stories\n\n**Basic story (no epic link):**\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<concise story title>\",  # NOT full user story format\n    issue_type=\"Story\",\n    description=\"<formatted description with full user story and AC>\",\n    components=\"<component name>\",  # if required by team\n    additional_fields={\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n**Story linked to epic:**\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<concise story title>\",  # NOT full user story format\n    issue_type=\"Story\",\n    description=\"<formatted description with full user story and AC>\",\n    components=\"<component name>\",  # if required by team\n    additional_fields={\n        \"customfield_12311140\": \"<epic-key>\",  # Epic Link (e.g., \"CNTRLPLANE-456\")\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### For CNTRLPLANE Epics\n\n**Basic epic (no parent feature):**\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<concise epic title>\",\n    issue_type=\"Epic\",\n    description=\"<epic description with scope and AC>\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        \"customfield_12311141\": \"<epic name>\",  # required, same as summary\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n**Epic linked to parent feature:**\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<concise epic title>\",\n    issue_type=\"Epic\",\n    description=\"<epic description with scope and AC>\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        \"customfield_12311141\": \"<epic name>\",  # required, same as summary\n        \"customfield_12313140\": \"CNTRLPLANE-123\",  # Parent Link - feature key as STRING\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### For CNTRLPLANE Features\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<concise feature title>\",\n    issue_type=\"Feature\",\n    description=\"<feature description with market problem and success criteria>\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n        # Target version is optional - omit unless specifically required\n    }\n)\n```\n\n### For CNTRLPLANE Tasks\n\n**Task linked to epic (via Epic Link):**\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<task summary>\",\n    issue_type=\"Task\",\n    description=\"<task description with what/why/AC>\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        \"customfield_12311140\": \"CNTRLPLANE-456\",  # Epic Link (if linking to epic)\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n**Note:** If you need to link a task to a parent story, use Epic Link field (`customfield_12311140`) with the story key.\n\n### Field Mapping Reference\n\n| Requirement | MCP Parameter | Value | Required? |\n|-------------|---------------|-------|-----------|\n| Project | `project_key` | `\"CNTRLPLANE\"` | Yes |\n| Issue Type | `issue_type` | `\"Story\"`, `\"Epic\"`, `\"Feature\"`, `\"Task\"` | Yes |\n| Summary | `summary` | Concise title (5-10 words), NOT full user story | Yes |\n| Description | `description` | Formatted template (contains full user story) | Yes |\n| Component | `components` | Team-specific component name | Varies by team |\n| Target Version | `additional_fields.customfield_12319940` | Array: `[{\"id\": \"12448830\"}]` **Recommend omitting** | No |\n| Labels | `additional_fields.labels` | `[\"ai-generated-jira\"]` | Yes |\n| Security Level | `additional_fields.security` | `{\"name\": \"Red Hat Employee\"}` | Yes |\n| Epic Link (storiesepics) | `additional_fields.customfield_12311140` | Epic key as string: `\"CNTRLPLANE-123\"` | No |\n| Epic Name (epics only) | `additional_fields.customfield_epicname` | Same as summary | Yes (epics) |\n| Parent Link (epicsfeatures) | `additional_fields.parent` | `{\"key\": \"FEATURE-123\"}` | No |\n\n## Interactive Prompts\n\n**Note:** Detailed prompts for each issue type are defined in type-specific skills (create-story, create-epic, create-feature, create-task).\n\n**CNTRLPLANE-specific prompts:**\n- **Target version** (optional): \"Which OpenShift version should this target? (e.g., 4.22, openshift 4.22, OCP 4.22) or press Enter to skip\"\n- **Component** (if required by team): Defer to team-specific skills\n- **Parent link** (for epics/tasks): \"Link to parent Feature/Epic?\" (optional)\n\n## Examples\n\n**Note:** All examples automatically apply universal requirements (Security: Red Hat Employee, Labels: ai-generated-jira) as defined in `/jira:create` command.\n\n### Create CNTRLPLANE Story\n\n```bash\n/jira:create story CNTRLPLANE \"Enable pod disruption budgets for control plane\"\n```\n\n**Prompts:**\n- Target version (optional): User prompted, input normalized (e.g., \"4.22\"  \"openshift-4.22\")\n- See `create-story` skill for story-specific prompts\n\n### Create CNTRLPLANE Epic\n\n```bash\n/jira:create epic CNTRLPLANE \"Improve cluster lifecycle management\"\n```\n\n**CNTRLPLANE-specific requirements:**\n- Epic Name: Same as summary (required field)\n\n**Prompts:**\n- Target version (optional): User prompted, input normalized\n- See `create-epic` skill for epic-specific prompts\n\n### Create CNTRLPLANE Feature\n\n```bash\n/jira:create feature CNTRLPLANE \"Advanced observability capabilities\"\n```\n\n**Prompts:**\n- Target version (optional): User prompted, input normalized\n- See `create-feature` skill for feature-specific prompts\n\n### Create CNTRLPLANE Task\n\n```bash\n/jira:create task CNTRLPLANE \"Refactor cluster controller reconciliation logic\"\n```\n\n**Prompts:**\n- Target version (optional): User prompted, input normalized\n- See `create-task` skill for task-specific prompts\n\n## Error Handling\n\n### Invalid Version\n\n**Scenario:** User specifies a version that doesn't exist.\n\n**Action:**\n1. Use `mcp__atlassian__jira_get_project_versions` to fetch available versions\n2. Suggest closest match: \"Version 'openshift-4.21.5' not found. Did you mean 'openshift-4.21.0'?\"\n3. Show available versions: \"Available: openshift-4.20.0, openshift-4.21.0, openshift-4.22.0\"\n4. Wait for confirmation or correction\n\n### Component Required But Missing\n\n**Scenario:** Team requires component, but user didn't specify.\n\n**Action:**\n1. If team skill detected required components, show options\n2. Otherwise, generic prompt: \"Does this issue require a component?\"\n3. If yes, ask user to specify component name\n4. If no, proceed without component\n\n### Sensitive Data Detected\n\n**Scenario:** Credentials or secrets found in description.\n\n**Action:**\n1. STOP issue creation immediately\n2. Inform user: \"I detected potential credentials in the description.\"\n3. Show general location: \"Found in: Technical details section\"\n4. Do NOT echo the sensitive data back\n5. Suggest: \"Please use placeholder values like 'YOUR_API_KEY'\"\n6. Wait for user to provide sanitized content\n\n### Parent Issue Not Found\n\n**Scenario:** User specifies `--parent CNTRLPLANE-999` but issue doesn't exist.\n\n**Action:**\n1. Attempt to fetch parent issue using `mcp__atlassian__jira_get_issue`\n2. If not found: \"Parent issue CNTRLPLANE-999 not found. Would you like to proceed without a parent?\"\n3. Offer options:\n   - Proceed without parent\n   - Specify different parent\n   - Cancel creation\n\n### MCP Tool Failure\n\n**Scenario:** MCP tool returns an error.\n\n**Action:**\n1. Parse error message for actionable information\n2. Common errors:\n   - **\"Field 'component' is required\"**  Prompt for component (team-specific requirement)\n   - **\"Permission denied\"**  User may lack permissions\n   - **\"Version not found\"**  Use version error handling above\n   - **\"Issue type not available\"**  Project may not support this issue type\n3. Provide clear next steps\n4. Offer to retry after corrections\n\n### Wrong Issue Type\n\n**Scenario:** User tries to create a bug in CNTRLPLANE.\n\n**Action:**\n1. Inform user: \"Bugs should be created in OCPBUGS. CNTRLPLANE is for stories/epics/features/tasks.\"\n2. Suggest: \"Would you like to create this as a story in CNTRLPLANE, or as a bug in OCPBUGS?\"\n3. Wait for user decision\n\n**Note:** Jira description formatting (Wiki markup) is defined in the `/jira:create` command.\n\n## Team-Specific Extensions\n\nTeams using CNTRLPLANE may have additional team-specific requirements defined in separate skills:\n\n- **HyperShift team:** Uses `hypershift` skill for component selection (HyperShift / ARO, HyperShift / ROSA, HyperShift)\n- **Other teams:** May define their own skills with team-specific components and conventions\n\nTeam-specific skills are invoked automatically when team keywords are detected in the summary or when specific components are mentioned.\n\n## Workflow Summary\n\nWhen `/jira:create` is invoked for CNTRLPLANE:\n\n1.  **CNTRLPLANE skill loaded:** Applies project-specific conventions\n2.  **Apply CNTRLPLANE requirements:**\n   - Epic name field (for epics)\n3.  **Check for team-specific skills:** If team keywords detected, invoke team skill (e.g., `hypershift`)\n4.  **Interactive prompts:** Collect missing information:\n   - Target version (optional): Prompt user, normalize input (e.g., \"4.22\"  \"openshift-4.22\")\n   - See type-specific skills for additional prompts\n\n**Note:** Universal requirements (security, labels), security validation, and issue creation handled by `/jira:create` command.\n\n## Best Practices\n\n1. **Version input:** Always normalize user version input (e.g., \"4.22\", \"OCP 4.22\"  \"openshift-4.22\")\n2. **Template adherence:** Defer to type-specific skills for templates (create-story, create-epic, etc.)\n3. **Link hierarchy:** Link epics to features, tasks to stories/epics using `--parent` flag\n4. **Descriptive summaries:** Use clear, searchable issue summaries\n5. **Component selection:** Defer to team-specific skills when applicable (e.g., HyperShift)\n\n**Note:** Universal best practices (security, labels, formatting, credential scanning) are defined in the `/jira:create` command.\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill (includes Issue Hierarchy and Parent Linking documentation)\n- `ocpbugs` skill - For OCPBUGS bugs\n- Team-specific skills (e.g., `hypershift`) - For team-specific conventions\n- Type-specific skills (create-story, create-epic, create-feature, create-task) - For issue type best practices"
              },
              {
                "name": "Create Jira Bug",
                "description": "Implementation guide for creating well-formed Jira bug reports",
                "path": "plugins/jira/skills/create-bug/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Bug",
                  "description": "Implementation guide for creating well-formed Jira bug reports"
                },
                "content": "# Create Jira Bug\n\nThis skill provides implementation guidance for creating well-structured Jira bug reports with complete reproduction steps and clear problem descriptions.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create bug` command to guide the bug creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the target project\n- Bug information available (problem description, steps to reproduce, etc.)\n\n## Bug Report Best Practices\n\n### Complete Information\n\nA good bug report contains:\n1. **Clear summary** - Brief description that identifies the problem\n2. **Detailed description** - Complete context and background\n3. **Reproducibility** - How often the bug occurs\n4. **Steps to reproduce** - Exact sequence to trigger the bug\n5. **Actual vs expected results** - What happens vs what should happen\n6. **Environment details** - Version, platform, configuration\n7. **Additional context** - Logs, screenshots, error messages\n\n### Summary Guidelines\n\nThe summary should:\n- Be concise (one sentence)\n- Identify the problem clearly\n- Include key context when helpful\n- Avoid vague terms like \"broken\" or \"doesn't work\"\n\n**Good examples:**\n- \"API server returns 500 error when creating namespaces\"\n- \"Control plane pods crash on upgrade from 4.20 to 4.21\"\n- \"Memory leak in etcd container after 24 hours\"\n\n**Bad examples:**\n- \"Things are broken\"\n- \"Error in production\"\n- \"Fix the bug\"\n\n## Bug Description Template\n\nUse this template structure for consistency:\n\n```\nDescription of problem:\n<Clear, detailed description of the issue>\n\nVersion-Release number of selected component (if applicable):\n<e.g., 4.21.0, openshift-client-4.20.5>\n\nHow reproducible:\n<Always | Sometimes | Rarely>\n\nSteps to Reproduce:\n1. <First step - be specific>\n2. <Second step>\n3. <Third step>\n\nActual results:\n<What actually happens - include error messages>\n\nExpected results:\n<What should happen instead>\n\nAdditional info:\n<Logs, screenshots, stack traces, related issues, workarounds>\n```\n\n## Interactive Bug Collection Workflow\n\nWhen creating a bug, guide the user through each section interactively:\n\n### 1. Problem Description\n\n**Prompt:** \"What is the problem? Describe it clearly and in detail.\"\n\n**Tips to share:**\n- Provide context: What were you trying to do?\n- Be specific: What component or feature is affected?\n- Include impact: Who is affected? How severe is it?\n\n**Example response:**\n```\nThe kube-apiserver pod crashes immediately after upgrading a hosted control plane cluster from version 4.20 to 4.21. The pod enters CrashLoopBackOff state and all API requests to the cluster fail.\n```\n\n### 2. Version Information\n\n**Prompt:** \"Which version exhibits this issue? (e.g., 4.21.0, 4.20.5)\"\n\n**Tips:**\n- Include full version number if known\n- Specify component version if different from platform version\n- Note if issue affects multiple versions\n\n**Default:** Use project-specific default (e.g., 4.21 for OCPBUGS)\n\n### 3. Reproducibility\n\n**Prompt:** \"How reproducible is this issue?\"\n\n**Options:**\n- **Always** - Happens every time following the steps\n- **Sometimes** - Happens intermittently, even with same steps\n- **Rarely** - Hard to reproduce, happened once or few times\n\n**Use case for each:**\n- Always: Easiest to debug and fix\n- Sometimes: May be timing-related or race condition\n- Rarely: May be environmental or complex interaction\n\n### 4. Steps to Reproduce\n\n**Prompt:** \"What are the exact steps to reproduce this issue? Be as specific as possible.\"\n\n**Guidelines:**\n- Number each step\n- Be precise (exact commands, button clicks, inputs)\n- Include environment setup if needed\n- Use code blocks for commands\n- Mention any prerequisites\n\n**Example:**\n```\nSteps to Reproduce:\n1. Create a ROSA HCP cluster on version 4.20.0:\n   rosa create cluster --name test-cluster --version 4.20.0 --hosted-cp\n2. Wait for cluster to be fully ready (about 15 minutes)\n3. Initiate upgrade to 4.21.0:\n   rosa upgrade cluster --cluster test-cluster --version 4.21.0\n4. Monitor the control plane pods:\n   oc get pods -n clusters-test-cluster -w\n5. Observe kube-apiserver pod status\n```\n\n**Validation:**\n- Ensure at least one step is provided\n- Check that steps are numbered/ordered\n- Verify steps are specific enough to follow\n\n### 5. Actual Results\n\n**Prompt:** \"What actually happens when you follow those steps?\"\n\n**Guidelines:**\n- Describe exactly what occurs\n- Include error messages (full text)\n- Mention symptoms (crashes, hangs, wrong output)\n- Include relevant logs or stack traces\n- Note timing (immediate, after 5 minutes, etc.)\n\n**Example:**\n```\nActual results:\nThe kube-apiserver pod crashes immediately after the upgrade completes. The pod restarts continuously (CrashLoopBackOff). Error in pod logs:\n\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x...]\n\nAPI requests to the cluster fail with:\nError from server: error dialing backend: dial tcp: lookup kube-apiserver: no such host\n```\n\n### 6. Expected Results\n\n**Prompt:** \"What should happen instead? What is the expected behavior?\"\n\n**Guidelines:**\n- Describe the correct behavior\n- Be specific about expected state/output\n- Contrast with actual results\n\n**Example:**\n```\nExpected results:\nThe kube-apiserver pod should start successfully after the upgrade. The pod should be in Running state, and API requests to the cluster should succeed normally.\n```\n\n**Validation:**\n- Ensure expected results differ from actual results\n- Check that expected behavior is clearly stated\n\n### 7. Additional Information\n\n**Prompt:** \"Any additional context? (Optional: logs, screenshots, workarounds, related issues)\"\n\n**Helpful additions:**\n- Full logs or log excerpts\n- Screenshots or recordings\n- Stack traces\n- Related Jira issues or documentation\n- Workarounds discovered\n- Impact assessment (severity, affected users)\n- Environment specifics (region, network config, etc.)\n\n**Example:**\n```\nAdditional info:\n- Cluster ID: abc123-def456\n- Region: us-east-1\n- Full pod logs attached: kube-apiserver.log\n- Related issue: OCPBUGS-12340 (similar crash in 4.194.20 upgrade)\n- Workaround: Rollback to 4.20.0 and cluster recovers\n- Affects all ROSA HCP clusters in production\n```\n\n## Component and Version Handling\n\n### Auto-Detection\n\nAnalyze the bug description for component hints:\n- Product names: \"OpenShift\", \"ROSA\", \"ARO\", \"HyperShift\"\n- Component names: \"API server\", \"etcd\", \"networking\", \"storage\"\n- Platform: \"AWS\", \"Azure\", \"GCP\", \"bare metal\"\n\n### Version Fields\n\nDifferent projects may use versions differently:\n\n**OCPBUGS:**\n- **Affects Version/s** (`versions`): Version where bug was found\n- **Target Version** (`customfield_12319940`): Version where fix is targeted\n- Never set **Fix Version/s** (`fixVersions`)\n\n**General projects:**\n- May only have **Affects Version/s**\n- Check project configuration for version fields\n\n### Project-Specific Overrides\n\nIf bug is for a known project with specific conventions (e.g., CNTRLPLANE/OCPBUGS), the cntrlplane skill will be invoked automatically and will override defaults.\n\n## Field Validation\n\nBefore submitting the bug, validate:\n\n### Required Fields\n-  Summary is not empty and is clear\n-  Description contains problem description\n-  Component is specified (or project doesn't require it)\n-  Affects version is specified (if required by project)\n\n### Description Quality\n-  \"Steps to Reproduce\" has at least one step\n-  \"Actual results\" is different from \"Expected results\"\n-  \"How reproducible\" is specified (Always/Sometimes/Rarely)\n\n### Security\n-  No credentials, API keys, or secrets in any field\n-  Logs are sanitized (passwords, tokens redacted)\n-  Screenshots don't expose sensitive information\n\n## MCP Tool Parameters\n\n### Basic Bug Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"<PROJECT_KEY>\",  # e.g., \"OCPBUGS\", \"MYPROJECT\"\n    summary=\"<bug summary>\",\n    issue_type=\"Bug\",\n    description=\"<formatted bug template>\",\n    components=\"<component name>\",  # optional, if required by project\n    additional_fields={\n        \"versions\": [{\"name\": \"<version>\"}],  # affects version, if supported\n        # Add other project-specific fields as needed\n    }\n)\n```\n\n### With Project-Specific Fields (e.g., OCPBUGS)\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"OCPBUGS\",\n    summary=\"Control plane pods crash on upgrade\",\n    issue_type=\"Bug\",\n    description=\"\"\"\nh2. Description of problem\n\nControl plane pods crash immediately after upgrading from 4.20 to 4.21.\n\nh2. Version-Release number\n\n4.21.0\n\nh2. How reproducible\n\nAlways\n\nh2. Steps to Reproduce\n\n# Create a cluster on 4.20\n# Upgrade to 4.21\n# Observe control plane pod status\n\nh2. Actual results\n\nPods enter CrashLoopBackOff state.\n\nh2. Expected results\n\nPods should start successfully.\n\nh2. Additional info\n\nLogs attached.\n    \"\"\",\n    components=\"HyperShift\",\n    additional_fields={\n        \"versions\": [{\"name\": \"4.21\"}],           # affects version\n        \"customfield_12319940\": \"4.21\",            # target version\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}   # if required\n    }\n)\n```\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Headings\n```\nh1. Main Heading\nh2. Subheading\nh3. Sub-subheading\n```\n\n### Text Formatting\n```\n*bold text*\n_italic text_\n{{monospace}}\n{quote}quoted text{quote}\n```\n\n### Lists\n```\n* Bullet item 1\n* Bullet item 2\n** Nested bullet\n\n# Numbered item 1\n# Numbered item 2\n```\n\n### Code Blocks\n```\n{code}\ncommand line text or code\n{code}\n\n{code:java}\n// Language-specific syntax highlighting\npublic class Example {}\n{code}\n```\n\n### Links\n```\n[Link text|http://example.com]\n[OCPBUGS-123]  // Auto-links to Jira issue\n```\n\n## Error Handling\n\n### Missing Required Information\n\n**Scenario:** User doesn't provide required fields.\n\n**Action:**\n1. Identify missing required fields\n2. Prompt user for each missing field\n3. Provide context/examples to help\n4. Re-validate before submission\n\n**Example:**\n```\nSummary is required but not provided. Please provide a brief summary of the bug:\nExample: \"API server crashes when creating namespaces\"\n```\n\n### Invalid Version\n\n**Scenario:** Specified version doesn't exist in project.\n\n**Action:**\n1. Use `mcp__atlassian__jira_get_project_versions` to fetch valid versions\n2. Suggest closest match or list available versions\n3. Ask user to confirm or select different version\n\n**Example:**\n```\nVersion \"4.21.5\" not found for project OCPBUGS.\nAvailable versions: 4.19, 4.20, 4.21, 4.22\nDid you mean \"4.21\"?\n```\n\n### Component Required But Not Provided\n\n**Scenario:** Project requires component, but none specified.\n\n**Action:**\n1. Ask user which component the bug affects\n2. If available, fetch and display component list for project\n3. Accept user's component selection\n4. Validate component exists before submission\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in bug content.\n\n**Action:**\n1. STOP submission immediately\n2. Inform user what type of data was detected (without echoing it)\n3. Provide guidance on redaction\n4. Request sanitized version\n\n**Example:**\n```\nI detected what appears to be an API token in the \"Steps to Reproduce\" section.\nPlease replace with a placeholder like \"YOUR_API_TOKEN\" or \"<redacted>\" before proceeding.\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the bug.\n\n**Action:**\n1. Parse error message\n2. Translate to user-friendly explanation\n3. Suggest corrective action\n4. Offer to retry\n\n**Common errors:**\n- **\"Field 'component' is required\"**  Prompt for component\n- **\"Version not found\"**  Use version error handling\n- **\"Permission denied\"**  User may lack project permissions, inform them to contact admin\n\n## Examples\n\n### Example 1: Simple Bug\n\n**Input:**\n```bash\n/jira:create bug MYPROJECT \"Login button doesn't work on mobile\"\n```\n\n**Interactive prompts:**\n```\nWhat is the problem? Describe it clearly.\n> The login button on the mobile app doesn't respond to taps on iOS devices.\n\nWhich version exhibits this issue?\n> 2.1.0\n\nHow reproducible is this issue?\n> Always\n\nWhat are the exact steps to reproduce?\n> 1. Open mobile app on iPhone 13 (iOS 16.5)\n> 2. Navigate to login screen\n> 3. Tap the \"Login\" button\n> 4. Nothing happens\n\nWhat actually happens?\n> The button doesn't respond to taps. No visual feedback, no navigation.\n\nWhat should happen instead?\n> The button should navigate to the credentials input screen when tapped.\n\nAny additional context?\n> Works fine on Android. Only affects iOS.\n```\n\n**Result:**\n- Issue created in MYPROJECT\n- Type: Bug\n- Summary: \"Login button doesn't work on mobile\"\n- Description: Formatted with bug template\n\n### Example 2: Bug with Auto-Detection (CNTRLPLANE/OCPBUGS)\n\n**Input:**\n```bash\n/jira:create bug \"ROSA HCP control plane pods crash on upgrade\"\n```\n\n**Auto-applied (via cntrlplane skill):**\n- Project: OCPBUGS (default for bugs)\n- Component: HyperShift / ROSA (detected from \"ROSA HCP\")\n- Affects Version: 4.21\n- Target Version: 4.21\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- Bug template sections (same as Example 1)\n\n**Result:**\n- Full bug report created with all CNTRLPLANE conventions applied\n\n### Example 3: Bug with All Fields Provided\n\n**Input:**\n```bash\n/jira:create bug OCPBUGS \"etcd pod OOMKilled after 48 hours\" --component \"HyperShift\" --version \"4.21\"\n```\n\n**Minimal prompts:**\n- Description of problem\n- Steps to reproduce\n- Actual/expected results\n- Additional info\n\n**Result:**\n- Bug created with provided component and version\n- Only prompts for description content\n\n## Best Practices Summary\n\n1. **Clear summaries:** One sentence, specific problem\n2. **Complete steps:** Exact sequence to reproduce\n3. **Specific results:** Include error messages and symptoms\n4. **Sanitize content:** Remove all credentials and secrets\n5. **Add context:** Logs, environment details, workarounds\n6. **Use template:** Follow standard bug template structure\n7. **Validate before submit:** Check all required fields populated\n\n## Workflow Summary\n\n1.  Parse command arguments (project, summary, flags)\n2.  Auto-detect component/version from summary keywords\n3.  Apply project-specific defaults (if applicable)\n4.  Interactively collect bug template sections\n5.  Scan for sensitive data\n6.  Validate required fields\n7.  Format description with Jira markup\n8.  Create bug via MCP tool\n9.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `cntrlplane` skill - CNTRLPLANE/OCPBUGS specific conventions\n- Jira documentation on bug workflows"
              },
              {
                "name": "Create Jira Epic",
                "description": "Implementation guide for creating Jira epics with proper scope and parent linking",
                "path": "plugins/jira/skills/create-epic/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Epic",
                  "description": "Implementation guide for creating Jira epics with proper scope and parent linking"
                },
                "content": "# Create Jira Epic\n\nThis skill provides implementation guidance for creating well-structured Jira epics that organize related stories and tasks into cohesive bodies of work.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create epic` command to guide the epic creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the target project\n- Understanding of the epic scope and related work\n\n## What is an Epic?\n\nAn agile epic is:\n- A **body of work** that can be broken down into specific items (stories/tasks)\n- Based on the needs/requests of customers or end-users\n- An important practice for agile and DevOps teams\n- A tool to **manage work** at a higher level than individual stories\n\n### Epic Characteristics\n\nEpics should:\n- Be **more narrow** than a market problem or feature\n- Be **broader** than a user story\n- **Fit inside a time box** (quarter/release)\n- Stories within the epic should **fit inside a sprint**\n- Include **acceptance criteria** that define when the epic is done\n\n### Epic vs Feature vs Story\n\n| Level | Scope | Duration | Example |\n|-------|-------|----------|---------|\n| Feature | Strategic objective, market problem | Multiple releases | \"Advanced cluster observability\" |\n| Epic | Specific capability, narrower than feature | One quarter/release | \"Multi-cluster metrics aggregation\" |\n| Story | Single user-facing functionality | One sprint | \"As an SRE, I want to view metrics from multiple clusters in one dashboard\" |\n\n## Epic Name Field Requirement\n\n**IMPORTANT:** Many Jira configurations require the **epic name** field to be set.\n\n- **Epic Name** = **Summary** (should be identical)\n- This is a separate required field in addition to the summary field\n- If epic name is missing, epic creation will fail\n\n**MCP Tool Handling:**\n- Some projects auto-populate epic name from summary\n- Some require explicit epic name field\n- Always set epic name = summary to ensure compatibility\n\n## Epic Description Best Practices\n\n### Clear Objective\n\nThe epic description should:\n- State the overall goal or objective\n- Explain the value or benefit\n- Identify the target users or stakeholders\n- Define the scope (what's included and excluded)\n\n**Good example:**\n```\nEnable administrators to manage multiple hosted control plane clusters from a single observability dashboard.\n\nThis epic delivers unified metrics aggregation, alerting, and visualization across clusters, reducing the operational overhead of managing multiple cluster environments.\n\nTarget users: SREs, Platform administrators\n```\n\n### Acceptance Criteria for Epics\n\nEpic-level acceptance criteria define when the epic is complete:\n\n**Format:**\n```\nh2. Epic Acceptance Criteria\n\n* <High-level outcome 1>\n* <High-level outcome 2>\n* <High-level outcome 3>\n```\n\n**Example:**\n```\nh2. Epic Acceptance Criteria\n\n* Administrators can view aggregated metrics from all clusters in a single dashboard\n* Alert rules can be configured to fire based on cross-cluster conditions\n* Historical metrics are retained for 30 days across all clusters\n* Documentation is complete for multi-cluster setup and configuration\n```\n\n**Characteristics:**\n- Broader than story AC (focus on overall capability, not implementation details)\n- Measurable outcomes\n- User-observable (not technical implementation)\n- Typically 3-6 criteria (if more, consider splitting epic)\n\n### Timeboxing\n\nInclude timeframe information:\n- Target quarter or release\n- Key milestones or dependencies\n- Known constraints\n\n**Example:**\n```\nh2. Timeline\n\n* Target: Q1 2025 / OpenShift 4.21\n* Milestone 1: Metrics collection infrastructure (Sprint 1-2)\n* Milestone 2: Dashboard and visualization (Sprint 3-4)\n* Milestone 3: Alerting and historical data (Sprint 5-6)\n```\n\n### Parent Link to Feature\n\nIf the epic belongs to a larger feature:\n- Link to parent feature using `--parent` flag\n- Explain how this epic contributes to the feature\n- Reference feature key in description\n\n**Example:**\n```\nh2. Parent Feature\n\nThis epic is part of [PROJ-100] \"Advanced cluster observability\" and specifically addresses the multi-cluster aggregation capability.\n```\n\n## Interactive Epic Collection Workflow\n\nWhen creating an epic, guide the user through:\n\n### 1. Epic Objective\n\n**Prompt:** \"What is the main objective or goal of this epic? What capability will it deliver?\"\n\n**Helpful questions:**\n- What is the overall goal?\n- What high-level capability will be delivered?\n- Who will benefit from this epic?\n\n**Example response:**\n```\nEnable SREs to manage and monitor multiple ROSA HCP clusters from a unified observability dashboard, reducing the operational complexity of multi-cluster environments.\n```\n\n### 2. Epic Scope\n\n**Prompt:** \"What is included in this epic? What is explicitly out of scope?\"\n\n**Helpful questions:**\n- What functionality is included?\n- What related work is NOT part of this epic?\n- Where are the boundaries?\n\n**Example response:**\n```\nIn scope:\n- Metrics aggregation from multiple clusters\n- Unified dashboard for cluster health\n- Cross-cluster alerting\n- 30-day historical metrics retention\n\nOut of scope:\n- Log aggregation (separate epic)\n- Cost reporting (different feature)\n- Support for non-HCP clusters (future work)\n```\n\n### 3. Epic Acceptance Criteria\n\n**Prompt:** \"What are the high-level outcomes that define this epic as complete?\"\n\n**Guidance:**\n- Focus on capabilities, not implementation\n- Should be measurable/demonstrable\n- Typically 3-6 criteria\n\n**Example response:**\n```\n- SREs can view aggregated metrics from all managed clusters in one dashboard\n- Alert rules can be defined for cross-cluster conditions (e.g., \"any cluster CPU >80%\")\n- Historical metrics available for 30 days\n- Configuration documented and tested\n```\n\n### 4. Timeframe\n\n**Prompt:** \"What is the target timeframe for this epic? (quarter, release, or estimated sprints)\"\n\n**Example responses:**\n- \"Q1 2025\"\n- \"OpenShift 4.21\"\n- \"Estimate 6 sprints\"\n- \"Must complete by March 2025\"\n\n### 5. Parent Feature (Optional)\n\n**Prompt:** \"Is this epic part of a larger feature? If yes, provide the feature key.\"\n\n**If yes:**\n- Validate parent exists\n- Confirm parent is a Feature (not another Epic)\n- Link epic to parent\n\n## Field Validation\n\nBefore submitting the epic, validate:\n\n### Required Fields\n-  Summary is clear and describes the capability\n-  Epic name field is set (same as summary)\n-  Description includes objective\n-  Epic acceptance criteria present\n-  Timeframe specified\n-  Component is specified (if required by project)\n-  Target version is set (if required by project)\n\n### Epic Quality\n-  Scope is broader than a story, narrower than a feature\n-  Can fit in a quarter/release timeframe\n-  Has measurable acceptance criteria\n-  Clearly identifies target users/stakeholders\n-  Defines what's in scope and out of scope\n\n### Parent Validation (if specified)\n-  Parent issue exists\n-  Parent is a Feature (not Epic or Story)\n-  Epic contributes to parent's objective\n\n### Security\n-  No credentials, API keys, or secrets in any field\n\n## MCP Tool Parameters\n\n### Basic Epic Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"<PROJECT_KEY>\",\n    summary=\"<epic summary>\",\n    issue_type=\"Epic\",\n    description=\"\"\"\n<Epic objective and description>\n\nh2. Epic Acceptance Criteria\n\n* <Outcome 1>\n* <Outcome 2>\n* <Outcome 3>\n\nh2. Scope\n\nh3. In Scope\n* <What's included>\n\nh3. Out of Scope\n* <What's not included>\n\nh2. Timeline\n\nTarget: <quarter/release>\n    \"\"\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        \"customfield_epicname\": \"<epic name>\",  # if required, same as summary\n        # Add other project-specific fields\n    }\n)\n```\n\n### With Project-Specific Fields (e.g., CNTRLPLANE)\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Multi-cluster metrics aggregation for ROSA HCP\",\n    issue_type=\"Epic\",\n    description=\"\"\"\nEnable SREs to manage and monitor multiple ROSA HCP clusters from a unified observability dashboard, reducing operational complexity of multi-cluster environments.\n\nh2. Epic Acceptance Criteria\n\n* SREs can view aggregated metrics from all managed clusters in one dashboard\n* Alert rules can be defined for cross-cluster conditions (e.g., \"any cluster CPU >80%\")\n* Historical metrics retained for 30 days across all clusters\n* Multi-cluster setup documented and tested with production workloads\n* Performance acceptable with 100+ clusters\n\nh2. Scope\n\nh3. In Scope\n* Metrics aggregation across ROSA HCP clusters\n* Unified dashboard for cluster health and performance\n* Cross-cluster alerting capabilities\n* 30-day historical metrics retention\n* Configuration via CLI and API\n\nh3. Out of Scope\n* Log aggregation (separate epic CNTRLPLANE-200)\n* Cost reporting (different feature)\n* Support for standalone OCP clusters (future consideration)\n* Integration with external monitoring systems (post-MVP)\n\nh2. Timeline\n\n* Target: Q1 2025 / OpenShift 4.21\n* Estimated: 6 sprints\n* Key milestone: MVP dashboard by end of Sprint 3\n\nh2. Target Users\n\n* SREs managing multiple ROSA HCP clusters\n* Platform administrators\n* Operations teams\n\nh2. Dependencies\n\n* Requires centralized metrics storage infrastructure ([CNTRLPLANE-150])\n* Depends on cluster registration API ([CNTRLPLANE-175])\n    \"\"\",\n    components=\"HyperShift / ROSA\",\n    additional_fields={\n        \"customfield_12319940\": \"openshift-4.21\",  # target version\n        \"customfield_epicname\": \"Multi-cluster metrics aggregation for ROSA HCP\",  # epic name\n        \"labels\": [\"ai-generated-jira\", \"observability\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### With Parent Feature Link\n\nWhen linking an epic to a parent feature via `--parent` flag, use the **Parent Link** custom field (NOT Epic Link, NOT standard `parent` field):\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Multi-cluster monitoring dashboard\",\n    issue_type=\"Epic\",\n    description=\"<epic content with scope and AC>\",\n    components=\"HyperShift\",\n    additional_fields={\n        \"customfield_12311141\": \"Multi-cluster monitoring dashboard\",  # Epic Name (required)\n        \"customfield_12313140\": \"CNTRLPLANE-100\",  # Parent Link - links to parent FEATURE (STRING!)\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n**IMPORTANT:**\n- EpicFeature uses **Parent Link** (`customfield_12313140`) - value is a STRING\n- StoryEpic uses **Epic Link** (`customfield_12311140`) - value is a STRING\n- The standard `parent` field does NOT work for these relationships\n\n**See:** `/jira:create` command documentation for complete parent linking hierarchy and implementation strategy.\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Epic Template Format\n\n```\n<Epic objective - what capability will be delivered and why it matters>\n\nh2. Epic Acceptance Criteria\n\n* <High-level outcome 1>\n* <High-level outcome 2>\n* <High-level outcome 3>\n\nh2. Scope\n\nh3. In Scope\n* <Functionality included in this epic>\n* <Capabilities to be delivered>\n\nh3. Out of Scope\n* <Related work NOT in this epic>\n* <Future considerations>\n\nh2. Timeline\n\n* Target: <quarter or release>\n* Estimated: <sprints>\n* Key milestones: <major deliverables>\n\nh2. Target Users\n\n* <User group 1>\n* <User group 2>\n\nh2. Dependencies (optional)\n\n* [PROJ-XXX] - <dependency description>\n\nh2. Parent Feature (if applicable)\n\nThis epic is part of [PROJ-YYY] \"<feature name>\" and addresses <how this epic contributes>.\n```\n\n## Error Handling\n\n### Epic Name Field Missing\n\n**Scenario:** Epic creation fails due to missing epic name field.\n\n**Action:**\n1. Check if project requires epic name field\n2. If required, set `customfield_epicname` = summary\n3. Retry creation\n\n**Note:** Field ID may vary by Jira instance:\n- `customfield_epicname` (common)\n- `customfield_10011` (numbered field)\n- Check project configuration if standard field names don't work\n\n### Epic Too Large\n\n**Scenario:** Epic seems too large (would take >1 quarter).\n\n**Action:**\n1. Suggest splitting into multiple epics\n2. Identify natural split points\n3. Consider if this should be a Feature instead\n\n**Example:**\n```\nThis epic seems quite large (estimated 12+ sprints). Consider:\n\nOption 1: Split into multiple epics\n- Epic 1: Core metrics aggregation (sprints 1-6)\n- Epic 2: Advanced dashboards and alerting (sprints 7-12)\n\nOption 2: Create as Feature instead\n- This might be better as a Feature with multiple child Epics\n\nWhich would you prefer?\n```\n\n### Epic Too Small\n\n**Scenario:** Epic could be completed in one sprint.\n\n**Action:**\n1. Suggest creating as a Story instead\n2. Explain epic should be multi-sprint effort\n\n**Example:**\n```\nThis epic seems small enough to be a single Story (completable in one sprint).\n\nEpics should typically:\n- Span multiple sprints (2-8 sprints)\n- Contain multiple stories\n- Deliver a cohesive capability\n\nWould you like to create this as a Story instead? (yes/no)\n```\n\n### Parent Not a Feature\n\n**Scenario:** User specifies parent, but it's not a Feature.\n\n**Action:**\n1. Check parent issue type\n2. If parent is Epic or Story, inform user\n3. Suggest correction\n\n**Example:**\n```\nParent issue PROJ-100 is an Epic, but epics should typically link to Features (not other Epics).\n\nOptions:\n1. Link to the parent Feature instead (if PROJ-100 has a parent)\n2. Proceed without parent link\n3. Create a Feature first, then link this Epic to it\n\nWhat would you like to do?\n```\n\n### Missing Acceptance Criteria\n\n**Scenario:** User doesn't provide epic acceptance criteria.\n\n**Action:**\n1. Explain importance of epic AC\n2. Ask probing questions\n3. Help construct AC\n\n**Example:**\n```\nEpic acceptance criteria help define when this epic is complete. Let's add some.\n\nWhat are the key outcomes that must be achieved?\n- What capabilities will exist when this epic is done?\n- How will you demonstrate the epic is complete?\n- What must work end-to-end?\n\nExample: \"Administrators can view aggregated metrics from all clusters\"\n```\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in epic content.\n\n**Action:**\n1. STOP submission\n2. Inform user what type of data was detected\n3. Ask for redaction\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the epic.\n\n**Action:**\n1. Parse error message\n2. Provide user-friendly explanation\n3. Suggest corrective action\n\n**Common errors:**\n- **\"Field 'epic name' is required\"**  Set epic name = summary\n- **\"Invalid parent\"**  Verify parent is Feature type\n- **\"Issue type 'Epic' not available\"**  Check if project supports Epics\n\n## Examples\n\n### Example 1: Epic with Parent Feature\n\n**Input:**\n```bash\n/jira:create epic CNTRLPLANE \"Multi-cluster metrics aggregation\" --parent CNTRLPLANE-100\n```\n\n**Interactive prompts:**\n```\nWhat is the main objective of this epic?\n> Enable SREs to monitor multiple ROSA HCP clusters from one dashboard\n\nWhat is included in scope?\n> Metrics aggregation, unified dashboard, cross-cluster alerting, 30-day retention\n\nWhat is out of scope?\n> Log aggregation, cost reporting, non-HCP cluster support\n\nEpic acceptance criteria?\n> - View aggregated metrics from all clusters\n> - Configure cross-cluster alerts\n> - 30-day historical retention\n> - Complete documentation\n\nTimeframe?\n> Q1 2025, estimate 6 sprints\n```\n\n**Implementation:**\n1. Pre-validate that CNTRLPLANE-100 exists and is a Feature\n2. Create epic with Parent Link field:\n   ```python\n   additional_fields={\n       \"customfield_12311141\": \"Multi-cluster metrics aggregation\",  # Epic Name\n       \"customfield_12313140\": \"CNTRLPLANE-100\",  # Parent Link (STRING, not object!)\n       \"labels\": [\"ai-generated-jira\"],\n       \"security\": {\"name\": \"Red Hat Employee\"}\n   }\n   ```\n3. If creation fails, use fallback: create without parent, then update to add parent link\n\n**Result:**\n- Epic created with complete description\n- Linked to parent feature CNTRLPLANE-100 via Parent Link field (`customfield_12313140`)\n- All CNTRLPLANE conventions applied\n\n### Example 2: Epic with Auto-Detection\n\n**Input:**\n```bash\n/jira:create epic CNTRLPLANE \"Advanced node pool autoscaling for ARO HCP\"\n```\n\n**Auto-applied (via cntrlplane skill):**\n- Component: HyperShift / ARO (detected from \"ARO HCP\")\n- Target Version: openshift-4.21\n- Epic Name: Same as summary\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- Epic objective and scope\n- Acceptance criteria\n- Timeframe\n\n**Result:**\n- Full epic with ARO component\n\n### Example 3: Standalone Epic (No Parent)\n\n**Input:**\n```bash\n/jira:create epic MYPROJECT \"Improve test coverage for API endpoints\"\n```\n\n**Result:**\n- Epic created without parent\n- Standard epic fields applied\n- Ready for stories to be linked\n\n## Best Practices Summary\n\n1. **Clear objective:** State what capability will be delivered\n2. **Define scope:** Explicitly state what's in and out of scope\n3. **Epic AC:** High-level outcomes that define \"done\"\n4. **Right size:** 2-8 sprints, fits in a quarter\n5. **Timebox:** Specify target quarter/release\n6. **Link to feature:** If part of larger initiative\n7. **Target users:** Identify who benefits\n8. **Epic name field:** Always set (same as summary)\n\n## Anti-Patterns to Avoid\n\n **Epic is actually a story**\n```\n\"As a user, I want to view a dashboard\"\n```\n Too small, create as Story instead\n\n **Epic is actually a feature**\n```\n\"Complete observability platform redesign\" (12 months, 50+ stories)\n```\n Too large, create as Feature with child Epics\n\n **Vague acceptance criteria**\n```\n- Epic is done when everything works\n```\n Be specific: \"SREs can view metrics from 100+ clusters with <1s load time\"\n\n **Implementation details in AC**\n```\n- Backend uses PostgreSQL for metrics storage\n- API implements gRPC endpoints\n```\n Focus on outcomes, not implementation\n\n **No scope definition**\n```\nDescription: \"Improve monitoring\"\n```\n Define what's included and what's not\n\n## Workflow Summary\n\n1.  Parse command arguments (project, summary, flags)\n2.  Auto-detect component from summary keywords\n3.  Apply project-specific defaults\n4.  Interactively collect epic objective and scope\n5.  Interactively collect epic acceptance criteria\n6.  Collect timeframe and parent link (if applicable)\n7.  Scan for sensitive data\n8.  Validate epic size and quality\n9.  Set epic name field = summary\n10.  Format description with Jira markup\n11.  Create epic via MCP tool\n12.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill (includes Issue Hierarchy and Parent Linking documentation)\n- `create-feature` skill - For creating parent features\n- `create-story` skill - For stories within epics (uses Epic Link field, NOT parent field)\n- `cntrlplane` skill - CNTRLPLANE specific conventions\n- Agile epic management best practices"
              },
              {
                "name": "Create Jira Feature Request",
                "description": "Implementation guide for creating Feature Requests in the RFE project",
                "path": "plugins/jira/skills/create-feature-request/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Feature Request",
                  "description": "Implementation guide for creating Feature Requests in the RFE project"
                },
                "content": "# Create Jira Feature Request\n\nThis skill provides implementation guidance for creating Feature Requests in the RFE Jira project, which captures customer-driven enhancement requests.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create feature-request RFE` command to guide the feature request creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the RFE project\n- Understanding of the customer need and business justification\n- Knowledge of affected components/teams\n\n## What is a Feature Request?\n\nA Feature Request (RFE) is:\n- A **customer-driven request** for new functionality or enhancement\n- Submitted to the **RFE project** in Jira\n- Captures **business requirements** and customer justification\n- Links specific **components and teams** that need to implement the request\n- Focuses on **what** is needed rather than **how** to implement it\n\n### Feature Request vs Feature vs Story\n\n| Type | Purpose | Project | Example |\n|------|---------|---------|---------|\n| **Feature Request (RFE)** | Customer request for capability | RFE | \"Support for Foo in ProductA managed control planes\" |\n| Feature | Strategic product objective | CNTRLPLANE, etc. | \"Advanced hosted control plane security\" |\n| Story | Single user-facing functionality | Any | \"User can upload custom SSL certificate via console\" |\n\n### Feature Request Characteristics\n\nFeature Requests should:\n- Clearly state the **customer need** or problem\n- Provide **business justification** for the request\n- Identify **affected components** and teams\n- Be **customer-focused** (what they need, not how to build it)\n- Include enough detail for engineering teams to **understand and estimate**\n\n## Feature Request Description Best Practices\n\n### Title\n\nThe title should be:\n- **Clear and concise** (50-80 characters)\n- **Customer-focused** (describes the capability needed)\n- **Specific** (not vague or overly broad)\n\n**Good examples:**\n```\nSupport Foo for ProductA managed control planes\nEnable pod autoscaling based on custom metrics\nMulti-cluster backup and restore for ProductB\n```\n\n**Bad examples:**\n```\nBetter security (too vague)\nSSL stuff (not specific)\nMake autoscaling work better (not clear)\n```\n\n### Nature and Description\n\nClearly describe:\n- **What** is being requested (the capability or enhancement)\n- **Current limitations** or gaps (what doesn't work today)\n- **Desired behavior** (what should happen)\n- **Use case** (how customers will use this)\n\n**Good example:**\n```\nh2. Nature and Description of Request\n\nCustomers need the ability to use Foo for ProductA managed control plane endpoints, rather than relying on vendor-provided defaults.\n\nh3. Current Limitation\nToday, ProductA clusters use vendor-managed configuration for the API server endpoint. Customers cannot provide their own configuration, which creates issues for:\n- Corporate security policies requiring organization-specific settings\n- Integration with existing enterprise infrastructure\n- Compliance requirements (SOC2, ISO 27001)\n\nh3. Desired Behavior\nCustomers should be able to:\n- Upload their own configuration during cluster creation\n- Rotate configuration after cluster creation\n- Validate configuration before cluster becomes active\n- Receive alerts when configuration changes are needed\n\nh3. Use Case\nEnterprise customers with strict security policies need all infrastructure to use internally-managed configuration. This blocks ProductA adoption in regulated industries (finance, healthcare, government) where configuration management is a compliance requirement.\n```\n\n**Bad example:**\n```\nWe need better SSL support.\n```\n\n### Business Requirements\n\nExplain **why** the customer needs this:\n- **Business impact** (what happens without this)\n- **Customer segment** affected (who needs this)\n- **Regulatory/compliance** drivers (if applicable)\n- **Competitive** context (if relevant)\n- **Priority** indicators (blocking deals, customer escalations)\n\n**Good example:**\n```\nh2. Business Requirements\n\nh3. Customer Impact\n- **Primary segment**: Enterprise customers in regulated industries (finance, healthcare, government)\n- **Affected customers**: 10+ customers have requested this capability\n- **Deal blockers**: Multiple active deals blocked by this limitation\n- **Escalations**: Several P1 customer escalations due to compliance failures\n\nh3. Regulatory Requirements\n- SOC2 Type II compliance requires organization-specific configuration\n- ISO 27001 mandates lifecycle management\n- Financial services regulations (PCI-DSS) require integration with enterprise infrastructure\n- Government contracts require validated configuration chains\n\nh3. Business Justification\nWithout this capability:\n- Cannot close enterprise deals in regulated industries\n- Risk losing existing customers to competitors during renewals\n- Increasing support burden from compliance audit failures\n- Limiting market expansion into high-value segments\n\nh3. Competitive Context\nMajor competitors (CompetitorA, CompetitorB, CompetitorC) all support this capability for managed Kubernetes control planes. This is a gap that affects product competitive positioning.\n```\n\n**Bad example:**\n```\nCustomers need this for security.\n```\n\n### Affected Packages and Components\n\nIdentify:\n- **Teams** that need to implement this (use team names like \"HyperShift\", \"ROSA SRE\")\n- **Operators** or controllers affected (e.g., \"cluster-ingress-operator\")\n- **Components** in Jira (this will be set as the Component field)\n- **Related services** (APIs, CLIs, consoles)\n\n**Good example:**\n```\nh2. Affected Packages and Components\n\nh3. Teams\n- **HyperShift Team**: Control plane infrastructure, configuration management\n- **ROSA SRE Team**: Operational validation, configuration rotation\n- **OCM Team**: Console and API for configuration upload\n- **Networking Team**: Networking and configuration distribution\n\nh3. Technical Components\n- **cluster-ingress-operator**: Configuration provisioning and installation\n- **hypershift-operator**: Control plane configuration\n- **openshift-console**: UI for configuration upload and management\n- **rosa CLI**: CLI support for configuration operations\n\nh3. Jira Component\nBased on the primary team and technical area, this should be filed under:\n**Component**: HyperShift / ProductA\n\nh3. Related Services\n- Product API (configuration upload endpoint)\n- Configuration management service (validation, storage)\n- Control plane API server (configuration installation)\n```\n\n**Note:** The \"Jira Component\" will be used to set the `components` field when creating the issue.\n\n## Interactive Feature Request Collection Workflow\n\nWhen creating a feature request, guide the user through these specific questions:\n\n### 1. Proposed Title\n\n**Prompt:** \"What is the proposed title for this feature request? Make it clear, specific, and customer-focused (50-80 characters).\"\n\n**Validation:**\n- Not too vague (\"Better performance\")\n- Not too technical (\"Implement TLS 1.3 in ingress controller\")\n- Customer-facing capability\n\n**Example response:**\n```\nSupport Foo for ProductA managed control planes\n```\n\n### 2. Nature and Description\n\n**Prompt:** \"What is the nature and description of the request? Describe:\n- What capability is being requested\n- Current limitations\n- Desired behavior\n- Use case\"\n\n**Probing questions if needed:**\n- What doesn't work today that should?\n- What would you like to be able to do?\n- How would customers use this capability?\n\n**Example response:**\n```\nCustomers need to use Foo for ProductA API endpoints instead of vendor-provided defaults.\n\nCurrent limitation: ProductA only supports vendor-managed configuration, blocking customers with corporate requirements.\n\nDesired behavior: Customers can upload, validate, and rotate custom configuration for the control plane API.\n\nUse case: Enterprise customers in regulated industries (finance, healthcare) need organization-specific configuration for compliance.\n```\n\n### 3. Business Requirements\n\n**Prompt:** \"Why does the customer need this? List the business requirements, including:\n- Customer impact and affected segment\n- Regulatory/compliance drivers\n- Business justification\n- What happens without this capability\"\n\n**Helpful questions:**\n- Who is asking for this? (customer segment)\n- Why is this blocking them? (compliance, policy, competitive)\n- What is the business impact? (deals, escalations, churn risk)\n- Are there regulatory requirements?\n\n**Example response:**\n```\nCustomer impact:\n- 10+ enterprise customers have requested this\n- Multiple active deals blocked\n- Several P1 escalations from compliance failures\n\nRegulatory requirements:\n- SOC2, ISO 27001, PCI-DSS require organization-specific configuration\n- Government contracts require validated configuration chains\n\nBusiness justification:\n- Cannot close deals in regulated industries (finance, healthcare, government)\n- Competitive gap (CompetitorA, CompetitorB, CompetitorC all support this capability)\n- Risk of customer churn during renewals\n```\n\n### 4. Affected Packages and Components\n\n**Prompt:** \"What packages, components, teams, or operators are affected? This helps route the request to the right teams.\n\nProvide details like:\n- Team names (e.g., 'HyperShift', 'Networking', 'OCM')\n- Operators (e.g., 'cluster-ingress-operator')\n- Technical areas (e.g., 'control plane', 'API server')\n- Services (e.g., 'OCM console', 'rosa CLI')\"\n\n**Follow-up:** After collecting this information, help the user determine the appropriate **Jira Component** value.\n\n**Component mapping guidance:**\n- If **HyperShift**, **ProductA**, or **ProductB** mentioned  Component: \"HyperShift\"\n- If **Networking**, **Ingress** mentioned  Component: \"Networking\"\n- If **OCM**, **Console** mentioned  Component: \"OCM\"\n- If **Multi-cluster**, **Observability** mentioned  Component: \"Observability\"\n- If unclear, ask: \"Based on the teams and technical areas mentioned, which component should this be filed under?\"\n\n**Example response:**\n```\nTeams affected:\n- HyperShift Team (primary - control plane configuration management)\n- ROSA SRE Team (configuration rotation, operations)\n- OCM Team (console UI for configuration upload)\n- Networking Team (networking configuration distribution)\n\nOperators/components:\n- cluster-ingress-operator\n- hypershift-operator\n- openshift-console\n\nSuggested Jira Component: HyperShift\n```\n\n## Field Validation\n\nBefore submitting the feature request, validate:\n\n### Required Fields\n-  Title is clear, specific, and customer-focused\n-  Nature and description explains what is requested\n-  Business requirements justify why this is needed\n-  Affected components and teams are identified\n-  Jira Component is set appropriately\n-  Project is set to \"RFE\"\n-  Issue type is \"Feature Request\"\n\n### Content Quality\n-  Describes customer need (not implementation details)\n-  Business justification is clear\n-  Enough detail for engineering teams to understand\n-  No vague statements (\"better performance\", \"more secure\")\n-  Use case is explained\n\n### Security\n-  No credentials, API keys, or secrets in any field\n-  No confidential customer information (use anonymized references if needed)\n\n## MCP Tool Parameters\n\n### Basic Feature Request Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"RFE\",\n    summary=\"<title of feature request>\",\n    issue_type=\"Feature Request\",\n    description=\"\"\"\n<Brief overview of the request>\n\nh2. Nature and Description of Request\n\n<What is being requested - capability, current limitations, desired behavior, use case>\n\nh2. Business Requirements\n\nh3. Customer Impact\n* <Customer segment affected>\n* <Number of customers requesting>\n* <Deals blocked or escalations>\n\nh3. Regulatory/Compliance Requirements (if applicable)\n* <Compliance driver 1>\n* <Compliance driver 2>\n\nh3. Business Justification\n<Why this is important, what happens without it>\n\nh3. Competitive Context (if applicable)\n<How competitors handle this, gaps>\n\nh2. Affected Packages and Components\n\nh3. Teams\n* <Team 1>: <Responsibility>\n* <Team 2>: <Responsibility>\n\nh3. Technical Components\n* <Operator/component 1>\n* <Operator/component 2>\n\nh3. Related Services\n* <Service 1>\n* <Service 2>\n    \"\"\",\n    components=\"<component name>\",  # Based on affected teams/areas\n    additional_fields={\n        # NOTE: Custom field IDs need to be discovered for RFE project\n        # Placeholder examples below - replace with actual field IDs\n        # \"customfield_12345\": \"<customer name>\",  # If RFE has customer field\n        # \"customfield_67890\": \"<priority level>\",  # If RFE has priority field\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### Example: Foo Feature Request\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"RFE\",\n    summary=\"Support Foo for ProductA managed control planes\",\n    issue_type=\"Feature Request\",\n    description=\"\"\"\nEnable customers to use Foo for ProductA managed control plane API server endpoints, replacing the current vendor-managed approach.\n\nh2. Nature and Description of Request\n\nCustomers need the ability to use Foo for ProductA API endpoints rather than relying on vendor-provided defaults.\n\nh3. Current Limitation\nProductA clusters currently use vendor-managed configuration for the API server endpoint. Customers cannot provide their own configuration, which creates issues for:\n* Corporate security policies requiring organization-specific settings\n* Integration with existing enterprise infrastructure\n* Compliance requirements (SOC2, ISO 27001, PCI-DSS)\n\nh3. Desired Behavior\nCustomers should be able to:\n* Upload their own configuration during cluster creation\n* Rotate custom configuration after cluster creation without cluster downtime\n* Validate configuration before cluster becomes active\n* Receive proactive alerts when configuration updates are needed (30 days, 7 days)\n* View configuration details in product console\n\nh3. Use Case\nEnterprise customers with strict security policies need all infrastructure components to use internally-managed configuration. This capability is required for ProductA adoption in regulated industries (finance, healthcare, government) where configuration management is a compliance requirement and external configuration violates security policies.\n\nh2. Business Requirements\n\nh3. Customer Impact\n* **Primary segment**: Enterprise customers in regulated industries (finance, healthcare, government, defense)\n* **Affected customers**: 10+ enterprise customers have explicitly requested this capability\n* **Deal blockers**: Multiple active enterprise deals are currently blocked by this limitation\n* **Escalations**: Several Priority 1 customer escalations due to failed compliance audits\n\nh3. Regulatory/Compliance Requirements\n* SOC2 Type II compliance requires use of organization-specific configuration with documented lifecycle management\n* ISO 27001 certification mandates configuration lifecycle management and infrastructure integration\n* PCI-DSS (Payment Card Industry) requires configuration from approved infrastructure\n* Government contracts (FedRAMP, DoD) require validated configuration chains\n* Industry-specific regulations (HIPAA, GDPR) require organizational control of configuration\n\nh3. Business Justification\nWithout this capability:\n* Cannot close enterprise deals in regulated industries (blocking market expansion)\n* Risk losing existing customers to competitors during renewal periods\n* Increasing support burden from compliance audit failures and customer escalations\n* Limiting addressable market to non-regulated segments\n* Missing revenue opportunity in high-value enterprise segments\n\nh3. Competitive Context\nAll major competitors support this capability for managed Kubernetes control planes:\n* CompetitorA: Supports custom configuration via service manager\n* CompetitorB: Allows bring-your-own configuration for API server\n* CompetitorC: Supports custom configuration for control plane endpoints\n\nThis is a competitive gap affecting ProductA positioning in enterprise sales cycles.\n\nh2. Affected Packages and Components\n\nh3. Teams\n* **HyperShift Team**: Primary owner - control plane infrastructure, configuration management, rotation logic\n* **ROSA SRE Team**: Operational validation, configuration rotation procedures, monitoring and alerting\n* **OCM Team**: Console UI for configuration upload, validation, and lifecycle management\n* **Networking Team**: Networking configuration, configuration distribution to load balancers\n* **Security Team**: Configuration validation, security review, key management\n\nh3. Technical Components\n* **hypershift-operator**: Control plane configuration and installation\n* **cluster-ingress-operator**: Configuration provisioning for API server\n* **openshift-console**: UI components for configuration upload and management\n* **rosa CLI**: CLI commands for configuration operations (upload, rotate, view)\n* **control-plane-operator**: API server configuration mounting\n\nh3. Related Services\n* OCM API: New endpoints for configuration upload, validation, and management\n* Configuration storage service: Secure storage for sensitive data (encryption at rest)\n* Control plane API server: Configuration installation and serving\n* Monitoring and alerting: Configuration monitoring and proactive alerts\n\nh2. Jira Component\n**Component**: HyperShift\n\n(Primary team is HyperShift, primary technical area is control plane infrastructure)\n    \"\"\",\n    components=\"HyperShift\",\n    additional_fields={\n        # TODO: Discover actual custom field IDs for RFE project\n        # These are placeholders - need to query Jira API to get correct field IDs\n        # Common RFE fields might include:\n        #   - Customer name (customfield_XXXXX)\n        #   - Request priority (customfield_XXXXX)\n        #   - Target release (customfield_XXXXX)\n        \"labels\": [\"ai-generated-jira\", \"security\", \"compliance\", \"product-a\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Feature Request Template Format\n\n```\n<Brief overview>\n\nh2. Nature and Description of Request\n\n<What is being requested>\n\nh3. Current Limitation\n<What doesn't work today>\n\nh3. Desired Behavior\n<What should work>\n\nh3. Use Case\n<How customers will use this>\n\nh2. Business Requirements\n\nh3. Customer Impact\n* <Affected segment>\n* <Number of requests>\n* <Deal impacts>\n\nh3. Regulatory/Compliance Requirements\n* <Requirement 1>\n* <Requirement 2>\n\nh3. Business Justification\n<Why this matters, impact of not having it>\n\nh3. Competitive Context (optional)\n<Competitor capabilities, market gaps>\n\nh2. Affected Packages and Components\n\nh3. Teams\n* <Team>: <Responsibility>\n\nh3. Technical Components\n* <Component/operator>\n\nh3. Related Services\n* <Service>\n\nh2. Jira Component\n**Component**: <component name>\n```\n\n## Error Handling\n\n### Missing Business Justification\n\n**Scenario:** User provides feature request without clear business justification.\n\n**Action:**\n1. Explain importance of business requirements\n2. Ask probing questions about customer impact\n3. Help articulate business case\n\n**Example:**\n```\nFor a Feature Request to be prioritized, we need to understand the business impact.\n\nCan you provide:\n1. Who is requesting this? (customer segment, specific customers)\n2. Why is it blocking them? (compliance, policy, competitive)\n3. What is the business impact? (deals blocked, escalations, churn risk)\n4. Are there regulatory requirements driving this?\n\nThis helps product management and engineering teams understand priority and urgency.\n```\n\n### Vague Description\n\n**Scenario:** Description lacks specifics about what is needed.\n\n**Action:**\n1. Identify vague areas\n2. Ask clarifying questions\n3. Help user be more specific\n\n**Example:**\n```\nThe description \"better security\" is too vague. Let's be more specific:\n\n1. What security capability is needed? (authentication, encryption, access control)\n2. What doesn't work today? (specific limitation or gap)\n3. What should work? (desired behavior)\n4. How would customers use this? (use case)\n\nFor example: \"Support custom SSL certificates\" is specific and actionable.\n```\n\n### Missing Component Information\n\n**Scenario:** User doesn't know which teams or components are affected.\n\n**Action:**\n1. Ask about technical area or keywords\n2. Provide component mapping guidance\n3. Suggest likely component based on description\n\n**Example:**\n```\nTo route this Feature Request correctly, we need to identify the component.\n\nBased on your description mentioning \"ProductA control plane\" and \"configuration\", this likely affects:\n- **HyperShift Team** (control plane infrastructure)\n- **Networking Team** (networking and configuration)\n\nI recommend setting the Jira Component to: **HyperShift**\n\nDoes this seem correct based on your understanding?\n```\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in feature request content.\n\n**Action:**\n1. STOP submission\n2. Inform user what type of data was detected\n3. Ask for sanitization\n\n**Example:**\n```\nI detected potentially confidential information (customer names, specific revenue figures).\n\nIf this is a public Jira project, please anonymize:\n- Use \"Enterprise Customer A\" instead of actual customer names\n- Use ranges ($1-5M) instead of exact revenue figures\n- Remove any confidential business information\n\nWould you like to revise the content?\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the feature request.\n\n**Action:**\n1. Parse error message\n2. Provide user-friendly explanation\n3. Suggest corrective action\n\n**Common errors:**\n- **\"Issue type 'Feature Request' not available\"**  Verify RFE project configuration, may need to use \"Story\" or \"Enhancement\" instead\n- **\"Component 'X' does not exist\"**  List valid components for RFE project\n- **\"Field 'customfield_xyz' does not exist\"**  Remove custom field, update placeholder in skill\n\n## Examples\n\n### Example 1: Enterprise Customer Request\n\n**Input:**\n```bash\n/jira:create feature-request RFE \"Support Foo for ProductA\"\n```\n\n**Interactive prompts collect:**\n- Title: \"Support Foo for ProductA managed control planes\"\n- Nature: Current limitation with vendor defaults, need for custom configuration, use case for regulated industries\n- Business requirements: 10+ customers, multiple blocked deals, compliance drivers\n- Components: HyperShift team, cluster-ingress-operator, hypershift-operator\n\n**Result:**\n- Complete Feature Request with business justification\n- Component: HyperShift\n- Routed to appropriate teams for review\n\n### Example 2: Compliance-Driven Request\n\n**Input:**\n```bash\n/jira:create feature-request RFE \"Multi-cluster backup and restore for ProductB\"\n```\n\n**Auto-detected:**\n- Component: HyperShift (ProductB keyword)\n- Compliance: GDPR data residency, disaster recovery requirements\n\n**Result:**\n- Feature Request with regulatory justification\n- Clear business impact (compliance blocking deals)\n\n## Best Practices Summary\n\n1. **Customer-focused:** Describe what customers need, not how to implement it\n2. **Business justification:** Always include customer impact, deals, escalations, compliance drivers\n3. **Specific and actionable:** Avoid vague descriptions like \"better performance\" or \"more secure\"\n4. **Component routing:** Identify affected teams and set appropriate Jira component\n5. **Regulatory context:** Include compliance requirements if applicable (SOC2, ISO, PCI, HIPAA, etc.)\n6. **Competitive awareness:** Note if competitors have this capability\n7. **No implementation details:** Focus on \"what\" is needed, not \"how\" to build it\n\n## Anti-Patterns to Avoid\n\n **Vague title**\n```\n\"Better security\"\n```\n Use specific capability: \"Support Foo for ProductA managed control planes\"\n\n **No business justification**\n```\n\"Customers want this\"\n```\n Provide specifics: \"10+ customers requested, multiple blocked deals, SOC2 compliance requirement\"\n\n **Technical implementation details**\n```\n\"Implement TLS 1.3 in ingress-operator using new controller\"\n```\n Focus on customer need: \"Support Foo with modern security standards\"\n\n **No component information**\n```\n\"Someone should look at this\"\n```\n Identify teams: \"HyperShift team (control plane), Networking team (ingress)\"\n\n## Workflow Summary\n\n1.  Parse command arguments (project=RFE, summary)\n2.  Interactively collect: Proposed title\n3.  Interactively collect: Nature and description of request\n4.  Interactively collect: Business requirements (why customer needs this)\n5.  Interactively collect: Affected packages and components\n6.  Determine appropriate Jira Component from team/operator information\n7.  Scan for sensitive data (credentials, confidential customer info)\n8.  Validate feature request quality and completeness\n9.  Format description with Jira Wiki markup\n10.  Create Feature Request via MCP tool\n11.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `create-feature` skill - For strategic product features\n- `create-epic` skill - For implementation epics\n- RFE project documentation (if available)\n\n## Notes\n\n### Custom Field Discovery\n\nThis skill uses placeholder comments for custom fields because the actual field IDs for the RFE project need to be discovered. To find the correct field IDs:\n\n1. **Query Jira API for RFE project metadata:**\n   ```bash\n   curl -X GET \"https://issues.redhat.com/rest/api/2/issue/createmeta?projectKeys=RFE&expand=projects.issuetypes.fields\"\n   ```\n\n2. **Look for custom fields** like:\n   - Customer name\n   - Request priority\n   - Target release/version\n   - Business impact\n\n3. **Update `additional_fields` dictionary** with actual field IDs\n\n**TODO:** Once field IDs are discovered, update the MCP tool examples with real field mappings."
              },
              {
                "name": "Create Jira Feature",
                "description": "Implementation guide for creating Jira features representing strategic objectives and market problems",
                "path": "plugins/jira/skills/create-feature/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Feature",
                  "description": "Implementation guide for creating Jira features representing strategic objectives and market problems"
                },
                "content": "# Create Jira Feature\n\nThis skill provides implementation guidance for creating Jira features, which represent high-level strategic objectives and solutions to market problems.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create feature` command to guide the feature creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the target project\n- Understanding of the strategic objective and market problem\n- Product/business context for the feature\n\n## What is a Feature?\n\nA feature is:\n- A **strategic objective** that addresses a market problem or customer need\n- **Broader than an epic** - typically contains multiple epics\n- A **product capability** that delivers business value\n- Aligned with **product roadmap** and business goals\n- Typically spans **one or more releases**\n\n### Feature vs Epic vs Story\n\n| Level | Scope | Duration | Example |\n|-------|-------|----------|---------|\n| **Feature** | Strategic objective, market problem | 1-3 releases (3-9 months) | \"Advanced hosted control plane observability\" |\n| Epic | Specific capability within feature | 1 quarter/release | \"Multi-cluster metrics aggregation\" |\n| Story | Single user-facing functionality | 1 sprint | \"View aggregated cluster metrics in dashboard\" |\n\n### Feature Characteristics\n\nFeatures should:\n- Address a **specific market problem** or customer need\n- Be **more strategic** than implementation details\n- Contain **multiple epics** (typically 3-8 epics)\n- Deliver **measurable business value**\n- Align with **product strategy** and roadmap\n- Have clear **success criteria** (not just completion criteria)\n\n## Feature Description Best Practices\n\n### Market Problem\n\nEvery feature should clearly state:\n- **What customer/business problem** does this solve?\n- **Who** is affected by this problem?\n- **Why** is solving this problem important now?\n- **What happens** if we don't solve it?\n\n**Good example:**\n```\nh2. Market Problem\n\nEnterprise customers managing multiple ROSA HCP clusters (10+) struggle with operational visibility and control. They must navigate between separate dashboards for each cluster, making it difficult to:\n- Identify issues across their cluster fleet\n- Track resource utilization trends\n- Respond quickly to incidents\n- Optimize costs across clusters\n\nThis leads to increased operational overhead, slower incident response, and higher support costs. Without a unified observability solution, customers face scalability challenges as their cluster count grows.\n```\n\n**Bad example:**\n```\nWe need better observability.\n```\n\n### Strategic Value\n\nExplain why this feature matters to the business:\n- **Business impact:** Revenue, cost reduction, market differentiation\n- **Customer value:** What do customers gain?\n- **Competitive advantage:** How does this position us vs competitors?\n- **Strategic alignment:** How does this support company/product strategy?\n\n**Example:**\n```\nh2. Strategic Value\n\nh3. Customer Value\n- 60% reduction in time spent on cluster management\n- Faster incident detection and response (10min  2min)\n- Better resource optimization across cluster fleet\n- Improved operational confidence at scale\n\nh3. Business Impact\n- Supports enterprise expansion (critical for deals >100 clusters)\n- Reduces support burden (fewer escalations, faster resolution)\n- Competitive differentiator (no competitor offers unified HCP observability)\n- Enables upsell opportunities (advanced monitoring add-ons)\n\nh3. Strategic Alignment\n- Aligns with \"Enterprise-First\" product strategy for FY2025\n- Prerequisite for multi-cluster management capabilities in roadmap\n- Supports OpenShift Hybrid Cloud Platform vision\n```\n\n### Success Criteria\n\nDefine how you'll measure success (not just completion):\n- **Adoption metrics:** How many customers will use this?\n- **Usage metrics:** How will it be used?\n- **Outcome metrics:** What improves as a result?\n- **Business metrics:** Revenue, cost, satisfaction impact\n\n**Example:**\n```\nh2. Success Criteria\n\nh3. Adoption\n- 50% of customers with 10+ clusters adopt within 6 months of GA\n- Feature enabled by default for new cluster deployments\n\nh3. Usage\n- Average of 5 dashboard views per day per customer\n- Alert configuration adoption >30% of customers\n- API usage growing 20% month-over-month\n\nh3. Outcomes\n- 40% reduction in time-to-detect incidents (measured via support metrics)\n- Customer satisfaction (CSAT) improvement from 7.2 to 8.5 for multi-cluster users\n- 25% reduction in cluster management support tickets\n\nh3. Business\n- Closes 10+ enterprise deals blocked by observability gap\n- Reduces support costs by $200K annually\n- Enables $500K in advanced monitoring upsell revenue\n```\n\n## Interactive Feature Collection Workflow\n\nWhen creating a feature, guide the user through strategic thinking:\n\n### 1. Market Problem\n\n**Prompt:** \"What customer or market problem does this feature solve? Be specific about who is affected and why it matters.\"\n\n**Probing questions:**\n- Who has this problem? (customer segment, user type)\n- What pain do they experience today?\n- What is the impact of not solving this?\n- Why is solving this important now?\n\n**Example response:**\n```\nEnterprise customers with large ROSA HCP deployments (50+ clusters) struggle with operational visibility. They must manage each cluster separately, leading to slow incident response, difficulty tracking compliance, and high operational overhead. This is blocking large enterprise deals and causing customer escalations.\n```\n\n### 2. Proposed Solution\n\n**Prompt:** \"How will this feature solve the problem? What capability will be delivered?\"\n\n**Example response:**\n```\nDeliver a unified observability platform for ROSA HCP that aggregates metrics, logs, and events across all clusters in a customer's fleet. Provide centralized dashboards, fleet-wide alerting, and compliance reporting.\n```\n\n### 3. Strategic Value\n\n**Prompt:** \"Why is this strategically important? What business value does it deliver?\"\n\n**Helpful questions:**\n- What business impact will this have? (revenue, cost, market position)\n- How does this align with product strategy?\n- What competitive advantage does this provide?\n- What customer value is delivered?\n\n**Example response:**\n```\nCustomer value: 50% reduction in cluster management time\nBusiness impact: Unblocks $5M in enterprise deals, reduces support costs\nCompetitive advantage: No competitor offers unified HCP observability\nStrategic alignment: Critical for enterprise market expansion\n```\n\n### 4. Success Criteria\n\n**Prompt:** \"How will you measure success? What metrics will tell you this feature achieved its goals?\"\n\n**Categories to consider:**\n- Adoption (who/how many use it)\n- Usage (how they use it)\n- Outcomes (what improves)\n- Business (revenue, cost, satisfaction)\n\n**Example response:**\n```\nAdoption: 50% of enterprise customers within 6 months\nUsage: Daily active usage by SREs in 80% of adopting customers\nOutcomes: 40% faster incident detection\nBusiness: Closes 15+ blocked enterprise deals\n```\n\n### 5. Scope and Epics\n\n**Prompt:** \"What are the major components or epics within this feature?\"\n\n**Identify 3-8 major work streams:**\n\n**Example response:**\n```\n1. Multi-cluster metrics aggregation\n2. Unified observability dashboard\n3. Fleet-wide alerting and automation\n4. Compliance and audit reporting\n5. API and CLI for programmatic access\n6. Documentation and enablement\n```\n\n### 6. Timeline and Milestones\n\n**Prompt:** \"What is the timeline? What are key milestones?\"\n\n**Example response:**\n```\nTimeline: 9 months (3 releases)\nMilestones:\n- Q1 2025: MVP metrics aggregation (Epic 1)\n- Q2 2025: Dashboard and alerting (Epics 2-3)\n- Q3 2025: Compliance, API, GA (Epics 4-6)\n```\n\n## Field Validation\n\nBefore submitting the feature, validate:\n\n### Required Fields\n-  Summary clearly states the strategic objective\n-  Description includes market problem\n-  Strategic value articulated\n-  Success criteria defined (measurable)\n-  Component is specified (if required by project)\n-  Target version/release is set (if required)\n\n### Feature Quality\n-  Addresses a real market problem\n-  Strategic value is clear\n-  Success criteria are measurable\n-  Scope is larger than an epic (multi-epic)\n-  Timeline is realistic (1-3 releases)\n-  Aligns with product strategy\n\n### Security\n-  No credentials, API keys, or secrets in any field\n-  No confidential business information (if public project)\n\n## MCP Tool Parameters\n\n### Basic Feature Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"<PROJECT_KEY>\",\n    summary=\"<feature summary>\",\n    issue_type=\"Feature\",\n    description=\"\"\"\n<Brief overview of the feature>\n\nh2. Market Problem\n\n<Describe the customer/business problem this solves>\n\nh2. Proposed Solution\n\n<Describe the capability/solution being delivered>\n\nh2. Strategic Value\n\nh3. Customer Value\n* <Customer benefit 1>\n* <Customer benefit 2>\n\nh3. Business Impact\n* <Business impact 1>\n* <Business impact 2>\n\nh3. Strategic Alignment\n<How this aligns with product strategy>\n\nh2. Success Criteria\n\nh3. Adoption\n* <Adoption metric 1>\n\nh3. Outcomes\n* <Outcome metric 1>\n\nh3. Business\n* <Business metric 1>\n\nh2. Scope\n\nh3. Epics (Planned)\n* Epic 1: <epic name>\n* Epic 2: <epic name>\n* Epic 3: <epic name>\n\nh2. Timeline\n\n* Target: <release/timeframe>\n* Key milestones: <major deliverables>\n    \"\"\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        # Add project-specific fields\n    }\n)\n```\n\n### With Project-Specific Fields (e.g., CNTRLPLANE)\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Advanced observability for hosted control planes\",\n    issue_type=\"Feature\",\n    description=\"\"\"\nDeliver unified observability capabilities for ROSA and ARO hosted control planes, enabling enterprise customers to manage large cluster fleets with centralized monitoring, alerting, and compliance reporting.\n\nh2. Market Problem\n\nEnterprise customers managing multiple ROSA HCP clusters (50+) face significant operational challenges:\n\n* Must navigate separate dashboards for each cluster (time-consuming, error-prone)\n* Cannot track compliance posture across cluster fleet\n* Slow incident detection and response (10-30 minutes to identify cross-cluster issues)\n* Difficulty optimizing resources and costs across clusters\n* High operational overhead preventing scaling to larger deployments\n\nThis problem affects our largest customers and is blocking expansion into enterprise segments. Competitors are beginning to offer fleet management capabilities, creating competitive pressure.\n\nh2. Proposed Solution\n\nBuild a comprehensive observability platform for hosted control planes that provides:\n\n* Centralized metrics aggregation across all customer clusters\n* Unified dashboards for cluster health, performance, and capacity\n* Fleet-wide alerting with intelligent cross-cluster correlation\n* Compliance and audit reporting across cluster fleet\n* APIs and CLI for programmatic access and automation\n* Integration with existing customer monitoring tools\n\nh2. Strategic Value\n\nh3. Customer Value\n* 60% reduction in time spent on cluster operational tasks\n* 80% faster incident detection and response (30min  6min)\n* Improved compliance posture with automated reporting\n* Confidence to scale to 100+ clusters\n* Better resource optimization (20% cost savings through right-sizing)\n\nh3. Business Impact\n* Unblocks $5M in enterprise pipeline (15+ deals require this capability)\n* Reduces support escalations by 40% (better self-service visibility)\n* Competitive differentiator (no competitor offers unified HCP observability at this level)\n* Enables $500K annual upsell revenue (advanced monitoring add-ons)\n* Improves customer retention (reducing churn in enterprise segment)\n\nh3. Competitive Advantage\n* First-to-market with truly unified HCP observability\n* Deep integration with OpenShift ecosystem\n* AI-powered insights (future capability)\n\nh3. Strategic Alignment\n* Aligns with \"Enterprise-First\" product strategy for FY2025\n* Supports \"Hybrid Cloud Platform\" vision\n* Prerequisite for future multi-cluster management capabilities on roadmap\n* Enables shift to \"fleet management\" business model\n\nh2. Success Criteria\n\nh3. Adoption\n* 50% of customers with 10+ clusters adopt within 6 months of GA\n* Feature enabled by default for all new ROSA HCP deployments\n* 30% adoption in ARO HCP customer base within 9 months\n\nh3. Usage\n* Daily active usage by SREs in 80% of adopting customers\n* Average 10+ dashboard views per customer per day\n* Alert configuration adoption >40% of customers\n* API usage growing 25% month-over-month\n\nh3. Outcomes\n* 40% reduction in time-to-detect incidents (measured via support metrics)\n* 50% reduction in time-to-resolve incidents (via support ticket analysis)\n* Customer satisfaction (CSAT) improvement from 7.2 to 8.5 for multi-cluster customers\n* 30% reduction in cluster management support tickets\n\nh3. Business Metrics\n* Close 15 blocked enterprise deals ($5M+ in ARR)\n* Reduce support costs by $250K annually\n* Generate $500K in upsell revenue (advanced monitoring)\n* Improve enterprise customer retention by 15%\n\nh2. Scope\n\nh3. Epics (Planned)\n* Epic 1: Multi-cluster metrics aggregation infrastructure\n* Epic 2: Unified observability dashboard and visualization\n* Epic 3: Fleet-wide alerting and intelligent correlation\n* Epic 4: Compliance and audit reporting\n* Epic 5: API and CLI for programmatic access\n* Epic 6: Customer monitoring tool integrations (Datadog, Splunk)\n* Epic 7: Documentation, training, and customer enablement\n\nh3. Out of Scope (Future Considerations)\n* Log aggregation (separate feature planned for 2026)\n* AI-powered predictive analytics (follow-on feature)\n* Support for standalone OpenShift clusters (not HCP)\n* Cost optimization recommendations (different feature)\n\nh2. Timeline\n\n* Total duration: 9 months (3 releases)\n* Target GA: Q3 2025 (OpenShift 4.23)\n\nh3. Milestones\n* Q1 2025 (4.21): MVP metrics aggregation, basic dashboard (Epics 1-2)\n* Q2 2025 (4.22): Alerting, compliance reporting (Epics 3-4)\n* Q3 2025 (4.23): API, integrations, GA (Epics 5-7)\n\nh2. Dependencies\n\n* Centralized metrics storage infrastructure ([CNTRLPLANE-50])\n* Cluster registration and inventory service ([CNTRLPLANE-75])\n* Identity and access management for multi-cluster ([CNTRLPLANE-120])\n\nh2. Risks and Mitigation\n\nh3. Risks\n* Performance degradation with >500 clusters (scalability testing needed)\n* Integration complexity with third-party monitoring tools\n* Customer adoption if migration from existing tools is complex\n\nh3. Mitigation\n* Performance benchmarking sprint in Epic 1\n* Partner early with Datadog/Splunk on integration design\n* Provide migration tools and dedicated customer success support\n    \"\"\",\n    components=\"HyperShift\",\n    additional_fields={\n        \"customfield_12319940\": \"openshift-4.21\",  # target version (initial)\n        \"labels\": [\"ai-generated-jira\", \"observability\", \"enterprise\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Feature Template Format\n\n```\n<Brief feature overview>\n\nh2. Market Problem\n\n<Detailed description of customer/business problem>\n<Who is affected, what pain they experience, impact of not solving>\n\nh2. Proposed Solution\n\n<Description of the capability/solution being delivered>\n\nh2. Strategic Value\n\nh3. Customer Value\n* <Benefit 1>\n* <Benefit 2>\n\nh3. Business Impact\n* <Impact 1>\n* <Impact 2>\n\nh3. Competitive Advantage\n<How this differentiates us>\n\nh3. Strategic Alignment\n<How this supports product/company strategy>\n\nh2. Success Criteria\n\nh3. Adoption\n* <Adoption metrics>\n\nh3. Usage\n* <Usage metrics>\n\nh3. Outcomes\n* <Customer outcome metrics>\n\nh3. Business Metrics\n* <Revenue, cost, satisfaction metrics>\n\nh2. Scope\n\nh3. Epics (Planned)\n* Epic 1: <name>\n* Epic 2: <name>\n* Epic 3: <name>\n\nh3. Out of Scope\n* <Related work NOT in this feature>\n\nh2. Timeline\n\n* Total duration: <timeframe>\n* Target GA: <date/release>\n\nh3. Milestones\n* <Quarter/Release>: <deliverable>\n* <Quarter/Release>: <deliverable>\n\nh2. Dependencies (if any)\n\n* [PROJ-XXX] - <dependency description>\n\nh2. Risks and Mitigation (optional)\n\nh3. Risks\n* <Risk 1>\n* <Risk 2>\n\nh3. Mitigation\n* <Mitigation strategy 1>\n* <Mitigation strategy 2>\n```\n\n## Error Handling\n\n### Feature Too Small\n\n**Scenario:** Feature could be accomplished as a single epic.\n\n**Action:**\n1. Suggest creating as Epic instead\n2. Explain feature should be multi-epic effort\n\n**Example:**\n```\nThis feature seems small enough to be a single Epic (1-2 months, single capability).\n\nFeatures should typically:\n- Contain 3-8 epics\n- Span multiple releases (6-12 months)\n- Address strategic market problem\n\nWould you like to create this as an Epic instead? (yes/no)\n```\n\n### Missing Strategic Context\n\n**Scenario:** User doesn't provide market problem or strategic value.\n\n**Action:**\n1. Explain importance of strategic framing\n2. Ask probing questions\n3. Help articulate business case\n\n**Example:**\n```\nFor a feature, we need to understand the strategic context:\n\n1. What market problem does this solve?\n2. Why is this strategically important to the business?\n3. What value do customers get?\n\nThese help stakeholders understand why we're investing in this feature.\n\nLet's start with: What customer problem does this solve?\n```\n\n### Vague Success Criteria\n\n**Scenario:** Success criteria are vague or not measurable.\n\n**Action:**\n1. Identify vague criteria\n2. Ask for specific metrics\n3. Suggest measurable alternatives\n\n**Example:**\n```\nSuccess criteria should be measurable. \"Feature is successful\" is too vague.\n\nInstead, consider metrics like:\n- Adoption: \"50% of enterprise customers within 6 months\"\n- Usage: \"10+ daily dashboard views per customer\"\n- Outcomes: \"40% faster incident response time\"\n- Business: \"Close 10 blocked deals worth $3M\"\n\nWhat specific metrics would indicate success for this feature?\n```\n\n### No Epic Breakdown\n\n**Scenario:** User doesn't identify component epics.\n\n**Action:**\n1. Explain features are delivered through epics\n2. Help identify major work streams\n3. Suggest 3-8 epic areas\n\n**Example:**\n```\nFeatures are typically delivered through 3-8 epics. What are the major components or work streams?\n\nFor observability, typical epics might be:\n1. Metrics collection infrastructure\n2. Dashboard and visualization\n3. Alerting system\n4. Reporting capabilities\n5. API development\n6. Documentation\n\nWhat would be the major components for your feature?\n```\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in feature content.\n\n**Action:**\n1. STOP submission\n2. Inform user what type of data was detected\n3. Ask for redaction\n\n**Example:**\n```\nI detected confidential business information (customer names, revenue figures).\n\nIf this is a public Jira project, please sanitize:\n- Use \"Enterprise Customer A\" instead of actual customer names\n- Use ranges ($1-5M) instead of exact revenue figures\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the feature.\n\n**Action:**\n1. Parse error message\n2. Provide user-friendly explanation\n3. Suggest corrective action\n\n**Common errors:**\n- **\"Issue type 'Feature' not available\"**  Check if project supports Features\n- **\"Field 'customfield_xyz' does not exist\"**  Remove unsupported custom field\n\n## Examples\n\n### Example 1: Complete Feature\n\n**Input:**\n```bash\n/jira:create feature CNTRLPLANE \"Advanced hosted control plane observability\"\n```\n\n**Interactive prompts collect:**\n- Market problem (enterprise customer pain)\n- Strategic value (business impact, customer value)\n- Success criteria (measurable metrics)\n- Epic breakdown (7 major components)\n- Timeline (9 months, 3 releases)\n\n**Result:**\n- Complete feature with strategic framing\n- All CNTRLPLANE conventions applied\n- Ready for epic planning\n\n### Example 2: Feature with Component Detection\n\n**Input:**\n```bash\n/jira:create feature CNTRLPLANE \"Multi-cloud cost optimization for ROSA and ARO HCP\"\n```\n\n**Auto-detected:**\n- Component: HyperShift (affects both ROSA and ARO)\n- Target version: openshift-4.21\n\n**Result:**\n- Feature with appropriate component\n- Multi-platform scope\n\n## Best Practices Summary\n\n1. **Strategic framing:** Always articulate market problem and business value\n2. **Measurable success:** Define specific metrics for adoption, usage, outcomes, business\n3. **Multi-epic scope:** Feature should contain 3-8 epics\n4. **Customer-focused:** Describe value from customer perspective\n5. **Business case:** Clear ROI or strategic justification\n6. **Realistic timeline:** 1-3 releases (6-12 months typical)\n7. **Risk awareness:** Identify and mitigate known risks\n\n## Anti-Patterns to Avoid\n\n **Feature is actually an epic**\n```\n\"Multi-cluster dashboard\" (single capability, 1 epic)\n```\n Too small, create as Epic\n\n **No strategic context**\n```\n\"Build monitoring system\"\n```\n Must explain market problem and business value\n\n **Vague success criteria**\n```\n\"Feature is successful when customers like it\"\n```\n Use measurable metrics: \"50% adoption, 8.5 CSAT score, $2M revenue\"\n\n **Technical implementation as feature**\n```\n\"Migrate to Kubernetes operator pattern\"\n```\n Technical work should be epic/task. Features describe customer-facing value.\n\n## Workflow Summary\n\n1.  Parse command arguments (project, summary)\n2.  Auto-detect component from summary keywords\n3.  Apply project-specific defaults\n4.  Interactively collect market problem\n5.  Interactively collect strategic value\n6.  Interactively collect success criteria\n7.  Collect epic breakdown and timeline\n8.  Scan for sensitive data\n9.  Validate feature quality and scope\n10.  Format description with Jira markup\n11.  Create feature via MCP tool\n12.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `create-epic` skill - For epics within features\n- `cntrlplane` skill - CNTRLPLANE specific conventions\n- Product management and roadmap planning resources"
              },
              {
                "name": "Create Release Note",
                "description": "Detailed implementation guide for generating bug fix release notes from Jira and GitHub PRs",
                "path": "plugins/jira/skills/create-release-note/SKILL.md",
                "frontmatter": {
                  "name": "Create Release Note",
                  "description": "Detailed implementation guide for generating bug fix release notes from Jira and GitHub PRs"
                },
                "content": "# Create Release Note\n\nThis skill provides detailed step-by-step implementation guidance for the `/jira:create-release-note` command, which automatically generates bug fix release notes by analyzing Jira bug tickets and their linked GitHub pull requests.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create-release-note` command and should not be called directly by users.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- GitHub CLI (`gh`) installed and authenticated\n- User has read access to the Jira bug\n- User has write access to Release Note fields in Jira\n- User has read access to linked GitHub repositories\n\n## Implementation Steps\n\n### Step 1: Fetch and Validate Jira Bug\n\n**Objective**: Retrieve the bug ticket and validate it's appropriate for release note generation.\n\n**Actions**:\n\n1. **Fetch the bug using MCP**:\n   ```\n   mcp__atlassian__jira_get_issue(\n     issue_key=<issue-key>,\n     fields=\"summary,description,issuetype,status,issuelinks,customfield_12320850,customfield_12317313,comment\"\n   )\n   ```\n\n2. **Parse the response**:\n   - Extract `issuetype.name` - verify it's \"Bug\"\n   - Extract `description` - full bug description text\n   - Extract `issuelinks` - array of linked issues\n   - Extract `customfield_12320850` - current Release Note Type (if already set)\n   - Extract `customfield_12317313` - current Release Note Text (if already set)\n   - Extract `comment.comments` - array of comment objects\n\n3. **Validate issue type**:\n   - If `issuetype.name != \"Bug\"`, show warning:\n     ```\n     Warning: {issue-key} is not a Bug (it's a {issuetype.name}).\n     Release notes are typically for bugs. Continue anyway? (yes/no)\n     ```\n   - If user says no, exit gracefully\n\n4. **Check if release note already exists**:\n   - If `customfield_12317313` is not empty, show warning:\n     ```\n     This bug already has a release note:\n     ---\n     {existing release note}\n     ---\n\n     Do you want to regenerate it? (yes/no)\n     ```\n   - If user says no, exit gracefully\n\n### Step 2: Parse Bug Description for Cause and Consequence\n\n**Objective**: Extract the required Cause and Consequence sections from the bug description.\n\n**Bug Description Format**:\n\nJira bug descriptions often follow this structure:\n```\nDescription of problem:\n{code:none}\n<problem description>\n{code}\n\nCause:\n{code:none}\n<root cause>\n{code}\n\nConsequence:\n{code:none}\n<impact>\n{code}\n\nVersion-Release number of selected component (if applicable):\n...\n\nHow reproducible:\n...\n\nSteps to Reproduce:\n...\n\nActual results:\n...\n\nExpected results:\n...\n```\n\n**Parsing Strategy**:\n\n1. **Look for \"Cause:\" section**:\n   - Search for the string \"Cause:\" (case-insensitive)\n   - Extract text between \"Cause:\" and the next major section\n   - Remove Jira markup: `{code:none}`, `{code}`, etc.\n   - Trim whitespace\n\n2. **Look for \"Consequence:\" section**:\n   - Search for the string \"Consequence:\" (case-insensitive)\n   - Extract text between \"Consequence:\" and the next major section\n   - Remove Jira markup\n   - Trim whitespace\n\n3. **Alternative patterns**:\n   - Some bugs may use \"Root Cause:\" instead of \"Cause:\"\n   - Some bugs may use \"Impact:\" instead of \"Consequence:\"\n   - Try variations if exact match not found\n\n4. **Handle missing sections**:\n   - If Cause is missing:\n     ```\n     Bug description is missing the \"Cause\" section.\n\n     Would you like to:\n     1. Provide the Cause interactively\n     2. Update the bug description in Jira first\n     3. Cancel\n     ```\n   - If Consequence is missing:\n     ```\n     Bug description is missing the \"Consequence\" section.\n\n     Would you like to:\n     1. Provide the Consequence interactively\n     2. Update the bug description in Jira first\n     3. Cancel\n     ```\n\n5. **Interactive input** (if user chooses option 1):\n   - Prompt: \"What is the root cause of this bug?\"\n   - Collect user input\n   - Use as Cause value\n   - Repeat for Consequence if needed\n\n**Example Parsing**:\n\nInput:\n```\nDescription of problem:\n{code:none}\nThe control plane operator crashes when CloudProviderConfig.Subnet is not specified\n{code}\n\nCause:\n{code:none}\nhostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\n{code}\n\nConsequence:\n{code:none}\ncontrol-plane-operator enters a crash loop\n{code}\n```\n\nOutput:\n```\nCause: \"hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\"\nConsequence: \"control-plane-operator enters a crash loop\"\n```\n\n### Step 3: Extract Linked GitHub PRs\n\n**Objective**: Find all GitHub PR URLs associated with this bug.\n\n**Sources to check** (in priority order):\n\n1. **Remote Links** (Primary source - web links in Jira):\n   - Check the Jira issue response for web links\n   - Field name varies: `remotelinks`, or `issuelinks` with outward GitHub PR links\n   - Extract GitHub PR URLs matching pattern: `https://github\\.com/[\\w-]+/[\\w-]+/pull/\\d+`\n   - **IMPORTANT**: Never use `gh issue view {JIRA-KEY}` - Jira keys are NOT GitHub issue numbers\n\n2. **Bug Description**:\n   - Scan the `description` field for GitHub PR URLs\n   - Use regex: `https://github\\.com/([\\w-]+)/([\\w-]+)/pull/(\\d+)`\n   - Extract and parse all matches\n   - **IMPORTANT**: Only extract full PR URLs, not issue references\n\n3. **Bug Comments**:\n   - Iterate through `comment.comments` array\n   - For each comment, scan `body` field for GitHub PR URLs\n   - Use same regex pattern\n   - Extract all matches\n\n4. **Search by bug number** (Fallback if no PR URLs found):\n   - If no PRs found via links, search GitHub for PRs mentioning the bug\n   - **For OCPBUGS**: Try common OpenShift repos:\n     ```bash\n     for repo in \"openshift/hypershift\" \"openshift/cluster-api-provider-aws\" \"openshift/origin\"; do\n       gh pr list --repo \"$repo\" --search \"{issue-key} in:title,body\" --state all --limit 10 --json number,url,title\n     done\n     ```\n   - Display found PRs and ask user to confirm which are relevant:\n     ```\n     Found PRs mentioning {issue-key}:\n     1. openshift/hypershift#4567 - Fix panic when CloudProviderConfig.Subnet is undefined\n     2. openshift/hypershift#4568 - Add tests for Subnet validation\n\n     Which PRs should be included in the release note? (enter numbers separated by commas, or 'all')\n     ```\n   - **IMPORTANT**: Never use `gh issue view {JIRA-KEY}` - this will fail\n\n**URL Parsing**:\n\nFor each found URL `https://github.com/openshift/hypershift/pull/4567`:\n- Extract `org`: \"openshift\"\n- Extract `repo`: \"hypershift\"\n- Extract `pr_number`: \"4567\"\n- Store as: `{\"url\": \"...\", \"repo\": \"openshift/hypershift\", \"number\": \"4567\"}`\n\n**Deduplication**:\n\n- Keep only unique PR URLs\n- If same PR is mentioned multiple times, include it only once\n\n**Validation**:\n\n- If no PRs found after all attempts:\n  ```\n  No GitHub PRs found linked to {issue-key}.\n\n  Please link at least one PR to generate release notes.\n\n  How to link PRs in Jira:\n  1. Edit the bug in Jira\n  2. Add a web link to the GitHub PR URL\n  3. Or mention the PR URL in a comment\n  4. Then run this command again\n  ```\n  Exit without updating the ticket.\n\n**Example**:\n\nFound PRs:\n```\n[\n  {\n    \"url\": \"https://github.com/openshift/hypershift/pull/4567\",\n    \"repo\": \"openshift/hypershift\",\n    \"number\": \"4567\"\n  },\n  {\n    \"url\": \"https://github.com/openshift/hypershift/pull/4568\",\n    \"repo\": \"openshift/hypershift\",\n    \"number\": \"4568\"\n  }\n]\n```\n\n### Step 4: Analyze Each GitHub PR\n\n**Objective**: Extract Fix, Result, and Workaround information from each linked PR.\n\n**For each PR in the list**:\n\n#### 4.1: Fetch PR Details\n\n**Command**:\n```bash\ngh pr view {number} --json body,title,commits,url,state --repo {repo}\n```\n\n**Error handling**:\n```bash\nif ! gh pr view {number} --json body,title,commits,url,state --repo {repo} 2>/dev/null; then\n  echo \"Warning: Unable to access PR {url}\"\n  echo \"Verify the PR exists and you have permissions.\"\n  # Skip this PR, continue with next\nfi\n```\n\n**Expected output** (JSON):\n```json\n{\n  \"body\": \"This PR fixes the panic when CloudProviderConfig.Subnet is not specified...\",\n  \"title\": \"Fix panic when CloudProviderConfig.Subnet is not specified\",\n  \"commits\": [\n    {\n      \"messageHeadline\": \"Add nil check for Subnet field\",\n      \"messageBody\": \"This prevents the controller from crashing...\"\n    }\n  ],\n  \"url\": \"https://github.com/openshift/hypershift/pull/4567\",\n  \"state\": \"MERGED\"\n}\n```\n\n**Parse and store**:\n- `title`: PR title (short summary)\n- `body`: Full PR description\n- `commits`: Array of commit objects with messages\n- `state`: PR state (MERGED, OPEN, CLOSED)\n\n#### 4.2: Fetch PR Diff\n\n**Command**:\n```bash\ngh pr diff {number} --repo {repo}\n```\n\n**Purpose**: Understand what code was actually changed\n\n**Analysis strategy**:\n- Look for added lines (starting with `+`)\n- Identify key changes:\n  - New error handling (`if err != nil`)\n  - New validation checks (`if x == nil`)\n  - New return statements\n  - New error messages\n- Don't include the entire diff in the release note\n- Summarize the nature of changes\n\n**Example diff analysis**:\n```diff\n+if hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet == nil {\n+    return fmt.Errorf(\"Subnet configuration is required\")\n+}\n```\n\n**Summary**: \"Added nil check for CloudProviderConfig.Subnet field\"\n\n#### 4.3: Fetch PR Comments\n\n**Command**:\n```bash\ngh pr view {number} --json comments --repo {repo}\n```\n\n**Expected output** (JSON):\n```json\n{\n  \"comments\": [\n    {\n      \"author\": {\"login\": \"reviewer1\"},\n      \"body\": \"This looks good. The nil check will prevent the crash.\"\n    },\n    {\n      \"author\": {\"login\": \"author\"},\n      \"body\": \"Yes, and I also added a more descriptive error message.\"\n    }\n  ]\n}\n```\n\n**Analysis strategy**:\n- Look for mentions of:\n  - \"workaround\"\n  - \"temporary fix\"\n  - \"until this is merged\"\n  - \"users can work around this by...\"\n- Extract workaround information if found\n- Look for clarifications about the fix\n- Ignore unrelated discussion\n\n#### 4.4: Synthesize PR Analysis\n\n**Combine all sources** (title, body, commits, diff, comments) to extract:\n\n**Fix** (what was changed):\n- Prefer: PR body description of the fix\n- Fallback: PR title + commit message summaries\n- Focus on: What code/configuration was modified\n- Keep concise: 1-3 sentences\n- Avoid: Implementation details (specific function names, line numbers)\n\n**Example Fix**:\n```\nAdded nil check for CloudProviderConfig.Subnet before accessing Subnet.ID field to prevent nil pointer dereference\n```\n\n**Result** (outcome after the fix):\n- Prefer: PR body description of expected behavior\n- Fallback: Inverse of the bug's \"Actual results\"\n- Focus on: What changed for users\n- Keep concise: 1-2 sentences\n\n**Example Result**:\n```\nThe control-plane-operator no longer crashes when CloudProviderConfig.Subnet is not specified\n```\n\n**Workaround** (temporary solution before fix):\n- Only include if explicitly mentioned in PR or comments\n- Look for keywords: \"workaround\", \"temporary\", \"manually\"\n- If not found: Omit this section entirely\n\n**Example Workaround** (if found):\n```\nUsers can manually specify a Subnet value in the HostedCluster spec to avoid the crash\n```\n\n### Step 5: Combine Multiple PRs\n\n**Objective**: If multiple PRs are linked, synthesize them into a single coherent release note.\n\n**Scenarios**:\n\n#### Scenario A: Multiple PRs with different fixes\n\nExample:\n- PR #123: Fixes nil pointer crash\n- PR #456: Adds better error message\n- PR #789: Adds validation tests\n\n**Strategy**: Combine all fixes into a narrative\n```\nFix: Added nil check for CloudProviderConfig.Subnet field (PR #123), improved error messaging for missing configuration (PR #456), and added validation tests to prevent regression (PR #789)\n```\n\n#### Scenario B: Multiple PRs for same fix (backports)\n\nExample:\n- PR #123: Fix for main branch\n- PR #456: Backport to release-4.20\n- PR #789: Backport to release-4.19\n\n**Strategy**: Mention the fix once, note the backports\n```\nFix: Added nil check for CloudProviderConfig.Subnet field (backported to 4.20 and 4.19)\n```\n\n#### Scenario C: Multiple PRs with conflicting descriptions\n\nExample:\n- PR #123 says: \"Fixed by adding validation\"\n- PR #456 says: \"Fixed by removing the field access\"\n\n**Strategy**: Analyze the code diffs to determine what actually happened, or ask user:\n```\nFound multiple PRs with different fix descriptions:\n- PR #123: \"Fixed by adding validation\"\n- PR #456: \"Fixed by removing the field access\"\n\nWhich description is more accurate, or should I combine them?\n```\n\n### Step 6: Format Release Note\n\n**Objective**: Create the final release note text following the standard template.\n\n**Template**:\n```\nCause: {cause from Jira}\nConsequence: {consequence from Jira}\nFix: {synthesized from PRs}\nResult: {synthesized from PRs}\nWorkaround: {synthesized from PRs - optional}\n```\n\n**Formatting rules**:\n- Each line starts with the field name followed by a colon and space\n- No blank lines between fields\n- Workaround field is optional - omit if no workaround exists\n- Keep each field concise but complete\n- Use proper capitalization and punctuation\n\n**Example output**:\n```\nCause: hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\nConsequence: control-plane-operator enters a crash loop\nFix: Added nil check for CloudProviderConfig.Subnet before accessing Subnet.ID field\nResult: The control-plane-operator no longer crashes when CloudProviderConfig.Subnet is not specified\n```\n\n### Step 7: Security Validation\n\n**Objective**: Scan the release note text for sensitive data before submission.\n\n**Patterns to detect**:\n\n1. **API Tokens and Keys**:\n   - Pattern: `(sk|pk)_[a-zA-Z0-9]{20,}`\n   - Pattern: `ghp_[a-zA-Z0-9]{36}`\n   - Pattern: `gho_[a-zA-Z0-9]{36}`\n   - Pattern: `github_pat_[a-zA-Z0-9]{22}_[a-zA-Z0-9]{59}`\n\n2. **AWS Credentials**:\n   - Pattern: `AKIA[0-9A-Z]{16}`\n   - Pattern: `aws_access_key_id\\s*=\\s*[A-Z0-9]+`\n   - Pattern: `aws_secret_access_key\\s*=\\s*[A-Za-z0-9/+=]+`\n\n3. **Passwords in URLs**:\n   - Pattern: `https?://[^:]+:[^@]+@`\n   - Example: `https://user:password@example.com`\n\n4. **JWT Tokens**:\n   - Pattern: `eyJ[a-zA-Z0-9_-]+\\.eyJ[a-zA-Z0-9_-]+\\.[a-zA-Z0-9_-]+`\n\n5. **SSH Private Keys**:\n   - Pattern: `-----BEGIN (RSA|OPENSSH|DSA|EC|PGP) PRIVATE KEY-----`\n\n6. **Kubernetes Secrets**:\n   - Pattern: `[a-zA-Z0-9+/]{40,}==?` (base64 encoded secrets)\n   - Context: If appears after \"token:\", \"secret:\", \"password:\"\n\n**Validation logic**:\n\n```python\nsensitive_patterns = {\n    \"API Token\": r\"(sk|pk)_[a-zA-Z0-9]{20,}\",\n    \"GitHub Token\": r\"gh[po]_[a-zA-Z0-9]{36}\",\n    \"AWS Access Key\": r\"AKIA[0-9A-Z]{16}\",\n    \"URL with credentials\": r\"https?://[^:]+:[^@]+@\",\n    \"JWT Token\": r\"eyJ[a-zA-Z0-9_-]+\\.eyJ[a-zA-Z0-9_-]+\",\n    \"Private Key\": r\"-----BEGIN .* PRIVATE KEY-----\"\n}\n\nfor name, pattern in sensitive_patterns.items():\n    if re.search(pattern, release_note_text):\n        print(f\"Security validation failed!\")\n        print(f\"Detected what appears to be {name} in the release note text.\")\n        print(f\"Please review the source PRs and remove any credentials.\")\n        exit(1)\n```\n\n**If validation fails**:\n```\nSecurity validation failed!\n\nDetected what appears to be an API token in the release note text.\n\nThis may have come from:\n- PR description\n- Commit messages\n- Code changes (diff)\n- PR comments\n\nPlease review the source PRs and remove any credentials before proceeding.\n\nUse placeholder values instead:\n- YOUR_API_KEY\n- <redacted>\n- ********\n\nAborting release note creation.\n```\n\n### Step 8: Select Release Note Type\n\n**Objective**: Determine the appropriate Release Note Type for this bug.\n\n**Available types** (from Jira dropdown):\n1. Bug Fix\n2. Release Note Not Required\n3. Known Issue\n4. Enhancement\n5. Rebase\n6. Technology Preview\n7. Deprecated Functionality\n8. CVE\n\n**Auto-detection strategy**:\n\n1. **For OCPBUGS**: Default suggestion is \"Bug Fix\" (most common)\n\n2. **Check for CVE references**:\n   - If bug description or PRs mention \"CVE-\"  Suggest \"CVE\"\n\n3. **Check for deprecation**:\n   - If PRs mention \"deprecated\", \"deprecating\", \"removing\"  Suggest \"Deprecated Functionality\"\n\n4. **Check for new features**:\n   - If PRs add significant new functionality  Suggest \"Enhancement\"\n   - Though typically bugs should be \"Bug Fix\"\n\n5. **Check for known issues**:\n   - If PRs don't actually fix the issue, just document it  Suggest \"Known Issue\"\n\n**User interaction**:\n\nUse the `AskUserQuestion` tool:\n\n```\nquestions = [{\n  \"question\": \"What type of release note is this?\",\n  \"header\": \"Type\",\n  \"multiSelect\": false,\n  \"options\": [\n    {\n      \"label\": \"Bug Fix\",\n      \"description\": \"Fix for a defect (most common)\"\n    },\n    {\n      \"label\": \"Known Issue\",\n      \"description\": \"Documents a known problem without a fix\"\n    },\n    {\n      \"label\": \"Enhancement\",\n      \"description\": \"New feature or improvement\"\n    },\n    {\n      \"label\": \"CVE\",\n      \"description\": \"Security vulnerability fix\"\n    }\n  ]\n}]\n```\n\n**Store selection** for use in the next step.\n\n### Step 9: Update Jira Ticket\n\n**Objective**: Write the release note to the Jira ticket.\n\n**MCP tool call**:\n```\nmcp__atlassian__jira_update_issue(\n  issue_key=<issue-key>,\n  fields={\n    \"customfield_12320850\": {\"value\": \"<selected_type>\"},\n    \"customfield_12317313\": \"<formatted_release_note_text>\"\n  }\n)\n```\n\n**Field details**:\n- `customfield_12320850`: Release Note Type (must be exact match from dropdown)\n- `customfield_12317313`: Release Note Text (plain text)\n\n**Error handling**:\n\n1. **Permission denied**:\n   ```\n   Failed to update {issue-key}.\n\n   Error: You do not have permission to edit Release Note fields\n\n   Please contact your Jira administrator or manually add the release note.\n\n   Generated release note (for manual entry):\n   ---\n   {release_note_text}\n   ---\n   ```\n\n2. **Invalid field value**:\n   ```\n   Failed to update Release Note Type field.\n\n   Error: Value \"{selected_type}\" is not valid\n\n   Please select a different type or manually update in Jira.\n   ```\n\n3. **Field not found**:\n   ```\n   Failed to update {issue-key}.\n\n   Error: Field customfield_12320850 not found\n\n   This may indicate a different Jira instance or configuration.\n   Please manually add the release note.\n   ```\n\n**Success response**:\n```\n{\n  \"success\": true,\n  \"issue\": {\n    \"key\": \"OCPBUGS-38358\",\n    \"fields\": {\n      \"customfield_12320850\": {\"value\": \"Bug Fix\"},\n      \"customfield_12317313\": \"Cause: ... Consequence: ... Fix: ... Result: ...\"\n    }\n  }\n}\n```\n\n### Step 10: Display Results\n\n**Objective**: Show the user what was created and provide next steps.\n\n**Output format**:\n```\n Release Note Created for {issue-key}\n\nType: {Release Note Type}\n\nText:\n---\n{Release Note Text with proper formatting}\n---\n\nUpdated: https://issues.redhat.com/browse/{issue-key}\n\nNext steps:\n- Review the release note in Jira\n- Edit manually if any adjustments are needed\n- The release note will be included in the next release\n```\n\n**Example**:\n```\n Release Note Created for OCPBUGS-38358\n\nType: Bug Fix\n\nText:\n---\nCause: hostedcontrolplane controller crashes when hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID is undefined\nConsequence: control-plane-operator enters a crash loop\nFix: Added nil check for CloudProviderConfig.Subnet before accessing Subnet.ID field\nResult: The control-plane-operator no longer crashes when CloudProviderConfig.Subnet is not specified\n---\n\nUpdated: https://issues.redhat.com/browse/OCPBUGS-38358\n\nNext steps:\n- Review the release note in Jira\n- Edit manually if any adjustments are needed\n- The release note will be included in the next release\n```\n\n## Error Recovery Strategies\n\n### PR Analysis Failures\n\n**Problem**: Some PRs can't be accessed or analyzed\n\n**Recovery**:\n1. Log warning for each failed PR\n2. Continue with successfully analyzed PRs\n3. If all PRs fail, treat as \"No PRs linked\" error\n4. Show summary of what was analyzed:\n   ```\n   Analyzed 2 of 3 linked PRs:\n    PR #123\n    PR #456\n    PR #789 (access denied)\n   ```\n\n### Incomplete Bug Description\n\n**Problem**: Missing Cause or Consequence sections\n\n**Recovery**:\n1. Offer interactive input option\n2. Provide template for user\n3. Allow user to update bug and retry\n4. Don't proceed without both fields\n\n### Ambiguous PR Content\n\n**Problem**: PRs don't clearly explain the fix\n\n**Recovery**:\n1. Use PR title as fallback\n2. Analyze code diff more carefully\n3. Ask user for clarification:\n   ```\n   The linked PR doesn't clearly describe the fix.\n\n   Based on code analysis, it appears to:\n   {best guess from diff}\n\n   Is this correct? If not, please describe the fix:\n   ```\n\n### Conflicting Information\n\n**Problem**: Multiple PRs describe different fixes\n\n**Recovery**:\n1. Present both descriptions to user\n2. Ask which is correct or how to combine\n3. Use AI to attempt intelligent synthesis\n4. If synthesis fails, escalate to user\n\n## Best Practices for Implementation\n\n1. **Always validate inputs**: Check that issue exists, PRs are accessible, fields are present\n2. **Fail gracefully**: Provide helpful error messages with recovery instructions\n3. **Be defensive**: Handle missing data, API errors, permission issues\n4. **Log progress**: Show user what's happening at each step\n5. **Preserve data**: If update fails, show generated content so it's not lost\n6. **Security first**: Always validate before updating Jira\n7. **User in control**: Confirm selections, allow overrides\n\n## Testing Checklist\n\n- [ ] Bug with single linked PR\n- [ ] Bug with multiple linked PRs\n- [ ] Bug with no linked PRs (error case)\n- [ ] Bug with inaccessible PR (warning case)\n- [ ] Bug missing Cause section (error case)\n- [ ] Bug missing Consequence section (error case)\n- [ ] Bug with existing release note (warning case)\n- [ ] Bug that's not a Bug type (warning case)\n- [ ] Release note with detected credentials (security failure)\n- [ ] Different Release Note Type selections\n- [ ] Update permission denied (error case)\n- [ ] MCP server not configured (error case)\n- [ ] gh CLI not authenticated (error case)"
              },
              {
                "name": "Create Jira Story",
                "description": "Implementation guide for creating well-formed Jira user stories with acceptance criteria",
                "path": "plugins/jira/skills/create-story/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Story",
                  "description": "Implementation guide for creating well-formed Jira user stories with acceptance criteria"
                },
                "content": "# Create Jira Story\n\nThis skill provides implementation guidance for creating well-structured Jira user stories following agile best practices, including proper user story format and comprehensive acceptance criteria.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create story` command to guide the story creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the target project\n- Understanding of the user story and acceptance criteria to be created\n\n##  Summary vs Description: CRITICAL DISTINCTION\n\n**This is the #1 mistake when creating stories. The summary field and description field serve different purposes:**\n\n### Summary Field (Issue Title)\n- **SHORT, concise title** (5-10 words maximum)\n- Action-oriented, describes WHAT will be done\n- **Does NOT contain the full \"As a... I want... So that...\" format**\n- Think of it as a newspaper headline\n\n**Good summary examples:**\n-  \"Enable ImageTagMirrorSet configuration in HostedCluster CRs\"\n-  \"Add automatic node pool scaling for ROSA HCP\"\n-  \"Implement webhook validation for HostedCluster resources\"\n\n**Bad summary examples:**\n-  \"As a cluster admin, I want to configure ImageTagMirrorSet in HostedCluster CRs so that I can enable tag-based image proxying\" (Full user story - belongs in description!)\n-  \"As a developer, I want to view metrics so that I can debug issues\" (User story format - belongs in description!)\n\n### Description Field (Issue Body)\n- Contains the **FULL user story format**: \"As a... I want... So that...\"\n- Includes **acceptance criteria**\n- Includes **additional context**\n- Can be lengthy and detailed\n\n**Correct usage:**\n```\nSummary: \"Enable ImageTagMirrorSet configuration in HostedCluster CRs\"\n\nDescription:\n  As a cluster admin, I want to configure ImageTagMirrorSet in HostedCluster CRs,\n  so that I can enable tag-based image proxying for my workloads.\n\n  Acceptance Criteria:\n  - Test that ImageTagMirrorSet can be specified...\n```\n\n### When Collecting Story Information\n1. First collect the full user story (As a... I want... So that...)\n2. Then extract/generate a concise summary title from that story\n3. Present both to user for confirmation\n4. Summary goes in `summary` parameter, full story goes in `description`\n\n## User Story Best Practices\n\n### What is a User Story?\n\nA user story:\n- Describes product functionality from a customer's perspective\n- Is a collaboration tool - a reminder to have a conversation\n- Shifts focus from writing documentation to talking with stakeholders\n- Describes concrete business scenarios in shared language\n- Is the right size for planning - level of detail based on implementation horizon\n\n### The 3 Cs of User Stories\n\nEvery user story should have three components:\n\n1. **Card** - The story itself (As a... I want... So that...)\n2. **Conversation** - Discussion between team and stakeholders about implementation\n3. **Confirmation** - Acceptance criteria that define \"done\"\n\n## User Story Template\n\n### Standard Format\n\n```\nAs a <User/Who>, I want to <Action/What>, so that <Purpose/Why>.\n```\n\n**Components:**\n\n- **Who (User/Role):** The person, device, or system that will benefit from or use the output\n  - Examples: \"cluster admin\", \"developer\", \"end user\", \"monitoring system\", \"CI pipeline\"\n\n- **What (Action):** What they can do with the system\n  - Examples: \"configure automatic scaling\", \"view cluster metrics\", \"deploy applications\"\n\n- **Why (Purpose):** Why they want to do the activity, the value they gain\n  - Examples: \"to handle traffic spikes\", \"to identify performance issues\", \"to reduce deployment time\"\n\n### Good Examples\n\n```\nAs a cluster admin, I want to configure automatic node pool scaling based on CPU utilization, so that I can handle traffic spikes without manual intervention.\n```\n\n```\nAs a developer, I want to view real-time cluster metrics in the web console, so that I can quickly identify performance issues before they impact users.\n```\n\n```\nAs an SRE, I want to set up alerting rules for control plane health, so that I can be notified immediately when issues occur.\n```\n\n### Bad Examples (and why)\n\n \"Add scaling feature\"\n- **Why bad:** No user, no value statement, too vague\n\n \"As a user, I want better performance\"\n- **Why bad:** Not actionable, no specific action, unclear benefit\n\n \"Implement autoscaling API\"\n- **Why bad:** Technical task, not user-facing value\n\n **Convert to:** \"As a cluster admin, I want to configure autoscaling policies via the API, so that I can automate cluster capacity management\"\n\n## Acceptance Criteria\n\nAcceptance criteria:\n- Express conditions that need to be satisfied for the customer\n- Provide context and details for the team\n- Help the team know when they are done\n- Provide testing point of view\n- Are written by Product Owner or dev team members\n- Are refined during backlog grooming and iteration planning\n\n### Formats for Acceptance Criteria\n\nChoose the format that best fits the story:\n\n#### Format 1: Test-Based\n```\n- Test that <criteria>\n```\n\n**Example:**\n```\n- Test that node pools scale up when CPU exceeds 80%\n- Test that node pools scale down when CPU drops below 30%\n- Test that scaling respects configured min/max node limits\n```\n\n#### Format 2: Demonstration-Based\n```\n- Demonstrate that <this happens>\n```\n\n**Example:**\n```\n- Demonstrate that scaling policies can be configured via CLI\n- Demonstrate that scaling events appear in the audit log\n- Demonstrate that users receive notifications when scaling occurs\n```\n\n#### Format 3: Verification-Based\n```\n- Verify that when <a role> does <some action> they get <this result>\n```\n\n**Example:**\n```\n- Verify that when a cluster admin sets max nodes to 10, the node pool never exceeds 10 nodes\n- Verify that when scaling is disabled, node count remains constant regardless of load\n```\n\n#### Format 4: Given-When-Then (BDD)\n```\n- Given <a context> when <this event occurs> then <this happens>\n```\n\n**Example:**\n```\n- Given CPU utilization is at 85%, when the scaling policy is active, then a new node is provisioned within 2 minutes\n- Given the node pool is at maximum capacity, when scaling is triggered, then an alert is raised and no nodes are added\n```\n\n### How Much Acceptance Criteria is Enough?\n\nYou have enough AC when:\n-  You have enough to size/estimate the story\n-  The testing approach is clear but not convoluted\n-  You've made 2-3 revisions of the criteria\n-  The story is independently testable\n\n**If you need more AC:**\n- Consider splitting the story into multiple smaller stories\n- Each story should be completable in one sprint\n\n**If AC is too detailed:**\n- Move implementation details to subtasks or technical design docs\n- Keep AC focused on user-observable behavior\n\n## Interactive Story Collection Workflow\n\nWhen creating a story, guide the user through the process:\n\n### 1. Collect User Story Statement\n\n**Prompt:** \"Let's create the user story. I can help you format it properly.\"\n\n**Ask three questions:**\n\n1. **Who benefits?**\n   ```\n   Who is the user or role that will benefit from this feature?\n   Examples: cluster admin, developer, SRE, end user, system administrator\n   ```\n\n2. **What action?**\n   ```\n   What do they want to be able to do?\n   Examples: configure autoscaling, view metrics, set up alerts\n   ```\n\n3. **What value/why?**\n   ```\n   Why do they want this? What value does it provide?\n   Examples: to handle traffic spikes, to improve visibility, to reduce downtime\n   ```\n\n**Construct the story:**\n```\nAs a <answer1>, I want to <answer2>, so that <answer3>.\n```\n\n**Present to user and ask for confirmation:**\n```\nHere's the user story:\n\nAs a cluster admin, I want to configure automatic node pool scaling, so that I can handle traffic spikes without manual intervention.\n\nDoes this look correct? (yes/no/modify)\n```\n\n### 2. Collect Acceptance Criteria\n\n**Prompt:** \"Now let's define the acceptance criteria. These help the team know when the story is complete.\"\n\n**Approach 1: Guided Questions**\n\nAsk probing questions:\n```\n1. What are the key behaviors that must work?\n2. What are the edge cases or boundaries?\n3. How will this be tested?\n4. What shouldn't happen?\n```\n\n**Approach 2: Template Assistance**\n\nOffer format templates:\n```\nWhich format would you like to use for acceptance criteria?\n1. Test that... (test-based)\n2. Verify that when... they get... (verification-based)\n3. Given... when... then... (BDD)\n4. I'll write them in my own format\n```\n\n**Approach 3: Free-Form**\n\n```\nPlease provide the acceptance criteria (one per line, or I can help you structure them):\n```\n\n**Validate AC:**\n- At least 2-3 criteria provided\n- Criteria are specific and testable\n- Criteria cover happy path and edge cases\n- Criteria are user-observable (not implementation details)\n\n### 3. Collect Additional Context (Optional)\n\n**Prompt:** \"Any additional context for the team? (Optional)\"\n\n**Helpful additions:**\n- Background: Why is this needed now?\n- Dependencies: What must exist before this can be done?\n- Constraints: Any technical or business constraints?\n- Out of scope: What is explicitly not included?\n- References: Links to designs, docs, related issues\n\n**Example:**\n```\nAdditional Context:\n- This builds on the existing monitoring infrastructure introduced in PROJ-100\n- Must integrate with Prometheus metrics\n- Out of scope: Custom metrics (will be separate story)\n- Design doc: https://docs.example.com/autoscaling-design\n```\n\n## Story Sizing and Splitting\n\n### Right-Sized Stories\n\nA well-sized story:\n- Can be completed in one sprint (typically 1-2 weeks)\n- Can be demonstrated as working software\n- Delivers incremental value\n- Has clear acceptance criteria\n\n### When to Split Stories\n\nSplit a story if:\n- It would take more than one sprint\n- It has too many acceptance criteria (>7-8)\n- It contains multiple distinct features\n- It has hard dependencies that could be separate\n- Testing becomes too complex\n\n### Splitting Techniques\n\n**By workflow steps:**\n```\nOriginal: As a user, I want to manage my account settings\nSplit:\n- As a user, I want to view my account settings\n- As a user, I want to update my account settings\n- As a user, I want to delete my account\n```\n\n**By acceptance criteria:**\n```\nOriginal: Complex story with 10 AC\nSplit:\n- Story 1: AC 1-4 (core functionality)\n- Story 2: AC 5-7 (edge cases)\n- Story 3: AC 8-10 (advanced features)\n```\n\n**By platform/component:**\n```\nOriginal: Add feature to all platforms\nSplit:\n- Add feature to web interface\n- Add feature to CLI\n- Add feature to API\n```\n\n## Field Validation\n\nBefore submitting the story, validate:\n\n### Required Fields\n-  Summary is concise title (5-10 words), NOT full user story (see \"Summary vs Description\" section above)\n-  Description contains full user story in \"As a... I want... So that...\" format\n-  Acceptance criteria are present (at least 2)\n-  Component is specified (if required by project)\n-  Target version is set (if required by project)\n\n### Story Quality\n-  Story describes user-facing value (not implementation)\n-  Acceptance criteria are testable\n-  Acceptance criteria are specific (not vague)\n-  Story is sized appropriately (can fit in one sprint)\n\n### Security\n-  No credentials, API keys, or secrets in any field\n-  No sensitive customer data in examples\n\n## MCP Tool Parameters\n\n### Basic Story Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"<PROJECT_KEY>\",\n    summary=\"<concise title>\",  # 5-10 words, NOT full user story\n    issue_type=\"Story\",\n    description=\"\"\"\nAs a <user>, I want to <action>, so that <value>.\n\nh2. Acceptance Criteria\n\n* Test that <criteria 1>\n* Test that <criteria 2>\n* Verify that <criteria 3>\n\nh2. Additional Context\n\n<context if provided>\n    \"\"\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        # Add project-specific fields\n    }\n)\n```\n\n### With Project-Specific Fields (e.g., CNTRLPLANE)\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Enable automatic node pool scaling for ROSA HCP\",\n    issue_type=\"Story\",\n    description=\"\"\"\nAs a cluster admin, I want to configure automatic node pool scaling based on CPU utilization, so that I can handle traffic spikes without manual intervention.\n\nh2. Acceptance Criteria\n\n* Test that node pools scale up when average CPU exceeds 80% for 5 minutes\n* Test that node pools scale down when average CPU drops below 30% for 10 minutes\n* Test that scaling respects configured min/max node limits\n* Verify that when scaling is disabled, node count remains constant regardless of load\n* Verify that scaling events are logged to the cluster audit trail\n* Demonstrate that scaling policies can be configured via rosa CLI\n\nh2. Additional Context\n\nThis builds on the existing monitoring infrastructure. Must integrate with Prometheus metrics for CPU utilization data.\n\nOut of scope: Custom metrics-based scaling (will be separate story CNTRLPLANE-457)\n    \"\"\",\n    components=\"HyperShift / ROSA\",\n    additional_fields={\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n        # Note: Target version omitted (optional in CNTRLPLANE)\n    }\n)\n```\n\n### With Parent Epic Link\n\nWhen linking a story to a parent epic via `--parent` flag, use the Epic Link custom field:\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Add metrics endpoint for cluster health\",\n    issue_type=\"Story\",\n    description=\"<story description with user story format and AC>\",\n    components=\"HyperShift / ROSA\",\n    additional_fields={\n        \"customfield_12311140\": \"CNTRLPLANE-456\",  # Epic Link - parent epic key as STRING\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n**Note:** For epic linking, parent field handling, and other project-specific requirements, refer to the appropriate project-specific skill (e.g., CNTRLPLANE, OCPBUGS).\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Story Template Format\n\n```\nAs a <user>, I want to <action>, so that <value>.\n\nh2. Acceptance Criteria\n\n* Test that <criteria 1>\n* Verify that <criteria 2>\n* Given <context> when <event> then <outcome>\n\nh2. Additional Context\n\n<optional context>\n\nh3. Dependencies\n* [PROJ-123] - Parent epic or related story\n\nh3. Out of Scope\n* Feature X (will be separate story)\n* Platform Y support (future release)\n```\n\n### Formatting Elements\n\n**Headings:**\n```\nh1. Main Heading\nh2. Subheading\nh3. Sub-subheading\n```\n\n**Lists:**\n```\n* Bullet item 1\n* Bullet item 2\n** Nested bullet\n\n# Numbered item 1\n# Numbered item 2\n```\n\n**Text Formatting:**\n```\n*bold text*\n_italic text_\n{{monospace}}\n```\n\n**Code Blocks:**\n```\n{code}\nrosa create cluster --autoscaling-min 3 --autoscaling-max 10\n{code}\n```\n\n**Links:**\n```\n[Design doc|https://docs.example.com/design]\n[PROJ-123]  // Auto-links to Jira issue\n```\n\n## Error Handling\n\n### Invalid Story Format\n\n**Scenario:** User provides a story that doesn't follow the template.\n\n**Action:**\n1. Identify the issue (missing \"Who\", \"What\", or \"Why\")\n2. Explain the user story format\n3. Ask questions to extract missing components\n4. Reconstruct the story properly\n\n**Example:**\n```\nThe story \"Add autoscaling\" doesn't follow the user story format.\n\nLet me help you structure it:\n- Who will use this feature? (e.g., cluster admin, developer)\n- What do they want to do? (e.g., configure autoscaling)\n- Why do they want it? (e.g., to handle traffic spikes)\n```\n\n### Missing Acceptance Criteria\n\n**Scenario:** User doesn't provide acceptance criteria.\n\n**Action:**\n1. Explain importance of AC\n2. Offer to help create them\n3. Ask probing questions about expected behavior\n4. Suggest format that fits the story\n\n**Example:**\n```\nAcceptance criteria help define when this story is complete. Let's add some.\n\nWhat are the key behaviors that must work for this story?\nFor example:\n- What actions should users be able to perform?\n- What should happen in edge cases?\n- How will you know the feature works correctly?\n```\n\n### Story Too Large\n\n**Scenario:** Story has too many acceptance criteria or sounds too complex.\n\n**Action:**\n1. Suggest the story might be too large for one sprint\n2. Identify potential split points\n3. Offer to create multiple stories\n4. Create parent epic if multiple related stories\n\n**Example:**\n```\nThis story has 12 acceptance criteria, which suggests it might be too large for one sprint.\n\nI can help split this into smaller stories:\n1. Core functionality (AC 1-4)\n2. Advanced features (AC 5-8)\n3. Edge cases and validation (AC 9-12)\n\nWould you like me to create these as separate stories under an epic?\n```\n\n### Vague Acceptance Criteria\n\n**Scenario:** AC is too vague or not testable.\n\n**Action:**\n1. Identify vague criteria\n2. Ask for specifics\n3. Suggest more concrete phrasing\n\n**Example:**\n```\nThis acceptance criteria is a bit vague: \"Test that it works well\"\n\nCan you be more specific? For example:\n- What does \"works well\" mean? (fast response time? handles errors?)\n- How would you test this? (what specific behavior would you verify?)\n\nSuggested revision: \"Verify that API response time is under 100ms for 95% of requests\"\n```\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in story content.\n\n**Action:**\n1. STOP submission\n2. Inform user what type of data was detected\n3. Ask for redaction or placeholder values\n\n**Example:**\n```\nI detected what appears to be API credentials in the acceptance criteria.\nPlease use placeholder values like \"YOUR_API_KEY\" instead of real credentials.\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the story.\n\n**Action:**\n1. Parse error message\n2. Provide user-friendly explanation\n3. Suggest corrective action\n4. Offer to retry\n\n**Common errors:**\n- **\"Field 'parent' does not exist\"**  Remove parent link or verify epic exists\n- **\"Invalid issue type\"**  Verify project supports \"Story\" issue type\n- **\"Component not found\"**  List available components, ask user to select\n\n## Examples\n\n### Example 1: Interactive Story Creation\n\n**Input:**\n```bash\n/jira:create story MYPROJECT \"Add user dashboard\"\n```\n\n**Conversation:**\n```\nLet's create the user story. I'll ask a few questions:\n\nWho is the user or role that will benefit from this feature?\n> End users\n\nWhat do they want to be able to do?\n> View their activity dashboard with recent actions and notifications\n\nWhy do they want this? What value does it provide?\n> To quickly see what's happened in their account without navigating multiple pages\n\nGreat! Here's the user story:\n\nAs an end user, I want to view my activity dashboard with recent actions and notifications, so that I can quickly see what's happened in my account without navigating multiple pages.\n\nDoes this look correct?\n> Yes\n\nNow let's define acceptance criteria. What are the key behaviors that must work?\n> 1. Dashboard shows last 10 actions\n> 2. Dashboard shows unread notifications\n> 3. Dashboard updates in real-time\n> 4. Users can filter by action type\n\nExcellent. I'll format these as test-based criteria:\n\n* Test that dashboard displays the last 10 user actions\n* Test that dashboard displays all unread notifications\n* Test that dashboard updates in real-time when new actions occur\n* Verify that users can filter actions by type (login, purchase, settings change)\n\nAny additional context?\n> This is part of the Q1 user experience improvements\n\nPerfect! Creating the story now...\n```\n\n**Result:**\n```\nCreated: MYPROJECT-123\nTitle: Add user dashboard\nURL: https://jira.example.com/browse/MYPROJECT-123\n```\n\n### Example 2: Story with Auto-Detection (CNTRLPLANE)\n\n**Input:**\n```bash\n/jira:create story CNTRLPLANE \"Enable pod disruption budgets for ROSA HCP control plane\"\n```\n\n**Auto-applied (via cntrlplane skill):**\n- Component: HyperShift / ROSA (detected from \"ROSA HCP\")\n- Target Version: openshift-4.21\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- User story format (Who/What/Why)\n- Acceptance criteria\n\n**Result:**\n- Full story created with CNTRLPLANE conventions\n\n### Example 3: Story with Parent Epic\n\n**Input:**\n```bash\n/jira:create story CNTRLPLANE \"Add scaling metrics to observability dashboard\" --parent CNTRLPLANE-100\n```\n\n**Implementation:**\n1. Pre-validate that CNTRLPLANE-100 exists and is an Epic\n2. Create story with Epic Link field:\n   ```python\n   additional_fields={\n       \"customfield_12311140\": \"CNTRLPLANE-100\",  # Epic Link (NOT parent field!)\n       \"labels\": [\"ai-generated-jira\"],\n       \"security\": {\"name\": \"Red Hat Employee\"}\n   }\n   ```\n3. If creation fails, use fallback: create without link, then update to add link\n\n**Result:**\n- Story created\n- Linked to epic CNTRLPLANE-100 via Epic Link field\n- All standard fields applied\n\n**See:** `/jira:create` command documentation for complete parent linking implementation strategy\n\n## Best Practices Summary\n\n1. **User-focused:** Always describe value from user perspective\n2. **Specific actions:** Clear what the user can do\n3. **Clear value:** Explicit why (benefit to user)\n4. **Testable AC:** Specific, observable criteria\n5. **Right-sized:** Can complete in one sprint\n6. **Conversational:** Story prompts discussion, not full spec\n7. **Independent:** Story can be implemented standalone\n8. **Valuable:** Delivers user value when complete\n\n## Anti-Patterns to Avoid\n\n **Technical tasks disguised as stories**\n```\nAs a developer, I want to refactor the database layer\n```\n Use a Task instead, or reframe with user value\n\n **Too many stories in one**\n```\nAs a user, I want to create, edit, delete, and share documents\n```\n Split into 4 separate stories\n\n **Vague acceptance criteria**\n```\n- Test that it works correctly\n- Verify good performance\n```\n Be specific: \"Response time under 200ms\", \"Handles 1000 concurrent users\"\n\n **Implementation details in AC**\n```\n- Test that the function uses Redis cache\n- Verify that the API calls the UserService.get() method\n```\n Focus on user-observable behavior, not implementation\n\n## Workflow Summary\n\n1.  Parse command arguments (project, summary, flags)\n2.  Auto-detect component from summary keywords\n3.  Apply project-specific defaults\n4.  Interactively collect user story (Who/What/Why)\n5.  Interactively collect acceptance criteria\n6.  Optionally collect additional context\n7.  Scan for sensitive data\n8.  Validate story quality and completeness\n9.  Format description with Jira markup\n10.  Create story via MCP tool\n11.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill (includes Issue Hierarchy and Parent Linking documentation)\n- `cntrlplane` skill - CNTRLPLANE specific conventions\n- `create-epic` skill - For creating parent epics\n- Agile Alliance: User Story resources\n- Mike Cohn: User Stories Applied"
              },
              {
                "name": "Create Jira Task",
                "description": "Implementation guide for creating Jira tasks for technical and operational work",
                "path": "plugins/jira/skills/create-task/SKILL.md",
                "frontmatter": {
                  "name": "Create Jira Task",
                  "description": "Implementation guide for creating Jira tasks for technical and operational work"
                },
                "content": "# Create Jira Task\n\nThis skill provides implementation guidance for creating Jira tasks, which are used for technical or operational work that doesn't necessarily deliver direct user-facing value.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:create task` command to guide the task creation process.\n\n## Prerequisites\n\n- MCP Jira server configured and accessible\n- User has permissions to create issues in the target project\n- Understanding of the technical work to be performed\n\n## Tasks vs Stories\n\n### When to Use a Task\n\nUse a **Task** when the work is:\n- **Technical/operational** - Infrastructure, refactoring, configuration\n- **Not user-facing** - No direct end-user functionality\n- **Internal improvement** - Code quality, performance, maintenance\n- **Enabler work** - Supports future stories but isn't user-visible\n\n**Examples of tasks:**\n- \"Update scaling documentation\"\n- \"Refactor authentication utility package\"\n- \"Configure CI pipeline for integration tests\"\n- \"Upgrade dependency X to version Y\"\n- \"Investigate performance regression in component Z\"\n\n### When to Use a Story Instead\n\nUse a **Story** when the work:\n- Delivers user-facing functionality\n- Can be expressed as \"As a... I want... so that...\"\n- Has business value to end users\n- Is part of a user workflow\n\n**If in doubt:** Ask \"Would an end user notice or care about this change?\"\n- **Yes**  Story\n- **No**  Task\n\n## Task Best Practices\n\n### Clear Summary\n\nThe summary should:\n- Be concise (one sentence)\n- Use action verbs (Update, Refactor, Configure, Investigate, Fix)\n- Identify what is being changed\n- Optionally include \"why\" if not obvious\n\n**Good examples:**\n- \"Update autoscaling documentation for 4.21 release\"\n- \"Refactor scaling controller to reduce code duplication\"\n- \"Configure Prometheus alerts for control plane memory usage\"\n- \"Investigate intermittent timeout in etcd health checks\"\n\n**Bad examples:**\n- \"Do some work on docs\" (too vague)\n- \"Technical debt\" (not specific)\n- \"Various improvements\" (not actionable)\n\n### Detailed Description\n\nThe description should include:\n\n1. **What needs to be done** - Clear statement of the work\n2. **Why it's needed** - Context or motivation\n3. **Acceptance criteria** (optional but recommended) - How to know it's done\n4. **Technical details** (if helpful) - Specific files, commands, approaches\n\n**Example:**\n```\nUpdate the autoscaling documentation to reflect changes in the 4.21 release.\n\nWhy: The autoscaling API changed in 4.21 with new fields and behavior. Documentation currently reflects 4.20 and will confuse users.\n\nAcceptance Criteria:\n- All autoscaling examples updated to use 4.21 API\n- New fields documented with descriptions and examples\n- Deprecated fields marked as deprecated\n- Documentation builds without warnings\n\nFiles to update:\n- docs/content/how-to/autoscaling.md\n- docs/content/reference/api.md\n```\n\n## Task Description Template\n\nUse this structure for consistency:\n\n```\n<What needs to be done>\n\nh2. Why\n\n<Context, motivation, or reason this is needed>\n\nh2. Acceptance Criteria\n\n* <Criterion 1>\n* <Criterion 2>\n* <Criterion 3>\n\nh2. Technical Details (optional)\n\n* Files to modify: <list>\n* Dependencies: <related issues or work>\n* Approach: <suggested implementation approach>\n```\n\n## Interactive Task Collection Workflow\n\nWhen creating a task, collect necessary information:\n\n### 1. Task Description\n\n**Prompt:** \"What work needs to be done? Be specific about what you'll change or update.\"\n\n**Helpful questions:**\n- What component or area is being worked on?\n- What specific changes will be made?\n- What's the end state after this task is complete?\n\n**Example response:**\n```\nRefactor the scaling controller to extract common validation logic into a shared utility package. Currently, validation code is duplicated across three controller files.\n```\n\n### 2. Motivation/Context\n\n**Prompt:** \"Why is this task needed? What problem does it solve?\"\n\n**Helpful questions:**\n- What prompted this work?\n- What will improve after this is done?\n- Is this addressing a specific issue or enabling future work?\n\n**Example response:**\n```\nCode duplication makes maintenance difficult. When validation logic changes, we have to update it in three places, which is error-prone. Consolidating into a shared utility will make the code easier to maintain and reduce bugs.\n```\n\n### 3. Acceptance Criteria (Optional but Recommended)\n\n**Prompt:** \"How will you know this task is complete? (Optional: skip if obvious)\"\n\n**For technical tasks, AC might include:**\n- Tests passing\n- Documentation updated\n- Code review completed\n- Specific functionality working\n\n**Example response:**\n```\n- Validation logic extracted to support/validation package\n- All three controllers updated to use shared validation\n- Existing tests pass\n- New unit tests added for validation utility\n- Code review approved\n```\n\n### 4. Parent Link (Optional)\n\n**Prompt:** \"Is this task part of a larger story or epic? (Optional)\"\n\n**If yes:**\n- Collect parent issue key\n- Verify parent exists\n- Link task to parent\n\n### 5. Additional Technical Details (Optional)\n\n**Prompt:** \"Any technical details to include? (files to change, dependencies, approach)\"\n\n**Example response:**\n```\nFiles to modify:\n- hypershift/operator/controllers/nodepool/autoscaling.go\n- hypershift/operator/controllers/hostedcluster/autoscaling.go\n- hypershift/operator/controllers/manifests/autoscaling.go\n- hypershift/support/validation/autoscaling.go (new)\n\nDependencies:\n- Must complete after PROJ-100 (validation refactor epic)\n\nApproach:\n- Extract common validation functions to support/validation\n- Add comprehensive unit tests for new package\n- Update controllers to import and use new package\n- Remove duplicated code\n```\n\n## Field Validation\n\nBefore submitting the task, validate:\n\n### Required Fields\n-  Summary is clear and specific\n-  Description explains what needs to be done\n-  Description includes why (motivation)\n-  Component is specified (if required by project)\n-  Target version is set (if required by project)\n\n### Task Quality\n-  Summary uses action verb (Update, Refactor, Configure, etc.)\n-  Work is technical/operational (not user-facing functionality)\n-  Description is detailed enough for someone else to understand\n-  Acceptance criteria present (if work is non-trivial)\n-  Task is sized appropriately (can complete in reasonable time)\n\n### Security\n-  No credentials, API keys, or secrets in any field\n-  No sensitive technical details that shouldn't be public\n\n## MCP Tool Parameters\n\n### Basic Task Creation\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"<PROJECT_KEY>\",\n    summary=\"<task summary>\",\n    issue_type=\"Task\",\n    description=\"\"\"\n<What needs to be done>\n\nh2. Why\n\n<Context and motivation>\n\nh2. Acceptance Criteria\n\n* <Criterion 1>\n* <Criterion 2>\n\nh2. Technical Details\n\n<Optional technical details>\n    \"\"\",\n    components=\"<component name>\",  # if required\n    additional_fields={\n        # Add project-specific fields\n    }\n)\n```\n\n### With Project-Specific Fields (e.g., CNTRLPLANE)\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"Update autoscaling documentation for 4.21 release\",\n    issue_type=\"Task\",\n    description=\"\"\"\nUpdate the autoscaling documentation to reflect API changes in the 4.21 release.\n\nh2. Why\n\nThe autoscaling API changed in 4.21 with new fields (maxNodeGracePeriod, scaleDownDelay) and modified behavior. Current documentation reflects 4.20 API and will confuse users upgrading to 4.21.\n\nh2. Acceptance Criteria\n\n* All autoscaling examples updated to use 4.21 API syntax\n* New fields (maxNodeGracePeriod, scaleDownDelay) documented with descriptions and examples\n* Deprecated fields marked as deprecated with migration guidance\n* Documentation builds successfully without warnings or broken links\n* Changes reviewed by docs team\n\nh2. Technical Details\n\nFiles to update:\n* docs/content/how-to/cluster-autoscaling.md\n* docs/content/reference/api/nodepool.md\n* docs/content/tutorials/autoscaling-rosa.md\n\nReference: API changes introduced in PR #1234\n    \"\"\",\n    components=\"HyperShift\",\n    additional_fields={\n        \"customfield_12319940\": \"openshift-4.21\",  # target version\n        \"labels\": [\"ai-generated-jira\", \"documentation\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### With Parent Link\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"MYPROJECT\",\n    summary=\"Add unit tests for scaling validation\",\n    issue_type=\"Task\",\n    description=\"<task content>\",\n    additional_fields={\n        \"parent\": {\"key\": \"MYPROJECT-100\"}  # link to story or epic\n    }\n)\n```\n\n## Jira Description Formatting\n\nUse Jira's native formatting (Wiki markup):\n\n### Task Template Format\n\n```\n<Brief description of what needs to be done>\n\nh2. Why\n\n<Context, motivation, or problem this solves>\n\nh2. Acceptance Criteria\n\n* <Criterion 1>\n* <Criterion 2>\n* <Criterion 3>\n\nh2. Technical Details\n\nh3. Files to Modify\n* {{path/to/file1.go}}\n* {{path/to/file2.go}}\n\nh3. Dependencies\n* Must complete after [PROJ-100]\n* Requires library X version Y\n\nh3. Approach\n<Suggested implementation approach or technical notes>\n\nh2. Additional Context\n\n<Optional: Links to designs, related issues, background>\n```\n\n### Formatting Elements\n\n**Headings:**\n```\nh1. Main Heading\nh2. Subheading\nh3. Sub-subheading\n```\n\n**Lists:**\n```\n* Bullet item 1\n* Bullet item 2\n\n# Numbered item 1\n# Numbered item 2\n```\n\n**Code/Paths:**\n```\n{{path/to/file.go}}\n{{package.function()}}\n\n{code}\nmake test\nmake build\n{code}\n```\n\n**Links:**\n```\n[Design doc|https://docs.example.com/design]\n[PROJ-123]  // Auto-links to Jira issue\n```\n\n## Error Handling\n\n### Task vs Story Confusion\n\n**Scenario:** User tries to create a task for user-facing functionality.\n\n**Action:**\n1. Detect user-facing language in summary/description\n2. Ask if this should be a story instead\n3. Explain the difference\n4. Offer to create as story if appropriate\n\n**Example:**\n```\nThis sounds like it might deliver user-facing functionality. The summary mentions \"users can configure autoscaling\".\n\nShould this be a Story instead of a Task?\n- Story: For user-facing features (visible to end users)\n- Task: For internal/technical work (not visible to end users)\n\nWould you like me to create this as a Story? (yes/no)\n```\n\n### Missing Context\n\n**Scenario:** User provides minimal description without context.\n\n**Action:**\n1. Ask for more details\n2. Prompt for \"why\" if missing\n3. Suggest adding acceptance criteria if non-trivial\n\n**Example:**\n```\nThe description \"Update docs\" is a bit brief. Can you provide more detail?\n\n- Which documentation needs updating?\n- Why does it need updating? (new features, corrections, clarifications?)\n- What specific changes should be made?\n```\n\n### Parent Not Found\n\n**Scenario:** User specifies `--parent` but issue doesn't exist.\n\n**Action:**\n1. Attempt to fetch parent issue\n2. If not found, inform user\n3. Offer options: proceed without parent, specify different parent, cancel\n\n**Example:**\n```\nParent issue PROJ-999 not found.\n\nOptions:\n1. Proceed without parent link\n2. Specify different parent\n3. Cancel task creation\n\nWhat would you like to do?\n```\n\n### Security Validation Failure\n\n**Scenario:** Sensitive data detected in task content.\n\n**Action:**\n1. STOP submission\n2. Inform user what type of data was detected\n3. Ask for redaction\n\n**Example:**\n```\nI detected what appears to be an API key in the technical details section.\nPlease use placeholder values like \"YOUR_API_KEY\" instead of real credentials.\n```\n\n### MCP Tool Error\n\n**Scenario:** MCP tool returns an error when creating the task.\n\n**Action:**\n1. Parse error message\n2. Provide user-friendly explanation\n3. Suggest corrective action\n\n**Common errors:**\n- **\"Field 'component' is required\"**  Prompt for component\n- **\"Invalid parent\"**  Verify parent issue exists and is correct type\n- **\"Permission denied\"**  User may lack project permissions\n\n## Examples\n\n### Example 1: Documentation Task\n\n**Input:**\n```bash\n/jira:create task CNTRLPLANE \"Update autoscaling documentation for 4.21 release\"\n```\n\n**Interactive prompts:**\n```\nWhat work needs to be done?\n> Update the autoscaling documentation to include new fields added in 4.21\n\nWhy is this task needed?\n> API changed in 4.21, docs need updating to match\n\nHow will you know this is complete?\n> All examples work with 4.21, new fields documented, no build warnings\n\nAny technical details?\n> Files: docs/content/how-to/autoscaling.md, docs/content/reference/api.md\n```\n\n**Result:**\n- Task created with complete description\n- Target version: 4.21\n- Component: HyperShift (or auto-detected)\n\n### Example 2: Refactoring Task\n\n**Input:**\n```bash\n/jira:create task MYPROJECT \"Refactor validation logic to reduce duplication\"\n```\n\n**Interactive prompts:**\n```\nWhat work needs to be done?\n> Extract common validation code from three controller files into shared utility\n\nWhy is this needed?\n> Code duplication makes maintenance difficult and error-prone\n\nAcceptance criteria?\n> - Validation extracted to support/validation package\n> - All controllers use shared validation\n> - Tests pass\n> - New unit tests for validation utility\n\nAny technical details?\n> Files to modify:\n> - controllers/nodepool/autoscaling.go\n> - controllers/hostedcluster/autoscaling.go\n> - controllers/manifests/autoscaling.go\n> New file: support/validation/autoscaling.go\n```\n\n**Result:**\n- Task with detailed technical plan\n- Clear acceptance criteria\n- Ready for implementation\n\n### Example 3: Task with Parent\n\n**Input:**\n```bash\n/jira:create task CNTRLPLANE \"Add integration tests for node autoscaling\" --parent CNTRLPLANE-100\n```\n\n**Auto-applied:**\n- Linked to parent story CNTRLPLANE-100\n- Inherits component from parent (if applicable)\n- CNTRLPLANE conventions applied\n\n**Result:**\n- Task created under parent story\n- All fields properly set\n\n### Example 4: Investigation Task\n\n**Input:**\n```bash\n/jira:create task CNTRLPLANE \"Investigate intermittent timeouts in etcd health checks\"\n```\n\n**Description pattern for investigation tasks:**\n```\nInvestigate intermittent timeout errors occurring in etcd health checks on ROSA HCP clusters.\n\nh2. Why\n\nUsers report clusters occasionally showing unhealthy status despite normal operation. Logs show intermittent timeout errors from etcd health checks.\n\nh2. Acceptance Criteria\n\n* Root cause identified and documented\n* Recommendation provided (fix, workaround, or \"no action needed\")\n* Findings shared with team in investigation summary\n\nh2. Technical Details\n\nError pattern:\n{code}\netcd health check failed: context deadline exceeded (timeout: 2s)\n{code}\n\nFrequency: ~5% of health checks\nAffected clusters: ROSA HCP in us-east-1\nLogs to review: control-plane-operator, etcd-operator\n\nRelated issues: OCPBUGS-1234 (similar symptoms)\n```\n\n**Result:**\n- Investigation task with clear scope\n- Defined outcome (root cause + recommendation)\n- Context for debugging\n\n## Best Practices Summary\n\n1. **Specific summaries:** Use action verbs, identify what's changing\n2. **Explain why:** Always include motivation/context\n3. **Add AC:** Even for tasks, AC helps define \"done\"\n4. **Technical details:** Include file paths, commands, approaches when helpful\n5. **Right size:** Task should be completable in reasonable time (days, not weeks)\n6. **Link to parent:** If task supports a story/epic, link it\n7. **Not a story:** If it's user-facing, create a story instead\n\n## Anti-Patterns to Avoid\n\n **Vague summaries**\n```\n\"Update stuff\"\n\"Fix things\"\n```\n Be specific: \"Update autoscaling documentation for 4.21 API changes\"\n\n **User-facing work as tasks**\n```\n\"Add user dashboard feature\"\n```\n Should be a Story if it delivers user value\n\n **Too large**\n```\n\"Refactor entire codebase\"\n\"Update all documentation\"\n```\n Break into smaller, focused tasks\n\n **No context**\n```\nSummary: \"Update docs\"\nDescription: <empty>\n```\n Always explain why and what specifically\n\n## Workflow Summary\n\n1.  Parse command arguments (project, summary, flags)\n2.  Auto-detect component from summary keywords\n3.  Apply project-specific defaults\n4.  Interactively collect task description and context\n5.  Interactively collect acceptance criteria (optional)\n6.  Optionally collect technical details\n7.  Scan for sensitive data\n8.  Validate task is appropriate (not a story)\n9.  Format description with Jira markup\n10.  Create task via MCP tool\n11.  Return issue key and URL\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `create-story` skill - For user-facing functionality\n- `cntrlplane` skill - CNTRLPLANE specific conventions\n- Agile task management best practices"
              },
              {
                "name": "Jira Pull Request Extractor",
                "description": "Recursively extract GitHub Pull Request links from Jira issues",
                "path": "plugins/jira/skills/extract-prs/SKILL.md",
                "frontmatter": {
                  "name": "Jira Pull Request Extractor",
                  "description": "Recursively extract GitHub Pull Request links from Jira issues"
                },
                "content": "# Jira Pull Request Extractor\n\nThis skill recursively discovers Jira issues and extracts all associated GitHub Pull Request links.\n\n**IMPORTANT FOR AI**: This is a **procedural skill** - when invoked, you should directly execute the implementation steps defined in this document. Do NOT look for or execute external scripts (like `extract_prs.py`). Follow the step-by-step instructions in the \"Implementation\" section below.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Discover all GitHub PRs associated with a Jira feature and its subtasks\n- Extract PR metadata without analyzing PR content\n- Build a complete map of PRs for documentation or release notes\n\n**Key characteristics**:\n-  **Always recursive**: Automatically discovers all descendant issues via `childIssuesOf()` JQL\n-  **PR-only**: Extracts only Pull Request URLs (ignores Issue and Commit URLs)\n-  **Dual-source**: Extracts from both changelog remote links and text content (description/comments)\n-  **Structured output**: Returns JSON with PR metadata and deduplication across sources\n\n## Prerequisites\n\n- **MCP Jira server configured and running** (required - see `plugins/jira/README.md` for setup)\n- `jq` installed for JSON parsing\n- GitHub CLI (`gh`) installed and authenticated for fetching PR metadata\n- User has read permissions for target Jira issues (including private issues via MCP authentication)\n- User has read access to linked GitHub repositories\n\n## Output Format\n\n**Purpose**: This skill returns structured JSON that serves as an interface contract for consuming skills and commands.\n\n**Delivery Method**:\n- **Primary**: Output JSON directly to the response (no file writes, no user prompts)\n- **Secondary**: Only save to `.work/extract-prs/{issue-key}/output.json` if user explicitly requests to save results\n\n**Schema Version**: `1.0`\n\n### Structure\n\n```json\n{\n  \"schema_version\": \"1.0\",\n  \"metadata\": {\n    \"generated_at\": \"2025-11-24T10:30:00Z\",\n    \"command\": \"extract_prs\",\n    \"input_issue\": \"OCPSTRAT-1612\"\n  },\n  \"pull_requests\": [\n    {\n      \"url\": \"https://github.com/openshift/hypershift/pull/6444\",\n      \"state\": \"MERGED\",\n      \"title\": \"Add support for custom OVN subnets\",\n      \"isDraft\": false,\n      \"sources\": [\"comment\", \"description\", \"remote_link\"],\n      \"found_in_issues\": [\"CNTRLPLANE-1201\", \"OCPSTRAT-1612\"]\n    }\n  ]\n}\n```\n\n**Fields**:\n- `schema_version`: Format version (`\"1.0\"`)\n- `metadata`: Generation timestamp, command name, and input issue\n- `pull_requests`: Array of PR objects with `url`, `state`, `title`, `isDraft`, `sources`, and `found_in_issues`\n\n\n## Implementation\n\nThe skill operates in three main phases:\n\n###  Phase 1: Descendant Issue Discovery\n\nDiscovers all descendant issues using Jira's `childIssuesOf()` JQL function (automatically recursive).\n\n**Implementation**:\n\n1. **Fetch issue metadata using MCP Jira tool**:\n   ```\n   mcp__atlassian__jira_get_issue(\n     issue_key=<issue-key>,\n     fields=\"summary,description,issuetype,status,comment\",\n     expand=\"changelog\"\n   )\n   ```\n   - Extract `fields.description` - for text-based PR URL extraction\n   - Extract `fields.comment.comments` - for PR URLs mentioned in comments\n   - Extract `changelog.histories` - for remote link PR URLs from `RemoteIssueLink` field changes\n\n2. **Search for ALL descendant issues using JQL**:\n   ```\n   mcp__atlassian__jira_search(\n     jql=\"issue in childIssuesOf(<issue-key>)\",\n     fields=\"key\",\n     limit=100\n   )\n   ```\n   - **Important**: `childIssuesOf()` is **already recursive** - returns ALL descendant issues (Epics, Stories, Subtasks, etc.) regardless of depth\n   - Single JQL query gets everything - no manual recursion needed\n   - Only fetch `key` field here - will fetch full data (including changelog) per-issue in Phase 2\n   - **Note**: `jira_search` does NOT support `expand` parameter - use `jira_get_issue` with `expand=\"changelog\"` for each issue\n\n3. **Fetch full data for each issue** (including root + all descendants):\n   ```\n   for each issue_key:\n     mcp__atlassian__jira_get_issue(\n       issue_key=<issue-key>,\n       fields=\"summary,description,issuetype,status,comment\",\n       expand=\"changelog\"\n     )\n   ```\n   - This fetches description, comments, and changelog (which includes remote links)\n   - Excludes issue links (`relates to`, `blocks`, etc.) - only parent-child relationships\n\n###  Phase 2: GitHub PR Extraction\n\nExtracts PR URLs from two sources:\n\n### Source 1: Jira Remote Links via Changelog (primary)\n- **Extract remote links from issue changelog** (using MCP with authenticated access):\n  ```bash\n  # Fetch issue with changelog expansion (store in variable)\n  issue_json=$(mcp__atlassian__jira_get_issue \\\n    issue_key=\"${issue_key}\" \\\n    expand=\"changelog\")\n\n  # Extract RemoteIssueLink entries from changelog\n  pr_urls=$(echo \"$issue_json\" | jq -r '\n    .changelog.histories[]?.items[]? |\n    select(.field == \"RemoteIssueLink\") |\n    .toString // .to_string |\n    match(\"https://github\\\\.com/[^/]+/[^/]+/(pull|pulls)/[0-9]+\") |\n    .string\n  ' | sort -u)\n  ```\n- **Important**:\n  - Remote links appear in changelog as `RemoteIssueLink` field changes\n  - Changelog contains link creation events with GitHub PR URLs in `toString` or `to_string` field\n  - Store results in variables, not temporary files\n  - Uses MCP authentication (no separate curl needed)\n- Filters for GitHub PR URLs matching `/pull/` or `/pulls/` pattern\n\n### Source 2: Text Content (backup)\n- Searches both `fields.description` and `fields.comment.comments[]` (already fetched in Phase 1)\n- **Description**: Extract from `fields.description` (plain text or Jira wiki format)\n- **Comments**: Iterate through `fields.comment.comments[]` array and search each `comment.body`\n- Uses regex: `https?://github\\.com/([\\w-]+)/([\\w-]+)/pulls?/(\\d+)`\n- **Example extraction (using variables)**:\n  ```bash\n  # From description (stored in variable from MCP response)\n  description_prs=$(echo \"$description\" | \\\n    grep -oE 'https?://github\\.com/[^/]+/[^/]+/pulls?/[0-9]+')\n\n  # From comments (parse JSON in memory)\n  comment_prs=$(echo \"$issue_json\" | \\\n    jq -r '.fields.comment.comments[]?.body // empty' | \\\n    grep -oE 'https?://github\\.com/[^/]+/[^/]+/pulls?/[0-9]+')\n\n  # Combine all PRs\n  all_prs=$(echo -e \"${description_prs}\\n${comment_prs}\" | sort -u)\n  ```\n\n**Deduplication**:\n- Merges URLs found in multiple sources/issues\n- Tracks `sources` array: `[\"comment\", \"description\", \"remote_link\"]` (alphabetically sorted)\n  - `\"comment\"`: Found in issue comments\n  - `\"description\"`: Found in issue description\n  - `\"remote_link\"`: Found via Jira Remote Links API\n- Tracks `found_in_issues` array: `[\"OCPSTRAT-1612\", \"CNTRLPLANE-1201\"]` (alphabetically sorted)\n- **Important**: If same PR URL found in multiple sources or issues, merge into single entry with combined arrays\n\n###  Phase 3: Data Structuring and Output\n\n**PR Metadata**: Fetch via `gh pr view {url} --json state,title,isDraft`. **CRITICAL**: When building the output JSON, you MUST use the exact values returned by `gh pr view` - do NOT manually type or guess PR states/titles.\n\n**Output**: Build JSON in memory using `jq -n`, output to console. Only save to `.work/extract-prs/{issue-key}/output.json` if user explicitly requests.\n\n**Important**: Use bash variables for all data - no temporary files to avoid user confirmation prompts.\n\n## Error Handling\n\n- **MCP server not available**: Display error message directing user to configure MCP server (see Prerequisites)\n- **Issue not found**: Log warning and continue with remaining issues\n- **Permission denied**:\n  - If MCP returns 403 for private issues, verify JIRA_PERSONAL_TOKEN is configured correctly\n  - MCP authentication handles all Jira access (including changelog and remote links)\n- **Changelog expansion fails**: If `expand=\"changelog\"` returns error, continue with text-based extraction only (graceful degradation)\n- **No PRs found**: Return empty `pull_requests` array (valid result)\n- **Too many descendants**: If hierarchy has >100 issues, increase `limit` parameter in `jira_search`\n- **GitHub rate limit**: If `gh pr view` fails due to rate limiting, display error with reset time\n- **PR metadata fetch fails**: If `gh pr view` returns error (PR deleted/private), exclude that PR from output\n\n## Performance Considerations\n\n**API calls**: 1 `jira_search` + N `jira_get_issue` (with changelog) + M `gh pr view`\n- Example: 11 issues = 12 MCP calls + M PR fetches\n- Changelog expansion includes remote links (no extra calls needed)\n- Can parallelize: issue fetching and PR metadata fetching\n\n**File I/O**: Save PR metadata to `.work/extract-prs/{issue-key}/pr-*-metadata.json`, build final JSON by reading files"
              },
              {
                "name": "HyperShift Jira Conventions",
                "description": "HyperShift team-specific Jira requirements for component selection and conventions",
                "path": "plugins/jira/skills/hypershift/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift Jira Conventions",
                  "description": "HyperShift team-specific Jira requirements for component selection and conventions"
                },
                "content": "# HyperShift Jira Conventions\n\nThis skill provides HyperShift team-specific conventions for creating Jira issues in CNTRLPLANE and OCPBUGS projects.\n\n## When to Use This Skill\n\nThis skill is automatically invoked when:\n- Summary or description contains HyperShift keywords: \"HyperShift\", \"ARO HCP\", \"ROSA HCP\", \"hosted control plane\"\n- Component contains \"HyperShift\"\n- User explicitly requests HyperShift conventions\n\nThis skill works **in conjunction with** the `cntrlplane` skill, adding HyperShift-specific requirements on top of generic CNTRLPLANE/OCPBUGS conventions.\n\n## Component Requirements\n\n**ALL** HyperShift issues in CNTRLPLANE and OCPBUGS **must** have a component set to one of:\n\n1. **HyperShift / ARO** - ARO HCP (Azure Red Hat OpenShift Hosted Control Planes)\n2. **HyperShift / ROSA** - ROSA HCP (Red Hat OpenShift Service on AWS Hosted Control Planes)\n3. **HyperShift** - When it's not clear if the issue is about AWS, Azure, or agent platform\n\n### Component Selection Logic\n\n**Auto-detection based on summary/description keywords:**\n\n| Keywords | Component | Confidence |\n|----------|-----------|------------|\n| ARO, Azure, \"ARO HCP\" | **HyperShift / ARO** | High |\n| ROSA, AWS, \"ROSA HCP\" | **HyperShift / ROSA** | High |\n| Both ARO and ROSA mentioned | **HyperShift** | High (multi-platform) |\n| \"All platforms\", \"platform-agnostic\" | **HyperShift** | Medium (verify with user) |\n| **No platform keywords** | **Prompt user** | N/A (cannot auto-detect) |\n\n**Important:** If no platform keywords are found, do NOT assume platform-agnostic. Prompt the user to clarify which component.\n\n**Examples:**\n```\nSummary: \"Enable autoscaling for ROSA HCP clusters\"\n Component: HyperShift / ROSA (auto-detected)\n\nSummary: \"ARO HCP control plane pods crash on upgrade\"\n Component: HyperShift / ARO (auto-detected)\n\nSummary: \"Multi-cloud support for ARO and ROSA HCP\"\n Component: HyperShift (auto-detected, mentions both platforms)\n\nSummary: \"Improve control plane pod scheduling\"\n Component: Prompt user (no keywords, cannot determine platform)\n```\n\n### When Auto-Detection is Uncertain\n\nIf component cannot be confidently auto-detected:\n1. Present options to user with descriptions\n2. Ask for clarification\n\n**Prompt example:**\n```\nWhich HyperShift platform does this issue affect?\n\n1. HyperShift / ARO - for ARO HCP (Azure) issues\n2. HyperShift / ROSA - for ROSA HCP (AWS) issues\n3. HyperShift - for platform-agnostic issues or affects both\n\nSelect (1-3):\n```\n\n## Version Defaults\n\nHyperShift team uses specific version defaults:\n\n### CNTRLPLANE Issues\n\n**Target Version** (customfield_12319940):\n- **Default:** `openshift-4.21`\n- **Override:** User may specify different versions (e.g., `4.20`, `4.22`, `4.23`)\n\n### OCPBUGS Issues\n\n**Affects Version/s**:\n- **Default:** `4.21`\n- **User should specify:** The actual version where the bug was found\n\n**Target Version** (customfield_12319940):\n- **Default:** `4.21`\n- **Override:** May be different based on severity and backport requirements\n\n## Labels\n\nIn addition to `ai-generated-jira` (from CNTRLPLANE skill), HyperShift issues may include:\n\n**Platform-specific:**\n- `aro-hcp` - ARO HCP specific\n- `rosa-hcp` - ROSA HCP specific\n\n**Feature area:**\n- `autoscaling`\n- `networking`\n- `observability`\n- `upgrade`\n- `lifecycle`\n\n**Priority/type:**\n- `technical-debt`\n- `security`\n- `performance`\n\n## MCP Tool Integration\n\n### For HyperShift Stories/Tasks in CNTRLPLANE\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"CNTRLPLANE\",\n    summary=\"<issue summary>\",\n    issue_type=\"Story\" | \"Task\" | \"Epic\" | \"Feature\",\n    description=\"<formatted description>\",\n    components=\"HyperShift / ARO\" | \"HyperShift / ROSA\" | \"HyperShift\",\n    additional_fields={\n        \"customfield_12319940\": \"openshift-4.21\",  # target version\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### For HyperShift Bugs in OCPBUGS\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"OCPBUGS\",\n    summary=\"<bug summary>\",\n    issue_type=\"Bug\",\n    description=\"<formatted bug template>\",\n    components=\"HyperShift / ARO\" | \"HyperShift / ROSA\" | \"HyperShift\",\n    additional_fields={\n        \"versions\": [{\"name\": \"4.21\"}],          # affects version\n        \"customfield_12319940\": \"4.21\",           # target version\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n## Examples\n\n### Example 1: ROSA HCP Story (Auto-Detection)\n\n**Input:**\n```bash\n/jira:create story CNTRLPLANE \"Enable automatic node pool scaling for ROSA HCP\"\n```\n\n**Auto-detected:**\n- Component: **HyperShift / ROSA** (detected from \"ROSA HCP\")\n- Target Version: openshift-4.21\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- User story format (As a... I want... So that...)\n- Acceptance criteria\n\n**Result:**\n- Story created with HyperShift / ROSA component\n- All CNTRLPLANE conventions applied\n\n### Example 2: ARO HCP Bug\n\n**Input:**\n```bash\n/jira:create bug \"ARO HCP control plane pods crash on upgrade\"\n```\n\n**Auto-detected:**\n- Project: OCPBUGS (default for bugs)\n- Component: **HyperShift / ARO** (detected from \"ARO HCP\")\n- Affected Version: 4.21 (default, user can override)\n- Target Version: 4.21\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- Bug template sections\n\n**Result:**\n- Bug created in OCPBUGS with HyperShift / ARO component\n\n### Example 3: Platform-Agnostic Epic\n\n**Input:**\n```bash\n/jira:create epic CNTRLPLANE \"Improve HyperShift operator observability\"\n```\n\n**Auto-detected:**\n- Component: **HyperShift** (platform-agnostic, from \"HyperShift operator\")\n- Target Version: openshift-4.21\n- Epic Name: Same as summary\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- Epic objective and scope\n- Acceptance criteria\n\n**Result:**\n- Epic created with HyperShift component (not platform-specific)\n\n### Example 4: Multi-Platform Feature\n\n**Input:**\n```bash\n/jira:create feature CNTRLPLANE \"Advanced observability for ROSA and ARO HCP\"\n```\n\n**Auto-detected:**\n- Component: **HyperShift** (affects both platforms)\n- Target Version: openshift-4.21\n- Labels: ai-generated-jira\n- Security: Red Hat Employee\n\n**Interactive prompts:**\n- Market problem\n- Strategic value\n- Success criteria\n- Epic breakdown\n\n**Result:**\n- Feature with HyperShift component (since it affects both platforms)\n\n### Example 5: Uncertain Component (Prompts User)\n\n**Input:**\n```bash\n/jira:create story CNTRLPLANE \"Improve control plane pod scheduling\"\n```\n\n**Detection:** Summary doesn't contain platform-specific keywords\n\n**Prompt:**\n```\nWhich HyperShift platform does this issue affect?\n\n1. HyperShift / ARO - for ARO HCP (Azure) issues\n2. HyperShift / ROSA - for ROSA HCP (AWS) issues\n3. HyperShift - for platform-agnostic issues or affects both\n\nSelect (1-3):\n```\n\n**User selects:** 3\n\n**Result:**\n- Component set to **HyperShift**\n\n## Component Override\n\nUser can override auto-detection using `--component` flag:\n\n```bash\n# Override auto-detection\n/jira:create story CNTRLPLANE \"Enable autoscaling for ROSA HCP\" --component \"HyperShift\"\n```\n\nThis will use \"HyperShift\" component instead of auto-detected \"HyperShift / ROSA\".\n\n## Error Handling\n\n### Invalid Component\n\n**Scenario:** User specifies component that's not a valid HyperShift component.\n\n**Action:**\n```\nComponent \"Networking\" is not a valid HyperShift component.\n\nHyperShift issues must use one of:\n- HyperShift / ARO\n- HyperShift / ROSA\n- HyperShift\n\nWhich component would you like to use?\n```\n\n### Component Required but Missing\n\n**Scenario:** Component cannot be auto-detected and user didn't specify.\n\n**Action:**\n```\nHyperShift issues require a component. Which component?\n\n1. HyperShift / ARO - for ARO HCP (Azure) issues\n2. HyperShift / ROSA - for ROSA HCP (AWS) issues\n3. HyperShift - for platform-agnostic issues\n\nSelect (1-3):\n```\n\n## Workflow Summary\n\nWhen creating a HyperShift issue:\n\n1.  **CNTRLPLANE skill loads** - Applies generic conventions (security, labels, versions)\n2.  **HyperShift skill loads** - Adds HyperShift-specific requirements\n3.  **Auto-detect component** - Analyze summary/description for ARO/ROSA keywords\n4.  **Apply component:**\n   - If auto-detected with high confidence  Use detected component\n   - If uncertain  Prompt user for component selection\n   - If `--component` flag provided  Use specified component (validate it's HyperShift)\n5.  **Interactive prompts** - Collect issue type-specific information\n6.  **Security scan** - Validate no credentials/secrets\n7.  **Create issue** - Use MCP tool with HyperShift component\n8.  **Return result** - Issue key, URL, applied defaults (including component)\n\n## Best Practices\n\n1. **Include platform keywords in summary** - Makes auto-detection more accurate\n   -  \"Enable autoscaling for ROSA HCP\"\n   -  \"Enable autoscaling\" (unclear which platform)\n\n2. **Be specific about platform when known**\n   - If issue is ARO-specific, mention \"ARO\" or \"Azure\" in summary\n   - If issue is ROSA-specific, mention \"ROSA\" or \"AWS\" in summary\n\n3. **Use platform-agnostic component wisely**\n   - Only use \"HyperShift\" (without /ARO or /ROSA) when issue truly affects all platforms\n   - When in doubt, ask the team\n\n4. **Component consistency within epic**\n   - Stories within an epic should generally have the same component as the epic\n   - Exception: Epic is platform-agnostic but stories target specific platforms\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `cntrlplane` skill - Generic CNTRLPLANE/OCPBUGS conventions\n- HyperShift team documentation"
              },
              {
                "name": "Jira Feature Documentation Generator",
                "description": "Detailed implementation guide for recursively analyzing Jira features and generating comprehensive documentation",
                "path": "plugins/jira/skills/jira-doc-generator/SKILL.md",
                "frontmatter": {
                  "name": "Jira Feature Documentation Generator",
                  "description": "Detailed implementation guide for recursively analyzing Jira features and generating comprehensive documentation"
                },
                "content": "# Jira Feature Documentation Generator\n\nThis skill provides detailed step-by-step implementation guidance for the `/jira:generate-feature-doc` command, which generates comprehensive feature documentation by recursively analyzing a Jira feature and all its related issues and GitHub pull requests.\n\n**IMPORTANT FOR AI**: This is a **procedural skill** - when invoked, you should directly execute the implementation steps defined in this document. Do NOT look for or execute external scripts. Follow the step-by-step instructions below, starting with Step 1.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/jira:generate-feature-doc` command and should not be called directly by users.\n\n## Prerequisites\n\n- **MCP Jira server configured and running** (required - see `plugins/jira/README.md` for setup)\n- GitHub CLI (`gh`) installed and authenticated (for analyzing PRs)\n- User has read access to Jira issues (including private issues via MCP authentication)\n- User has read access to linked GitHub repositories\n- Working directory has `.work/jira/feature-doc/` for output (will be created if needed)\n\n## Implementation Steps\n\n### Step 1: Initialize and Fetch Main Feature Issue\n\n**Objective**: Set up environment and fetch main feature issue.\n\n**Actions**:\n\n1. **Save initial directory**: `INITIAL_DIR=$(pwd)` (save at start, before any cd commands)\n\n2. **Check prerequisites**: Verify `jq` and `gh` CLI are installed and authenticated\n   - If missing, display error with installation instructions\n\n3. **Create output directory**: `WORK_DIR=$INITIAL_DIR/.work/jira/feature-doc/<feature-key>` (use `mkdir -p`)\n\n4. **Fetch main feature**: Use MCP Jira tool to get issue with all fields:\n   ```\n   mcp__atlassian__jira_get_issue(issue_key=<feature-key>, fields=\"*all\")\n   ```\n   - If MCP unavailable, display error pointing to `plugins/jira/README.md`\n\n5. **Parse response**: Extract `key`, `summary`, `description`, `issuetype`, `status`\n   - If fetch fails, display error and exit\n\n6. **Display progress**: Show feature summary and type\n\n### Step 2: Analyze Each GitHub PR\n\n**Important**: This step expects PR data as input from the `jira:extract-prs` skill (invoked by the command file). The input is structured JSON containing all discovered PRs with their metadata.\n\n**Input Format**:\n```json\n{\n  \"pull_requests\": [\n    {\n      \"url\": \"https://github.com/org/repo/pull/123\",\n      \"state\": \"MERGED\",\n      \"title\": \"PR title\",\n      \"isDraft\": false,\n      \"sources\": [\"remote_link\", \"description\"],\n      \"found_in_issues\": [\"ISSUE-123\"]\n    }\n  ]\n}\n```\n\n**Objective**: Fetch and analyze PR details to extract implementation information.\n\n**Important**: This command is for documenting **completed features**. Only analyze PRs that have been **MERGED**. Skip OPEN, DRAFT, WIP, or CLOSED (but not merged) PRs.\n\n**Actions**:\n\n1. **Filter for MERGED PRs**: Only analyze PRs with `state == \"MERGED\"` AND `isDraft == false`\n   - Skip OPEN PRs (work in progress)\n   - Skip PRs with `isDraft == true` (not ready for review)\n   - Skip CLOSED PRs (not merged)\n\n2. **For each MERGED PR, fetch detailed data**:\n   - Parse PR URL to extract org/repo/number\n   - Fetch metadata: `gh pr view {number} --repo {org}/{repo} --json title,body,mergedAt,author,commits,files`\n   - Fetch diff: `gh pr diff {number} --repo {org}/{repo}`\n   - Fetch comments: `gh pr view {number} --repo {org}/{repo} --json comments`\n\n3. **Extract key information**:\n   - **From body**: Purpose, approach, breaking changes\n   - **From commits**: Implementation steps, testing\n   - **From diff**: New APIs, architectural changes, config updates\n   - **From comments**: Design decisions, rationale, considerations\n\n4. **Handle errors**: If PR inaccessible (403, 404), log warning and continue\n\n5. **Save data**: Store metadata, diff, and comments to `${WORK_DIR}/pr-{org}-{repo}-{number}.*`\n\n### Step 3: Synthesize Documentation Structure\n\n**Objective**: Organize information into structured outline.\n\n**Available sections** (calling command specifies which to generate):\n\n1. **Overview**: Jira link, status, counts, dates, authors (from main issue + PR metadata)\n2. **Background and Goals**: Description from main issue (clean Jira formatting)\n3. **Architecture and Design**: High-level changes, components, design decisions (from PRs)\n4. **Implementation Details**: Core changes, API changes, configuration (from PR diffs, code snippets)\n5. **Usage Guide**: Prerequisites, basic/advanced usage (from README updates, PR descriptions)\n6. **Testing**: Test coverage, strategies, key test PRs\n7. **Related Resources**: Can include external links, issue tables, PR tables, dependency graphs\n\n### Step 4: Generate Documentation Content\n\n**Objective**: Fill in the outline with actual content based on command requirements.\n\n**Section generation guidelines**:\n\n1. **Overview**: Extract from main issue + PR metadata (Jira link, status, counts, dates, authors)\n2. **Background**: Clean Jira formatting from main issue description\n3. **Architecture**: Synthesize from PR descriptions and comments (high-level overview, components, decisions with PR links)\n4. **Implementation**: Group by core changes, API changes, configuration (include code snippets, link to PRs, list key files)\n5. **Usage**: Extract from README updates and PR descriptions (prerequisites, YAML/CLI examples)\n6. **Testing**: Summarize by category (unit, E2E, CI), list key test PRs\n7. **Related Resources**: Format depends on command requirements (external links, tables, graphs)\n\n**Note**: Only generate sections specified by the calling command. Check the command file for exact requirements.\n\n### Step 5: Output\n\n**Objective**: Save documentation and display summary.\n\n**Actions**:\n\n1. **Write documentation**: Save to `${WORK_DIR}/feature-doc.md` with footer:\n   ```markdown\n   ---\n   *Generated by `/jira:generate-feature-doc` on <timestamp>*\n   *Source: <feature-key> and <count> related issues*\n   ```\n\n2. **Save metadata**: Store analysis log with timestamps, counts, output files, errors/warnings\n\n3. **Display summary**:\n   - Success message with file location\n   - Statistics: issues analyzed, PRs analyzed, commits, files changed, doc lines\n   - Warnings if any (inaccessible PRs, missing descriptions)\n   - Additional files for debugging\n\n## Error Handling\n\n**Issue Not Found** (404, 403, network error):\n- Display error with verification steps (issue key format, permissions, MCP config)\n- Exit gracefully without creating files\n\n**No PRs Found**:\n- Display warning (feature not implemented, PRs not linked, or small feature)\n- Generate documentation from main issue only\n\n**GitHub Rate Limit**:\n- Display error with progress and reset time\n- Offer options: wait, generate from partial data, or cancel\n\n**Large Feature** (>50 PRs):\n- Display warning with estimated time and API calls\n- Offer options: continue or cancel\n\n**Malformed Issue Data**:\n- Log warning about missing/invalid fields\n- Continue with remaining issues (don't fail entire process)\n\n## Performance Optimization\n\n**Parallel PR Analysis**:\n- For >10 PRs, use parallel processing (`xargs -P 5`)\n- Limit to ~5 concurrent requests to avoid rate limits\n\n**Smart Diff Analysis**:\n- Use `--stat` to identify key files\n- Skip vendor/, generated files, test fixtures\n- Fetch full diff only for critical files\n\n## Best Practices for AI Implementation\n\n1. **Progress feedback**: Show progress after each major step (discovery, PR analysis, etc.)\n\n2. **Error resilience**: Don't fail the entire process if one PR is inaccessible\n\n3. **Smart synthesis**: Don't just concatenate PR descriptions - synthesize into coherent narrative\n\n4. **Context awareness**: Understand the codebase domain (e.g., Kubernetes, OpenShift) to better interpret changes\n\n5. **Structured output**: Use consistent markdown formatting with proper headers, code blocks, tables\n\n6. **Link preservation**: Always provide clickable links to Jira issues and GitHub PRs\n\n7. **Timestamp tracking**: Note when PRs were merged to understand timeline\n\n8. **Author attribution**: Credit authors of PRs and issues where relevant\n\n9. **Code examples**: Include actual code snippets from PRs to illustrate changes\n\n10. **Visual hierarchy**: Use tables, lists, and headers to make documentation scannable\n\n## Example Workflow\n\n```\nUser runs: /jira:generate-feature-doc OCPSTRAT-1612\n\n1. Initialize\n   - Fetch main feature issue (OCPSTRAT-1612)\n   - Create working directory (.work/jira/feature-doc/OCPSTRAT-1612/)\n   - Verify prerequisites (jq, gh CLI)\n\n2. Extract PRs (via extract-prs skill)\n   - Discover descendants using childIssuesOf() JQL  3 issues total\n   - Extract PRs from remote links (primary) + text (backup)  7 PRs\n   - Fetch PR state from GitHub  5 MERGED, 1 OPEN, 1 CLOSED\n\n3. Analyze MERGED PRs\n   - Filter for MERGED PRs  5 PRs to analyze\n   - For each: fetch metadata + diff + comments\n   - Extract implementation details, design decisions\n\n4. Generate Documentation\n   - Synthesize sections: Overview, Architecture, Implementation, Usage, Testing\n   - Create tables for issues and PRs\n   - Write to feature-doc.md\n\n5. Display Results\n    Documentation generated successfully!\n    File: .work/jira/feature-doc/OCPSTRAT-1612/feature-doc.md\n    3 issues, 5 MERGED PRs, ~380 lines generated\n```"
              },
              {
                "name": "JIRA Release Blocker Validator",
                "description": "Detailed implementation guide for validating proposed release blockers",
                "path": "plugins/jira/skills/jira-validate-blockers/SKILL.md",
                "frontmatter": {
                  "name": "JIRA Release Blocker Validator",
                  "description": "Detailed implementation guide for validating proposed release blockers",
                  "command": "/jira:validate-blockers"
                },
                "content": "# JIRA Release Blocker Validator - Implementation Guide\n\nThis skill provides detailed implementation guidance for the `/jira:validate-blockers` command, which helps release managers make data-driven blocker approval/rejection decisions.\n\n## When to Use This Skill\n\nThis skill is invoked automatically when the `/jira:validate-blockers` command is executed. It provides step-by-step implementation details for:\n- Querying JIRA for proposed release blockers (Release Blocker = Proposed)\n- Scoring blockers against Red Hat OpenShift release blocker criteria\n- Generating APPROVE/REJECT/DISCUSS recommendations\n\n## Prerequisites\n\n- Jira MCP server must be configured (see plugin README)\n- MCP tools available: `mcp__atlassian__jira_*`\n- Read-only access to JIRA APIs (no credentials required for public Red Hat JIRA issues)\n- For single bug mode: bug must be accessible and exist\n\n## Detailed Implementation Steps\n\n### Phase 1: Parse Arguments\n\n**Parse command-line arguments:**\n- Extract target version from $1 (optional, format: X.Y like \"4.21\")\n- Extract component filter from $2 (optional, supports comma-separated values)\n- Extract `--bug` flag value (optional, for single bug validation mode)\n\n**Project:**\n- Hardcoded to \"OCPBUGS\" project\n\n**Validate inputs:**\n- If neither `--bug` nor `target-version` is provided, error out with message: \"Error: Either target-version or --bug must be provided. Usage: /jira:validate-blockers [target-version] [component-filter] [--bug issue-key]\"\n- If target version provided, verify it matches pattern X.Y (e.g., \"4.21\", \"4.22\")\n- If component filter provided without target version and without --bug, error out\n\n### Phase 2: Build JQL Query for Proposed Blockers\n\n**Determine query mode:**\n\n1. **Single bug mode** (if `--bug` is provided):\n   - Skip JQL query construction\n   - Use `mcp__atlassian__jira_get_issue` to fetch the single bug\n   - Target version and component filter are ignored in this mode\n   - Proceed to analysis with single bug only\n\n2. **Version + component mode** (if both target version and component are provided):\n   - Build JQL query for proposed blockers matching target version and component filter\n   - Continue with query construction below\n\n3. **Version only mode** (if only target version provided):\n   - Build JQL query for all proposed blockers for the target version\n   - Continue with query construction below\n\n**Base JQL for proposed blockers:**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed\n```\n\n**IMPORTANT**: Use `\"Release Blocker\" = Proposed` NOT `cf[12319743]`. The field ID `customfield_12319743` is the Release Blocker field, but in JQL use the field name.\n\n**Version filter construction:**\n\nWhen target version is provided (e.g., \"4.21\"), expand to search for both X.Y and X.Y.0:\n\n```jql\nAND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0))\n```\n\n**Status exclusion filter:**\n\nAlways exclude already-fixed bugs:\n\n```jql\nAND status not in (Closed, \"Release Pending\", Verified, ON_QA)\n```\n\n**Component filter construction:**\n\n**No component specified:**\n- Query all components (no component filter in JQL)\n\n**Single component:**\n```jql\nAND component = \"{COMPONENT}\"\n```\n\n**Multiple components (comma-separated):**\n```jql\nAND component IN ({COMPONENT_LIST})\n```\n\n**Final JQL examples:**\n\n**Version only (4.21):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA)\n```\n\n**Version + component (4.21, \"Hypershift\"):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA) AND component = \"Hypershift\"\n```\n\n**Version + multiple components (4.21, \"Hypershift,CVO\"):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA) AND component IN (\"Hypershift\", \"Cluster Version Operator\")\n```\n\n### Phase 3: Query Proposed Blockers\n\n**Use MCP tools to fetch proposed blockers:**\n\nFor version/component mode, use `mcp__atlassian__jira_search`:\n- **jql**: The constructed JQL query from Phase 2\n- **fields**: \"key,summary,priority,severity,status,assignee,created,updated,labels,components,description,reporter,customfield_12319743,customfield_12319940\"\n- **expand**: \"renderedFields\" (to get comments for workaround analysis)\n- **limit**: 1000 (adjust based on expected results)\n\nParse the response to extract:\n- Total count of proposed blockers\n- List of bug objects with all required fields\n\nCustom fields to include:\n- `customfield_12319743` - Release Blocker status (should be \"Proposed\")\n- `customfield_12319940` - Target Version\n\nFor single bug mode (`--bug` flag), use `mcp__atlassian__jira_get_issue`:\n- **issue_key**: The bug key provided by user\n- **fields**: Same fields as above plus custom fields\n- **expand**: \"renderedFields\"\n- **comment_limit**: 100 (need to check for workaround mentions)\n\n**Handle query results:**\n- If total is 0, display message: \" No proposed blockers found\" with filter summary\n- If total > 20, show progress indicator\n- Cache all bug data for analysis (avoid re-querying)\n\n### Phase 4: Analyze Each Proposed Blocker\n\nAnalyze each proposed blocker using Red Hat OpenShift release blocker criteria.\n\n**Red Hat OpenShift Release Blocker Criteria:**\n\nBased on the official OpenShift blocker definition, bugs should be approved as release blockers when they meet these criteria:\n\n**Automatic/Strong Blockers (Recommend APPROVE):**\n- **Component Readiness regressions** (label: ComponentReadinessRegression) - even tech-preview jobs, unless covered by approved exceptions\n- **Service Delivery blockers** (label: ServiceDeliveryBlocker) - most bugs with this label are blockers\n- **Data loss, service unavailability, or data corruption** - most bugs in this category are blockers\n- **Install/upgrade failures** - may be blockers based on scope (all platforms vs specific form-factor)\n- **Perception of failed upgrade** - bugs that appear as upgrade failures to users\n- **Regressions from previous release** - most regressions are blockers (e.g., from Layered Product Testing)\n- **Bugs severely impacting Service Delivery** - regressions/bugs in default ROSA/OSD/ARO fleet features without acceptable workaround\n\n**Never Blockers (Recommend REJECT):**\n- **Severity below Important** - no bugs with Low/Medium severity are blockers\n- **New features without regressions** - most new feature bugs are NOT blockers unless they regress existing functionality\n- **CI-only issues** - bugs that only affect CI infrastructure/jobs and don't impact product functionality are NOT release blockers\n  - Look for labels: `ci-fail`, `ci-only`, `test-flake`\n  - Check summary/description for keywords: \"CI job\", \"test failure\", \"rehearsal\", \"periodic job\", \"e2e test\"\n  - Check comments for explicit statements like \"Won't affect the product\", \"CI-only\", \"infrastructure issue\"\n  - Even if the bug describes install/upgrade failures, if it only manifests in CI environments, recommend REJECT\n\n**Workaround Assessment (may affect recommendation):**\n\nAn acceptable workaround must meet ALL three criteria:\n1. **Idempotent** - can be applied repeatedly without resulting change\n2. **Safe at scale** - can be safely deployed to 1000's of clusters without material risk via automation\n3. **Timely** - SD can implement before release is pushed to more Cincinnati channels (candidate, fast, stable)\n\nIf a workaround doesn't meet all three criteria, it's NOT an acceptable workaround.\n\n**For each proposed blocker:**\n\n1. **Fetch bug details** including summary, description, labels, priority, severity, comments\n2. **Check for CI-only indicators** (REJECT criteria):\n   - Check labels: `ci-fail`, `ci-only`, `test-flake`\n   - Check summary/description for CI-specific keywords:\n     - \"CI job\", \"test failure\", \"rehearsal\", \"periodic job\", \"e2e test\", \"periodic-ci-\"\n   - Check comments for explicit CI-only statements:\n     - \"Won't affect the product\"\n     - \"CI-only\"\n     - \"infrastructure issue\"\n     - \"only affects CI\"\n   - **If CI-only indicators found, recommend REJECT regardless of severity or failure type**\n3. **Analyze blocker criteria** (if not CI-only):\n   - Check labels: ComponentReadinessRegression, ServiceDeliveryBlocker, UpgradeBlocker\n   - Check severity: Must be Important or higher (Critical/Urgent)\n   - Analyze summary/description for keywords:\n     - Data loss, corruption, service unavailable\n     - Install failure, upgrade failure\n     - Regression\n   - Identify scope: All platforms vs specific form-factor/configuration\n4. **Check for acceptable workarounds**:\n   - Use `expand=\"renderedFields\"` to get comment text\n   - Search for keywords: \"workaround\", \"work around\", \"alternative\", \"bypass\"\n   - Assess if workaround meets all 3 criteria (idempotent, safe at scale, timely)\n5. **Generate recommendation**:\n   -  **APPROVE** - Meets automatic/strong blocker criteria, no acceptable workaround\n   -  **REJECT** - CI-only issue, OR severity below Important, OR new feature without regression, OR has acceptable workaround\n   -  **DISCUSS** - Edge cases requiring team discussion\n\n**Use MCP tools:**\n- `mcp__atlassian__jira_get_issue` with expand=\"renderedFields\" to get comments\n- Analyze comment text for workaround mentions\n\n### Phase 5: Generate Validation Report\n\nCreate comprehensive Markdown report with all blocker validation results.\n\n**Report Structure:**\n\n```markdown\n#  Release Blocker Validation Report\n**Components**: {component list or \"All\"} | **Project**: OCPBUGS | **Proposed Blockers**: {count} | **Generated**: {timestamp}\n\n## Summary\n-  **Recommend APPROVE**: X\n-  **Recommend REJECT**: Y\n-  **Needs DISCUSSION**: Z\n\n---\n\n## Blocker Analysis\n\n### {BUG-KEY}: {Summary} {VERDICT}\n\n**Recommendation**: {APPROVE/REJECT/DISCUSS} - {One-line justification}\n\n**Criteria Matched**:\n- {/} {Criterion name}\n- {/} {Criterion name}\n- ...\n\n**Justification**:\n{Detailed explanation of why this bug should or shouldn't be a blocker}\n\n**Suggested Action**: {What to do next}\n\n---\n\n[Repeat for each proposed blocker]\n\n---\n\n## Next Steps\n1. Review APPROVE recommendations - add to blocker list\n2. Review REJECT recommendations - remove blocker status\n3. Discuss unclear cases in triage meeting\n```\n\n**Special case for single bug mode:**\n\nWhen `--bug` flag is used, adapt the report to focus on a single bug:\n- Summary shows single bug details (key, summary, verdict)\n- Analysis section shows detailed criteria analysis for this specific bug\n- Next Steps adapted for single bug action\n\n### Phase 6: Error Handling\n\n**Invalid issue ID (single bug mode):**\n- Display error: \"Could not find issue {issue-id}\"\n- Verify issue ID is correct format\n- Check user has access to the issue\n\n**Invalid arguments:**\n- Invalid component name: Warn but continue (JIRA will return no results)\n\n**No proposed blockers found:**\n- Display success message: \" No proposed blockers found\"\n- Show filter summary (components, project: OCPBUGS)\n- Confirm no blocker decisions needed\n\n**MCP tool errors:**\n- If `mcp__atlassian__jira_search` fails, display JQL query and error message\n- If `mcp__atlassian__jira_get_issue` fails:\n  1. **Fallback to WebFetch**: Try fetching via `https://issues.redhat.com/browse/{issue-key}`\n  2. **If WebFetch succeeds**: Parse the web page to extract bug details (summary, severity, description) and continue with validation\n  3. **If WebFetch also fails**: Display clear error indicating bug doesn't exist or isn't accessible\n- Provide troubleshooting guidance (check MCP server, verify credentials)\n\n**Large result sets (>50 blockers):**\n- Show progress indicators during analysis\n- Consider warning user: \"Found {count} proposed blockers. This may take a moment to analyze.\"\n\n## Performance Considerations\n\n- **Query optimization**: Only fetch proposed blockers (cf[12319940] = \"Proposed\")\n- **Component scoping**: Use component filters to reduce result set size\n- **Batch operations**: Use `mcp__atlassian__jira_search` with appropriate limits (avoid pagination when possible)\n- **Caching**: Store bug data in memory during execution to avoid re-querying JIRA\n\n## JQL Query Examples\n\n**Version only (4.21):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA)\n```\n\n**Version + single component (4.21, \"Hypershift\"):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA) AND component = \"Hypershift\"\n```\n\n**Version + multiple components (4.21, multiple):**\n```jql\nproject = OCPBUGS AND type = Bug AND \"Release Blocker\" = Proposed AND (\"Target Version\" in (4.21, 4.21.0) OR \"Target Backport Versions\" in (4.21, 4.21.0) OR affectedVersion in (4.21, 4.21.0)) AND status not in (Closed, \"Release Pending\", Verified, ON_QA) AND component IN (\"Hypershift\", \"Cluster Version Operator\")\n```\n\n**Field IDs Reference:**\n- Release Blocker field: `customfield_12319743` (use `\"Release Blocker\"` in JQL)\n- Target Version field: `customfield_12319940` (use `\"Target Version\"` in JQL)"
              },
              {
                "name": "OCPBUGS Jira Conventions",
                "description": "Jira conventions and bug templates for the OCPBUGS project",
                "path": "plugins/jira/skills/ocpbugs/SKILL.md",
                "frontmatter": {
                  "name": "OCPBUGS Jira Conventions",
                  "description": "Jira conventions and bug templates for the OCPBUGS project"
                },
                "content": "# OCPBUGS Jira Conventions\n\nThis skill provides conventions and requirements for creating bug reports in the OCPBUGS project, which is used by all OpenShift product teams for bug tracking.\n\n## When to Use This Skill\n\nUse this skill when creating bugs in the OCPBUGS project:\n- **Project: OCPBUGS** - OpenShift Bugs\n- **Issue Type: Bug** - Bug reports only\n\nThis skill is automatically invoked by the `/jira:create` command when the project_key is \"OCPBUGS\" or when issue type is \"bug\" without a project specified.\n\n## Project Information\n\n### OCPBUGS Project\n**Full name:** OpenShift Bugs\n\n**Key:** OCPBUGS\n\n**Used for:** Bugs only\n\n**Used by:** All OpenShift product teams\n\n## Version Requirements\n\n**Note:** Universal requirements (Security Level: Red Hat Employee, Labels: ai-generated-jira) are defined in the `/jira:create` command and automatically applied to all tickets.\n\n### Affects Version/s (`versions`)\n**Purpose:** Version where the bug was found\n\n**Common values:** `4.19`, `4.20`, `4.21`, `4.22`, etc.\n\n**Handling:**\n- User should specify the version where they encountered the bug\n- If not specified, prompt user: \"Which version did you encounter this bug in?\"\n- Multiple versions can be specified if bug affects multiple releases\n\n### Target Version (customfield_12319940)\n**Purpose:** Version where the fix is targeted\n\n**Common default:** `openshift-4.21` (or current development release)\n\n**Override:** May be different based on:\n- Severity (critical bugs may target earlier releases)\n- Backport requirements\n- Release schedule\n\n**Never set:**\n- Fix Version/s (`fixVersions`) - This is managed by the release team\n\n### Version Override Handling\n\nWhen user specifies a different version:\n1. Accept the version as provided\n2. Validate version exists using MCP tool `jira_get_project_versions` if needed\n3. If version doesn't exist, suggest closest match or ask user to confirm\n\n## Component Requirements\n\n**IMPORTANT:** Component requirements are **team-specific**.\n\nSome teams require specific components, while others do not. The OCPBUGS skill does NOT enforce component selection.\n\n**Team-specific component handling:**\n- Teams may have their own skills that define required components\n- For example, HyperShift team uses `hypershift` skill for component selection\n- Other teams may use different components based on their structure\n\n**If component is not specified:**\n- Prompt user: \"Does this bug require a component? (optional)\"\n- If yes, ask user to specify component name\n- If no, proceed without component\n\n## Bug Description Template\n\n**Note:** Bug template structure and sections are defined in the `create-bug` skill.\n\n**OCPBUGS-specific:**\n- All bugs must follow the bug template format\n- Version-Release number field may differ from Affects Version (can be more specific)\n\n**Note:** Security validation (credential scanning) is defined in the `/jira:create` command and automatically applied to all tickets.\n\n## MCP Tool Integration\n\n### For OCPBUGS Bugs\n\n```python\nmcp__atlassian__jira_create_issue(\n    project_key=\"OCPBUGS\",\n    summary=\"<bug summary>\",\n    issue_type=\"Bug\",\n    description=\"<formatted bug template>\",\n    components=\"<component name>\",  # if required by team\n    additional_fields={\n        \"versions\": [{\"name\": \"4.21\"}],           # affects version (user-specified)\n        \"customfield_12319940\": \"openshift-4.21\", # target version (default or user-specified)\n        \"labels\": [\"ai-generated-jira\"],\n        \"security\": {\"name\": \"Red Hat Employee\"}\n    }\n)\n```\n\n### Field Mapping Reference\n\n| Requirement | MCP Parameter | Value |\n|-------------|---------------|-------|\n| Project | `project_key` | `\"OCPBUGS\"` |\n| Issue Type | `issue_type` | `\"Bug\"` |\n| Summary | `summary` | User-provided text |\n| Description | `description` | Formatted bug template |\n| Component | `components` | Team-specific (optional) |\n| Affects Version | `additional_fields.versions` | `[{\"name\": \"4.21\"}]` (user-specified) |\n| Target Version | `additional_fields.customfield_12319940` | `\"openshift-4.21\"` (default or user-specified) |\n| Labels | `additional_fields.labels` | `[\"ai-generated-jira\"]` (required) |\n| Security Level | `additional_fields.security` | `{\"name\": \"Red Hat Employee\"}` (required) |\n\n## Interactive Prompts\n\n**Note:** Detailed bug template prompts are defined in the `create-bug` skill.\n\n**OCPBUGS-specific prompts:**\n- **Affects Version** (required): \"Which version did you encounter this bug in?\"\n  - Show common versions: 4.19, 4.20, 4.21, 4.22\n- **Target Version** (optional): \"Which version should this be fixed in? (default: openshift-4.21)\"\n- **Component** (if required by team): Defer to team-specific skills\n\n## Examples\n\n**Note:** All examples automatically apply universal requirements (Security: Red Hat Employee, Labels: ai-generated-jira) as defined in `/jira:create` command.\n\n### Create Bug with Minimal Info\n\n```bash\n/jira:create bug \"Control plane pods crash on upgrade from 4.20 to 4.21\"\n```\n\n**OCPBUGS-specific defaults:**\n- Project: OCPBUGS (default for bugs)\n- Target Version: openshift-4.21 (default)\n\n**Prompts:** See `create-bug` skill for bug template prompts, plus Affects Version\n\n### Create Bug with Full Details\n\n```bash\n/jira:create bug OCPBUGS \"API server returns 500 error when creating namespaces\" --component \"API\" --version \"4.21\"\n```\n\n**OCPBUGS-specific defaults:**\n- Affects Version: 4.21 (from --version flag)\n- Target Version: openshift-4.21 (default)\n- Component: API (from --component flag)\n\n**Prompts:** See `create-bug` skill for bug template prompts\n\n## Error Handling\n\n### Invalid Version\n\n**Scenario:** User specifies a version that doesn't exist.\n\n**Action:**\n1. Use `mcp__atlassian__jira_get_project_versions` to fetch available versions\n2. Suggest closest match: \"Version '4.21.5' not found. Did you mean '4.21.0'?\"\n3. Show available versions: \"Available: 4.20.0, 4.21.0, 4.22.0\"\n4. Wait for confirmation or correction\n\n### Component Required But Missing\n\n**Scenario:** Team requires component, but user didn't specify.\n\n**Action:**\n1. If team skill detected required components, show options\n2. Otherwise, generic prompt: \"Does this bug require a component?\"\n3. If yes, ask user to specify component name\n4. If no, proceed without component\n\n### Sensitive Data Detected\n\n**Scenario:** Credentials or secrets found in bug description or logs.\n\n**Action:**\n1. STOP issue creation immediately\n2. Inform user: \"I detected potential credentials in the bug report.\"\n3. Show general location: \"Found in: Additional info section\"\n4. Do NOT echo the sensitive data back\n5. Suggest: \"Please sanitize logs and use placeholder values like 'YOUR_API_KEY'\"\n6. Wait for user to provide sanitized content\n\n### MCP Tool Failure\n\n**Scenario:** MCP tool returns an error.\n\n**Action:**\n1. Parse error message for actionable information\n2. Common errors:\n   - **\"Field 'component' is required\"**  Prompt for component (team-specific requirement)\n   - **\"Permission denied\"**  User may lack permissions to create bugs in OCPBUGS\n   - **\"Version not found\"**  Use version error handling above\n3. Provide clear next steps\n4. Offer to retry after corrections\n\n### Wrong Issue Type\n\n**Scenario:** User tries to create a story/task/epic in OCPBUGS.\n\n**Action:**\n1. Inform user: \"OCPBUGS is for bugs only. Stories/Tasks/Epics should be created in CNTRLPLANE.\"\n2. Suggest: \"Would you like to create a bug instead, or change the project to CNTRLPLANE?\"\n3. Wait for user decision\n\n**Note:** Jira description formatting (Wiki markup) is defined in the `/jira:create` command.\n\n## Team-Specific Extensions\n\nTeams using OCPBUGS may have additional team-specific requirements defined in separate skills:\n\n- **HyperShift team:** Uses `hypershift` skill for component selection (HyperShift / ARO, HyperShift / ROSA, HyperShift)\n- **Other teams:** May define their own skills with team-specific components and conventions\n\nTeam-specific skills are invoked automatically when team keywords are detected in the summary or when specific components are mentioned.\n\n## Workflow Summary\n\nWhen `/jira:create bug` is invoked:\n\n1.  **OCPBUGS skill loaded:** Applies project-specific conventions\n2.  **Apply OCPBUGS defaults:**\n   - Project: OCPBUGS (default for bugs)\n   - Target version: openshift-4.21 (default)\n3.  **Check for team-specific skills:** If team keywords detected, invoke team skill (e.g., `hypershift`)\n4.  **Interactive prompts:**\n   - Affects version (required)\n   - Bug template sections (see `create-bug` skill)\n   - Component (if required by team)\n\n**Note:** Universal requirements (security, labels), security validation, and issue creation handled by `/jira:create` command.\n\n## Best Practices\n\n1. **Version specificity:** Use exact versions (4.21.0) not just major versions (4.21) for Affects Version\n2. **Template adherence:** Defer to `create-bug` skill for bug template best practices\n3. **Link related issues:** Reference related bugs, PRs, or stories\n\n**Note:** Universal best practices (security, credential sanitization, formatting) are defined in the `/jira:create` command.\n\n## See Also\n\n- `/jira:create` - Main command that invokes this skill\n- `cntrlplane` skill - For CNTRLPLANE stories/epics/features/tasks\n- Team-specific skills (e.g., `hypershift`) - For team-specific conventions\n- `create-bug` skill - General bug report best practices"
              }
            ]
          },
          {
            "name": "ci",
            "description": "A plugin to work with OpenShift CI",
            "source": "./plugins/ci",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install ci@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/add-debug-wait",
                "description": "Add a wait step to a CI workflow for debugging test failures",
                "path": "plugins/ci/commands/add-debug-wait.md",
                "frontmatter": {
                  "description": "Add a wait step to a CI workflow for debugging test failures",
                  "argument-hint": "<workflow-or-job-name> [timeout]"
                },
                "content": "## Name\nci:add-debug-wait\n\n## Synopsis\n```\n/ci:add-debug-wait <workflow-or-job-name> [timeout]\n```\n\n## Description\n\nThe `ci:add-debug-wait` command adds a `wait` step to a CI job/workflow for debugging test failures.\n\n**What it does:**\n1. Takes job name, OCP version, and optional timeout as input\n2. Finds and edits the job config or workflow file\n3. Adds `- ref: wait` before the last test step (with optional timeout configuration)\n4. Commits and pushes the change\n5. Gives you a GitHub link to create the PR\n\n**That's it!** Simple, fast, and automated.\n\n## Implementation\n\nThe command performs the following steps:\n\n### Step 1: Gather Required Information\n\n**Prompt user for** (in this order):\n\n1. **Workflow/Job Name**: (from command argument $1 or prompt)\n   ```\n   Workflow or job name: <user-input>\n   Example: aws-c2s-ipi-disc-priv-fips-f7\n   Example: baremetalds-two-node-arbiter-e2e-openshift-test-private-tests\n   ```\n\n2. **Timeout** (optional, from command argument $2):\n   ```\n   Wait timeout in hours (optional, default: 3h):\n   Examples: \"1h\", \"2h\", \"8h\", \"24h\", \"72h\"\n   Valid range: 1h to 72h\n   ```\n   - If not provided, uses the wait step's default behavior (3 hours)\n   - Format: Integer followed by 'h' (e.g., \"1h\", \"2h\", \"8h\")\n   - Valid range: 1h to 72h (maximum enforced by wait step's timeout setting)\n   - Will be normalized to Go duration format (e.g., \"8h\"  \"8h0m0s\")\n   - This will be set as the `timeout:` property on the wait step in the workflow/job YAML\n\n3. **OCP Version**: (prompt - REQUIRED for searching job configs)\n   ```\n   OCP version for debugging (e.g., 4.18, 4.19, 4.20, 4.21, 4.22):\n   ```\n   This is used to:\n   - Search the correct job config file (e.g., release-4.21)\n   - Document which version needs debugging\n   - Add context to the PR\n\n4. **OpenShift Release Repo Path**: (prompt if not in current directory)\n   ```\n   Path to openshift/release repository:\n   Default: ~/repos/openshift-release\n   ```\n\n### Step 2: Validate Environment\n\n**Silently validate** (no user prompts):\n\n```bash\ncd <repo-path>\n\n# Check 1: Repository exists and is correct\ngit remote -v | grep \"openshift/release\" || exit 1\n\n# Skip repo update - work with current state\n# User can manually update their repo if needed\n```\n\n### Step 3: Search for Job/Test Configuration\n\n**Priority 1: Search job configs first** (more specific and targeted):\n\n```bash\ncd <repo-path>\n\n# Search for job config files matching the OCP version\n# The job name could be in various config files, so search broadly\ngrep -r \"as: ${job_name}\" ci-operator/config/ --include=\"*release-${ocp_version}*.yaml\" -l\n```\n\n**Example searches**:\n- For `aws-c2s-ipi-disc-priv-fips-f7` and OCP 4.21:\n  ```bash\n  grep -r \"as: aws-c2s-ipi-disc-priv-fips-f7\" ci-operator/config/ --include=\"*release-4.21*.yaml\" -l\n  ```\n\n**Handle job config search results**:\n\n- **1 file found**:\n  ```\n   Found job configuration:\n  ${file_path}\n\n  Type: Job configuration file\n\n  Proceeding with job config modification...\n  ```\n   Continue to **Step 4a: Analyze Job Configuration**\n\n- **Multiple files found**:\n  ```\n  Found ${count} matching job config files:\n\n  1. ci-operator/config/.../release-4.21__amd64-nightly.yaml\n  2. ci-operator/config/.../release-4.21__arm64-nightly.yaml\n  3. ci-operator/config/.../release-4.21__ppc64le-nightly.yaml\n\n  Select file (1-${count}) or 'q' to quit:\n  ```\n\n  **Prompt user to select** which file to modify, then continue to **Step 4a: Analyze Job Configuration**\n\n- **0 files found**:\n  ```\n    No job config found for: ${job_name} (OCP ${ocp_version})\n\n  Searching for workflow files instead...\n  ```\n   Continue to **Priority 2** below\n\n**Priority 2: Search workflow files** (if job config not found):\n\n```bash\ncd <repo-path>\n\n# Search for workflow files\nfind ci-operator/step-registry -type f -name \"*${workflow_name}*workflow*.yaml\"\n```\n\n**Handle workflow search results**:\n\n- **0 files found**:\n  ```\n   No job config or workflow file found for: ${job_name}\n\n  Suggestions:\n  1. Check spelling of job/workflow name\n  2. Verify OCP version (${ocp_version})\n  3. Try with partial name\n  4. Search manually:\n     - Job configs: grep -r \"as: ${job_name}\" ci-operator/config/\n     - Workflows: find ci-operator/step-registry -name \"*workflow*.yaml\" | grep <partial-name>\n  ```\n\n- **1 file found**:\n  ```\n   Found workflow file:\n  ${file_path}\n\n  Type: Workflow file\n\n  Proceeding with workflow modification...\n  ```\n   Continue to **Step 4b: Analyze Workflow File**\n\n- **Multiple files found**:\n  ```\n  Found ${count} matching workflow files:\n\n  1. ci-operator/step-registry/.../workflow1.yaml\n  2. ci-operator/step-registry/.../workflow2.yaml\n  3. ci-operator/step-registry/.../workflow3.yaml\n\n  Select file (1-${count}) or 'q' to quit:\n  ```\n\n  **Prompt user to select** which file to modify, then continue to **Step 4b: Analyze Workflow File**\n\n### Step 4a: Analyze Job Configuration\n\n**Read and parse the job config YAML**:\n\n```bash\n# Find the specific test definition\ngrep -A 30 \"as: ${job_name}\" <job-config-file>\n```\n\n**Check for**:\n1.  Has `steps:` section\n2.  Has `test:` section inside steps\n3.  Does NOT already have `- ref: wait`\n\n**Example current structure**:\n```yaml\n- as: aws-c2s-ipi-disc-priv-fips-f7\n  cron: 36 16 3,12,19,26 * *\n  steps:\n    cluster_profile: aws-c2s-qe\n    env:\n      BASE_DOMAIN: qe.devcluster.openshift.com\n      FIPS_ENABLED: \"true\"\n    test:\n    - chain: openshift-e2e-test-qe\n    workflow: cucushift-installer-rehearse-aws-c2s-ipi-disconnected-private\n```\n\n**If wait already exists**:\n```\n  Wait step already configured in job config\n\nCurrent test section:\n  test:\n  - ref: wait\n  - chain: openshift-e2e-test-qe\n\nNo changes needed. The job is already set up for debugging.\n```\n\n**If no test section found**:\n```\n  Job config found but no test: section\n\nThis job uses only the workflow's test steps.\nSearching for the workflow: ${workflow_name}\n```\n Fall back to searching for workflow (Priority 2 in Step 3)\n\n Continue to **Step 5a: Show Diff for Job Config**\n\n### Step 4b: Analyze Workflow File\n\n**Read and parse the workflow YAML**:\n\n```bash\ncat <workflow-file>\n```\n\n**Check for**:\n1.  Has `workflow:` section\n2.  Has `test:` section\n3.  Does NOT already have `- ref: wait`\n\n**Example current structure**:\n```yaml\nworkflow:\n  as: baremetalds-two-node-arbiter-upgrade\n  steps:\n    pre:\n      - chain: baremetalds-ipi-pre\n    test:\n      - chain: baremetalds-ipi-test\n    post:\n      - chain: baremetalds-ipi-post\n```\n\n**If wait already exists**:\n```\n  Wait step already configured in workflow\n\nCurrent test section:\n  test:\n    - ref: wait\n    - chain: baremetalds-ipi-test\n\nNo changes needed. The workflow is already set up for debugging.\n```\n\n**If no test section exists**:\n```\n  Workflow has no test: section\n\nThis workflow is provision/deprovision only.\nThe test steps must be defined in the job config.\n\nPlease provide the full job name to modify the job config instead.\n```\n Exit or prompt for job name\n\n Continue to **Step 5b: Modify Workflow File**\n\n### Step 5a: Modify Job Config File\n\n**Edit the job config file directly** - no confirmation needed:\n\n```bash\n# Add wait step before the last test step\n# If timeout is provided, add it as a step property\n# See Step 6 for the YAML modification algorithm\n```\n\n**Two scenarios**:\n\n1. **Without custom timeout** (uses wait step's built-in default of 3h):\n   ```yaml\n   test:\n   - ref: wait\n   - chain: openshift-e2e-test-qe\n   ```\n   Note: No timeout or best_effort needed - the wait step will use its default TIMEOUT env var (3 hours)\n\n2. **With custom timeout** (user provided timeout parameter):\n   ```yaml\n   test:\n   - ref: wait\n     timeout: 8h0m0s\n     best_effort: true\n   - chain: openshift-e2e-test-qe\n   ```\n   Note: `best_effort: true` is required when timeout is customized to prevent the wait step from failing the job if it times out\n\n**Show brief confirmation**:\n```\n Modified: ${job_name} (OCP ${ocp_version})\n   File: <job-config-file-path>\n   Added: - ref: wait${timeout:+ (timeout: ${timeout})}\n```\n\n### Step 5b: Modify Workflow File\n\n**Edit the workflow file directly** - no confirmation needed:\n\n```bash\n# Add wait step before the last test step\n# If timeout is provided, add it as a step property\n# See Step 6 for the YAML modification algorithm\n```\n\n**Two scenarios**:\n\n1. **Without custom timeout** (uses wait step's built-in default of 3h):\n   ```yaml\n   test:\n   - ref: wait\n   - chain: baremetalds-ipi-test\n   ```\n   Note: No timeout or best_effort needed - the wait step will use its default TIMEOUT env var (3 hours)\n\n2. **With custom timeout** (user provided timeout parameter):\n   ```yaml\n   test:\n   - ref: wait\n     timeout: 8h0m0s\n     best_effort: true\n   - chain: baremetalds-ipi-test\n   ```\n   Note: `best_effort: true` is required when timeout is customized to prevent the wait step from failing the job if it times out\n\n**Show brief confirmation**:\n```\n Modified: ${workflow_name} workflow\n   File: <workflow-file-path>\n   Added: - ref: wait${timeout:+ (timeout: ${timeout})}\n     Impact: Affects ALL jobs using this workflow\n```\n\n### Step 6: Create Branch and Commit\n\n**Branch naming**:\n```\ndebug-${workflow_name}-${ocp_version}-$(date +%Y%m%d)\n```\n\nExample: `debug-baremetalds-two-node-arbiter-4.21-20250131`\n\n**Git operations**:\n```bash\n# Create branch\ngit checkout -b \"${branch_name}\"\n\n# Modify the file (add wait step using the implementation below)\n# Add '- ref: wait' as the first step in the test: section\n\n# Stage change\ngit add <workflow-file>\n\n# Commit\ngit commit -m \"[Debug] Add wait step to ${workflow_name} for OCP ${ocp_version}\n\nThis adds a wait step to enable debugging of test failures in OCP ${ocp_version}.\n\nThe wait step pauses the workflow before tests run, allowing QE to:\n- SSH into the test environment\n- Inspect system state and logs\n- Debug configuration issues\n- Investigate test failures\n\nOCP Version: ${ocp_version}\nWorkflow: ${workflow_name}\"\n```\n\n**YAML Modification Algorithm**:\n\nThe modification process for both job configs and workflow files follows the same pattern:\n\n1. **Locate the target**: Find the `test:` section\n   - For job configs: Within the specific job definition (`- as: ${job_name}`)\n   - For workflows: At the workflow level\n\n2. **Find test steps**: Identify all steps (lines with `- ref:` or `- chain:`)\n\n3. **Check for duplicates**: Ensure `- ref: wait` doesn't already exist\n\n4. **Insert wait step**: Add before the **last** test step with matching indentation\n\n5. **Handle timeout**:\n   - Without timeout: Add simple `- ref: wait`\n   - With timeout: Add as multi-line with `timeout` and `best_effort` properties\n\n**Example transformation:**\n\nBefore:\n```yaml\ntest:\n- chain: openshift-e2e-test-qe\n```\n\nAfter (without timeout):\n```yaml\ntest:\n- ref: wait\n- chain: openshift-e2e-test-qe\n```\n\nAfter (with timeout=8h):\n```yaml\ntest:\n- ref: wait\n  timeout: 8h0m0s\n  best_effort: true\n- chain: openshift-e2e-test-qe\n```\n\n**Critical constraints:**\n- Preserve exact YAML indentation (typically 2 spaces per level)\n- Insert BEFORE the last step, not after\n- When timeout is set, `best_effort: true` is required to prevent job failure\n- Normalize timeout format to Go duration (e.g., \"8h\"  \"8h0m0s\")\n\n### Step 7: Push and Show GitHub Link\n\n**Auto-push the branch**:\n```bash\ngit push origin \"${branch_name}\"\n```\n\n**Display GitHub PR creation link**:\n```\n Changes pushed successfully!\n\nCreate PR here:\nhttps://github.com/openshift/release/compare/master...${branch_name}\n\nBranch: ${branch_name}\nJob: ${job_name}\nOCP: ${ocp_version}\n\n  Remember to close PR after debugging (DO NOT MERGE)\n```\n\nThat's it! Simple and clean.\n\n### Error Handling\n\n**Error: Repository Not Found**\n```\n Error: Repository not found at ${repo_path}\n\nPlease provide the correct path to openshift/release repository.\n\nTo clone:\ngit clone https://github.com/openshift/release.git\n```\n\n**Error: Not in openshift/release Repo**\n```\n Error: This doesn't appear to be the openshift/release repository\n\nRemote URL: ${current_remote}\nExpected: github.com/openshift/release\n\nPlease navigate to the correct repository.\n```\n\n**Error: Workflow File Not Found**\n```\n Error: Workflow file not found\n\nSearched for: *${workflow_name}*workflow*.yaml\nLocation: ci-operator/step-registry/\n\nSuggestions:\n1. Verify the workflow name\n2. Try a partial match\n3. Search manually: find ci-operator/step-registry -name \"*workflow*.yaml\"\n```\n\n**Error: Wait Step Already Exists**\n```\n  Wait step already configured in this workflow\n\nNo action needed - you can proceed with debugging using the existing wait step.\n```\n\n**Error: Invalid OCP Version**\n```\n Invalid OCP version: ${version}\n\nValid versions: 4.18, 4.19, 4.20, 4.21, 4.22, master\n\nPlease provide a valid version.\n```\n\n### Error: Invalid Timeout Format\n```\n Invalid timeout format: ${timeout}\n\nValid format: Integer followed by 'h' (e.g., \"1h\", \"2h\", \"8h\", \"24h\", \"72h\")\nValid range: 1h to 72h\n\nExamples:\n- \"1h\" (1 hour)\n- \"8h\" (8 hours)\n- \"24h\" (24 hours)\n- \"72h\" (72 hours, maximum)\n\nPlease provide a valid timeout in hours.\n```\n\n### Note: Timeout Normalization\n\nWhen a user provides a timeout like \"8h\", the implementation should normalize it to the standard Go duration format \"8h0m0s\" for consistency with existing configurations in the codebase.\n\n## Return Value\n\n- **Success**: PR URL and debugging instructions\n- **Error**: Error message with suggestions for resolution\n- **Format**: Text output with emoji indicators for status\n\n## Examples\n\n### Example 1: Without Timeout (Default 3h)\n\n```bash\n/ci:add-debug-wait aws-ipi-f7-longduration-workload\n```\n\nPrompts for: OCP version (4.21), repo path\n\nResult:\n```yaml\ntest:\n- ref: wait\n- chain: openshift-e2e-test-qe\n```\n\nReturns: PR creation link\n\n### Example 2: With Custom Timeout\n\n```bash\n/ci:add-debug-wait aws-ipi-f7-longduration-workload 8h\n```\n\nPrompts for: OCP version (4.21), repo path\n\nResult:\n```yaml\ntest:\n- ref: wait\n  timeout: 8h0m0s\n  best_effort: true\n- chain: openshift-e2e-test-qe\n```\n\nReturns: PR creation link with timeout info\n\n### Example 3: Workflow File\n\n```bash\n/ci:add-debug-wait baremetalds-two-node-arbiter-upgrade 24h\n```\n\nBehavior: Searches job config first, falls back to workflow if not found. Warns that workflow changes affect ALL jobs using it.\n\nReturns: PR creation link\n\n## Arguments\n\n- **$1** (workflow-or-job-name): The name of the CI workflow or job to add the wait step to (required)\n- **$2** (timeout): Optional timeout in hours (1h-72h). Examples: \"1h\", \"8h\", \"24h\", \"72h\". If not provided, uses wait step's default (3h)\n\n## Notes\n\n### Best Practices for QE\n\n**Before Running Command**:\n-  Confirm test is actually failing\n-  Check existing debug PRs\n-  Know which OCP version is affected\n\n**During Debugging**:\n-  Take detailed notes\n-  Save logs and screenshots\n-  Document root cause\n-  Record all findings\n\n**After Debugging**:\n-  Document findings\n-  Close the debug PR\n-  Delete the branch\n-  Share learnings with team\n-  Create fix PR if needed\n\n### Future Enhancements\n\nConsider adding companion commands:\n- `/ci:close-debug-pr` - Lists open debug PRs, prompts for findings, closes PR\n- `/ci:list-debug-prs` - Show all open debug PRs\n- `/ci:revert-debug-pr` - Revert a debug PR that was merged by mistake"
              },
              {
                "name": "/ask-sippy",
                "description": "Ask the Sippy AI agent questions about OpenShift CI payloads, jobs, and test results",
                "path": "plugins/ci/commands/ask-sippy.md",
                "frontmatter": {
                  "description": "Ask the Sippy AI agent questions about OpenShift CI payloads, jobs, and test results",
                  "argument-hint": "[question]"
                },
                "content": "## Name\nci:ask-sippy\n\n## Synopsis\n```\n/ask-sippy [question]\n```\n\n## Description\n\nThe `ask-sippy` command allows you to query the Sippy AI agent, which has deep knowledge about OpenShift CI infrastructure, including:\n- CI payload status and rejection reasons\n- Job failures and patterns\n- Test results and trends\n- Release quality metrics\n- Historical CI data analysis\n\nThe command sends your question to the Sippy API and returns the agent's\nresponse. Note that complex queries may take some time to process as the\nagent analyzes CI data. Please inform the user of this.\n\n## Security\n\n**IMPORTANT SECURITY REQUIREMENTS:**\n\nClaude is granted LIMITED and SPECIFIC access to the DPCR cluster token for the following AUTHORIZED operations ONLY:\n- **READ operations**: Querying the Sippy API for CI data analysis\n\nClaude is EXPLICITLY PROHIBITED from:\n- Modifying cluster resources (deployments, pods, services, etc.)\n- Deleting or altering any data\n- Accessing secrets, configmaps, or sensitive data beyond Sippy API responses\n- Making any cluster modifications\n- Using the token for any purpose other than the specific operations listed above\n\n**Token Usage:**\nThe DPCR cluster token is used solely for authentication with the Sippy API. This token grants the same permissions as the authenticated user and must be handled with appropriate care. The `curl_with_token.sh` wrapper handles all authentication automatically.\n\n## Implementation\n\n1. **Validate Arguments**: Checks that a question was provided\n2. **Notify User**: Informs the user that the query is being processed (may take time)\n3. **API Request**: Sends a POST request to the Sippy API using the `oc-auth` skill's curl wrapper:\n   ```bash\n   # Use curl_with_token.sh from oc-auth skill - it automatically adds the OAuth token\n   # DPCR cluster API: https://api.cr.j7t7.p1.openshiftapps.com:6443\n   curl_with_token.sh https://api.cr.j7t7.p1.openshiftapps.com:6443 -s -X POST \"https://sippy-auth.dptools.openshift.org/api/chat\" \\\n     -H \"Content-Type: application/json\" \\\n     -d @- <<'EOF'\n{\n  \"message\": \"$1\",\n  \"chat_history\": [],\n  \"show_thinking\": false,\n  \"persona\": \"default\"\n}\nEOF\n   ```\n4. **Return JSON**: Returns the full JSON response for Claude to parse\n\n## Return Value\n- **Success**: JSON response from Sippy API with the following structure:\n  - `response`: Markdown-formatted answer from the agent (this is what should be displayed to the user)\n  - `visualizations`: Optional field containing Plotly JSON for interactive charts and graphs\n  - `error`: null if successful\n- **Error**: JSON with `error` field populated if the request fails\n\n**Important for Claude**:\n1. **REQUIRED**: Before executing this command, you MUST ensure the `ci:oc-auth` skill is loaded by invoking it with the Skill tool. The curl_with_token.sh script depends on this skill being active.\n2. You must locate and verify curl_with_token.sh before running it, you (Claude Code) have a bug that tries to use the script from the wrong directory!\n3. **Before invoking this command**, inform the user that querying Sippy may take 10-60 seconds for complex queries\n4. Extract the `response` field from the JSON and render it as markdown to the user\n5. If the response includes a `visualizations` field, it contains Plotly JSON. Render the visualization(s) in an interactive, user-friendly way by creating an HTML file with the Plotly chart(s) embedded. Open it in the user's browser for them.\n6. If there's an `error` field, display that instead\n\n## Examples\n\n1. **Query about payload rejection**:\n   ```\n   /ask-sippy Why was the last 4.21 payload rejected?\n   ```\n   Response will include analysis of the latest 4.21 payload rejection with specific job failures and reasons.\n\n2. **Ask about job failures**:\n   ```\n   /ask-sippy What are the most common test failures in the e2e-aws job this week?\n   ```\n   Response will analyze recent test failure patterns in the specified job.\n\n3. **Investigate CI trends**:\n   ```\n   /ask-sippy How is the overall CI health for 4.20 compared to last week?\n   ```\n   Response will provide comparative analysis of CI metrics.\n\n4. **Specific test inquiry**:\n   ```\n   /ask-sippy Why is the test \"sig-network Feature:SCTP should create a Pod with SCTP HostPort\" failing?\n   ```\n   Response will analyze failure patterns and potential causes for the specific test.\n\n## Notes\n\n- **Response Time**: Complex queries analyzing large datasets may take 30-60 seconds\n- **Chat History**: Each query is independent; no conversation context is maintained between calls\n- **Response Format**: The API returns JSON with a `response` field containing markdown-formatted text\n- **Markdown Rendering**: Claude will automatically render the markdown response nicely with proper formatting\n- **Visualizations**: When available, the `visualizations` field contains Plotly JSON for interactive charts and graphs. Claude should render these as HTML files for the user to view\n- **Error Handling**: If the API returns an error, it will be displayed in the `error` field of the JSON response\n\n## Data Sources Available\n\nSippy can query and analyze:\n- **Release Payloads**: Status, rejections, promotions for all 4.x versions\n- **CI Jobs**: Failure rates, patterns, infrastructure issues (aws, gcp, azure, metal, vsphere, etc.)\n- **Test Results**: Pass/fail rates, flakes, regressions, execution times\n- **Historical Analysis**: Week-over-week and release-to-release comparisons\n- **Infrastructure Metrics**: Provisioning issues, platform problems, resource patterns\n\n## Arguments\n- **$1** (question): The question to ask the Sippy AI agent. Should be a clear, specific question about OpenShift CI infrastructure, payloads, jobs, or test results."
              },
              {
                "name": "/list-unstable-tests",
                "description": "List unstable tests with pass rate below 95%",
                "path": "plugins/ci/commands/list-unstable-tests.md",
                "frontmatter": {
                  "description": "List unstable tests with pass rate below 95%",
                  "argument-hint": "<version> <keywords> [sippy-url]"
                },
                "content": "## Name\nci:list-unstable-tests\n\n## Synopsis\n```\n/ci:list-unstable-tests <version> <keywords> [sippy-url]\n```\n\n## Description\nThe `ci:list-unstable-tests` command queries OpenShift CI test results from Sippy and lists all tests matching the keywords that have a pass rate below 95%. This is useful for quickly identifying unstable tests that need attention.\n\nBy default, it queries the production Sippy instance at `sippy.dptools.openshift.org`. You can optionally specify a different Sippy instance URL to query alternative environments (e.g., QE component readiness).\n\nThis command is useful for:\n- Identifying unstable tests with inconsistent pass rates\n- Finding regression candidates for investigation\n- Generating reports of unstable test cases\n- Prioritizing test stabilization efforts\n- Quality gate checks before releases\n\n## Arguments\n- `$1` (version): OpenShift version to query (e.g., \"4.21\", \"4.20\", \"4.19\")\n- `$2` (keywords): Keywords to search in test names (e.g., \"olmv1\", \"sig-storage\", \"operator\")\n- `$3` (sippy-url) [optional]: Sippy instance base URL. Defaults to \"sippy.dptools.openshift.org\" if not provided. Examples: \"qe-component-readiness.dptools.openshift.org\"\n\n## Implementation\n\n1. **Parse Arguments**\n   - Extract version from `$1` (e.g., \"4.20\")\n   - Extract keywords from `$2` (e.g., \"olmv1\")\n   - Extract Sippy URL from `$3` if provided, otherwise use default \"sippy.dptools.openshift.org\"\n   - Normalize URL to extract base domain for API endpoint (strip \"/sippy-ng/\" suffix if present)\n   - Add \"https://\" prefix if not already present\n\n2. **Build Sippy API Request**\n   - Construct filter JSON for the `/api/tests` endpoint:\n     ```python\n     filters = {\n         \"items\": [\n             {\n                 \"columnField\": \"name\",\n                 \"not\": False,\n                 \"operatorValue\": \"contains\",\n                 \"value\": keywords\n             }\n         ],\n         \"linkOperator\": \"and\"\n     }\n     ```\n   - Set query parameters:\n     - `release`: The OpenShift version\n     - `filter`: JSON-encoded filter object\n     - `sort`: \"asc\"\n     - `sortField`: \"current_pass_percentage\"\n\n3. **Query Test Statistics**\n   - Construct API endpoint from provided URL: `https://{base_url}/api/tests`\n   - Make GET request to the constructed endpoint\n   - Parse response to extract test data\n\n4. **Filter Tests Below 95% Pass Rate**\n   - Iterate through all returned tests\n   - Filter tests where `current_pass_percentage < 95`\n   - Sort filtered results by pass percentage (ascending, worst first)\n   - Collect the following data for each unstable test:\n     - Test name\n     - Current pass percentage\n     - Total runs\n     - Passes\n     - Failures\n     - Net improvement (trend indicator)\n\n5. **Format and Display Results**\n   - Display summary header with:\n     - Total number of tests matching keywords\n     - Number of tests below 95% pass rate\n     - Percentage of unstable tests\n   - List each unstable test with:\n     - Test name\n     - Pass rate percentage\n     - Run statistics (runs/passes/failures)\n     - Trend indicator (net improvement)\n   - Sort by pass percentage (worst tests first)\n   - If no tests are below 95%, display success message indicating all tests are stable\n\n## Return Value\n\n**Format**: Formatted text output with:\n\n**Summary Section:**\n- Total tests matching keywords\n- Tests below 95% pass rate (unstable tests)\n- Overall stability percentage\n\n**Unstable Tests List:**\nFor each test with pass rate < 95%:\n- Test name\n- Pass rate percentage\n- Total runs\n- Passes\n- Failures\n- Net improvement\n\nIf all tests pass at 95% or above, display a success message indicating all tests are stable.\n\n## Examples\n\n1. **List unstable OLMv1 tests from QE Sippy**:\n   ```\n   /ci:list-unstable-tests 4.20 olmv1 qe-component-readiness.dptools.openshift.org\n   ```\n\n   Lists all OLMv1-related tests in version 4.20 from QE Sippy that have a pass rate below 95%.\n\n2. **List unstable storage tests (using default Sippy)**:\n   ```\n   /ci:list-unstable-tests 4.21 sig-storage\n   ```\n\n   Lists all storage-related tests in version 4.21 from production Sippy with pass rate below 95%.\n\n3. **List unstable operator tests**:\n   ```\n   /ci:list-unstable-tests 4.19 operator\n   ```\n\n   Lists all operator-related tests in version 4.19 with pass rate below 95%.\n\n4. **Check specific component stability**:\n   ```\n   /ci:list-unstable-tests 4.20 sig-network qe-component-readiness.dptools.openshift.org\n   ```\n\n   Lists all network-related unstable tests from QE Sippy.\n\n## Notes\n\n- **Pass Rate Threshold**: Fixed at 95% - tests with pass rate >= 95% are considered stable\n- **Default Sippy URL**: If no Sippy URL is provided, the command uses `sippy.dptools.openshift.org` by default\n- The command queries data from the last 7 days by default\n- Ensure you can access the Sippy API endpoints\n- Results are sorted by pass percentage (ascending) to show most unstable tests first\n- The net improvement metric shows if the test is getting worse (negative) or better (positive)\n- If no tests match the keywords, an appropriate message will be displayed\n- If all matching tests have pass rate >= 95%, a success message will be shown indicating all tests are stable\n\n## Output Example\n\n```\n================================================================================\nUnstable Tests Report - 4.20 olmv1\n================================================================================\nSippy Instance: qe-component-readiness.dptools.openshift.org\nPass Rate Threshold: < 95%\n\nSummary:\n  Total Tests Matching 'olmv1': 45\n  Unstable Tests (< 95%): 8 (17.8%)\n  Stable Tests (>= 95%): 37 (82.2%)\n\n================================================================================\nTests Below 95% Pass Rate (sorted by worst first):\n================================================================================\n\n1. Test: [sig-olmv1] clusterextension install should fail validation\n   Pass Rate: 23.5%\n   Runs: 17 | Passes: 4 | Failures: 13\n   Net Improvement: -45.2\n\n2. Test: [sig-olmv1] clusterextension upgrade from v1 to v2\n   Pass Rate: 67.8%\n   Runs: 28 | Passes: 19 | Failures: 9\n   Net Improvement: -12.3\n\n[... additional tests ...]\n\n================================================================================\n```\n\n## See Also\n\n- `/ci:query-test-result` - Query detailed results for a specific test\n- Sippy UI (Production): https://sippy.dptools.openshift.org/sippy-ng/\n- Sippy UI (QE): https://qe-component-readiness.dptools.openshift.org\n- Sippy API Documentation: https://github.com/openshift/sippy"
              },
              {
                "name": "/query-job-status",
                "description": "Query the status of a gangway job execution by ID",
                "path": "plugins/ci/commands/query-job-status.md",
                "frontmatter": {
                  "description": "Query the status of a gangway job execution by ID",
                  "argument-hint": "<execution-id>"
                },
                "content": "## Name\nci:query-job-status\n\n## Synopsis\n```\n/query-job-status <execution-id>\n```\n\n## Description\n\nThe `query-job-status` command queries the status of a gangway job execution via the REST API using the execution ID returned when a job is triggered.\n\nThe command accepts:\n- Execution ID (required, UUID returned when triggering a job)\n\nIt makes a GET request to the gangway API and returns the current status of the job including its name, type, status, and GCS path to artifacts if available. The `curl_with_token.sh` wrapper handles all authentication automatically.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - $1: execution ID (required, UUID format)\n\n2. **Execute API Request**: Make a GET request to query the job status using the `oc-auth` skill's curl wrapper:\n   ```bash\n   # Use curl_with_token.sh from oc-auth skill - it automatically adds the OAuth token\n   # app.ci cluster API: https://api.ci.l2s4.p1.openshiftapps.com:6443\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -X GET \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions/<EXECUTION_ID>\n   ```\n   The `curl_with_token.sh` wrapper retrieves the OAuth token from the app.ci cluster and adds it as an Authorization header automatically, without exposing the token.\n\n3. **Display Results**: Parse and present the JSON response with:\n   - `id`: The execution ID\n   - `job_name`: The name of the job\n   - `job_type`: The type of job execution (PERIODIC, POSTSUBMIT, PRESUBMIT)\n   - `job_status`: Current status (SUCCESS, FAILURE, PENDING, RUNNING, ABORTED)\n   - `gcs_path`: Path to job artifacts in GCS (if available)\n\n4. **Offer Follow-up Actions**:\n   - If status is PENDING or RUNNING: Offer to check again after a delay\n   - If status is SUCCESS or FAILURE with gcs_path: Offer to help access logs/artifacts\n\n## Return Value\n- **Success**: JSON response with job status details\n- **Error**: HTTP error, authentication failure, or invalid execution ID\n\n**Important for Claude**:\n1. **REQUIRED**: Before executing this command, you MUST ensure the `ci:oc-auth` skill is loaded by invoking it with the Skill tool. The curl_with_token.sh script depends on this skill being active.\n2. You must locate and verify curl_with_token.sh before running it, you (Claude Code) have a bug that tries to use the script from the wrong directory!\n3. Parse the JSON response and present it in a readable format\n4. Highlight the job status prominently\n5. If PENDING/RUNNING, mention the job is still in progress\n6. If SUCCESS/FAILURE, indicate completion status\n7. If gcs_path is available, provide the path to artifacts\n\n## Examples\n\n1. **Query status of a triggered job**:\n   ```\n   /query-job-status ca249d50-dee8-4424-a0a7-6dd9d5605267\n   ```\n   Returns:\n   ```json\n   {\n     \"id\": \"ca249d50-dee8-4424-a0a7-6dd9d5605267\",\n     \"job_name\": \"periodic-ci-openshift-release-master-ci-4.14-e2e-aws-ovn\",\n     \"job_type\": \"PERIODIC\",\n     \"job_status\": \"SUCCESS\",\n     \"gcs_path\": \"gs://origin-ci-test/logs/periodic-ci-openshift-release-master-ci-4.14-e2e-aws-ovn/1234567890\"\n   }\n   ```\n\n2. **Check running job**:\n   ```\n   /query-job-status 8f3a9b2c-1234-5678-9abc-def012345678\n   ```\n   Status shows \"RUNNING\" - Claude offers to check again later.\n\n3. **Check failed job**:\n   ```\n   /query-job-status 5a6b7c8d-9e0f-1a2b-3c4d-5e6f7a8b9c0d\n   ```\n   Status shows \"FAILURE\" - Claude displays the gcs_path for log analysis.\n\n## Notes\n\n- **Execution ID Format**: UUID format (e.g., `ca249d50-dee8-4424-a0a7-6dd9d5605267`)\n- **Job Status Values**: SUCCESS, FAILURE, PENDING, RUNNING, ABORTED\n- **Rate Limits**: The REST API has rate limits\n- **Authentication**: Tokens expire and may need to be refreshed via browser login\n- **GCS Path**: Provides access to job logs and artifacts when available\n- **Polling**: For long-running jobs, you may need to query multiple times\n\n## Arguments\n- **$1** (execution-id): The UUID execution ID returned when a job was triggered (required)"
              },
              {
                "name": "/query-test-result",
                "description": "Query test results from Sippy by version and test keywords",
                "path": "plugins/ci/commands/query-test-result.md",
                "frontmatter": {
                  "description": "Query test results from Sippy by version and test keywords",
                  "argument-hint": "<version> <keywords> [sippy-url]"
                },
                "content": "## Name\nci:query-test-result\n\n## Synopsis\n```\n/ci:query-test-result <version> <keywords> [sippy-url]\n```\n\n## Description\nThe `ci:query-test-result` command queries OpenShift CI test results from Sippy based on the OpenShift version and test name keywords. It retrieves test statistics including pass rate, number of runs, failures, and links to failed job runs.\n\nBy default, it queries the production Sippy instance at `sippy.dptools.openshift.org`. You can optionally specify a different Sippy instance URL to query alternative environments (e.g., QE component readiness).\n\nThis command is useful for:\n- Checking the health of specific test cases\n- Finding test failure patterns\n- Getting links to failed Prow job runs for debugging\n- Monitoring test regression trends\n- Querying different Sippy instances (production, QE, etc.)\n\n## Arguments\n- `$1` (version): OpenShift version to query (e.g., \"4.21\", \"4.20\", \"4.19\")\n- `$2` (keywords): Keywords to search in test names (e.g., \"PolarionID:81664\", \"olmv1\", \"sig-storage\")\n- `$3` (sippy-url) [optional]: Sippy instance base URL. Defaults to \"sippy.dptools.openshift.org\" if not provided. Examples: \"qe-component-readiness.dptools.openshift.org\"\n\n## Implementation\n\n1. **Parse Arguments**\n   - Extract version from `$1` (e.g., \"4.21\")\n   - Extract keywords from `$2` (e.g., \"81664\" or \"olmv1\")\n   - Extract Sippy URL from `$3` if provided, otherwise use default \"sippy.dptools.openshift.org\"\n   - Normalize URL to extract base domain for API endpoint (strip \"/sippy-ng/\" suffix if present)\n   - Add \"https://\" prefix if not already present\n\n2. **Build Sippy API Request**\n   - Construct filter JSON for the `/api/tests` endpoint:\n     ```python\n     filters = {\n         \"items\": [\n             {\n                 \"columnField\": \"name\",\n                 \"not\": False,\n                 \"operatorValue\": \"contains\",\n                 \"value\": keywords\n             }\n         ],\n         \"linkOperator\": \"and\"\n     }\n     ```\n   - Set query parameters:\n     - `release`: The OpenShift version\n     - `filter`: JSON-encoded filter object\n     - `sort`: \"asc\"\n     - `sortField`: \"net_improvement\"\n\n3. **Query Test Statistics**\n   - Construct API endpoint from provided URL: `https://{base_url}/api/tests`\n     - Example: If input is \"sippy.dptools.openshift.org\", use `https://sippy.dptools.openshift.org/api/tests`\n     - Example: If input is \"qe-component-readiness.dptools.openshift.org\", use `https://qe-component-readiness.dptools.openshift.org/api/tests`\n   - Make GET request to the constructed endpoint\n   - Parse response to extract:\n     - Test name\n     - Current pass percentage\n     - Total runs, passes, failures\n     - Net improvement (trend indicator)\n\n4. **Query Failed Job Runs** (for each matching test)\n   - Calculate timestamp for 7 days ago: `start_time = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)`\n   - Build filter for failed runs:\n     ```python\n     filters = {\n         \"items\": [\n             {\n                 \"columnField\": \"failed_test_names\",\n                 \"operatorValue\": \"contains\",\n                 \"value\": test_name\n             },\n             {\n                 \"columnField\": \"timestamp\",\n                 \"operatorValue\": \">\",\n                 \"value\": str(start_time)\n             }\n         ],\n         \"linkOperator\": \"and\"\n     }\n     ```\n   - Make GET request to `https://{base_url}/api/jobs/runs` with parameters:\n     - `release`: version\n     - `filter`: JSON-encoded filter\n     - `limit`: \"20\"\n     - `sortField`: \"timestamp\"\n     - `sort`: \"desc\"\n   - **Parse Response Structure**:\n     - Response is a dict with structure: `{\"rows\": [...], \"page\": N, \"page_size\": N, \"total_rows\": N}`\n     - Extract job runs from `response[\"rows\"]`\n     - Each run contains:\n       - `timestamp`: Unix timestamp in milliseconds\n       - `brief_name` or `job`: Job name\n       - `test_grid_url`: Link to Prow job details\n\n5. **Format and Display Results**\n   - Show summary statistics for each matching test\n   - List failed job runs with:\n     - Timestamp of the failure\n     - Job name\n     - Clickable Prow URL for each failed run\n   - **After listing individual runs, provide a summary section:**\n     - Create a \"Failed Prow URLs (for easy copying)\" section\n     - List all Prow URLs from the failed runs in plain text format (one per line)\n     - This allows users to easily copy all URLs at once for further analysis\n   - Format output in a clear, readable structure with proper spacing\n   - Present URLs as markdown links for easy clicking\n\n## Return Value\n\n**Format**: Formatted text output with:\n- Test name(s) matching the keywords\n- Statistics section showing:\n  - Pass Rate (percentage)\n  - Total Runs\n  - Passes\n  - Failures\n  - Net Improvement\n- Failed Job Runs section listing (for last 7 days):\n  - Sequential numbering (1, 2, 3...)\n  - Timestamp (formatted as YYYY-MM-DD HH:MM:SS)\n  - Job name (brief name)\n  - Clickable Prow URL (as markdown link or plain URL)\n- Failed Prow URLs summary section:\n  - Plain text list of all Prow URLs (one per line)\n  - Allows easy copying of all URLs for batch analysis\n\n**Output Format Example:**\n```\nFailed Job Runs (Last 7 Days):\n1. 2025-11-03 12:12:31 - periodic-ci-openshift-operator-framework-...\n   https://prow.ci.openshift.org/view/gs/test-platform-results/logs/...\n\n2. 2025-11-02 12:12:29 - periodic-ci-openshift-operator-framework-...\n   https://prow.ci.openshift.org/view/gs/test-platform-results/logs/...\n```\n\nIf no tests match the keywords, inform the user that no results were found.\nIf a test has no failed runs in the last 7 days, display a success message.\n\n## Examples\n\n1. **Query by Polarion ID (using default Sippy instance)**:\n   ```\n   /ci:query-test-result 4.21 81664\n   ```\n\n   Returns test results for tests containing \"81664\" in version 4.21 from the default production Sippy instance (sippy.dptools.openshift.org).\n\n2. **Query by test signature (using default Sippy instance)**:\n   ```\n   /ci:query-test-result 4.20 olmv1\n   ```\n\n   Returns all OLMv1-related test results for version 4.20 from the default Sippy instance.\n\n3. **Query from QE Sippy instance (custom URL)**:\n   ```\n   /ci:query-test-result 4.20 olmv1 qe-component-readiness.dptools.openshift.org\n   ```\n\n   Returns all OLMv1-related test results for version 4.20 from the QE component readiness Sippy instance.\n\n4. **Query by component with custom Sippy URL**:\n   ```\n   /ci:query-test-result 4.19 sig-storage sippy.dptools.openshift.org\n   ```\n\n   Returns all storage-related test results for version 4.19 from the specified Sippy instance.\n\n5. **Custom URL variations (all valid formats)**:\n   ```\n   /ci:query-test-result 4.21 olmv1 sippy.dptools.openshift.org\n   /ci:query-test-result 4.21 olmv1 https://sippy.dptools.openshift.org\n   ```\n\n   Both URL formats are accepted and will query the same Sippy instance.\n\n## Output Example\n\n```\n====================================================================================================\nTest Results for PolarionID: 81664 (Version 4.21)\n====================================================================================================\n\nTest Name:\n[sig-olmv1][Jira:OLM] clusterextension PolarionID:81664-[Skipped:Disconnected]preflight check\n\nStatistics (Last 7 Days):\n   Pass Rate: 0.00%\n   Total Runs: 6\n   Passes: 0\n   Failures: 6\n   Net Improvement: -100.00\n\nFailed Job Runs (Last 7 Days):\n----------------------------------------------------------------------------------------------------\n\n1. 2025-11-03 12:12:31\n   Job: periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1\n   https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1/1985198377557561344\n\n2. 2025-11-02 12:12:29\n   Job: periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1\n   https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1/1984835985292136448\n\n[... additional failures ...]\n\n----------------------------------------------------------------------------------------------------\nFailed Prow URLs (for easy copying):\n----------------------------------------------------------------------------------------------------\nhttps://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1/1985198377557561344\nhttps://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-operator-framework-operator-controller-release-4.21-periodics-e2e-aws-ovn-techpreview-extended-f1/1984835985292136448\n[... additional URLs ...]\n\n====================================================================================================\n```\n\n## Notes\n\n- **Default Sippy URL**: If no Sippy URL is provided, the command uses `sippy.dptools.openshift.org` by default\n- The command queries data from the last 7 days by default\n- Ensure you can access the Sippy API endpoints\n- Results are sorted by net improvement to show regressed tests first\n- Failed job runs are limited to the most recent 20 occurrences\n- **URLs are displayed as clickable links** for easy access to Prow job details\n- If multiple tests match the keywords, results for all matches will be displayed\n- URL normalization:\n  - The command automatically strips common suffixes like \"/sippy-ng/\" from the URL\n  - It adds \"https://\" prefix if not provided\n  - Both domain-only and full path formats are supported\n\n## See Also\n\n- Sippy UI (Production): https://sippy.dptools.openshift.org/sippy-ng/\n- Sippy UI (QE): https://qe-component-readiness.dptools.openshift.org\n- Sippy API Documentation: https://github.com/openshift/sippy"
              },
              {
                "name": "/trigger-periodic",
                "description": "Trigger a periodic gangway job with optional environment variable overrides",
                "path": "plugins/ci/commands/trigger-periodic.md",
                "frontmatter": {
                  "description": "Trigger a periodic gangway job with optional environment variable overrides",
                  "argument-hint": "<job-name> [ENV_VAR=value ...]"
                },
                "content": "## Name\nci:trigger-periodic\n\n## Synopsis\n```\n/trigger-periodic <job-name> [ENV_VAR=value ...]\n```\n\n## Description\n\nThe `trigger-periodic` command triggers a periodic gangway job via the REST API. Periodic jobs run on a schedule but can be manually triggered for testing or urgent runs.\n\nThe command accepts:\n- Job name (required, first argument)\n- Environment variable overrides (optional, additional arguments in KEY=VALUE format)\n\nIt then constructs and executes the appropriate curl command to trigger the job via the gangway REST API.\n\n## Security\n\n**IMPORTANT SECURITY REQUIREMENTS:**\n\nClaude is granted LIMITED and SPECIFIC access to the app.ci cluster token for the following AUTHORIZED operations ONLY:\n- **READ operations**: Checking authentication status (`oc whoami`)\n- **TRIGGERING jobs**: POST requests to the gangway API to trigger jobs\n\nClaude is EXPLICITLY PROHIBITED from:\n- Modifying cluster resources (deployments, pods, services, etc.)\n- Deleting or altering existing jobs or executions\n- Accessing secrets, configmaps, or sensitive data\n- Making any cluster modifications beyond job triggering\n- Using the token for any purpose other than the specific operations listed above\n\n**MANDATORY USER CONFIRMATION:**\nBefore executing ANY POST operation (job trigger), Claude MUST:\n1. Display the complete payload that will be sent\n2. Show the exact curl command that will be executed\n3. Request explicit user confirmation with a clear \"yes/no\" prompt\n4. Only proceed after receiving affirmative confirmation\n\n**Token Usage:**\nThe app.ci cluster token is used solely for authentication with the gangway REST API. This token grants the same permissions as the authenticated user and must be handled with appropriate care. The `curl_with_token.sh` wrapper handles all authentication automatically.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - First argument is the job name (required)\n   - Remaining arguments are environment variable overrides in KEY=VALUE format\n   - Note: Variables that need to override multistage parameters should be prefixed with `MULTISTAGE_PARAM_OVERRIDE_`\n\n2. **Construct API Request**: Build the appropriate curl command using the `oc-auth` skill's curl wrapper:\n\n   **Without overrides:**\n   ```bash\n   # Use curl_with_token.sh from oc-auth skill - it automatically adds the OAuth token\n   # app.ci cluster API: https://api.ci.l2s4.p1.openshiftapps.com:6443\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -v -X POST \\\n     -d '{\"job_name\": \"<JOB_NAME>\", \"job_execution_type\": \"1\"}' \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n   ```\n\n   **With overrides:**\n   ```bash\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -v -X POST \\\n     -d '{\"job_name\": \"<JOB_NAME>\", \"job_execution_type\": \"1\", \"pod_spec_options\": {\"envs\": {\"ENV_VAR\": \"value\"}}}' \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n   ```\n\n   **With multistage parameter override:**\n   ```bash\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -v -X POST \\\n     -d '{\"job_name\": \"periodic-to-trigger\", \"job_execution_type\": \"1\", \"pod_spec_options\": {\"envs\": {\"MULTISTAGE_PARAM_OVERRIDE_FOO\": \"bar\"}}}' \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n   ```\n   \n   The `curl_with_token.sh` wrapper retrieves the OAuth token from the app.ci cluster and adds it as an Authorization header automatically, without exposing the token.\n\n3. **Request User Confirmation**: Display the complete JSON payload and curl command to the user, then explicitly ask for confirmation before proceeding. Wait for affirmative user response.\n\n4. **Execute Request**: Only after receiving user confirmation, run the constructed curl command\n\n6. **Display Results**: Show the API response including the execution ID\n\n7. **Offer Follow-up**: Optionally offer to query the job status using `/query-job-status`\n\n## Return Value\n- **Success**: JSON response with execution ID and job details\n- **Error**: HTTP error, authentication failure, or missing job name\n\n**Important for Claude**:\n1. **REQUIRED**: Before executing this command, you MUST ensure the `ci:oc-auth` skill is loaded by invoking it with the Skill tool. The curl_with_token.sh script depends on this skill being active.\n2. You must locate and verify curl_with_token.sh before running it, you (Claude Code) have a bug that tries to use the script from the wrong directory!\n3. Parse the JSON response and extract the execution ID\n4. Display the execution ID to the user\n5. Offer to check job status with `/query-job-status`\n\n## Examples\n\n1. **Trigger a periodic job without overrides**:\n   ```\n   /trigger-periodic periodic-ci-openshift-release-master-ci-4.14-e2e-aws-ovn\n   ```\n\n2. **Trigger a periodic job with payload override**:\n   ```\n   /trigger-periodic periodic-ci-openshift-release-master-ci-4.14-e2e-aws-ovn RELEASE_IMAGE_LATEST=quay.io/openshift-release-dev/ocp-release:4.18.8-x86_64\n   ```\n\n3. **Trigger with multistage parameter override**:\n   ```\n   /trigger-periodic periodic-to-trigger MULTISTAGE_PARAM_OVERRIDE_FOO=bar\n   ```\n\n4. **Trigger with multiple environment overrides**:\n   ```\n   /trigger-periodic periodic-ci-job RELEASE_IMAGE_LATEST=quay.io/image:4.18.8 MULTISTAGE_PARAM_OVERRIDE_TIMEOUT=3600\n   ```\n\n## Notes\n\n- **Job Execution Type**: For periodic jobs, always use `\"1\"`\n- **Rate Limits**: The REST API has rate limits; username is recorded in annotations\n- **Authentication**: Tokens expire and may need to be refreshed via browser login\n- **Multistage Overrides**: Prefix variables with `MULTISTAGE_PARAM_OVERRIDE_` to override multistage job parameters\n- **Execution ID**: Save the execution ID from the response to query job status later\n\n## Arguments\n- **$1** (job-name): The name of the periodic job to trigger (required)\n- **$2-$N** (ENV_VAR=value): Optional environment variable overrides in KEY=VALUE format"
              },
              {
                "name": "/trigger-postsubmit",
                "description": "Trigger a postsubmit gangway job with repository refs",
                "path": "plugins/ci/commands/trigger-postsubmit.md",
                "frontmatter": {
                  "description": "Trigger a postsubmit gangway job with repository refs",
                  "argument-hint": "<job-name> <org> <repo> <base-ref> <base-sha> [ENV_VAR=value ...]"
                },
                "content": "## Name\nci:trigger-postsubmit\n\n## Synopsis\n```\n/trigger-postsubmit <job-name> <org> <repo> <base-ref> <base-sha> [ENV_VAR=value ...]\n```\n\n## Description\n\nThe `trigger-postsubmit` command triggers a postsubmit gangway job via the REST API. Postsubmit jobs run after code is merged and require repository reference information.\n\nThe command accepts:\n- Job name (required)\n- Organization (required, e.g., \"openshift\")\n- Repository name (required, e.g., \"assisted-installer\")\n- Base ref/branch (required, e.g., \"release-4.12\")\n- Base SHA/commit hash (required)\n- Environment variable overrides (optional, additional arguments in KEY=VALUE format)\n\nIt constructs the necessary JSON payload with refs structure and executes the curl command to trigger the job via the gangway REST API.\n\n## Security\n\n**IMPORTANT SECURITY REQUIREMENTS:**\n\nClaude is granted LIMITED and SPECIFIC access to the app.ci cluster token for the following AUTHORIZED operations ONLY:\n- **READ operations**: Checking authentication status (`oc whoami`)\n- **TRIGGERING jobs**: POST requests to the gangway API to trigger jobs\n\nClaude is EXPLICITLY PROHIBITED from:\n- Modifying cluster resources (deployments, pods, services, etc.)\n- Deleting or altering existing jobs or executions\n- Accessing secrets, configmaps, or sensitive data\n- Making any cluster modifications beyond job triggering\n- Using the token for any purpose other than the specific operations listed above\n\n**MANDATORY USER CONFIRMATION:**\nBefore executing ANY POST operation (job trigger), Claude MUST:\n1. Display the complete payload that will be sent\n2. Show the exact curl command that will be executed\n3. Request explicit user confirmation with a clear \"yes/no\" prompt\n4. Only proceed after receiving affirmative confirmation\n\n**Token Usage:**\nThe app.ci cluster token is used solely for authentication with the gangway REST API. This token grants the same permissions as the authenticated user and must be handled with appropriate care. The `curl_with_token.sh` wrapper handles all authentication automatically.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - $1: job name (required)\n   - $2: organization (required)\n   - $3: repository name (required)\n   - $4: base ref/branch (required)\n   - $5: base SHA (required)\n   - $6-$N: environment variable overrides in KEY=VALUE format (optional)\n\n3. **Construct JSON Payload**: Build the payload with refs structure:\n\n   **Without overrides:**\n   ```json\n   {\n     \"job_name\": \"<JOB_NAME>\",\n     \"job_execution_type\": \"2\",\n     \"refs\": {\n       \"org\": \"<ORG>\",\n       \"repo\": \"<REPO>\",\n       \"base_ref\": \"<BASE_REF>\",\n       \"base_sha\": \"<BASE_SHA>\",\n       \"repo_link\": \"https://github.com/<ORG>/<REPO>\"\n     }\n   }\n   ```\n\n   **With overrides:**\n   ```json\n   {\n     \"job_name\": \"<JOB_NAME>\",\n     \"job_execution_type\": \"2\",\n     \"refs\": {\n       \"org\": \"<ORG>\",\n       \"repo\": \"<REPO>\",\n       \"base_ref\": \"<BASE_REF>\",\n       \"base_sha\": \"<BASE_SHA>\",\n       \"repo_link\": \"https://github.com/<ORG>/<REPO>\"\n     },\n     \"pod_spec_options\": {\n       \"envs\": {\"ENV_VAR\": \"value\"}\n     }\n   }\n   ```\n\n4. **Save JSON to Temporary File**: Write the payload to a temp file (e.g., `/tmp/postsubmit-spec.json`)\n\n5. **Request User Confirmation**: Display the complete JSON payload and curl command to the user, then explicitly ask for confirmation before proceeding. Wait for affirmative user response.\n\n6. **Execute Request**: Only after receiving user confirmation, run the curl command using the `oc-auth` skill's curl wrapper:\n   ```bash\n   # Use curl_with_token.sh from oc-auth skill - it automatically adds the OAuth token\n   # app.ci cluster API: https://api.ci.l2s4.p1.openshiftapps.com:6443\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -v -X POST \\\n     -d @/tmp/postsubmit-spec.json \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n   ```\n   The `curl_with_token.sh` wrapper retrieves the OAuth token from the app.ci cluster and adds it as an Authorization header automatically, without exposing the token.\n\n7. **Clean Up**: Remove the temporary JSON file\n\n8. **Display Results**: Show the API response including the execution ID\n\n9. **Offer Follow-up**: Optionally offer to query the job status using `/query-job-status`\n\n## Return Value\n- **Success**: JSON response with execution ID and job details\n- **Error**: HTTP error, authentication failure, or missing required arguments\n\n**Important for Claude**:\n1. **REQUIRED**: Before executing this command, you MUST ensure the `ci:oc-auth` skill is loaded by invoking it with the Skill tool. The curl_with_token.sh script depends on this skill being active.\n2. You must locate and verify curl_with_token.sh before running it, you (Claude Code) have a bug that tries to use the script from the wrong directory!\n3. Validate all required arguments are provided\n4. Parse the JSON response and extract the execution ID\n5. Display the execution ID to the user\n6. Offer to check job status with `/query-job-status`\n\n## Examples\n\n1. **Trigger a postsubmit job without overrides**:\n   ```\n   /trigger-postsubmit branch-ci-openshift-assisted-installer-release-4.12-images openshift assisted-installer release-4.12 7336f38f75f91a876313daacbfw97f25dfe21bbf\n   ```\n\n2. **Trigger a postsubmit job with environment override**:\n   ```\n   /trigger-postsubmit branch-ci-openshift-origin-master-images openshift origin master abc123def456 RELEASE_IMAGE_LATEST=quay.io/image:latest\n   ```\n\n3. **Trigger with multiple environment overrides**:\n   ```\n   /trigger-postsubmit my-postsubmit-job openshift cluster-api-provider-aws master def789ghi012 MULTISTAGE_PARAM_OVERRIDE_TIMEOUT=7200 BUILD_ID=custom-123\n   ```\n\n## Notes\n\n- **Job Execution Type**: For postsubmit jobs, always use `\"2\"`\n- **Rate Limits**: The REST API has rate limits; username is recorded in annotations\n- **Authentication**: Tokens expire and may need to be refreshed via browser login\n- **Refs Structure**: The refs object is required for postsubmit jobs to identify the repository and commit\n- **Repo Link**: Automatically constructed as `https://github.com/<org>/<repo>`\n- **Execution ID**: Save the execution ID from the response to query job status later\n\n## Arguments\n- **$1** (job-name): The name of the postsubmit job to trigger (required)\n- **$2** (org): GitHub organization (e.g., \"openshift\") (required)\n- **$3** (repo): Repository name (e.g., \"assisted-installer\") (required)\n- **$4** (base-ref): Base branch/ref (e.g., \"release-4.12\", \"master\") (required)\n- **$5** (base-sha): Base commit SHA hash (required)\n- **$6-$N** (ENV_VAR=value): Optional environment variable overrides in KEY=VALUE format"
              },
              {
                "name": "/trigger-presubmit",
                "description": "Trigger a presubmit gangway job (typically use GitHub Prow commands instead)",
                "path": "plugins/ci/commands/trigger-presubmit.md",
                "frontmatter": {
                  "description": "Trigger a presubmit gangway job (typically use GitHub Prow commands instead)",
                  "argument-hint": "<job-name> <org> <repo> <base-ref> <base-sha> <pr-number> <pr-sha> [ENV_VAR=value ...]"
                },
                "content": "## Name\nci:trigger-presubmit\n\n## Synopsis\n```\n/trigger-presubmit <job-name> <org> <repo> <base-ref> <base-sha> <pr-number> <pr-sha> [ENV_VAR=value ...]\n```\n\n## Description\n\nThe `trigger-presubmit` command triggers a presubmit gangway job via the REST API.\n\n**WARNING:** Triggering presubmit jobs via REST is generally unnecessary and not recommended. Presubmit jobs should typically be triggered using Prow commands like `/test` and `/retest` via GitHub interactions. Only use this command if you have a specific reason to trigger via REST API.\n\nThe command accepts:\n- Job name (required)\n- Organization (required, e.g., \"openshift\")\n- Repository name (required, e.g., \"origin\")\n- Base ref/branch (required, e.g., \"master\")\n- Base SHA/commit hash (required)\n- Pull request number (required)\n- Pull request SHA/head commit (required)\n- Environment variable overrides (optional, additional arguments in KEY=VALUE format)\n\nIt constructs the necessary JSON payload with refs and pulls structure and executes the curl command to trigger the job via the gangway REST API.\n\n## Security\n\n**IMPORTANT SECURITY REQUIREMENTS:**\n\nClaude is granted LIMITED and SPECIFIC access to the app.ci cluster token for the following AUTHORIZED operations ONLY:\n- **READ operations**: Checking authentication status (`oc whoami`)\n- **TRIGGERING jobs**: POST requests to the gangway API to trigger jobs\n\nClaude is EXPLICITLY PROHIBITED from:\n- Modifying cluster resources (deployments, pods, services, etc.)\n- Deleting or altering existing jobs or executions\n- Accessing secrets, configmaps, or sensitive data\n- Making any cluster modifications beyond job triggering\n- Using the token for any purpose other than the specific operations listed above\n\n**MANDATORY USER CONFIRMATION:**\nBefore executing ANY POST operation (job trigger), Claude MUST:\n1. Display the complete payload that will be sent\n2. Show the exact curl command that will be executed\n3. Request explicit user confirmation with a clear \"yes/no\" prompt\n4. Only proceed after receiving affirmative confirmation\n\n**Token Usage:**\nThe app.ci cluster token is used solely for authentication with the gangway REST API. This token grants the same permissions as the authenticated user and must be handled with appropriate care. The `curl_with_token.sh` wrapper handles all authentication automatically.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Warn User**: Display a warning that presubmit jobs should typically use GitHub Prow commands (`/test`, `/retest`)\n\n2. **Parse Arguments**:\n   - $1: job name (required)\n   - $2: organization (required)\n   - $3: repository name (required)\n   - $4: base ref/branch (required)\n   - $5: base SHA (required)\n   - $6: pull request number (required)\n   - $7: pull request SHA (required)\n   - $8-$N: environment variable overrides in KEY=VALUE format (optional)\n\n4. **Construct JSON Payload**: Build the payload with refs and pulls structure:\n\n   **Without overrides:**\n   ```json\n   {\n     \"job_name\": \"<JOB_NAME>\",\n     \"job_execution_type\": \"3\",\n     \"refs\": {\n       \"org\": \"<ORG>\",\n       \"repo\": \"<REPO>\",\n       \"base_ref\": \"<BASE_REF>\",\n       \"base_sha\": \"<BASE_SHA>\",\n       \"pulls\": [{\n         \"number\": <PR_NUMBER>,\n         \"sha\": \"<PR_SHA>\",\n         \"link\": \"https://github.com/<ORG>/<REPO>/pull/<PR_NUMBER>\"\n       }]\n     }\n   }\n   ```\n\n   **With overrides:**\n   ```json\n   {\n     \"job_name\": \"<JOB_NAME>\",\n     \"job_execution_type\": \"3\",\n     \"refs\": {\n       \"org\": \"<ORG>\",\n       \"repo\": \"<REPO>\",\n       \"base_ref\": \"<BASE_REF>\",\n       \"base_sha\": \"<BASE_SHA>\",\n       \"pulls\": [{\n         \"number\": <PR_NUMBER>,\n         \"sha\": \"<PR_SHA>\",\n         \"link\": \"https://github.com/<ORG>/<REPO>/pull/<PR_NUMBER>\"\n       }]\n     },\n     \"pod_spec_options\": {\n       \"envs\": {\"ENV_VAR\": \"value\"}\n     }\n   }\n   ```\n\n5. **Save JSON to Temporary File**: Write the payload to a temp file (e.g., `/tmp/presubmit-spec.json`)\n\n6. **Request User Confirmation**: Display the complete JSON payload and curl command to the user, then explicitly ask for confirmation before proceeding. Wait for affirmative user response.\n\n7. **Execute Request**: Only after receiving user confirmation, run the curl command using the `oc-auth` skill's curl wrapper:\n   ```bash\n   # Use curl_with_token.sh from oc-auth skill - it automatically adds the OAuth token\n   # app.ci cluster API: https://api.ci.l2s4.p1.openshiftapps.com:6443\n   curl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -v -X POST \\\n     -d @/tmp/presubmit-spec.json \\\n     https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n   ```\n   The `curl_with_token.sh` wrapper retrieves the OAuth token from the app.ci cluster and adds it as an Authorization header automatically, without exposing the token.\n\n8. **Clean Up**: Remove the temporary JSON file\n\n9. **Display Results**: Show the API response including the execution ID\n\n10. **Offer Follow-up**: Optionally offer to query the job status using `/query-job-status`\n\n## Return Value\n- **Success**: JSON response with execution ID and job details\n- **Error**: HTTP error, authentication failure, or missing required arguments\n\n**Important for Claude**:\n1. **REQUIRED**: Before executing this command, you MUST ensure the `ci:oc-auth` skill is loaded by invoking it with the Skill tool. The curl_with_token.sh script depends on this skill being active.\n2. You must locate and verify curl_with_token.sh before running it, you (Claude Code) have a bug that tries to use the script from the wrong directory!\n3. Display the warning about using GitHub Prow commands instead\n4. Validate all required arguments are provided\n5. Parse the JSON response and extract the execution ID\n6. Display the execution ID to the user\n7. Offer to check job status with `/query-job-status`\n\n## Examples\n\n1. **Trigger a presubmit job without overrides**:\n   ```\n   /trigger-presubmit pull-ci-openshift-origin-master-e2e-aws openshift origin master abc123def456 1234 def456ghi789\n   ```\n\n2. **Trigger a presubmit job with environment override**:\n   ```\n   /trigger-presubmit my-presubmit-job openshift installer master 1a2b3c4d 5678 4d5e6f7g RELEASE_IMAGE_INITIAL=quay.io/image:test\n   ```\n\n3. **Trigger with multiple environment overrides**:\n   ```\n   /trigger-presubmit pull-ci-test openshift cluster-version-operator master abcdef12 999 fedcba98 MULTISTAGE_PARAM_OVERRIDE_TIMEOUT=5400 TEST_SUITE=custom\n   ```\n\n## Notes\n\n- **Recommended Approach**: Use GitHub Prow commands (`/test <job-name>`, `/retest`) instead of this REST API\n- **Job Execution Type**: For presubmit jobs, always use `\"3\"`\n- **Rate Limits**: The REST API has rate limits; username is recorded in annotations\n- **Authentication**: Tokens expire and may need to be refreshed via browser login\n- **Refs Structure**: The refs object with pulls array is required for presubmit jobs\n- **Pull Link**: Automatically constructed as `https://github.com/<org>/<repo>/pull/<pr-number>`\n- **Execution ID**: Save the execution ID from the response to query job status later\n\n## Arguments\n- **$1** (job-name): The name of the presubmit job to trigger (required)\n- **$2** (org): GitHub organization (e.g., \"openshift\") (required)\n- **$3** (repo): Repository name (e.g., \"origin\") (required)\n- **$4** (base-ref): Base branch/ref (e.g., \"master\") (required)\n- **$5** (base-sha): Base commit SHA hash (required)\n- **$6** (pr-number): Pull request number (required)\n- **$7** (pr-sha): Pull request head commit SHA (required)\n- **$8-$N** (ENV_VAR=value): Optional environment variable overrides in KEY=VALUE format"
              }
            ],
            "skills": [
              {
                "name": "OC Authentication Helper",
                "description": "Helper skill to retrieve OAuth tokens from the correct OpenShift cluster context when multiple clusters are configured",
                "path": "plugins/ci/skills/oc-auth/SKILL.md",
                "frontmatter": {
                  "name": "OC Authentication Helper",
                  "description": "Helper skill to retrieve OAuth tokens from the correct OpenShift cluster context when multiple clusters are configured"
                },
                "content": "# OC Authentication Helper\n\nThis skill provides a centralized way to retrieve OAuth tokens from specific OpenShift clusters when multiple cluster contexts are configured in the user's kubeconfig.\n\n## When to Use This Skill\n\nUse this skill whenever you need to:\n- Get an OAuth token for API authentication from a specific OpenShift cluster\n- Verify authentication to a specific cluster\n- Work with multiple OpenShift cluster contexts simultaneously\n\nThis skill is used by all commands that need to authenticate with OpenShift clusters:\n- `ask-sippy` command (DPCR cluster)\n- `trigger-periodic`, `trigger-postsubmit`, `trigger-presubmit` commands (app.ci cluster)\n- `query-job-status` command (app.ci cluster)\n\nThe skill provides a single `curl_with_token.sh` script that wraps curl and automatically handles OAuth token retrieval and injection, preventing accidental token exposure.\n\n**Due to a known Claude Code bug with git-installed marketplace plugins:**\n\n  When referencing files from this skill (scripts, configuration files, etc.), you MUST:\n\n  1. **Always use the \"Base directory\" path** provided at the top of this skill prompt\n  2. **Never assume** skills are located in `~/.claude/plugins/`\n  3. **Construct full absolute paths** by combining the base directory with the relative file path\n\n  **Example:**\n  -  WRONG: `~/.claude/plugins/ci/skills/oc-auth/curl_with_token.sh`\n  -  CORRECT: Use the base directory shown above + `/curl_with_token.sh`\n\n  If you see \"no such file or directory\" errors, verify you're using the base directory path, not the assumed marketplace cache location.\n\n## Prerequisites\n\n1. **oc CLI Installation**\n   - Check if installed: `which oc`\n   - If not installed, provide instructions for the user's platform\n   - Installation guide: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html\n\n2. **User Authentication**\n   - User must be logged in to the target cluster via browser-based authentication\n   - Each `oc login` creates a new context in the kubeconfig\n\n## How It Works\n\nThe `oc` CLI maintains multiple cluster contexts in `~/.kube/config`. When a user runs `oc login` to different clusters, each login creates a separate context. This skill:\n\n1. Lists all available contexts\n2. Searches for the context matching the target cluster by API server URL\n3. Retrieves the OAuth token from that specific context\n4. Returns the token for use in API calls\n\n## Common Clusters\n\nHere are commonly used OpenShift clusters:\n\n### 1. `app.ci` - OpenShift CI Cluster\n- **Console URL**: https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/\n- **API Server**: https://api.ci.l2s4.p1.openshiftapps.com:6443\n- **Used by**: trigger-periodic, trigger-postsubmit, trigger-presubmit, query-job-status\n\n### 2. `dpcr` - DPCR Cluster\n- **Console URL**: https://console-openshift-console.apps.cr.j7t7.p1.openshiftapps.com/\n- **API Server**: https://api.cr.j7t7.p1.openshiftapps.com:6443\n- **Used by**: ask-sippy\n\n**Note**: The skill supports any OpenShift cluster - simply provide the cluster's API server URL.\n\n## Usage\n\n### Script: `curl_with_token.sh`\n\nA curl wrapper that automatically retrieves the OAuth token and adds it to the request, preventing token exposure.\n\n```bash\ncurl_with_token.sh <cluster_api_url> [curl arguments...]\n```\n\n**Parameters:**\n- `<cluster_api_url>`: Full cluster API server URL (e.g., `https://api.ci.l2s4.p1.openshiftapps.com:6443`)\n- `[curl arguments...]`: All standard curl arguments (URL, headers, data, etc.)\n\n**How it works:**\n1. Finds the oc context matching the specified cluster API URL\n2. Retrieves OAuth token from that cluster context\n3. Adds `Authorization: Bearer <token>` header automatically\n4. Executes curl with all provided arguments\n5. Token never appears in output or command history\n\n**Exit Codes:**\n- `0`: Success\n- `1`: Invalid cluster_id or missing arguments\n- `2`: No context found for the specified cluster\n- `3`: Failed to retrieve token from context\n- Other: curl exit codes\n\n### Example Usage in Commands\n\nUse the curl wrapper instead of regular curl for authenticated requests:\n\n```bash\n# Query app.ci API\ncurl_with_token.sh https://api.ci.l2s4.p1.openshiftapps.com:6443 -X POST \\\n  -d '{\"job_name\": \"my-job\", \"job_execution_type\": \"1\"}' \\\n  https://gangway-ci.apps.ci.l2s4.p1.openshiftapps.com/v1/executions\n\n# Query Sippy API (DPCR cluster)\ncurl_with_token.sh https://api.cr.j7t7.p1.openshiftapps.com:6443 -s -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"question\", \"chat_history\": []}' \\\n  https://sippy-auth.dptools.openshift.org/api/chat\n\n# Query any other OpenShift cluster API\ncurl_with_token.sh https://api.your-cluster.example.com:6443 -X GET \\\n  https://your-api.example.com/endpoint\n```\n\n**Benefits:**\n- Token never exposed in logs or output\n- Automatic authentication error handling\n- Same curl arguments you're already familiar with\n- Works with any curl flags (-v, -s, -X, -H, -d, etc.)\n\n## Error Handling\n\nThe script provides clear error messages for common scenarios:\n\n1. **Missing or invalid arguments**\n   - Error: \"Usage: curl_with_token.sh <cluster_api_url> [curl arguments...]\"\n   - Shows example usage\n\n2. **No context found**\n   - Error: \"No oc context found for cluster with API server: {url}\"\n   - Provides authentication instructions\n\n3. **Token retrieval failed**\n   - Error: \"Failed to retrieve token from context {context}\"\n   - Suggests re-authenticating to the cluster\n\n## Authentication Instructions\n\n### General Authentication Process:\n```\nPlease authenticate first:\n1. Visit the cluster's console URL in your browser\n2. Log in through the browser with your credentials\n3. Click on username  'Copy login command'\n4. Paste and execute the 'oc login' command in terminal\n\nVerify authentication with:\n  oc config get-contexts\n  oc cluster-info\n```\n\n### Examples:\n\n**For app.ci cluster:**\n1. Visit https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/\n2. Follow the authentication process above\n3. Verify with `oc cluster-info` - should show API server: https://api.ci.l2s4.p1.openshiftapps.com:6443\n\n**For DPCR cluster:**\n1. Visit https://console-openshift-console.apps.cr.j7t7.p1.openshiftapps.com/\n2. Follow the authentication process above\n3. Verify with `oc cluster-info` - should show API server: https://api.cr.j7t7.p1.openshiftapps.com:6443\n\n## Benefits\n\n1. **Single Source of Truth**: All context discovery logic is in one place\n2. **Consistency**: All commands use the same authentication method\n3. **Maintainability**: Changes to cluster names or patterns only need to be updated in one place\n4. **Error Handling**: Centralized error messages and authentication instructions\n5. **Multi-Cluster Support**: Users can be authenticated to multiple clusters simultaneously\n\n## Implementation Details\n\nThe script uses the following approach:\n\n1. **Get all context names**\n   ```bash\n   oc config get-contexts -o name\n   ```\n\n2. **Find matching context by API server URL**\n   ```bash\n   for ctx in $contexts; do\n     cluster_name=$(oc config view -o jsonpath=\"{.contexts[?(@.name=='$ctx')].context.cluster}\")\n     server=$(oc config view -o jsonpath=\"{.clusters[?(@.name=='$cluster_name')].cluster.server}\")\n     if [ \"$server\" = \"$target_url\" ]; then\n       echo \"$ctx\"\n       break\n     fi\n   done\n   ```\n\n3. **Retrieve token from context**\n   ```bash\n   oc whoami -t --context=$context_name\n   ```\n\nThis ensures we get the token from the correct cluster by matching the exact API server URL, even when multiple cluster contexts exist."
              }
            ]
          },
          {
            "name": "component-health",
            "description": "Analyze component health and track regressions across OpenShift releases",
            "source": "./plugins/component-health",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install component-health@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Analyze and grade component health based on regression and JIRA bug metrics",
                "path": "plugins/component-health/commands/analyze.md",
                "frontmatter": {
                  "description": "Analyze and grade component health based on regression and JIRA bug metrics",
                  "argument-hint": "<release> [--components comp1 comp2 ...] [--project JIRAPROJECT]"
                },
                "content": "## Name\n\ncomponent-health:analyze\n\n## Synopsis\n\n```\n/component-health:analyze <release> [--components comp1 comp2 ...] [--project JIRAPROJECT]\n```\n\n## Description\n\nThe `component-health:analyze` command provides comprehensive component health analysis for a specified OpenShift release by **automatically combining** regression management metrics with JIRA bug backlog data.\n\n**CRITICAL**: This command REQUIRES and AUTOMATICALLY fetches BOTH data sources:\n1. Regression data (via summarize-regressions)\n2. JIRA bug data (via summarize-jiras)\n\nThe analysis is INCOMPLETE without both data sources. Both are fetched automatically without user prompting.\n\nThe command evaluates component health based on:\n\n1. **Regression Management** (ALWAYS fetched automatically): How well components are managing test regressions\n   - Triage coverage (% of regressions triaged to JIRA bugs)\n   - Triage timeliness (average time from detection to triage)\n   - Resolution speed (average time from detection to closure)\n\n2. **Bug Backlog Health** (ALWAYS fetched automatically): Current state of open bugs for components\n   - Open bug counts by component\n   - Bug age distribution\n   - Bug priority breakdown\n   - Recent bug flow (opened vs closed in last 30 days)\n\nThis command is useful for:\n\n- **Grading overall component health** using multiple quality metrics\n- **Identifying components** that need help with regression or bug management\n- **Tracking quality trends** across releases\n- **Generating comprehensive quality scorecards** for stakeholders\n- **Prioritizing engineering investment** based on data-driven insights\n\nGrading is subjective and not meant to be a critique of team performance. This is intended to help identify where help is needed and track progress as we improve our quality practices.\n\n## Implementation\n\n**CRITICAL WORKFLOW**: The analyze command MUST execute steps 3 and 4 (fetch regression data AND fetch JIRA data) automatically without waiting for user prompting. Both data sources are required for a complete analysis.\n\n1. **Parse Arguments**: Extract release version and optional filters from arguments\n\n   - Release format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n   - Optional filters:\n     - `--components`: Space-separated list of component search strings (fuzzy match)\n     - `--project`: JIRA project key (default: \"OCPBUGS\")\n\n2. **Resolve Component Names**: Use fuzzy matching to find actual component names\n\n   - Run list_components.py to get all available components:\n     ```bash\n     python3 plugins/component-health/skills/list-components/list_components.py --release <release>\n     ```\n   - If `--components` was provided:\n     - For each search string, find all components containing that string (case-insensitive)\n     - Combine all matches into a single list\n     - Remove duplicates\n     - If no matches found for a search string, warn the user and show available components\n   - If `--components` was NOT provided:\n     - Use all available components from the list\n\n3. **Fetch Regression Summary**: REQUIRED - Always call the summarize-regressions command\n\n   **IMPORTANT**: This step is REQUIRED for the analyze command. Regression data must ALWAYS be fetched automatically without user prompting. The analyze command combines both regression and bug metrics - it is incomplete without both data sources.\n\n   - **ALWAYS execute this step** - do not skip or wait for user to request it\n   - Execute: `/component-health:summarize-regressions <release> [--components ...]`\n   - Pass resolved component names\n   - Extract regression metrics:\n     - Total regressions, triage percentages, timing metrics\n     - Per-component breakdowns\n     - Open vs closed regression counts\n   - Note development window dates for context\n   - If regression API is unreachable, inform the user and note this in the report but continue with bug-only analysis\n\n4. **Fetch JIRA Bug Summary**: REQUIRED - Always call the summarize-jiras command\n\n   **IMPORTANT**: This step is REQUIRED for the analyze command. JIRA bug data must ALWAYS be fetched automatically without user prompting. The analyze command combines both regression and bug metrics - it is incomplete without both data sources.\n\n   - **ALWAYS execute this step** - do not skip or wait for user to request it\n   - For each resolved component name:\n     - Execute: `/component-health:summarize-jiras --project <project> --component \"<component>\" --limit 1000`\n     - Note: Must iterate over components because JIRA queries can be too large otherwise\n   - Aggregate bug metrics across all components:\n     - Total open bugs by component\n     - Bug age distribution\n     - Opened vs closed in last 30 days\n     - Priority breakdowns\n   - If JIRA authentication is not configured, inform the user and provide setup instructions\n   - If JIRA queries fail, note this in the report but continue with regression-only analysis\n\n5. **Calculate Combined Health Grades**: REQUIRED - Analyze BOTH regression and bug data\n\n   **IMPORTANT**: This step requires data from BOTH step 3 (regressions) AND step 4 (JIRA bugs). Do not perform analysis with only one data source unless the other failed to fetch.\n\n   **For each component, grade based on:**\n\n   a. **Regression Health** (from step 3: summarize-regressions):\n      - Triage Coverage: % of regressions triaged\n        - 90-100%: Excellent \n        - 70-89%: Good \n        - 50-69%: Needs Improvement \n        - <50%: Poor \n      - Triage Timeliness: Average hours to triage\n        - <24 hours: Excellent \n        - 24-72 hours: Good \n        - 72-168 hours (1 week): Needs Improvement \n        - >168 hours: Poor \n      - Resolution Speed: Average hours to close\n        - <168 hours (1 week): Excellent \n        - 168-336 hours (1-2 weeks): Good \n        - 336-720 hours (2-4 weeks): Needs Improvement \n        - >720 hours (4+ weeks): Poor \n\n   b. **Bug Backlog Health** (from step 4: summarize-jiras):\n      - Open Bug Count: Total open bugs\n        - Component-relative thresholds (compare across components)\n      - Bug Age: Average/maximum age of open bugs\n        - <30 days average: Excellent \n        - 30-90 days: Good \n        - 90-180 days: Needs Improvement \n        - >180 days: Poor \n      - Bug Flow: Opened vs closed in last 30 days\n        - More closed than opened: Positive trend \n        - Equal: Stable \n        - More opened than closed: Growing backlog \n\n   c. **Combined Health Score**: Weighted average of regression and bug health\n      - Weight regression health more heavily (e.g., 60%) as it's more actionable\n      - Bug backlog provides context (40%)\n\n6. **Display Overall Health Report**: Present comprehensive analysis combining BOTH data sources\n\n   **IMPORTANT**: The report MUST include BOTH regression metrics AND JIRA bug metrics. Do not present regression-only analysis unless JIRA data fetch failed.\n\n   - Show which components were matched (if fuzzy search was used)\n   - Inform user that both regression and bug data were analyzed\n\n   **Section 1: Overall Release Health**\n   - Release version and development window\n   - Overall regression metrics (from summarize-regressions):\n     - Total regressions, triage %, timing metrics\n   - Overall bug metrics (from summarize-jiras):\n     - Total open bugs, opened/closed last 30 days, priority breakdown\n   - High-level combined health grade\n\n   **Section 2: Per-Component Health Scorecard**\n   - Ranked table of components from best to worst combined health\n   - Key metrics per component (BOTH regression AND bug data):\n     - Regression triage coverage\n     - Average triage time\n     - Average resolution time\n     - Open bug count (from JIRA)\n     - Bug age metrics (from JIRA)\n     - Bug flow (opened vs closed, from JIRA)\n     - Combined health grade\n   - Visual indicators (  ) for quick assessment\n\n   **Section 3: Components Needing Attention**\n   - Prioritized list of components with specific issues from BOTH sources\n   - Actionable recommendations for each component:\n     - \"X open untriaged regressions need triage\" (only OPEN, not closed)\n     - \"High bug backlog: X open bugs (Y older than 90 days)\" (from JIRA)\n     - \"Growing bug backlog: +X net bugs in last 30 days\" (from JIRA)\n     - \"Slow regression triage: X hours average\"\n   - Context for each issue\n\n7. **Offer HTML Report Generation** (AFTER displaying the text report):\n   - Ask the user if they would like an interactive HTML report\n   - If yes, generate an HTML report combining both data sources\n   - Use template from: `plugins/component-health/skills/analyze-regressions/report_template.html`\n   - Enhance template to include bug backlog metrics\n   - Save report to: `.work/component-health-{release}/health-report.html`\n   - Open the report in the user's default browser\n   - Display the file path to the user\n\n8. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid release format\n   - Missing regression or JIRA data\n   - API errors\n   - No matches for component filter\n   - JIRA authentication issues\n\n## Return Value\n\nThe command outputs a **Comprehensive Component Health Report**:\n\n### Overall Health Grade\n\nFrom combined regression and bug data:\n\n- **Release**: OpenShift version and development window\n- **Regression Metrics**:\n  - Total regressions: X (Y% triaged)\n  - Average triage time: X hours\n  - Average resolution time: X hours\n  - Open vs closed breakdown\n- **Bug Backlog Metrics**:\n  - Total open bugs: X across all components\n  - Bugs opened/closed in last 30 days\n  - Priority distribution\n- **Overall Health**: Combined grade (Excellent/Good/Needs Improvement/Poor)\n\n### Per-Component Health Scorecard\n\nRanked table combining both metrics:\n\n| Component | Regression Triage | Triage Time | Resolution Time | Open Bugs | Bug Age | Health Grade |\n|-----------|-------------------|-------------|-----------------|-----------|---------|--------------|\n| kube-apiserver | 100.0% | 58 hrs | 144 hrs | 15 | 45d avg |  Excellent |\n| etcd | 95.0% | 84 hrs | 192 hrs | 8 | 30d avg |  Good |\n| Monitoring | 86.7% | 68 hrs | 156 hrs | 23 | 120d avg |  Needs Improvement |\n\n### Components Needing Attention\n\nPrioritized list with actionable items:\n\n```\n1. Monitoring (Needs Improvement):\n   - 1 open untriaged regression (needs triage)\n   - High bug backlog: 23 open bugs (8 older than 90 days)\n   - Growing backlog: +5 net bugs in last 30 days\n   - Recommendation: Focus on triaging open regression and addressing oldest bugs\n\n2. Example-Component (Poor):\n   - 5 open untriaged regressions (urgent triage needed)\n   - Slow triage response: 120 hours average\n   - Very high bug backlog: 45 open bugs (15 older than 180 days)\n   - Recommendation: Immediate triage sprint needed; consider bug backlog cleanup initiative\n```\n\n**IMPORTANT**: When listing untriaged regressions:\n- **Only list OPEN untriaged regressions** - these are actionable\n- **Do NOT recommend triaging closed regressions** - tooling doesn't support retroactive triage\n- Calculate actionable count as: `open.total - open.triaged`\n\n### Additional Sections\n\nIf requested:\n- Detailed regression metrics by component\n- Detailed bug breakdowns by status and priority\n- Links to Sippy dashboards for regression analysis\n- Links to JIRA queries for bug investigation\n- Trends compared to previous releases (if available)\n\n## Examples\n\n1. **Analyze overall component health for a release**:\n\n   ```\n   /component-health:analyze 4.17\n   ```\n\n   Automatically fetches and analyzes BOTH data sources for release 4.17:\n   - Regression management metrics (via summarize-regressions)\n   - JIRA bug backlog metrics (via summarize-jiras)\n   - Combined health grades based on both sources\n   - Prioritized recommendations using both regression and bug data\n\n2. **Analyze specific components (exact match)**:\n\n   ```\n   /component-health:analyze 4.21 --components Monitoring Etcd\n   ```\n\n   Automatically fetches BOTH regression and bug data for Monitoring and Etcd:\n   - Compares combined health between the two components\n   - Shows regression metrics AND bug backlog for each\n   - Identifies which component needs more attention\n   - Provides targeted recommendations based on both data sources\n\n3. **Analyze by fuzzy search**:\n\n   ```\n   /component-health:analyze 4.21 --components network\n   ```\n\n   Automatically fetches BOTH data sources for all components containing \"network\":\n   - Finds all networking components (e.g., \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.)\n   - Compares combined health across all networking components\n   - Shows regression metrics AND bug backlog for each\n   - Identifies networking-related quality issues from both sources\n   - Provides targeted recommendations\n\n4. **Analyze with custom JIRA project**:\n\n   ```\n   /component-health:analyze 4.21 --project OCPSTRAT\n   ```\n\n   Analyzes health using bugs from OCPSTRAT project instead of default OCPBUGS.\n\n5. **In-development release analysis**:\n\n   ```\n   /component-health:analyze 4.21\n   ```\n\n   Automatically fetches BOTH data sources for an in-development release:\n   - Shows current regression management state\n   - Shows current bug backlog state\n   - Tracks bug flow trends (opened vs closed)\n   - Identifies areas to focus on before GA based on both regression and bug metrics\n\n## Arguments\n\n- `$1` (required): Release version\n  - Format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n  - Must be a valid OpenShift release number\n\n- `$2+` (optional): Filter flags\n  - `--components <search1> [search2 ...]`: Filter by component names using fuzzy search\n    - Space-separated list of component search strings\n    - Case-insensitive substring matching\n    - Each search string matches all components containing that substring\n    - If no components provided, all components are analyzed\n    - Applied to both regression and bug queries\n    - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n    - Example: \"kube-\" matches \"kube-apiserver\", \"kube-controller-manager\", etc.\n\n  - `--project <PROJECT>`: JIRA project key\n    - Default: \"OCPBUGS\"\n    - Use alternative project if component bugs are tracked elsewhere\n    - Examples: \"OCPSTRAT\", \"OCPQE\"\n\n## Prerequisites\n\n1. **Python 3**: Required to run the underlying data fetching scripts\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **JIRA Authentication**: Environment variables must be configured for bug data\n\n   - `JIRA_URL`: Your JIRA instance URL\n   - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n   - See `/component-health:summarize-jiras` for setup instructions\n\n3. **Network Access**: Must be able to reach both component health API and JIRA\n\n   - Ensure HTTPS requests can be made to both services\n   - Check firewall and VPN settings if needed\n\n## Notes\n\n- **CRITICAL**: This command AUTOMATICALLY fetches data from TWO sources:\n  1. Regression API (via `/component-health:summarize-regressions`)\n  2. JIRA API (via `/component-health:summarize-jiras`)\n- Both data sources are REQUIRED and fetched automatically without user prompting\n- The analysis is incomplete without both regression and bug data\n- Health grades are subjective and intended as guidance, not criticism\n- Recommendations focus on actionable items (open untriaged regressions, not closed)\n- Infrastructure regressions are automatically filtered from regression counts\n- JIRA queries default to open bugs + bugs closed in last 30 days\n- HTML reports provide interactive visualizations combining both data sources\n- If one data source fails, the command continues with the available data and notes the failure\n- For detailed regression data only, use `/component-health:list-regressions`\n- For detailed JIRA data only, use `/component-health:list-jiras`\n- This command provides the most comprehensive view by combining both sources\n\n## See Also\n\n- Related Command: `/component-health:summarize-regressions` (regression metrics)\n- Related Command: `/component-health:summarize-jiras` (bug backlog metrics)\n- Related Command: `/component-health:list-regressions` (raw regression data)\n- Related Command: `/component-health:list-jiras` (raw JIRA data)\n- Skill Documentation: `plugins/component-health/skills/analyze-regressions/SKILL.md`\n- Script: `plugins/component-health/skills/list-regressions/list_regressions.py`\n- Script: `plugins/component-health/skills/summarize-jiras/summarize_jiras.py`"
              },
              {
                "name": "/list-components",
                "description": "List all components tracked in Sippy for a release",
                "path": "plugins/component-health/commands/list-components.md",
                "frontmatter": {
                  "description": "List all components tracked in Sippy for a release",
                  "argument-hint": "<release>"
                },
                "content": "## Name\n\ncomponent-health:list-components\n\n## Synopsis\n\n```\n/component-health:list-components <release>\n```\n\n## Description\n\nThe `component-health:list-components` command fetches and displays all component names tracked in the Sippy component readiness system for a specified OpenShift release.\n\nThis command is useful for:\n\n- Discovering available components for a release\n- Validating component names before analysis\n- Understanding which teams/components are tracked\n- Generating component lists for reports\n- Finding exact component names for use in other commands\n\n## Implementation\n\n1. **Verify Prerequisites**: Check that Python 3 is installed\n\n   - Run: `python3 --version`\n   - Verify version 3.6 or later is available\n\n2. **Parse Arguments**: Extract release version from arguments\n\n   - Release format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n\n3. **Execute Python Script**: Run the list_components.py script\n\n   - Script location: `plugins/component-health/skills/list-components/list_components.py`\n   - Pass release as `--release` argument\n   - The script automatically appends \"-main\" suffix to construct the view\n   - Capture JSON output from stdout\n\n4. **Parse Output**: Process the JSON response\n\n   - Extract component count and component list\n   - Components are returned alphabetically sorted and unique\n\n5. **Present Results**: Display components in a readable format\n\n   - Show total count\n   - Display components in a numbered or bulleted list\n   - Optionally group by category (e.g., Networking, Storage, etc.)\n\n6. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid release format\n   - API errors (400, 404, 500, etc.)\n   - Empty results\n\n## Return Value\n\nThe command outputs a **Component List** with the following information:\n\n### Component Summary\n\n- **Release**: The release version queried\n- **View**: The constructed view parameter (release + \"-main\")\n- **Total Components**: Count of unique components found\n\n### Component List\n\nAn alphabetically sorted list of all components, for example:\n\n```\n1. Bare Metal Hardware Provisioning\n2. Build\n3. Cloud Compute / Cloud Controller Manager\n4. Cluster Version Operator\n5. Etcd\n6. HyperShift\n7. Image Registry\n8. Installer / openshift-installer\n9. kube-apiserver\n10. Machine Config Operator\n11. Management Console\n12. Monitoring\n13. Networking / ovn-kubernetes\n14. OLM\n15. Storage\n...\n```\n\n## Examples\n\n1. **List all components for release 4.21**:\n\n   ```\n   /component-health:list-components 4.21\n   ```\n\n   Displays all components tracked in Sippy for release 4.21.\n\n2. **List components for release 4.20**:\n\n   ```\n   /component-health:list-components 4.20\n   ```\n\n   Displays all components for the 4.20 release.\n\n## Arguments\n\n- `$1` (required): Release version\n  - Format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n  - Must be a valid OpenShift release number\n\n## Prerequisites\n\n1. **Python 3**: Required to run the data fetching script\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **Network Access**: Must be able to reach the Sippy API\n\n   - Ensure HTTPS requests can be made to `sippy.dptools.openshift.org`\n\n## Notes\n\n- The script automatically appends \"-main\" to the release version\n- Component names are case-sensitive\n- Component names are returned in alphabetical order\n- Some components use hierarchical names with \"/\" separator (e.g., \"Networking / ovn-kubernetes\")\n- The script has a 30-second timeout for HTTP requests\n- Component names returned can be used directly in other component-health commands\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/list-components/SKILL.md`\n- Script: `plugins/component-health/skills/list-components/list_components.py`\n- Related Command: `/component-health:list-regressions` (for regression data)\n- Related Command: `/component-health:summarize-jiras` (for bug data)\n- Related Command: `/component-health:analyze` (for health analysis)"
              },
              {
                "name": "/list-jiras",
                "description": "Query and list raw JIRA bug data for a specific project",
                "path": "plugins/component-health/commands/list-jiras.md",
                "frontmatter": {
                  "description": "Query and list raw JIRA bug data for a specific project",
                  "argument-hint": "<project> [--component comp1 comp2 ...] [--status status1 status2 ...] [--include-closed] [--limit N]"
                },
                "content": "## Name\n\ncomponent-health:list-jiras\n\n## Synopsis\n\n```\n/component-health:list-jiras <project> [--component comp1 comp2 ...] [--status status1 status2 ...] [--include-closed] [--limit N]\n```\n\n## Description\n\nThe `component-health:list-jiras` command queries JIRA bugs for a specified project and returns raw issue data. It fetches JIRA issues with all their fields and metadata without performing any summarization or aggregation.\n\nBy default, the command includes:\n- All currently open bugs\n- Bugs closed in the last 30 days (to track recent closure activity)\n\nThis command is useful for:\n\n- Fetching raw JIRA issue data for further processing\n- Accessing complete issue details including all fields\n- Building custom analysis workflows\n- Providing data to other commands (like `summarize-jiras`)\n- Exporting JIRA data for offline analysis\n\n## Implementation\n\n1. **Verify Prerequisites**: Check that Python 3 is installed\n\n   - Run: `python3 --version`\n   - Verify version 3.6 or later is available\n\n2. **Verify Environment Variables**: Ensure JIRA authentication is configured\n\n   - Check that the following environment variables are set:\n     - `JIRA_URL`: Base URL for JIRA instance (e.g., \"https://issues.redhat.com\")\n     - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n\n   - Verify with:\n     ```bash\n     echo \"JIRA_URL: ${JIRA_URL}\"\n     echo \"JIRA_PERSONAL_TOKEN: ${JIRA_PERSONAL_TOKEN:+***set***}\"\n     ```\n\n   - If missing, guide the user to set them:\n     ```bash\n     export JIRA_URL=\"https://issues.redhat.com\"\n     export JIRA_PERSONAL_TOKEN=\"your-token-here\"\n     ```\n\n3. **Parse Arguments**: Extract project key and optional filters from arguments\n\n   - Project key: Required first argument (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n   - Optional filters:\n     - `--component`: Space-separated list of component search strings (fuzzy match)\n     - `--status`: Space-separated list of status values\n     - `--include-closed`: Flag to include closed bugs\n     - `--limit`: Maximum number of issues to fetch per component (default: 1000, max: 1000)\n\n4. **Resolve Component Names** (if component filter provided): Use fuzzy matching to find actual component names\n\n   - Extract release from context or ask user for release version\n   - Run list_components.py to get all available components:\n     ```bash\n     python3 plugins/component-health/skills/list-components/list_components.py --release <release>\n     ```\n   - For each search string in `--component`:\n     - Find all components containing that string (case-insensitive)\n     - Combine all matches into a single list\n     - Remove duplicates\n     - If no matches found for a search string, warn the user and show available components\n\n5. **Execute Python Script**: Run the list_jiras.py script for each component\n\n   - Script location: `plugins/component-health/skills/list-jiras/list_jiras.py`\n   - **Important**: Iterate over each resolved component separately to avoid overly large queries\n   - For each component:\n     - Build command with project, single component, and other filters\n     - Execute: `python3 list_jiras.py --project <project> --component \"<component>\" [other args]`\n     - Capture JSON output from stdout\n   - Aggregate results from all components into a combined response\n\n6. **Parse Output**: Process the aggregated JSON response\n\n   - Extract metadata:\n     - `project`: Project key queried\n     - `total_count`: Total matching issues in JIRA\n     - `fetched_count`: Number of issues actually fetched\n     - `query`: JQL query that was executed\n     - `filters`: Applied filters\n   - Extract raw issues array:\n     - `issues`: Array of complete JIRA issue objects with all fields\n\n7. **Present Results**: Display or store the raw JIRA data\n\n   - Show which components were matched (if fuzzy search was used)\n   - The command returns the aggregated JSON response with metadata and raw issues from all components\n   - Inform the user about total count vs fetched count per component\n   - The raw issue data can be passed to other commands for analysis\n   - Suggest using `/component-health:summarize-jiras` for summary statistics\n   - Highlight any truncation (if fetched_count < total_count for any component)\n   - Suggest increasing --limit if results are truncated\n\n8. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid JIRA credentials\n   - Invalid project key\n   - HTTP errors (401, 404, 500, etc.)\n   - Rate limiting (429)\n\n## Return Value\n\nThe command outputs **raw JIRA issue data** in JSON format with the following structure:\n\n### Metadata\n\n- **project**: JIRA project key that was queried\n- **total_count**: Total number of matching issues in JIRA\n- **fetched_count**: Number of issues actually fetched (may be less than total if limited)\n- **query**: JQL query that was executed (includes filters)\n- **filters**: Object containing applied filters:\n  - `components`: List of component filters or null\n  - `statuses`: List of status filters or null\n  - `include_closed`: Boolean indicating if closed bugs were included\n  - `limit`: Maximum number of issues fetched\n\n### Issues Array\n\n- **issues**: Array of raw JIRA issue objects, each containing:\n  - `key`: Issue key (e.g., \"OCPBUGS-12345\")\n  - `fields`: Object containing all issue fields:\n    - `summary`: Issue title/summary\n    - `status`: Status object with name and ID\n    - `priority`: Priority object with name and ID\n    - `components`: Array of component objects\n    - `assignee`: Assignee object with user details\n    - `created`: Creation timestamp\n    - `updated`: Last updated timestamp\n    - `resolutiondate`: Resolution timestamp (if closed)\n    - `versions`: Affects Version/s array\n    - `fixVersions`: Fix Version/s array\n    - `customfield_12319940`: Target Version (custom field)\n    - And other JIRA fields as applicable\n\n### Additional Information\n\n- **note**: (Optional) If results are truncated, includes a note suggesting to increase the limit\n- **component_queries**: (Optional) When multiple components are queried, this array shows the individual query executed for each component. Each entry contains:\n  - `component`: The component name\n  - `query`: The JQL query executed for this component\n  - `total_count`: Total matching issues for this component\n  - `fetched_count`: Number of issues fetched for this component\n\n### Example Output Structure\n\n```json\n{\n  \"project\": \"OCPBUGS\",\n  \"total_count\": 1500,\n  \"fetched_count\": 100,\n  \"query\": \"project = OCPBUGS AND (status != Closed OR (status = Closed AND resolved >= \\\"2025-10-11\\\"))\",\n  \"filters\": {\n    \"components\": null,\n    \"statuses\": null,\n    \"include_closed\": false,\n    \"limit\": 100\n  },\n  \"component_queries\": [\n    {\n      \"component\": \"kube-apiserver\",\n      \"query\": \"project = OCPBUGS AND component = \\\"kube-apiserver\\\" AND ...\",\n      \"total_count\": 800,\n      \"fetched_count\": 50\n    },\n    {\n      \"component\": \"kube-controller-manager\",\n      \"query\": \"project = OCPBUGS AND component = \\\"kube-controller-manager\\\" AND ...\",\n      \"total_count\": 700,\n      \"fetched_count\": 50\n    }\n  ],\n  \"issues\": [\n    {\n      \"key\": \"OCPBUGS-12345\",\n      \"fields\": {\n        \"summary\": \"Bug title here\",\n        \"status\": {\"name\": \"New\", \"id\": \"1\"},\n        \"priority\": {\"name\": \"Major\", \"id\": \"3\"},\n        \"components\": [{\"name\": \"kube-apiserver\"}],\n        \"created\": \"2025-11-01T10:30:00.000+0000\",\n        ...\n      }\n    },\n    ...\n  ],\n  \"note\": \"Showing first 100 of 1500 total results. Increase --limit for more data.\"\n}\n```\n\n## Examples\n\n1. **List all open bugs for a project**:\n\n   ```\n   /component-health:list-jiras OCPBUGS\n   ```\n\n   Fetches all open bugs in the OCPBUGS project (up to default limit of 1000) and returns raw issue data.\n\n2. **Filter by specific component (exact match)**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --component \"kube-apiserver\"\n   ```\n\n   Returns raw data for bugs in the kube-apiserver component only.\n\n3. **Filter by fuzzy search**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --component network\n   ```\n\n   Finds all components containing \"network\" (case-insensitive) and returns bugs for all matches (e.g., \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.).\n   Makes separate JIRA queries for each component and aggregates results.\n\n4. **Filter by multiple search strings**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --component etcd kube-\n   ```\n\n   Finds all components containing \"etcd\" OR \"kube-\" and returns combined bug data.\n   Iterates over each component separately to avoid overly large queries.\n\n5. **Include closed bugs**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --include-closed --limit 500\n   ```\n\n   Returns both open and closed bugs, fetching up to 500 issues per component.\n\n6. **Filter by status**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --status New \"In Progress\" Verified\n   ```\n\n   Returns only bugs in New, In Progress, or Verified status.\n\n7. **Combine fuzzy search with other filters**:\n\n   ```\n   /component-health:list-jiras OCPBUGS --component network --status New Assigned --limit 200\n   ```\n\n   Returns bugs for all networking components that are in New or Assigned status.\n\n## Arguments\n\n- `$1` (required): JIRA project key\n  - Format: Project key in uppercase (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n  - Must be a valid JIRA project you have access to\n\n- `$2+` (optional): Filter flags\n  - `--component <search1> [search2 ...]`: Filter by component names using fuzzy search\n    - Space-separated list of component search strings\n    - Case-insensitive substring matching\n    - Each search string matches all components containing that substring\n    - Makes separate JIRA queries for each matched component to avoid overly large results\n    - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n    - Example: \"kube-\" matches \"kube-apiserver\", \"kube-controller-manager\", etc.\n    - Note: Requires release context (inferred from recent commands or specified by user)\n\n  - `--status <status1> [status2 ...]`: Filter by status values\n    - Space-separated list of status names\n    - Examples: `New`, `\"In Progress\"`, `Verified`, `Modified`, `ON_QA`\n\n  - `--include-closed`: Include closed bugs in results\n    - By default, only open bugs are returned\n    - When specified, closed bugs are included\n\n  - `--limit <N>`: Maximum number of issues to fetch per component\n    - Default: 1000\n    - Range: 1-1000\n    - When using component filters, this limit applies to each component separately\n    - Higher values provide more accurate statistics but slower performance\n\n## Prerequisites\n\n1. **Python 3**: Required to run the data fetching script\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **JIRA Authentication**: Environment variables must be configured\n\n   - `JIRA_URL`: Your JIRA instance URL\n   - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n\n   How to get a JIRA token:\n   - Navigate to JIRA  Profile  Personal Access Tokens\n   - Generate a new token with appropriate permissions\n   - Export it as an environment variable\n\n3. **Network Access**: Must be able to reach your JIRA instance\n\n   - Ensure HTTPS requests can be made to JIRA_URL\n   - Check firewall and VPN settings if needed\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- Output is JSON format for easy parsing and further processing\n- Diagnostic messages are written to stderr, data to stdout\n- The script has a 30-second timeout for HTTP requests\n- For large projects, consider using component filters to reduce query size\n- The returned data includes ALL JIRA fields for each issue, providing complete information\n- If you need summary statistics, use `/component-health:summarize-jiras` instead\n- If results show truncation, increase the --limit parameter to fetch more issues\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/list-jiras/SKILL.md`\n- Script: `plugins/component-health/skills/list-jiras/list_jiras.py`\n- Related Command: `/component-health:summarize-jiras` (for summary statistics)\n- Related Command: `/component-health:analyze`"
              },
              {
                "name": "/list-regressions",
                "description": "Fetch and list raw regression data for OpenShift releases",
                "path": "plugins/component-health/commands/list-regressions.md",
                "frontmatter": {
                  "description": "Fetch and list raw regression data for OpenShift releases",
                  "argument-hint": "<release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]"
                },
                "content": "## Name\n\ncomponent-health:list-regressions\n\n## Synopsis\n\n```\n/component-health:list-regressions <release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n```\n\n## Description\n\nThe `component-health:list-regressions` command fetches regression data for a specified OpenShift release and returns raw regression details without performing any summarization or analysis. It provides complete regression information including test names, timestamps, triages, and metadata.\n\nThis command is useful for:\n\n- Fetching raw regression data for further processing\n- Accessing complete regression details for specific components\n- Building custom analysis workflows\n- Providing data to other commands (like `summarize-regressions` and `analyze`)\n- Exporting regression data for offline analysis\n- Investigating specific test failures across releases\n\n## Implementation\n\n1. **Verify Prerequisites**: Check that Python 3 is installed\n\n   - Run: `python3 --version`\n   - Verify version 3.6 or later is available\n\n2. **Parse Arguments**: Extract release version and optional filters from arguments\n\n   - Release format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n   - Optional filters:\n     - `--components`: Space-separated list of component search strings (fuzzy match)\n     - `--start`: Start date for filtering (YYYY-MM-DD)\n     - `--end`: End date for filtering (YYYY-MM-DD)\n     - `--short`: Exclude regression arrays from output (only summaries)\n\n3. **Resolve Component Names**: Use fuzzy matching to find actual component names\n\n   - Run list_components.py to get all available components:\n     ```bash\n     python3 plugins/component-health/skills/list-components/list_components.py --release <release>\n     ```\n   - If `--components` was provided:\n     - For each search string, find all components containing that string (case-insensitive)\n     - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n     - Combine all matches into a single list\n     - Remove duplicates\n     - If no matches found for a search string, warn the user and show available components\n   - If `--components` was NOT provided:\n     - Use all available components from the list\n\n4. **Fetch Release Dates** (if date filtering needed): Run the get_release_dates.py script\n\n   - Script location: `plugins/component-health/skills/get-release-dates/get_release_dates.py`\n   - Pass release as `--release` argument\n   - Extract `development_start` and `ga` dates from JSON output\n   - Use these dates for `--start` and `--end` parameters if not explicitly provided\n\n5. **Execute Python Script**: Run the list_regressions.py script\n\n   - Script location: `plugins/component-health/skills/list-regressions/list_regressions.py`\n   - Pass release as `--release` argument\n   - Pass resolved component names as `--components` argument\n   - Pass `--start` date if filtering by start date\n   - Pass `--end` date if filtering by end date\n   - Capture JSON output from stdout\n\n6. **Parse Output**: Process the JSON response\n\n   - The script outputs JSON with the following structure:\n     - `summary`: Overall statistics (total, triaged, percentages, timing metrics)\n     - `components`: Dictionary mapping component names to regression data\n       - Each component has:\n         - `summary`: Component-specific statistics\n         - `open`: Array of open regression objects\n         - `closed`: Array of closed regression objects\n   - **Note**: When using `--short` flag, regression arrays are excluded (only summaries)\n\n7. **Present Results**: Display or store the raw regression data\n\n   - Show which components were matched (if fuzzy search was used)\n   - The command returns the complete JSON response with metadata and raw regressions\n   - Inform the user about overall counts from the summary\n   - The raw regression data can be passed to other commands for analysis\n   - Suggest using `/component-health:summarize-regressions` for summary statistics\n   - Suggest using `/component-health:analyze` for health grading\n\n8. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid release format\n   - API errors (404, 500, etc.)\n   - Empty results\n   - No matches for component filter\n\n## Return Value\n\nThe command outputs **raw regression data** in JSON format with the following structure:\n\n### Overall Summary\n\n- `summary.total`: Total number of regressions\n- `summary.triaged`: Total number of regressions triaged to JIRA bugs\n- `summary.triage_percentage`: Percentage of regressions that have been triaged\n- `summary.filtered_suspected_infra_regressions`: Count of infrastructure regressions filtered\n- `summary.time_to_triage_hrs_avg`: Average hours from opened to first triage\n- `summary.time_to_triage_hrs_max`: Maximum hours from opened to first triage\n- `summary.time_to_close_hrs_avg`: Average hours from opened to closed (closed only)\n- `summary.time_to_close_hrs_max`: Maximum hours from opened to closed (closed only)\n- `summary.open`: Summary statistics for open regressions\n  - `total`: Number of open regressions\n  - `triaged`: Number of open regressions triaged\n  - `triage_percentage`: Percentage of open regressions triaged\n  - `time_to_triage_hrs_avg`, `time_to_triage_hrs_max`: Triage timing metrics\n  - `open_hrs_avg`, `open_hrs_max`: How long regressions have been open\n- `summary.closed`: Summary statistics for closed regressions\n  - `total`: Number of closed regressions\n  - `triaged`: Number of closed regressions triaged\n  - `triage_percentage`: Percentage of closed regressions triaged\n  - `time_to_triage_hrs_avg`, `time_to_triage_hrs_max`: Triage timing metrics\n  - `time_to_close_hrs_avg`, `time_to_close_hrs_max`: Time to close metrics\n  - `time_triaged_closed_hrs_avg`, `time_triaged_closed_hrs_max`: Time from triage to close\n\n### Per-Component Data\n\n- `components`: Dictionary mapping component names to objects containing:\n  - `summary`: Component-specific statistics (same structure as overall summary)\n  - `open`: Array of open regression objects\n  - `closed`: Array of closed regression objects\n\n### Regression Object Structure\n\nEach regression object (in `components.*.open` or `components.*.closed` arrays) contains:\n\n- `id`: Unique regression identifier\n- `view`: Release view (e.g., \"4.21-main\")\n- `release`: Release version\n- `base_release`: Base release for comparison\n- `component`: Component name\n- `capability`: Test capability/area\n- `test_name`: Full test name\n- `variants`: Array of test variants where regression occurred\n- `opened`: Timestamp when regression was first detected\n- `closed`: Timestamp when regression was closed (null if still open)\n- `triages`: Array of triage objects (JIRA bugs linked to this regression)\n  - Each triage has `jira_key`, `created_at`, `url` fields\n- `last_failure`: Timestamp of most recent test failure\n- `max_failures`: Maximum number of failures detected\n\n### Example Output Structure\n\n```json\n{\n  \"summary\": {\n    \"total\": 62,\n    \"triaged\": 59,\n    \"triage_percentage\": 95.2,\n    \"filtered_suspected_infra_regressions\": 8,\n    \"time_to_triage_hrs_avg\": 68,\n    \"time_to_triage_hrs_max\": 240,\n    \"time_to_close_hrs_avg\": 168,\n    \"time_to_close_hrs_max\": 480,\n    \"open\": { \"total\": 2, \"triaged\": 1, ... },\n    \"closed\": { \"total\": 60, \"triaged\": 58, ... }\n  },\n  \"components\": {\n    \"Monitoring\": {\n      \"summary\": {\n        \"total\": 15,\n        \"triaged\": 13,\n        \"triage_percentage\": 86.7,\n        ...\n      },\n      \"open\": [\n        {\n          \"id\": 12894,\n          \"component\": \"Monitoring\",\n          \"test_name\": \"[sig-instrumentation] Prometheus ...\",\n          \"opened\": \"2025-10-15T10:30:00Z\",\n          \"closed\": null,\n          \"triages\": [],\n          ...\n        }\n      ],\n      \"closed\": [...]\n    },\n    \"etcd\": {\n      \"summary\": { \"total\": 20, \"triaged\": 19, ... },\n      \"open\": [],\n      \"closed\": [...]\n    }\n  }\n}\n```\n\n**Note**: When using `--short` flag, the `open` and `closed` arrays are excluded from component objects to reduce response size.\n\n## Examples\n\n1. **List all regressions for a release**:\n\n   ```\n   /component-health:list-regressions 4.17\n   ```\n\n   Fetches all regression data for release 4.17, including all components.\n\n2. **Filter by specific component (exact match)**:\n\n   ```\n   /component-health:list-regressions 4.21 --components Monitoring\n   ```\n\n   Returns regression data for only the Monitoring component.\n\n3. **Filter by fuzzy search**:\n\n   ```\n   /component-health:list-regressions 4.21 --components network\n   ```\n\n   Finds all components containing \"network\" (case-insensitive):\n   - Networking / ovn-kubernetes\n   - Networking / DNS\n   - Networking / router\n   - Networking / cluster-network-operator\n   - ... and returns regression data for all matches\n\n4. **Filter by multiple search strings**:\n\n   ```\n   /component-health:list-regressions 4.21 --components etcd kube-\n   ```\n\n   Finds all components containing \"etcd\" OR \"kube-\":\n   - Etcd\n   - kube-apiserver\n   - kube-controller-manager\n   - kube-scheduler\n   - kube-storage-version-migrator\n\n5. **Filter by development window** (GA'd release):\n\n   ```\n   /component-health:list-regressions 4.17 --start 2024-05-17 --end 2024-10-29\n   ```\n\n   Fetches regressions within the development window:\n   - Excludes regressions closed before 2024-05-17\n   - Excludes regressions opened after 2024-10-29\n\n6. **Filter for in-development release**:\n\n   ```\n   /component-health:list-regressions 4.21 --start 2025-09-02\n   ```\n\n   Fetches regressions for an in-development release:\n   - Excludes regressions closed before development started\n   - No end date (release still in development)\n\n7. **Combine fuzzy component search and date filters**:\n\n   ```\n   /component-health:list-regressions 4.21 --components network --start 2025-09-02\n   ```\n\n   Returns regressions for all networking components from the development window.\n\n## Arguments\n\n- `$1` (required): Release version\n  - Format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n  - Must be a valid OpenShift release number\n\n- `$2+` (optional): Filter flags\n  - `--components <search1> [search2 ...]`: Filter by component names using fuzzy search\n    - Space-separated list of component search strings\n    - Case-insensitive substring matching\n    - Each search string matches all components containing that substring\n    - If no components provided, all components are analyzed\n    - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n    - Example: \"kube-\" matches \"kube-apiserver\", \"kube-controller-manager\", etc.\n\n  - `--start <YYYY-MM-DD>`: Filter regressions by start date\n    - Excludes regressions closed before this date\n    - Typically the development_start date from release metadata\n\n  - `--end <YYYY-MM-DD>`: Filter regressions by end date\n    - Excludes regressions opened after this date\n    - Typically the GA date for released versions\n    - Omit for in-development releases\n\n  - `--short`: Exclude regression arrays from output\n    - Only include summary statistics\n    - Significantly reduces response size for large datasets\n    - Use when you only need counts and metrics, not individual regressions\n\n## Prerequisites\n\n1. **Python 3**: Required to run the data fetching script\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **Network Access**: Must be able to reach the component health API\n\n   - Ensure HTTPS requests can be made\n   - Check firewall and VPN settings if needed\n\n3. **API Configuration**: The API endpoint must be configured in the script\n   - Location: `plugins/component-health/skills/list-regressions/list_regressions.py`\n   - The script should have the correct API base URL\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- Output is JSON format for easy parsing and further processing\n- Diagnostic messages are written to stderr, data to stdout\n- The script has a 30-second timeout for HTTP requests\n- For large result sets, consider using component filters or the `--short` flag\n- Date filtering helps focus on relevant regressions within the development window\n- Infrastructure regressions (closed quickly on high-volume days) are automatically filtered\n- The returned data includes complete regression information, not summaries\n- If you need summary statistics, use `/component-health:summarize-regressions` instead\n- If you need health grading, use `/component-health:analyze` instead\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/list-regressions/SKILL.md`\n- Script: `plugins/component-health/skills/list-regressions/list_regressions.py`\n- Related Command: `/component-health:summarize-regressions` (for summary statistics)\n- Related Command: `/component-health:analyze` (for health grading and analysis)\n- Related Skill: `get-release-dates` (for fetching development window dates)"
              },
              {
                "name": "/summarize-jiras",
                "description": "Query and summarize JIRA bugs for a specific project with counts by component",
                "path": "plugins/component-health/commands/summarize-jiras.md",
                "frontmatter": {
                  "description": "Query and summarize JIRA bugs for a specific project with counts by component",
                  "argument-hint": "--project <project> [--component comp1 comp2 ...] [--status status1 status2 ...] [--include-closed] [--limit N]"
                },
                "content": "## Name\n\ncomponent-health:summarize-jiras\n\n## Synopsis\n\n```\n/component-health:summarize-jiras --project <project> [--component comp1 comp2 ...] [--status status1 status2 ...] [--include-closed] [--limit N]\n```\n\n## Description\n\nThe `component-health:summarize-jiras` command queries JIRA bugs for a specified project and generates summary statistics. It leverages the `list-jiras` command to fetch raw JIRA data and then calculates counts by status, priority, and component to help understand the bug backlog at a glance.\n\nBy default, the command includes:\n- All currently open bugs\n- Bugs closed in the last 30 days (to track recent closure activity)\n\nThis command is useful for:\n\n- Getting a quick count of open bugs in a JIRA project\n- Analyzing bug distribution by status, priority, or component\n- Tracking recent bug flow (opened vs closed in last 30 days)\n- Generating summary reports for bug backlog\n- Monitoring bug velocity and closure rates by component\n- Comparing bug counts across different components\n\n## Implementation\n\n1. **Verify Prerequisites**: Check that Python 3 is installed\n\n   - Run: `python3 --version`\n   - Verify version 3.6 or later is available\n\n2. **Verify Environment Variables**: Ensure JIRA authentication is configured\n\n   - Check that the following environment variables are set:\n     - `JIRA_URL`: Base URL for JIRA instance (e.g., \"https://issues.redhat.com\")\n     - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n\n   - Verify with:\n     ```bash\n     echo \"JIRA_URL: ${JIRA_URL}\"\n     echo \"JIRA_PERSONAL_TOKEN: ${JIRA_PERSONAL_TOKEN:+***set***}\"\n     ```\n\n   - If missing, guide the user to set them:\n     ```bash\n     export JIRA_URL=\"https://issues.redhat.com\"\n     export JIRA_PERSONAL_TOKEN=\"your-token-here\"\n     ```\n\n3. **Parse Arguments**: Extract project key and optional filters from arguments\n\n   - Project key: Required `--project` flag (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n   - Optional filters:\n     - `--component`: Space-separated list of component search strings (fuzzy match)\n     - `--status`: Space-separated list of status values\n     - `--include-closed`: Flag to include closed bugs\n     - `--limit`: Maximum number of issues to fetch per component (default: 1000, max: 1000)\n\n4. **Resolve Component Names** (if component filter provided): Use fuzzy matching to find actual component names\n\n   - Extract release from context or ask user for release version\n   - Run list_components.py to get all available components:\n     ```bash\n     python3 plugins/component-health/skills/list-components/list_components.py --release <release>\n     ```\n   - For each search string in `--component`:\n     - Find all components containing that string (case-insensitive)\n     - Combine all matches into a single list\n     - Remove duplicates\n     - If no matches found for a search string, warn the user and show available components\n\n5. **Execute Python Script**: Run the summarize_jiras.py script for each component\n\n   - Script location: `plugins/component-health/skills/summarize-jiras/summarize_jiras.py`\n   - The script internally calls `list_jiras.py` to fetch raw data\n   - **Important**: Iterate over each resolved component separately to avoid overly large queries\n   - For each component:\n     - Build command with project, single component, and other filters\n     - Execute: `python3 summarize_jiras.py --project <project> --component \"<component>\" [other args]`\n     - Capture JSON output from stdout\n   - Aggregate summary statistics from all components into a combined response\n\n6. **Parse Output**: Process the aggregated JSON response\n\n   - Extract summary statistics:\n     - `total_count`: Total matching issues in JIRA\n     - `fetched_count`: Number of issues actually fetched\n     - `summary.by_status`: Count of issues per status\n     - `summary.by_priority`: Count of issues per priority\n     - `summary.by_component`: Count of issues per component\n   - Extract per-component breakdowns:\n     - Each component has its own counts by status and priority\n     - Includes opened/closed in last 30 days per component\n\n7. **Present Results**: Display summary in a clear format\n\n   - Show which components were matched (if fuzzy search was used)\n   - Show total bug count across all components\n   - Display status breakdown (e.g., New, In Progress, Verified, etc.)\n   - Display priority breakdown (Critical, Major, Normal, Minor, etc.)\n   - Display component distribution\n   - Show per-component breakdowns with status and priority counts\n   - Highlight any truncation (if fetched_count < total_count for any component)\n   - Suggest increasing --limit if results are truncated\n\n8. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid JIRA credentials\n   - Invalid project key\n   - HTTP errors (401, 404, 500, etc.)\n   - Rate limiting (429)\n\n## Return Value\n\nThe command outputs a **JIRA Bug Summary** with the following information:\n\n### Project Overview\n\n- **Project**: JIRA project key\n- **Total Count**: Total number of matching bugs (open + recently closed)\n- **Query**: JQL query that was executed (includes 30-day closed bug filter)\n- **Fetched Count**: Number of bugs actually fetched (may be less than total if limited)\n\n### Summary Statistics\n\n**Overall Metrics**:\n- Total bugs fetched\n- Bugs opened in last 30 days\n- Bugs closed in last 30 days\n\n**By Status**: Count of bugs in each status (includes recently closed)\n\n| Status | Count |\n|--------|-------|\n| New | X |\n| In Progress | X |\n| Verified | X |\n| Closed | X |\n| ... | ... |\n\n**By Priority**: Count of bugs by priority level\n\n| Priority | Count |\n|----------|-------|\n| Critical | X |\n| Major | X |\n| Normal | X |\n| Minor | X |\n| Undefined | X |\n\n**By Component**: Count of bugs per component\n\n| Component | Count |\n|-----------|-------|\n| kube-apiserver | X |\n| Management Console | X |\n| Networking | X |\n| ... | ... |\n\n### Per-Component Breakdown\n\nFor each component:\n- **Total**: Number of bugs assigned to this component\n- **Opened (30d)**: Bugs created in the last 30 days\n- **Closed (30d)**: Bugs closed in the last 30 days\n- **By Status**: Status distribution for this component\n- **By Priority**: Priority distribution for this component\n\n### Additional Information\n\n- **Filters Applied**: Lists any component, status, or other filters used\n- **Note**: If results are truncated, suggests increasing the limit\n- **Query Scope**: By default includes open bugs and bugs closed in the last 30 days\n\n## Examples\n\n1. **Summarize all open bugs for a project**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS\n   ```\n\n   Fetches all open bugs in the OCPBUGS project (up to default limit of 1000) and displays summary statistics.\n\n2. **Filter by specific component**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS --component \"kube-apiserver\"\n   ```\n\n   Shows bug counts for only the kube-apiserver component.\n\n3. **Filter by multiple components**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS --component \"kube-apiserver\" \"etcd\" \"Networking\"\n   ```\n\n   Shows bug counts for kube-apiserver, etcd, and Networking components.\n\n4. **Include closed bugs**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS --include-closed --limit 500\n   ```\n\n   Includes both open and closed bugs, fetching up to 500 issues.\n\n5. **Filter by status**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS --status New \"In Progress\" Verified\n   ```\n\n   Shows only bugs in New, In Progress, or Verified status.\n\n6. **Combine multiple filters**:\n\n   ```\n   /component-health:summarize-jiras --project OCPBUGS --component \"Management Console\" --status New Assigned --limit 200\n   ```\n\n   Shows bugs for Management Console component that are in New or Assigned status.\n\n## Arguments\n\n- `--project <project>` (required): JIRA project key\n  - Format: Project key in uppercase (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n  - Must be a valid JIRA project you have access to\n\n- Additional optional flags:\n  - `--component <search1> [search2 ...]`: Filter by component names using fuzzy search\n    - Space-separated list of component search strings\n    - Case-insensitive substring matching\n    - Each search string matches all components containing that substring\n    - Makes separate JIRA queries for each matched component to avoid overly large results\n    - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n    - Example: \"kube-\" matches \"kube-apiserver\", \"kube-controller-manager\", etc.\n    - Note: Requires release context (inferred from recent commands or specified by user)\n\n  - `--status <status1> [status2 ...]`: Filter by status values\n    - Space-separated list of status names\n    - Examples: `New`, `\"In Progress\"`, `Verified`, `Modified`, `ON_QA`\n\n  - `--include-closed`: Include closed bugs in results\n    - By default, only open bugs are returned\n    - When specified, closed bugs are included\n\n  - `--limit <N>`: Maximum number of issues to fetch per component\n    - Default: 1000\n    - Range: 1-1000\n    - When using component filters, this limit applies to each component separately\n    - Higher values provide more accurate statistics but slower performance\n\n## Prerequisites\n\n1. **Python 3**: Required to run the data fetching and summarization scripts\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **JIRA Authentication**: Environment variables must be configured\n\n   - `JIRA_URL`: Your JIRA instance URL\n   - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n\n   How to get a JIRA token:\n   - Navigate to JIRA  Profile  Personal Access Tokens\n   - Generate a new token with appropriate permissions\n   - Export it as an environment variable\n\n3. **Network Access**: Must be able to reach your JIRA instance\n\n   - Ensure HTTPS requests can be made to JIRA_URL\n   - Check firewall and VPN settings if needed\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- Output is JSON format for easy parsing\n- Diagnostic messages are written to stderr, data to stdout\n- The script has a 30-second timeout for HTTP requests\n- For large projects, consider using component filters to reduce query size\n- Summary statistics are based on fetched issues (controlled by --limit), not total matching issues\n- If results show truncation, increase the --limit parameter for more accurate statistics\n- This command internally uses `/component-health:list-jiras` to fetch raw data\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/summarize-jiras/SKILL.md`\n- Script: `plugins/component-health/skills/summarize-jiras/summarize_jiras.py`\n- Related Command: `/component-health:list-jiras` (for raw JIRA data)\n- Related Command: `/component-health:analyze`"
              },
              {
                "name": "/summarize-regressions",
                "description": "Query and summarize regression data for OpenShift releases with counts and metrics",
                "path": "plugins/component-health/commands/summarize-regressions.md",
                "frontmatter": {
                  "description": "Query and summarize regression data for OpenShift releases with counts and metrics",
                  "argument-hint": "<release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]"
                },
                "content": "## Name\n\ncomponent-health:summarize-regressions\n\n## Synopsis\n\n```\n/component-health:summarize-regressions <release> [--components comp1 comp2 ...] [--start YYYY-MM-DD] [--end YYYY-MM-DD]\n```\n\n## Description\n\nThe `component-health:summarize-regressions` command queries regression data for a specified OpenShift release and generates summary statistics. It leverages the `list-regressions` command to fetch raw regression data and then presents counts, percentages, and timing metrics to help understand regression trends at a glance.\n\nBy default, the command analyzes:\n- All regressions within the release development window\n- Both open and closed regressions\n- Triage coverage and timing metrics\n- Per-component breakdowns\n\nThis command is useful for:\n\n- Getting a quick count of regressions in a release\n- Analyzing regression distribution by component\n- Tracking triage coverage and response times\n- Generating summary reports for regression management\n- Monitoring regression resolution speed by component\n- Comparing regression metrics across different components\n- Understanding open vs closed regression breakdown\n\n## Implementation\n\n1. **Verify Prerequisites**: Check that Python 3 is installed\n\n   - Run: `python3 --version`\n   - Verify version 3.6 or later is available\n\n2. **Parse Arguments**: Extract release version and optional filters from arguments\n\n   - Release format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n   - Optional filters:\n     - `--components`: Space-separated list of component search strings (fuzzy match)\n     - `--start`: Start date for filtering (YYYY-MM-DD)\n     - `--end`: End date for filtering (YYYY-MM-DD)\n\n3. **Resolve Component Names**: Use fuzzy matching to find actual component names\n\n   - Run list_components.py to get all available components:\n     ```bash\n     python3 plugins/component-health/skills/list-components/list_components.py --release <release>\n     ```\n   - If `--components` was provided:\n     - For each search string, find all components containing that string (case-insensitive)\n     - Combine all matches into a single list\n     - Remove duplicates\n     - If no matches found for a search string, warn the user and show available components\n   - If `--components` was NOT provided:\n     - Use all available components from the list\n\n4. **Fetch Release Dates**: Run the get_release_dates.py script to get development window dates\n\n   - Script location: `plugins/component-health/skills/get-release-dates/get_release_dates.py`\n   - Pass release as `--release` argument\n   - Extract `development_start` and `ga` dates from JSON output\n   - Convert timestamps to simple date format (YYYY-MM-DD)\n   - Use these dates if `--start` and `--end` are not explicitly provided\n\n5. **Execute Python Script**: Run the list_regressions.py script with appropriate arguments\n\n   - Script location: `plugins/component-health/skills/list-regressions/list_regressions.py`\n   - Pass release as `--release` argument\n   - Pass resolved component names as `--components` argument\n   - Pass `development_start` date as `--start` argument (if available)\n     - Always applied (for both GA'd and in-development releases)\n     - Excludes regressions closed before development started\n   - Pass `ga` date as `--end` argument (only if GA date is not null)\n     - Only applied for GA'd releases\n     - Excludes regressions opened after GA\n     - For in-development releases (null GA date), no end date filtering is applied\n   - **Always pass `--short` flag** to exclude regression arrays (only summaries)\n\n6. **Parse Output**: Process the JSON output from the script\n\n   - Script writes JSON to stdout with summary structure:\n     - `summary`: Overall statistics (total, triaged, percentages, timing)\n     - `components`: Per-component summary statistics\n   - **ALWAYS use the summary fields** for counts and metrics\n   - Regression arrays are not included (due to `--short` flag)\n\n7. **Present Results**: Display summary in a clear, readable format\n\n   - Show which components were matched (if fuzzy search was used)\n   - Show overall summary statistics\n   - Display per-component breakdowns\n   - Highlight key metrics:\n     - Triage coverage percentages\n     - Average time to triage\n     - Average time to close (for closed regressions)\n     - Open vs closed counts\n   - Present data in tables or structured format\n   - Note any date filtering applied\n\n8. **Error Handling**: Handle common error scenarios\n\n   - Network connectivity issues\n   - Invalid release format\n   - API errors (404, 500, etc.)\n   - Empty results\n   - No matches for component filter\n   - Release dates not found\n\n## Return Value\n\nThe command outputs a **Regression Summary Report** with the following information:\n\n### Overall Summary\n\n- **Release**: OpenShift release version\n- **Development Window**: Start and end dates (or \"In Development\" if no GA date)\n- **Total Regressions**: `summary.total`\n- **Filtered Infrastructure Regressions**: `summary.filtered_suspected_infra_regressions`\n- **Triaged**: `summary.triaged` regressions (`summary.triage_percentage`%)\n- **Open**: `summary.open.total` regressions (`summary.open.triage_percentage`% triaged)\n- **Closed**: `summary.closed.total` regressions (`summary.closed.triage_percentage`% triaged)\n\n### Timing Metrics\n\n**Overall Metrics**:\n- **Average Time to Triage**: `summary.time_to_triage_hrs_avg` hours\n- **Maximum Time to Triage**: `summary.time_to_triage_hrs_max` hours\n- **Average Time to Close**: `summary.time_to_close_hrs_avg` hours (closed regressions only)\n- **Maximum Time to Close**: `summary.time_to_close_hrs_max` hours (closed regressions only)\n\n**Open Regression Metrics**:\n- **Average Open Duration**: `summary.open.open_hrs_avg` hours\n- **Maximum Open Duration**: `summary.open.open_hrs_max` hours\n- **Average Time to Triage** (open): `summary.open.time_to_triage_hrs_avg` hours\n- **Maximum Time to Triage** (open): `summary.open.time_to_triage_hrs_max` hours\n\n**Closed Regression Metrics**:\n- **Average Time to Close**: `summary.closed.time_to_close_hrs_avg` hours\n- **Maximum Time to Close**: `summary.closed.time_to_close_hrs_max` hours\n- **Average Time to Triage** (closed): `summary.closed.time_to_triage_hrs_avg` hours\n- **Maximum Time to Triage** (closed): `summary.closed.time_to_triage_hrs_max` hours\n- **Average Triage-to-Close Time**: `summary.closed.time_triaged_closed_hrs_avg` hours\n- **Maximum Triage-to-Close Time**: `summary.closed.time_triaged_closed_hrs_max` hours\n\n### Per-Component Summary\n\nFor each component (from `components.*.summary`):\n\n| Component | Total | Open | Closed | Triaged | Triage % | Avg Time to Triage | Avg Time to Close |\n|-----------|-------|------|--------|---------|----------|--------------------|-------------------|\n| Monitoring | 15 | 1 | 14 | 13 | 86.7% | 68 hrs | 156 hrs |\n| etcd | 20 | 0 | 20 | 19 | 95.0% | 84 hrs | 192 hrs |\n| kube-apiserver | 27 | 1 | 26 | 27 | 100.0% | 58 hrs | 144 hrs |\n\n### Additional Information\n\n- **Filters Applied**: Lists any component or date filters used\n- **Data Scope**: Notes which regressions are included based on date filtering\n  - For GA'd releases: Regressions within development window (start to GA)\n  - For in-development releases: Regressions from development start onwards\n\n## Examples\n\n1. **Summarize all regressions for a release**:\n\n   ```\n   /component-health:summarize-regressions 4.17\n   ```\n\n   Fetches and summarizes all regressions for release 4.17, automatically applying development window date filtering.\n\n2. **Filter by specific component (exact match)**:\n\n   ```\n   /component-health:summarize-regressions 4.21 --components Monitoring\n   ```\n\n   Shows summary statistics for only the Monitoring component in release 4.21.\n\n3. **Filter by fuzzy search**:\n\n   ```\n   /component-health:summarize-regressions 4.21 --components network\n   ```\n\n   Finds all components containing \"network\" (case-insensitive) and shows summary statistics for all matches (e.g., \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.).\n\n4. **Filter by multiple search strings**:\n\n   ```\n   /component-health:summarize-regressions 4.21 --components etcd kube-\n   ```\n\n   Finds all components containing \"etcd\" OR \"kube-\" and shows combined summary statistics.\n\n5. **Specify custom date range**:\n\n   ```\n   /component-health:summarize-regressions 4.17 --start 2024-05-17 --end 2024-10-29\n   ```\n\n   Summarizes regressions within a specific date range:\n   - Excludes regressions closed before 2024-05-17\n   - Excludes regressions opened after 2024-10-29\n\n6. **In-development release**:\n\n   ```\n   /component-health:summarize-regressions 4.21\n   ```\n\n   Summarizes regressions for an in-development release:\n   - Automatically fetches development_start date\n   - No end date filtering (release not yet GA'd)\n   - Shows current state of regression management\n\n## Arguments\n\n- `$1` (required): Release version\n  - Format: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n  - Must be a valid OpenShift release number\n\n- `$2+` (optional): Filter flags\n  - `--components <search1> [search2 ...]`: Filter by component names using fuzzy search\n    - Space-separated list of component search strings\n    - Case-insensitive substring matching\n    - Each search string matches all components containing that substring\n    - If no components provided, all components are analyzed\n    - Example: \"network\" matches \"Networking / ovn-kubernetes\", \"Networking / DNS\", etc.\n    - Example: \"kube-\" matches \"kube-apiserver\", \"kube-controller-manager\", etc.\n\n  - `--start <YYYY-MM-DD>`: Filter by start date\n    - Excludes regressions closed before this date\n    - Defaults to development_start from release metadata if not provided\n\n  - `--end <YYYY-MM-DD>`: Filter by end date\n    - Excludes regressions opened after this date\n    - Defaults to GA date from release metadata if not provided and release is GA'd\n    - Omitted for in-development releases\n\n## Prerequisites\n\n1. **Python 3**: Required to run the data fetching script\n\n   - Check: `which python3`\n   - Version: 3.6 or later\n\n2. **Network Access**: Must be able to reach the component health API\n\n   - Ensure HTTPS requests can be made\n   - Check firewall and VPN settings if needed\n\n3. **API Configuration**: The API endpoint must be configured in the script\n   - Location: `plugins/component-health/skills/list-regressions/list_regressions.py`\n   - The script should have the correct API base URL\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- Output presents summary statistics in a readable format\n- Diagnostic messages are written to stderr\n- The script has a 30-second timeout for HTTP requests\n- Summary statistics are based on all matching regressions (not limited by pagination)\n- The `--short` flag is always used internally to optimize performance\n- Infrastructure regressions are automatically filtered from counts\n- Date filtering focuses analysis on the development window for accuracy\n- This command internally uses `/component-health:list-regressions` to fetch data\n- For raw regression data, use `/component-health:list-regressions` instead\n- For health grading and analysis, use `/component-health:analyze` instead\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/list-regressions/SKILL.md`\n- Script: `plugins/component-health/skills/list-regressions/list_regressions.py`\n- Related Command: `/component-health:list-regressions` (for raw regression data)\n- Related Command: `/component-health:analyze` (for health grading and analysis)\n- Related Skill: `get-release-dates` (for fetching development window dates)"
              }
            ],
            "skills": [
              {
                "name": "Analyze Regressions",
                "description": "Grade component health based on regression triage metrics for OpenShift releases",
                "path": "plugins/component-health/skills/analyze-regressions/SKILL.md",
                "frontmatter": {
                  "name": "Analyze Regressions",
                  "description": "Grade component health based on regression triage metrics for OpenShift releases"
                },
                "content": "# Analyze Regressions\n\nThis skill provides functionality to analyze and grade component health for OpenShift releases based on regression management metrics. It evaluates how well components are managing their test regressions by analyzing triage coverage, triage timeliness, and resolution speed.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Grade component health for a specific OpenShift release\n- Identify components that need help with regression handling\n- Track triage and resolution efficiency across releases\n- Generate component quality scorecards\n- Produce health reports (text or HTML) for stakeholders\n\n**Important Note**: Grading is subjective and not meant to be a critique of team performance. This is intended to help identify where help is needed and track progress as we try to improve our regression response rates.\n\n## Prerequisites\n\n1. **Python 3 Installation**\n\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **Network Access**\n\n   - The scripts require network access to reach the component health API and release dates API\n   - Ensure you can make HTTPS requests\n\n3. **Required Scripts**\n\n   - `plugins/component-health/skills/get-release-dates/get_release_dates.py`\n   - `plugins/component-health/skills/list-regressions/list_regressions.py`\n   - `plugins/component-health/skills/analyze-regressions/generate_html_report.py` (for HTML reports)\n   - `plugins/component-health/skills/analyze-regressions/report_template.html` (for HTML reports)\n\n## Implementation Steps\n\n### Step 1: Parse Arguments\n\nExtract the release version and optional component filter from the command arguments:\n\n- **Release format**: \"X.Y\" (e.g., \"4.17\", \"4.21\")\n- **Components** (optional): List of component names to filter by\n\n**Example argument parsing**:\n\n```\n/component-health:analyze-regressions 4.17\n/component-health:analyze-regressions 4.21 --components Monitoring etcd\n```\n\n### Step 2: Fetch Release Dates\n\nRun the `get_release_dates.py` script to determine the development window for the release:\n\n```bash\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 4.17\n```\n\n**Expected output** (JSON on stdout):\n\n```json\n{\n  \"release\": \"4.17\",\n  \"development_start\": \"2024-05-17T00:00:00Z\",\n  \"feature_freeze\": \"2024-08-26T00:00:00Z\",\n  \"code_freeze\": \"2024-09-30T00:00:00Z\",\n  \"ga\": \"2024-10-29T00:00:00Z\"\n}\n```\n\n**Processing steps**:\n\n1. Parse the JSON output\n2. Extract `development_start` date - convert to YYYY-MM-DD format\n3. Extract `ga` date - convert to YYYY-MM-DD format (may be null for in-development releases)\n4. Handle null dates appropriately:\n   - `development_start`: Usually always present; if null, omit `--start` parameter\n   - `ga`: Will be null for in-development releases; if null, omit `--end` parameter\n\n**Date conversion example**:\n\n```\n\"2024-05-17T00:00:00Z\"  \"2024-05-17\"\nnull  do not use this parameter\n```\n\n### Step 3: Execute List Regressions Script\n\nRun the `list_regressions.py` script with the appropriate arguments:\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17 \\\n  --start 2024-05-17 \\\n  --end 2024-10-29 \\\n  --short\n```\n\n**Parameter rules**:\n\n- `--release`: Always required (from Step 1)\n- `--components`: Optional, only if specified by user (from Step 1)\n- `--start`: Use `development_start` date from Step 2 (if not null)\n  - **Always applied** for both GA'd and in-development releases\n  - Excludes regressions closed before development started (not relevant to this release)\n- `--end`: Use `ga` date from Step 2 (only if not null)\n  - **Only applied for GA'd releases** (when GA date is not null)\n  - Excludes regressions opened after GA (post-release regressions, often not monitored/triaged)\n  - **Not applied for in-development releases** (when GA date is null)\n- `--short`: **Always include** this flag\n  - Excludes regression data arrays from response\n  - Only includes summary statistics\n  - Prevents truncation problems with large datasets\n\n**Example for GA'd release** (4.17):\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17 \\\n  --start 2024-05-17 \\\n  --end 2024-10-29 \\\n  --short\n```\n\n**Example for in-development release** (4.21 with null GA):\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --start 2025-09-02 \\\n  --short\n```\n\n**Example with component filter**:\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --components Monitoring etcd \\\n  --start 2025-09-02 \\\n  --short\n```\n\n### Step 4: Parse Output Structure\n\nThe script outputs JSON to stdout with the following structure:\n\n```json\n{\n  \"summary\": {\n    \"total\": 62,\n    \"triaged\": 59,\n    \"triage_percentage\": 95.2,\n    \"filtered_suspected_infra_regressions\": 8,\n    \"time_to_triage_hrs_avg\": 68,\n    \"time_to_triage_hrs_max\": 240,\n    \"time_to_close_hrs_avg\": 168,\n    \"time_to_close_hrs_max\": 480,\n    \"open\": {\n      \"total\": 2,\n      \"triaged\": 1,\n      \"triage_percentage\": 50.0,\n      \"time_to_triage_hrs_avg\": 48,\n      \"time_to_triage_hrs_max\": 48,\n      \"open_hrs_avg\": 120,\n      \"open_hrs_max\": 200\n    },\n    \"closed\": {\n      \"total\": 60,\n      \"triaged\": 58,\n      \"triage_percentage\": 96.7,\n      \"time_to_triage_hrs_avg\": 72,\n      \"time_to_triage_hrs_max\": 240,\n      \"time_to_close_hrs_avg\": 168,\n      \"time_to_close_hrs_max\": 480,\n      \"time_triaged_closed_hrs_avg\": 96,\n      \"time_triaged_closed_hrs_max\": 240\n    }\n  },\n  \"components\": {\n    \"ComponentName\": {\n      \"summary\": {\n        \"total\": 15,\n        \"triaged\": 13,\n        \"triage_percentage\": 86.7,\n        \"filtered_suspected_infra_regressions\": 0,\n        \"time_to_triage_hrs_avg\": 68,\n        \"time_to_triage_hrs_max\": 180,\n        \"time_to_close_hrs_avg\": 156,\n        \"time_to_close_hrs_max\": 360,\n        \"open\": {\n          \"total\": 1,\n          \"triaged\": 0,\n          \"triage_percentage\": 0.0,\n          \"time_to_triage_hrs_avg\": null,\n          \"time_to_triage_hrs_max\": null,\n          \"open_hrs_avg\": 72,\n          \"open_hrs_max\": 72\n        },\n        \"closed\": {\n          \"total\": 14,\n          \"triaged\": 13,\n          \"triage_percentage\": 92.9,\n          \"time_to_triage_hrs_avg\": 68,\n          \"time_to_triage_hrs_max\": 180,\n          \"time_to_close_hrs_avg\": 156,\n          \"time_to_close_hrs_max\": 360,\n          \"time_triaged_closed_hrs_avg\": 88,\n          \"time_triaged_closed_hrs_max\": 180\n        }\n      }\n    }\n  }\n}\n```\n\n**CRITICAL - Use Summary Counts**:\n\n- **ALWAYS use `summary.total`, `summary.open.total`, `summary.closed.total`** for counts\n- **ALWAYS use `components.*.summary.*`** for per-component counts\n- Do NOT attempt to count regression arrays (they are excluded with `--short` flag)\n- This ensures accuracy even with large datasets\n\n**Key Metrics to Extract**:\n\nFrom `summary` object:\n\n- `summary.total` - Total regressions\n- `summary.triaged` - Total triaged regressions\n- `summary.triage_percentage` - **KEY HEALTH METRIC**: Percentage triaged\n- `summary.filtered_suspected_infra_regressions` - Count of filtered infrastructure regressions\n- `summary.time_to_triage_hrs_avg` - **KEY HEALTH METRIC**: Average hours to triage\n- `summary.time_to_triage_hrs_max` - Maximum hours to triage\n- `summary.time_to_close_hrs_avg` - **KEY HEALTH METRIC**: Average hours to close\n- `summary.time_to_close_hrs_max` - Maximum hours to close\n- `summary.open.total` - Open regressions count\n- `summary.open.triaged` - Open triaged count\n- `summary.open.triage_percentage` - Open triage percentage\n- `summary.closed.total` - Closed regressions count\n- `summary.closed.triaged` - Closed triaged count\n- `summary.closed.triage_percentage` - Closed triage percentage\n\nFrom `components` object:\n\n- Same fields as summary, but per-component\n- Use `components.*.summary.*` for all per-component statistics\n\n### Step 5: Calculate Health Grades\n\n**IMPORTANT - Closed Regression Triage**:\n\n- **DO NOT recommend retroactively triaging closed regressions** - the tooling does not support this\n- When identifying untriaged regressions that need attention, **only consider open regressions**: `summary.open.total - summary.open.triaged`\n- Closed regression triage percentages are provided for historical analysis only, not as actionable items\n\n#### Overall Health Grade\n\nCalculate grades based on three key metrics:\n\n**1. Triage Coverage** (`summary.triage_percentage`):\n\n- 90-100%: Excellent \n- 70-89%: Good \n- 50-69%: Needs Improvement \n- <50%: Poor \n\n**2. Triage Timeliness** (`summary.time_to_triage_hrs_avg`):\n\n- <24 hours: Excellent \n- 24-72 hours: Good \n- 72-168 hours (1 week): Needs Improvement \n- > 168 hours: Poor \n\n**3. Resolution Speed** (`summary.time_to_close_hrs_avg`):\n\n- <168 hours (1 week): Excellent \n- 168-336 hours (1-2 weeks): Good \n- 336-720 hours (2-4 weeks): Needs Improvement \n- > 720 hours (4+ weeks): Poor \n\n#### Per-Component Health Grades\n\nFor each component in `components`:\n\n1. Calculate the same three grades using `components.*.summary.*` fields\n2. Rank components from best to worst health\n3. Highlight components needing attention:\n   - Low triage coverage (<50%)\n   - Slow triage response (>72 hours average)\n   - Slow resolution time (>336 hours / 2 weeks average)\n   - High open regression counts\n   - High overall regression counts\n\n### Step 6: Display Text Report\n\nPresent a well-formatted text report with:\n\n#### Overall Health Grade Section\n\nDisplay overall statistics from `summary`:\n\n```\n=== Overall Health Grade for Release 4.17 ===\nDevelopment Window: 2024-05-17 to 2024-10-29 (GA'd release)\n\nTotal Regressions: 62\nFiltered Infrastructure Regressions: 8\nTriaged: 59 (95.2%)\nOpen: 2 (50.0% triaged)\nClosed: 60 (96.7% triaged)\n\nTriage Coverage:  Excellent (95.2%)\nTriage Timeliness:  Good (68 hours average, 240 hours max)\nResolution Speed:  Excellent (168 hours average, 480 hours max)\n```\n\n**Important**: If the GA date is null (in-development release), note:\n\n```\nDevelopment Window: 2025-09-02 onwards (In Development)\n```\n\n#### Per-Component Health Scorecard\n\nDisplay ranked table from `components.*.summary`:\n\n```\n=== Component Health Scorecard ===\n\n| Component       | Triage Coverage | Triage Time | Resolution Time | Open | Grade |\n|-----------------|-----------------|-------------|-----------------|------|-------|\n| kube-apiserver  | 100.0%          | 58 hrs      | 144 hrs         | 1    |     |\n| etcd            | 95.0%           | 84 hrs      | 192 hrs         | 0    |     |\n| Monitoring      | 86.7%           | 68 hrs      | 156 hrs         | 1    |     |\n```\n\n#### Components Needing Attention\n\nHighlight specific components with issues:\n\n```\n=== Components Needing Attention ===\n\nMonitoring:\n  - 1 open untriaged regression (needs triage)\n  - Triage coverage: 86.7% (below 90%)\n\nExample-Component:\n  - 5 open untriaged regressions (needs triage)\n  - Slow triage response: 120 hours average\n  - High open count: 5 open regressions\n```\n\n**CRITICAL**: When listing untriaged regressions that need action:\n\n- **Only list OPEN untriaged regressions** - these are actionable\n- **Do NOT recommend triaging closed regressions** - the tooling does not support retroactive triage\n- Calculate actionable untriaged count as: `components.*.summary.open.total - components.*.summary.open.triaged`\n\n### Step 7: Offer HTML Report Generation\n\nAfter displaying the text report, ask the user if they want an interactive HTML report:\n\n```\nWould you like me to generate an interactive HTML report? (yes/no)\n```\n\nIf the user responds affirmatively:\n\n#### Step 7a: Prepare Data for HTML Report\n\nThe HTML report requires data in a specific structure. Transform the JSON data:\n\n```python\n# Prepare component data for HTML template\ncomponent_data = []\nfor component_name, component_obj in components.items():\n    summary = component_obj['summary']\n    component_data.append({\n        'name': component_name,\n        'total': summary['total'],\n        'open': summary['open']['total'],\n        'closed': summary['closed']['total'],\n        'triaged': summary['triaged'],\n        'triage_percentage': summary['triage_percentage'],\n        'time_to_triage_hrs_avg': summary.get('time_to_triage_hrs_avg'),\n        'time_to_close_hrs_avg': summary.get('time_to_close_hrs_avg'),\n        'health_grade': calculate_health_grade(summary)  # Calculate combined grade\n    })\n```\n\n#### Step 7b: Generate HTML Report\n\nUse the `generate_html_report.py` script (or inline Python code):\n\n```bash\npython3 plugins/component-health/skills/analyze-regressions/generate_html_report.py \\\n  --release 4.17 \\\n  --data regression_data.json \\\n  --output .work/component-health-4.17/report.html\n```\n\nOr use inline Python with the template:\n\n```python\nimport json\nfrom datetime import datetime\n\n# Load template\nwith open('plugins/component-health/skills/analyze-regressions/report_template.html', 'r') as f:\n    template = f.read()\n\n# Replace placeholders\ntemplate = template.replace('{{RELEASE}}', '4.17')\ntemplate = template.replace('{{GENERATED_DATE}}', datetime.now().isoformat())\ntemplate = template.replace('{{SUMMARY_DATA}}', json.dumps(summary))\ntemplate = template.replace('{{COMPONENT_DATA}}', json.dumps(component_data))\n\n# Write output\noutput_path = '.work/component-health-4.17/report.html'\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nwith open(output_path, 'w') as f:\n    f.write(template)\n```\n\n#### Step 7c: Open the Report\n\nOpen the HTML report in the user's default browser:\n\n**macOS**:\n\n```bash\nopen .work/component-health-4.17/report.html\n```\n\n**Linux**:\n\n```bash\nxdg-open .work/component-health-4.17/report.html\n```\n\n**Windows**:\n\n```bash\nstart .work/component-health-4.17/report.html\n```\n\nDisplay the file path to the user:\n\n```\nHTML report generated: .work/component-health-4.17/report.html\nOpening in your default browser...\n```\n\n## Error Handling\n\n### Common Errors\n\n1. **Network Errors**\n\n   - **Symptom**: `URLError` or connection timeout\n   - **Solution**: Check network connectivity and firewall rules\n   - **Retry**: Both scripts have 30-second timeouts\n\n2. **Invalid Release Format**\n\n   - **Symptom**: Empty results or error response\n   - **Solution**: Verify the release format (e.g., \"4.17\", not \"v4.17\" or \"4.17.0\")\n\n3. **Release Dates Not Found**\n\n   - **Symptom**: `get_release_dates.py` returns error\n   - **Solution**: Verify the release exists in the system; may be too old or not yet created\n   - **Fallback**: Proceed without date filtering (omit `--start` and `--end` parameters)\n\n4. **No Regressions Found**\n\n   - **Symptom**: Empty components object\n   - **Solution**: Verify the release has regression data; may be too early in development\n   - **Action**: Inform user that no regressions exist yet for this release\n\n5. **Component Filter No Matches**\n\n   - **Symptom**: Empty components object after filtering\n   - **Solution**: Check component name spelling; component names are case-insensitive\n   - **Action**: List available components from unfiltered query\n\n6. **HTML Template Not Found**\n   - **Symptom**: FileNotFoundError when generating HTML report\n   - **Solution**: Verify template exists at `plugins/component-health/skills/analyze-regressions/report_template.html`\n   - **Fallback**: Offer text report only\n\n### Debugging\n\nEnable verbose output by examining stderr:\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17 \\\n  --short 2>&1 | tee debug.log\n```\n\nDiagnostic messages include:\n\n- URL being queried\n- Number of regressions fetched\n- Number after filtering\n- Number of suspected infrastructure regressions filtered\n\n## Output Format\n\n### Text Report Structure\n\nThe text report should include:\n\n1. **Header**\n\n   - Release version\n   - Development window dates (start and end/GA)\n   - Release status (GA'd or In Development)\n\n2. **Overall Health Grade**\n\n   - Total regressions\n   - Filtered infrastructure regressions count\n   - Open/closed breakdown\n   - Triage coverage score with grade\n   - Triage timeliness score with grade\n   - Resolution speed score with grade\n\n3. **Component Health Scorecard**\n\n   - Ranked table of all components\n   - Key metrics per component\n   - Health grade per component\n\n4. **Components Needing Attention**\n\n   - List of components with specific issues\n   - Actionable recommendations (only for open untriaged regressions)\n   - Context for each issue\n\n5. **Footer**\n   - Link to Sippy dashboard (if applicable)\n   - Timestamp of report generation\n\n### HTML Report Features\n\nThe HTML report should include:\n\n- **Interactive table** with sorting and filtering\n- **Visual indicators** for health grades (colors, icons)\n- **Charts/graphs** showing:\n  - Triage coverage by component\n  - Time to triage distribution\n  - Open vs closed breakdown\n- **Detailed metrics** on hover or click\n- **Export functionality** (CSV, PDF)\n- **Responsive design** for mobile viewing\n\n## Examples\n\n### Example 1: Grade Overall Release Health\n\n```\n/component-health:analyze-regressions 4.17\n```\n\n**Execution flow**:\n\n1. Fetch release dates for 4.17\n2. Run list_regressions.py with --start and --end (GA'd release)\n3. Display overall health grade\n4. Display per-component scorecard\n5. Highlight components needing attention\n6. Offer HTML report generation\n\n### Example 2: Grade Specific Components\n\n```\n/component-health:analyze-regressions 4.21 --components Monitoring etcd\n```\n\n**Execution flow**:\n\n1. Fetch release dates for 4.21 (may have null GA)\n2. Run list_regressions.py with --components and --start only (in-development)\n3. Display health grades for Monitoring and etcd only\n4. Compare the two components\n5. Identify which needs more attention\n\n### Example 3: Grade Single Component\n\n```\n/component-health:analyze-regressions 4.21 --components \"kube-apiserver\"\n```\n\n**Execution flow**:\n\n1. Fetch release dates for 4.21\n2. Run list_regressions.py with single component filter\n3. Display detailed health metrics for kube-apiserver\n4. Show open vs closed breakdown\n5. List count of open untriaged regressions (if any)\n\n## Health Grade Calculation Details\n\n### Combined Health Grade\n\nTo calculate an overall health grade for a component, consider all three metrics:\n\n```python\ndef calculate_health_grade(summary):\n    \"\"\"Calculate combined health grade based on three key metrics.\"\"\"\n    triage_coverage = summary['triage_percentage']\n    triage_time = summary.get('time_to_triage_hrs_avg')\n    resolution_time = summary.get('time_to_close_hrs_avg')\n\n    # Score each metric (0-3)\n    coverage_score = (\n        3 if triage_coverage >= 90 else\n        2 if triage_coverage >= 70 else\n        1 if triage_coverage >= 50 else\n        0\n    )\n\n    time_score = 3  # Default to excellent if no data\n    if triage_time is not None:\n        time_score = (\n            3 if triage_time < 24 else\n            2 if triage_time < 72 else\n            1 if triage_time < 168 else\n            0\n        )\n\n    resolution_score = 3  # Default to excellent if no data\n    if resolution_time is not None:\n        resolution_score = (\n            3 if resolution_time < 168 else\n            2 if resolution_time < 336 else\n            1 if resolution_time < 720 else\n            0\n        )\n\n    # Average the scores\n    avg_score = (coverage_score + time_score + resolution_score) / 3\n\n    # Return grade\n    if avg_score >= 2.5:\n        return \"Excellent \"\n    elif avg_score >= 1.5:\n        return \"Good \"\n    elif avg_score >= 0.5:\n        return \"Needs Improvement \"\n    else:\n        return \"Poor \"\n```\n\n### Prioritizing Components Needing Attention\n\nRank components by priority based on:\n\n1. **High open untriaged count** (most urgent)\n\n   - Calculate: `summary.open.total - summary.open.triaged`\n   - Threshold: >3 open untriaged regressions\n\n2. **Low triage coverage** (second priority)\n\n   - Use: `summary.triage_percentage`\n   - Threshold: <50%\n\n3. **Slow triage response** (third priority)\n\n   - Use: `summary.time_to_triage_hrs_avg`\n   - Threshold: >72 hours\n\n4. **High total regression count** (fourth priority)\n   - Use: `summary.total`\n   - Threshold: Component-relative (top quartile)\n\n## Advanced Features\n\n### Trend Analysis (Future Enhancement)\n\nCompare metrics across releases:\n\n```\n/component-health:analyze-regressions 4.17 --compare 4.16\n```\n\n### Export to CSV\n\nGenerate CSV report for spreadsheet analysis:\n\n```\n/component-health:analyze-regressions 4.17 --export-csv\n```\n\n### Custom Thresholds\n\nAllow users to customize health grade thresholds:\n\n```\n/component-health:analyze-regressions 4.17 --triage-threshold 80\n```\n\n## Integration with Other Commands\n\nThis skill can be used by:\n\n- `/component-health:analyze-regressions` command (primary)\n- Quality metrics dashboards\n- Release readiness reports\n- Team performance tracking tools\n\n## Related Skills\n\n- `get-release-dates` - Fetches release development window dates\n- `list-regressions` - Fetches raw regression data\n- `prow-job:analyze-test-failure` - Analyzes individual test failures\n\n## Notes\n\n- All scripts use Python's standard library only (no external dependencies)\n- Output is cached in `.work/` directory for performance\n- Regression data is fetched in real-time from the API\n- HTML reports are standalone (no external dependencies, embedded CSS/JS)\n- The `--short` flag is critical to prevent output truncation with large datasets\n- Health grades are subjective and intended as guidance, not criticism\n- Infrastructure regressions (closed within 96 hours on high-volume days) are automatically filtered\n- Retroactive triage of closed regressions is not supported by the tooling\n\n## Troubleshooting\n\n### Issue: Report Shows 0 Regressions\n\n**Possible causes**:\n\n1. Release is too early in development\n2. Date filtering excluded all regressions\n3. Component filter didn't match any components\n\n**Solutions**:\n\n1. Check release dates with `get_release_dates.py`\n2. Try without date filtering\n3. List available components without filter first\n\n### Issue: Triage Percentages Seem Low\n\n**Context**:\n\n- Many teams are still ramping up regression triage practices\n- Low percentages indicate opportunity for improvement, not failure\n- Focus on the trend over time rather than absolute numbers\n\n**Actions**:\n\n- Identify specific untriaged open regressions that need attention\n- Prioritize by regression severity and frequency\n- Track improvement over subsequent releases\n\n### Issue: HTML Report Not Opening\n\n**Possible causes**:\n\n1. Browser security restrictions on local files\n2. Incorrect file path\n3. Missing file permissions\n\n**Solutions**:\n\n1. Manually open the file from file explorer\n2. Verify the file was created at the expected path\n3. Check file permissions: `ls -la .work/component-health-*/report.html`\n\n## Summary\n\nThis skill provides comprehensive component health analysis by:\n\n1. Fetching release development window dates\n2. Retrieving regression data filtered to the development window\n3. Calculating health grades based on triage metrics\n4. Generating actionable reports (text and HTML)\n5. Identifying components that need help\n\nThe key focus is on **actionable insights** - particularly identifying open untriaged regressions that need immediate attention, while avoiding recommendations for closed regressions which cannot be retroactively triaged."
              },
              {
                "name": "Get Release Dates",
                "description": "Fetch OpenShift release dates and metadata from Sippy API",
                "path": "plugins/component-health/skills/get-release-dates/SKILL.md",
                "frontmatter": {
                  "name": "Get Release Dates",
                  "description": "Fetch OpenShift release dates and metadata from Sippy API"
                },
                "content": "# Get Release Dates\n\nThis skill provides functionality to fetch OpenShift release information including GA dates and development start dates from the Sippy API.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Get GA (General Availability) date for a specific OpenShift release\n- Find when development started for a release\n- Identify the previous release in the sequence\n- Validate if a release exists in Sippy\n- Determine if a release is in development or has GA'd\n\n## Prerequisites\n\n1. **Python 3 Installation**\n\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **Network Access**\n\n   - The script requires network access to reach the Sippy API\n   - Ensure you can make HTTPS requests to `sippy.dptools.openshift.org`\n\n## Implementation Steps\n\n### Step 1: Verify Prerequisites\n\nFirst, ensure Python 3 is available:\n\n```bash\npython3 --version\n```\n\nIf Python 3 is not installed, guide the user through installation for their platform.\n\n### Step 2: Locate the Script\n\nThe script is located at:\n\n```\nplugins/component-health/skills/get-release-dates/get_release_dates.py\n```\n\n### Step 3: Run the Script\n\nExecute the script with the release parameter:\n\n```bash\n# Get dates for release 4.21\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 4.21\n\n# Get dates for release 4.20\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 4.20\n```\n\n### Step 4: Process the Output\n\nThe script outputs JSON data with the following structure:\n\n```json\n{\n  \"release\": \"4.21\",\n  \"found\": true,\n  \"ga\": \"2026-02-17T00:00:00Z\",\n  \"development_start\": \"2025-09-02T00:00:00Z\",\n  \"previous_release\": \"4.20\"\n}\n```\n\n**Field Descriptions**:\n\n- `release`: The release identifier that was queried\n- `found`: Boolean indicating if the release exists in Sippy\n- `ga`: GA (General Availability) date. **If null, the release is still in development.**\n- `development_start`: When development started for this release\n- `previous_release`: The previous release in the sequence (empty string if none)\n\n**If Release Not Found**:\n\n```json\n{\n  \"release\": \"99.99\",\n  \"found\": false\n}\n```\n\n**Release Status - Development vs GA'd**:\n\n- **In Development**: If `ga` is `null`, the release is still under active development\n  ```json\n  {\n    \"release\": \"4.21\",\n    \"found\": true,\n    \"development_start\": \"2025-09-02T00:00:00Z\",\n    \"previous_release\": \"4.20\"\n  }\n  ```\n- **GA'd (Released)**: If `ga` has a timestamp, the release has reached General Availability\n  ```json\n  {\n    \"release\": \"4.17\",\n    \"found\": true,\n    \"ga\": \"2024-10-01T00:00:00Z\",\n    \"development_start\": \"2024-05-17T00:00:00Z\",\n    \"previous_release\": \"4.16\"\n  }\n  ```\n\n### Step 5: Use the Information\n\nBased on the release dates:\n\n1. **Determine release status**: Check if release is in development or GA'd\n   - If `ga` is `null`: Release is still in development\n   - If `ga` has a timestamp: Release has reached General Availability\n2. **Determine release timeline**: Use `development_start` and `ga` dates\n   - Calculate time in development: `ga` - `development_start`\n   - For in-development releases: Calculate time since `development_start`\n3. **Find related releases**: Use `previous_release` to navigate the release sequence\n4. **Validate release**: Check `found` field before using the release in other operations\n\n## Error Handling\n\nThe script handles several error scenarios:\n\n1. **Network Errors**: If unable to reach Sippy API\n\n   ```\n   Error: URL Error: [reason]\n   ```\n\n2. **HTTP Errors**: If API returns an error status\n\n   ```\n   Error: HTTP Error 404: Not Found\n   ```\n\n3. **Invalid Release**: Script returns exit code 1 with `found: false` in output\n\n4. **Parsing Errors**: If API response is malformed\n   ```\n   Error: Failed to fetch release dates: [details]\n   ```\n\n## Output Format\n\nThe script outputs JSON to stdout with:\n\n- **Success**: Exit code 0, JSON with `found: true`\n- **Release Not Found**: Exit code 1, JSON with `found: false`\n- **Error**: Exit code 1, error message to stderr\n\n## API Details\n\nThe script queries the Sippy releases API:\n\n- **URL**: https://sippy.dptools.openshift.org/api/releases\n- **Method**: GET\n- **Response**: JSON containing all releases and their metadata\n\nThe full API response includes:\n\n- `releases`: Array of all available release identifiers\n- `ga_dates`: Simple mapping of release to GA date\n- `dates`: Detailed mapping with GA and development_start dates\n- `release_attrs`: Extended attributes including previous release\n\n## Examples\n\n### Example 1: Get Current Development Release\n\n```bash\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 4.21\n```\n\nOutput:\n\n```json\n{\n  \"release\": \"4.21\",\n  \"found\": true,\n  \"development_start\": \"2025-09-02T00:00:00Z\",\n  \"previous_release\": \"4.20\"\n}\n```\n\n### Example 2: Get GA'd Release\n\n```bash\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 4.17\n```\n\nOutput:\n\n```json\n{\n  \"release\": \"4.17\",\n  \"found\": true,\n  \"ga\": \"2024-10-01T00:00:00Z\",\n  \"development_start\": \"2024-05-17T00:00:00Z\",\n  \"previous_release\": \"4.16\"\n}\n```\n\n### Example 3: Query Non-Existent Release\n\n```bash\npython3 plugins/component-health/skills/get-release-dates/get_release_dates.py \\\n  --release 99.99\n```\n\nOutput:\n\n```json\n{\n  \"release\": \"99.99\",\n  \"found\": false\n}\n```\n\nExit code: 1\n\n## Integration with Other Commands\n\nThis skill can be used in conjunction with other component-health skills:\n\n1. **Before analyzing regressions**: Verify the release exists\n2. **Timeline context**: Understand how long a release has been in development\n3. **Release status**: Determine if a release is in development or has GA'd\n4. **Release navigation**: Find previous/next releases in the sequence\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- API responses are cached by Sippy, so repeated calls are fast\n- Release identifiers are case-sensitive (use \"4.21\" not \"4.21.0\")\n- OKD releases are suffixed with \"-okd\" (e.g., \"4.21-okd\")\n- ARO releases have special identifiers (e.g., \"aro-production\")\n- \"Presubmits\" is a special release for pull request data\n\n## See Also\n\n- Skill Documentation: `plugins/component-health/skills/list-regressions/SKILL.md`\n- Sippy API: https://sippy.dptools.openshift.org/api/releases\n- Component Health Plugin: `plugins/component-health/README.md`"
              },
              {
                "name": "List Components",
                "description": "Fetch component names from Sippy component readiness API",
                "path": "plugins/component-health/skills/list-components/SKILL.md",
                "frontmatter": {
                  "name": "List Components",
                  "description": "Fetch component names from Sippy component readiness API"
                },
                "content": "# List Components\n\nThis skill provides functionality to fetch a list of all component names tracked in the Sippy component readiness system for a specific OpenShift release.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Get a complete list of components for a specific release\n- Validate component names before querying regression or bug data\n- Discover available components for analysis\n- Generate component lists for reports or documentation\n- Understand which teams/components are tracked in Sippy\n- Provide autocomplete suggestions for component names\n\n## Prerequisites\n\n1. **Python 3 Installation**\n\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **Network Access**\n\n   - The script requires network access to reach the Sippy API\n   - Ensure you can make HTTPS requests to `sippy.dptools.openshift.org`\n\n## Implementation Steps\n\n### Step 1: Verify Prerequisites\n\nFirst, ensure Python 3 is available:\n\n```bash\npython3 --version\n```\n\nIf Python 3 is not installed, guide the user through installation for their platform.\n\n### Step 2: Locate the Script\n\nThe script is located at:\n\n```\nplugins/component-health/skills/list-components/list_components.py\n```\n\n### Step 3: Run the Script\n\nExecute the script with the release parameter:\n\n```bash\n# Get components for release 4.21\npython3 plugins/component-health/skills/list-components/list_components.py \\\n  --release 4.21\n\n# Get components for release 4.20\npython3 plugins/component-health/skills/list-components/list_components.py \\\n  --release 4.20\n```\n\n**Important**: The script automatically appends \"-main\" to the release version to construct the view parameter (e.g., \"4.21\" becomes \"4.21-main\").\n\n### Step 4: Process the Output\n\nThe script outputs JSON data with the following structure:\n\n```json\n{\n  \"release\": \"4.21\",\n  \"view\": \"4.21-main\",\n  \"component_count\": 42,\n  \"components\": [\n    \"API\",\n    \"Build\",\n    \"Cloud Compute\",\n    \"Cluster Version Operator\",\n    \"Etcd\",\n    \"Image Registry\",\n    \"Installer\",\n    \"Kubernetes\",\n    \"Management Console\",\n    \"Monitoring\",\n    \"Networking\",\n    \"OLM\",\n    \"Storage\",\n    \"etcd\",\n    \"kube-apiserver\",\n    \"...\"\n  ]\n}\n```\n\n**Field Descriptions**:\n\n- `release`: The release identifier that was queried\n- `view`: The constructed view parameter used in the API call (release + \"-main\")\n- `component_count`: Total number of unique components found\n- `components`: Alphabetically sorted array of unique component names\n\n**If View Not Found**:\n\nIf the release view doesn't exist, the script will return an HTTP 404 error:\n\n```\nHTTP Error 404: Not Found\nView '4.99-main' not found. Please check the release version.\n```\n\n### Step 5: Use the Component List\n\nBased on the component list, you can:\n\n1. **Validate component names**: Check if a component exists before querying data\n2. **Generate documentation**: Create component lists for reports\n3. **Filter queries**: Use component names to filter regression or bug queries\n4. **Autocomplete**: Provide suggestions when users type component names\n5. **Discover teams**: Understand which components/teams are tracked\n\n## Error Handling\n\nThe script handles several error scenarios:\n\n1. **Network Errors**: If unable to reach Sippy API\n\n   ```\n   Error: URL Error: [reason]\n   ```\n\n2. **HTTP Errors**: If API returns an error status\n\n   ```\n   Error: HTTP Error 404: Not Found\n   View '4.99-main' not found. Please check the release version.\n   ```\n\n3. **Invalid Release**: Script returns exit code 1 with error message\n\n4. **Parsing Errors**: If API response is malformed\n   ```\n   Error: Failed to fetch components: [details]\n   ```\n\n## Output Format\n\nThe script outputs JSON to stdout with:\n\n- **Success**: Exit code 0, JSON with component list\n- **Error**: Exit code 1, error message to stderr\n\nDiagnostic messages (like \"Fetching components from...\") are written to stderr, so they don't interfere with JSON parsing.\n\n## API Details\n\nThe script queries the Sippy component readiness API:\n\n- **URL**: `https://sippy.dptools.openshift.org/api/component_readiness?view={release}-main`\n- **Method**: GET\n- **Response**: JSON containing component readiness data with rows\n\nThe API response structure includes:\n\n```json\n{\n  \"rows\": [\n    {\n      \"component\": \"Networking\",\n      ...\n    },\n    {\n      \"component\": \"Monitoring\",\n      ...\n    }\n  ],\n  ...\n}\n```\n\nThe script:\n\n1. Extracts the `component` field from each row\n2. Filters out empty/null component names\n3. Returns unique components, sorted alphabetically\n\n## Examples\n\n### Example 1: Get Components for 4.21\n\n```bash\npython3 plugins/component-health/skills/list-components/list_components.py \\\n  --release 4.21\n```\n\nOutput:\n\n```json\n{\n  \"release\": \"4.21\",\n  \"view\": \"4.21-main\",\n  \"component_count\": 42,\n  \"components\": [\"API\", \"Build\", \"Etcd\", \"...\"]\n}\n```\n\n### Example 2: Query Non-Existent Release\n\n```bash\npython3 plugins/component-health/skills/list-components/list_components.py \\\n  --release 99.99\n```\n\nOutput (to stderr):\n\n```\nFetching components from: https://sippy.dptools.openshift.org/api/component_readiness?view=99.99-main\nHTTP Error 404: Not Found\nView '99.99-main' not found. Please check the release version.\nFailed to fetch components: HTTP Error 404: Not Found\n```\n\nExit code: 1\n\n## Integration with Other Commands\n\nThis skill can be used in conjunction with other component-health skills:\n\n1. **Before analyzing components**: Validate component names exist\n2. **Component discovery**: Find available components for a release\n3. **Autocomplete**: Provide component name suggestions to users\n4. **Batch operations**: Iterate over all components for comprehensive analysis\n\n**Example Integration**:\n\n```bash\n# Get all components for 4.21\nCOMPONENTS=$(python3 plugins/component-health/skills/list-components/list_components.py \\\n  --release 4.21 | jq -r '.components[]')\n\n# Analyze each component\nfor component in $COMPONENTS; do\n  echo \"Analyzing $component...\"\n  # Use component in other commands\ndone\n```\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- The script automatically appends \"-main\" to the release version\n- Component names are case-sensitive\n- Component names are returned in alphabetical order\n- Duplicate component names are automatically removed\n- Empty or null component names are filtered out\n- The script has a 30-second timeout for HTTP requests\n- Diagnostic messages go to stderr, JSON output goes to stdout\n\n## See Also\n\n- Related Skill: `plugins/component-health/skills/list-regressions/SKILL.md`\n- Related Skill: `plugins/component-health/skills/get-release-dates/SKILL.md`\n- Related Command: `/component-health:list-regressions` (for regression data)\n- Related Command: `/component-health:analyze` (for health grading)\n- Sippy API: https://sippy.dptools.openshift.org/api/component_readiness\n- Component Health Plugin: `plugins/component-health/README.md`"
              },
              {
                "name": "List JIRAs",
                "description": "Query and return raw JIRA bug data for a specific project",
                "path": "plugins/component-health/skills/list-jiras/SKILL.md",
                "frontmatter": {
                  "name": "List JIRAs",
                  "description": "Query and return raw JIRA bug data for a specific project"
                },
                "content": "# List JIRAs\n\nThis skill provides functionality to query JIRA bugs for a specified project and return raw issue data. It uses the JIRA REST API to fetch complete bug information with all fields and metadata, without performing any summarization.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Fetch raw JIRA issue data for further processing\n- Access complete issue details including all fields\n- Build custom analysis workflows\n- Provide data to other commands (like `summarize-jiras`)\n- Export JIRA data for offline analysis\n\n## Prerequisites\n\n1. **Python 3 Installation**\n\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **JIRA Authentication**\n\n   - Requires environment variables to be set:\n     - `JIRA_URL`: Base URL for JIRA instance (e.g., \"https://issues.redhat.com\")\n     - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n   - How to get a JIRA token:\n     - Navigate to JIRA  Profile  Personal Access Tokens\n     - Generate a new token with appropriate permissions\n     - Export it as an environment variable\n\n3. **Network Access**\n   - The script requires network access to reach your JIRA instance\n   - Ensure you can make HTTPS requests to the JIRA URL\n\n## Implementation Steps\n\n### Step 1: Verify Prerequisites\n\nFirst, ensure Python 3 is available:\n\n```bash\npython3 --version\n```\n\nIf Python 3 is not installed, guide the user through installation for their platform.\n\n### Step 2: Verify Environment Variables\n\nCheck that required environment variables are set:\n\n```bash\n# Verify JIRA credentials are configured\necho \"JIRA_URL: ${JIRA_URL}\"\necho \"JIRA_PERSONAL_TOKEN: ${JIRA_PERSONAL_TOKEN:+***set***}\"\n```\n\nIf any are missing, guide the user to set them:\n\n```bash\nexport JIRA_URL=\"https://issues.redhat.com\"\nexport JIRA_PERSONAL_TOKEN=\"your-token-here\"\n```\n\n### Step 3: Locate the Script\n\nThe script is located at:\n\n```\nplugins/component-health/skills/list-jiras/list_jiras.py\n```\n\n### Step 4: Run the Script\n\nExecute the script with appropriate arguments:\n\n```bash\n# Basic usage - all open bugs in a project\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS\n\n# Filter by component\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\"\n\n# Filter by multiple components\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\" \"Management Console\"\n\n# Include closed bugs\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --include-closed\n\n# Filter by status\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --status New \"In Progress\"\n\n# Set maximum results limit (default 100)\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --limit 500\n```\n\n### Step 5: Process the Output\n\nThe script outputs JSON data with the following structure:\n\n```json\n{\n  \"project\": \"OCPBUGS\",\n  \"total_count\": 1500,\n  \"fetched_count\": 100,\n  \"query\": \"project = OCPBUGS AND (status != Closed OR (status = Closed AND resolved >= \\\"2025-10-11\\\"))\",\n  \"filters\": {\n    \"components\": null,\n    \"statuses\": null,\n    \"include_closed\": false,\n    \"limit\": 100\n  },\n  \"issues\": [\n    {\n      \"key\": \"OCPBUGS-12345\",\n      \"fields\": {\n        \"summary\": \"Bug title here\",\n        \"status\": {\n          \"name\": \"New\",\n          \"id\": \"1\"\n        },\n        \"priority\": {\n          \"name\": \"Major\",\n          \"id\": \"3\"\n        },\n        \"components\": [\n          {\"name\": \"kube-apiserver\", \"id\": \"12345\"}\n        ],\n        \"assignee\": {\n          \"displayName\": \"John Doe\",\n          \"emailAddress\": \"jdoe@example.com\"\n        },\n        \"created\": \"2025-11-01T10:30:00.000+0000\",\n        \"updated\": \"2025-11-05T14:20:00.000+0000\",\n        \"resolutiondate\": null,\n        \"versions\": [\n          {\"name\": \"4.21\"}\n        ],\n        \"fixVersions\": [\n          {\"name\": \"4.22\"}\n        ],\n        \"customfield_12319940\": \"4.22.0\"\n      }\n    },\n    ...more issues...\n  ],\n  \"note\": \"Showing first 100 of 1500 total results. Increase --limit for more data.\"\n}\n```\n\n**Field Descriptions**:\n\n- `project`: The JIRA project queried\n- `total_count`: Total number of matching issues in JIRA (from search results)\n- `fetched_count`: Number of issues actually fetched (limited by --limit parameter)\n- `query`: The JQL query executed (includes filter for recently closed bugs)\n- `filters`: Applied filters (components, statuses, include_closed, limit)\n- `issues`: Array of raw JIRA issue objects, each containing:\n  - `key`: Issue key (e.g., \"OCPBUGS-12345\")\n  - `fields`: Object containing all JIRA fields for the issue:\n    - `summary`: Issue title/summary\n    - `status`: Status object with name and ID\n    - `priority`: Priority object with name and ID\n    - `components`: Array of component objects\n    - `assignee`: Assignee object with user details\n    - `created`: Creation timestamp\n    - `updated`: Last updated timestamp\n    - `resolutiondate`: Resolution timestamp (null if not closed)\n    - `versions`: Affects Version/s array\n    - `fixVersions`: Fix Version/s array\n    - `customfield_12319940`: Target Version (custom field)\n    - And many other JIRA fields as applicable\n- `note`: Informational message if results are truncated\n\n**Important Notes**:\n\n- **By default, the query includes**: Open bugs + bugs closed in the last 30 days\n- This allows tracking of recent closure activity alongside current open bugs\n- The script fetches a maximum number of issues (default 1000, configurable with `--limit`)\n- The `total_count` represents all matching issues in JIRA\n- The returned data includes ALL fields for each issue, providing complete information\n- For large datasets, increase the `--limit` parameter to fetch more issues\n- Issues can have multiple components\n- All JIRA field data is preserved in the raw format\n\n### Step 6: Present Results\n\nBased on the raw JIRA data:\n\n1. Inform the user about the total count vs fetched count\n2. Explain that the raw data includes all JIRA fields\n3. Suggest using `/component-health:summarize-jiras` if they need summary statistics\n4. The raw issue data can be passed to other commands for further processing\n5. Highlight any truncation and suggest increasing --limit if needed\n\n## Error Handling\n\n### Common Errors\n\n1. **Authentication Errors**\n\n   - **Symptom**: HTTP 401 Unauthorized\n   - **Solution**: Verify JIRA_PERSONAL_TOKEN is correct\n   - **Check**: Ensure token has not expired\n\n2. **Network Errors**\n\n   - **Symptom**: `URLError` or connection timeout\n   - **Solution**: Check network connectivity and JIRA_URL is accessible\n   - **Retry**: The script has a 30-second timeout, consider retrying\n\n3. **Invalid Project**\n\n   - **Symptom**: HTTP 400 or empty results\n   - **Solution**: Verify the project key is correct (e.g., \"OCPBUGS\", not \"ocpbugs\")\n\n4. **Missing Environment Variables**\n\n   - **Symptom**: Error message about missing credentials\n   - **Solution**: Set required environment variables (JIRA_URL, JIRA_USERNAME, JIRA_PERSONAL_TOKEN)\n\n5. **Rate Limiting**\n   - **Symptom**: HTTP 429 Too Many Requests\n   - **Solution**: Wait before retrying, reduce query frequency\n\n### Debugging\n\nEnable verbose output by examining stderr:\n\n```bash\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS 2>&1 | tee debug.log\n```\n\n## Script Arguments\n\n### Required Arguments\n\n- `--project`: JIRA project key to query\n  - Format: Project key (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n  - Must be a valid JIRA project\n\n### Optional Arguments\n\n- `--component`: Filter by component names\n\n  - Values: Space-separated list of component names\n  - Default: None (returns all components)\n  - Case-sensitive matching\n  - Examples: `--component \"kube-apiserver\" \"Management Console\"`\n\n- `--status`: Filter by status values\n\n  - Values: Space-separated list of status names\n  - Default: None (returns all statuses except Closed)\n  - Examples: `--status New \"In Progress\" Verified`\n\n- `--include-closed`: Include closed bugs in the results\n\n  - Default: false (only open bugs)\n  - When specified, includes bugs in \"Closed\" status\n\n- `--limit`: Maximum number of issues to fetch\n  - Default: 100\n  - Maximum: 1000 (JIRA API limit per request)\n  - Higher values provide more accurate statistics but slower performance\n\n## Output Format\n\nThe script outputs JSON with metadata and raw issue data:\n\n```json\n{\n  \"project\": \"OCPBUGS\",\n  \"total_count\": 5430,\n  \"fetched_count\": 100,\n  \"query\": \"project = OCPBUGS AND (status != Closed OR (status = Closed AND resolved >= \\\"2025-10-11\\\"))\",\n  \"filters\": {\n    \"components\": null,\n    \"statuses\": null,\n    \"include_closed\": false,\n    \"limit\": 100\n  },\n  \"issues\": [\n    {\n      \"key\": \"OCPBUGS-12345\",\n      \"fields\": {\n        \"summary\": \"Example bug\",\n        \"status\": {\"name\": \"New\"},\n        \"priority\": {\"name\": \"Major\"},\n        \"components\": [{\"name\": \"kube-apiserver\"}],\n        \"created\": \"2025-11-01T10:30:00.000+0000\",\n        ...\n      }\n    },\n    ...\n  ],\n  \"note\": \"Showing first 100 of 5430 total results. Increase --limit for more data.\"\n}\n```\n\n## Examples\n\n### Example 1: List All Open Bugs\n\n```bash\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS\n```\n\n**Expected Output**: JSON containing raw issue data for all open bugs in OCPBUGS project\n\n### Example 2: Filter by Component\n\n```bash\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\"\n```\n\n**Expected Output**: JSON containing raw issue data for the kube-apiserver component only\n\n### Example 3: Include Closed Bugs\n\n```bash\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --include-closed \\\n  --limit 500\n```\n\n**Expected Output**: JSON containing raw issue data for both open and closed bugs (up to 500 issues)\n\n### Example 4: Filter by Multiple Components\n\n```bash\npython3 plugins/component-health/skills/list-jiras/list_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\" \"etcd\" \"Networking\"\n```\n\n**Expected Output**: JSON containing raw issue data for bugs in specified components\n\n## Integration with Commands\n\nThis skill is designed to:\n\n- Provide raw JIRA data to other commands (like `summarize-jiras`)\n- Be used directly for ad-hoc JIRA queries\n- Serve as a data source for custom analysis workflows\n- Export JIRA data for offline processing\n\n## Related Skills\n\n- `summarize-jiras`: Calculate summary statistics from JIRA data\n- `list-regressions`: Fetch regression data for releases\n- `analyze-regressions`: Grade component health based on regressions\n- `get-release-dates`: Fetch OpenShift release dates\n\n## Notes\n\n- The script uses Python's `urllib` and `json` modules (no external dependencies)\n- Output is always JSON format for easy parsing and further processing\n- Diagnostic messages are written to stderr, data to stdout\n- The script has a 30-second timeout for HTTP requests\n- For large projects, consider using component filters to reduce query size\n- The returned data includes ALL JIRA fields for complete information\n- Use `/component-health:summarize-jiras` if you need summary statistics instead of raw data"
              },
              {
                "name": "List Regressions",
                "description": "Fetch and analyze component health regressions for OpenShift releases",
                "path": "plugins/component-health/skills/list-regressions/SKILL.md",
                "frontmatter": {
                  "name": "List Regressions",
                  "description": "Fetch and analyze component health regressions for OpenShift releases"
                },
                "content": "# List Regressions\n\nThis skill provides functionality to fetch regression data for OpenShift components across different releases. It uses a Python script to query a component health API and retrieve regression information.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Analyze component health for a specific OpenShift release\n- Track regressions across releases\n- Filter regressions by their open/closed status\n- Generate reports on component stability\n\n## Prerequisites\n\n1. **Python 3 Installation**\n\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **Network Access**\n\n   - The script requires network access to reach the component health API\n   - Ensure you can make HTTPS requests\n\n3. **API Endpoint Configuration**\n   - The script includes a placeholder API endpoint that needs to be updated\n   - Update the `base_url` in `list_regressions.py` with the actual component health API endpoint\n\n## Implementation Steps\n\n### Step 1: Verify Prerequisites\n\nFirst, ensure Python 3 is available:\n\n```bash\npython3 --version\n```\n\nIf Python 3 is not installed, guide the user through installation for their platform.\n\n### Step 2: Locate the Script\n\nThe script is located at:\n\n```\nplugins/component-health/skills/list-regressions/list_regressions.py\n```\n\n### Step 3: Run the Script\n\nExecute the script with appropriate arguments:\n\n```bash\n# Basic usage - all regressions for a release\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17\n\n# Filter by specific components\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --components Monitoring \"kube-apiserver\"\n\n# Filter by multiple components\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --components Monitoring etcd \"kube-apiserver\"\n\n# Filter by development window (GA'd release - both start and end)\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17 \\\n  --start 2024-05-17 \\\n  --end 2024-10-01\n\n# Filter by development window (in-development release - start only)\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --start 2025-09-02\n```\n\n### Step 4: Process the Output\n\nThe script outputs JSON data with the following structure:\n\n```json\n{\n  \"summary\": {\n    \"total\": <number>,\n    \"triaged\": <number>,\n    \"triage_percentage\": <number>,\n    \"time_to_triage_hrs_avg\": <number or null>,\n    \"time_to_triage_hrs_max\": <number or null>,\n    \"time_to_close_hrs_avg\": <number or null>,\n    \"time_to_close_hrs_max\": <number or null>,\n    \"open\": {\n      \"total\": <number>,\n      \"triaged\": <number>,\n      \"triage_percentage\": <number>,\n      \"time_to_triage_hrs_avg\": <number or null>,\n      \"time_to_triage_hrs_max\": <number or null>,\n      \"open_hrs_avg\": <number or null>,\n      \"open_hrs_max\": <number or null>\n    },\n    \"closed\": {\n      \"total\": <number>,\n      \"triaged\": <number>,\n      \"triage_percentage\": <number>,\n      \"time_to_triage_hrs_avg\": <number or null>,\n      \"time_to_triage_hrs_max\": <number or null>,\n      \"time_to_close_hrs_avg\": <number or null>,\n      \"time_to_close_hrs_max\": <number or null>,\n      \"time_triaged_closed_hrs_avg\": <number or null>,\n      \"time_triaged_closed_hrs_max\": <number or null>\n    }\n  },\n  \"components\": {\n    \"ComponentName\": {\n      \"summary\": {\n        \"total\": <number>,\n        \"triaged\": <number>,\n        \"triage_percentage\": <number>,\n        \"time_to_triage_hrs_avg\": <number or null>,\n        \"time_to_triage_hrs_max\": <number or null>,\n        \"time_to_close_hrs_avg\": <number or null>,\n        \"time_to_close_hrs_max\": <number or null>,\n        \"open\": {\n          \"total\": <number>,\n          \"triaged\": <number>,\n          \"triage_percentage\": <number>,\n          \"time_to_triage_hrs_avg\": <number or null>,\n          \"time_to_triage_hrs_max\": <number or null>,\n          \"open_hrs_avg\": <number or null>,\n          \"open_hrs_max\": <number or null>\n        },\n        \"closed\": {\n          \"total\": <number>,\n          \"triaged\": <number>,\n          \"triage_percentage\": <number>,\n          \"time_to_triage_hrs_avg\": <number or null>,\n          \"time_to_triage_hrs_max\": <number or null>,\n          \"time_to_close_hrs_avg\": <number or null>,\n          \"time_to_close_hrs_max\": <number or null>,\n          \"time_triaged_closed_hrs_avg\": <number or null>,\n          \"time_triaged_closed_hrs_max\": <number or null>\n        }\n      },\n      \"open\": [...],\n      \"closed\": [...]\n    }\n  }\n}\n```\n\n**CRITICAL**: The output includes pre-calculated counts and health metrics:\n\n- `summary`: Overall statistics across all components\n  - `summary.total`: Total number of regressions\n  - `summary.triaged`: Total number of regressions triaged (open + closed)\n  - **`summary.triage_percentage`**: Percentage of all regressions that have been triaged (KEY HEALTH METRIC)\n  - **`summary.time_to_triage_hrs_avg`**: Overall average hours to triage (combining open and closed, KEY HEALTH METRIC)\n  - `summary.time_to_triage_hrs_max`: Overall maximum hours to triage\n  - **`summary.time_to_close_hrs_avg`**: Overall average hours to close regressions (closed only, KEY HEALTH METRIC)\n  - `summary.time_to_close_hrs_max`: Overall maximum hours to close regressions (closed only)\n  - `summary.open.total`: Number of open regressions (where `closed` is null)\n  - `summary.open.triaged`: Number of open regressions that have been triaged to a JIRA bug\n  - `summary.open.triage_percentage`: Percentage of open regressions triaged\n  - `summary.open.time_to_triage_hrs_avg`: Average hours from opened to first triage (open only)\n  - `summary.open.time_to_triage_hrs_max`: Maximum hours from opened to first triage (open only)\n  - `summary.open.open_hrs_avg`: Average hours that open regressions have been open (from opened to current time)\n  - `summary.open.open_hrs_max`: Maximum hours that open regressions have been open (from opened to current time)\n  - `summary.closed.total`: Number of closed regressions (where `closed` is not null)\n  - `summary.closed.triaged`: Number of closed regressions that have been triaged to a JIRA bug\n  - `summary.closed.triage_percentage`: Percentage of closed regressions triaged\n  - `summary.closed.time_to_triage_hrs_avg`: Average hours from opened to first triage (closed only)\n  - `summary.closed.time_to_triage_hrs_max`: Maximum hours from opened to first triage (closed only)\n  - `summary.closed.time_to_close_hrs_avg`: Average hours from opened to closed timestamp (null if no valid data)\n  - `summary.closed.time_to_close_hrs_max`: Maximum hours from opened to closed timestamp (null if no valid data)\n  - `summary.closed.time_triaged_closed_hrs_avg`: Average hours from first triage to closed (null if no triaged closed regressions)\n  - `summary.closed.time_triaged_closed_hrs_max`: Maximum hours from first triage to closed (null if no triaged closed regressions)\n- `components`: Dictionary mapping component names to objects containing:\n  - `summary`: Per-component statistics (includes same fields as overall summary)\n  - `open`: Array of open regression objects for that component\n  - `closed`: Array of closed regression objects for that component\n\n**Time to Triage Calculation**:\n\nThe `time_to_triage_hrs_avg` field is calculated as:\n\n1. For each triaged regression, find the earliest `created_at` timestamp in the `triages` array\n2. Calculate the time difference between the regression's `opened` timestamp and the earliest triage timestamp\n3. Convert the difference to hours and round to the nearest hour\n4. Only include positive time differences (zero or negative values are skipped - these occur when triages are reused across regression instances)\n5. Average all valid time-to-triage values for open regressions separately from closed regressions\n6. Return `null` if no regressions have valid time-to-triage data in that category\n\n**Time to Close Calculation**:\n\nThe `time_to_close_hrs_avg` and `time_to_close_hrs_max` fields (only for closed regressions) are calculated as:\n\n1. For each closed regression, calculate the time difference between `opened` and `closed` timestamps\n2. Convert the difference to hours and round to the nearest hour\n3. Only include positive time differences (skip data inconsistencies)\n4. Calculate average and maximum of all valid time-to-close values\n5. Return `null` if no closed regressions have valid time data\n\n**Open Duration Calculation**:\n\nThe `open_hrs_avg` and `open_hrs_max` fields (only for open regressions) are calculated as:\n\n1. For each open regression, calculate the time difference between `opened` timestamp and current time\n2. Convert the difference to hours and round to the nearest hour\n3. Only include positive time differences\n4. Calculate average and maximum of all open duration values\n5. Return `null` if no open regressions have valid time data\n\n**Time Triaged to Closed Calculation**:\n\nThe `time_triaged_closed_hrs_avg` and `time_triaged_closed_hrs_max` fields (only for triaged closed regressions) are calculated as:\n\n1. For each closed regression that has been triaged, calculate the time difference between earliest `triages.created_at` timestamp and `closed` timestamp\n2. Convert the difference to hours and round to the nearest hour\n3. Only include positive time differences\n4. Calculate average and maximum of all triaged-to-closed values\n5. Return `null` if no triaged closed regressions have valid time data\n\n**ALWAYS use these summary counts** rather than attempting to count the regression arrays yourself. This ensures accuracy even when the output is truncated due to size.\n\nThe script automatically simplifies and optimizes the response:\n\n**Time field simplification** (`closed` and `last_failure`):\n\n- Original API format: `{\"Time\": \"2025-09-27T12:04:24.966914Z\", \"Valid\": true}`\n- Simplified format: `\"closed\": \"2025-09-27T12:04:24.966914Z\"` (if Valid is true)\n- Or: `\"closed\": null` (if Valid is false)\n- Same applies to `last_failure` field\n\n**Field removal for response size optimization**:\n\n- `links`: Removed from each regression (reduces response size significantly)\n- `test_id`: Removed from each regression (large field, can be reconstructed from test_name if needed)\n\n**Date filtering (optional)**:\n\n- Use `--start` and `--end` parameters to filter regressions to a specific time window\n- `--start YYYY-MM-DD`: Excludes regressions that were closed before this date\n- `--end YYYY-MM-DD`: Excludes regressions that were opened after this date\n- Typical use case: Filter to the development window\n  - `--start`: development_start date from get-release-dates skill (always applied)\n  - `--end`: GA date from get-release-dates skill (only for GA'd releases)\n- For GA'd releases: Both start and end filtering applied\n- For in-development releases (null GA date): Only start filtering applied (no end date)\n- Benefits: Focuses analysis on regressions during active development, excluding:\n  - Regressions closed before the release development started (not relevant)\n  - Regressions opened after GA (post-release, often not monitored/triaged - GA'd releases only)\n\nParse this JSON output to extract relevant information for analysis.\n\n### Step 5: Generate Analysis (Optional)\n\nBased on the regression data:\n\n1. **Use the summary counts** from the `summary` and `components.*.summary` objects (do NOT count the arrays)\n2. Identify most affected components using `components.*.summary.open.total`\n3. Compare with previous releases\n4. Analyze trends in open vs closed regressions per component\n5. Create visualizations if needed\n\n## Error Handling\n\n### Common Errors\n\n1. **Network Errors**\n\n   - **Symptom**: `URLError` or connection timeout\n   - **Solution**: Check network connectivity and firewall rules\n   - **Retry**: The script has a 30-second timeout, consider retrying\n\n2. **HTTP Errors**\n\n   - **Symptom**: HTTP 404, 500, etc.\n   - **Solution**: Verify the API endpoint URL is correct\n   - **Check**: Ensure the release parameter is valid\n\n3. **Invalid Release**\n\n   - **Symptom**: Empty results or error response\n   - **Solution**: Verify the release format (e.g., \"4.17\", not \"v4.17\")\n\n4. **Invalid Boolean Value**\n   - **Symptom**: `ValueError: Invalid boolean value`\n   - **Solution**: Use only \"true\" or \"false\" for the --opened flag\n\n### Debugging\n\nEnable verbose output by examining stderr:\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17 2>&1 | tee debug.log\n```\n\n## Script Arguments\n\n### Required Arguments\n\n- `--release`: Release version to query\n  - Format: `\"X.Y\"` (e.g., \"4.17\", \"4.16\")\n  - Must be a valid OpenShift release number\n\n### Optional Arguments\n\n- `--components`: Filter by component names\n  - Values: Space-separated list of component names\n  - Default: None (returns all components)\n  - Case-insensitive matching\n  - Examples: `--components Monitoring etcd \"kube-apiserver\"`\n  - Filtering is performed after fetching data from the API\n\n## Output Format\n\nThe script outputs JSON with summaries and regressions grouped by component:\n\n```json\n{\n  \"summary\": {\n    \"total\": 62,\n    \"triaged\": 59,\n    \"triage_percentage\": 95.2,\n    \"time_to_triage_hrs_avg\": 68,\n    \"time_to_triage_hrs_max\": 240,\n    \"time_to_close_hrs_avg\": 168,\n    \"time_to_close_hrs_max\": 480,\n    \"open\": {\n      \"total\": 2,\n      \"triaged\": 1,\n      \"triage_percentage\": 50.0,\n      \"time_to_triage_hrs_avg\": 48,\n      \"time_to_triage_hrs_max\": 48,\n      \"open_hrs_avg\": 120,\n      \"open_hrs_max\": 200\n    },\n    \"closed\": {\n      \"total\": 60,\n      \"triaged\": 58,\n      \"triage_percentage\": 96.7,\n      \"time_to_triage_hrs_avg\": 72,\n      \"time_to_triage_hrs_max\": 240,\n      \"time_to_close_hrs_avg\": 168,\n      \"time_to_close_hrs_max\": 480,\n      \"time_triaged_closed_hrs_avg\": 96,\n      \"time_triaged_closed_hrs_max\": 240\n    }\n  },\n  \"components\": {\n    \"Monitoring\": {\n      \"summary\": {\n        \"total\": 15,\n        \"triaged\": 13,\n        \"triage_percentage\": 86.7,\n        \"time_to_triage_hrs_avg\": 68,\n        \"time_to_triage_hrs_max\": 180,\n        \"time_to_close_hrs_avg\": 156,\n        \"time_to_close_hrs_max\": 360,\n        \"open\": {\n          \"total\": 1,\n          \"triaged\": 0,\n          \"triage_percentage\": 0.0,\n          \"time_to_triage_hrs_avg\": null,\n          \"time_to_triage_hrs_max\": null,\n          \"open_hrs_avg\": 72,\n          \"open_hrs_max\": 72\n        },\n        \"closed\": {\n          \"total\": 14,\n          \"triaged\": 13,\n          \"triage_percentage\": 92.9,\n          \"time_to_triage_hrs_avg\": 68,\n          \"time_to_triage_hrs_max\": 180,\n          \"time_to_close_hrs_avg\": 156,\n          \"time_to_close_hrs_max\": 360,\n          \"time_triaged_closed_hrs_avg\": 88,\n          \"time_triaged_closed_hrs_max\": 180\n        }\n      },\n      \"open\": [\n        {\n          \"id\": 12894,\n          \"component\": \"Monitoring\",\n          \"closed\": null,\n          ...\n        }\n      ],\n      \"closed\": [\n        {\n          \"id\": 12893,\n          \"view\": \"4.21-main\",\n          \"release\": \"4.21\",\n          \"base_release\": \"4.18\",\n          \"component\": \"Monitoring\",\n          \"capability\": \"operator-conditions\",\n          \"test_name\": \"...\",\n          \"variants\": [...],\n          \"opened\": \"2025-09-26T00:02:51.385944Z\",\n          \"closed\": \"2025-09-27T12:04:24.966914Z\",\n          \"triages\": [],\n          \"last_failure\": \"2025-09-25T14:41:17Z\",\n          \"max_failures\": 9\n        }\n      ]\n    },\n    \"etcd\": {\n      \"summary\": {\n        \"total\": 20,\n        \"triaged\": 19,\n        \"triage_percentage\": 95.0,\n        \"time_to_triage_hrs_avg\": 84,\n        \"time_to_triage_hrs_max\": 220,\n        \"time_to_close_hrs_avg\": 192,\n        \"time_to_close_hrs_max\": 500,\n        \"open\": {\n          \"total\": 0,\n          \"triaged\": 0,\n          \"triage_percentage\": 0.0,\n          \"time_to_triage_hrs_avg\": null,\n          \"time_to_triage_hrs_max\": null,\n          \"open_hrs_avg\": null,\n          \"open_hrs_max\": null\n        },\n        \"closed\": {\n          \"total\": 20,\n          \"triaged\": 19,\n          \"triage_percentage\": 95.0,\n          \"time_to_triage_hrs_avg\": 84,\n          \"time_to_triage_hrs_max\": 220,\n          \"time_to_close_hrs_avg\": 192,\n          \"time_to_close_hrs_max\": 500,\n          \"time_triaged_closed_hrs_avg\": 108,\n          \"time_triaged_closed_hrs_max\": 280\n        }\n      },\n      \"open\": [],\n      \"closed\": [...]\n    },\n    \"kube-apiserver\": {\n      \"summary\": {\n        \"total\": 27,\n        \"triaged\": 27,\n        \"triage_percentage\": 100.0,\n        \"time_to_triage_hrs_avg\": 58,\n        \"time_to_triage_hrs_max\": 168,\n        \"time_to_close_hrs_avg\": 144,\n        \"time_to_close_hrs_max\": 400,\n        \"open\": {\n          \"total\": 1,\n          \"triaged\": 1,\n          \"triage_percentage\": 100.0,\n          \"time_to_triage_hrs_avg\": 36,\n          \"time_to_triage_hrs_max\": 36,\n          \"open_hrs_avg\": 96,\n          \"open_hrs_max\": 96\n        },\n        \"closed\": {\n          \"total\": 26,\n          \"triaged\": 26,\n          \"triage_percentage\": 100.0,\n          \"time_to_triage_hrs_avg\": 60,\n          \"time_to_triage_hrs_max\": 168,\n          \"time_to_close_hrs_avg\": 144,\n          \"time_to_close_hrs_max\": 400,\n          \"time_triaged_closed_hrs_avg\": 84,\n          \"time_triaged_closed_hrs_max\": 232\n        }\n      },\n      \"open\": [...],\n      \"closed\": [...]\n    }\n  }\n}\n```\n\n**Important - Summary Objects**:\n\n- The `summary` object contains overall pre-calculated counts for accuracy\n- Each component in the `components` object has its own `summary` with per-component counts\n- The `components` object maps component names (sorted alphabetically) to objects containing:\n  - `summary`: Statistics for this component (total, open, closed)\n  - `open`: Array of open regression objects (where `closed` is null)\n  - `closed`: Array of closed regression objects (where `closed` has a timestamp)\n- **ALWAYS use the `summary` and `components.*.summary` fields** for counts (including `total`, `open.total`, `open.triaged`, `closed.total`, `closed.triaged`)\n- Do NOT attempt to count the `components.*.open` or `components.*.closed` arrays yourself\n\n**Note**: Time fields are simplified from the API response:\n\n- `closed`: If the regression is closed: `\"closed\": \"2025-09-27T12:04:24.966914Z\"` (timestamp string), otherwise `null`\n- `last_failure`: If valid: `\"last_failure\": \"2025-09-25T14:41:17Z\"` (timestamp string), otherwise `null`\n\n## Examples\n\n### Example 1: List All Regressions\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.17\n```\n\n**Expected Output**: JSON containing all regressions for release 4.17\n\n### Example 2: Filter by Component\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --components Monitoring etcd\n```\n\n**Expected Output**: JSON containing regressions for only Monitoring and etcd components in release 4.21\n\n### Example 3: Filter by Single Component\n\n```bash\npython3 plugins/component-health/skills/list-regressions/list_regressions.py \\\n  --release 4.21 \\\n  --components \"kube-apiserver\"\n```\n\n**Expected Output**: JSON containing regressions for the kube-apiserver component in release 4.21\n\n## Customization\n\n### Updating the API Endpoint\n\nThe script includes a placeholder API endpoint. Update it in `list_regressions.py`:\n\n```python\n# Current placeholder\nbase_url = f\"https://component-health-api.example.com/api/v1/regressions\"\n\n# Update to actual endpoint\nbase_url = f\"https://actual-api.example.com/api/v1/regressions\"\n```\n\n### Adding Custom Filters\n\nTo add additional query parameters, modify the `fetch_regressions` function:\n\n```python\ndef fetch_regressions(release: str, opened: Optional[bool] = None,\n                     component: Optional[str] = None) -> dict:\n    params = [f\"release={release}\"]\n    if opened is not None:\n        params.append(f\"opened={'true' if opened else 'false'}\")\n    if component is not None:\n        params.append(f\"component={component}\")\n    # ... rest of function\n```\n\n## Integration with Commands\n\nThis skill is designed to be used by the `/component-health:analyze-regressions` command, but can also be invoked directly by other commands or scripts that need regression data.\n\n## Related Skills\n\n- Component health analysis\n- Release comparison\n- Regression tracking\n- Quality metrics reporting\n\n## Notes\n\n- The script uses Python's built-in `urllib` module (no external dependencies)\n- Output is always JSON format for easy parsing\n- Diagnostic messages are written to stderr, data to stdout\n- The script has a 30-second timeout for HTTP requests"
              },
              {
                "name": "Summarize JIRAs",
                "description": "Query and summarize JIRA bugs for a specific project with counts by component",
                "path": "plugins/component-health/skills/summarize-jiras/SKILL.md",
                "frontmatter": {
                  "name": "Summarize JIRAs",
                  "description": "Query and summarize JIRA bugs for a specific project with counts by component"
                },
                "content": "# Summarize JIRAs\n\nThis skill provides functionality to query JIRA bugs for a specified project and generate summary statistics. It leverages the `list-jiras` skill to fetch raw JIRA data, then calculates counts by status, priority, and component to provide insights into the bug backlog.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Get a count of open bugs in a JIRA project\n- Analyze bug distribution by status, priority, or component\n- Generate summary reports for bug backlog\n- Track bug trends and velocity over time (opened vs closed in last 30 days)\n- Compare bug counts across different components\n- Monitor component health based on bug metrics\n\n## Prerequisites\n\n1. **Python 3 Installation**\n   - Check if installed: `which python3`\n   - Python 3.6 or later is required\n   - Comes pre-installed on most systems\n\n2. **JIRA Authentication**\n   - Requires environment variables to be set:\n     - `JIRA_URL`: Base URL for JIRA instance (e.g., \"https://issues.redhat.com\")\n     - `JIRA_PERSONAL_TOKEN`: Your JIRA bearer token or personal access token\n   - How to get a JIRA token:\n     - Navigate to JIRA  Profile  Personal Access Tokens\n     - Generate a new token with appropriate permissions\n     - Export it as an environment variable\n\n3. **Network Access**\n   - The script requires network access to reach your JIRA instance\n   - Ensure you can make HTTPS requests to the JIRA URL\n\n## Implementation Steps\n\n### Step 1: Verify Prerequisites\n\nFirst, ensure Python 3 is available:\n\n```bash\npython3 --version\n```\n\nIf Python 3 is not installed, guide the user through installation for their platform.\n\n### Step 2: Verify Environment Variables\n\nCheck that required environment variables are set:\n\n```bash\n# Verify JIRA credentials are configured\necho \"JIRA_URL: ${JIRA_URL}\"\necho \"JIRA_PERSONAL_TOKEN: ${JIRA_PERSONAL_TOKEN:+***set***}\"\n```\n\nIf any are missing, guide the user to set them:\n\n```bash\nexport JIRA_URL=\"https://issues.redhat.com\"\nexport JIRA_PERSONAL_TOKEN=\"your-token-here\"\n```\n\n### Step 3: Locate the Script\n\nThe script is located at:\n\n```\nplugins/component-health/skills/summarize-jiras/summarize_jiras.py\n```\n\n### Step 4: Run the Script\n\nExecute the script with appropriate arguments:\n\n```bash\n# Basic usage - summarize all open bugs in a project\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS\n\n# Filter by component\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\"\n\n# Filter by multiple components\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\" \"Management Console\"\n\n# Include closed bugs\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --include-closed\n\n# Filter by status\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --status New \"In Progress\"\n\n# Set maximum results limit (default 100)\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --limit 500\n```\n\n### Step 5: Process the Output\n\nThe script outputs JSON data with the following structure:\n\n```json\n{\n  \"project\": \"OCPBUGS\",\n  \"total_count\": 1500,\n  \"fetched_count\": 100,\n  \"query\": \"project = OCPBUGS AND (status != Closed OR (status = Closed AND resolved >= \\\"2025-10-11\\\"))\",\n  \"filters\": {\n    \"components\": null,\n    \"statuses\": null,\n    \"include_closed\": false,\n    \"limit\": 100\n  },\n  \"summary\": {\n    \"total\": 100,\n    \"opened_last_30_days\": 15,\n    \"closed_last_30_days\": 8,\n    \"by_status\": {\n      \"New\": 35,\n      \"In Progress\": 25,\n      \"Verified\": 20,\n      \"Modified\": 15,\n      \"ON_QA\": 5,\n      \"Closed\": 8\n    },\n    \"by_priority\": {\n      \"Normal\": 50,\n      \"Major\": 30,\n      \"Minor\": 12,\n      \"Critical\": 5,\n      \"Undefined\": 3\n    },\n    \"by_component\": {\n      \"kube-apiserver\": 25,\n      \"Management Console\": 30,\n      \"Networking\": 20,\n      \"etcd\": 15,\n      \"No Component\": 10\n    }\n  },\n  \"components\": {\n    \"kube-apiserver\": {\n      \"total\": 25,\n      \"opened_last_30_days\": 4,\n      \"closed_last_30_days\": 2,\n      \"by_status\": {\n        \"New\": 10,\n        \"In Progress\": 8,\n        \"Verified\": 5,\n        \"Modified\": 2,\n        \"Closed\": 2\n      },\n      \"by_priority\": {\n        \"Major\": 12,\n        \"Normal\": 10,\n        \"Minor\": 2,\n        \"Critical\": 1\n      }\n    },\n    \"Management Console\": {\n      \"total\": 30,\n      \"opened_last_30_days\": 6,\n      \"closed_last_30_days\": 3,\n      \"by_status\": {\n        \"New\": 12,\n        \"In Progress\": 10,\n        \"Verified\": 6,\n        \"Modified\": 2,\n        \"Closed\": 3\n      },\n      \"by_priority\": {\n        \"Normal\": 18,\n        \"Major\": 8,\n        \"Minor\": 3,\n        \"Critical\": 1\n      }\n    },\n    \"etcd\": {\n      \"total\": 15,\n      \"opened_last_30_days\": 3,\n      \"closed_last_30_days\": 2,\n      \"by_status\": {\n        \"New\": 8,\n        \"In Progress\": 4,\n        \"Verified\": 3,\n        \"Closed\": 2\n      },\n      \"by_priority\": {\n        \"Normal\": 10,\n        \"Major\": 4,\n        \"Critical\": 1\n      }\n    }\n  },\n  \"note\": \"Showing first 100 of 1500 total results. Increase --limit for more accurate statistics.\"\n}\n```\n\n**Field Descriptions**:\n\n- `project`: The JIRA project queried\n- `total_count`: Total number of matching issues (from JIRA search results)\n- `fetched_count`: Number of issues actually fetched (limited by --limit parameter)\n- `query`: The JQL query executed (includes filter for recently closed bugs)\n- `filters`: Applied filters (components, statuses, include_closed, limit)\n- `summary`: Overall statistics across all fetched issues\n  - `total`: Count of fetched issues (same as `fetched_count`)\n  - `opened_last_30_days`: Number of issues created in the last 30 days\n  - `closed_last_30_days`: Number of issues closed/resolved in the last 30 days\n  - `by_status`: Count of issues per status (includes recently closed issues)\n  - `by_priority`: Count of issues per priority\n  - `by_component`: Count of issues per component (note: issues can have multiple components)\n- `components`: Per-component breakdown with individual summaries\n  - Each component key maps to:\n    - `total`: Number of issues assigned to this component\n    - `opened_last_30_days`: Number of issues created in the last 30 days for this component\n    - `closed_last_30_days`: Number of issues closed in the last 30 days for this component\n    - `by_status`: Status distribution for this component\n    - `by_priority`: Priority distribution for this component\n- `note`: Informational message if results are truncated\n\n**Important Notes**:\n\n- **By default, the query includes**: Open bugs + bugs closed in the last 30 days\n- This allows tracking of recent closure activity alongside current open bugs\n- The script fetches a maximum number of issues (default 100, configurable with `--limit`)\n- The `total_count` represents all matching issues in JIRA\n- Summary statistics are based on the fetched issues only\n- For accurate statistics across large datasets, increase the `--limit` parameter\n- Issues can have multiple components, so component totals may sum to more than the overall total\n- `opened_last_30_days` and `closed_last_30_days` help track recent bug flow and velocity\n\n### Step 6: Present Results\n\nBased on the summary data:\n\n1. Present total bug counts\n2. Highlight distribution by status (e.g., how many in \"New\" vs \"In Progress\")\n3. Identify priority breakdown (Critical, Major, Normal, etc.)\n4. Show component distribution\n5. Display per-component breakdowns with status and priority counts\n6. Calculate actionable metrics (e.g., New + Assigned = bugs needing triage/work)\n7. Highlight recent activity (opened/closed in last 30 days) per component\n\n## Error Handling\n\n### Common Errors\n\n1. **Authentication Errors**\n   - **Symptom**: HTTP 401 Unauthorized\n   - **Solution**: Verify JIRA_URL and JIRA_PERSONAL_TOKEN are correct\n   - **Check**: Ensure token has not expired\n\n2. **Network Errors**\n   - **Symptom**: `URLError` or connection timeout\n   - **Solution**: Check network connectivity and JIRA_URL is accessible\n   - **Retry**: The script has a 30-second timeout, consider retrying\n\n3. **Invalid Project**\n   - **Symptom**: HTTP 400 or empty results\n   - **Solution**: Verify the project key is correct (e.g., \"OCPBUGS\", not \"ocpbugs\")\n\n4. **Missing Environment Variables**\n   - **Symptom**: Error message about missing credentials\n   - **Solution**: Set required environment variables (JIRA_URL, JIRA_PERSONAL_TOKEN)\n\n5. **Rate Limiting**\n   - **Symptom**: HTTP 429 Too Many Requests\n   - **Solution**: Wait before retrying, reduce query frequency\n\n### Debugging\n\nEnable verbose output by examining stderr:\n\n```bash\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS 2>&1 | tee debug.log\n```\n\n## Script Arguments\n\n### Required Arguments\n\n- `--project`: JIRA project key to query\n  - Format: Project key (e.g., \"OCPBUGS\", \"OCPSTRAT\")\n  - Must be a valid JIRA project\n\n### Optional Arguments\n\n- `--component`: Filter by component names\n  - Values: Space-separated list of component names\n  - Default: None (returns all components)\n  - Case-sensitive matching\n  - Examples: `--component \"kube-apiserver\" \"Management Console\"`\n\n- `--status`: Filter by status values\n  - Values: Space-separated list of status names\n  - Default: None (returns all statuses except Closed)\n  - Examples: `--status New \"In Progress\" Verified`\n\n- `--include-closed`: Include closed bugs in the results\n  - Default: false (only open bugs)\n  - When specified, includes bugs in \"Closed\" status\n\n- `--limit`: Maximum number of issues to fetch\n  - Default: 100\n  - Maximum: 1000 (JIRA API limit per request)\n  - Higher values provide more accurate statistics but slower performance\n\n## Output Format\n\nThe script outputs JSON with summary statistics and per-component breakdowns:\n\n```json\n{\n  \"project\": \"OCPBUGS\",\n  \"total_count\": 5430,\n  \"fetched_count\": 100,\n  \"query\": \"project = OCPBUGS AND (status != Closed OR (status = Closed AND resolved >= \\\"2025-10-11\\\"))\",\n  \"filters\": {\n    \"components\": null,\n    \"statuses\": null,\n    \"include_closed\": false,\n    \"limit\": 100\n  },\n  \"summary\": {\n    \"total\": 100,\n    \"opened_last_30_days\": 15,\n    \"closed_last_30_days\": 8,\n    \"by_status\": {\n      \"New\": 1250,\n      \"In Progress\": 800,\n      \"Verified\": 650\n    },\n    \"by_priority\": {\n      \"Critical\": 50,\n      \"Major\": 450,\n      \"Normal\": 2100\n    },\n    \"by_component\": {\n      \"kube-apiserver\": 146,\n      \"Management Console\": 392\n    }\n  },\n  \"components\": {\n    \"kube-apiserver\": {\n      \"total\": 146,\n      \"opened_last_30_days\": 20,\n      \"closed_last_30_days\": 12,\n      \"by_status\": {...},\n      \"by_priority\": {...}\n    }\n  },\n  \"note\": \"Showing first 100 of 5430 total results. Increase --limit for more accurate statistics.\"\n}\n```\n\n## Examples\n\n### Example 1: Summarize All Open Bugs\n\n```bash\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS\n```\n\n**Expected Output**: JSON containing summary statistics of all open bugs in OCPBUGS project\n\n### Example 2: Filter by Component\n\n```bash\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\"\n```\n\n**Expected Output**: JSON containing summary for the kube-apiserver component only\n\n### Example 3: Include Closed Bugs\n\n```bash\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --include-closed \\\n  --limit 500\n```\n\n**Expected Output**: JSON containing summary of both open and closed bugs (up to 500 issues)\n\n### Example 4: Filter by Multiple Components\n\n```bash\npython3 plugins/component-health/skills/summarize-jiras/summarize_jiras.py \\\n  --project OCPBUGS \\\n  --component \"kube-apiserver\" \"etcd\" \"Networking\"\n```\n\n**Expected Output**: JSON containing summary for specified components\n\n## Integration with Commands\n\nThis skill is designed to:\n- Provide summary statistics for JIRA bug analysis\n- Be used by component health analysis workflows\n- Generate reports for bug triage and planning\n- Track component health metrics over time\n- Leverage the `list-jiras` skill for raw data fetching\n\n## Related Skills\n\n- `list-jiras`: Fetch raw JIRA issue data\n- `list-regressions`: Fetch regression data for releases\n- `analyze-regressions`: Grade component health based on regressions\n- `get-release-dates`: Fetch OpenShift release dates\n\n## Notes\n\n- The script uses Python's standard library only (no external dependencies)\n- Output is always JSON format for easy parsing\n- Diagnostic messages are written to stderr, data to stdout\n- The script internally calls `list_jiras.py` to fetch raw data\n- The script has a 30-second timeout for HTTP requests (inherited from list_jiras.py)\n- For large projects, consider using component filters to reduce query size\n- Summary statistics are based on fetched issues (controlled by --limit), not total matching issues\n- For raw JIRA data without summarization, use `/component-health:list-jiras` instead"
              }
            ]
          },
          {
            "name": "doc",
            "description": "A plugin for engineering documentation and notes",
            "source": "./plugins/doc",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install doc@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/note",
                "description": "Generate professional engineering notes and append them to a log file",
                "path": "plugins/doc/commands/note.md",
                "frontmatter": {
                  "description": "Generate professional engineering notes and append them to a log file",
                  "argument-hint": "[task description]"
                },
                "content": "## Name\ndoc:note\n\n## Synopsis\n```\n/doc:note [task description]\n```\n\n## Description\nThe `doc:note` command generates professional engineering notes about completed tasks and appends them to a persistent log file (`~/engineering-notes.md`). It automatically searches for relevant context including GitHub PR URLs, issue numbers, and Jira ticket references in the conversation history and git repository.\n\nThis command helps engineers maintain a structured record of their daily work, making it easier to:\n- Track accomplishments for performance reviews\n- Generate status reports and weekly updates\n- Maintain a searchable history of technical decisions\n- Document completed work with proper attribution\n\n## Implementation\nThe command performs the following steps:\n1. **Context gathering**: Searches conversation history for GitHub PR URLs, issue numbers, or Jira ticket keys (e.g., PROJ-123)\n2. **Git analysis**: If in a git repository, checks recent commits and current branch name for references\n3. **Note generation**: Creates a 1-2 sentence note with:\n   - Today's date in YYYY-MM-DD format\n   - Accomplishment framed in past tense\n   - Technical details and specific technologies used\n   - Impact and value delivered\n   - All relevant links inline\n4. **File management**: Appends the note to `~/engineering-notes.md` (creates file if it doesn't exist) with proper spacing\n\nIf the task description argument is omitted, the command will attempt to discover a task description from relevant context (e.g. git repository status and conversation history). If no relevant context is discovered, or if more information is needed, the command will prompt for further context.\n\n## Return Value\n- **Success**: Confirmation message with the generated note\n- **File created**: `~/engineering-notes.md` (if it didn't exist)\n- **File updated**: Note appended with blank line separator\n\n## Examples\n\n1. **Basic usage with task description**:\n   ```\n   /doc:note Implemented user authentication with OAuth2\n   ```\n   Generates:\n   ```\n   2025-10-24 - Implemented user authentication using OAuth2. Integrated with Google and GitHub providers, added JWT token management, and secured API endpoints with role-based access control.\n\n   ```\n\n2. **Without task description (auto-discovers from context)**:\n   ```\n   /doc:note\n   ```\n   The command analyzes git repository and conversation history to generate a note. If insufficient context is available, it will prompt for details.\n\n3. **With git context**:\n   ```\n   /doc:note Fixed critical bug in payment processor\n   ```\n   If on a branch named `fix/payment-timeout` with recent commits, generates:\n   ```\n   2025-10-24 - Fixed critical timeout bug in payment processor (PR #123). Optimized database queries and added connection pooling, reducing payment processing time by 60% and eliminating timeout errors.\n\n   ```\n\n## Arguments\n- `[task description]`: Optional description of the completed task. If omitted, the command attempts to discover context automatically."
              }
            ],
            "skills": []
          },
          {
            "name": "session",
            "description": "A plugin for Claude session management and persistence",
            "source": "./plugins/session",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install session@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/save-session",
                "description": "Save current conversation session to markdown file for future continuation",
                "path": "plugins/session/commands/save-session.md",
                "frontmatter": {
                  "description": "Save current conversation session to markdown file for future continuation",
                  "argument-hint": "[optional-description]"
                },
                "content": "## Name\nsession:save-session\n\n## Synopsis\n\n```\n/save-session\n/save-session [description]\n```\n\n## Description\n\nSaves the current conversation session to a comprehensive markdown file that enables seamless resumption of work after extended time intervals (days, weeks, or months).\n\nThis command addresses limitations of Claude Code's built-in session management by capturing:\n- Complete conversation context and technical rationale\n- Detailed file modification tracking with line numbers\n- Key technical decisions and alternatives considered\n- Commands executed during the session\n- Clear resumption instructions\n\nThe generated session file is designed for engineers working across multiple projects with long gaps between sessions, providing all necessary context to continue work without losing momentum.\n\n## Implementation\n\nThe command follows a five-phase process:\n\n### Phase 0: Input Sanitization\nIf a description argument is provided, sanitize it for safe filename usage:\n- Convert all spaces to hyphens\n- Convert to lowercase\n- Remove or replace special characters (keep only alphanumeric, hyphens, and underscores)\n- Truncate to 100 characters maximum if longer\n- Example: \"investigating OCPBUGS-12345 regarding routes\"  \"investigating-ocpbugs-12345-regarding-routes\"\n\n### Phase 1: Context Analysis\n- Summarizes main topics and goals discussed\n- Lists all accomplishments and completed tasks\n- Identifies all files that were read, modified, or created\n- Extracts important technical decisions and their rationale\n- Captures any error messages encountered and how they were resolved\n- Notes any commands that were run (make, linter, tests, etc.)\n\n### Phase 2: File Modification Tracking\n- Reads and verifies current state of modified files\n- Lists specific line numbers and code changes\n- Includes before/after comparisons for critical changes\n- Notes which files were created vs modified vs deleted\n- Tracks any generated files (like bindata)\n\n### Phase 3: Session File Creation\nCreates a comprehensive markdown document with these sections:\n\n1. **Session Summary** - Brief 1-2 paragraph overview\n2. **Current State** - Status of work and modifications\n3. **Accomplishments** - Detailed completion checklist\n4. **Files Modified** - Organized by Created/Modified/Deleted\n5. **Key Technical Decisions** - Rationale and implications\n6. **Pending Tasks** - Unfinished work (checkbox format)\n7. **Commands Used** - All executed commands\n8. **Context for Resumption** - Critical continuation information\n9. **Full Conversation Summary** - Key discussion points\n10. **Next Steps** - Clear action items\n11. **How to Resume This Session** - Step-by-step guide\n\n### Phase 4: Verification and Output\n- Confirms file was created successfully\n- Displays file path and size\n- Provides brief summary of what was saved\n- Shows resumption instructions in terminal and saved file\n\n## Return Value\n\nCreates a markdown file in the repository root directory with filename:\n- `session-YYYY-MM-DD-HHMM.md` (without description)\n- `session-YYYY-MM-DD-<description>.md` (with custom description)\n\nTerminal output:\n```\n Session saved successfully!\n\nFile: session-YYYY-MM-DD-description.md (XX KB)\nLocation: /full/path/to/file\n\n To resume this session:\n   Please read `/full/path/to/session-YYYY-MM-DD-description.md` and continue from where we left off\n```\n\n## Examples\n\n**Basic usage with auto-generated timestamp:**\n```\n/save-session\n```\nCreates: `session-2025-10-16-1430.md`\n\n**With custom description for easy identification:**\n```\n/save-session parallel-test-fixes\n```\nCreates: `session-2025-10-16-parallel-test-fixes.md`\n\n**Multiple sessions in one project:**\n```\n/save-session initial-implementation\n/save-session pr-review-feedback\n/save-session final-testing\n```\n\n**With spaces and special characters (automatically sanitized):**\n```\n/save-session investigating OCPBUGS-12345 regarding routes\n```\nCreates: `session-2025-10-16-investigating-ocpbugs-12345-regarding-routes.md`\n\n**Resuming a saved session:**\nOpen Claude Code and say:\n```\nPlease read `/path/to/session-2025-10-16-parallel-test-fixes.md` and continue from where we left off\n```\n\n## Arguments\n\n**description** (optional)\n- Custom identifier appended to the filename\n- Helps identify the session purpose when resuming after long intervals\n- **Input handling**: Description is automatically sanitized for safe filename usage (spaces converted to hyphens, special characters removed, truncated to 100 chars if needed)\n- **Good examples**: `feature-name`, `bug-fix`, `refactoring`, `investigating-ocpbugs-12345`\n- Automatically added to filename: `session-YYYY-MM-DD-<description>.md`\n\nIf no description is provided, timestamp alone is used: `session-YYYY-MM-DD-HHMM.md`\n\n**Note**: You can use spaces and special characters in your description - they will be automatically sanitized. For example, \"investigating OCPBUGS-12345 regarding routes\" becomes \"investigating-ocpbugs-12345-regarding-routes\"."
              }
            ],
            "skills": []
          },
          {
            "name": "sosreport",
            "description": "Analyze sosreport archives for system diagnostics and troubleshooting",
            "source": "./plugins/sosreport",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install sosreport@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Analyze sosreport archive for system diagnostics and issues",
                "path": "plugins/sosreport/commands/analyze.md",
                "frontmatter": {
                  "description": "Analyze sosreport archive for system diagnostics and issues",
                  "argument-hint": "<path-to-sosreport> [--only <areas>] [--skip <areas>]"
                },
                "content": "## Name\nsosreport:analyze\n\n## Synopsis\n```\n/sosreport:analyze <path-to-sosreport> [--only <areas>] [--skip <areas>]\n```\n\n**Analysis Areas:**\n\n- **`logs`**: Analyze system and application logs (journald, syslog, dmesg, application logs)\n  - Identifies errors, warnings, critical messages\n  - Detects OOM killer events, kernel panics, segfaults\n  - Counts and categorizes errors by severity\n  - Provides timeline of critical events\n\n- **`resources`**: Analyze system resource usage (memory, CPU, disk, processes)\n  - Memory usage, swap, and pressure indicators\n  - CPU information and load averages\n  - Disk usage and filesystem capacity\n  - Top resource consumers and zombie processes\n\n- **`network`**: Analyze network configuration and connectivity\n  - Network interface status and IP addresses\n  - Routing table and default gateway\n  - Active connections and listening services\n  - Firewall rules (firewalld/iptables)\n  - DNS configuration and hostname resolution\n\n- **`system-config`**: Analyze system configuration (packages, services, security)\n  - OS version and kernel information\n  - Installed package versions\n  - Systemd service status and failures\n  - SELinux/AppArmor configuration and denials\n  - Kernel parameters and resource limits\n\n## Description\nThe `sosreport:analyze` command performs comprehensive analysis of a sosreport archive (from <https://github.com/sosreport/sos>) to identify system issues, configuration problems, and potential causes of failures. It examines system logs, resource usage, network configuration, installed packages, and other diagnostic data collected by sosreport.\n\nBy default, all analysis areas are executed. Use `--only` to run specific areas or `--skip` to exclude areas from analysis.\n\n## Arguments\n- `$1` (required): Path to the sosreport archive file (`.tar.gz` or `.tar.xz`) or extracted directory\n- `--only <areas>` (optional): Comma-separated list of analysis areas to run. Valid areas: `logs`, `resources`, `network`, `system-config`. If not specified, all areas are analyzed.\n- `--skip <areas>` (optional): Comma-separated list of analysis areas to skip. Valid areas: `logs`, `resources`, `network`, `system-config`. Cannot be used with `--only`.\n\n## Implementation\n\nThe sosreport analysis is organized into several specialized phases, each with detailed implementation guidance in separate skill documents. The command supports selective analysis through optional arguments.\n\n### 1. Parse Arguments and Determine Analysis Scope\n\n1. **Parse command-line arguments**\n   - Extract the sosreport path (required first argument)\n   - Check for `--only` flag and parse comma-separated areas\n   - Check for `--skip` flag and parse comma-separated areas\n   - Validate that `--only` and `--skip` are not used together\n\n2. **Validate analysis areas**\n   - Valid areas: `logs`, `resources`, `network`, `system-config`\n   - If invalid area specified, return error with list of valid areas\n   - Normalize area names (case-insensitive, accept variations like `system` for `system-config`)\n\n3. **Determine which skills to run**\n   - If no flags specified: Run all skills (default comprehensive analysis)\n   - If `--only` specified: Run only the specified skills\n   - If `--skip` specified: Run all skills except the specified ones\n   - Store the list of skills to execute for later phases\n\n4. **Example argument parsing**:\n   ```bash\n   # Parse: /sosreport:analyze /path/sos.tar.gz --only logs,network\n   # Result: Run only logs-analysis and network-analysis skills\n\n   # Parse: /sosreport:analyze /path/sos.tar.gz --skip resources\n   # Result: Run logs, network, and system-config (skip resources)\n\n   # Parse: /sosreport:analyze /path/sos.tar.gz\n   # Result: Run all skills (comprehensive analysis)\n   ```\n\n### 2. Extract and Validate Sosreport\n\n1. **Check if path exists**\n   - Verify the provided path points to a valid file or directory\n   - If file doesn't exist, return error with helpful message\n\n2. **Extract archive if needed**\n   - If path is a `.tar.gz` or `.tar.xz` file:\n     - Create extraction directory: `.work/sosreport-analyze/{timestamp}/`\n     - Extract archive: `tar -xf <path> -C .work/sosreport-analyze/{timestamp}/`\n     - Store extracted directory path for analysis\n   - If path is already a directory:\n     - Verify it's a valid sosreport directory (check for `sos_commands/`, `sos_logs/`, etc.)\n     - Use the directory directly\n\n3. **Identify sosreport structure**\n   - Locate the root directory (usually has format `sosreport-{hostname}-{date}/`)\n   - Verify expected directories exist: `sos_commands/`, `sos_logs/`, `sos_reports/`\n\n### 3. Analyze System Logs\n\n**Run condition**: Only if `logs` area is selected (or no filters specified)\n**Detailed implementation**: See `plugins/sosreport/skills/logs-analysis/SKILL.md`\n\nPerform comprehensive log analysis including:\n- Journald logs (journalctl output)\n- System logs (messages, dmesg, secure)\n- Application-specific logs\n- Error counting and categorization\n- Timeline of critical events\n- OOM killer events, kernel panics, segfaults\n\n**Key outputs**:\n- Error statistics by severity\n- Top error messages by frequency\n- Critical findings with timestamps\n- Log file locations for investigation\n\n### 4. Analyze Resource Usage\n\n**Run condition**: Only if `resources` area is selected (or no filters specified)\n**Detailed implementation**: See `plugins/sosreport/skills/resource-analysis/SKILL.md`\n\nPerform resource analysis including:\n- Memory usage and pressure indicators\n- CPU information and load averages\n- Disk usage and I/O errors\n- Process analysis (top consumers, zombies)\n- Resource exhaustion patterns\n\n**Key outputs**:\n- Memory usage metrics and swap status\n- CPU count and load per CPU\n- Filesystems near capacity\n- Top CPU and memory-consuming processes\n- Resource-related issues and recommendations\n\n### 5. Analyze Network Configuration\n\n**Run condition**: Only if `network` area is selected (or no filters specified)\n**Detailed implementation**: See `plugins/sosreport/skills/network-analysis/SKILL.md`\n\nPerform network analysis including:\n- Network interface configuration and status\n- Routing table and default gateway\n- Active connections and listening services\n- Firewall rules (firewalld/iptables)\n- DNS configuration and hostname resolution\n- Network errors from logs\n\n**Key outputs**:\n- Interface status with IP addresses\n- Routing configuration\n- Connection statistics by state\n- Firewall configuration summary\n- DNS and hostname settings\n- Network-related errors and issues\n\n### 6. Analyze Installed Packages and System Configuration\n\n**Run condition**: Only if `system-config` area is selected (or no filters specified)\n**Detailed implementation**: See `plugins/sosreport/skills/system-config-analysis/SKILL.md`\n\nPerform system configuration analysis including:\n- OS version and kernel information\n- Installed package versions\n- Systemd service status\n- Failed services with reasons\n- SELinux/AppArmor configuration and denials\n- Kernel parameters and resource limits\n\n**Key outputs**:\n- System information summary\n- Key package versions\n- Failed services with failure reasons\n- SELinux status and denial count\n- Configuration issues and recommendations\n\n### 7. Generate Interactive Summary\n\n1. **Create findings structure**\n   - Organize findings by category (Critical, High, Medium, Low, Info)\n   - Include only findings from the selected analysis areas\n   - For each finding, include:\n     - Severity level\n     - Category (logs, resources, network, packages, config)\n     - Description of the issue\n     - Evidence (file paths, log snippets, metrics)\n     - Recommended actions\n\n2. **Display summary in terminal**\n   - Show executive summary with key statistics\n   - List critical and high-severity findings\n   - Provide file paths for detailed investigation\n   - Include timeline of significant events\n   - Suggest next steps for troubleshooting\n\n3. **Format output**\n   ```bash\n   SOSREPORT ANALYSIS SUMMARY\n   ==========================\n\n   System: {hostname}\n   Report Date: {date}\n   OS: {os_version}\n   Kernel: {kernel_version}\n\n   CRITICAL ISSUES (count)\n   -----------------------\n   - [Issue description with file reference]\n\n   HIGH PRIORITY (count)\n   ---------------------\n   - [Issue description with file reference]\n\n   MEDIUM PRIORITY (count)\n   -----------------------\n   - [Issue description with file reference]\n\n   RESOURCE SUMMARY\n   ----------------\n   - Memory: X GB used / Y GB total (Z% used)\n   - Disk: Most full filesystem at X%\n   - Load Average: X.XX, X.XX, X.XX\n\n   TOP ERRORS IN LOGS\n   ------------------\n   1. [Error message] (count occurrences)\n   2. [Error message] (count occurrences)\n\n   FAILED SERVICES\n   ---------------\n   - [service name]: [reason]\n\n   RECOMMENDATIONS\n   ---------------\n   1. [Actionable recommendation]\n   2. [Actionable recommendation]\n\n   ANALYSIS LOCATION\n   -----------------\n   Extracted to: {extraction_path}\n   ```\n\n4. **Interactive drill-down**\n   - Offer to explore specific areas in more detail\n   - Allow user to ask follow-up questions about findings\n   - Provide file paths for manual investigation\n\n## Return Value\n\n- **Format**: Interactive summary displayed in terminal with categorized findings\n- **Exit code**:\n  - 0 if analysis completes successfully\n  - 1 if sosreport path is invalid\n  - 2 if sosreport structure is malformed\n\n## Examples\n\n1. **Comprehensive analysis (default)**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport-server01-2024-01-15.tar.xz\n   ```\n\n   Extracts archive to `.work/sosreport-analyze/{timestamp}/` and performs comprehensive analysis using all skills (logs, resources, network, system-config).\n\n2. **Analyze only logs and network**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport-server01-2024-01-15.tar.xz --only logs,network\n   ```\n\n   Performs only log analysis and network analysis. Useful when investigating connectivity or service issues without needing full resource analysis.\n\n3. **Skip resource analysis**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport.tar.gz --skip resources\n   ```\n\n   Performs all analysis except resource analysis. Useful when you already know resource metrics and want to focus on configuration and logs.\n\n4. **Quick log-only analysis**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport.tar.xz --only logs\n   ```\n\n   Performs only log analysis. Fastest option for quickly identifying errors and critical events without analyzing configuration or resources.\n\n5. **Analyze extracted sosreport directory**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport-server01-2024-01-15/\n   ```\n\n   Analyzes an already extracted sosreport directory with comprehensive analysis.\n\n6. **Selective analysis on extracted directory**:\n   ```bash\n   /sosreport:analyze /tmp/sosreport-server01-2024-01-15/ --only system-config,network\n   ```\n\n   Analyzes only system configuration and network from an already extracted directory.\n\n7. **Follow-up investigation**:\n   ```bash\n   User: /sosreport:analyze /tmp/sosreport.tar.gz --only logs\n   Agent: [Shows log analysis summary]\n   User: Can you now analyze the resources as well?\n   Agent: /sosreport:analyze /tmp/sosreport.tar.gz --only resources\n   Agent: [Shows resource analysis]\n   ```\n\n## Notes\n\n- Sosreport structure varies by OS version and sosreport version\n- Command handles both compressed archives and extracted directories\n- Analysis focuses on common issues but can be extended for specific use cases\n- For OpenShift/Kubernetes sosreports, additional pod/container analysis may be relevant\n- Large sosreports (>1GB) may take several minutes to analyze\n- **Selective analysis**: Use `--only` or `--skip` to run specific analysis areas for faster results\n- **Performance**: Running only needed analysis areas reduces analysis time significantly\n- **Valid areas**: `logs`, `resources`, `network`, `system-config`\n\n## Prerequisites\n\n1. **tar utility**: Required for extracting compressed sosreports\n   - Check: `which tar`\n   - Usually pre-installed on Linux/macOS\n\n2. **Sufficient disk space**: Extracted sosreports can be large\n   - Check available space: `df -h .work/`\n   - Recommend at least 2x the compressed archive size\n\n## See Also\n\n### Analysis Skills\n- **Logs Analysis**: `plugins/sosreport/skills/logs-analysis/SKILL.md` - Detailed guidance for analyzing system and application logs\n- **Resource Analysis**: `plugins/sosreport/skills/resource-analysis/SKILL.md` - Detailed guidance for analyzing memory, CPU, disk, and processes\n- **Network Analysis**: `plugins/sosreport/skills/network-analysis/SKILL.md` - Detailed guidance for analyzing network configuration and connectivity\n- **System Configuration Analysis**: `plugins/sosreport/skills/system-config-analysis/SKILL.md` - Detailed guidance for analyzing packages, services, and security settings\n\n### External Resources\n- Sosreport documentation: <https://github.com/sosreport/sos>\n- Red Hat sosreport guide: <https://access.redhat.com/solutions/3592>"
              },
              {
                "name": "/ovs-db",
                "description": "Analyze OVS data from sosreport (text files or database)",
                "path": "plugins/sosreport/commands/ovs-db.md",
                "frontmatter": {
                  "description": "Analyze OVS data from sosreport (text files or database)",
                  "argument-hint": "[sosreport-path] [--db] [--flows-only] [--query <json>]"
                },
                "content": "## Name\nsosreport:ovs-db\n\n## Synopsis\n```\n/sosreport:ovs-db [sosreport-path] [--db] [--flows-only] [--query <json>]\n```\n\n## Description\n\nThe `ovs-db` command analyzes Open vSwitch data collected in sosreports. It operates in four modes:\n\n1. **Default mode**: Full analysis - conf.db + all text files (requires ovsdb-tool)\n2. **Database mode** (`--db`): Database only - analyze conf.db (requires ovsdb-tool)\n3. **Text files mode** (`--flows-only`): Text files only - no ovsdb-tool needed!\n4. **Query mode** (`--query`): Run custom OVSDB JSON queries (requires ovsdb-tool)\n\n**What it analyzes:**\n\n### From Database (Default or `--db` Mode)\n- **System Information**: OVS version, DPDK settings, external IDs (from conf.db)\n- **Bridge Details**: Datapath type (kernel/userspace), fail mode, ports (from conf.db)\n- **Interface Inventory**: By type, with pod-to-interface mapping (from conf.db)\n\n### From Text Files (Default or `--flows-only` Mode)\n- **System Information**: OVS version, DPDK settings, external IDs (from `ovs-vsctl list Open_vSwitch`)\n- **Topology**: Bridges with ports grouped by type (from `ovs-vsctl show`)\n- **Bridge Details**: Datapath type, fail mode, CT zones (from `ovs-vsctl list bridge`)\n- **Interface Inventory**: By type, with pod-to-interface mapping (from `ovs-vsctl list interface`)\n- **OpenFlow Flows**: Flow counts, drop detection, top flows (from `ovs-ofctl dump-flows`)\n- **Port Statistics**: RX/TX drops and errors (from `ovs-ofctl dump-ports`)\n- **Tunnel Ports**: Configured tunnels (from `ovs-appctl tnl.ports.show`)\n- **Datapath Health**: Flow table usage vs limit (from `ovs-appctl upcall.show`)\n- **OVS Internal Stats**: Netlink, OpenFlow, OVSDB counters (from `ovs-appctl coverage.show`)\n\n### Custom Queries (`--query` Mode)\n- Direct OVSDB table queries (requires ovsdb-tool)\n\n**Modes of operation:**\n1. **Default**: Full analysis - conf.db + all text files (requires ovsdb-tool, falls back to text files if not available)\n2. **Database** (`--db`): Database only - analyze conf.db (requires ovsdb-tool)\n3. **Text Files** (`--flows-only`): Text files only - no ovsdb-tool needed\n4. **Query Mode** (`--query`): Run custom OVSDB JSON queries (requires ovsdb-tool)\n\n## Prerequisites\n\n**Default mode** (full analysis):\n- `ovsdb-tool` must be installed (from openvswitch package)\n- Falls back to `--flows-only` if ovsdb-tool not found\n- Sosreport with `sos_commands/openvswitch/` directory and `conf.db`\n\n**Database mode** (`--db`):\n- `ovsdb-tool` must be installed (from openvswitch package)\n  - Check: `which ovsdb-tool`\n  - Fedora/RHEL: `sudo dnf install openvswitch`\n  - Ubuntu/Debian: `sudo apt install openvswitch-common`\n- Sosreport with `conf.db` file\n\n**Text files mode** (`--flows-only`):\n- No special tools needed - works out of the box!\n- Sosreport with `sos_commands/openvswitch/` directory\n\n**Query mode** (`--query`):\n- `ovsdb-tool` must be installed\n- Incompatible with `--flows-only`\n\n**Sosreport Data:**\n\nThe sosreport should contain:\n```\nsosreport-hostname-date/\n etc/openvswitch/conf.db              (for --db mode)\n   OR var/lib/openvswitch/conf.db\n sos_commands/openvswitch/            (default mode)\n     ovs-vsctl_-t_5_show              (topology)\n     ovs-vsctl_-t_5_list_*            (tables)\n     ovs-ofctl_dump-flows_<bridge>    (flows)\n     ovs-ofctl_dump-ports_<bridge>    (port stats)\n     ovs-appctl_coverage.show         (internal stats)\n     ovs-appctl_upcall.show           (datapath health)\n     ...\n```\n\n**Analysis Script:**\n\nThe script is bundled with this plugin:\n```\n<plugin-root>/skills/ovs-db-analysis/scripts/analyze_ovs_db.py\n```\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Locate Analysis Script**:\n   ```bash\n   SCRIPT_PATH=$(find ~ -name \"analyze_ovs_db.py\" -path \"*/sosreport/skills/ovs-db-analysis/scripts/*\" 2>/dev/null | head -1)\n   \n   if [ -z \"$SCRIPT_PATH\" ]; then\n       echo \"ERROR: analyze_ovs_db.py script not found.\"\n       exit 1\n   fi\n   ```\n\n2. **Handle Input Path**:\n   - If sosreport archive (`.tar.gz`, `.tar.xz`): Extract to temporary directory\n   - If directory: Use directly\n   - If `conf.db` file: Use database mode automatically\n\n3. **Run Analysis**:\n   - **Default mode**: Parse text files in `sos_commands/openvswitch/`\n   - **--db mode**: Query `conf.db` using `ovsdb-tool`\n   - **--query mode**: Execute custom OVSDB query\n\n4. **Analyze Data**:\n   - Parse topology and system info\n   - Parse bridge and interface details\n   - Analyze OpenFlow flows (in default and `--flows-only` modes)\n   - Report drops, errors, and health indicators\n\n## Return Value\n\nThe command outputs structured analysis:\n\n```\n================================================================================\nOVS ANALYSIS - sosreport-hostname-2024-01-15\n================================================================================\nMode: Text file analysis (no ovsdb-tool required)\n\n================================================================================\nOVS SYSTEM INFORMATION\n================================================================================\n\n  Field                     Value\n  ------------------------- --------------------------------------------------\n  OVS Version               \"3.3.4-62.el9fdp\"\n  DB Version                \"8.5.0\"\n  System Type               rhcos\n  System Version            \"4.16\"\n  DPDK Initialized          false\n  Datapath Types            [netdev, system]\n\n  External IDs:\n    hostname: master2.example.com\n    ovn-encap-ip: 10.32.110.5\n    ovn-encap-type: geneve\n    ovn-bridge-mappings: physnet:br-ex\n\n================================================================================\nOVS TOPOLOGY\n================================================================================\n\n  System UUID: 7e9a3f70-86fa-4578-a849-4fd807a64a10\n  Total Bridges: 2\n\n  Bridge: br-ex\n    ports: 3\n      internal: br-ex\n      patch: patch-br-ex-to-br-int\n      system: nm-bond\n\n  Bridge: br-int\n    fail_mode: secure\n    datapath_type: system\n    ports: 12\n      geneve: 9 ports\n      internal: ovn-k8s-mp0, br-int\n      patch: patch-br-int-to-br-ex\n\n================================================================================\nBRIDGE DETAILS\n================================================================================\n\n  Bridge: br-int\n  ------------------------------------------------------------\n    Datapath: system (kernelspace)\n    Fail Mode: secure\n    Datapath ID: \"00005ac5dfc26094\"\n    Port Count: 12\n    CT Zones: 19\n\n================================================================================\nINTERFACE ANALYSIS\n================================================================================\n\n  Total Interfaces: 15\n\n  By Type:\n    geneve: 9 interfaces\n    internal: br-int, ovn-k8s-mp0, br-ex\n    patch: patch-br-ex-to-br-int, patch-br-int-to-br-ex\n    system: nm-bond\n\n  Pod Interfaces: 0\n  ----------------------------------------------------------------------\n    (none on this node)\n\n================================================================================\nOPENFLOW ANALYSIS\n================================================================================\n\n  Bridge: br-int\n  ----------------------------------------------------------------------\n  Total flows: 2,017\n  Flows with hits: 318\n  Drop flows: 150 (9 actively dropping)\n  Tables used: 53 (0-79)\n  Top tables by flow count:\n    Table 21: 200 flows\n    Table 13: 163 flows\n\n    ACTIVE DROP FLOWS (9):\n    table=40, priority=0, packets=8,105\n    table=79, priority=100, packets=1,356\n      match: ip,reg14=0x2,metadata=0x5,dl_src=00:62:0b:ea:b5:e0\n\n--------------------------------------------------------------------------------\nPORT STATISTICS\n--------------------------------------------------------------------------------\n\n  Bridge: br-int\n  Total ports: 12\n\n    Ports with drops/errors:\n    Port 1: drops=11, errors=0\n      RX: 852 pkts, 23,856 bytes\n      TX: 7 pkts, 826 bytes\n\n--------------------------------------------------------------------------------\nDATAPATH FLOW TABLE HEALTH\n--------------------------------------------------------------------------------\n\n  Current flows: 155 / 200,000 (0.1% used)\n  Average: 156, Max seen: 215\n   Flow table healthy\n\n--------------------------------------------------------------------------------\nOVS INTERNAL STATISTICS\n--------------------------------------------------------------------------------\n\n  METRIC                    DESCRIPTION                         TOTAL           RATE/s\n  ------------------------- ----------------------------------- --------------- ----------\n  netlink_sent              Netlink messages sent               46,153          12.8\n  netlink_received          Netlink messages received           56,078          15.5\n  txn_success               OVSDB transactions (success)        471             0.1\n```\n\n## Examples\n\n### Default Mode (Full Analysis)\n\n1. **Full analysis** (requires ovsdb-tool):\n   ```\n   /sosreport:ovs-db /tmp/sosreport-server01-2024-01-15/\n   ```\n   Analyzes conf.db + all text files. Falls back to text files if ovsdb-tool not installed.\n\n2. **Analyze from archive**:\n   ```\n   /sosreport:ovs-db /tmp/sosreport-server01-2024-01-15.tar.xz\n   ```\n   Extracts and runs full analysis.\n\n### Database Mode (`--db`)\n\n3. **Database only** (requires ovsdb-tool):\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --db\n   ```\n   Queries `conf.db` only - no flow analysis.\n\n4. **Analyze conf.db directly**:\n   ```\n   /sosreport:ovs-db /var/lib/openvswitch/conf.db\n   ```\n   Automatically uses database mode.\n\n### Text Files Mode (`--flows-only`)\n\n5. **Text files only** (no ovsdb-tool needed):\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --flows-only\n   ```\n   Parses all text files in `sos_commands/openvswitch/`.\n\n### Query Mode (`--query`)\n\n6. **Query all bridges**:\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Bridge\", \"where\":[], \"columns\":[\"name\",\"datapath_type\"]}]'\n   ```\n\n7. **Query VXLAN tunnels**:\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[[\"type\",\"==\",\"vxlan\"]], \"columns\":[\"name\",\"options\"]}]'\n   ```\n\n8. **Check interface errors**:\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[], \"columns\":[\"name\",\"error\",\"link_state\"]}]'\n   ```\n\n9. **Check DPDK configuration**:\n   ```\n   /sosreport:ovs-db /tmp/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Open_vSwitch\", \"where\":[], \"columns\":[\"dpdk_initialized\",\"other_config\"]}]'\n   ```\n\n## Error Handling\n\n**Missing ovsdb-tool (only for --db mode):**\n```\nError: ovsdb-tool not found. Please install openvswitch package.\n  Fedora/RHEL: sudo dnf install openvswitch\n  Ubuntu/Debian: sudo apt install openvswitch-common\n```\nSolution: Either install ovsdb-tool or use default mode (without `--db`).\n\n**sos_commands/openvswitch not found:**\n```\nError: sos_commands/openvswitch not found in /path/to/sosreport\n```\nSolution: Ensure the sosreport has OVS data collected.\n\n**conf.db not found (for --db mode):**\n```\nError: conf.db not found in /path/to/sosreport\n\nLooked for conf.db in:\n  - etc/openvswitch/conf.db\n  - var/lib/openvswitch/conf.db\n\nTip: Run without --db to analyze text files only\n```\n\n## Notes\n\n- **Default mode** runs full analysis (conf.db + text files), requires ovsdb-tool, falls back to text files if not available\n- **Database mode** (`--db`) analyzes conf.db only, requires `ovsdb-tool`\n- **Text files mode** (`--flows-only`) parses text files only - no special tools needed\n- **Query mode** (`--query`) runs raw OVSDB JSON queries, requires ovsdb-tool, incompatible with `--flows-only`\n- **Pod-to-Interface Mapping**: Uses `external_ids` and interface naming conventions\n- **Drop Detection**: Identifies flows with `actions=drop` that have packet hits\n- **Datapath Health**: Checks flow table usage vs limit from upcall stats\n- **Query Format**: Accepts raw OVSDB JSON queries in format `[\"Open_vSwitch\", {\"op\":\"select\", ...}]`\n\n## Use Cases\n\n1. **Troubleshoot Packet Drops**:\n   - Run default analysis to see active drop flows\n   - Check port statistics for RX/TX drops\n\n2. **Check Datapath Health**:\n   - Review flow table usage vs limit\n   - If usage > 90%, flows are being evicted too aggressively\n\n3. **Map Pods to Interfaces**:\n   - See pod-to-OVS interface mapping\n   - Find which ofport a pod uses\n\n4. **Investigate DPDK Configuration**:\n   - Check DPDK initialization status\n   - Review datapath types (kernelspace vs userspace)\n\n5. **Debug OVN Connectivity**:\n   - Check external_ids for OVN configuration\n   - Review tunnel endpoints (geneve ports)\n   - Verify patch port connections between bridges\n\n6. **Audit Configuration**:\n   - Review all bridges and their ports\n   - Check fail modes and CT zones\n   - Review OVS version and system info\n\n## Arguments\n\n- **$1** (sosreport-path): Required. Path to sosreport archive (`.tar.gz`, `.tar.xz`), extracted directory, or direct `conf.db` file.\n- **--db**: Optional. Database only mode - analyze conf.db only (requires ovsdb-tool).\n- **--flows-only**: Optional. Text files only mode - no ovsdb-tool needed.\n- **--query, -q** (json-query): Optional. Run a raw OVSDB JSON query (requires ovsdb-tool, incompatible with `--flows-only`).\n\n## See Also\n\n- **OVN Database Analysis**: `/must-gather:ovn-dbs` - For analyzing OVN Northbound/Southbound databases\n- **Sosreport Analysis**: `/sosreport:analyze` - Comprehensive sosreport analysis\n- **Network Analysis**: `/sosreport:analyze --only network` - Network-focused analysis"
              }
            ],
            "skills": [
              {
                "name": "Logs Analysis",
                "description": "Analyze system and application log data from sosreport archives, extracting error patterns, kernel panics, OOM events, service failures, and application crashes from journald logs and traditional log files within the sosreport directory structure to identify root causes of system failures and issues",
                "path": "plugins/sosreport/skills/logs-analysis/SKILL.md",
                "frontmatter": {
                  "name": "Logs Analysis",
                  "description": "Analyze system and application log data from sosreport archives, extracting error patterns, kernel panics, OOM events, service failures, and application crashes from journald logs and traditional log files within the sosreport directory structure to identify root causes of system failures and issues"
                },
                "content": "# Logs Analysis Skill\n\nThis skill provides detailed guidance for analyzing logs from sosreport archives, including journald logs, system logs, kernel messages, and application logs.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing the `/sosreport:analyze` command's log analysis phase\n- Investigating specific log-related errors or warnings in a sosreport\n- Performing deep-dive analysis of system failures from logs\n- Identifying patterns and root causes in system logs\n\n## Prerequisites\n\n- Sosreport archive must be extracted to a working directory\n- Path to the sosreport root directory must be known\n- Basic understanding of Linux log structure and journald\n\n## Key Log Locations in Sosreport\n\nSosreports contain logs in several locations:\n\n1. **Journald logs**: `sos_commands/logs/journalctl_*`\n   - `journalctl_--no-pager_--boot` - Current boot logs\n   - `journalctl_--no-pager` - All available logs\n   - `journalctl_--no-pager_--priority_err` - Error priority logs\n\n2. **Traditional system logs**: `var/log/`\n   - `messages` - System-level messages\n   - `dmesg` - Kernel ring buffer\n   - `secure` - Authentication and security logs\n   - `cron` - Cron job logs\n\n3. **Application logs**: `var/log/` (varies by application)\n   - `httpd/` - Apache logs\n   - `nginx/` - Nginx logs\n   - `audit/audit.log` - SELinux audit logs\n\n## Implementation Steps\n\n### Step 1: Identify Available Log Sources\n\n1. **Check for journald logs**:\n   ```bash\n   ls -la sos_commands/logs/journalctl_* 2>/dev/null || echo \"No journald logs found\"\n   ```\n\n2. **Check for traditional system logs**:\n   ```bash\n   ls -la var/log/{messages,dmesg,secure} 2>/dev/null || echo \"No traditional logs found\"\n   ```\n\n3. **Identify application-specific logs**:\n   ```bash\n   find var/log/ -type f -name \"*.log\" 2>/dev/null | head -20\n   ```\n\n### Step 2: Analyze Journald Logs\n\n1. **Parse journalctl output for error patterns**:\n   ```bash\n   # Look for common error indicators\n   grep -iE \"(error|failed|failure|critical|panic|segfault|oom)\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -100\n   ```\n\n2. **Identify OOM (Out of Memory) killer events**:\n   ```bash\n   grep -i \"out of memory\\|oom.*kill\" sos_commands/logs/journalctl_--no-pager 2>/dev/null\n   ```\n\n3. **Find kernel panics**:\n   ```bash\n   grep -i \"kernel panic\\|bug:\\|oops:\" sos_commands/logs/journalctl_--no-pager 2>/dev/null\n   ```\n\n4. **Check for segmentation faults**:\n   ```bash\n   grep -i \"segfault\\|sigsegv\\|core dump\" sos_commands/logs/journalctl_--no-pager 2>/dev/null\n   ```\n\n5. **Extract service failures**:\n   ```bash\n   grep -i \"failed to start\\|failed with result\" sos_commands/logs/journalctl_--no-pager 2>/dev/null\n   ```\n\n### Step 3: Analyze System Logs (var/log)\n\n1. **Check messages for errors**:\n   ```bash\n   # If file exists and is readable\n   if [ -f var/log/messages ]; then\n     grep -iE \"(error|failed|failure|critical)\" var/log/messages | tail -100\n   fi\n   ```\n\n2. **Check dmesg for hardware issues**:\n   ```bash\n   if [ -f var/log/dmesg ]; then\n     grep -iE \"(error|fail|warning|i/o error|bad sector)\" var/log/dmesg\n   fi\n   ```\n\n3. **Analyze authentication logs**:\n   ```bash\n   if [ -f var/log/secure ]; then\n     grep -iE \"(failed|failure|invalid|denied)\" var/log/secure | tail -50\n   fi\n   ```\n\n### Step 4: Count and Categorize Errors\n\n1. **Count errors by severity**:\n   ```bash\n   # Critical errors\n   grep -ic \"critical\\|panic\\|fatal\" sos_commands/logs/journalctl_--no-pager 2>/dev/null || echo \"0\"\n\n   # Errors\n   grep -ic \"error\" sos_commands/logs/journalctl_--no-pager 2>/dev/null || echo \"0\"\n\n   # Warnings\n   grep -ic \"warning\\|warn\" sos_commands/logs/journalctl_--no-pager 2>/dev/null || echo \"0\"\n   ```\n\n2. **Find most frequent error messages**:\n   ```bash\n   grep -iE \"(error|failed)\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | \\\n     sed 's/^.*\\]: //' | \\\n     sort | uniq -c | sort -rn | head -10\n   ```\n\n3. **Extract timestamps for error timeline**:\n   ```bash\n   # Get first and last error timestamps\n   grep -i \"error\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | \\\n     head -1 | awk '{print $1, $2, $3}'\n   grep -i \"error\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | \\\n     tail -1 | awk '{print $1, $2, $3}'\n   ```\n\n### Step 5: Analyze Application-Specific Logs\n\n1. **Identify application logs**:\n   ```bash\n   find var/log/ -type f \\( -name \"*.log\" -o -name \"*_log\" \\) 2>/dev/null\n   ```\n\n2. **Check for stack traces and exceptions**:\n   ```bash\n   # Python tracebacks\n   grep -A 10 \"Traceback (most recent call last)\" var/log/*.log 2>/dev/null | head -50\n\n   # Java exceptions\n   grep -B 2 -A 10 \"Exception\\|Error:\" var/log/*.log 2>/dev/null | head -50\n   ```\n\n3. **Look for common application errors**:\n   ```bash\n   # Database connection errors\n   grep -i \"connection.*refused\\|connection.*timeout\\|database.*error\" var/log/*.log 2>/dev/null\n\n   # HTTP/API errors\n   grep -E \"HTTP [45][0-9]{2}|status.*[45][0-9]{2}\" var/log/*.log 2>/dev/null | head -20\n   ```\n\n### Step 6: Generate Log Analysis Summary\n\nCreate a structured summary with the following information:\n\n1. **Error Statistics**:\n   - Total critical errors\n   - Total errors\n   - Total warnings\n   - Time range of errors (first to last)\n\n2. **Critical Findings**:\n   - Kernel panics (with timestamps)\n   - OOM killer events (with victim processes)\n   - Segmentation faults (with process names)\n   - Service failures (with service names)\n\n3. **Top Error Messages** (sorted by frequency):\n   - Error message\n   - Count\n   - First occurrence timestamp\n   - Affected component/service\n\n4. **Application-Specific Issues**:\n   - Stack traces found\n   - Database errors\n   - Network/connectivity errors\n   - Authentication failures\n\n5. **Log File Locations**:\n   - Provide paths to specific log files for manual investigation\n   - Indicate which logs contain the most relevant information\n\n## Error Handling\n\n1. **Missing log files**:\n   - If journalctl logs are missing, fall back to var/log/* files\n   - If traditional logs are missing, document this in the summary\n   - Some sosreports may have limited logs due to collection parameters\n\n2. **Large log files**:\n   - For files larger than 100MB, sample the beginning and end\n   - Use `head -n 10000` and `tail -n 10000` to avoid memory issues\n   - Inform user that analysis is based on sampling\n\n3. **Compressed logs**:\n   - Check for `.gz` files in `var/log/`\n   - Use `zgrep` instead of `grep` for compressed files\n   - Example: `zgrep -i \"error\" var/log/messages*.gz`\n\n4. **Binary log formats**:\n   - Some logs may be in binary format (e.g., journald binary logs)\n   - Rely on `sos_commands/logs/journalctl_*` text outputs\n   - Do not attempt to parse binary files directly\n\n## Output Format\n\nThe log analysis should produce:\n\n```bash\nLOG ANALYSIS SUMMARY\n====================\n\nTime Range: {first_log_entry} to {last_log_entry}\n\nERROR STATISTICS\n----------------\nCritical: {count}\nErrors: {count}\nWarnings: {count}\n\nCRITICAL FINDINGS\n-----------------\nKernel Panics: {count}\n  - {timestamp}: {panic_message}\n\nOOM Killer Events: {count}\n  - {timestamp}: Killed {process_name} (PID: {pid})\n\nSegmentation Faults: {count}\n  - {timestamp}: {process_name} segfaulted\n\nService Failures: {count}\n  - {service_name}: {failure_reason}\n\nTOP ERROR MESSAGES\n------------------\n1. [{count}x] {error_message}\n   First seen: {timestamp}\n   Component: {component}\n\n2. [{count}x] {error_message}\n   First seen: {timestamp}\n   Component: {component}\n\nAPPLICATION ERRORS\n------------------\nStack Traces: {count} found in {log_files}\nDatabase Errors: {count}\nNetwork Errors: {count}\nAuth Failures: {count}\n\nLOG FILES FOR INVESTIGATION\n---------------------------\n- Primary: {sosreport_path}/sos_commands/logs/journalctl_--no-pager\n- System: {sosreport_path}/var/log/messages\n- Kernel: {sosreport_path}/var/log/dmesg\n- Security: {sosreport_path}/var/log/secure\n- Application: {sosreport_path}/var/log/{app_specific}\n\nRECOMMENDATIONS\n---------------\n1. {actionable_recommendation_based_on_findings}\n2. {actionable_recommendation_based_on_findings}\n```\n\n## Examples\n\n### Example 1: OOM Killer Analysis\n\n```bash\n# Detect OOM events\ngrep -B 5 -A 15 \"Out of memory\" sos_commands/logs/journalctl_--no-pager\n\n# Output interpretation:\n# - Which process was killed\n# - Memory state at the time\n# - What triggered the OOM\n```\n\n### Example 2: Service Failure Pattern\n\n```bash\n# Find failed services\ngrep \"failed to start\\|Failed with result\" sos_commands/logs/journalctl_--no-pager | \\\n  awk -F'[][]' '{print $2}' | sort | uniq -c | sort -rn\n\n# This shows which services failed most frequently\n```\n\n### Example 3: Timeline of Errors\n\n```bash\n# Create error timeline\ngrep -i \"error\\|fail\" sos_commands/logs/journalctl_--no-pager | \\\n  awk '{print $1, $2, $3}' | sort | uniq -c\n\n# Shows error frequency over time\n```\n\n## Tips for Effective Analysis\n\n1. **Start with critical errors**: Focus on panics, OOMs, and segfaults first\n2. **Look for patterns**: Repeated errors often indicate systemic issues\n3. **Check timestamps**: Correlate errors with the reported incident time\n4. **Consider context**: Read surrounding log lines for context\n5. **Cross-reference**: Correlate log findings with resource analysis\n6. **Be thorough**: Check both journald and traditional logs\n7. **Document findings**: Note file paths and line numbers for reference\n\n## Common Log Patterns to Look For\n\n1. **OOM Killer**: \"Out of memory: Kill process\"  Memory pressure issue\n2. **Segfault**: \"segfault at\"  Application crash, possible bug\n3. **I/O Error**: \"I/O error\" in dmesg  Hardware or filesystem issue\n4. **Connection Refused**: \"Connection refused\"  Service not running or firewall\n5. **Permission Denied**: \"Permission denied\"  SELinux, file permissions, or ACL issue\n6. **Timeout**: \"timeout\"  Network or resource contention\n7. **Failed to start**: \"Failed to start\"  Service configuration or dependency issue\n\n## See Also\n\n- Resource Analysis Skill: For correlating log errors with resource constraints\n- System Configuration Analysis Skill: For investigating service failures\n- Network Analysis Skill: For investigating connectivity errors"
              },
              {
                "name": "Network Analysis",
                "description": "Analyze network configuration data from sosreport archives, extracting interface configurations, routing tables, active connections, firewall rules (firewalld/iptables), and DNS settings from the sosreport directory structure to diagnose network connectivity and configuration issues",
                "path": "plugins/sosreport/skills/network-analysis/SKILL.md",
                "frontmatter": {
                  "name": "Network Analysis",
                  "description": "Analyze network configuration data from sosreport archives, extracting interface configurations, routing tables, active connections, firewall rules (firewalld/iptables), and DNS settings from the sosreport directory structure to diagnose network connectivity and configuration issues"
                },
                "content": "# Network Analysis Skill\n\nThis skill provides detailed guidance for analyzing network configuration and connectivity from sosreport archives, including interfaces, routing, firewall rules, and DNS configuration.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing the `/sosreport:analyze` command's network analysis phase\n- Investigating network connectivity issues\n- Diagnosing firewall or routing problems\n- Verifying network configuration\n\n## Prerequisites\n\n- Sosreport archive must be extracted to a working directory\n- Path to the sosreport root directory must be known\n- Understanding of Linux networking concepts\n\n## Key Network Data Locations in Sosreport\n\n1. **Network Interfaces**:\n   - `sos_commands/networking/ip_-o_addr` - IP addresses\n   - `sos_commands/networking/ip_link` - Link status\n   - `sos_commands/networking/ip_-s_link` - Link statistics with errors\n   - `etc/sysconfig/network-scripts/` - Network configuration files (RHEL)\n\n2. **Routing**:\n   - `sos_commands/networking/ip_route` - Routing table\n   - `sos_commands/networking/ip_-6_route` - IPv6 routing table\n   - `proc/net/route` - Kernel routing table\n\n3. **Network Connections**:\n   - `sos_commands/networking/netstat_-neopa` - Active connections\n   - `sos_commands/networking/ss_-tupna` - Socket statistics\n   - `proc/net/tcp` - TCP connections\n   - `proc/net/udp` - UDP connections\n\n4. **Firewall**:\n   - `sos_commands/firewalld/` - Firewalld configuration\n   - `sos_commands/iptables/iptables_-vnxL` - iptables rules\n   - `sos_commands/nftables/` - nftables configuration\n\n5. **DNS and Resolution**:\n   - `etc/resolv.conf` - DNS servers\n   - `etc/hosts` - Static hostname mappings\n   - `etc/nsswitch.conf` - Name resolution order\n\n6. **Network Services**:\n   - `sos_commands/networking/networkmanager_info` - NetworkManager status\n   - `systemctl status NetworkManager` output\n\n## Implementation Steps\n\n### Step 1: Analyze Network Interfaces\n\n1. **List all network interfaces**:\n   ```bash\n   if [ -f sos_commands/networking/ip_-o_addr ]; then\n     cat sos_commands/networking/ip_-o_addr\n   fi\n   ```\n\n2. **Check interface states**:\n   ```bash\n   if [ -f sos_commands/networking/ip_link ]; then\n     # Look for interface states (UP/DOWN)\n     grep -E \"^[0-9]+:\" sos_commands/networking/ip_link\n   fi\n   ```\n\n3. **Parse interface information**:\n   - Interface name (eth0, ens192, etc.)\n   - State (UP/DOWN)\n   - IP addresses (IPv4 and IPv6)\n   - MAC address\n   - MTU size\n\n4. **Check for interface errors**:\n   ```bash\n   if [ -f sos_commands/networking/ip_-s_link ]; then\n     # Look for RX/TX errors, drops, overruns\n     cat sos_commands/networking/ip_-s_link\n   fi\n   ```\n\n5. **Identify interface issues**:\n   - Interfaces with no IP address (when expected)\n   - Interfaces in DOWN state (when should be UP)\n   - High error counts (RX/TX errors, drops)\n   - Duplicate IP addresses\n   - MTU mismatches\n\n### Step 2: Analyze Routing Configuration\n\n1. **Check default route**:\n   ```bash\n   if [ -f sos_commands/networking/ip_route ]; then\n     grep \"^default\" sos_commands/networking/ip_route || echo \"No default route found\"\n   fi\n   ```\n\n2. **Review routing table**:\n   ```bash\n   if [ -f sos_commands/networking/ip_route ]; then\n     cat sos_commands/networking/ip_route\n   fi\n   ```\n\n3. **Check IPv6 routing**:\n   ```bash\n   if [ -f sos_commands/networking/ip_-6_route ]; then\n     cat sos_commands/networking/ip_-6_route\n   fi\n   ```\n\n4. **Identify routing issues**:\n   - Missing default route\n   - Multiple default routes (conflicting)\n   - Incorrect gateway addresses\n   - Route to nowhere (unreachable gateway)\n\n### Step 3: Analyze Network Connectivity\n\n1. **Check active connections**:\n   ```bash\n   if [ -f sos_commands/networking/netstat_-neopa ]; then\n     cat sos_commands/networking/netstat_-neopa\n   elif [ -f sos_commands/networking/ss_-tupna ]; then\n     cat sos_commands/networking/ss_-tupna\n   fi\n   ```\n\n2. **Count connections by state**:\n   ```bash\n   # Count TCP connection states\n   if [ -f sos_commands/networking/netstat_-neopa ]; then\n     grep \"^tcp\" sos_commands/networking/netstat_-neopa | awk '{print $6}' | sort | uniq -c\n   fi\n   ```\n\n3. **Find listening services**:\n   ```bash\n   # Show what's listening on which ports\n   if [ -f sos_commands/networking/netstat_-neopa ]; then\n     grep \"LISTEN\" sos_commands/networking/netstat_-neopa\n   fi\n   ```\n\n4. **Check for connection issues**:\n   - Excessive TIME_WAIT connections\n   - Many connections in SYN_SENT (connection attempts failing)\n   - High number of CLOSE_WAIT (application not closing)\n   - Port conflicts (multiple services on same port)\n\n### Step 4: Analyze Firewall Configuration\n\n1. **Check if firewalld is active**:\n   ```bash\n   if [ -d sos_commands/firewalld ]; then\n     # Firewalld is present\n     if [ -f sos_commands/firewalld/firewall-cmd_--list-all-zones ]; then\n       cat sos_commands/firewalld/firewall-cmd_--list-all-zones\n     fi\n   fi\n   ```\n\n2. **Review iptables rules**:\n   ```bash\n   if [ -f sos_commands/iptables/iptables_-vnxL ]; then\n     cat sos_commands/iptables/iptables_-vnxL\n   fi\n   ```\n\n3. **Check firewall zones and rules**:\n   - Active zones\n   - Allowed services\n   - Allowed ports\n   - Rich rules\n   - Drop/reject policies\n\n4. **Identify firewall issues**:\n   - Required ports blocked\n   - Overly permissive rules (any any accept)\n   - Conflicting rules\n   - Missing rules for services\n\n### Step 5: Analyze DNS Configuration\n\n1. **Check DNS servers**:\n   ```bash\n   if [ -f etc/resolv.conf ]; then\n     cat etc/resolv.conf\n   fi\n   ```\n\n2. **Review /etc/hosts**:\n   ```bash\n   if [ -f etc/hosts ]; then\n     # Show non-comment, non-empty lines\n     grep -v \"^#\\|^$\" etc/hosts\n   fi\n   ```\n\n3. **Check hostname resolution**:\n   ```bash\n   # Check hostname\n   if [ -f hostname ]; then\n     cat hostname\n   fi\n\n   # Check FQDN\n   if [ -f etc/hostname ]; then\n     cat etc/hostname\n   fi\n   ```\n\n4. **Verify nsswitch configuration**:\n   ```bash\n   if [ -f etc/nsswitch.conf ]; then\n     grep \"^hosts:\" etc/nsswitch.conf\n   fi\n   ```\n\n5. **Identify DNS issues**:\n   - No DNS servers configured\n   - Unreachable DNS servers (check connectivity in logs)\n   - Incorrect search domains\n   - Hostname resolution failures in logs\n\n### Step 6: Check for Network Errors in Logs\n\n1. **Look for network-related errors**:\n   ```bash\n   # Connection refused errors\n   grep -i \"connection refused\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n\n   # Timeout errors\n   grep -i \"timeout\\|timed out\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n\n   # Network unreachable\n   grep -i \"network.*unreachable\\|no route to host\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n\n   # DNS resolution failures\n   grep -i \"could not resolve\\|dns.*fail\\|name resolution\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n2. **Check for link state changes**:\n   ```bash\n   grep -i \"link.*up\\|link.*down\\|carrier.*lost\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n3. **Look for network device errors**:\n   ```bash\n   grep -i \"network.*error\\|eth[0-9].*error\\|transmit.*error\" var/log/dmesg 2>/dev/null\n   ```\n\n### Step 7: Generate Network Analysis Summary\n\nCreate a structured summary with the following sections:\n\n1. **Interface Summary**:\n   - List of all interfaces with status\n   - IP addresses assigned\n   - Interface errors/drops\n   - Link speeds and duplex settings\n\n2. **Routing Summary**:\n   - Default gateway\n   - Number of routes\n   - Any routing anomalies\n\n3. **Connectivity Summary**:\n   - Active connection count by state\n   - Listening services and ports\n   - Connection issues detected\n\n4. **Firewall Summary**:\n   - Firewall type (firewalld/iptables/nftables)\n   - Active zones (if firewalld)\n   - Key allowed services/ports\n   - Potential blocking rules\n\n5. **DNS Summary**:\n   - DNS servers configured\n   - Search domains\n   - Hostname configuration\n   - DNS resolution issues\n\n6. **Network Issues**:\n   - Critical network problems\n   - Warnings and recommendations\n   - Evidence from logs\n\n## Error Handling\n\n1. **Missing network files**:\n   - Different sosreport versions may have different file names\n   - Fall back to alternative files (netstat vs ss)\n   - Document missing data in summary\n\n2. **Multiple network configurations**:\n   - System may use NetworkManager, systemd-networkd, or traditional ifcfg\n   - Identify which is in use and analyze accordingly\n\n3. **IPv6 presence**:\n   - Check if IPv6 is enabled\n   - Analyze IPv6 configuration if present\n   - Note if IPv6 is disabled when expected\n\n## Output Format\n\nThe network analysis should produce:\n\n```bash\nNETWORK CONFIGURATION SUMMARY\n==============================\n\nNETWORK INTERFACES\n------------------\nInterface: {name}\n  State: {UP|DOWN}\n  IP Addresses: {ipv4}, {ipv6}\n  MAC: {mac_address}\n  MTU: {mtu}\n  RX Errors: {rx_errors} packets, {rx_dropped} dropped\n  TX Errors: {tx_errors} packets, {tx_dropped} dropped\n  Status: {OK|WARNING|CRITICAL}\n\nROUTING\n-------\nDefault Gateway: {gateway_ip} via {interface}\nTotal Routes: {count}\n\nKey Routes:\n  {destination} via {gateway} dev {interface}\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {routing_issue_description}\n\nCONNECTIVITY\n------------\nTotal Active Connections: {count}\n\nConnections by State:\n  ESTABLISHED: {count}\n  TIME_WAIT: {count}\n  CLOSE_WAIT: {count}\n  SYN_SENT: {count}\n\nListening Services:\n  {port}/{protocol} - {service_name} (PID {pid})\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {connectivity_issue_description}\n\nFIREWALL\n--------\nType: {firewalld|iptables|nftables|none}\nDefault Zone: {zone_name} (if firewalld)\n\nAllowed Services: {service1}, {service2}, ...\nAllowed Ports: {port1/protocol}, {port2/protocol}, ...\n\nActive Rules Count: {count}\n\nStatus: {OK|WARNING|CRITICAL}\nPotential Issues:\n  - {firewall_issue_description}\n\nDNS CONFIGURATION\n-----------------\nDNS Servers: {dns1}, {dns2}, {dns3}\nSearch Domains: {domain1}, {domain2}\nHostname: {hostname}\nFQDN: {fqdn}\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {dns_issue_description}\n\nNETWORK ERRORS FROM LOGS\n------------------------\nConnection Refused: {count} occurrences\nTimeouts: {count} occurrences\nDNS Failures: {count} occurrences\nLink State Changes: {count} occurrences\n\nRecent Network Errors:\n  {timestamp}: {error_message}\n\nCRITICAL NETWORK ISSUES\n-----------------------\n{severity}: {issue_description}\n  Evidence: {file_path_or_log_excerpt}\n  Impact: {impact_description}\n  Recommendation: {remediation_action}\n\nRECOMMENDATIONS\n---------------\n1. {actionable_recommendation}\n2. {actionable_recommendation}\n\nDATA SOURCES\n------------\n- Interfaces: {sosreport_path}/sos_commands/networking/ip_-o_addr\n- Routes: {sosreport_path}/sos_commands/networking/ip_route\n- Connections: {sosreport_path}/sos_commands/networking/netstat_-neopa\n- Firewall: {sosreport_path}/sos_commands/firewalld/\n- DNS: {sosreport_path}/etc/resolv.conf\n```\n\n## Examples\n\n### Example 1: Interface Analysis\n\n```bash\n# Check interface IP addresses\n$ cat sos_commands/networking/ip_-o_addr\n1: lo    inet 127.0.0.1/8 scope host lo\n2: eth0  inet 192.168.1.100/24 brd 192.168.1.255 scope global eth0\n2: eth0  inet6 fe80::a00:27ff:fe4e:66a1/64 scope link\n\n# Check for errors\n$ cat sos_commands/networking/ip_-s_link | grep -A 4 \"eth0\"\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500\n    RX: bytes  packets  errors  dropped overrun mcast\n    15234567   98234    0       0       0       123\n    TX: bytes  packets  errors  dropped carrier collsns\n    8765432    54321    15      0       0       0\n\n# Interpretation: eth0 has 15 TX errors - investigate cable/switch\n```\n\n### Example 2: Firewall Rule Analysis\n\n```bash\n# Check firewalld active zone\n$ grep -A 20 \"public\" sos_commands/firewalld/firewall-cmd_--list-all-zones\npublic (active)\n  target: default\n  services: ssh dhcpv6-client http https\n  ports: 8080/tcp 9090/tcp\n  ...\n\n# Interpretation: HTTP/HTTPS allowed, custom ports 8080 and 9090 open\n```\n\n### Example 3: Connection State Issues\n\n```bash\n# Count connection states\n$ grep \"^tcp\" sos_commands/networking/netstat_-neopa | awk '{print $6}' | sort | uniq -c\n    234 ESTABLISHED\n   1523 TIME_WAIT\n     12 CLOSE_WAIT\n      5 SYN_SENT\n\n# Interpretation:\n# - Excessive TIME_WAIT (normal after closing connections)\n# - CLOSE_WAIT suggests application not properly closing sockets\n# - SYN_SENT indicates outbound connection attempts failing\n```\n\n## Tips for Effective Analysis\n\n1. **Check interface consistency**: Ensure IP addresses match expected configuration\n2. **Verify gateway reachability**: Default gateway should be on the same subnet\n3. **Look for asymmetric routing**: Packets in/out may take different paths\n4. **Check MTU settings**: MTU mismatches can cause packet fragmentation issues\n5. **Correlate with logs**: Network errors in logs often explain configuration issues\n6. **Consider network topology**: Understand expected network layout\n7. **Check both IPv4 and IPv6**: Be sure to check IPv6 if it's in use\n\n## Common Network Patterns and Issues\n\n1. **No default route**: \"Network unreachable\" errors, can't reach internet\n2. **Interface down**: \"Network is down\" errors, no connectivity\n3. **Duplicate IP**: ARP conflicts, intermittent connectivity\n4. **Firewall blocking**: \"Connection refused/timeout\" for specific ports\n5. **DNS failure**: Can't resolve hostnames, but IP connectivity works\n6. **Port exhaustion**: Too many TIME_WAIT connections, can't create new connections\n7. **MTU issues**: Large packets fail, small packets work (PMTUD failure)\n\n## Network Issue Severity Classification\n\n| Issue Type | Severity | Impact |\n|------------|----------|--------|\n| No network interface | Critical | Complete loss of connectivity |\n| No default route | Critical | No external connectivity |\n| Interface errors >1% | Warning | Potential packet loss |\n| Excessive TIME_WAIT | Warning | May indicate performance issue |\n| Missing DNS server | Critical | Name resolution failure |\n| Firewall blocking required port | High | Service unavailable |\n| IPv6 autoconfiguration failure | Low | IPv6 connectivity issue |\n\n## See Also\n\n- Logs Analysis Skill: For detailed network error log analysis\n- System Configuration Analysis Skill: For network service status\n- Resource Analysis Skill: For network I/O statistics"
              },
              {
                "name": "OVS Database and Flow Analysis",
                "description": "Analyze Open vSwitch data from sosreport",
                "path": "plugins/sosreport/skills/ovs-db-analysis/SKILL.md",
                "frontmatter": {
                  "name": "OVS Database and Flow Analysis",
                  "description": "Analyze Open vSwitch data from sosreport"
                },
                "content": "# OVS Database and Flow Analysis\n\nThis skill provides detailed analysis of Open vSwitch (OVS) data collected via sosreport. It operates in four modes:\n\n1. **Default mode**: Full analysis - conf.db + all text files (requires ovsdb-tool, falls back to text files if not available)\n2. **Database mode (`--db`)**: Database only - analyze conf.db (requires ovsdb-tool)\n3. **Text files mode (`--flows-only`)**: Text files only - no ovsdb-tool needed\n4. **Query mode (`--query`)**: Run custom OVSDB JSON queries (requires ovsdb-tool)\n\n## When to Use This Skill\n\nUse this skill when:\n- **Troubleshooting packet drops** - Find drop flows being hit\n- **Analyzing flow performance** - View top flows by packet count\n- Analyzing OVS configuration from a sosreport\n- Troubleshooting bridge, port, or interface issues\n- Investigating DPDK vs kernelspace datapath\n- Reviewing tunnel configurations (VXLAN, Geneve, GRE)\n- Checking port statistics (drops, errors, traffic)\n- Mapping Kubernetes pods to OVS interfaces\n\n## Prerequisites\n\n**Default mode** (full analysis):\n- **ovsdb-tool** must be installed (from openvswitch package)\n- Falls back to `--flows-only` if ovsdb-tool not found\n- Sosreport with `sos_commands/openvswitch/` directory and `conf.db`\n\n**Database mode** (`--db`):\n- **ovsdb-tool** must be installed (from openvswitch package)\n  - Check: `which ovsdb-tool`\n  - Fedora/RHEL: `sudo dnf install openvswitch`\n  - Ubuntu/Debian: `sudo apt install openvswitch-common`\n- Sosreport with `conf.db` file\n\n**Text files mode** (`--flows-only`):\n- No special tools needed - works out of the box!\n- Sosreport with `sos_commands/openvswitch/` directory\n\n**Query mode** (`--query`):\n- **ovsdb-tool** must be installed\n- Incompatible with `--flows-only`\n\n## File Locations in Sosreport\n\nThe sosreport collects:\n```\nsosreport-hostname-date/\n etc/openvswitch/conf.db              (OVS database - for --db mode)\n var/lib/openvswitch/conf.db          (alternate location)\n sos_commands/openvswitch/            (default mode uses these)\n     ovs-vsctl_-t_5_show              (topology)\n     ovs-vsctl_-t_5_list_bridge       (bridge details)\n     ovs-vsctl_-t_5_list_interface    (interface details)\n     ovs-vsctl_-t_5_list_Open_vSwitch (system info)\n     ovs-ofctl_dump-flows_<bridge>    (OpenFlow entries)\n     ovs-ofctl_dump-ports_<bridge>    (Port statistics)\n     ovs-appctl_coverage.show         (internal stats)\n     ovs-appctl_upcall.show           (datapath health)\n     ovs-appctl_tnl.ports.show_-v     (tunnel ports)\n     ...\n```\n\n## Analysis Script\n\nThe analysis script is located at:\n```\n<plugin-root>/skills/ovs-db-analysis/scripts/analyze_ovs_db.py\n```\n\n## Implementation Steps\n\n### Step 1: Locate the Analysis Script\n\n```bash\nSCRIPT_PATH=$(find ~ -name \"analyze_ovs_db.py\" -path \"*/sosreport/skills/ovs-db-analysis/scripts/*\" 2>/dev/null | head -1)\n\nif [ -z \"$SCRIPT_PATH\" ]; then\n    echo \"ERROR: analyze_ovs_db.py script not found.\"\n    exit 1\nfi\n```\n\n### Step 2: Run the Analysis\n\n```bash\n# Default: Full analysis - conf.db + text files (requires ovsdb-tool)\npython3 \"$SCRIPT_PATH\" /path/to/sosreport-hostname-date/\n\n# Analyze from archive\npython3 \"$SCRIPT_PATH\" /path/to/sosreport-hostname-date.tar.xz\n\n# Database only mode (requires ovsdb-tool)\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --db\n\n# Text files only mode (no ovsdb-tool needed)\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --flows-only\n```\n\n### Step 3: Custom Queries (Optional, requires --db)\n\nFor specific investigations, use raw OVSDB queries:\n\n```bash\n# Query all bridges\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Bridge\", \"where\":[], \"columns\":[\"name\",\"datapath_type\",\"fail_mode\"]}]'\n\n# Query VXLAN interfaces\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[[\"type\",\"==\",\"vxlan\"]], \"columns\":[\"name\",\"options\"]}]'\n\n# Query interfaces with errors\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Interface\", \"where\":[], \"columns\":[\"name\",\"error\"]}]'\n```\n\n## Analysis Output\n\nThe default mode analyzes:\n\n| Analysis | Source File | Description |\n|----------|-------------|-------------|\n| System Info | `ovs-vsctl_-t_5_list_Open_vSwitch` | OVS/DPDK version, system type, external IDs |\n| Topology | `ovs-vsctl_-t_5_show` | Bridge overview with ports grouped by type |\n| Bridge Details | `ovs-vsctl_-t_5_list_bridge` | Datapath type, fail mode, CT zones |\n| Interfaces | `ovs-vsctl_-t_5_list_interface` | By type, pod interfaces with mapping |\n| OpenFlow | `ovs-ofctl_dump-flows_*` | Flow counts, drops, top flows |\n| Port Stats | `ovs-ofctl_dump-ports_*` | RX/TX drops and errors |\n| Tunnels | `ovs-appctl_tnl.ports.show_-v` | Configured tunnel ports |\n| Datapath Health | `ovs-appctl_upcall.show` | Flow table usage vs limit |\n| OVS Stats | `ovs-appctl_coverage.show` | Internal counters (netlink, OVSDB, etc.) |\n\n## OpenFlow Analysis Features\n\n| Feature | Description |\n|---------|-------------|\n| Flow Count | Total flows per bridge (total, drop, with hits) |\n| Drop Detection | Flows with `actions=drop` that have packet hits |\n| Top Flows | Most active flows sorted by n_packets |\n| Table Distribution | Flow counts per OpenFlow table |\n| Port Drops | RX/TX drop counters per port |\n| Port Errors | RX/TX error counters per port |\n| Datapath Health | Flow table usage vs limit (from upcall.show) |\n| OVS Stats | Internal statistics (netlink, OpenFlow, OVSDB transactions) |\n\n## OVN Internal Tables (Ignore Drops in These)\n\nWhen analyzing drop flows in OVN-managed bridges (br-int), **ignore drops in table 44 and tables 64-87** as these are internal OVN mechanics, not security policy or real packet drops.\n\n> ** Note:** OVN and OVS are complex systems and table mappings can change between releases. Always verify drop analysis against the specific OVN version in use. When in doubt, check the OVN source code for your release.\n\n| Table | Name | Purpose |\n|-------|------|---------|\n| **44** | **CHK_LB_OUTPUT** | **Loopback prevention - drops packets that would loop back (high volume, normal)** |\n| 64 | SAVE_INPORT | Save ingress port for later |\n| 65 | LOG_TO_PHY | Logical to physical mapping |\n| 66 | MAC_BINDING | MAC binding lookups |\n| 67 | MAC_LOOKUP | MAC address table lookups |\n| 68-69 | CHK_LB_HAIRPIN | Load balancer hairpin checks |\n| 70 | CT_SNAT_HAIRPIN | Conntrack SNAT hairpin |\n| 71-72 | GET/LOOKUP_FDB | FDB (forwarding DB) lookups |\n| 73-74 | CHK_IN_PORT_SEC | Ingress port security |\n| 75 | CHK_OUT_PORT_SEC | Egress port security |\n| 76-77 | ECMP_NH | ECMP next-hop handling |\n| 78 | CHK_LB_AFFINITY | Load balancer affinity |\n| **79** | **MAC_CACHE_USE** | **MAC cache miss (high volume, normal)** |\n| 80 | CT_ZONE_LOOKUP | Conntrack zone lookup |\n| 81-83 | CT_ORIG_*_LOAD | Conntrack original tuple loading |\n| 84 | FLOOD_REMOTE_CHASSIS | Remote chassis flooding |\n| 85 | CT_STATE_SAVE | Conntrack state save |\n| 86 | CT_ORIG_PROTO_LOAD | Conntrack protocol loading |\n| 87 | GET_REMOTE_FDB | Remote FDB lookup |\n\n**Relevant drop tables for troubleshooting:**\n- **Table 9**: Ingress ACLs - actual policy drops (look for `reg0=0x8000`)\n- **Table 0**: Initial classification drops\n- **Tables < 64**: Generally meaningful drops\n\n## Common Analysis Scenarios\n\n### 1. Troubleshooting Packet Drops\n\n```bash\n# Quick analysis (default mode, no ovsdb-tool)\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/\n```\n\nLook for:\n- **ACTIVE DROP FLOWS** section showing flows dropping packets\n- **PORT STATISTICS** showing ports with drops/errors\n\n### 2. Checking DPDK Configuration\n\n```bash\n# Default mode shows DPDK info from text files\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/\n\n# Or use database mode for custom queries\npython3 \"$SCRIPT_PATH\" /path/to/sosreport/ --query '[\"Open_vSwitch\", {\"op\":\"select\", \"table\":\"Open_vSwitch\", \"where\":[], \"columns\":[\"dpdk_initialized\",\"other_config\"]}]'\n```\n\n### 3. Investigating Pod Connectivity\n\nThe interface analysis shows pod-to-OVS mapping:\n- Interfaces with `external_ids` containing `iface-id`\n- Interface names ending with `_h` (veth host side)\n\n### 4. Datapath Health Check\n\nThe upcall stats show:\n```\nDATAPATH FLOW TABLE HEALTH\n  Current flows: 155 / 200,000 (0.1% used)\n  Average: 156, Max seen: 215\n   Flow table healthy\n```\n\nIf usage > 90%, flows are being evicted too aggressively.\n\n## Output Example\n\n```\n================================================================================\nOVS ANALYSIS - sosreport-hostname-2024-01-15\n================================================================================\nMode: Text file analysis (no ovsdb-tool required)\n\n================================================================================\nOVS SYSTEM INFORMATION\n================================================================================\n\n  Field                     Value\n  ------------------------- --------------------------------------------------\n  OVS Version               \"3.3.4-62.el9fdp\"\n  DB Version                \"8.5.0\"\n  System Type               rhcos\n  System Version            \"4.16\"\n  DPDK Initialized          false\n  Datapath Types            [netdev, system]\n\n  External IDs:\n    hostname: master2.example.com\n    ovn-encap-ip: 10.32.110.5\n    ovn-encap-type: geneve\n\n================================================================================\nOVS TOPOLOGY\n================================================================================\n\n  System UUID: 7e9a3f70-86fa-4578-a849-4fd807a64a10\n  Total Bridges: 2\n\n  Bridge: br-ex\n    ports: 3\n      internal: br-ex\n      patch: patch-br-ex-to-br-int\n      system: nm-bond\n\n  Bridge: br-int\n    fail_mode: secure\n    datapath_type: system\n    ports: 12\n      geneve: 9 ports\n      internal: ovn-k8s-mp0, br-int\n      patch: patch-br-int-to-br-ex\n\n================================================================================\nOPENFLOW ANALYSIS\n================================================================================\n\n  Bridge: br-int\n  ----------------------------------------------------------------------\n  Total flows: 2,017\n  Flows with hits: 318\n  Drop flows: 150 (9 actively dropping)\n\n    ACTIVE DROP FLOWS (9):\n    table=40, priority=0, packets=8,105\n    table=79, priority=100, packets=1,356\n      match: ip,reg14=0x2,metadata=0x5,dl_src=00:62:0b:ea:b5:e0\n\n--------------------------------------------------------------------------------\nDATAPATH FLOW TABLE HEALTH\n--------------------------------------------------------------------------------\n\n  Current flows: 155 / 200,000 (0.1% used)\n  Average: 156, Max seen: 215\n   Flow table healthy\n```\n\n## Error Handling\n\n| Error | Solution |\n|-------|----------|\n| `ovsdb-tool not found` | Install openvswitch package OR use default mode (no --db) |\n| `conf.db not found` | Use default mode (analyzes text files instead) |\n| `sos_commands/openvswitch not found` | Ensure sosreport has OVS data collected |\n\n## See Also\n\n- [OVN Database Analysis](../../must-gather/skills/must-gather-analyzer/scripts/analyze_ovn_dbs.py) - For OVN NB/SB databases\n- [sosreport openvswitch plugin](https://github.com/sosreport/sos/blob/main/sos/report/plugins/openvswitch.py) - What sosreport collects"
              },
              {
                "name": "Resource Analysis",
                "description": "Analyze system resource usage data from sosreport archives, extracting memory statistics, CPU load averages, disk space utilization, and process information from the sosreport directory structure to diagnose resource exhaustion, performance bottlenecks, and capacity issues",
                "path": "plugins/sosreport/skills/resource-analysis/SKILL.md",
                "frontmatter": {
                  "name": "Resource Analysis",
                  "description": "Analyze system resource usage data from sosreport archives, extracting memory statistics, CPU load averages, disk space utilization, and process information from the sosreport directory structure to diagnose resource exhaustion, performance bottlenecks, and capacity issues"
                },
                "content": "# Resource Analysis Skill\n\nThis skill provides detailed guidance for analyzing system resource usage from sosreport archives, including memory, CPU, disk space, and process information.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing the `/sosreport:analyze` command's resource analysis phase\n- Investigating performance issues or resource bottlenecks\n- Identifying resource exhaustion problems\n- Correlating resource usage with system failures\n\n## Prerequisites\n\n- Sosreport archive must be extracted to a working directory\n- Path to the sosreport root directory must be known\n- Understanding of Linux resource management\n\n## Key Resource Data Locations in Sosreport\n\n1. **Memory Information**:\n   - `sos_commands/memory/free` - Memory usage snapshot\n   - `proc/meminfo` - Detailed memory statistics\n   - `sos_commands/memory/swapon_-s` - Swap usage\n   - `proc/buddyinfo` - Memory fragmentation\n\n2. **CPU Information**:\n   - `sos_commands/processor/lscpu` - CPU architecture and features\n   - `proc/cpuinfo` - Detailed CPU information\n   - `sos_commands/processor/turbostat` - CPU frequency and power states (if available)\n   - `uptime` - Load averages\n\n3. **Disk Information**:\n   - `sos_commands/filesys/df_-al` - Filesystem usage\n   - `sos_commands/block/lsblk` - Block device information\n   - `sos_commands/filesys/mount` - Mounted filesystems\n   - `proc/diskstats` - Disk I/O statistics\n\n4. **Process Information**:\n   - `sos_commands/process/ps_auxwww` - Process list with details\n   - `sos_commands/process/top` - Process snapshot (if available)\n   - `proc/[pid]/` - Per-process information\n\n## Implementation Steps\n\n### Step 1: Analyze Memory Usage\n\n1. **Parse free command output**:\n   ```bash\n   # Check if free output exists\n   if [ -f sos_commands/memory/free ]; then\n     cat sos_commands/memory/free\n   fi\n   ```\n\n2. **Extract memory metrics**:\n   ```bash\n   # Parse /proc/meminfo for detailed stats\n   if [ -f proc/meminfo ]; then\n     grep -E \"^(MemTotal|MemFree|MemAvailable|Buffers|Cached|SwapTotal|SwapFree|Dirty|Slab):\" proc/meminfo\n   fi\n   ```\n\n3. **Calculate memory usage percentage**:\n   - Total memory = MemTotal\n   - Used memory = MemTotal - MemAvailable\n   - Usage percentage = (Used / Total) * 100\n   - Parse from `free` output or calculate from `meminfo`\n\n4. **Check for memory pressure indicators**:\n   ```bash\n   # Look for OOM events in logs\n   grep -i \"out of memory\\|oom killer\" sos_commands/logs/journalctl_--no-pager 2>/dev/null\n\n   # Check swap usage\n   if [ -f sos_commands/memory/swapon_-s ]; then\n     cat sos_commands/memory/swapon_-s\n   fi\n   ```\n\n5. **Identify memory issues**:\n   - Memory usage > 90%  Critical\n   - Memory usage > 80%  Warning\n   - Heavy swap usage (>50% swap used)  Performance issue\n   - OOM killer events  Critical memory exhaustion\n\n### Step 2: Analyze CPU Usage\n\n1. **Extract CPU information**:\n   ```bash\n   # Get CPU count and model\n   if [ -f sos_commands/processor/lscpu ]; then\n     grep -E \"^(CPU\\(s\\)|Model name|Thread|Core|Socket|CPU MHz):\" sos_commands/processor/lscpu\n   fi\n   ```\n\n2. **Check load averages**:\n   ```bash\n   # Parse uptime for load averages\n   if [ -f uptime ]; then\n     cat uptime\n   fi\n\n   # Or from proc/loadavg\n   if [ -f proc/loadavg ]; then\n     cat proc/loadavg\n   fi\n   ```\n\n3. **Interpret load averages**:\n   - Load average format: 1-min, 5-min, 15-min\n   - Compare with CPU count from lscpu\n   - Load > CPU count  System overloaded\n   - Load >> CPU count (2x or more)  Critical overload\n\n4. **Check for CPU throttling**:\n   ```bash\n   # Look for thermal throttling in logs\n   grep -i \"throttl\\|temperature\\|thermal\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n5. **Identify CPU issues**:\n   - 1-min load > 2x CPU count  Critical\n   - 5-min load > CPU count  Warning\n   - Thermal throttling present  Hardware/cooling issue\n\n### Step 3: Analyze Disk Usage\n\n1. **Parse df output for filesystem usage**:\n   ```bash\n   if [ -f sos_commands/filesys/df_-al ]; then\n     # Skip header and special filesystems, show only regular filesystems\n     grep -v \"^Filesystem\\|tmpfs\\|devtmpfs\\|overlay\" sos_commands/filesys/df_-al | grep -v \"^$\"\n   fi\n   ```\n\n2. **Identify full or nearly-full filesystems**:\n   ```bash\n   # Extract filesystems with usage > 85%\n   if [ -f sos_commands/filesys/df_-al ]; then\n     awk 'NR>1 && $5+0 >= 85 {print $5, $6, $1}' sos_commands/filesys/df_-al | grep -v \"tmpfs\\|devtmpfs\"\n   fi\n   ```\n\n3. **Check disk I/O errors**:\n   ```bash\n   # Look for I/O errors in logs\n   grep -i \"i/o error\\|read error\\|write error\\|bad sector\" var/log/dmesg 2>/dev/null\n   grep -i \"i/o error\\|read error\\|write error\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n4. **Analyze block devices**:\n   ```bash\n   if [ -f sos_commands/block/lsblk ]; then\n     cat sos_commands/block/lsblk\n   fi\n   ```\n\n5. **Identify disk issues**:\n   - Filesystem > 95% full  Critical\n   - Filesystem > 85% full  Warning\n   - I/O errors present  Hardware issue\n   - Root filesystem full  System stability risk\n\n### Step 4: Analyze Process Information\n\n1. **Parse ps output**:\n   ```bash\n   if [ -f sos_commands/process/ps_auxwww ]; then\n     # Show header\n     head -1 sos_commands/process/ps_auxwww\n   fi\n   ```\n\n2. **Find top CPU consumers**:\n   ```bash\n   # Sort by CPU usage (column 3), show top 10\n   if [ -f sos_commands/process/ps_auxwww ]; then\n     tail -n +2 sos_commands/process/ps_auxwww | sort -k3 -rn | head -10\n   fi\n   ```\n\n3. **Find top memory consumers**:\n   ```bash\n   # Sort by memory usage (column 4), show top 10\n   if [ -f sos_commands/process/ps_auxwww ]; then\n     tail -n +2 sos_commands/process/ps_auxwww | sort -k4 -rn | head -10\n   fi\n   ```\n\n4. **Check for zombie processes**:\n   ```bash\n   # Look for processes in Z state\n   if [ -f sos_commands/process/ps_auxwww ]; then\n     grep \" Z \" sos_commands/process/ps_auxwww || echo \"No zombie processes found\"\n   fi\n   ```\n\n5. **Count processes by state**:\n   ```bash\n   # Count processes by state (R=running, S=sleeping, D=uninterruptible, Z=zombie, T=stopped)\n   if [ -f sos_commands/process/ps_auxwww ]; then\n     tail -n +2 sos_commands/process/ps_auxwww | awk '{print $8}' | cut -c1 | sort | uniq -c\n   fi\n   ```\n\n6. **Identify process issues**:\n   - Zombie processes present  Parent process not reaping children\n   - Many processes in D state  I/O bottleneck\n   - Single process using >80% memory  Memory leak or expected behavior\n   - Many processes using high CPU  CPU contention\n\n### Step 5: Correlate Resource Usage with Issues\n\n1. **Cross-reference with logs**:\n   - If high memory usage, check for OOM events in logs\n   - If high disk usage, check for disk full errors\n   - If high load, check for performance-related errors\n\n2. **Identify resource exhaustion patterns**:\n   - Memory exhaustion  OOM killer  Service crashes\n   - Disk full  Write failures  Application errors\n   - CPU overload  Timeouts  Request failures\n\n3. **Build timeline**:\n   - When did resource issues start?\n   - Correlate with log timestamps\n   - Identify triggering event if possible\n\n### Step 6: Generate Resource Analysis Summary\n\nCreate a structured summary with the following sections:\n\n1. **Memory Summary**:\n   - Total memory\n   - Used memory (GB and %)\n   - Available memory\n   - Swap usage (GB and %)\n   - Memory pressure indicators (OOM events)\n\n2. **CPU Summary**:\n   - CPU count and model\n   - Load averages (1-min, 5-min, 15-min)\n   - Load per CPU\n   - CPU issues (throttling, overload)\n\n3. **Disk Summary**:\n   - Filesystems and usage percentages\n   - Full or nearly-full filesystems\n   - I/O errors count\n   - Most full filesystem\n\n4. **Process Summary**:\n   - Total process count\n   - Top CPU consumers (top 5)\n   - Top memory consumers (top 5)\n   - Zombie process count\n   - Processes in uninterruptible sleep (D state)\n\n5. **Critical Resource Issues**:\n   - List issues by severity\n   - Provide evidence (file paths, metrics)\n   - Suggest remediation\n\n## Error Handling\n\n1. **Missing resource files**:\n   - If `free` is missing, parse `proc/meminfo` directly\n   - If `ps` is missing, check `proc/` for process information\n   - Document missing data in summary\n\n2. **Parsing errors**:\n   - Handle different output formats (free -h vs free -m)\n   - Account for locale differences in number formats\n   - Validate data before calculations\n\n3. **Incomplete data**:\n   - Some sosreports may not include all resource files\n   - Indicate which metrics are unavailable\n   - Work with available data only\n\n## Output Format\n\nThe resource analysis should produce:\n\n```bash\nRESOURCE USAGE SUMMARY\n======================\n\nMEMORY\n------\nTotal:      {total_gb} GB\nUsed:       {used_gb} GB ({used_pct}%)\nAvailable:  {available_gb} GB ({available_pct}%)\nBuffers:    {buffers_gb} GB\nCached:     {cached_gb} GB\nSwap Total: {swap_total_gb} GB\nSwap Used:  {swap_used_gb} GB ({swap_used_pct}%)\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {memory_issue_description}\n\nCPU\n---\nModel:        {cpu_model}\nCPU Count:    {cpu_count}\nThreads/Core: {threads_per_core}\n\nLoad Averages: {load_1m}, {load_5m}, {load_15m}\nLoad per CPU:  {load_1m_per_cpu}, {load_5m_per_cpu}, {load_15m_per_cpu}\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {cpu_issue_description}\n\nDISK USAGE\n----------\nFilesystem                    Size  Used  Avail  Use%  Mounted on\n{filesystem}                 {size} {used} {avail} {pct}% {mount}\n\nNearly Full Filesystems (>85%):\n  - {mount}: {pct}% full ({available} available)\n\nI/O Errors: {count} errors found in logs\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {disk_issue_description}\n\nPROCESSES\n---------\nTotal Processes: {total}\nRunning:         {running}\nSleeping:        {sleeping}\nZombie:          {zombie}\nUninterruptible: {uninterruptible}\n\nTop CPU Consumers:\n  1. {process_name} (PID {pid}): {cpu}% CPU, {mem}% MEM\n  2. {process_name} (PID {pid}): {cpu}% CPU, {mem}% MEM\n  3. {process_name} (PID {pid}): {cpu}% CPU, {mem}% MEM\n\nTop Memory Consumers:\n  1. {process_name} (PID {pid}): {mem}% MEM, {cpu}% CPU\n  2. {process_name} (PID {pid}): {mem}% MEM, {cpu}% CPU\n  3. {process_name} (PID {pid}): {mem}% MEM, {cpu}% CPU\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {process_issue_description}\n\nCRITICAL RESOURCE ISSUES\n------------------------\n{severity}: {issue_description}\n  Evidence: {file_path}\n  Impact: {impact_description}\n  Recommendation: {remediation_action}\n\nRECOMMENDATIONS\n---------------\n1. {actionable_recommendation}\n2. {actionable_recommendation}\n\nDATA SOURCES\n------------\n- Memory: {sosreport_path}/sos_commands/memory/free\n- Memory: {sosreport_path}/proc/meminfo\n- CPU: {sosreport_path}/sos_commands/processor/lscpu\n- Load: {sosreport_path}/uptime\n- Disk: {sosreport_path}/sos_commands/filesys/df_-al\n- Processes: {sosreport_path}/sos_commands/process/ps_auxwww\n```\n\n## Examples\n\n### Example 1: Memory Analysis\n\n```bash\n# Parse free command output\n$ cat sos_commands/memory/free\n              total        used        free      shared  buff/cache   available\nMem:       16277396     8123456     2145678      123456     6008262     7654321\nSwap:       8388604      512000     7876604\n\n# Interpretation:\n# - Total RAM: ~16 GB\n# - Used: ~8 GB (50%)\n# - Available: ~7.6 GB (47%)\n# - Swap used: ~500 MB (6%)\n# Status: OK - healthy memory usage\n```\n\n### Example 2: Disk Full Detection\n\n```bash\n# Find filesystems > 85% full\n$ awk 'NR>1 && $5+0 >= 85' sos_commands/filesys/df_-al\n/dev/sda1      50G   45G   5G   90%  /\n/dev/sdb1      100G  96G   4G   96%  /var/log\n\n# Critical: Root filesystem at 90%, /var/log at 96%\n# Action required: Clean up disk space\n```\n\n### Example 3: High Load Investigation\n\n```bash\n# Check load averages\n$ cat uptime\n14:23:45 up 10 days, 3:42, 2 users, load average: 8.45, 7.23, 6.12\n\n# With lscpu showing 4 CPUs:\n# Load per CPU: 2.1, 1.8, 1.5\n# System is overloaded (load > 2x CPU count)\n```\n\n## Tips for Effective Analysis\n\n1. **Context matters**: High resource usage isn't always bad - consider the workload\n2. **Look for trends**: Compare 1-min, 5-min, 15-min loads to see if issues are growing\n3. **Correlate metrics**: High load + high memory + disk full = multiple issues\n4. **Check ratios**: Usage percentages are more meaningful than absolute values\n5. **Validate findings**: Cross-reference with log analysis for confirmation\n6. **Consider capacity**: Is the system appropriately sized for its workload?\n\n## Common Resource Patterns\n\n1. **Memory leak**: Steadily increasing memory usage, eventual OOM\n2. **Disk full**: Application writes failing, log rotation issues\n3. **CPU spike**: Load average spike, potentially from runaway process\n4. **I/O bottleneck**: High load but low CPU usage, many D-state processes\n5. **Swap thrashing**: High swap usage, poor performance\n6. **Zombie accumulation**: Parent process bug not reaping children\n\n## Severity Classification\n\n| Metric | OK | Warning | Critical |\n|--------|----|---------| ---------|\n| Memory Usage | < 80% | 80-90% | > 90% |\n| Swap Usage | < 20% | 20-50% | > 50% |\n| Disk Usage | < 85% | 85-95% | > 95% |\n| Load (per CPU) | < 1.0 | 1.0-2.0 | > 2.0 |\n| Root FS Usage | < 80% | 80-90% | > 90% |\n\n## See Also\n\n- Logs Analysis Skill: For finding resource-related errors in logs\n- System Configuration Analysis Skill: For investigating service resource limits\n- Network Analysis Skill: For network-related performance issues"
              },
              {
                "name": "System Configuration Analysis",
                "description": "Analyze system configuration data from sosreport archives, extracting OS details, installed packages, systemd service status, SELinux/AppArmor policies, and kernel parameters from the sosreport directory structure to diagnose configuration-related system issues",
                "path": "plugins/sosreport/skills/system-config-analysis/SKILL.md",
                "frontmatter": {
                  "name": "System Configuration Analysis",
                  "description": "Analyze system configuration data from sosreport archives, extracting OS details, installed packages, systemd service status, SELinux/AppArmor policies, and kernel parameters from the sosreport directory structure to diagnose configuration-related system issues"
                },
                "content": "# System Configuration Analysis Skill\n\nThis skill provides detailed guidance for analyzing system configuration from sosreport archives, including OS information, installed packages, systemd services, and SELinux/AppArmor settings.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing the `/sosreport:analyze` command's system configuration phase\n- Investigating service failures or misconfigurations\n- Verifying package versions and updates\n- Checking security policy settings (SELinux/AppArmor)\n- Understanding system state and configuration\n\n## Prerequisites\n\n- Sosreport archive must be extracted to a working directory\n- Path to the sosreport root directory must be known\n- Understanding of Linux system administration\n\n## Key Configuration Data Locations in Sosreport\n\n1. **System Information**:\n   - `uname` - Kernel version\n   - `etc/os-release` - OS distribution and version\n   - `uptime` - System uptime\n   - `proc/uptime` - Uptime in seconds\n   - `sos_commands/release/` - Release information\n\n2. **Package Information**:\n   - `installed-rpms` - RPM packages (RHEL/Fedora/CentOS)\n   - `installed-debs` - DEB packages (Debian/Ubuntu)\n   - `sos_commands/yum/` - Yum/DNF information\n   - `sos_commands/rpm/` - RPM database queries\n\n3. **Service Status**:\n   - `sos_commands/systemd/systemctl_list-units` - All units\n   - `sos_commands/systemd/systemctl_list-units_--failed` - Failed units\n   - `sos_commands/systemd/systemctl_status_--all` - Detailed service status\n   - `sos_commands/systemd/systemctl_list-unit-files` - Unit files\n\n4. **SELinux**:\n   - `sos_commands/selinux/sestatus` - SELinux status\n   - `sos_commands/selinux/getenforce` - Current enforcement mode\n   - `sos_commands/selinux/selinux-policy` - Policy information\n   - `var/log/audit/audit.log` - SELinux denials\n\n5. **AppArmor** (if applicable):\n   - `sos_commands/apparmor/` - AppArmor configuration\n   - `etc/apparmor.d/` - AppArmor profiles\n\n6. **System Configuration Files**:\n   - `etc/` - System-wide configuration\n   - `etc/sysctl.conf` or `etc/sysctl.d/` - Kernel parameters\n   - `etc/security/limits.conf` - Resource limits\n\n## Implementation Steps\n\n### Step 1: Analyze System Information\n\n1. **Check OS version and distribution**:\n   ```bash\n   if [ -f etc/os-release ]; then\n     cat etc/os-release\n   fi\n   ```\n\n2. **Get kernel version**:\n   ```bash\n   if [ -f uname ]; then\n     cat uname\n   elif [ -f proc/version ]; then\n     cat proc/version\n   fi\n   ```\n\n3. **Check system uptime**:\n   ```bash\n   if [ -f uptime ]; then\n     cat uptime\n   elif [ -f proc/uptime ]; then\n     # Parse uptime from proc/uptime (seconds)\n     awk '{printf \"%.2f days\\n\", $1/86400}' proc/uptime\n   fi\n   ```\n\n4. **Extract key system details**:\n   - OS name and version\n   - Kernel version\n   - System architecture (x86_64, aarch64, etc.)\n   - Uptime (days)\n\n5. **Check for outdated kernel or OS**:\n   - Compare kernel version with current stable\n   - Note if system hasn't been rebooted in a very long time (>365 days)\n   - Identify if OS version is EOL\n\n### Step 2: Analyze Installed Packages\n\n1. **List installed packages**:\n   ```bash\n   # For RPM-based systems\n   if [ -f installed-rpms ]; then\n     cat installed-rpms\n   fi\n\n   # For DEB-based systems\n   if [ -f installed-debs ]; then\n     cat installed-debs\n   fi\n   ```\n\n2. **Extract key package versions**:\n   ```bash\n   # Important system packages\n   grep -E \"^(kernel|systemd|glibc|openssh|openssl)\" installed-rpms 2>/dev/null\n\n   # Or use awk to parse package name and version\n   awk '{print $1}' installed-rpms | head -20\n   ```\n\n3. **Check for known problematic versions**:\n   - Security vulnerabilities (if known CVEs)\n   - Buggy package versions\n   - Compatibility issues\n\n4. **Identify package manager issues**:\n   ```bash\n   # Check yum/dnf logs for errors\n   if [ -d sos_commands/yum ]; then\n     grep -i \"error\\|fail\" sos_commands/yum/* 2>/dev/null\n   fi\n   ```\n\n5. **Count packages and categorize**:\n   - Total packages installed\n   - Key package versions (kernel, systemd, glibc, etc.)\n   - Recently updated packages (if timestamps available)\n\n### Step 3: Analyze Service Status\n\n1. **List all systemd units**:\n   ```bash\n   if [ -f sos_commands/systemd/systemctl_list-units ]; then\n     cat sos_commands/systemd/systemctl_list-units\n   fi\n   ```\n\n2. **Identify failed services**:\n   ```bash\n   if [ -f sos_commands/systemd/systemctl_list-units_--failed ]; then\n     cat sos_commands/systemd/systemctl_list-units_--failed\n   elif [ -f sos_commands/systemd/systemctl_list-units ]; then\n     grep \"failed\" sos_commands/systemd/systemctl_list-units\n   fi\n   ```\n\n3. **Check service details**:\n   ```bash\n   # Parse detailed status for failed services\n   if [ -f sos_commands/systemd/systemctl_status_--all ]; then\n     # Extract service names and their status\n     grep -E \"|Active:\" sos_commands/systemd/systemctl_status_--all | head -50\n   fi\n   ```\n\n4. **Count services by state**:\n   ```bash\n   # Count running, failed, inactive services\n   if [ -f sos_commands/systemd/systemctl_list-units ]; then\n     awk '{print $4}' sos_commands/systemd/systemctl_list-units | sort | uniq -c\n   fi\n   ```\n\n5. **Identify critical service failures**:\n   - System services (systemd-*, dbus, NetworkManager)\n   - Application services (httpd, nginx, postgresql, etc.)\n   - Custom services\n\n6. **Extract failure reasons from logs**:\n   ```bash\n   # For each failed service, find related log entries\n   grep -i \"failed to start\\|service.*failed\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n### Step 4: Analyze SELinux Configuration\n\n1. **Check SELinux status**:\n   ```bash\n   if [ -f sos_commands/selinux/sestatus ]; then\n     cat sos_commands/selinux/sestatus\n   fi\n   ```\n\n2. **Get SELinux mode**:\n   ```bash\n   if [ -f sos_commands/selinux/getenforce ]; then\n     cat sos_commands/selinux/getenforce\n   fi\n   ```\n\n3. **Check for SELinux denials**:\n   ```bash\n   # Look for AVC denials in audit log\n   if [ -f var/log/audit/audit.log ]; then\n     grep \"avc.*denied\" var/log/audit/audit.log | head -50\n   fi\n\n   # Or in journald logs\n   grep -i \"selinux.*denied\\|avc.*denied\" sos_commands/logs/journalctl_--no-pager 2>/dev/null | head -20\n   ```\n\n4. **Parse denial information**:\n   - Extract denied operations (read, write, execute, etc.)\n   - Identify source and target contexts\n   - Note which services are affected\n\n5. **Check for SELinux booleans**:\n   ```bash\n   if [ -f sos_commands/selinux/getsebool_-a ]; then\n     cat sos_commands/selinux/getsebool_-a\n   fi\n   ```\n\n6. **Identify SELinux issues**:\n   - SELinux in permissive mode (may hide errors)\n   - SELinux disabled (security concern)\n   - Frequent AVC denials (policy may need adjustment)\n   - Context mismatches\n\n### Step 5: Check System Configuration\n\n1. **Review kernel parameters**:\n   ```bash\n   # Check sysctl settings\n   if [ -f sos_commands/kernel/sysctl_-a ]; then\n     cat sos_commands/kernel/sysctl_-a\n   elif [ -d etc/sysctl.d ]; then\n     cat etc/sysctl.d/*.conf 2>/dev/null\n   fi\n   ```\n\n2. **Check resource limits**:\n   ```bash\n   if [ -f etc/security/limits.conf ]; then\n     grep -v \"^#\\|^$\" etc/security/limits.conf\n   fi\n\n   # Check limits.d directory\n   if [ -d etc/security/limits.d ]; then\n     cat etc/security/limits.d/*.conf 2>/dev/null\n   fi\n   ```\n\n3. **Review boot parameters**:\n   ```bash\n   if [ -f sos_commands/boot/grub2-editenv_list ]; then\n     cat sos_commands/boot/grub2-editenv_list\n   elif [ -f proc/cmdline ]; then\n     cat proc/cmdline\n   fi\n   ```\n\n4. **Check systemd configuration**:\n   ```bash\n   # Look for systemd configuration overrides\n   if [ -d etc/systemd/system ]; then\n     find etc/systemd/system -name \"*.conf\" 2>/dev/null\n   fi\n   ```\n\n### Step 6: Generate System Configuration Summary\n\nCreate a structured summary with the following sections:\n\n1. **System Information**:\n   - OS name and version\n   - Kernel version\n   - Architecture\n   - System uptime\n   - Last boot time\n\n2. **Package Summary**:\n   - Total packages installed\n   - Key package versions (kernel, systemd, glibc, openssl, openssh)\n   - Known problematic packages (if any)\n   - Package manager issues\n\n3. **Service Status**:\n   - Total services\n   - Running services count\n   - Failed services count\n   - List of failed services with reasons\n   - Critical service status\n\n4. **SELinux/AppArmor**:\n   - SELinux status (enabled/disabled)\n   - SELinux mode (enforcing/permissive)\n   - Denial count\n   - Top denied operations\n   - Policy recommendations\n\n5. **Configuration Issues**:\n   - Kernel parameter anomalies\n   - Resource limit issues\n   - Boot parameter problems\n   - Configuration file errors\n\n## Error Handling\n\n1. **Missing configuration files**:\n   - Different distributions have different file locations\n   - Some files may not be collected based on sosreport options\n   - Document missing data in summary\n\n2. **Package manager variations**:\n   - Handle both RPM and DEB systems\n   - Account for different package naming conventions\n   - Support multiple package managers (yum, dnf, apt)\n\n3. **SELinux vs AppArmor**:\n   - Check which MAC system is in use\n   - Analyze accordingly\n   - Note if both or neither are present\n\n4. **Systemd vs init**:\n   - Older systems may use init instead of systemd\n   - Check for both service management systems\n   - Adapt analysis based on what's present\n\n## Output Format\n\nThe system configuration analysis should produce:\n\n```bash\nSYSTEM CONFIGURATION SUMMARY\n============================\n\nSYSTEM INFORMATION\n------------------\nOS: {os_name} {os_version}\nKernel: {kernel_version}\nArchitecture: {arch}\nUptime: {uptime_days} days ({last_boot_time})\n\nStatus: {OK|WARNING|CRITICAL}\nNotes:\n  - {system_info_note}\n\nINSTALLED PACKAGES\n------------------\nTotal Packages: {count}\n\nKey Package Versions:\n  kernel: {version}\n  systemd: {version}\n  glibc: {version}\n  openssl: {version}\n  openssh-server: {version}\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {package_issue_description}\n\nSYSTEMD SERVICES\n----------------\nTotal Units: {total}\nActive: {active_count}\nFailed: {failed_count}\nInactive: {inactive_count}\n\nFailed Services:\n   {service_name}.service - {description}\n    Reason: {failure_reason}\n    Last Failed: {timestamp}\n\n   {service_name}.service - {description}\n    Reason: {failure_reason}\n    Last Failed: {timestamp}\n\nStatus: {OK|WARNING|CRITICAL}\nRecommendations:\n  - {service_recommendation}\n\nSELINUX\n-------\nStatus: {enabled|disabled}\nMode: {enforcing|permissive|disabled}\nPolicy: {policy_name}\n\nAVC Denials: {count} denials found\n\nTop Denied Operations:\n  [{count}x] {operation} on {target} by {source}\n  [{count}x] {operation} on {target} by {source}\n\nSELinux Booleans: {count} custom settings\n\nStatus: {OK|WARNING|CRITICAL}\nIssues:\n  - {selinux_issue_description}\n\nRecommendations:\n  - {selinux_recommendation}\n\nKERNEL PARAMETERS\n-----------------\nKey sysctl Settings:\n  vm.swappiness: {value}\n  net.ipv4.ip_forward: {value}\n  kernel.panic: {value}\n\nCustom Parameters: {count} custom settings found\n\nStatus: {OK|WARNING|CRITICAL}\nNotes:\n  - {kernel_param_note}\n\nRESOURCE LIMITS\n---------------\nCustom Limits Found: {count}\n\n{user_or_group}  {type}  {item}  {value}\n\nStatus: {OK|WARNING}\nNotes:\n  - {limits_note}\n\nCRITICAL CONFIGURATION ISSUES\n-----------------------------\n{severity}: {issue_description}\n  Evidence: {file_path}\n  Impact: {impact_description}\n  Recommendation: {remediation_action}\n\nRECOMMENDATIONS\n---------------\n1. {actionable_recommendation}\n2. {actionable_recommendation}\n\nDATA SOURCES\n------------\n- OS Info: {sosreport_path}/etc/os-release\n- Kernel: {sosreport_path}/uname\n- Packages: {sosreport_path}/installed-rpms\n- Services: {sosreport_path}/sos_commands/systemd/systemctl_list-units\n- SELinux: {sosreport_path}/sos_commands/selinux/sestatus\n- Audit Log: {sosreport_path}/var/log/audit/audit.log\n```\n\n## Examples\n\n### Example 1: Failed Service Analysis\n\n```bash\n# List failed services\n$ cat sos_commands/systemd/systemctl_list-units_--failed\n  UNIT                    LOAD   ACTIVE SUB    DESCRIPTION\n httpd.service          loaded failed failed Apache Web Server\n postgresql.service     loaded failed failed PostgreSQL database\n\n# Find failure reason in logs\n$ grep \"httpd.service\" sos_commands/logs/journalctl_--no-pager | grep -i \"failed\\|error\"\nJan 15 10:23:45 server systemd[1]: httpd.service: Main process exited, code=exited, status=1/FAILURE\nJan 15 10:23:45 server systemd[1]: httpd.service: Failed with result 'exit-code'\nJan 15 10:23:45 server httpd[12345]: (98)Address already in use: AH00072: make_sock: could not bind to address [::]:80\n\n# Interpretation: httpd failed because port 80 is already in use\n```\n\n### Example 2: SELinux Denial Analysis\n\n```bash\n# Check for AVC denials\n$ grep \"avc.*denied\" var/log/audit/audit.log | head -5\ntype=AVC msg=audit(1705320245.123:456): avc: denied { write } for pid=1234 comm=\"httpd\" name=\"index.html\" dev=\"sda1\" ino=789012 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:user_home_t:s0 tclass=file permissive=0\n\n# Interpretation:\n# - httpd (web server) was denied write access\n# - Target file: index.html with context user_home_t\n# - Issue: Web server trying to write to user home directory\n# - Solution: Fix file context or move file to proper location\n```\n\n### Example 3: Package Version Check\n\n```bash\n# Check for specific package versions\n$ grep \"^openssl\" installed-rpms\nopenssl-1.1.1k-7.el8_6.x86_64\nopenssl-libs-1.1.1k-7.el8_6.x86_64\n\n$ grep \"^kernel\" installed-rpms\nkernel-4.18.0-425.el8.x86_64\nkernel-4.18.0-477.el8.x86_64\nkernel-core-4.18.0-425.el8.x86_64\nkernel-core-4.18.0-477.el8.x86_64\n\n# Interpretation:\n# - OpenSSL version 1.1.1k (check for known CVEs)\n# - Multiple kernels installed (good for rollback)\n# - Current kernel is 4.18.0-477 (from uname)\n```\n\n## Tips for Effective Analysis\n\n1. **Check service dependencies**: Failed service may be due to dependency failure\n2. **Correlate with logs**: Service failures often have detailed errors in logs\n3. **Verify configurations**: Check service config files for syntax errors\n4. **Consider timing**: When did service fail? Correlate with system events\n5. **SELinux context matters**: File contexts must match policy expectations\n6. **Package versions**: Compare with known good/bad versions\n7. **Uptime significance**: Very long uptime may mean missed security updates\n\n## Common Configuration Patterns and Issues\n\n1. **Service dependency failure**: ServiceB fails because ServiceA is not running\n2. **Port conflict**: Service fails to bind - port already in use\n3. **Permission denied**: Service can't access required files/directories\n4. **SELinux blocking**: Service denied access by SELinux policy\n5. **Missing dependencies**: Required package not installed\n6. **Configuration error**: Syntax error in config file\n7. **Resource limits**: Service hits ulimit (open files, processes, etc.)\n8. **Outdated kernel**: Running kernel doesn't match installed packages\n\n## Configuration Issue Severity Classification\n\n| Issue Type | Severity | Impact |\n|------------|----------|--------|\n| Critical service failed | High | Core functionality unavailable |\n| Optional service failed | Low | Non-essential feature unavailable |\n| SELinux in permissive | Warning | Reduced security, hiding issues |\n| SELinux disabled | Critical | No mandatory access control |\n| Kernel very outdated | High | Missing security fixes |\n| EOL OS version | Critical | No security updates |\n| Many AVC denials | Warning | Policy may need tuning |\n\n## See Also\n\n- Logs Analysis Skill: For detailed service failure log analysis\n- Resource Analysis Skill: For resource limit issues\n- Network Analysis Skill: For network service configuration"
              }
            ]
          },
          {
            "name": "utils",
            "description": "A generic utilities plugin serving as a catch-all for various helper commands",
            "source": "./plugins/utils",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install utils@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/address-reviews",
                "description": "Fetch and address all PR review comments",
                "path": "plugins/utils/commands/address-reviews.md",
                "frontmatter": {
                  "description": "Fetch and address all PR review comments",
                  "argument-hint": "[PR number (optional - uses current branch if omitted)]"
                },
                "content": "## Name\nutils:address-reviews\n\n## Synopsis\n/utils:address-reviews [PR number (optional - uses current branch if omitted)]\n\n## Description\nThis command automates the process of addressing PR review comments by fetching all comments from a pull request, categorizing them by priority (blocking, change requests, questions, suggestions), and systematically addressing each one. It intelligently filters out outdated comments, bot-generated content, and oversized responses to optimize context usage. The command handles code changes, posts replies to reviewers, and maintains a clean git history by amending relevant commits rather than creating unnecessary new ones.\n\n## Implementation\n\n### Step 0: Checkout the PR Branch\n\n1. **Determine PR number**: Use $ARGUMENTS if provided, otherwise `gh pr list --head <current-branch>`\n2. **Checkout**: Use `gh pr checkout <PR_NUMBER>` if not already on the branch, then `git pull`\n3. **Verify clean working tree**: Run `git status`. If uncommitted changes exist, ask user how to proceed\n\n### Step 1: Fetch PR Context\n\n1. **Fetch PR metadata with selective filtering**:\n\n   a. **First pass - Get metadata only** (IDs, authors, lengths, URLs):\n   ```bash\n   # Get issue comments (general PR comments - main conversation)\n   gh pr view <PR_NUMBER> --json comments --jq '.comments | map({\n     id,\n     author: .author.login,\n     length: (.body | length),\n     url,\n     createdAt,\n     type: \"issue_comment\"\n   })'\n\n   # Get reviews (need REST API for numeric IDs)\n   gh api repos/{owner}/{repo}/pulls/<PR_NUMBER>/reviews --jq 'map({\n     id,\n     author: .user.login,\n     length: (.body | length),\n     state,\n     submitted_at,\n     type: \"review\"\n   })'\n\n   # Get review comments (inline code comments)\n   gh api repos/{owner}/{repo}/pulls/<PR_NUMBER>/comments --jq 'map({\n     id,\n     author: .user.login,\n     length: (.body | length),\n     path,\n     line,\n     created_at,\n     type: \"review_comment\"\n   })'\n   ```\n\n   b. **Apply filtering logic** (DO NOT fetch full body yet):\n   - Filter out: `line == null` (outdated review comments)\n   - Filter out: `length > 5000`\n   - Filter out: CI/automation bots `author in [\"openshift-ci-robot\", \"openshift-ci\"]` (keep coderabbitai for code review insights)\n   - Keep track of filtered items and stats for reporting\n\n   c. **Second pass - Fetch ONLY essential fields for kept items**:\n   ```bash\n   # For issue comments - fetch only body and minimal metadata:\n   gh api repos/{owner}/{repo}/issues/comments/<comment_id> --jq '{id, body, user: .user.login, created_at, url}'\n\n   # For reviews - fetch only body and state:\n   gh api repos/{owner}/{repo}/pulls/<PR_NUMBER>/reviews/<review_id> --jq '{id, body, user: .user.login, state, submitted_at}'\n\n   # For review comments - fetch only body and code context:\n   gh api repos/{owner}/{repo}/pulls/comments/<comment_id> --jq '{id, body, user: .user.login, path, position, diff_hunk, created_at}'\n   ```\n\n   **Note**: Using `--jq` to select only needed fields minimizes context usage. Avoid fetching full API responses with all metadata.\n\n   d. **Log filtering results**:\n   ```\n     Fetched N/M comments (filtered out K large/bot comments saving ~X chars)\n   ```\n\n2. **Fetch commit messages**: `gh pr view <PR_NUMBER> --json commits -q '.commits[] | \"\\(.messageHeadline)\\n\\n\\(.messageBody)\"'`\n\n3. Store ONLY the kept (filtered) comments for analysis\n\n### Step 2: Categorize and Prioritize Comments\n\n**Note**: Most filtering already happened in Step 1 to save context window space.\n\n1. **Additional filtering** (for remaining fetched comments):\n   - Already resolved comments\n   - Pure acknowledgments (\"LGTM\", \"Thanks!\", etc.)\n\n2. **Categorize**:\n   - **BLOCKING**: Critical changes (security, bugs, breaking issues)\n   - **CHANGE_REQUEST**: Code improvements or refactoring\n   - **QUESTION**: Requests for clarification\n   - **SUGGESTION**: Optional improvements (nits, non-critical)\n\n3. **Group by context**: Group by file, then by proximity (within 10 lines)\n\n4. **Prioritize**: BLOCKING  CHANGE_REQUEST  QUESTION  SUGGESTION\n\n5. **Present summary**: Show counts by category and file groupings, ask user to confirm\n\n### Step 3: Address Comments\n\n#### Grouped Comments\n\nWhen multiple comments relate to the same concern/fix:\n- Make the code change once\n- Reply to EACH comment individually (don't copy-paste, tailor each reply)\n- Optional reference: `Done. (Also addresses feedback from @user)`\n\n#### Code Change Requests\n\n**a. Validate**: Thoroughly analyze if the change is valid and fixes an issue or improves code. Don't be afraid to reject the change if it doesn't make sense.\n\n**b. If requested change is valid**:\n- Plan and implement changes\n- Commit and Push\n   1. **Review changes**: `git diff`\n\n   2. **Analyze commit structure**: `git log --oneline origin/main..HEAD`\n      - Identify which commit the changes relate to\n\n   3. **Commit strategy**:\n\n      **DEFAULT: Amend the relevant commit**\n\n      -  **AMEND**: Review fixes, bug fixes, style improvements, refactoring, docs, tests within PR scope\n      -  **NEW COMMIT**: Only for substantial new features beyond PR's original scope\n      - **When unsure**: Amend (keep git history clean)\n      - **Multiple commits**: Use `git rebase -i origin/main` to amend the specific relevant commit\n\n   4. **Create and push commit**:\n      - Follow [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) format\n      - Always include body explaining \"why\"\n      - **Amend**: `git commit --amend --no-edit && git push --force-with-lease` (or update message if scope changed)\n      - **New commit**: Standard commit with message\n\n- **Concise Reply template**: `Done. [1-line what changed]. [Optional 1-line why]`\n  - Max 2 sentences + attribution footer\n- Post reply:\n  ```\n  gh api repos/{owner}/{repo}/pulls/<PR_NUMBER>/comments/<comment_id>/replies -f body=\"<reply>\"\n  ```\n  If fails: `gh pr comment <PR_NUMBER> --body=\"@<author> <reply>\"`\n\n**c. If declining change**:\n- **Reply with technical explanation** (3-5 sentences):\n  - Why current implementation is correct\n  - Specific reasoning with file:line references\n- Use same posting method as (b)\n\n**d. If unsure**: Ask user for clarification\n\n#### Clarification Requests\n\n- Provide clear, detailed answer (2-4 sentences)\n- Include file:line references when applicable\n- Post using same method as code changes\n\n#### Informational Comments\n\n- No action unless response is courteous\n\n**All replies must include**: `---\\n*AI-assisted response via Claude Code*`\n\n### Step 4: Summary\n\nShow user:\n- Total comments found (raw count from API)\n- Comments filtered out (with reason: outdated/large/bot-generated)\n- Comments addressed with code changes\n- Comments replied to\n- Comments requiring user input\n\n## Guidelines\n\n- Be thorough but efficient\n- Maintain professional tone in all replies\n- Prioritize code quality over quick fixes\n- Ensure code builds and passes tests after changes\n- When in doubt, ask the user\n- Use TodoWrite to track progress through multiple comments\n\n\n## Arguments:\n- $1: [PR number to address reviews (optional - uses current branch if omitted)]"
              },
              {
                "name": "/auto-approve-konflux-prs",
                "description": "Automate approving Konflux bot PRs for the given repository by adding /lgtm and /approve",
                "path": "plugins/utils/commands/auto-approve-konflux-prs.md",
                "frontmatter": {
                  "description": "Automate approving Konflux bot PRs for the given repository by adding /lgtm and /approve",
                  "argument-hint": "<target-repository>"
                },
                "content": "## Name\n\nutils:auto-approve-konflux-prs\n\n## Synopsis\n\n/utils:auto-approve-konflux-prs <target-repository>\n\n## Description\n\nThe command automates the approval of open PRs created by the `red-hat-konflux[bot]` for the given repository.\n\nIt filters all open PRs from the given repository, checks whether the PR already has `/lgtm` and `/approve` comments, verifies that all required checks (CI jobs or other mandatory checks) have passed, and if any labels/comments are missing and all checks succeed, posts `/lgtm` and `/approve` comments to trigger approval.\n\nThis ensures that PRs are only auto-approved if all required checks succeed and the author is `red-hat-konflux[bot]`, reducing the risk of approving failing or unauthorized changes.\n\n\n## Arguments\n\n- **$1  target-repositories** *(required)*: GitHub repository in `OWNER/REPO` format.\n  - Example: openshift/multiarch-tuning-operator.\n\n## Implementation\n\nThe command executes the following workflow:\n\n### 1. Restrict Author\n\nThe command only processes PRs authored by `red-hat-konflux[bot]`. If a PR from any other author is encountered, it reports an error such as below:\n```\n Only PRs from red-hat-konflux[bot] can be automatically processed\n```\nand exits.\n\n### 2. Get Open PRs\n\nFetch all open PRs authored by `red-hat-konflux[bot]` for the specified repository:\n\n```bash\ngh pr list --repo <target-repository> --author app/red-hat-konflux --state open --json number,title,baseRefName,labels\n```\n- Extract: number,title,baseRefName,labels\n\n### 3. Check CI Status and Labels\n\n#### **For Each PR:**:\n\n1. Fetch detailed PR information:\n```bash\ngh pr view <PR_NUMBER> --repo <target-repository> --json statusCheckRollup,labels\n```\n- Extract: statusCheckRollup,labels\n- Handle errors: If PR is inaccessible, log warning and skip\n\n2. Verify all required checks:\n- Verify all required checks have \"conclusion\": \"SUCCESS\"\n- If any check has failed or is pending(except one pending tide job), skip adding /lgtm or /approve and log:\n```\n Skipping PR #<PR_NUMBER>: CI checks not all passed\n```\n\n3. Inspect labels:\n    - Check for lgtm label\n    - Check for approved label\n\n4. Add missing labels via comments:\n   - If /lgtm is missing, post a comment /lgtm\n   - If /approve is missing, post a comment /approve\n   - If both are missing, post a single comment containing both commands.\n\n5. Log each action:\n```\n Added /lgtm and/or /approve to PR #<PR_NUMBER>: <PR_TITLE> (merge into <MERGE_BRANCH>)\n```\n\n## Return Value\n\n- **Claude agent text**: Summary of processed PRs and actions taken.\n- **Side effects**:\n  - Comments posted to PRs to trigger /lgtm and /approve.\n  - Progress updates for multiple PRs.\n\n## Examples\n\n1. **Process all open PRs from `red-hat-konflux[bot]` in a repository**:\n\n  ```\n  /utils:auto-approve-konflux-prs openshift/multiarch-tuning-operator\n  ```\n   \n  Output:\n  ```\n  Processing 3 open Konflux PRs...  \n  [1/3] PR #84 - chore(deps): update konflux references (merge into main)\n   Added /lgtm and /approve (all CI passed)\n  \n  [2/3] PR #83 - chore(deps): update konflux references (merge into v1.x)\n   Skipping: CI checks not all passed\n  \n  [3/3] PR #82 - chore(deps): update konflux references (merge into fbc)\n   Added /lgtm (already had /approve, all CI passed)\n  \n  Summary:\n   Processed 2 PRs successfully, 1 skipped due to CI failures\n  ```\n\n## Prerequisites\n\n### Required Tools\n\n1. **GitHub CLI (`gh`)**: Must be installed and authenticated\n   - Install: `brew install gh` (macOS) or see [GitHub CLI docs](https://cli.github.com/)\n   - Authenticate: `gh auth login`\n   - Verify: `gh auth status`\n\n2. **Access to GitHub Repositories**: Must have read access to repos where PRs are located\n   - PRs in private repos require appropriate GitHub permissions\n   - Public repos should work without additional configuration\n\n### Required Permissions\n\n1. **GitHub Permissions**:\n   - Read access to pull requests\n   - Write access to create comments on pull requests\n\n## Error Handling\n\n- **Repository inaccessible**: Reports error and exits.\n- **PRs authored by someone other than `red-hat-konflux[bot]`**: Reports error and exits.\n- **No open PRs from Konflux bot**: Logs \"No PRs to process\".\n- **GitHub authentication failure**: Suggests re-login with `gh auth login`.\n- **Comment posting failure**: Logs PR number and error for manual review.\n\n## Notes\n\n- The command only processes open PRs authored by `app/red-hat-konflux`.\n- Compatible with repositories in which the user has write permission to post PR comments.\n- Designed to minimize manual PR review effort and maintain consistent approvals."
              },
              {
                "name": "/generate-test-plan",
                "description": "Generate test steps for one or more related PRs",
                "path": "plugins/utils/commands/generate-test-plan.md",
                "frontmatter": {
                  "description": "Generate test steps for one or more related PRs",
                  "argument-hint": "[GitHub PR URLs]"
                },
                "content": "## Name\nutils:generate-test-plan\n\n## Synopsis\n/utils:generate-test-plan [GitHub PR URLs]\n\n## Description\nThe 'utils:generate-test-plan' command takes one or more GitHub PR URLs, fetches the PR details including description, commits, and file changes, analyzes the changes to understand what features/fixes were implemented, and generates a comprehensive testing guide with step-by-step instructions. When multiple PRs are provided, it analyzes them collectively to understand how they work together to fix a bug or implement a feature.\n\n**PR Testing Guide Generator**\n\n## Implementation\n\n- The command uses `gh pr view` to fetch PR data for one or more PRs\n- Analyzes PR descriptions, commits, and changed files across all provided PRs\n- Identifies relationships and dependencies between multiple PRs\n- Generates test scenarios based on the collective changes\n- Creates a comprehensive testing guide with prerequisites and verification steps\n\n## Process Flow:\n\n1. **PR Analysis**: Parse GitHub URLs and fetch PR details:\n   - Extract PR numbers from all provided URLs (supports multiple PRs)\n   - For each PR, use `gh pr view {PR_NUMBER} --json title,body,commits,files,labels` to fetch:\n     - PR title and description\n     - Commit messages and history\n     - Changed files and their diffs\n     - PR labels (bug, enhancement, etc.)\n   - Read the changed files to understand the implementation\n   - When multiple PRs are provided:\n     - Identify common JIRA issues or bug references across PRs\n     - Analyze how changes in different PRs complement each other\n     - Determine the order in which PRs should be tested (if dependencies exist)\n\n2. **Change Analysis**: Understand what was changed:\n   - For single PR:\n     - Identify the type of change (feature, bug fix, refactor, etc.)\n     - Determine affected components (API, CLI, operator, control-plane, etc.)\n     - Find related platform-specific changes (AWS, Azure, KubeVirt, etc.)\n   - For multiple PRs:\n     - Identify the overall objective (complete bug fix, multi-component feature, etc.)\n     - Map which PR addresses which component or aspect of the fix\n     - Identify overlapping or complementary changes\n     - Determine if PRs target different repositories or components\n   - Review test files to understand expected behavior\n   - Use Grep and Glob tools to:\n     - Find related configuration or documentation\n     - Locate example usage in existing tests\n     - Identify dependencies or related features\n\n3. **Test Scenario Generation**: Create comprehensive test plan:\n   - Analyze the PR description(s) for:\n     - Feature requirements and acceptance criteria\n     - Bug reproduction steps\n     - Related JIRA issues or issue references\n   - For multiple PRs, create integrated test scenarios that:\n     - Test the complete fix/feature with all PRs applied\n     - Verify each PR's contribution to the overall solution\n     - Ensure PRs work correctly together without conflicts\n   - Generate test scenarios covering:\n     - Happy path scenarios\n     - Edge cases and error handling\n     - Platform-specific variations if applicable\n     - Upgrade/downgrade scenarios if relevant\n     - Performance impact if significant changes\n\n4. **Test Guide Creation**: Create detailed manual testing document:\n   - For single PR: Save to `test-pr-{PR_NUMBER}.md`\n   - For multiple PRs: Save to `test-pr-{PR_NUMBER1}-{PR_NUMBER2}-{PR_NUMBERN}.md` (e.g., `test-pr-6888-6889-6890.md`)\n   - Include the following sections:\n     - **PR Summary**:\n       - For single PR: Title, description, and key changes\n       - For multiple PRs: List all PRs with their titles, show the common objective, and explain how they work together\n     - **Prerequisites**:\n       - Required infrastructure (AWS account, S3 bucket, etc.)\n       - Tools and CLI versions needed\n       - Environment setup steps\n     - **Test Scenarios**:\n       - Numbered test cases with clear steps\n       - Expected results for each step\n       - Verification commands and their expected output\n       - For multiple PRs: Include integration test scenarios\n     - **Regression Testing**:\n       - Suggestions for related features to verify\n     - **Notes**:\n       - Known limitations or areas requiring special attention\n       - Links to related PRs or documentation\n       - For multiple PRs: Dependencies between PRs and recommended testing order\n\n5. **Output**: Display the testing guide:\n   - Show the file path where the guide was saved\n   - Provide a brief summary of the test scenarios\n   - Highlight any critical test cases or prerequisites\n   - Ask if the user would like any modifications to the test guide\n\n## Examples:\n\n1. **Generate test steps for a single PR**:\n   `/utils:generate-test-plan https://github.com/openshift/hypershift/pull/6888`\n\n2. **Generate test steps for multiple related PRs**:\n   `/utils:generate-test-plan https://github.com/openshift/hypershift/pull/6888 https://github.com/openshift/hypershift/pull/6889 https://github.com/openshift/hypershift/pull/6890`\n\n## Arguments:\n- $1, $2, $3, ..., $N: One or more GitHub PR URLs (at least one required)\n  - Single PR: `/utils:generate-test-plan https://github.com/openshift/hypershift/pull/6888`\n  - Multiple PRs: `/utils:generate-test-plan https://github.com/openshift/hypershift/pull/6888 https://github.com/openshift/hypershift/pull/6889`\n\nThe command will provide a comprehensive manual testing guide that can be used by QE or developers to thoroughly test the PR changes. When multiple PRs are provided, the guide will include integrated test scenarios that verify the PRs work correctly together."
              },
              {
                "name": "/gh-attention",
                "description": "List PRs and issues requiring your attention",
                "path": "plugins/utils/commands/gh-attention.md",
                "frontmatter": {
                  "description": "List PRs and issues requiring your attention",
                  "argument-hint": "[--repo <org/repo>]"
                },
                "content": "## Name\nutils:gh-attention\n\n## Synopsis\n```\n/utils:gh-attention [--repo <org/repo>]\n```\n\n## Description\nThe `utils:gh-attention` command identifies pull requests and issues that are waiting for your action. It scans PRs you've authored, PRs where you're requested as a reviewer, and PRs where you've participated in discussions. For each, it detects specific states where you are the blocker: unresolved review comments, unanswered PR conversation comments, change requests you haven't addressed, merge conflicts, and unanswered questions on assigned issues.\n\nThis command helps cut through notification noise by focusing only on actionable items where others are waiting for your input.\n\n## Implementation\n\n### Step 0: Prerequisites and Setup\n\n1. **Check `gh` CLI installation and authentication**:\n   ```bash\n   gh version || echo \"gh CLI not installed. Install from https://cli.github.com/\"\n   gh auth status || gh auth login\n   ```\n\n2. **Parse arguments**:\n   - Check for `--repo <org/repo>` flag\n   - Extract repository if provided\n\n3. **Get current username**:\n   ```bash\n   CURRENT_USER=$(gh api user -q .login)\n   ```\n\n### Step 1: Repository Discovery\n\n**If `--repo` is provided**:\n- Use the specified repository\n- Skip discovery phase\n\n**If no `--repo` argument**:\n\n1. **Find all open PRs where you're involved**:\n\n   a. **PRs authored by you**:\n   ```bash\n   gh search prs --author=@me --state=open --json number,repository,title,url,isDraft --limit 100\n   ```\n\n   b. **PRs where you're requested as reviewer**:\n   ```bash\n   gh search prs --review-requested=@me --state=open --json number,repository,title,url,isDraft --limit 100\n   ```\n\n   c. **PRs where you've commented/reviewed**:\n   ```bash\n   gh search prs --commenter=@me --state=open --json number,repository,title,url,isDraft --limit 100\n   ```\n\n2. **Find all open issues assigned to current user**:\n   ```bash\n   gh search issues --assignee=@me --state=open --json number,repository,title,url --limit 100\n   ```\n\n3. **Combine and deduplicate PRs** from steps 1a, 1b, and 1c\n\n4. **Filter out draft PRs immediately** to reduce API calls\n\n5. **Extract unique repositories** from the results\n\n### Step 2: Data Collection\n\n**Use GraphQL for efficient data fetching:** GraphQL allows us to fetch all needed data in a single query and provides direct access to the `isResolved` field on review threads.\n\nFor each PR (non-draft only):\n\n1. **Fetch all PR data using GraphQL**:\n\n   Split the repository owner and name from `<REPO>` (format: `owner/name`).\n\n   ```bash\n   gh api graphql -f query='\n   query($owner: String!, $name: String!, $number: Int!) {\n     repository(owner: $owner, name: $name) {\n       pullRequest(number: $number) {\n         number\n         title\n         url\n         createdAt\n         author { login }\n         mergeable\n         reviewThreads(first: 100) {\n           nodes {\n             isResolved\n             isOutdated\n             comments(first: 20) {\n               nodes {\n                 author { login }\n                 body\n                 createdAt\n                 path\n                 line\n               }\n             }\n           }\n         }\n         reviews(first: 50) {\n           nodes {\n             author { login }\n             state\n             submittedAt\n             body\n           }\n         }\n         comments(first: 100) {\n           nodes {\n             author { login }\n             body\n             createdAt\n           }\n         }\n         commits(last: 50) {\n           nodes {\n             commit {\n               committedDate\n             }\n           }\n         }\n       }\n     }\n   }' -f owner='<OWNER>' -f name='<NAME>' -F number=<PR_NUMBER>\n   ```\n\n2. **Parse the GraphQL response** to extract:\n   - PR metadata (number, title, url, mergeable, author)\n   - Review threads with resolution status\n   - Reviews with state and timestamp\n   - PR comments (general conversation comments)\n   - Commit timestamps\n\n3. **Determine your role in the PR**:\n   - If `author.login == CURRENT_USER`: You are the PR author\n   - Otherwise: You are a reviewer/commenter\n\nFor each issue:\n\n1. **Fetch issue data using GraphQL**:\n\n   ```bash\n   gh api graphql -f query='\n   query($owner: String!, $name: String!, $number: Int!) {\n     repository(owner: $owner, name: $name) {\n       issue(number: $number) {\n         number\n         title\n         url\n         body\n         comments(first: 100) {\n           nodes {\n             author { login }\n             body\n             createdAt\n           }\n         }\n       }\n     }\n   }' -f owner='<OWNER>' -f name='<NAME>' -F number=<ISSUE_NUMBER>\n   ```\n\n2. **Parse the GraphQL response** to extract issue details and comments\n\n### Step 3: Analysis and Detection\n\n**Detection logic differs based on your role** (determined in Step 2.3):\n\n#### A. Detect Merge Conflicts (CRITICAL Priority) - **Only for PRs you authored**\n\nFor each PR where `author.login == CURRENT_USER`:\n- Check `mergeable` field from Step 2\n- If `mergeable == \"CONFLICTING\"`, flag as CRITICAL\n- Record: `Merge conflict needs resolution`\n\n#### B. Detect Review Feedback (HIGH Priority)\n\nFor each PR (from GraphQL data), check for any pending review feedback:\n\n1. **Check for unresolved review threads**:\n   - Filter review threads where:\n     - `isResolved == false` (direct from GraphQL)\n     - `isOutdated == false` (ignore outdated code comments)\n     - First comment author is not a bot (`dependabot`, `renovate`, `openshift-ci-robot`, `openshift-ci`)\n   - For each unresolved thread:\n     - Get the last comment in the thread\n     - If `last_comment.author.login == CURRENT_USER`, skip (you already responded)\n     - Otherwise, record: `path:line - @author commented (X days ago)` with snippet (first 80 chars)\n\n2. **Check for unaddressed change request reviews** - **Only for PRs you authored**:\n   - **If `author.login == CURRENT_USER`**:\n     - Filter reviews where `state == \"CHANGES_REQUESTED\"`\n     - For each change request:\n       - Compare review `submittedAt` with commit dates (`commits.nodes[].commit.committedDate`)\n       - If NO commits after the review, record: `@reviewer requested changes (X days ago)`\n       - If multiple from same reviewer, use most recent\n\n3. **Check for unanswered PR comments** (general conversation comments):\n   - Get PR comments from GraphQL response\n   - Find your last comment timestamp in the PR conversation\n   - For comments after your last comment where:\n     - `author.login != CURRENT_USER`\n     - Author is not a bot (exclude `dependabot`, `renovate`, `openshift-ci-robot`, `openshift-ci`)\n     - `body` contains `@{CURRENT_USER}` OR `body` contains `?`\n   - Record: `@author commented in PR conversation (X days ago)` with snippet (first 80 chars)\n\n4. **Combine and flag as HIGH priority** if any of: unresolved threads, unaddressed change requests (if you're the author), or unanswered PR comments exist\n\n#### C. Detect Unanswered Issue Questions (LOW Priority)\n\nFor each assigned issue (from GraphQL data):\n\n1. **Find your last comment timestamp**:\n   - Scan comments from GraphQL response\n   - Find most recent comment where `author.login == CURRENT_USER`\n\n2. **Check for questions after your last comment**:\n   - Look for comments where:\n     - `createdAt > your_last_comment_timestamp`\n     - `body` contains `@{CURRENT_USER}` OR `body` contains `?`\n     - `author.login != CURRENT_USER`\n     - Author is not a bot (exclude `dependabot`, `renovate`, `openshift-ci-robot`, `openshift-ci`)\n\n3. **Also check the issue body** for unanswered questions if you haven't commented yet\n\n4. **For each unanswered question**:\n   - Flag as LOW priority\n   - Record: `@author asked a question (X days ago)`\n   - Use comment's `createdAt` to calculate waiting time\n\n### Step 4: Calculate Waiting Time\n\nFor each detected item:\n- Calculate days/hours since the triggering event (review submitted, comment posted, conflict detected)\n- Format as human-readable: `3 days ago`, `5 hours ago`, `just now`\n\n### Step 5: Sort and Prioritize\n\n1. **Primary sort**: By priority level\n   - CRITICAL (merge conflicts)\n   - HIGH (review feedback - change requests, unresolved threads, and unanswered PR comments)\n   - LOW (issue questions)\n\n2. **Secondary sort**: By waiting time (oldest first within each priority)\n\n### Step 6: Generate Output\n\n**Header**:\n```text\nFound X items requiring attention across Y repositories\n```\n\n**For each item, display**:\n```text\n[PRIORITY_LEVEL] Repository: org/repo\n  PR #123: Title of the pull request\n  URL: https://github.com/org/repo/pull/123\n  Reason: [specific reason - e.g., \"2 unresolved comment threads\"]\n  Waiting: X days since last comment\n\n  Details:\n   path/to/file.go:45 - @reviewer asked about error handling (2 days ago)\n    \"Should we add retry logic here?\"\n   path/to/other.go:120 - @reviewer requested refactoring (3 days ago)\n    \"Consider extracting this into a helper function\"\n```\n\n**Footer summary**:\n```text\nSummary:\n  1 PR with merge conflicts\n  3 PRs with review feedback\n  1 issue with unanswered questions\n```\n\n**If no items found**:\n```text\n No items requiring attention! All caught up.\n```\n\n### Step 7: Error Handling\n\n**No repositories found**:\n```text\nNo open PRs or assigned issues found.\n```\n\n**API rate limit hit**:\n```text\n  GitHub API rate limit reached after checking X items.\nTry using --repo <org/repo> to narrow the scope.\nRate limit resets at: [timestamp from gh api rate_limit]\n```\n\n**Authentication failure**:\n```text\n GitHub CLI not authenticated.\nRun: gh auth login\n```\n\n**Repository access denied**:\n```text\n  Unable to access repository <org/repo> (private or insufficient permissions)\nSkipping...\n```\n\n## Return Value\n\nThe command outputs a prioritized list of actionable items with:\n- **Priority level**: CRITICAL, HIGH, or LOW\n- **Repository and PR/issue number**: For navigation\n- **URL**: Direct link to the item\n- **Reason**: Why it requires attention\n- **Waiting time**: How long since the triggering event\n- **Details**: Specific comments, reviewers, or questions with context\n- **Summary**: Count of items by category\n\nExit codes:\n- `0`: Success (items found or no items found)\n- `1`: Error (authentication failure, gh CLI not found)\n\n## Examples\n\n### Example 1: Check all repositories\n\n```text\n/utils:gh-attention\n```\n\nOutput:\n```text\nFound 3 items requiring attention across 2 repositories\n\n[HIGH] Repository: openshift/console\n  PR #5678: Add dark mode toggle\n  URL: https://github.com/openshift/console/pull/5678\n  Reason: 2 unresolved review threads\n  Waiting: 3 days since last comment\n\n  Details:\n   src/components/Header.tsx:120 - @designer commented (3 days ago)\n    \"Can we use the theme constant instead of hardcoding?\"\n   src/styles/theme.css:15 - @reviewer commented (2 days ago)\n    \"Should this support high contrast mode?\"\n\n[HIGH] Repository: openshift/origin\n  PR #1234: Fix authentication timeout bug\n  URL: https://github.com/openshift/origin/pull/1234\n  Reason: 1 change request, 1 unresolved thread\n  Waiting: 2 days since review\n\n  Details:\n   @reviewer-name requested changes (2 days ago)\n    \"Please add unit tests for the timeout logic\"\n   pkg/auth/handler.go:45 - @reviewer commented (1 day ago)\n    \"Should we log this error before returning?\"\n\n[LOW] Repository: openshift/enhancements\n  Issue #234: Enhancement proposal for new API\n  URL: https://github.com/openshift/enhancements/issues/234\n  Reason: Unanswered question\n  Waiting: 4 days since question\n\n  Details:\n   @team-member asked a question (4 days ago)\n    \"@you What's the timeline for implementing this?\"\n\nSummary:\n  2 PRs with review feedback\n  1 issue with unanswered questions\n```\n\n### Example 2: Check specific repository\n\n```text\n/utils:gh-attention --repo openshift/origin\n```\n\nOutput:\n```text\nFound 1 item requiring attention in openshift/origin\n\n[HIGH] Repository: openshift/origin\n  PR #1234: Fix authentication timeout bug\n  URL: https://github.com/openshift/origin/pull/1234\n  Reason: Change request not addressed\n  Waiting: 2 days since review\n\n  Details:\n   @reviewer-name requested changes (2 days ago)\n    \"Please add unit tests for the timeout logic\"\n\nSummary:\n  1 PR with unaddressed change requests\n```\n\n### Example 3: No items requiring attention\n\n```text\n/utils:gh-attention\n```\n\nOutput:\n```text\n No items requiring attention! All caught up.\n```\n\n## Arguments:\n- `--repo <org/repo>`: (Optional) Limit check to a specific repository. If omitted, checks all repositories with your open PRs or assigned issues."
              },
              {
                "name": "/placeholder",
                "description": "Placeholder command for the utils plugin",
                "path": "plugins/utils/commands/placeholder.md",
                "frontmatter": {
                  "description": "Placeholder command for the utils plugin"
                },
                "content": "## Name\nutils:placeholder\n\n## Synopsis\n```\n/utils:placeholder\n```\n\n## Description\nThis is a placeholder command for the utils plugin. The utils plugin serves as a catch-all location for introducing new generic commands. Once enough related commands are accumulated, they can be segregated into more targeted, specialized plugins.\n\nThis placeholder exists to maintain the plugin structure and will be replaced with actual utility commands as they are developed.\n\n## Implementation\nThe utils plugin provides a home for:\n- Generic helper commands that don't fit into existing specialized plugins\n- Experimental commands that may later be moved to dedicated plugins\n- Common utilities that benefit multiple workflows\n- Commands that are waiting to be grouped with similar functionality\n\n## Arguments:\nNone"
              },
              {
                "name": "/process-renovate-pr",
                "description": "Process Renovate dependency PR(s) to meet repository contribution standards",
                "path": "plugins/utils/commands/process-renovate-pr.md",
                "frontmatter": {
                  "description": "Process Renovate dependency PR(s) to meet repository contribution standards",
                  "argument-hint": "<PR_NUMBER|open> [JIRA_PROJECT] [COMPONENT]"
                },
                "content": "## Name\n\nutils:process-renovate-pr\n\n## Synopsis\n\n```\n/utils:process-renovate-pr <PR_NUMBER|open> [JIRA_PROJECT] [COMPONENT]\n```\n\n## Description\n\nThe `utils:process-renovate-pr` command automates the processing of Renovate/Konflux dependency update pull requests to meet repository contribution standards. It analyzes dependencies, creates comprehensive Jira tickets, and updates PR titles with proper references.\n\nThis command significantly reduces manual PR processing time from approximately 15 minutes to 2 minutes by automating:\n\n- Dependency analysis (direct vs indirect)\n- OpenShift version detection from release branches\n- Jira ticket creation with comprehensive details\n- PR title updates with Jira references\n- Testing strategy identification\n\nThe command can process either a single PR by number or all open dependency PRs from the Konflux bot.\n\n## Implementation\n\nThe command executes the following workflow:\n\n### 1. Validation\n\n- Verifies the PR is from `red-hat-konflux[bot]`\n- Checks PR title matches pattern: `chore(deps): update * digest to * (main)`\n- Filters out \"Pipelines as Code configuration\" PRs\n- If argument is \"open\", fetches all open dependency PRs\n\n### 2. Target Version Determination\n\n- Fetches latest state: `git fetch origin`\n- Gets commit hash for `origin/main`\n- Finds all release branches matching main's commit: `origin/release-*`\n- Selects the lowest version number (e.g., if both 4.21 and 4.22 match, uses 4.21)\n\n### 3. Dependency Analysis\n\nFrom the PR diff (go.mod changes):\n\n**Type Classification:**\n\n- Checks for `// indirect` comment in go.mod\n- For indirect: uses `go mod why <package>` to identify parent dependency\n\n**Usage Analysis:**\n\n- Direct dependencies: Searches codebase for import statements\n  - Identifies importing files\n  - Determines purpose (e.g., \"OpenStack image management\", \"AWS integration\")\n  - Distinguishes runtime code vs tooling (check hack/tools/)\n- Indirect dependencies: Documents parent dependency usage\n\n**Version Changes:**\n\n- Extracts old and new pseudo-versions from go.mod\n- Fetches upstream commit messages via GitHub API\n- Categorizes as patch/minor/major or digest update\n\n**Testing Strategy:**\n\n- hack/tools dependencies: Identifies Makefile targets\n- Runtime dependencies: Suggests component testing\n\n### 4. Jira Ticket Management\n\n- Checks existing PR comments for Jira references\n- Creates new ticket if none exists with:\n  - **Summary**: `{Package name} ({Brief purpose})`\n  - **Type**: Task\n  - **Components**: From $3 or default \"HyperShift\"\n  - **Labels**: [\"dependencies\", \"renovate\", \"ai-generated\"] plus context labels\n  - **Description**: Comprehensive details including:\n    - Dependency information (type, versions, location)\n    - Usage in repository\n    - Changes in update\n    - Step-by-step testing instructions\n  - **Target Version**: Sets customfield_12319940 to openshift-X.Y\n\n### 5. PR Title Update\n\nPosts comment with `/retitle` command and processing summary:\n\n```\n/retitle [PROJECT-XXXX](https://issues.redhat.com/browse/PROJECT-XXXX): {Package name} ({Brief description})\n```\n\nIncludes checklist of completed actions and link to Jira ticket.\n\n## Return Value\n\n- **Claude agent text**: Processing status and summary\n- **Side effects**:\n  - Jira ticket created or referenced\n  - PR comment posted with /retitle command\n  - Progress updates for multiple PRs\n\n## Examples\n\n1. **Process a single PR**:\n\n   ```\n   /utils:process-renovate-pr 7051\n   ```\n\n   Output:\n\n   ```\n    Processed PR #7051\n   - Dependency: github.com/go-logr/logr\n   - Type: Direct\n   - Jira: CNTRLPLANE-1234\n   - Target Version: openshift-4.21\n   - PR title updated with Jira reference\n   ```\n\n2. **Process with custom Jira project**:\n\n   ```\n   /utils:process-renovate-pr 7051 OCPBUGS\n   ```\n\n   Creates ticket in OCPBUGS project instead of default CNTRLPLANE.\n\n3. **Process with custom component**:\n\n   ```\n   /utils:process-renovate-pr 7051 CNTRLPLANE \"Control Plane Operator\"\n   ```\n\n   Creates ticket with specified component name.\n\n4. **Process all open dependency PRs**:\n\n   ```\n   /utils:process-renovate-pr open\n   ```\n\n   Output:\n\n   ```\n   Processing 3 dependency PRs...\n\n   [1/3] Processing PR #7051...\n    Completed PR #7051\n   - Dependency: github.com/go-logr/logr\n   - Jira: CNTRLPLANE-1234\n\n   [2/3] Processing PR #7049...\n    Completed PR #7049\n   - Dependency: golang.org/x/net\n   - Jira: CNTRLPLANE-1235\n\n   [3/3] Processing PR #7048...\n    Completed PR #7048\n   - Dependency: k8s.io/api\n   - Jira: CNTRLPLANE-1236\n\n   Summary:\n    Processed 3 PRs successfully\n   - Jira project: CNTRLPLANE\n   - Component: HyperShift\n   - Target Version: openshift-4.21\n   ```\n\n5. **Process all open PRs with custom settings**:\n   ```\n   /utils:process-renovate-pr open OCPBUGS Infrastructure\n   ```\n\n## Arguments\n\n- **$1** (required): PR number (e.g., `7051`) or `open` to process all open dependency PRs from Konflux bot\n  - When `open`: Automatically fetches and filters dependency PRs, excluding \"Pipelines as Code configuration\" PRs\n\n- **$2** (optional): Jira project key (default: `CNTRLPLANE`)\n  - Examples: `CNTRLPLANE`, `OCPBUGS`, `HOSTEDCP`\n\n- **$3** (optional): Jira component name (default: `HyperShift`)\n  - Use quotes for multi-word components: `\"Control Plane Operator\"`\n\n## Error Handling\n\nThe command handles common error cases:\n\n- **PR not from Konflux bot**: Explains requirement and exits\n- **Pipeline configuration PR**: Explains this command only handles dependency updates\n- **Jira creation failure**: Provides ticket content for manual creation\n- **Version field update failure**: Notes it may need manual setting\n- **Invalid PR number**: Validates PR exists before processing\n\n## Notes\n\n- Repository name is automatically detected from `git remote -v` (non-fork remote)\n- Direct dependencies include file-level usage analysis\n- Indirect dependencies focus on dependency chain documentation\n- Testing instructions are tailored to dependency type (tooling vs runtime)\n- All Jira tickets are labeled with \"ai-generated\" for tracking"
              },
              {
                "name": "/review-ai-helpers-overlap",
                "description": "Review potential overlaps with existing ai-helpers (Claude Code Plugins, Commands, Skills, Sub-agents, or Hooks) and open PRs",
                "path": "plugins/utils/commands/review-ai-helpers-overlap.md",
                "frontmatter": {
                  "description": "Review potential overlaps with existing ai-helpers (Claude Code Plugins, Commands, Skills, Sub-agents, or Hooks) and open PRs",
                  "argument-hint": "[--idea TEXT] [--pr NUMBER] [--verbose]"
                },
                "content": "## Name\nutils:review-ai-helpers-overlap\n\n## Synopsis\n```\n/utils:review-ai-helpers-overlap [--idea \"description\"] [--pr NUMBER] [--verbose]\n```\n\n## Description\nReview potential overlaps between your planned or implemented work and existing **Claude Code Plugins, Commands, Skills, Sub-agents, or Hooks** in the `openshift-eng/ai-helpers` repository to avoid duplicating effort and find collaboration opportunity.\n\nThis command is specifically designed for contributors and reviewers to the `openshift-eng/ai-helpers` repository. It checks for overlaps in:\n- Plugin commands (`plugins/*/commands/*.md`)\n- Skills (`plugins/*/skills/*/SKILL.md` and helper scripts)\n- Sub-agents (`agents/*.md`)\n- Hooks (`.claude/hooks/*.sh` and `.claude/hooks/*.py`)\n\n**Modes**:\n- **Idea check** (`--idea \"TEXT\"`): Check if your idea already exists before writing code\n- **Local check** (default): Compare your local changes against main branch and open PRs\n- **PR review mode** (`--pr NUMBER`): Check if a specific PR overlaps with others\n\nThis command helps contributors avoid duplicate work and helps reviewers identify related PRs that should be consolidated.\n\n## Implementation\n\nPhased duplicate detection with token-efficient early returns:\n- Checks local git changes, open PRs, and main branch commands\n- Uses semantic matching (\"analyze\"  \"inspect\") via Claude's LLM\n- Progressive disclosure: summary  findings  verbose details\n\n### Step 0: Determine Mode\n\nParse `$ARGUMENTS`:\n1. **Idea Mode**: If `--idea \"TEXT\"` provided - extract idea description\n2. **PR Review Mode**: If `--pr <NUMBER>` provided - extract PR number\n3. **Contributor Mode** (default): Check local git diff\n\nStore flags: `verbose`\n\n### Step 1: Set Repository Context\n\nThis command only works with the `openshift-eng/ai-helpers` repository.\n\nValidate repository:\n```bash\n# Check if we're in the ai-helpers repository\nREPO_URL=$(git remote get-url upstream 2>/dev/null || git remote get-url origin)\nif ! echo \"$REPO_URL\" | grep -q \"openshift-eng/ai-helpers\"; then\n  echo \"Error: This command only works in the openshift-eng/ai-helpers repository\"\n  exit 1\nfi\n\n# Hardcode repository for all gh commands\nREPO=\"openshift-eng/ai-helpers\"\n```\n\n### Step 2: Gather Context\n\n**Idea Mode**: Extract keywords from idea description using Claude's semantic understanding\n\n**Contributor Mode**:\n- Get changed files from ai-helpers structure:\n  ```bash\n  # Get all ai-helpers changes in one pass (commands, skills, agents, hooks)\n  CHANGED_FILES=$(git diff --name-only main...HEAD | grep -E '(^plugins/.*/commands/.*\\.md$|^plugins/.*/skills/.*/SKILL\\.md$|^plugins/.*/skills/.*\\.py$|^agents/.*\\.md$|^\\.claude/hooks/.*\\.(sh|py)$)')\n  ```\n- If no ai-helpers-specific files changed: Show message \"No ai-helpers changes detected (commands, skills, agents, or hooks). This tool is designed for ai-helpers repository contributions.\" and exit\n- For each file, use Read to extract: plugin name, command/skill/agent name, description (first 500 chars)\n\n**PR Review Mode**:\n- Get PR files: `gh pr view <NUMBER> --repo \"$REPO\" --json files --jq '.files[].path'`\n- Get content: `gh pr diff <NUMBER> --repo \"$REPO\" -- <file_path>`\n- Parse diff for added lines and extract metadata\n\n### Step 3: Check PR Titles (Lightweight)\n\nFetch open PRs (metadata only): `gh pr list --repo \"$REPO\" --state open --json number,title,body,author,createdAt --limit 100`\n\nMatch keywords semantically:\n- **Idea Mode**: Check PR titles/bodies for idea keywords\n- **Contributor/PR Review Mode**: Check for plugin name, command name, similar keywords\n\nIf potential overlap found, prompt user: \"Continue with deep comparison? (y/n)\"\n- If \"no\": Exit with recommendation to review the PR\n- If \"yes\" or no overlap: Continue to Step 4\n\n### Step 4: Deep Semantic Comparison\n\nRun if Step 3 flagged overlaps (user confirmed) OR `--verbose` flag OR no obvious overlaps in Step 3.\n\nFor flagged PRs:\n1. Fetch content based on file type:\n   - Commands: `gh pr diff <PR_NUMBER> --repo \"$REPO\" -- plugins/<plugin>/commands/<command>.md`\n   - Skills: `gh pr diff <PR_NUMBER> --repo \"$REPO\" -- plugins/<plugin>/skills/<skill>/SKILL.md`\n   - Agents: `gh pr diff <PR_NUMBER> --repo \"$REPO\" -- agents/<agent>.md`\n   - Hooks: `gh pr diff <PR_NUMBER> --repo \"$REPO\" -- .claude/hooks/<hook>.(sh|py)`\n2. Parse diff to extract Description and Implementation sections\n3. Claude compares purposes, workflows, functionality and assesses overlap:\n   - **HIGH (85-100%)**: Near-identical  Strong warning to collaborate\n   - **MODERATE (60-85%)**: Similar goal, different approach  Soft warning to differentiate\n   - **LOW (<60%)**: Different purposes  OK to proceed\n\n### Step 5: Check Main Branch\n\n**Idea Mode**: Scan all ai-helpers files:\n- Commands: `find plugins/*/commands/*.md`\n- Skills: `find plugins/*/skills/*/SKILL.md`\n- Agents: `find agents/*.md`\n- Hooks: `find .claude/hooks/*.{sh,py}`\n\nUse Read for frontmatter descriptions, flag if 3+ keyword overlap or semantic similarity\n\n**Contributor/PR Review Mode**: List existing files in changed areas:\n- Commands: `ls plugins/<plugin>/commands/*.md`\n- Skills: `ls plugins/<plugin>/skills/*/SKILL.md`\n- Agents: `ls agents/*.md`\n- Hooks: `ls .claude/hooks/*.{sh,py}`\n\nCheck for duplicate names and description similarity\n\n### Step 6: Present Findings\n\nUse progressive disclosure (3 layers):\n- **Layer 1 (always)**: Summary of checks performed and files analyzed\n- **Layer 2 (if overlaps)**: Detailed findings with severity ([HIGH/MODERATE/LOW OVERLAP]), PR links, and recommendations for joint effort\n- **Layer 3 (`--verbose`)**: Side-by-side comparison with differentiation suggestions\n\n### Step 7: Save Report (Optional)\n\nIf overlaps found, save detailed report to `.work/review-ai-helpers-overlap/report-{timestamp}.md` with summary, PRs compared, findings, and recommendations. Display file path to user.\n\n## Examples\n\n1. **Idea validation (before coding)**:\n   ```\n   /utils:review-ai-helpers-overlap --idea \"command to analyze Jira issues and create PRs\"\n\n   /utils:review-ai-helpers-overlap --idea \"command to analyze test coverage\"\n   ```\n   Checks if a similar command already exists in ai-helpers plugins or there are opened PR(s) for similar purpose\n\n2. **Check local changes before raising a PR** (ai-helpers contributor workflow):\n   ```\n   /utils:review-ai-helpers-overlap\n   ```\n   Checks your local command/skill/agent/hook changes against main branch and open PRs.\n\n3. **PR review (check if PR overlaps with others)**:\n   ```\n   /utils:review-ai-helpers-overlap --pr 123\n   ```\n   Check if PR #123 overlaps with other open PRs\n\n## Return Value\n\nStructured overlap analysis with progressive disclosure:\n- **Summary**: Checks performed and files analyzed\n- **Findings** (if overlaps): Severity ratings (HIGH/MODERATE/LOW), PR links, recommendations\n- **Verbose details** (`--verbose`): Side-by-side comparisons with differentiation suggestions\n\nOptionally saves report to `.work/review-ai-helpers-overlap/report-{timestamp}.md` when overlaps found.\n\n## Notes\n\n- **Repository Scope**: This command only works in the `openshift-eng/ai-helpers` repository\n- **Requirements**: GitHub CLI (`gh`) and git\n- **Best Practice**: Contributors run `/utils:review-ai-helpers-overlap --idea` during planning, run `/utils:review-ai-helpers-overlap` during developing; reviewers use `--pr <NUMBER>` during reviewing\n- Uses semantic matching (\"analyze\"  \"inspect\") and phased checks (PR titles  full diffs) for token efficiency\n\n## Arguments\n\n- `--idea \"TEXT\"`: Check if your idea exists before coding (activates idea validation mode)\n- `--pr <NUMBER>`: Check if specific PR overlaps with others (reviewer mode)\n- `--verbose`: Show detailed side-by-side comparisons (Layer 3 output)"
              }
            ],
            "skills": []
          },
          {
            "name": "olm",
            "description": "OLM (Operator Lifecycle Manager) plugin for operator management and debugging",
            "source": "./plugins/olm",
            "category": null,
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install olm@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/approve",
                "description": "Approve pending InstallPlans for operator installations and upgrades",
                "path": "plugins/olm/commands/approve.md",
                "frontmatter": {
                  "description": "Approve pending InstallPlans for operator installations and upgrades",
                  "argument-hint": "<operator-name> [namespace] [--all]"
                },
                "content": "## Name\nolm:approve\n\n## Synopsis\n```\n/olm:approve <operator-name> [namespace] [--all]\n```\n\n## Description\nThe `olm:approve` command approves pending InstallPlans for operators with manual approval mode. This is required for operators that have `installPlanApproval: Manual` in their Subscription to proceed with installation or upgrades.\n\nThis command helps you:\n- Approve operator installations that are waiting for manual approval\n- Approve operator upgrades\n- Review what will be installed/upgraded before approval\n- Batch approve multiple pending InstallPlans\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (required) - Name of the operator\n   - `$2`: Namespace (optional) - Namespace where operator is installed\n     - If not provided, searches for the operator across all namespaces\n   - `$3`: Flag (optional):\n     - `--all`: Approve all pending InstallPlans in the namespace\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - Check if user has sufficient privileges\n\n3. **Locate Operator**:\n   - If namespace provided, verify operator exists:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} --ignore-not-found\n     ```\n   - If no namespace provided, search across all namespaces:\n     ```bash\n     oc get subscription --all-namespaces -o json | jq -r '.items[] | select(.spec.name==\"{operator-name}\") | .metadata.namespace'\n     ```\n   - If not found, display error with suggestions\n\n4. **Check Subscription Approval Mode**:\n   - Get Subscription approval mode:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} -o jsonpath='{.spec.installPlanApproval}'\n     ```\n   - If mode is \"Automatic\", display informational message:\n     ```\n       Operator '{operator-name}' has automatic approval enabled.\n     InstallPlans are approved automatically and don't require manual intervention.\n     \n     Current Subscription approval mode: Automatic\n     \n     To switch to manual approval mode:\n     oc patch subscription {operator-name} -n {namespace} \\\n       --type merge --patch '{\"spec\":{\"installPlanApproval\":\"Manual\"}}'\n     ```\n   - Exit if automatic (no approval needed)\n\n5. **Find Pending InstallPlans**:\n   - Get all InstallPlans for the operator:\n     ```bash\n     oc get installplan -n {namespace} -o json\n     ```\n   - Filter for unapproved plans related to this operator:\n     ```bash\n     oc get installplan -n {namespace} -o json | \\\n       jq '.items[] | select(.spec.approved==false and .spec.clusterServiceVersionNames[] | contains(\"{operator-name}\"))'\n     ```\n   - If no pending InstallPlans found:\n     ```\n      No pending InstallPlans found for operator '{operator-name}'\n     \n     The operator is up to date or already approved.\n     \n     To check operator status: /olm:status {operator-name} {namespace}\n     ```\n     - Exit with success\n\n6. **Display InstallPlan Details**:\n   For each pending InstallPlan, display:\n   ```\n     Pending InstallPlan Found\n   \n   InstallPlan: {installplan-name}\n   Namespace: {namespace}\n   Phase: {phase}\n   Approved: false\n   \n   ClusterServiceVersions to be installed/upgraded:\n     - {csv-name-1} ({version-1})\n     - {csv-name-2} ({version-2})\n   \n   Resources to be created/updated:\n     - CustomResourceDefinitions: {crd-count}\n     - ServiceAccounts: {sa-count}\n     - ClusterRoles: {role-count}\n     - Deployments: {deployment-count}\n   \n   [If upgrade:]\n   Current Version: {current-version}\n   Target Version: {target-version}\n   ```\n\n7. **Request User Confirmation** (unless `--all` or `--force` flag):\n   - Display confirmation prompt:\n     ```\n     Do you want to approve this InstallPlan? (yes/no)\n     ```\n   - If user says no, skip this InstallPlan\n   - If user says yes, proceed to approval\n\n8. **Approve InstallPlan**:\n   - Patch the InstallPlan to approve it:\n     ```bash\n     oc patch installplan {installplan-name} -n {namespace} \\\n       --type merge --patch '{\"spec\":{\"approved\":true}}'\n     ```\n   - Verify approval:\n     ```bash\n     oc get installplan {installplan-name} -n {namespace} -o jsonpath='{.spec.approved}'\n     ```\n   - Display confirmation:\n     ```\n      InstallPlan approved: {installplan-name}\n     ```\n   - Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators\n\n9. **Monitor InstallPlan Execution** (optional):\n   - Watch InstallPlan phase change to \"Complete\":\n     ```bash\n     oc get installplan {installplan-name} -n {namespace} -w --timeout=120s\n     ```\n   - Display progress:\n     ```\n      InstallPlan executing...\n      Installing resources...\n     ```\n\n10. **Verify Installation/Upgrade**:\n    - Wait for CSV to reach \"Succeeded\" phase:\n      ```bash\n      oc get csv -n {namespace} -o json | \\\n        jq -r '.items[] | select(.status.phase==\"Succeeded\") | .metadata.name'\n      ```\n    - Display result:\n      ```\n       Operator installation/upgrade complete\n      \n      CSV: {csv-name}\n      Version: {version}\n      Phase: Succeeded\n      \n      To check operator status: /olm:status {operator-name} {namespace}\n      ```\n\n11. **Handle Multiple InstallPlans** (if `--all` flag):\n    - Process all pending InstallPlans for the operator\n    - Display summary:\n      ```\n       Approved {count} InstallPlan(s)\n      \n      Approved:\n        - {installplan-1}\n        - {installplan-2}\n      \n      Monitoring installation progress...\n      ```\n\n12. **Display Approval Summary**:\n    ```\n     Approval Complete!\n    \n    Operator: {operator-name}\n    Namespace: {namespace}\n    Approved InstallPlans: {count}\n    \n    InstallPlan Status:\n      - {installplan-1}: Complete\n      - {installplan-2}: Installing...\n    \n    Monitor progress: watch oc get csv,installplan -n {namespace}\n    ```\n\n## Return Value\n- **Success**: InstallPlan(s) approved successfully\n- **No Pending Plans**: No InstallPlans require approval\n- **Automatic Mode**: Operator has automatic approval (no action needed)\n- **Error**: Approval failed with specific error message\n- **Format**: Structured output showing:\n  - Approved InstallPlan names\n  - Installation/upgrade status\n  - Next steps or related commands\n\n## Examples\n\n1. **Approve pending InstallPlan for an operator**:\n   ```\n   /olm:approve openshift-cert-manager-operator\n   ```\n\n2. **Approve with specific namespace**:\n   ```\n   /olm:approve external-secrets-operator eso-operator\n   ```\n\n3. **Approve all pending InstallPlans**:\n   ```\n   /olm:approve openshift-cert-manager-operator cert-manager-operator --all\n   ```\n   This approves all pending InstallPlans for the operator in the namespace.\n\n4. **Check and approve after upgrade command**:\n   ```\n   /olm:upgrade openshift-cert-manager-operator --channel=tech-preview\n   # Wait for InstallPlan to be created\n   /olm:approve openshift-cert-manager-operator\n   ```\n\n## Arguments\n- **$1** (operator-name): Name of the operator (required)\n  - Example: \"openshift-cert-manager-operator\"\n  - Must match the operator's Subscription name\n- **$2** (namespace): Namespace where operator is installed (optional)\n  - If not provided, searches all namespaces\n  - Example: \"cert-manager-operator\"\n- **$3** (flag): Optional flag\n  - `--all`: Approve all pending InstallPlans for this operator\n    - Useful when multiple upgrades are pending\n    - Skips individual confirmation prompts\n\n## Notes\n\n- **Manual Approval Mode**: This command only works for operators with `installPlanApproval: Manual` in their Subscription\n- **Automatic Operators**: Operators with automatic approval don't need this command\n- **Review Before Approval**: Always review what will be installed/upgraded before approving\n- **Multiple InstallPlans**: An operator may have multiple pending InstallPlans if updates accumulated while waiting for approval\n- **InstallPlan Retention**: Approved InstallPlans remain in the namespace for audit purposes\n\n## Troubleshooting\n\n- **No pending InstallPlans**:\n  ```bash\n  # List all InstallPlans\n  oc get installplan -n {namespace}\n  \n  # Check if operator is in automatic mode\n  oc get subscription {operator-name} -n {namespace} -o jsonpath='{.spec.installPlanApproval}'\n  ```\n\n- **InstallPlan not executing after approval**:\n  ```bash\n  # Check InstallPlan status\n  oc describe installplan {installplan-name} -n {namespace}\n  \n  # Check for errors\n  oc get events -n {namespace} --sort-by='.lastTimestamp' | grep InstallPlan\n  ```\n\n- **CSV not reaching Succeeded phase**:\n  ```bash\n  # Check CSV status\n  oc describe csv -n {namespace}\n  \n  # Check operator deployment\n  oc get deployments -n {namespace}\n  \n  # Check operator logs\n  oc logs -n {namespace} deployment/{operator-deployment}\n  ```\n\n- **Permission denied**:\n  ```bash\n  # Check if you can patch InstallPlans\n  oc auth can-i patch installplan -n {namespace}\n  ```\n\n- **Multiple namespaces found**:\n  - Specify the namespace explicitly in the command:\n    ```\n    /olm:approve {operator-name} {specific-namespace}\n    ```\n\n## Related Commands\n\n- `/olm:status <operator-name>` - Check if InstallPlans are pending approval\n- `/olm:upgrade <operator-name>` - Trigger upgrade and approve in one command\n- `/olm:install <operator-name>` - Install operator with approval mode\n- `/olm:list` - List operators and their approval modes\n\n## Additional Resources\n\n- [Red Hat OpenShift: Approving Operator Upgrades](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators)\n- [Red Hat OpenShift: Updating Installed Operators](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-updating-operators)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/catalog",
                "description": "Manage catalog sources for discovering and installing operators",
                "path": "plugins/olm/commands/catalog.md",
                "frontmatter": {
                  "description": "Manage catalog sources for discovering and installing operators",
                  "argument-hint": "<list|add|remove|refresh|status> [arguments]"
                },
                "content": "## Name\nolm:catalog\n\n## Synopsis\n```\n/olm:catalog list\n/olm:catalog add <name> <image> [--namespace=openshift-marketplace]\n/olm:catalog remove <name> [--namespace=openshift-marketplace]\n/olm:catalog refresh <name> [--namespace=openshift-marketplace]\n/olm:catalog status <name> [--namespace=openshift-marketplace]\n```\n\n## Description\nThe `olm:catalog` command manages catalog sources for operator discovery and installation. Catalog sources provide the list of operators available for installation in the cluster.\n\nThis command helps you:\n- List all available catalog sources and their health status\n- Add custom or private catalog sources\n- Remove catalog sources\n- Refresh catalog sources to get latest operator updates\n\n## Implementation\n\n### Subcommand: list\n\n1. **Get All CatalogSources**:\n   ```bash\n   oc get catalogsource -n openshift-marketplace -o json\n   ```\n\n2. **Parse CatalogSource Data**:\n   For each catalog, extract:\n   - Name: `.metadata.name`\n   - Display Name: `.spec.displayName`\n   - Publisher: `.spec.publisher`\n   - Source Type: `.spec.sourceType` (grpc, configmap, etc.)\n   - Image: `.spec.image` (for grpc type)\n   - Connection State: `.status.connectionState.lastObservedState`\n   - Last Updated: `.status.connectionState.lastUpdatedTime`\n   - Number of Operators: Count from PackageManifests with this catalog\n\n3. **Get Catalog Pod Status**:\n   ```bash\n   oc get pods -n openshift-marketplace -l olm.catalogSource={catalog-name}\n   ```\n\n4. **Format Output**:\n   ```\n   \n   CATALOG SOURCES\n   \n   \n   NAME                     STATUS    OPERATORS  LAST UPDATED  SOURCE TYPE\n   redhat-operators         READY     150        2h ago        grpc\n   certified-operators      READY     45         3h ago        grpc\n   community-operators      READY     200        1h ago        grpc\n   redhat-marketplace       READY     30         4h ago        grpc\n   custom-catalog           FAILED    0          -             grpc\n   \n   \n   DETAILS\n   \n   \n   redhat-operators:\n     Display Name: Red Hat Operators\n     Publisher: Red Hat\n     Image: registry.redhat.io/redhat/redhat-operator-index:v4.20\n     Pod: redhat-operators-abc123 (Running)\n   \n   custom-catalog (FAILED):\n     Display Name: Custom Catalog\n     Publisher: My Company\n     Image: registry.example.com/custom-catalog:latest\n     Pod: custom-catalog-xyz789 (CrashLoopBackOff)\n     Error: ImagePullBackOff\n     \n     To troubleshoot:\n     /olm:catalog status custom-catalog\n   ```\n\n### Subcommand: add\n\n1. **Parse Arguments**:\n   - `name`: Catalog source name (required)\n   - `image`: Catalog image (required)\n   - `--namespace`: Target namespace (default: openshift-marketplace)\n   - `--display-name`: Display name (optional)\n   - `--publisher`: Publisher name (optional)\n\n2. **Validate Image**:\n   - Check if image format is valid\n   - Optionally test image accessibility (if possible)\n\n3. **Create CatalogSource Manifest**:\n   ```yaml\n   apiVersion: operators.coreos.com/v1alpha1\n   kind: CatalogSource\n   metadata:\n     name: {name}\n     namespace: {namespace}\n   spec:\n     sourceType: grpc\n     image: {image}\n     displayName: {display-name}\n     publisher: {publisher}\n     updateStrategy:\n       registryPoll:\n         interval: 30m\n   ```\n\n4. **Apply CatalogSource**:\n   ```bash\n   oc apply -f /tmp/catalogsource-{name}.yaml\n   ```\n\n5. **Wait for CatalogSource to be Ready**:\n   ```bash\n   oc wait --for=condition=READY catalogsource/{name} -n {namespace} --timeout=300s\n   ```\n\n6. **Verify Pod is Running**:\n   ```bash\n   oc get pods -n {namespace} -l olm.catalogSource={name}\n   ```\n\n7. **Display Result**:\n   ```\n    Catalog source added: {name}\n   \n   Name: {name}\n   Namespace: {namespace}\n   Image: {image}\n   Status: READY\n   Pod: {pod-name} (Running)\n   \n   To search operators: /olm:search --catalog {name}\n   ```\n\n### Subcommand: remove\n\n1. **Parse Arguments**:\n   - `name`: Catalog source name (required)\n   - `--namespace`: Namespace (default: openshift-marketplace)\n\n2. **Check if CatalogSource Exists**:\n   ```bash\n   oc get catalogsource {name} -n {namespace} --ignore-not-found\n   ```\n\n3. **Check for Operators Using This Catalog**:\n   ```bash\n   oc get subscription --all-namespaces -o json | \\\n     jq -r '.items[] | select(.spec.source==\"{name}\") | \"\\(.metadata.namespace)/\\(.metadata.name)\"'\n   ```\n\n4. **Display Warning** (if operators found):\n   ```\n   WARNING: The following operators are using this catalog:\n   - namespace-1/operator-1\n   - namespace-2/operator-2\n   \n   Removing this catalog will prevent these operators from receiving updates.\n   \n   Do you want to continue? (yes/no)\n   ```\n\n5. **Delete CatalogSource**:\n   ```bash\n   oc delete catalogsource {name} -n {namespace}\n   ```\n\n6. **Wait for Pod to be Deleted**:\n   ```bash\n   oc wait --for=delete pod -l olm.catalogSource={name} -n {namespace} --timeout=60s\n   ```\n\n7. **Display Result**:\n   ```\n    Catalog source removed: {name}\n   ```\n\n### Subcommand: refresh\n\n1. **Parse Arguments**:\n   - `name`: Catalog source name (required)\n   - `--namespace`: Namespace (default: openshift-marketplace)\n\n2. **Get Current CatalogSource**:\n   ```bash\n   oc get catalogsource {name} -n {namespace} -o json\n   ```\n\n3. **Trigger Refresh by Deleting Pod**:\n   ```bash\n   oc delete pod -n {namespace} -l olm.catalogSource={name}\n   ```\n   - This forces OLM to recreate the pod and re-fetch catalog data\n\n4. **Wait for New Pod to be Ready**:\n   ```bash\n   oc wait --for=condition=Ready pod -l olm.catalogSource={name} -n {namespace} --timeout=300s\n   ```\n\n5. **Verify Catalog is Updated**:\n   ```bash\n   oc get catalogsource {name} -n {namespace} -o json | \\\n     jq -r '.status.connectionState.lastUpdatedTime'\n   ```\n\n6. **Display Result**:\n   ```\n    Catalog source refreshed: {name}\n   \n   Last Updated: {timestamp}\n   Status: READY\n   Pod: {pod-name} (Running)\n   \n   New operators may now be available: /olm:search --catalog {name}\n   ```\n\n### Subcommand: status\n\n1. **Parse Arguments**:\n   - `name`: Catalog source name (required)\n   - `--namespace`: Namespace (default: openshift-marketplace)\n\n2. **Get CatalogSource Details**:\n   ```bash\n   oc get catalogsource {name} -n {namespace} -o json\n   ```\n\n3. **Get Pod Details**:\n   ```bash\n   oc get pods -n {namespace} -l olm.catalogSource={name} -o json\n   ```\n\n4. **Get Recent Events**:\n   ```bash\n   oc get events -n {namespace} --field-selector involvedObject.name={name} --sort-by='.lastTimestamp'\n   ```\n\n5. **Count Available Operators**:\n   ```bash\n   oc get packagemanifests -n openshift-marketplace -o json | \\\n     jq -r '.items[] | select(.status.catalogSource==\"{name}\") | .metadata.name' | wc -l\n   ```\n\n6. **Verify Catalog Connectivity**:\n   - Check if catalog is serving content by verifying PackageManifest count > 0\n   - If count is 0 but pod is Running, indicates connectivity or catalog index issues\n   - Review catalog pod logs for gRPC errors, image pull issues, or index corruption:\n     ```bash\n     oc logs -n {namespace} {catalog-pod-name}\n     ```\n\n7. **Format Comprehensive Status Report**:\n   ```\n   \n   CATALOG SOURCE STATUS: {name}\n   \n   \n   General Information:\n     Name: {name}\n     Namespace: {namespace}\n     Display Name: {display-name}\n     Publisher: {publisher}\n     Source Type: {source-type}\n     Image: {image}\n   \n   Connection Status:\n     State: {state} (READY | CONNECTING | CONNECTION_FAILED)\n     Last Updated: {timestamp}\n     Last Successful: {timestamp}\n   \n   Pod Status:\n     Name: {pod-name}\n     Status: {status} (Running | CrashLoopBackOff | ImagePullBackOff)\n     Ready: {ready-containers}/{total-containers}\n     Restarts: {restart-count}\n     Age: {age}\n   \n   Catalog Content:\n     Operators Available: {count}\n   \n   [If issues detected:]\n     Issues Detected:\n     - Pod in CrashLoopBackOff\n     - Last update: 24h ago (stale)\n     - Connection state: CONNECTION_FAILED\n   \n   Recent Events:\n     {timestamp} Warning: Failed to pull image\n     {timestamp} Warning: Back-off restarting failed container\n   \n   Troubleshooting Steps:\n     1. Check pod logs: oc logs -n {namespace} {pod-name}\n     2. Check image accessibility\n     3. Refresh catalog: /olm:catalog refresh {name}\n     4. Verify network connectivity (for disconnected environments)\n   \n   Related Commands:\n     - Refresh: /olm:catalog refresh {name}\n     - List operators: /olm:search --catalog {name}\n   ```\n\n## Return Value\n- **list**: Table of all catalog sources with status\n- **add**: Confirmation of added catalog with details\n- **remove**: Confirmation of removed catalog\n- **refresh**: Confirmation of refresh with updated timestamp\n- **status**: Comprehensive status report for specific catalog\n\n## Examples\n\n1. **List all catalog sources**:\n   ```\n   /olm:catalog list\n   ```\n\n2. **Add custom catalog**:\n   ```\n   /olm:catalog add my-catalog registry.example.com/my-catalog:v1.0\n   ```\n\n3. **Add catalog with metadata**:\n   ```\n   /olm:catalog add my-catalog registry.example.com/catalog:latest \\\n     --display-name=\"My Custom Catalog\" \\\n     --publisher=\"My Company\"\n   ```\n\n4. **Remove catalog**:\n   ```\n   /olm:catalog remove my-catalog\n   ```\n\n5. **Refresh catalog to get latest operators**:\n   ```\n   /olm:catalog refresh redhat-operators\n   ```\n\n6. **Check catalog health**:\n   ```\n   /olm:catalog status custom-catalog\n   ```\n\n7. **Add catalog for disconnected environment**:\n   ```\n   /olm:catalog add disconnected-operators \\\n     mirror-registry.local:5000/olm/redhat-operators:v4.20 \\\n     --namespace=openshift-marketplace\n   ```\n\n## Arguments\n\n### list\nNo arguments required.\n\n### add\n- **name** (required): Name for the catalog source\n- **image** (required): Container image containing the catalog\n- **--namespace**: Target namespace (default: openshift-marketplace)\n- **--display-name**: Human-readable display name\n- **--publisher**: Publisher/organization name\n\n### remove\n- **name** (required): Name of the catalog source to remove\n- **--namespace**: Namespace (default: openshift-marketplace)\n\n### refresh\n- **name** (required): Name of the catalog source to refresh\n- **--namespace**: Namespace (default: openshift-marketplace)\n\n### status\n- **name** (required): Name of the catalog source to check\n- **--namespace**: Namespace (default: openshift-marketplace)\n\n## Troubleshooting\n\n- **Catalog pod failing**:\n  ```bash\n  # Check pod logs\n  oc logs -n openshift-marketplace {catalog-pod-name}\n  \n  # Check image pull issues\n  oc describe pod -n openshift-marketplace {catalog-pod-name}\n  ```\n\n- **No operators showing up**:\n  ```bash\n  # Verify catalog is ready\n  /olm:catalog status {catalog-name}\n  \n  # Check PackageManifests\n  oc get packagemanifests -n openshift-marketplace\n  ```\n\n- **Image pull errors (disconnected environment)**:\n  - Verify image registry is accessible\n  - Check pull secrets are configured\n  - Ensure image has been mirrored correctly\n\n- **Stale catalog data**:\n  ```bash\n  # Force refresh\n  /olm:catalog refresh {catalog-name}\n  ```\n\n- **Connection failures**:\n  ```bash\n  # Check catalog source definition\n  oc get catalogsource {catalog-name} -n openshift-marketplace -o yaml\n  \n  # Run cluster diagnostics\n  /olm:diagnose --cluster\n  ```\n\n## Related Commands\n\n- `/olm:search` - Search for operators in catalogs\n- `/olm:install` - Install operators from catalogs\n- `/olm:diagnose` - Diagnose catalog health issues\n\n## Additional Resources\n- [Building Catalog Images with opm](https://olm.operatorframework.io/docs/tasks/creating-catalog-from-index/)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/debug",
                "description": "Debug OLM issues using must-gather logs and source code analysis",
                "path": "plugins/olm/commands/debug.md",
                "frontmatter": {
                  "description": "Debug OLM issues using must-gather logs and source code analysis",
                  "argument-hint": "<issue-description> <must-gather-path> [olm-version]"
                },
                "content": "## Name\nolm:debug\n\n## Synopsis\n```\n/olm:debug <issue-description> <must-gather-path> [olm-version]\n```\n\n## Description\nThe `olm:debug` command analyzes OLM (Operator Lifecycle Manager) issues by correlating must-gather logs with the appropriate OLM source code. It automatically determines the OCP version from the must-gather logs, checks out the corresponding branch from the relevant OLM repositories, queries Jira for known bugs in the OCPBUGS project (OLM component), and provides detailed analysis and debugging insights.\n\n## Arguments\n- **$1** (required): Issue description - A brief description of the OLM issue being investigated\n- **$2** (required): Must-gather path - Absolute or relative path to the must-gather log directory\n- **$3** (optional): OLM version - Either `olmv0` (default) or `olmv1`\n  - `olmv0`: Uses operator-framework-olm repository\n  - `olmv1`: Uses operator-framework-operator-controller and cluster-olm-operator repositories\n\n## Implementation\n\n### Phase 1: Environment Setup and Validation\n\n1. **Validate arguments**\n   - Check that issue description is provided\n   - Verify must-gather path exists and is accessible\n   - Set OLM version to `olmv0` if not specified\n\n2. **Parse must-gather logs to determine OCP version**\n   - Look for version information in must-gather logs\n   - Common locations:\n     - `cluster-scoped-resources/core/nodes/*.yaml` - check node annotations\n     - `cluster-scoped-resources/config.openshift.io/clusterversions/*.yaml`\n   - Extract OCP version (e.g., `4.14`, `4.15`, `4.16`)\n   - Determine corresponding branch name (e.g., `release-4.14`)\n\n3. **Create working directory**\n   - Use `.work/olm-debug/<timestamp>/` for temporary files\n   - Create subdirectories: `repos/`, `analysis/`, `logs/`\n\n### Phase 2: Repository Setup\n\n4. **Clone appropriate repositories based on OLM version**\n\n   **For olmv0:**\n   - Clone `https://github.com/openshift/operator-framework-olm.git`\n   - Checkout branch `release-<ocp-version>` (e.g., `release-4.14`)\n   - If branch doesn't exist, try `main` or `master` branch\n\n   **For olmv1:**\n   - Clone `https://github.com/openshift/operator-framework-operator-controller.git`\n   - Clone `https://github.com/openshift/cluster-olm-operator.git`\n   - For each repo, checkout branch `release-<ocp-version>`\n   - If branch doesn't exist, try `main` or `master` branch\n\n5. **Verify repository setup**\n   - Confirm branches are checked out successfully\n   - List key directories to understand codebase structure\n\n### Phase 3: Log Analysis\n\n6. **Extract relevant OLM logs from must-gather**\n   - For olmv0, look for:\n     - `namespaces/openshift-operator-lifecycle-manager/` logs\n     - OLM operator logs: `pods/catalog-operator-*/`, `pods/olm-operator-*/`\n     - CSV (ClusterServiceVersion) resources\n     - Subscription resources\n     - InstallPlan resources\n   - For olmv1, look for:\n     - `namespaces/openshift-operator-controller/` logs\n     - Operator controller logs\n     - ClusterExtension resources\n     - Catalog resources\n\n7. **Identify error patterns and relevant logs**\n   - Search for ERROR, WARN, FATAL level logs\n   - Extract stack traces\n   - Identify failed reconciliations\n   - Note timestamps of issues\n\n### Phase 4: Known Bug Search in Jira\n\n8. **Query Jira for known OLM bugs**\n   - Search OCPBUGS project with component \"olm\"\n   - Use Jira REST API or web scraping to fetch bugs\n   - Query parameters:\n     - Project: `OCPBUGS`\n     - Component: `olm`\n     - Affects Version: Matches the OCP version (e.g., `4.14.0`, `4.15.0`)\n     - Status: Open, In Progress, or Recently Resolved\n   - API endpoint example:\n     ```\n     https://issues.redhat.com/rest/api/2/search?jql=project=OCPBUGS AND component=olm AND affectedVersion~\"4.14\"\n     ```\n\n9. **Match errors with known bugs**\n   - Extract error messages and keywords from logs\n   - Search for matching patterns in Jira bug summaries and descriptions\n   - Look for similar symptoms in bug reports\n   - Identify potential matches based on:\n     - Error message similarity\n     - Affected OCP version\n     - Component affected (catalog-operator, olm-operator, etc.)\n     - Symptom descriptions\n\n10. **Categorize and prioritize matches**\n    - High priority: Exact error message match with same OCP version\n    - Medium priority: Similar symptoms with same component\n    - Low priority: Related issues in same version range\n    - Note bugs that have patches or workarounds available\n\n### Phase 5: Code Correlation\n\n11. **Map errors to source code**\n    - Search cloned repositories for:\n      - Error messages found in logs\n      - Function names from stack traces\n      - Related controllers and reconcilers\n    - Use grep/ripgrep to find relevant code sections\n\n12. **Analyze relevant code sections**\n    - Read the source code around identified errors\n    - Understand the reconciliation logic\n    - Identify potential root causes\n\n### Phase 6: Analysis and Recommendations\n\n13. **Generate detailed analysis report**\n    - Summary of the issue\n    - OCP and OLM version information\n    - Timeline of events from logs\n    - Known bugs section with Jira links\n    - Relevant code sections with explanations\n    - Potential root causes\n    - Recommended debugging steps\n    - Suggested fixes or workarounds\n\n14. **Create output files**\n    - `analysis.md`: Detailed analysis report\n    - `relevant-logs.txt`: Extracted relevant log entries\n    - `code-references.md`: Links to relevant source code sections with line numbers\n    - `known-bugs.md`: List of potentially related Jira bugs with match confidence\n\n### Error Handling\n\n- **Must-gather path not found**: Provide clear error message with expected path format\n- **Unable to determine OCP version**: Ask user to provide OCP version manually\n- **Repository clone failures**: Check network connectivity, provide manual clone instructions\n- **Branch not found**: Fall back to main/master branch and warn user about version mismatch\n- **No relevant logs found**: Provide guidance on what logs to look for manually\n- **Jira access failures**: Continue with analysis if Jira is unavailable; note in report that known bug search was skipped\n- **Jira authentication required**: Provide instructions for setting up Jira credentials if needed\n\n## Return Value\n\nThe command generates the following outputs in `.work/olm-debug/<timestamp>/`:\n\n- **analysis.md**: Comprehensive analysis report including:\n  - Issue summary\n  - Version information (OCP, OLM)\n  - Log analysis with timeline\n  - Known bugs section with links to matching Jira issues\n  - Code correlation and root cause analysis\n  - Recommendations\n\n- **relevant-logs.txt**: Extracted relevant log entries from must-gather\n\n- **code-references.md**: Links to relevant source code files with line numbers\n\n- **known-bugs.md**: List of potentially related Jira bugs including:\n  - Bug ID and link (e.g., OCPBUGS-12345)\n  - Bug summary and status\n  - Match confidence (High/Medium/Low)\n  - Affected versions\n  - Available workarounds or patches\n\n- **repos/**: Cloned repository directories for further manual investigation\n\n## Examples\n\n1. **Basic usage with olmv0 (default)**:\n   ```\n   /olm:debug \"CSV stuck in pending state\" /path/to/must-gather\n   ```\n\n2. **Debug olmv1 issue**:\n   ```\n   /olm:debug \"ClusterExtension installation failing\" /path/to/must-gather olmv1\n   ```\n\n3. **Debug with detailed issue description**:\n   ```\n   /olm:debug \"Operator upgrade from v1.0 to v2.0 fails with dependency resolution error\" ~/Downloads/must-gather.local.123456 olmv0\n   ```\n\n## Notes\n\n- The command requires `git` to be installed for cloning repositories\n- Network access is required to clone from GitHub and access Jira\n- Large must-gather archives may take time to process\n- The analysis is based on pattern matching and may require manual verification\n- For private repositories, ensure GitHub credentials are configured\n- Jira access to https://issues.redhat.com/ may require authentication for full access\n- Known bug matching is based on text similarity and may produce false positives\n- Always verify suggested bug matches by reading the full bug description\n\n## See Also\n\n- OLM Documentation: https://olm.operatorframework.io/\n- OpenShift OLM: https://docs.openshift.com/container-platform/latest/operators/understanding/olm/olm-understanding-olm.html\n- Must-gather documentation: https://docs.openshift.com/container-platform/latest/support/gathering-cluster-data.html\n- OCPBUGS Jira Project: https://issues.redhat.com/projects/OCPBUGS/\n- Jira REST API: https://docs.atlassian.com/jira-software/REST/latest/"
              },
              {
                "name": "/diagnose",
                "description": "Diagnose and optionally fix common OLM and operator issues",
                "path": "plugins/olm/commands/diagnose.md",
                "frontmatter": {
                  "description": "Diagnose and optionally fix common OLM and operator issues",
                  "argument-hint": "[operator-name] [namespace] [--fix] [--cluster]"
                },
                "content": "## Name\nolm:diagnose\n\n## Synopsis\n```\n/olm:diagnose [operator-name] [namespace] [--fix] [--cluster]\n```\n\n## Description\nThe `olm:diagnose` command diagnoses common OLM and operator issues, including orphaned CRDs, stuck namespaces, failed installations, and catalog source problems. It can optionally attempt to fix detected issues automatically.\n\nThis command helps you:\n- Detect and clean up orphaned CRDs from deleted operators\n- Fix namespaces stuck in Terminating state\n- Identify and resolve failed operator installations\n- Detect conflicting OperatorGroups\n- Check catalog source health\n- Identify resources preventing clean uninstallation\n- Generate comprehensive troubleshooting reports\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (optional) - Specific operator to diagnose\n   - `$2`: Namespace (optional) - Specific namespace to check\n   - `$3+`: Flags (optional):\n     - `--fix`: Automatically attempt to fix detected issues (requires confirmation)\n     - `--cluster`: Run cluster-wide diagnostics (catalog sources, global CRDs, etc.)\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - Check if user has cluster-admin or sufficient privileges\n   - Warn if running without `--fix` flag (dry-run mode)\n\n3. **Determine Scope**:\n   - **Operator-specific**: If operator name provided, focus on that operator\n   - **Namespace-specific**: If namespace provided, check all operators in that namespace\n   - **Cluster-wide**: If `--cluster` flag or no arguments, check entire cluster\n\n4. **Scan for Orphaned CRDs**:\n   - Get all CRDs in the cluster:\n     ```bash\n     oc get crd -o json\n     ```\n   - For each CRD, check if there's a corresponding operator:\n     - Look for CSVs that own this CRD\n     - Look for active Subscriptions related to this CRD\n   - Identify orphaned CRDs (no owning operator found):\n     ```bash\n     # Find CRDs without active operators\n     # This is a simplified check - actual implementation should verify operator ownership\n     oc get crd -o json | jq -r '.items[] | \n       select(.metadata.annotations[\"operators.coreos.com/owner\"] // \"\" | length == 0) | \n       .metadata.name'\n     ```\n   - Check if CRs exist for orphaned CRDs:\n     ```bash\n     oc get <crd-kind> --all-namespaces --ignore-not-found\n     ```\n   - Report findings:\n     ```\n       Orphaned CRDs Detected\n     \n     The following CRDs have no active operator:\n     - certificates.cert-manager.io (3 CR instances in 2 namespaces)\n     - issuers.cert-manager.io (5 CR instances in 3 namespaces)\n     \n     These CRDs may be leftovers from uninstalled operators.\n     \n     [If --fix flag:]\n     Do you want to delete these CRDs and their CRs? (yes/no)\n     WARNING: This will delete all custom resources of these types!\n     ```\n\n5. **Check for Stuck Namespaces**:\n   - Get all namespaces in Terminating state:\n     ```bash\n     oc get namespaces -o json | jq -r '.items[] | select(.status.phase==\"Terminating\") | .metadata.name'\n     ```\n   - For each stuck namespace:\n     - Check remaining resources:\n       ```bash\n       oc api-resources --verbs=list --namespaced -o name | \\\n         xargs -n 1 oc get --show-kind --ignore-not-found -n {namespace}\n       ```\n     - Check namespace finalizers:\n       ```bash\n       oc get namespace {namespace} -o jsonpath='{.metadata.finalizers}'\n       ```\n     - Identify blocking resources\n   - Report findings:\n     ```\n      Stuck Namespace Detected\n     \n     Namespace: {namespace}\n     State: Terminating (stuck for {duration})\n     \n     Blocking resources:\n     - CustomResourceDefinition: {crd-name} (finalizer: {finalizer})\n     - ServiceAccount: {sa-name} (token secret)\n     \n     Finalizers on namespace:\n     - kubernetes\n     \n     [If --fix flag:]\n     Attempted fixes:\n     1. Delete remaining resources\n     2. Remove finalizers from CRs\n     3. Patch namespace to remove finalizers (CAUTION)\n     \n     WARNING: Force-deleting namespace can cause cluster instability.\n     ```\n\n6. **Scan for Failed Operator Installations**:\n   - Get all CSVs not in \"Succeeded\" phase:\n     ```bash\n     oc get csv --all-namespaces -o json | \\\n       jq -r '.items[] | select(.status.phase != \"Succeeded\") | \"\\(.metadata.namespace)/\\(.metadata.name): \\(.status.phase)\"'\n     ```\n   - For each failed CSV:\n     - Get failure reason: `.status.reason`\n     - Get failure message: `.status.message`\n     - Check related InstallPlan status\n     - Check deployment status\n     - Check recent events\n   - Report findings:\n     ```\n      Failed Operator Installation\n     \n     Operator: {operator-name}\n     Namespace: {namespace}\n     CSV: {csv-name}\n     Phase: Failed\n     Reason: {reason}\n     Message: {message}\n     \n     Related InstallPlan: {installplan-name} (Phase: {phase})\n     \n     Recent Events:\n     - {timestamp} Warning: {event-message}\n     \n     Troubleshooting suggestions:\n     - Check operator logs: oc logs -n {namespace} deployment/{deployment}\n     - Check image pull issues: oc describe pod -n {namespace}\n     - Verify catalog source health\n     - Check RBAC permissions\n     ```\n\n7. **Check for Conflicting OperatorGroups**:\n   - Get all OperatorGroups per namespace:\n     ```bash\n     oc get operatorgroup --all-namespaces -o json\n     ```\n   - Identify namespaces with multiple OperatorGroups (conflict):\n     ```bash\n     oc get operatorgroup --all-namespaces -o json | \\\n       jq -r '.items | group_by(.metadata.namespace) | .[] | select(length > 1) | .[0].metadata.namespace'\n     ```\n   - Check for OperatorGroups with overlapping target namespaces\n   - Report findings:\n     ```\n       Conflicting OperatorGroups Detected\n     \n     Namespace: {namespace}\n     OperatorGroups: {count}\n     - {og-1} (targets: {target-namespaces-1})\n     - {og-2} (targets: {target-namespaces-2})\n     \n     Multiple OperatorGroups in a namespace can cause conflicts.\n     Only one OperatorGroup should exist per namespace.\n     \n     [If --fix flag:]\n     Keep which OperatorGroup? (1/2)\n     ```\n\n8. **Verify Catalog Source Health** (if `--cluster` flag):\n   - Get all CatalogSources:\n     ```bash\n     oc get catalogsource -n openshift-marketplace -o json\n     ```\n   - For each catalog:\n     - Check status: `.status.connectionState.lastObservedState`\n     - Check pod status\n     - Check last update time\n     - Verify grpc connection\n   - Report findings:\n     ```\n      Catalog Source Health Check\n     \n      redhat-operators: READY (last updated: 2h ago)\n      certified-operators: READY (last updated: 3h ago)\n      community-operators: READY (last updated: 1h ago)\n      custom-catalog: CONNECTION_FAILED (pod: CrashLoopBackOff)\n     \n     [If issues found:]\n     Unhealthy Catalog: custom-catalog\n     Pod: custom-catalog-abc123 (Status: CrashLoopBackOff)\n     \n     To troubleshoot:\n     oc logs -n openshift-marketplace custom-catalog-abc123\n     oc describe catalogsource custom-catalog -n openshift-marketplace\n     ```\n\n9. **Check for Subscription/CSV Mismatches**:\n   - Get all Subscriptions:\n     ```bash\n     oc get subscription --all-namespaces -o json\n     ```\n   - For each Subscription:\n     - Compare `installedCSV` with `currentCSV`\n     - Check if CSV exists\n     - Verify CSV phase\n   - Report findings:\n     ```\n       Subscription/CSV Mismatch\n     \n     Operator: {operator-name}\n     Namespace: {namespace}\n     Installed CSV: {installed-csv}\n     Current CSV: {current-csv}\n     \n     CSV {installed-csv} not found in namespace.\n     This may indicate a failed installation or upgrade.\n     \n     Suggested fix:\n     oc delete subscription {operator-name} -n {namespace}\n     /olm:install {operator-name} {namespace}\n     ```\n\n10. **Check for Pending Manual Approvals**:\n    - Find all unapproved InstallPlans:\n      ```bash\n      oc get installplan --all-namespaces -o json | \\\n        jq -r '.items[] | select(.spec.approved==false)'\n      ```\n    - Report findings:\n      ```\n        Pending Manual Approvals\n      \n      The following operators have pending InstallPlans requiring approval:\n      \n      - Operator: openshift-cert-manager-operator\n        Namespace: cert-manager-operator\n        InstallPlan: install-abc123\n        Target Version: v1.14.0\n        To approve: /olm:approve openshift-cert-manager-operator cert-manager-operator\n      \n      - Operator: external-secrets-operator\n        Namespace: eso-operator\n        InstallPlan: install-def456\n        Target Version: v0.11.0\n        To approve: /olm:approve external-secrets-operator eso-operator\n      ```\n\n11. **Generate Comprehensive Report**:\n    ```\n    \n    OLM HEALTH CHECK REPORT\n    \n    \n    Scan Scope: [Operator-specific | Namespace | Cluster-wide]\n    Scan Time: {timestamp}\n    \n     HEALTHY CHECKS: {count}\n    - Catalog sources operational\n    - No conflicting OperatorGroups\n    - All CSVs in Succeeded phase\n    \n      WARNINGS: {count}\n    - {warning-count} orphaned CRDs detected\n    - {warning-count} pending manual approvals\n    \n     ERRORS: {count}\n    - {error-count} stuck namespaces\n    - {error-count} failed operator installations\n    - {error-count} unhealthy catalog sources\n    \n    \n    DETAILED FINDINGS\n    \n    \n    [Details for each finding...]\n    \n    \n    RECOMMENDATIONS\n    \n    \n    1. Clean up orphaned CRDs: /olm:diagnose --fix\n    2. Fix stuck namespace: /olm:diagnose {namespace} --fix\n    3. Approve pending upgrades: /olm:approve {operator-name}\n    \n    For more details on troubleshooting, see:\n    https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-troubleshooting-operator-issues\n    ```\n\n12. **Auto-Fix Issues** (if `--fix` flag):\n    - For each detected issue, ask for confirmation\n    - Attempt fixes based on issue type:\n      - **Orphaned CRDs**: Delete CRs first, then CRDs\n      - **Stuck namespaces**: Delete remaining resources, remove finalizers\n      - **Failed installations**: Restart by deleting and recreating\n      - **Conflicting OperatorGroups**: Remove unwanted OperatorGroup\n      - **Unhealthy catalogs**: Restart catalog pod\n    - Display results of each fix attempt\n    - Generate final summary\n\n## Return Value\n- **Success**: Report generated with findings\n- **Issues Found**: Detailed report with warnings and errors\n- **Fixed**: Issues resolved (if `--fix` flag used)\n- **Format**: Structured report showing:\n  - Summary of health checks\n  - Detailed findings for each issue\n  - Recommendations and next steps\n  - Links to documentation\n\n## Examples\n\n1. **Check specific operator**:\n   ```\n   /olm:diagnose openshift-cert-manager-operator\n   ```\n\n2. **Cluster-wide health check**:\n   ```\n   /olm:diagnose --cluster\n   ```\n\n3. **Diagnose and fix issues**:\n   ```\n   /olm:diagnose openshift-cert-manager-operator cert-manager-operator --fix\n   ```\n\n4. **Full cluster scan with auto-fix**:\n   ```\n   /olm:diagnose --cluster --fix\n   ```\n\n## Arguments\n- **$1** (operator-name): Name of specific operator to diagnose (optional)\n  - If not provided, checks all operators (or cluster-wide with `--cluster`)\n  - Example: \"openshift-cert-manager-operator\"\n- **$2** (namespace): Specific namespace to check (optional)\n  - If not provided with operator-name, searches all namespaces\n  - Example: \"cert-manager-operator\"\n- **$3+** (flags): Optional flags\n  - `--fix`: Attempt to automatically fix detected issues\n    - Prompts for confirmation before each fix\n    - Use with caution in production environments\n  - `--cluster`: Run cluster-wide diagnostics\n    - Checks catalog sources\n    - Scans for orphaned CRDs across all namespaces\n    - Identifies global issues\n\n## Troubleshooting\n\n- **Permission denied**:\n  ```bash\n  # Check required permissions\n  oc auth can-i get crd\n  oc auth can-i get csv --all-namespaces\n  oc auth can-i patch namespace\n  ```\n\n- **Unable to fix stuck namespace**:\n  - Some resources may require manual intervention\n  - Check API service availability:\n    ```bash\n    oc get apiservice\n    ```\n\n- **CRDs won't delete**:\n  ```bash\n  # Check for remaining CRs\n  oc get <crd-kind> --all-namespaces\n  \n  # Check for finalizers\n  oc get crd <crd-name> -o jsonpath='{.metadata.finalizers}'\n  ```\n\n- **Catalog source issues persist**:\n  ```bash\n  # Restart catalog pod\n  oc delete pod -n openshift-marketplace <catalog-pod>\n  \n  # Check catalog source definition\n  oc get catalogsource <catalog-name> -n openshift-marketplace -o yaml\n  ```\n\n## Related Commands\n\n- `/olm:status <operator-name>` - Check specific operator status\n- `/olm:list` - List all operators\n- `/olm:uninstall <operator-name>` - Clean uninstall with orphan cleanup\n- `/olm:approve <operator-name>` - Approve pending InstallPlans\n\n## Additional Resources\n\n- [Troubleshooting Operator Issues](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-troubleshooting-operator-issues)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/install",
                "description": "Install a day-2 operator using Operator Lifecycle Manager",
                "path": "plugins/olm/commands/install.md",
                "frontmatter": {
                  "description": "Install a day-2 operator using Operator Lifecycle Manager",
                  "argument-hint": "<operator-name> [namespace] [channel] [source] [--approval=Automatic|Manual]"
                },
                "content": "## Name\nolm:install\n\n## Synopsis\n```\n/olm:install <operator-name> [namespace] [channel] [source] [--approval=Automatic|Manual]\n```\n\n## Description\nThe `olm:install` command installs a day-2 operator in an OpenShift cluster using Operator Lifecycle Manager (OLM). It automates the creation of the required namespace, OperatorGroup, and Subscription resources needed to install an operator.\n\nThis command handles the complete operator installation workflow:\n- Creates or verifies the target namespace exists\n- Creates an OperatorGroup if needed\n- Creates a Subscription to install the operator\n- Verifies the installation by checking the operator's CSV (ClusterServiceVersion) status\n- Provides detailed feedback on the installation progress\n\nThe command is designed to work with operators from the OperatorHub catalog, including Red Hat certified operators, community operators, and custom catalog sources.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (required) - The name of the operator to install (e.g., \"openshift-cert-manager-operator\")\n   - `$2`: Namespace (optional) - Target namespace for the operator. If not provided, defaults to `{operator-name}-operator` (e.g., \"cert-manager-operator\")\n   - `$3`: Channel (optional) - Subscription channel. If not provided, discovers the default channel from the operator's PackageManifest\n   - `$4`: Source (optional) - CatalogSource name. Defaults to \"redhat-operators\" for Red Hat operators\n   - `$5+`: Flags (optional):\n     - `--approval=Automatic|Manual`: InstallPlan approval mode (default: Automatic)\n     - Automatic: Operator upgrades are automatically installed\n     - Manual: Operator upgrades require manual approval via `/olm:approve` or `oc patch`\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - Check if user has cluster-admin or sufficient privileges\n   - If not installed or not authenticated, provide clear instructions\n\n3. **Discover Operator Metadata** (if channel or source not provided):\n   - Search for the operator in available catalogs:\n     ```bash\n     oc get packagemanifests -n openshift-marketplace | grep {operator-name}\n     ```\n   - Get the PackageManifest details:\n     ```bash\n     oc get packagemanifest {operator-name} -n openshift-marketplace -o json\n     ```\n   - Extract:\n     - Default channel: `.status.defaultChannel`\n     - CatalogSource: `.status.catalogSource`\n     - CatalogSourceNamespace: `.status.catalogSourceNamespace`\n   - If operator not found, provide error with list of available operators\n\n4. **Create Namespace**:\n   - Check if namespace exists: `oc get namespace {namespace} --ignore-not-found`\n   - If not exists, create it:\n     ```bash\n     oc create namespace {namespace}\n     ```\n   - If exists, inform user and continue\n\n5. **Create OperatorGroup**:\n   - Check if OperatorGroup exists in the namespace:\n     ```bash\n     oc get operatorgroup -n {namespace} --ignore-not-found\n     ```\n   - If no OperatorGroup exists, create one:\n     ```yaml\n     apiVersion: operators.coreos.com/v1\n     kind: OperatorGroup\n     metadata:\n       name: {namespace}-operatorgroup\n       namespace: {namespace}\n     spec:\n       targetNamespaces:\n       - {namespace}\n     ```\n   - Save to temporary file and apply:\n     ```bash\n     oc apply -f /tmp/operatorgroup-{operator-name}.yaml\n     ```\n   - If OperatorGroup already exists, inform user and continue\n\n6. **Create Subscription**:\n   - Parse approval mode from flags (default: Automatic)\n   - Create Subscription manifest:\n     ```yaml\n     apiVersion: operators.coreos.com/v1alpha1\n     kind: Subscription\n     metadata:\n       name: {operator-name}\n       namespace: {namespace}\n     spec:\n       channel: {channel}\n       name: {operator-name}\n       source: {source}\n       sourceNamespace: openshift-marketplace\n       installPlanApproval: {Automatic|Manual}\n     ```\n   - Save to temporary file and apply:\n     ```bash\n     oc apply -f /tmp/subscription-{operator-name}.yaml\n     ```\n   - Display the created subscription details\n   - If approval mode is Manual, display informational message:\n     ```\n       InstallPlan approval set to Manual\n     You will need to manually approve InstallPlans for this operator.\n     Use: /olm:approve {operator-name} {namespace}\n     \n     Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators\n     ```\n\n7. **Verify Installation**:\n   - Wait for InstallPlan to be created:\n     ```bash\n     oc get installplan -n {namespace} -l operators.coreos.com/operator={operator-name}\n     ```\n   - If approval mode is Manual, check if InstallPlan needs approval:\n     ```bash\n     oc get installplan -n {namespace} -o json | jq '.items[] | select(.spec.approved==false)'\n     ```\n   - If Manual and not approved, display message:\n     ```\n       InstallPlan created but requires manual approval\n     \n     InstallPlan: {installplan-name}\n     To approve: /olm:approve {operator-name} {namespace}\n     Or manually: oc patch installplan {installplan-name} -n {namespace} \\\n                    --type merge --patch '{\"spec\":{\"approved\":true}}'\n     \n     Waiting for approval...\n     ```\n   - Wait for CSV to be created and reach \"Succeeded\" phase:\n     ```bash\n     oc get csv -n {namespace} -w\n     ```\n   - Use a timeout of 5 minutes for the installation to complete (10 minutes if Manual approval)\n   - Poll every 10 seconds to check CSV status\n   - Display progress updates to the user\n\n8. **Display Results**:\n   - Show the installed operator's CSV name and version\n   - Show the operator deployment status:\n     ```bash\n     oc get deployments -n {namespace}\n     ```\n   - List any pods created by the operator:\n     ```bash\n     oc get pods -n {namespace}\n     ```\n   - Display success message with next steps or usage instructions\n\n9. **Cleanup Temporary Files**:\n   - Remove temporary YAML files created during installation:\n     ```bash\n     rm -f /tmp/operatorgroup-{operator-name}.yaml /tmp/subscription-{operator-name}.yaml\n     ```\n\n## Return Value\n- **Success**: Operator installed successfully with details about the CSV, deployments, and pods\n- **Error**: Installation failed with specific error message and troubleshooting suggestions\n- **Format**: Structured output showing:\n  - Namespace created/used\n  - OperatorGroup status\n  - Subscription created\n  - CSV status and version\n  - Deployment and pod status\n\n## Examples\n\n1. **Install cert-manager-operator with defaults**:\n   ```\n   /olm:install openshift-cert-manager-operator\n   ```\n   This will:\n   - Create namespace `cert-manager-operator`\n   - Discover default channel from PackageManifest\n   - Use `redhat-operators` catalog source\n   - Install the operator\n\n2. **Install cert-manager-operator with custom namespace**:\n   ```\n   /olm:install openshift-cert-manager-operator my-cert-manager\n   ```\n   This will install the operator in the `my-cert-manager` namespace.\n\n3. **Install with specific channel**:\n   ```\n   /olm:install openshift-cert-manager-operator cert-manager-operator stable-v1\n   ```\n   This will install from the `stable-v1` channel.\n\n4. **Install from community catalog**:\n   ```\n   /olm:install prometheus community-operators stable community-operators\n   ```\n   This will install Prometheus from the community-operators catalog.\n\n5. **Install Red Hat Advanced Cluster Security**:\n   ```\n   /olm:install rhacs-operator rhacs-operator stable\n   ```\n\n6. **Install with manual approval mode**:\n   ```\n   /olm:install openshift-cert-manager-operator cert-manager-operator stable-v1 redhat-operators --approval=Manual\n   ```\n   This will install the operator but require manual approval for all upgrades.\n\n7. **Install with all parameters specified**:\n   ```\n   /olm:install external-secrets-operator eso-operator stable-v0.10 redhat-operators --approval=Automatic\n   ```\n\n## Arguments\n- **$1** (operator-name): The name of the operator to install (required)\n  - Example: \"openshift-cert-manager-operator\"\n  - Must match the name in the operator's PackageManifest\n- **$2** (namespace): Target namespace for the operator installation (optional)\n  - Default: `{operator-name}` (operator name without \"openshift-\" prefix if present)\n  - Example: \"cert-manager-operator\"\n- **$3** (channel): Subscription channel (optional)\n  - Default: Auto-discovered from PackageManifest's default channel\n  - Example: \"stable-v1\", \"tech-preview\", \"stable\"\n- **$4** (source): CatalogSource name (optional)\n  - Default: \"redhat-operators\"\n  - Other options: \"certified-operators\", \"community-operators\", \"redhat-marketplace\"\n- **$5+** (flags): Optional flags\n  - `--approval=Automatic|Manual`: InstallPlan approval mode\n    - **Automatic** (default): Operator upgrades are automatically installed without user intervention\n    - **Manual**: Operator upgrades require explicit approval. Useful for:\n      - Production environments requiring change control\n      - Testing upgrades before applying\n      - Preventing unexpected operator updates\n    - Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators\n\n## Notes\n\n- **Automatic Channel Discovery**: If no channel is specified, the command automatically discovers and uses the operator's default channel from its PackageManifest\n- **Namespace Convention**: By default, operators are installed in a namespace following the pattern `{operator-name}-operator`\n- **OperatorGroup Scope**: The created OperatorGroup targets only the installation namespace for better isolation\n- **InstallPlan Approval**: Set to \"Automatic\" by default for seamless installation. Can be changed to \"Manual\" using `--approval=Manual` flag\n- **Manual Approval Mode**: When using `--approval=Manual`:\n  - Initial installation may require manual approval of the InstallPlan\n  - All future upgrades will require explicit approval via `/olm:approve` command\n  - Provides better control over operator updates in production environments\n- **Verification Timeout**: The command waits up to 5 minutes for the operator to install successfully (10 minutes for manual approval mode)\n- **Cleanup**: Temporary YAML files are automatically removed after installation\n\n## Troubleshooting\n\n- **Operator not found**: Run `oc get packagemanifests -n openshift-marketplace` to see available operators\n- **Permission denied**: Ensure you have cluster-admin privileges or the necessary RBAC permissions\n- **Installation timeout**: Check the InstallPlan and CSV status manually:\n  ```bash\n  oc get installplan -n {namespace}\n  oc get csv -n {namespace}\n  oc describe csv -n {namespace}\n  ```\n- **Operator pod not starting**: Check pod logs:\n  ```bash\n  oc logs -n {namespace} deployment/{operator-deployment}\n  ```"
              },
              {
                "name": "/list",
                "description": "List installed operators in the cluster",
                "path": "plugins/olm/commands/list.md",
                "frontmatter": {
                  "description": "List installed operators in the cluster",
                  "argument-hint": "[namespace] [--all-namespaces]"
                },
                "content": "## Name\nolm:list\n\n## Synopsis\n```\n/olm:list [namespace] [--all-namespaces]\n```\n\n## Description\nThe `olm:list` command lists all installed operators in an OpenShift cluster, showing their status, version, and namespace. This command provides a quick overview of the operator landscape in your cluster.\n\nThis command helps you:\n- Discover what operators are currently installed\n- Check operator versions and status at a glance\n- Identify operators that may need attention (failed, upgrading, etc.)\n- Get a comprehensive view across namespaces\n\nThe command presents information in an easy-to-read table format with key details about each operator's ClusterServiceVersion (CSV) and Subscription.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Namespace (optional) - Target namespace to list operators from\n   - `$2`: Flag (optional):\n     - `--all-namespaces` or `-A`: List operators across all namespaces (default behavior if no namespace specified)\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - If not installed or not authenticated, provide clear instructions\n\n3. **Determine Scope**:\n   - If namespace is specified: List operators only in that namespace\n   - If `--all-namespaces` flag or no arguments: List operators cluster-wide\n   - Default behavior: Show all operators across all namespaces\n\n4. **Fetch Operator Data**:\n   - Get all ClusterServiceVersions (CSVs):\n     ```bash\n     # For specific namespace\n     oc get csv -n {namespace} -o json\n     \n     # For all namespaces\n     oc get csv --all-namespaces -o json\n     ```\n   - Get all Subscriptions:\n     ```bash\n     # For specific namespace\n     oc get subscription -n {namespace} -o json\n     \n     # For all namespaces\n     oc get subscription --all-namespaces -o json\n     ```\n\n5. **Parse and Correlate Data**:\n   - For each CSV, extract:\n     - Name: `.metadata.name`\n     - Namespace: `.metadata.namespace`\n     - Display Name: `.spec.displayName`\n     - Version: `.spec.version`\n     - Phase/Status: `.status.phase` (e.g., \"Succeeded\", \"Installing\", \"Failed\")\n     - Install Time: `.metadata.creationTimestamp`\n   - For each Subscription, extract:\n     - Operator Name: `.spec.name`\n     - Channel: `.spec.channel`\n     - Source: `.spec.source`\n     - Installed CSV: `.status.installedCSV`\n     - Current CSV: `.status.currentCSV`\n   - Correlate Subscriptions with CSVs to show complete operator information\n\n6. **Format Output as Table**:\n   Create a formatted table with columns:\n   ```\n   NAMESPACE                    OPERATOR NAME                           VERSION    STATUS      CHANNEL       SOURCE\n   cert-manager-operator        cert-manager-operator                   v1.13.1    Succeeded   stable-v1     redhat-operators\n   external-secrets-operator    external-secrets-operator               v0.10.5    Succeeded   stable-v0.10  redhat-operators\n   openshift-pipelines          openshift-pipelines-operator-rh         v1.14.4    Succeeded   latest        redhat-operators\n   ```\n\n7. **Add Summary Statistics**:\n   - Total operators installed: X\n   - By status:\n     - Succeeded: X\n     - Installing: X\n     - Upgrading: X\n     - Failed: X\n   - By catalog source:\n     - redhat-operators: X\n     - certified-operators: X\n     - community-operators: X\n     - custom catalogs: X\n\n8. **Highlight Issues** (if any):\n   - List operators with status other than \"Succeeded\":\n     ```\n      Operators requiring attention:\n     - namespace/operator-name: Failed (reason: ...)\n     - namespace/operator-name: Installing (waiting for...)\n     ```\n\n9. **Provide Actionable Suggestions**:\n   - If operators are in \"Failed\" state, suggest: `/olm:status {operator-name} {namespace}` for details\n   - If no operators found, suggest: `/olm:search {operator-name}` to find available operators\n   - If upgrades available, suggest: `/olm:status {operator-name}` to check upgrade options\n\n## Return Value\n- **Success**: Formatted table of installed operators with summary statistics\n- **Empty**: No operators found message with suggestion to install operators\n- **Error**: Connection or permission error with troubleshooting guidance\n- **Format**: \n  - Table with columns: NAMESPACE, OPERATOR NAME, VERSION, STATUS, CHANNEL, SOURCE\n  - Summary statistics\n  - Warnings for operators requiring attention\n\n## Examples\n\n1. **List all operators cluster-wide**:\n   ```\n   /olm:list\n   ```\n\n2. **List operators in a specific namespace**:\n   ```\n   /olm:list cert-manager-operator\n   ``\n\n## Arguments\n- **$1** (namespace): Target namespace to list operators from (optional)\n  - If not provided, lists operators from all namespaces\n  - Example: \"cert-manager-operator\"\n- **$2** (flag): Optional flag (optional)\n  - `--all-namespaces` or `-A`: Explicitly list all operators cluster-wide\n  - Default behavior if no namespace is provided\n\n## Notes\n\n- **Performance**: For large clusters with many operators, the command may take a few seconds to collect all data\n- **Status Values**: Common CSV status values include:\n  - `Succeeded`: Operator is healthy and running\n  - `Installing`: Operator is being installed\n  - `Upgrading`: Operator is being upgraded\n  - `Failed`: Operator installation or operation failed\n  - `Replacing`: Old version being replaced\n  - `Deleting`: Operator is being removed\n- **Correlation**: The command correlates Subscriptions with CSVs to provide complete operator information\n- **Sorting**: Results are sorted by namespace, then by operator name\n\n## Troubleshooting\n\n- **Permission denied**: Ensure you have permissions to list CSVs and Subscriptions:\n  ```bash\n  oc auth can-i list csv --all-namespaces\n  oc auth can-i list subscription --all-namespaces\n  ```\n- **Slow response**: For large clusters, use namespace-specific queries to speed up results\n- **Missing operators**: Some operators may not have Subscriptions if installed manually; these will still appear based on CSV presence\n- **Version mismatch**: If Subscription's `installedCSV` differs from `currentCSV`, an upgrade may be in progress\n\n## Related Commands\n\n- `/olm:status <operator-name> [namespace]` - Get detailed status of a specific operator\n- `/olm:install <operator-name>` - Install a new operator\n- `/olm:search <query>` - Search for available operators in catalogs\n\n## Additional Resources\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/opm",
                "description": "Execute opm (Operator Package Manager) commands for building and managing operator catalogs",
                "path": "plugins/olm/commands/opm.md",
                "frontmatter": {
                  "description": "Execute opm (Operator Package Manager) commands for building and managing operator catalogs",
                  "argument-hint": "<action> [arguments...]"
                },
                "content": "## Name\nolm:opm\n\n## Synopsis\n```bash\n/olm:opm build-index-image <catalog-path> <index-image-tag> [--cacheless] [--arch=<arch>] [--base-image=<image>] [--builder-image=<image>]\n/olm:opm build-semver-index-image <semver-template-file> <index-image-tag> [--cacheless] [--arch=<arch>] [--base-image=<image>] [--builder-image=<image>]\n/olm:opm generate-semver-template <bundle-list> [--output=<file>] [--major=true|false] [--minor=true|false]\n/olm:opm list packages <index-ref>\n/olm:opm list channels <index-ref> [package-name]\n/olm:opm list bundles <index-ref> [package-name]\n```\n\n## Description\nThe `olm:opm` command provides a unified interface to `opm` (Operator Package Manager) operations for building and managing operator catalog indexes. It supports building catalog indexes, generating semver templates, and querying catalog contents.\n\n## Arguments\n- `$1`: **action** - The action to perform:\n  - `build-index-image`: Build an index from an existing catalog directory\n  - `build-semver-index-image`: Build an index from a semver template\n  - `generate-semver-template`: Generate a semver template file\n  - `list`: List catalog contents (requires second argument: `packages`, `channels`, or `bundles`)\n- `$2+`: Additional arguments specific to each action (see Actions section below)\n\n## Actions\n\n### build-index-image\nBuild an operator catalog index image from an existing catalog directory.\n\n**Synopsis:**\n```bash\n/olm:opm build-index-image <catalog-path> <index-image-tag> [--cacheless] [--arch=<arch>] [--base-image=<image>] [--builder-image=<image>]\n```\n\n**Arguments:**\n- `$2`: **catalog-path** - Path to the catalog directory containing the index configuration\n- `$3`: **index-image-tag** - Full image tag for the resulting index image (e.g., `quay.io/myorg/mycatalog:v1.0.0`)\n- `--cacheless`: Optional flag to build a cacheless image (uses `scratch` as base image; `--base-image` and `--builder-image` are ignored when this is set)\n- `--arch=<arch>`: Optional architecture specification (default: `multi` for multi-arch build; can specify single arch like `amd64`, `arm64`, `ppc64le`, `s390x`)\n- `--base-image=<image>`: Optional base image for the index (default: `quay.io/operator-framework/opm:latest`; ignored if `--cacheless` is set)\n- `--builder-image=<image>`: Optional builder image (default: `quay.io/operator-framework/opm:latest`; ignored if `--cacheless` is set)\n\n**Examples:**\n```bash\n/olm:opm build-index-image catalog quay.io/myorg/mycatalog:v1.0.0\n/olm:opm build-index-image catalog quay.io/myorg/mycatalog:v1.0.0 --cacheless\n/olm:opm build-index-image catalog quay.io/myorg/mycatalog:v1.0.0 --arch=amd64\n```\n\n### build-semver-index-image\nBuild a multi-architecture operator catalog index image using the semver template format.\n\n**Synopsis:**\n```bash\n/olm:opm build-semver-index-image <semver-template-file> <index-image-tag> [--cacheless] [--arch=<arch>] [--base-image=<image>] [--builder-image=<image>]\n```\n\n**Arguments:**\n- `$2`: **semver-template-file** - Path to the semver template configuration file (e.g., `catalog-config.yaml`)\n- `$3`: **index-image-tag** - Full image tag for the resulting index image (e.g., `quay.io/myorg/mycatalog:v1.0.0`)\n- `--cacheless`: Optional flag to build a cacheless image (uses `scratch` as base image; `--base-image` and `--builder-image` are ignored when this is set)\n- `--arch=<arch>`: Optional architecture specification (default: `multi` for multi-arch build; can specify single arch like `amd64`, `arm64`, `ppc64le`, `s390x`)\n- `--base-image=<image>`: Optional base image for the index (default: `quay.io/operator-framework/opm:latest`; ignored if `--cacheless` is set)\n- `--builder-image=<image>`: Optional builder image (default: `quay.io/operator-framework/opm:latest`; ignored if `--cacheless` is set)\n\n**Examples:**\n```bash\n/olm:opm build-semver-index-image catalog-config.yaml quay.io/myorg/mycatalog:v1.0.0\n/olm:opm build-semver-index-image catalog-config.yaml quay.io/myorg/mycatalog:v1.0.0 --cacheless\n/olm:opm build-semver-index-image catalog-config.yaml quay.io/myorg/mycatalog:v1.0.0 --arch=amd64\n/olm:opm build-semver-index-image catalog-config.yaml quay.io/myorg/mycatalog:v1.0.0 --arch=multi\n```\n\n### generate-semver-template\nGenerate a semver template configuration file for building operator catalogs.\n\n**Synopsis:**\n```bash\n/olm:opm generate-semver-template <bundle-list> [--output=<file>] [--major=true|false] [--minor=true|false]\n```\n\n**Arguments:**\n- `$2`: **bundle-list** - Comma-separated list of bundle image references (e.g., `quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.0.1`)\n- `--output=<file>`: Optional output file path (default: `catalog-semver-config.yaml` in current directory)\n- `--major=true|false`: Optional flag to generate major version channels (default: `true`)\n- `--minor=true|false`: Optional flag to generate minor version channels (default: `false`)\n\n**Examples:**\n```bash\n/olm:opm generate-semver-template quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.0.1\n/olm:opm generate-semver-template quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.0.1 --output=my-catalog.yaml\n/olm:opm generate-semver-template quay.io/org/bundle:v1.0.0,quay.io/org/bundle:v1.1.0 --minor=true\n```\n\n### list packages\nList all operator packages available in a catalog index.\n\n**Synopsis:**\n```bash\n/olm:opm list packages <index-ref>\n```\n\n**Arguments:**\n- `$2`: **list** - Must be \"list\"\n- `$3`: **packages** - Must be \"packages\"\n- `$4`: **index-ref** - Catalog index reference, either:\n  - Image tag: `quay.io/myorg/mycatalog:v1.0.0`\n  - Directory path: `./catalog` or `/path/to/catalog`\n\n**Examples:**\n```bash\n/olm:opm list packages quay.io/olmqe/nginx8518-index-test:v1\n/olm:opm list packages ./catalog\n```\n\n### list channels\nList channels for operator packages in a catalog index.\n\n**Synopsis:**\n```bash\n/olm:opm list channels <index-ref> [package-name]\n```\n\n**Arguments:**\n- `$2`: **list** - Must be \"list\"\n- `$3`: **channels** - Must be \"channels\"\n- `$4`: **index-ref** - Catalog index reference (image tag or directory path)\n- `$5`: **package-name** (Optional) - Name of a specific package to list channels for\n\n**Examples:**\n```bash\n/olm:opm list channels quay.io/olmqe/nginx8518-index-test:v1\n/olm:opm list channels quay.io/olmqe/nginx8518-index-test:v1 nginx85187\n/olm:opm list channels ./catalog\n```\n\n### list bundles\nList bundles for operator packages in a catalog index.\n\n**Synopsis:**\n```bash\n/olm:opm list bundles <index-ref> [package-name]\n```\n\n**Arguments:**\n- `$2`: **list** - Must be \"list\"\n- `$3`: **bundles** - Must be \"bundles\"\n- `$4`: **index-ref** - Catalog index reference (image tag or directory path)\n- `$5`: **package-name** (Optional) - Name of a specific package to list bundles for\n\n**Examples:**\n```bash\n/olm:opm list bundles quay.io/olmqe/nginx8518-index-test:v1\n/olm:opm list bundles quay.io/olmqe/nginx8518-index-test:v1 nginx85187\n/olm:opm list bundles ./catalog\n```\n\n## Implementation\n\n### Step 1: Parse Action\n- Extract the action from `$1`\n- Validate the action is one of: `build-index-image`, `build-semver-index-image`, `generate-semver-template`, `list`\n- If invalid action, display error with available actions\n\n### Step 2: Check Prerequisites\nVerify required tools are installed:\n- Check for `opm`: `which opm`\n  - If not found, provide installation instructions: <https://github.com/operator-framework/operator-registry/releases>\n- For build actions, also check for `podman`: `which podman`\n  - If not found, provide installation instructions based on user's platform\n\n### Step 3: Route to Action Handler\nBased on the action, call the appropriate implementation:\n\n#### For `build-index-image`:\n1. **Parse Arguments and Set Defaults**\n   - Extract catalog path from `$2`\n   - Extract index image tag from `$3`\n   - Parse optional flags: `--cacheless`, `--arch`, `--base-image`, `--builder-image`\n   - Set defaults: arch=`multi`, base-image=`quay.io/operator-framework/opm:latest`, builder-image=`quay.io/operator-framework/opm:latest`\n\n2. **Verify Catalog Directory**\n   - Check catalog directory exists: `test -d <catalog-path>`\n\n3. **Validate Catalog**\n   ```bash\n   opm validate <catalog-path>\n   ```\n\n4. **Generate Dockerfile**\n   - If cacheless: `opm generate dockerfile <catalog-path> --base-image=scratch`\n   - If normal: `opm generate dockerfile <catalog-path> -b <builder-image> -i <base-image>`\n\n5. **Determine Build Platform**\n   - If arch=`multi`: `linux/amd64,linux/arm64,linux/ppc64le,linux/s390x`\n   - Otherwise: `linux/<arch>`\n\n6. **Create Podman Manifest**\n   ```bash\n   podman manifest rm <index-image-tag> 2>/dev/null || true\n   podman manifest create <index-image-tag>\n   ```\n\n7. **Build Image**\n   ```bash\n   podman build --platform <platform-list> --manifest <index-image-tag> . -f catalog.Dockerfile\n   ```\n\n8. **Push Manifest**\n   ```bash\n   podman manifest push <index-image-tag>\n   ```\n\n9. **List Bundles in Index**\n   ```bash\n   opm alpha list bundles <index-image-tag>\n   ```\n\n10. **Display Success Message**\n\n#### For `build-semver-index-image`:\n1. **Parse Arguments and Set Defaults**\n   - Extract semver template file from `$2`\n   - Extract index image tag from `$3`\n   - Parse optional flags: `--cacheless`, `--arch`, `--base-image`, `--builder-image`\n   - Set defaults: arch=`multi`, base-image=`quay.io/operator-framework/opm:latest`, builder-image=`quay.io/operator-framework/opm:latest`\n\n2. **Verify Template File**\n   - Check file exists: `test -f <semver-template-file>`\n\n3. **Create Catalog and Render Template**\n   ```bash\n   mkdir -p catalog\n   opm alpha render-template semver <semver-template-file> -o yaml > catalog/index.yaml\n   ```\n\n4. **Validate Catalog**\n   ```bash\n   opm validate catalog\n   ```\n\n5. **Generate Dockerfile**\n   - If cacheless: `opm generate dockerfile catalog --base-image=scratch`\n   - If normal: `opm generate dockerfile catalog -b <builder-image> -i <base-image>`\n\n6. **Determine Build Platform**\n   - If arch=`multi`: `linux/amd64,linux/arm64,linux/ppc64le,linux/s390x`\n   - Otherwise: `linux/<arch>`\n\n7. **Create Podman Manifest**\n   ```bash\n   podman manifest rm <index-image-tag> 2>/dev/null || true\n   podman manifest create <index-image-tag>\n   ```\n\n8. **Build Image**\n   ```bash\n   podman build --platform <platform-list> --manifest <index-image-tag> . -f catalog.Dockerfile\n   ```\n\n9. **Push Manifest**\n   ```bash\n   podman manifest push <index-image-tag>\n   ```\n\n10. **List Bundles in Index**\n   ```bash\n   opm alpha list bundles <index-image-tag>\n   ```\n\n11. **Display Success Message**\n\n#### For `generate-semver-template`:\n1. **Parse Arguments and Set Defaults**\n   - Extract bundle list from `$2`\n   - Parse optional flags: `--output`, `--major`, `--minor`\n   - Set defaults: output=`catalog-semver-config.yaml`, major=`true`, minor=`false`\n\n2. **Validate Bundle List**\n   - Split by commas\n   - Validate each bundle is a valid image reference\n\n3. **Generate YAML Content**\n   ```yaml\n   Schema: olm.semver\n   GenerateMajorChannels: <major-value>\n   GenerateMinorChannels: <minor-value>\n   Candidate:\n     Bundles:\n     - Image: <bundle-1>\n     - Image: <bundle-2>\n   ```\n\n4. **Write Template File**\n   - Check if file exists and confirm overwrite if needed\n   - Write YAML content\n\n5. **Validate Generated File**\n   - Read back and verify YAML is well-formed\n\n6. **Display Success Message**\n   - Show file path, bundles included, settings\n   - Suggest next step: `/olm:opm build-semver-index-image <output-file> <image-tag>`\n\n#### For `list`:\n1. **Parse List Type**\n   - Extract list type from `$2` (must be `packages`, `channels`, or `bundles`)\n   - If invalid, display error with available types\n\n2. **Parse Index Reference and Optional Package**\n   - Extract index-ref from `$3`\n   - Extract optional package-name from `$4` (for channels and bundles)\n\n3. **Determine Reference Type**\n   - Check if directory: `test -d <index-ref>`\n\n4. **Execute List Command**\n   - For packages: `opm alpha list packages <index-ref>`\n   - For channels: `opm alpha list channels <index-ref> [package-name]`\n   - For bundles: `opm alpha list bundles <index-ref> [package-name]`\n\n5. **Display Results**\n   - Show the output with appropriate formatting\n   - Display count of items found\n\n## Return Value\n\n**Format**: Varies by action\n\n- **build-index-image / build-semver-index-image**: Success message with image tag, architectures, and bundle list\n- **generate-semver-template**: Success message with file path and configuration details\n- **list**: Table or list of catalog contents\n\nOn failure, displays:\n- Clear error message indicating which step/action failed\n- Relevant tool output for debugging\n- Suggestions for resolution\n\n## Notes\n\n- Ensure you are authenticated to container registries before building/pushing images (use `podman login`)\n- For build operations, the `catalog.Dockerfile` is created in the current working directory\n- Multi-architecture builds can be time-consuming\n- Cacheless builds result in smaller images and use `scratch` as the base image\n- When using `--cacheless`, the `--base-image` and `--builder-image` options are ignored (scratch is always used as base)\n- Index references can be either image tags or local directory paths\n- Bundle images must be accessible from where you build the catalog\n- Image tags should include the full registry hostname (e.g., `quay.io/org/image:tag` not `quay/org/image:tag`)\n\n## Related Commands\n\n- `/olm:install` - Install an operator using OLM\n- `/olm:catalog` - Manage catalog sources\n- `/olm:debug` - Debug OLM issues"
              },
              {
                "name": "/search",
                "description": "Search for available operators in catalog sources",
                "path": "plugins/olm/commands/search.md",
                "frontmatter": {
                  "description": "Search for available operators in catalog sources",
                  "argument-hint": "[query] [--catalog <catalog-name>]"
                },
                "content": "## Name\nolm:search\n\n## Synopsis\n```\n/olm:search [query] [--catalog <catalog-name>]\n```\n\n## Description\nThe `olm:search` command searches for available operators in the cluster's catalog sources (OperatorHub). It helps you discover operators that can be installed, showing their names, descriptions, versions, channels, and catalog sources.\n\nThis command helps you:\n- Find operators by name, description, or keywords\n- Discover what operators are available for installation\n- View operator details before installing\n- Check available versions and channels\n- Identify which catalog source contains a specific operator\n\nThe command searches across all available catalog sources (redhat-operators, certified-operators, community-operators, redhat-marketplace, and custom catalogs) and presents results in an easy-to-read format.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Query string (optional) - Search term for filtering operators\n     - If not provided, lists all available operators\n     - Can be partial name, keyword, or description\n   - `$2+`: Flags (optional):\n     - `--catalog <catalog-name>`: Limit search to specific catalog source\n     - `--exact`: Only show exact name matches\n     - `--installed`: Show only installed operators (combination with /olm:list)\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - If not installed or not authenticated, provide clear instructions\n\n3. **Fetch Catalog Data**:\n   - Get all PackageManifests from openshift-marketplace:\n     ```bash\n     oc get packagemanifests -n openshift-marketplace -o json\n     ```\n   - If `--catalog` flag is specified, filter by catalog source:\n     ```bash\n     oc get packagemanifests -n openshift-marketplace -o json | jq '.items[] | select(.status.catalogSource==\"{catalog-name}\")'\n     ```\n\n4. **Parse PackageManifest Data**:\n   - For each PackageManifest, extract:\n     - Name: `.metadata.name`\n     - Display Name: `.status.channels[0].currentCSVDesc.displayName`\n     - Description: `.status.channels[0].currentCSVDesc.description`\n     - Provider: `.status.provider.name`\n     - Catalog Source: `.status.catalogSource`\n     - Catalog Namespace: `.status.catalogSourceNamespace`\n     - Default Channel: `.status.defaultChannel`\n     - All Channels: `.status.channels[].name`\n     - Latest Version: `.status.channels[] | select(.name==.status.defaultChannel) | .currentCSVDesc.version`\n     - Categories: `.status.channels[0].currentCSVDesc.annotations[\"categories\"]`\n     - Capabilities: `.status.channels[0].currentCSVDesc.annotations[\"capabilities\"]`\n\n5. **Apply Search Filter** (if query provided):\n   - Case-insensitive search across:\n     - Operator name (`.metadata.name`)\n     - Display name (`.status.channels[0].currentCSVDesc.displayName`)\n     - Description (`.status.channels[0].currentCSVDesc.description`)\n     - Provider name (`.status.provider.name`)\n     - Categories\n   - If `--exact` flag, only match exact operator names\n\n6. **Sort Results**:\n   - Primary sort: By catalog source (redhat-operators first, then certified, community, etc.)\n   - Secondary sort: By operator name alphabetically\n\n7. **Format Search Results**:\n   \n   **A. Summary Header**\n   ```\n   Found X operators matching \"{query}\"\n   ```\n   \n   **B. Results List**\n   For each operator:\n   ```\n   \n    cert-manager-operator for Red Hat OpenShift\n   \n    Name:        openshift-cert-manager-operator\n    Provider:    Red Hat\n    Catalog:     redhat-operators\n    Default:     stable-v1\n    Channels:    stable-v1, tech-preview-v1.13\n    Version:     v1.13.1\n    Categories:  Security\n    \n    Description: Manages the lifecycle of TLS certificates...\n    \n    Install:     /olm:install openshift-cert-manager-operator\n   \n   ```\n\n8. **Group by Catalog** (optional, for better readability):\n   ```\n   \n   RED HAT OPERATORS (3)\n   \n   \n   [List of operators from redhat-operators]\n   \n   \n   CERTIFIED OPERATORS (1)\n   \n   \n   [List of operators from certified-operators]\n   \n   \n   COMMUNITY OPERATORS (2)\n   \n   \n   [List of operators from community-operators]\n   ```\n\n9. **Provide Installation Guidance**:\n   - For each operator, show ready-to-use install command:\n     ```\n     To install: /olm:install {operator-name}\n     ```\n   - For operators with specific channel recommendations, note them\n\n10. **Handle No Results**:\n    - If no operators match the query:\n      ```\n      No operators found matching \"{query}\"\n      \n      Suggestions:\n      - Try a broader search term\n      - List all available operators: /olm:search\n      - Check specific catalog: /olm:search {query} --catalog redhat-operators\n      ```\n\n11. **Show Popular/Recommended Operators** (if no query provided):\n    - Highlight commonly used operators:\n      - cert-manager\n      - external-secrets-operator\n      - OpenShift Pipelines\n      - OpenShift GitOps\n      - Service Mesh\n      - etc.\n\n## Return Value\n- **Success**: List of matching operators with detailed information\n- **No Results**: Message indicating no matches with suggestions\n- **Error**: Connection or permission error with troubleshooting guidance\n- **Format**:\n  - Summary of search results\n  - Detailed operator information cards\n  - Installation commands for each operator\n  - Grouped by catalog source\n\n## Examples\n\n1. **Search for cert-manager operator**:\n   ```\n   /olm:search cert-manager\n   ```\n\n2. **Search for secrets-related operators**:\n   ```\n   /olm:search secrets\n   ```\n   Output listing multiple operators related to secrets management.\n\n3. **List all operators** (no query):\n   ```\n   /olm:search\n   ```\n\n4. **Search in specific catalog**:\n   ```\n   /olm:search prometheus --catalog community-operators\n   ```\n   Output showing only Prometheus-related operators from community-operators catalog.\n\n5. **Exact name match**:\n   ```\n   /olm:search external-secrets-operator --exact\n   ```\n   Output showing only the exact match for external-secrets-operator.\n\n6. **Search for operators by category** (e.g., security):\n   ```\n   /olm:search security\n   ```\n   Output listing all security-related operators.\n\n## Arguments\n- **$1** (query): Search term to filter operators (optional)\n  - If not provided, lists all available operators (may be very long)\n  - Searches across name, display name, description, provider\n  - Case-insensitive partial matching\n  - Example: \"cert\", \"secrets\", \"security\", \"monitoring\"\n- **$2+** (flags): Optional flags\n  - `--catalog <catalog-name>`: Limit search to specific catalog\n    - Values: \"redhat-operators\", \"certified-operators\", \"community-operators\", \"redhat-marketplace\", or custom catalog name\n  - `--exact`: Only show exact name matches (no partial matching)\n  - `--installed`: Show only operators that are currently installed\n\n\n## Troubleshooting\n\n- **No operators found**: \n  - Verify catalog sources are available:\n    ```bash\n    oc get catalogsources -n openshift-marketplace\n    ```\n  - Check if catalog sources are healthy:\n    ```bash\n    oc get pods -n openshift-marketplace\n    ```\n- **Slow search**:\n  - Use more specific search terms\n  - Search in specific catalog: `--catalog redhat-operators`\n- **Incomplete information**:\n  - Some operators may have limited metadata in their PackageManifest\n- **Permission denied**:\n  - Ensure you can read PackageManifests:\n    ```bash\n    oc auth can-i list packagemanifests -n openshift-marketplace\n    ```\n\n## Related Commands\n\n- `/olm:install <operator-name>` - Install an operator found in search results\n- `/olm:list` - List installed operators\n- `/olm:status <operator-name>` - Check status of an installed operator\n\n## Additional Resources\n\n- [OperatorHub.io](https://operatorhub.io/) - Browse operators online\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/status",
                "description": "Get detailed status and health information for an operator",
                "path": "plugins/olm/commands/status.md",
                "frontmatter": {
                  "description": "Get detailed status and health information for an operator",
                  "argument-hint": "<operator-name> [namespace]"
                },
                "content": "## Name\nolm:status\n\n## Synopsis\n```\n/olm:status <operator-name> [namespace]\n```\n\n## Description\nThe `olm:status` command provides comprehensive health and status information for a specific operator in an OpenShift cluster. It displays detailed information about the operator's CSV, Subscription, InstallPlan, deployments, and pods to help diagnose issues and verify proper operation.\n\nThis command helps you:\n- Check if an operator is running correctly\n- Diagnose installation or upgrade problems\n- View operator version and available updates\n- Inspect operator deployments and pods\n- Review recent events and conditions\n- Identify resource issues or configuration problems\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (required) - Name of the operator to inspect\n   - `$2`: Namespace (optional) - Namespace where operator is installed\n     - If not provided, searches for the operator across all namespaces\n     - If multiple instances found, prompts user to specify namespace\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - If not installed or not authenticated, provide clear instructions\n\n3. **Locate Operator**:\n   - If namespace provided, verify operator exists in that namespace:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} --ignore-not-found\n     ```\n   - If no namespace provided, search across all namespaces:\n     ```bash\n     oc get subscription --all-namespaces -o json | jq -r '.items[] | select(.spec.name==\"{operator-name}\") | .metadata.namespace'\n     ```\n   - If not found, display error with suggestions\n   - If multiple instances found, list them and ask user to specify namespace\n\n4. **Gather Subscription Information**:\n   - Get Subscription details:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} -o json\n     ```\n   - Extract:\n     - Channel: `.spec.channel`\n     - Install Plan Approval: `.spec.installPlanApproval`\n     - Source: `.spec.source`\n     - Source Namespace: `.spec.sourceNamespace`\n     - Installed CSV: `.status.installedCSV`\n     - Current CSV: `.status.currentCSV`\n     - State: `.status.state`\n     - Conditions: `.status.conditions[]`\n\n5. **Gather CSV Information**:\n   - Get CSV details:\n     ```bash\n     oc get csv {csv-name} -n {namespace} -o json\n     ```\n   - Extract:\n     - Display Name: `.spec.displayName`\n     - Version: `.spec.version`\n     - Phase: `.status.phase`\n     - Message: `.status.message`\n     - Reason: `.status.reason`\n     - Creation Time: `.metadata.creationTimestamp`\n     - Conditions: `.status.conditions[]`\n     - Requirements: `.status.requirementStatus[]`\n\n6. **Gather InstallPlan Information**:\n   - Get related InstallPlans:\n     ```bash\n     oc get installplan -n {namespace} -o json\n     ```\n   - Find InstallPlans related to this operator by checking `.spec.clusterServiceVersionNames`\n   - Extract:\n     - Name: `.metadata.name`\n     - Phase: `.status.phase` (e.g., \"Complete\", \"Installing\", \"Failed\")\n     - Approved: `.spec.approved`\n     - Bundle Resources: `.status.bundleLookups[]`\n\n7. **Gather Deployment Information**:\n   - Get deployments owned by the CSV:\n     ```bash\n     oc get deployments -n {namespace} -o json\n     ```\n   - Filter deployments with owner reference to the CSV\n   - For each deployment, extract:\n     - Name: `.metadata.name`\n     - Ready Replicas: `.status.readyReplicas` / `.status.replicas`\n     - Available: `.status.availableReplicas`\n     - Conditions: `.status.conditions[]`\n\n8. **Gather Pod Information**:\n   - Get pods managed by operator deployments:\n     ```bash\n     oc get pods -n {namespace} -l app={operator-label} -o json\n     ```\n   - For each pod, extract:\n     - Name: `.metadata.name`\n     - Status: `.status.phase`\n     - Ready: Count of ready containers vs total\n     - Restarts: Sum of `.status.containerStatuses[].restartCount`\n     - Age: Calculate from `.metadata.creationTimestamp`\n\n9. **Check for Recent Events**:\n   - Get events related to the operator:\n     ```bash\n     oc get events -n {namespace} --field-selector involvedObject.name={csv-name} --sort-by='.lastTimestamp'\n     ```\n   - Show last 5-10 events, especially warnings and errors\n\n10. **Check for Available Updates**:\n    - Get PackageManifest to check for newer versions:\n      ```bash\n      oc get packagemanifest {operator-name} -n openshift-marketplace -o json\n      ```\n    - Extract current channel information:\n      - Current channel from Subscription: `.spec.channel`\n      - Latest version in current channel\n      - Available channels\n    - Compare installed CSV version with latest available version\n    - Check for pending InstallPlans:\n      ```bash\n      oc get installplan -n {namespace} -o json | jq '.items[] | select(.spec.approved==false)'\n      ```\n    - Determine if manual approval is required:\n      ```bash\n      oc get subscription {operator-name} -n {namespace} -o jsonpath='{.spec.installPlanApproval}'\n      ```\n    - Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators\n\n11. **Format Comprehensive Report**:\n    Create a structured report with sections:\n    \n    **A. Overview**\n    ```\n    Operator: {display-name}\n    Name: {operator-name}\n    Namespace: {namespace}\n    Version: {version}\n    Status: {phase}\n    ```\n    \n    **B. Subscription**\n    ```\n    Channel: {channel}\n    Source: {source}\n    Install Plan Approval: {approval-mode} (Automatic|Manual)\n    State: {state}\n    Installed CSV: {installed-csv-name}\n    Current CSV: {current-csv-name}\n    ```\n    \n    **C. ClusterServiceVersion (CSV)**\n    ```\n    Name: {csv-name}\n    Phase: {phase}\n    Message: {message}\n    Requirements: [list requirements status]\n    ```\n    \n    **D. InstallPlan**\n    ```\n    Name: {installplan-name}\n    Phase: {phase} (Complete|Installing|RequiresApproval|Failed)\n    Approved: {true/false}\n    \n    [If Phase=RequiresApproval and Approved=false:]\n      Manual approval required for installation/upgrade\n    To approve: /olm:approve {operator-name} {namespace}\n    Or manually: oc patch installplan {installplan-name} -n {namespace} \\\n                   --type merge --patch '{\"spec\":{\"approved\":true}}'\n    ```\n    \n    **E. Deployments**\n    ```\n    NAME                     READY   AVAILABLE   AGE\n    cert-manager             1/1     1           5d\n    cert-manager-webhook     1/1     1           5d\n    ```\n    \n    **F. Pods**\n    ```\n    NAME                                      STATUS    READY   RESTARTS   AGE\n    cert-manager-7d4f8f8b4-abcde             Running   1/1     0          5d\n    cert-manager-webhook-6b7c9d5f-fghij      Running   1/1     0          5d\n    ```\n    \n    **G. Recent Events** (if any warnings/errors)\n    ```\n    5m    Warning   InstallPlanFailed   Failed to install...\n    2m    Normal    InstallSucceeded    Successfully installed\n    ```\n    \n    **H. Update Information**\n    ```\n    Current Version: {current-version}\n    Latest Available: {latest-version} (in channel: {channel})\n    Update Status: [Up to date | Update available | Unknown]\n    \n    Available Channels:\n    - stable-v1 (latest: v1.13.1)\n    - tech-preview-v1.14 (latest: v1.14.0)\n    \n    [If update available in current channel:]\n     Update available: {current-version}  {latest-version}\n    To update: /olm:upgrade {operator-name} {namespace}\n    \n    [If newer version in different channel:]\n     Newer version available in channel '{new-channel}': {newer-version}\n    To switch channels: /olm:upgrade {operator-name} {namespace} --channel={new-channel}\n    ```\n    \n    **I. Health Summary**\n    ```\n     Operator is healthy and running\n     Operator has warnings (see events)\n     Operator is not healthy (see details)\n     Operator is upgrading (Current: {old-version}  Target: {new-version})\n      Operator upgrade pending manual approval\n    ```\n\n12. **Provide Actionable Recommendations**:\n    - If operator is failed: \n      ```\n       Operator failed: {reason}\n      \n      Troubleshooting steps:\n      1. Check operator logs: oc logs -n {namespace} deployment/{operator-deployment}\n      2. Check events: oc get events -n {namespace} --sort-by='.lastTimestamp'\n      3. Check CSV conditions: oc describe csv {csv-name} -n {namespace}\n      4. Run diagnostics: /olm:diagnose {operator-name} {namespace}\n      ```\n    - If upgrade available:\n      ```\n       Update available: {current}  {latest}\n      To upgrade: /olm:upgrade {operator-name} {namespace}\n      ```\n    - If pods are crashing:\n      ```\n       Pods are crashing (restarts: {count})\n      Check logs: oc logs -n {namespace} {pod-name}\n      Previous logs: oc logs -n {namespace} {pod-name} --previous\n      ```\n    - If InstallPlan requires approval:\n      ```\n        InstallPlan requires manual approval\n      \n      InstallPlan: {installplan-name}\n      Version: {target-version}\n      \n      To approve: /olm:approve {operator-name} {namespace}\n      Or manually: oc patch installplan {installplan-name} -n {namespace} \\\n                     --type merge --patch '{\"spec\":{\"approved\":true}}'\n      \n      To switch to automatic approvals:\n      oc patch subscription {operator-name} -n {namespace} \\\n        --type merge --patch '{\"spec\":{\"installPlanApproval\":\"Automatic\"}}'\n      ```\n    - If operator is upgrading:\n      ```\n       Operator upgrade in progress: {old-version}  {new-version}\n      Monitor progress: watch oc get csv,installplan -n {namespace}\n      ```\n\n## Return Value\n- **Success**: Comprehensive status report with all operator details\n- **Not Found**: Error message with suggestions to list operators or check spelling\n- **Multiple Instances**: List of namespaces where operator is installed\n- **Error**: Connection or permission error with troubleshooting guidance\n- **Format**: Multi-section report with:\n  - Overview\n  - Subscription details\n  - CSV status\n  - InstallPlan status\n  - Deployment status\n  - Pod status\n  - Recent events\n  - Health summary\n  - Recommendations\n\n## Examples\n\n1. **Check status of cert-manager operator**:\n   ```\n   /olm:status openshift-cert-manager-operator\n   ```\n\n2. **Check status with specific namespace**:\n   ```\n   /olm:status external-secrets-operator external-secrets-operator\n   ```\n\n## Arguments\n- **$1** (operator-name): Name of the operator to inspect (required)\n  - Example: \"openshift-cert-manager-operator\"\n  - Must match the operator's Subscription name\n- **$2** (namespace): Namespace where operator is installed (optional)\n  - If not provided, searches all namespaces\n  - Example: \"cert-manager-operator\"\n\n## Notes\n\n- **Comprehensive View**: This command aggregates data from multiple resources (Subscription, CSV, InstallPlan, Deployments, Pods) for a complete picture\n- **Permissions**: Requires read permissions for subscriptions, csvs, installplans, deployments, pods, and events in the target namespace\n- **Performance**: May take a few seconds to gather all information for large operators with many resources\n- **Auto-Discovery**: If namespace is not specified, the command automatically finds the operator across all namespaces\n- **Health Checks**: The command evaluates multiple factors to determine overall operator health\n- **Troubleshooting**: Provides context-aware recommendations based on detected issues\n\n## Troubleshooting\n\n- **Operator not found**: \n  - Verify operator name: `oc get subscriptions --all-namespaces | grep {operator-name}`\n  - List all operators: `/olm:list`\n- **Multiple instances found**:\n  - Specify namespace explicitly: `/olm:status {operator-name} {namespace}`\n- **Permission denied**:\n  - Ensure you have read permissions in the target namespace\n  - Check: `oc auth can-i get csv -n {namespace}`\n- **Incomplete information**:\n  - Some operators may not have all resources (e.g., manually installed CSVs without Subscriptions)\n\n## Related Commands\n\n- `/olm:list` - List all installed operators\n- `/olm:install <operator-name>` - Install a new operator\n- `/olm:uninstall <operator-name>` - Uninstall an operator\n- `/olm:upgrade <operator-name>` - Upgrade an operator\n- `/olm:approve <operator-name>` - Approve pending InstallPlans\n- `/olm:diagnose <operator-name>` - Diagnose and fix operator issues\n\n## Additional Resources\n\n- [Viewing Operator Status](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-status-viewing-operator-status)\n- [Updating Installed Operators](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-updating-operators)\n- [Troubleshooting Operator Issues](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-troubleshooting-operator-issues)"
              },
              {
                "name": "/uninstall",
                "description": "Uninstall a day-2 operator and optionally remove its resources",
                "path": "plugins/olm/commands/uninstall.md",
                "frontmatter": {
                  "description": "Uninstall a day-2 operator and optionally remove its resources",
                  "argument-hint": "<operator-name> [namespace] [--remove-crds] [--remove-namespace]"
                },
                "content": "## Name\nolm:uninstall\n\n## Synopsis\n```\n/olm:uninstall <operator-name> [namespace] [--remove-crds] [--remove-namespace]\n```\n\n## Description\nThe `olm:uninstall` command uninstalls a day-2 operator from an OpenShift cluster by removing its Subscription, ClusterServiceVersion (CSV), and optionally its Custom Resource Definitions (CRDs) and namespace.\n\nThis command provides a comprehensive uninstallation workflow:\n- Removes the operator's Subscription\n- Deletes the ClusterServiceVersion (CSV)\n- Optionally removes operator-managed deployments\n- Optionally deletes Custom Resource Definitions (CRDs)\n- Optionally removes the operator's namespace\n- Provides detailed feedback on each step\n\nThe command is designed to safely clean up operators installed via OLM, with optional flags for thorough cleanup of all operator-related resources.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (required) - The name of the operator to uninstall\n   - `$2`: Namespace (optional) - The namespace where operator is installed. If not provided, defaults to `{operator-name}-operator`\n   - `$3+`: Flags (optional):\n     - `--remove-crds`: Remove Custom Resource Definitions after uninstalling\n     - `--remove-namespace`: Remove the operator's namespace after cleanup\n     - `--force`: Skip confirmation prompts\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - Check if user has cluster-admin or sufficient privileges\n\n3. **Verify Operator Installation**:\n   - Check if namespace exists:\n     ```bash\n     oc get namespace {namespace} --ignore-not-found\n     ```\n   - Check if subscription exists:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} --ignore-not-found\n     ```\n   - If not found, display error: \"Operator {operator-name} is not installed in namespace {namespace}\"\n   - List what will be uninstalled\n\n4. **Display Uninstallation Plan**:\n   - Show operator details:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} -o yaml\n     oc get csv -n {namespace}\n     ```\n   - Display what will be removed:\n     - Subscription name and namespace\n     - CSV name and version\n     - Deployments (if any)\n     - CRDs (if `--remove-crds` flag is set)\n     - Namespace (if `--remove-namespace` flag is set)\n\n5. **Request User Confirmation** (unless `--force` flag is set):\n   - Display warning:\n     ```\n     WARNING: You are about to uninstall {operator-name} from namespace {namespace}.\n     This will remove:\n       - Subscription: {subscription-name}\n       - ClusterServiceVersion: {csv-name}\n       - Operator deployments\n       [- Custom Resource Definitions (if --remove-crds is set)]\n       [- Namespace {namespace} (if --remove-namespace is set)]\n     \n     Are you sure you want to continue? (yes/no)\n     ```\n   - Wait for user confirmation\n   - If user says no, abort operation\n\n6. **Delete Subscription**:\n   - Remove the operator's subscription:\n     ```bash\n     oc delete subscription {operator-name} -n {namespace}\n     ```\n   - Verify deletion:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} --ignore-not-found\n     ```\n   - Display result\n\n7. **Delete ClusterServiceVersion (CSV)**:\n   - Get the CSV name:\n     ```bash\n     oc get csv -n {namespace} -o jsonpath='{.items[?(@.spec.displayName contains \"{operator-name}\")].metadata.name}'\n     ```\n   - Delete the CSV:\n     ```bash\n     oc delete csv {csv-name} -n {namespace}\n     ```\n   - This will automatically remove operator deployments\n   - Verify CSV is deleted:\n     ```bash\n     oc get csv -n {namespace} --ignore-not-found\n     ```\n\n8. **Remove Operator Deployments** (if still present):\n   - List deployments created by the operator:\n     ```bash\n     oc get deployments -n {namespace}\n     ```\n   - For operators like cert-manager with labeled resources:\n     ```bash\n     oc delete deployment -n {namespace} -l app.kubernetes.io/instance={operator-base-name}\n     ```\n   - Verify deployments are deleted:\n     ```bash\n     oc get deployments -n {namespace}\n     ```\n\n8.5. **Check for Orphaned Custom Resources** (before removing CRDs):\n   - Get list of CRDs managed by the operator from CSV:\n     ```bash\n     oc get csv -n {namespace} -o jsonpath='{.items[0].spec.customresourcedefinitions.owned[*].name}'\n     ```\n   - For each CRD, search for CR instances across all namespaces:\n     ```bash\n     oc get <crd-kind> --all-namespaces --ignore-not-found\n     ```\n   - If CRs exist, list them with details:\n     ```\n     WARNING: Found custom resources that may prevent clean uninstallation:\n       - namespace-1/<cr-name-1> (kind: <CRD-kind>)\n       - namespace-2/<cr-name-2> (kind: <CRD-kind>)\n     \n     These resources should be deleted before uninstalling the operator.\n     Do you want to delete these custom resources? (yes/no)\n     ```\n   - If user confirms, delete each CR:\n     ```bash\n     oc delete <crd-kind> <cr-name> -n <namespace>\n     ```\n   - This prevents namespace from getting stuck in Terminating state\n   - Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-reinstalling-operators-after-failed-uninstallation_olm-troubleshooting-operator-issues\n\n9. **Remove Custom Resource Definitions** (if `--remove-crds` flag is set):\n   - **WARNING**: Display critical warning to user:\n     ```\n     WARNING: Removing CRDs will delete ALL custom resources of these types across the entire cluster!\n     This action is irreversible and will affect all namespaces.\n     \n     Are you absolutely sure you want to remove CRDs? (yes/no)\n     ```\n   - If user confirms, proceed with CRD removal\n   - Get list of CRDs owned by the operator:\n     ```bash\n     oc get csv {csv-name} -n {namespace} -o jsonpath='{.spec.customresourcedefinitions.owned[*].name}'\n     ```\n   - For each CRD, check if custom resources exist:\n     ```bash\n     oc get {crd-name} --all-namespaces --ignore-not-found\n     ```\n   - Display warning if custom resources exist\n   - Delete CRDs:\n     ```bash\n     oc delete crd {crd-name}\n     ```\n\n10. **Remove Namespace** (if `--remove-namespace` flag is set):\n    - **WARNING**: Display warning:\n      ```\n      WARNING: Removing namespace {namespace} will delete all resources in this namespace!\n      \n      Are you sure you want to remove namespace {namespace}? (yes/no)\n      ```\n    - If user confirms:\n      ```bash\n      oc delete namespace {namespace}\n      ```\n    - Monitor namespace deletion with timeout:\n      ```bash\n      oc wait --for=delete namespace/{namespace} --timeout=120s\n      ```\n    - If namespace gets stuck in \"Terminating\" state after 120 seconds:\n      - Check for resources preventing deletion:\n        ```bash\n        oc api-resources --verbs=list --namespaced -o name | \\\n          xargs -n 1 oc get --show-kind --ignore-not-found -n {namespace}\n        ```\n      - Check for finalizers on the namespace:\n        ```bash\n        oc get namespace {namespace} -o jsonpath='{.metadata.finalizers}'\n        ```\n      - Display helpful error message:\n        ```\n        ERROR: Namespace {namespace} is stuck in Terminating state.\n        \n        Possible causes:\n        - Resources with finalizers preventing deletion\n        - API services that are unavailable\n        - Custom resources that cannot be deleted\n        \n        To diagnose and fix, run: /olm:diagnose {operator-name} {namespace}\n        \n        Manual troubleshooting:\n        1. Check remaining resources:\n           oc api-resources --verbs=list --namespaced -o name | \\\n             xargs -n 1 oc get --show-kind --ignore-not-found -n {namespace}\n        \n        2. Check namespace finalizers:\n           oc get namespace {namespace} -o yaml | grep -A5 finalizers\n        \n        WARNING: Do NOT force-delete the namespace as it can lead to unstable cluster behavior.\n        See: https://access.redhat.com/solutions/4165791\n        ```\n      - Exit with error code\n    - Note: OperatorGroup will be automatically deleted with the namespace\n\n11. **Post-Uninstall Verification**:\n    - Verify all resources are cleaned up:\n      ```bash\n      oc get subscription,csv,installplan -n {namespace} --ignore-not-found\n      ```\n    - Check if any CRDs remain (if they were supposed to be deleted):\n      ```bash\n      oc get crd | grep <operator-related-pattern>\n      ```\n    - If uninstalling without `--remove-namespace`, check namespace is clean:\n      ```bash\n      oc get all -n {namespace}\n      ```\n    - Display any remaining resources with suggestions for cleanup\n\n12. **Display Uninstallation Summary**:\n    - Show what was successfully removed:\n      ```\n       Uninstallation Summary:\n         Subscription '{operator-name}' deleted\n         CSV '{csv-name}' deleted\n         Operator deployments removed\n        [ X custom resources deleted]\n        [ Y CRDs removed]\n        [ Namespace '{namespace}' deleted]\n      ```\n    - If CRDs or namespace were NOT removed, provide instructions:\n      ```\n      Note: The following resources were NOT removed:\n      - Custom Resource Definitions (use --remove-crds to remove)\n      - Namespace {namespace} (use --remove-namespace to remove)\n      \n      To completely remove all operator resources, run:\n      /olm:uninstall {operator-name} {namespace} --remove-crds --remove-namespace\n      ```\n    - **Important warning about reinstallation**:\n      ```\n      IMPORTANT: Before reinstalling this operator, verify all resources are cleaned:\n      \n      oc get subscription,csv,installplan -n {namespace}\n      oc get crd | grep <operator-pattern>\n      \n      Failure to completely uninstall may cause reinstallation issues.\n      See: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-reinstalling-operators-after-failed-uninstallation_olm-troubleshooting-operator-issues\n      ```\n\n## Return Value\n- **Success**: Operator uninstalled successfully with summary of removed resources\n- **Partial Success**: Some resources removed with warnings about remaining resources\n- **Error**: Uninstallation failed with specific error message\n- **Format**: Structured output showing:\n  - Subscription deletion status\n  - CSV deletion status\n  - Deployment removal status\n  - CRD removal status (if applicable)\n  - Namespace deletion status (if applicable)\n\n## Examples\n\n1. **Uninstall cert-manager-operator (basic)**:\n   ```\n   /olm:uninstall openshift-cert-manager-operator\n   ```\n\n2. **Uninstall with custom namespace**:\n   ```\n   /olm:uninstall openshift-cert-manager-operator my-cert-manager\n   ```\n\n3. **Complete cleanup including namespace**:\n   ```\n   /olm:uninstall openshift-cert-manager-operator cert-manager-operator --remove-crds --remove-namespace\n   ```\n   This performs a complete cleanup of all operator-related resources.\n\n4. **Force uninstall without prompts**:\n   ```\n   /olm:uninstall openshift-cert-manager-operator cert-manager-operator --force\n   ```\n   Skips all confirmation prompts (use with caution!).\n\n## Arguments\n- **$1** (operator-name): The name of the operator to uninstall (required)\n  - Example: \"openshift-cert-manager-operator\"\n  - Must match the Subscription name\n- **$2** (namespace): The namespace where operator is installed (optional)\n  - Default: `{operator-name}` (operator name without \"openshift-\" prefix)\n  - Example: \"cert-manager-operator\"\n- **$3+** (flags): Optional flags (can combine multiple):\n  - `--remove-crds`: Remove Custom Resource Definitions (WARNING: affects entire cluster)\n  - `--remove-namespace`: Remove the operator's namespace and all its resources\n  - `--force`: Skip all confirmation prompts (use with caution)\n\n## Safety Features\n\n1. **Multiple Confirmations**: Separate confirmations for CRD and namespace removal\n2. **Detailed Warnings**: Clear warnings about the scope of deletions\n3. **Verification Steps**: Checks that resources exist before attempting deletion\n4. **Summary Report**: Detailed summary of what was and wasn't removed\n5. **Graceful Failures**: Continues with remaining steps if individual deletions fail\n\n## Troubleshooting\n\n- **Subscription not found**: Verify the operator name and namespace:\n  ```bash\n  oc get subscriptions --all-namespaces | grep {operator-name}\n  ```\n- **CSV won't delete**: Check for finalizers:\n  ```bash\n  oc get csv {csv-name} -n {namespace} -o yaml | grep finalizers\n  ```\n  If finalizers are present, they may be waiting for resources to be cleaned up. Check operator logs and events.\n\n- **Namespace stuck in Terminating**: This is a common issue after operator uninstallation.\n  ```bash\n  # Find remaining resources\n  oc api-resources --verbs=list --namespaced -o name | \\\n    xargs -n 1 oc get --show-kind --ignore-not-found -n {namespace}\n  \n  # Check namespace finalizers\n  oc get namespace {namespace} -o yaml | grep -A5 finalizers\n  ```\n  **IMPORTANT**: Do not force-delete the namespace. This can cause cluster instability.\n  Instead, use `/olm:diagnose {operator-name} {namespace}` to diagnose and fix the issue.\n\n- **CRDs won't delete**: Check for remaining custom resources:\n  ```bash\n  oc get {crd-name} --all-namespaces\n  ```\n  CRDs cannot be deleted while CR instances exist. Delete all CRs first.\n\n- **Custom resources won't delete**: Some CRs may have finalizers preventing deletion:\n  ```bash\n  oc get <crd-kind> <cr-name> -n <namespace> -o yaml | grep finalizers\n  ```\n  The operator controller (if still running) should remove finalizers. If operator is already deleted, you may need to manually patch the CR to remove finalizers (use with extreme caution).\n\n- **Permission denied**: Ensure you have cluster-admin privileges for CRD deletion:\n  ```bash\n  oc auth can-i delete crd\n  ```\n\n- **Reinstallation fails after uninstall**: This usually means cleanup was incomplete.\n  Run these checks before reinstalling:\n  ```bash\n  # Check for remaining subscriptions/CSVs\n  oc get subscription,csv -n {namespace}\n  \n  # Check for remaining CRDs\n  oc get crd | grep <operator-pattern>\n  \n  # Check if namespace is clean or stuck\n  oc get namespace {namespace}\n  ```\n  See: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-reinstalling-operators-after-failed-uninstallation_olm-troubleshooting-operator-issues\n\n## Related Commands\n\n- `/olm:install` - Install a day-2 operator\n- `/olm:list` - List installed operators\n- `/olm:status` - Check operator status before uninstalling\n- `/olm:diagnose` - Diagnose and fix uninstallation issues\n- `/olm:upgrade` - Upgrade an operator\n\n## Additional Resources\n\n- [Red Hat OpenShift: Deleting Operators from a cluster](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-deleting-operators-from-a-cluster)\n- [Red Hat OpenShift: Reinstalling Operators after failed uninstallation](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-reinstalling-operators-after-failed-uninstallation_olm-troubleshooting-operator-issues)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              },
              {
                "name": "/upgrade",
                "description": "Update an operator to the latest version or switch channels",
                "path": "plugins/olm/commands/upgrade.md",
                "frontmatter": {
                  "description": "Update an operator to the latest version or switch channels",
                  "argument-hint": "<operator-name> [namespace] [--channel=<channel>] [--approve]"
                },
                "content": "## Name\nolm:upgrade\n\n## Synopsis\n```\n/olm:upgrade <operator-name> [namespace] [--channel=<channel>] [--approve]\n```\n\n## Description\nThe `olm:upgrade` command updates an installed operator to the latest version in its current channel or switches to a different channel. It can also approve pending InstallPlans for operators with manual approval mode.\n\nThis command helps you:\n- Update operators to the latest version in their channel\n- Switch operators to different channels (e.g., stable to tech-preview)\n- Approve pending upgrade InstallPlans for manual approval mode\n- Monitor upgrade progress\n- Rollback on failure (if possible via OLM)\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Parse Arguments**:\n   - `$1`: Operator name (required) - Name of the operator to upgrade\n   - `$2`: Namespace (optional) - Namespace where operator is installed\n     - If not provided, searches for the operator across all namespaces\n   - `$3+`: Flags (optional):\n     - `--channel=<channel-name>`: Switch to a different channel\n     - `--approve`: Automatically approve pending InstallPlan (for manual approval mode)\n\n2. **Prerequisites Check**:\n   - Verify `oc` CLI is installed: `which oc`\n   - Verify cluster access: `oc whoami`\n   - Check if user has sufficient privileges\n\n3. **Locate Operator**:\n   - If namespace provided, verify operator exists:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} --ignore-not-found\n     ```\n   - If no namespace provided, search across all namespaces:\n     ```bash\n     oc get subscription --all-namespaces -o json | jq -r '.items[] | select(.spec.name==\"{operator-name}\") | .metadata.namespace'\n     ```\n   - If not found, display error with suggestions\n   - If multiple instances found, prompt user to specify namespace\n\n4. **Get Current State**:\n   - Get current Subscription:\n     ```bash\n     oc get subscription {operator-name} -n {namespace} -o json\n     ```\n   - Extract:\n     - Current channel: `.spec.channel`\n     - Install plan approval: `.spec.installPlanApproval`\n     - Installed CSV: `.status.installedCSV`\n     - Current CSV: `.status.currentCSV`\n   - Get current CSV version:\n     ```bash\n     oc get csv {installed-csv} -n {namespace} -o jsonpath='{.spec.version}'\n     ```\n\n5. **Check for Available Updates**:\n   - Get PackageManifest:\n     ```bash\n     oc get packagemanifest {operator-name} -n openshift-marketplace -o json\n     ```\n   - Extract available channels and their latest versions\n   - If `--channel` flag is specified, verify channel exists\n   - If no channel flag, check for updates in current channel\n   - Compare current version with latest available version\n   - Reference: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-updating-operators\n\n6. **Display Upgrade Plan**:\n   ```\n   Operator Upgrade Plan:\n   \n   Operator: {display-name}\n   Namespace: {namespace}\n   Current Version: {current-version}\n   Current Channel: {current-channel}\n   \n   [If switching channels:]\n   Target Channel: {new-channel}\n   Target Version: {new-version}\n   \n   [If upgrading in same channel:]\n   Latest Version: {latest-version} (in channel: {current-channel})\n   \n   Approval Mode: {Automatic|Manual}\n   ```\n\n7. **Check for Pending InstallPlans** (for manual approval mode):\n   - Get pending InstallPlans:\n     ```bash\n     oc get installplan -n {namespace} -o json | jq '.items[] | select(.spec.approved==false)'\n     ```\n   - If pending InstallPlan exists and `--approve` flag is set:\n     - Display InstallPlan details\n     - Approve the InstallPlan (skip to step 9)\n   - If pending InstallPlan exists and no `--approve` flag:\n     ```\n       Pending InstallPlan found (requires manual approval)\n     \n     InstallPlan: {installplan-name}\n     Target Version: {target-version}\n     \n     To approve: /olm:upgrade {operator-name} {namespace} --approve\n     Or use: /olm:approve {operator-name} {namespace}\n     ```\n     - Exit, waiting for user to approve\n\n8. **Perform Channel Switch** (if `--channel` flag provided):\n   - Confirm with user (unless `--force` flag):\n     ```\n     WARNING: Switching channels may upgrade or downgrade the operator.\n     \n     Current: {current-channel} ({current-version})\n     Target:  {new-channel} ({target-version})\n     \n     Continue? (yes/no)\n     ```\n   - Update Subscription to new channel:\n     ```bash\n     oc patch subscription {operator-name} -n {namespace} \\\n       --type merge --patch '{\"spec\":{\"channel\":\"{new-channel}\"}}'\n     ```\n   - Display confirmation:\n     ```\n      Subscription updated to channel: {new-channel}\n     ```\n\n9. **Approve Pending InstallPlan** (if `--approve` flag or automatic approval):\n   - If approval mode is Manual and `--approve` flag is set:\n     ```bash\n     oc patch installplan {installplan-name} -n {namespace} \\\n       --type merge --patch '{\"spec\":{\"approved\":true}}'\n     ```\n   - Display approval confirmation:\n     ```\n      InstallPlan approved: {installplan-name}\n     ```\n\n10. **Monitor Upgrade Progress**:\n    - Wait for new InstallPlan to be created (if switching channels):\n      ```bash\n      oc get installplan -n {namespace} -w --timeout=60s\n      ```\n    - Wait for new CSV to reach \"Succeeded\" phase:\n      ```bash\n      oc get csv -n {namespace} -w --timeout=300s\n      ```\n    - Display progress updates:\n      ```\n       Upgrade in progress...\n       Waiting for InstallPlan to complete...\n       New CSV installing: {new-csv-name}\n       Old CSV replacing: {old-csv-name}\n      ```\n    - Poll every 10 seconds to check status\n    - Timeout: 10 minutes for upgrade to complete\n\n11. **Verify Upgrade Success**:\n    - Check new CSV status:\n      ```bash\n      oc get csv -n {namespace} -o json\n      ```\n    - Verify new CSV phase is \"Succeeded\"\n    - Get new version:\n      ```bash\n      oc get csv {new-csv-name} -n {namespace} -o jsonpath='{.spec.version}'\n      ```\n    - Check deployments are healthy:\n      ```bash\n      oc get deployments -n {namespace}\n      ```\n    - Check pods are running:\n      ```bash\n      oc get pods -n {namespace}\n      ```\n\n12. **Display Upgrade Summary**:\n    ```\n     Operator Upgrade Complete!\n    \n    Operator: {display-name}\n    Namespace: {namespace}\n    Previous Version: {old-version}\n    Current Version: {new-version}\n    Channel: {channel}\n    \n    Deployment Status:\n      - {deployment-1}: 1/1 replicas ready\n      - {deployment-2}: 1/1 replicas ready\n    \n    To check status: /olm:status {operator-name} {namespace}\n    ```\n\n13. **Handle Upgrade Failures**:\n    - If upgrade fails or times out:\n      ```\n       Operator upgrade failed\n      \n      Current State:\n      - CSV: {csv-name} (Phase: {phase})\n      - Message: {error-message}\n      \n      Troubleshooting steps:\n      1. Check CSV status: oc describe csv {csv-name} -n {namespace}\n      2. Check events: oc get events -n {namespace} --sort-by='.lastTimestamp'\n      3. Check InstallPlan: oc get installplan -n {namespace}\n      4. Run diagnostics: /olm:diagnose {operator-name} {namespace}\n      \n      To rollback (if OLM supports):\n      oc patch subscription {operator-name} -n {namespace} \\\n        --type merge --patch '{\"spec\":{\"channel\":\"{old-channel}\"}}'\n      ```\n\n## Return Value\n- **Success**: Operator upgraded successfully with new version details\n- **Pending Approval**: Upgrade waiting for manual approval with instructions\n- **No Update Available**: Operator is already at the latest version\n- **Error**: Upgrade failed with specific error message and troubleshooting guidance\n- **Format**: Structured output showing:\n  - Previous and current versions\n  - Channel information\n  - Deployment and pod status\n  - Next steps or related commands\n\n## Examples\n\n1. **Check for and install updates in current channel**:\n   ```\n   /olm:upgrade openshift-cert-manager-operator\n   ```\n\n2. **Upgrade with specific namespace**:\n   ```\n   /olm:upgrade external-secrets-operator eso-operator\n   ```\n\n3. **Switch to a different channel**:\n   ```\n   /olm:upgrade openshift-cert-manager-operator cert-manager-operator --channel=tech-preview-v1.14\n   ```\n   This switches from stable-v1 to tech-preview-v1.14 channel.\n\n4. **Approve pending upgrade (manual approval mode)**:\n   ```\n   /olm:upgrade openshift-cert-manager-operator --approve\n   ```\n\n5. **Switch channel and approve in one command**:\n   ```\n   /olm:upgrade prometheus prometheus-operator --channel=beta --approve\n   ```\n\n## Arguments\n- **$1** (operator-name): Name of the operator to upgrade (required)\n  - Example: \"openshift-cert-manager-operator\"\n  - Must match the operator's Subscription name\n- **$2** (namespace): Namespace where operator is installed (optional)\n  - If not provided, searches all namespaces\n  - Example: \"cert-manager-operator\"\n- **$3+** (flags): Optional flags\n  - `--channel=<channel-name>`: Switch to specified channel\n    - Example: `--channel=stable-v1`, `--channel=tech-preview`\n    - Triggers upgrade/downgrade to the version in that channel\n  - `--approve`: Automatically approve pending InstallPlan\n    - Only needed for operators with Manual approval mode\n    - Equivalent to `/olm:approve` command\n\n## Notes\n\n- **Automatic Updates**: Operators with `installPlanApproval: Automatic` will upgrade automatically when new versions are available in their channel\n- **Manual Approval**: Operators with `installPlanApproval: Manual` require explicit approval via `--approve` flag or `/olm:approve` command\n- **Channel Switching**: Changing channels may result in upgrade or downgrade depending on the versions in each channel\n- **Rollback**: OLM has limited rollback support. Switching back to the previous channel may work, but data migration issues may occur\n- **Upgrade Timing**: Upgrades happen according to the operator's upgrade strategy (some may cause downtime)\n\n## Troubleshooting\n\n- **No updates available**:\n  ```bash\n  # Check current version\n  oc get csv -n {namespace}\n  \n  # Check available versions\n  oc get packagemanifest {operator-name} -n openshift-marketplace -o json\n  ```\n\n- **Upgrade stuck or pending**:\n  ```bash\n  # Check InstallPlan status\n  oc get installplan -n {namespace}\n  \n  # Check for events\n  oc get events -n {namespace} --sort-by='.lastTimestamp' | tail -20\n  ```\n\n- **Manual approval required**:\n  ```bash\n  # List pending InstallPlans\n  oc get installplan -n {namespace} -o json | jq '.items[] | select(.spec.approved==false)'\n  \n  # Approve specific InstallPlan\n  /olm:approve {operator-name} {namespace}\n  ```\n\n- **Upgrade failed**:\n  ```bash\n  # Check CSV status\n  oc describe csv -n {namespace}\n  \n  # Check operator logs\n  oc logs -n {namespace} deployment/{operator-deployment}\n  \n  # Run diagnostics\n  /olm:diagnose {operator-name} {namespace}\n  ```\n\n- **Rollback needed**:\n  - OLM doesn't have built-in rollback\n  - Can try switching back to previous channel, but may have issues:\n    ```bash\n    oc patch subscription {operator-name} -n {namespace} \\\n      --type merge --patch '{\"spec\":{\"channel\":\"{old-channel}\"}}'\n    ```\n  - Consider backup/restore of custom resources before upgrading\n\n## Related Commands\n\n- `/olm:status <operator-name>` - Check current version and available updates\n- `/olm:approve <operator-name>` - Approve pending InstallPlans\n- `/olm:install <operator-name>` - Install an operator\n- `/olm:diagnose <operator-name>` - Diagnose upgrade issues\n\n## Additional Resources\n\n- [Red Hat OpenShift: Updating Installed Operators](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-updating-operators)\n- [Red Hat OpenShift: Approving Operator Upgrades](https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/operators/administrator-tasks#olm-approving-operator-upgrades_olm-updating-operators)\n- [Operator Lifecycle Manager Documentation](https://olm.operatorframework.io/)"
              }
            ],
            "skills": []
          },
          {
            "name": "prow-job",
            "description": "A plugin to analyze and inspect Prow CI job results",
            "source": "./plugins/prow-job",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install prow-job@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-install-failure",
                "description": "Analyze OpenShift installation failures in Prow CI jobs",
                "path": "plugins/prow-job/commands/analyze-install-failure.md",
                "frontmatter": {
                  "description": "Analyze OpenShift installation failures in Prow CI jobs",
                  "argument-hint": "<prowjob-url>"
                },
                "content": "## Name\nprow-job:analyze-install-failure\n\n## Synopsis\n```\n/prow-job:analyze-install-failure <prowjob-url>\n```\n\n## Description\n\nThe `prow-job:analyze-install-failure` command analyzes OpenShift installation failures in Prow CI jobs by downloading and examining installer logs, log bundles, and sosreports (for metal jobs). This command is specifically designed to debug failures in the **\"install should succeed: overall\"** test, which indicates that the installation process failed at some stage.\n\n**Important**: All \"install should succeed\" tests have a specific suffix indicating the failure stage (configuration, infrastructure, cluster bootstrap, cluster creation, cluster operator stability, or other). The JUnit XML contains both the specific failure reason test (which fails) and the overall test (which also fails when any stage fails). This command analyzes the specific failure stage to provide targeted diagnostics.\n\nThe command accepts:\n- Prow job URL (required): URL to the failed CI job from prow.ci.openshift.org\n\nIt downloads relevant artifacts from Google Cloud Storage, analyzes them, and generates a comprehensive report with findings and recommended next steps.\n\n### Recognized Failure Modes\n\nThe command identifies the failure mode from `junit_install.xml` and tailors its analysis:\n\n- **\"install should succeed: configuration\"** - Extremely rare failure where install-config.yaml validation failed. Focus on installer log only.\n- **\"install should succeed: infrastructure\"** - Failed to create cloud resources. Usually due to cloud quota, rate limiting, or outages. Log bundle may not exist.\n- **\"install should succeed: cluster bootstrap\"** - Bootstrap node failed to start temporary control plane. Check bootkube logs in the bundle.\n- **\"install should succeed: cluster creation\"** - One or more operators unable to deploy. Check operator logs in gather-must-gather.\n- **\"install should succeed: cluster operator stability\"** - Operators never stabilized (stuck progressing/degraded). Check operator status and logs.\n- **\"install should succeed: other\"** - Unknown failure requiring comprehensive analysis of all available logs.\n\n## Implementation\n\nThe command performs the following steps by invoking the \"Prow Job Analyze Install Failure\" skill:\n\n1. **Parse Job URL**: Extract build ID and job details from the Prow URL\n\n2. **Download prowjob.json**: Identify the ci-operator target\n\n3. **Download JUnit XML**: Identify the specific failure mode (configuration, infrastructure, cluster bootstrap, etc.)\n\n4. **Download Installer Logs**: Get `.openshift_install*.log` files that contain the installation timeline\n\n5. **Download Log Bundle**: Get `log-bundle-*.tar` containing:\n   - Bootstrap node journals (bootkube, kubelet, crio, etc.)\n   - Serial console logs from all nodes\n   - Cluster API resources (etcd, kube-apiserver logs)\n   - Failed systemd units list\n\n6. **Invoke Metal Skill** (metal jobs only): Use the specialized `prow-job-analyze-metal-install-failure` skill to analyze:\n   - Dev-scripts setup logs (installation framework)\n   - libvirt console logs (VM/node boot sequence)\n   - sosreport (hypervisor diagnostics)\n   - squid logs (proxy logs for disconnected environments)\n\n7. **Analyze Logs**: Extract key failure indicators based on failure mode:\n   - **configuration**: install-config.yaml validation errors\n   - **infrastructure**: Cloud quota/rate limit/API errors\n   - **cluster bootstrap**: bootkube/etcd/API server failures\n   - **cluster creation**: Operator deployment failures\n   - **cluster operator stability**: Operators stuck in unstable state\n   - **other**: Comprehensive analysis of all logs\n\n8. **Generate Report**: Create comprehensive analysis with:\n   - Failure mode and summary\n   - Timeline of events\n   - Key error messages with context\n   - Failure mode-specific recommended steps\n   - Artifact locations\n\nThe skill handles all the implementation details including URL parsing, artifact downloading, archive extraction, log analysis, and report generation.\n\n## Return Value\n- **Success**: Comprehensive analysis report saved to `.work/prow-job-analyze-install-failure/{build_id}/analysis/report.txt`\n- **Error**: Error message explaining the issue (invalid URL, gcloud not installed, artifacts not found, etc.)\n\n**Important for Claude**:\n1. Parse the Prow job URL to extract the build ID and job name\n2. Invoke the \"prow-job:analyze-install-failure\" skill with the job details\n3. The skill will download all relevant artifacts and analyze them\n4. For metal jobs, the skill automatically invokes the specialized metal install failure skill\n5. Present the analysis report to the user with clear findings\n6. Provide actionable next steps based on the failure mode\n7. Highlight critical errors and their context\n\n## Examples\n\n1. **Analyze an AWS installation failure**:\n   ```\n   /prow-job:analyze-install-failure https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-release-master-ci-4.21-e2e-aws-ovn-techpreview/1983307151598161920\n   ```\n   Expected output:\n   - Downloads installer logs and log bundle\n   - Identifies failure mode from junit_install.xml\n   - Analyzes bootstrap and installation logs\n   - Reports: \"Bootstrap failed due to etcd cluster formation timeout\"\n   - Provides etcd logs and recommendations\n\n2. **Analyze a metal installation failure**:\n   ```\n   /prow-job:analyze-install-failure https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-release-master-nightly-4.21-e2e-metal-ipi-ovn-ipv6/1983304069657137152\n   ```\n   Expected output:\n   - Invokes specialized metal install failure skill\n   - Downloads dev-scripts logs, libvirt console logs, sosreport\n   - Analyzes dev-scripts setup and VM console logs\n   - Reports: \"Bootstrap VM failed to boot - Ignition config fetch failed\"\n   - Provides console logs and dev-scripts analysis\n\n3. **Analyze an infrastructure failure**:\n   ```\n   /prow-job:analyze-install-failure https://prow.ci.openshift.org/view/gs/test-platform-results/pr-logs/pull/openshift/installer/12345/pull-ci-openshift-installer-master-e2e-aws/7890\n   ```\n   Expected output:\n   - Identifies \"install should succeed: infrastructure\" failure\n   - Focuses on installer log (no log bundle expected)\n   - Reports: \"Cloud quota exceeded for instance type m5.xlarge\"\n   - Recommends checking quota limits\n\n4. **Analyze an operator stability failure**:\n   ```\n   /prow-job:analyze-install-failure https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-release-master-ci-4.21-e2e-gcp/1234567890123456789\n   ```\n   Expected output:\n   - Identifies \"install should succeed: cluster operator stability\" failure\n   - Checks gather-must-gather for operator logs\n   - Reports: \"kube-apiserver operator stuck progressing\"\n   - Provides operator logs and status conditions\n\n## Notes\n\n- **Failure Modes**: The installer has multiple failure modes detected from junit_install.xml. Each mode requires different analysis approaches.\n- **Log Bundle**: Contains detailed node-level diagnostics including journals, serial consoles, and cluster API resources\n- **Metal Jobs**: Identified by \"metal\" in the job name. These jobs automatically invoke the specialized `prow-job-analyze-metal-install-failure` skill.\n- **Metal Artifacts**: Metal jobs analyze dev-scripts logs, libvirt console logs, sosreport, and squid logs\n- **Artifacts Location**: All downloaded artifacts are cached in `.work/prow-job-analyze-install-failure/{build_id}/` for faster re-analysis\n- **gcloud Requirement**: Requires gcloud CLI to be installed to access GCS buckets\n- **Public Access**: The test-platform-results bucket is publicly accessible - no authentication needed\n\n## Arguments\n- **$1** (prowjob-url): The Prow job URL from prow.ci.openshift.org (required)"
              },
              {
                "name": "/analyze-resource",
                "description": "Analyze Kubernetes resource lifecycle in Prow job artifacts",
                "path": "plugins/prow-job/commands/analyze-resource.md",
                "frontmatter": {
                  "description": "Analyze Kubernetes resource lifecycle in Prow job artifacts",
                  "argument-hint": "prowjob-url resource-name"
                },
                "content": "## Name\n/prow-job:analyze-resource\n\n## Synopsis\nGenerate a report showing the lifecycle of a single resource:\n```\n/prow-job:analyze-resource <prowjob-url> [namespace:][kind/][resource-name]\n```\nGenerate a report showing multiple resource lifecycles:\n```\n/prow-job:analyze-resource <prowjob-url> [namespace:][kind/][resource-name],[namespace:][kind/][resource-name],...\n```\n\n## Description\nAnalyze the lifecycle of Kubernetes resource(s) in a Prow CI job by invoking the \"Prow Job Analyze Resource\" skill.\n\n## Implementation\nPass the user's request to the skill, which will:\n- Download Prow job artifacts from Google Cloud Storage\n- Parse audit logs and pod logs\n- Generate an interactive HTML report with timeline visualization\n\nThe skill handles all the implementation details including URL parsing, artifact downloading, log parsing, and HTML report generation."
              },
              {
                "name": "/analyze-test-failure",
                "description": "Analyzes test errors from console logs and Prow CI job artifacts",
                "path": "plugins/prow-job/commands/analyze-test-failure.md",
                "frontmatter": {
                  "description": "Analyzes test errors from console logs and Prow CI job artifacts",
                  "argument-hint": "prowjob-url test-name"
                },
                "content": "## Name\nprow-job:analyze-test-failure\n\n## Synopsis\nGenerate a test failure analysis for the given test:\n```\n/prow-job:analyze-test-failure <prowjob-url> <test-name>\n```\n\n## Description\nAnalyze a failed test by inspecting the test code in the current project and artifacts in Prow CI job. This is done by invoking the \"Prow Job Analyze Test Failure\" skill.\n\n## Implementation\nPass the user's request to the skill, which will:\n- Download the artifacts from Google Cloud Storage\n- Check source code of the test\n- Extract artifacts from Prow CI job and analyze the given test failure\n\nThe skill handles all the implementation details including URL parsing, artifact downloading, archive extraction, analyzing the error and providing evidence.\n\n## Arguments:\n- $1: Prow job URL (required)\n- $2: Test name (required)"
              },
              {
                "name": "/extract-must-gather",
                "description": "Extract and decompress must-gather archives from Prow job artifacts",
                "path": "plugins/prow-job/commands/extract-must-gather.md",
                "frontmatter": {
                  "description": "Extract and decompress must-gather archives from Prow job artifacts",
                  "argument-hint": "prowjob-url"
                },
                "content": "## Name\nprow-job:extract-must-gather\n\n## Synopsis\n```\n/prow-job:extract-must-gather <prowjob-url>\n```\n\n## Description\nExtract the must-gather archive from a Prow CI job by invoking the \"Prow Job Extract Must-Gather\" skill.\n\n## Implementation\nExtract the must-gather archive from a Prow CI job by invoking the \"Prow Job Extract Must-Gather\" skill.\n\nPass the user's request to the skill, which will:\n- Download the must-gather.tar from Google Cloud Storage\n- Extract and recursively decompress all nested archives\n- Generate an interactive HTML file browser with filters\n\nThe skill handles all the implementation details including URL parsing, artifact downloading, archive extraction, and HTML report generation."
              }
            ],
            "skills": [
              {
                "name": "Prow Job Analyze Install Failure",
                "description": "Analyze OpenShift installation failures in Prow CI jobs by examining installer logs, log bundles, and sosreports. Use when CI job fails \"install should succeed\" tests at bootstrap, cluster creation or other stages.",
                "path": "plugins/prow-job/skills/prow-job-analyze-install-failure/SKILL.md",
                "frontmatter": {
                  "name": "Prow Job Analyze Install Failure",
                  "description": "Analyze OpenShift installation failures in Prow CI jobs by examining installer logs, log bundles, and sosreports. Use when CI job fails \"install should succeed\" tests at bootstrap, cluster creation or other stages."
                },
                "content": "# Prow Job Analyze Install Failure\n\nThis skill helps debug OpenShift installation failures in CI jobs by downloading and analyzing installer logs, log bundles, and sosreports from Google Cloud Storage.\n\n## When to Use This Skill\n\nUse this skill when:\n- A CI job fails with \"install should succeed\" test failure\n- You need to debug installation failures at specific stages (bootstrap, install-complete, etc.)\n- You need to analyze installer logs and log bundles from failed CI jobs\n\n## Prerequisites\n\nBefore starting, verify these prerequisites:\n\n1. **gcloud CLI Installation**\n   - Check if installed: `which gcloud`\n   - If not installed, provide instructions for the user's platform\n   - Installation guide: https://cloud.google.com/sdk/docs/install\n\n2. **gcloud Authentication (Optional)**\n   - The `test-platform-results` bucket is publicly accessible\n   - No authentication is required for read access\n   - Skip authentication checks\n\n## Input Format\n\nThe user will provide:\n1. **Prow job URL** - URL to the failed CI job\n   - Example: `https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-release-master-ci-4.21-e2e-aws-ovn-techpreview/1983307151598161920`\n   - The URL should contain `test-platform-results/`\n\n## Understanding Job Types from Names\n\nJob names contain important clues about the test environment and what to look for:\n\n1. **Upgrade Jobs** (names containing \"upgrade\")\n   - These jobs perform a **fresh installation first**, then upgrade\n   - **Minor upgrade jobs**: Contain \"upgrade-from-stable-4.X\" in the name - upgrade from previous minor version (e.g., 4.21 job installs 4.20, then upgrades to 4.21)\n     - Example: `periodic-ci-openshift-release-master-ci-4.21-upgrade-from-stable-4.20-e2e-gcp-ovn-rt-upgrade`\n   - **Micro upgrade jobs**: Have \"upgrade\" in the name but NO \"upgrade-from-stable\" - upgrade within the same minor version (e.g., earlier 4.21 to newer 4.21)\n     - Example: `periodic-ci-openshift-release-master-nightly-4.21-e2e-aws-ovn-upgrade-fips`\n     - Example: `periodic-ci-openshift-release-master-ci-4.21-e2e-azure-ovn-upgrade`\n   - If installation fails, the upgrade never happens\n   - **Key point**: Installation failures in upgrade jobs are still installation failures, not upgrade failures\n\n2. **FIPS Jobs** (names containing \"fips\")\n   - FIPS mode enabled for cryptographic operations\n   - Pay special attention to errors related to:\n     - Cryptography libraries\n     - TLS/SSL handshakes\n     - Certificate validation\n     - Hash algorithms\n\n3. **IPv6 and Dualstack Jobs** (names containing \"ipv6\" or \"dualstack\")\n   - Using IPv6 or dual IPv4/IPv6 networking stack\n   - Most IPv6 jobs are **disconnected** (no internet access)\n   - Use a locally-hosted mirror registry for images\n   - Pay attention to:\n     - Network connectivity errors\n     - DNS resolution issues\n     - Mirror registry logs\n     - IPv6 address configuration\n\n4. **Metal Jobs with IPv6** (names containing \"metal\" and \"ipv6\")\n   - Disconnected environment with additional complexity\n   - The metal install failure skill will analyze squid proxy logs and disconnected environment configuration\n\n5. **Single-Node Jobs** (names containing \"single-node\")\n   - All control plane and compute workloads on one node\n   - More prone to resource exhaustion\n   - Pay attention to CPU, memory, and disk pressure\n\n6. **Platform-Specific Indicators**\n   - `aws`, `gcp`, `azure`: Cloud platform used\n   - `metal`, `baremetal`: Bare metal environment (uses specialized metal install failure skill)\n   - `ovn`: OVN-Kubernetes networking (standard)\n\n## Implementation Steps\n\n### Step 1: Parse and Validate URL\n\n1. **Extract bucket path**\n   - Find `test-platform-results/` in URL\n   - Extract everything after it as the GCS bucket relative path\n   - If not found, error: \"URL must contain 'test-platform-results/'\"\n\n2. **Extract build_id**\n   - Search for pattern `/(\\d{10,})/` in the bucket path\n   - build_id must be at least 10 consecutive decimal digits\n   - Handle URLs with or without trailing slash\n   - If not found, error: \"Could not find build ID (10+ digits) in URL\"\n\n3. **Determine job type**\n   - Check if job name contains \"metal\" (case-insensitive)\n   - Metal jobs: Set `is_metal_job = true`\n   - Other jobs: Set `is_metal_job = false`\n\n4. **Construct GCS paths**\n   - Bucket: `test-platform-results`\n   - Base GCS path: `gs://test-platform-results/{bucket-path}/`\n   - Ensure path ends with `/`\n\n### Step 2: Create Working Directory\n\n1. **Create directory structure**\n   ```bash\n   mkdir -p .work/prow-job-analyze-install-failure/{build_id}/logs\n   mkdir -p .work/prow-job-analyze-install-failure/{build_id}/analysis\n   ```\n   - Use `.work/prow-job-analyze-install-failure/` as the base directory (already in .gitignore)\n   - Use build_id as subdirectory name\n   - Create `logs/` subdirectory for all downloads\n   - Create `analysis/` subdirectory for analysis files\n   - Working directory: `.work/prow-job-analyze-install-failure/{build_id}/`\n\n### Step 3: Download prowjob.json and Identify Target\n\n1. **Download prowjob.json**\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/prowjob.json .work/prow-job-analyze-install-failure/{build_id}/logs/prowjob.json --no-user-output-enabled\n   ```\n\n2. **Parse and validate**\n   - Read `.work/prow-job-analyze-install-failure/{build_id}/logs/prowjob.json`\n   - Search for pattern: `--target=([a-zA-Z0-9-]+)`\n   - If not found:\n     - Display: \"This is not a ci-operator job. The prowjob cannot be analyzed by this skill.\"\n     - Explain: ci-operator jobs have a --target argument specifying the test target\n     - Exit skill\n\n3. **Extract target name**\n   - Capture the target value (e.g., `e2e-aws-ovn-techpreview`)\n   - Store for constructing artifact paths\n\n### Step 4: Download JUnit XML to Identify Failure Stage\n\n**Note on install-status.txt**: You may see an `install-status.txt` file in the artifacts. This file contains only the installer's exit code (a single number). The junit_install.xml file translates this exit code into a human-readable failure mode, so always prefer junit_install.xml for determining the failure stage.\n\n1. **Find junit_install.xml**\n   - Use recursive listing to find the file anywhere in the job artifacts:\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep \"junit_install.xml\"\n   ```\n   - The file location varies by job configuration - don't assume any specific path\n\n2. **Download junit_install.xml**\n   - Download from the discovered location:\n   ```bash\n   # Download from wherever it was found\n   gcloud storage cp {full-gcs-path-to-junit_install.xml} .work/prow-job-analyze-install-failure/{build_id}/logs/junit_install.xml --no-user-output-enabled\n   ```\n   - If file not found, continue anyway (older jobs or early failures may not have this file)\n\n3. **Parse junit_install.xml to find failure stage**\n   - Look for failed test cases with pattern `install should succeed: <stage>`\n   - Installation failure modes:\n     - **`cluster bootstrap`** - Early install failure where we failed to bootstrap the cluster. Bootstrap is typically an ephemeral VM that runs a temporary kube apiserver. Check bootkube logs in the bundle.\n     - **`infrastructure`** - Early failure before we're able to create all cloud resources. Often but not always due to cloud quota, rate limiting, or outages. Check installer log for cloud API errors.\n     - **`cluster creation`** - Usually means one or more operators was unable to stabilize. Check operator logs in gather-must-gather artifacts.\n     - **`configuration`** - Extremely rare failure mode where we failed to create the install-config.yaml for one reason or another. Check installer log for validation errors.\n     - **`cluster operator stability`** - Operators never stabilized (available=True, progressing=False, degraded=False). Check specific operator logs to determine why they didn't reach stable state.\n     - **`other`** - Unknown install failure, could be for one of the previously declared reasons, or an unknown one. Requires full log analysis.\n   - Extract the failure stage for targeted log analysis\n   - Use the failure mode to guide which logs to prioritize\n\n### Step 5: Locate and Download Installer Logs\n\n1. **List all artifacts to find installer logs**\n   - Installer logs follow the pattern `.openshift_install*.log`\n   - **IMPORTANT**: Exclude deprovision logs - they are from cluster teardown, not installation\n   - Use recursive listing to find all installer logs:\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep -E \"\\.openshift_install.*\\.log$\" | grep -v \"deprovision\"\n   ```\n   - This will find installer logs regardless of which CI step created them\n\n2. **Download all installer logs found**\n   ```bash\n   # For each installer log found in the listing (excluding deprovision)\n   gcloud storage cp {full-gcs-path-to-installer-log} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   ```\n   - Download all installer logs found that are NOT from deprovision steps\n   - If multiple installer logs exist, download all of them (they may be from different install phases)\n   - Deprovision logs are from cluster cleanup and not relevant for installation failures\n\n### Step 6: Locate and Download Log Bundle\n\n1. **List all artifacts to find log bundle**\n   - Log bundles are `.tar` files (NOT `.tar.gz`) starting with `log-bundle-`\n   - **IMPORTANT**: Prefer non-deprovision log bundles over deprovision ones\n   - Use recursive listing to find all log bundles:\n   ```bash\n   # Find all log bundles, preferring non-deprovision\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep -E \"log-bundle.*\\.tar$\"\n   ```\n   - This will find log bundles regardless of which CI step created them\n\n2. **Download log bundle**\n   ```bash\n   # If non-deprovision log bundles exist, download one of those (prefer most recent by timestamp)\n   # Otherwise, download deprovision log bundle if that's the only one available\n   gcloud storage cp {full-gcs-path-to-log-bundle} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   ```\n   - Prefer log bundles NOT from deprovision steps (they capture the failure state during installation)\n   - Deprovision log bundles may also contain useful info if no other bundle exists\n   - If multiple log bundles exist, prefer the one from a non-deprovision step\n   - If no log bundle found, continue with installer log analysis only (early failures may not produce log bundles)\n\n3. **Extract log bundle**\n   ```bash\n   tar -xf .work/prow-job-analyze-install-failure/{build_id}/logs/log-bundle-{timestamp}.tar -C .work/prow-job-analyze-install-failure/{build_id}/logs/\n   ```\n\n### Step 7: Invoke Metal Install Failure Skill (Metal Jobs Only)\n\n**IMPORTANT: Only perform this step if `is_metal_job = true`**\n\nMetal IPI jobs use **dev-scripts** with **Metal3** and **Ironic** to install OpenShift on bare metal. These require specialized analysis.\n\n1. **Invoke the metal install failure skill**\n   - Use the Skill tool to invoke: `prow-job:prow-job-analyze-metal-install-failure`\n   - Pass the following information:\n     - Build ID: `{build_id}`\n     - Bucket path: `{bucket-path}`\n     - Target name: `{target}`\n     - Working directory already created: `.work/prow-job-analyze-install-failure/{build_id}/`\n\n2. **The metal skill will**:\n   - Download and analyze dev-scripts logs (setup process before OpenShift installation)\n   - Download and analyze libvirt console logs (VM/node boot sequence)\n   - Download and analyze optional artifacts (sosreport, squid logs)\n   - Determine if failure was in dev-scripts setup or cluster installation\n   - Generate metal-specific analysis report\n\n3. **Continue with standard analysis**:\n   - After metal skill completes, continue with Step 8 (Analyze Installer Logs)\n   - The metal skill provides additional context about dev-scripts and console logs\n   - Standard installer log analysis is still relevant for understanding cluster creation failures\n\n### Step 8: Analyze Installer Logs\n\n**CRITICAL: Understanding OpenShift's Eventual Consistency**\n\nOpenShift installations exhibit \"eventual consistency\" behavior, which means:\n- Components may report errors while waiting for dependencies to become ready\n- Example: Ingress operator may error waiting for networking, which errors waiting for other components\n- These intermediate errors are **expected and normal** during installation\n- Early errors in the log often resolve themselves and are NOT the root cause\n\n**Error Analysis Strategy**:\n1. **Start with the NEWEST/FINAL errors** - Work backwards in time\n2. Focus on errors that persisted until installation timeout\n3. Track backwards from final errors to identify the dependency chain\n4. Early errors are only relevant if they directly relate to the final failure state\n5. Don't chase errors that occurred early and then disappeared - they likely resolved\n\n**Example**: If installation fails at 40 minutes with \"kube-apiserver not available\", an error at 5 minutes saying \"ingress operator degraded\" is likely irrelevant because it probably resolved. Focus on what was still broken when the timeout occurred.\n\n1. **Read installer log**\n   - The installer log is a sequential log file with timestamp, log level, and message\n   - Format: `time=\"YYYY-MM-DDTHH:MM:SSZ\" level=<level> msg=\"<message>\"`\n\n2. **Identify key failure indicators (WORK BACKWARDS FROM END)**\n   - **Start at the end of the log** - Look at final error/fatal messages\n   - **Error messages**: Lines with `level=error` or `level=fatal` near the end of the log\n   - **Last status messages**: The final \"Still waiting for\" or \"Cluster operators X, Y, Z are not available\" messages\n   - **Warning messages**: Lines with `level=warning` near the failure time that may indicate problems\n   - **Then work backwards** to find when the failing component first started having issues\n   - Ignore errors from early in the log unless they persist to the end\n\n3. **Extract relevant log sections (prioritize recent errors)**\n   - For bootstrap failures:\n     - Search for: \"bootstrap\", \"bootkube\", \"kube-apiserver\", \"etcd\" in the **last 20% of the log**\n   - For install-complete failures:\n     - Search for: \"Cluster operators\", \"clusteroperator\", \"degraded\", \"available\" in the **final messages**\n   - For timeout failures:\n     - Search for: \"context deadline exceeded\", \"timeout\", \"timed out\"\n     - Look at what component was being waited for when timeout occurred\n\n4. **Create installer log summary**\n   - Extract **final/last** error or fatal message (most important)\n   - Extract the last \"Still waiting for...\" message showing what didn't stabilize\n   - Extract surrounding context (10-20 lines before and after final errors)\n   - Optionally note early errors only if they relate to the final failure\n   - Save to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/installer-summary.txt`\n\n### Step 9: Analyze Log Bundle\n\n**Skip this step if no log bundle was downloaded**\n\n1. **Understand log bundle structure**\n   - `log-bundle-{timestamp}/`\n     - `bootstrap/journals/` - Journal logs from bootstrap node\n       - `bootkube.log` - Bootkube service that starts initial control plane\n       - `kubelet.log` - Kubelet service logs\n       - `crio.log` - Container runtime logs\n       - `journal.log.gz` - Complete system journal (gzipped)\n     - `bootstrap/network/` - Network configuration\n       - `ip-addr.txt` - IP addresses\n       - `ip-route.txt` - Routing table\n       - `hostname.txt` - Hostname\n     - `serial/` - Serial console logs from all nodes\n       - `{cluster-name}-bootstrap-serial.log` - Bootstrap node console\n       - `{cluster-name}-master-N-serial.log` - Master node consoles\n     - `clusterapi/` - Cluster API resources\n       - `*.yaml` - Kubernetes resource definitions\n       - `etcd.log` - etcd logs\n       - `kube-apiserver.log` - API server logs\n     - `failed-units.txt` - List of systemd units that failed\n     - `gather.log` - Log bundle collection process log\n\n2. **Analyze based on failure mode from junit_install.xml**\n\n   **For \"cluster bootstrap\" failures:**\n   - Check `bootstrap/journals/bootkube.log` for bootkube errors\n   - Check `bootstrap/journals/kubelet.log` for kubelet issues\n   - Check `clusterapi/kube-apiserver.log` for API server startup issues\n   - Check `clusterapi/etcd.log` for etcd cluster formation issues\n   - Check `serial/{cluster-name}-bootstrap-serial.log` for bootstrap VM boot issues\n   - Look for temporary control plane startup problems\n   - This is an early failure - focus on bootstrap node and initial control plane\n\n   **For \"infrastructure\" failures:**\n   - Primary focus on installer log, not log bundle (failure happens before bootstrap)\n   - Search installer log for cloud provider API errors\n   - Look for quota exceeded messages (e.g., \"QuotaExceeded\", \"LimitExceeded\")\n   - Look for rate limiting errors (e.g., \"RequestLimitExceeded\", \"Throttling\")\n   - Check for authentication/permission errors\n   - **Infrastructure provisioning methods** (varies by OpenShift version):\n     - **Newer versions**: Use **Cluster API (CAPI)** to provision infrastructure\n       - Look for errors in ClusterAPI-related logs and resources\n       - Check for Machine/MachineSet/MachineDeployment errors\n       - Search for \"clusterapi\" or \"machine-api\" related errors\n     - **Older versions**: Use **Terraform** to provision infrastructure\n       - Look for \"terraform\" in log entries\n       - Check for terraform state errors or apply failures\n       - Search for terraform-related error messages\n   - Log bundle may not exist or be incomplete for this failure mode\n\n   **For \"cluster creation\" failures:**\n   - Check if must-gather was successfully collected:\n     - Look for `must-gather*.tar` files in the gather-must-gather step directory\n     - If NO .tar file exists, must-gather collection failed (cluster was too unstable)\n     - Do NOT suggest downloading must-gather if the .tar file doesn't exist\n   - If must-gather exists, check for operator logs\n   - Look for degraded cluster operators\n   - Check operator-specific logs to see why they couldn't stabilize\n   - Review cluster operator status conditions\n   - This indicates cluster bootstrapped but operators failed to deploy\n\n   **For \"configuration\" failures:**\n   - Focus entirely on installer log\n   - Look for install-config.yaml validation errors\n   - Check for missing required fields or invalid values\n   - This is a very early failure before any infrastructure is created\n   - Log bundle will not exist for this failure mode\n\n   **For \"cluster operator stability\" failures:**\n   - Similar to \"cluster creation\" but operators are stuck in unstable state\n   - Check if must-gather was successfully collected (look for `must-gather*.tar` files)\n   - If must-gather doesn't exist, rely on installer log and log bundle only\n   - Check for operators with available=False, progressing=True, or degraded=True\n   - Review operator logs in gather-must-gather (if it exists)\n   - Check for resource conflicts or dependency issues\n   - Look at time-series of operator status changes\n\n   **For \"other\" failures:**\n   - Perform comprehensive analysis of all available logs\n   - Check installer log for any errors or fatal messages\n   - Review log bundle if available\n   - Look for unusual patterns or timeout messages\n\n3. **Extract key information**\n   - If `failed-units.txt` exists, read it to find failed services\n   - For each failed service, find corresponding journal log\n   - Extract error messages from journal logs\n   - Save findings to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/log-bundle-summary.txt`\n\n### Step 10: Generate Analysis Report\n\n1. **Create comprehensive analysis report**\n   - Combine findings from all sources:\n     - Installer log analysis\n     - Log bundle analysis\n     - sosreport analysis (if applicable)\n\n2. **Report structure**\n   ```\n   OpenShift Installation Failure Analysis\n   ========================================\n\n   Job: {job-name}\n   Build ID: {build_id}\n   Job Type: {metal/cloud}\n   Prow URL: {original-url}\n\n   Failure Stage: {stage from junit_install.xml}\n\n   Summary\n   -------\n   {High-level summary of the failure}\n\n   Installer Log Analysis\n   ----------------------\n   {Key findings from installer log}\n\n   First Error:\n   {First error message with timestamp}\n\n   Context:\n   {Surrounding log lines}\n\n   Log Bundle Analysis\n   -------------------\n   {Findings from log bundle}\n\n   Failed Units:\n   {List from failed-units.txt}\n\n   Key Journal Errors:\n   {Important errors from journal logs}\n\n   Metal Installation Analysis (Metal Jobs Only)\n   -----------------------------------------\n   {Summary from metal install failure skill}\n   - Dev-scripts setup status\n   - Console log findings\n   - Key metal-specific errors\n\n   See detailed metal analysis: .work/prow-job-analyze-install-failure/{build_id}/analysis/metal-analysis.txt\n\n   Recommended Next Steps\n   ----------------------\n   {Actionable debugging steps based on failure mode:\n\n   For \"configuration\" failures:\n   - Review install-config.yaml validation errors\n   - Check for missing required fields\n   - Verify credential format and availability\n\n   For \"infrastructure\" failures:\n   - Check cloud provider quota and limits\n   - Review cloud provider service status for outages\n   - Verify API credentials and permissions\n   - Check for rate limiting in cloud API calls\n\n   For \"cluster bootstrap\" failures:\n   - Review bootkube logs for control plane startup issues\n   - Check etcd cluster formation in etcd.log\n   - Examine kube-apiserver startup in kube-apiserver.log\n   - Review bootstrap VM serial console for boot issues\n\n   For \"cluster creation\" failures:\n   - Identify which operators failed to deploy\n   - Check if must-gather was collected (look for must-gather*.tar files)\n   - If must-gather exists: Review specific operator logs in gather-must-gather\n   - If must-gather doesn't exist: Cluster was too unstable to collect diagnostics; rely on installer log and log bundle\n   - Check for resource conflicts or missing dependencies\n\n   For \"cluster operator stability\" failures:\n   - Identify operators not reaching stable state\n   - Check operator conditions (available, progressing, degraded)\n   - Check if must-gather exists before suggesting to review it\n   - Review operator logs for stuck operations (if must-gather available)\n   - Look for time-series of operator status changes\n\n   For \"other\" failures:\n   - Perform comprehensive log review\n   - Look for timeout or unusual error patterns\n   - Check all available artifacts systematically\n   }\n\n   Artifacts Location\n   ------------------\n   All artifacts downloaded to:\n   .work/prow-job-analyze-install-failure/{build_id}/logs/\n\n   - Installer logs: .openshift_install*.log\n   - Log bundle: log-bundle-*/\n   - sosreport: sosreport-*/ (metal jobs only)\n   ```\n\n3. **Save report**\n   - Save to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/report.txt`\n\n### Step 11: Present Results to User\n\n1. **Display summary**\n   - Show the analysis report to the user\n   - Highlight the most critical findings\n   - Provide file paths for further investigation\n\n2. **Offer next steps**\n   - Based on the failure type, suggest specific debugging actions:\n     - For bootstrap failures: Check API server and etcd logs\n     - For install-complete failures: Check cluster operator status\n     - For network issues: Review network configuration\n     - For metal job failures: Examine VM console logs\n   - **IMPORTANT**: Only suggest reviewing must-gather if you verified the .tar file exists\n     - Don't suggest downloading must-gather if no .tar file was found\n     - If must-gather doesn't exist, note that the cluster was too unstable to collect it\n\n3. **Provide artifact locations**\n   - List all downloaded files with their paths\n   - Note whether must-gather was successfully collected or not\n   - Explain how to explore the logs further\n   - Mention that artifacts are cached for faster re-analysis\n\n## Installation Stages Reference\n\nUnderstanding the installation stages helps target analysis:\n\n1. **Pre-installation** (Failure mode: \"configuration\")\n   - Validation of install-config.yaml\n   - Credential checks\n   - Image resolution\n   - **Common failures**: Invalid install-config.yaml, missing required fields, validation errors\n\n2. **Infrastructure Creation** (Failure mode: \"infrastructure\")\n   - Creating cloud resources (VMs, networks, storage)\n   - For metal: VM provisioning on hypervisor\n   - **Common failures**: Cloud quota exceeded, rate limiting, API outages, permission errors\n\n3. **Bootstrap** (Failure mode: \"cluster bootstrap\")\n   - Bootstrap node boots with temporary control plane\n   - Bootstrap API server and etcd start\n   - Bootstrap creates master nodes\n   - **Common failures**: API server won't start, etcd formation issues, bootkube errors\n\n4. **Master Node Bootstrap**\n   - Master nodes boot and join bootstrap etcd\n   - Masters form permanent control plane\n   - Bootstrap control plane transfers to masters\n   - **Common failures**: Masters can't reach bootstrap, network issues, ignition failures\n\n5. **Bootstrap Complete**\n   - Bootstrap node is no longer needed\n   - Masters are running permanent control plane\n   - Cluster operators begin initialization\n   - **Common failures**: Control plane not transferring, master nodes not ready\n\n6. **Cluster Operators Initialization** (Failure mode: \"cluster creation\")\n   - Core cluster operators start\n   - Operators begin deployment\n   - Initial operator stabilization\n   - **Common failures**: Operators can't deploy, resource conflicts, dependency issues\n\n7. **Cluster Operators Stabilization** (Failure mode: \"cluster operator stability\")\n   - Operators reach stable state (available=True, progressing=False, degraded=False)\n   - Worker nodes can join\n   - **Common failures**: Operators stuck progressing, degraded state, availability issues\n\n8. **Install Complete**\n   - All cluster operators are available and stable\n   - Cluster is fully functional\n   - Installation successful\n\n## Failure Mode Mapping\n\n| JUnit Failure Mode | Installation Stage | Where to Look | Artifacts Available |\n|-------------------|-------------------|---------------|-------------------|\n| `configuration` | Pre-installation | Installer log only | No log bundle |\n| `infrastructure` | Infrastructure Creation | Installer log, cloud API errors | Partial or no log bundle |\n| `cluster bootstrap` | Bootstrap | Log bundle (bootkube, etcd, kube-apiserver) | Full log bundle |\n| `cluster creation` | Operators Initialization | gather-must-gather, operator logs | Full artifacts |\n| `cluster operator stability` | Operators Stabilization | gather-must-gather, operator status | Full artifacts |\n| `other` | Unknown | All available logs | Varies |\n\n## Key Files Reference\n\n### Installer Logs\n- **Location**: `.openshift_install-{timestamp}.log`\n- **Format**: Structured log with timestamp, level, message\n- **Key patterns**:\n  - `level=error` - Error messages\n  - `level=fatal` - Fatal errors that stop installation\n  - `waiting for` - Timeout/waiting messages\n\n### Log Bundle Structure\n- **bootstrap/journals/bootkube.log**: Bootstrap control plane initialization\n- **bootstrap/journals/kubelet.log**: Bootstrap kubelet (container orchestration)\n- **clusterapi/kube-apiserver.log**: API server logs\n- **clusterapi/etcd.log**: etcd cluster logs\n- **serial/*.log**: Node console output\n- **failed-units.txt**: systemd services that failed\n\n### Metal Job Artifacts\n\nMetal installations are handled by the `prow-job-analyze-metal-install-failure` skill.\nSee that skill's documentation for details on dev-scripts, libvirt logs, sosreport, and squid logs.\n\n## Tips\n\n- **CRITICAL**: Work backwards from the end of the installer log, not forwards from the beginning\n- Early errors often resolve themselves due to eventual consistency - focus on final errors\n- Always start by checking the installer log for the **LAST** error, not the first\n- The log bundle provides detailed node-level diagnostics\n- Serial console logs show the actual boot sequence and can reveal kernel panics\n- Failed systemd units in failed-units.txt are strong indicators of the problem\n- Bootstrap failures are often etcd or API server related\n- Install-complete failures are usually cluster operator issues\n- Metal jobs use a specialized skill for analysis (prow-job-analyze-metal-install-failure)\n- Cache artifacts in `.work/prow-job-analyze-install-failure/{build_id}/` for re-analysis\n- Use grep with relevant keywords to filter large log files\n- Timeline of events from installer log helps correlate issues across logs\n- Pay attention to job name clues: fips, ipv6, dualstack, metal, single-node, upgrade\n- IPv6 jobs are often disconnected and use mirror registries\n- Only suggest must-gather if the .tar file exists; if not, cluster was too unstable\n\n## Important Notes\n\n1. **Eventual Consistency Behavior**\n   - OpenShift installations exhibit eventual consistency\n   - Components report errors while waiting for dependencies\n   - Early errors are EXPECTED and usually resolve automatically\n   - **Always analyze backwards from the final timeout, not forwards from the start**\n   - Only errors that persist until failure are relevant root causes\n\n2. **Upgrade Jobs and Installation**\n   - Jobs with \"upgrade\" in the name perform installation FIRST, then upgrade\n   - If you're analyzing an installation failure in an upgrade job, it never got to the upgrade phase\n   - \"minor\" upgrade: Installs 4.n-1 version (e.g., 4.20 for a 4.21 upgrade job)\n   - \"micro\" upgrade: Installs earlier payload in same stream\n\n3. **Log Bundle Availability**\n   - Not all jobs produce log bundles\n   - Older jobs may not have this feature\n   - Installation must reach a certain point to generate log bundle\n\n4. **Must-Gather Availability**\n   - Must-gather only exists if a `must-gather*.tar` file is present\n   - If no .tar file exists, the cluster was too unstable to collect diagnostics\n   - **Never suggest downloading must-gather unless you verified the .tar file exists**\n\n5. **Metal Job Specifics**\n   - Metal jobs are analyzed using the specialized `prow-job-analyze-metal-install-failure` skill\n   - That skill handles dev-scripts, libvirt console logs, sosreport, and squid logs\n   - See the metal skill documentation for details\n\n6. **Debugging Workflow**\n   - Start with installer log to find **LAST** error (not first)\n   - Use failure stage to guide which logs to examine\n   - Log bundle provides node-level details\n   - For metal jobs, invoke the metal-specific skill for additional analysis\n   - Work backwards in time to trace dependency chains\n\n7. **Common Failure Patterns**\n   - **Bootstrap etcd not starting**: Check etcd.log and bootkube.log\n   - **API server not responding**: Check kube-apiserver.log\n   - **Masters not joining**: Check master serial logs\n   - **Operators degraded**: Check specific operator logs in must-gather (if it exists)\n   - **Network issues**: Check network configuration in bootstrap/network/\n\n8. **File Formats**\n   - Installer log: Plain text, structured format\n   - Journal logs: systemd journal format (plain text export)\n   - Serial logs: Raw console output\n   - YAML files: Kubernetes resource definitions\n   - Compressed files: .gz (gzip), .xz (xz)"
              },
              {
                "name": "Prow Job Analyze Metal Install Failure",
                "description": "Analyze OpenShift bare metal installation failures in Prow CI jobs using dev-scripts artifacts. Use for jobs with \"metal\" in name, for debugging Metal3/Ironic provisioning, installation, or dev-scripts setup failures. You may also use the prow-job-analyze-install-failure skill with this one.",
                "path": "plugins/prow-job/skills/prow-job-analyze-metal-install-failure/SKILL.md",
                "frontmatter": {
                  "name": "Prow Job Analyze Metal Install Failure",
                  "description": "Analyze OpenShift bare metal installation failures in Prow CI jobs using dev-scripts artifacts. Use for jobs with \"metal\" in name, for debugging Metal3/Ironic provisioning, installation, or dev-scripts setup failures. You may also use the prow-job-analyze-install-failure skill with this one."
                },
                "content": "# Prow Job Analyze Metal Install Failure\n\nThis skill helps debug OpenShift bare metal installation failures in CI jobs by analyzing dev-scripts logs, libvirt console logs, sosreports, and other metal-specific artifacts.\n\n## When to Use This Skill\n\nUse this skill when:\n- A bare metal CI job fails with \"install should succeed\" test failure\n- The job name contains \"metal\" or \"baremetal\"\n- You need to debug Metal3/Ironic provisioning issues\n- You need to analyze dev-scripts setup failures\n\nThis skill is invoked by the main `prow-job-analyze-install-failure` skill when it detects a metal job.\n\n## Metal Installation Overview\n\nMetal IPI jobs use **dev-scripts** (https://github.com/openshift-metal3/dev-scripts) with **Metal3** and **Ironic** to install OpenShift:\n- **dev-scripts**: Framework for setting up and installing OpenShift on bare metal\n- **Metal3**: Kubernetes-native interface to Ironic\n- **Ironic**: Bare metal provisioning service\n\nThe installation process has multiple layers:\n1. **dev-scripts setup**: Configures hypervisor, sets up Ironic/Metal3, builds installer\n2. **Ironic provisioning**: Provisions bare metal nodes (or VMs acting as bare metal)\n3. **OpenShift installation**: Standard installer runs on provisioned nodes\n\nFailures can occur at any layer, so analysis must check all of them.\n\n## Network Architecture (CRITICAL for Understanding IPv6/Disconnected Jobs)\n\n**IMPORTANT**: The term \"disconnected\" refers to the cluster nodes, NOT the hypervisor.\n\n### Hypervisor (dev-scripts host)\n- **HAS** full internet access\n- Downloads packages, container images, and dependencies from the public internet\n- Runs dev-scripts Ansible playbooks that download tools (Go, installer, etc.)\n- Hosts a local mirror registry to serve the cluster\n\n### Cluster VMs/Nodes\n- Run in a **private IPv6-only network** (when IP_STACK=v6)\n- **NO** direct internet access (truly disconnected)\n- Pull container images from the hypervisor's local mirror registry\n- Access to hypervisor services only (registry, DNS, etc.)\n\n### Common Misconception\nWhen analyzing failures in \"metal-ipi-ovn-ipv6\" jobs:\n-  WRONG: \"The hypervisor cannot access the internet, so downloads fail\"\n-  CORRECT: \"The hypervisor has internet access. If downloads fail, it's likely due to the remote service being unavailable, not network restrictions\"\n\n### Implications for Failure Analysis\n1. **Dev-scripts failures** (steps 01-05): If external downloads fail, check if the remote service/URL is down or has removed the resource\n2. **Installation failures** (step 06+): If cluster nodes cannot pull images, check the local mirror registry on the hypervisor\n3. **HTTP 403/404 errors during dev-scripts**: Usually means the resource was removed from the upstream source, not that the network is restricted\n\n## Prerequisites\n\n1. **gcloud CLI Installation**\n   - Check if installed: `which gcloud`\n   - If not installed, provide instructions for the user's platform\n   - Installation guide: https://cloud.google.com/sdk/docs/install\n\n2. **gcloud Authentication (Optional)**\n   - The `test-platform-results` bucket is publicly accessible\n   - No authentication is required for read access\n\n## Input Format\n\nThe user will provide:\n1. **Build ID** - Extracted by the main skill\n2. **Bucket path** - Extracted by the main skill\n3. **Target name** - Extracted by the main skill\n4. **Working directory** - Already created by main skill\n\n## Metal-Specific Artifacts\n\nMetal jobs produce several diagnostic archives:\n\n### OFCIR Acquisition Logs\n- **Location**: `{target}/ofcir-acquire/`\n- **Purpose**: Shows the OFCIR host acquisition process\n- **Contains**:\n  - `build-log.txt`: Log showing pool, provider, and host details\n  - `artifacts/junit_metal_setup.xml`: JUnit with test `[sig-metal] should get working host from infra provider`\n- **Critical for**: Determining if the job failed to acquire a host before installation started\n- **Key information**:\n  - Pool name (e.g., \"cipool-ironic-cluster-el9\", \"cipool-ibmcloud\")\n  - Provider (e.g., \"ironic\", \"equinix\", \"aws\", \"ibmcloud\")\n  - Host name and details\n\n### Dev-scripts Logs\n- **Location**: `{target}/baremetalds-devscripts-setup/artifacts/root/dev-scripts/logs/`\n- **Purpose**: Shows installation setup process and cluster installation\n- **Contains**: Numbered log files showing each setup step (requirements, host config, Ironic setup, installer build, cluster creation). **Note**: dev-scripts invokes the installer, so installer logs (`.openshift_install*.log`) will also be present in the devscripts folders.\n- **Critical for**: Early failures before cluster creation, Ironic/Metal3 setup issues, installation failures\n\n### libvirt-logs.tar\n- **Location**: `{target}/baremetalds-devscripts-gather/artifacts/`\n- **Purpose**: VM/node console logs showing boot sequence\n- **Contains**: Console output from bootstrap and master VMs/nodes\n- **Critical for**: Boot failures, Ignition errors, kernel panics, network configuration issues\n\n### log-bundle-*.tar (from gather or post-installation)\n- **Location**: `{target}/baremetalds-devscripts-gather/artifacts/`\n- **Purpose**: Cluster-level diagnostics including Ironic/Metal3 logs\n- **Contains**:\n  - **Bootstrap Ironic logs**: Located at `bootstrap/journals/ironic.log` and `bootstrap/journals/metal3-baremetal-operator.log`\n    - Shows master node provisioning during bootstrap phase\n    - Contains Redfish/IPMI BMC communication for masters\n  - **Control-plane Ironic logs**: Located at `control-plane/{node-ip}/containers/metal3-ironic-*.log` and `control-plane/{node-ip}/containers/metal3-baremetal-operator-*.log`\n    - Shows worker node provisioning\n  - Bootstrap node journals (bootkube, kubelet, crio)\n  - Control plane container logs\n  - Cluster API resources\n- **Critical for**: BareMetalHost registration failures, BMC connectivity issues (IPMI/Redfish), provisioning state problems, power management errors\n- **Key Ironic errors to look for**:\n  - BMC (IPMI, Redfish) errors\n  - Node registration failures in Ironic\n  - Power state query failures\n  - Provisioning state transitions stuck\n- **IMPORTANT**:\n  - Bootstrap Ironic logs only show master provisioning\n  - Control-plane Ironic logs show worker provisioning\n  - Always check control-plane logs when investigating worker issues\n\n### sosreport\n- **Location**: `{target}/baremetalds-devscripts-gather/artifacts/`\n- **Purpose**: Hypervisor system diagnostics\n- **Contains**: Hypervisor logs, system configuration, diagnostic command output\n- **Useful for**: Hypervisor-level issues, not typically needed for VM boot problems\n\n### squid-logs.tar\n- **Location**: `{target}/baremetalds-devscripts-gather/artifacts/`\n- **Purpose**: Squid proxy logs for inbound CI access to the cluster\n- **Contains**: Logs showing CI system's inbound connections to the cluster under test. **Note**: The squid proxy runs on the hypervisor for INBOUND access (CI  cluster), NOT for outbound access (cluster  registry).\n- **Critical for**: Debugging CI access issues to the cluster, particularly in IPv6/disconnected environments\n\n## Implementation Steps\n\n### Step 1: Check OFCIR Acquisition\n\n1. **Download OFCIR logs**\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/artifacts/{target}/ofcir-acquire/build-log.txt .work/prow-job-analyze-install-failure/{build_id}/logs/ofcir-build-log.txt --no-user-output-enabled 2>&1 || echo \"OFCIR build log not found\"\n   gcloud storage cp gs://test-platform-results/{bucket-path}/artifacts/{target}/ofcir-acquire/artifacts/junit_metal_setup.xml .work/prow-job-analyze-install-failure/{build_id}/logs/junit_metal_setup.xml --no-user-output-enabled 2>&1 || echo \"OFCIR JUnit not found\"\n   ```\n\n2. **Check junit_metal_setup.xml for acquisition failure**\n   - Read the JUnit file\n   - Look for test case: `[sig-metal] should get working host from infra provider`\n   - If the test failed, OFCIR failed to acquire a host\n   - This means installation never started - the failure is in host acquisition\n\n3. **Extract OFCIR details from build-log.txt**\n   - Parse the JSON in the build log to extract:\n     - `pool`: The OFCIR pool name\n     - `provider`: The infrastructure provider\n     - `name`: The host name allocated\n   - Save these for the final report\n\n4. **If OFCIR acquisition failed**\n   - Stop analysis - installation never started\n   - Report: \"OFCIR host acquisition failed\"\n   - Include pool and provider information\n   - Suggest: Check OFCIR pool availability and provider status\n\n### Step 2: Download Dev-Scripts Logs\n\n1. **Download dev-scripts logs directory**\n   ```bash\n   gcloud storage cp -r gs://test-platform-results/{bucket-path}/artifacts/{target}/baremetalds-devscripts-setup/artifacts/root/dev-scripts/logs/ .work/prow-job-analyze-install-failure/{build_id}/logs/devscripts/ --no-user-output-enabled\n   ```\n\n2. **Handle missing dev-scripts logs gracefully**\n   - Some metal jobs may not have dev-scripts artifacts\n   - If missing, note this in the analysis and proceed with other artifacts\n\n### Step 2: Download libvirt Console Logs\n\n1. **Find and download libvirt-logs.tar**\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep \"libvirt-logs\\.tar$\"\n   gcloud storage cp {full-gcs-path-to-libvirt-logs.tar} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   ```\n\n2. **Extract libvirt logs**\n   ```bash\n   tar -xf .work/prow-job-analyze-install-failure/{build_id}/logs/libvirt-logs.tar -C .work/prow-job-analyze-install-failure/{build_id}/logs/\n   ```\n\n### Step 3: Download Optional Artifacts\n\n1. **Download sosreport (optional)**\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep \"sosreport.*\\.tar\\.xz$\"\n   gcloud storage cp {full-gcs-path-to-sosreport} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   tar -xf .work/prow-job-analyze-install-failure/{build_id}/logs/sosreport-{name}.tar.xz -C .work/prow-job-analyze-install-failure/{build_id}/logs/\n   ```\n\n2. **Download squid-logs (optional, for IPv6/disconnected jobs)**\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep \"squid-logs.*\\.tar$\"\n   gcloud storage cp {full-gcs-path-to-squid-logs} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   tar -xf .work/prow-job-analyze-install-failure/{build_id}/logs/squid-logs-{name}.tar -C .work/prow-job-analyze-install-failure/{build_id}/logs/\n   ```\n\n### Step 4: Analyze Dev-Scripts Logs\n\n**Check dev-scripts logs FIRST** - they show what happened during setup and installation.\n\n1. **Read dev-scripts logs in order**\n   - Logs are numbered sequentially showing setup steps\n   - **Note**: dev-scripts invokes the installer, so you'll find `.openshift_install*.log` files in the devscripts directories\n   - Look for the first error or failure\n\n2. **Key errors to look for**:\n   - **Host configuration failures**: Networking, DNS, storage setup issues\n   - **Ironic/Metal3 setup issues**: BMC connectivity, provisioning network, node registration failures\n   - **Installer build failures**: Problems building the OpenShift installer binary\n   - **Install-config validation errors**: Invalid configuration before cluster creation\n   - **Installation failures**: Check installer logs (`.openshift_install*.log`) present in devscripts folders\n\n3. **Important distinction**:\n   - If failure is in dev-scripts setup logs (01-05), the problem is in the setup process\n   - If failure is in installer logs or 06_create_cluster, the problem is in the cluster installation (also analyzed by main skill)\n\n4. **Save dev-scripts analysis**:\n   - Save findings to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/devscripts-summary.txt`\n\n### Step 5: Analyze Ironic Logs (from log-bundle)\n\n**CRITICAL: Check the RIGHT Ironic logs based on what failed**\n\nThe log bundle contains TWO sets of Ironic logs in different locations:\n- **Bootstrap Ironic logs**: For master node provisioning\n- **Control-plane Ironic logs**: For worker node provisioning\n\n**Which logs to check:**\n- Masters failed to provision  Check `bootstrap/journals/ironic.log`\n- Workers failed to provision  Check `control-plane/{ip}/containers/metal3-ironic-*.log`\n- Unsure which failed  Check all\n\n1. **Download and extract log bundle**\n   ```bash\n   gcloud storage ls -r gs://test-platform-results/{bucket-path}/artifacts/ 2>&1 | grep \"log-bundle.*\\.tar$\"\n   gcloud storage cp {full-gcs-path-to-log-bundle.tar} .work/prow-job-analyze-install-failure/{build_id}/logs/ --no-user-output-enabled\n   tar -xf .work/prow-job-analyze-install-failure/{build_id}/logs/log-bundle-*.tar -C .work/prow-job-analyze-install-failure/{build_id}/logs/\n   ```\n\n2. **Find ALL Ironic logs**\n   ```bash\n   # Bootstrap Ironic (master provisioning)\n   find .work/prow-job-analyze-install-failure/{build_id}/logs/ -path \"*/bootstrap/journals/ironic.log\"\n   find .work/prow-job-analyze-install-failure/{build_id}/logs/ -path \"*/bootstrap/journals/metal3-baremetal-operator.log\"\n\n   # Control-plane Ironic (worker provisioning) - CRITICAL for worker failures\n   find .work/prow-job-analyze-install-failure/{build_id}/logs/ -path \"*/control-plane/*/containers/metal3-ironic-*.log\"\n   find .work/prow-job-analyze-install-failure/{build_id}/logs/ -path \"*/control-plane/*/containers/metal3-baremetal-operator-*.log\"\n   ```\n\n3. **Analyze the Ironic logs**:\n\n   **For Master Provisioning Issues** (check bootstrap logs):\n   - Location: `bootstrap/journals/ironic.log` and `bootstrap/journals/metal3-baremetal-operator.log`\n   - What to search: Master node UUIDs, master BareMetalHost names\n\n   **For Worker Provisioning Issues** (check control-plane logs):\n   - Location: `control-plane/{node-ip}/containers/metal3-ironic-*.log`\n   - What to search: Worker node UUIDs, worker BareMetalHost names\n\n4. **Map node UUIDs to BareMetalHost names**:\n   - Ironic logs use node UUIDs (e.g., `b7fa5b83-91d0-46ee-acd2-e4b33e9ac983`)\n   - Find the corresponding BareMetalHost name from installer logs or must-gather\n   - This helps identify which specific worker or master failed\n\n5. **Save Ironic analysis**:\n   - Save findings to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/ironic-summary.txt`\n   - Include:\n     - Which Ironic logs were checked (bootstrap vs control-plane)\n     - Node UUIDs with errors\n     - Specific error messages (SSL, BMC connection, etc.)\n     - Whether masters or workers were affected\n\n### Step 6: Analyze libvirt Console Logs\n\n**Console logs are CRITICAL for metal failures during cluster creation.**\n\n1. **Find console logs**\n   ```bash\n   find .work/prow-job-analyze-install-failure/{build_id}/logs/ -name \"*console*.log\"\n   ```\n   - Look for patterns like `{cluster-name}-bootstrap_console.log`, `{cluster-name}-master-{N}_console.log`\n\n2. **Analyze console logs for boot/provisioning issues**:\n   - **Kernel boot failures or panics**: Look for \"panic\", \"kernel\", \"oops\"\n   - **Ignition failures**: Look for \"ignition\", \"config fetch failed\", \"Ignition failed\"\n   - **Network configuration issues**: Look for \"dhcp\", \"network unreachable\", \"DNS\", \"timeout\"\n   - **Disk mounting failures**: Look for \"mount\", \"disk\", \"filesystem\"\n   - **Service startup failures**: Look for systemd errors, service failures\n\n3. **Console logs show the complete boot sequence**:\n   - As if you were watching a physical console\n   - Shows kernel messages, Ignition provisioning, CoreOS startup\n   - Critical for understanding what happened before the system was fully booted\n\n4. **Save console log analysis**:\n   - Save findings to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/console-summary.txt`\n\n### Step 7: Analyze sosreport\n\n**Only needed for hypervisor-level issues.**\n\n1. **Check sosreport for hypervisor diagnostics**:\n   - `var/log/messages` - Hypervisor system log\n   - `sos_commands/` - Output of diagnostic commands\n   - `etc/libvirt/` - Libvirt configuration\n\n2. **Look for hypervisor-level issues**:\n   - Libvirt errors\n   - Network configuration problems on hypervisor\n   - Resource constraints (CPU, memory, disk)\n\n### Step 8: Analyze squid-logs (If Downloaded)\n\n**Important for debugging CI access to the cluster.**\n\n1. **Check squid proxy logs**:\n   - Look for failed connections from CI to the cluster\n   - Look for HTTP errors or blocked requests\n   - Check patterns of CI test framework access issues\n\n2. **Common issues**:\n   - CI unable to connect to cluster API\n   - Proxy configuration errors blocking CI access\n   - Network routing issues between CI and cluster\n   - **Note**: These logs are for INBOUND access (CI  cluster), not for cluster's outbound access to registries\n\n### Step 9: Generate Metal-Specific Analysis Report\n\n1. **Create comprehensive metal analysis report**:\n   ```\n   Metal Installation Failure Analysis\n   ====================================\n\n   Job: {job-name}\n   Build ID: {build_id}\n   Prow URL: {original-url}\n\n   Installation Method: dev-scripts + Metal3 + Ironic\n\n   OFCIR Host Acquisition\n   ----------------------\n   Pool: {pool name from OFCIR build log}\n   Provider: {provider from OFCIR build log}\n   Host: {host name from OFCIR build log}\n   Status: {Success or Failure}\n\n   {If OFCIR acquisition failed, note that installation never started}\n\n   Dev-Scripts Analysis\n   --------------------\n   {Summary of dev-scripts logs}\n\n   Key Findings:\n   - {First error in dev-scripts setup}\n   - {Related errors}\n\n   If dev-scripts failed: The problem is in the setup process (host config, Ironic, installer build)\n   If dev-scripts succeeded: The problem is in cluster installation (see main analysis)\n\n   Console Logs Analysis\n   ---------------------\n   {Summary of VM/node console logs}\n\n   Bootstrap Node:\n   - {Boot sequence status}\n   - {Ignition status}\n   - {Network configuration}\n   - {Key errors}\n\n   Master Nodes:\n   - {Status for each master}\n   - {Key errors}\n\n   Hypervisor Diagnostics (sosreport)\n   -----------------------------------\n   {Summary of sosreport findings, if applicable}\n\n   Proxy Logs (squid)\n   ------------------\n   {Summary of proxy logs, if applicable}\n   Note: Squid logs show CI access to the cluster, not cluster's registry access\n\n   Metal-Specific Recommended Steps\n   ---------------------------------\n   Based on the failure:\n\n   For dev-scripts setup failures:\n   - Review host configuration (networking, DNS, storage)\n   - Check Ironic/Metal3 setup logs for BMC/provisioning issues\n   - Verify installer build completed successfully\n   - Check installer logs in devscripts folders\n\n   For console boot failures:\n   - Check Ignition configuration and network connectivity\n   - Review kernel boot messages for hardware issues\n   - Verify network configuration (DHCP, DNS, routing)\n\n   For CI access issues:\n   - Check squid proxy logs for failed CI connections to cluster\n   - Verify network routing between CI and cluster\n   - Check proxy configuration\n\n   Artifacts Location\n   ------------------\n   Dev-scripts logs: .work/prow-job-analyze-install-failure/{build_id}/logs/devscripts/\n   Console logs: .work/prow-job-analyze-install-failure/{build_id}/logs/\n   sosreport: .work/prow-job-analyze-install-failure/{build_id}/logs/sosreport-*/\n   squid logs: .work/prow-job-analyze-install-failure/{build_id}/logs/squid-logs-*/\n   ```\n\n2. **Save report**:\n   - Save to: `.work/prow-job-analyze-install-failure/{build_id}/analysis/metal-analysis.txt`\n\n### Step 10: Return Metal Analysis to Main Skill\n\n1. **Provide summary to main skill**:\n   - Brief summary of metal-specific findings\n   - Indication of whether failure was in dev-scripts setup or cluster installation\n   - Key error messages and recommended actions\n\n## Common Metal Failure Patterns\n\n| Issue | Symptoms | Where to Look |\n|-------|----------|---------------|\n| **Dev-scripts host config** | Early failure before cluster creation | Dev-scripts logs (host configuration step) |\n| **Ironic/Metal3 setup** | Provisioning failures, BMC errors | Dev-scripts logs (Ironic setup) |\n| **BMC communication** | BareMetalHost stuck registering, power state failures | Ironic logs (in log-bundle), BareMetalHost status |\n| **Node boot failure** | VMs/nodes won't boot | Console logs (kernel, boot sequence) |\n| **Ignition failure** | Nodes boot but don't provision | Console logs (Ignition messages) |\n| **Network config** | DHCP failures, DNS issues | Console logs (network messages), dev-scripts host config |\n| **CI access issues** | Tests can't connect to cluster | squid logs (proxy logs for CI  cluster access) |\n| **Hypervisor issues** | Resource constraints, libvirt errors | sosreport (system logs, libvirt config) |\n\n## Tips\n\n- **Check dev-scripts logs FIRST**: They show setup and installation (dev-scripts invokes the installer)\n- **Installer logs in devscripts**: Look for `.openshift_install*.log` files in devscripts directories\n- **Check Ironic logs for BMC issues**: BareMetalHost provisioning failures usually show detailed errors in Ironic logs\n- **Console logs are critical**: They show the actual boot sequence like a physical console\n- **Ironic/Metal3 setup errors** often appear in dev-scripts setup logs\n- **BMC communication errors** appear in Ironic container logs in the log-bundle\n- **Squid logs are for CI access**: They show inbound CI  cluster access, not outbound cluster  registry\n- **Boot vs. provisioning**: Boot failures appear in console logs, provisioning failures in Ironic logs\n- **Layer distinction**: Separate dev-scripts setup from Ironic provisioning from OpenShift installation"
              },
              {
                "name": "Prow Job Analyze Resource",
                "description": "Analyze Kubernetes resource lifecycle in Prow CI job artifacts by parsing audit logs and pod logs from GCS, generating interactive HTML reports with timelines",
                "path": "plugins/prow-job/skills/prow-job-analyze-resource/SKILL.md",
                "frontmatter": {
                  "name": "Prow Job Analyze Resource",
                  "description": "Analyze Kubernetes resource lifecycle in Prow CI job artifacts by parsing audit logs and pod logs from GCS, generating interactive HTML reports with timelines"
                },
                "content": "# Prow Job Analyze Resource\n\nThis skill analyzes the lifecycle of Kubernetes resources during Prow CI job execution by downloading and parsing artifacts from Google Cloud Storage.\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Debug Prow CI test failures by tracking resource state changes\n- Understand when and how a Kubernetes resource was created, modified, or deleted during a test\n- Analyze resource lifecycle across audit logs and pod logs from ephemeral test clusters\n- Generate interactive HTML reports showing resource events over time\n- Search for specific resources (pods, deployments, configmaps, etc.) in Prow job artifacts\n\n## Prerequisites\n\nBefore starting, verify these prerequisites:\n\n1. **gcloud CLI Installation**\n   - Check if installed: `which gcloud`\n   - If not installed, provide instructions for the user's platform\n   - Installation guide: https://cloud.google.com/sdk/docs/install\n\n2. **gcloud Authentication (Optional)**\n   - The `test-platform-results` bucket is publicly accessible\n   - No authentication is required for read access\n   - Skip authentication checks\n\n## Input Format\n\nThe user will provide:\n1. **Prow job URL** - gcsweb URL containing `test-platform-results/`\n   - Example: `https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/30393/pull-ci-openshift-origin-main-okd-scos-e2e-aws-ovn/1978913325970362368/`\n   - URL may or may not have trailing slash\n\n2. **Resource specifications** - Comma-delimited list in format `[namespace:][kind/]name`\n   - Supports regex patterns for matching multiple resources\n   - Examples:\n     - `pod/etcd-0` - pod named etcd-0 in any namespace\n     - `openshift-etcd:pod/etcd-0` - pod in specific namespace\n     - `etcd-0` - any resource named etcd-0 (no kind filter)\n     - `pod/etcd-0,configmap/cluster-config` - multiple resources\n     - `resource-name-1|resource-name-2` - multiple resources using regex OR\n     - `e2e-test-project-api-.*` - all resources matching the pattern\n\n## Implementation Steps\n\n### Step 1: Parse and Validate URL\n\n1. **Extract bucket path**\n   - Find `test-platform-results/` in URL\n   - Extract everything after it as the GCS bucket relative path\n   - If not found, error: \"URL must contain 'test-platform-results/'\"\n\n2. **Extract build_id**\n   - Search for pattern `/(\\d{10,})/` in the bucket path\n   - build_id must be at least 10 consecutive decimal digits\n   - Handle URLs with or without trailing slash\n   - If not found, error: \"Could not find build ID (10+ digits) in URL\"\n\n3. **Extract prowjob name**\n   - Find the path segment immediately preceding build_id\n   - Example: In `.../pull-ci-openshift-origin-main-okd-scos-e2e-aws-ovn/1978913325970362368/`\n   - Prowjob name: `pull-ci-openshift-origin-main-okd-scos-e2e-aws-ovn`\n\n4. **Construct GCS paths**\n   - Bucket: `test-platform-results`\n   - Base GCS path: `gs://test-platform-results/{bucket-path}/`\n   - Ensure path ends with `/`\n\n### Step 2: Parse Resource Specifications\n\nFor each comma-delimited resource spec:\n\n1. **Parse format** `[namespace:][kind/]name`\n   - Split on `:` to get namespace (optional)\n   - Split remaining on `/` to get kind (optional) and name (required)\n   - Store as structured data: `{namespace, kind, name}`\n\n2. **Validate**\n   - name is required\n   - namespace and kind are optional\n   - Examples:\n     - `pod/etcd-0`  `{kind: \"pod\", name: \"etcd-0\"}`\n     - `openshift-etcd:pod/etcd-0`  `{namespace: \"openshift-etcd\", kind: \"pod\", name: \"etcd-0\"}`\n     - `etcd-0`  `{name: \"etcd-0\"}`\n\n### Step 3: Create Working Directory\n\n1. **Check for existing artifacts first**\n   - Check if `.work/prow-job-analyze-resource/{build_id}/logs/` directory exists and has content\n   - If it exists with content:\n     - Use AskUserQuestion tool to ask:\n       - Question: \"Artifacts already exist for build {build_id}. Would you like to use the existing download or re-download?\"\n       - Options:\n         - \"Use existing\" - Skip to artifact parsing step (Step 6)\n         - \"Re-download\" - Continue to clean and re-download\n     - If user chooses \"Re-download\":\n       - Remove all existing content: `rm -rf .work/prow-job-analyze-resource/{build_id}/logs/`\n       - Also remove tmp directory: `rm -rf .work/prow-job-analyze-resource/{build_id}/tmp/`\n       - This ensures clean state before downloading new content\n     - If user chooses \"Use existing\":\n       - Skip directly to Step 6 (Parse Audit Logs)\n       - Still need to download prowjob.json if it doesn't exist\n\n2. **Create directory structure**\n   ```bash\n   mkdir -p .work/prow-job-analyze-resource/{build_id}/logs\n   mkdir -p .work/prow-job-analyze-resource/{build_id}/tmp\n   ```\n   - Use `.work/prow-job-analyze-resource/` as the base directory (already in .gitignore)\n   - Use build_id as subdirectory name\n   - Create `logs/` subdirectory for all downloads\n   - Create `tmp/` subdirectory for temporary files (intermediate JSON, etc.)\n   - Working directory: `.work/prow-job-analyze-resource/{build_id}/`\n\n### Step 4: Download and Validate prowjob.json\n\n1. **Download prowjob.json**\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/prowjob.json .work/prow-job-analyze-resource/{build_id}/logs/prowjob.json --no-user-output-enabled\n   ```\n\n2. **Parse and validate**\n   - Read `.work/prow-job-analyze-resource/{build_id}/logs/prowjob.json`\n   - Search for pattern: `--target=([a-zA-Z0-9-]+)`\n   - If not found:\n     - Display: \"This is not a ci-operator job. The prowjob cannot be analyzed by this skill.\"\n     - Explain: ci-operator jobs have a --target argument specifying the test target\n     - Exit skill\n\n3. **Extract target name**\n   - Capture the target value (e.g., `e2e-aws-ovn`)\n   - Store for constructing gather-extra path\n\n### Step 5: Download Audit Logs and Pod Logs\n\n1. **Construct gather-extra paths**\n   - GCS path: `gs://test-platform-results/{bucket-path}/artifacts/{target}/gather-extra/`\n   - Local path: `.work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/`\n\n2. **Download audit logs**\n   ```bash\n   mkdir -p .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/audit_logs\n   gcloud storage cp -r gs://test-platform-results/{bucket-path}/artifacts/{target}/gather-extra/artifacts/audit_logs/ .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/audit_logs/ --no-user-output-enabled\n   ```\n   - Create directory first to avoid gcloud errors\n   - Use `--no-user-output-enabled` to suppress progress output\n   - If directory not found, warn: \"No audit logs found. Job may not have completed or audit logging may be disabled.\"\n\n3. **Download pod logs**\n   ```bash\n   mkdir -p .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/pods\n   gcloud storage cp -r gs://test-platform-results/{bucket-path}/artifacts/{target}/gather-extra/artifacts/pods/ .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/pods/ --no-user-output-enabled\n   ```\n   - Create directory first to avoid gcloud errors\n   - Use `--no-user-output-enabled` to suppress progress output\n   - If directory not found, warn: \"No pod logs found.\"\n\n### Step 6: Parse Audit Logs and Pod Logs\n\n**IMPORTANT: Use the provided Python script `parse_all_logs.py` from the skill directory to parse both audit logs and pod logs efficiently.**\n\n**Usage:**\n```bash\npython3 plugins/prow-job/skills/prow-job-analyze-resource/parse_all_logs.py <resource_pattern> \\\n  .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/audit_logs \\\n  .work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/pods \\\n  > .work/prow-job-analyze-resource/{build_id}/tmp/all_entries.json\n```\n\n**Resource Pattern Parameter:**\n- The `<resource_pattern>` parameter supports **regex patterns**\n- Use `|` (pipe) to search for multiple resources: `resource1|resource2|resource3`\n- Use `.*` for wildcards: `e2e-test-project-.*`\n- Simple substring matching still works: `my-namespace`\n- Examples:\n  - Single resource: `e2e-test-project-api-pkjxf`\n  - Multiple resources: `e2e-test-project-api-pkjxf|e2e-test-project-api-7zdxx`\n  - Pattern matching: `e2e-test-project-api-.*`\n\n**Note:** The script outputs status messages to stderr which will display as progress. The JSON output to stdout is clean and ready to use.\n\n**What the script does:**\n\n1. **Find all log files**\n   - Audit logs: `.work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/audit_logs/**/*.log`\n   - Pod logs: `.work/prow-job-analyze-resource/{build_id}/logs/artifacts/{target}/gather-extra/artifacts/pods/**/*.log`\n\n2. **Parse audit log files (JSONL format)**\n   - Read file line by line\n   - Each line is a JSON object (JSONL format)\n   - Parse JSON into object `e`\n\n3. **Extract fields from each audit log entry**\n   - `e.verb` - action (get, list, create, update, patch, delete, watch)\n   - `e.user.username` - user making request\n   - `e.responseStatus.code` - HTTP response code (integer)\n   - `e.objectRef.namespace` - namespace (if namespaced)\n   - `e.objectRef.resource` - lowercase plural kind (e.g., \"pods\", \"configmaps\")\n   - `e.objectRef.name` - resource name\n   - `e.requestReceivedTimestamp` - ISO 8601 timestamp\n\n4. **Filter matches for each resource spec**\n   - Uses **regex matching** on `e.objectRef.namespace` and `e.objectRef.name`\n   - Pattern matches if found in either namespace or name field\n   - Supports all regex features:\n     - Pipe operator: `resource1|resource2` matches either resource\n     - Wildcards: `e2e-test-.*` matches all resources starting with `e2e-test-`\n     - Character classes: `[abc]` matches a, b, or c\n   - Simple substring matching still works for patterns without regex special chars\n   - Performance optimization: plain strings use fast substring search\n\n5. **For each audit log match, capture**\n   - **Source**: \"audit\"\n   - **Filename**: Full path to .log file\n   - **Line number**: Line number in file (1-indexed)\n   - **Level**: Based on `e.responseStatus.code`\n     - 200-299: \"info\"\n     - 400-499: \"warn\"\n     - 500-599: \"error\"\n   - **Timestamp**: Parse `e.requestReceivedTimestamp` to datetime\n   - **Content**: Full JSON line (for expandable details)\n   - **Summary**: Generate formatted summary\n     - Format: `{verb} {resource}/{name} in {namespace} by {username}  HTTP {code}`\n     - Example: `create pod/etcd-0 in openshift-etcd by system:serviceaccount:kube-system:deployment-controller  HTTP 201`\n\n6. **Parse pod log files (plain text format)**\n   - Read file line by line\n   - Each line is plain text (not JSON)\n   - Search for resource pattern in line content\n\n7. **For each pod log match, capture**\n   - **Source**: \"pod\"\n   - **Filename**: Full path to .log file\n   - **Line number**: Line number in file (1-indexed)\n   - **Level**: Detect from glog format or default to \"info\"\n     - Glog format: `E0910 11:43:41.153414 ...` (E=error, W=warn, I=info, F=fatalerror)\n     - Non-glog format: default to \"info\"\n   - **Timestamp**: Extract from start of line if present (format: `YYYY-MM-DDTHH:MM:SS.mmmmmmZ`)\n   - **Content**: Full log line\n   - **Summary**: First 200 characters of line (after timestamp if present)\n\n8. **Combine and sort all entries**\n   - Merge audit log entries and pod log entries\n   - Sort all entries chronologically by timestamp\n   - Entries without timestamps are placed at the end\n\n### Step 7: Generate HTML Report\n\n**IMPORTANT: Use the provided Python script `generate_html_report.py` from the skill directory.**\n\n**Usage:**\n```bash\npython3 plugins/prow-job/skills/prow-job-analyze-resource/generate_html_report.py \\\n  .work/prow-job-analyze-resource/{build_id}/tmp/all_entries.json \\\n  \"{prowjob_name}\" \\\n  \"{build_id}\" \\\n  \"{target}\" \\\n  \"{resource_pattern}\" \\\n  \"{gcsweb_url}\"\n```\n\n**Resource Pattern Parameter:**\n- The `{resource_pattern}` should be the **same pattern used in the parse script**\n- For single resources: `e2e-test-project-api-pkjxf`\n- For multiple resources: `e2e-test-project-api-pkjxf|e2e-test-project-api-7zdxx`\n- The script will parse the pattern to display the searched resources in the HTML header\n\n**Output:** The script generates `.work/prow-job-analyze-resource/{build_id}/{first_resource_name}.html`\n\n**What the script does:**\n\n1. **Determine report filename**\n   - Format: `.work/prow-job-analyze-resource/{build_id}/{resource_name}.html`\n   - Uses the primary resource name for the filename\n\n2. **Sort all entries by timestamp**\n   - Loads audit log entries from JSON\n   - Sort chronologically (ascending)\n   - Entries without timestamps go at the end\n\n3. **Calculate timeline bounds**\n   - min_time: Earliest timestamp found\n   - max_time: Latest timestamp found\n   - Time range: max_time - min_time\n\n4. **Generate HTML structure**\n\n   **Header Section:**\n   ```html\n   <div class=\"header\">\n     <h1>Prow Job Resource Lifecycle Analysis</h1>\n     <div class=\"metadata\">\n       <p><strong>Prow Job:</strong> {prowjob-name}</p>\n       <p><strong>Build ID:</strong> {build_id}</p>\n       <p><strong>gcsweb URL:</strong> <a href=\"{original-url}\">{original-url}</a></p>\n       <p><strong>Target:</strong> {target}</p>\n       <p><strong>Resources:</strong> {resource-list}</p>\n       <p><strong>Total Entries:</strong> {count}</p>\n       <p><strong>Time Range:</strong> {min_time} to {max_time}</p>\n     </div>\n   </div>\n   ```\n\n   **Interactive Timeline:**\n   ```html\n   <div class=\"timeline-container\">\n     <svg id=\"timeline\" width=\"100%\" height=\"100\">\n       <!-- For each entry, render colored vertical line -->\n       <line x1=\"{position}%\" y1=\"0\" x2=\"{position}%\" y2=\"100\"\n             stroke=\"{color}\" stroke-width=\"2\"\n             class=\"timeline-event\" data-entry-id=\"{entry-id}\"\n             title=\"{summary}\">\n       </line>\n     </svg>\n   </div>\n   ```\n   - Position: Calculate percentage based on timestamp between min_time and max_time\n   - Color: white/lightgray (info), yellow (warn), red (error)\n   - Clickable: Jump to corresponding entry\n   - Tooltip on hover: Show summary\n\n   **Log Entries Section:**\n   ```html\n   <div class=\"entries\">\n     <div class=\"filters\">\n       <!-- Filter controls: by level, by resource, by time range -->\n     </div>\n\n     <div class=\"entry\" id=\"entry-{index}\">\n       <div class=\"entry-header\">\n         <span class=\"timestamp\">{formatted-timestamp}</span>\n         <span class=\"level badge-{level}\">{level}</span>\n         <span class=\"source\">{filename}:{line-number}</span>\n       </div>\n       <div class=\"entry-summary\">{summary}</div>\n       <details class=\"entry-details\">\n         <summary>Show full content</summary>\n         <pre><code>{content}</code></pre>\n       </details>\n     </div>\n   </div>\n   ```\n\n   **CSS Styling:**\n   - Modern, clean design with good contrast\n   - Responsive layout\n   - Badge colors: info=gray, warn=yellow, error=red\n   - Monospace font for log content\n   - Syntax highlighting for JSON (in audit logs)\n\n   **JavaScript Interactivity:**\n   ```javascript\n   // Timeline click handler\n   document.querySelectorAll('.timeline-event').forEach(el => {\n     el.addEventListener('click', () => {\n       const entryId = el.dataset.entryId;\n       document.getElementById(entryId).scrollIntoView({behavior: 'smooth'});\n     });\n   });\n\n   // Filter controls\n   // Expand/collapse details\n   // Search within entries\n   ```\n\n5. **Write HTML to file**\n   - Script automatically writes to `.work/prow-job-analyze-resource/{build_id}/{resource_name}.html`\n   - Includes proper HTML5 structure\n   - All CSS and JavaScript are inline for portability\n\n### Step 8: Present Results to User\n\n1. **Display summary**\n   ```\n   Resource Lifecycle Analysis Complete\n\n   Prow Job: {prowjob-name}\n   Build ID: {build_id}\n   Target: {target}\n\n   Resources Analyzed:\n   - {resource-spec-1}\n   - {resource-spec-2}\n   ...\n\n   Artifacts downloaded to: .work/prow-job-analyze-resource/{build_id}/logs/\n\n   Results:\n   - Audit log entries: {audit-count}\n   - Pod log entries: {pod-count}\n   - Total entries: {total-count}\n   - Time range: {min_time} to {max_time}\n\n   Report generated: .work/prow-job-analyze-resource/{build_id}/{resource_name}.html\n\n   Open in browser to view interactive timeline and detailed entries.\n   ```\n\n2. **Open report in browser**\n   - Detect platform and automatically open the HTML report in the default browser\n   - Linux: `xdg-open .work/prow-job-analyze-resource/{build_id}/{resource_name}.html`\n   - macOS: `open .work/prow-job-analyze-resource/{build_id}/{resource_name}.html`\n   - Windows: `start .work/prow-job-analyze-resource/{build_id}/{resource_name}.html`\n   - On Linux (most common for this environment), use `xdg-open`\n\n3. **Offer next steps**\n   - Ask if user wants to search for additional resources in the same job\n   - Ask if user wants to analyze a different Prow job\n   - Explain that artifacts are cached in `.work/prow-job-analyze-resource/{build_id}/` for faster subsequent searches\n\n## Error Handling\n\nHandle these error scenarios gracefully:\n\n1. **Invalid URL format**\n   - Error: \"URL must contain 'test-platform-results/' substring\"\n   - Provide example of valid URL\n\n2. **Build ID not found**\n   - Error: \"Could not find build ID (10+ decimal digits) in URL path\"\n   - Explain requirement and show URL parsing\n\n3. **gcloud not installed**\n   - Detect with: `which gcloud`\n   - Provide installation instructions for user's platform\n   - Link: https://cloud.google.com/sdk/docs/install\n\n4. **gcloud not authenticated**\n   - Detect with: `gcloud auth list`\n   - Instruct: \"Please run: gcloud auth login\"\n\n5. **No access to bucket**\n   - Error from gcloud storage commands\n   - Explain: \"You need read access to the test-platform-results GCS bucket\"\n   - Suggest checking project access\n\n6. **prowjob.json not found**\n   - Suggest verifying URL and checking if job completed\n   - Provide gcsweb URL for manual verification\n\n7. **Not a ci-operator job**\n   - Error: \"This is not a ci-operator job. No --target found in prowjob.json.\"\n   - Explain: Only ci-operator jobs can be analyzed by this skill\n\n8. **gather-extra not found**\n   - Warn: \"gather-extra directory not found for target {target}\"\n   - Suggest: Job may not have completed or target name is incorrect\n\n9. **No matches found**\n   - Display: \"No log entries found matching the specified resources\"\n   - Suggest:\n     - Check resource names for typos\n     - Try searching without kind or namespace filters\n     - Verify resources existed during this job execution\n\n10. **Timestamp parsing failures**\n    - Warn about unparseable timestamps\n    - Fall back to line order for sorting\n    - Still include entries in report\n\n## Performance Considerations\n\n1. **Avoid re-downloading**\n   - Check if `.work/prow-job-analyze-resource/{build_id}/logs/` already has content\n   - Ask user before re-downloading\n\n2. **Efficient downloads**\n   - Use `gcloud storage cp -r` for recursive downloads\n   - Use `--no-user-output-enabled` to suppress verbose output\n   - Create target directories with `mkdir -p` before downloading to avoid gcloud errors\n\n3. **Memory efficiency**\n   - The `parse_all_logs.py` script processes log files incrementally (line by line)\n   - Don't load entire files into memory\n   - Script outputs to JSON for efficient HTML generation\n\n4. **Content length limits**\n   - The HTML generator trims JSON content to ~2000 chars in display\n   - Full content is available in expandable details sections\n\n5. **Progress indicators**\n   - Show \"Downloading audit logs...\" before gcloud commands\n   - Show \"Parsing audit logs...\" before running parse script\n   - Show \"Generating HTML report...\" before running report generator\n\n## Examples\n\n### Example 1: Search for a namespace/project\n```\nUser: \"Analyze e2e-test-project-api-p28m in this Prow job: https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/logs/periodic-ci-openshift-release-master-okd-scos-4.20-e2e-aws-ovn-techpreview/1964725888612306944\"\n\nOutput:\n- Downloads artifacts to: .work/prow-job-analyze-resource/1964725888612306944/logs/\n- Finds actual resource name: e2e-test-project-api-p28mx (namespace)\n- Parses 382 audit log entries\n- Finds 86 pod log mentions\n- Creates: .work/prow-job-analyze-resource/1964725888612306944/e2e-test-project-api-p28mx.html\n- Shows timeline from creation (18:11:02) to deletion (18:17:32)\n```\n\n### Example 2: Search for a pod\n```\nUser: \"Analyze pod/etcd-0 in this Prow job: https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/30393/pull-ci-openshift-origin-main-okd-scos-e2e-aws-ovn/1978913325970362368/\"\n\nOutput:\n- Creates: .work/prow-job-analyze-resource/1978913325970362368/etcd-0.html\n- Shows timeline of all pod/etcd-0 events across namespaces\n```\n\n### Example 3: Search by name only\n```\nUser: \"Find all resources named cluster-version-operator in job {url}\"\n\nOutput:\n- Searches without kind filter\n- Finds deployments, pods, services, etc. all named cluster-version-operator\n- Creates: .work/prow-job-analyze-resource/{build_id}/cluster-version-operator.html\n```\n\n### Example 4: Search for multiple resources using regex\n```\nUser: \"Analyze e2e-test-project-api-pkjxf and e2e-test-project-api-7zdxx in job {url}\"\n\nOutput:\n- Uses regex pattern: `e2e-test-project-api-pkjxf|e2e-test-project-api-7zdxx`\n- Finds all events for both namespaces in a single pass\n- Parses 1,047 total entries (501 for first namespace, 546 for second)\n- Passes the same pattern to generate_html_report.py\n- HTML displays: \"Resources: e2e-test-project-api-7zdxx, e2e-test-project-api-pkjxf\"\n- Creates: .work/prow-job-analyze-resource/{build_id}/e2e-test-project-api-pkjxf.html\n- Timeline shows interleaved events from both namespaces chronologically\n```\n\n## Tips\n\n- Always verify gcloud prerequisites before starting (gcloud CLI must be installed)\n- Authentication is NOT required - the bucket is publicly accessible\n- Use `.work/prow-job-analyze-resource/{build_id}/` directory structure for organization\n- All work files are in `.work/` which is already in .gitignore\n- The Python scripts handle all parsing and HTML generation - use them!\n- Cache artifacts in `.work/prow-job-analyze-resource/{build_id}/` to speed up subsequent searches\n- The parse script supports **regex patterns** for flexible matching:\n  - Use `resource1|resource2` to search for multiple resources in a single pass\n  - Use `.*` wildcards to match resource name patterns\n  - Simple substring matching still works for basic searches\n- The resource name provided by the user may not exactly match the actual resource name in logs\n  - Example: User asks for `e2e-test-project-api-p28m` but actual resource is `e2e-test-project-api-p28mx`\n  - Use regex patterns like `e2e-test-project-api-p28m.*` to find partial matches\n- For namespaces/projects, search for the resource name - it will match both `namespace` and `project` resources\n- Provide helpful error messages with actionable solutions\n\n## Important Notes\n\n1. **Resource Name Matching:**\n   - The parse script uses **regex pattern matching** for maximum flexibility\n   - Supports pipe operator (`|`) to search for multiple resources: `resource1|resource2`\n   - Supports wildcards (`.*`) for pattern matching: `e2e-test-.*`\n   - Simple substrings still work for basic searches\n   - May match multiple related resources (e.g., namespace, project, rolebindings in that namespace)\n   - Report all matches - this provides complete lifecycle context\n\n2. **Namespace vs Project:**\n   - In OpenShift, a `project` is essentially a `namespace` with additional metadata\n   - Searching for a namespace will find both namespace and project resources\n   - The audit logs contain events for both resource types\n\n3. **Target Extraction:**\n   - Must extract the `--target` argument from prowjob.json\n   - This is critical for finding the correct gather-extra path\n   - Non-ci-operator jobs cannot be analyzed (they don't have --target)\n\n4. **Working with Scripts:**\n   - All scripts are in `plugins/prow-job/skills/prow-job-analyze-resource/`\n   - `parse_all_logs.py` - Parses audit logs and pod logs, outputs JSON\n     - Detects glog severity levels (E=error, W=warn, I=info, F=fatal)\n     - Supports regex patterns for resource matching\n   - `generate_html_report.py` - Generates interactive HTML report from JSON\n   - Scripts output status messages to stderr for progress display. JSON output to stdout is clean.\n\n5. **Pod Log Glog Format Support:**\n   - The parser automatically detects and parses glog format logs\n   - Glog format: `E0910 11:43:41.153414 ...`\n     - `E` = severity (E/F  error, W  warn, I  info)\n     - `0910` = month/day (MMDD)\n     - `11:43:41.153414` = time with microseconds\n   - Timestamp parsing: Extracts timestamp and infers year (2025)\n   - Severity mapping allows filtering by level in HTML report\n   - Non-glog logs default to info level"
              },
              {
                "name": "Prow Job Analyze Test Failure",
                "description": "Analyze a failed test by inspecting the code in the current project and artifacts in Prow CI job. Provide a detailed analysis of the test failure in a pre-defined format.",
                "path": "plugins/prow-job/skills/prow-job-analyze-test-failure/SKILL.md",
                "frontmatter": {
                  "name": "Prow Job Analyze Test Failure",
                  "description": "Analyze a failed test by inspecting the code in the current project and artifacts in Prow CI job. Provide a detailed analysis of the test failure in a pre-defined format."
                },
                "content": "# Prow Job Analyze Test Failure\n\nThis skill analyzes the given test failure by downloading artifacts using the \"Prow Job Analyze Resource\" skill, checking test logs, inspecting resources, logs and events from the artifacts, and the test source code.\n\n## When to Use This Skill\n\nUse this skill when the user wants to do an initial analysis of a Prow CI test failure.\n\n## Prerequisites\n\nIdentical with \"Prow Job Analyze Resource\" skill.\n\n## Input Format\n\nThe user will provide:\n\n1. **Prow job URL** - gcsweb URL containing `test-platform-results/`\n\n   - Example: `https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/pr-logs/pull/openshift_hypershift/6731/pull-ci-openshift-hypershift-main-e2e-aws/1962527613477982208`\n   - URL may or may not have trailing slash\n\n2. **Test name** - test name that failed\n   - Examples:\n     - `TestKarpenter/EnsureHostedCluster/ValidateMetricsAreExposed`\n     - `TestCreateClusterCustomConfig`\n     - `The openshift-console downloads pods [apigroup:console.openshift.io] should be scheduled on different nodes`\n\n## Implementation Steps\n\n### Step 1: Parse and Validate URL\n\nUse the \"Parse and Validate URL\" steps from \"Prow Job Analyze Resource\" skill\n\n### Step 2: Create Working Directory\n\n1. **Check for existing artifacts first**\n\n   - Check if `.work/prow-job-analyze-test-failure/{build_id}/logs/` directory exists and has content\n   - If it exists with content:\n     - Use AskUserQuestion tool to ask:\n       - Question: \"Artifacts already exist for build {build_id}. Would you like to use the existing download or re-download?\"\n       - Options:\n         - \"Use existing\" - Skip to step Analyze Test Failure\n         - \"Re-download\" - Continue to clean and re-download\n     - If user chooses \"Re-download\":\n       - Remove all existing content: `rm -rf .work/prow-job-analyze-test-failure/{build_id}/logs/`\n       - Also remove tmp directory: `rm -rf .work/prow-job-analyze-test-failure/{build_id}/tmp/`\n       - This ensures clean state before downloading new content\n     - If user chooses \"Use existing\":\n       - Skip directly to Step 4 (Analyze Test Failure)\n       - Still need to download prowjob.json if it doesn't exist\n\n2. **Create directory structure**\n   ```bash\n   mkdir -p .work/prow-job-analyze-test-failure/{build_id}/logs\n   mkdir -p .work/prow-job-analyze-test-failure/{build_id}/tmp\n   ```\n   - Use `.work/prow-job-analyze-test-failure/` as the base directory (already in .gitignore)\n   - Use build_id as subdirectory name\n   - Create `logs/` subdirectory for all downloads\n   - Create `tmp/` subdirectory for temporary files (intermediate JSON, etc.)\n   - Working directory: `.work/prow-job-analyze-test-failure/{build_id}/`\n\n### Step 3: Download and Validate prowjob.json\n\nUse the \"Download and Validate prowjob.json\" steps from \"Prow Job Analyze Resource\" skill.\n\n### Step 4: Analyze Test Failure\n\n1. **Download build-log.txt**\n\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/build-log.txt .work/prow-job-analyze-test-failure/{build_id}/logs/build-log.txt --no-user-output-enabled\n   ```\n\n2. **Parse and validate**\n\n   - Read `.work/prow-job-analyze-resource/{build_id}/logs/build-log.txt`\n   - Search for the Test name\n   - Gather stack trace related to the test\n\n3. **Examine intervals files for cluster activity during E2E failures**\n\n   - Search recursively for E2E timeline artifacts (known as \"interval files\") within the bucket-path:\n     ```bash\n     gcloud storage ls 'gs://test-platform-results/{bucket-path}/**/e2e-timelines_spyglass_*json'\n     ```\n   - The files can be nested at unpredictable levels below the bucket-path\n   - There could be as many as two matching files\n   - Download all matching interval files (use the full paths from the search results):\n     ```bash\n     gcloud storage cp gs://test-platform-results/{bucket-path}/**/e2e-timelines_spyglass_*.json .work/prow-job-analyze-test-failure/{build_id}/logs/ --no-user-output-enabled\n     ```\n   - If the wildcard copy doesn't work, copy each file individually using the full paths from the search results\n   - **Scan interval files for test failure timing:**\n     - Look for intervals where `source = \"E2ETest\"` and `message.annotations.status = \"Failed\"`\n     - Note the `from` and `to` timestamps on this interval - this indicates when the test was running\n   - **Scan interval files for related cluster events:**\n     - Look for intervals that overlap the timeframe when the failed test was running\n     - Filter for intervals with:\n       - `level = \"Error\"` or `level = \"Warning\"`\n       - `source = \"OperatorState\"`\n     - These events may indicate cluster issues that caused or contributed to the test failure\n\n4. **Determine root cause**\n   - Determine a possible root cause for the test failure\n   - Analyze stack traces\n   - Analyze related code in the code repository\n   - Store artifacts from Prow CI job (json/yaml files) related to the failure under `.work/prow-job-analyze-resource/{build_id}/tmp`\n   - Store logs under `.work/prow-job-analyze-resource/{build_id}/logs/`\n   - Provide evidence for the failure\n   - Try to find additional evidence. For example, in logs and events and other json/yaml files\n\n### Step 5: Present Results to User\n\n1. **Display summary**\n\n   ```text\n   Test Failure Analysis Complete\n\n   Prow Job: {prowjob-name}\n   Build ID: {build_id}\n   Error: {error message}\n\n   Summary: {failure analysis}\n   Evidence: {evidence}\n   Additional evidence: {additional evidence}\n\n   Artifacts downloaded to: .work/prow-job-analyze-test-failure/{build_id}/logs/\n   ```\n\n## Error Handling\n\nHandle errors in the same way as \"Error handling\" in \"Prow Job Analyze Resource\" skill\n\n## Performance Considerations\n\nFollow the instructions in \"Performance Considerations\" in \"Prow Job Analyze Resource\" skill"
              },
              {
                "name": "Prow Job Extract Must-Gather",
                "description": "Extract and decompress must-gather archives from Prow CI job artifacts, generating an interactive HTML file browser with filters",
                "path": "plugins/prow-job/skills/prow-job-extract-must-gather/SKILL.md",
                "frontmatter": {
                  "name": "Prow Job Extract Must-Gather",
                  "description": "Extract and decompress must-gather archives from Prow CI job artifacts, generating an interactive HTML file browser with filters"
                },
                "content": "# Prow Job Extract Must-Gather\n\nThis skill extracts and decompresses must-gather archives from Prow CI job artifacts, automatically handling nested tar and gzip archives, and generating an interactive HTML file browser.\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n- Extract must-gather archives from Prow CI job artifacts\n- Avoid manually downloading and extracting nested archives\n- Browse must-gather contents with an interactive HTML interface\n- Search for specific files or file types in must-gather data\n- Analyze OpenShift cluster state from CI test runs\n\n## Prerequisites\n\nBefore starting, verify these prerequisites:\n\n1. **gcloud CLI Installation**\n   - Check if installed: `which gcloud`\n   - If not installed, provide instructions for the user's platform\n   - Installation guide: https://cloud.google.com/sdk/docs/install\n\n2. **gcloud Authentication (Optional)**\n   - The `test-platform-results` bucket is publicly accessible\n   - No authentication is required for read access\n   - Skip authentication checks\n\n## Input Format\n\nThe user will provide:\n1. **Prow job URL** - gcsweb URL containing `test-platform-results/`\n   - Example: `https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/logs/periodic-ci-openshift-release-master-ci-4.20-e2e-aws-ovn-techpreview/1965715986610917376/`\n   - URL may or may not have trailing slash\n\n## Implementation Steps\n\n### Step 1: Parse and Validate URL\n\n1. **Extract bucket path**\n   - Find `test-platform-results/` in URL\n   - Extract everything after it as the GCS bucket relative path\n   - If not found, error: \"URL must contain 'test-platform-results/'\"\n\n2. **Extract build_id**\n   - Search for pattern `/(\\\\d{10,})/` in the bucket path\n   - build_id must be at least 10 consecutive decimal digits\n   - Handle URLs with or without trailing slash\n   - If not found, error: \"Could not find build ID (10+ digits) in URL\"\n\n3. **Extract prowjob name**\n   - Find the path segment immediately preceding build_id\n   - Example: In `.../periodic-ci-openshift-release-master-ci-4.20-e2e-aws-ovn-techpreview/1965715986610917376/`\n   - Prowjob name: `periodic-ci-openshift-release-master-ci-4.20-e2e-aws-ovn-techpreview`\n\n4. **Construct GCS paths**\n   - Bucket: `test-platform-results`\n   - Base GCS path: `gs://test-platform-results/{bucket-path}/`\n   - Ensure path ends with `/`\n\n### Step 2: Create Working Directory\n\n1. **Check for existing extraction first**\n   - Check if `.work/prow-job-extract-must-gather/{build_id}/logs/` directory exists and has content\n   - If it exists with content:\n     - Use AskUserQuestion tool to ask:\n       - Question: \"Must-gather already extracted for build {build_id}. Would you like to use the existing extraction or re-extract?\"\n       - Options:\n         - \"Use existing\" - Skip to HTML report generation (Step 6)\n         - \"Re-extract\" - Continue to clean and re-download\n     - If user chooses \"Re-extract\":\n       - Remove all existing content: `rm -rf .work/prow-job-extract-must-gather/{build_id}/logs/`\n       - Also remove tmp directory: `rm -rf .work/prow-job-extract-must-gather/{build_id}/tmp/`\n       - This ensures clean state before downloading new content\n     - If user chooses \"Use existing\":\n       - Skip directly to Step 6 (Generate HTML Report)\n\n2. **Create directory structure**\n   ```bash\n   mkdir -p .work/prow-job-extract-must-gather/{build_id}/logs\n   mkdir -p .work/prow-job-extract-must-gather/{build_id}/tmp\n   ```\n   - Use `.work/prow-job-extract-must-gather/` as the base directory (already in .gitignore)\n   - Use build_id as subdirectory name\n   - Create `logs/` subdirectory for extraction\n   - Create `tmp/` subdirectory for temporary files\n   - Working directory: `.work/prow-job-extract-must-gather/{build_id}/`\n\n### Step 3: Download and Validate prowjob.json\n\n1. **Download prowjob.json**\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/prowjob.json .work/prow-job-extract-must-gather/{build_id}/tmp/prowjob.json --no-user-output-enabled\n   ```\n\n2. **Parse and validate**\n   - Read `.work/prow-job-extract-must-gather/{build_id}/tmp/prowjob.json`\n   - Search for pattern: `--target=([a-zA-Z0-9-]+)`\n   - If not found:\n     - Display: \"This is not a ci-operator job. The prowjob cannot be analyzed by this skill.\"\n     - Explain: ci-operator jobs have a --target argument specifying the test target\n     - Exit skill\n\n3. **Extract target name**\n   - Capture the target value (e.g., `e2e-aws-ovn-techpreview`)\n   - Store for constructing must-gather path\n\n### Step 4: Download Must-Gather Archive\n\n1. **Construct must-gather path**\n   - GCS path: `gs://test-platform-results/{bucket-path}/artifacts/{target}/gather-must-gather/artifacts/must-gather.tar`\n   - Local path: `.work/prow-job-extract-must-gather/{build_id}/tmp/must-gather.tar`\n\n2. **Download must-gather.tar**\n   ```bash\n   gcloud storage cp gs://test-platform-results/{bucket-path}/artifacts/{target}/gather-must-gather/artifacts/must-gather.tar .work/prow-job-extract-must-gather/{build_id}/tmp/must-gather.tar --no-user-output-enabled\n   ```\n   - Use `--no-user-output-enabled` to suppress progress output\n   - If file not found, error: \"No must-gather archive found. Job may not have completed or gather-must-gather may not have run.\"\n\n### Step 5: Extract and Process Archives\n\n**IMPORTANT: Use the provided Python script `extract_archives.py` from the skill directory.**\n\n**Usage:**\n```bash\npython3 plugins/prow-job/skills/prow-job-extract-must-gather/extract_archives.py \\\n  .work/prow-job-extract-must-gather/{build_id}/tmp/must-gather.tar \\\n  .work/prow-job-extract-must-gather/{build_id}/logs\n```\n\n**What the script does:**\n\n1. **Extract must-gather.tar**\n   - Extract to `{build_id}/logs/` directory\n   - Uses Python's tarfile module for reliable extraction\n\n2. **Rename long subdirectory to \"content/\"**\n   - Find subdirectory containing \"-ci-\" in the name\n   - Example: `registry-build09-ci-openshift-org-ci-op-m8t77165-stable-sha256-d1ae126eed86a47fdbc8db0ad176bf078a5edebdbb0df180d73f02e5f03779e0/`\n   - Rename to: `content/`\n   - Preserves all files and subdirectories\n\n3. **Recursively process nested archives**\n   - Walk entire directory tree\n   - Find and process archives:\n\n   **For .tar.gz and .tgz files:**\n   ```python\n   # Extract in place\n   with tarfile.open(archive_path, 'r:gz') as tar:\n       tar.extractall(path=parent_dir)\n   # Remove original archive\n   os.remove(archive_path)\n   ```\n\n   **For .gz files (no tar):**\n   ```python\n   # Gunzip in place\n   with gzip.open(gz_path, 'rb') as f_in:\n       with open(output_path, 'wb') as f_out:\n           shutil.copyfileobj(f_in, f_out)\n   # Remove original archive\n   os.remove(gz_path)\n   ```\n\n4. **Progress reporting**\n   - Print status for each extracted archive\n   - Count total files and archives processed\n   - Report final statistics\n\n5. **Error handling**\n   - Skip corrupted archives with warning\n   - Continue processing other files\n   - Report all errors at the end\n\n### Step 6: Generate HTML File Browser\n\n**IMPORTANT: Use the provided Python script `generate_html_report.py` from the skill directory.**\n\n**Usage:**\n```bash\npython3 plugins/prow-job/skills/prow-job-extract-must-gather/generate_html_report.py \\\n  .work/prow-job-extract-must-gather/{build_id}/logs \\\n  \"{prowjob_name}\" \\\n  \"{build_id}\" \\\n  \"{target}\" \\\n  \"{gcsweb_url}\"\n```\n\n**Output:** The script generates `.work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html`\n\n**What the script does:**\n\n1. **Scan directory tree**\n   - Recursively walk `{build_id}/logs/` directory\n   - Collect all files with metadata:\n     - Relative path from logs/\n     - File size (human-readable: KB, MB, GB)\n     - File extension\n     - Directory depth\n     - Last modified time\n\n2. **Classify files**\n   - Detect file types based on extension:\n     - Logs: `.log`, `.txt`\n     - YAML: `.yaml`, `.yml`\n     - JSON: `.json`\n     - XML: `.xml`\n     - Certificates: `.crt`, `.pem`, `.key`\n     - Binaries: `.tar`, `.gz`, `.tgz`, `.tar.gz`\n     - Other\n   - Count files by type for statistics\n\n3. **Generate HTML structure**\n\n   **Header Section:**\n   ```html\n   <div class=\"header\">\n     <h1>Must-Gather File Browser</h1>\n     <div class=\"metadata\">\n       <p><strong>Prow Job:</strong> {prowjob-name}</p>\n       <p><strong>Build ID:</strong> {build_id}</p>\n       <p><strong>gcsweb URL:</strong> <a href=\"{original-url}\">{original-url}</a></p>\n       <p><strong>Target:</strong> {target}</p>\n       <p><strong>Total Files:</strong> {count}</p>\n       <p><strong>Total Size:</strong> {human-readable-size}</p>\n     </div>\n   </div>\n   ```\n\n   **Filter Controls:**\n   ```html\n   <div class=\"filters\">\n     <div class=\"filter-group\">\n       <label class=\"filter-label\">File Type (multi-select)</label>\n       <div class=\"filter-buttons\">\n         <button class=\"filter-btn\" data-filter=\"type\" data-value=\"log\">Logs ({count})</button>\n         <button class=\"filter-btn\" data-filter=\"type\" data-value=\"yaml\">YAML ({count})</button>\n         <button class=\"filter-btn\" data-filter=\"type\" data-value=\"json\">JSON ({count})</button>\n         <!-- etc -->\n       </div>\n     </div>\n     <div class=\"filter-group\">\n       <label class=\"filter-label\">Filter by Regex Pattern</label>\n       <input type=\"text\" class=\"search-box\" id=\"pattern\" placeholder=\"Enter regex pattern (e.g., .*etcd.*, .*\\\\.log$)\">\n     </div>\n     <div class=\"filter-group\">\n       <label class=\"filter-label\">Search by Name</label>\n       <input type=\"text\" class=\"search-box\" id=\"search\" placeholder=\"Search file names...\">\n     </div>\n   </div>\n   ```\n\n   **File List:**\n   ```html\n   <div class=\"file-list\">\n     <div class=\"file-item\" data-type=\"{type}\" data-path=\"{path}\">\n       <div class=\"file-icon\">{icon}</div>\n       <div class=\"file-info\">\n         <div class=\"file-name\">\n           <a href=\"{relative-path}\" target=\"_blank\">{filename}</a>\n         </div>\n         <div class=\"file-meta\">\n           <span class=\"file-path\">{directory-path}</span>\n           <span class=\"file-size\">{size}</span>\n           <span class=\"file-type badge badge-{type}\">{type}</span>\n         </div>\n       </div>\n     </div>\n   </div>\n   ```\n\n   **CSS Styling:**\n   - Use same dark theme as analyze-resource skill\n   - Modern, clean design with good contrast\n   - Responsive layout\n   - File type color coding\n   - Monospace fonts for paths\n   - Hover effects on file items\n\n   **JavaScript Interactivity:**\n   ```javascript\n   // Multi-select file type filters\n   document.querySelectorAll('.filter-btn').forEach(btn => {\n     btn.addEventListener('click', function() {\n       // Toggle active state\n       // Apply filters\n     });\n   });\n\n   // Regex pattern filter\n   document.getElementById('pattern').addEventListener('input', function() {\n     const pattern = this.value;\n     if (pattern) {\n       const regex = new RegExp(pattern);\n       // Filter files matching regex\n     }\n   });\n\n   // Name search filter\n   document.getElementById('search').addEventListener('input', function() {\n     const query = this.value.toLowerCase();\n     // Filter files by name substring\n   });\n\n   // Combine all active filters\n   function applyFilters() {\n     // Show/hide files based on all active filters\n   }\n   ```\n\n4. **Statistics Section:**\n   ```html\n   <div class=\"stats\">\n     <div class=\"stat\">\n       <div class=\"stat-value\">{total-files}</div>\n       <div class=\"stat-label\">Total Files</div>\n     </div>\n     <div class=\"stat\">\n       <div class=\"stat-value\">{total-size}</div>\n       <div class=\"stat-label\">Total Size</div>\n     </div>\n     <div class=\"stat\">\n       <div class=\"stat-value\">{log-count}</div>\n       <div class=\"stat-label\">Log Files</div>\n     </div>\n     <div class=\"stat\">\n       <div class=\"stat-value\">{yaml-count}</div>\n       <div class=\"stat-label\">YAML Files</div>\n     </div>\n     <!-- etc -->\n   </div>\n   ```\n\n5. **Write HTML to file**\n   - Script automatically writes to `.work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html`\n   - Includes proper HTML5 structure\n   - All CSS and JavaScript are inline for portability\n\n### Step 7: Present Results to User\n\n1. **Display summary**\n   ```\n   Must-Gather Extraction Complete\n\n   Prow Job: {prowjob-name}\n   Build ID: {build_id}\n   Target: {target}\n\n   Extraction Statistics:\n   - Total files: {file-count}\n   - Total size: {human-readable-size}\n   - Archives extracted: {archive-count}\n   - Log files: {log-count}\n   - YAML files: {yaml-count}\n   - JSON files: {json-count}\n\n   Extracted to: .work/prow-job-extract-must-gather/{build_id}/logs/\n\n   File browser generated: .work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html\n\n   Open in browser to browse and search extracted files.\n   ```\n\n2. **Open report in browser**\n   - Detect platform and automatically open the HTML report in the default browser\n   - Linux: `xdg-open .work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html`\n   - macOS: `open .work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html`\n   - Windows: `start .work/prow-job-extract-must-gather/{build_id}/must-gather-browser.html`\n   - On Linux (most common for this environment), use `xdg-open`\n\n3. **Offer next steps**\n   - Ask if user wants to search for specific files\n   - Explain that extracted files are available in `.work/prow-job-extract-must-gather/{build_id}/logs/`\n   - Mention that extraction is cached for faster subsequent browsing\n\n## Error Handling\n\nHandle these error scenarios gracefully:\n\n1. **Invalid URL format**\n   - Error: \"URL must contain 'test-platform-results/' substring\"\n   - Provide example of valid URL\n\n2. **Build ID not found**\n   - Error: \"Could not find build ID (10+ decimal digits) in URL path\"\n   - Explain requirement and show URL parsing\n\n3. **gcloud not installed**\n   - Detect with: `which gcloud`\n   - Provide installation instructions for user's platform\n   - Link: https://cloud.google.com/sdk/docs/install\n\n4. **prowjob.json not found**\n   - Suggest verifying URL and checking if job completed\n   - Provide gcsweb URL for manual verification\n\n5. **Not a ci-operator job**\n   - Error: \"This is not a ci-operator job. No --target found in prowjob.json.\"\n   - Explain: Only ci-operator jobs can be analyzed by this skill\n\n6. **must-gather.tar not found**\n   - Warn: \"Must-gather archive not found at expected path\"\n   - Suggest: Job may not have completed or gather-must-gather may not have run\n   - Provide full GCS path that was checked\n\n7. **Corrupted archive**\n   - Warn: \"Could not extract {archive-path}: {error}\"\n   - Continue processing other archives\n   - Report all errors in final summary\n\n8. **No \"-ci-\" subdirectory found**\n   - Warn: \"Could not find expected subdirectory to rename to 'content/'\"\n   - Continue with extraction anyway\n   - Files will be in original directory structure\n\n## Performance Considerations\n\n1. **Avoid re-extracting**\n   - Check if `.work/prow-job-extract-must-gather/{build_id}/logs/` already has content\n   - Ask user before re-extracting\n\n2. **Efficient downloads**\n   - Use `gcloud storage cp` with `--no-user-output-enabled` to suppress verbose output\n\n3. **Memory efficiency**\n   - Process archives incrementally\n   - Don't load entire files into memory\n   - Use streaming extraction\n\n4. **Progress indicators**\n   - Show \"Downloading must-gather archive...\" before gcloud command\n   - Show \"Extracting must-gather.tar...\" before extraction\n   - Show \"Processing nested archives...\" during recursive extraction\n   - Show \"Generating HTML file browser...\" before report generation\n\n## Examples\n\n### Example 1: Extract must-gather from periodic job\n```\nUser: \"Extract must-gather from this Prow job: https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/test-platform-results/logs/periodic-ci-openshift-release-master-ci-4.20-e2e-aws-ovn-techpreview/1965715986610917376\"\n\nOutput:\n- Downloads must-gather.tar to: .work/prow-job-extract-must-gather/1965715986610917376/tmp/\n- Extracts to: .work/prow-job-extract-must-gather/1965715986610917376/logs/\n- Renames long subdirectory to: content/\n- Processes 247 nested archives (.tar.gz, .tgz, .gz)\n- Creates: .work/prow-job-extract-must-gather/1965715986610917376/must-gather-browser.html\n- Opens browser with interactive file list (3,421 files, 234 MB)\n```\n\n## Tips\n\n- Always verify gcloud prerequisites before starting (gcloud CLI must be installed)\n- Authentication is NOT required - the bucket is publicly accessible\n- Use `.work/prow-job-extract-must-gather/{build_id}/` directory structure for organization\n- All work files are in `.work/` which is already in .gitignore\n- The Python scripts handle all extraction and HTML generation - use them!\n- Cache extracted files in `.work/prow-job-extract-must-gather/{build_id}/` to avoid re-extraction\n- The HTML file browser supports regex patterns for powerful file filtering\n- Extracted files can be opened directly from the HTML browser (links are relative)\n\n## Important Notes\n\n1. **Archive Processing:**\n   - The script automatically handles nested archives\n   - Original compressed files are removed after successful extraction\n   - Corrupted archives are skipped with warnings\n\n2. **Directory Renaming:**\n   - The long subdirectory name (containing \"-ci-\") is renamed to \"content/\" for brevity\n   - Files within \"content/\" are NOT altered\n   - This makes paths more readable in the HTML browser\n\n3. **File Type Detection:**\n   - File types are detected based on extension\n   - Common types are color-coded in the HTML browser\n   - All file types can be filtered\n\n4. **Regex Pattern Filtering:**\n   - Users can enter regex patterns in the filter input\n   - Patterns match against full file paths\n   - Invalid regex patterns are ignored gracefully\n\n5. **Working with Scripts:**\n   - All scripts are in `plugins/prow-job/skills/prow-job-extract-must-gather/`\n   - `extract_archives.py` - Extracts and processes archives\n   - `generate_html_report.py` - Generates interactive HTML file browser"
              }
            ]
          },
          {
            "name": "agendas",
            "description": "A plugin to create various meeting agendas",
            "source": "./plugins/agendas",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install agendas@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/outcome-refinement",
                "description": "Analyze the list of JIRA outcome issues to prepare an outcome refinement meeting agenda.",
                "path": "plugins/agendas/commands/outcome-refinement.md",
                "frontmatter": {
                  "description": "Analyze the list of JIRA outcome issues to prepare an outcome refinement meeting agenda."
                },
                "content": "## Name\nagendas:outcome-refinement\n\n## Synopsis\n```\n/agendas:outcome-refinement\n```\n\n## Description\nThe `agendas:outcome-refinement` command helps analyze the outcome issues and should be used to assist in preparting for outcome refinement collaboration sessions. It automatically checks for common issues that we observe that indicate follow-up actions are needed by humans. This command generates structured outcome refinement meeting agenda.\n\n## Examples\n```bash\n/agendas:outcome-refinement\n```\n\n## Implementation\n\nThe `agendas:outcome-refinement` command runs in three main phases:\n\n###  Phase 1: Data Collection\n- Queries JIRA for outcome issues in the OCPSTRAT project that require work to be done, meaning the issue status is not closed or release pending.\n\n###  Phase 2: Analysis & Processing\n- Flags routine hygiene issues to resolve with follow up actions.\n- People assignments (Assignee, Architect, QA Contact, Doc Contact) should all be filled in.\n- Outcome issues should only have Feature issue types as child issues.\n- Identified if child issue are actively being worked on but the outcome doesn't represent the right status.\n- Shows how long an outcome issue has been open and their corresponding priority.\n- Identifies incomplete or unclear issues that need clarification.\n- If and outcome issue has child issues that are actively being updated but the outcome has been open for a more than a year we should discuss the scope.\n- If an outcome has stayed in the new status for over a year, we should probably discuss whether it's a real outcome priority.\n- Looking at all the OCPSTRAT outcomes, if any specific component is commonly assigned to the child feature issues, this indicates an team overload, so we should discuss this.\n- Highlights risks, dependencies, and recommended next actions.\n\n###  Phase 3: Report Generation\n- Automatically generates a **structured outcome refinement meeting agenda** in Markdown format.\n- Includes discussion points, decision checklists, and action items.\n- Output can be copied directly into Confluence or shared with the team.\n\n## Output Format\n\n### Outcome Refinement Meeting Agenda\n\nThe command outputs a ready-to-use Markdown document that can be copied into Confluence or shared with your team.\n\n```markdown\n# Outcome Refinement Agenda\n**Outcome Issues**: [count]\n\n##  Critical Issues ([count])\n- **[OCPSTRAT-1234]** BGP integration with public clouds - *Critical, needs immediate attention*\n- **[OCPSTRAT-1235]** Consistent Ingress/Egress into OpenShift clusters across providers - *High, assign to team lead*\n\n##  Needs Clarification ([count])\n- **[OCPSTRAT-1238]** Missing architect\n- **[OCPSTRAT-1239]** Component team is overloaded\n- **[OCPSTRAT-1240]** Outcome has been open for years with no delivery\n\n##  Action Items\n- [ ] Set architect for OCPSTRAT-1236 to SME architect (immediate)\n- [ ] Schedule review for OCPSTRAT-1236 (this week)\n```\n\n## Return Value\n- **Markdown Report**: Ready-to-use outcome refinement agenda with categorized issues and action items"
              }
            ],
            "skills": []
          },
          {
            "name": "openshift",
            "description": "OpenShift development utilities and helpers",
            "source": "./plugins/openshift",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install openshift@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/add-enhancement",
                "description": "Create a new OpenShift Enhancement Proposal",
                "path": "plugins/openshift/commands/add-enhancement.md",
                "frontmatter": {
                  "description": "Create a new OpenShift Enhancement Proposal",
                  "argument-hint": "[area] <name> <description> <jira>"
                },
                "content": "## Name\nopenshift:add-enhancement\n\n## Synopsis\n```\n/openshift:add-enhancement [area] <name> <description> <jira>\n```\n\n## Description\n\nThe `openshift:add-enhancement` command creates a new OpenShift Enhancement\nProposal (EP) based on the official template from the openshift/enhancements\nrepository. It generates a comprehensive enhancement document with all required\nsections, metadata, and guidance following OpenShift's enhancement process.\n\nThis command automates the creation of enhancement proposals by:\n- Fetching the latest enhancement template from the openshift/enhancements\n  repository\n- Analyzing the provided description to extract what, why, and who information\n- Generating user stories, goals, and non-goals based on the requirements\n- Creating properly formatted enhancement files with all required sections\n- Applying OpenShift-specific conventions for feature gates, API design, and\n  testing requirements\n\n## Arguments\n\n- **area** (optional): Enhancement area (subdirectory under enhancements/). If not provided, the command will attempt to infer the best area from the enhancement description and ask for confirmation.\n- **name**: One-line title describing the enhancement\n- **description**: Detailed description (what, why, who)\n- **jira**: JIRA ticket URL for tracking\n\n## Implementation\n\nAct as an experienced software architect to create a comprehensive enhancement proposal. Follow these steps:\n\n**Important**: Reference the guidance in the OpenShift enhancements repository at https://github.com/openshift/enhancements/blob/master/dev-guide/feature-zero-to-hero.md, particularly the section \"Writing an OpenShift Enhancement\", when creating enhancement proposals. This guide provides essential context on the OpenShift Enhancement Proposal process, feature gates, API design conventions, testing requirements, and promotion criteria.\n\n0. **Verify Repository**: Before proceeding, ensure you are operating in the correct repository:\n   - Check if the current directory is a git repository: `git rev-parse --is-inside-work-tree`\n   - Verify the repository is openshift/enhancements or a fork:\n     - Get the remote URL: `git config --get remote.origin.url`\n     - Check if the URL contains \"openshift/enhancements\" (e.g., `git@github.com:openshift/enhancements.git` or `https://github.com/openshift/enhancements.git`)\n     - For forks, the URL might be different (e.g., `git@github.com:username/enhancements.git`) - in this case, check if it's a fork by examining the repository structure (presence of `enhancements/` directory and `guidelines/` directory with enhancement_template.md)\n   - If verification fails, exit with a clear error message: \"Error: This command must be run in the openshift/enhancements repository or a fork. Please clone the repository first or navigate to the correct directory.\"\n   - If successful, continue to the next step\n\n1. **Determine Enhancement Area**: Identify or validate the area where the enhancement should be created:\n   - List available areas by examining subdirectories in `enhancements/`: `ls -d enhancements/*/` (extract just the directory names)\n   - **If the `<area>` argument WAS provided by the user:**\n     - Check if `enhancements/<area>/` exists\n     - If it exists: proceed with this area\n     - If it does NOT exist:\n       - Show the list of available areas\n       - Use AskUserQuestion to ask: \"The area '<area>' does not exist. Do you want to create a new area called '<area>'?\" with options:\n         - \"Yes, create new area '<area>'\" (then confirm they're sure about creating a new area)\n         - \"No, let me choose from existing areas\" (then show available areas and ask them to select)\n   - **If the `<area>` argument was NOT provided:**\n     - Analyze the enhancement name and description to infer the most appropriate area\n     - Look for keywords that match existing area names (e.g., \"authentication\", \"network\", \"storage\", \"installer\", \"api\", \"cluster-logging\", \"monitoring\", etc.)\n     - Use AskUserQuestion to confirm: \"Based on your enhancement description, I think the best area is '<inferred-area>'. Is this correct?\" with options:\n       - \"Yes, use '<inferred-area>'\"\n       - \"No, let me choose a different area\" (then list available areas and ask them to select or specify a new one)\n     - If the user chooses a different area that doesn't exist, ask if they want to create it (same flow as above)\n   - **Creating a new area (if chosen):**\n     - Create the directory: `mkdir -p enhancements/<new-area>/`\n     - Consider adding a README.md in the new area directory to document its purpose\n   - Proceed with the validated/created area\n\n2. **Fetch the Enhancement Template**: Before starting, fetch the latest template from the openshift/enhancements repository:\n   - Download the template via HTTP using whatever web-fetch mechanism is available; otherwise ask the user to paste the template from: https://raw.githubusercontent.com/openshift/enhancements/master/guidelines/enhancement_template.md\n   - Store this template for reference when creating the enhancement file\n   - This ensures you always use the most current template structure and required sections\n\n3. **Parse the Description**: Extract the following from the description:\n   - **What**: What is this enhancement about\n   - **Why**: Why this change is required (motivation)\n   - **Who**: Which personas this applies to (use this to generate user stories)\n\n4. **Ask Clarifying Questions** (if needed): Use the AskUserQuestion tool to gather:\n   - Specific user stories or motivations if not clear from the description\n   - Explicit Goals or Non-Goals the user wants to be included\n   - Any specific technical constraints or requirements\n   - Topology considerations (Hypershift, SNO, MicroShift, OKE relevance)\n   - Whether this proposal adds/changes CRDs, admission and conversion webhooks, ValidatingAdmissionPlugin, MutatingAdmissionPlugin, aggregated API servers, or finalizers (needed for API Extensions section)\n   - Feature gate information: According to the OpenShift enhancement dev guide (https://github.com/openshift/enhancements/blob/master/dev-guide/feature-zero-to-hero.md), ALL new OpenShift features must start disabled by default using feature gates. Ask about the proposed feature gate name and initial feature set (DevPreviewNoUpgrade or TechPreviewNoUpgrade).\n   - Ask clarifying questions about telemetry, security, upgrade and downgrade process, rollbacks, dependencies, in case it is not possible to assert these fields.\n\n5. **Generate the Enhancement File**:\n   - Create the file at `enhancements/<area>/<filename>.md` where filename is the kebab-case version of the name argument\n   - Fill in the template with:\n     - **Title**: Use the provided name\n     - **Summary**: One paragraph describing what this enhancement is about\n     - **Motivation**: Explain why this change is required based on the description\n     - **User Stories**: Generate 2-4 user stories based on the \"who\" information using the format:\n       > \"As a _role_, I want to _take some action_ so that I can _accomplish a goal_.\"\n       Include a story on how the proposal will be operationalized: life-cycled, monitored and remediated at scale.\n     - **Goals**: List specific, measurable goals (3-5 items). Goals should describe what users want from their perspective, not implementation details.\n     - **Non-Goals**: List what is explicitly out of scope (2-3 items)\n     - **Proposal**: High-level description of the proposed solution\n     - **Workflow Description**: Detailed workflow with actors and steps\n     - **Mermaid Diagram**: Add a sequence diagram when the workflow involves multiple actors or complex interactions between components (e.g., user -> API server -> controller -> operator). Simple single-actor workflows may not need a diagram.\n     - **API Extensions**: Only fill this section if the user confirms the proposal adds/changes CRDs, admission and conversion webhooks, ValidatingAdmissionPlugin, MutatingAdmissionPlugin, aggregated API servers, or finalizers. Per the template, name the API extensions and describe if this enhancement modifies the behaviour of existing resources. Otherwise, add a TODO comment asking the user to complete this section if applicable.\n     - **Topology Considerations**: Include subsections for Hypershift/Hosted Control Planes, Standalone Clusters, Single-node Deployments or MicroShift, and OKE (OpenShift Kubernetes Engine). Address how the proposal affects each topology.\n     - **Implementation Details/Notes/Constraints**: Provide a high-level overview of the code changes required. Follow the guidance from the template: \"While it is useful to go into the details of the code changes required, it is not necessary to show how the code will be rewritten in the enhancement.\" Keep it as an overview; the developer should fill in the specific implementation details. Include a reminder about creating a feature gate: Per the OpenShift dev guide (https://github.com/openshift/enhancements/blob/master/dev-guide/feature-zero-to-hero.md), all new features must be gated behind a feature gate in https://github.com/openshift/api/blob/master/features/features.go with the appropriate feature set (DevPreviewNoUpgrade or TechPreviewNoUpgrade initially).\n     - **Test Plan**: Add a TODO comment with guidance on required test labels per the OpenShift dev guide (https://github.com/openshift/enhancements/blob/master/dev-guide/feature-zero-to-hero.md): Tests must include `[OCPFeatureGate:FeatureName]` label for the feature gate, `[Jira:\"Component Name\"]` for the component, and appropriate test type labels like `[Suite:...]`, `[Serial]`, `[Slow]`, or `[Disruptive]` as needed. Reference the test conventions guide (https://github.com/openshift/enhancements/blob/master/dev-guide/test-conventions.md) for details.\n     - **Graduation Criteria**: Add a TODO comment referencing the specific promotion requirements from the OpenShift dev guide (https://github.com/openshift/enhancements/blob/master/dev-guide/feature-zero-to-hero.md): minimum 5 tests, 7 runs per week, 14 runs per supported platform, 95% pass rate, and tests running on all supported platforms (AWS, Azure, GCP, vSphere, Baremetal with various network stacks).\n     - **Metadata**: Fill in creation-date with today's date, tracking-link with the provided JIRA ticket URL, set other fields to TBD. For api-approvers: use \"None\" if there are no API changes (no new/modified CRDs, webhooks, aggregated API servers, or finalizers); otherwise use \"TBD\" as a placeholder (the enhancement author will request an API reviewer from the #forum-api-review Slack channel later).\n\n6. **Handle Unfilled Sections**: For sections that cannot be filled based on the input:\n   - Add a clear comment like `<!-- TODO: This section needs to be filled in -->`\n   - Provide guidance on what should be included\n\n7. **Writing Guidelines**:\n   - Write in a clear, concise, professional manner\n   - Focus on the essential information\n   - Use bullet points and structured formatting\n   - Avoid unnecessary verbosity\n   - **Line Length**: Keep lines in the generated enhancement at a maximum of 80 characters, but prioritize validity over line length limits. Only break lines at 80 characters if doing so will NOT create:\n     - Invalid or broken URLs (URLs themselves should never be split, but the line CAN and SHOULD be broken before or after the URL)\n     - Invalid Markdown syntax (e.g., breaking Markdown links, code blocks, or formatting)\n     - Invalid code examples (e.g., breaking code in the middle of statements)\n     If breaking at 80 characters would split a URL, code, or Markdown syntax, find the nearest valid break point such as: after a sentence, before a URL starts, after a URL ends, or at a natural paragraph break. For regular prose, it is acceptable to exceed 80 characters by 10-15 characters to avoid breaking words mid-word. Only allow lines >95 characters when the line contains a single unbreakable element (like a standalone URL with no surrounding text, or a single line of code).\n\n8. **Validate**:\n   - Create a valid filename from the name (lowercase, replace spaces with dashes)\n   - Verify all required YAML metadata is present\n   - Verify the JIRA ticket URL is included in the tracking-link metadata field\n   - Ensure the enhancement file is created in the correct path: `enhancements/<area>/<filename>.md`\n\n## Output\n\nAfter creating the enhancement file, provide:\n- The full path to the created file\n- A brief summary of what was included\n- A list of sections that need further attention (marked with TODO comments)\n\nBegin by analyzing the inputs and asking any necessary clarifying questions before generating the enhancement proposal."
              },
              {
                "name": "/bootstrap-om",
                "description": "Bootstrap OpenShift Manager (OM) integration for OpenShift operators with automated resource discovery",
                "path": "plugins/openshift/commands/bootstrap-om.md",
                "frontmatter": {
                  "description": "Bootstrap OpenShift Manager (OM) integration for OpenShift operators with automated resource discovery",
                  "argument-hint": ""
                },
                "content": "## Name\n\nopenshift:bootstrap-om\n\n## Synopsis\n\n```bash\n/openshift:bootstrap-om\n```\n\n## Description\n\nThe `openshift:bootstrap-om` command automates the complete integration of OpenShift Manager (OM) into OpenShift operators. OM (formerly Multi-Operator Manager/MOM) is a framework designed to reduce duplicate effort and improve consistency across different OpenShift cluster topologies (standalone/OCP and Hypershift/HCP) by centralizing operator management and enabling comprehensive testing.\n\nThis command automates:\n- **Automatic resource discovery** - Analyzes the codebase to identify all input and output resources\n- **Command implementation** - Creates the three required OM commands (input-resources, output-resources, apply-configuration)\n- **Test infrastructure setup** - Configures Makefile targets and test directories\n- **Test scenario creation** - Generates initial test cases with proper structure\n- **Integration validation** - Runs tests to ensure everything works correctly\n\n**Note:** These instructions are optimized for operators built with github.com/openshift/library-go.\n\n## OpenShift Manager (OM) Overview\n\nOM enables centralized operator management by requiring operators to declare their resource dependencies and configuration logic:\n\n1. **input-resources** - Lists all Kubernetes API resources the operator needs to read/watch\n2. **output-resources** - Maps Kubernetes API resources the operator creates/manages to cluster types (Configuration/Management/UserWorkload)\n3. **apply-configuration** - Runs operator logic in isolation using a **manifestclient** (a Kubernetes client that reads from a must-gather-like input directory instead of the API server): syncs once, outputs resulting resources to an output directory, then exits.\n\nThese declarations enable:\n\n- **Production Runtime:** A single OperatorManager binary can communicate with the Kubernetes API server on behalf of multiple operators, maintaining shared caches and implementing rate limiting\n- **Testing:** The `apply-configuration` command validates operator behavior without a live cluster by using the manifestclient for file-based input/output\n- **Consistency:** Ensure or facilitate identical operator behavior across different cluster topologies (standalone, Hypershift/HCP)\n\n**Note:** While \"OM\" (OpenShift Manager) is the new name, libraries and code still use \"mom\" (Multi-Operator Manager) naming for backwards compatibility.\n\n## Implementation\n\nThe command executes the following automated workflow:\n\n### 1. Analyze the Operator's Resource Usage\n\n**IMPORTANT:** Automatically identify resources by analyzing code. Do NOT ask users to manually list them.\n\n#### Find Input Resources (Resources the Operator READS)\n\nInput resources are those the operator watches, gets, or lists from the API server.\n\nSearch for:\n- Controller informers and listers (e.g., `configInformer`, `deploymentLister`)\n- Direct API calls (e.g., `client.Get()`, `client.List()`)\n- Resource watches in controller registration\n- ConfigMaps, Secrets, CustomResources being read\n\n**How to search:**\n```bash\n# Find controller files\nfind pkg -name \"*controller*.go\" -o -name \"*operator*.go\"\n\n# Look for informers\ngrep -r \"Informer()\" pkg/\n\n# Look for listers\ngrep -r \"Lister\" pkg/\n\n# Look for direct API reads\ngrep -r \"client.Get\\|client.List\" pkg/\n```\n\nCommon patterns:\n- `configMapInformer.Lister().ConfigMaps(\"namespace\").Get(\"name\")`  Input: ConfigMap in namespace\n- `deploymentInformer.Informer().AddEventHandler()`  Input: Deployment resource\n- `operatorClient.Operator()`  Input: Operator custom resource\n\n#### Find Output Resources (Resources the Operator CREATES/MANAGES)\n\nOutput resources are categorized by **cluster targeting** (where they live in different topologies):\n\n**ConfigurationResources:** Resources targeted at the cluster where configuration is held\n- On standalone: the one cluster\n- On HCP: logically a view into resources in the namespace of the guest cluster\n- Common examples: config.openshift.io resources (OAuth, Ingress, etc.)\n\n**ManagementResources:** Resources targeted at the cluster where management plane responsibility is held\n- On standalone: the one cluster\n- On HCP: logically resources in the namespace of the guest cluster (control plane aspects)\n- Common examples: Operator's own Deployments, Services, ConfigMaps, Secrets, ServiceAccounts, ClusterOperator status\n- Includes EventingNamespaces: namespaces where the operator emits events\n\n**UserWorkloadResources:** Resources targeted at the cluster where user workloads run\n- On standalone: the one cluster\n- On HCP: the guest cluster\n- Common examples: ClusterRoles, ClusterRoleBindings, CRDs, resources that run alongside user workloads\n\n**How to search:**\n```bash\n# Find manifests and bindata\nfind bindata/ manifests/ -type f 2>/dev/null\n\n# Look for resource creation/apply\ngrep -r \"Apply\\|Create\\|Update\" pkg/ | grep -i \"deployment\\|service\\|configmap\"\n\n# Look for manifest files\ngrep -r \"apiVersion:\" bindata/ manifests/ 2>/dev/null\n```\n\nCommon patterns:\n- Files in `bindata/` or `manifests/`  Output resources\n- `resourceapply.ApplyDeployment()`  Output: Deployment\n- `resourceapply.ApplyConfigMap()`  Output: ConfigMap\n- ClusterOperator status updates  Output: ClusterOperator\n\n### 2. Prepare Dependencies\n\nBefore creating command stubs, manually add the dependency to go.mod:\n\n```go\n// In the require section of go.mod, add:\ngithub.com/openshift/multi-operator-manager v0.0.0-20250930141021-05cb0b9abdb4\n```\n\n**Note:** Don't run `go mod tidy` yet - it will remove the dependency since no code imports it yet. After creating the stub files that import the library, you'll run `go mod tidy && go mod vendor`, and the dependency will be committed together with the stubs in the first commit\n\n### 3. Implement the Three OM Commands (Incrementally)\n\n**IMPORTANT:** Create command stubs first, then fill them in with actual resource declarations. This creates a clean, logical git history with ~6-8 focused commits:\n\n1. Create command stub  integrate into CLI  commit\n2. Fill in the stub with discovered resources  commit\n3. Repeat for each command\n4. Set up test infrastructure  commit\n5. (Optional) Add test scenario if apply-configuration works  commit\n\n#### Common Patterns\n\n**Directory structure:**\n```bash\nmkdir -p pkg/cmd/mom\n```\nNote: We keep the `mom` directory name for consistency with library imports.\n\n**Helper functions** for declaring resources (use in both input-resources and output-resources):\n\nCommon resources:\n- `ExactConfigMap(namespace, name)`, `ExactSecret(namespace, name)`, `ExactDeployment(namespace, name)`\n- `ExactService(namespace, name)`, `ExactServiceAccount(namespace, name)`, `ExactNamespace(name)`\n- `ExactRole(namespace, name)`, `ExactRoleBinding(namespace, name)`\n\nOpenShift-specific:\n- `ExactConfigResource(name)` - For config.openshift.io resources (e.g., \"oauths\", \"ingresses\")\n- `ExactLowLevelOperator(name)` - For operator.openshift.io resources\n- `ExactClusterOperator(name)` - For ClusterOperators\n\nCluster-scoped RBAC:\n- `ExactClusterRole(name)`, `ExactClusterRoleBinding(name)`\n\nOther:\n- `ExactPDB(namespace, name)`, `ExactOAuthClient(name)`, `GeneratedCSR(prefix)`\n- `ExactResource(group, version, resource, namespace, name)` - For any other resource\n\n**CLI integration pattern** (add to `cmd/*/main.go`):\n```go\nimport (\n        \"github.com/openshift/YOUR-OPERATOR/pkg/cmd/mom\"\n)\n\n// In your main command function:\ncmd.AddCommand(mom.NewInputResourcesCommand(ioStreams))\ncmd.AddCommand(mom.NewOutputResourcesCommand(ioStreams))\ncmd.AddCommand(mom.NewApplyConfigurationCommand(ioStreams))\n```\n\n**Build/test pattern:**\n```bash\nmake build\n./your-operator-binary <command-name>  # Test the command\ngit add <files>\ngit commit -m \"<title>\n\n<optional description>\n\nGenerated with Claude Code\"\n```\n\n#### 3.1 Create and Commit input-resources Command Stub\n\nCreate `pkg/cmd/mom/input_resources_command.go`:\n\n```go\npackage mom\n\nimport (\n        \"context\"\n\n        \"github.com/openshift/multi-operator-manager/pkg/library/libraryinputresources\"\n        \"github.com/spf13/cobra\"\n        \"k8s.io/cli-runtime/pkg/genericiooptions\"\n)\n\nfunc NewInputResourcesCommand(streams genericiooptions.IOStreams) *cobra.Command {\n        return libraryinputresources.NewInputResourcesCommand(runInputResources, runOutputResources, streams)\n}\n\nfunc runInputResources(ctx context.Context) (*libraryinputresources.InputResources, error) {\n        return &libraryinputresources.InputResources{\n                ApplyConfigurationResources: libraryinputresources.ResourceList{\n                        ExactResources: []libraryinputresources.ExactResourceID{\n                                // TODO: Fill in discovered resources\n                        },\n                },\n        }, nil\n}\n```\n\nIntegrate into CLI (see Common Patterns), build, test, and commit with title: \"Add OM input-resources command stub\"\n\n#### 3.2 Fill in input-resources and Commit\n\nAnalyze the codebase to discover all input resources (see Section 1 for search patterns).\n\nUpdate `runInputResources` function with discovered resources:\n\n```go\nfunc runInputResources(ctx context.Context) (*libraryinputresources.InputResources, error) {\n        return &libraryinputresources.InputResources{\n                ApplyConfigurationResources: libraryinputresources.ResourceList{\n                        ExactResources: []libraryinputresources.ExactResourceID{\n                                // Example discovered resources (see Helper Functions):\n                                libraryinputresources.ExactLowLevelOperator(\"youroperators\"),\n                                libraryinputresources.ExactConfigResource(\"infrastructures\"),\n                                libraryinputresources.ExactNamespace(\"your-operator-namespace\"),\n                                libraryinputresources.ExactConfigMap(\"namespace\", \"config-name\"),\n                                // ... all discovered input resources\n                        },\n                },\n        }, nil\n}\n```\n\nBuild, test, and commit with title: \"Populate OM input-resources with discovered resources\"\n\n#### 3.3 Create and Commit output-resources Command Stub\n\nCreate `pkg/cmd/mom/output_resources_command.go`:\n\n```go\npackage mom\n\nimport (\n        \"context\"\n\n        \"github.com/openshift/multi-operator-manager/pkg/library/libraryoutputresources\"\n        \"github.com/spf13/cobra\"\n        \"k8s.io/cli-runtime/pkg/genericiooptions\"\n)\n\nfunc NewOutputResourcesCommand(streams genericiooptions.IOStreams) *cobra.Command {\n        return libraryoutputresources.NewOutputResourcesCommand(runOutputResources, streams)\n}\n\nfunc runOutputResources(ctx context.Context) (*libraryoutputresources.OutputResources, error) {\n        return &libraryoutputresources.OutputResources{\n                ConfigurationResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{},\n                },\n                ManagementResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{},\n                        EventingNamespaces: []string{},\n                },\n                UserWorkloadResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{},\n                },\n        }, nil\n}\n```\n\nIntegrate into CLI, build, test, and commit with title: \"Add OM output-resources command stub\"\n\n#### 3.4 Fill in output-resources and Commit\n\nAnalyze the codebase to discover all output resources (see Section 1 for search patterns and category descriptions).\n\nUpdate `runOutputResources` function with discovered resources:\n\n```go\nfunc runOutputResources(ctx context.Context) (*libraryoutputresources.OutputResources, error) {\n        return &libraryoutputresources.OutputResources{\n                ConfigurationResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{\n                                libraryoutputresources.ExactConfigResource(\"ingresses\"),\n                        },\n                },\n                ManagementResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{\n                                libraryoutputresources.ExactClusterOperator(\"operatorname\"),\n                                libraryoutputresources.ExactNamespace(\"namespace\"),\n                                libraryoutputresources.ExactDeployment(\"namespace\", \"name\"),\n                                // ... all discovered management resources\n                        },\n                        EventingNamespaces: []string{\n                                \"operator-namespace\",\n                        },\n                },\n                UserWorkloadResources: libraryoutputresources.ResourceList{\n                        ExactResources: []libraryoutputresources.ExactResourceID{\n                                libraryoutputresources.ExactClusterRole(\"name\"),\n                                libraryoutputresources.ExactClusterRoleBinding(\"name\"),\n                        },\n                },\n        }, nil\n}\n```\n\nBuild, test, and commit with title: \"Populate OM output-resources with discovered resources\"\n\n#### 3.5 Create apply-configuration Command Stub and Commit\n\n**This is the most complex command.** It requires running your operator's reconciliation logic once using the manifestclient.\n\nCreate `pkg/cmd/mom/apply_configuration_command.go`:\n\n```go\npackage mom\n\nimport (\n        \"context\"\n        \"fmt\"\n\n        \"github.com/openshift/multi-operator-manager/pkg/library/libraryapplyconfiguration\"\n        \"github.com/spf13/cobra\"\n        \"k8s.io/cli-runtime/pkg/genericiooptions\"\n)\n\nfunc NewApplyConfigurationCommand(streams genericiooptions.IOStreams) *cobra.Command {\n        return libraryapplyconfiguration.NewApplyConfigurationCommand(RunApplyConfiguration, runOutputResources, streams)\n}\n\nfunc RunApplyConfiguration(ctx context.Context, input libraryapplyconfiguration.ApplyConfigurationInput) (*libraryapplyconfiguration.ApplyConfigurationRunResult, libraryapplyconfiguration.AllDesiredMutationsGetter, error) {\n        // TODO: Implement operator reconciliation logic\n        //\n        // The manifestclient (input.ManagementClient) is a drop-in replacement for standard k8s clients.\n        // Pass it to your operator and run sync logic ONCE (not in a loop).\n        //\n        // Example: op := operator.NewOperator(input.ManagementClient, input.ManagementEventRecorder)\n        //          if err := op.Sync(ctx); err != nil { return nil, nil, err }\n        //          return &libraryapplyconfiguration.ApplyConfigurationRunResult{}, nil, nil\n        //\n        // See: github.com/openshift/cluster-authentication-operator/pkg/cmd/mom/apply_configuration_command.go\n\n        return nil, nil, fmt.Errorf(\"not yet implemented - see TODO above\")\n}\n```\n\nIntegrate into CLI, build (verify it compiles), and commit with title: \"Add OM apply-configuration command stub\"\n\n**Note:** If you cannot implement apply-configuration fully, that's acceptable! The stub provides instructions for manual implementation later.\n\n### 4. Set Up Test Infrastructure and Commit\n\n**IMPORTANT:** Check if the project uses `build-machinery-go` before proceeding:\n\n```bash\nls vendor/github.com/openshift/build-machinery-go/make/targets/openshift/operator/mom.mk\n```\n\n**If build-machinery-go is NOT present:**\n- Inform the user that test infrastructure requires `build-machinery-go`\n- Explain they need to vendor `github.com/openshift/build-machinery-go` first\n- Skip test infrastructure setup (OM commands are still usable without it)\n\n**If build-machinery-go IS present:**\n\n#### 4.1 Update Makefile\n\nAdd to your `Makefile`:\n\n```makefile\ninclude $(addprefix ./vendor/github.com/openshift/build-machinery-go/make/, \\\n        targets/openshift/operator/mom.mk \\\n)\n```\n\nThis provides:\n- `make test-operator-integration` - Runs OM tests and validates output\n- `make update-test-operator-integration` - Updates expected test output\n\n#### 4.2 Create Test Scenarios (Optional - only if apply-configuration is implemented)\n\nCreate test directory structure:\n\n```bash\nmkdir -p test-data/apply-configuration/overall/minimal-cluster/input-dir\n```\n\nEach test scenario needs:\n- **`test.yaml`** - Test metadata (binaryName, testName, description, testType, now)\n- **`input-dir/`** - Input resources in must-gather format organized by `cluster-scoped-resources/<group>/<kind>.yaml` or `namespaces/<namespace>/<group>/<kind>/<name>.yaml`\n\nRun initial test:\n```bash\nmake build\nmake test-operator-integration\nmake update-test-operator-integration  # Update expected output\nmake test-operator-integration  # Should now pass\n```\n\nCommit Makefile (and test-data/ if created) with title: \"Add OM test infrastructure\"\n\n### 5. Verify and Iterate\n\n1. Run `make test-operator-integration` - Should pass after updating expected output\n2. Create additional test scenarios for different configurations\n3. Verify input-resources and output-resources are complete by checking test output\n\n## Reference Implementation\n\n**Study these files in `cluster-authentication-operator`:**\n- `pkg/cmd/mom/input_resources_command.go` - Complete input resource declarations\n- `pkg/cmd/mom/output_resources_command.go` - Complete output resource declarations\n- `pkg/cmd/mom/apply_configuration_command.go` - Integration with operator core logic\n- `cmd/*/main.go` - CLI integration pattern\n- `Makefile` - OM makefile inclusion\n- `test-data/apply-configuration/overall/` - Multiple test scenario examples\n\n**Documentation:**\n- [Multi-Operator Manager Development Guide](https://github.com/openshift/enhancements/blob/master/dev-guide/multi-operator-manager.md)\n- [multi-operator-manager Repository](https://github.com/openshift/multi-operator-manager)\n- [cluster-authentication-operator](https://github.com/openshift/cluster-authentication-operator)\n\n**Key Concept - manifestclient:**\nImplements standard Kubernetes client interfaces (`client.Client`, `kubernetes.Interface`, `dynamic.Interface`). It's a drop-in replacement that reads from files instead of the API server - no refactoring needed!\n\n## Validation Checklist\n\nBefore considering OM integration complete:\n\n- [ ] `pkg/cmd/mom/input_resources_command.go` exists and lists all input resources\n- [ ] `pkg/cmd/mom/output_resources_command.go` exists and lists all output resources\n- [ ] `pkg/cmd/mom/apply_configuration_command.go` exists (stub or implementation)\n- [ ] Commands integrated into main CLI\n- [ ] `Makefile` includes `mom.mk` (if build-machinery-go available)\n- [ ] At least one test scenario in `test-data/apply-configuration/overall/` (if apply-configuration implemented)\n- [ ] `make test-operator-integration` passes (if tests created)\n\n## Usage Guidelines\n\n1. **Separate Dependency Commits** - ALWAYS commit dependency changes separately. Any changes to `go.mod`, `go.sum`, or `vendor/` must go in their own commits. NEVER mix vendor changes with code implementation.\n\n2. **Automatic Analysis** - Analyze codebase automatically. Do NOT ask users to list resources.\n\n3. **apply-configuration Can Be a Stub** - If you cannot implement it fully, create a comprehensive stub with TODO comments. This is acceptable!\n\n4. **Validate Early** - Build and test after each command stub/fill-in to catch issues immediately.\n\nThe command should autonomously complete all steps from resource discovery through test infrastructure setup (if build-machinery-go is available), creating focused, logical commits that are easy to review."
              },
              {
                "name": "/bump-deps",
                "description": "Bump dependencies in OpenShift projects with automated analysis and PR creation",
                "path": "plugins/openshift/commands/bump-deps.md",
                "frontmatter": {
                  "description": "Bump dependencies in OpenShift projects with automated analysis and PR creation",
                  "argument-hint": "<dependency> [version] [--create-jira] [--create-pr]"
                },
                "content": "## Name\n\nopenshift:bump-deps\n\n## Synopsis\n\n```\n/openshift:bump-deps <dependency> [version] [--create-jira] [--create-pr]\n```\n\n## Description\n\nThe `openshift:bump-deps` command automates the process of bumping dependencies in OpenShift organization projects. It analyzes the dependency, determines the appropriate version to bump to, updates the necessary files (go.mod, go.sum, package.json, etc.), runs tests, and optionally creates Jira tickets and pull requests.\n\nThis command significantly reduces the manual effort required for dependency updates by automating:\n\n- Dependency version discovery and analysis\n- Compatibility checking with current codebase\n- File updates (go.mod, package.json, Dockerfile, etc.)\n- Test execution to verify the update\n- Jira ticket creation with comprehensive details\n- Pull request creation with proper formatting\n- Release notes generation\n\nThe command intelligently handles different dependency types (Go modules, npm packages, container images, etc.) and can process single or multiple dependencies at once.\n\n## Implementation\n\nThe command executes the following workflow:\n\n### 1. Repository Analysis\n\n- Detects repository type (Go, Node.js, Python, etc.)\n- Identifies dependency management files (go.mod, package.json, requirements.txt, etc.)\n- Determines current project structure and conventions\n- Checks for existing CI/CD configuration\n\n### 2. Dependency Discovery\n\n**For Go Projects:**\n- Parses go.mod to find current version\n- Uses `go list -m -versions <module>` to list available versions\n- Checks for major version compatibility (v0, v1, v2+)\n- Identifies if dependency is direct or indirect\n\n**For Node.js Projects:**\n- Parses package.json for current version\n- Uses npm/yarn to find latest versions\n- Checks semantic versioning constraints\n- Identifies devDependencies vs dependencies\n\n**For Container Images:**\n- Parses Dockerfile and related files\n- Checks registry for available tags\n- Verifies image digest and signatures\n- Identifies base images and tool images\n\n**For Python Projects:**\n- Parses requirements.txt or pyproject.toml\n- Uses pip to find available versions\n- Checks for version constraints\n\n### 3. Version Selection\n\nIf no version is specified:\n- Suggests latest stable version\n- Considers semantic versioning (patch, minor, major)\n- Checks for breaking changes in release notes\n- Validates against project's minimum version requirements\n\nIf version is specified:\n- Validates version exists\n- Checks compatibility with current project version\n- Warns about major version jumps\n\n### 4. Impact Analysis\n\n- Searches codebase for usage of the dependency\n- Identifies files importing/using the dependency\n- Analyzes API changes between versions\n- Checks for deprecated features being used\n- Reviews upstream changelog and release notes\n- Identifies potential breaking changes\n\n### 5. File Updates\n\n**Go Projects:**\n- Updates go.mod with new version\n- Runs `go mod tidy` to update go.sum\n- Runs `go mod vendor` if vendor directory exists\n- Updates any version constraints in comments\n\n**Node.js Projects:**\n- Updates package.json\n- Runs `npm install` or `yarn install`\n- Updates package-lock.json or yarn.lock\n\n**Container Images:**\n- Updates Dockerfile(s)\n- Updates related manifests (kubernetes, etc.)\n- Updates any CI configuration using the image\n\n**Python Projects:**\n- Updates requirements.txt or pyproject.toml\n- Generates new lock file if applicable\n\n### 6. Testing Strategy\n\n- Identifies relevant test suites\n- Runs unit tests: `make test` or equivalent\n- Runs integration tests if available\n- Runs e2e tests for critical dependencies\n- Checks for test failures and analyzes logs\n- Verifies build succeeds: `make build`\n\n### 7. Jira Ticket Creation (if --create-jira)\n\nCreates a Jira ticket with:\n- **Summary**: `Bump {dependency} from {old_version} to {new_version}`\n- **Type**: Task or Bug (if security update)\n- **Components**: Auto-detected from repository\n- **Labels**: [\"dependencies\", \"automated-update\", \"ai-generated\"]\n  - Adds \"security\" if CVE-related\n  - Adds \"breaking-change\" if major version bump\n- **Description**: Includes:\n  - Dependency information and type\n  - Current and new versions\n  - Changelog summary\n  - Breaking changes (if any)\n  - Files modified\n  - Test results\n  - Migration steps (if needed)\n  - Links to upstream release notes\n- **Target Version**: Auto-detected from release branches\n\n### 8. Pull Request Creation (if --create-pr)\n\nCreates a pull request with:\n- **Title**: `[{JIRA-ID}] Bump {dependency} from {old_version} to {new_version}`\n- **Body**: Includes:\n  - Link to Jira ticket\n  - Summary of changes\n  - Breaking changes callout\n  - Testing performed\n  - Checklist for reviewers\n  - Release notes snippet\n- **Labels**: Auto-applied based on change type\n- **Branch naming**: `deps/{dependency}-{new_version}` or `{jira-id}-bump-{dependency}`\n\n### 9. Conflict Resolution\n\nIf updates cause issues:\n- Identifies conflicting dependencies\n- Suggests resolution strategies\n- Can attempt automatic resolution for common cases\n- Provides manual resolution steps for complex scenarios\n\n## Return Value\n\n- **Claude agent text**: Processing status, test results, and summary\n- **Side effects**:\n  - Modified dependency files (go.mod, package.json, etc.)\n  - Updated lock files\n  - Jira ticket created (if --create-jira)\n  - Pull request created (if --create-pr)\n  - Git branch created with changes\n\n## Examples\n\n1. **Bump a Go dependency to latest**:\n\n   ```\n   /openshift:bump-deps k8s.io/api\n   ```\n\n   Output:\n\n   ```\n   Analyzing dependency: k8s.io/api\n   Current version: v0.28.0\n   Latest version: v0.29.1\n\n   Checking compatibility...\n    No breaking changes detected\n\n   Updating go.mod...\n   Running go mod tidy...\n\n   Running tests...\n    All tests passed\n\n   Summary:\n   - Dependency: k8s.io/api\n   - Old version: v0.28.0\n   - New version: v0.29.1\n   - Files modified: go.mod, go.sum\n   - Tests:  Passed\n\n   Changes are ready. Use --create-pr to create a pull request.\n   ```\n\n2. **Bump to a specific version with Jira ticket**:\n\n   ```\n   /openshift:bump-deps golang.org/x/net v0.20.0 --create-jira\n   ```\n\n   Output:\n\n   ```\n   Analyzing dependency: golang.org/x/net\n   Current version: v0.19.0\n   Target version: v0.20.0\n\n   Reviewing changes...\n     Breaking changes detected in v0.20.0:\n   - http2: Server.IdleTimeout applies to idle h2 connections\n\n   Updating go.mod...\n   Running tests...\n    All tests passed\n\n   Creating Jira ticket...\n    Created: OCPBUGS-12345\n\n   Summary:\n   - Jira: https://issues.redhat.com/browse/OCPBUGS-12345\n   - Dependency: golang.org/x/net\n   - Version: v0.19.0  v0.20.0\n   - Breaking changes: Yes\n   ```\n\n3. **Bump and create PR in one step**:\n\n   ```\n   /openshift:bump-deps github.com/spf13/cobra --create-jira --create-pr\n   ```\n\n   Output:\n\n   ```\n   Processing dependency bump for github.com/spf13/cobra...\n\n   [1/7] Analyzing dependency...\n   Current: v1.7.0\n   Latest: v1.8.0\n\n   [2/7] Checking changelog...\n   Changes include:\n   - New features: Enhanced shell completion\n   - Bug fixes: 5 issues resolved\n   - No breaking changes\n\n   [3/7] Updating files...\n    go.mod updated\n    go.sum updated\n\n   [4/7] Running tests...\n    Unit tests: 156/156 passed\n    Integration tests: 23/23 passed\n\n   [5/7] Creating Jira ticket...\n    Created: OCPBUGS-12346\n\n   [6/7] Creating git branch...\n    Branch: OCPBUGS-12346-bump-cobra\n\n   [7/7] Creating pull request...\n    PR created: #1234\n\n   Summary:\n   - Jira: https://issues.redhat.com/browse/OCPBUGS-12346\n   - PR: https://github.com/openshift/repo/pull/1234\n   - Dependency: github.com/spf13/cobra\n   - Version: v1.7.0  v1.8.0\n   - Tests: All passed\n\n   Next steps:\n   1. Review the PR at the link above\n   2. Address any reviewer comments\n   3. Merge when approved\n   ```\n\n4. **Bump multiple related dependencies**:\n\n   ```\n   /openshift:bump-deps \"k8s.io/*\"\n   ```\n\n   Output:\n\n   ```\n   Found 8 Kubernetes dependencies to update:\n\n   [1/8] k8s.io/api: v0.28.0  v0.29.1\n   [2/8] k8s.io/apimachinery: v0.28.0  v0.29.1\n   [3/8] k8s.io/client-go: v0.28.0  v0.29.1\n   [4/8] k8s.io/kubectl: v0.28.0  v0.29.1\n   ...\n\n   These should be updated together to maintain compatibility.\n   Proceed with batch update? [y/N]\n   ```\n\n5. **Bump a container base image**:\n\n   ```\n   /openshift:bump-deps registry.access.redhat.com/ubi9/ubi-minimal\n   ```\n\n   Output:\n\n   ```\n   Analyzing container image: ubi9/ubi-minimal\n   Current: 9.3-1361\n   Latest: 9.4-1194\n\n   Checking for security updates...\n    3 CVEs fixed in new version\n\n   Updating Dockerfile...\n   Building test image...\n   Running container tests...\n    All tests passed\n\n   Files modified:\n   - Dockerfile\n   - .github/workflows/build.yml\n   ```\n\n## Arguments\n\n- **$1** (required): Dependency identifier\n  - Go module: `github.com/org/repo` or `golang.org/x/net`\n  - npm package: `@types/node` or `react`\n  - Container image: `registry.access.redhat.com/ubi9/ubi-minimal`\n  - Wildcard for batch: `k8s.io/*` (requires confirmation)\n\n- **$2** (optional): Target version\n  - Semantic version: `v1.2.3`, `1.2.3`\n  - Version range: `^1.2.0`, `~1.2.0`\n  - Special: `latest`, `latest-stable`\n  - If omitted: suggests latest stable version\n\n- **--create-jira** (flag): Create a Jira ticket for the update\n  - Auto-detects project from repository\n  - Can be configured with JIRA_PROJECT env var\n  - Ticket includes full change analysis\n\n- **--create-pr** (flag): Create a pull request with the changes\n  - Implies creating a git branch\n  - Includes --create-jira automatically\n  - PR is created as draft if tests fail\n\n- **--jira-project** (option): Specify Jira project (default: auto-detect)\n  - Example: `--jira-project OCPBUGS`\n\n- **--component** (option): Specify Jira component (default: auto-detect)\n  - Example: `--component \"Control Plane\"`\n\n- **--branch** (option): Specify git branch name (default: auto-generate)\n  - Example: `--branch feature/update-deps`\n\n- **--skip-tests** (flag): Skip running tests (not recommended)\n  - Use only for non-critical updates\n  - PR will be marked as draft\n\n- **--force** (flag): Force update even if tests fail\n  - Creates PR as draft\n  - Includes test failure details in PR\n\n## Error Handling\n\nThe command handles common error cases:\n\n- **Dependency not found**: Lists similar dependencies in project\n- **Version not found**: Shows available versions\n- **Test failures**:\n  - Provides detailed error logs\n  - Suggests potential fixes\n  - Asks whether to create draft PR anyway\n- **Conflicting dependencies**:\n  - Identifies conflicts\n  - Suggests resolution order\n  - Can attempt batch update\n- **Breaking changes**:\n  - Highlights breaking changes\n  - Links to migration guides\n  - Requires explicit confirmation for major bumps\n- **Network failures**: Retries with exponential backoff\n- **Permission errors**: Checks git/GitHub authentication\n\n## Notes\n\n- Repository name and organization are auto-detected from `git remote -v`\n- For Go dependencies, supports both versioned (v2+) and unversioned modules\n- Automatically detects if running in a fork vs upstream repository\n- Respects `.gitignore` and doesn't commit generated/vendored files unnecessarily\n- Can handle dependencies with replace directives in go.mod\n- Supports monorepos with multiple go.mod files\n- All Jira tickets are labeled with \"ai-generated\" for tracking\n- PR creation requires GitHub CLI (gh) to be installed and authenticated\n- For security updates (CVEs), automatically prioritizes and labels appropriately\n- Compatible with Renovate - can be used to customize/enhance Renovate PRs\n\n## Environment Variables\n\n- **JIRA_PROJECT**: Default Jira project for ticket creation\n- **JIRA_COMPONENT**: Default component for Jira tickets\n- **GITHUB_TOKEN**: GitHub authentication (if not using gh auth)\n- **DEFAULT_BRANCH**: Override default branch detection (default: main)\n\n## See Also\n\n- `utils:process-renovate-pr` - Process existing Renovate dependency PRs\n- `git:create-pr` - General PR creation command\n- `jira:create` - Manual Jira ticket creation"
              },
              {
                "name": "/cluster-health-check",
                "description": "Perform comprehensive health check on OpenShift cluster and report issues",
                "path": "plugins/openshift/commands/cluster-health-check.md",
                "frontmatter": {
                  "description": "Perform comprehensive health check on OpenShift cluster and report issues",
                  "argument-hint": "[--verbose] [--output-format]"
                },
                "content": "## Name\nopenshift:cluster-health-check\n\n## Synopsis\n```\n/openshift:cluster-health-check [--verbose] [--output-format json|text]\n```\n\n## Description\n\nThe `cluster-health-check` command performs a comprehensive health analysis of an OpenShift/Kubernetes cluster and reports any detected issues. It examines cluster operators, nodes, deployments, pods, persistent volumes, and other critical resources to identify problems that may affect cluster stability or workload availability.\n\nThis command is useful for:\n- Quick cluster status assessment\n- Troubleshooting cluster issues\n- Pre-deployment validation\n- Regular health monitoring\n- Identifying degraded components\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **Kubernetes/OpenShift CLI**: Either `oc` (OpenShift) or `kubectl` (Kubernetes)\n   - Install `oc` from: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n   - Or install `kubectl` from: https://kubernetes.io/docs/tasks/tools/\n   - Verify with: `oc version` or `kubectl version`\n\n2. **Active cluster connection**: Must be connected to a running cluster\n   - Verify with: `oc whoami` or `kubectl cluster-info`\n   - Ensure KUBECONFIG is set if needed\n\n3. **Sufficient permissions**: Must have read access to cluster resources\n   - Cluster-admin or monitoring role recommended for comprehensive checks\n   - Minimum: ability to view nodes, pods, and cluster operators\n\n## Arguments\n\n- **--verbose** (optional): Enable detailed output with additional context\n  - Shows resource-level details\n  - Includes warning conditions\n  - Provides remediation suggestions\n\n- **--output-format** (optional): Output format for results\n  - `text` (default): Human-readable text format\n  - `json`: Machine-readable JSON format for automation\n\n## Implementation\n\nThe command performs the following health checks:\n\n### 1. Determine CLI Tool\n\nDetect which Kubernetes CLI is available:\n\n```bash\nif command -v oc &> /dev/null; then\n    CLI=\"oc\"\n    CLUSTER_TYPE=\"OpenShift\"\nelif command -v kubectl &> /dev/null; then\n    CLI=\"kubectl\"\n    CLUSTER_TYPE=\"Kubernetes\"\nelse\n    echo \"Error: Neither 'oc' nor 'kubectl' CLI found. Please install one of them.\"\n    exit 1\nfi\n```\n\n### 2. Verify Cluster Connectivity\n\nCheck if connected to a cluster:\n\n```bash\nif ! $CLI cluster-info &> /dev/null; then\n    echo \"Error: Not connected to a cluster. Please configure your KUBECONFIG.\"\n    exit 1\nfi\n\n# Get cluster version info\nif [ \"$CLUSTER_TYPE\" = \"OpenShift\" ]; then\n    CLUSTER_VERSION=$($CLI version -o json 2>/dev/null | jq -r '.openshiftVersion // \"unknown\"')\nelse\n    CLUSTER_VERSION=$($CLI version --short 2>/dev/null | grep -i server | awk '{print $3}')\nfi\n```\n\n### 3. Initialize Health Check Report\n\nCreate a report structure to collect findings:\n\n```bash\nREPORT_FILE=\".work/cluster-health-check/report-$(date +%Y%m%d-%H%M%S).txt\"\nmkdir -p .work/cluster-health-check\n\n# Initialize counters\nCRITICAL_ISSUES=0\nWARNING_ISSUES=0\nINFO_MESSAGES=0\n```\n\n### 4. Check Cluster Operators (OpenShift only)\n\nFor OpenShift clusters, check cluster operator health:\n\n```bash\nif [ \"$CLUSTER_TYPE\" = \"OpenShift\" ]; then\n    echo \"Checking Cluster Operators...\"\n\n    # Get all cluster operators\n    DEGRADED_COs=$($CLI get clusteroperators -o json | jq -r '.items[] | select(.status.conditions[] | select(.type==\"Degraded\" and .status==\"True\")) | .metadata.name')\n\n    UNAVAILABLE_COs=$($CLI get clusteroperators -o json | jq -r '.items[] | select(.status.conditions[] | select(.type==\"Available\" and .status==\"False\")) | .metadata.name')\n\n    PROGRESSING_COs=$($CLI get clusteroperators -o json | jq -r '.items[] | select(.status.conditions[] | select(.type==\"Progressing\" and .status==\"True\")) | .metadata.name')\n\n    if [ -n \"$DEGRADED_COs\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$DEGRADED_COs\" | wc -l)))\n        echo \" CRITICAL: Degraded cluster operators found:\"\n        echo \"$DEGRADED_COs\" | while read co; do\n            echo \"  - $co\"\n            # Get degraded message\n            $CLI get clusteroperator \"$co\" -o json | jq -r '.status.conditions[] | select(.type==\"Degraded\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n        done\n    fi\n\n    if [ -n \"$UNAVAILABLE_COs\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$UNAVAILABLE_COs\" | wc -l)))\n        echo \" CRITICAL: Unavailable cluster operators found:\"\n        echo \"$UNAVAILABLE_COs\" | while read co; do\n            echo \"  - $co\"\n        done\n    fi\n\n    if [ -n \"$PROGRESSING_COs\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$PROGRESSING_COs\" | wc -l)))\n        echo \"  WARNING: Cluster operators in progress:\"\n        echo \"$PROGRESSING_COs\" | while read co; do\n            echo \"  - $co\"\n        done\n    fi\nfi\n```\n\n### 5. Check Node Health\n\nExamine all cluster nodes for issues:\n\n```bash\necho \"Checking Node Health...\"\n\n# Get nodes that are not Ready\nNOT_READY_NODES=$($CLI get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type==\"Ready\" and .status!=\"True\")) | .metadata.name')\n\nif [ -n \"$NOT_READY_NODES\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$NOT_READY_NODES\" | wc -l)))\n    echo \" CRITICAL: Nodes not in Ready state:\"\n    echo \"$NOT_READY_NODES\" | while read node; do\n        echo \"  - $node\"\n        # Get node conditions\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | \"    \\(.type): \\(.status) - \\(.message // \"N/A\")\"'\n    done\nfi\n\n# Check for SchedulingDisabled nodes\nDISABLED_NODES=$($CLI get nodes -o json | jq -r '.items[] | select(.spec.unschedulable==true) | .metadata.name')\n\nif [ -n \"$DISABLED_NODES\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$DISABLED_NODES\" | wc -l)))\n    echo \"  WARNING: Nodes with scheduling disabled:\"\n    echo \"$DISABLED_NODES\" | while read node; do\n        echo \"  - $node\"\n    done\nfi\n\n# Check for node pressure conditions (MemoryPressure, DiskPressure, PIDPressure)\nPRESSURE_NODES=$($CLI get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select((.type==\"MemoryPressure\" or .type==\"DiskPressure\" or .type==\"PIDPressure\") and .status==\"True\")) | .metadata.name')\n\nif [ -n \"$PRESSURE_NODES\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$PRESSURE_NODES\" | wc -l)))\n    echo \"  WARNING: Nodes under resource pressure:\"\n    echo \"$PRESSURE_NODES\" | while read node; do\n        echo \"  - $node\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select((.type==\"MemoryPressure\" or .type==\"DiskPressure\" or .type==\"PIDPressure\") and .status==\"True\") | \"    \\(.type): \\(.message // \"N/A\")\"'\n    done\nfi\n\n# Check node resource utilization if metrics-server is available\nif $CLI top nodes &> /dev/null; then\n    echo \"Node Resource Utilization:\"\n    $CLI top nodes\nfi\n```\n\n### 6. Check Pod Health Across All Namespaces\n\nIdentify problematic pods:\n\n```bash\necho \"Checking Pod Health...\"\n\n# Get pods that are not Running or Completed\nFAILED_PODS=$($CLI get pods --all-namespaces -o json | jq -r '.items[] | select(.status.phase != \"Running\" and .status.phase != \"Succeeded\") | \"\\(.metadata.namespace)/\\(.metadata.name) [\\(.status.phase)]\"')\n\nif [ -n \"$FAILED_PODS\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$FAILED_PODS\" | wc -l)))\n    echo \" CRITICAL: Pods in failed/pending state:\"\n    echo \"$FAILED_PODS\"\nfi\n\n# Check for pods with restarts\nHIGH_RESTART_PODS=$($CLI get pods --all-namespaces -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .restartCount > 5) | \"\\(.metadata.namespace)/\\(.metadata.name) [Restarts: \\(.status.containerStatuses[0].restartCount)]\"')\n\nif [ -n \"$HIGH_RESTART_PODS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$HIGH_RESTART_PODS\" | wc -l)))\n    echo \"  WARNING: Pods with high restart count (>5):\"\n    echo \"$HIGH_RESTART_PODS\"\nfi\n\n# Check for CrashLoopBackOff pods\nCRASHLOOP_PODS=$($CLI get pods --all-namespaces -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .state.waiting?.reason == \"CrashLoopBackOff\") | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n \"$CRASHLOOP_PODS\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$CRASHLOOP_PODS\" | wc -l)))\n    echo \" CRITICAL: Pods in CrashLoopBackOff:\"\n    echo \"$CRASHLOOP_PODS\"\nfi\n\n# Check for ImagePullBackOff pods\nIMAGE_PULL_PODS=$($CLI get pods --all-namespaces -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .state.waiting?.reason == \"ImagePullBackOff\" or .state.waiting?.reason == \"ErrImagePull\") | \"\\(.metadata.namespace)/\\(.metadata.name)\"')\n\nif [ -n \"$IMAGE_PULL_PODS\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$IMAGE_PULL_PODS\" | wc -l)))\n    echo \" CRITICAL: Pods with image pull errors:\"\n    echo \"$IMAGE_PULL_PODS\"\nfi\n```\n\n### 7. Check Deployment/StatefulSet/DaemonSet Health\n\nVerify workload controllers:\n\n```bash\necho \"Checking Deployments...\"\n\n# Check deployments with unavailable replicas\nUNHEALTHY_DEPLOYMENTS=$($CLI get deployments --all-namespaces -o json | jq -r '.items[] | select(.status.unavailableReplicas > 0 or .status.replicas != .status.readyReplicas) | \"\\(.metadata.namespace)/\\(.metadata.name) [Ready: \\(.status.readyReplicas // 0)/\\(.spec.replicas)]\"')\n\nif [ -n \"$UNHEALTHY_DEPLOYMENTS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$UNHEALTHY_DEPLOYMENTS\" | wc -l)))\n    echo \"  WARNING: Deployments with unavailable replicas:\"\n    echo \"$UNHEALTHY_DEPLOYMENTS\"\nfi\n\necho \"Checking StatefulSets...\"\n\nUNHEALTHY_STATEFULSETS=$($CLI get statefulsets --all-namespaces -o json | jq -r '.items[] | select(.status.replicas != .status.readyReplicas) | \"\\(.metadata.namespace)/\\(.metadata.name) [Ready: \\(.status.readyReplicas // 0)/\\(.spec.replicas)]\"')\n\nif [ -n \"$UNHEALTHY_STATEFULSETS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$UNHEALTHY_STATEFULSETS\" | wc -l)))\n    echo \"  WARNING: StatefulSets with unavailable replicas:\"\n    echo \"$UNHEALTHY_STATEFULSETS\"\nfi\n\necho \"Checking DaemonSets...\"\n\nUNHEALTHY_DAEMONSETS=$($CLI get daemonsets --all-namespaces -o json | jq -r '.items[] | select(.status.numberReady != .status.desiredNumberScheduled) | \"\\(.metadata.namespace)/\\(.metadata.name) [Ready: \\(.status.numberReady)/\\(.status.desiredNumberScheduled)]\"')\n\nif [ -n \"$UNHEALTHY_DAEMONSETS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$UNHEALTHY_DAEMONSETS\" | wc -l)))\n    echo \"  WARNING: DaemonSets with unavailable pods:\"\n    echo \"$UNHEALTHY_DAEMONSETS\"\nfi\n```\n\n### 8. Check Persistent Volume Claims\n\nCheck for storage issues:\n\n```bash\necho \"Checking Persistent Volume Claims...\"\n\n# Get PVCs that are not Bound\nPENDING_PVCS=$($CLI get pvc --all-namespaces -o json | jq -r '.items[] | select(.status.phase != \"Bound\") | \"\\(.metadata.namespace)/\\(.metadata.name) [\\(.status.phase)]\"')\n\nif [ -n \"$PENDING_PVCS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$PENDING_PVCS\" | wc -l)))\n    echo \"  WARNING: PVCs not in Bound state:\"\n    echo \"$PENDING_PVCS\"\nfi\n```\n\n### 9. Check Critical Namespace Health\n\nFor OpenShift, check critical namespaces:\n\n```bash\nif [ \"$CLUSTER_TYPE\" = \"OpenShift\" ]; then\n    echo \"Checking Critical Namespaces...\"\n\n    CRITICAL_NAMESPACES=\"openshift-kube-apiserver openshift-etcd openshift-authentication openshift-console openshift-monitoring\"\n\n    for ns in $CRITICAL_NAMESPACES; do\n        # Check if namespace exists\n        if ! $CLI get namespace \"$ns\" &> /dev/null; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n            echo \" CRITICAL: Critical namespace missing: $ns\"\n            continue\n        fi\n\n        # Check for failed pods in critical namespace\n        FAILED_IN_NS=$($CLI get pods -n \"$ns\" -o json | jq -r '.items[] | select(.status.phase != \"Running\" and .status.phase != \"Succeeded\") | .metadata.name')\n\n        if [ -n \"$FAILED_IN_NS\" ]; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$FAILED_IN_NS\" | wc -l)))\n            echo \" CRITICAL: Failed pods in critical namespace $ns:\"\n            echo \"$FAILED_IN_NS\" | while read pod; do\n                echo \"  - $pod\"\n            done\n        fi\n    done\nfi\n```\n\n### 10. Check Events for Recent Errors\n\nLook for recent warning/error events:\n\n```bash\necho \"Checking Recent Events...\"\n\n# Get events from last 30 minutes with Warning or Error type\nRECENT_WARNINGS=$($CLI get events --all-namespaces --field-selector type=Warning -o json | jq -r --arg since \"$(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ 2>/dev/null || date -u -v-30M +%Y-%m-%dT%H:%M:%SZ)\" '.items[] | select(.lastTimestamp > $since) | \"\\(.lastTimestamp) [\\(.involvedObject.namespace)/\\(.involvedObject.name)]: \\(.message)\"' | head -20)\n\nif [ -n \"$RECENT_WARNINGS\" ]; then\n    echo \"  Recent Warning Events (last 30 minutes):\"\n    echo \"$RECENT_WARNINGS\"\nfi\n```\n\n### 11. Generate Summary Report\n\nCreate a summary of findings:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"Cluster Health Check Summary\"\necho \"===============================================\"\necho \"Cluster Type: $CLUSTER_TYPE\"\necho \"Cluster Version: $CLUSTER_VERSION\"\necho \"Check Time: $(date)\"\necho \"\"\necho \"Results:\"\necho \"  Critical Issues: $CRITICAL_ISSUES\"\necho \"  Warnings: $WARNING_ISSUES\"\necho \"\"\n\nif [ $CRITICAL_ISSUES -eq 0 ] && [ $WARNING_ISSUES -eq 0 ]; then\n    echo \" Cluster is healthy - no issues detected\"\n    exit 0\nelif [ $CRITICAL_ISSUES -gt 0 ]; then\n    echo \" Cluster has CRITICAL issues requiring immediate attention\"\n    exit 1\nelse\n    echo \"  Cluster has warnings - monitoring recommended\"\n    exit 0\nfi\n```\n\n### 12. Optional: Export to JSON Format\n\nIf `--output-format json` is specified, export findings as JSON:\n\n```json\n{\n  \"cluster\": {\n    \"type\": \"OpenShift\",\n    \"version\": \"4.21.0\",\n    \"checkTime\": \"2025-10-31T12:00:00Z\"\n  },\n  \"summary\": {\n    \"criticalIssues\": 2,\n    \"warnings\": 5,\n    \"healthy\": false\n  },\n  \"findings\": {\n    \"clusterOperators\": {\n      \"degraded\": [\"authentication\", \"monitoring\"],\n      \"unavailable\": [],\n      \"progressing\": [\"network\"]\n    },\n    \"nodes\": {\n      \"notReady\": [\"worker-1\"],\n      \"schedulingDisabled\": [\"worker-2\"],\n      \"underPressure\": []\n    },\n    \"pods\": {\n      \"failed\": [\"namespace/pod-1\", \"namespace/pod-2\"],\n      \"crashLooping\": [],\n      \"imagePullErrors\": [\"namespace/pod-3\"]\n    },\n    \"workloads\": {\n      \"unhealthyDeployments\": [],\n      \"unhealthyStatefulSets\": [],\n      \"unhealthyDaemonSets\": []\n    },\n    \"storage\": {\n      \"pendingPVCs\": []\n    }\n  }\n}\n```\n\n## Examples\n\n### Example 1: Basic health check\n```\n/openshift:cluster-health-check\n```\n\nOutput:\n```\nChecking Cluster Operators...\n All cluster operators healthy\n\nChecking Node Health...\n  WARNING: Nodes with scheduling disabled:\n  - ip-10-0-51-201.us-east-2.compute.internal\n\nChecking Pod Health...\n All pods healthy\n\n...\n\n===============================================\nCluster Health Check Summary\n===============================================\nCluster Type: OpenShift\nCluster Version: 4.21.0\nCheck Time: 2025-10-31 12:00:00\n\nResults:\n  Critical Issues: 0\n  Warnings: 1\n\n  Cluster has warnings - monitoring recommended\n```\n\n### Example 2: Verbose health check\n```\n/openshift:cluster-health-check --verbose\n```\n\n### Example 3: JSON output for automation\n```\n/openshift:cluster-health-check --output-format json\n```\n\n## Return Value\n\nThe command returns different exit codes based on findings:\n\n- **Exit 0**: No critical issues found (cluster is healthy or has only warnings)\n- **Exit 1**: Critical issues detected requiring immediate attention\n\n**Output Format**:\n- **Text** (default): Human-readable report with emoji indicators\n- **JSON**: Structured data suitable for parsing/automation\n\n## Common Issues and Remediation\n\n### Degraded Cluster Operators\n\n**Symptoms**: Cluster operators showing Degraded=True or Available=False\n\n**Investigation**:\n```bash\noc get clusteroperator <operator-name> -o yaml\noc logs -n openshift-<operator-namespace> -l app=<operator-name>\n```\n\n**Remediation**: Check operator logs and events for specific errors\n\n### Nodes Not Ready\n\n**Symptoms**: Nodes in NotReady state\n\n**Investigation**:\n```bash\noc describe node <node-name>\noc get events --field-selector involvedObject.name=<node-name>\n```\n\n**Remediation**: Common causes include network issues, disk pressure, or kubelet problems\n\n### Pods in CrashLoopBackOff\n\n**Symptoms**: Pods continuously restarting\n\n**Investigation**:\n```bash\noc logs <pod-name> -n <namespace> --previous\noc describe pod <pod-name> -n <namespace>\n```\n\n**Remediation**: Check application logs, resource limits, and configuration\n\n### ImagePullBackOff Errors\n\n**Symptoms**: Pods unable to pull container images\n\n**Investigation**:\n```bash\noc describe pod <pod-name> -n <namespace>\n```\n\n**Remediation**: Verify image name, registry credentials, and network connectivity\n\n## Security Considerations\n\n- **Read-only access**: This command only reads cluster state, no modifications\n- **Sensitive data**: Be cautious when sharing reports as they may contain cluster topology information\n- **RBAC requirements**: Ensure user has appropriate permissions for all resource types checked\n\n## See Also\n\n- OpenShift Documentation: https://docs.openshift.com/container-platform/latest/support/troubleshooting/\n- Kubernetes Troubleshooting: https://kubernetes.io/docs/tasks/debug/\n- Related commands: `/prow-job:analyze-test-failure`, `/must-gather:analyze`\n\n## Notes\n\n- The command checks cluster state at a point in time; transient issues may not be detected\n- For OpenShift clusters, cluster operator checks are performed\n- For vanilla Kubernetes, cluster operator checks are skipped\n- Resource utilization checks require metrics-server to be installed\n- Some checks may be skipped if user lacks sufficient permissions"
              },
              {
                "name": "/crd-review",
                "description": "Review Kubernetes CRDs against Kubernetes and OpenShift API conventions",
                "path": "plugins/openshift/commands/crd-review.md",
                "frontmatter": {
                  "description": "Review Kubernetes CRDs against Kubernetes and OpenShift API conventions",
                  "argument-hint": "[repository-path]"
                },
                "content": "## Name\nopenshift:crd-review\n\n## Synopsis\n```\n/openshift:crd-review [repository-path]\n```\n\n## Description\n\nThe `openshift:crd-review` command analyzes Go Kubernetes Custom Resource Definitions (CRDs) in a repository against both:\n- **Kubernetes API Conventions** as defined in the [Kubernetes community guidelines](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md)\n- **OpenShift API Conventions** as defined in the [OpenShift development guide](https://github.com/openshift/enhancements/blob/master/dev-guide/api-conventions.md)\n\nThis command helps ensure CRDs follow best practices for:\n- API naming conventions and patterns\n- Resource structure and field organization\n- Status field design and patterns\n- Field types and validation\n- Documentation standards\n- OpenShift-specific requirements\n\nThe review covers Go API type definitions, providing actionable feedback to improve API design.\n\n## Key Convention Checks\n\n### Kubernetes API Conventions\n\n#### Naming Conventions\n- **Resource Names**: Must follow DNS label format (lowercase, alphanumeric, hyphens)\n- **Field Names**: PascalCase for Go, camelCase for JSON\n- **Avoid**: Abbreviations, underscores, ambiguous names\n- **Include**: Units/types in field names when needed (e.g., `timeoutSeconds`)\n\n#### API Structure\n- **Required Fields**: Every API object must embed a `k8s.io/apimachinery/pkg/apis/meta/v1` `TypeMeta` struct\n- **Metadata**: Every API object must include a `k8s.io/apimachinery/pkg/apis/meta/v1` `ObjectMeta` struct called `metadata`\n- **Spec/Status Separation**: Clear separation between desired state (spec) and observed state (status)\n\n#### Status Field Design\n- **Conditions**: Must include conditions array with:\n  - `type`: Clear, human-readable condition type\n  - `status`: `True`, `False`, or `Unknown`\n  - `reason`: Machine-readable reason code\n  - `message`: Human-readable message\n  - `lastTransitionTime`: RFC 3339 timestamp\n\n#### Field Types\n- **Integers**: Prefer `int32` over `int64`\n- **Avoid**: Unsigned integers, floating-point values\n- **Enums**: Use string constants, not numeric values\n- **Optional Fields**: Use pointers in Go\n\n#### Versioning\n- **Group Names**: Use domain format (e.g., `myapp.example.com`)\n- **Version Strings**: Must match DNS label format (e.g., `v1`, `v1beta1`)\n- **Migration**: Provide clear paths between versions\n\n### OpenShift API Conventions\n\n#### Configuration vs Workload APIs\n- **Configuration APIs**: Typically cluster-scoped, manage cluster behavior\n- **Workload APIs**: Usually namespaced, user-facing resources\n\n#### Field Design\n- **Avoid Boolean Fields**: Use enumerations that describe end-user behavior instead of binary true/false\n  -  Bad: `paused: true`\n  -  Good: `lifecycle: \"Paused\"` with enum values `[\"Paused\", \"Active\"]`\n- **Object References**: Use specific types, omit \"Ref\" suffix\n- **Clear Semantics**: Each field should have one clear purpose\n\n#### Documentation Requirements\n- **Godoc Comments**: Comprehensive documentation for all exported types and fields\n- **JSON Field Names**: Use JSON names in documentation (not Go names)\n- **User-Facing**: Write for users, not just developers\n- **Explain Interactions**: Document how fields interact with each other\n\n#### Validation\n- **Kubebuilder Tags**: Use validation markers (`+kubebuilder:validation:*`)\n- **Enum Values**: Explicitly define allowed values\n- **Field Constraints**: Define minimums, maximums, patterns\n- **Meaningful Errors**: Validation messages should guide users\n\n#### Union Types\n- **Discriminated Unions**: Use a discriminator field to select variant\n- **Optional Pointers**: All union members should be optional pointers\n- **Validation**: Ensure exactly one union member is set\n\n## Implementation\n\nThe command performs the following analysis workflow:\n\n1. **Repository Discovery**\n   - Find Go API types (typically in `api/`, `pkg/apis/` directories)\n   - Identify CRD generation markers (`+kubebuilder` comments)\n\n2. **Kubernetes Convention Validation**\n   - **Naming validation**: Check resource names, field names, condition types\n   - **Structure validation**: Verify required fields, metadata, spec/status separation\n   - **Status validation**: Ensure conditions array, proper condition structure\n   - **Field type validation**: Check integer types, avoid floats, validate enums\n   - **Versioning validation**: Verify group names and version strings\n\n3. **OpenShift Convention Validation**\n   - **API classification**: Identify configuration vs workload APIs\n   - **Field design**: Flag boolean fields, check enumerations\n   - **Documentation**: Verify Godoc comments, user-facing descriptions\n   - **Validation markers**: Check kubebuilder validation tags\n   - **Union types**: Validate discriminated union patterns\n\n4. **Report Generation**\n   - List all findings with severity levels (Critical, Warning, Info)\n   - Provide specific file and line references\n   - Include remediation suggestions\n   - Highlight whether a suggested change might lead to breaking API changes\n   - Link to relevant convention documentation\n\n## Output Format\n\nThe command generates a structured report with:\n- **Summary**: Overview of findings by severity\n- **Kubernetes Findings**: Issues related to upstream conventions\n- **OpenShift Findings**: Issues related to OpenShift-specific patterns\n- **Recommendations**: Actionable steps to improve API design\n- **openshift/api crd-command reference**: Add a prominent note notifying the user of the existence of the openshift/api repository's api-review command (https://github.com/openshift/api/blob/master/.claude/commands/api-review.md) for PR reviews against that repository.  \n\nEach finding includes:\n- Severity level ( Critical,  Warning,  Info)\n- File location and line number\n- Description of the issue\n- Remediation suggestion\n- Link to relevant documentation\n\n## Examples\n\n### Example 1: Review current repository\n```\n/crd-review\n```\nAnalyzes CRDs in the current working directory.\n\n### Example 2: Review specific repository\n```\n/crd-review /path/to/operator-project\n```\nAnalyzes CRDs in the specified directory.\n\n### Example 3: Review with detailed output\nThe command automatically provides detailed output including:\n- All CRD files found\n- Go API type definitions\n- Compliance summary\n- Specific violations with file references\n\n## Common Findings\n\n### Kubernetes Convention Issues\n\n#### Boolean vs Enum Fields\n**Issue**: Using boolean where enum is better\n```go\n//  Bad\ntype MySpec struct {\n    Enabled bool `json:\"enabled\"`\n}\n\n//  Good\ntype MySpec struct {\n    // State defines the operational state\n    // Valid values are: \"Enabled\", \"Disabled\", \"Auto\"\n    // +kubebuilder:validation:Enum=Enabled;Disabled;Auto\n    State string `json:\"state\"`\n}\n```\n\n#### Missing Status Conditions\n**Issue**: Status without conditions array\n```go\n//  Bad\ntype MyStatus struct {\n    Ready bool `json:\"ready\"`\n}\n\n//  Good\ntype MyStatus struct {\n    // Conditions represent the latest available observations\n    // +listType=map\n    // +listMapKey=type\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n}\n```\n\n#### Improper Field Naming\n**Issue**: Ambiguous or abbreviated names\n```go\n//  Bad\ntype MySpec struct {\n    Timeout int `json:\"timeout\"` // Ambiguous unit\n    Cnt     int `json:\"cnt\"`     // Abbreviation\n}\n\n//  Good\ntype MySpec struct {\n    // TimeoutSeconds is the timeout in seconds\n    // +kubebuilder:validation:Minimum=1\n    TimeoutSeconds int32 `json:\"timeoutSeconds\"`\n\n    // Count is the number of replicas\n    // +kubebuilder:validation:Minimum=0\n    Count int32 `json:\"count\"`\n}\n```\n\n### OpenShift Convention Issues\n\n#### Missing Documentation\n**Issue**: Exported fields without Godoc\n```go\n//  Bad\ntype MySpec struct {\n    Field string `json:\"field\"`\n}\n\n//  Good\ntype MySpec struct {\n    // field specifies the configuration field for...\n    // This value determines how the operator will...\n    // Valid values include...\n    Field string `json:\"field\"`\n}\n```\n\n#### Missing Validation\n**Issue**: Fields without kubebuilder validation\n```go\n//  Bad\ntype MySpec struct {\n    Mode string `json:\"mode\"`\n}\n\n//  Good\ntype MySpec struct {\n    // mode defines the operational mode\n    // +kubebuilder:validation:Enum=Standard;Advanced;Debug\n    // +kubebuilder:validation:Required\n    Mode string `json:\"mode\"`\n}\n```\n\n## Best Practices\n\n1. **Start with Conventions**: Review conventions before writing APIs\n2. **Use Code Generation**: Leverage controller-gen and kubebuilder markers\n3. **Document Early**: Write Godoc comments as you define types\n4. **Validate Everything**: Add validation markers for all fields\n5. **Review Regularly**: Run this command during development and before PRs\n6. **Follow Examples**: Study well-designed APIs in OpenShift core\n\n## Arguments\n\n- **repository-path** (optional): Path to repository containing CRDs. Defaults to current working directory.\n\n## Exit Codes\n\n- **0**: Analysis completed successfully\n- **1**: Error during analysis (e.g., invalid path, no CRDs found)\n\n## See Also\n\n- [Kubernetes API Conventions](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md)\n- [OpenShift API Conventions](https://github.com/openshift/enhancements/blob/master/dev-guide/api-conventions.md)\n- [Kubebuilder Documentation](https://book.kubebuilder.io/)\n- [Controller Runtime API](https://pkg.go.dev/sigs.k8s.io/controller-runtime)"
              },
              {
                "name": "/create-cluster",
                "description": "Extract OpenShift installer from release image and create an OCP cluster",
                "path": "plugins/openshift/commands/create-cluster.md",
                "frontmatter": {
                  "description": "Extract OpenShift installer from release image and create an OCP cluster",
                  "argument-hint": "[release-image] [platform] [options]"
                },
                "content": "## Name\nopenshift:create-cluster\n\n## Synopsis\n```\n/openshift:create-cluster [release-image] [platform] [options]\n```\n\n## Description\n\nThe `create-cluster` command automates the process of extracting the OpenShift installer from a release image (if not already present) and creating a new OpenShift Container Platform (OCP) cluster. It handles installer extraction from OCP release images, configuration preparation, and cluster creation in a streamlined workflow.\n\nThis command is useful for:\n- Setting up development/test clusters quickly\n\n##  When to Use This Tool\n\n**IMPORTANT**: This is a last resort tool for advanced use cases. For most development workflows, you should use one of these better alternatives:\n\n### Recommended Alternatives\n\n1. **Cluster Bot**: Request ephemeral test clusters without managing infrastructure\n   - No cloud credentials needed\n   - Supports dependent PR testing\n   - Automatically cleaned up\n\n2. **Gangway**\n\n3. **Multi-PR Testing in CI**: Test multiple dependent PRs together using `/test-with` commands\n\n### When to Use create-cluster\n\nOnly use this command when:\n- You need full control over cluster configuration\n- You're testing installer changes that aren't suitable for CI\n- You need a long-lived development cluster on your own cloud account\n- The alternatives don't meet your specific requirements\n\n**Note**: This command requires significant setup (cloud credentials, pull secrets, DNS configuration, understanding of OCP versions). If you're new to OpenShift development, start with Cluster Bot or Gangway instead.\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **OpenShift CLI (`oc`)**: Required to extract the installer from the release image\n   - Install from: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n   - Or use your package manager: `brew install openshift-cli` (macOS)\n   - Verify with: `oc version`\n\n2. **Cloud Provider Credentials** configured for your chosen platform:\n   - **AWS**: `~/.aws/credentials` configured with appropriate permissions\n   - **Azure**: Azure CLI authenticated (`az login`)\n   - **GCP**: The command will guide you through service account setup (either using an existing service account JSON or creating a new one)\n   - **vSphere**: vCenter credentials\n   - **OpenStack**: clouds.yaml configured\n\n3. **Pull Secret**: Download from [Red Hat Console](https://console.redhat.com/openshift/install/pull-secret)\n\n4. **Domain/DNS Configuration**:\n   - AWS: Route53 hosted zone\n   - Other platforms: Appropriate DNS setup\n\n## Arguments\n\nThe command accepts arguments in multiple ways:\n\n### Positional Arguments\n```\n/openshift:create-cluster [release-image] [platform]\n```\n\n### Interactive Mode\nIf arguments are not provided, the command will interactively prompt for:\n- OpenShift release image\n- Platform (aws, azure, gcp, vsphere, openstack, none/baremetal)\n- Cluster name\n- Base domain\n- Pull secret location\n\n### Argument Details\n\n- **release-image** (required): OpenShift release image to extract the installer from\n  - Production release: `quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64`\n  - CI build: `registry.ci.openshift.org/ocp/release:4.21.0-0.ci-2025-10-27-031915`\n  - Stable release: `quay.io/openshift-release-dev/ocp-release:4.20.1-x86_64`\n  - The command will prompt for this if not provided\n\n- **platform** (optional): Target platform for the cluster\n  - `aws`: Amazon Web Services\n  - `azure`: Microsoft Azure\n  - `gcp`: Google Cloud Platform\n  - `vsphere`: VMware vSphere\n  - `openstack`: OpenStack\n  - `none`: Bare metal / platform-agnostic\n  - Default: Prompts user to select\n\n- **cluster-name** (optional): Name for the cluster\n  - Default: `ocp-cluster`\n  - Must be DNS-compatible\n\n- **base-domain** (required): Base domain for the cluster\n  - Example: `example.com`  Cluster API will be `api.{cluster-name}.{base-domain}`\n\n- **pull-secret** (required): Path to pull secret file\n  - User will be prompted to provide the path\n\n- **installer-dir** (optional): Directory to store/find installer binaries\n  - Default: `~/.openshift-installers`\n\n## Implementation\n\nThe command performs the following steps:\n\n### 1. Validate Prerequisites\n\nCheck that required tools and credentials are available:\n- Verify `oc` CLI is installed and available\n- Verify cloud provider credentials are configured (if applicable)\n- Confirm domain/DNS requirements\n\nIf any prerequisites are missing, provide clear instructions on how to configure them.\n\n### 2. Get Release Image from User\n\nIf not provided as an argument, **prompt the user** for the OpenShift release image:\n\n```\nPlease provide the OpenShift release image:\n\nExamples:\n  - Production release: quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64\n  - CI build:          registry.ci.openshift.org/ocp/release:4.21.0-0.ci-2025-10-27-031915\n  - Stable release:    quay.io/openshift-release-dev/ocp-release:4.20.1-x86_64\n\nRelease image:\n```\n\nStore the user's input as `$RELEASE_IMAGE`.\n\n**Extract version from image** for naming:\n```bash\n# Parse version from image tag (e.g., \"4.21.0-ec.2\" or \"4.21.0-0.ci-2025-10-27-031915\")\nVERSION=$(echo \"$RELEASE_IMAGE\" | grep -oE '[0-9]+\\.[0-9]+\\.[0-9]+[^\"]*' | head -1)\n```\n\n### 3. Determine Installer Location and Extract if Needed\n\n```bash\nINSTALLER_DIR=\"${installer-dir:-$HOME/.openshift-installers}\"\nINSTALLER_PATH=\"$INSTALLER_DIR/openshift-install-${VERSION}\"\n```\n\n**Check if installer directory exists**:\n- If `$INSTALLER_DIR` does not exist:\n  - **Ask user for confirmation**: \"The installer directory `$INSTALLER_DIR` does not exist. Would you like to create it?\"\n  - If user confirms (yes): Create the directory with `mkdir -p \"$INSTALLER_DIR\"`\n  - If user declines (no): Exit with error message suggesting an alternative path\n\n**Check if the installer already exists** at `$INSTALLER_PATH`:\n- If present: Verify it works with `\"$INSTALLER_PATH\" version`\n  - If version matches the release image: Skip extraction\n  - If different or fails: Proceed with extraction\n- If not present: Proceed with extraction\n\n**Extract installer from release image**:\n\n1. **Verify `oc` CLI is available**:\n   ```bash\n   if ! command -v oc &> /dev/null; then\n       echo \"Error: 'oc' CLI not found. Please install the OpenShift CLI.\"\n       exit 1\n   fi\n   ```\n\n2. **Extract the installer binary**:\n   ```bash\n   oc adm release extract \\\n       --tools \\\n       --from=\"$RELEASE_IMAGE\" \\\n       --to=\"$INSTALLER_DIR\"\n   ```\n\n   This extracts the `openshift-install` binary and other tools from the release image.\n\n3. **Locate and rename the extracted installer**:\n   ```bash\n   # The extract command creates a tar.gz with the tools\n   # Find the most recently extracted openshift-install tar (compatible with both GNU and BSD find)\n   INSTALLER_TAR=$(find \"$INSTALLER_DIR\" -name \"openshift-install-*.tar.gz\" -type f -exec ls -t {} + | head -1)\n\n   # Extract from tar and rename\n   cd \"$INSTALLER_DIR\"\n   tar -xzf \"$INSTALLER_TAR\" openshift-install\n   mv openshift-install \"openshift-install-${VERSION}\"\n   chmod +x \"openshift-install-${VERSION}\"\n\n   # Clean up the tar file\n   rm \"$INSTALLER_TAR\"\n   ```\n\n4. **Verify the installer**:\n   ```bash\n   \"$INSTALLER_PATH\" version\n   ```\n\n   Expected output should show the version matching `$VERSION`.\n\n### 4. Prepare Installation Directory\n\nCreate a clean installation directory:\n```bash\nINSTALL_DIR=\"${cluster-name}-install-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$INSTALL_DIR\"\ncd \"$INSTALL_DIR\"\n```\n\n### 5. Collect Required Information and Generate install-config.yaml\n\n**IMPORTANT**: Do NOT run the installer interactively. Instead, collect all required information from the user and generate the install-config.yaml programmatically.\n\n**Step 5.1: Collect Information**\n\nPrompt the user for the following information (if not already provided as arguments):\n\n1. **SSH Public Key**:\n   - Check for existing SSH keys: `ls -la ~/.ssh/*.pub`\n   - Ask user to select from available keys or specify path\n   - Default: `~/.ssh/id_rsa.pub`\n\n2. **Platform** (if not provided as argument):\n   - Ask user to select: aws, azure, gcp, vsphere, openstack, none\n\n3. **Platform-specific details**:\n   - For AWS:\n     - Region (e.g., us-east-1, us-west-2)\n   - For Azure:\n     - Region (e.g., centralus, eastus)\n     - Cloud name (e.g., AzurePublicCloud)\n   - For GCP:\n     - Follow the **GCP Service Account Setup** (see Step 5.2a below)\n     - Project ID\n     - Region (e.g., us-central1)\n   - For other platforms: collect required platform-specific info\n\n4. **Base Domain**:\n   - Ask for base domain (e.g., example.com, devcluster.openshift.com)\n   - Validate that domain is configured (e.g., Route53 hosted zone for AWS)\n\n5. **Cluster Name**:\n   - Ask for cluster name or use default: `ocp-cluster`\n   - Validate DNS compatibility (lowercase, hyphens only)\n\n6. **Pull Secret**:\n   - **IMPORTANT**: Always ask user to provide the path to their pull secret file\n   - Do NOT use default paths like `~/pull-secret.txt` or `~/Downloads/pull-secret.txt`\n   - Prompt: \"Please provide the path to your pull secret file (download from https://console.redhat.com/openshift/install/pull-secret):\"\n   - Read contents of pull secret file from the provided path\n\n**Step 5.2a: GCP Service Account Setup** (Only for GCP platform)\n\nIf the platform is GCP, the installer requires a service account JSON file with appropriate permissions. Present the user with two options:\n\n1. **Use an existing service account JSON file**\n2. **Create a new service account**\n\n**Ask the user**: \"Do you want to use an existing service account JSON file or create a new one?\"\n\n**Option 1: Use Existing Service Account**\n\nIf the user chooses to use an existing service account:\n- Prompt: \"Please provide the path to your GCP service account JSON file:\"\n- Store the path as `$GCP_SERVICE_ACCOUNT_PATH`\n- Verify the file exists and is valid JSON\n- Set the environment variable:\n  ```bash\n  export GOOGLE_APPLICATION_CREDENTIALS=\"$GCP_SERVICE_ACCOUNT_PATH\"\n  ```\n\n**Option 2: Create New Service Account**\n\nIf the user chooses to create a new service account:\n\n1. **Verify gcloud CLI is installed**:\n   ```bash\n   if ! command -v gcloud &> /dev/null; then\n       echo \"Error: 'gcloud' CLI not found. Please install the Google Cloud SDK.\"\n       echo \"Visit: https://cloud.google.com/sdk/docs/install\"\n       exit 1\n   fi\n   ```\n\n2. **Prompt for Kerberos ID**:\n   - Ask: \"Please provide your Kerberos ID (e.g., jsmith):\"\n   - Store as `$KERBEROS_ID`\n   - Validate it's not empty\n\n3. **Set service account name**:\n   ```bash\n   SERVICE_ACCOUNT_NAME=\"${KERBEROS_ID}-development\"\n   ```\n\n4. **Create the service account**:\n   ```bash\n   echo \"Creating service account: $SERVICE_ACCOUNT_NAME\"\n   gcloud iam service-accounts create \"$SERVICE_ACCOUNT_NAME\" --display-name=\"$SERVICE_ACCOUNT_NAME\"\n   ```\n\n5. **Extract service account details**:\n   ```bash\n   # Get service account information\n   SERVICE_ACCOUNT_JSON=\"$(gcloud iam service-accounts list --format json | jq -r '.[] | select(.name | match(\"/\\(env.SERVICE_ACCOUNT_NAME)@\"))')\"\n   SERVICE_ACCOUNT_EMAIL=\"$(jq -r .email <<< \"$SERVICE_ACCOUNT_JSON\")\"\n   PROJECT_ID=\"$(jq -r .projectId <<< \"$SERVICE_ACCOUNT_JSON\")\"\n\n   echo \"Service Account Email: $SERVICE_ACCOUNT_EMAIL\"\n   echo \"Project ID: $PROJECT_ID\"\n   ```\n\n6. **Grant required permissions**:\n   ```bash\n   echo \"Granting IAM roles to service account...\"\n\n   while IFS= read -r ROLE_TO_ADD ; do\n      echo \"Adding role: $ROLE_TO_ADD\"\n      gcloud projects add-iam-policy-binding \"$PROJECT_ID\" \\\n         --condition=\"None\" \\\n         --member=\"serviceAccount:$SERVICE_ACCOUNT_EMAIL\" \\\n         --role=\"$ROLE_TO_ADD\"\n   done << 'END_OF_ROLES'\n   roles/compute.admin\n   roles/iam.securityAdmin\n   roles/iam.serviceAccountAdmin\n   roles/iam.serviceAccountKeyAdmin\n   roles/iam.serviceAccountUser\n   roles/storage.admin\n   roles/dns.admin\n   roles/compute.loadBalancerAdmin\n   roles/iam.roleAdmin\n   END_OF_ROLES\n\n   echo \"All roles granted successfully.\"\n   ```\n\n7. **Create and download service account key**:\n   ```bash\n   KEY_FILE=\"${HOME}/.gcp/${SERVICE_ACCOUNT_NAME}-key.json\"\n   mkdir -p \"$(dirname \"$KEY_FILE\")\"\n\n   echo \"Creating service account key...\"\n   gcloud iam service-accounts keys create \"$KEY_FILE\" \\\n      --iam-account=\"$SERVICE_ACCOUNT_EMAIL\"\n\n   echo \"Service account key saved to: $KEY_FILE\"\n   ```\n\n8. **Set environment variable**:\n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=\"$KEY_FILE\"\n   echo \"GOOGLE_APPLICATION_CREDENTIALS set to: $KEY_FILE\"\n   ```\n\n9. **Store PROJECT_ID for later use** in install-config.yaml generation.\n\n**Step 5.2: Generate install-config.yaml**\n\nCreate the install-config.yaml file programmatically based on collected information:\n\n```bash\n# Read SSH public key\nSSH_KEY=$(cat \"$SSH_KEY_PATH\")\n\n# Read pull secret\nPULL_SECRET=$(cat \"$PULL_SECRET_PATH\")\n\n# Generate install-config.yaml\ncat > install-config.yaml <<EOF\napiVersion: v1\nbaseDomain: ${BASE_DOMAIN}\nmetadata:\n  name: ${CLUSTER_NAME}\ncompute:\n- name: worker\n  replicas: 3\ncontrolPlane:\n  name: master\n  replicas: 3\nnetworking:\n  networkType: OVNKubernetes\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  ${PLATFORM}:\n    region: ${REGION}\npullSecret: '${PULL_SECRET}'\nsshKey: '${SSH_KEY}'\nEOF\n```\n\n**Platform-specific configurations**:\n\nFor **AWS**:\n```yaml\nplatform:\n  aws:\n    region: us-east-1\n```\n\nFor **Azure**:\n```yaml\nplatform:\n  azure:\n    region: centralus\n    baseDomainResourceGroupName: ${RESOURCE_GROUP_NAME}\n    cloudName: AzurePublicCloud\n```\n\nFor **GCP**:\n```yaml\nplatform:\n  gcp:\n    projectID: ${PROJECT_ID}\n    region: us-central1\n```\n\nFor **None/Baremetal**:\n```yaml\nplatform:\n  none: {}\n```\n\n**IMPORTANT**: Always backup install-config.yaml after creation:\n```bash\ncp install-config.yaml install-config.yaml.backup\n```\n\nThe installer consumes this file, so the backup is essential for reference.\n\n### 6. Create the Cluster\n\nRun the installer:\n```bash\n\"$INSTALLER_PATH\" create cluster --dir=.\n```\n\nMonitor the installation progress. This typically takes 30-45 minutes.\n\n### 7. Post-Installation\n\nOnce installation completes:\n\n1. **Display kubeconfig location**:\n   ```\n   Kubeconfig: $INSTALL_DIR/auth/kubeconfig\n   ```\n\n2. **Display cluster credentials**:\n   ```\n   Console URL: https://console-openshift-console.apps.${cluster-name}.${base-domain}\n   Username: kubeadmin\n   Password: (from $INSTALL_DIR/auth/kubeadmin-password)\n   ```\n\n3. **Export KUBECONFIG** (offer to add to shell profile):\n   ```bash\n   export KUBECONFIG=\"$PWD/auth/kubeconfig\"\n   ```\n\n4. **Verify cluster access**:\n   ```bash\n   oc get nodes\n   oc get co  # cluster operators\n   ```\n\n5. **Save cluster information** to a summary file:\n   ```\n   Cluster: ${cluster-name}\n   Version: ${VERSION}\n   Release Image: ${RELEASE_IMAGE}\n   Platform: ${platform}\n   Console: https://console-openshift-console.apps.${cluster-name}.${base-domain}\n   API: https://api.${cluster-name}.${base-domain}:6443\n   Kubeconfig: $INSTALL_DIR/auth/kubeconfig\n   Created: $(date)\n   ```\n\n### 8. Error Handling\n\nIf installation fails:\n\n1. **Capture logs**: Installation logs are in `.openshift_install.log`\n2. **Provide diagnostics**: Check common failure points:\n   - Quota limits on cloud provider\n   - DNS configuration issues\n   - Invalid pull secret\n   - Network/firewall issues\n3. **Cleanup guidance**: Inform user about cleanup:\n   ```bash\n   \"$INSTALLER_PATH\" destroy cluster --dir=.\n   ```\n\n## Examples\n\n### Example 1: Basic cluster creation (interactive)\n```\n/openshift:create-cluster\n```\nThe command will prompt for release image and all necessary information.\n\n### Example 2: Create AWS cluster with production release\n```\n/openshift:create-cluster quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64 aws\n```\n\n### Example 3: Create cluster with CI build\n```\n/openshift:create-cluster registry.ci.openshift.org/ocp/release:4.21.0-0.ci-2025-10-27-031915 gcp\n```\n\n## Cleanup\n\nTo destroy the cluster after testing:\n```bash\ncd $INSTALL_DIR\n\"$INSTALLER_PATH\" destroy cluster --dir=.\n```\n\n**WARNING**: This will permanently delete all cluster resources.\n\n## Common Issues\n\n1. **Pull secret not found**:\n   - Download from https://console.redhat.com/openshift/install/pull-secret\n   - Save to a secure location of your choice\n   - Provide the path when prompted during cluster creation\n\n2. **Insufficient cloud quotas**:\n   - Check cloud provider quota limits\n   - Request quota increase if needed\n\n3. **DNS issues**:\n   - Ensure base domain is properly configured\n   - For AWS, verify Route53 hosted zone exists\n\n4. **SSH key not found**:\n   - Generate with `ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa`\n\n5. **Unauthorized access to release image**:\n   - Error: `error: unable to read image quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:...: unauthorized: access to the requested resource is not authorized`\n   - For `quay.io/openshift-release-dev/ocp-v4.0-art-dev` you can get the pull secret from https://console.redhat.com/openshift/install/pull-secret and save it in a file and provide it here.\n\n## Security Considerations\n\n- **Pull secret**: Contains authentication for Red Hat registries. Keep secure.\n- **kubeadmin password**: Stored in plaintext in auth directory. Rotate after cluster creation.\n- **kubeconfig**: Contains cluster admin credentials. Protect appropriately.\n- **Cloud credentials**: Never commit to version control.\n\n## Return Value\n\n- **Success**: Returns 0 and displays cluster information including kubeconfig path\n- **Failure**: Returns non-zero and displays error diagnostics\n\n## See Also\n\n- OpenShift Documentation: https://docs.openshift.com/container-platform/latest/installing/\n- OpenShift Install: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n- Platform-specific installation guides\n\n## Arguments:\n\n- **$1** (release-image): OpenShift release image to extract the installer from (e.g., `quay.io/openshift-release-dev/ocp-release:4.21.0-ec.2-x86_64`)\n- **$2** (platform): Target cloud platform for cluster deployment (aws, azure, gcp, vsphere, openstack, none)"
              },
              {
                "name": "/destroy-cluster",
                "description": "Destroy an OpenShift cluster created by create-cluster command",
                "path": "plugins/openshift/commands/destroy-cluster.md",
                "frontmatter": {
                  "description": "Destroy an OpenShift cluster created by create-cluster command",
                  "argument-hint": "[install-dir]"
                },
                "content": "## Name\nopenshift:destroy-cluster\n\n## Synopsis\n```\n/openshift:destroy-cluster [install-dir]\n```\n\n## Description\n\nThe `destroy-cluster` command safely destroys an OpenShift Container Platform (OCP) cluster that was previously created using the `/openshift:create-cluster` command. It locates the appropriate installer binary, verifies the cluster information, and performs cleanup of all cloud resources.\n\nThis command is useful for:\n- Cleaning up development/test clusters after testing\n- Removing failed cluster installations\n- Freeing up cloud resources and quotas\n\n** WARNING**: This operation is **irreversible** and will permanently delete:\n- All cluster resources (VMs, load balancers, storage, etc.)\n- All data stored in the cluster\n- All configuration and credentials\n- DNS records (if managed by the installer)\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **Installation directory** from the original cluster creation\n   - Contains the cluster metadata and terraform state\n   - Located at `{cluster-name}-install-{timestamp}` by default\n\n2. **OpenShift installer binary** that matches the cluster version\n   - Should be available at `~/.openshift-installers/openshift-install-{version}`\n   - Same version used to create the cluster\n\n3. **Cloud Provider Credentials** still configured and valid\n   - Same credentials used during cluster creation\n   - Must have permissions to delete resources\n\n4. **Network connectivity** to the cloud provider\n   - Required to communicate with cloud APIs\n\n## Arguments\n\n- **install-dir** (optional): Path to the cluster installation directory\n  - Default: Interactive prompt to select from available installation directories\n  - Must contain cluster metadata files (metadata.json, terraform.tfstate, etc.)\n  - Example: `./my-cluster-install-20251028-120000`\n\n## Implementation\n\nThe command performs the following steps:\n\n### 1. Locate Installation Directory\n\nIf `install-dir` is not provided:\n- Search for installation directories in the current directory\n- Look for directories matching pattern `*-install-*` or containing `.openshift_install_state.json`\n- Present a list of found directories to the user for selection\n- Allow user to manually enter a path if directory not found\n\nIf `install-dir` is provided:\n- Validate the directory exists\n- Verify it contains cluster metadata files\n\n### 2. Extract Cluster Information\n\nRead cluster details from the installation directory:\n```bash\n# Read cluster metadata\nif [ -f \"$INSTALL_DIR/metadata.json\" ]; then\n    CLUSTER_NAME=$(jq -r '.clusterName' \"$INSTALL_DIR/metadata.json\")\n    INFRA_ID=$(jq -r '.infraID' \"$INSTALL_DIR/metadata.json\")\n    PLATFORM=$(jq -r '.platform' \"$INSTALL_DIR/metadata.json\")\nfi\n\n# Try to extract version from cluster-info or log files\nVERSION=$(grep -oE 'openshift-install.*v[0-9]+\\.[0-9]+\\.[0-9]+' \"$INSTALL_DIR/.openshift_install.log\" | head -1 | grep -oE '[0-9]+\\.[0-9]+\\.[0-9]+[^\"]*' | head -1)\n```\n\n### 3. Display Cluster Information and Confirm\n\nShow the user what will be destroyed:\n```\nCluster Information:\n  Name: ${CLUSTER_NAME}\n  Infrastructure ID: ${INFRA_ID}\n  Platform: ${PLATFORM}\n  Installation Directory: ${INSTALL_DIR}\n  Version: ${VERSION}\n\n  WARNING: This will permanently destroy the cluster and all its resources!\n\nThis action will delete:\n  - All cluster VMs and compute resources\n  - Load balancers and networking resources\n  - Storage volumes and persistent data\n  - DNS records\n  - All cluster configuration\n\nAre you sure you want to destroy this cluster? (yes/no):\n```\n\n**Important**: Require the user to type \"yes\" (not just \"y\") to confirm destruction.\n\n### 4. Locate the Correct Installer\n\nFind the installer binary that matches the cluster version:\n```bash\nINSTALLER_DIR=\"${HOME}/.openshift-installers\"\nINSTALLER_PATH=\"$INSTALLER_DIR/openshift-install-${VERSION}\"\n\n# Check if the version-specific installer exists\nif [ ! -f \"$INSTALLER_PATH\" ]; then\n    echo \"Warning: Installer for version ${VERSION} not found at ${INSTALLER_PATH}\"\n    echo \"Searching for alternative installers...\"\n\n    # Look for any installer in the installers directory\n    AVAILABLE_INSTALLERS=$(find \"$INSTALLER_DIR\" -name \"openshift-install-*\" -type f 2>/dev/null)\n\n    if [ -n \"$AVAILABLE_INSTALLERS\" ]; then\n        echo \"Found installers:\"\n        echo \"$AVAILABLE_INSTALLERS\"\n        echo \"\"\n        echo \"You may use a different version installer, but this may cause issues.\"\n        echo \"Would you like to:\"\n        echo \"  1. Use an available installer from the list above\"\n        echo \"  2. Extract the correct installer from the release image\"\n        echo \"  3. Cancel the operation\"\n    else\n        echo \"No installers found. Would you like to extract the installer? (yes/no):\"\n    fi\nfi\n\n# Verify installer works\n\"$INSTALLER_PATH\" version\n```\n\n### 5. Backup Important Files (Optional)\n\nOffer to backup key files before destruction:\n```\nWould you like to backup cluster information before destroying? (yes/no):\n```\n\nIf yes, create a backup:\n```bash\nBACKUP_DIR=\"${INSTALL_DIR}-backup-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup key files\ncp \"$INSTALL_DIR/metadata.json\" \"$BACKUP_DIR/\" 2>/dev/null\ncp \"$INSTALL_DIR/auth/kubeconfig\" \"$BACKUP_DIR/\" 2>/dev/null\ncp \"$INSTALL_DIR/auth/kubeadmin-password\" \"$BACKUP_DIR/\" 2>/dev/null\ncp \"$INSTALL_DIR/.openshift_install.log\" \"$BACKUP_DIR/\" 2>/dev/null\ncp \"$INSTALL_DIR/install-config.yaml.backup\" \"$BACKUP_DIR/\" 2>/dev/null\n\necho \"Backup created at: $BACKUP_DIR\"\n```\n\n### 6. Run Cluster Destroy\n\nExecute the destroy command:\n```bash\ncd \"$INSTALL_DIR\"\n\necho \"Starting cluster destruction...\"\necho \"This may take 10-15 minutes...\"\n\n\"$INSTALLER_PATH\" destroy cluster --dir=. --log-level=debug\n\nDESTROY_EXIT_CODE=$?\n```\n\nMonitor the destruction progress and display status updates.\n\n### 7. Verify Cleanup\n\nAfter the destroy command completes:\n\n1. **Check exit code**:\n   ```bash\n   if [ $DESTROY_EXIT_CODE -eq 0 ]; then\n       echo \" Cluster destroyed successfully\"\n   else\n       echo \" Cluster destruction failed with exit code: $DESTROY_EXIT_CODE\"\n       echo \"Check logs at: $INSTALL_DIR/.openshift_install.log\"\n   fi\n   ```\n\n2. **Verify cloud resources** (platform-specific):\n   - AWS: Check for lingering resources with tag `kubernetes.io/cluster/${INFRA_ID}`\n   - Azure: Verify resource group deletion\n   - GCP: Check project for remaining resources\n\n3. **List any remaining resources**:\n   ```\n   If any resources remain, provide commands to manually clean them up.\n   ```\n\n### 8. Cleanup Installation Directory (Optional)\n\nAsk the user if they want to remove the installation directory:\n```\nThe cluster has been destroyed. Would you like to delete the installation directory? (yes/no):\n  Directory: $INSTALL_DIR\n  Size: $(du -sh \"$INSTALL_DIR\" | cut -f1)\n```\n\nIf yes:\n```bash\nrm -rf \"$INSTALL_DIR\"\necho \"Installation directory removed\"\n```\n\nIf no:\n```bash\necho \"Installation directory preserved at: $INSTALL_DIR\"\necho \"You can manually remove it later with: rm -rf $INSTALL_DIR\"\n```\n\n### 9. Display Summary\n\nShow final summary:\n```\nCluster Destruction Summary:\n  Cluster Name: ${CLUSTER_NAME}\n  Status: Successfully destroyed\n  Platform: ${PLATFORM}\n  Duration: ${DURATION}\n  Backup: ${BACKUP_DIR} (if created)\n\nNext steps:\n  - Verify your cloud console for any lingering resources\n  - Check your cloud billing to ensure resources are no longer incurring charges\n  - Remove installation directory if not already deleted: ${INSTALL_DIR}\n```\n\n## Error Handling\n\nIf destruction fails, the command should:\n\n1. **Capture error logs** from `.openshift_install.log`\n2. **Identify the failure point**:\n   - Timeout waiting for resource deletion\n   - Permission errors\n   - API rate limiting\n   - Network connectivity issues\n   - Resources locked or in use\n3. **Provide recovery options**:\n   - Retry the destroy operation\n   - Manual cleanup instructions for specific resources\n   - Contact support if critical errors occur\n\nCommon failure scenarios:\n\n**Timeout errors**:\n```bash\n# Some resources may take longer to delete\n# Retry the destroy command:\n\"$INSTALLER_PATH\" destroy cluster --dir=\"$INSTALL_DIR\"\n```\n\n**Permission errors**:\n```\nError: Cloud credentials may have expired or lack permissions\nSolution:\n  1. Verify cloud credentials are still valid\n  2. Check IAM permissions for resource deletion\n  3. Re-run the destroy command after fixing credentials\n```\n\n**Partial destruction**:\n```\nWarning: Some resources could not be deleted automatically.\n\nRemaining resources:\n  - Load balancer: ${LB_NAME}\n  - Security group: ${SG_NAME}\n  - S3 bucket: ${BUCKET_NAME}\n\nManual cleanup commands:\n  [Platform-specific commands to delete remaining resources]\n```\n\n## Examples\n\n### Example 1: Destroy cluster with interactive directory selection\n```\n/openshift:destroy-cluster\n```\nThe command will search for installation directories and prompt you to select one.\n\n### Example 2: Destroy cluster with specific directory\n```\n/openshift:destroy-cluster ./my-cluster-install-20251028-120000\n```\n\n### Example 3: Destroy cluster with full path\n```\n/openshift:destroy-cluster /home/user/clusters/test-cluster-install-20251028-120000\n```\n\n## Common Issues\n\n1. **Installation directory not found**:\n   - Ensure you're in the correct directory\n   - Provide the full path to the installation directory\n   - Check if the directory was moved or renamed\n\n2. **Installer binary not found**:\n   - The command will help you extract the correct installer\n   - Alternatively, manually place the installer in `~/.openshift-installers/`\n\n3. **Cloud credentials expired**:\n   - Refresh your cloud credentials\n   - Re-authenticate with the cloud provider CLI\n   - Re-run the destroy command\n\n4. **Resources already deleted manually**:\n   - The destroy command may fail if resources were manually deleted\n   - Check the logs and manually clean up any remaining resources\n   - Remove the installation directory manually\n\n5. **Destroy hangs or times out**:\n   - Some resources may take longer to delete (especially load balancers)\n   - Wait for the operation to complete (can take 15-30 minutes)\n   - If truly stuck, cancel and retry\n   - Check cloud console for resource status\n\n## Safety Features\n\nThis command includes several safety measures:\n\n1. **Confirmation required**: Must type \"yes\" to proceed\n2. **Cluster information displayed**: Shows what will be destroyed before proceeding\n3. **Backup option**: Offers to backup important files\n4. **Validation checks**: Verifies installation directory and metadata\n5. **Detailed logging**: All operations logged for troubleshooting\n6. **Error recovery**: Provides manual cleanup instructions if automated cleanup fails\n\n## Return Value\n\n- **Success**: Returns 0 and displays destruction summary\n- **Failure**: Returns non-zero and displays error diagnostics with recovery instructions\n\n## See Also\n\n- `/openshift:create-cluster` - Create a new OCP cluster\n- OpenShift Documentation: https://docs.openshift.com/container-platform/latest/installing/\n- Platform-specific cleanup guides\n\n## Arguments:\n\n- **$1** (install-dir): Path to the cluster installation directory created by create-cluster (optional, interactive if not provided)"
              },
              {
                "name": "/expand-test-case",
                "description": "Expand basic test ideas or existing oc commands into comprehensive test scenarios with edge cases in oc CLI or Ginkgo format",
                "path": "plugins/openshift/commands/expand-test-case.md",
                "frontmatter": {
                  "description": "Expand basic test ideas or existing oc commands into comprehensive test scenarios with edge cases in oc CLI or Ginkgo format",
                  "argument-hint": "[test-idea-or-file-or-commands] [format]"
                },
                "content": "## Name\nopenshift:expand-test-case\n\n## Synopsis\n```\n/openshift:expand-test-case [test-idea-or-file-or-commands] [format]\n```\n\n## Description\n\nThe `expand-test-case` command transforms basic test ideas or existing oc commands into comprehensive test scenarios. It accepts three types of input:\n\n1. **Test idea**: Simple description of what to test (e.g., \"verify pod deployment\")\n2. **File path**: Path to existing test file to expand (e.g., `/path/to/test.sh` or `/path/to/test.go`)\n3. **oc commands**: Direct oc CLI commands to analyze and expand (e.g., `oc create pod nginx`)\n\nThe command expands the input to cover positive flows, negative scenarios, edge cases, and boundary conditions, helping QE engineers ensure thorough test coverage.\n\nSupports two output formats:\n- **oc CLI**: Shell scripts with oc commands for manual or automated execution\n- **Ginkgo**: Go test code using Ginkgo/Gomega framework for E2E tests\n\n## Implementation\n\nThe command analyzes the input and generates comprehensive scenarios:\n\n1. **Parse Input**: Determine if input is a test idea, file path, or oc commands\n   - If file path: Read and analyze existing test code\n   - If oc commands: Parse commands to understand what's being tested\n   - If test idea: Understand the core feature or behavior\n2. **Identify Test Dimensions**: Determine coverage aspects (functionality, security, performance, edge cases)\n3. **Generate Positive Tests**: Happy path scenarios where everything works\n4. **Generate Negative Tests**: Error handling, invalid inputs, permission issues\n5. **Add Edge Cases**: Boundary values, race conditions, resource limits\n6. **Define Validation**: Clear success criteria and assertions\n7. **Format Output**: Generate in requested format (oc CLI or Ginkgo) - **MUST follow the standards in \"Test Coverage Guidelines\" section below**\n\n**CRITICAL**: All generated test scenarios MUST adhere to the coverage dimensions, best practices, and standards defined in the **\"Test Coverage Guidelines\"** section below. Use the referenced examples and patterns from OpenShift origin repository.\n\n## Test Coverage Guidelines\n\nThe command generates comprehensive test scenarios following industry best practices:\n\n**Test Coverage Dimensions:**\n- **Positive Tests**: Valid inputs and expected workflows\n- **Negative Tests**: Invalid inputs, permission errors, missing dependencies\n- **Edge Cases**: Boundary values (0, max values, empty inputs, special characters)\n- **Security Tests**: RBAC validation, security context enforcement, privilege escalation\n- **Resource Tests**: Low memory, disk pressure, network issues, rate limiting\n- **Concurrency**: Multiple operations happening simultaneously\n- **Failure Recovery**: Restart behavior, cleanup on failure\n\n**References:**\n- OpenShift Test Examples: https://github.com/openshift/origin/tree/master/test/extended\n- Ginkgo BDD Framework: https://onsi.github.io/ginkgo/\n- Test Pattern Catalog: https://github.com/openshift/origin/blob/master/test/extended/README.md\n- oc CLI Reference: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/developer-cli-commands.html\n\n**Best Practices Applied:**\n- Use stable, descriptive test names (no dynamic IDs or timestamps)\n- Ensure proper resource cleanup (prevent resource leaks)\n- Include meaningful assertions with clear failure messages\n- Isolate tests (each test creates its own resources)\n- Add appropriate timeouts to prevent hanging tests\n- Follow Ginkgo patterns: Describe/Context/It hierarchy\n- Use framework helpers: e2epod, e2enode, e2enamespace\n\n## Arguments\n\n- **$1** (test-idea-or-file-or-commands): One of:\n  - **Test idea**: Description of what to test\n  - **File path**: Path to existing test file\n  - **oc commands**: Set of oc CLI commands to analyze and expand\n- **$2** (format): Output format - \"oc CLI\" or \"Ginkgo\" (optional, will prompt if not provided)"
              },
              {
                "name": "/ironic-status",
                "description": "Check status of Ironic baremetal nodes in OpenShift cluster",
                "path": "plugins/openshift/commands/ironic-status.md",
                "frontmatter": {
                  "description": "Check status of Ironic baremetal nodes in OpenShift cluster"
                },
                "content": "## Name\nopenshift:ironic-status\n\n## Synopsis\n```\n/openshift:ironic-status\n```\n\n## Description\n\nThe `openshift:ironic-status` command checks the status of Ironic baremetal nodes in an OpenShift cluster.\n\nThis command is useful for:\n- Monitoring baremetal node health and provisioning status\n- Troubleshooting node provisioning issues\n- Verifying node enrollment and availability\n- Checking node maintenance states\n- Diagnosing baremetal infrastructure problems\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **OpenShift CLI (`oc`)**: Must be installed and configured\n   - Install from: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n   - Verify with: `oc version`\n\n2. **Active cluster connection**: Must be connected to an OpenShift cluster with baremetal nodes\n   - Verify with: `oc whoami`\n   - Ensure KUBECONFIG is set if needed\n\n3. **Sufficient permissions**: Must have access to the openshift-machine-api namespace\n   - Ability to exec into pods in openshift-machine-api namespace\n   - Read access to services and deployments\n\n4. **baremetal cluster**: The cluster must be running on baremetal infrastructure with Metal3/Ironic enabled\n   - Verify Metal3 components: `oc get pods -n openshift-machine-api | grep metal3-ironic`\n\n## Implementation\n\nThe command performs the following steps to retrieve Ironic node status:\n\n### 1. Detect Ironic Service Endpoint\n\nLocate the Ironic service in the openshift-machine-api namespace:\n\n```bash\n# Find the Ironic service (typically named metal3-state)\noc get service -n openshift-machine-api metal3-state\n\n# Get the ClusterIP of the Ironic service\nIRONIC_SERVICE=$(oc get service -n openshift-machine-api metal3-state)\nIRONIC_IP=$(oc get service -n openshift-machine-api $IRONIC_SERVICE -o jsonpath='{.spec.clusterIP}')\n\n# Ironic API typically runs on port 6385\nIRONIC_ENDPOINT=\"https://localhost:6385\"\n```\n\n### 2. Retrieve Ironic Credentials\n\nExtract authentication credentials from the baremetal operator container:\n\n```bash\n# Find the metal3-baremetal-operator pod\nBAREMETAL_OPERATOR_POD=$(oc get pods -n openshift-machine-api -l baremetal.openshift.io/cluster-baremetal-operator=metal3-baremetal-operator -o jsonpath='{.items[0].metadata.name}')\n\n# Extract username from the mounted auth volume\nIRONIC_USERNAME=$(oc exec -n openshift-machine-api $BAREMETAL_OPERATOR_POD -c metal3-baremetal-operator -- cat /auth/ironic/username)\n\n# Extract password from the mounted auth volume\nIRONIC_PASSWORD=$(oc exec -n openshift-machine-api $BAREMETAL_OPERATOR_POD -c metal3-baremetal-operator -- cat /auth/ironic/password)\n```\n\n### 3. Query Ironic Node List\n\nExecute the baremetal client command to retrieve node status:\n\n```bash\n# Find the metal3-ironic pod\n# Example: oc get pods -n openshift-machine-api\n# Output may show: metal3-65bd97647f-hbd48\n\n# Execute baremetal node list command\noc exec -n openshift-machine-api $METAL3_POD -c metal3-ironic -- \\\n  baremetal \\\n    --os-cacert /certs/ironic/tls.crt\n    --os-auth-type http_basic \\\n    --os-username \"$IRONIC_USERNAME\" \\\n    --os-password \"$IRONIC_PASSWORD\" \\\n    --os-endpoint \"$IRONIC_ENDPOINT\" \\\n    node list\n```\n\n### 4. Parse and Present Node Information\n\nThe output typically includes:\n\n- **UUID**: Unique identifier for each node\n- **Name**: Node name (usually matches BareMetalHost name)\n- **Instance UUID**: Associated instance if provisioned\n- **Power State**: Current power status (power on, power off)\n- **Provisioning State**: Current provisioning status (available, active, deploying, etc.)\n- **Maintenance**: Whether the node is in maintenance mode\n\n### 5. Error Handling\n\nHandle common error scenarios:\n\n```bash\n# Check if Metal3 components are running\nif ! oc get pods -n openshift-machine-api | grep -q metal3; then\n    echo \"Error: Metal3 components not found. This may not be a baremetal cluster.\"\n    exit 1\nfi\n\n# Check if credentials were retrieved successfully\nif [ -z \"$IRONIC_USERNAME\" ] || [ -z \"$IRONIC_PASSWORD\" ]; then\n    echo \"Error: Failed to retrieve Ironic credentials\"\n    exit 1\nfi\n\n# Handle connection failures\nif ! oc exec ... 2>&1 | grep -q \"UUID\"; then\n    echo \"Error: Failed to connect to Ironic API\"\n    echo \"Check that the Ironic service is accessible\"\n    exit 1\nfi\n```\n\n## Adaptation Guidance\n\nIf expected values don't match your environment, use these techniques to discover the correct values:\n\n### Finding the Ironic API Port\n\nIf port 6385 doesn't work, discover the actual port:\n```bash\n# Check service ports\noc get service -n openshift-machine-api metal3-state -o jsonpath='{.spec.ports[*].port}'\n\n# Then update IRONIC_ENDPOINT\nIRONIC_ENDPOINT=\"https://localhost:${DISCOVERED_PORT}\"\n```\n\n### Finding Services and Pods\n\nIf service/pod names differ from expected patterns:\n```bash\n# List all services/pods in the namespace\noc get services -n openshift-machine-api\noc get pods -n openshift-machine-api\n\n# Filter by labels\noc get services -n openshift-machine-api -l app=metal3\noc get pods -n openshift-machine-api -l app=metal3 -o jsonpath='{.items[0].metadata.name}'\n```\n\n### Finding Container Names and Credential Paths\n\nIf container names or credential paths are different:\n```bash\n# List containers in a pod\noc get pod -n openshift-machine-api <pod-name> -o jsonpath='{.spec.containers[*].name}'\n\n# Find credential files\noc exec -n openshift-machine-api $POD -c $CONTAINER -- find /auth -type f 2>/dev/null\n```\n\n### Alternative Commands\n\nIf `baremetal` command is unavailable, try:\n```bash\n# Check what's available\noc exec -n openshift-machine-api $METAL3_POD -c metal3-ironic -- which openstack\n\n# Try alternatives: \"openstack baremetal node list\" or \"ironic node-list\"\n\n# Or use curl directly\noc exec -n openshift-machine-api $METAL3_POD -c metal3-ironic -- \\\n  curl -k -u \"$IRONIC_USERNAME:$IRONIC_PASSWORD\" \"$IRONIC_ENDPOINT/v1/nodes\"\n```\n\n**General strategy:** When commands fail, list resources with `oc get`, check labels with `-l`, inspect with `oc describe`, and explore interactively with `oc exec`.\n\n## Return Value\n\nThe command outputs a table with the following columns:\n\n- **Format**: ASCII table with columns for UUID, Name, Instance UUID, Power State, Provisioning State, and Maintenance\n\n**Example output:**\n```\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name                   | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n| 12345678-1234-1234-1234-123456789012 | openshift-worker-0     | abcdef12-3456-7890-abcd-ef1234567890 | power on    | active             | False       |\n| 23456789-2345-2345-2345-234567890123 | openshift-worker-1     | bcdef123-4567-8901-bcde-f12345678901 | power on    | active             | False       |\n| 34567890-3456-3456-3456-345678901234 | openshift-worker-2     | None                                 | power off   | available          | False       |\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n```\n\n**Exit codes:**\n- **0**: Successfully retrieved and displayed node status\n- **1**: Error occurred (Metal3 not found, credential retrieval failed, connection error)\n\n## Examples\n\n### Example 1: Basic usage\n\n```\n/openshift:ironic-status\n```\n\nOutput:\n```\nDetecting Ironic endpoint...\nFound Ironic service: metal3-state at 172.30.123.45:6385\n\nRetrieving Ironic credentials...\nSuccessfully retrieved credentials from baremetal operator\n\nQuerying Ironic node list...\n\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n| UUID                                 | Name                   | Instance UUID                        | Power State | Provisioning State | Maintenance |\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n| 12345678-1234-1234-1234-123456789012 | openshift-worker-0     | abcdef12-3456-7890-abcd-ef1234567890 | power on    | active             | False       |\n| 23456789-2345-2345-2345-234567890123 | openshift-worker-1     | bcdef123-4567-8901-bcde-f12345678901 | power on    | active             | False       |\n| 34567890-3456-3456-3456-345678901234 | openshift-worker-2     | None                                 | power off   | available          | False       |\n+--------------------------------------+------------------------+--------------------------------------+-------------+--------------------+-------------+\n\nSummary: 3 nodes total (2 active, 1 available)\n```\n\n## Common Provisioning States\n\nUnderstanding the provisioning states:\n\n- **available**: Node is ready to be provisioned\n- **active**: Node is provisioned and in use\n- **deploying**: Node is currently being provisioned\n- **deploy failed**: Provisioning attempt failed\n- **cleaning**: Node is being cleaned for reuse\n- **manageable**: Node is manageable but not available for deployment\n- **inspect failed**: Introspection failed\n- **error**: Node is in an error state\n\n## Troubleshooting\n\n### Metal3 Components Not Found\n\nIf Metal3 components are not running:\n\n```bash\noc get pods -n openshift-machine-api\n# Look for pods starting with 'metal3-'\n\n# Check BareMetalHost resources\noc get baremetalhosts -n openshift-machine-api\n```\n\n### Credential Retrieval Failure\n\nIf unable to retrieve credentials:\n\n```bash\n# Check if the secret exists\noc get secrets -n openshift-machine-api | grep metal3\n\n# Verify the baremetal operator pod is running\noc get pods -n openshift-machine-api -l k8s-app=metal3-baremetal-operator\n```\n\n### Connection Timeout\n\nIf unable to connect to Ironic API:\n\n```bash\n# Check if the Ironic pod is running\noc get pods -n openshift-machine-api -l app=metal3\n\n# Check Ironic logs\noc logs -n openshift-machine-api <metal3-pod-name> -c metal3-ironic\n```\n\n## Security Considerations\n\n- **Credentials**: The command retrieves sensitive Ironic credentials; ensure output is not shared publicly\n- **Cluster access**: Requires exec permissions into cluster pods\n- **Read-only operation**: This command only reads node status and does not modify any resources\n\n## See Also\n\n- Metal3 Documentation: https://metal3.io/\n- OpenShift baremetal Documentation: https://docs.openshift.com/container-platform/latest/installing/installing_bare_metal/\n- Ironic API Reference: https://docs.openstack.org/ironic/latest/\n- Related commands: `/openshift:cluster-health-check`\n\n## Notes\n\n- This command is specific to OpenShift clusters deployed on baremetal infrastructure\n- The Ironic service endpoint and pod names may vary depending on the OpenShift version\n- Ensure you have network connectivity to the cluster"
              },
              {
                "name": "/new-e2e-test",
                "description": "Write and validate new OpenShift E2E tests using Ginkgo framework",
                "path": "plugins/openshift/commands/new-e2e-test.md",
                "frontmatter": {
                  "description": "Write and validate new OpenShift E2E tests using Ginkgo framework",
                  "argument-hint": "[test-specification]"
                },
                "content": "## Name\nopenshift:new-e2e-test\n\n## Synopsis\n```\n/new-e2e-test [test-specification]\n```\n\n## Description\n\nThe `new-e2e-test` command assists in writing and validating\nnew tests for the OpenShift test suite. It follows best practices for\nGinkgo-based testing and ensures test reliability through automated\nvalidation.\n\nThis command handles the complete lifecycle of test development:\n- Writes tests following Ginkgo patterns and OpenShift conventions\n- Validates tests for reliability through multiple test runs\n- Ensures proper test naming and structure\n- Handles both origin repository and extension tests appropriately\n\n## Test Framework Guidelines\n\n### Ginkgo Framework\n- OpenShift-tests uses **Ginkgo** as its testing framework\n- Tests are organized in a BDD (Behavior-Driven Development) style with Describe/Context/It blocks\n- All tests should follow Ginkgo patterns and conventions except\n    - You MUST NOT use BeforeAll, AfterAll hooks\n    - MUST NOT use ginkgo.Serial, instead use the [Serial] annotation in the test name if non-parallel execution is required\n\n### Repository-Specific Guidelines\n\n#### Origin Repository Tests\n\nIf working in the \"origin\" code repository:\n- All tests should go into the `test/extended` directory\n- If creating a new package, import it into `test/extended/include.go`\n- After writing your test, **MUST** rebuild the openshift-tests binary using `make openshift-tests`\n\n#### Other repositories\n\nOther repositories have have different conventions for locations of\ntests and how they get imported. Examine the code base and follow the\nconventions defined.\n\n## Critical Test Requirements\n\n### Test Names\n\n**CRITICAL**: Test names must be stable and deterministic.\n\n####  NEVER Include Dynamic Information:\n- Pod names (e.g., \"test-pod-abc123\")\n- Timestamps\n- Random UUIDs or generated identifiers\n- Node names\n- Namespace names with random suffixes\n- Limits that may change later\n\n####  ALWAYS Use Descriptive, Static Names:\n- **Good example**: \"should create a pod with custom security context\"\n- **Bad example**: \"should create pod test-pod-xyz123 with custom security context\"\n\n- **Good example**: \"should create a pod within a reasonable timeframe\"\n- **Bad example**: \"should create a pod within 15 seconds\"\n\n### Results\n\n**CRITICAL**: Tests must always produce a pass, fail or skip result. Do\nnot create tests that only produce pass or only produce a fail result.\n\n## Test Structure Guidelines\n\n### Best Practices\n\n- Tests should be focused and test one specific behavior\n- Use proper setup and cleanup in BeforeEach/AfterEach blocks\n- Include appropriate timeouts for operations\n- Add meaningful assertions with clear failure messages\n- Follow existing patterns in the codebase for consistency\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Analyze Specification**: Parse the test specification provided by the user\n2. **Write Test**: Create a new test file following Ginkgo and OpenShift conventions\n   - Determine correct location\n   - Follow proper test structure\n   - Use stable, descriptive naming\n   - Implement proper setup/cleanup\n3. **Build Binary**: Rebuild the appropriate test binary (openshift-tests or a test extension)\n\n## Arguments\n\n- **$1** (test-specification): Description of the test behavior to validate. Should clearly specify:\n  - What feature/behavior to test\n  - Expected outcomes\n  - Any specific conditions or configurations"
              },
              {
                "name": "/rebase",
                "description": "Rebase OpenShift fork of an upstream repository to a new upstream release.",
                "path": "plugins/openshift/commands/rebase.md",
                "frontmatter": {
                  "argument-hint": "<tag>",
                  "description": "Rebase OpenShift fork of an upstream repository to a new upstream release."
                },
                "content": "## Name\nopenshift:rebase\n\n## Synopsis\n```\n/openshift:rebase [tag]\n```\n\n## Description\n\nThe `/openshift:rebase` command rebases git repository in the current working directory\nto a new upstream release specified by `[tag]`. If no `[tag]` is specified, the command\ntries to find the latest stable upstream release.\n\nThe repository must follow rules described in https://github.com/openshift/kubernetes/blob/master/REBASE.openshift.md,\nnamely all OpenShift-specific commits must have prefix `UPSTREAM:`.\n\n## Implementation\n\n### Pre-requisites\nThree local remote repositories should be tracked from a local machine: `origin`\ntracking the user's fork of this repository, `openshift` tracking this\nrepository and `upstream` tracking the upstream repository.\n\nTo verify the correct setup, use\n```bash\ngit remote -v\n```\n\nFail, if there is no `upstream`, `origin` or `openshift` remote.\n\n### Rebase to the new upstream version\n\n1. Fetch all the remote repositories including tags\n    ```bash\n    git fetch --all\n    ```\n\n2. Find the main branch of the repository. It's either `master` or `main`. In the following steps, we will use `master`, but replace it with the main branch.\n\n3. If user did not specify an upstream tag to rebase to as `<tag>`, find the greatest upstream tag that is not alpha, beta or rc.\n\n4. Create a new branch based on the newest tag $1 of the upstream\n    repository. Name it after the tag.\n    ```bash\n    git checkout -b rebase-<tag> <tag>\n    ```\n\n5. Merge `openshift/master` branch into the `rebase-$1` branch with merge strategy `ours`:\n    ```bash\n    git merge -s ours openshift/master\n    ```\n\n6. Find the last rebase that has been done to `openshift/master`. We will use the upstream tag used for this rebase as `$previous_tag`.\n\n7. Find the merge base of the `openshift/master` and `$previous_tag` by running `git merge-base openshift/master $previous_tag`. We will use this merge base as `$mergebase`.\n\n8. Prepare `commits.tsv` tab-separated values file containing the set of carry\n    commits in the openshift/master branch that need to be considered for picking:\n\n    Create the commits file:\n    ```\n    echo -e 'Sha\\tMessage\\tDecision' > commits.tsv\n    git log ${mergebase}..openshift/master --ancestry-path --reverse --no-merges --pretty=\"tformat:%h%x09%s%x09\" | grep \"UPSTREAM:\" > commits.tsv\n    ```\n\n9. Go through the commits in the `commits.tsv` file and for each of them decide\n    whether to pick, drop or squash it. Commits carried on rebase branches have commit\n    messages prefixed as follows:\n\n    * `UPSTREAM: <carry>: Add OpenShift files`:\n        ALWAYS carry this commit and mark it as \"cherry-pick\".\n        This is a persistent carry that contains all OpenShift-specific files and should be present in every rebase.\n\n    * Other `UPSTREAM: <carry>` commit:\n        A persistent carry that needs to be considered for squashing.\n        Examine what files it modifies using `git show --stat <commit-sha>`.\n        If it modifies ONLY OpenShift-specific files (Dockerfile, OWNERS, .ci-operator.yaml, .snyk, etc.), mark it as \"squash\",\n        otherwise mark is as \"cherry-pick\".\n\n    * `UPSTREAM: <drop>`:\n        A carry that should probably not be picked for the subsequent rebase branch.\n        In general, these commits are used to maintain the codebase in ways that are branch-specific,\n        like the update of generated files or dependencies.\n        Mark such commit as \"drop\".\n\n    * `UPSTREAM: (upstream PR number)`:\n        The number identifies a PR in upstream repository (e.g. https://github.com/<upstream project>/<upstrem repository>/pull/<pr id>).\n        A commit with this message should only be picked into the subsequent rebase branch if the commits\n        of the referenced PR are not included in the upstream branch. To check if a given commit is included\n        in the upstream branch, open the referenced upstream PR and check any of its commits for the release tag.\n\n    For each commit:\n    - Print the decision you made and why.\n    - Update commits.tsv with the decision (\"cherry-pick\", \"drop\", or \"squash\").\n\n10. Cherry-pick all commits marked as \"cherry-pick\" in commits.tsv.\n    Then squash ALL commits marked as \"squash\" into a single commit named \"UPSTREAM: <carry>: Add OpenShift files\"\n    to keep the number of <carry> commits as low as possible.\n\n    Use `git reset --soft` to squash multiple commits together, then create a single commit with all the changes.\n    The commit message should list what was included (e.g., \"Additional changes: remove .github files, add .snyk file, update Dockerfile and .ci-operator.yaml\").\n\n11. If the upstream repository DOES NOT include `vendor/` directory and the OpenShift fork DOES, then update the vendor directory with `go mod tidy` and `go mod vendor`.\n    Amend these vendor updates into the \"UPSTREAM: <carry>: Add OpenShift files\" commit using `git commit --amend --no-edit`.\n\n12. As a verification step, see the last rebase and ensure that all changes made in the last rebase are present in the current one.\n    Either as a cherry pick or were part of the rebase.\n    Verify all changes were applied during the rebase. Either as a cherry-picked patch or they were included in the new upstream tag.\n    List all these commits, together with checks you made and their result.\n\n13. Verify the changes by running `make` and `make test` (or a similar command like like `go build ./...` and `go test ./...`).\n    Stop here if there are compilation errors or test failures that indicate real code issues.\n    If you make any new commits to fix compilation or tests, let user review these changes and then squash them into the commit \"UPSTREAM: <carry>: Add OpenShift files\" too.\n\n14. Find links to upstream changelogs between `$previous_tag` and $1.\n    Make sure they are links to changelogs, not tags.\n    Print list of the links.\n\n15. Create a github pull request against the OpenShift github repository (openshift/<repo-name>).\n    IMPORTANT: Use `--repo openshift/<repo-name>` to ensure the PR is created against the correct OpenShift repository, not the upstream.\n    The PR title should be \"Rebase to $1 for OCP <current OCP version>\".\n    Follow the repository .github/PULL_REQUEST_TEMPLATE.md, if it exists.\n    Description of the PR must look like:\n    ```\n    ## Upstream changelogs\n    <List links to all upstream changelogs, as composed in the previous step.>\n\n    ## Summary of changes\n    <List all new major features and breaking changes that happened between $previous_tag and $1.\n    Do not list upstream commits or PRs, make a human readable summary of them.\n    Do not include small bug fixes, small updates, or dependency bumps.>\n\n    ## Carried commits\n    <List of commits from commits.tsv. For each commit print a decision you made - either \"drop\", \"cherry-pick\", or \"squash\".>\n\n    Diff to upstream: <link to a diff between the upstream project/upstream repository/tag $1 and this PR (i.e. my personal fork with branch `rebase-$1`>\n\n    Previous rebase: <link to the previous rebase PR on github>\n    ```\n    When opening the PR, ALWAYS use `gh pr create --web --repo openshift/<repo-name>` to allow user edit the PR before creation."
              },
              {
                "name": "/review-test-cases",
                "description": "Review test cases for completeness, quality, and best practices - accepts file path or direct oc commands/test code",
                "path": "plugins/openshift/commands/review-test-cases.md",
                "frontmatter": {
                  "description": "Review test cases for completeness, quality, and best practices - accepts file path or direct oc commands/test code",
                  "argument-hint": "[file-path-or-test-code-or-commands]"
                },
                "content": "## Name\nopenshift:review-test-cases\n\n## Synopsis\n```\n/openshift:review-test-cases [file-path-or-test-code-or-commands]\n```\n\n## Description\n\nThe `review-test-cases` command provides comprehensive review of OpenShift test cases to ensure quality, completeness, and adherence to best practices. It accepts three types of input:\n\n1. **File path**: Path to test file (e.g., `/path/to/test.sh` or `/path/to/test.go`)\n2. **oc commands**: Direct oc CLI commands to review (e.g., paste a set of oc commands)\n3. **Test code**: Pasted Ginkgo test code to analyze\n\nThe command analyzes test code in both oc CLI shell scripts and Ginkgo Go tests, helping QE engineers identify gaps in test coverage, improve test reliability, and ensure tests follow OpenShift testing standards.\n\n## Implementation\n\nThe command analyzes test cases and provides structured feedback:\n\n1. **Parse Test Input**: Determine if input is a file path, oc commands, or test code\n   - If file path: Read and analyze the test file\n   - If oc commands: Parse command sequence\n   - If test code: Analyze pasted Ginkgo/test code\n2. **Identify Test Format**: Detect if it's oc CLI shell script or Ginkgo Go code\n3. **Analyze Test Structure**: Review organization, naming, and patterns\n4. **Check Coverage**: Verify positive, negative, and edge case coverage\n5. **Review Assertions**: Ensure proper validation and error checking\n6. **Evaluate Cleanup**: Verify resource cleanup and namespace management\n7. **Assess Best Practices**: **MUST follow the standards defined in \"Testing Guidelines and References\" section below**\n8. **Generate Recommendations**: Provide actionable improvement suggestions based on the guidelines\n\n**CRITICAL**: All reviews MUST be evaluated against the specific standards, references, and best practices listed in the **\"Testing Guidelines and References\"** section below. Do not use generic testing advice - follow the OpenShift-specific guidelines provided.\n\n## Testing Guidelines and References\n\nThe review follows established testing best practices from:\n\n**For Ginkgo/E2E Tests:**\n- OpenShift Origin Test Extended: https://github.com/openshift/origin/tree/master/test/extended\n- Ginkgo Testing Framework: https://onsi.github.io/ginkgo/\n- OpenShift Test Best Practices: https://github.com/openshift/origin/blob/master/test/extended/README.md\n\n**For oc CLI Tests:**\n- OpenShift CLI Documentation: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/developer-cli-commands.html\n- Bash Best Practices: https://google.github.io/styleguide/shellguide.html\n\n**Key Testing Standards:**\n- Use descriptive, stable test names (no timestamps, random IDs)\n- Proper resource cleanup (AfterEach, defer, trap)\n- Meaningful assertions with clear failure messages\n- Test isolation (each test creates own resources)\n- Appropriate timeouts and waits\n- No BeforeAll/AfterAll in Ginkgo tests\n- Use framework helpers (e2epod, e2enode) when available\n\n## Arguments\n\n- **$1** (file-path-or-test-code-or-commands): One of:\n  - **File path**: Path to test file (shell script or Go test file)\n  - **oc commands**: Set of oc CLI commands to review\n  - **Test code**: Pasted test code (Ginkgo or shell script)"
              },
              {
                "name": "/visualize-ovn-topology",
                "description": "Generate and visualize OVN-Kubernetes network topology diagram",
                "path": "plugins/openshift/commands/visualize-ovn-topology.md",
                "frontmatter": {
                  "description": "Generate and visualize OVN-Kubernetes network topology diagram",
                  "argument-hint": null
                },
                "content": "## Name\n\nopenshift:visualize-ovn-topology\n\n## Synopsis\n\n```\n/openshift:visualize-ovn-topology\n```\n\n## Description\n\nThe `openshift:visualize-ovn-topology` command generates a comprehensive Mermaid diagram of the OVN-Kubernetes network topology for a running cluster. The diagram shows:\n\n- Logical switches and routers\n- Switch and router ports with MAC/IP addresses\n- Pod connectivity\n- External network connections\n- Per-node component placement (interconnect mode) or centralized components (default mode)\n\nThe command automatically detects the cluster architecture (interconnect vs default mode) and creates an accurate topology diagram based on real data from the OVN databases.\n\n## Implementation\n\nThis command invokes the `generating-ovn-topology` skill which implements a data-driven architecture discovery approach:\n\n1. **Cluster Detection**: Automatically finds and connects to an OVN-Kubernetes cluster\n2. **Permission Check**: Verifies Kubernetes access level and warns if write permissions detected\n   - If you have cluster admin permissions, you'll be asked to confirm before proceeding\n   - The command only performs read-only operations regardless of your permission level\n   - This check ensures informed consent when using admin credentials\n3. **Architecture Discovery**: Analyzes UUID patterns across node databases to determine component placement (per-node vs cluster-wide)\n4. **Data Collection**: Queries OVN northbound databases for topology information\n5. **Diagram Generation**: Creates a Mermaid graph with proper component placement\n6. **Output**: Saves diagram to `ovn-topology-diagram.md` (or timestamped/custom path if file exists)\n\n**Key Features:**\n- **Data-driven**: Never generates synthetic data - always queries real cluster\n- **Architecture-aware**: Correctly handles both interconnect and default deployment modes\n- **Complete topology**: Shows all logical switches, routers, and ports\n- **Visual clarity**: Uses color-coded components and node subgraphs for organization\n\n**Skill Reference:**\n- Implementation details: `plugins/openshift/skills/generating-ovn-topology/SKILL.md`\n- Helper scripts: `plugins/openshift/skills/generating-ovn-topology/scripts/`\n\n## Return Value\n\n- **Format**: Mermaid diagram saved to file\n- **Location**: `./ovn-topology-diagram.md` (current directory) or custom path if specified\n- **Output**: Summary statistics and preview of the generated diagram\n\n## Examples\n\n1. **Basic usage** (generates topology for detected cluster):\n   ```shell\n   /openshift:visualize-ovn-topology\n   ```\n\n   Output:\n   ```text\n    Successfully generated OVN-Kubernetes topology diagram\n\n    Diagram saved to: ovn-topology-diagram.md\n\n   Summary:\n   - 3 nodes (ovn-control-plane, ovn-worker, ovn-worker2)\n   - 10 logical switches, 4 logical routers\n   - 27 logical switch ports, 13 logical router ports\n   - 9 running pods\n   - Mode: Interconnect (distributed control plane)\n\n    Open the file in your IDE to view the full rendered Mermaid diagram!\n   ```\n\n2. **With existing file** (prompts for action):\n   ```shell\n   /openshift:visualize-ovn-topology\n   ```\n\n   You'll be asked:\n   ```text\n   File ovn-topology-diagram.md already exists. Would you like to:\n   (1) Overwrite it\n   (2) Save to a different location\n   (3) Append timestamp to filename\n   (4) Cancel\n   ```\n\n## Prerequisites\n\n- **kubectl**: Must be installed and configured with access to an OVN-Kubernetes cluster\n- **Cluster**: A running Kubernetes cluster with OVN-Kubernetes CNI deployed\n- **Access**: Permission to exec into pods in the OVN namespace (e.g., `ovn-kubernetes` or `openshift-ovn-kubernetes`)\n\n## Security & Safety\n\n**This command performs ONLY read-only operations:**\n-  `kubectl get` - Query pods and nodes\n-  `kubectl exec` - Run read-only `ovn-nbctl list` commands\n-  Local file writes - Save topology diagram\n\n**Operations NEVER performed:**\n-  No `kubectl create/delete/patch/apply`\n-  No `ovn-nbctl` modifications\n-  No cluster state changes\n\n**Permission Check:**\nIf you have cluster admin permissions, you'll receive a warning message before the command proceeds. This is for transparency - you'll be informed about your access level and asked to confirm. The command will still only perform read-only operations.\n\n## Notes\n\n- The command is cluster-agnostic and works with any OVN-Kubernetes deployment\n- For KIND clusters created via `/openshift:create-cluster`, the kubeconfig is automatically detected\n- The diagram uses bottom-to-top layout (graph BT) following network topology conventions\n- All component placement is determined by UUID analysis, not hardcoded assumptions"
              }
            ],
            "skills": [
              {
                "name": "generating-ovn-topology",
                "description": "Generates and displays OVN-Kubernetes network topology diagrams showing logical switches, routers, ports with IP/MAC addresses in Mermaid format",
                "path": "plugins/openshift/skills/generating-ovn-topology/SKILL.md",
                "frontmatter": {
                  "name": "generating-ovn-topology",
                  "description": "Generates and displays OVN-Kubernetes network topology diagrams showing logical switches, routers, ports with IP/MAC addresses in Mermaid format",
                  "tools": [
                    "Bash",
                    "Read",
                    "Write"
                  ]
                },
                "content": "# Quick Start - OVN Topology Generation\n\n**IMMEDIATE ACTIONS** (follow these steps in order):\n\n1. **Detect Cluster**: Find the OVN-Kubernetes cluster kubeconfig\n\n   Run: `scripts/detect-cluster.sh 2>/dev/null`\n\n   The script discovers OVN-Kubernetes clusters:\n   - Scans all kubeconfig files: current KUBECONFIG env, ~/.kube/kind-config, ~/ovn.conf, ~/.kube/config\n   - Tests ALL contexts in each kubeconfig (not just current-context)\n   - Returns parseable list to stdout: `index|kubeconfig|cluster_name|node_count|namespace`\n   - Diagnostics go to stderr\n   - Exit code: 0=success, 1=no clusters found\n\n   **How to handle the output:**\n\n   The script returns pipe-delimited lines to stdout, one per cluster found, e.g.:\n   ```text\n   1|/home/user/.kube/kind-config|kind-ovn|3|ovn-kubernetes\n   2|/home/user/.kube/config|prod-cluster|12|openshift-ovn-kubernetes\n   ```\n\n   **Decision logic:**\n   - If **one cluster** found  automatically use it (extract kubeconfig path from column 2)\n   - If **multiple clusters** found  show the list to user and ask them to choose by number\n   - After selection, extract the kubeconfig path from column 2 of the chosen line\n   - Store the selected kubeconfig path in variable `KC` for use in subsequent steps\n\n   **Example output format parsing:**\n   - Column 1: Index number (for user selection)\n   - Column 2: Kubeconfig file path (this is what you need for `$KC`)\n   - Column 3: Cluster display name\n   - Column 4: Number of nodes\n   - Column 5: OVN namespace name\n\n   **Important**: Parse the output using standard text processing. The exact implementation is up to you - use whatever approach works best (awk, Python, inline parsing, etc.).\n\n2. **Check Permissions**: Verify user's Kubernetes access level and inform about write permissions\n\n   Run: `scripts/check_permissions.py \"$KC\"`\n\n   The script returns:\n   - **Exit 0**: Read-only access or user confirmed  proceed\n   - **Exit 1**: Error or user cancelled  stop\n   - **Exit 2**: Write permissions detected  AI must ask user for confirmation\n\n   **When exit code 2 is returned:**\n   1. Parse the stdout to get the list of write permissions\n   2. Display the permissions clearly to the user using a formatted message\n   3. Explain that:\n      - This skill performs ONLY read-only operations\n      - No cluster modifications will be made\n      - The warning is for transparency about their access level\n      - List read-only operations: kubectl get, kubectl exec (ovn-nbctl list), local file writes\n      - List forbidden operations: kubectl create/delete/patch, ovn-nbctl modifications\n   4. **Ask the user explicitly**: \"You have cluster admin permissions. This command will only perform read-only operations. Do you want to proceed?\"\n   5. If user says yes  continue, if no  stop\n\n   **Example of proper user communication:**\n   ```text\n     WARNING: Write Permissions Detected\n\n   Your kubeconfig has cluster admin permissions:\n      Delete pods, deployments, services\n      Create and modify resources\n      Full cluster access\n\n    IMPORTANT:\n   This command will ONLY perform read-only operations:\n      kubectl get (pods, nodes)\n      kubectl exec (to run read-only ovn-nbctl list commands)\n      Local file writes (topology diagram)\n\n   Operations that will NEVER be performed:\n      kubectl create/delete/patch/apply\n      ovn-nbctl modifications\n      Any cluster state changes\n\n   Do you want to proceed with read-only topology generation?\n   ```\n\n   **Security Note**: This step ensures informed consent. The user must be explicitly aware that their cluster admin credentials are accessible to the AI agent (acting on their behalf), even though only read-only operations will be performed. This transparency is critical for security and trust.\n\n3. **Check Output File**: Ask user if `ovn-topology-diagram.md` exists:\n   - (1) Overwrite, (2) Custom path, (3) Timestamp, (4) Cancel\n\n4. **Create Private Temp Directory**: Create a private temporary directory using `mkdtemp` and use it for all temporary files.\n\n   ```bash\n   TMPDIR=$(mktemp -d)\n   ```\n\n5. **Collect OVN Data**: Get full topology data from the cluster\n\n   Run: `scripts/collect_ovn_data.py \"$KC\" \"$TMPDIR\"`\n\n   Detail files written to `$TMPDIR`:\n   - `ovn_switches_detail.txt` - node|uuid|name|other_config\n   - `ovn_routers_detail.txt` - node|uuid|name|external_ids|options\n   - `ovn_lsps_detail.txt` - node|name|addresses|type|options\n   - `ovn_lrps_detail.txt` - node|name|mac|networks|options\n   - `ovn_pods_detail.txt` - namespace|name|ip|node\n\n6. **Analyze Placement**: Determine per-node vs cluster-wide components\n\n   Run: `scripts/analyze_placement.py \"$TMPDIR\"`\n\n   Placement results written to `$TMPDIR`:\n   - `ovn_switch_placement.txt` - name|placement (per-node|cluster-wide|cluster-wide-visual)\n   - `ovn_router_placement.txt` - name|placement (per-node|cluster-wide|cluster-wide-visual)\n\n7. **Generate Diagram**: Create Mermaid `graph BT` diagram\n   - Read `$TMPDIR/ovn_switch_placement.txt` to determine where each switch goes\n   - Read `$TMPDIR/ovn_router_placement.txt` to determine where each router goes\n   - Read detail files directly (ovn_switches_detail.txt, ovn_routers_detail.txt, etc.)\n   - Skip UUID column when parsing switches/routers detail files\n   - If placement is `per-node`  put inside node subgraph\n   - If placement is `cluster-wide` or `cluster-wide-visual`  put outside subgraphs\n\n8. **Save & Report**: Write diagram to file, show summary, clean up temporary files\n\n**CRITICAL RULES**:\n-  NO codebase searching for IPs/MACs\n-  NO synthetic/example data\n-  NO inline multi-line bash (use helper scripts)\n-  NO direct kubectl commands (must use helper scripts only)\n-  Use helper scripts for all kubectl interactions and architecture discovery\n-  **For helper scripts only**: If kubectl is required, use `KUBECONFIG=\"$KC\" kubectl --kubeconfig=\"$KC\"`\n-  **SECURITY**: Create private temp directory with `TMPDIR=$(mktemp -d)` - never use `/tmp` directly\n-  Temporary files use `$TMPDIR` (private directory created with mkdtemp)\n-  Clean up temporary files when done: `rm -rf \"$TMPDIR\"`\n\n## Safety & Security Guarantees\n\n###  Read-Only Operations\n\nThis skill performs **ONLY read-only operations** against your Kubernetes cluster. No cluster state is modified.\n\n**Allowed Operations:**\n-  `kubectl get` - Query resources\n-  `kubectl exec ... ovn-nbctl list` - Query OVN database (read-only)\n-  Local file writes (temporary files in `$TMPDIR`, output diagram)\n\n**Forbidden Operations (NEVER used):**\n-  `kubectl create/apply/delete/patch` - No resource modifications\n-  `kubectl scale/drain/cordon` - No cluster operations\n-  `ovn-nbctl create/set/add/remove/destroy` - No OVN modifications\n-  No pod restarts or service disruptions\n\n**Privacy Consideration**: The generated diagram contains network topology information. Control sharing appropriately based on your security policies.\n\n---\n\n# Architecture Concepts\n\n## Interconnect Mode (Distributed NBDB)\n\nIn **interconnect mode**, each node runs its own NBDB with local copies of components:\n\n**Per-Node Components** (different UUIDs on each node):\n- `ovn_cluster_router` - Each node has its own cluster router instance\n- `join` switch - Each node has its own join switch instance\n- `transit_switch` - Each node has its own transit switch instance\n- Node switches (e.g., `ovn-control-plane`, `ovn-worker`)\n- External switches (e.g., `ext_ovn-control-plane`)\n- Gateway routers (e.g., `GR_ovn-control-plane`)\n\n**Visualization Overrides**:\n- `transit_switch`: PER-NODE in reality  shown CLUSTER-WIDE for visualization clarity\n- `join`: PER-NODE  kept PER-NODE (no override)\n\n## Helper Scripts\n\nAll helper scripts are in the `scripts/` directory.\n\n| Script | Purpose | Input | Output |\n|--------|---------|-------|--------|\n| [detect-cluster.sh](scripts/detect-cluster.sh) | Find OVN cluster kubeconfig across all contexts. Scans multiple kubeconfig files and all their contexts. Returns parseable list. | None | Parseable list to stdout: `index\\|kubeconfig\\|cluster\\|nodes\\|namespace`. Exit: 0=success, 1=none found |\n| [check_permissions.py](scripts/check_permissions.py) | Check user permissions and warn if write access detected. | KUBECONFIG path | Exit: 0=proceed, 1=cancelled/error, 2=write perms (needs user confirmation) |\n| [collect_ovn_data.py](scripts/collect_ovn_data.py) | **Data collector**: Queries each node for all data, with **graceful degradation** (continues on node failures). Writes detail files. | KUBECONFIG path, TMPDIR | Detail files: `ovn_switches_detail.txt`, `ovn_routers_detail.txt`, `ovn_lsps_detail.txt`, `ovn_lrps_detail.txt`, `ovn_pods_detail.txt` |\n| [analyze_placement.py](scripts/analyze_placement.py) | **Placement analyzer**: Analyzes UUID patterns from detail files to determine per-node vs cluster-wide placement. | TMPDIR (reads detail files) | Placement files: `ovn_switch_placement.txt`, `ovn_router_placement.txt` |\n\n---\n\n# Diagram Generation Rules\n\n## Structure\n\n```mermaid\ngraph BT\n    subgraph node1[\"<b style='color:black'>Node: name (node_ip)</b>\"]\n        direction BT\n\n        %% LAYER 1 (Bottom): Pods and Management Ports\n        POD_example[\"Pod: pod-name<br/>Namespace: ns<br/>IP: x.x.x.x\"]\n        MGMT[\"Management Port: k8s-node<br/>IP: x.x.x.x\"]\n\n        %% LAYER 2: Pod LSPs\n        LSP_pod[\"LSP: namespace_pod-name<br/>MAC: xx:xx:xx:xx:xx:xx<br/>IP: x.x.x.x\"]\n        LSP_mgmt[\"LSP: k8s-node<br/>MAC: xx:xx:xx:xx:xx:xx<br/>IP: x.x.x.x\"]\n\n        %% LAYER 3: Node Switch\n        LS_node[\"Logical Switch: node-name<br/>Subnet: x.x.x.x/24\"]\n\n        %% LAYER 4: Node Switch LSPs\n        LSP_stor[\"LSP: stor-node<br/>Type: router\"]\n\n        %% LAYER 5: Cluster Router\n        LR_cluster[\"Logical Router: ovn_cluster_router\"]\n        LRP_rtos[\"LRP: rtos-node<br/>MAC: xx:xx<br/>IP: x.x.x.x/24\"]\n        LRP_rtoj[\"LRP: rtoj-ovn_cluster_router<br/>IP: x.x.x.x/16\"]\n        LRP_rtots[\"LRP: rtots-node<br/>IP: x.x.x.x/16\"]\n\n        %% LAYER 6: Join Switch\n        LS_join[\"Logical Switch: join\"]\n        LSP_jtor_cr[\"LSP: jtor-ovn_cluster_router<br/>Type: router\"]\n        LSP_jtor_gr[\"LSP: jtor-GR_node<br/>Type: router\"]\n\n        %% LAYER 7: Gateway Router\n        LR_gr[\"Gateway Router: GR_node\"]\n        LRP_rtoj_gr[\"LRP: rtoj-GR_node<br/>IP: x.x.x.x/16\"]\n        LRP_rtoe[\"LRP: rtoe-GR_node<br/>IP: x.x.x.x/24\"]\n\n        %% LAYER 8: External Switch\n        LS_ext[\"Logical Switch: ext_node\"]\n        LSP_etor[\"LSP: etor-GR_node<br/>Type: router\"]\n        LSP_breth0[\"LSP: breth0_node<br/>Type: localnet\"]\n\n        %% LAYER 9 (Top): External Network\n        EXT_NET[\"External Network<br/>Physical bridge: breth0<br/>Node IP: x.x.x.x\"]\n\n        %% Connections (bottom-to-top flow)\n        POD_example --> LSP_pod --> LS_node\n        MGMT --> LSP_mgmt --> LS_node\n        LS_node --> LSP_stor\n        LSP_stor -.->|peer| LRP_rtos --> LR_cluster\n        LR_cluster --> LRP_rtoj -.->|peer| LSP_jtor_cr --> LS_join\n        LS_join --> LSP_jtor_gr -.->|peer| LRP_rtoj_gr --> LR_gr\n        LR_gr --> LRP_rtoe -.->|peer| LSP_etor --> LS_ext\n        LS_ext --> LSP_breth0 -.->|physical| EXT_NET\n        LR_cluster --> LRP_rtots\n    end\n\n    %% Cluster-wide components (AFTER nodes to appear on top)\n    %% Only include components with placement=cluster-wide or cluster-wide-visual\n    %% Example: transit_switch in interconnect mode\n    LS_cluster_component[\"Component Name<br/>Details\"]\n\n    %% Connections from nodes to cluster-wide components\n    LRP_from_node -.->|connects to| LSP_cluster_port --> LS_cluster_component\n```\n\n## Key Requirements\n\n1. **Graph Direction**: Always `graph BT` (bottom-to-top)\n\n2. **Component Placement**: Determined by `$TMPDIR/ovn_*_placement.txt`\n   - `per-node`  INSIDE node subgraph\n   - `cluster-wide` or `cluster-wide-visual`  OUTSIDE all subgraphs, **defined AFTER all node subgraphs**\n   - **CRITICAL**: Define ALL cluster-wide components AFTER all nodes to position them at the TOP\n     - Prevents connection lines from overlapping with node subgraphs\n     - Applies to ANY component with `cluster-wide` or `cluster-wide-visual` placement\n\n3. **Node Subgraphs**: Each physical node gets a subgraph with `direction BT`\n   - **Node Ordering**: ALWAYS order nodes as: control-plane node first, then worker nodes sorted alphabetically by name\n   - Title format: `\"<b style='color:black'>Node: {node_name} ({external_ip})</b>\"`\n   - Get external IP from `rtoe-GR_{node}` router port network field\n   - Use different background colors for each node for visual distinction\n   - **CRITICAL: Define components in BOTTOM-TO-TOP order** (matches packet flow):\n     1. **Bottom Layer**: Pods and Management Ports (traffic originates here)\n     2. **Layer 2**: Pod LSPs\n     3. **Layer 3**: Node Switch (where pods connect)\n     4. **Layer 4**: Node Switch LSPs (stor, breth0)\n     5. **Layer 5**: Cluster Router + Router Ports (rtos, rtoj, rtots)\n     6. **Layer 6**: Join Switch + Join LSPs\n     7. **Layer 7**: Gateway Router + Router Ports (rtoj, rtoe)\n     8. **Layer 8**: External Switch + External LSPs (etor, breth0)\n     9. **Top Layer**: External Network (physical bridge)\n\n4. **Pod Representation**:\n   - **CRITICAL**: Show ALL pods from `$TMPDIR/ovn_pods_detail.txt` as SEPARATE entities\n   - **DO NOT** discover pods from LSPs - many pods (host-network) don't have individual LSPs\n   - Pod format: `POD_{id}[\"Pod: {name}<br/>Namespace: {ns}<br/>IP: {ip}\"]`\n   - Connect pods to their respective LSPs:\n     - **Host-network pods** (IP == node IP): `POD_{id} --> MGMT_{node} --> LSP_k8s_{node}`\n     - **Pod-network pods** (IP in 10.244.x.x): `POD_{id} --> LSP_{namespace}_{podname}`\n\n5. **Physical Network Layer**:\n   - Show explicit external network entities per node\n   - Format: `EXT_NET_{node}[\"External Network<br/>Physical bridge: breth0<br/>Node IP: {external_ip}\"]`\n   - Connect localnet LSPs to external network: `LSP_breth0_{node} -.->|physical| EXT_NET_{node}`\n\n6. **Colors** (apply using classDef with **color:#000** for black text):\n   - Pods: `fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#000`\n   - Switches: `fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000`\n   - Routers: `fill:#fff9c4,stroke:#f57f17,stroke-width:2px,color:#000`\n   - LSPs (ALL types): `fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#000`\n   - LRPs: `fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#000`\n   - External Network: `fill:#e0e0e0,stroke:#424242,stroke-width:2px,color:#000`\n   - Management Ports (MGMT entities only, not LSPs): `fill:#fff8e1,stroke:#f57c00,stroke-width:2px,color:#000`\n   - **Node Subgraph Backgrounds** (use `style` statements, NOT classDef): Apply at END of diagram after all class assignments. Rotate through these 3 colors:\n     - Node 1 (index 0): `style node1 fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,stroke-dasharray: 5 5,color:#000` (green)\n     - Node 2 (index 1): `style node2 fill:#e1f5fe,stroke:#0277bd,stroke-width:3px,stroke-dasharray: 5 5,color:#000` (blue)\n     - Node 3 (index 2): `style node3 fill:#fff3e0,stroke:#ef6c00,stroke-width:3px,stroke-dasharray: 5 5,color:#000` (orange)\n     - Node 4+ repeats: Use index % 3 to rotate colors (0=green, 1=blue, 2=orange)\n\n7. **Required Info**:\n   - Pods: Name, Namespace, IP\n   - Switches: Name, Subnet (from other_config), Gateway (optional)\n   - LSPs: Name, MAC, IP (or Type + Router Port for router ports)\n   - LRPs: Name, MAC, Network (IP/CIDR)\n   - Routers: Name, Description\n   - External Network: Physical bridge name (breth0), Node IP\n\n8. **Connections**:\n   - Solid arrows: Layer connections\n   - Dashed arrows with `|peer|`: Peer port relationships\n\n## Pod Discovery\n\n**CRITICAL: Discover pods from `$TMPDIR/ovn_pods_detail.txt`, NOT from LSPs**\n\nThe file `$TMPDIR/ovn_pods_detail.txt` contains ALL running pods in the cluster (populated by collect_ovn_data.py).\nFormat: `namespace|pod_name|pod_ip|node_name`\n\n**Pod-to-LSP Mapping Logic:**\n\n1. **Read ALL pods** from `$TMPDIR/ovn_pods_detail.txt`\n2. **For each pod**, determine which LSP it connects to:\n\n   a. **Host-Network Pods** (pod IP == node IP, e.g., 10.89.0.10):\n      - **Connect to management port LSP**: `k8s-{node}`\n      - These pods share the management port's IP address\n      - **MUST be included in diagram** despite not having individual LSPs\n\n   b. **Pod-Network Pods** (pod IP in pod network range, e.g., 10.244.x.x):\n      - **Connect to individual LSP**: `{namespace}_{pod-name-with-hash}`\n      - LSP name format: `kube-system_coredns-674b8bbfcf-qhfrq`\n      - Filter LSPs where `type=\"\"` (empty string)\n      - Extract MAC and IP from LSP addresses field\n\n3. **Diagram representation**:\n   ```mermaid\n   POD_id[\"Pod: {name}<br/>Namespace: {ns}<br/>IP: {ip}\"]\n   POD_id --> LSP_id\n   ```\n\n**Special LSPs (NOT pods, treat as infrastructure):**\n- `k8s-{node}`: Management port LSP (multiple host-network pods connect to this)\n- `stor-{node}`: Router port (type=\"router\")\n- `breth0_{node}`: LocalNet port (type=\"localnet\")\n- `jtor-*`, `etor-*`, `tstor-*`: Router ports (type=\"router\")\n\n### Example: Control Plane Node with Host-Network Pods\n\n```mermaid\n%% 8 host-network pods sharing management port\nPOD_etcd[\"Pod: etcd-ovn-control-plane<br/>Namespace: kube-system<br/>IP: 10.89.0.10\"]\nPOD_apiserver[\"Pod: kube-apiserver-ovn-control-plane<br/>Namespace: kube-system<br/>IP: 10.89.0.10\"]\nPOD_controller[\"Pod: kube-controller-manager-ovn-control-plane<br/>Namespace: kube-system<br/>IP: 10.89.0.10\"]\nPOD_scheduler[\"Pod: kube-scheduler-ovn-control-plane<br/>Namespace: kube-system<br/>IP: 10.89.0.10\"]\nPOD_ovnkube_cp[\"Pod: ovnkube-control-plane-ovn-control-plane<br/>Namespace: ovn-kubernetes<br/>IP: 10.89.0.10\"]\nPOD_ovnkube_id[\"Pod: ovnkube-identity-ovn-control-plane<br/>Namespace: ovn-kubernetes<br/>IP: 10.89.0.10\"]\nPOD_ovnkube_node[\"Pod: ovnkube-node-xyz<br/>Namespace: ovn-kubernetes<br/>IP: 10.89.0.10\"]\nPOD_ovs_node[\"Pod: ovs-node-xyz<br/>Namespace: ovn-kubernetes<br/>IP: 10.89.0.10\"]\n\n%% Management port LSP (shared by all host-network pods)\nMGMT_cp[\"Management Port: k8s-ovn-control-plane<br/>IP: 10.89.0.10\"]\nLSP_mgmt[\"LSP: k8s-ovn-control-plane<br/>MAC: xx:xx:xx:xx:xx:xx<br/>IP: 10.244.0.2\"]\n\n%% All host-network pods connect to same management port\nPOD_etcd --> MGMT_cp\nPOD_apiserver --> MGMT_cp\nPOD_controller --> MGMT_cp\nPOD_scheduler --> MGMT_cp\nPOD_ovnkube_cp --> MGMT_cp\nPOD_ovnkube_id --> MGMT_cp\nPOD_ovnkube_node --> MGMT_cp\nPOD_ovs_node --> MGMT_cp\n\nMGMT_cp --> LSP_mgmt --> LS_node\n```\n\n---\n\n# Final Steps\n\n1. Generate complete Mermaid diagram following structure above\n2. Save to file chosen by user\n3. Show summary: nodes, switches, routers, ports, mode\n4. Clean up temporary directory:\n   ```bash\n   rm -rf \"$TMPDIR\"\n   ```\n5. Tell user to open file in IDE to view rendered diagram"
              }
            ]
          },
          {
            "name": "etcd",
            "description": "Etcd cluster health monitoring and performance analysis utilities",
            "source": "./plugins/etcd",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install etcd@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-performance",
                "description": "Analyze etcd performance metrics, latency, and identify bottlenecks",
                "path": "plugins/etcd/commands/analyze-performance.md",
                "frontmatter": {
                  "description": "Analyze etcd performance metrics, latency, and identify bottlenecks",
                  "argument-hint": "[--duration <minutes>]"
                },
                "content": "## Name\netcd:analyze-performance\n\n## Synopsis\n```\n/etcd:analyze-performance [--duration <minutes>]\n```\n\n## Description\n\nThe `analyze-performance` command analyzes etcd performance metrics to identify latency issues, slow operations, and potential bottlenecks. It examines disk performance, commit latency, network latency, and provides recommendations for optimization.\n\nEtcd performance is critical for cluster responsiveness. Slow etcd operations can cause:\n- API server timeouts\n- Slow pod creation and updates\n- Controller delays\n- Overall cluster sluggishness\n\nThis command is useful for:\n- Diagnosing slow cluster operations\n- Identifying disk I/O bottlenecks\n- Detecting network latency issues\n- Capacity planning\n- Performance tuning\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **OpenShift CLI (oc)**\n   - Install from: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n   - Verify with: `oc version`\n\n2. **Active cluster connection**\n   - Must be connected to an OpenShift cluster\n   - Verify with: `oc whoami`\n\n3. **Cluster admin permissions**\n   - Required to access etcd pods and metrics\n   - Verify with: `oc auth can-i get pods -n openshift-etcd`\n\n4. **Running etcd pods**\n   - At least one etcd pod must be running\n   - Check with: `oc get pods -n openshift-etcd -l app=etcd`\n\n## Arguments\n\n- **--duration** (optional): Duration in minutes to analyze logs (default: 5)\n  - Analyzes recent logs for the specified duration\n  - Longer durations provide more comprehensive analysis\n  - Example: `--duration 15` for 15-minute window\n\n## Implementation\n\nThe command performs the following analysis:\n\n### 1. Verify Prerequisites\n\n```bash\nif ! command -v oc &> /dev/null; then\n    echo \"Error: oc CLI not found\"\n    exit 1\nfi\n\nif ! oc whoami &> /dev/null; then\n    echo \"Error: Not connected to cluster\"\n    exit 1\nfi\n\n# Parse duration argument (default: 5 minutes)\nDURATION=5\nif [[ \"$1\" == \"--duration\" ]] && [[ -n \"$2\" ]]; then\n    DURATION=$2\nfi\n\necho \"Analyzing etcd performance (last $DURATION minutes)...\"\n```\n\n### 2. Get Running Etcd Pod\n\n```bash\nETCD_POD=$(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\nif [ -z \"$ETCD_POD\" ]; then\n    echo \"Error: No running etcd pod found\"\n    exit 1\nfi\n\necho \"Using etcd pod: $ETCD_POD\"\necho \"\"\n```\n\n### 3. Analyze Database Performance\n\nGet database statistics using etcdctl:\n\n```bash\necho \"===============================================\"\necho \"DATABASE PERFORMANCE ANALYSIS\"\necho \"===============================================\"\necho \"\"\necho \"Fetching database statistics...\"\n\n# Get database sizes from endpoint status\nDB_STATUS=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl endpoint status --cluster -w json 2>/dev/null)\n\necho \"Database Statistics:\"\necho \"$DB_STATUS\" | jq -r '.[] |\n    \"Endpoint: \\(.Endpoint)\n  Version: \\(.Status.version)\n  DB Size: \\(.Status.dbSize) bytes (\\((.Status.dbSize / 1024 / 1024) | floor)MB)\n  DB In Use: \\(.Status.dbSizeInUse) bytes (\\((.Status.dbSizeInUse / 1024 / 1024) | floor)MB)\n  Keys: \\(.Status.header.revision)\n  Raft Index: \\(.Status.raftIndex)\n  Raft Term: \\(.Status.raftTerm)\n  Leader: \\(if .Status.leader == .Status.header.member_id then \"YES\" else \"NO\" end)\n\"'\n\necho \"\"\necho \"Fragmentation Analysis:\"\necho \"$DB_STATUS\" | jq -r '.[] |\n    if .Status.dbSize > 0 then\n        ((.Status.dbSize - .Status.dbSizeInUse) * 100 / .Status.dbSize) as $frag |\n        \"Endpoint: \\(.Endpoint)\n  Fragmentation: \\($frag | floor)%\" +\n        if $frag > 50 then\n            \" - WARNING: High fragmentation detected, consider defragmentation\"\n        elif $frag > 30 then\n            \" - NOTICE: Moderate fragmentation\"\n        else\n            \" - OK\"\n        end\n    else\n        \"Endpoint: \\(.Endpoint)\n  Fragmentation: N/A\"\n    end'\n```\n\n### 4. Check Cluster Health\n\nVerify etcd cluster health:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"CLUSTER HEALTH\"\necho \"===============================================\"\necho \"\"\noc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl endpoint health --cluster 2>/dev/null || echo \"Health check failed\"\n```\n\n### 5. Analyze Logs for Performance Issues\n\nParse etcd logs for performance warnings:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"LOG ANALYSIS (Last $DURATION minutes)\"\necho \"===============================================\"\necho \"\"\necho \"Searching for performance-related warnings...\"\n\n# Get recent logs\nLOGS=$(oc logs -n openshift-etcd \"$ETCD_POD\" -c etcd --since=\"${DURATION}m\" 2>/dev/null)\n\n# Count slow operations\nSLOW_OPS=$(echo \"$LOGS\" | grep -i \"slow\" | wc -l)\necho \"Slow operations logged: $SLOW_OPS\"\n\nif [ \"$SLOW_OPS\" -gt 0 ]; then\n    echo \"\"\n    echo \"Recent slow operations (last 10):\"\n    echo \"$LOGS\" | grep -i \"slow\" | tail -10\nfi\n\necho \"\"\n\n# Check for disk warnings\nDISK_WARNINGS=$(echo \"$LOGS\" | grep -iE \"disk|fdatasync|fsync\" | grep -iE \"slow|took|latency\" | wc -l)\necho \"Disk-related warnings: $DISK_WARNINGS\"\n\nif [ \"$DISK_WARNINGS\" -gt 0 ]; then\n    echo \"\"\n    echo \"Disk performance warnings:\"\n    echo \"$LOGS\" | grep -iE \"disk|fdatasync|fsync\" | grep -iE \"slow|took|latency\" | tail -5\nfi\n\necho \"\"\n\n# Check for apply warnings\nAPPLY_WARNINGS=$(echo \"$LOGS\" | grep -iE \"apply.*took|slow.*apply\" | wc -l)\necho \"Apply operation warnings: $APPLY_WARNINGS\"\n\nif [ \"$APPLY_WARNINGS\" -gt 0 ]; then\n    echo \"\"\n    echo \"Apply warnings:\"\n    echo \"$LOGS\" | grep -iE \"apply.*took|slow.*apply\" | tail -5\nfi\n\necho \"\"\n\n# Check for compaction info\necho \"Recent compaction operations:\"\necho \"$LOGS\" | grep \"finished scheduled compaction\" | tail -3\nif [ $(echo \"$LOGS\" | grep \"finished scheduled compaction\" | wc -l) -eq 0 ]; then\n    echo \"  No compaction operations in this time window\"\nfi\n\necho \"\"\n\n# Check for snapshot operations\necho \"Snapshot operations:\"\nSNAPSHOTS=$(echo \"$LOGS\" | grep -i \"snapshot\" | wc -l)\necho \"Snapshot events: $SNAPSHOTS\"\nif [ \"$SNAPSHOTS\" -gt 0 ]; then\n    echo \"$LOGS\" | grep -i \"snapshot\" | tail -3\nfi\n```\n\n### 6. Analyze Leader Stability\n\nCheck for leader changes and stability issues:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"LEADER STABILITY ANALYSIS\"\necho \"===============================================\"\necho \"\"\n\nLEADER_CHANGES=$(echo \"$LOGS\" | grep -i \"leader.*changed\\|became leader\\|lost leader\" | wc -l)\necho \"Leader change events: $LEADER_CHANGES\"\n\nif [ \"$LEADER_CHANGES\" -gt 0 ]; then\n    echo \"\"\n    echo \"Leader change events:\"\n    echo \"$LOGS\" | grep -i \"leader.*changed\\|became leader\\|lost leader\"\nfi\n\n# Check for proposal/commit issues\necho \"\"\necho \"Proposal and commit operations:\"\nPROPOSAL_LOGS=$(echo \"$LOGS\" | grep -iE \"proposal|commit\" | grep -iE \"slow|took|failed\" | wc -l)\necho \"Slow proposal/commit operations: $PROPOSAL_LOGS\"\n\nif [ \"$PROPOSAL_LOGS\" -gt 0 ]; then\n    echo \"\"\n    echo \"Sample slow operations:\"\n    echo \"$LOGS\" | grep -iE \"proposal|commit\" | grep -iE \"slow|took|failed\" | tail -5\nfi\n```\n\n### 7. Analyze Network Performance\n\nCheck for network-related issues:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"NETWORK ANALYSIS\"\necho \"===============================================\"\necho \"\"\n\nNETWORK_ISSUES=$(echo \"$LOGS\" | grep -iE \"network|connection|timeout|peer\" | grep -iE \"error|fail|slow\" | wc -l)\necho \"Network-related issues: $NETWORK_ISSUES\"\n\nif [ \"$NETWORK_ISSUES\" -gt 0 ]; then\n    echo \"\"\n    echo \"Network issues:\"\n    echo \"$LOGS\" | grep -iE \"network|connection|timeout|peer\" | grep -iE \"error|fail|slow\" | tail -5\nfi\n```\n\n### 8. Generate Performance Summary\n\nCreate summary with recommendations:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"PERFORMANCE SUMMARY & RECOMMENDATIONS\"\necho \"===============================================\"\necho \"\"\n\nISSUES=0\nWARNINGS=0\n\n# Check fragmentation from DB status\nMAX_FRAG=$(echo \"$DB_STATUS\" | jq -r '[.[] | if .Status.dbSize > 0 then ((.Status.dbSize - .Status.dbSizeInUse) * 100 / .Status.dbSize) else 0 end] | max')\n\nif (( $(echo \"$MAX_FRAG > 50\" | bc -l 2>/dev/null || echo 0) )); then\n    echo \"ISSUE: High database fragmentation (${MAX_FRAG}%)\"\n    echo \"  Recommendation: Run defragmentation on all etcd members\"\n    echo \"  Command: oc exec -n openshift-etcd <pod> -c etcdctl -- etcdctl defrag\"\n    echo \"\"\n    ISSUES=$((ISSUES + 1))\nelif (( $(echo \"$MAX_FRAG > 30\" | bc -l 2>/dev/null || echo 0) )); then\n    echo \"WARNING: Moderate database fragmentation (${MAX_FRAG}%)\"\n    echo \"  Recommendation: Monitor and consider defragmentation if performance degrades\"\n    echo \"\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\nif [ \"$LEADER_CHANGES\" -gt 5 ]; then\n    echo \"WARNING: Frequent leader changes ($LEADER_CHANGES in last ${DURATION}m)\"\n    echo \"  Recommendation: Check network stability between etcd nodes\"\n    echo \"  - Verify network latency between control plane nodes\"\n    echo \"  - Check for packet loss or network congestion\"\n    echo \"\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\nif [ \"$SLOW_OPS\" -gt 10 ]; then\n    echo \"WARNING: High number of slow operations ($SLOW_OPS in last ${DURATION}m)\"\n    echo \"  Recommendation: Investigate disk I/O and workload patterns\"\n    echo \"  - Check disk performance with 'fio' benchmarks\"\n    echo \"  - Review etcd workload and consider optimization\"\n    echo \"\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\nif [ \"$DISK_WARNINGS\" -gt 5 ]; then\n    echo \"WARNING: Multiple disk performance warnings ($DISK_WARNINGS in last ${DURATION}m)\"\n    echo \"  Recommendation: Investigate disk I/O performance\"\n    echo \"  - Ensure etcd is using SSD/NVMe storage\"\n    echo \"  - Check for disk saturation or competing I/O\"\n    echo \"  - Verify disk benchmarks meet etcd requirements (> 50 sequential IOPS)\"\n    echo \"\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\n# Get average DB size\nAVG_DB_SIZE=$(echo \"$DB_STATUS\" | jq -r '[.[] | .Status.dbSize] | add / length')\nAVG_DB_SIZE_MB=$(echo \"scale=0; $AVG_DB_SIZE / 1024 / 1024\" | bc)\n\nif [ \"$AVG_DB_SIZE_MB\" -gt 8000 ]; then\n    echo \"WARNING: Large database size (${AVG_DB_SIZE_MB}MB)\"\n    echo \"  Recommendation: Review data retention and compaction policies\"\n    echo \"  - Check event retention policies\"\n    echo \"  - Consider more frequent compaction\"\n    echo \"\"\n    WARNINGS=$((WARNINGS + 1))\nfi\n\necho \"Performance Metrics Summary:\"\necho \"  - Database size: ${AVG_DB_SIZE_MB}MB (recommended: < 8GB)\"\necho \"  - Fragmentation: ${MAX_FRAG}% (recommended: < 30%)\"\necho \"  - Slow operations (${DURATION}m): $SLOW_OPS (recommended: < 10)\"\necho \"  - Leader changes (${DURATION}m): $LEADER_CHANGES (recommended: < 5)\"\necho \"\"\n\nif [ \"$ISSUES\" -eq 0 ] && [ \"$WARNINGS\" -eq 0 ]; then\n    echo \"Status:  HEALTHY - Performance within acceptable ranges\"\n    exit 0\nelif [ \"$ISSUES\" -gt 0 ]; then\n    echo \"Status:  CRITICAL - Found $ISSUES performance issues requiring attention\"\n    exit 1\nelse\n    echo \"Status:  WARNING - Found $WARNINGS performance warnings\"\n    exit 0\nfi\n```\n\n## Return Value\n\n- **Exit 0**: Performance is acceptable (may have warnings)\n- **Exit 1**: Critical performance issues detected\n\n**Output Format**:\n- Structured sections for different performance aspects\n- Metrics with percentile values (P50, P99)\n- Warnings for values exceeding thresholds\n- Recommendations for remediation\n\n## Examples\n\n### Example 1: Basic performance analysis\n```\n/etcd:analyze-performance\n```\n\nOutput:\n```\n===============================================\nETCD PERFORMANCE ANALYSIS\n===============================================\nAnalyzing etcd performance (last 5 minutes)...\nUsing etcd pod: etcd-dis016-p6vvv-master-0.us-central1-a.c.openshift-qe.internal\n\n===============================================\nDATABASE PERFORMANCE ANALYSIS\n===============================================\n\nFetching database statistics...\nDatabase Statistics:\nEndpoint: https://10.0.0.5:2379\n  Version: 3.5.24\n  DB Size: 94941184 bytes (90MB)\n  DB In Use: 51789824 bytes (49MB)\n  Keys: 50240\n  Raft Index: 57097\n  Raft Term: 8\n  Leader: YES\n\nEndpoint: https://10.0.0.3:2379\n  Version: 3.5.24\n  DB Size: 95363072 bytes (90MB)\n  DB In Use: 51789824 bytes (49MB)\n  Keys: 50240\n  Raft Index: 57097\n  Raft Term: 8\n  Leader: NO\n\nEndpoint: https://10.0.0.6:2379\n  Version: 3.5.24\n  DB Size: 94613504 bytes (90MB)\n  DB In Use: 51834880 bytes (49MB)\n  Keys: 50240\n  Raft Index: 57097\n  Raft Term: 8\n  Leader: NO\n\nFragmentation Analysis:\nEndpoint: https://10.0.0.5:2379\n  Fragmentation: 45% - NOTICE: Moderate fragmentation\nEndpoint: https://10.0.0.3:2379\n  Fragmentation: 45% - NOTICE: Moderate fragmentation\nEndpoint: https://10.0.0.6:2379\n  Fragmentation: 45% - NOTICE: Moderate fragmentation\n\n===============================================\nCLUSTER HEALTH\n===============================================\n\nhttps://10.0.0.5:2379 is healthy: successfully committed proposal: took = 9.848973ms\nhttps://10.0.0.3:2379 is healthy: successfully committed proposal: took = 14.309216ms\nhttps://10.0.0.6:2379 is healthy: successfully committed proposal: took = 14.829731ms\n\n===============================================\nLOG ANALYSIS (Last 5 minutes)\n===============================================\n\nSearching for performance-related warnings...\nSlow operations logged: 0\nDisk-related warnings: 0\nApply operation warnings: 0\n\nRecent compaction operations:\n{\"level\":\"info\",\"ts\":\"2025-11-19T06:15:10.136401Z\",\"caller\":\"mvcc/kvstore_compaction.go:72\",\"msg\":\"finished scheduled compaction\",\"compact-revision\":48026,\"took\":\"175.577699ms\",\"hash\":1330697744}\n\n===============================================\nLEADER STABILITY ANALYSIS\n===============================================\n\nLeader change events: 0\n\n===============================================\nNETWORK ANALYSIS\n===============================================\n\nNetwork-related issues: 0\n\n===============================================\nPERFORMANCE SUMMARY & RECOMMENDATIONS\n===============================================\n\nWARNING: Moderate database fragmentation (45%)\n  Recommendation: Monitor and consider defragmentation if performance degrades\n\nPerformance Metrics Summary:\n  - Database size: 90MB (recommended: < 8GB)\n  - Fragmentation: 45% (recommended: < 30%)\n  - Slow operations (5m): 0 (recommended: < 10)\n  - Leader changes (5m): 0 (recommended: < 5)\n\nStatus:  WARNING - Found 1 performance warnings\n```\n\n### Example 2: Extended analysis window\n```\n/etcd:analyze-performance --duration 30\n```\n\n## Common Performance Issues\n\n### High Database Fragmentation\n\n**Symptoms**: Database size significantly larger than in-use size (>30% fragmentation)\n\n**Investigation**:\n```bash\n# Check current fragmentation\noc exec -n openshift-etcd <pod> -c etcdctl -- etcdctl endpoint status --cluster -w json | jq\n```\n\n**Remediation**:\n```bash\n# Defragment each etcd member (run one at a time)\noc exec -n openshift-etcd <pod> -c etcdctl -- etcdctl defrag --cluster\n```\n\n**Recommendations**:\n- Schedule regular defragmentation during maintenance windows\n- Monitor fragmentation trends over time\n- Consider defragmentation when >30% fragmented\n\n### Slow Disk I/O\n\n**Symptoms**:\n- Disk-related warnings in logs (fsync, fdatasync)\n- Slow apply operations\n- High compaction times (>500ms)\n\n**Investigation**:\n```bash\n# Check disk performance on etcd nodes\noc debug node/<node-name> -- chroot /host fio --name=test --rw=write --bs=4k --size=1G --direct=1\n```\n\n**Recommendations**:\n- Use SSD or NVMe storage for etcd\n- Ensure dedicated disks for etcd (not shared with OS)\n- Check for disk saturation or competing I/O\n- Verify disk benchmarks meet etcd requirements (> 50 sequential IOPS)\n\n### Frequent Leader Changes\n\n**Symptoms**: Multiple leader change events in logs\n\n**Investigation**:\n```bash\n# Test network latency between control plane nodes\noc debug node/<node1> -- ping <node2-ip>\n\n# Check for network packet loss\noc debug node/<node1> -- ping -c 100 <node2-ip>\n```\n\n**Recommendations**:\n- Ensure etcd nodes are in same datacenter/availability zone\n- Check for network congestion or packet loss\n- Verify MTU settings across cluster network\n- Review network firewall rules and QoS settings\n\n### Large Database Size\n\n**Symptoms**:\n- Database size >8GB\n- Slow operations\n- High memory usage\n\n**Investigation**:\n```bash\n# Check database size across cluster\noc exec -n openshift-etcd <pod> -c etcdctl -- etcdctl endpoint status --cluster -w table\n```\n\n**Remediation**:\n```bash\n# Check event retention settings\noc get kubeapiserver cluster -o yaml | grep -A5 eventTTL\n\n# Review compaction settings\noc logs -n openshift-etcd <pod> -c etcd | grep compaction\n```\n\n**Recommendations**:\n- Review event retention policies\n- Consider more frequent compaction\n- Check for key churn and unnecessary data\n- Monitor database growth trends\n\n## Security Considerations\n\n- Metrics may expose cluster operational details\n- Requires cluster-admin permissions\n- Log analysis may contain sensitive data\n- Performance data should be treated as confidential\n\n## See Also\n\n- Etcd performance guide: https://etcd.io/docs/latest/tuning/\n- OpenShift etcd docs: https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-performance-scale-practices/\n- Related commands: `/etcd:health-check`\n\n## Notes\n\n- This command uses `etcdctl` and log analysis rather than direct metrics endpoint access\n- Performance thresholds are based on etcd upstream recommendations\n- Disk benchmarks should show > 50 sequential IOPS for etcd\n- Network latency < 50ms recommended between members\n- Analysis is point-in-time; trends require repeated checks over time\n- Compatible with etcd 3.5+ (OpenShift 4.x)\n- Log analysis window can be adjusted with `--duration` parameter\n- For production clusters, consider running during low-traffic periods\n- Health check latency is measured by actual proposal commits to the cluster"
              },
              {
                "name": "/health-check",
                "description": "Check etcd cluster health, member status, and identify issues",
                "path": "plugins/etcd/commands/health-check.md",
                "frontmatter": {
                  "description": "Check etcd cluster health, member status, and identify issues",
                  "argument-hint": "[--verbose]"
                },
                "content": "## Name\netcd:health-check\n\n## Synopsis\n```\n/etcd:health-check [--verbose]\n```\n\n## Description\n\nThe `health-check` command performs a comprehensive health check of the etcd cluster in an OpenShift environment. It examines etcd member status, cluster health, leadership, connectivity, and identifies potential issues that could affect cluster stability.\n\nEtcd is the critical key-value store that holds all cluster state for Kubernetes/OpenShift. Issues related to etcd can cause cluster-wide failures, so monitoring its health is essential.\n\nThis command is useful for:\n- Diagnosing cluster control plane issues\n- Verifying etcd cluster stability\n- Identifying split-brain scenarios\n- Checking member synchronization\n- Detecting disk space issues\n- Monitoring etcd performance\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **OpenShift CLI (oc)**\n   - Install from: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\n   - Verify with: `oc version`\n\n2. **Active cluster connection**\n   - Must be connected to an OpenShift cluster\n   - Verify with: `oc whoami`\n\n3. **Cluster admin permissions**\n   - Required to access etcd pods and execute commands\n   - Verify with: `oc auth can-i get pods -n openshift-etcd`\n\n4. **Healthy etcd namespace**\n   - The openshift-etcd namespace must exist\n   - At least one etcd pod must be running\n\n## Arguments\n\n- **--verbose** (optional): Enable detailed output\n  - Shows etcd member details\n  - Displays performance metrics\n  - Includes log snippets for errors\n  - Provides additional diagnostic information\n\n## Implementation\n\nThe command performs the following checks:\n\n### 1. Verify Prerequisites\n\nCheck if oc CLI is available and cluster is accessible:\n\n```bash\nif ! command -v oc &> /dev/null; then\n    echo \"Error: oc CLI not found. Please install OpenShift CLI.\"\n    exit 1\nfi\n\nif ! oc whoami &> /dev/null; then\n    echo \"Error: Not connected to an OpenShift cluster.\"\n    exit 1\nfi\n```\n\n### 2. Check Etcd Namespace and Pods\n\nVerify the etcd namespace exists and get pod status:\n\n```bash\necho \"Checking etcd namespace and pods...\"\n\nif ! oc get namespace openshift-etcd &> /dev/null; then\n    echo \"CRITICAL: openshift-etcd namespace not found\"\n    exit 1\nfi\n\n# Get etcd pod status\nETCD_PODS=$(oc get pods -n openshift-etcd -l app=etcd -o json)\nTOTAL_PODS=$(echo \"$ETCD_PODS\" | jq '.items | length')\nRUNNING_PODS=$(echo \"$ETCD_PODS\" | jq '[.items[] | select(.status.phase == \"Running\")] | length')\n\necho \"Etcd pods: $RUNNING_PODS/$TOTAL_PODS running\"\n\nif [ \"$RUNNING_PODS\" -eq 0 ]; then\n    echo \"CRITICAL: No etcd pods are running\"\n    exit 1\nfi\n\n# List all etcd pods with status\necho \"\"\necho \"Etcd Pod Status:\"\noc get pods -n openshift-etcd -l app=etcd -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,READY:.status.containerStatuses[0].ready,RESTARTS:.status.containerStatuses[0].restartCount,NODE:.spec.nodeName\n```\n\n### 3. Check Etcd Cluster Health\n\nUse etcdctl to check cluster health from each running etcd pod:\n\n```bash\necho \"\"\necho \"Checking etcd cluster health...\"\n\n# Get the first running etcd pod\nETCD_POD=$(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\nif [ -z \"$ETCD_POD\" ]; then\n    echo \"CRITICAL: No running etcd pod found\"\n    exit 1\nfi\n\n# Check cluster health\nHEALTH_OUTPUT=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl endpoint health --cluster -w table 2>&1)\n\nif echo \"$HEALTH_OUTPUT\" | grep -q \"is healthy\"; then\n    echo \"Cluster Health Status:\"\n    echo \"$HEALTH_OUTPUT\"\nelse\n    echo \"CRITICAL: Etcd cluster health check failed\"\n    echo \"$HEALTH_OUTPUT\"\n    exit 1\nfi\n```\n\n### 4. Check Etcd Member List\n\nList all etcd members and verify quorum:\n\n```bash\necho \"\"\necho \"Checking etcd member list...\"\n\nMEMBER_LIST=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl member list -w table 2>&1)\n\necho \"Etcd Members:\"\necho \"$MEMBER_LIST\"\n\n# Count members\nMEMBER_COUNT=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl member list -w json 2>/dev/null | jq '.members | length')\n\necho \"\"\necho \"Total members: $MEMBER_COUNT\"\n\nif [ \"$MEMBER_COUNT\" -lt 3 ]; then\n    echo \"WARNING: Etcd cluster has less than 3 members (quorum at risk)\"\nfi\n\n# Check for unstarted members\nUNSTARTED=$(echo \"$MEMBER_LIST\" | grep \"unstarted\" | wc -l)\nif [ \"$UNSTARTED\" -gt 0 ]; then\n    echo \"WARNING: $UNSTARTED member(s) in unstarted state\"\nfi\n```\n\n### 5. Check Etcd Leadership\n\nVerify there is a healthy leader:\n\n```bash\necho \"\"\necho \"Checking etcd leadership...\"\n\nENDPOINT_STATUS=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl endpoint status --cluster -w table 2>&1)\n\necho \"Endpoint Status:\"\necho \"$ENDPOINT_STATUS\"\n\n# Check if there's a leader\nif echo \"$ENDPOINT_STATUS\" | grep -q \"true\"; then\n    LEADER_ENDPOINT=$(echo \"$ENDPOINT_STATUS\" | grep \"true\" | awk '{print $2}')\n    echo \"\"\n    echo \"Leader: $LEADER_ENDPOINT\"\nelse\n    echo \"CRITICAL: No etcd leader elected\"\n    exit 1\nfi\n```\n\n### 6. Check Etcd Database Size\n\nCheck database size and fragmentation:\n\n```bash\necho \"\"\necho \"Checking etcd database size...\"\n\n# Get database size from endpoint status\nDB_SIZE=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcdctl -- etcdctl endpoint status --cluster -w json 2>/dev/null)\n\necho \"$DB_SIZE\" | jq -r '.[] | \"Endpoint: \\(.Endpoint) | DB Size: \\(.Status.dbSize) bytes | DB Size in Use: \\(.Status.dbSizeInUse) bytes\"'\n\n# Calculate fragmentation percentage\necho \"$DB_SIZE\" | jq -r '.[] |\n    if .Status.dbSize > 0 then\n        \"Fragmentation: \\(((.Status.dbSize - .Status.dbSizeInUse) * 100 / .Status.dbSize) | floor)%\"\n    else\n        \"Fragmentation: N/A\"\n    end'\n\n# Warn if database is too large\nMAX_DB_SIZE=$((8 * 1024 * 1024 * 1024))  # 8GB threshold\nCURRENT_SIZE=$(echo \"$DB_SIZE\" | jq -r '.[0].Status.dbSize')\n\nif [ \"$CURRENT_SIZE\" -gt \"$MAX_DB_SIZE\" ]; then\n    echo \"WARNING: Etcd database size ($CURRENT_SIZE bytes) exceeds recommended maximum (8GB)\"\n    echo \"Consider defragmentation or checking for excessive key growth\"\nfi\n```\n\n### 7. Check Disk Space on Etcd Nodes\n\nVerify disk space on nodes running etcd:\n\n```bash\necho \"\"\necho \"Checking disk space on etcd nodes...\"\n\nfor pod in $(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'); do\n    echo \"Pod: $pod\"\n    oc exec -n openshift-etcd \"$pod\" -c etcd -- df -h /var/lib/etcd | tail -1\n\n    # Get disk usage percentage\n    DISK_USAGE=$(oc exec -n openshift-etcd \"$pod\" -c etcd -- df -h /var/lib/etcd | tail -1 | awk '{print $5}' | sed 's/%//')\n\n    if [ \"$DISK_USAGE\" -gt 80 ]; then\n        echo \"WARNING: Disk usage on $pod is ${DISK_USAGE}% (threshold: 80%)\"\n    fi\n    echo \"\"\ndone\n```\n\n### 8. Check for Recent Etcd Errors\n\nCheck recent logs for errors or warnings:\n\n```bash\necho \"\"\necho \"Checking recent etcd logs for errors...\"\n\nRECENT_ERRORS=$(oc logs -n openshift-etcd \"$ETCD_POD\" -c etcd --tail=100 | grep -i \"error\\|warn\\|fatal\" | tail -10)\n\nif [ -n \"$RECENT_ERRORS\" ]; then\n    echo \"Recent errors/warnings found:\"\n    echo \"$RECENT_ERRORS\"\nelse\n    echo \"No recent errors in etcd logs\"\nfi\n```\n\n### 9. Check Etcd Performance Metrics (if --verbose)\n\nIf verbose mode is enabled, check performance metrics:\n\n```bash\nif [ \"$VERBOSE\" = \"true\" ]; then\n    echo \"\"\n    echo \"Checking etcd performance metrics...\"\n\n    # Get metrics from etcd pod\n    METRICS=$(oc exec -n openshift-etcd \"$ETCD_POD\" -c etcd -- curl -s http://localhost:2379/metrics 2>/dev/null)\n\n    # Parse key metrics\n    echo \"Backend Commit Duration (p99):\"\n    echo \"$METRICS\" | grep \"etcd_disk_backend_commit_duration_seconds\" | grep \"quantile=\\\"0.99\\\"\" | head -1\n\n    echo \"\"\n    echo \"WAL Fsync Duration (p99):\"\n    echo \"$METRICS\" | grep \"etcd_disk_wal_fsync_duration_seconds\" | grep \"quantile=\\\"0.99\\\"\" | head -1\n\n    echo \"\"\n    echo \"Leader Changes:\"\n    echo \"$METRICS\" | grep \"etcd_server_leader_changes_seen_total\" | head -1\nfi\n```\n\n### 10. Generate Summary Report\n\nCreate a summary of findings:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"Etcd Health Check Summary\"\necho \"===============================================\"\necho \"Check Time: $(date)\"\necho \"Cluster: $(oc whoami --show-server)\"\necho \"\"\necho \"Results:\"\necho \"  Etcd Pods Running: $RUNNING_PODS/$TOTAL_PODS\"\necho \"  Cluster Members: $MEMBER_COUNT\"\necho \"  Leader Elected: Yes\"\necho \"  Cluster Health: Healthy\"\necho \"\"\n\nif [ \"$WARNINGS\" -gt 0 ]; then\n    echo \"Status: WARNING - Found $WARNINGS warnings requiring attention\"\n    exit 0\nelse\n    echo \"Status: HEALTHY - All checks passed\"\n    exit 0\nfi\n```\n\n## Return Value\n\nThe command returns different exit codes:\n\n- **Exit 0**: Etcd cluster is healthy (may have warnings)\n- **Exit 1**: Critical issues detected (no running pods, no leader, health check failed)\n\n**Output Format**:\n- Human-readable report with section headers\n- Critical issues marked with \"CRITICAL:\"\n- Warnings marked with \"WARNING:\"\n- Success indicators for healthy checks\n\n## Examples\n\n### Example 1: Basic health check\n```\n/etcd:health-check\n```\n\nOutput:\n```\nChecking etcd namespace and pods...\nEtcd pods: 3/3 running\n\nEtcd Pod Status:\nNAME                                     STATUS    READY  RESTARTS  NODE\netcd-ip-10-0-21-125.us-east-2...        Running   true   0         ip-10-0-21-125\netcd-ip-10-0-43-249.us-east-2...        Running   true   0         ip-10-0-43-249\netcd-ip-10-0-68-109.us-east-2...        Running   true   0         ip-10-0-68-109\n\nChecking etcd cluster health...\nCluster Health Status:\n+------------------------------------------+--------+\n|                ENDPOINT                  | HEALTH |\n+------------------------------------------+--------+\n| https://10.0.21.125:2379                | true   |\n| https://10.0.43.249:2379                | true   |\n| https://10.0.68.109:2379                | true   |\n+------------------------------------------+--------+\n\nChecking etcd member list...\nEtcd Members:\n+------------------+---------+------------------------+\n|        ID        | STATUS  |          NAME          |\n+------------------+---------+------------------------+\n| 3a2b1c4d5e6f7890 | started | ip-10-0-21-125         |\n| 4b3c2d5e6f708901 | started | ip-10-0-43-249         |\n| 5c4d3e6f70890123 | started | ip-10-0-68-109         |\n+------------------+---------+------------------------+\n\nTotal members: 3\n\nChecking etcd leadership...\nLeader: https://10.0.21.125:2379\n\n===============================================\nEtcd Health Check Summary\n===============================================\nStatus: HEALTHY - All checks passed\n```\n\n### Example 2: Verbose health check with metrics\n```\n/etcd:health-check --verbose\n```\n\n## Common Issues and Remediation\n\n### No Etcd Leader\n\n**Symptoms**: Cluster shows no leader elected\n\n**Investigation**:\n```bash\noc logs -n openshift-etcd <etcd-pod> -c etcd | grep -i \"leader\"\noc get events -n openshift-etcd\n```\n\n**Remediation**:\n- Check network connectivity between etcd members\n- Verify etcd pods are running on different nodes\n- Check for clock skew between nodes\n\n### High Database Size\n\n**Symptoms**: Database size exceeds 8GB\n\n**Investigation**:\n```bash\noc exec -n openshift-etcd <etcd-pod> -c etcdctl -- etcdctl endpoint status -w table\n```\n\n**Remediation**:\n- Run defragmentation: `/etcd:defrag` (if command exists)\n- Check for excessive key creation (e.g., many events)\n- Review retention policies\n\n### Disk Space Issues\n\n**Symptoms**: Disk usage > 80% on etcd data directory\n\n**Investigation**:\n```bash\noc exec -n openshift-etcd <etcd-pod> -c etcd -- df -h /var/lib/etcd\n```\n\n**Remediation**:\n- Clean up old snapshots\n- Defragment database\n- Increase disk size if needed\n\n### Member Not Started\n\n**Symptoms**: Member shows \"unstarted\" status\n\n**Investigation**:\n```bash\noc logs -n openshift-etcd <etcd-pod> -c etcd\noc describe pod -n openshift-etcd <etcd-pod>\n```\n\n**Remediation**:\n- Check pod logs for errors\n- Verify certificates are valid\n- Check network policies and firewall rules\n\n## Security Considerations\n\n- Requires cluster-admin or equivalent permissions\n- Access to etcd data allows viewing all cluster secrets\n- Etcd metrics may contain sensitive information\n- Always use secure connections when accessing etcd\n\n## See Also\n\n- Etcd documentation: https://etcd.io/docs/\n- OpenShift etcd docs: https://docs.openshift.com/container-platform/latest/backup_and_restore/control_plane_backup_and_restore/\n- Related commands: `/etcd:analyze-performance`\n\n## Notes\n\n- This command is read-only and does not modify etcd\n- Checks are performed from within etcd pods using etcdctl\n- Some checks require etcd to be running\n- Performance may vary on large clusters with many keys\n- Database size recommendations are based on upstream etcd guidance"
              }
            ],
            "skills": []
          },
          {
            "name": "yaml",
            "description": "YAML documentation and utilities",
            "source": "./plugins/yaml",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install yaml@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/docs",
                "description": "Generate comprehensive YAML documentation from Go struct definitions with sensible default values",
                "path": "plugins/yaml/commands/docs.md",
                "frontmatter": {
                  "description": "Generate comprehensive YAML documentation from Go struct definitions with sensible default values",
                  "argument-hint": "[file:StructName] [output.md]"
                },
                "content": "## Name\nyaml:docs\n\n## Synopsis\n```\n/yaml:docs [file:StructName] [output.md]\n```\n\n## Description\nThe `yaml:docs` command generates comprehensive YAML documentation from Go struct definitions. It analyzes Go structs and produces complete, well-documented YAML configuration examples with intelligent default values for all fields.\n\nThis command is designed to help developers quickly create YAML configuration documentation by:\n- Automatically generating sensible default values for all struct fields\n- Adding inline comments explaining each field's purpose and constraints\n- Maintaining proper YAML formatting and structure\n- Supporting nested structs, slices, maps, and complex types\n- Respecting struct tags (yaml, json, validate, default)\n\nThe spec sections is inspired by https://man7.org/linux/man-pages/man7/man-pages.7.html#top_of_page\n\n## Implementation\n\nYou are a specialized tool for generating comprehensive YAML documentation from Go struct definitions.\n\n### Task\n\nAnalyze the provided Go struct and generate complete YAML documentation with:\n- All fields populated with intelligent, sensible default values (never leave fields empty)\n- Inline comments explaining each field's purpose and constraints\n- Proper YAML formatting and structure\n- Nested YAML for embedded structs with all sub-fields populated\n\n### Input Handling\n\nThe user may provide input in these formats:\n1. `$1 $2` - File path with struct name (e.g., `pkg/api/types.go:MetricsConfig`) and optional output file path\n2. `$1` - Just the file path with struct name\n3. Selected code containing a Go struct definition (no arguments)\n\n### Instructions\n\n1. **Locate the struct:**\n   - If a file path is provided (format: `file.go:StructName`), read that file and find the specified struct\n   - If code is selected, use the selected Go struct definition\n   - Search for the struct definition and any embedded struct types\n\n2. **Analyze struct metadata:**\n   - Examine struct tags: `yaml`, `json`, `validate`, `default`\n   - Note validation constraints (min, max, required, etc.)\n   - Identify field types (strings, ints, bools, slices, maps, nested structs, pointers)\n   - Preserve field ordering from the struct definition\n\n3. **Generate intelligent defaults:**\n   - **Strings**: Use contextually appropriate values based on field names (e.g., \"localhost\" for host, \"info\" for log level)\n   - **Integers**: Use common sensible values (e.g., 8080 for port, 30 for timeout seconds)\n   - **Booleans**: Default to `false` unless the field name suggests otherwise\n   - **Durations**: Use human-readable format (e.g., \"30s\", \"5m\", \"1h\")\n   - **Slices**: Provide 1-2 example values in array format\n   - **Maps**: Provide 1-2 example key-value pairs\n   - **Nested structs**: Recursively populate all sub-fields\n   - **Pointers**: Treat as optional but still provide example values\n\n4. **Format the output:**\n   - Use proper YAML indentation (2 spaces)\n   - Add inline comments with `#` explaining each field\n   - Include validation constraints in comments where applicable\n   - Add section headers for major struct groups\n   - Ensure valid YAML syntax\n\n5. **Write the output:**\n   - If an output file path is provided as `$2`, use the Write tool to create that file with the generated YAML content (write pure YAML, not markdown)\n   - Otherwise, display the generated YAML to the user in a markdown code block with yaml syntax highlighting\n\n### Important Behaviors\n\n- **ALWAYS populate all fields** - never leave fields empty or use placeholder text\n- Infer contextually appropriate defaults from field names and types\n- Include helpful comments explaining what each field does\n- Maintain the struct's field order in the YAML output\n- Handle complex nested structures by recursively applying these rules\n\n## Return Value\n- **Claude agent text**: Generated YAML documentation with intelligent defaults and inline comments\n- **File output** (if $2 provided): YAML file written to the specified path\n\n## Examples\n\n### Example 1: Basic usage with file path and struct name\n```\n/yaml:docs pkg/config/server.go:ServerConfig\n```\n\nInput struct:\n```go\ntype ServerConfig struct {\n    Host     string        `yaml:\"host\" json:\"host\" validate:\"required\"`\n    Port     int           `yaml:\"port\" json:\"port\" validate:\"min=1,max=65535\"`\n    Timeout  time.Duration `yaml:\"timeout\" json:\"timeout\"`\n    Debug    bool          `yaml:\"debug\" json:\"debug\"`\n    Features []string      `yaml:\"features\" json:\"features\"`\n}\n```\n\nOutput:\n```yaml\n# Server configuration\nhost: \"localhost\"          # Required: Server hostname or IP address\nport: 8080                 # Port number (1-65535)\ntimeout: \"30s\"             # Request timeout duration\ndebug: false               # Enable debug logging\nfeatures: [\"metrics\", \"tracing\"]  # List of enabled features\n```\n\n### Example 2: Complex nested structs with output file\n```\n/yaml:docs pkg/config/database.go:DatabaseConfig config/database.yaml\n```\n\nInput struct:\n```go\ntype DatabaseConfig struct {\n    Host     string            `yaml:\"host\"`\n    Port     int               `yaml:\"port\"`\n    SSL      SSLConfig         `yaml:\"ssl\"`\n    Pools    map[string]int    `yaml:\"pools\"`\n    Metadata *MetadataConfig   `yaml:\"metadata,omitempty\"`\n}\n\ntype SSLConfig struct {\n    Enabled  bool   `yaml:\"enabled\"`\n    CertFile string `yaml:\"cert_file\"`\n    KeyFile  string `yaml:\"key_file\"`\n}\n```\n\nGenerated YAML (written to config/database.yaml):\n```yaml\n# Database configuration\nhost: \"localhost\"                    # Database host\nport: 5432                          # Database port\nssl:                                # SSL configuration\n  enabled: true                     # Enable SSL connection\n  cert_file: \"/etc/ssl/certs/db.crt\" # SSL certificate file path\n  key_file: \"/etc/ssl/private/db.key\" # SSL private key file path\npools:                              # Connection pools configuration\n  read: 10                          # Read connection pool size\n  write: 5                          # Write connection pool size\nmetadata:                           # Optional metadata configuration\n  cache_ttl: \"1h\"                   # Cache time-to-live\n  sync_interval: \"5m\"               # Sync interval\n```\n\n### Example 3: Using with selected code\nSelect a Go struct definition in your editor, then run:\n```\n/yaml:docs\n```\n\nThe command will generate YAML documentation from the selected struct.\n\n## Arguments\n- $1: File path and struct name in format `file.go:StructName` (e.g., `pkg/api/types.go:MetricsConfig`), or selected code containing a Go struct definition\n- $2: (Optional) Output file path where the generated YAML will be written (e.g., `config/example.yaml`)"
              }
            ],
            "skills": []
          },
          {
            "name": "must-gather",
            "description": "A plugin to analyze and report on must-gather data",
            "source": "./plugins/must-gather",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install must-gather@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Quick analysis of must-gather data - runs all analysis scripts and provides comprehensive cluster diagnostics",
                "path": "plugins/must-gather/commands/analyze.md",
                "frontmatter": {
                  "description": "Quick analysis of must-gather data - runs all analysis scripts and provides comprehensive cluster diagnostics",
                  "argument-hint": "[must-gather-path] [component]"
                },
                "content": "## Name\nmust-gather:analyze\n\n## Synopsis\n```\n/must-gather:analyze [must-gather-path] [component]\n```\n\n## Description\n\nThe `analyze` command performs comprehensive analysis of OpenShift must-gather diagnostic data. It runs specialized Python analysis scripts to extract and summarize cluster health information across multiple components.\n\nThe command can analyze:\n- Cluster version and update status\n- Cluster operator health (degraded, progressing, unavailable)\n- Node conditions and resource status\n- Pod failures, restarts, and crash loops\n- Network configuration and OVN health\n- OVN databases - logical topology, ACLs, pods\n- Kubernetes events (warnings and errors)\n- etcd cluster health and quorum status\n- Persistent volume and claim status\n- Prometheus alerts\n\nYou can request analysis of the entire cluster or focus on a specific component.\n\n## Prerequisites\n\n**Required Directory Structure:**\n\nMust-gather data typically has this structure:\n```\nmust-gather/\n registry-ci-openshift-org-origin-...-sha256-<hash>/\n     cluster-scoped-resources/\n     namespaces/\n     ...\n```\n\nThe actual must-gather directory is the subdirectory with the hash name, not the parent directory.\n\n**Required Scripts:**\n\nAnalysis scripts are bundled with this plugin at:\n```\n<plugin-root>/skills/must-gather-analyzer/scripts/\n analyze_clusterversion.py\n analyze_clusteroperators.py\n analyze_nodes.py\n analyze_pods.py\n analyze_network.py\n analyze_ovn_dbs.py\n analyze_events.py\n analyze_etcd.py\n analyze_pvs.py\n```\n\nWhere `<plugin-root>` is the directory where this plugin is installed (typically `~/.cursor/commands/ai-helpers/plugins/must-gather/` or similar).\n\n## Error Handling\n\n**CRITICAL: Script-Only Analysis**\n\n- **NEVER** attempt to analyze must-gather data directly using bash commands, grep, or manual file reading\n- **ONLY** use the provided Python scripts in `plugins/must-gather/skills/must-gather-analyzer/scripts/`\n- If scripts are missing or not found:\n  1. Stop immediately\n  2. Inform the user that the analysis scripts are not available\n  3. Ask the user to ensure the scripts are installed at the correct path\n  4. Do NOT attempt alternative approaches\n\n**Script Availability Check:**\n\nBefore running any analysis:\n\n1. Locate the scripts directory by searching for a known script:\n   ```bash\n   SCRIPT_PATH=$(find ~ -name \"analyze_clusteroperators.py\" -path \"*/must-gather/skills/must-gather-analyzer/scripts/*\" 2>/dev/null | head -1)\n   \n   if [ -z \"$SCRIPT_PATH\" ]; then\n       echo \"ERROR: Must-gather analysis scripts not found.\"\n       echo \"Please ensure the must-gather plugin from ai-helpers is properly installed.\"\n       exit 1\n   fi\n   \n   # All scripts are in the same directory, so just get the directory\n   SCRIPTS_DIR=$(dirname \"$SCRIPT_PATH\")\n   ```\n\n2. If scripts cannot be found, STOP and report to the user:\n   ```\n   The must-gather analysis scripts could not be located. Please ensure the must-gather plugin from openshift-eng/ai-helpers is properly installed in your Claude Code plugins directory.\n   ```\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Validate Must-Gather Path**:\n   - If path not provided as argument, ask the user\n   - Check if path contains `cluster-scoped-resources/` and `namespaces/` directories\n   - If user provides root directory, automatically find the correct subdirectory\n   - Verify the path exists and is readable\n\n2. **Determine Analysis Scope**:\n\n   **STEP 1: Check for SPECIFIC component keywords**\n\n   If the user mentions a specific component, run ONLY that script:\n   - \"pods\", \"pod status\", \"containers\", \"crashloop\", \"failing pods\"  `analyze_pods.py` ONLY\n   - \"etcd\", \"etcd health\", \"quorum\"  `analyze_etcd.py` ONLY\n   - \"network\", \"networking\", \"ovn\", \"connectivity\"  `analyze_network.py` ONLY\n   - \"ovn databases\", \"ovn-dbs\", \"ovn db\", \"logical switches\", \"acls\"  `analyze_ovn_dbs.py` ONLY\n   - \"nodes\", \"node status\", \"node conditions\"  `analyze_nodes.py` ONLY\n   - \"operators\", \"cluster operators\", \"degraded\"  `analyze_clusteroperators.py` ONLY\n   - \"version\", \"cluster version\", \"update\", \"upgrade\"  `analyze_clusterversion.py` ONLY\n   - \"events\", \"warnings\", \"errors\"  `analyze_events.py` ONLY\n   - \"storage\", \"pv\", \"pvc\", \"volumes\", \"persistent\"  `analyze_pvs.py` ONLY\n   - \"alerts\", \"prometheus\", \"monitoring\"  `analyze_prometheus.py` ONLY\n   - \"windows\", \"windows nodes\", \"windows logs\", \"hns\", \"containerd\", \"hybrid-overlay\"  `analyze_windows_logs.py` ONLY\n\n   **STEP 2: No specific component mentioned**\n\n   If generic request like \"analyze must-gather\", \"/must-gather:analyze\", or \"check the cluster\", run ALL scripts in this order:\n   1. ClusterVersion (`analyze_clusterversion.py`)\n   2. Cluster Operators (`analyze_clusteroperators.py`)\n   3. Nodes (`analyze_nodes.py`)\n   4. Pods - problems only (`analyze_pods.py --problems-only`)\n   5. Network (`analyze_network.py`)\n   6. Events - warnings only (`analyze_events.py --type Warning --count 50`)\n   7. etcd (`analyze_etcd.py`)\n   8. Storage (`analyze_pvs.py`)\n   9. Monitoring (`analyze_prometheus.py`)\n   10. Windows Logs - ONLY if Windows nodes detected:\n       - First check if `host_service_logs/windows/` directory exists in the must-gather\n       - If directory exists, run: `analyze_windows_logs.py <must-gather-path>`\n       - If directory does not exist, skip silently (cluster has no Windows nodes)\n\n3. **Locate Plugin Scripts**:\n   - Use the script availability check from the Error Handling section to find the plugin root\n   - Store the scripts directory path in `$SCRIPTS_DIR`\n\n4. **Execute Analysis Scripts**:\n   ```bash\n   python3 \"$SCRIPTS_DIR/<script>.py\" <must-gather-path>\n   ```\n   \n   Example:\n   ```bash\n   python3 \"$SCRIPTS_DIR/analyze_clusteroperators.py\" ./must-gather.local.123/quay-io-...\n   ```\n\n5. **Synthesize Results**: Generate findings and recommendations based on script output\n\n## Return Value\n\nThe command outputs structured analysis results to stdout:\n\n**For Component-Specific Analysis:**\n- Script output for the requested component only\n- Focused findings and recommendations\n\n**For Full Analysis:**\n- Organized sections for each component\n- Executive summary of overall cluster health\n- Prioritized list of critical issues\n- Actionable recommendations\n- Suggested log files to review\n\n## Output Structure\n\n```\n================================================================================\nMUST-GATHER ANALYSIS SUMMARY\n================================================================================\n\n[Script outputs organized by component]\n\nCLUSTER VERSION:\n[output from analyze_clusterversion.py]\n\nCLUSTER OPERATORS:\n[output from analyze_clusteroperators.py]\n\nNODES:\n[output from analyze_nodes.py]\n\nPROBLEMATIC PODS:\n[output from analyze_pods.py --problems-only]\n\nNETWORK STATUS:\n[output from analyze_network.py]\n\nWARNING EVENTS (Last 50):\n[output from analyze_events.py --type Warning --count 50]\n\nETCD CLUSTER HEALTH:\n[output from analyze_etcd.py]\n\nSTORAGE (PVs/PVCs):\n[output from analyze_pvs.py]\n\nMONITORING (Alerts):\n[output from analyze_prometheus.py]\n\n================================================================================\nFINDINGS AND RECOMMENDATIONS\n================================================================================\n\nCritical Issues:\n- [Critical problems requiring immediate attention]\n\nWarnings:\n- [Potential issues or degraded components]\n\nRecommendations:\n- [Specific next steps for investigation]\n\nLogs to Review:\n- [Specific log files to examine based on findings]\n```\n\n## Examples\n\n1. **Full cluster analysis**:\n   ```\n   /must-gather:analyze ./must-gather/registry-ci-openshift-org-origin-4-20-...-sha256-abc123/\n   ```\n   Runs all analysis scripts and provides comprehensive cluster diagnostics.\n\n2. **Analyze pod issues only**:\n   ```\n   /must-gather:analyze ./must-gather/registry-ci-openshift-org-origin-4-20-...-sha256-abc123/ analyze the pod statuses\n   ```\n   Runs only `analyze_pods.py` to focus on pod-related issues.\n\n3. **Check etcd health**:\n   ```\n   /must-gather:analyze check etcd health\n   ```\n   Asks for must-gather path, then runs only `analyze_etcd.py`.\n\n4. **Network troubleshooting**:\n   ```\n   /must-gather:analyze ./must-gather/registry-ci-openshift-org-origin-4-20-...-sha256-abc123/ show me network issues\n   ```\n   Runs only `analyze_network.py` for network-specific analysis.\n\n## Notes\n\n- **Must-Gather Path**: Always use the subdirectory containing `cluster-scoped-resources/` and `namespaces/`, not the parent directory\n- **Script Dependencies**: Analysis scripts must be executable and have required Python dependencies installed\n- **Error Handling**: If scripts are not found or must-gather path is invalid, clear error messages are displayed\n- **Cross-Referencing**: The analysis attempts to correlate issues across components (e.g., degraded operator  failing pods)\n- **Pattern Detection**: Identifies patterns like multiple pod failures on the same node\n- **Actionable Output**: Focuses on insights and recommendations rather than raw data dumps\n- **Priority**: Issues are prioritized by severity (Critical > Warning > Info)\n\n## Arguments\n\n- **$1** (must-gather-path): Optional. Path to the must-gather directory (the subdirectory with the hash name). If not provided, the user will be asked.\n- **$2+** (component): Optional. If keywords for a specific component are detected, only that component's analysis script will run. Otherwise, all scripts run."
              },
              {
                "name": "/ovn-dbs",
                "description": "Analyze OVN databases from a must-gather using ovsdb-tool",
                "path": "plugins/must-gather/commands/ovn-dbs.md",
                "frontmatter": {
                  "description": "Analyze OVN databases from a must-gather using ovsdb-tool",
                  "argument-hint": "[must-gather-path]"
                },
                "content": "## Name\nmust-gather:ovn-dbs\n\n## Synopsis\n```\n/must-gather:ovn-dbs [must-gather-path] [--node <node-name>] [--query <json>]\n```\n\n## Description\n\nThe `ovn-dbs` command analyzes OVN Northbound and Southbound databases collected from clusters. It uses `ovsdb-tool` to query the binary database files (`.db`) collected per-node, providing detailed information about the logical network topology, pods, ACLs, and routers on each node.\n\nThe command automatically maps ovnkube pods to their corresponding nodes by reading pod specifications from the must-gather data.\n\n**Two modes of operation:**\n1. **Standard Analysis** (default): Runs pre-built analysis showing switches, ports, ACLs, and routers\n2. **Query Mode** (`--query`): Run custom OVSDB JSON queries for specific data extraction\n\n**What it analyzes:**\n- **Per-zone logical network topology**\n- **Logical Switches** and their ports\n- **Pod Logical Switch Ports** with namespace, pod name, and IP addresses\n- **Access Control Lists (ACLs)** with priorities, directions, and match rules\n- **Logical Routers** and their ports\n\n**Important:** This command only works with must-gathers from clusters, where each node/zone has its own database files.\n\n## Prerequisites\n\nThe must-gather should contain:\n```\nnetwork_logs/\n ovnk_database_store.tar.gz\n```\n\n**Required Tools:**\n\n- `ovsdb-tool` must be installed (from openvswitch package)\n  - Check with: `which ovsdb-tool`\n  - Install: `sudo dnf install openvswitch` or `sudo apt install openvswitch-common`\n\n**Analysis Script:**\n\nThe script is bundled with this plugin:\n```\n<plugin-root>/skills/must-gather-analyzer/scripts/analyze_ovn_dbs.py\n```\n\nWhere `<plugin-root>` is the directory where this plugin is installed (typically `~/.cursor/commands/ai-helpers/plugins/must-gather/` or similar).\n\nClaude will automatically locate it by searching for the script in the plugin installation directory, regardless of your current working directory.\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Locate Analysis Script**:\n   ```bash\n   SCRIPT_PATH=$(find ~ -name \"analyze_ovn_dbs.py\" -path \"*/must-gather/skills/must-gather-analyzer/scripts/*\" 2>/dev/null | head -1)\n   \n   if [ -z \"$SCRIPT_PATH\" ]; then\n       echo \"ERROR: analyze_ovn_dbs.py script not found.\"\n       echo \"Please ensure the must-gather plugin from ai-helpers is properly installed.\"\n       exit 1\n   fi\n   \n   SCRIPTS_DIR=$(dirname \"$SCRIPT_PATH\")\n   ```\n\n2. **Extract Database Tarball**:\n   - Locate `network_logs/ovnk_database_store.tar.gz`\n   - Extract if not already extracted\n   - Find all `*_nbdb` and `*_sbdb` files\n\n3. **Query Each Zone's Database**:\n   For each zone (node), query the Northbound database using `ovsdb-tool query`:\n\n   ```bash\n   ovsdb-tool query <zone>_nbdb '[\"OVN_Northbound\", {\"op\":\"select\", \"table\":\"<table>\", \"where\":[], \"columns\":[...]}]'\n   ```\n\n4. **Analyze and Display**:\n   - **Logical Switches**: Names and port counts\n   - **Logical Switch Ports**: Filter for pods (external_ids.pod=true), show namespace, pod name, and IP\n   - **ACLs**: Priority, direction, match rules, and actions\n   - **Logical Routers**: Names and port counts\n\n5. **Present Zone Summary**:\n   - Total counts per zone\n   - Detailed breakdowns\n   - Sorted and formatted output\n\n## Return Value\n\nThe command outputs structured analysis for each node:\n\n```\nFound 6 node(s)\n\n================================================================================\nNode: ip-10-0-26-145.us-east-2.compute.internal\nPod:  ovnkube-node-79cbh\n================================================================================\n  Logical Switches:      4\n  Logical Switch Ports:  55\n  ACLs:                  7\n  Logical Routers:       2\n\n  LOGICAL SWITCHES (4):\n  NAME                                                         PORTS\n  --------------------------------------------------------------------------------\n  transit_switch                                               6\n  ip-10-0-1-10.us-east-2.compute.internal                      7\n  ext_ip-10-0-1-10.us-east-2.compute.internal                  2\n  join                                                         2\n\n  POD LOGICAL SWITCH PORTS (5):\n  NAMESPACE                                POD                                           IP\n  ------------------------------------------------------------------------------------------------------------------------\n  openshift-dns                            dns-default-abc123                            10.128.0.5\n  openshift-monitoring                     prometheus-k8s-0                              10.128.0.10\n  openshift-etcd                           etcd-master-0                                 10.128.0.3\n  ...\n\n  ACCESS CONTROL LISTS (7):\n  PRIORITY   DIRECTION       ACTION          MATCH\n  ------------------------------------------------------------------------------------------------------------------------\n  1012       from-lport      allow           inport == @a4743249366342378346 && (ip4.mcast ...\n  1011       to-lport        drop            (ip4.mcast || mldv1 || mldv2 || ...\n  1001       to-lport        allow-related   ip4.src==10.128.0.2\n  ...\n\n  LOGICAL ROUTERS (2):\n  NAME                                                         PORTS\n  --------------------------------------------------------------------------------\n  ovn_cluster_router                                           3\n  GR_ip-10-0-1-10.us-east-2.compute.internal                   2\n```\n\n## Examples\n\n1. **Analyze all nodes in a must-gather**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/registry-ci-openshift-org-origin-4-20-...-sha256-abc123/\n   ```\n   Shows logical network topology for all nodes.\n\n2. **Analyze specific node**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../ --node ip-10-0-26-145\n   ```\n   Shows OVN database information only for the specified node (supports partial name matching).\n\n3. **Analyze master node**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../ --node master-0\n   ```\n   Filter to a specific master node using partial name matching.\n\n4. **Interactive usage without path**:\n   ```\n   /must-gather:ovn-dbs\n   ```\n   The command will ask for the must-gather path.\n\n5. **Check if pod exists in OVN**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../\n   ```\n   Then search the output for the pod name to see which node it's on and its IP allocation.\n\n6. **Investigate ACL rules on a specific node**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../ --node worker-1\n   ```\n   Review the ACL section for a specific node to understand traffic filtering rules.\n\n7. **Run custom OVSDB query** (Query Mode):\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../ --query '[\"OVN_Northbound\", {\"op\":\"select\", \"table\":\"ACL\", \"where\":[[\"priority\", \">\", 1000]], \"columns\":[\"priority\",\"match\",\"action\"]}]'\n   ```\n   Query ACLs with priority > 1000 across all nodes. Claude can construct the JSON query for any OVSDB table.\n\n8. **Query specific node with custom query**:\n   ```\n   /must-gather:ovn-dbs ./must-gather/.../ --node master-0 --query '[\"OVN_Northbound\", {\"op\":\"select\", \"table\":\"Logical_Switch\", \"where\":[], \"columns\":[\"name\",\"ports\"]}]'\n   ```\n   List all logical switches with their ports on master-0.\n\n9. **Query specific table** (Claude constructs JSON):\n   Just ask Claude to query a specific OVSDB table and it will construct the appropriate JSON query. For example:\n   - \"Show all Logical_Router_Static_Route entries\"\n   - \"Find ACLs with action 'drop'\"\n   - \"List Logical_Switch_Port entries where external_ids contains 'openshift-etcd'\"\n\n## Error Handling\n\n**Missing ovsdb-tool:**\n```\nError: ovsdb-tool not found. Please install openvswitch package.\n```\nSolution: Install openvswitch: `sudo dnf install openvswitch`\n\n**Missing database tarball:**\n```\nError: Database tarball not found: network_logs/ovnk_database_store.tar.gz\n```\nSolution: Ensure this is a must-gather from an OVN cluster.\n\n\n**Node not found:**\n```\nError: No databases found for node matching 'master-5'\n\nAvailable nodes:\n  - ip-10-0-77-117.us-east-2.compute.internal\n  - ip-10-0-26-145.us-east-2.compute.internal\n  - ip-10-0-1-194.us-east-2.compute.internal\n```\nSolution: Use one of the listed node names or a partial match.\n\n## Notes\n\n- **Binary Database Format**: Uses `ovsdb-tool` to read OVSDB binary files directly\n- **Per-Node Analysis**: Each node in IC mode has its own database (one NB and one SB per zone)\n- **Node Mapping**: Automatically correlates ovnkube pods to nodes by reading pod specs from must-gather\n- **Pod Discovery**: Pods are identified by `external_ids` with `pod=true`\n- **IP Extraction**: Pod IPs are parsed from the `addresses` field (format: \"MAC IP\")\n- **ACL Priorities**: Higher priority ACLs are processed first (shown at top)\n- **Node Filtering**: Supports partial name matching for convenience (e.g., \"--node master\" matches all masters)\n- **Query Mode**: Accepts raw OVSDB JSON queries in the format `[\"OVN_Northbound\", {\"op\":\"select\", \"table\":\"...\", ...}]`\n- **Claude Query Construction**: Claude can automatically construct OVSDB JSON queries based on natural language requests\n- **Performance**: Querying large databases may take a few seconds per node\n\n## Use Cases\n\n1. **Verify Pod Network Configuration**:\n   - Check if pods are registered in OVN\n   - Verify IP address assignments\n   - Confirm logical switch port creation\n\n2. **Troubleshoot Connectivity Issues**:\n   - Review ACL rules blocking traffic\n   - Check if pods are in correct logical switches\n   - Verify router configurations\n\n3. **Understand Topology**:\n   - See how zones are interconnected via transit_switch\n   - Review gateway router configurations\n   - Understand logical network structure\n\n4. **Audit Network Policies**:\n   - See ACL rules generated from NetworkPolicies\n   - Identify overly permissive or restrictive rules\n   - Check rule priorities and match conditions\n\n## Arguments\n\n- **$1** (must-gather-path): Optional. Path to the must-gather directory containing network_logs/. If not provided, user will be prompted.\n- **--node, -n** (node-name): Optional. Filter analysis to a specific node. Supports partial name matching (e.g., \"master-0\", \"ip-10-0-26-145\"). If no match is found, displays list of available nodes.\n- **--query, -q** (json-query): Optional. Run a raw OVSDB JSON query instead of standard analysis. Claude can construct the JSON query based on OVSDB transaction format. When provided, outputs raw JSON results instead of formatted analysis."
              },
              {
                "name": "/windows",
                "description": "Analyze Windows node logs and issues in must-gather data",
                "path": "plugins/must-gather/commands/windows.md",
                "frontmatter": {
                  "description": "Analyze Windows node logs and issues in must-gather data",
                  "argument-hint": "[must-gather-path] [--component COMPONENT]"
                },
                "content": "## Name\nmust-gather:windows\n\n## Synopsis\n```\n/must-gather:windows [must-gather-path] [--component COMPONENT] [--errors-only]\n```\n\n## Description\n\nThe `must-gather:windows` command analyzes Windows-specific logs collected during must-gather from Windows nodes in OpenShift clusters. It parses logs from Windows-specific components and identifies common Windows node issues.\n\nThis command analyzes logs from:\n- **kube-proxy** - Windows networking service\n- **hybrid-overlay** - OVN-Kubernetes hybrid networking (Linux-Windows pod communication)\n- **kubelet** - Windows node agent\n- **containerd** - Container runtime for Windows\n- **WICD** - Windows Instance Config Daemon (node configuration)\n- **csi-proxy** - Storage plugin for Windows\n\nUse this command when:\n- Troubleshooting Windows node issues\n- Investigating Windows pod failures\n- Analyzing Windows container runtime problems\n- Debugging hybrid-overlay networking issues\n- Reviewing HNS (Host Network Service) failures\n\n## Prerequisites\n\n**Windows Node Logs Collection:**\n\nWindows node logs must be collected during must-gather. This requires:\n1. The cluster has Windows nodes (labeled with `kubernetes.io/os=windows`)\n2. Must-gather was run with Windows log collection enabled\n\n**Expected Directory Structure:**\n```\nmust-gather/\n host_service_logs/\n     windows/\n         log_files/\n             kube-proxy/kube-proxy.log\n             hybrid-overlay/hybrid-overlay.log\n             kubelet/kubelet.log\n             containerd/containerd.log\n             wicd/\n                windows-instance-config-daemon.exe.INFO\n                windows-instance-config-daemon.exe.ERROR\n                windows-instance-config-daemon.exe.WARNING\n             csi-proxy/csi-proxy.log\n```\n\n## Implementation\n\n1. **Locate Windows logs**:\n   - Check for `host_service_logs/windows/log_files/` directory\n   - If not found, inform user that cluster may not have Windows nodes or logs weren't collected\n\n2. **Run Windows log analyzer**:\n   ```bash\n   python3 plugins/must-gather/skills/must-gather-analyzer/scripts/analyze_windows_logs.py \\\n     <must-gather-path>\n   ```\n\n3. **Parse logs for errors and warnings**:\n   - Search for error patterns specific to each component\n   - Categorize errors (HNS, Containerd, Hybrid-Overlay, etc.)\n   - Count occurrences and identify trends\n\n4. **Detect common Windows issues**:\n   - **HNS failures** - Pods stuck in ContainerCreating\n   - **Containerd errors** - Container runtime failures\n   - **Hybrid-overlay issues** - Linux-Windows connectivity problems\n   - **Kubelet failures** - Pod scheduling and management issues\n   - **WICD errors** - Node configuration problems\n   - **CSI-Proxy failures** - Storage mount issues\n\n5. **Generate report with**:\n   - Component status summary\n   - Error and warning counts\n   - Detected issues with severity (CRITICAL, HIGH, etc.)\n   - Recommendations for remediation\n   - Detailed error messages categorized by type\n\n## Return Value\n\nThe command outputs:\n\n```\n================================================================================\nWINDOWS NODE LOGS ANALYSIS\n================================================================================\nLog directory: <path>\n\nComponents analyzed: 6/8\nTotal log lines:     125,432\nTotal errors found:  23\nTotal warnings:      15\n\nCOMPONENT STATUS:\nCOMPONENT                 LINES      ERRORS     WARNINGS   STATUS\n--------------------------------------------------------------------------------\nkube-proxy                15,234     0          2           OK\nhybrid-overlay            8,912      5          3           ERRORS\nkubelet                   45,123     12         5           ERRORS\ncontainerd                32,456     6          4           ERRORS\nwicd-info                 12,345     0          1           OK\nwicd-error                234        0          0           OK\nwicd-warning              456        0          0           OK\ncsi-proxy                 10,672     0          0           OK\n\n================================================================================\nDETECTED ISSUES\n================================================================================\n\n1. [CRITICAL] HNS (Host Network Service) Failures Detected\n   Found 5 HNS-related errors. This typically causes pods to fail in ContainerCreating state.\n    Check Windows node networking configuration. May need to restart HNS service or reboot node.\n\n2. [CRITICAL] Container Runtime Failures\n   Found 6 containerd errors. Containers may fail to start.\n    Check containerd service status on Windows node. Review container image compatibility.\n\n================================================================================\nDETAILED ERRORS BY CATEGORY\n================================================================================\n\nHNS ERRORS (5):\n--------------------------------------------------------------------------------\n  [hybrid-overlay:1234] failed to create HNS endpoint for pod default/test-pod\n  [hybrid-overlay:2345] HNS network attach failed: endpoint not found\n  ...\n\nCONTAINERD ERRORS (6):\n--------------------------------------------------------------------------------\n  [containerd:567] failed to start container: runtime error\n  [kubelet:890] failed to create pod sandbox: containerd timeout\n  ...\n```\n\n## Examples\n\n1. **Analyze all Windows logs**:\n   ```\n   /must-gather:windows ./must-gather.local.123456789\n   ```\n   Analyzes all Windows component logs and provides comprehensive report.\n\n2. **Analyze specific component**:\n   ```\n   /must-gather:windows ./must-gather.local.123456789 --component kubelet\n   ```\n   Analyzes only kubelet logs from Windows nodes.\n\n3. **Summary only (skip detailed errors)**:\n   ```\n   /must-gather:windows ./must-gather.local.123456789 --errors-only\n   ```\n   Shows component status and detected issues without detailed error listing.\n\n4. **Analyze hybrid-overlay networking**:\n   ```\n   /must-gather:windows ./must-gather.local.123456789 --component hybrid-overlay\n   ```\n   Focuses on hybrid-overlay logs to troubleshoot Linux-Windows pod connectivity.\n\n5. **Analyze containerd runtime**:\n   ```\n   /must-gather:windows ./must-gather.local.123456789 --component containerd\n   ```\n   Reviews container runtime logs for Windows container failures.\n\n## Common Windows Issues Detected\n\n### HNS (Host Network Service) Failures\n**Symptoms:**\n- Pods stuck in `ContainerCreating`\n- `failed to create HNS endpoint` errors\n- Network attach failures\n\n**Recommendations:**\n- Restart HNS service: `Restart-Service hns`\n- Check Windows Firewall rules\n- Verify network adapter configuration\n- Consider node reboot if HNS is unresponsive\n\n### Containerd Runtime Errors\n**Symptoms:**\n- Containers fail to start\n- `runtime error` messages\n- Image pull failures\n\n**Recommendations:**\n- Verify image OS matches (Windows vs Linux)\n- Check containerd service status\n- Review image platform compatibility\n- Inspect containerd configuration\n\n### Hybrid-Overlay Networking Issues\n**Symptoms:**\n- Linux pods cannot reach Windows pods\n- Tunnel setup failures\n- OVN errors\n\n**Recommendations:**\n- Verify OVN-Kubernetes hybrid-overlay configuration\n- Check tunnel connectivity between nodes\n- Review network policies\n- Validate VXLAN configuration\n\n### Kubelet Issues\n**Symptoms:**\n- Pods not scheduling to Windows nodes\n- Pod sandbox creation failures\n- Runtime not ready errors\n\n**Recommendations:**\n- Check kubelet service status\n- Review node conditions\n- Verify container runtime connectivity\n- Check pod node selectors\n\n### WICD Configuration Errors\n**Symptoms:**\n- Node configuration failures\n- Instance config daemon errors\n\n**Recommendations:**\n- Review WICD error logs\n- Check node bootstrap configuration\n- Verify cloud provider integration\n\n## Notes\n\n- **No Windows Nodes**: If the command reports no Windows logs found, the cluster either has no Windows nodes or the logs weren't collected during must-gather\n- **Log Location**: Windows logs are collected from nodes labeled `kubernetes.io/os=windows`\n- **Collection Script**: Windows logs are gathered by the `gather_windows_nodes` script in the must-gather collection phase\n- **Error Patterns**: The analyzer uses regex patterns specific to each Windows component\n- **Max Errors**: By default, shows first 50 errors per log file; use `--max-errors` to adjust\n- **Cross-Reference**: Combine with `/must-gather:analyze` for full cluster analysis including Windows resources\n\n## Arguments\n\n- **$1** (must-gather-path): Optional. Path to must-gather directory. If not provided, user will be prompted.\n- **--component COMPONENT**: Optional. Analyze only a specific component (kubelet, containerd, hybrid-overlay, kube-proxy, wicd, csi-proxy).\n- **--errors-only**: Optional. Show summary and detected issues only, skip detailed error listings.\n- **--max-errors N**: Optional. Maximum number of errors to collect per log file (default: 50)."
              }
            ],
            "skills": [
              {
                "name": "Must-Gather Analyzer",
                "description": "Analyze OpenShift must-gather diagnostic data including cluster operators, pods, nodes,\nand network components. Use this skill when the user asks about cluster health, operator status,\npod issues, node conditions, or wants diagnostic insights from must-gather data.\n\nTriggers: \"analyze must-gather\", \"check cluster health\", \"operator status\", \"pod issues\",\n\"node status\", \"failing pods\", \"degraded operators\", \"cluster problems\", \"crashlooping\",\n\"network issues\", \"etcd health\", \"analyze clusteroperators\", \"analyze pods\", \"analyze nodes\"\n",
                "path": "plugins/must-gather/skills/must-gather-analyzer/SKILL.md",
                "frontmatter": {
                  "name": "Must-Gather Analyzer",
                  "description": "Analyze OpenShift must-gather diagnostic data including cluster operators, pods, nodes,\nand network components. Use this skill when the user asks about cluster health, operator status,\npod issues, node conditions, or wants diagnostic insights from must-gather data.\n\nTriggers: \"analyze must-gather\", \"check cluster health\", \"operator status\", \"pod issues\",\n\"node status\", \"failing pods\", \"degraded operators\", \"cluster problems\", \"crashlooping\",\n\"network issues\", \"etcd health\", \"analyze clusteroperators\", \"analyze pods\", \"analyze nodes\"\n"
                },
                "content": "# Must-Gather Analyzer Skill\n\nComprehensive analysis of OpenShift must-gather diagnostic data with helper scripts that parse YAML and display output in `oc`-like format.\n\n## Overview\n\nThis skill provides analysis for:\n- **ClusterVersion**: Current version, update status, and capabilities\n- **Cluster Operators**: Status, degradation, and availability\n- **Pods**: Health, restarts, crashes, and failures across namespaces\n- **Nodes**: Conditions, capacity, and readiness\n- **Network**: OVN/SDN diagnostics and connectivity\n- **Events**: Warning and error events across namespaces\n- **etcd**: Cluster health, member status, and quorum\n- **Storage**: PersistentVolumes and PersistentVolumeClaims status\n\n## Must-Gather Directory Structure\n\n**Important**: Must-gather data is contained in a subdirectory with a long hash name:\n```\nmust-gather/\n registry-ci-openshift-org-origin-...-sha256-<hash>/\n     cluster-scoped-resources/\n        config.openshift.io/clusteroperators/\n        core/nodes/\n     namespaces/\n        <namespace>/\n            pods/\n                <pod-name>/\n                    <pod-name>.yaml\n     network_logs/\n```\n\nThe analysis scripts expect the path to the **subdirectory** (the one with the hash), not the root must-gather folder.\n\n## Instructions\n\n### 1. Get Must-Gather Path\nAsk the user for the must-gather directory path if not already provided.\n- If they provide the root directory, look for the subdirectory with the hash name\n- The correct path contains `cluster-scoped-resources/` and `namespaces/` directories\n\n### 2. Choose Analysis Type\n\nBased on user's request, run the appropriate helper script:\n\n#### ClusterVersion Analysis\n```bash\n./scripts/analyze_clusterversion.py <must-gather-path>\n```\n\nShows cluster version information similar to `oc get clusterversion`:\n- Current version and update status\n- Progressing state\n- Available updates\n- Version conditions\n- Enabled capabilities\n- Update history\n\n#### Cluster Operators Analysis\n```bash\n./scripts/analyze_clusteroperators.py <must-gather-path>\n```\n\nShows cluster operator status similar to `oc get clusteroperators`:\n- Available, Progressing, Degraded conditions\n- Version information\n- Time since condition change\n- Detailed messages for operators with issues\n\n#### Pods Analysis\n```bash\n# All namespaces\n./scripts/analyze_pods.py <must-gather-path>\n\n# Specific namespace\n./scripts/analyze_pods.py <must-gather-path> --namespace <namespace>\n\n# Show only problematic pods\n./scripts/analyze_pods.py <must-gather-path> --problems-only\n```\n\nShows pod status similar to `oc get pods -A`:\n- Ready/Total containers\n- Status (Running, Pending, CrashLoopBackOff, etc.)\n- Restart counts\n- Age\n- Categorized issues (crashlooping, pending, failed)\n\n#### Nodes Analysis\n```bash\n./scripts/analyze_nodes.py <must-gather-path>\n\n# Show only nodes with issues\n./scripts/analyze_nodes.py <must-gather-path> --problems-only\n```\n\nShows node status similar to `oc get nodes`:\n- Ready status\n- Roles (master, worker)\n- Age\n- Kubernetes version\n- Node conditions (DiskPressure, MemoryPressure, etc.)\n- Capacity and allocatable resources\n\n#### Network Analysis\n```bash\n./scripts/analyze_network.py <must-gather-path>\n```\n\nShows network health:\n- Network type (OVN-Kubernetes, OpenShift SDN)\n- Network operator status\n- OVN pod health\n- PodNetworkConnectivityCheck results\n- Network-related issues\n\n#### Events Analysis\n```bash\n# Recent events (last 100)\n./scripts/analyze_events.py <must-gather-path>\n\n# Warning events only\n./scripts/analyze_events.py <must-gather-path> --type Warning\n\n# Events in specific namespace\n./scripts/analyze_events.py <must-gather-path> --namespace openshift-etcd\n\n# Show last 50 events\n./scripts/analyze_events.py <must-gather-path> --count 50\n```\n\nShows cluster events:\n- Event type (Warning, Normal)\n- Last seen timestamp\n- Reason and message\n- Affected object\n- Event count\n\n#### etcd Analysis\n```bash\n./scripts/analyze_etcd.py <must-gather-path>\n```\n\nShows etcd cluster health:\n- Member health status\n- Member list with IDs and URLs\n- Endpoint status (leader, version, DB size)\n- Quorum status\n- Cluster summary\n\n#### Storage Analysis\n```bash\n# All PVs and PVCs\n./scripts/analyze_pvs.py <must-gather-path>\n\n# PVCs in specific namespace\n./scripts/analyze_pvs.py <must-gather-path> --namespace openshift-monitoring\n```\n\nShows storage resources:\n- PersistentVolumes (capacity, status, claims)\n- PersistentVolumeClaims (binding, capacity)\n- Storage classes\n- Pending/unbound volumes\n\n#### Monitoring Analysis\n```bash\n# All alerts.\n./scripts/analyze_prometheus.py <must-gather-path>\n\n# Alerts in specific namespace\n./scripts/analyze_prometheus.py <must-gather-path> --namespace openshift-monitoring\n```\n\nShows monitoring information:\n- Alerts (state, namespace, name, active since, labels)\n- Total of pending/firing alerts\n\n### 3. Interpret and Report\n\nAfter running the scripts:\n1. Review the summary statistics\n2. Focus on items flagged with issues\n3. Provide actionable insights and next steps\n4. Suggest log analysis for specific components if needed\n5. Cross-reference issues (e.g., degraded operator  failing pods  node issues)\n\n## Output Format\n\nAll scripts provide:\n- **Summary Section**: High-level statistics with emoji indicators\n- **Table View**: `oc`-like formatted output\n- **Issues Section**: Detailed breakdown of problems\n\nExample summary format:\n```\n================================================================================\nSUMMARY: 25/28 operators healthy\n    3 operators with issues\n   1 progressing\n   2 degraded\n================================================================================\n```\n\n## Helper Scripts Reference\n\n### scripts/analyze_clusterversion.py\nParses: `cluster-scoped-resources/config.openshift.io/clusterversions/version.yaml`\nOutput: ClusterVersion table with detailed version info, conditions, and capabilities\n\n### scripts/analyze_clusteroperators.py\nParses: `cluster-scoped-resources/config.openshift.io/clusteroperators/`\nOutput: ClusterOperator status table with conditions\n\n### scripts/analyze_pods.py\nParses: `namespaces/*/pods/*/*.yaml` (individual pod directories)\nOutput: Pod status table with issues categorized\n\n### scripts/analyze_nodes.py\nParses: `cluster-scoped-resources/core/nodes/`\nOutput: Node status table with conditions and capacity\n\n### scripts/analyze_network.py\nParses: `network_logs/`, network operator, OVN resources\nOutput: Network health summary and diagnostics\n\n### scripts/analyze_events.py\nParses: `namespaces/*/core/events.yaml`\nOutput: Event table sorted by last occurrence\n\n### scripts/analyze_etcd.py\nParses: `etcd_info/` (endpoint_health.json, member_list.json, endpoint_status.json)\nOutput: etcd cluster health and member status\n\n### scripts/analyze_pvs.py\nParses: `cluster-scoped-resources/core/persistentvolumes/`, `namespaces/*/core/persistentvolumeclaims.yaml`\nOutput: PV and PVC status tables\n\n## Tips for Analysis\n\n1. **Start with Cluster Operators**: They often reveal system-wide issues\n2. **Check Timing**: Look at \"SINCE\" columns to understand when issues started\n3. **Follow Dependencies**: Degraded operator  check its namespace pods  check hosting nodes\n4. **Look for Patterns**: Multiple pods failing on same node suggests node issue\n5. **Cross-reference**: Use multiple scripts together for complete picture\n\n## Common Scenarios\n\n### \"Why is my cluster degraded?\"\n1. Run `analyze_clusteroperators.py` - identify degraded operators\n2. Run `analyze_pods.py --namespace <operator-namespace>` - check operator pods\n3. Run `analyze_nodes.py` - verify node health\n\n### \"Pods keep crashing\"\n1. Run `analyze_pods.py --problems-only` - find crashlooping pods\n2. Check which nodes they're on\n3. Run `analyze_nodes.py` - verify node conditions\n4. Suggest checking pod logs in must-gather data\n\n### \"Network connectivity issues\"\n1. Run `analyze_network.py` - check network health\n2. Run `analyze_pods.py --namespace openshift-ovn-kubernetes`\n3. Check PodNetworkConnectivityCheck results\n\n## Next Steps After Analysis\n\nBased on findings, suggest:\n- Examining specific pod logs in `namespaces/<ns>/pods/<pod>/<container>/logs/`\n- Reviewing events in `namespaces/<ns>/core/events.yaml`\n- Checking audit logs in `audit_logs/`\n- Analyzing metrics data if available\n- Looking at host service logs in `host_service_logs/`"
              }
            ]
          },
          {
            "name": "lvms",
            "description": "LVMS (Logical Volume Manager Storage) plugin for troubleshooting and debugging storage issues",
            "source": "./plugins/lvms",
            "category": null,
            "version": "0.1.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install lvms@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Comprehensive LVMS troubleshooting - analyzes LVMCluster, volume groups, PVCs, and storage issues on live clusters or must-gather",
                "path": "plugins/lvms/commands/analyze.md",
                "frontmatter": {
                  "description": "Comprehensive LVMS troubleshooting - analyzes LVMCluster, volume groups, PVCs, and storage issues on live clusters or must-gather",
                  "argument-hint": "[must-gather-path|--live] [--component storage|operator|volumes]"
                },
                "content": "## Name\nlvms:analyze\n\n## Synopsis\n```\n/lvms:analyze [must-gather-path] [--live] [--component <component>]\n```\n\n## Description\n\nThe `lvms:analyze` command provides comprehensive troubleshooting for OpenShift LVMS (Logical Volume Manager Storage). It analyzes the health and configuration of LVMCluster, volume groups, PVCs, TopoLVM CSI driver, and node-level storage to identify and diagnose common LVMS issues.\n\nThe command can operate in two modes:\n- **Must-gather analysis**: Analyzes LVMS must-gather data offline\n- **Live cluster analysis**: Connects to a running cluster and performs real-time diagnostics\n\nCommon issues detected:\n- PVCs stuck in Pending state\n- LVMCluster not reaching Ready state\n- Volume group creation failures\n- Missing or unhealthy physical volumes\n- TopoLVM CSI driver issues\n- Node-level disk availability problems\n- Thin pool configuration issues\n- Device class misconfigurations\n- Operator and vg-manager pod errors (from log analysis)\n\n## Prerequisites\n\n**For Live Cluster Analysis:**\n- `oc` CLI installed and configured\n- Active cluster connection: `oc whoami`\n- Read access to LVMS namespace (`openshift-lvm-storage` or older `openshift-storage`)\n- Ability to read cluster-scoped resources (CRDs, Nodes, PVs)\n\n**For Must-Gather Analysis:**\n- LVMS must-gather data extracted to a directory\n- Must-gather structure:\n  ```\n  must-gather/\n   registry-ci-openshift-org.../\n       cluster-scoped-resources/\n       namespaces/\n          openshift-lvm-storage/  (or openshift-storage for older versions)\n       ...\n  ```\n\n**Namespace Compatibility:**\n- LVMS namespace changed from `openshift-storage` to `openshift-lvm-storage` in recent versions\n- The command automatically detects which namespace is used in the must-gather\n- Both namespaces are supported for backward compatibility\n\n**Analysis Script:**\n- Python 3 script at: `plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py`\n- If script is missing, command will use built-in analysis logic\n\n## Implementation\n\nThe command performs the following steps:\n\n1. **Determine Analysis Mode**:\n   - If the `--live` flag is present, proceed with live cluster analysis\n   - If path argument is provided, proceed with must-gather analysis\n   - If neither provided, ask user which mode to use\n   - Validate prerequisites for selected mode\n\n2. **Validate Environment**:\n\n   **For Live Cluster:**\n   ```bash\n   # Verify oc CLI\n   which oc\n\n   # Verify cluster connection\n   oc whoami\n\n   # Check LVMS namespace exists (try both namespaces)\n   oc get namespace openshift-lvm-storage 2>/dev/null || oc get namespace openshift-storage\n   ```\n\n   **For Must-Gather:**\n   ```bash\n   # Verify path exists (checks both old and new namespaces)\n   ls {must-gather-path}/namespaces/openshift-lvm-storage 2>/dev/null || \\\n     ls {must-gather-path}/namespaces/openshift-storage\n\n   # Check for required directories\n   ls {must-gather-path}/cluster-scoped-resources/core/persistentvolumes\n\n   # Note: The analysis script automatically detects which namespace is used\n   ```\n\n3. **Determine Analysis Scope**:\n\n   Check for component-specific keywords in arguments:\n   - If the argument contains one or more of `storage`, `pvc`, `pv`, `volumes`, `pending` then only do storage/pvc analysis.\n   - If the argument contains one or more of `operator`, `lvmcluster`, `deployment`, `pods` then analyze operator health only\n   - If the argument contains one or more of `vg`, `volume group`, `disk`, `device` then do Volume group analysis only\n   - If the argument contains one or more of `node`, `devices`, `lsblk` then do node-level device analysis only (live cluster only)\n   - If the argument contains one or more of `logs`, `errors` then do pod log analysis only\n   - If no specific component provided then do full comprehensive analysis\n\n4. **Collect LVMS Resources**:\n\n   **Live Cluster Collection:**\n\n   First, detect which namespace LVMS is using:\n   ```bash\n   # Detect LVMS namespace (newer versions use openshift-lvm-storage, older use openshift-storage)\n   LVMS_NS=$(oc get namespace openshift-lvm-storage -o name 2>/dev/null | cut -d/ -f2)\n   if [ -z \"$LVMS_NS\" ]; then\n       LVMS_NS=\"openshift-storage\"\n   fi\n   ```\n\n   Then collect resources:\n   ```bash\n   # LVMCluster resources\n   oc get lvmcluster -n $LVMS_NS -o yaml\n\n   # LVMVolumeGroup status\n   oc get lvmvolumegroup -A -o yaml\n   oc get lvmvolumegroupnodestatus -A -o yaml\n\n   # Operator pods\n   oc get pods -n $LVMS_NS -o wide\n   oc get pods -n $LVMS_NS -o yaml\n\n   # Storage resources\n   oc get pvc -A -o yaml | grep -A 50 \"storageClassName: lvms-\"\n   oc get pv -o yaml | grep -A 50 \"storageClassName: lvms-\"\n\n   # Events in LVMS namespace\n   oc get events -n $LVMS_NS --sort-by='.lastTimestamp'\n\n   # Storage classes\n   oc get storageclass | grep lvms\n   oc get storageclass -o yaml | grep -A 20 \"provisioner: topolvm.io\"\n\n   # Node information\n   oc get nodes -o wide\n\n   # TopoLVM CSI components\n   oc get daemonset -n $LVMS_NS\n   oc get deployment -n $LVMS_NS\n   ```\n\n   **Must-Gather Collection:**\n   Use Python script if available (automatically detects namespace):\n   ```bash\n   python3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py {must-gather-path}\n   ```\n\n   The script automatically detects and uses the correct namespace (openshift-lvm-storage or openshift-storage).\n\n   Or use built-in file reading:\n   ```bash\n   # Find LVMCluster resources\n   find {must-gather-path} -name \"lvmclusters.yaml\"\n\n   # Find LVMVolumeGroup resources\n   find {must-gather-path} -name \"lvmvolumegroups.yaml\"\n\n   # Find operator pods\n   cat {must-gather-path}/namespaces/openshift-lvm-storage/pods.yaml\n\n   # Find events\n   cat {must-gather-path}/namespaces/openshift-lvm-storage/events.yaml\n   ```\n\n5. **Analyze LVMCluster Health**:\n\n   Check critical status fields:\n   ```yaml\n   # LVMCluster Status\n   status:\n     state: Ready | Progressing | Failed | Degraded | Unknown\n     ready: true | false\n     conditions:\n       - type: ResourcesAvailable\n         status: True | False\n         reason: ...\n         message: ...\n       - type: VolumeGroupsReady\n         status: True | False\n         reason: ...\n         message: ...\n   ```\n\n   Report findings:\n   ```\n   \n   LVMCLUSTER STATUS\n   \n\n   LVMCluster: lvmcluster-sample\n   State: Ready  | Progressing  | Failed \n   Ready: true  | false \n\n   Conditions:\n    ResourcesAvailable: True (All resources deployed)\n    VolumeGroupsReady: False (Volume group vg1 not found on node worker-0)\n\n   Device Classes:\n   - vg1: 3 nodes, thin pool enabled\n     Status: 2/3 nodes ready\n     Missing: worker-0\n\n   Issues Detected:\n    CRITICAL: Volume group not created on worker-0\n     WARNING: Thin pool size at 85% capacity\n   ```\n\n6. **Analyze Volume Groups**:\n\n   For each LVMVolumeGroup and LVMVolumeGroupNodeStatus:\n   ```bash\n   # Check VG status across nodes\n   oc get lvmvolumegroup -A -o json | jq -r '.items[] | {\n     name: .metadata.name,\n     namespace: .metadata.namespace,\n     status: .status\n   }'\n\n   # Check node-level VG status\n   oc get lvmvolumegroupnodestatus -A -o json | jq -r '.items[] | {\n     node: .metadata.name,\n     vgs: .spec.volumeGroups,\n     status: .status\n   }'\n   ```\n\n   Report findings:\n   ```\n   \n   VOLUME GROUP STATUS\n   \n\n   Volume Group: vg1\n   Nodes: 3\n\n   Node: master-0\n    VG Created: vg1\n    PV Count: 1\n    Free Space: 450 GiB / 500 GiB\n    Thin Pool: lvm-thin-pool-0 (90% allocated, 75% used)\n\n   Node: worker-0\n    VG Status: Failed\n    Error: No available devices found\n     Devices: /dev/sdb (rejected: already in use)\n\n   Issues Detected:\n    worker-0: Device /dev/sdb already has filesystem\n     master-0: Thin pool nearing capacity\n   ```\n\n7. **Analyze PVC/PV Issues**:\n\n   Find problematic PVCs:\n   ```bash\n   # Find pending PVCs using LVMS\n   oc get pvc -A -o json | jq -r '.items[] |\n     select(.spec.storageClassName | startswith(\"lvms-\")) |\n     select(.status.phase != \"Bound\") |\n     {namespace: .metadata.namespace, name: .metadata.name,\n      phase: .status.phase, storageClass: .spec.storageClassName}'\n   ```\n\n   For each problematic PVC:\n   ```bash\n   # Get PVC details\n   oc describe pvc {pvc-name} -n {namespace}\n\n   # Check events\n   oc get events -n {namespace} --field-selector involvedObject.name={pvc-name}\n   ```\n\n   Report findings:\n   ```\n   \n   STORAGE (PVC/PV) STATUS\n   \n\n   Total PVCs using LVMS: 15\n   Bound: 12 \n   Pending: 3 \n\n   Pending PVCs:\n\n   1. test-app/data-volume\n      Status: Pending (10m)\n      Storage Class: lvms-vg1\n      Requested: 100Gi\n\n      Recent Events:\n      - 10m Warning ProvisioningFailed:\n        \"failed to provision volume: no node has enough free space\"\n\n      Root Cause:\n       Insufficient free space in volume group vg1\n      Current available: 45Gi across all nodes\n      Largest available on single node: 20Gi\n\n   2. database/postgres-data\n      Status: Pending (5m)\n      Storage Class: lvms-vg1\n      Requested: 50Gi\n\n      Recent Events:\n      - 5m Warning ProvisioningFailed:\n        \"topology constraint not satisfied\"\n\n      Root Cause:\n        PVC has node affinity requiring worker-0\n       worker-0 has no functional volume group\n   ```\n\n8. **Analyze Operator Health**:\n\n   Check operator pods:\n   ```bash\n   # Get all pods in LVMS namespace\n   oc get pods -n $LVMS_NS -o json\n\n   # Check for crashloops, errors, restarts\n   oc get pods -n $LVMS_NS -o json | jq -r '.items[] |\n     {name: .metadata.name,\n      phase: .status.phase,\n      ready: .status.conditions[] | select(.type==\"Ready\") | .status,\n      restarts: .status.containerStatuses[].restartCount}'\n   ```\n\n   Check deployments and daemonsets:\n   ```bash\n   oc get deployment -n $LVMS_NS -o wide\n   oc get daemonset -n $LVMS_NS -o wide\n   ```\n\n   Report findings:\n   ```\n   \n   OPERATOR HEALTH\n   \n\n   Deployments:\n    lvms-operator: 1/1 replicas ready\n\n   DaemonSets:\n    vg-manager: 3/3 nodes ready\n\n   Pod Issues:\n\n    vg-manager-abc123 (worker-0)\n      Status: CrashLoopBackOff\n      Restarts: 15\n\n      Container Logs (last 20 lines):\n      Error: failed to create volume group: exit status 5\n      Error: volume group \"vg1\" creation failed\n\n      Root Cause:\n      Volume group vg1 not created on worker-0 due to device conflicts\n   ```\n\n9. **Analyze Node Device Status**:\n\n   For live clusters, check devices on nodes:\n   ```bash\n   # For each node, check available block devices\n   oc debug node/{node-name} -- chroot /host lsblk --paths --json -o NAME,ROTA,TYPE,SIZE,MODEL,FSTYPE,MOUNTPOINT\n\n   # Check which devices are being used by LVMS\n   oc debug node/{node-name} -- chroot /host vgs -o vg_name,pv_name,vg_size,vg_free\n   oc debug node/{node-name} -- chroot /host pvs -o pv_name,vg_name,pv_size,pv_free,pv_used\n   ```\n\n   Report findings:\n   ```\n   \n   NODE DEVICE STATUS\n   \n\n   Node: worker-0\n\n   Block Devices:\n    /dev/sda: 100GB (system disk, mounted as /)\n     /dev/sdb: 500GB (has ext4 filesystem, not available)\n    /dev/sdc: 500GB (available for LVMS)\n\n   Current VG Configuration:\n    No volume groups found\n\n   Issues:\n    Device /dev/sdb has existing filesystem (ext4)\n     Device /dev/sdc is available but not configured\n\n   Recommendations:\n   1. Wipe filesystem on /dev/sdb: wipefs -a /dev/sdb\n   2. Update LVMCluster to use /dev/sdc\n   3. Or remove /dev/sdb from LVMCluster deviceSelector\n   ```\n\n10. **Check TopoLVM Configuration**:\n\n    Verify operator installation:\n    ```bash\n    # Check operator pods\n    oc get pods -n $LVMS_NS -l app.kubernetes.io/component=lvms-operator\n\n    # Check storage classes\n    oc get storageclass -o json | jq -r '.items[] |\n      select(.provisioner == \"topolvm.io\") |\n      {name: .metadata.name,\n       parameters: .parameters,\n       volumeBindingMode: .volumeBindingMode}'\n    ```\n\n    Report findings:\n    ```\n    \n    TOPOLVM CSI DRIVER\n    \n\n    Operator Deployment:\n     lvms-operator: Running\n\n    Storage Classes:\n     lvms-vg1\n      Provisioner: topolvm.io\n      Volume Binding: WaitForFirstConsumer\n      Device Class: vg1\n      Filesystem: xfs\n\n    Note: CSI driver is integrated into the LVMS operator and vg-manager components\n    ```\n\n11. **Analyze Pod Logs**:\n\n    Extract and analyze error/warning messages from pod logs:\n\n    **Live Cluster:**\n    ```bash\n    # Get logs from vg-manager pods\n    for pod in $(oc get pods -n $LVMS_NS -l app.kubernetes.io/component=vg-manager -o name); do\n        oc logs -n $LVMS_NS $pod --tail=1000\n    done\n\n    # Get logs from lvms-operator pod\n    oc logs -n $LVMS_NS deployment/lvms-operator --tail=1000\n    ```\n\n    **Must-Gather:**\n    ```bash\n    # Pod logs are located at:\n    # namespaces/{lvms-namespace}/pods/{pod-name}/{container}/{container}/logs/current.log\n    ```\n\n    **Processing:**\n    ```bash\n    # Parse JSON-formatted log entries\n    # Extract error and warning level messages\n    # Deduplicate repeated errors from reconciliation loops\n    ```\n\n    Report findings:\n    ```\n    \n    POD LOGS ANALYSIS\n    \n\n    Pod: vg-manager-abc123\n    Unique errors/warnings: 2\n\n     2025-10-28T10:47:28Z: Reconciler error\n      Controller: lvmvolumegroup\n      Error Details:\n        failed to create/extend volume group vg1: failed to extend volume group vg1:\n        WARNING: VG name vg0 is used by VGs ...\n        Cannot use /dev/dm-10: device has a signature\n        Command requires all devices to be found.\n\n    Pod: lvms-operator-xyz456\n    Unique errors/warnings: 1\n\n     2025-10-28T10:52:48Z: failed to validate device class setup\n      Controller: lvmcluster\n      Error: VG vg1 on node Degraded is not in ready state\n    ```\n\n12. **Generate Comprehensive Report**:\n\n    Synthesize all findings:\n    ```\n    \n    LVMS ANALYSIS SUMMARY\n    \n\n    Analysis Mode: Live Cluster | Must-Gather\n    Cluster: {cluster-name}\n    LVMS Version: {version}\n    Analysis Time: {timestamp}\n\n     HEALTHY: {count}\n    - LVMCluster in Ready state\n    - 12/15 PVCs successfully bound\n    - Operator pods running on 2/3 nodes\n\n      WARNINGS: {count}\n    - Thin pool at 85% capacity on master-0\n    - vg-manager daemonset not ready on all nodes\n\n     CRITICAL ISSUES: {count}\n    - Volume group vg1 not created on worker-0\n    - 3 PVCs stuck in Pending state\n    - Device /dev/sdb on worker-0 has conflicting filesystem\n\n    \n    ROOT CAUSE ANALYSIS\n    \n\n    Primary Issue: Volume Group Creation Failure on worker-0\n\n    Chain of Impact:\n    1. Device /dev/sdb on worker-0 has existing ext4 filesystem\n    2. vg-manager cannot create volume group vg1\n    3. Volume group missing on worker-0\n    4. Storage provisioning not functional on worker-0\n    5. PVCs with node affinity to worker-0 stuck Pending\n\n    \n    RECOMMENDATIONS (Prioritized)\n    \n\n    CRITICAL (Fix Immediately):\n\n    1. Clean device on worker-0:\n       # Access the node\n       oc debug node/worker-0\n\n       # Wipe the filesystem\n       chroot /host wipefs -a /dev/sdb\n\n       # Verify device is clean\n       chroot /host lsblk /dev/sdb\n\n    2. Restart vg-manager to recreate volume group:\n       oc delete pod -n openshift-lvm-storage -l app.kubernetes.io/component=vg-manager\n\n    3. Verify volume group created:\n       oc debug node/worker-0 -- chroot /host vgs\n\n    4. Restart vg-manager on worker-0:\n       oc delete pod -n openshift-lvm-storage -l app.kubernetes.io/component=vg-manager\n\n    5. Verify PVCs bind:\n       oc get pvc -A | grep Pending\n\n    WARNINGS (Address Soon):\n\n    6. Expand thin pool or clean up unused volumes:\n       # List logical volumes by size\n       oc debug node/master-0 -- chroot /host lvs --units g\n\n       # Consider expanding thin pool or removing old volumes\n\n    \n    NEXT STEPS\n    \n\n    1. Review and execute recommendations above\n    2. Monitor LVMS operator logs:\n       oc logs -n openshift-lvm-storage deployment/lvms-operator -f\n    3. Check volume group status after fixes:\n       /lvms:analyze --live --component volumes\n    4. If issues persist, collect must-gather:\n       oc adm must-gather --image=quay.io/lvms_dev/lvms-must-gather:latest\n\n    \n    ADDITIONAL RESOURCES\n    \n\n    - LVMS Documentation:\n      https://github.com/openshift/lvm-operator/tree/main/docs\n\n    - Troubleshooting Guide:\n      https://github.com/openshift/lvm-operator/blob/main/docs/troubleshooting.md\n\n    - TopoLVM Documentation:\n      https://github.com/topolvm/topolvm\n\n    Logs to Review:\n    - /namespaces/openshift-lvm-storage/pods/lvms-operator-*/logs/manager/current.log\n    - /namespaces/openshift-lvm-storage/pods/vg-manager-*/logs/vg-manager/current.log\n    ```\n\n12. **Component-Specific Analysis**:\n\n    If user requests specific component:\n    - Run only relevant analysis sections\n    - Provide focused output for that component\n    - Skip irrelevant checks\n\n## Return Value\n\nThe command outputs a comprehensive analysis report to stdout:\n\n**Format:**\n- Structured sections for each component\n- Visual indicators:  (healthy),  (warning),  (critical)\n- Root cause analysis connecting related issues\n- Prioritized recommendations with specific commands\n- Links to relevant logs and documentation\n\n**Success States:**\n- **All Healthy**: Summary of healthy state with key metrics\n- **Warnings Found**: Issues identified with recommendations\n- **Critical Issues**: Detailed diagnosis with step-by-step remediation\n\n## Examples\n\n1. **Analyze live cluster (full analysis)**:\n   ```\n   /lvms:analyze --live\n   ```\n   Connects to current cluster and runs comprehensive LVMS diagnostics.\n\n2. **Analyze must-gather data**:\n   ```\n   /lvms:analyze ./must-gather/registry-ci-openshift-org-origin-4-18.../\n   ```\n   Analyzes LVMS must-gather data offline.\n\n3. **Check only PVC issues on live cluster**:\n   ```\n   /lvms:analyze --live check pending PVCs\n   ```\n   Runs focused analysis on storage/PVC issues only.\n\n4. **Analyze volume groups in must-gather**:\n   ```\n   /lvms:analyze ./must-gather/... --component volumes\n   ```\n   Analyzes only volume group status and configuration.\n\n5. **Debug operator health**:\n   ```\n   /lvms:analyze --live analyze operator pods\n   ```\n   Focuses on LVMS operator and TopoLVM component health.\n\n6. **Check specific node's storage**:\n   ```\n   /lvms:analyze --live check devices on worker-0\n   ```\n   Analyzes block devices and volume groups on specific node.\n\n7. **Analyze pod logs only (must-gather)**:\n   ```\n   /lvms:analyze ./must-gather/... --component logs\n   ```\n   Extracts and analyzes error messages from vg-manager and lvms-operator pod logs.\n\n8. **Analyze pod logs on live cluster**:\n   ```\n   /lvms:analyze --live --component logs\n   ```\n   Retrieves and analyzes current pod logs from running cluster.\n\n## Notes\n\n- **Must-Gather Path**: Always use the subdirectory containing `cluster-scoped-resources/` and `namespaces/`, not the parent directory\n- **Namespace Compatibility**: LVMS namespace changed from `openshift-storage` (older versions) to `openshift-lvm-storage` (newer versions). The command automatically detects and uses the correct namespace in both live clusters and must-gathers\n- **Live Cluster Access**: Requires read permissions to LVMS namespace and cluster-scoped resources\n- **Node Debugging**: For device-level analysis on live clusters, the command uses `oc debug node/...` which requires elevated privileges\n- **Pod Log Analysis**: Available for both live clusters (via `oc logs`) and must-gather data. Parses JSON-formatted logs, extracts errors/warnings, and deduplicates repeated reconciliation errors\n- **Python Script**: If `analyze_lvms.py` script is available, it will be used for must-gather analysis for better performance\n- **Cross-Component Correlation**: The analysis attempts to correlate issues across components (e.g., missing VG  pod crash  PVC pending  pod log errors)\n- **Actionable Output**: Focuses on root causes and specific remediation steps rather than raw data dumps\n- **Safety**: All recommendations include verification steps; no destructive operations are performed automatically\n\n## Arguments\n\n- **$1** (must-gather-path): Optional. Path to LVMS must-gather directory. If provided without `--live`, assumes must-gather analysis mode.\n  - Example: `./must-gather/registry-ci-openshift-org-origin-4-18.../`\n\n- **--live**: Optional flag. Use live cluster analysis mode. Requires active `oc` connection.\n  - Example: `/lvms:analyze --live`\n\n- **--component**: Optional. Focus analysis on specific component:\n  - `storage` / `pvc` / `volumes`: PVC and PV analysis\n  - `operator` / `pods`: Operator health and pod status\n  - `vg` / `volume-group`: Volume group configuration\n  - `node` / `devices`: Node-level device analysis\n  - `logs`: Pod log analysis\n  - `all`: Full analysis (default)\n\n- **Additional text**: Natural language text describing what to focus on (parsed for component keywords)\n  - Example: `check why PVCs are pending`\n  - Example: `analyze volume group on worker-0`\n\n## Troubleshooting\n\n**Cannot connect to cluster:**\n```bash\n# Verify oc is configured\noc whoami\noc cluster-info\n\n# Check namespace exists (try both old and new namespaces)\noc get namespace openshift-lvm-storage 2>/dev/null || \\\n  oc get namespace openshift-storage\n```\n\n**Must-gather path not found:**\n```bash\n# Verify directory structure (checks both namespaces)\nls {must-gather-path}/namespaces/openshift-lvm-storage 2>/dev/null || \\\n  ls {must-gather-path}/namespaces/openshift-storage\n\n# Use the correct subdirectory\nls {must-gather-path}/*/namespaces/openshift-lvm-storage 2>/dev/null || \\\n  ls {must-gather-path}/*/namespaces/openshift-storage\n```\n\n**Permission denied for node debugging:**\n```bash\n# Check permissions\noc auth can-i debug node\n\n# May require cluster-admin or privileged SCC\n```\n\n**Python script not found:**\n- Command will fall back to built-in analysis\n- For better performance, ensure script exists at:\n  `plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py`\n\n## Related Commands\n\n- `/must-gather:analyze` - General cluster analysis\n- `/olm:diagnose` - OLM and operator troubleshooting\n- `/prow-job:analyze-test-failure` - CI test failure analysis\n\n## Additional Resources\n\n- [LVMS GitHub Repository](https://github.com/openshift/lvm-operator)\n- [LVMS Troubleshooting Guide](https://github.com/openshift/lvm-operator/blob/main/docs/troubleshooting.md)\n- [TopoLVM Documentation](https://github.com/topolvm/topolvm)\n- [OpenShift Storage Documentation](https://docs.openshift.com/container-platform/latest/storage/index.html)"
              }
            ],
            "skills": [
              {
                "name": "LVMS Analyzer",
                "description": "Analyzes LVMS must-gather data to diagnose storage issues",
                "path": "plugins/lvms/skills/lvms-analyzer/SKILL.md",
                "frontmatter": {
                  "name": "LVMS Analyzer",
                  "description": "Analyzes LVMS must-gather data to diagnose storage issues"
                },
                "content": "# LVMS Analyzer Skill\n\nThis skill provides detailed guidance for analyzing LVMS (Logical Volume Manager Storage) must-gather data to identify and troubleshoot storage issues.\n\n## When to Use This Skill\n\nUse this skill when:\n- Analyzing LVMS must-gather data offline\n- Diagnosing PVCs stuck in Pending state\n- Investigating LVMCluster readiness issues\n- Troubleshooting volume group creation failures\n- Debugging TopoLVM CSI driver problems\n- Checking operator health in LVMS namespace\n\nThis skill is automatically invoked by the `/lvms:analyze` command when working with must-gather data.\n\n## Prerequisites\n\n**Required:**\n- LVMS must-gather directory extracted and accessible\n- Must-gather contains LVMS namespace directory:\n  - `namespaces/openshift-lvm-storage/` (newer versions)\n  - OR `namespaces/openshift-storage/` (older versions)\n- Python 3.6 or higher installed\n- PyYAML library: `pip install pyyaml`\n\n**Namespace Compatibility:**\n- LVMS namespace changed from `openshift-storage` to `openshift-lvm-storage` in recent versions\n- The analysis script automatically detects which namespace is present\n- Both namespaces are fully supported for backward compatibility\n\n**Must-Gather Structure:**\n```\nmust-gather/\n registry-{image-registry}-lvms-must-gather-{version}-sha256-{hash}/\n     cluster-scoped-resources/\n        core/\n           persistentvolumes/\n               pvc-*.yaml               # Individual PV files\n        storage.k8s.io/\n           storageclasses/\n               lvms-vg1.yaml\n               lvms-vg1-immediate.yaml\n        security.openshift.io/\n            securitycontextconstraints/\n                lvms-vgmanager.yaml\n     namespaces/\n        openshift-lvm-storage/           # or openshift-storage for older versions\n            oc_output/                   # IMPORTANT: Primary location for LVMS resources\n               lvmcluster.yaml          # Full LVMCluster resource with status\n               lvmcluster               # Text output (oc describe)\n               lvmvolumegroup           # Text output\n               lvmvolumegroupnodestatus # Text output\n               logicalvolume            # Text output\n               pods                     # Text output (oc get pods)\n               events                   # Text output\n            pods/\n               lvms-operator-{hash}/\n                  lvms-operator-{hash}.yaml\n               vg-manager-{hash}/\n                   vg-manager-{hash}.yaml\n            apps/                        # May contain deployments/daemonsets\n     ...\n```\n\n**Key Note:** LVMS resources are primarily in the `oc_output/` directory, with `lvmcluster.yaml` being the most important file containing full cluster and node status.\n\n## Implementation Steps\n\n### Step 1: Validate Must-Gather Path\n\nBefore running analysis, verify the must-gather directory structure:\n\n```bash\n# Check if LVMS namespace directory exists (try both namespaces)\nls {must-gather-path}/namespaces/openshift-lvm-storage 2>/dev/null || \\\n  ls {must-gather-path}/namespaces/openshift-storage\n\n# Verify required resource directories\nls {must-gather-path}/cluster-scoped-resources/core/persistentvolumes\n```\n\n**Namespace Detection:**\nThe analysis script automatically detects which namespace is present:\n- Newer LVMS versions use `openshift-lvm-storage`\n- Older LVMS versions use `openshift-storage`\n- The script will inform you which namespace was detected\n\n**Common Issue:** User provides parent directory instead of subdirectory\n- Must-gather extracts to a directory like `must-gather.local.12345/`\n- Inside is a subdirectory like `registry-ci-openshift-org-origin-4-18.../`\n- Always use the **subdirectory** (the one with cluster-scoped-resources/ and namespaces/)\n\n**Handling:**\n```bash\n# If user provides parent directory, try to find the correct subdirectory\nif [ ! -d \"{path}/namespaces/openshift-lvm-storage\" ] && \\\n   [ ! -d \"{path}/namespaces/openshift-storage\" ]; then\n    # Try to find either namespace\n    find {path} -type d \\( -name \"openshift-lvm-storage\" -o -name \"openshift-storage\" \\) -path \"*/namespaces/*\"\n    # Suggest the correct path to user\nfi\n```\n\n### Step 2: Run Analysis Script\n\nUse the Python analysis script for structured analysis:\n\n```bash\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    {must-gather-path}\n```\n\n**Script Location:**\n- Always use: `plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py`\n- Use relative path from repository root\n- Script is part of the LVMS plugin\n\n**Component-Specific Analysis:**\n\nFor focused analysis on specific components:\n\n```bash\n# Analyze only storage/PVC issues\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    {must-gather-path} --component storage\n\n# Analyze only operator health\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    {must-gather-path} --component operator\n\n# Analyze only volume groups\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    {must-gather-path} --component volumes\n\n# Analyze only pod logs\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    {must-gather-path} --component logs\n```\n\n### Step 3: Interpret Analysis Results\n\nThe script provides structured output across several sections:\n\n**1. LVMCluster Status**\n\nKey fields to check:\n- `state`: Should be \"Ready\"\n- `ready`: Should be true\n- `conditions`: All should have status \"True\"\n  - ResourcesAvailable: Resources deployed successfully\n  - VolumeGroupsReady: VGs created on all nodes\n\nExample healthy output:\n```\nLVMCluster: lvmcluster-sample\n State: Ready\n Ready: true\n\nConditions:\n ResourcesAvailable: True\n VolumeGroupsReady: True\n```\n\nExample unhealthy output (real case from must-gather):\n```\nLVMCluster: my-lvmcluster\n State: Degraded\n Ready: false\n\nConditions:\n ResourcesAvailable: True\n  Reason: ResourcesAvailable\n  Message: Reconciliation is complete and all the resources are available\n VolumeGroupsReady: False\n  Reason: VGsDegraded\n  Message: One or more VGs are degraded\n```\n\n**2. Volume Group Status**\n\nChecks volume group creation per node and device availability:\n\nExample output (real case from must-gather):\n```\nVolume Group/Device Class: vg1\nNodes: 3\n\n  Node: ocpnode1.ocpiopex.growipx.com\n    Status: Progressing\n\n  Devices: /dev/mapper/3600a098038315048302b586c38397562, /dev/mapper/mpatha\n\n  Excluded devices: 24 device(s)\n    - /dev/sdb: /dev/sdb has children block devices and could not be considered\n    - /dev/sdb4: /dev/sdb4 has an invalid filesystem signature (xfs) and cannot be used\n    - /dev/mapper/3600a098038315047433f586c53477272: has an invalid filesystem signature (xfs)\n    ... and 21 more excluded devices\n\n  Node: ocpnode2.ocpiopex.growipx.com\n   Status: Degraded\n\n  Reason:\n  failed to create/extend volume group vg1: failed to extend volume group vg1:\n  WARNING: VG name vg0 is used by VGs VVnkhP-khYQ-blyc-2TNo-d3cv-b6di-4RbSyY and EUV3xv-ft6q-39xK-J3ki-rglf-9H44-rVIHIq.\n  Fix duplicate VG names with vgrename uuid, a device filter, or system IDs.\n  Physical volume '/dev/mapper/3600a098038315048302b586c38397578p3' is already in volume group 'vg0'\n  Unable to add physical volume '/dev/mapper/3600a098038315048302b586c38397578p3' to volume group 'vg0'\n  ... (truncated, see LVMCluster status for full details)\n\n  Devices: /dev/mapper/mpatha\n```\n\nThis real example shows a common LVMS issue: duplicate volume group names preventing VG extension.\n\n**3. Storage (PVC/PV) Status**\n\nLists pending or failed PVCs:\n\nExample output:\n```\nPending PVCs:\n\ndatabase/postgres-data\n Status: Pending (10m)\n  Storage Class: lvms-vg1\n  Requested: 100Gi\n\n  Recent Events:\n    ProvisioningFailed: no node has enough free space\n```\n\n**4. Operator Health**\n\nChecks LVMS operator pods, deployments, and daemonsets:\n\nExample issues:\n```\n vg-manager-abc123 (worker-0)\n  Status: CrashLoopBackOff\n  Restarts: 15\n  Error: volume group \"vg1\" not found\n```\n\n**5. Pod Logs**\n\nExtracts and analyzes error/warning messages from pod logs:\n\nExample output (from real must-gather):\n```\n\nPOD LOGS ANALYSIS\n\n\nPod: vg-manager-nz4pc\nUnique errors/warnings: 1\n\n 2025-10-28T10:47:28Z: Reconciler error\n  Controller: lvmvolumegroup\n  Error Details:\n    failed to create/extend volume group vg1: failed to extend volume group vg1:\n    WARNING: VG name vg0 is used by VGs WsNJwk-DK3q-tSHg-zvQJ-imF1-SdRv-8oh4e0 ...\n    Cannot use /dev/dm-10: device is too small (pv_min_size)\n    Command requires all devices to be found.\n\nPod: lvms-operator-65df9f4dbb-92jwl\nUnique errors/warnings: 1\n\n 2025-10-28T10:52:48Z: failed to validate device class setup\n  Controller: lvmcluster\n  Error: VG vg1 on node Degraded is not in ready state (ocpnode1.ocpiopex.growipx.com)\n```\n\n**Key Points:**\n- Logs are parsed from JSON format\n- Errors are deduplicated (same error repeated in reconciliation loops)\n- Shows unique error messages with first occurrence timestamp\n- Provides additional context not visible in resource status\n\n### Step 4: Analyze Root Causes\n\nConnect related issues to identify root causes:\n\n**Common Pattern 1: Device Filesystem Conflict**\n```\nChain of failures:\n1. Device /dev/sdb has existing ext4 filesystem\n2. vg-manager cannot create volume group\n3. Volume group missing on node\n4. PVCs stuck in Pending\n\nRoot cause: Device not properly wiped before LVMS use\n```\n\n**Common Pattern 2: Insufficient Capacity**\n```\nChain of failures:\n1. Thin pool at 95% capacity\n2. No free space for new volumes\n3. PVCs stuck in Pending\n\nRoot cause: Insufficient storage capacity or old volumes not cleaned up\n```\n\n**Common Pattern 3: Node-Specific Failures**\n```\nChain of failures:\n1. Volume group missing on specific node\n2. TopoLVM CSI driver not functional on that node\n3. PVCs with node affinity to that node stuck Pending\n\nRoot cause: Node-specific device configuration issue\n```\n\n### Step 5: Generate Remediation Plan\n\nBased on analysis results, provide prioritized recommendations:\n\n**CRITICAL Issues (Fix Immediately):**\n\n1. **Device Conflicts:**\n   ```bash\n   # Clean device on affected node\n   oc debug node/{node-name}\n   chroot /host wipefs -a /dev/{device}\n\n   # Restart vg-manager to recreate VG\n   oc delete pod -n openshift-lvm-storage -l app.kubernetes.io/component=vg-manager\n   ```\n\n2. **Pod Crashes:**\n   ```bash\n   # After fixing underlying issue, restart failed pods\n   oc delete pod -n openshift-lvm-storage {pod-name}\n   ```\n\n3. **LVMCluster Not Ready:**\n   ```bash\n   # Review and fix device configuration\n   oc edit lvmcluster -n openshift-lvm-storage\n\n   # Ensure devices match actual available devices\n   ```\n\n**WARNING Issues (Address Soon):**\n\n1. **Capacity Issues:**\n   ```bash\n   # Check logical volume usage\n   oc debug node/{node} -- chroot /host lvs --units g\n\n   # Remove unused volumes or expand thin pool\n   ```\n\n2. **Partial Node Coverage:**\n   ```bash\n   # Investigate why daemonsets not on all nodes\n   oc get nodes --show-labels\n   oc describe daemonset -n openshift-lvm-storage\n   ```\n\n### Step 6: Provide Next Steps\n\nAlways provide clear next steps:\n\n1. **Review logs** (if available in must-gather):\n   - Operator logs: `namespaces/openshift-lvm-storage/pods/lvms-operator-*/logs/`\n   - VG-manager logs: `namespaces/openshift-lvm-storage/pods/vg-manager-*/logs/`\n   - TopoLVM logs: `namespaces/openshift-lvm-storage/pods/topolvm-*/logs/`\n\n2. **Verify fixes** (if cluster is accessible):\n   ```bash\n   # After implementing fixes, verify:\n   oc get lvmcluster -n openshift-lvm-storage\n   oc get lvmvolumegroup -A\n   oc get pvc -A | grep Pending\n   ```\n\n3. **Re-collect must-gather** (if making changes):\n   ```bash\n   oc adm must-gather --image=quay.io/lvms_dev/lvms-must-gather:latest\n   ```\n\n## Error Handling\n\n### Script Execution Errors\n\n**Script not found:**\n```bash\n# Verify script exists\nls plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py\n\n# Ensure it's executable\nchmod +x plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py\n```\n\n**Python dependencies missing:**\n```bash\n# Install PyYAML\npip install pyyaml\n\n# Or use pip3\npip3 install pyyaml\n```\n\n**Invalid YAML in must-gather:**\n- Script handles YAML parsing errors gracefully\n- Reports which files failed to parse\n- Continues analysis with available data\n\n### Must-Gather Issues\n\n**Missing directories:**\n- Script validates required directories exist\n- Reports missing components\n- Provides guidance on what's missing\n\n**Incomplete must-gather:**\n- If critical resources missing, script reports what it can analyze\n- Suggests re-collecting must-gather\n\n## Examples\n\n### Example 1: Full Analysis\n\n```bash\n# Run comprehensive analysis\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    ./must-gather/registry-ci-openshift-org-origin-4-18.../\n```\n\nOutput:\n```\n\nLVMCLUSTER STATUS\n\n\nLVMCluster: lvmcluster-sample\n State: Failed\n Ready: false\n...\n\n\nLVMS ANALYSIS SUMMARY\n\n\n CRITICAL ISSUES: 3\n  - LVMCluster not Ready (state: Failed)\n  - Volume group vg1 not created on worker-0\n  - 3 PVCs stuck in Pending state\n```\n\n### Example 2: Storage-Only Analysis\n\n```bash\n# Focus on PVC issues\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    ./must-gather/... --component storage\n```\n\nAnalyzes only:\n- PVC/PV status\n- Storage class configuration\n- Volume provisioning issues\n\n### Example 3: Operator Health Check\n\n```bash\n# Check operator components\npython3 plugins/lvms/skills/lvms-analyzer/scripts/analyze_lvms.py \\\n    ./must-gather/... --component operator\n```\n\nAnalyzes only:\n- LVMCluster resource\n- Deployments and daemonsets\n- Pod status and crashes\n\n## Best Practices\n\n1. **Always validate path first:**\n   - Check for `namespaces/openshift-lvm-storage/` directory\n   - Use the correct subdirectory, not parent\n\n2. **Run full analysis first:**\n   - Get overall health picture\n   - Then drill down with component-specific analysis if needed\n\n3. **Correlate issues:**\n   - Look for patterns across components\n   - Connect pod failures to VG issues to PVC problems\n\n4. **Check timestamps:**\n   - Events and pod restarts have timestamps\n   - Helps understand sequence of failures\n\n5. **Provide actionable output:**\n   - Don't just list issues\n   - Explain root causes\n   - Give specific remediation steps\n   - Include verification commands\n\n6. **Reference documentation:**\n   - Link to LVMS troubleshooting guide\n   - Point to relevant sections in must-gather logs\n\n## Additional Resources\n\n- [LVMS Troubleshooting Guide](https://github.com/openshift/lvm-operator/blob/main/docs/troubleshooting.md)\n- [LVMS Architecture](https://github.com/openshift/lvm-operator/tree/main/docs)\n- [TopoLVM Documentation](https://github.com/topolvm/topolvm)\n- [Must-Gather Collection](https://github.com/openshift/lvm-operator/tree/main/must-gather)"
              }
            ]
          },
          {
            "name": "metrics",
            "description": "Anonymous metrics usage for ai-helpers",
            "source": "./plugins/metrics",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install metrics@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "hcp",
            "description": "Generate HyperShift cluster creation commands via hcp CLI from natural language descriptions",
            "source": "./plugins/hcp",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install hcp@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/cluster-health-check",
                "description": "Perform comprehensive health check on HCP cluster and report issues",
                "path": "plugins/hcp/commands/cluster-health-check.md",
                "frontmatter": {
                  "description": "Perform comprehensive health check on HCP cluster and report issues",
                  "argument-hint": "<cluster-name> [--verbose] [--output-format json|text]"
                },
                "content": "## Name\nhcp:cluster-health-check\n\n## Synopsis\n\n```\n/hcp:cluster-health-check <cluster-name> [--verbose] [--output-format json|text]\n```\n\n## Description\n\nThe `/hcp:cluster-health-check` command performs an extensive diagnostic of a Hosted Control Plane (HCP) cluster to assess its operational health, stability, and performance. It automates the validation of multiple control plane and infrastructure components to ensure the cluster is functioning as expected.\n\nThe command runs a comprehensive set of health checks covering control plane pods, etcd, node pools, networking, and infrastructure readiness. It also detects degraded or progressing states, certificate issues, and recent warning events.\n\nSpecifically, it performs the following:\n\n- Detects and validates the availability of oc or kubectl CLI tools and verifies cluster connectivity.\n- Checks the HostedCluster resource status (Available, Progressing, Degraded) and reports any abnormal conditions with detailed reasons and messages.\n- Validates control plane pod health in the management cluster, detecting non-running pods, crash loops, high restart counts, and image pull errors.\n- Performs etcd health checks to ensure data consistency and availability.\n- Inspects NodePools for replica mismatches, readiness, and auto-scaling configuration accuracy.\n- Evaluates infrastructure readiness (including AWS/Azure-specific validations like endpoint services).\n- Examines networking and ingress components, including Konnectivity servers and router pods.\n- Scans for recent warning events across HostedCluster, NodePool, and control plane namespaces.\n- Reviews certificate conditions to identify expiring or invalid certificates.\n- Generates a clear, color-coded summary report and optionally exports findings in JSON format for automation or CI integration.\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **Kubernetes/OpenShift CLI**: Either `oc` (OpenShift) or `kubectl` (Kubernetes)\n   - Install `oc` from: <https://mirror.openshift.com/pub/openshift-v4/clients/ocp/>\n   - Or install `kubectl` from: <https://kubernetes.io/docs/tasks/tools/>\n   - Verify with: `oc version` or `kubectl version`\n\n2. **Active cluster connection**: Must be connected to a running cluster\n   - Verify with: `oc whoami` or `kubectl cluster-info`\n   - Ensure KUBECONFIG is set if needed\n\n3. **Sufficient permissions**: Must have read access to cluster resources\n   - Cluster-admin or monitoring role recommended for comprehensive checks\n   - Minimum: ability to view nodes, pods, and cluster operators\n\n## Arguments\n\n- **[cluster-name]** (required): Name of the HostedCluster to check. This uniquely identifies the HCP instance whose health will be analyzed. Example: `hcp-demo`\n- **[namespace]** (optional): The namespace in which the HostedCluster resides. Defaults to clusters if not provided. Example: `/hcp:cluster-health-check hcp-demo clusters-dev`\n- **--verbose** (optional): Enable detailed output with additional context\n  - Shows resource-level details\n  - Includes warning conditions\n  - Provides remediation suggestions\n\n- **--output-format** (optional): Output format for results\n  - `text` (default): Human-readable text format\n  - `json`: Machine-readable JSON format for automation\n\n## Implementation\n\nThe command performs the following health checks:\n\n### 1. Determine CLI Tool and Verify Connectivity\n\nDetect which Kubernetes CLI is available and verify cluster connection:\n\n```bash\nif command -v oc &> /dev/null; then\n    CLI=\"oc\"\nelif command -v kubectl &> /dev/null; then\n    CLI=\"kubectl\"\nelse\n    echo \"Error: Neither 'oc' nor 'kubectl' CLI found. Please install one of them.\"\n    exit 1\nfi\n\n# Verify cluster connectivity\nif ! $CLI cluster-info &> /dev/null; then\n    echo \"Error: Not connected to a cluster. Please configure your KUBECONFIG.\"\n    exit 1\nfi\n```\n\n### 2. Initialize Health Check Report\n\nCreate a report structure to collect findings:\n\n```bash\nCLUSTER_NAME=$1\nNAMESPACE=${2:-\"clusters\"}\nCONTROL_PLANE_NS=\"${NAMESPACE}-${CLUSTER_NAME}\"\n\nREPORT_FILE=\".work/hcp-health-check/report-${CLUSTER_NAME}-$(date +%Y%m%d-%H%M%S).txt\"\nmkdir -p .work/hcp-health-check\n\n# Initialize counters\nCRITICAL_ISSUES=0\nWARNING_ISSUES=0\nINFO_MESSAGES=0\n```\n\nArguments:\n- $1 (cluster-name): Name of the HostedCluster resource to check. This is the name visible in `kubectl get hostedcluster` output. Required.\n- $2 (namespace): Namespace containing the HostedCluster resource. Defaults to \"clusters\" if not specified. Optional.\n\n### 3. Check HostedCluster Status\n\nVerify the HostedCluster resource health:\n\n```bash\necho \"Checking HostedCluster Status...\"\n\n# Check if HostedCluster exists\nif ! $CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" &> /dev/null; then\n    echo \" CRITICAL: HostedCluster '$CLUSTER_NAME' not found in namespace '$NAMESPACE'\"\n    exit 1\nfi\n\n# Get HostedCluster conditions\nHC_AVAILABLE=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Available\") | .status')\nHC_PROGRESSING=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Progressing\") | .status')\nHC_DEGRADED=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Degraded\") | .status')\n\nif [ \"$HC_AVAILABLE\" != \"True\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n    echo \" CRITICAL: HostedCluster is not Available\"\n    # Get reason and message\n    $CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Available\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\nfi\n\nif [ \"$HC_DEGRADED\" == \"True\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n    echo \" CRITICAL: HostedCluster is Degraded\"\n    $CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Degraded\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\nfi\n\nif [ \"$HC_PROGRESSING\" == \"True\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + 1))\n    echo \"  WARNING: HostedCluster is Progressing\"\n    $CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Progressing\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\nfi\n\n# Check version and upgrade status\nHC_VERSION=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.version.history[0].version // \"unknown\"')\necho \"  HostedCluster version: $HC_VERSION\"\n```\n\n### 4. Check Control Plane Pod Health\n\nExamine control plane components in the management cluster:\n\n```bash\necho \"Checking Control Plane Pods...\"\n\n# Check if control plane namespace exists\nif ! $CLI get namespace \"$CONTROL_PLANE_NS\" &> /dev/null; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n    echo \" CRITICAL: Control plane namespace '$CONTROL_PLANE_NS' not found\"\nelse\n    # Get pods that are not Running\n    FAILED_CP_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.status.phase != \"Running\" and .status.phase != \"Succeeded\") | \"\\(.metadata.name) [\\(.status.phase)]\"')\n\n    if [ -n \"$FAILED_CP_PODS\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$FAILED_CP_PODS\" | wc -l)))\n        echo \" CRITICAL: Control plane pods in failed state:\"\n        echo \"$FAILED_CP_PODS\" | while read pod; do\n            echo \"  - $pod\"\n        done\n    fi\n\n    # Check for pods with high restart count\n    HIGH_RESTART_CP_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .restartCount > 3) | \"\\(.metadata.name) [Restarts: \\(.status.containerStatuses[0].restartCount)]\"')\n\n    if [ -n \"$HIGH_RESTART_CP_PODS\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$HIGH_RESTART_CP_PODS\" | wc -l)))\n        echo \"  WARNING: Control plane pods with high restart count (>3):\"\n        echo \"$HIGH_RESTART_CP_PODS\" | while read pod; do\n            echo \"  - $pod\"\n        done\n    fi\n\n    # Check for CrashLoopBackOff pods\n    CRASHLOOP_CP_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .state.waiting?.reason == \"CrashLoopBackOff\") | .metadata.name')\n\n    if [ -n \"$CRASHLOOP_CP_PODS\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$CRASHLOOP_CP_PODS\" | wc -l)))\n        echo \" CRITICAL: Control plane pods in CrashLoopBackOff:\"\n        echo \"$CRASHLOOP_CP_PODS\" | while read pod; do\n            echo \"  - $pod\"\n        done\n    fi\n\n    # Check for ImagePullBackOff\n    IMAGE_PULL_CP_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .state.waiting?.reason == \"ImagePullBackOff\" or .state.waiting?.reason == \"ErrImagePull\") | .metadata.name')\n\n    if [ -n \"$IMAGE_PULL_CP_PODS\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + $(echo \"$IMAGE_PULL_CP_PODS\" | wc -l)))\n        echo \" CRITICAL: Control plane pods with image pull errors:\"\n        echo \"$IMAGE_PULL_CP_PODS\" | while read pod; do\n            echo \"  - $pod\"\n        done\n    fi\n\n    # Check critical control plane components\n    echo \"Checking critical control plane components...\"\n    CRITICAL_COMPONENTS=\"kube-apiserver etcd kube-controller-manager kube-scheduler\"\n    \n    for component in $CRITICAL_COMPONENTS; do\n        COMPONENT_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -l app=\"$component\" -o json 2>/dev/null | jq -r '.items[].metadata.name')\n        \n        if [ -z \"$COMPONENT_PODS\" ]; then\n            # Try alternative label\n            COMPONENT_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r \".items[] | select(.metadata.name | startswith(\\\"$component\\\")) | .metadata.name\")\n        fi\n        \n        if [ -z \"$COMPONENT_PODS\" ]; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: No pods found for component: $component\"\n        else\n            # Check if any are not running\n            for pod in $COMPONENT_PODS; do\n                POD_STATUS=$($CLI get pod \"$pod\" -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.status.phase')\n                if [ \"$POD_STATUS\" != \"Running\" ]; then\n                    CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n                    echo \" CRITICAL: $component pod $pod is not Running (Status: $POD_STATUS)\"\n                fi\n            done\n        fi\n    done\nfi\n```\n\n### 5. Check Etcd Health\n\nVerify etcd cluster health and performance:\n\n```bash\necho \"Checking Etcd Health...\"\n\n# Find etcd pods\nETCD_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -l app=etcd -o json 2>/dev/null | jq -r '.items[].metadata.name')\n\nif [ -z \"$ETCD_PODS\" ]; then\n    ETCD_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.metadata.name | startswith(\"etcd\")) | .metadata.name')\nfi\n\nif [ -n \"$ETCD_PODS\" ]; then\n    for pod in $ETCD_PODS; do\n        # Check etcd endpoint health\n        ETCD_HEALTH=$($CLI exec -n \"$CONTROL_PLANE_NS\" \"$pod\" -- etcdctl endpoint health 2>/dev/null || echo \"failed\")\n        \n        if echo \"$ETCD_HEALTH\" | grep -q \"unhealthy\"; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n            echo \" CRITICAL: Etcd pod $pod is unhealthy\"\n        elif echo \"$ETCD_HEALTH\" | grep -q \"failed\"; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: Could not check etcd health for pod $pod\"\n        fi\n    done\nelse\n    WARNING_ISSUES=$((WARNING_ISSUES + 1))\n    echo \"  WARNING: No etcd pods found\"\nfi\n```\n\n### 6. Check NodePool Status\n\nVerify NodePool health and scaling:\n\n```bash\necho \"Checking NodePool Status...\"\n\n# Get all NodePools for this HostedCluster\nNODEPOOLS=$($CLI get nodepool -n \"$NAMESPACE\" -l hypershift.openshift.io/hostedcluster=\"$CLUSTER_NAME\" -o json | jq -r '.items[].metadata.name')\n\nif [ -z \"$NODEPOOLS\" ]; then\n    WARNING_ISSUES=$((WARNING_ISSUES + 1))\n    echo \"  WARNING: No NodePools found for HostedCluster $CLUSTER_NAME\"\nelse\n    for nodepool in $NODEPOOLS; do\n        # Get NodePool status\n        NP_REPLICAS=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.spec.replicas // 0')\n        NP_READY=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.status.replicas // 0')\n        NP_AVAILABLE=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Ready\") | .status')\n\n        echo \"  NodePool: $nodepool [Ready: $NP_READY/$NP_REPLICAS]\"\n\n        if [ \"$NP_AVAILABLE\" != \"True\" ]; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n            echo \" CRITICAL: NodePool $nodepool is not Ready\"\n            $CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"Ready\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n        fi\n\n        if [ \"$NP_READY\" != \"$NP_REPLICAS\" ]; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: NodePool $nodepool has mismatched replicas (Ready: $NP_READY, Desired: $NP_REPLICAS)\"\n        fi\n\n        # Check for auto-scaling\n        AUTOSCALING=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.spec.autoScaling // empty')\n        if [ -n \"$AUTOSCALING\" ]; then\n            MIN=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.spec.autoScaling.min')\n            MAX=$($CLI get nodepool \"$nodepool\" -n \"$NAMESPACE\" -o json | jq -r '.spec.autoScaling.max')\n            echo \"    Auto-scaling enabled: Min=$MIN, Max=$MAX\"\n        fi\n    done\nfi\n```\n\n### 7. Check Infrastructure Status\n\nValidate infrastructure components (AWS/Azure specific):\n\n```bash\necho \"Checking Infrastructure Status...\"\n\n# Get infrastructure platform\nPLATFORM=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.spec.platform.type')\necho \"  Platform: $PLATFORM\"\n\n# Check for infrastructure-related conditions\nINFRA_READY=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"InfrastructureReady\") | .status')\n\nif [ \"$INFRA_READY\" != \"True\" ]; then\n    CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n    echo \" CRITICAL: Infrastructure is not ready\"\n    $CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type==\"InfrastructureReady\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\nfi\n\n# Platform-specific checks\nif [ \"$PLATFORM\" == \"AWS\" ]; then\n    # Check AWS-specific resources\n    echo \"  Performing AWS-specific checks...\"\n    \n    # Check if endpoint service is configured\n    ENDPOINT_SERVICE=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.platform.aws.endpointService // \"not-found\"')\n    if [ \"$ENDPOINT_SERVICE\" == \"not-found\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: AWS endpoint service not configured\"\n    fi\nfi\n```\n\n### 8. Check Network and Ingress\n\nVerify network connectivity and ingress configuration:\n\n```bash\necho \"Checking Network and Ingress...\"\n\n# Check if connectivity is healthy (for private clusters)\nCONNECTIVITY_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -l app=connectivity-server -o json 2>/dev/null | jq -r '.items[].metadata.name')\n\nif [ -n \"$CONNECTIVITY_PODS\" ]; then\n    for pod in $CONNECTIVITY_PODS; do\n        POD_STATUS=$($CLI get pod \"$pod\" -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.status.phase')\n        if [ \"$POD_STATUS\" != \"Running\" ]; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n            echo \" CRITICAL: Connectivity server pod $pod is not Running\"\n        fi\n    done\nfi\n\n# Check ingress/router pods\nROUTER_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -l app=router -o json 2>/dev/null | jq -r '.items[].metadata.name')\n\nif [ -z \"$ROUTER_PODS\" ]; then\n    ROUTER_PODS=$($CLI get pods -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.items[] | select(.metadata.name | contains(\"router\") or contains(\"ingress\")) | .metadata.name')\nfi\n\nif [ -n \"$ROUTER_PODS\" ]; then\n    for pod in $ROUTER_PODS; do\n        POD_STATUS=$($CLI get pod \"$pod\" -n \"$CONTROL_PLANE_NS\" -o json | jq -r '.status.phase')\n        if [ \"$POD_STATUS\" != \"Running\" ]; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: Router/Ingress pod $pod is not Running\"\n        fi\n    done\nfi\n```\n\n### 9. Check Recent Events\n\nLook for recent warning/error events:\n\n```bash\necho \"Checking Recent Events...\"\n\n# Get recent warning events for HostedCluster\nHC_EVENTS=$($CLI get events -n \"$NAMESPACE\" --field-selector involvedObject.name=\"$CLUSTER_NAME\",involvedObject.kind=HostedCluster,type=Warning --sort-by='.lastTimestamp' | tail -10)\n\nif [ -n \"$HC_EVENTS\" ]; then\n    echo \"  Recent Warning Events for HostedCluster:\"\n    echo \"$HC_EVENTS\"\nfi\n\n# Get recent warning events in control plane namespace\nCP_EVENTS=$($CLI get events -n \"$CONTROL_PLANE_NS\" --field-selector type=Warning --sort-by='.lastTimestamp' 2>/dev/null | tail -10)\n\nif [ -n \"$CP_EVENTS\" ]; then\n    echo \"  Recent Warning Events in Control Plane:\"\n    echo \"$CP_EVENTS\"\nfi\n\n# Get recent events for NodePools\nfor nodepool in $NODEPOOLS; do\n    NP_EVENTS=$($CLI get events -n \"$NAMESPACE\" --field-selector involvedObject.name=\"$nodepool\",involvedObject.kind=NodePool,type=Warning --sort-by='.lastTimestamp' 2>/dev/null | tail -5)\n    \n    if [ -n \"$NP_EVENTS\" ]; then\n        echo \"  Recent Warning Events for NodePool $nodepool:\"\n        echo \"$NP_EVENTS\"\n    fi\ndone\n```\n\n### 10. Check Certificate Status\n\nVerify certificate validity:\n\n```bash\necho \"Checking Certificate Status...\"\n\n# Check certificate expiration warnings\nCERT_CONDITIONS=$($CLI get hostedcluster \"$CLUSTER_NAME\" -n \"$NAMESPACE\" -o json | jq -r '.status.conditions[] | select(.type | contains(\"Certificate\")) | \"\\(.type): \\(.status) - \\(.message // \"N/A\")\"')\n\nif [ -n \"$CERT_CONDITIONS\" ]; then\n    echo \"$CERT_CONDITIONS\" | while read line; do\n        if echo \"$line\" | grep -q \"False\"; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: $line\"\n        fi\n    done\nfi\n```\n\n### 11. Generate Summary Report\n\nCreate a summary of findings:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"HCP Cluster Health Check Summary\"\necho \"===============================================\"\necho \"Cluster Name: $CLUSTER_NAME\"\necho \"Namespace: $NAMESPACE\"\necho \"Control Plane Namespace: $CONTROL_PLANE_NS\"\necho \"Platform: $PLATFORM\"\necho \"Version: $HC_VERSION\"\necho \"Check Time: $(date)\"\necho \"\"\necho \"Results:\"\necho \"  Critical Issues: $CRITICAL_ISSUES\"\necho \"  Warnings: $WARNING_ISSUES\"\necho \"\"\n\nif [ $CRITICAL_ISSUES -eq 0 ] && [ $WARNING_ISSUES -eq 0 ]; then\n    echo \" OVERALL STATUS: HEALTHY - No issues detected\"\n    exit 0\nelif [ $CRITICAL_ISSUES -gt 0 ]; then\n    echo \" OVERALL STATUS: CRITICAL - Immediate attention required\"\n    exit 1\nelse\n    echo \"  OVERALL STATUS: WARNING - Monitoring recommended\"\n    exit 0\nfi\n```\n\n### 12. Optional: Export to JSON Format\n\nIf `--output-format json` is specified, export findings as JSON:\n\n```json\n{\n  \"cluster\": {\n    \"name\": \"my-hcp-cluster\",\n    \"namespace\": \"clusters\",\n    \"controlPlaneNamespace\": \"clusters-my-hcp-cluster\",\n    \"platform\": \"AWS\",\n    \"version\": \"4.21.0\",\n    \"checkTime\": \"2025-11-11T12:00:00Z\"\n  },\n  \"summary\": {\n    \"criticalIssues\": 1,\n    \"warnings\": 3,\n    \"overallStatus\": \"WARNING\"\n  },\n  \"findings\": {\n    \"hostedCluster\": {\n      \"available\": true,\n      \"progressing\": true,\n      \"degraded\": false,\n      \"infrastructureReady\": true\n    },\n    \"controlPlane\": {\n      \"failedPods\": [],\n      \"crashLoopingPods\": [],\n      \"highRestartPods\": [\"kube-controller-manager-xxx\"],\n      \"imagePullErrors\": []\n    },\n    \"etcd\": {\n      \"healthy\": true,\n      \"pods\": [\"etcd-0\", \"etcd-1\", \"etcd-2\"]\n    },\n    \"nodePools\": {\n      \"total\": 2,\n      \"ready\": 2,\n      \"details\": [\n        {\n          \"name\": \"workers\",\n          \"replicas\": 3,\n          \"ready\": 3,\n          \"autoScaling\": {\n            \"enabled\": true,\n            \"min\": 2,\n            \"max\": 5\n          }\n        }\n      ]\n    },\n    \"network\": {\n      \"konnectivityHealthy\": true,\n      \"ingressHealthy\": true\n    },\n    \"events\": {\n      \"recentWarnings\": 5\n    }\n  }\n}\n```\n\n## Examples\n\n### Example 1: Basic health check with default namespace\n```bash\n/hcp:cluster-health-check my-cluster\n```\n\nOutput, for healthy cluster:\n```text\nHCP Cluster Health Check: my-cluster (namespace: clusters)\n================================================================================\n\nOVERALL STATUS:  HEALTHY\n\nCOMPONENT STATUS:\n HostedCluster: Available\n Control Plane: All pods running (0 restarts)\n NodePool: 3/3 nodes ready\n Infrastructure: AWS resources validated\n Network: Operational\n Storage/Etcd: Healthy\n\nNo critical issues found. Cluster is operating normally.\n\nDIAGNOSTIC COMMANDS:\nRun these commands for detailed information:\n\nkubectl get hostedcluster my-cluster -n clusters -o yaml\nkubectl get pods -n clusters-my-cluster\nkubectl get nodepool -n clusters\n```\n\nOutput, with warnings:\n```text\nHCP Cluster Health Check: test-cluster (namespace: clusters)\n================================================================================\n\nOVERALL STATUS:   WARNING\n\nCOMPONENT STATUS:\n HostedCluster: Available\n  Control Plane: 1 pod restarting\n NodePool: 2/2 nodes ready\n Infrastructure: Validated\n Network: Operational\n  Storage/Etcd: High latency detected\n\nISSUES FOUND:\n\n[WARNING] Control Plane Pod Restarting\n- Component: kube-controller-manager\n- Location: clusters-test-cluster namespace\n- Restarts: 3 in the last hour\n- Impact: May cause temporary API instability\n- Recommended Action:\n  kubectl logs -n clusters-test-cluster kube-controller-manager-xxx --previous\n  Check for configuration issues or resource constraints\n\n[WARNING] Etcd High Latency\n- Component: etcd\n- Metrics: Backend commit duration > 100ms\n- Impact: Slower cluster operations\n- Recommended Action:\n  Check management cluster node performance\n  Review etcd disk I/O with: kubectl exec -n clusters-test-cluster etcd-0 -- etcdctl endpoint status\n\nDIAGNOSTIC COMMANDS:\n\n1. Check HostedCluster details:\n   kubectl get hostedcluster test-cluster -n clusters -o yaml\n\n2. View control plane events:\n   kubectl get events -n clusters-test-cluster --sort-by='.lastTimestamp' | tail -20\n\n3. Check etcd metrics:\n   kubectl exec -n clusters-test-cluster etcd-0 -- etcdctl endpoint health\n```\n\n### Example 2: Health check with custom namespace\n```bash\n/hcp:cluster-health-check production-cluster prod-clusters\n```\nPerforms health check on cluster \"production-cluster\" in the \"prod-clusters\" namespace.\n\n### Example 3: Verbose health check\n```bash\n/hcp:cluster-health-check my-cluster --verbose\n```\n\n### Example 4: JSON output for automation\n```bash\n/hcp:cluster-health-check my-cluster --output-format json\n```\n\n## Return Value\n\nThe command returns a structured health report containing:\n\n- **OVERALL STATUS**: Health summary (Healthy  / Warning  / Critical )\n- **COMPONENT STATUS**: Status of each checked component with visual indicators\n- **ISSUES FOUND**: Detailed list of problems with:\n  - Severity level (Critical/Warning/Info)\n  - Component location\n  - Impact assessment\n  - Recommended actions\n- **DIAGNOSTIC COMMANDS**: kubectl commands for further investigation\n\n## Common Issues and Remediation\n\n### Degraded Cluster Operators\n\n**Symptoms**: Cluster operators showing Degraded=True or Available=False\n\n**Investigation**:\n```bash\noc get clusteroperator <operator-name> -o yaml\noc logs -n openshift-<operator-namespace> -l app=<operator-name>\n```\n\n**Remediation**: Check operator logs and events for specific errors\n\n### Nodes Not Ready\n\n**Symptoms**: Nodes in NotReady state\n\n**Investigation**:\n```bash\noc describe node <node-name>\noc get events --field-selector involvedObject.name=<node-name>\n```\n\n**Remediation**: Common causes include network issues, disk pressure, or kubelet problems\n\n### Pods in CrashLoopBackOff\n\n**Symptoms**: Pods continuously restarting\n\n**Investigation**:\n```bash\noc logs <pod-name> -n <namespace> --previous\noc describe pod <pod-name> -n <namespace>\n```\n\n**Remediation**: Check application logs, resource limits, and configuration\n\n### ImagePullBackOff Errors\n\n**Symptoms**: Pods unable to pull container images\n\n**Investigation**:\n```bash\noc describe pod <pod-name> -n <namespace>\n```\n\n**Remediation**: Verify image name, registry credentials, and network connectivity\n\n## Security Considerations\n\n- **Read-only access**: This command only reads cluster state, no modifications\n- **Sensitive data**: Be cautious when sharing reports as they may contain cluster topology information\n- **RBAC requirements**: Ensure user has appropriate permissions for all resource types checked\n\n## Notes\n\n- This command requires appropriate RBAC permissions to view HostedCluster, NodePool, and Pod resources\n- The command provides diagnostic guidance but does not automatically remediate issues\n- For critical production issues, always follow your organization's incident response procedures\n- Regular health checks (daily or before changes) help catch issues early\n- Some checks may be skipped if user lacks sufficient permissions"
              },
              {
                "name": "/generate",
                "description": "Generate ready-to-execute hypershift cluster creation commands from natural language descriptions",
                "path": "plugins/hcp/commands/generate.md",
                "frontmatter": {
                  "description": "Generate ready-to-execute hypershift cluster creation commands from natural language descriptions",
                  "argument-hint": "<provider> <cluster-description>"
                },
                "content": "## Name\nhcp:generate\n\n## Synopsis\n```\n/hcp:generate <provider> <cluster-description>\n```\n\n## Description\nThe `hcp:generate` command translates natural language descriptions into precise, ready-to-execute `hypershift create cluster` commands. It supports multiple cloud providers and platforms, each with their specific requirements and best practices.\n\n**Important**: This command **generates commands for you to run** - it does not provision clusters directly.\n\nThis command is particularly useful for:\n- Generating complete, copy-paste-ready hypershift commands with proper parameters\n- Applying provider-specific best practices and configurations automatically\n- Handling complex parameter validation and smart defaults\n- Providing interactive prompts for missing critical information\n- Learning proper hypershift command syntax and options\n\n## Key Features\n\n- **Multi-Provider Support** - AWS, Azure, KubeVirt, OpenStack, PowerVS, and Agent providers\n- **Smart Analysis** - Extracts platform, configuration, and requirements from natural language\n- **Interactive Prompts** - Asks for missing critical information with helpful guidance\n- **Provider Expertise** - Applies platform-specific best practices and configurations\n- **Security Validation** - Ensures safe parameter handling and credential management\n- **Namespace Management** - Implements best practices for cluster isolation\n\n## Implementation\n\nThe `hcp:generate` command runs in multiple phases:\n\n###  Phase 1: Load Provider-Specific Implementation Guidance\n\nInvoke the appropriate skill based on provider using the Skill tool:\n\n- **Provider: `aws`**  Invoke `hcp-create-aws` skill\n  - Loads AWS-specific requirements and configurations\n  - Provides STS credentials handling\n  - Offers region and availability zone guidance\n  - Handles IAM roles and VPC configuration\n\n- **Provider: `azure`**  Invoke `hcp-create-azure` skill\n  - Loads Azure-specific requirements (self-managed control plane only)\n  - Provides resource group and location guidance\n  - Handles identity configuration options\n  - Manages virtual network integration\n\n- **Provider: `kubevirt`**  Invoke `hcp-create-kubevirt` skill\n  - Loads KubeVirt-specific network conflict prevention\n  - Provides virtual machine configuration guidance\n  - Handles storage class requirements\n  - Manages IPv4/IPv6 CIDR validation\n\n- **Provider: `openstack`**  Invoke `hcp-create-openstack` skill\n  - Loads OpenStack-specific requirements\n  - Provides floating IP network guidance\n  - Handles flavor selection and custom images\n  - Manages network topology configuration\n\n- **Provider: `powervs`**  Invoke `hcp-create-powervs` skill\n  - Loads PowerVS/IBM Cloud specific requirements\n  - Provides region and zone guidance\n  - Handles IBM Cloud API key configuration\n  - Manages processor and memory specifications\n\n- **Provider: `agent`**  Invoke `hcp-create-agent` skill\n  - Loads bare metal and edge deployment guidance\n  - Provides pre-provisioned agent requirements\n  - Handles manual network configuration\n  - Manages disconnected environment setup\n\n###  Phase 2: Parse Arguments & Detect Context\n\nParse command arguments:\n- **Required:** `provider`, `cluster-description`\n- Parse natural language description for:\n  - Environment type (development, production, disconnected)\n  - Special requirements (FIPS, architecture, storage)\n  - Resource constraints (cost-optimization, performance)\n  - Network requirements\n\n###  Phase 3: Apply Provider-Specific Defaults and Validation\n\n**Universal requirements (applied to ALL clusters):**\n- **Namespace strategy:** Generate unique namespace based on cluster name (`{cluster-name}-ns`)\n- **Security validation:** Scan for credentials or sensitive data\n- **Release image:** Always include `--release-image` with proper version\n\n**Provider-specific validation:**\n- Network conflict prevention (especially KubeVirt)\n- Credential requirements validation\n- Region/zone availability checks\n- Resource limit validation\n\n###  Phase 4: Interactive Prompts (Provider-Guided)\n\nEach provider skill guides the collection of missing information:\n\n**Common prompts for all providers:**\n- Cluster name (if not specified)\n- Pull secret path\n- OpenShift version/release image\n- Base domain (where applicable)\n\n**Provider-specific prompts:**\n- AWS: STS credentials, IAM role ARN, region selection\n- Azure: Identity configuration method, resource group name\n- KubeVirt: Management cluster network CIDRs, storage classes\n- OpenStack: External network UUID, flavor selection\n- PowerVS: IBM Cloud resource group, processor specifications\n- Agent: Agent namespace, pre-provisioned agent details\n\n###  Phase 5: Security and Configuration Validation\n\n**Security checks:**\n- Scan all inputs for credentials, API keys, or secrets\n- Validate that sensitive information uses placeholder values\n- Ensure proper credential file references\n\n**Configuration validation:**\n- Verify required parameters are present\n- Validate parameter combinations and dependencies\n- Check for common misconfigurations\n\n###  Phase 6: Generate Command\n\nBased on provider skill guidance:\n- Construct complete `hypershift create cluster` command\n- Apply smart defaults for optional parameters\n- Include all required provider-specific flags\n- Format command for copy-paste execution\n\n###  Phase 7: Return Result\n\nDisplay to user:\n- **Summary:** Brief description of what will be created\n- **Generated Command:** Complete, executable command\n- **Key Decisions:** Explanation of important choices made\n- **Next Steps:** What to do after running the command\n- **Provider-specific notes:** Any special considerations\n\n## Usage Examples\n\n1. **Create AWS development cluster**:\n   ```\n   /hcp:generate aws \"development cluster for testing new features\"\n   ```\n    Invokes `hcp-create-aws` skill, prompts for AWS-specific details\n\n2. **Create production KubeVirt cluster**:\n   ```\n   /hcp:generate kubevirt \"production cluster with high availability\"\n   ```\n    Invokes `hcp-create-kubevirt` skill, handles network conflict prevention\n\n3. **Create cost-optimized Azure cluster**:\n   ```\n   /hcp:generate azure \"small cluster for dev work, minimize costs\"\n   ```\n    Invokes `hcp-create-azure` skill, applies cost optimization\n\n4. **Create disconnected agent cluster**:\n   ```\n   /hcp:generate agent \"airgapped cluster for secure environment\"\n   ```\n    Invokes `hcp-create-agent` skill, handles disconnected requirements\n\n5. **Create FIPS-enabled OpenStack cluster**:\n   ```\n   /hcp:generate openstack \"production cluster with FIPS compliance\"\n   ```\n    Invokes `hcp-create-openstack` skill, applies FIPS configuration\n\n6. **Create ARM-based PowerVS cluster**:\n   ```\n   /hcp:generate powervs \"arm64 cluster for multi-arch testing\"\n   ```\n    Invokes `hcp-create-powervs` skill, handles ARM architecture\n\n## Arguments\n\n- **$1  provider** *(required)*\n  Cloud provider or platform to use.\n  **Options:** `aws` | `azure` | `kubevirt` | `openstack` | `powervs` | `agent`\n\n- **$2  cluster-description** *(required)*\n  Natural language description of the desired cluster.\n  Use quotes for multi-word descriptions: `\"production cluster with HA\"`\n\n  **Description should include:**\n  - Environment type (development, production, testing)\n  - Special requirements (FIPS, architecture, storage)\n  - Resource preferences (cost-optimized, high-performance)\n  - Network requirements (disconnected, private)\n\n## Return Value\n\n- **Summary**: Brief description of the cluster that will be created\n- **Generated Command**: Complete `hypershift create cluster` command\n- **Key Decisions**: Explanation of choices made during generation\n- **Next Steps**: Instructions for executing the command and post-creation tasks\n\n**Example output:**\n```\n## Summary\nCreating a development AWS hosted cluster with basic configuration.\n\n## Generated Command\n```bash\nhypershift create cluster aws \\\n  --name dev-cluster \\\n  --namespace dev-cluster-ns \\\n  --region us-east-1 \\\n  --instance-type m5.large \\\n  --pull-secret /path/to/pull-secret.json \\\n  --node-pool-replicas 2 \\\n  --zones us-east-1a,us-east-1b \\\n  --control-plane-availability-policy SingleReplica \\\n  --sts-creds /path/to/sts-creds.json \\\n  --role-arn arn:aws:iam::123456789:role/hypershift-role \\\n  --base-domain example.com \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi\n```\n\n## Key Decisions\n- Used SingleReplica for development (cost-effective)\n- Selected 2 zones for basic redundancy\n- m5.large instances balance cost and performance for dev workloads\n- **Unique namespace**: `dev-cluster-ns` for better isolation and disaster recovery\n\n## Next Steps\n1. Ensure your pull secret file exists at the specified path\n2. Verify AWS credentials are configured\n3. Confirm STS credentials file is accessible\n4. Run the command above to create your cluster\n```\n\n## Error Handling\n\n### Invalid Provider\n\n**Scenario:** User specifies unsupported provider.\n\n**Action:**\n```\nInvalid provider \"gcp\". Supported providers: aws, azure, kubevirt, openstack, powervs, agent\n\nDid you mean \"aws\"?\n```\n\n### Missing Description\n\n**Scenario:** User doesn't provide cluster description.\n\n**Action:**\n```\nCluster description is required. Please describe what kind of cluster you want.\n\nExamples:\n- \"development cluster for testing\"\n- \"production cluster with high availability\"\n- \"cost-optimized cluster for demos\"\n\nUsage: /hcp:generate aws \"development cluster for testing\"\n```\n\n### Ambiguous Requirements\n\n**Scenario:** Description is too vague or contradictory.\n\n**Action:**\n```\nThe description \"fast and cheap cluster\" has conflicting requirements.\n\nLet me help clarify:\n1. Performance-optimized (higher costs, better resources)\n2. Cost-optimized (lower costs, minimal resources)\n3. Balanced (moderate costs and performance)\n\nWhich approach do you prefer?\n```\n\n### Provider-Specific Errors\n\n**Scenario:** Provider-specific validation fails.\n\n**Action:**\n- Forward to appropriate skill for specialized error handling\n- Provide provider-specific guidance and solutions\n- Offer alternative configurations when possible\n\n## Best Practices\n\n1. **Be descriptive:** Include environment type, requirements, and constraints\n2. **Specify architecture:** Mention if you need ARM64 or specific architectures\n3. **Include network needs:** Specify if disconnected or special networking required\n4. **Mention compliance:** Include FIPS, security, or regulatory requirements\n5. **Consider costs:** Specify if cost optimization is important\n6. **Plan for growth:** Mention if cluster needs to scale or handle specific workloads\n\n## Anti-Patterns to Avoid\n\n **Vague descriptions**\n```\n/hcp:generate aws \"cluster\"\n```\n Be specific: \"development cluster for API testing with minimal resources\"\n\n **Conflicting requirements**\n```\n/hcp:generate aws \"high-performance cluster but very cheap\"\n```\n Be realistic: \"balanced cluster optimizing for cost while maintaining decent performance\"\n\n **Provider mismatches**\n```\n/hcp:generate azure \"cluster for my on-premises lab\"\n```\n Use appropriate provider: \"kubevirt cluster for my on-premises lab\"\n\n## See Also\n\n- `hypershift create cluster --help` - Official hypershift CLI documentation\n- Provider-specific skills:\n  - `hcp-create-aws` - AWS-specific guidance\n  - `hcp-create-azure` - Azure-specific guidance\n  - `hcp-create-kubevirt` - KubeVirt-specific guidance\n  - `hcp-create-openstack` - OpenStack-specific guidance\n  - `hcp-create-powervs` - PowerVS-specific guidance\n  - `hcp-create-agent` - Agent provider guidance\n\n## Skills Reference\n\nThe following skills are automatically invoked by this command based on provider:\n\n**Provider-specific skills:**\n- **hcp-create-aws** - AWS provider implementation details\n- **hcp-create-azure** - Azure provider implementation details\n- **hcp-create-kubevirt** - KubeVirt provider implementation details\n- **hcp-create-openstack** - OpenStack provider implementation details\n- **hcp-create-powervs** - PowerVS provider implementation details\n- **hcp-create-agent** - Agent provider implementation details\n\nTo view skill details:\n```bash\nls plugins/hypershift/skills/\ncat plugins/hypershift/skills/hcp-create-aws/SKILL.md\ncat plugins/hypershift/skills/hcp-create-kubevirt/SKILL.md\n# ... etc for other providers\n```"
              }
            ],
            "skills": [
              {
                "name": "HyperShift Agent Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on bare metal, edge environments, or disconnected infrastructures using pre-provisioned agents",
                "path": "plugins/hcp/skills/hcp-create-agent/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift Agent Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on bare metal, edge environments, or disconnected infrastructures using pre-provisioned agents"
                },
                "content": "# HyperShift Agent Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters using the Agent provider, which is designed for bare metal and edge deployments where pre-provisioned agents are available.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate agent` command to guide the Agent provider cluster creation process.\n\n## Prerequisites\n\n- HyperShift operator installed and configured\n- Pre-provisioned agents available in a Kubernetes namespace\n- Pull secret for accessing OpenShift images\n- Understanding of the target deployment environment\n\n## Agent Provider Overview\n\n### What is the Agent Provider?\n\nThe Agent provider:\n- Designed for bare metal and edge deployments\n- Requires pre-provisioned agents (nodes registered in advance)\n- No cloud provider automation - manual network configuration required\n- Suitable for disconnected/airgapped environments\n- Direct control over hardware and network configuration\n\n### Common Use Cases\n\n- **Edge Computing**: Remote locations with limited connectivity\n- **Bare Metal**: On-premises hardware without cloud automation\n- **Disconnected Environments**: Airgapped environments with security requirements\n- **Custom Hardware**: Specialized hardware configurations\n- **Regulatory Compliance**: Environments requiring specific data locality\n\n## Implementation Steps\n\n### Step 1: Analyze Cluster Description\n\nParse the natural language description for Agent-specific requirements:\n\n**Environment Type Detection:**\n- **Edge**: \"edge\", \"remote\", \"limited connectivity\", \"small footprint\"\n- **Bare Metal**: \"bare metal\", \"on-premises\", \"physical hardware\", \"custom hardware\"\n- **Disconnected**: \"airgapped\", \"disconnected\", \"offline\", \"secure environment\"\n\n**Resource Indicators:**\n- **Minimal**: \"small\", \"edge\", \"minimal resources\", \"single node\"\n- **Standard**: \"production\", \"multi-node\", \"high availability\"\n\n**Special Requirements:**\n- **FIPS**: \"fips\", \"compliance\", \"security\"\n- **Custom Storage**: \"local storage\", \"storage class\", \"persistent volumes\"\n- **Network Isolation**: \"isolated\", \"private network\", \"custom networking\"\n\n### Step 2: Apply Agent Provider Defaults\n\n**Required Parameters:**\n- `--agent-namespace`: Namespace where agents are located\n- `--pull-secret`: Path to pull secret file\n- `--release-image`: OpenShift release image\n- `--base-domain`: Base domain for the cluster (prompt user if not in description)\n\n**Smart Defaults by Environment:**\n\n**Edge Environment:**\n```bash\n--control-plane-availability-policy SingleReplica\n--node-pool-replicas 1\n--arch amd64  # or arm64 if specified\n```\n\n**Bare Metal Environment:**\n```bash\n--control-plane-availability-policy HighlyAvailable\n--node-pool-replicas 3\n--auto-repair true\n```\n\n**Disconnected Environment:**\n```bash\n--render  # Always render for review\n--image-content-sources /path/to/image-content-sources.yaml\n--additional-trust-bundle /path/to/ca-bundle.pem\n```\n\n### Step 3: Interactive Parameter Collection\n\n**Required Information Collection:**\n\n1. **Cluster Name**\n   ```\n    **Cluster Name**: What would you like to name your cluster?\n      - Must be DNS-compatible (lowercase, hyphens allowed)\n      - Will be used for resource naming\n      - Example: edge-lab-01, production-cluster\n   ```\n\n2. **Agent Namespace**\n   ```\n    **Agent Namespace**: In which namespace are your agents located?\n      - This should be the namespace where you registered your agents\n      - Example: default, agent-system, cluster-agents\n      - [Press Enter for default: default]\n   ```\n\n3. **Pull Secret**\n   ```\n    **Pull Secret**: Path to your OpenShift pull secret file?\n      - Required for accessing OpenShift container images\n      - Download from: https://console.redhat.com/openshift/install/pull-secret\n      - Example: /home/user/pull-secret.json\n   ```\n\n4. **Base Domain** (if not specified in description)\n   ```\n    **Base Domain**: What base domain should be used for cluster DNS?\n      - This will be used for cluster API and application routes\n      - Example: example.com, lab.internal, edge.local\n   ```\n\n5. **OpenShift Version**\n   ```\n    **OpenShift Version**: Which OpenShift version do you want to use?\n\n       **Check supported versions**: https://amd64.ocp.releases.ci.openshift.org/\n\n      - Enter release image URL: quay.io/openshift-release-dev/ocp-release:X.Y.Z-multi\n      - [Press Enter for default: quay.io/openshift-release-dev/ocp-release:4.18.0-multi]\n   ```\n\n**Optional Configuration (based on description analysis):**\n\n6. **Architecture** (if detected)\n   ```\n    **Architecture**: Detected ARM architecture mention. Confirm architecture:\n      - amd64 (x86_64)\n      - arm64 (aarch64)\n      - [Press Enter for default: amd64]\n   ```\n\n7. **FIPS Mode** (if compliance mentioned)\n   ```\n    **FIPS Mode**: Enable FIPS mode for compliance?\n      - Required for certain regulatory environments\n      - May impact performance\n      - [yes/no] [Press Enter for default: no]\n   ```\n\n### Step 4: Disconnected Environment Handling\n\nIf disconnected/airgapped environment is detected:\n\n**Additional Required Information:**\n\n1. **Mirror Registry**\n   ```\n    **Mirror Registry**: What is your mirror registry domain?\n      - This registry should contain mirrored OpenShift images\n      - Example: registry.example.com:5000\n   ```\n\n2. **Image Content Sources**\n   ```\n    **Image Content Sources**: Path to image content sources file?\n      - Required for mapping official images to mirror registry\n      - Example: /path/to/image-content-sources.yaml\n   ```\n\n3. **Additional Trust Bundle** (optional)\n   ```\n    **Additional Trust Bundle**: Path to custom CA bundle? (Optional)\n      - Required if mirror registry uses custom certificates\n      - Example: /path/to/ca-bundle.pem\n      - [Press Enter to skip]\n   ```\n\n**Always Use --render for Disconnected:**\n- Include `--render` flag to generate manifests for review\n- Provide post-generation instructions for required manifest modifications\n\n### Step 5: Agent Availability Validation\n\n**Provide Agent Check Commands:**\n```\nBefore creating the cluster, verify your agents are available:\n\nCheck agents in namespace:\n  kubectl get agents -n <agent-namespace>\n\nVerify agent status:\n  kubectl describe agents -n <agent-namespace>\n\nEnsure agents are:\n  - In \"Available\" state\n  - Not bound to other clusters\n  - Meet minimum resource requirements\n```\n\n### Step 6: Generate Command\n\n**Basic Agent Cluster Command:**\n```bash\nhypershift create cluster agent \\\n  --name <cluster-name> \\\n  --namespace <cluster-name>-ns \\\n  --agent-namespace <agent-namespace> \\\n  --pull-secret <pull-secret-path> \\\n  --release-image <release-image> \\\n  --base-domain <base-domain>\n```\n\n**With Environment-Specific Flags:**\n\n**Edge/Minimal Configuration:**\n```bash\nhypershift create cluster agent \\\n  --name edge-cluster \\\n  --namespace edge-cluster-ns \\\n  --agent-namespace default \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain edge.local \\\n  --control-plane-availability-policy SingleReplica \\\n  --node-pool-replicas 1\n```\n\n**Production/HA Configuration:**\n```bash\nhypershift create cluster agent \\\n  --name production-cluster \\\n  --namespace production-cluster-ns \\\n  --agent-namespace agent-system \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain example.com \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --node-pool-replicas 3 \\\n  --auto-repair\n```\n\n**Disconnected Configuration:**\n```bash\nhypershift create cluster agent \\\n  --name secure-cluster \\\n  --namespace secure-cluster-ns \\\n  --agent-namespace agent-system \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain internal.example.com \\\n  --image-content-sources /path/to/image-content-sources.yaml \\\n  --additional-trust-bundle /path/to/ca-bundle.pem \\\n  --render\n```\n\n**FIPS-Enabled Configuration:**\n```bash\nhypershift create cluster agent \\\n  --name compliance-cluster \\\n  --namespace compliance-cluster-ns \\\n  --agent-namespace agent-system \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain secure.example.com \\\n  --fips\n```\n\n### Step 7: Post-Generation Instructions\n\n**For Disconnected Environments:**\n\nProvide additional configuration needed after cluster creation:\n\n```\n## Post-Creation Configuration (Disconnected)\n\nAfter running the command above (with --render), you'll need to modify the generated manifests:\n\n1. **Add ImageContentSources to HostedCluster:**\n```yaml\nspec:\n  imageContentSources:\n  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    mirrors:\n    - registry.example.com:5000/openshift/release\n  - source: quay.io/openshift-release-dev/ocp-release\n    mirrors:\n    - registry.example.com:5000/openshift/release-images\n  - source: registry.redhat.io/multicluster-engine\n    mirrors:\n    - registry.example.com:5000/openshift/multicluster-engine\n```\n\n2. **Disable Default OperatorHub Sources:**\n```yaml\nspec:\n  configuration:\n    operatorhub:\n      disableAllDefaultSources: true\n```\n\n3. **Apply the manifests:**\n```bash\nkubectl apply -f <rendered-manifest-files>\n```\n```\n\n**For All Environments:**\n\n```\n## Next Steps\n\n1. **Verify agents are available:**\n   kubectl get agents -n <agent-namespace>\n\n2. **Monitor cluster creation:**\n   kubectl get hostedcluster -n <cluster-namespace>\n   kubectl get nodepool -n <cluster-namespace>\n\n3. **Check agent assignment:**\n   kubectl get agents -n <agent-namespace> -o wide\n\n4. **Access cluster when ready:**\n   hypershift create kubeconfig --name <cluster-name> --namespace <cluster-namespace>\n```\n\n## Error Handling\n\n### No Agents Available\n\n**Scenario:** No agents found in specified namespace.\n\n**Action:**\n```\nNo agents found in namespace \"default\".\n\nPlease ensure:\n1. Agents are properly registered\n2. Agents are in \"Available\" state\n3. Correct namespace specified\n\nCheck agents:\n  kubectl get agents -A\n\nRegister new agents using the agent-based installer or manual process.\n```\n\n### Insufficient Agents\n\n**Scenario:** Not enough agents for requested replicas.\n\n**Action:**\n```\nOnly 2 agents available, but you requested 3 node replicas.\n\nOptions:\n1. Reduce node-pool-replicas to 2\n2. Register additional agents\n3. Use SingleReplica control plane (reduces total requirement)\n\nWould you like me to adjust the configuration?\n```\n\n### Agent Already Bound\n\n**Scenario:** Agents are already assigned to another cluster.\n\n**Action:**\n```\nSome agents are already bound to other clusters.\n\nAvailable agents: 1\nBound agents: 2\n\nCheck agent status:\n  kubectl describe agents -n <namespace>\n\nFree up agents by deleting unused clusters or register new agents.\n```\n\n### Disconnected Image Access\n\n**Scenario:** Can't access release images in disconnected environment.\n\n**Action:**\n```\nRelease image access failed. In disconnected environments:\n\n1. Ensure mirror registry contains required images\n2. Verify image-content-sources file mapping\n3. Check custom CA bundle configuration\n4. Validate network connectivity to mirror registry\n\nMirror required images using:\n  oc adm release mirror --to <mirror-registry>\n```\n\n## Best Practices\n\n### Agent Management\n\n1. **Pre-provision agents:** Register agents before cluster creation\n2. **Agent naming:** Use descriptive names for easier management\n3. **Resource allocation:** Ensure agents meet minimum requirements\n4. **Health monitoring:** Monitor agent status regularly\n\n### Network Configuration\n\n1. **DNS planning:** Ensure proper DNS resolution for base domain\n2. **Load balancing:** Configure external load balancer for API access\n3. **Ingress:** Plan for application ingress and routing\n4. **Firewall:** Configure appropriate firewall rules\n\n### Security\n\n1. **Pull secret security:** Protect pull secret files\n2. **FIPS compliance:** Enable FIPS for regulated environments\n3. **Certificate management:** Plan for custom certificate authorities\n4. **Network isolation:** Implement appropriate network segmentation\n\n### Disconnected Deployments\n\n1. **Mirror planning:** Pre-mirror all required images\n2. **Catalog management:** Prepare custom operator catalogs\n3. **Certificate management:** Plan for custom CA bundles\n4. **Registry security:** Secure mirror registry access\n\n## Anti-Patterns to Avoid\n\n **Assuming cloud automation**\n```\n\"Create cluster with autoscaling\"\n```\n Agent provider requires manual agent provisioning\n\n **Insufficient agent planning**\n```\nNot checking agent availability before cluster creation\n```\n Always verify sufficient agents are available\n\n **Ignoring disconnected requirements**\n```\nUsing public image references in airgapped environment\n```\n Properly configure image content sources and mirror registry\n\n **Inadequate resource planning**\n```\nUsing minimal agents for production workloads\n```\n Size agents appropriately for expected workloads\n\n## Example Workflows\n\n### Edge Deployment\n```\nInput: \"small edge cluster for remote office monitoring\"\n\nAnalysis:\n- Environment: Edge\n- Scale: Minimal\n- Use case: Monitoring\n\nGenerated Command:\nhypershift create cluster agent \\\n  --name edge-monitoring \\\n  --namespace edge-monitoring-ns \\\n  --agent-namespace default \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain edge.local \\\n  --control-plane-availability-policy SingleReplica \\\n  --node-pool-replicas 1\n```\n\n### Secure Data Center\n```\nInput: \"airgapped production cluster for financial data processing\"\n\nAnalysis:\n- Environment: Disconnected/Secure\n- Scale: Production\n- Use case: Financial (compliance required)\n\nGenerated Command:\nhypershift create cluster agent \\\n  --name financial-prod \\\n  --namespace financial-prod-ns \\\n  --agent-namespace secure-agents \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --base-domain secure.financial.internal \\\n  --image-content-sources /path/to/image-content-sources.yaml \\\n  --additional-trust-bundle /path/to/ca-bundle.pem \\\n  --fips \\\n  --render\n```\n\n## See Also\n\n- Agent-based OpenShift Installation Documentation\n- HyperShift Agent Provider Documentation\n- OpenShift Disconnected Installation Guide\n- Agent-based Installer Documentation"
              },
              {
                "name": "HyperShift AWS Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on AWS infrastructure with proper STS credentials, IAM roles, and VPC configuration",
                "path": "plugins/hcp/skills/hcp-create-aws/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift AWS Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on AWS infrastructure with proper STS credentials, IAM roles, and VPC configuration"
                },
                "content": "# HyperShift AWS Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters on AWS, handling AWS-specific requirements including STS credentials, IAM roles, VPC configuration, and regional best practices.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate aws` command to guide the AWS provider cluster creation process.\n\n## Prerequisites\n\n- AWS CLI configured with appropriate credentials\n- HyperShift operator installed and configured\n- STS credentials file for the target AWS account\n- IAM role with required permissions for HyperShift\n- Pull secret for accessing OpenShift images\n\n## AWS Provider Overview\n\n### AWS Provider Peculiarities\n\n- **Requires AWS credentials (STS):** Must have valid STS credentials file\n- **Region selection affects availability zones:** Different regions have different AZ availability\n- **Instance types vary by region:** Not all instance types available in all regions\n- **VPC CIDR must not conflict:** Must not overlap with existing infrastructure\n- **IAM roles:** Can be auto-created or use pre-existing roles\n\n### Common AWS Configurations\n\n**Development Environment:**\n- Single replica control plane (cost-effective)\n- m5.large instances (balanced performance/cost)\n- 2 availability zones (basic redundancy)\n- Basic networking (public endpoints)\n\n**Production Environment:**\n- Highly available control plane\n- m5.xlarge+ instances (better performance)\n- 3+ availability zones (high availability)\n- Custom VPC configuration\n- KMS encryption enabled\n\n**Cost-Optimized Environment:**\n- Single NAT gateway\n- Smaller instance types\n- Minimal replicas\n- Spot instances (where applicable)\n\n## Implementation Steps\n\n### Step 1: Analyze Cluster Description\n\nParse the natural language description for AWS-specific requirements:\n\n**Environment Type Detection:**\n- **Development**: \"dev\", \"development\", \"testing\", \"demo\", \"sandbox\"\n- **Production**: \"prod\", \"production\", \"critical\", \"enterprise\"\n- **Cost-Optimized**: \"cheap\", \"cost\", \"minimal\", \"budget\", \"demo\"\n\n**Performance Indicators:**\n- **High Performance**: \"performance\", \"fast\", \"high-compute\", \"intensive\"\n- **Standard**: Default moderate configuration\n- **Minimal**: \"small\", \"minimal\", \"basic\", \"simple\"\n\n**Security/Compliance:**\n- **FIPS**: \"fips\", \"compliance\", \"security\", \"regulated\"\n- **Private**: \"private\", \"isolated\", \"secure\", \"internal\"\n\n**Special Requirements:**\n- **Multi-AZ**: \"highly available\", \"ha\", \"multi-zone\", \"resilient\"\n- **Single-AZ**: \"single zone\", \"simple\", \"minimal\"\n\n### Step 2: Apply AWS Provider Defaults\n\n**Required Parameters:**\n- `--region`: AWS region (default: us-east-1)\n- `--pull-secret`: Path to pull secret file\n- `--release-image`: OpenShift release image\n- `--sts-creds`: **REQUIRED** - Path to STS credentials file\n- `--role-arn`: **REQUIRED** - ARN of the IAM role to assume\n- `--base-domain`: **REQUIRED** - Base domain for the cluster\n\n**Smart Defaults by Environment:**\n\n**Development Environment:**\n```bash\n--instance-type m5.large\n--node-pool-replicas 2\n--control-plane-availability-policy SingleReplica\n--endpoint-access Public\n--root-volume-size 120\n--zones auto-select 2 zones based on region\n```\n\n**Production Environment:**\n```bash\n--instance-type m5.xlarge\n--node-pool-replicas 3\n--control-plane-availability-policy HighlyAvailable\n--endpoint-access PublicAndPrivate\n--root-volume-size 120\n--auto-repair true\n--zones auto-select 3+ zones based on region\n```\n\n**Cost-Optimized Environment:**\n```bash\n--instance-type m5.large\n--node-pool-replicas 2\n--control-plane-availability-policy SingleReplica\n--endpoint-access Public\n--root-volume-size 120\n--zones auto-select 2 zones (minimal redundancy)\n```\n\n### Step 3: Interactive Parameter Collection\n\n**Required Information Collection:**\n\n1. **Cluster Name**\n   ```\n    **Cluster Name**: What would you like to name your cluster?\n      - Must be DNS-compatible (lowercase, hyphens allowed)\n      - Used for AWS resource naming\n      - Example: dev-cluster, prod-app, demo-env\n   ```\n\n2. **AWS Region**\n   ```\n    **AWS Region**: Which AWS region should host your cluster?\n      - Consider latency to your users\n      - Verify desired instance types are available\n      - [Press Enter for default: us-east-1]\n\n      Popular regions:\n      - us-east-1 (N. Virginia) - Largest service availability\n      - us-west-2 (Oregon) - West coast, latest services\n      - eu-west-1 (Ireland) - Europe\n      - ap-southeast-1 (Singapore) - Asia Pacific\n   ```\n\n3. **STS Credentials**\n   ```\n    **STS Credentials**: Path to your AWS STS credentials file?\n      - Required for AWS authentication\n      - Generate using: aws sts get-session-token\n      - Example: /home/user/.aws/sts-creds.json\n      - Format: {\"AccessKeyId\": \"...\", \"SecretAccessKey\": \"...\", \"SessionToken\": \"...\"}\n   ```\n\n4. **IAM Role ARN**\n   ```\n    **IAM Role ARN**: ARN of the IAM role for HyperShift?\n      - Role must have required HyperShift permissions\n      - Example: arn:aws:iam::123456789012:role/hypershift-operator-role\n      - See: https://hypershift.openshift.io/aws-setup/\n   ```\n\n5. **Base Domain**\n   ```\n    **Base Domain**: What base domain should be used for cluster DNS?\n      - Must be a domain you control in Route53\n      - Used for cluster API and application routes\n      - Example: example.com, clusters.mycompany.com\n   ```\n\n6. **Pull Secret**\n   ```\n    **Pull Secret**: Path to your OpenShift pull secret file?\n      - Required for accessing OpenShift container images\n      - Download from: https://console.redhat.com/openshift/install/pull-secret\n      - Example: /home/user/pull-secret.json\n   ```\n\n7. **OpenShift Version**\n   ```\n    **OpenShift Version**: Which OpenShift version do you want to use?\n\n       **Check supported versions**: https://amd64.ocp.releases.ci.openshift.org/\n\n      - Enter release image URL: quay.io/openshift-release-dev/ocp-release:X.Y.Z-multi\n      - [Press Enter for default: quay.io/openshift-release-dev/ocp-release:4.18.0-multi]\n   ```\n\n**Optional Configuration (based on description analysis):**\n\n8. **Instance Type** (if performance requirements detected)\n   ```\n    **Instance Type**: Select instance type based on your performance needs:\n      - m5.large (2 vCPU, 8GB RAM) - Development, light workloads\n      - m5.xlarge (4 vCPU, 16GB RAM) - Production, balanced workloads\n      - m5.2xlarge (8 vCPU, 32GB RAM) - High-performance workloads\n      - c5.xlarge (4 vCPU, 8GB RAM) - Compute-optimized\n      - [Press Enter for default based on environment type]\n   ```\n\n9. **Node Pool Replicas**\n   ```\n    **Node Pool Replicas**: How many worker nodes do you need?\n      - Minimum: 2 (for basic redundancy)\n      - Production recommended: 3+\n      - [Press Enter for default based on environment type]\n   ```\n\n10. **Availability Zones** (auto-selected, but confirmed)\n    ```\n     **Availability Zones**: Detected region: us-east-1\n       Auto-selecting zones for optimal distribution:\n       - Development: us-east-1a, us-east-1b (2 zones)\n       - Production: us-east-1a, us-east-1b, us-east-1c (3 zones)\n\n       Modify zone selection? [y/N]\n    ```\n\n### Step 4: Advanced Configuration (Conditional)\n\n**For FIPS Compliance** (if detected):\n```\n **FIPS Mode**: Enable FIPS mode for compliance?\n   - Required for government/regulated workloads\n   - May impact performance\n   - [yes/no] [Press Enter for default: no]\n```\n\n**For High-Performance Workloads**:\n```\n **Root Volume Size**: Increase root volume size?\n   - Default: 120GB\n   - High-performance workloads: 200GB+\n   - [Press Enter for default: 120]\n```\n\n**For Production Environments**:\n```\n **Auto-Repair**: Enable automatic node repair?\n   - Automatically replaces unhealthy nodes\n   - Recommended for production\n   - [yes/no] [Press Enter for default: yes for production]\n```\n\n### Step 5: Generate Command\n\n**Basic AWS Cluster Command:**\n```bash\nhypershift create cluster aws \\\n  --name <cluster-name> \\\n  --namespace <cluster-name>-ns \\\n  --region <region> \\\n  --instance-type <instance-type> \\\n  --pull-secret <pull-secret-path> \\\n  --node-pool-replicas <replica-count> \\\n  --zones <zone-list> \\\n  --control-plane-availability-policy <policy> \\\n  --sts-creds <sts-creds-path> \\\n  --role-arn <role-arn> \\\n  --base-domain <base-domain> \\\n  --release-image <release-image>\n```\n\n**Development Configuration Example:**\n```bash\nhypershift create cluster aws \\\n  --name dev-cluster \\\n  --namespace dev-cluster-ns \\\n  --region us-east-1 \\\n  --instance-type m5.large \\\n  --pull-secret /path/to/pull-secret.json \\\n  --node-pool-replicas 2 \\\n  --zones us-east-1a,us-east-1b \\\n  --control-plane-availability-policy SingleReplica \\\n  --endpoint-access Public \\\n  --root-volume-size 120 \\\n  --sts-creds /path/to/sts-creds.json \\\n  --role-arn arn:aws:iam::123456789012:role/hypershift-role \\\n  --base-domain example.com \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi\n```\n\n**Production Configuration Example:**\n```bash\nhypershift create cluster aws \\\n  --name production-cluster \\\n  --namespace production-cluster-ns \\\n  --region us-west-2 \\\n  --instance-type m5.xlarge \\\n  --pull-secret /path/to/pull-secret.json \\\n  --node-pool-replicas 3 \\\n  --zones us-west-2a,us-west-2b,us-west-2c \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --endpoint-access PublicAndPrivate \\\n  --root-volume-size 120 \\\n  --auto-repair \\\n  --sts-creds /path/to/sts-creds.json \\\n  --role-arn arn:aws:iam::123456789012:role/hypershift-prod-role \\\n  --base-domain clusters.company.com \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi\n```\n\n**FIPS-Enabled Configuration:**\n```bash\nhypershift create cluster aws \\\n  --name compliance-cluster \\\n  --namespace compliance-cluster-ns \\\n  --region us-gov-east-1 \\\n  --instance-type m5.xlarge \\\n  --pull-secret /path/to/pull-secret.json \\\n  --node-pool-replicas 3 \\\n  --zones us-gov-east-1a,us-gov-east-1b,us-gov-east-1c \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --fips \\\n  --sts-creds /path/to/sts-creds.json \\\n  --role-arn arn:aws-us-gov:iam::123456789012:role/hypershift-fips-role \\\n  --base-domain secure.gov.example.com \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi\n```\n\n### Step 6: Pre-Flight Validation\n\n**Provide validation commands:**\n```\n## Pre-Flight Checks\n\nBefore creating the cluster, verify your setup:\n\n1. **AWS Credentials:**\n   aws sts get-caller-identity\n\n2. **STS Credentials File:**\n   cat /path/to/sts-creds.json | jq .\n\n3. **IAM Role Access:**\n   aws iam get-role --role-name hypershift-role\n\n4. **Route53 Domain:**\n   aws route53 list-hosted-zones --query \"HostedZones[?Name=='example.com.']\"\n\n5. **Region Availability:**\n   aws ec2 describe-availability-zones --region us-east-1\n\n6. **Instance Type Availability:**\n   aws ec2 describe-instance-type-offerings --location-type availability-zone --filters Name=instance-type,Values=m5.large --region us-east-1\n```\n\n### Step 7: Post-Generation Instructions\n\n**Next Steps:**\n```\n## Next Steps\n\n1. **Verify prerequisites are met:**\n   - AWS credentials configured\n   - STS credentials file exists and is valid\n   - IAM role has required permissions\n   - Base domain exists in Route53\n\n2. **Run the generated command:**\n   Copy and paste the command above\n\n3. **Monitor cluster creation:**\n   kubectl get hostedcluster -n <cluster-namespace>\n   kubectl get nodepool -n <cluster-namespace>\n\n4. **Check AWS resources:**\n   - EC2 instances in AWS console\n   - Load balancers created\n   - VPC and networking resources\n\n5. **Access cluster when ready:**\n   hypershift create kubeconfig --name <cluster-name> --namespace <cluster-namespace>\n   export KUBECONFIG=<cluster-name>-kubeconfig\n   oc get nodes\n```\n\n## Error Handling\n\n### Invalid AWS Credentials\n\n**Scenario:** AWS credentials are invalid or expired.\n\n**Action:**\n```\nAWS credentials validation failed.\n\nPlease check:\n1. AWS CLI configuration: aws configure list\n2. STS credentials file validity\n3. IAM permissions\n\nRegenerate STS credentials:\n  aws sts get-session-token --duration-seconds 3600\n```\n\n### IAM Role Not Found\n\n**Scenario:** Specified IAM role doesn't exist or can't be assumed.\n\n**Action:**\n```\nIAM role \"arn:aws:iam::123456789012:role/hypershift-role\" not found or inaccessible.\n\nPlease verify:\n1. Role exists: aws iam get-role --role-name hypershift-role\n2. Role has required permissions\n3. Trust relationship allows your account to assume the role\n\nSee HyperShift AWS setup guide: https://hypershift.openshift.io/aws-setup/\n```\n\n### Region/Zone Issues\n\n**Scenario:** Instance type not available in selected region/zones.\n\n**Action:**\n```\nInstance type \"m5.large\" not available in zone \"us-east-1f\".\n\nChecking alternative zones in us-east-1:\n us-east-1a (available)\n us-east-1b (available)\n us-east-1f (not available)\n\nSuggested zones: us-east-1a,us-east-1b\n\nWould you like me to update the command?\n```\n\n### Route53 Domain Issues\n\n**Scenario:** Base domain not found in Route53 or not accessible.\n\n**Action:**\n```\nBase domain \"example.com\" not found in Route53.\n\nPlease ensure:\n1. Domain exists in Route53: aws route53 list-hosted-zones\n2. Account has access to the hosted zone\n3. Domain spelling is correct\n\nAlternative: Use a subdomain you control (e.g., clusters.mydomain.com)\n```\n\n### Resource Limits\n\n**Scenario:** AWS account limits would be exceeded.\n\n**Action:**\n```\nAWS service limits may be exceeded:\n- EC2 instances: Current: 18/20, Requested: 5 more\n- Elastic IPs: Current: 4/5, Requested: 2 more\n\nConsider:\n1. Request limit increases via AWS Support\n2. Choose smaller instance types\n3. Reduce node count\n4. Clean up unused resources\n```\n\n## Best Practices\n\n### Cost Optimization\n\n1. **Right-size instances:** Don't over-provision for development\n2. **Use Spot instances:** Where appropriate for non-critical workloads\n3. **Monitor resource usage:** Regularly review AWS costs\n4. **Clean up unused clusters:** Delete development clusters when not needed\n\n### Security\n\n1. **Least privilege IAM:** Use minimal required permissions\n2. **STS credentials:** Use short-lived credentials when possible\n3. **Private networking:** Use PrivateAndPublic endpoints for production\n4. **KMS encryption:** Enable for sensitive workloads\n\n### High Availability\n\n1. **Multi-AZ deployment:** Use 3+ availability zones for production\n2. **Instance distribution:** Spread nodes across zones\n3. **Auto-repair:** Enable for automatic recovery\n4. **Monitoring:** Set up CloudWatch monitoring\n\n### Network Planning\n\n1. **VPC design:** Plan CIDR ranges carefully\n2. **Subnet strategy:** Use public/private subnet design\n3. **Load balancer:** Configure appropriate load balancer types\n4. **DNS:** Ensure proper Route53 configuration\n\n## Anti-Patterns to Avoid\n\n **Using root AWS credentials**\n```\nNever use root account credentials for HyperShift\n```\n Use IAM roles and STS credentials\n\n **Single availability zone for production**\n```\n--zones us-east-1a  # Single point of failure\n```\n Use multiple zones: `--zones us-east-1a,us-east-1b,us-east-1c`\n\n **Over-provisioning for development**\n```\n--instance-type m5.8xlarge --node-pool-replicas 10  # Expensive for dev\n```\n Use appropriate sizing: `--instance-type m5.large --node-pool-replicas 2`\n\n **Ignoring region-specific limitations**\n```\nChoosing regions without checking instance type availability\n```\n Verify instance types and services are available in target region\n\n## Example Workflows\n\n### Startup Development Environment\n```\nInput: \"cheap AWS cluster for testing our new microservice\"\n\nAnalysis:\n- Environment: Development\n- Cost focus: High priority\n- Scale: Minimal\n\nGenerated Command:\nhypershift create cluster aws \\\n  --name dev-microservice \\\n  --namespace dev-microservice-ns \\\n  --region us-east-1 \\\n  --instance-type m5.large \\\n  --node-pool-replicas 2 \\\n  --control-plane-availability-policy SingleReplica \\\n  --endpoint-access Public\n```\n\n### Enterprise Production\n```\nInput: \"highly available AWS production cluster for customer-facing applications\"\n\nAnalysis:\n- Environment: Production\n- Availability: High priority\n- Scale: Enterprise\n\nGenerated Command:\nhypershift create cluster aws \\\n  --name prod-customer-apps \\\n  --namespace prod-customer-apps-ns \\\n  --region us-west-2 \\\n  --instance-type m5.xlarge \\\n  --node-pool-replicas 5 \\\n  --zones us-west-2a,us-west-2b,us-west-2c \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --endpoint-access PublicAndPrivate \\\n  --auto-repair\n```\n\n## See Also\n\n- [HyperShift AWS Provider Documentation](https://hypershift.openshift.io/aws-setup/)\n- [AWS IAM Roles for HyperShift](https://hypershift.openshift.io/aws-setup/#_prerequisites)\n- [AWS CLI Configuration Guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)\n- [OpenShift on AWS Best Practices](https://docs.openshift.com/container-platform/latest/installing/installing_aws/)"
              },
              {
                "name": "HyperShift Azure Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on Microsoft Azure with proper identity configuration and resource management",
                "path": "plugins/hcp/skills/hcp-create-azure/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift Azure Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on Microsoft Azure with proper identity configuration and resource management"
                },
                "content": "# HyperShift Azure Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters on Azure, focusing on self-managed control plane configuration, resource group management, and Azure identity integration.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate azure` command to guide the Azure provider cluster creation process.\n\n## Prerequisites\n\n- Azure CLI configured with appropriate credentials\n- Azure subscription with sufficient quotas\n- HyperShift operator installed and configured\n- Pull secret for accessing OpenShift images\n\n## Azure Provider Overview\n\n### Azure Provider Peculiarities\n\n- **Self-managed control plane only:** For ARO HCP use ARO CLI instead\n- **Resource groups:** Auto-created during cluster creation\n- **Limited region availability:** Not all Azure regions support all features\n- **Azure identity required:** Service principal or managed identity configuration\n- **Virtual network integration:** Requires proper VNet configuration\n- **Control plane runs on Azure VMs:** Managed by HyperShift operator\n\n### Identity Configuration Options\n\nChoose one of three identity methods:\n\n1. **Managed + Data Plane Identities:** Use `--managed-identities-file` AND `--data-plane-identities-file`\n2. **Workload Identities:** Use `--workload-identities-file`\n3. **OIDC Integration:** Use `--oidc-issuer-url`\n\n## Implementation Steps\n\n### Step 1: Parse Environment Requirements\n\n**Environment Detection:**\n- **Development:** \"dev\", \"testing\", \"demo\"  Standard_D4s_v3, SingleReplica\n- **Production:** \"prod\", \"enterprise\"  Standard_D8s_v3+, HighlyAvailable\n\n### Step 2: Interactive Parameter Collection\n\n**Required Parameters:**\n\n1. **Cluster Name & Location**\n   ```\n    **Cluster Name**: What would you like to name your cluster?\n    **Azure Location**: Which Azure region? [default: eastus]\n   ```\n\n2. **Identity Configuration Method**\n   ```\n    **Identity Method**: Choose Azure identity configuration:\n      1. Managed + Data Plane Identities (recommended)\n      2. Workload Identities\n      3. OIDC Integration\n   ```\n\n3. **Resource Group Configuration**\n   ```\n    **Resource Group**: Name for the resource group?\n      [default: {cluster-name}-rg]\n   ```\n\n### Step 3: Generate Command\n\n**Development Configuration:**\n```bash\nhypershift create cluster azure \\\n  --name dev-cluster \\\n  --namespace dev-cluster-ns \\\n  --location eastus \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --resource-group-name dev-cluster-rg \\\n  --base-domain example.com \\\n  --managed-identities-file /path/to/managed-identities.json \\\n  --data-plane-identities-file /path/to/data-plane-identities.json\n```\n\n**Production Configuration:**\n```bash\nhypershift create cluster azure \\\n  --name production-cluster \\\n  --namespace production-cluster-ns \\\n  --location eastus \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --resource-group-name production-cluster-rg \\\n  --base-domain clusters.company.com \\\n  --managed-identities-file /path/to/managed-identities.json \\\n  --data-plane-identities-file /path/to/data-plane-identities.json \\\n  --control-plane-availability-policy HighlyAvailable\n```\n\n## Error Handling\n\n### Identity Configuration Issues\n```\nAzure identity files not found or invalid.\n\nRequired files for managed identity method:\n1. managed-identities.json\n2. data-plane-identities.json\n\nGenerate using Azure CLI:\n  az identity create --name hypershift-managed-identity\n```\n\n### Resource Group Conflicts\n```\nResource group \"cluster-rg\" already exists.\n\nOptions:\n1. Use existing resource group (ensure proper permissions)\n2. Choose different name\n3. Delete existing resource group (if safe)\n```\n\n## See Also\n\n- [HyperShift Azure Provider Documentation](https://hypershift.openshift.io/how-to/azure/)\n- [Azure Resource Manager Documentation](https://docs.microsoft.com/en-us/azure/azure-resource-manager/)"
              },
              {
                "name": "HyperShift KubeVirt Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on existing Kubernetes clusters using KubeVirt virtualization",
                "path": "plugins/hcp/skills/hcp-create-kubevirt/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift KubeVirt Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on existing Kubernetes clusters using KubeVirt virtualization"
                },
                "content": "# HyperShift KubeVirt Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters using the KubeVirt provider, which runs on existing Kubernetes clusters with special attention to network conflict prevention and virtual machine management.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate kubevirt` command to guide the KubeVirt provider cluster creation process.\n\n## Prerequisites\n\n- Kubernetes cluster with KubeVirt installed and configured\n- Sufficient compute resources on the host cluster\n- Storage classes configured for VM disks\n- HyperShift operator installed\n- Pull secret for accessing OpenShift images\n\n## KubeVirt Provider Overview\n\n### KubeVirt Provider Peculiarities\n\n- **Runs on existing Kubernetes cluster:** Host cluster provides compute resources\n- **CRITICAL Network Isolation:** Management cluster network cannot conflict with HostedCluster network\n- **Storage Requirements:** Requires storage classes for VM disks and persistent volumes\n- **Virtual Machine Templates:** Uses VM-based nodes instead of bare metal\n- **IPv6 Support:** Full IPv6 support available\n- **Disconnected Capable:** Can run in airgapped environments\n- **Resource Planning:** Requires sufficient compute resources on host cluster\n\n### Network Conflict Prevention (CRITICAL)\n\nThe most important aspect of KubeVirt clusters is preventing network conflicts:\n\n- **Service/Cluster/Machine CIDRs must not overlap with management cluster**\n- **Default CIDRs are designed to avoid common management cluster ranges**\n- **Always check management cluster CIDRs before setting HostedCluster CIDRs**\n\n**Common management cluster ranges to avoid:**\n- 10.128.0.0/14 (common OCP default)\n- 10.0.0.0/16 (common private range)\n- 192.168.0.0/16 (common private range)\n\n## Implementation Steps\n\n### Step 1: Analyze Cluster Description\n\nParse the natural language description for KubeVirt-specific requirements:\n\n**Environment Type Detection:**\n- **Development**: \"dev\", \"development\", \"testing\", \"lab\", \"demo\"\n- **Production**: \"prod\", \"production\", \"critical\", \"enterprise\"\n- **Disconnected**: \"airgapped\", \"disconnected\", \"offline\", \"air-gapped\"\n\n**Resource Indicators:**\n- **High Performance**: \"performance\", \"fast\", \"high-compute\", \"intensive\"\n- **Standard**: Default moderate configuration\n- **Minimal**: \"small\", \"minimal\", \"basic\", \"edge\"\n\n**Network Requirements:**\n- **IPv6**: \"ipv6\", \"dual-stack\", \"ipv6-only\"\n- **Isolated**: \"isolated\", \"private\", \"secure\"\n\n**Storage Requirements:**\n- **Local Storage**: \"local\", \"local storage\", \"hostpath\"\n- **Replicated**: \"replicated\", \"distributed\", \"ceph\", \"longhorn\"\n\n### Step 2: Management Cluster Network Discovery\n\n**CRITICAL FIRST STEP:** Always check management cluster networks to prevent conflicts.\n\n**Prompt for management cluster information:**\n```\n **Management Cluster Networks**: To avoid conflicts, please run this command on your management cluster:\n\n   `oc get network cluster -o yaml`\n\n   From the output, what are the serviceNetwork and clusterNetwork CIDRs?\n   - serviceNetwork CIDR: [e.g., 172.30.0.0/16]\n   - clusterNetwork CIDR: [e.g., 10.128.0.0/14]\n   - [Press Enter if you don't know - I'll use safe defaults]\n```\n\n**If user provides management cluster CIDRs:**\n- Validate they don't overlap with our proposed defaults\n- Adjust HostedCluster CIDRs if conflicts detected\n- Document the conflict avoidance in the output\n\n### Step 3: Apply KubeVirt Provider Defaults\n\n**Required Parameters:**\n- `--memory`: Memory allocation for VMs (default: 8Gi)\n- `--cores`: CPU cores for VMs (default: 2)\n- `--pull-secret`: Path to pull secret file\n- `--release-image`: OpenShift release image\n\n**IPv4 Non-Conflicting Defaults:**\n- `--service-cidr`: 172.30.0.0/16 (avoids common 10.x ranges)\n- `--cluster-cidr`: 10.132.0.0/14 (avoids common 10.128.x range)\n- `--machine-cidr`: 192.168.126.0/24 (avoids common 192.168.1.x range)\n\n**IPv6 Non-Conflicting Defaults:**\n- `--service-cidr`: fd02::/112\n- `--cluster-cidr`: fd01::/48\n- `--machine-cidr`: fd03::/64\n\n**Smart Defaults by Environment:**\n\n**Development Environment:**\n```bash\n--memory 8Gi\n--cores 2\n--control-plane-availability-policy SingleReplica\n--node-pool-replicas 2\n```\n\n**Production Environment:**\n```bash\n--memory 16Gi\n--cores 4\n--control-plane-availability-policy HighlyAvailable\n--node-pool-replicas 3\n--auto-repair true\n```\n\n**High-Performance Environment:**\n```bash\n--memory 32Gi\n--cores 8\n--control-plane-availability-policy HighlyAvailable\n--node-pool-replicas 5\n```\n\n### Step 4: Interactive Parameter Collection\n\n**Required Information Collection:**\n\n1. **Cluster Name**\n   ```\n    **Cluster Name**: What would you like to name your cluster?\n      - Must be DNS-compatible (lowercase, hyphens allowed)\n      - Used for VM and resource naming\n      - Example: dev-cluster, prod-app, test-env\n   ```\n\n2. **Management Cluster Network Check** (see Step 2)\n\n3. **VM Resource Configuration**\n   ```\n    **VM Memory**: How much memory should each VM have?\n      - Development: 8Gi (minimum recommended)\n      - Production: 16Gi+ (better performance)\n      - High-performance: 32Gi+ (intensive workloads)\n      - [Press Enter for default based on environment]\n   ```\n\n   ```\n    **VM CPU Cores**: How many CPU cores should each VM have?\n      - Development: 2 cores (minimum recommended)\n      - Production: 4+ cores (better performance)\n      - High-performance: 8+ cores (intensive workloads)\n      - [Press Enter for default based on environment]\n   ```\n\n4. **Pull Secret**\n   ```\n    **Pull Secret**: Path to your OpenShift pull secret file?\n      - Required for accessing OpenShift container images\n      - Download from: https://console.redhat.com/openshift/install/pull-secret\n      - Example: /home/user/pull-secret.json\n   ```\n\n5. **OpenShift Version**\n   ```\n    **OpenShift Version**: Which OpenShift version do you want to use?\n\n       **Check supported versions**: https://amd64.ocp.releases.ci.openshift.org/\n\n      - Enter release image URL: quay.io/openshift-release-dev/ocp-release:X.Y.Z-multi\n      - [Press Enter for default: quay.io/openshift-release-dev/ocp-release:4.18.0-multi]\n   ```\n\n**Optional Configuration:**\n\n6. **IPv6 Support** (if detected)\n   ```\n    **IPv6 Configuration**: Detected IPv6 requirement. Configure network stack:\n      - ipv4: IPv4 only (default)\n      - ipv6: IPv6 only\n      - dual: Dual-stack (IPv4 + IPv6)\n      - [Press Enter for default: ipv4]\n   ```\n\n7. **Storage Class**\n   ```\n    **Storage Class**: Which storage class should be used for VM disks?\n      - List available storage classes on your cluster:\n        kubectl get storageclass\n      - [Press Enter to use cluster default]\n   ```\n\n8. **Node Count**\n   ```\n    **Node Pool Replicas**: How many worker nodes do you need?\n      - Minimum: 2 (for basic redundancy)\n      - Production recommended: 3+\n      - [Press Enter for default based on environment type]\n   ```\n\n### Step 5: Network CIDR Validation\n\n**If management cluster CIDRs provided, validate conflicts:**\n\n```\n## Network Conflict Check\n\nManagement cluster networks:\n- Service Network: 172.30.0.0/16\n- Cluster Network: 10.128.0.0/14\n\nHostedCluster networks (checking for conflicts):\n Service CIDR: 172.30.0.0/16 (safe - different range)\n Cluster CIDR: 10.132.0.0/14 (CONFLICT with 10.128.0.0/14)\n Machine CIDR: 192.168.126.0/24 (safe)\n\nAdjusting Cluster CIDR to avoid conflict:\nNew Cluster CIDR: 10.140.0.0/14\n```\n\n**If no management cluster info provided, use safe defaults:**\n```\n## Network Configuration (Safe Defaults)\n\nUsing safe default CIDRs that avoid common management cluster ranges:\n- Service CIDR: 172.30.0.0/16 (avoids 10.x ranges)\n- Cluster CIDR: 10.132.0.0/14 (avoids common 10.128.x)\n- Machine CIDR: 192.168.126.0/24 (avoids common 192.168.1.x)\n```\n\n### Step 6: Generate Command\n\n**Basic KubeVirt Cluster Command:**\n```bash\nhypershift create cluster kubevirt \\\n  --name <cluster-name> \\\n  --namespace <cluster-name>-ns \\\n  --memory <memory> \\\n  --cores <cores> \\\n  --pull-secret <pull-secret-path> \\\n  --release-image <release-image> \\\n  --service-cidr <service-cidr> \\\n  --cluster-cidr <cluster-cidr> \\\n  --machine-cidr <machine-cidr>\n```\n\n**Development Configuration Example:**\n```bash\nhypershift create cluster kubevirt \\\n  --name dev-cluster \\\n  --namespace dev-cluster-ns \\\n  --memory 8Gi \\\n  --cores 2 \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --service-cidr 172.30.0.0/16 \\\n  --cluster-cidr 10.132.0.0/14 \\\n  --machine-cidr 192.168.126.0/24 \\\n  --control-plane-availability-policy SingleReplica \\\n  --node-pool-replicas 2\n```\n\n**Production Configuration Example:**\n```bash\nhypershift create cluster kubevirt \\\n  --name production-cluster \\\n  --namespace production-cluster-ns \\\n  --memory 16Gi \\\n  --cores 4 \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --service-cidr 172.30.0.0/16 \\\n  --cluster-cidr 10.132.0.0/14 \\\n  --machine-cidr 192.168.126.0/24 \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --node-pool-replicas 3 \\\n  --auto-repair\n```\n\n**IPv6 Configuration Example:**\n```bash\nhypershift create cluster kubevirt \\\n  --name ipv6-cluster \\\n  --namespace ipv6-cluster-ns \\\n  --memory 16Gi \\\n  --cores 4 \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --service-cidr fd02::/112 \\\n  --cluster-cidr fd01::/48 \\\n  --machine-cidr fd03::/64\n```\n\n**Disconnected Configuration Example:**\n```bash\nhypershift create cluster kubevirt \\\n  --name airgapped-cluster \\\n  --namespace airgapped-cluster-ns \\\n  --memory 16Gi \\\n  --cores 4 \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --service-cidr 172.30.0.0/16 \\\n  --cluster-cidr 10.132.0.0/14 \\\n  --machine-cidr 192.168.126.0/24 \\\n  --image-content-sources /path/to/image-content-sources.yaml \\\n  --additional-trust-bundle /path/to/ca-bundle.pem \\\n  --render\n```\n\n### Step 7: Pre-Flight Validation\n\n**Provide validation commands:**\n```\n## Pre-Flight Checks\n\nBefore creating the cluster, verify your KubeVirt setup:\n\n1. **KubeVirt Status:**\n   kubectl get kubevirt -A\n\n2. **Available Storage Classes:**\n   kubectl get storageclass\n\n3. **Node Resources:**\n   kubectl top nodes\n\n4. **Available Memory/CPU:**\n   kubectl describe nodes | grep -E \"(Allocatable|Allocated)\"\n\n5. **Network Configuration:**\n   oc get network cluster -o yaml\n\n6. **Required Compute Resources:**\n   - Control Plane VMs: 3 x (<cores> cores, <memory> RAM)\n   - Worker VMs: <replica-count> x (<cores> cores, <memory> RAM)\n   - Total Required: <total-cores> cores, <total-memory> RAM\n```\n\n### Step 8: Post-Generation Instructions\n\n**For Disconnected Environments:**\n```\n## Post-Creation Configuration (Disconnected)\n\nAfter running the command above (with --render), you'll need to modify the generated manifests:\n\n1. **Add ImageContentSources to HostedCluster:**\n```yaml\nspec:\n  imageContentSources:\n  - source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n    mirrors:\n    - registry.example.com:5000/openshift/release\n  - source: quay.io/openshift-release-dev/ocp-release\n    mirrors:\n    - registry.example.com:5000/openshift/release-images\n```\n\n2. **Apply the manifests:**\n```bash\nkubectl apply -f <rendered-manifest-files>\n```\n\n**For All Environments:**\n```\n## Next Steps\n\n1. **Monitor cluster creation:**\n   kubectl get hostedcluster -n <cluster-namespace>\n   kubectl get nodepool -n <cluster-namespace>\n\n2. **Check VM creation:**\n   kubectl get vmi -A\n\n3. **Monitor VM startup:**\n   kubectl get vmi -A -w\n\n4. **Access cluster when ready:**\n   hypershift create kubeconfig --name <cluster-name> --namespace <cluster-namespace>\n   export KUBECONFIG=<cluster-name>-kubeconfig\n   oc get nodes\n\n5. **Verify network isolation:**\n   oc get network cluster -o yaml  # Check HostedCluster networks\n```\n\n## Error Handling\n\n### Network CIDR Conflicts\n\n**Scenario:** HostedCluster CIDRs conflict with management cluster.\n\n**Action:**\n```\nCIDR conflict detected:\n- Management cluster: 10.128.0.0/14\n- HostedCluster (proposed): 10.132.0.0/14\n- Overlap detected!\n\nSuggested alternative CIDRs:\n- Service CIDR: 172.30.0.0/16 (safe)\n- Cluster CIDR: 10.140.0.0/14 (avoids conflict)\n- Machine CIDR: 192.168.126.0/24 (safe)\n\nUpdate command with new CIDRs? [y/N]\n```\n\n### Insufficient Resources\n\n**Scenario:** Host cluster lacks sufficient resources for VMs.\n\n**Action:**\n```\nInsufficient resources on host cluster:\n\nRequired:\n- CPU: 24 cores (3 control plane + 6 workers @ 4 cores each)\n- Memory: 144Gi (3 control plane + 6 workers @ 16Gi each)\n\nAvailable:\n- CPU: 16 cores\n- Memory: 96Gi\n\nSuggestions:\n1. Reduce VM resources (--memory 8Gi --cores 2)\n2. Reduce worker count (--node-pool-replicas 2)\n3. Use SingleReplica control plane\n4. Add more nodes to host cluster\n```\n\n### Storage Class Issues\n\n**Scenario:** No suitable storage class available.\n\n**Action:**\n```\nNo default storage class found. Available storage classes:\n\nNAME                 PROVISIONER\nlocal-path          rancher.io/local-path\nceph-rbd           kubernetes.io/rbd\n\nSpecify storage class in NodePool configuration:\n  storageClassName: local-path\n\nOr set a default storage class:\n  kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n```\n\n### KubeVirt Not Ready\n\n**Scenario:** KubeVirt is not properly installed or configured.\n\n**Action:**\n```\nKubeVirt is not ready:\n\nCheck KubeVirt status:\n  kubectl get kubevirt -A\n  kubectl get pods -n kubevirt\n\nInstall KubeVirt if missing:\n  kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/kubevirt-operator.yaml\n  kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/kubevirt-cr.yaml\n\nVerify installation:\n  kubectl wait kv kubevirt --for condition=Available -n kubevirt --timeout=300s\n```\n\n## Best Practices\n\n### Resource Planning\n\n1. **Right-size VMs:** Don't over-provision for development environments\n2. **Monitor resource usage:** Keep track of host cluster resource consumption\n3. **Plan for growth:** Ensure host cluster can accommodate scaling\n4. **Storage planning:** Use appropriate storage classes for performance needs\n\n### Network Management\n\n1. **Always check management cluster CIDRs:** Prevent network conflicts\n2. **Document network design:** Keep records of CIDR allocations\n3. **Plan for multiple clusters:** Reserve CIDR ranges for future clusters\n4. **IPv6 considerations:** Plan dual-stack if needed\n\n### Security\n\n1. **Network isolation:** Ensure proper network segmentation\n2. **Storage security:** Use encrypted storage where required\n3. **Image security:** Use trusted image registries\n4. **RBAC:** Implement proper role-based access control\n\n### Performance\n\n1. **VM sizing:** Balance resource allocation with performance needs\n2. **Storage performance:** Use high-performance storage for production\n3. **Network performance:** Consider SR-IOV for high-throughput workloads\n4. **CPU pinning:** Consider CPU pinning for performance-critical workloads\n\n## Anti-Patterns to Avoid\n\n **Ignoring network conflicts**\n```\nUsing default CIDRs without checking management cluster\n```\n Always check management cluster networks first\n\n **Under-provisioning VMs**\n```\n--memory 4Gi --cores 1  # Too small for OpenShift\n```\n Use minimum 8Gi memory and 2 cores\n\n **Over-provisioning for development**\n```\n--memory 64Gi --cores 16 --node-pool-replicas 10  # Excessive for dev\n```\n Use appropriate sizing for environment\n\n **Conflicting CIDR ranges**\n```\nUsing 10.128.0.0/14 when management cluster uses 10.128.0.0/14\n```\n Use non-overlapping CIDR ranges\n\n## Example Workflows\n\n### Development Lab\n```\nInput: \"small kubevirt cluster for development testing\"\n\nManagement cluster check:\n- Service: 172.30.0.0/16\n- Cluster: 10.128.0.0/14\n\nAnalysis:\n- Environment: Development\n- Scale: Small\n- Network: Avoid 10.128.x range\n\nGenerated Command:\nhypershift create cluster kubevirt \\\n  --name dev-lab \\\n  --namespace dev-lab-ns \\\n  --memory 8Gi \\\n  --cores 2 \\\n  --service-cidr 172.31.0.0/16 \\\n  --cluster-cidr 10.132.0.0/14 \\\n  --machine-cidr 192.168.126.0/24 \\\n  --control-plane-availability-policy SingleReplica \\\n  --node-pool-replicas 2\n```\n\n### Production Environment\n```\nInput: \"high-performance kubevirt cluster for production workloads\"\n\nAnalysis:\n- Environment: Production\n- Performance: High priority\n- Availability: HA required\n\nGenerated Command:\nhypershift create cluster kubevirt \\\n  --name prod-workloads \\\n  --namespace prod-workloads-ns \\\n  --memory 32Gi \\\n  --cores 8 \\\n  --service-cidr 172.30.0.0/16 \\\n  --cluster-cidr 10.132.0.0/14 \\\n  --machine-cidr 192.168.126.0/24 \\\n  --control-plane-availability-policy HighlyAvailable \\\n  --node-pool-replicas 5 \\\n  --auto-repair\n```\n\n## See Also\n\n- [KubeVirt Documentation](https://kubevirt.io/user-guide/)\n- [HyperShift KubeVirt Provider Guide](https://hypershift.openshift.io/how-to/kubevirt/)\n- [OpenShift Virtualization](https://docs.openshift.com/container-platform/latest/virt/about-virt.html)\n- [Network Configuration Best Practices](https://docs.openshift.com/container-platform/latest/networking/understanding-networking.html)"
              },
              {
                "name": "HyperShift OpenStack Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on OpenStack infrastructure with proper flavor selection and network configuration",
                "path": "plugins/hcp/skills/hcp-create-openstack/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift OpenStack Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on OpenStack infrastructure with proper flavor selection and network configuration"
                },
                "content": "# HyperShift OpenStack Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters on OpenStack, handling OpenStack-specific requirements including credentials, networking, and flavor selection.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate openstack` command to guide the OpenStack provider cluster creation process.\n\n## Prerequisites\n\n- OpenStack CLI configured with appropriate credentials\n- OpenStack project with sufficient quotas\n- External network configured for floating IPs\n- HyperShift operator installed and configured\n\n## OpenStack Provider Overview\n\n### OpenStack Provider Peculiarities\n\n- **Requires OpenStack credentials:** Must have valid clouds.yaml or environment variables\n- **Floating IP networks needed:** External network access for cluster API\n- **Flavor selection critical:** Instance flavors affect performance and cost\n- **Custom images may be required:** RHCOS images for worker nodes\n- **Network topology affects routing:** Proper network configuration essential\n\n## Implementation Steps\n\n### Step 1: Interactive Parameter Collection\n\n**Required Parameters:**\n\n1. **OpenStack Credentials**\n   ```\n    **OpenStack Credentials**: Path to OpenStack credentials file?\n      - Usually clouds.yaml format\n      - Example: /home/user/.config/openstack/clouds.yaml\n   ```\n\n2. **External Network**\n   ```\n    **External Network ID**: OpenStack external network UUID?\n      - Required for floating IP allocation\n      - Find with: openstack network list --external\n   ```\n\n3. **Flavor Selection**\n   ```\n    **Node Flavor**: Choose instance flavor:\n      - m1.large (4 vCPU, 8GB RAM) - Standard workloads\n      - m1.xlarge (8 vCPU, 16GB RAM) - Performance workloads\n      - [default: m1.large]\n   ```\n\n### Step 2: Generate Command\n\n**Standard Configuration:**\n```bash\nhypershift create cluster openstack \\\n  --name openstack-cluster \\\n  --namespace openstack-cluster-ns \\\n  --openstack-credentials-file /path/to/clouds.yaml \\\n  --openstack-external-network-id <external-network-uuid> \\\n  --openstack-node-flavor m1.large \\\n  --base-domain example.com \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi\n```\n\n## Error Handling\n\n### External Network Not Found\n```\nExternal network with ID \"<uuid>\" not found.\n\nList available external networks:\n  openstack network list --external\n\nEnsure network has proper routing configuration.\n```\n\n### Flavor Not Available\n```\nFlavor \"m1.large\" not available in this OpenStack deployment.\n\nList available flavors:\n  openstack flavor list\n\nChoose appropriate flavor for your workload requirements.\n```\n\n## See Also\n\n- [OpenStack Documentation](https://docs.openstack.org/)\n- [HyperShift OpenStack Provider](https://hypershift.openshift.io/how-to/openstack/)"
              },
              {
                "name": "HyperShift PowerVS Provider",
                "description": "Use this skill when you need to deploy HyperShift clusters on IBM Cloud PowerVS with proper processor configuration and resource management",
                "path": "plugins/hcp/skills/hcp-create-powervs/SKILL.md",
                "frontmatter": {
                  "name": "HyperShift PowerVS Provider",
                  "description": "Use this skill when you need to deploy HyperShift clusters on IBM Cloud PowerVS with proper processor configuration and resource management"
                },
                "content": "# HyperShift PowerVS Provider\n\nThis skill provides implementation guidance for creating HyperShift clusters on IBM Cloud PowerVS, handling PowerVS-specific requirements including IBM Cloud API keys, processor types, and resource group management.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/hcp:generate powervs` command to guide the PowerVS provider cluster creation process.\n\n## Prerequisites\n\n- IBM Cloud CLI configured with API key\n- PowerVS service instance configured\n- IBM Cloud resource group access\n- HyperShift operator installed and configured\n\n## PowerVS Provider Overview\n\n### PowerVS Provider Peculiarities\n\n- **IBM Cloud specific:** Requires IBM Cloud API key and resource group\n- **Different regions have different capabilities:** Service availability varies by region\n- **Limited instance types:** Fewer processor types compared to other clouds\n- **Network setup complex:** Requires careful network planning\n- **Processor type selection:** Shared, dedicated, or capped options\n\n## Implementation Steps\n\n### Step 1: Interactive Parameter Collection\n\n**Required Parameters:**\n\n1. **IBM Cloud Authentication**\n   ```\n    **IBM Cloud API Key**: Configure IBM Cloud authentication\n      - Set IBMCLOUD_API_KEY environment variable, OR\n      - Provide IBMCLOUD_CREDENTIALS file path\n   ```\n\n2. **Resource Group**\n   ```\n    **Resource Group**: IBM Cloud resource group name?\n      - Must exist in your IBM Cloud account\n      - Example: default, hypershift-rg\n   ```\n\n3. **Region Configuration**\n   ```\n    **Region**: IBM Cloud region?\n      [default: us-south]\n    **Zone**: Availability zone?\n      [default: us-south]\n   ```\n\n4. **Processor Configuration**\n   ```\n    **Memory**: Memory allocation per instance?\n      [default: 32GB]\n    **Processors**: Number of processors?\n      [default: 0.5]\n    **Processor Type**: Processor type?\n      - shared (default) - Shared processor pool\n      - dedicated - Dedicated processors\n      - capped - Capped shared processors\n   ```\n\n### Step 2: Generate Command\n\n**Standard Configuration:**\n```bash\nhypershift create cluster powervs \\\n  --name powervs-cluster \\\n  --namespace powervs-cluster-ns \\\n  --region us-south \\\n  --zone us-south \\\n  --resource-group default \\\n  --base-domain example.com \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --memory 32GB \\\n  --processors 0.5 \\\n  --proc-type shared \\\n  --sys-type s922 \\\n  --vpc-region us-south\n```\n\n**High-Performance Configuration:**\n```bash\nhypershift create cluster powervs \\\n  --name powervs-prod \\\n  --namespace powervs-prod-ns \\\n  --region us-south \\\n  --zone us-south \\\n  --resource-group production-rg \\\n  --base-domain clusters.company.com \\\n  --pull-secret /path/to/pull-secret.json \\\n  --release-image quay.io/openshift-release-dev/ocp-release:4.18.0-multi \\\n  --memory 64GB \\\n  --processors 2.0 \\\n  --proc-type dedicated \\\n  --sys-type s922 \\\n  --vpc-region us-south\n```\n\n## Error Handling\n\n### API Key Issues\n```\nIBM Cloud API key not configured or invalid.\n\nConfigure authentication:\n  export IBMCLOUD_API_KEY=\"your-api-key\"\n\nOr verify existing configuration:\n  ibmcloud auth list\n```\n\n### Resource Group Not Found\n```\nResource group \"hypershift-rg\" not found.\n\nList available resource groups:\n  ibmcloud resource groups\n\nCreate new resource group:\n  ibmcloud resource group-create hypershift-rg\n```\n\n### Region/Zone Issues\n```\nZone \"us-south-3\" not available for PowerVS.\n\nAvailable zones in us-south:\n  ibmcloud pi service-list\n\nChoose appropriate zone for your region.\n```\n\n## See Also\n\n- [IBM Cloud PowerVS Documentation](https://cloud.ibm.com/docs/power-iaas)\n- [HyperShift PowerVS Provider](https://hypershift.openshift.io/how-to/powervs/)"
              }
            ]
          },
          {
            "name": "compliance",
            "description": "Security compliance and vulnerability analysis tools for Go projects",
            "source": "./plugins/compliance",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install compliance@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-cve",
                "description": "Analyze Go codebase for CVE vulnerabilities and suggest fixes",
                "path": "plugins/compliance/commands/analyze-cve.md",
                "frontmatter": {
                  "description": "Analyze Go codebase for CVE vulnerabilities and suggest fixes",
                  "argument-hint": "<CVE-ID>"
                },
                "content": "## Name\ncompliance:analyze-cve\n\n## Synopsis\n```\n/compliance:analyze-cve <CVE-ID>\n```\n\n## Description\nThe `compliance:analyze-cve` command performs comprehensive security vulnerability analysis for Go projects. Given a CVE identifier, it fetches complete vulnerability details from authoritative sources, analyzes the codebase for potential impact, and provides actionable remediation guidance.\n\nThis command helps developers:\n- Understand the full scope of a CVE vulnerability\n- Determine if their Go codebase is affected\n- Get specific fix recommendations\n- Optionally apply fixes automatically\n\n## Implementation\n\n### Phase 1: CVE Intelligence Gathering\n\n1. **Validate CVE Format**\n   - Verify CVE ID follows standard format (e.g., CVE-2024-1234)\n   - Extract year and number components\n\n2. **Fetch CVE Details from Multiple Sources**\n   \n   Use web_search tool to gather information from these sources:\n   \n   - **Primary Sources**:\n     - **NVD**: Search for \"CVE-{ID} site:nvd.nist.gov\"\n       - URL pattern: https://nvd.nist.gov/vuln/detail/{CVE-ID}\n       - Extract: CVSS score, severity, affected versions, vulnerability type\n     \n     - **MITRE**: Search for \"CVE-{ID} site:cve.mitre.org\"\n       - URL pattern: https://cve.mitre.org/cgi-bin/cvename.cgi?name={CVE-ID}\n       - Extract: Description, references, CWE classification\n   \n   - **Go-Specific Sources**:\n     - **Go Vulnerability Database**: Search for \"CVE-{ID} golang vulnerability\"\n       - Check: https://go.dev/security/vuln/\n       - Search GitHub: \"CVE-{ID} site:github.com/golang/vulndb\"\n       - Extract: Affected Go packages, versions, fix versions\n     \n     - **GitHub Security Advisories**: Search for \"CVE-{ID} golang GHSA\"\n       - May have GHSA-* aliases\n       - Often contains detailed remediation steps\n   \n   - **General Go Security**: \n     - Search: \"CVE-{ID} golang fix\" or \"CVE-{ID} go security\"\n     - Look for blog posts, security advisories, and discussions\n\n3. **Handle Search Issues and Limited Results**\n   \n   - **If CVE details cannot be fetched** (network error, search failure, insufficient results):\n     - Inform user about the lookup issue\n     - Try alternative search strategies:\n       - Search for package name + \"vulnerability\" + year\n       - Search for GHSA (GitHub Security Advisory) aliases\n       - Check if govulncheck finds it (most reliable for Go CVEs)\n     - If still unsuccessful, ask user to provide available information:\n       - CVE description and severity\n       - Affected Go packages/modules\n       - Vulnerable version ranges\n       - Fixed versions (if known)\n       - Any relevant links or references\n     - Document the source as \"User-provided information\"\n     - Note limitations in final report\n   \n   - **If CVE is very new** (e.g., CVE-2025-xxxxx):\n     - May not be in NVD or Go vulndb yet\n     - Search for: \"CVE-{ID} disclosure\" or \"CVE-{ID} advisory\"\n     - Check vendor security pages directly\n     - Run govulncheck anyway - it may know about it via GHSA\n   \n   - **If suggested fixes cannot be found**:\n     - Check the package's GitHub releases for recent security fixes\n     - Look for security-related commits in the repository\n     - Ask user if they have:\n       - Official security advisories\n       - Patch information\n       - Workaround documentation\n       - Any relevant fix details\n     - Proceed with available information\n     - Clearly mark sections as \"Based on user input\" vs \"Verified online\"\n\n4. **Gather Remediation Intelligence**\n   - Search for:\n     - Official security advisories\n     - GitHub Security Advisories (GHSA)\n     - Vendor patches and updates\n     - Community discussions on GitHub, Go forums\n   - Follow hyperlinks to:\n     - Pull requests with fixes\n     - Security mailing list threads\n     - Blog posts with analysis\n     - Proof-of-concept exploits (for context)\n   - If searches fail or return insufficient results:\n     - Request user input for any known fixes or workarounds\n     - Accept partial information and document gaps\n\n5. **Compile Vulnerability Profile**\n   - Create structured summary with:\n     - CVE ID and aliases (GHSA-*, etc.)\n     - Severity and CVSS metrics\n     - Affected packages/modules\n     - Vulnerable version ranges\n     - Fixed versions\n     - Attack vectors and prerequisites\n     - Impact assessment (confidentiality, integrity, availability)\n     - Recommended mitigations\n   - **Clearly distinguish**:\n     - Information from authoritative sources (NVD, MITRE, etc.)\n     - Information from web searches\n     - Information provided by user\n     - Information gaps or uncertainties\n\n### Phase 2: Codebase Impact Analysis\n\n1. **Identify Go Module Dependencies**\n   - Read `go.mod` file from workspace root\n   - Parse direct and indirect dependencies\n   - Extract module versions using `go list -m all`\n   - Build dependency tree if needed\n\n2. **Cross-Reference Vulnerable Packages**\n   - **Method 1: Dependency Matching**\n     - Compare CVE-affected packages with `go.mod` dependencies\n     - Check if affected package versions are in use\n     - Account for version ranges and semantic versioning\n   \n   - **Method 2: Go Vulnerability Scanner**\n     - Run `govulncheck` if available in the environment\n     - Command: `govulncheck ./...`\n     - Parse output for CVE matches\n     - Alternative: Use `go list -json -m all` and cross-reference\n   \n   - **Method 3: Direct Dependency Check**\n     - Use `go list` to verify package presence\n     - Command: `go list -mod=mod <vulnerable-package>`\n     - Example: `go list -mod=mod golang.org/x/net/html`\n     - Confirms package is included (directly or transitively)\n     - Note: This alone doesn't prove vulnerable functions are called\n   \n   - **Method 4: Call Graph Reachability Analysis** (Highest Confidence)\n     - Build complete program call graph using `callgraph` tool\n     - Search for vulnerable function signatures in the graph\n     - Commands:\n       ```bash\n       # Check if vulnerable function exists in call graph\n       callgraph -format=digraph . | digraph nodes | grep \"<vulnerable-function-signature>\"\n       \n       # Find execution path from main to vulnerable function\n       callgraph -format=digraph . | digraph somepath command-line-arguments.main <vulnerable-function> | digraph to dot\n       ```\n     - Example (for CVE-2024-45338 affecting `golang.org/x/net/html.Parse`):\n       ```bash\n       # Step 1: Check if Parse is called anywhere\n       callgraph -format=digraph . | digraph nodes | grep \"golang.org/x/net/html.Parse$\"\n       \n       # Step 2: Find path from main() to Parse()\n       callgraph -format=digraph . | digraph somepath command-line-arguments.main golang.org/x/net/html.Parse\n       ```\n     - **Interpretation**:\n       - If path exists: Code is DEFINITELY vulnerable (reachable code path)\n       - If no path: Function may be dead code or only called conditionally\n       - Generates DOT graph showing exact call chain\n     - **Visualization** (optional):\n       ```bash\n       callgraph -format=digraph . | digraph somepath <entrypoint> <vulnerable-func> | digraph to dot | sfdp -Tsvg -o callgraph.svg\n       ```\n     - Prerequisites: Install tools if missing\n       ```bash\n       go install golang.org/x/tools/cmd/callgraph@latest\n       go install golang.org/x/tools/cmd/digraph@latest\n       ```\n   \n   - **Method 5: Source Code Analysis**\n     - Search for import statements of vulnerable packages\n     - Use grep/codebase_search to find package usage\n     - Identify actual code paths that use vulnerable functions\n     - Check if vulnerable functions are called in reachable code\n\n3. **Verify Impact with Multiple Methods & Confidence Levels**\n   \n   Use multiple verification layers, with each providing increasing confidence:\n   \n   - **Level 1: Basic Presence (Low Confidence)**\n     - Check `go.mod` for vulnerable package\n     - Run: `go list -mod=mod <vulnerable-package>`\n     - Result: Confirms package is a dependency (direct or transitive)\n     -  Limitation: Doesn't prove the vulnerable code is actually used\n   \n   - **Level 2: Import & Version Analysis (Medium Confidence)**\n     - Verify package is imported in source code (grep/codebase_search)\n     - Check version is in vulnerable range\n     - Search for vulnerable function/method names in codebase\n     -  Limitation: Function may exist but not be in reachable code paths\n   \n   - **Level 3: Vulnerability Scanner (Medium-High Confidence)**\n     - Run `govulncheck ./...` (official Go vulnerability checker)\n     - Performs reachability analysis automatically\n     - Reports if vulnerable functions are actually called\n     -  Advantage: Maintained by Go team, knows about CVEs\n   \n   - **Level 4: Call Graph Reachability (Highest Confidence)**\n     - Use `callgraph` + `digraph` to prove execution path exists\n     - Trace from `main()` (or test entry points) to vulnerable function\n     - Generate visual call graph showing exact path\n     -  Advantage: Provides definitive proof with traceable evidence\n     - Example output shows complete call chain:\n       ```\n       main  MyHandler  ParseHTML  html.Parse (VULNERABLE)\n       ```\n   \n   - **Level 5: Configuration & Context Analysis**\n     - Review if vulnerable features are actually enabled\n     - Check if vulnerable code paths are behind feature flags\n     - Verify if inputs can reach vulnerable functions\n     - Consider security controls (input validation, sandboxing)\n   \n   **Recommended Approach**: Use multiple methods and assign confidence:\n   - **High Confidence (DEFINITELY AFFECTED)**: \n     - Call graph shows reachable path AND version is vulnerable\n     - OR govulncheck explicitly reports the CVE\n   - **Medium Confidence (LIKELY AFFECTED)**:\n     - Package present + vulnerable version + function calls found\n     - But no call graph or reachability proof\n   - **Low Confidence (POSSIBLY AFFECTED)**:\n     - Vulnerable package present but no direct usage evidence\n   - **Not Affected**:\n     - Package not present OR version not vulnerable OR dead code\n\n4. **Build Evidence Package**\n   \n   Collect comprehensive evidence for the report:\n   \n   - **Dependency Evidence**:\n     - `go.mod` entries showing vulnerable package\n     - `go list` output confirming presence\n     - Version information from `go list -m <package>`\n   \n   - **Static Code Evidence**:\n     - File paths where vulnerable packages are imported\n     - Line numbers where vulnerable functions are called\n     - Code snippets showing usage context\n   \n   - **Reachability Evidence** (if call graph analysis performed):\n     - Call graph output showing vulnerable function in nodes\n     - Execution path from entry points to vulnerable code\n     - DOT graph visualization (saved to `.work/compliance/analyze-cve/{CVE-ID}/callgraph.svg`)\n     - Complete call chain as text (e.g., \"main  handler  parse  VULN\")\n   \n   - **Scanner Evidence**:\n     - `govulncheck` output (full text)\n     - Vulnerability findings with line numbers\n   \n   - **Mitigation Factors**:\n     - Input validation or sanitization in place\n     - Vulnerable features disabled by configuration\n     - Code behind feature flags or conditional execution\n     - Security controls limiting exposure\n   \n   - **Confidence Assessment**:\n     - List which verification methods were used\n     - Assign overall confidence level based on evidence\n     - Note any gaps in analysis or areas needing manual review\n\n### Phase 3: Report Generation\n\n1. **Create Analysis Report**\n   - Location: `.work/compliance/analyze-cve/{CVE-ID}/report.md`\n   - Additional artifacts: \n     - `callgraph.svg` (if generated)\n     - `govulncheck-output.txt` (if run)\n     - `evidence.json` (structured evidence data)\n   \n   - Include sections:\n     - **Executive Summary**: \n       - Impact verdict (AFFECTED/NOT AFFECTED/UNKNOWN)\n       - Confidence level badge (HIGH/MEDIUM/LOW)\n       - Quick summary of findings\n     \n     - **CVE Details**: \n       - Full vulnerability information\n       - Tag information sources (e.g., \"Source: NVD\", \"Source: User-provided\")\n       - Affected package/function signatures\n       - Vulnerability type and attack vector\n     \n     - **Analysis Methodology**:\n       - List all verification methods used\n       - Note which tools were available (callgraph, govulncheck, etc.)\n       - Explain confidence level determination\n       - Example:\n         ```\n          Method 1: Dependency check (go list) - POSITIVE\n          Method 2: Version analysis - VULNERABLE VERSION FOUND\n          Method 3: govulncheck scan - CVE REPORTED\n          Method 4: Call graph analysis - REACHABLE PATH FOUND\n          Confidence: HIGH\n         ```\n     \n     - **Dependency Analysis**: \n       - Package versions from go.mod\n       - Direct vs. transitive dependencies\n       - Vulnerable package version range\n     \n     - **Impact Assessment**: \n       - Specific findings in codebase\n       - File paths and line numbers\n       - Code snippets showing vulnerable usage\n       - **Reachability Analysis** (if performed):\n         - Call chain from entry points to vulnerable functions\n         - Visual call graph (link to SVG)\n         - Interpretation of findings\n     \n     - **Risk Level**: \n       - Based on exploitability, exposure, and reachability\n       - Consider CVSS score + actual codebase context\n     \n     - **Evidence**: \n       - All collected evidence organized by type\n       - Terminal output from tools\n       - Screenshots or links to visualizations\n     \n     - **Confidence Assessment**:\n       - Final confidence: High/Medium/Low\n       - Justification based on methods used\n       - Gaps or limitations noted\n       - Recommendations for additional verification if needed\n     \n     - **Remediation Steps**: \n       - Specific fixes needed (version updates, code changes)\n       - Verification commands (prefer make targets, fallback to go commands)\n       - Note which make targets are available in the project\n       - Priority based on confidence level and risk\n     \n     - **References**: \n       - All sources consulted (automated + user-provided)\n       - Tool versions used\n       - Timestamp of analysis\n\n2. **Format Report**\n   - Use clear markdown formatting\n   - Include severity badges\n   - Add code blocks for examples\n   - Link to external references\n   - Provide actionable recommendations\n   - **Clearly mark user-provided information** with labels like:\n     - \" Based on user-provided information\"\n     - \" Verified from authoritative sources\"\n     - \" Partial information - manual verification recommended\"\n\n### Phase 4: Remediation Guidance\n\n1. **If Codebase is NOT Affected**\n   - Explain why (version not vulnerable, package not used, etc.)\n   - Suggest preventive measures\n   - Recommend ongoing monitoring\n\n2. **If Codebase IS Affected**\n   - Provide specific remediation steps:\n     1. **Update Dependencies**\n        - Exact `go get` commands to upgrade packages\n        - Target version that fixes the CVE\n        - Consider semantic versioning compatibility\n        - Note: Use `go mod tidy` after updates\n     \n     2. **Code Changes** (if needed)\n        - Identify functions that need modification\n        - Provide before/after code examples\n        - Explain breaking changes if any\n     \n     3. **Workarounds** (if no fix available)\n        - Suggest temporary mitigations\n        - Configuration changes to reduce risk\n        - Input validation or sanitization\n     \n     4. **Verification Commands**\n        - Check for project's Makefile first\n        - Prefer project-specific make targets: `make verify`, `make build`, `make test`\n        - Fall back to standard Go commands if no Makefile\n        - Command to check for make targets: `make -qp | grep \"^[a-zA-Z]\" | head -20`\n     \n     5. **Testing Recommendations**\n        - Suggest tests to verify the fix\n        - Security test cases to add\n        - Regression testing guidance\n        - Re-run `govulncheck` to confirm vulnerability is resolved\n\n### Phase 5: Interactive Fix Application\n\n1. **Present Remediation Plan**\n   - Show complete analysis report\n   - Highlight critical findings\n   - List all proposed fixes\n\n2. **Ask User for Permission**\n   - \"Would you like me to apply these fixes automatically?\"\n   - Wait for explicit user confirmation\n   - Do NOT proceed without approval\n\n3. **If User Approves, Apply Fixes**\n   - **Update go.mod and go.sum**\n     - Run `go get -u <package>@<fixed-version>`\n     - Run `go mod tidy` to clean up\n   \n   - **Modify Source Code** (if required)\n     - Apply code changes identified in Phase 4\n     - Use search_replace or write tools\n     - Maintain code style and formatting\n   \n   - **Verify Changes**\n     - Check if project has Makefile with common targets\n     - **For verification**:\n       - Try `make verify` first (if target exists)\n       - Fallback: `go mod verify`\n     - **For building**:\n       - Try `make build` first (if target exists)\n       - Fallback: `go build ./...`\n     - **For testing**:\n       - Try `make test` first (if target exists)\n       - Fallback: `go test ./...`\n     - **Re-check vulnerability**:\n       - Run `govulncheck ./...` to confirm fix\n\n4. **Document Changes**\n   - Create summary of changes made\n   - List files modified\n   - Provide git diff summary\n   - Suggest commit message\n\n## Return Value\n\n- **Format**: Markdown report at `.work/compliance/analyze-cve/{CVE-ID}/report.md`\n- **Content**:\n  - Vulnerability details and severity\n  - Impact assessment (AFFECTED/NOT AFFECTED/UNCLEAR)\n  - Evidence from codebase analysis\n  - Specific remediation recommendations\n  - Applied fixes (if user approved)\n\n## Examples\n\n1. **CVE analysis**:\n   ```\n   /compliance:analyze-cve CVE-2024-45338\n   ```\n   Analyzes the codebase for CVE-2024-45338\n\n## Arguments\n\n- `<CVE-ID>`: The CVE identifier to analyze (e.g., CVE-2024-1234, CVE-2023-45678)\n  - Format: CVE-YYYY-NNNNN\n  - Case insensitive\n  - Required argument\n\n## Notes\n\n- The command focuses on Go-specific vulnerabilities\n- **Flexible Information Sources**:\n  - Prefers automatic CVE lookup from authoritative sources (NVD, MITRE, Go vulndb)\n  - Falls back to user-provided information if internet access fails or CVE data is unavailable\n  - Clearly distinguishes between verified and user-provided information in reports\n- Analysis may take several minutes for complex codebases\n- If `govulncheck` is not installed, the command will use alternative methods\n- The command does NOT make changes without explicit user approval\n- Generated reports are saved locally and not committed to git\n- **When providing manual CVE information**, include as much detail as possible:\n  - Affected Go package/module names\n  - Vulnerable and fixed version numbers\n  - Severity and CVSS score (if known)\n  - Links to security advisories or patches\n\n## Prerequisites\n\n- **Required**:\n  - Go toolchain installed (`go version` should work)\n  - Read access to `go.mod` and source files in the workspace\n\n- **Recommended** (for comprehensive analysis):\n  - Internet connectivity for automatic CVE data fetching\n  - `govulncheck` - Go's official vulnerability checker\n    ```bash\n    go install golang.org/x/vuln/cmd/govulncheck@latest\n    ```\n  - `callgraph` & `digraph` - For reachability analysis (highest confidence)\n    ```bash\n    go install golang.org/x/tools/cmd/callgraph@latest\n    go install golang.org/x/tools/cmd/digraph@latest\n    ```\n  - `sfdp` or `graphviz` - For call graph visualization (optional)\n    ```bash\n    # macOS\n    brew install graphviz\n    # Linux\n    sudo apt-get install graphviz\n    ```\n\n- **Alternative**: If internet access is unavailable, be prepared to provide:\n  - CVE description and details\n  - Affected package information\n  - Specific vulnerable function signatures\n  - Remediation guidance from other sources\n\n**Tool Availability Check**: The command will automatically detect which tools are available and use the most comprehensive methods possible. Missing tools will result in lower confidence levels but analysis will still proceed.\n\n## Exit Conditions\n\n- **Success**: Report generated with clear impact assessment based on complete information\n- **Success with User Input**: Report generated based on user-provided CVE details when internet access fails\n- **Partial Success**: Report generated but impact is unclear (needs manual review)\n  - Marked with confidence level: \"Low\" or \"Medium\"\n  - Includes recommendations for further investigation\n- **Failure Scenarios**:\n  - Invalid CVE format (must be CVE-YYYY-NNNNN)\n  - User declines to provide information when automatic lookup fails\n  - No Go-related information available (CVE is for different technology)\n  - Insufficient information to proceed with analysis"
              }
            ],
            "skills": []
          },
          {
            "name": "test-coverage",
            "description": "Analyze test coverage and identify gaps in test scenarios",
            "source": "./plugins/test-coverage",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install test-coverage@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze",
                "description": "Analyze test code structure without running tests to identify coverage gaps",
                "path": "plugins/test-coverage/commands/analyze.md",
                "frontmatter": {
                  "description": "Analyze test code structure without running tests to identify coverage gaps",
                  "argument-hint": "<path-or-url> [--output <path>] [--priority <level>] [--test-structure-only]"
                },
                "content": "## Name\ntest-coverage:analyze\n\n## Synopsis\n```\n/test-coverage:analyze <path-or-url> [--output <path>] [--priority <level>] [--test-structure-only]\n```\n\n## Description\n\nThe `test-coverage:analyze` command analyzes test code structure **without running tests**. This command examines test files and source files to identify:\n- What e2e/integration tests exist in the codebase\n- What source code has corresponding tests\n- What source code lacks tests\n- Test organization and coverage gaps\n\n**Focus on E2E Tests:** By default, this command focuses on e2e (end-to-end) and integration tests, excluding unit tests. This ensures analysis targets higher-level test coverage gaps that validate real-world scenarios and system integration.\n\n**Language Support:** This command currently supports Go projects only.\n\nThis command is the foundation for QE or Dev teams to understand their e2e test coverage baseline and identify areas requiring additional testing.\n\n## Arguments\n\n- `<source-directory>`: Path or URL to source code directory/file to analyze\n  - **Local path**: `./pkg/`, `/home/user/project/test/e2e/networking/infw.go`\n  - **GitHub URL**: `https://github.com/owner/repo/blob/main/test/file_test.go`\n  - **GitHub raw URL**: `https://raw.githubusercontent.com/owner/repo/main/test/file_test.go`\n  - **GitLab URL**: `https://gitlab.com/owner/repo/-/blob/main/test/file_test.go`\n  - **HTTP(S) URL**: Any direct file URL\n  - URLs are automatically downloaded and cached in `.work/test-coverage/cache/`\n\n- `--output <path>`: Output directory for generated reports (default: `.work/test-coverage/analyze/`)\n  - Generates HTML report, JSON summary, and text summary\n\n- `--priority <level>`: Filter results by priority (optional)\n  - Values: `all`, `high`, `medium`, `low`\n  - Default: `all`\n\n- `--include-test-utils`: Include test utility/helper files in analysis (optional)\n  - By default, utility files are excluded (*_util.go, *_utils.go, *_helper.go, helpers.go, etc.)\n  - Use this flag to analyze test utility functions for e2e test coverage\n  - Useful for auditing test infrastructure code\n\n- `--include-unit-tests`: Include unit tests in analysis (optional)\n  - By default, only e2e/integration tests are analyzed\n  - Use this flag to include unit tests in the coverage analysis\n  - E2E tests are identified by:\n    - File naming patterns (e.g., *e2e*_test.go, *integration*_test.go)\n    - Directory location (e.g., test/e2e/, test/integration/)\n    - Test markers (e.g., [Serial], [Disruptive] for Ginkgo)\n\n- `--test-pattern <pattern>`: Custom test file pattern (optional)\n  - Example: `--test-pattern \"**/*_test.go,**/test_*.go\"`\n\n- `--test-structure-only`: Analyze only test file structure, skip source file analysis (optional)\n  - When enabled, analyzes ONLY test files to document what tests exist and what they cover\n  - Does NOT look for corresponding source files or identify coverage gaps\n  - Useful for:\n    - Understanding test organization and structure\n    - Documenting test cases in existing test files\n    - Quick analysis of a single test file or test directory\n    - Generating test documentation (test cases, helper functions, resource types)\n  - Generates reports focused on test structure:\n    - Test cases and their purpose\n    - Helper functions used in tests\n    - Resource types and test utilities\n    - Protocol/feature coverage matrices\n  - Much faster than full coverage analysis\n  - Example: `/test-coverage:analyze ./test/extended/networking/infw.go --test-structure-only`\n\n## Implementation\n\nThe command uses test structure analysis (backend) to analyze test files and source files without running tests.\n\n**Two Analysis Modes:**\n\n1. **Full Coverage Analysis (default):** Analyzes both test files AND source files to identify coverage gaps\n   - Maps tests to source code\n   - Identifies untested files and functions\n   - Generates coverage gap reports\n   - Follows Steps 1-8 below\n\n2. **Test Structure Only (--test-structure-only):** Analyzes ONLY test files to document their structure\n   - Extracts test cases, helper functions, resource types\n   - Documents what each test covers\n   - Does NOT analyze source files or identify gaps\n   - Much faster, useful for test documentation\n   - Follows Steps 1, 2 (test files only), 3, 7 (test-focused reports)\n\n### Step 1: Resolve Input\n\n1. **Resolve input path or URL**:\n   - If input is a URL:\n     - Convert GitHub/GitLab blob URLs to raw URLs\n     - Download file to `.work/test-coverage/cache/`\n     - Use cached version if already downloaded\n     - Extract to temporary directory if it's an archive (zip, tar.gz)\n   - If input is a local path:\n     - Convert to absolute path\n     - Validate that path exists and is readable\n2. Load Go-specific configuration (test patterns, source patterns, parsers)\n\n### Step 2: Discover Test and Source Files\n\n**Note:** If `--test-structure-only` is used, only test files are discovered; source file discovery is skipped.\n\n1. Walk directory tree, excluding common directories:\n   - `vendor/`, `node_modules/`, `__pycache__/`, `.git/`, etc.\n   - Unit test directories (e.g., `test/unit/`, `unit/`) unless `--include-unit-tests` is specified\n2. Identify e2e/integration test files based on:\n   - **File naming patterns:**\n     - `*e2e*_test.go`, `*integration*_test.go`\n   - **Directory location:**\n     - `test/e2e/`, `test/integration/`, `e2e/`, `integration/`\n   - **Content markers (if file can be read):**\n     - Ginkgo markers: `[Serial]`, `[Disruptive]`, `g.Describe(`, `g.It(`\n3. Identify source files based on language patterns\n4. Filter out test utility/helper files unless `--include-test-utils` is specified:\n   - `*_util.go`, `*_utils.go`, `*_helper.go`, `helpers.go`\n5. Apply `--exclude` patterns if specified\n\n### Step 3: Parse Test Files\n\nFor each test file:\n1. Extract test functions using Go patterns:\n   - Functions matching `func Test*`, `func Benchmark*`\n2. Extract imports to identify tested modules\n3. Extract function calls within tests (potential test targets)\n4. Infer corresponding source file:\n   - `handler_test.go`  `handler.go`\n\n### Step 4: Parse Source Files\n\n**Note:** This step is skipped when `--test-structure-only` is used.\n\nFor each source file:\n1. Extract functions/methods using Go patterns\n2. Determine function visibility:\n   - Exported (capitalized) vs private (lowercase)\n3. Calculate cyclomatic complexity (count decision points)\n4. Record line ranges for each function\n\n### Step 5: Map Tests to Source Code\n\n**Note:** This step is skipped when `--test-structure-only` is used.\n\n1. Create mapping between test files and source files\n2. For each function in source files:\n   - Find tests that reference this function\n   - Mark as tested/untested\n   - Count number of tests covering it\n3. Calculate file-level statistics:\n   - Total functions\n   - Tested functions\n   - Untested functions\n   - Function coverage percentage\n\n### Step 6: Identify Coverage Gaps\n\n**Note:** This step is skipped when `--test-structure-only` is used.\n\n1. **Untested Files**: Source files with no corresponding test file\n   - Priority: High if file has exported/public functions\n2. **Untested Functions**: Functions not referenced in any tests\n   - Priority: High for exported/public, Low for private\n3. **Partially Tested Files**: Files with some but not all functions tested\n   - Priority: Based on percentage of untested functions\n4. Apply `--priority` filter if specified\n\n### Step 7: Generate Reports\n\n**In test-structure-only mode**, generates test-focused reports:\n- `test-structure-report.html` - Interactive report with test cases, helper functions, resource types\n- `test-structure-analysis.json` - Machine-readable test metadata\n- `test-structure-summary.txt` - Terminal-friendly test summary\n\n**In full coverage mode**, generates coverage gap reports:\n\n1. **HTML Report** (`test-coverage-report.html`):\n   - Interactive web-based report combining structure summary and gaps\n   - Includes collapsible sections for test cases, helper functions, and gaps\n   - Color-coded priority indicators (High=red, Medium=yellow, Low=blue)\n   - Coverage matrices for protocols and IP stacks\n   - Filterable gaps by priority\n   - Recommendations section\n   - Best viewed in a web browser\n\n2. **JSON Report** (`test-structure-report.json`):\n   - Complete gap data with file paths, functions, priorities\n   - Machine-readable format for automation and CI/CD integration\n\n3. **Text Summary** (`test-structure-summary.txt`):\n   - Overall statistics (files with/without tests, function coverage)\n   - High-priority gaps\n   - Recommendations\n   - Plain text format for console viewing\n\n4. **Console Output**:\n   - Summary of findings\n   - Paths to all generated reports\n\n### Step 8: Invoke Test Structure Analyzer\n\n**Invoke the analyze skill** to generate analyzer script at runtime and execute analysis. The skill will:\n- Generate the analyzer from the specification in SKILL.md\n- Execute analysis on the source directory\n- Generate all three report formats (HTML, JSON, Text)\n\n## Return Value\n\n- **Format**: Terminal output with summary + generated report files\n\n**Terminal Output:**\n```\nTest Structure Analysis Complete\n\nSummary:\n  Total Source Files:    45\n  Files With Tests:      30 (66.7%)\n  Files Without Tests:   15 (33.3%)\n\n  Total Functions:       234\n  Tested Functions:      189 (80.8%)\n  Untested Functions:    45 (19.2%)\n\nHigh Priority Gaps:\n  1. pkg/config.go - No test file (3 exported functions)\n  2. pkg/validator.go - No test file (5 exported functions)\n  3. cmd/server/auth.go - Partially tested (4/8 functions)\n\nReports Generated:\n  HTML Report:    .work/test-coverage/analyze/test-coverage-report.html\n  JSON Report:    .work/test-coverage/analyze/test-structure-report.json\n  Text Summary:   .work/test-coverage/analyze/test-structure-summary.txt\n\nRecommendations:\n  - Create test files for 15 untested source files\n  - Add tests for 45 untested functions\n  - Focus on high-priority gaps first\n```\n\n**Exit Status:**\n- 0: Analysis successful\n- 2: Analysis failed (parsing error, invalid input)\n\n## Examples\n\n### Example 1: Analyze e2e test structure without running tests (Go project)\n\n```\n/test-coverage:analyze ./pkg/\n```\n\nAnalyzes e2e/integration test file structure for a Go project to identify untested functions and files without running any tests. Unit tests are excluded by default.\n\n### Example 2: Analyze test structure with high priority filter\n\n```\n/test-coverage:analyze ./pkg/ --priority high\n```\n\nAnalyzes Go test structure and shows only high-priority gaps (files without tests, untested public functions).\n\n### Example 3: Analyze with custom output directory\n\n```\n/test-coverage:analyze ./pkg/ --output reports/test-gaps/\n```\n\nAnalyzes test structure and generates reports in custom output directory.\n\n### Example 4: Analyze only test file structure (single file)\n\n```\n/test-coverage:analyze ./test/extended/networking/infw.go --test-structure-only\n```\n\nAnalyzes ONLY the test file structure without looking for source files. Generates documentation showing:\n- All test cases and what they cover\n- Helper functions used in tests\n- Resource types and test utilities\n- Coverage matrices (protocols, IP stacks, platforms)\n\nUseful for quickly understanding what a test file covers without analyzing source code.\n\n### Example 5: Analyze test directory structure only\n\n```\n/test-coverage:analyze ./test/e2e/ --test-structure-only\n```\n\nAnalyzes all test files in the e2e directory to document the test suite structure without source file analysis.\n\n### Example 6: Analyze remote test file from GitHub\n\n```\n/test-coverage:analyze https://github.com/openshift/origin/blob/master/test/extended/networking/infw.go --test-structure-only\n```\n\nDownloads and analyzes a test file directly from GitHub. The command automatically converts the GitHub blob URL to a raw URL and caches the download.\n\n### Example 7: Analyze remote test file using raw URL\n\n```\n/test-coverage:analyze https://raw.githubusercontent.com/openshift/origin/master/test/extended/networking/infw.go --test-structure-only\n```\n\nAnalyzes a test file using the raw GitHub URL directly.\n\n### Example 8: Analyze test file with forced re-download\n\nTo force re-downloading a cached URL, simply delete the cache and run again:\n\n```\nrm -rf .work/test-coverage/cache/\n/test-coverage:analyze https://github.com/user/repo/blob/main/test/file_test.go --test-structure-only\n```\n\n## Prerequisites\n\n### Python Dependencies\n\nThe command uses Python for parsing and report generation. No external packages are required - only standard library modules are used.\n\n### Checking Prerequisites\n\nThe command will automatically check for Python 3.8+ and provide installation instructions if missing.\n\n## Notes\n\n- **URL Support:** The command accepts both local paths and URLs (GitHub, GitLab, or any HTTP(S) URL)\n  - Remote files are automatically detected, downloaded, and cached\n  - Downloaded files are cached in `.work/test-coverage/cache/` for reuse\n  - GitHub blob URLs are automatically converted to raw URLs\n  - Clear cache with `rm -rf .work/test-coverage/cache/` to force re-download\n- **E2E Focus:** By default, this command focuses on e2e/integration tests. Use `--include-unit-tests` to include unit tests.\n- **Two Modes:** Use `--test-structure-only` to analyze only test files (fast, for documentation), or omit it for full coverage gap analysis\n- This command analyzes test structure without running tests, making it very fast\n- Works even if tests are broken or code doesn't compile\n- Useful for identifying e2e test coverage gaps during development and code review\n- HTML report provides interactive visualization of e2e test coverage and gaps\n- JSON output enables integration with CI/CD pipelines and dashboards\n- Text summary is ideal for console viewing and quick reference\n- **Test Structure Only mode** is perfect for:\n  - Documenting existing test suites\n  - Understanding what a test file covers\n  - Quick analysis of a single test file\n  - Generating test case reports for review\n\n## See Also\n\n- `/test-coverage:gaps` - Identify untested code paths with priority-based analysis"
              },
              {
                "name": "/gaps",
                "description": "Identify E2E test scenario gaps in OpenShift/Kubernetes tests (component-agnostic)",
                "path": "plugins/test-coverage/commands/gaps.md",
                "frontmatter": {
                  "description": "Identify E2E test scenario gaps in OpenShift/Kubernetes tests (component-agnostic)",
                  "argument-hint": "<test-file-or-url> [--output <path>]"
                },
                "content": "## Name\ntest-coverage:gaps\n\n## Synopsis\n```bash\n/test-coverage:gaps <test-file-or-url> [--output <path>]\n```\n\n## Description\n\nThe `test-coverage:gaps` command **intelligently analyzes OpenShift/Kubernetes test files to identify missing test coverage**. It is **component-agnostic** and works for any OpenShift/K8s component (networking, storage, ETCD, Kube API, operators, etc.). **This command always generates three report formats: HTML (interactive), JSON (machine-readable), and Text (terminal-friendly).**\n\n**Component-Agnostic Analysis** (works for all OpenShift/K8s components):\n- **Platform Coverage**: Which platforms (AWS, Azure, GCP, vSphere, Bare Metal, etc.) lack tests\n- **Scenario Coverage**: Missing error handling, upgrade, security/RBAC, scale, performance tests\n- **Priority-based recommendations**: Focus on high-impact gaps first\n- **Component detection**: Automatically detects component type (networking, storage, kube-api, etcd, etc.) for informational purposes\n\n**Supported Components:**\n- Networking (ingress, egress, SDN, OVN, network policies)\n- Storage (volumes, storage classes, CSI, PV/PVC)\n- Kube API, ETCD\n- Auth/RBAC, OAuth\n- Operators, controllers\n- Observability, monitoring\n- Image registry, builds\n- Any other OpenShift/K8s component\n\n**Language Support:** This command currently supports Go projects only.\n\nThis command helps OpenShift/Kubernetes QE or Dev teams focus testing efforts on the most critical untested scenarios.\n\n## Arguments\n\n- `$1` (test-file-or-url): Path or URL to OpenShift/Kubernetes test file\n  - **Local path**: `./test/extended/networking/ingress.go`, `/path/to/storage_test.go`\n  - **GitHub URL**: `https://github.com/openshift/origin/blob/master/test/extended/storage/volume.go`\n  - **URL**: Any HTTP(S) URL to a test file\n  - URLs are automatically downloaded and cached in `.work/test-coverage/cache/`\n\n### Optional Arguments\n\n- `--output <path>`: Output directory for gap analysis reports (default: `.work/test-coverage/gaps/`)\n\n## Implementation\n\n### Step 1: Resolve Test File Input\n\n1. **Resolve test file path or URL**:\n   - If input is a URL:\n     - Download test file to `.work/test-coverage/cache/`\n     - Use cached version if already downloaded\n   - If input is a local path:\n     - Convert to absolute path and validate existence\n     - Verify file is a Go test file (contains `g.It`, `g.Describe`, or `Test` functions)\n\n### Step 2: Detect Component Type and Parse Test File\n\n1. **Detect component type** from file path and content:\n   - **Networking**: `/networking/`, network policy, ingress, egress patterns\n   - **Storage**: `/storage/`, volume, PV, PVC, storage class patterns\n   - **KAPI**: `/kapi/`, `/api/`, apiserver patterns\n   - **Auth**: `/auth/`, RBAC, OAuth patterns\n   - **Generic**: Fallback for unrecognized components\n\n2. **Extract test cases** using regex patterns:\n   - Ginkgo tests: `g.It(`, `g.Describe(`, `g.Context(`\n   - Standard Go tests: `func Test*`\n   - Extract test metadata from names (priority, bug IDs, tags)\n\n3. **Analyze component-specific coverage**:\n   - **For Networking**: Protocols, service types, IP stacks\n   - **For Storage**: Storage classes, volume modes, provisioners\n   - **For All Components**: Platforms, scenarios\n\n4. **Build coverage matrices**:\n   - Track component-specific dimensions\n   - Track platform coverage\n   - Track scenario coverage (error handling, upgrades, RBAC, scale)\n\n### Step 3: Identify Component-Aware Gaps\n\n1. **Compare tested vs. expected**:\n   - For each component-specific dimension, identify what's not tested\n   - Categorize gaps by priority based on production importance\n\n2. **Calculate priority scores** (component-specific):\n   - **High Priority**:\n     - Major cloud providers (AWS, Azure, GCP)\n     - Core component features (protocols for networking, storage classes for storage)\n     - Error handling scenarios\n     - Operator upgrades\n   - **Medium Priority**:\n     - Secondary platforms (Bare Metal, OpenStack)\n     - RBAC, scale, performance scenarios\n   - **Low Priority**:\n     - Edge case scenarios\n\n3. **Generate component-aware recommendations**:\n   - For each gap, provide specific test recommendation\n   - Estimate impact of gap\n   - Suggest test case to fill gap\n\n### Step 4: Generate Reports\n\n**Invoke the gaps skill** to generate analyzer script at runtime and produce all three report formats:\n\n1. **HTML Report** (`test-gaps-report.html`):\n   - Coverage scores dashboard\n   - What's tested vs. not tested matrices\n   - Priority-sorted gap list with recommendations\n   - Visual charts for protocol/platform coverage\n\n2. **JSON Report** (`test-gaps-report.json`):\n   - Test case metadata\n   - Coverage matrices\n   - Gap list with priorities\n   - Machine-readable for CI/CD\n\n3. **Text Summary** (`test-gaps-summary.txt`):\n   - Coverage percentages\n   - High priority gaps\n   - Recommendations\n   - Terminal-friendly format\n\n## Return Value\n\n- **Format**: Terminal output with summary + generated report files\n\n**Terminal Output (Networking Component Example):**\n```text\nDetected component: networking\n\nTest Coverage Gap Analysis Complete\n\nSummary:\n  Test Cases:        15\n  Overall Coverage:  20.8%\n\nCoverage Scores:\n  Protocol Coverage:     0.0%\n  Platform Coverage:     83.3%\n  Service Type:          0.0%\n  Scenario Coverage:     0.0%\n\nHigh Priority Gaps (5):\n  1. TCP - Most common protocol not tested\n  2. UDP - Common protocol for DNS, streaming not tested\n  3. LoadBalancer - External traffic not tested\n  4. Error handling - Invalid configs not validated\n  5. Operator upgrades - Upgrade path not tested\n\nReports Generated:\n   HTML:  .work/test-coverage/gaps/test-gaps-report.html\n   JSON:  .work/test-coverage/gaps/test-gaps-report.json\n   Text:  .work/test-coverage/gaps/test-gaps-summary.txt\n\nRecommendation:\n  Add 5-7 test cases to address high-priority gaps\n  Target: Improve coverage from 21% to 41%\n```\n\n**Terminal Output (Storage Component Example):**\n```text\nDetected component: storage\n\nTest Coverage Gap Analysis Complete\n\nSummary:\n  Test Cases:        12\n  Overall Coverage:  35.0%\n\nCoverage Scores:\n  Storage Class Coverage:     33.3%\n  Volume Mode Coverage:       66.7%\n  Platform Coverage:          50.0%\n  Scenario Coverage:          20.0%\n\nHigh Priority Gaps (4):\n  1. gp2/gp3 - AWS EBS storage not tested\n  2. CSI - CSI drivers not tested\n  3. ReadWriteOnce - Single-node write access not tested\n  4. Error handling - Invalid configs not validated\n```\n\n**Exit Status:**\n- 0: Analysis successful\n- 1: Analysis failed (parsing error, missing file)\n\n## Examples\n\n### Example 1: Analyze networking test file\n\n```bash\n/test-coverage:gaps ./test/extended/networking/egressip_udn.go\n```\n\nDetects networking component and analyzes protocol coverage (TCP, UDP, SCTP), service types, and scenarios.\n\n**Output:** Component: networking, 20.8% overall coverage, identifies gaps in TCP/UDP protocols, LoadBalancer service type, error handling, operator upgrades.\n\n### Example 2: Analyze storage test file\n\n```bash\n/test-coverage:gaps ./test/e2e/storage/csi.go\n```\n\nDetects storage component and analyzes storage class coverage, volume modes, and provisioners.\n\n**Output:** Component: storage, identifies gaps in gp2/gp3 storage classes, ReadWriteMany volume mode, CSI drivers.\n\n### Example 3: Analyze remote test file\n\n```bash\n/test-coverage:gaps https://github.com/openshift/origin/blob/master/test/extended/storage/volume.go\n```\n\nDownloads test file from GitHub and analyzes component-specific coverage gaps.\n\n### Example 5: Custom output directory\n\n```bash\n/test-coverage:gaps ./test/e2e/auth/rbac.go --output ./reports/e2e-gaps/\n```\n\nGenerates component-aware gap reports in custom directory.\n\n## Prerequisites\n\n**General Requirements**:\n- Python 3.8+\n- Access to test files\n- Go test files (Ginkgo or standard Go tests)\n\n### Checking Prerequisites\n\nThe command will check for required tools and suggest installation if missing.\n\n## Notes\n\n### General\n\n- **Test Scenario Analysis**: This command identifies missing test scenarios, platforms, and protocols in your e2e test suite\n- **CRITICAL**: This command MUST always generate all three report formats (HTML, JSON, and Text). Failing to generate any report format should be treated as a command failure.\n- **URL Support:** Test files can be URLs\n  - Supports GitHub, GitLab, and any HTTP(S) URLs\n  - Downloaded files are cached in `.work/test-coverage/cache/`\n  - GitHub blob URLs are automatically converted to raw URLs\n  - Clear cache with `rm -rf .work/test-coverage/cache/` to force re-download\n\n### Component-Aware Gap Analysis Notes\n\n- **Context-Aware Analysis**: The tool automatically detects component type and provides relevant recommendations\n- **Component Types Supported**:\n  - **Networking**: Analyzes protocols, service types, IP stacks\n  - **Storage**: Analyzes storage classes, volume modes, provisioners\n  - **Generic**: Analyzes platforms and scenarios for unrecognized components\n- **Focus on Production Readiness**: Gaps highlight missing scenarios that could impact production deployments\n- **Platform Coverage Critical**: Missing tests for major cloud providers (AWS, Azure, GCP) are production blockers\n- **Component-Specific Coverage**: Each component type has specific dimensions analyzed (protocols for networking, storage classes for storage, etc.)\n- **Scenario Coverage**: Error handling, upgrades, RBAC, and scale tests are often overlooked but critical\n- **Coverage Scores**: Overall coverage below 50% indicates significant e2e testing gaps\n- Re-run after adding test cases to track improvement in component-specific coverage\n\n### Report Format Notes\n\n- The HTML report provides the best interactive experience with expandable details, sortable tables, and visual charts\n- The JSON report is ideal for CI/CD integration and automated issue creation\n- The Text report is useful for email summaries and terminal display\n- JSON output can be integrated with issue tracking systems to create testing tasks\n- Re-run this command after adding tests to measure progress\n\n## See Also\n\n- `/test-coverage:analyze` - Analyze test structure and organization"
              }
            ],
            "skills": [
              {
                "name": "Test Structure Analysis",
                "description": "Analyze test code structure directly to provide coverage analysis",
                "path": "plugins/test-coverage/skills/analyze/SKILL.md",
                "frontmatter": {
                  "name": "Test Structure Analysis",
                  "description": "Analyze test code structure directly to provide coverage analysis"
                },
                "content": "# Test Structure Analysis Skill\n\nThis skill provides the ability to analyze test code structure **directly from test files** without running tests. It examines test files and source files to identify what is tested and what is not.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Analyze test code organization without running tests\n- Identify files and functions without tests\n- Understand what e2e/integration tests cover\n- Find coverage gaps by examining test structure\n- Generate comprehensive test structure reports\n- Fast analysis (seconds, not minutes)\n\n## Prerequisites\n\n### Required Tools\n\n- **Python 3.8+** for test structure analysis\n- **Go toolchain** for the target Go project\n\n### Installation\n\n```bash\n# Ensure Python 3.8+ is installed\npython3 --version\n\n# Go toolchain for target project\ngo version\n```\n\n## How It Works\n\n**Note: This skill currently supports Go projects only.**\n\n### Step 1: Discover Test and Source Files\n\nThe analyzer discovers test and source files based on Go conventions:\n\n**Test Files:**\n- Files ending with `_test.go`\n- E2E/integration tests identified by:\n  - File naming patterns: `*e2e*_test.go`, `*integration*_test.go`\n  - Directory location: `test/e2e/`, `test/integration/`, `e2e/`, `integration/`\n  - Content markers: Ginkgo markers like `[Serial]`, `[Disruptive]`, `g.Describe(`, `g.It(`\n\n**Source Files:**\n- Files ending with `.go` (excluding test files)\n- Optionally exclude vendor, generated code, etc.\n\n### Step 2: Parse Test Files\n\nFor each test file, extract:\n\n1. **Test functions/methods**:\n   - Function name\n   - Line number range\n   - Test framework (Go testing, Ginkgo)\n   - Test type (unit, integration, e2e)\n\n2. **Test targets** (what the test is testing):\n   - Imports and references to source files\n   - Function calls and instantiations\n   - Inferred from test names\n\n3. **Test metadata**:\n   - Test descriptions/documentation\n   - Test tags/markers\n   - Helper functions\n\n**Example for Go:**\n```go\n// File: pkg/handler_test.go\npackage handler_test\n\nimport (\n    \"testing\"\n    \"myapp/pkg/handler\"\n)\n\nfunc TestHandleRequest(t *testing.T) {  //  Test function\n    h := handler.New()                   //  Target: handler.New\n    result := h.HandleRequest(\"test\")    //  Target: handler.HandleRequest\n    // ...\n}\n```\n\n**Extraction result:**\n```json\n{\n  \"test_file\": \"pkg/handler_test.go\",\n  \"source_file\": \"pkg/handler.go\",\n  \"tests\": [\n    {\n      \"name\": \"TestHandleRequest\",\n      \"lines\": [6, 10],\n      \"targets\": [\"handler.New\", \"handler.HandleRequest\"],\n      \"type\": \"unit\"\n    }\n  ]\n}\n```\n\n### Step 3: Parse Source Files\n\nFor each source file, extract:\n\n1. **Functions/methods**:\n   - Function name\n   - Line number range\n   - Visibility (public/private/exported)\n   - Parameters and return types\n   - Complexity metrics\n\n2. **Classes/structs**:\n   - Type definitions\n   - Methods\n   - Fields\n\n**Example for Go:**\n```go\n// File: pkg/handler.go\npackage handler\n\ntype Handler struct {\n    config Config\n}\n\nfunc New() *Handler {              //  Function: New\n    return &Handler{}\n}\n\nfunc (h *Handler) HandleRequest(req string) (string, error) {  //  Function: HandleRequest\n    if req == \"\" {\n        return \"\", errors.New(\"empty request\")\n    }\n    return process(req), nil\n}\n```\n\n**Extraction result:**\n```json\n{\n  \"source_file\": \"pkg/handler.go\",\n  \"functions\": [\n    {\n      \"name\": \"New\",\n      \"lines\": [8, 10],\n      \"visibility\": \"exported\",\n      \"complexity\": 1\n    },\n    {\n      \"name\": \"HandleRequest\",\n      \"lines\": [12, 20],\n      \"visibility\": \"exported\",\n      \"complexity\": 3,\n      \"receiver\": \"Handler\"\n    }\n  ]\n}\n```\n\n### Step 4: Map Tests to Source Code\n\nCreate a mapping between tests and source code:\n\n1. **Direct mapping** (test file  source file):\n   - `handler_test.go`  `handler.go`\n\n2. **Function-level mapping** (test  function):\n   - `TestHandleRequest` tests `HandleRequest`\n\n3. **Import-based mapping**:\n   - Analyze imports in test files to identify tested modules\n\n**Mapping result:**\n```json\n{\n  \"pkg/handler.go\": {\n    \"test_file\": \"pkg/handler_test.go\",\n    \"functions\": {\n      \"New\": {\n        \"tested\": true,\n        \"tests\": [\"TestHandleRequest\"],\n        \"test_count\": 1\n      },\n      \"HandleRequest\": {\n        \"tested\": true,\n        \"tests\": [\"TestHandleRequest\"],\n        \"test_count\": 1\n      }\n    },\n    \"overall_tested_functions\": 2,\n    \"overall_untested_functions\": 0,\n    \"function_test_coverage\": 100.0\n  }\n}\n```\n\n### Step 5: Identify Coverage Gaps\n\nIdentify what is **not tested**:\n\n1. **Untested source files**:\n   - Source files with no corresponding test file\n   - Priority: Based on file importance (exported functions)\n\n2. **Untested functions**:\n   - Functions not referenced in any tests\n   - Priority: Exported/public functions > private functions\n\n3. **Partially tested files**:\n   - Files with test file but missing tests for some functions\n\n**Gap categorization:**\n\n```json\n{\n  \"gaps\": {\n    \"untested_files\": [\n      {\n        \"file\": \"pkg/config.go\",\n        \"functions\": 5,\n        \"exported_functions\": 3,\n        \"priority\": \"high\",\n        \"reason\": \"No corresponding test file found\"\n      }\n    ],\n    \"untested_functions\": [\n      {\n        \"file\": \"pkg/handler.go\",\n        \"function\": \"process\",\n        \"visibility\": \"private\",\n        \"priority\": \"low\",\n        \"reason\": \"Not referenced in any tests\"\n      }\n    ]\n  },\n  \"summary\": {\n    \"total_source_files\": 45,\n    \"files_with_tests\": 30,\n    \"files_without_tests\": 15,\n    \"total_functions\": 234,\n    \"tested_functions\": 189,\n    \"untested_functions\": 45,\n    \"function_coverage_percentage\": 80.8\n  }\n}\n```\n\n### Step 6: Generate Reports\n\n**IMPORTANT:** Claude Code generates all three report formats at runtime based on the analyzer's structured output. The analyzer script returns structured data (as JSON to stdout or via Python data structures), and Claude Code is responsible for generating all report files.\n\nThe analyzer generates structured data containing full analysis results. Claude Code reads this data and generates three report formats:\n\n#### 1. JSON Report (`test-structure-report.json`)\n\n**Generated by:** Claude Code at runtime based on analyzer output\n\nMachine-readable format containing full analysis data. See Step 5 for structure.\n\n**How to generate:**\n- Read structured data from analyzer (returned as JSON to stdout)\n- Write to JSON file with `indent=2` for readability\n\n#### 2. Text Summary (`test-structure-summary.txt`)\n\n**Generated by:** Claude Code at runtime based on analyzer output\n\nTerminal-friendly summary showing:\n- Overall statistics (files with/without tests, function coverage)\n- High-priority gaps\n- Recommendations\n\n**Format Structure:**\n```text\n============================================================\nTest Structure Analysis\n============================================================\n\nFile: {filename}\nLanguage: {language}\nAnalysis Date: {timestamp}\n\n============================================================\nCoverage Summary\n============================================================\n\nTotal Source Files:    {count}\nFiles With Tests:      {count} ({percentage}%)\nFiles Without Tests:   {count} ({percentage}%)\n\nTotal Functions:       {count}\nTested Functions:      {count} ({percentage}%)\nUntested Functions:    {count} ({percentage}%)\n\n============================================================\nHigh Priority Gaps\n============================================================\n\nUNTESTED FILES:\n  1. {filepath} - {reason} ({function_count} functions, {exported_count} exported)\n  ...\n\nUNTESTED FUNCTIONS:\n  1. {filepath}::{function} - {reason} (visibility: {visibility})\n  ...\n\n============================================================\nRecommendations\n============================================================\n\nCurrent Coverage: {current}%\nTarget Coverage: {target}%\n\nFocus on addressing HIGH priority gaps first to maximize\ntest coverage and ensure production readiness.\n```\n\n#### 3. HTML Report (`test-structure-report.html`)\n\n**Generated by:** Claude Code at runtime based on analyzer output\n\nInteractive HTML report with:\n\n**Required Sections:**\n\n1. **Header** with project info, language, and generation timestamp\n2. **Summary Dashboard** with score cards showing:\n   - Total source files and files with/without tests\n   - Function coverage percentage\n   - High-priority gap count\n3. **Untested Files Table** with columns:\n   - File path\n   - Function count\n   - Exported function count\n   - Priority (high/medium/low)\n4. **Untested Functions Table** with columns:\n   - File path\n   - Function name\n   - Visibility (exported/private)\n   - Complexity score\n   - Priority\n5. **Recommendations Section** grouped by priority\n\n**Styling:**\n- Use the same CSS as gaps skill (modern gradient, cards, tables)\n- Priority badges: high (red), medium (orange), low (blue)\n- Escape all content with `html.escape()`\n\n## Implementation Steps\n\nWhen implementing this skill in a command:\n\n### Step 0: Generate Analyzer Script at Runtime\n\n**CRITICAL:** Before running any analysis, generate the analyzer script from the reference implementation.\n\n```bash\n# Create output directory\nmkdir -p .work/test-coverage/analyze/\n\n# Generate the analyzer script from the specification below\n# Claude Code will write test_structure_analyzer.py based on the Analyzer Specification section\n```\n\n**Analyzer Specification:**\n\nGenerate a Python script (`test_structure_analyzer.py`) that analyzes Go test structure without running tests:\n\n**Input:** Path or URL to a Go test file or directory\n**Output:** Structured JSON data printed to stdout, plus optional text summary to stderr\n\n**Core Algorithm:**\n\n0. **Input Processing** (handle URLs and local paths):\n   - Check if input starts with `http://` or `https://`\n   - If URL: Use `urllib.request.urlopen()` to fetch content, save to temp file\n   - If local path: Use directly\n   - After analysis: Clean up temp file if created\n   - Note: Directory URLs not supported, only single file URLs\n\n1. **File Discovery**:\n   - Test files: `*_test.go` (exclude vendor, generated code)\n   - E2E tests: Files in `test/e2e/`, `test/integration/`, or containing `[Serial]`, `[Disruptive]` markers\n   - Source files: `*.go` (exclude `*_test.go`, vendor)\n\n2. **Test Parsing** (regex-based):\n   - Ginkgo: `(?:g\\.|o\\.)?It\\(\\s*[\"']([^\"']+)[\"']`  extract test name, line number\n   - Standard Go: `func (Test\\w+)\\(t \\*testing\\.T\\)`  extract test function name\n   - Extract targets (functions called): regex for `\\w+\\([^)]*\\)` inside test body\n\n3. **Source File Analysis**:\n   - Parse function definitions: `func (\\w+)\\(` or `func \\(\\w+ \\*?\\w+\\) (\\w+)\\(`\n   - Track exported vs unexported (capitalized vs lowercase first letter)\n\n4. **Test-to-Source Mapping**:\n   - Convention: `handler_test.go`  `handler.go`\n   - Function-level: Match test names to source function names (e.g., `TestHandleRequest`  `HandleRequest`)\n   - Import analysis: Parse `import` statements in test files\n\n5. **Single File Mode** (when input is a file, not directory):\n   - Analyze only the test file structure\n   - Extract: test count, test names, imports, line numbers\n   - Skip source file mapping and gap detection\n   - Output: Test structure analysis only\n\n6. **Output Format** (JSON to stdout):\n```json\n{\n  \"language\": \"go\",\n  \"source_dir\": \"/path/to/src\",\n  \"test_only_mode\": false,\n  \"summary\": {\n    \"total_source_files\": 45,\n    \"total_test_files\": 32,\n    \"untested_files_count\": 8\n  },\n  \"test_file_details\": {\n    \"path\": \"/path/to/test.go\",\n    \"test_count\": 15,\n    \"tests\": [\n      {\"name\": \"TestFoo\", \"line_start\": 10, \"line_end\": 20, \"targets\": [\"Foo\", \"Bar\"]}\n    ],\n    \"imports\": [\"testing\", \"github.com/onsi/ginkgo\"]\n  }\n}\n```\n\n**Command-line Interface:**\n```\npython3 .work/test-coverage/analyze/test_structure_analyzer.py <source-path> [--test-structure-only] [--output <dir>]\n```\n\n**Why Runtime Generation:**\n- Claude Code generates the analyzer from this specification\n- No separate `.py` file to maintain\n- SKILL.md is the single source of truth\n- Simpler and more maintainable\n\n### Step 1: Validate Inputs\n\nCheck that source directory exists and detect language if not specified.\n\n### Step 2: Execute Test Structure Analyzer\n\n```bash\n# Run analyzer (outputs structured JSON to stdout)\npython3 .work/test-coverage/analyze/test_structure_analyzer.py \\\n    <source-directory> \\\n    --priority <priority> \\\n    --output-json\n```\n\nThe analyzer will output structured JSON to stdout containing:\n- Test file analysis\n- Source file analysis\n- Test-to-source mappings\n- Coverage gaps\n- Summary statistics\n\n### Step 3: Generate All Three Report Formats at Runtime\n\n**IMPORTANT:** Claude Code generates all three report formats based on the analyzer's structured output.\n\n#### 3.1: Capture and Parse Analyzer Output\n\n```python\nimport json\nimport subprocess\n\n# Run analyzer and capture JSON output\nresult = subprocess.run(\n    ['python3', '.work/test-coverage/analyze/test_structure_analyzer.py', source_dir, '--output-json'],\n    capture_output=True,\n    text=True\n)\n\n# Parse structured data\nanalysis_data = json.loads(result.stdout)\n```\n\n#### 3.2: Generate JSON Report\n\n```python\njson_path = '.work/test-coverage/analyze/test-structure-report.json'\nwith open(json_path, 'w') as f:\n    json.dump(analysis_data, f, indent=2)\n```\n\n#### 3.3: Generate Text Summary Report\n\nFollow the text format specification in Step 6 to generate a terminal-friendly summary.\n\n```python\ntext_path = '.work/test-coverage/analyze/test-structure-summary.txt'\n# Generate text content following format in Step 6\nwith open(text_path, 'w') as f:\n    f.write(text_content)\n```\n\n#### 3.4: Generate HTML Report\n\nFollow the HTML specification in Step 6 to generate an interactive report.\n\n```python\nhtml_path = '.work/test-coverage/analyze/test-structure-report.html'\n# Generate HTML content following specification in Step 6\nwith open(html_path, 'w') as f:\n    f.write(html_content)\n```\n\n### Step 4: Display Results\n\nShow summary and report locations to user:\n\n```\nTest Structure Analysis Complete\n\nReports Generated:\n   HTML:  .work/test-coverage/analyze/test-structure-report.html\n   JSON:  .work/test-coverage/analyze/test-structure-report.json\n   Text:  .work/test-coverage/analyze/test-structure-summary.txt\n```\n\n##  MANDATORY PRE-COMPLETION VALIDATION\n\n**CRITICAL:** Before declaring this skill complete, you MUST execute ALL validation checks below. Failure to validate is considered incomplete execution.\n\n### Validation Checklist\n\nExecute these verification steps in order. ALL must pass:\n\n#### 1. File Existence Check\n\n```bash\n# Verify all three reports exist\ntest -f .work/test-coverage/analyze/test-structure-report.html && echo \" HTML exists\" || echo \" HTML MISSING\"\ntest -f .work/test-coverage/analyze/test-structure-report.json && echo \" JSON exists\" || echo \" JSON MISSING\"\ntest -f .work/test-coverage/analyze/test-structure-summary.txt && echo \" Text exists\" || echo \" Text MISSING\"\n```\n\n**Required:** All three files must exist. If any are missing, regenerate them.\n\n#### 2. Test Case Extraction Verification\n\n```bash\n# Verify test cases were extracted\npython3 << 'EOF'\nimport json\ntry:\n    with open('.work/test-coverage/analyze/test-structure-report.json', 'r') as f:\n        data = json.load(f)\n\n    test_count = data.get('summary', {}).get('test_cases_count', 0)\n\n    if test_count > 0:\n        print(f\" Test cases extracted: {test_count}\")\n    else:\n        print(\" NO TEST CASES FOUND - verify test file contains Ginkgo tests\")\n        exit(1)\nexcept Exception as e:\n    print(f\" ERROR: {e}\")\n    exit(1)\nEOF\n```\n\n**Required:** Test cases must be extracted. Zero test cases indicates a parsing issue.\n\n#### 3. HTML Report Structure Verification\n\n```bash\n# Verify HTML has required sections\ngrep -q \"<h2>Test Cases\" .work/test-coverage/analyze/test-structure-report.html && \\\n  echo \" Test Cases section present\" || \\\n  echo \" MISSING: Test Cases section\"\n\ngrep -q \"<h2>Coverage Summary\" .work/test-coverage/analyze/test-structure-report.html && \\\n  echo \" Coverage Summary section present\" || \\\n  echo \" MISSING: Coverage Summary section\"\n```\n\n**Required:** HTML must have all structural sections.\n\n#### 4. JSON Structure Verification\n\n```python\n# Verify JSON has all required fields\npython3 << 'EOF'\nimport json\ntry:\n    with open('.work/test-coverage/analyze/test-structure-report.json', 'r') as f:\n        data = json.load(f)\n\n    required_fields = [\n        ('summary.language', lambda d: d['summary']['language']),\n        ('summary.test_cases_count', lambda d: d['summary']['test_cases_count']),\n        ('test_cases', lambda d: d['test_cases']),\n    ]\n\n    missing = []\n    for name, getter in required_fields:\n        try:\n            getter(data)\n            print(f\" {name}\")\n        except (KeyError, TypeError):\n            print(f\" MISSING: {name}\")\n            missing.append(name)\n\n    if not missing:\n        print(\"\\n All required JSON fields present\")\n    else:\n        print(f\"\\n INCOMPLETE: Missing {len(missing)} required fields\")\n        exit(1)\nexcept Exception as e:\n    print(f\" ERROR: {e}\")\n    exit(1)\nEOF\n```\n\n**Required:** All required JSON fields must be present.\n\n### Validation Summary\n\n**Before declaring this skill complete:**\n\n1.  All three report files exist\n2.  Test cases successfully extracted (count > 0)\n3.  HTML has all required sections\n4.  JSON contains all required fields\n\n**If ANY check fails:** Fix the issue and re-run all validation checks. Do NOT declare the skill complete until ALL checks pass.\n\n## Error Handling\n\n### Common Issues and Solutions\n\n1. **Unable to parse test/source files**:\n   - Use fallback regex-based parsing\n   - Log warnings for unparseable files\n   - Continue with partial analysis\n\n2. **No test files found**:\n   - Check if test patterns are correct for the Go project\n   - Ensure test files follow `*_test.go` naming convention\n\n3. **Complex project structures**:\n   - Allow excluding certain directories via `--exclude`\n\n## Examples\n\n### Example 1: Go Project - Basic Analysis\n\n```bash\n# Analyze test structure for Go project\npython3 .work/test-coverage/analyze/test_structure_analyzer.py /path/to/go/project\n\n# Output:\n# Language: go\n# Discovered 45 source files, 32 test files\n# Function coverage: 80.8% (189/234 functions tested)\n# High priority gaps: 8 files without tests\n```\n\n### Example 2: Go Project with Filters\n\n```bash\n# Analyze only high-priority gaps\npython3 .work/test-coverage/analyze/test_structure_analyzer.py /path/to/go/project \\\n    --priority high \\\n    --exclude \"*/vendor/*\" \\\n    --output reports/test-gaps/\n```\n\n### Example 3: Single Test File Analysis\n\n```bash\n# Analyze single test file structure\npython3 .work/test-coverage/analyze/test_structure_analyzer.py ./test/e2e/networking/infw.go \\\n    --test-structure-only \\\n    --output ./reports/\n```\n\n## Integration with Claude Code Commands\n\nThis skill is used by:\n- `/test-coverage:analyze <source-dir>`\n\nThe command invokes this skill to perform test structure analysis without running tests.\n\n## See Also\n\n- [Test Coverage Plugin README](../../README.md) - User guide and installation"
              },
              {
                "name": "Component-Aware Test Gap Analysis",
                "description": "Intelligently identify missing test coverage based on component type",
                "path": "plugins/test-coverage/skills/gaps/SKILL.md",
                "frontmatter": {
                  "name": "Component-Aware Test Gap Analysis",
                  "description": "Intelligently identify missing test coverage based on component type"
                },
                "content": "# Component-Aware Test Gap Analysis Skill\n\nThis skill **automatically detects component type** (networking, storage, API, etc.) and provides **context-aware gap analysis**. It analyzes e2e test files to identify missing test coverage specific to the component being tested.\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- **Automatically detect component type** from test file path and content\n- **Component-specific gap analysis**:\n  - **Networking**: Identify missing protocol tests (TCP, UDP, SCTP), service type coverage, IP stack testing\n  - **Storage**: Find gaps in storage class coverage, volume mode testing, provisioner tests\n  - **Generic**: Analyze platform coverage and common scenarios for other components\n- **Always analyze**: Cloud platform coverage (AWS, Azure, GCP, etc.) and scenario testing (error handling, upgrades, RBAC, scale)\n- Prioritize testing efforts based on component-specific production importance\n- Generate comprehensive component-aware gap analysis reports\n\n##  CRITICAL REQUIREMENT\n\n**This skill MUST ALWAYS generate all three report formats (HTML, JSON, and Text) at runtime.**\n\nThe gap analyzer script (generated at runtime to `.work/test-coverage/gaps/gap_analyzer.py`) performs the analysis and returns structured data. Claude Code is responsible for generating all three report formats based on this data.\n\n**Required Actions:**\n1.  **Execute**: `python3 .work/test-coverage/gaps/gap_analyzer.py <test-file> --output-json` (outputs structured JSON to stdout)\n2.  **Generate**: Create all three report files (HTML, JSON, Text) at runtime\n3.  **Verify**: All three reports are generated successfully\n4.  **Display**: Show report locations and summary to the user\n\n**Failure to generate any of the three report formats** should be treated as a skill execution failure.\n\n## Prerequisites\n\n### Required Tools\n\n- **Python 3.8+** for test structure analysis\n- **Go toolchain** for the target project\n\n### Installation\n\n```bash\n# Python dependencies (standard library only, no external packages required)\n# Ensure Python 3.8+ is installed\n\n# Optional Go analysis tools\ngo install golang.org/x/tools/cmd/guru@latest\ngo install golang.org/x/tools/cmd/goimports@latest\n```\n\n## How It Works\n\n**Note: This skill currently supports E2E/integration test files for OpenShift/Kubernetes components written in Go (Ginkgo framework).**\n\n### Current Implementation\n\nThe analyzer performs **single test file analysis** with two analysis layers:\n\n1. **Generic Coverage Analysis** (keyword-based)\n   - Platforms, protocols, IP stacks, service types\n   - Uses regex pattern matching on file content\n\n2. **Feature-Based Analysis** (runtime extraction)\n   - Dynamically extracts features from test names\n   - Infers missing features based on patterns\n   - No hardcoded feature matrices - works for ANY component\n\nIt does **not** perform repository traversal, Go AST parsing, or test-to-source mapping.\n\n### Analysis Flow\n\n#### Step 1: Component Type Detection\n\nThe analyzer automatically detects the component type from:\n\n1. **File path patterns**:\n   - `/networking/`  networking component\n   - `/storage/`  storage component\n   - `/kapi/`, `/api/`  kube-api component\n   - `/etcd/`  etcd component\n   - `/auth/`, `/rbac/`  auth component\n\n2. **File content patterns**:\n   - Keywords like `sig-networking`, `networkpolicy`, `egressip`  networking\n   - Keywords like `sig-storage`, `persistentvolume`  storage\n   - Keywords like `sig-api`, `apiserver`  kube-api\n\n#### Step 2: Extract Test Cases\n\nParses the test file using regex to extract:\n\n- **Test names** from Ginkgo `g.It(\"test name\")` patterns\n- **Line numbers** where tests are defined\n- **Test tags** like `[Serial]`, `[Disruptive]`, `[NonPreRelease]`\n- **Test IDs** from patterns like `-12345-` in test names\n\n**Example:**\n```go\ng.It(\"egressip-12345-should work on AWS [Serial]\", func() {\n    // Test implementation\n})\n```\n\nExtracted:\n- Name: `egressip-12345-should work on AWS [Serial]`\n- ID: `12345`\n- Tags: `[Serial]`\n- Line: 42\n\n#### Step 3: Analyze Coverage Using Regex\n\nFor each component type, the analyzer searches the file content for specific keywords to determine what is tested:\n\n**Networking components:**\n- **Platforms**: `vsphere`, `AWS`, `azure`, `GCP`, `baremetal`\n- **Protocols**: `TCP`, `UDP`, `SCTP`\n- **Service types**: `NodePort`, `LoadBalancer`, `ClusterIP`\n- **Scenarios**: `invalid`, `upgrade`, `concurrent`, `performance`, `rbac`\n\n**Storage components:**\n- **Platforms**: `vsphere`, `AWS`, `azure`, `GCP`, `baremetal`\n- **Storage classes**: `gp2`, `gp3`, `csi`\n- **Volume modes**: `ReadWriteOnce`, `ReadWriteMany`, `ReadOnlyMany`\n- **Scenarios**: `invalid`, `upgrade`, `concurrent`, `performance`, `rbac`\n\n**Other components:**\n- **Platforms**: `vsphere`, `AWS`, `azure`, `GCP`, `baremetal`\n- **Scenarios**: `invalid`, `upgrade`, `concurrent`, `performance`, `rbac`\n\n#### Step 4: Identify Gaps\n\nFor each coverage dimension, if a keyword is **not found** in the file, it's flagged as a gap:\n\n**Example:**\n```python\n# If file content doesn't contain \"azure\" (case-insensitive)\ngaps.append({\n    'platform': 'Azure',\n    'priority': 'high',\n    'impact': 'Major cloud provider - production blocker',\n    'recommendation': 'Add Azure platform-specific tests'\n})\n```\n\n#### Step 5: Calculate Component-Aware Coverage Scores\n\nScoring is component-specific to avoid penalizing components for irrelevant metrics:\n\n**Networking components:**\n- Overall = avg(platform_score, protocol_score, service_type_score, scenario_score)\n\n**Storage components:**\n- Overall = avg(platform_score, storage_class_score, volume_mode_score, scenario_score)\n\n**Other components:**\n- Overall = avg(platform_score, scenario_score)\n\nEach dimension score = (items_found / total_items)  100\n\n#### Step 5a: Dynamic Feature Extraction (Runtime Analysis)\n\nIn addition to the keyword-based coverage analysis above, the analyzer performs **dynamic feature extraction** to identify component-specific features from test names at runtime, without any hardcoded feature matrices.\n\n**How Runtime Feature Extraction Works:**\n\n1. **Extract Features from Test Names**\n\n   Parse test names to identify features being tested:\n\n   **Example Test Name:**\n   ```\n   \"Validate egressIP with mixed of multiple non-overlapping UDNs and default network(layer3/2 and IPv4 only)\"\n   ```\n\n   **Extracted Features:**\n   -  Non-overlapping configuration\n   -  Multiple resource configuration\n   -  Mixed configuration\n   -  User Defined Networks (UDN)\n   -  Default network\n   -  Layer 3 networking\n\n2. **Group Features into Categories**\n\n   Features are automatically categorized:\n\n   - **Configuration Patterns**: overlapping, non-overlapping, single, multiple, mixed\n   - **Network Topology**: UDN, default network, layer2, layer3, gateway modes\n   - **Lifecycle Operations**: creation, deletion, recreation, assignment\n   - **Network Features**: failover, load balancing, isolation\n   - **Resilience & Recovery**: reboot, restart, node deletion\n\n3. **Infer Missing Features**\n\n   Based on patterns, infer what's missing:\n\n   - **Opposite patterns**: If \"overlapping\" tested  suggest \"non-overlapping\"\n   - **Single vs Multiple**: If \"single resource\" tested  suggest \"multiple resources\"\n   - **Completeness**: If \"deletion\" tested  suggest \"recreation\"\n   - **Layer coverage**: If \"layer2\" tested  suggest \"layer3\"\n\n**Benefits of Runtime Feature Extraction:**\n\n **No Hardcoding** - Works for ANY component without configuration\n **Intelligent Gap Detection** - Infers missing features based on patterns\n **Component-Agnostic** - Automatically adapts to any component type\n **Always Current** - Extracts from actual test names, not assumed features\n\n**Example: EgressIP Test Analysis**\n\n**Input (Test Names):**\n```\n1. Validate egressIP with mixed of multiple non-overlapping UDNs\n2. Validate egressIP with mixed of multiple overlapping UDNs\n3. Validate egressIP Failover with UDNs\n4. egressIP after UDN deleted then recreated\n5. egressIP after OVNK restarted\n6. Traffic is load balanced between egress nodes\n```\n\n**Output (Extracted Features):**\n```\nConfiguration Patterns:\n   Non-overlapping configuration\n   Overlapping configuration\n   Multiple resource configuration\n   Mixed configuration\n\nNetwork Topology:\n   User Defined Networks (UDN)\n\nLifecycle Operations:\n   Resource deletion\n   Resource recreation\n\nNetwork Features:\n   Failover\n   Load balancing\n\nResilience & Recovery:\n   OVN-Kubernetes restart\n```\n\n**Output (Inferred Feature Gaps):**\n```\n[HIGH] Single resource configuration\n  - Pattern suggests \"multiple\" tested but not \"single\"\n  - Recommendation: Add single resource baseline tests\n\n[HIGH] Layer 2 networking\n  - Layer 3 tested but Layer 2 missing\n  - Recommendation: Add Layer 2 network topology tests\n\n[MEDIUM] Local gateway mode\n  - Gateway mode mentioned but local vs shared not clear\n  - Recommendation: Add explicit gateway mode tests\n```\n\n**Integration in gap_analyzer.py:**\n\nThe dynamic feature extractor is built into the analyzer (no separate import needed):\n\n```python\n# After extracting test cases\nfeature_analysis = extract_features_from_tests(test_cases)\n\n# Results included in analysis output\ntested_features = feature_analysis['tested_features']\n# {'Configuration Patterns': ['Overlapping', 'Non-overlapping', ...],\n#  'Network Topology': ['UDN', 'Layer3', ...]}\n\nfeature_gaps = feature_analysis['feature_gaps']\n# [{'feature': 'Multiple resources', 'priority': 'high', ...}]\n\ncoverage_stats = feature_analysis['coverage_stats']\n# {'features_tested': 14, 'features_missing': 5}\n```\n\n**Report Integration:**\n\nFeature analysis is included in all three report formats:\n\n- **HTML Reports**: Feature sections with tested/missing features\n- **Text Reports**: Feature lists grouped by category\n- **JSON Reports**: Structured feature data for CI/CD integration\n\n### Limitations\n\nThe current implementation has the following limitations:\n\n **No repository traversal** - Analyzes only the single test file provided as input\n **No Go AST parsing** - Uses regex pattern matching instead of parsing Go syntax trees\n **No test-to-source mapping** - Cannot map test functions to source code functions\n **No function-level coverage** - Cannot determine which source functions are tested\n **No project-wide analysis** - Cannot analyze multiple test files or aggregate results\n **Keyword-based detection only** - Gap detection relies on keyword presence in test file\n **Single file focus** - Reports cover only the analyzed test file, not the entire codebase\n\nThese limitations mean the analyzer provides **scenario and platform coverage analysis** for a single E2E test file, not structural code coverage across a codebase.\n\n#### Step 6: Generate Reports\n\nThe analyzer generates three report formats. You should generate Python code at runtime to create these reports.\n\n#### 1. HTML Gap Report (`test-gaps-report.html`)\n\n**Purpose:** Interactive, filterable HTML report for visual gap analysis with professional styling\n\n**HTML Document Structure:**\n\nGenerate a complete HTML5 document with the following structure:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Test Coverage Gap Analysis - {filename}</title>\n    <style>\n        /* Inline all CSS styles here - see CSS Styles section below */\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <!-- Content sections -->\n    </div>\n    <script>\n        /* JavaScript for gap filtering - see JavaScript section below */\n    </script>\n</body>\n</html>\n```\n\n**CSS Styles (Inline in `<style>` tag):**\n\nGenerate comprehensive CSS with the following style rules:\n\n1. **Reset and Base Styles:**\n   - `*`: box-sizing: border-box, margin: 0, padding: 0\n   - `body`: font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; padding: 20px\n\n2. **Container and Layout:**\n   - `.container`: max-width: 1400px, margin: 0 auto, background: white, padding: 30px, border-radius: 8px, box-shadow: 0 2px 10px rgba(0,0,0,0.1)\n   - `h1`: color: #2c3e50, margin-bottom: 10px, font-size: 2em\n   - `h2`: color: #34495e, margin-top: 30px, margin-bottom: 15px, padding-bottom: 10px, border-bottom: 2px solid #e74c3c, font-size: 1.5em\n   - `h3`: color: #34495e, margin-top: 20px, margin-bottom: 10px, font-size: 1.2em\n\n3. **Metadata Section:**\n   - `.metadata`: background: #ecf0f1, padding: 15px, border-radius: 5px, margin-bottom: 25px\n   - `.metadata p`: margin: 5px 0\n\n4. **Score Cards:**\n   - `.score-grid`: display: grid, grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)), gap: 15px, margin: 20px 0\n   - `.score-card`: padding: 20px, border-radius: 8px, text-align: center, color: white\n   - `.score-card.excellent`: background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%) (score >= 80)\n   - `.score-card.good`: background: linear-gradient(135deg, #3498db 0%, #2980b9 100%) (score >= 60)\n   - `.score-card.fair`: background: linear-gradient(135deg, #f39c12 0%, #e67e22 100%) (score >= 40)\n   - `.score-card.poor`: background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%) (score < 40)\n   - `.score-card .number`: font-size: 2.5em, font-weight: bold, margin: 10px 0\n   - `.score-card .label`: font-size: 0.9em, opacity: 0.9\n\n5. **Gap Cards:**\n   - `.gap-card`: background: #fff, border-left: 4px solid #e74c3c, padding: 20px, margin: 15px 0, border-radius: 5px, box-shadow: 0 2px 5px rgba(0,0,0,0.1)\n   - `.gap-card.high`: border-left-color: #e74c3c\n   - `.gap-card.medium`: border-left-color: #f39c12\n   - `.gap-card.low`: border-left-color: #3498db\n   - `.gap-card h4`: color: #2c3e50, margin-bottom: 10px, font-size: 1.1em\n   - `.gap-card .gap-id`: font-family: \"Courier New\", monospace, font-size: 0.85em, color: #7f8c8d, margin-bottom: 5px\n   - `.priority`: display: inline-block, padding: 4px 12px, border-radius: 12px, font-size: 0.75em, font-weight: bold, margin-right: 8px\n   - `.priority.high`: background: #e74c3c, color: white\n   - `.priority.medium`: background: #f39c12, color: white\n   - `.priority.low`: background: #3498db, color: white\n   - `.gap-card .impact`: background: #fff3cd, border-left: 3px solid #ffc107, padding: 10px, margin: 10px 0, border-radius: 3px\n   - `.gap-card .recommendation`: background: #d4edda, border-left: 3px solid #28a745, padding: 10px, margin: 10px 0, border-radius: 3px\n\n6. **Tables:**\n   - `table`: width: 100%, border-collapse: collapse, margin: 20px 0\n   - `th, td`: padding: 12px, text-align: left, border-bottom: 1px solid #ddd\n   - `th`: background: #34495e, color: white, font-weight: 600\n   - `tr:hover`: background: #f5f5f5\n   - `.tested`: background: #d4edda, color: #155724, font-weight: bold, text-align: center\n   - `.not-tested`: background: #f8d7da, color: #721c24, font-weight: bold, text-align: center\n\n7. **Summary Boxes:**\n   - `.summary-box`: background: #e3f2fd, border-left: 4px solid #2196f3, padding: 15px, margin: 20px 0, border-radius: 5px\n   - `.warning-box`: background: #fff3cd, border-left: 4px solid #ffc107, padding: 15px, margin: 20px 0, border-radius: 5px\n   - `.success-box`: background: #d4edda, border-left: 4px solid #28a745, padding: 15px, margin: 20px 0, border-radius: 5px\n\n8. **Filter Buttons:**\n   - `.filter-buttons`: margin: 20px 0\n   - `.filter-btn`: padding: 10px 20px, margin-right: 10px, border: none, border-radius: 5px, cursor: pointer, font-weight: bold\n   - `.filter-btn.active`: box-shadow: 0 0 0 3px rgba(0,0,0,0.2)\n   - `.filter-btn.all`: background: #95a5a6, color: white\n   - `.filter-btn.high`: background: #e74c3c, color: white\n   - `.filter-btn.medium`: background: #f39c12, color: white\n   - `.filter-btn.low`: background: #3498db, color: white\n\n**HTML Content Sections:**\n\n1. **Metadata Section:**\n   ```html\n   <div class=\"metadata\">\n       <p><strong>File:</strong> <code>{escaped_filename}</code></p>\n       <p><strong>Path:</strong> <code>{escaped_filepath}</code></p>\n       <p><strong>Component:</strong> {escaped_component_title}</p>\n       <p><strong>Analysis Date:</strong> {YYYY-MM-DD}</p>\n       <p><strong>Total Test Cases:</strong> {count}</p>\n   </div>\n   ```\n\n2. **Coverage Scores Section:**\n   - Display score cards in a grid\n   - Show scores dynamically based on what's calculated for the component\n   - Score display order and labels:\n     ```python\n     SCORE_DISPLAY = {\n         'overall': {'order': 1, 'label': 'Overall Coverage'},\n         'platform_coverage': {'order': 2, 'label': 'Platform Coverage'},\n         'ip_stack_coverage': {'order': 3, 'label': 'IP Stack Coverage'},\n         'topology_coverage': {'order': 4, 'label': 'Topology Coverage'},\n         'network_layer_coverage': {'order': 5, 'label': 'Network Layer Coverage'},\n         'gateway_mode_coverage': {'order': 6, 'label': 'Gateway Mode Coverage'},\n         'protocol_coverage': {'order': 5, 'label': 'Protocol Coverage'},\n         'service_type_coverage': {'order': 6, 'label': 'Service Type Coverage'},\n         'storage_class_coverage': {'order': 7, 'label': 'Storage Class Coverage'},\n         'volume_mode_coverage': {'order': 8, 'label': 'Volume Mode Coverage'},\n         'scenario_coverage': {'order': 99, 'label': 'Scenario Coverage'},\n     }\n     ```\n   - Sort scores by order, render only non-zero/non-None scores\n   - Apply CSS class based on score value: `get_score_class(score)`\n   - If overall < 60%, add a warning box with key findings\n\n3. **What's Tested Section:**\n   - Generate tables showing tested vs not-tested items\n   - For networking components: platforms, protocols, service types, IP stacks, topologies, scenarios\n   - For storage components: platforms, storage classes, volume modes, scenarios\n   - For other components: platforms, scenarios only\n   - Table format:\n     ```html\n     <table>\n       <tr>\n         <th>Item</th>\n         <th>Status</th>\n       </tr>\n       <tr>\n         <td>AWS</td>\n         <td class=\"tested\"> Tested</td>\n       </tr>\n     </table>\n     ```\n\n4. **Coverage Gaps Section:**\n   - Summary box with gap counts by priority\n   - Filter buttons for All/High/Medium/Low priority\n   - Gap cards with data-priority attribute for filtering:\n     ```html\n     <div class=\"gap-card {priority}\" data-priority=\"{priority}\">\n         <div class=\"gap-id\">GAP-{001}</div>\n         <h4>\n             <span class=\"priority {priority}\">{PRIORITY} PRIORITY</span>\n             <span class=\"category\">{Category}</span>\n             {gap_name}\n         </h4>\n         <div class=\"impact\"><strong>Impact:</strong> {impact_description}</div>\n         <div class=\"recommendation\"><strong>Recommendation:</strong> {recommendation_text}</div>\n     </div>\n     ```\n   - Assign sequential GAP IDs: GAP-001, GAP-002, etc.\n   - Sort gaps by priority (high, medium, low)\n\n5. **Recommendations Section:**\n   - Success box with top 5 high-priority gaps listed\n   - Bulleted list of immediate actions\n\n**JavaScript for Filtering (Inline in `<script>` tag):**\n\n```javascript\nfunction filterGaps(priority) {\n    const cards = document.querySelectorAll('.gap-card');\n    const buttons = document.querySelectorAll('.filter-btn');\n\n    buttons.forEach(btn => btn.classList.remove('active'));\n    document.querySelector(`.filter-btn.${priority}`).classList.add('active');\n\n    cards.forEach(card => {\n        if (priority === 'all' || card.dataset.priority === priority) {\n            card.style.display = 'block';\n        } else {\n            card.style.display = 'none';\n        }\n    });\n}\n```\n\n**Security Requirements:**\n- Use `html.escape()` for all user-provided content (filenames, test names, gap descriptions)\n- Sanitize priority values to only allow: 'high', 'medium', 'low'\n- Never inject raw HTML from analysis data\n\n**Helper Functions to Implement:**\n\n1. `get_score_class(score)`:\n   - score >= 80: return 'excellent'\n   - score >= 60: return 'good'\n   - score >= 40: return 'fair'\n   - else: return 'poor'\n\n2. Escape all strings using `from html import escape`\n\n**Component-Specific Behavior:**\n\n- **Networking components** (networking, router, dns, network-observability):\n  - Show: platforms, protocols, service types, IP stacks, topologies, network layers, gateway modes, scenarios\n\n- **Storage components** (storage, csi):\n  - Show: platforms, storage classes, volume modes, volumes, CSI drivers, snapshots, scenarios\n\n- **Other components**:\n  - Show: platforms, scenarios only\n\n#### 2. JSON Report (`test-gaps-report.json`)\n\n**Generated by:** Claude Code at runtime based on analyzer output\n\n**Purpose:** Machine-readable format for CI/CD integration\n\n**Structure:**\n```json\n{\n  \"analysis\": {\n    \"file\": \"path/to/test/file.go\",\n    \"component_type\": \"networking\",\n    \"test_count\": 15,\n    \"test_cases\": [\n      {\n        \"name\": \"test name\",\n        \"line\": 42,\n        \"id\": \"12345\",\n        \"tags\": [\"Serial\", \"Disruptive\"]\n      }\n    ],\n    \"coverage\": {\n      \"platforms\": {\n        \"tested\": [\"AWS\", \"GCP\"],\n        \"not_tested\": [\"Azure\", \"vSphere\", \"Bare Metal\"]\n      },\n      \"protocols\": {\n        \"tested\": [\"TCP\"],\n        \"not_tested\": [\"UDP\", \"SCTP\"]\n      }\n    },\n    \"gaps\": {\n      \"platforms\": [\n        {\n          \"platform\": \"Azure\",\n          \"priority\": \"high\",\n          \"impact\": \"Major cloud provider\",\n          \"recommendation\": \"Add Azure tests\"\n        }\n      ],\n      \"protocols\": [],\n      \"scenarios\": []\n    }\n  },\n  \"scores\": {\n    \"overall\": 45.0,\n    \"platform_coverage\": 33.3,\n    \"protocol_coverage\": 33.3,\n    \"scenario_coverage\": 40.0\n  },\n  \"generated_at\": \"2025-11-10T10:00:00Z\"\n}\n```\n\n**Implementation:** Use `json.dump()` with `indent=2` for readable output\n\n#### 3. Text Summary (`test-gaps-summary.txt`)\n\n**Generated by:** Claude Code at runtime based on analyzer output\n\n**Purpose:** Terminal-friendly summary for quick review\n\n**Format Structure:**\n```text\n============================================================\nTest Coverage Gap Analysis\n============================================================\n\nFile: {filename}\nComponent: {component_type}\nTest Cases: {count}\nAnalysis Date: {timestamp}\n\n============================================================\nCoverage Scores\n============================================================\n\nOverall Coverage:          {score}%\nPlatform Coverage:         {score}%\n[Component-specific scores based on type]\nScenario Coverage:         {score}%\n\n============================================================\nWhat's Tested\n============================================================\n\nPlatforms:\n   {platform1}\n   {platform2}\n\n[Additional tested items based on component type]\n\n============================================================\nIdentified Gaps\n============================================================\n\nPLATFORM GAPS:\n  [PRIORITY] {platform}\n    Impact: {impact}\n    Recommendation: {recommendation}\n\n[Additional gap sections based on component type]\n\n============================================================\nRecommendations\n============================================================\n\nCurrent Coverage: {current}%\nTarget Coverage: {target}%\n\nFocus on addressing HIGH priority gaps first to maximize\ntest coverage and ensure production readiness.\n```\n\n**Component-Specific Sections:**\n- **Networking components**: Include protocol, service type, IP stack, topology gaps\n- **Storage components**: Include storage class, volume mode gaps\n- **Other components**: Only include platform and scenario gaps\n\n**Implementation:** Use `'\\n'.join(lines)` to build the text content\n\n## Implementation Steps\n\nWhen implementing this skill in a command:\n\n### Step 0: Generate Analyzer Script at Runtime\n\n**CRITICAL:** Before running any analysis, generate the analyzer script from the reference implementation.\n\n```bash\n# Create output directory\nmkdir -p .work/test-coverage/gaps/\n\n# Generate the analyzer script from the specification below\n# Claude Code will write gap_analyzer.py based on the Analyzer Specification section\n```\n\n**Analyzer Specification:**\n\nGenerate a Python script (`gap_analyzer.py`) that performs component-aware E2E test gap analysis:\n\n**Input:** Path or URL to a Go test file (Ginkgo framework)\n**Output:** JSON to stdout with analysis results and coverage scores\n\n**Core Algorithm:**\n\n0. **Input Processing** (handle URLs and local paths):\n   - Check if input starts with `http://` or `https://`\n   - If URL: Use `urllib.request.urlopen()` to fetch content, save to temp file\n   - If local path: Use directly\n   - After analysis: Clean up temp file if created\n\n1. **Component Detection** (auto-detect from file path/content):\n   - Networking: `/networking/`, `egressip`, `networkpolicy`  component_type='networking'\n   - Storage: `/storage/`, `persistentvolume`  component_type='storage'\n   - Other components: etcd, apiserver, mco, operators, etc.\n\n2. **Test Extraction** (regex-based):\n   - Pattern: `(?:g\\.|o\\.)?It\\(\\s*[\"']([^\"']+)[\"']`\n   - Extract: test name, line number, tags ([Serial], [Disruptive]), test ID (pattern: `-\\d+-`)\n\n3. **Coverage Analysis** (keyword search in file content):\n   - **Platforms**: Search for `aws|azure|gcp|vsphere|baremetal|rosa` (case-insensitive)\n   - **Protocols**: Search for `\\bTCP\\b`, `\\bUDP\\b`, `\\bSCTP\\b`, `curl|wget|http` (TCP via HTTP)\n   - **IP Stacks**: Search for `ipv4`, `ipv6`, `dualstack`\n   - **Service Types**: Search for `NodePort`, `LoadBalancer`, `ClusterIP`\n   - **Network Layers** (networking only): `layer2|l2`, `layer3|l3`, `default network`\n   - **Gateway Modes** (networking only):\n     - Search for `local.gateway|lgw`  if found, Local Gateway is tested\n     - Shared Gateway is DEFAULT in OVN-K: if tests exist but no local gateway pattern found  Shared Gateway is tested\n     - If no tests exist  neither is tested\n   - **Topologies**: `sno|single-node`, `multi-node|HA cluster`, `hypershift|hcp`, check NonHyperShiftHOST tag\n   - **Scenarios**: `failover`, `reboot`, `restart`, `delete`, `invalid`, `upgrade`, `concurrent`, `performance`, `rbac`, `traffic disruption`\n\n4. **Gap Identification**:\n   - For each category, items NOT found = gaps\n   - Assign priority: high (production-critical), medium (important), low (nice-to-have)\n   - Platform gaps: Azure/GCP/AWS = high, vSphere/Bare Metal = medium\n   - Protocol gaps: UDP = high, SCTP = medium, TCP non-HTTP = low\n   - Service type gaps: LoadBalancer = high, others = medium\n   - Scenario gaps: Error Handling = high, Traffic Disruption = high (networking only)\n\n5. **Coverage Scoring** (component-aware):\n   - Networking: avg(platform, protocol, service_type, ip_stack, network_layer, gateway_mode, topology, scenario)\n   - Storage: avg(platform, storage_class, volume_mode, scenario)\n   - Other: avg(platform, scenario)\n   - Each dimension: (tested_count / total_count)  100\n\n6. **Output Format** (JSON to stdout):\n```json\n{\n  \"analysis\": {\n    \"file\": \"/path/to/test.go\",\n    \"component_type\": \"networking\",\n    \"test_count\": 15,\n    \"test_cases\": [...],\n    \"coverage\": {\n      \"platforms\": {\"tested\": [...], \"not_tested\": [...]},\n      \"protocols\": {\"tested\": [...], \"not_tested\": [...]},\n      ...\n    },\n    \"gaps\": {\n      \"platforms\": [{\"platform\": \"Azure\", \"priority\": \"high\", \"impact\": \"...\", \"recommendation\": \"...\"}],\n      ...\n    }\n  },\n  \"scores\": {\n    \"overall\": 62.0,\n    \"platform_coverage\": 100.0,\n    ...\n  }\n}\n```\n\n**Why Runtime Generation:**\n- Claude Code generates the analyzer from this specification\n- No separate `.py` file to maintain\n- SKILL.md is the single source of truth\n- Claude Code is excellent at generating code from specifications\n\n### Step 1: Execute Gap Analyzer Script (MANDATORY)\n\n**Execute the gap analyzer script to perform analysis and return structured data:**\n\n```bash\n# Run gap analyzer (outputs structured JSON to stdout)\npython3 .work/test-coverage/gaps/gap_analyzer.py <test-file-path> --output-json\n```\n\nThe analyzer will output structured JSON to stdout containing:\n- Component type detection\n- Test case extraction\n- Coverage analysis\n- Gap identification\n- Priority scoring\n- Component-specific recommendations\n\n**IMPORTANT:** Do not skip this step. Do not attempt manual analysis. The script is the authoritative implementation.\n\n### Step 2: Capture and Parse Analyzer Output\n\n```python\nimport json\nimport subprocess\n\n# Run analyzer and capture JSON output\nresult = subprocess.run(\n    ['python3', '.work/test-coverage/gaps/gap_analyzer.py', test_file, '--output-json'],\n    capture_output=True,\n    text=True\n)\n\n# Parse structured data\nanalysis_data = json.loads(result.stdout)\n```\n\n### Step 3: Generate All Three Report Formats at Runtime (MANDATORY)\n\n**IMPORTANT:** Claude Code generates all three report formats based on the analyzer's structured output.\n\n#### 3.1: Generate JSON Report\n\n```python\njson_path = '.work/test-coverage/gaps/test-gaps-report.json'\nwith open(json_path, 'w') as f:\n    json.dump(analysis_data, f, indent=2)\n```\n\n#### 3.2: Generate Text Summary Report\n\nFollow the text format specification in Step 6 to generate a terminal-friendly summary.\n\n```python\ntext_path = '.work/test-coverage/gaps/test-gaps-summary.txt'\n# Generate text content following format in Step 6\nwith open(text_path, 'w') as f:\n    f.write(text_content)\n```\n\n#### 3.3: Generate HTML Report\n\nFollow the HTML specification in Step 6 to generate an interactive report.\n\n```python\nhtml_path = '.work/test-coverage/gaps/test-gaps-report.html'\n# Generate HTML content following specification in Step 6\n# Include all CSS styles, JavaScript filtering, and component-specific sections\nwith open(html_path, 'w') as f:\n    f.write(html_content)\n```\n\n**Key Requirements:**\n- Generate HTML following the exact structure in \"Step 6: Generate Reports\" above\n- Include all CSS styles inline in `<style>` tag\n- Include JavaScript filtering function in `<script>` tag\n- Escape all user-provided content with `html.escape()`\n- Apply component-specific sections based on component type\n\n### Step 4: Display Results\n\nAfter generating all three reports, display the results to the user:\n\n```python\n# Display summary (from text report or analysis_data)\nprint(f\"Component detected: {analysis_data['analysis']['component_type']}\")\nprint(f\"Overall coverage: {analysis_data['scores']['overall']}%\")\nprint(f\"High-priority gaps: {high_priority_count}\")\n\n# Provide report locations\nprint(\"\\nReports Generated:\")\nprint(\"   HTML:  .work/test-coverage/gaps/test-gaps-report.html\")\nprint(\"   JSON:  .work/test-coverage/gaps/test-gaps-report.json\")\nprint(\"   Text:  .work/test-coverage/gaps/test-gaps-summary.txt\")\n```\n\n### Step 5: Parse Analysis Data (Optional)\n\nFor programmatic access to gap data, use the `analysis_data` from Step 2:\n\n```python\n# Access analysis results (from analysis_data captured in Step 2)\ncomponent_type = analysis_data['analysis']['component_type']\ntest_count = analysis_data['analysis']['test_count']\noverall_score = analysis_data['scores']['overall']\n\n# Access gaps\nplatform_gaps = analysis_data['analysis']['gaps']['platforms']\nprotocol_gaps = analysis_data['analysis']['gaps'].get('protocols', [])\nscenario_gaps = analysis_data['analysis']['gaps']['scenarios']\n\n# Filter high-priority gaps\nhigh_priority_gaps = [\n    gap for category in analysis_data['analysis']['gaps'].values()\n    for gap in category if gap.get('priority') == 'high'\n]\n```\n\n##  MANDATORY PRE-COMPLETION VALIDATION\n\n**CRITICAL:** Before declaring this skill complete, you MUST execute ALL validation checks below. Failure to validate is considered incomplete execution.\n\n### Validation Checklist\n\nExecute these verification steps in order. ALL must pass:\n\n#### 1. File Existence Check\n\n```bash\n# Verify all three reports exist\ntest -f .work/test-coverage/gaps/test-gaps-report.html && echo \" HTML exists\" || echo \" HTML MISSING\"\ntest -f .work/test-coverage/gaps/test-gaps-report.json && echo \" JSON exists\" || echo \" JSON MISSING\"\ntest -f .work/test-coverage/gaps/test-gaps-summary.txt && echo \" Text exists\" || echo \" Text MISSING\"\n```\n\n**Required:** All three files must exist. If any are missing, regenerate them.\n\n#### 2. Dynamic Feature Extraction Verification\n\n```bash\n# Verify HTML has \"Tested Features (Dynamic Feature Extraction)\" section\ngrep -q \"Tested Features (Dynamic Feature Extraction)\" .work/test-coverage/gaps/test-gaps-report.html && \\\n  echo \" Feature extraction section present\" || \\\n  echo \" MISSING: Dynamic Feature Extraction section\"\n\n# Verify JSON has feature data\ngrep -q '\"tested_features\"' .work/test-coverage/gaps/test-gaps-report.json && \\\ngrep -q '\"feature_gaps\"' .work/test-coverage/gaps/test-gaps-report.json && \\\n  echo \" Feature data in JSON\" || \\\n  echo \" MISSING: Feature data in JSON\"\n\n# Verify Text has feature section\ngrep -q \"Tested Features\" .work/test-coverage/gaps/test-gaps-summary.txt && \\\n  echo \" Feature section in Text\" || \\\n  echo \" MISSING: Feature section in Text\"\n```\n\n**Required:** Dynamic Feature Extraction must be present in all three reports. This is a critical requirement from Step 5a (lines 163-280).\n\n#### 3. HTML Coverage Dimension Verification\n\n**CRITICAL:** The HTML report must display ALL coverage dimension tables based on component type.\n\n```bash\n# For networking components, verify ALL 8 dimension tables exist\ngrep -c \"<h3>Platforms</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Protocols</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Service Types</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>IP Stacks</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Network Layers</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Gateway Modes</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Topologies</h3>\" .work/test-coverage/gaps/test-gaps-report.html\ngrep -c \"<h3>Scenarios</h3>\" .work/test-coverage/gaps/test-gaps-report.html\n```\n\n**Expected Results:**\n- **Networking components:** 8 dimension tables (Platforms, Protocols, Service Types, IP Stacks, Network Layers, Gateway Modes, Topologies, Scenarios)\n- **Storage components:** 5 dimension tables (Platforms, Storage Classes, Volume Modes, Provisioners, Scenarios)\n- **Other components:** 2 dimension tables (Platforms, Scenarios)\n\n**Verification Command:**\n```bash\n# Count total coverage dimension tables\nTABLE_COUNT=$(grep -E \"<h3>(Platforms|Protocols|Service Types|IP Stacks|Network Layers|Gateway Modes|Topologies|Scenarios|Storage Classes|Volume Modes|Provisioners)</h3>\" .work/test-coverage/gaps/test-gaps-report.html | wc -l)\necho \"Coverage dimension tables found: $TABLE_COUNT\"\n\n# Verify based on component type\nCOMPONENT=$(grep -oP 'Component:</strong> \\K[^<]+' .work/test-coverage/gaps/test-gaps-report.html | head -1 | tr -d '</p>')\necho \"Component type: $COMPONENT\"\n\ncase \"$COMPONENT\" in\n  Networking)\n    [ \"$TABLE_COUNT\" -eq 8 ] && echo \" All 8 networking dimensions present\" || echo \" INCOMPLETE: Expected 8 tables, found $TABLE_COUNT\"\n    ;;\n  Storage)\n    [ \"$TABLE_COUNT\" -eq 5 ] && echo \" All 5 storage dimensions present\" || echo \" INCOMPLETE: Expected 5 tables, found $TABLE_COUNT\"\n    ;;\n  *)\n    [ \"$TABLE_COUNT\" -eq 2 ] && echo \" All 2 generic dimensions present\" || echo \" INCOMPLETE: Expected 2 tables, found $TABLE_COUNT\"\n    ;;\nesac\n```\n\n**Required:** All component-specific dimension tables must be present. Missing tables indicate incomplete HTML generation.\n\n#### 4. Effort Estimates Verification\n\n```bash\n# Verify gaps include effort estimates\ngrep -q \"Effort Required\" .work/test-coverage/gaps/test-gaps-report.html && \\\n  echo \" Effort estimates in HTML\" || \\\n  echo \" MISSING: Effort estimates\"\n```\n\n**Required:** Gaps must include effort estimates (Low, Medium, High) as specified in Step 5a.\n\n#### 5. Gap Analyzer Implementation Verification\n\n```bash\n# Verify analyzer has feature extraction function\ngrep -q \"def extract_features_from_tests\" .work/test-coverage/gaps/gap_analyzer.py && \\\n  echo \" Feature extraction function exists\" || \\\n  echo \" MISSING: extract_features_from_tests() function\"\n\n# Verify all 5 feature categories are defined\ngrep -q \"Configuration Patterns\" .work/test-coverage/gaps/gap_analyzer.py && \\\ngrep -q \"Network Topology\" .work/test-coverage/gaps/gap_analyzer.py && \\\ngrep -q \"Lifecycle Operations\" .work/test-coverage/gaps/gap_analyzer.py && \\\ngrep -q \"Network Features\" .work/test-coverage/gaps/gap_analyzer.py && \\\ngrep -q \"Resilience & Recovery\" .work/test-coverage/gaps/gap_analyzer.py && \\\n  echo \" All 5 feature categories defined\" || \\\n  echo \" MISSING: Some feature categories not implemented\"\n```\n\n**Required:** The gap analyzer must implement Dynamic Feature Extraction with all 5 categories.\n\n#### 6. JSON Structure Verification\n\n```python\n# Verify JSON has all required fields\npython3 << 'EOF'\nimport json\ntry:\n    with open('.work/test-coverage/gaps/test-gaps-report.json', 'r') as f:\n        data = json.load(f)\n\n    required_fields = [\n        ('analysis.file', lambda d: d['analysis']['file']),\n        ('analysis.component_type', lambda d: d['analysis']['component_type']),\n        ('analysis.test_count', lambda d: d['analysis']['test_count']),\n        ('analysis.tested_features', lambda d: d['analysis']['tested_features']),\n        ('analysis.feature_gaps', lambda d: d['analysis']['feature_gaps']),\n        ('scores.overall', lambda d: d['scores']['overall']),\n    ]\n\n    missing = []\n    for name, getter in required_fields:\n        try:\n            getter(data)\n            print(f\" {name}\")\n        except (KeyError, TypeError):\n            print(f\" MISSING: {name}\")\n            missing.append(name)\n\n    if not missing:\n        print(\"\\n All required JSON fields present\")\n    else:\n        print(f\"\\n INCOMPLETE: Missing {len(missing)} required fields\")\n        exit(1)\nexcept Exception as e:\n    print(f\" ERROR: {e}\")\n    exit(1)\nEOF\n```\n\n**Required:** All required JSON fields must be present.\n\n### Validation Summary\n\n**Before declaring this skill complete:**\n\n1.  All three report files exist\n2.  Dynamic Feature Extraction present in all reports\n3.  HTML shows ALL component-specific coverage dimension tables\n4.  Effort estimates included in gaps\n5.  Gap analyzer implements feature extraction function\n6.  JSON contains all required fields\n\n**If ANY check fails:** Fix the issue and re-run all validation checks. Do NOT declare the skill complete until ALL checks pass.\n\n## Error Handling\n\n### Common Issues and Solutions\n\n1. **File not found**:\n   - Verify the test file path is correct\n   - Check that the file exists and is readable\n\n2. **Invalid file format**:\n   - Ensure the file is a Go test file (`.go`)\n   - Check that the file uses Ginkgo framework (`g.It`, `g.Describe`)\n\n3. **No test cases found**:\n   - Verify the file contains Ginkgo test cases\n   - Check for `g.It(\"...\")` patterns\n\n## Examples\n\n### Example 1: Analyze Networking Test File\n\n```bash\n# Run gap analyzer on a networking test file\ncd /home/anusaxen/git/ai-helpers/plugins/test-coverage\npython3 .work/test-coverage/gaps/gap_analyzer.py \\\n  /path/to/test/extended/networking/egressip_test.go \\\n  --output .work/gaps/\n\n# Output:\n# Component detected: networking\n# Test cases found: 25\n# Overall coverage: 45.0%\n# High-priority gaps: Azure platform, UDP protocol, Error handling scenarios\n#\n# Reports generated:\n#   HTML: .work/gaps/test-gaps-report.html\n#   JSON: .work/gaps/test-gaps-report.json\n#   Text: .work/gaps/test-gaps-summary.txt\n```\n\n### Example 2: Analyze Storage Test File\n\n```bash\n# Run gap analyzer on a storage test file\npython3 .work/test-coverage/gaps/gap_analyzer.py \\\n  /path/to/test/extended/storage/persistent_volumes_test.go \\\n  --output .work/gaps/\n\n# Output:\n# Component detected: storage\n# Test cases found: 18\n# Overall coverage: 52.0%\n# High-priority gaps: ReadWriteMany volumes, CSI storage class, Snapshot scenarios\n```\n\n### Example 3: Analyze from GitHub URL\n\n```bash\n# Analyze file from GitHub raw URL\npython3 .work/test-coverage/gaps/gap_analyzer.py \\\n  https://raw.githubusercontent.com/openshift/origin/master/test/extended/networking/egressip.go \\\n  --output .work/gaps/\n```\n\nThe analyzer automatically detects URLs and downloads the file for analysis.\n\n## Integration with Claude Code Commands\n\nThis skill is used by:\n- `/test-coverage:gaps <test-file-or-url>` - Analyze E2E test scenario gaps\n\nThe command invokes this skill to perform component-aware gap analysis on the specified test file.\n\n## See Also\n\n- [Test Coverage Plugin README](../../README.md) - User guide and installation"
              }
            ]
          },
          {
            "name": "node-tuning",
            "description": "Generate and analyze OpenShift node tuning profiles",
            "source": "./plugins/node-tuning",
            "category": null,
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install node-tuning@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-node-tuning",
                "description": "Analyze kernel/sysctl tuning from a live node or sosreport snapshot and propose NTO recommendations",
                "path": "plugins/node-tuning/commands/analyze-node-tuning.md",
                "frontmatter": {
                  "description": "Analyze kernel/sysctl tuning from a live node or sosreport snapshot and propose NTO recommendations",
                  "argument-hint": "[--sosreport PATH] [--format json|markdown] [--max-irq-samples N]"
                },
                "content": "## Name\nnode-tuning:analyze-node-tuning\n\n## Synopsis\n```text\n/node-tuning:analyze-node-tuning [--sosreport PATH] [--collect-sosreport|--no-collect-sosreport] [--sosreport-output PATH] [--node NODE] [--kubeconfig PATH] [--oc-binary PATH] [--format json|markdown] [--max-irq-samples N] [--keep-snapshot]\n```\n\n## Description\nThe `node-tuning:analyze-node-tuning` command inspects kernel tuning signals gathered from either a live OpenShift node (`/proc`, `/sys`), an `oc debug node/<name>` snapshot captured via KUBECONFIG, or an extracted sosreport directory. It parses CPU isolation parameters, IRQ affinity, huge page allocation, critical sysctl settings, and networking counters before compiling actionable recommendations that can be enforced through Tuned profiles or MachineConfig updates.\n\nUse this command when you need to:\n- Audit a node for tuning regressions after upgrades or configuration changes.\n- Translate findings into remediation steps for the Node Tuning Operator.\n- Produce JSON or Markdown reports suitable for incident response, CI gates, or documentation.\n\n## Implementation\n1. **Establish data source**\n   - Live (local) analysis: the helper script defaults to `/proc` and `/sys`. Ensure the command runs on the target node (or within an SSH session / debug pod).\n   - Remote analysis via `oc debug`: provide `--node <name>` (plus optional `--kubeconfig` and `--oc-binary`). The helper defaults to entering the RHCOS `toolbox` (backed by the `registry.redhat.io/rhel9/support-tools` image) via `oc debug node/<name>`, running `sosreport --batch --quiet -e openshift -e openshift_ovn -e openvswitch -e podman -e crio -k crio.all=on -k crio.logs=on -k podman.all=on -k podman.logs=on -k networking.ethtool-namespaces=off --all-logs --plugin-timeout=600`, streaming the archive locally (respecting `--sosreport-output` when set), and analyzing the extracted data. Use `--toolbox-image` (or `TOOLBOX_IMAGE`) to point at a mirrored support-tools image, `--sosreport-arg` to append extra flags (repeat per flag), or `--skip-default-sosreport-flags` to take full control. Host HTTP(S) proxy variables are forwarded when present but entirely optional. Add `--no-collect-sosreport` to skip sosreport generation entirely, and `--keep-snapshot` if you want to retain the downloaded files.\n   - Offline analysis: provide `--sosreport /path/to/sosreport-<timestamp>` pointing to an extracted sosreport directory; the script auto-discovers embedded `proc/` and `sys/` trees.\n   - Override non-standard layouts with `--proc-root` or `--sys-root` as needed.\n\n2. **Prepare workspace**\n   - Create `.work/node-tuning/<hostname>/` to store generated reports (remote snapshots and sosreport captures may reuse this path or default to a temporary directory).\n   - Decide whether you want Markdown (human-readable) or JSON (automation-ready) output. Set `--format json` and `--output` for machine consumption.\n\n3. **Invoke the analysis helper**\n   ```bash\n   python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n     --sosreport \"$SOS_DIR\" \\\n     --format markdown \\\n     --max-irq-samples 10 \\\n     --output \".work/node-tuning/${HOSTNAME}/analysis.md\"\n   ```\n   - Omit `--sosreport` and `--node` to evaluate the local environment.\n   - Lower `--max-irq-samples` to cap the number of IRQ affinity overlaps listed in the report.\n\n4. **Interpret results**\n   - **System Overview**: Validates kernel release, NUMA nodes, and kernel cmdline flags (isolcpus, nohz_full, tuned.non_isolcpus).\n   - **CPU & Isolation**: Highlights SMT detection, isolated CPU masks, and mismatches between default IRQ affinity and isolated cores.\n   - **Huge Pages**: Summarizes global and per-NUMA huge page pools, reserved counts, and sysctl targets.\n   - **Sysctl Highlights**: Surfaces values for tuning-critical keys (e.g., `net.core.netdev_max_backlog`, `vm.swappiness`, THP state) with recommendations when thresholds are missed.\n   - **Network Signals**: Examines `TcpExt` counters and sockstat data for backlog drops, syncookie failures, or orphaned sockets.\n   - **IRQ Affinity**: Lists IRQs overlapping isolated CPUs so you can adjust tuned profiles or irqbalance policies.\n   - **Process Snapshot**: When available in sosreport snapshots, shows top CPU consumers and flags irqbalance presence.\n\n5. **Apply remediation**\n   - Feed the recommendations into `/node-tuning:generate-tuned-profile` or MachineConfig workflows.\n   - For immediate live tuning, adjust sysctls or interrupt affinities manually, then rerun the analysis to confirm remediation.\n\n## Return Value\n- **Success**: Returns a Markdown or JSON report summarizing findings and recommended actions.\n- **Failure**: Reports descriptive errors (e.g., missing `proc/` or `sys/` directories, unreadable sosreport path) and exits non-zero.\n\n## Examples\n\n1. **Analyze a live node and print Markdown**\n   ```text\n   /node-tuning:analyze-node-tuning --format markdown\n   ```\n\n2. **Capture `/proc` and `/sys` via `oc debug` (sosreport by default) and analyze remotely**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --node worker-rt-0 \\\n     --kubeconfig ~/.kube/prod \\\n     --format markdown\n   ```\n\n3. **Collect a sosreport via `oc debug` (custom image + flags) and analyze it locally**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --node worker-rt-0 \\\n     --toolbox-image registry.example.com/support-tools:latest \\\n     --sosreport-arg \"--case-id=01234567\" \\\n     --sosreport-output .work/node-tuning/sosreports \\\n     --format json\n   ```\n\n4. **Inspect an extracted sosreport and save JSON to disk**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --sosreport ~/Downloads/sosreport-worker-001 \\\n     --format json \\\n     --max-irq-samples 20\n   ```\n\n5. **Limit the recommendation set to a handful of IRQ overlaps**\n   ```text\n   /node-tuning:analyze-node-tuning --sosreport /tmp/sosreport --max-irq-samples 5\n   ```\n\n## Arguments:\n- **--sosreport**: Path to an extracted sosreport directory to analyze instead of the live filesystem.\n- **--format**: Output format (`markdown` default or `json` for structured data).\n- **--output**: Optional file path where the helper writes the report.\n- **--max-irq-samples**: Maximum number of IRQ affinity overlaps to include in the output (default 15).\n- **--proc-root**: Override path to the procfs tree when auto-detection is insufficient.\n- **--sys-root**: Override path to the sysfs tree when auto-detection is insufficient.\n- **--node**: OpenShift node name to analyze via `oc debug node/<name>` when direct access is not possible.\n- **--kubeconfig**: Path to the kubeconfig file used for `oc debug`; relies on the current oc context when omitted.\n- **--oc-binary**: Path to the `oc` binary (defaults to `$OC_BIN` or `oc`).\n- **--keep-snapshot**: Preserve the temporary directory produced from `oc debug` (snapshots or sosreports) for later inspection.\n- **--collect-sosreport**: Trigger `sosreport` via `oc debug node/<name>`, download the archive, and analyze the extracted contents automatically (default behavior whenever `--node` is supplied and no other source is chosen).\n- **--no-collect-sosreport**: Disable the default sosreport workflow when `--node` is supplied, falling back to the raw `/proc`/`/sys` snapshot.\n- **--sosreport-output**: Directory where downloaded sosreport archives and their extraction should be placed (defaults to a temporary directory).\n- **--toolbox-image**: Override the container image that toolbox pulls when collecting sosreport (defaults to `registry.redhat.io/rhel9/support-tools:latest` or `TOOLBOX_IMAGE` env).\n- **--sosreport-arg**: Append an additional argument to the sosreport command (repeatable).\n- **--skip-default-sosreport-flags**: Do not include the default OpenShift-focused sosreport plugins/collectors; only use values supplied via `--sosreport-arg`."
              },
              {
                "name": "/generate-tuned-profile",
                "description": "Generate a Tuned (tuned.openshift.io/v1) profile manifest for the Node Tuning Operator",
                "path": "plugins/node-tuning/commands/generate-tuned-profile.md",
                "frontmatter": {
                  "description": "Generate a Tuned (tuned.openshift.io/v1) profile manifest for the Node Tuning Operator",
                  "argument-hint": "[profile-name] [--summary ...] [--sysctl ...] [options]"
                },
                "content": "## Name\nnode-tuning:generate-tuned-profile\n\n## Synopsis\n```text\n/node-tuning:generate-tuned-profile [profile-name] [--summary TEXT] [--include VALUE ...] [--sysctl KEY=VALUE ...] [--match-label KEY[=VALUE] ...] [options]\n```\n\n## Description\nThe `node-tuning:generate-tuned-profile` command streamlines creation of `tuned.openshift.io/v1` manifests for the OpenShift Node Tuning Operator. It captures the desired Tuned profile metadata, tuned daemon configuration blocks (e.g. `[sysctl]`, `[variables]`, `[bootloader]`), and recommendation rules, then invokes the helper script at `plugins/node-tuning/skills/scripts/generate_tuned_profile.py` to render a ready-to-apply YAML file.\n\nUse this command whenever you need to:\n- Bootstrap a new Tuned custom profile targeting selected nodes or machine config pools\n- Generate manifests that can be version-controlled alongside other automation\n- Iterate on sysctl, bootloader, or service parameters without hand-editing multi-line YAML\n\nThe generated manifest follows the structure expected by the cluster Node Tuning Operator:\n```\napiVersion: tuned.openshift.io/v1\nkind: Tuned\nmetadata:\n  name: <profile-name>\n  namespace: openshift-cluster-node-tuning-operator\nspec:\n  profile:\n  - data: |\n      [main]\n      summary=...\n      include=...\n      ...\n    name: <profile-name>\n  recommend:\n  - machineConfigLabels: {...}\n    match:\n    - label: ...\n      value: ...\n    priority: <priority>\n    profile: <profile-name>\n```\n\n## Implementation\n1. **Collect inputs**\n   - Confirm Python 3.8+ is available (`python3 --version`).\n   - Gather the Tuned profile name, summary, optional include chain, sysctl values, variables, and any additional section lines (e.g. `[bootloader]`, `[service]`).\n   - Determine targeting rules: either `--match-label` entries (node labels) or `--machine-config-label` entries (MachineConfigPool selectors).\n   - Decide whether an accompanying MachineConfigPool (MCP) workflow is required for kernel boot arguments (see **Advanced Workflow** below).\n   - Use the helper's `--list-nodes` and `--label-node` flags when you need to inspect or label nodes prior to manifest generation.\n\n2. **Build execution workspace**\n   - Create or reuse `.work/node-tuning/<profile-name>/`.\n   - Decide on the manifest filename (default `tuned.yaml` inside the workspace) or provide `--output` to override.\n\n3. **Invoke the generator script**\n   - Run the helper with the collected switches:\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --profile-name \"$PROFILE_NAME\" \\\n       --summary \"$SUMMARY\" \\\n       --include openshift-node \\\n       --sysctl net.core.netdev_max_backlog=16384 \\\n       --variable isolated_cores=1 \\\n       --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n       --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n       --match-label tuned.openshift.io/elasticsearch=\"\" \\\n       --priority 25 \\\n       --output \".work/node-tuning/$PROFILE_NAME/tuned.yaml\"\n     ```\n   - Use `--dry-run` to print the manifest to stdout before writing, if desired.\n\n4. **Validate output**\n   - Inspect the generated YAML (`yq e . .work/node-tuning/$PROFILE_NAME/tuned.yaml` or open in an editor).\n   - Optionally run `oc apply --server-dry-run=client -f .work/node-tuning/$PROFILE_NAME/tuned.yaml` to confirm schema compatibility.\n\n5. **Apply or distribute**\n   - Apply to a cluster with `oc apply -f .work/node-tuning/$PROFILE_NAME/tuned.yaml`.\n   - Commit the manifest to Git or attach to automated pipelines as needed.\n\n## Advanced Workflow: Huge Pages with a Dedicated MachineConfigPool\nUse this workflow when enabling huge pages or other kernel boot parameters that require coordinating the Node Tuning Operator with the Machine Config Operator while minimizing reboots.\n\n1. **Label target nodes**\n   - Preview candidates: `python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py --list-nodes --node-selector \"node-role.kubernetes.io/worker\" --skip-manifest`.\n   - Label workers with the helper (repeat per node):\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --label-node ip-10-0-1-23.ec2.internal:node-role.kubernetes.io/worker-hp= \\\n       --overwrite-labels \\\n       --skip-manifest\n     ```\n   - Alternatively run `oc label node <node> node-role.kubernetes.io/worker-hp=` directly if you prefer the CLI.\n\n2. **Generate the Tuned manifest**\n   - Include bootloader arguments via the helper script:\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --profile-name \"openshift-node-hugepages\" \\\n       --summary \"Boot time configuration for hugepages\" \\\n       --include openshift-node \\\n       --section bootloader:cmdline_openshift_node_hugepages=\"hugepagesz=2M hugepages=50\" \\\n       --machine-config-label machineconfiguration.openshift.io/role=worker-hp \\\n       --priority 30 \\\n       --output .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml\n     ```\n   - Review the `[bootloader]` section to ensure the kernel arguments match the desired configuration (e.g. `kernel-rt`, huge pages, additional sysctls).\n\n3. **Author the MachineConfigPool manifest**\n   - Create `.work/node-tuning/openshift-node-hugepages/hugepages-mcp.yaml` with:\n     ```yaml\n     apiVersion: machineconfiguration.openshift.io/v1\n     kind: MachineConfigPool\n     metadata:\n       name: worker-hp\n       labels:\n         worker-hp: \"\"\n     spec:\n       machineConfigSelector:\n         matchExpressions:\n           - key: machineconfiguration.openshift.io/role\n             operator: In\n             values:\n               - worker\n               - worker-hp\n       nodeSelector:\n         matchLabels:\n           node-role.kubernetes.io/worker-hp: \"\"\n     ```\n\n4. **Apply manifests (optional `--dry-run`)**\n   - `oc apply -f .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml`\n   - `oc apply -f .work/node-tuning/openshift-node-hugepages/hugepages-mcp.yaml`\n   - Watch progress: `oc get mcp worker-hp -w`\n\n5. **Verify results**\n   - Confirm huge page allocation after the reboot: `oc get node <node> -o jsonpath=\"{.status.allocatable.hugepages-2Mi}\"`\n   - Inspect kernel arguments: `oc debug node/<node> -q -- chroot /host cat /proc/cmdline`\n\n## Return Value\n- **Success**: Path to the generated manifest and the profile name are returned to the caller.\n- **Failure**: Script exits non-zero with stderr diagnostics (e.g. invalid `KEY=VALUE` pair, missing labels, unwritable output path).\n\n## Examples\n\n1. **Realtime worker profile targeting worker-rt MCP**\n   ```text\n   /node-tuning:generate-tuned-profile openshift-realtime \\\n     --summary \"Custom realtime tuned profile\" \\\n     --include openshift-node --include realtime \\\n     --variable isolated_cores=1 \\\n     --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n     --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n     --output .work/node-tuning/openshift-realtime/realtime.yaml\n   ```\n\n2. **Sysctl-only profile matched by node label**\n   ```text\n   /node-tuning:generate-tuned-profile custom-net-tuned \\\n     --summary \"Increase conntrack table\" \\\n     --sysctl net.netfilter.nf_conntrack_max=262144 \\\n     --match-label tuned.openshift.io/custom-net \\\n     --priority 18\n   ```\n\n3. **Preview manifest without writing to disk**\n   ```text\n   /node-tuning:generate-tuned-profile pidmax-test \\\n     --summary \"Raise pid max\" \\\n     --sysctl kernel.pid_max=131072 \\\n     --match-label tuned.openshift.io/pidmax=\"\" \\\n     --dry-run\n   ```\n\n## Arguments:\n- **$1** (`profile-name`): Name for the Tuned profile and manifest resource.\n- **--summary**: Required summary string placed in the `[main]` section.\n- **--include**: Optional include chain entries (multiple allowed).\n- **--main-option**: Additional `[main]` section key/value pairs (`KEY=VALUE`).\n- **--variable**: Add entries to the `[variables]` section (`KEY=VALUE`).\n- **--sysctl**: Add sysctl settings to the `[sysctl]` section (`KEY=VALUE`).\n- **--section**: Add lines to arbitrary sections using `SECTION:KEY=VALUE`.\n- **--machine-config-label**: MachineConfigPool selector labels (`key=value`) applied under `machineConfigLabels`.\n- **--match-label**: Node selector labels for the `recommend[].match[]` block; omit `=value` to match existence only.\n- **--priority**: Recommendation priority (integer, default 20).\n- **--namespace**: Override the manifest namespace (default `openshift-cluster-node-tuning-operator`).\n- **--output**: Destination file path; defaults to `<profile-name>.yaml` in the current directory.\n- **--dry-run**: Print manifest to stdout instead of writing to a file.\n- **--skip-manifest**: Skip manifest generation; useful when only listing or labeling nodes.\n- **--list-nodes**: List nodes via `oc get nodes` (works with `--node-selector`).\n- **--node-selector**: Label selector applied when `--list-nodes` is used.\n- **--label-node**: Apply labels to nodes using `NODE:KEY[=VALUE]` notation; repeatable.\n- **--overwrite-labels**: Allow overwriting existing labels when labeling nodes.\n- **--oc-binary**: Path to the `oc` executable (defaults to `$OC_BIN` or `oc`)."
              }
            ],
            "skills": [
              {
                "name": "Node Tuning Helper Scripts",
                "description": "Generate tuned manifests and evaluate node tuning snapshots",
                "path": "plugins/node-tuning/skills/scripts/SKILL.md",
                "frontmatter": {
                  "name": "Node Tuning Helper Scripts",
                  "description": "Generate tuned manifests and evaluate node tuning snapshots"
                },
                "content": "# Node Tuning Helper Scripts\n\nDetailed instructions for invoking the helper utilities that back `/node-tuning` commands:\n- `generate_tuned_profile.py` renders Tuned manifests (`tuned.openshift.io/v1`).\n- `analyze_node_tuning.py` inspects live nodes or sosreports for tuning gaps.\n\n## When to Use These Scripts\n- Translate structured command inputs into Tuned manifests for the Node Tuning Operator.\n- Iterate on generated YAML outside the assistant or integrate the generator into automation.\n- Analyze CPU isolation, IRQ affinity, huge pages, sysctl values, and networking counters from live clusters or archived sosreports.\n\n## Prerequisites\n- Python 3.8 or newer (`python3 --version`).\n- Repository checkout so the scripts under `plugins/node-tuning/skills/scripts/` are accessible.\n- Optional: `oc` CLI when validating or applying manifests.\n- Optional: Extracted sosreport directory when running the analysis script offline.\n- Optional (remote analysis): `oc` CLI access plus a valid `KUBECONFIG` when capturing `/proc`/`/sys` or sosreport via `oc debug node/<name>`. The sosreport workflow pulls the `registry.redhat.io/rhel9/support-tools` image (override with `--toolbox-image` or `TOOLBOX_IMAGE`) and requires registry access. HTTP(S) proxy env vars from the host are forwarded automatically when present, but using a proxy is optional.\n\n---\n\n## Script: `generate_tuned_profile.py`\n\n### Implementation Steps\n1. **Collect Inputs**\n   - `--profile-name`: Tuned resource name.\n   - `--summary`: `[main]` section summary.\n   - Repeatable options: `--include`, `--main-option`, `--variable`, `--sysctl`, `--section` (`SECTION:KEY=VALUE`).\n   - Target selectors: `--machine-config-label key=value`, `--match-label key[=value]`.\n   - Optional: `--priority` (default 20), `--namespace`, `--output`, `--dry-run`.\n   - Use `--list-nodes`/`--node-selector` to inspect nodes and `--label-node NODE:KEY[=VALUE]` (plus `--overwrite-labels`) to tag machines.\n\n2. **Inspect or Label Nodes (optional)**\n   ```bash\n   # List all worker nodes\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py --list-nodes --node-selector \"node-role.kubernetes.io/worker\" --skip-manifest\n\n   # Label a specific node for the worker-hp pool\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n     --label-node ip-10-0-1-23.ec2.internal:node-role.kubernetes.io/worker-hp= \\\n     --overwrite-labels \\\n     --skip-manifest\n   ```\n\n3. **Render the Manifest**\n   ```bash\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n     --profile-name \"$PROFILE\" \\\n     --summary \"$SUMMARY\" \\\n     --sysctl net.core.netdev_max_backlog=16384 \\\n     --match-label tuned.openshift.io/custom-net \\\n     --output .work/node-tuning/$PROFILE/tuned.yaml\n   ```\n   - Omit `--output` to write `<profile-name>.yaml` in the current directory.\n   - Add `--dry-run` to print the manifest to stdout.\n\n4. **Review Output**\n   - Inspect the generated YAML for accuracy.\n   - Optionally format with `yq` or open in an editor for readability.\n\n5. **Validate and Apply**\n   - Dry-run: `oc apply --server-dry-run=client -f <manifest>`.\n   - Apply: `oc apply -f <manifest>`.\n\n### Error Handling\n- Missing required options raise `ValueError` with descriptive messages.\n- The script exits non-zero when no target selectors (`--machine-config-label` or `--match-label`) are supplied.\n- Invalid key/value or section inputs identify the failing argument explicitly.\n\n### Examples\n```bash\npython3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n  --profile-name realtime-worker \\\n  --summary \"Realtime tuned profile\" \\\n  --include openshift-node --include realtime \\\n  --variable isolated_cores=1 \\\n  --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n  --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n  --priority 25 \\\n  --output .work/node-tuning/realtime-worker/tuned.yaml\n```\n```bash\npython3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n  --profile-name openshift-node-hugepages \\\n  --summary \"Boot time configuration for hugepages\" \\\n  --include openshift-node \\\n  --section bootloader:cmdline_openshift_node_hugepages=\"hugepagesz=2M hugepages=50\" \\\n  --machine-config-label machineconfiguration.openshift.io/role=worker-hp \\\n  --priority 30 \\\n  --output .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml\n```\n\n---\n\n## Script: `analyze_node_tuning.py`\n\n### Purpose\nInspect either a live node (`/proc`, `/sys`) or an extracted sosreport snapshot for tuning signals (CPU isolation, IRQ affinity, huge pages, sysctl state, networking counters) and emit actionable recommendations.\n\n### Usage Patterns\n- **Live node analysis**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py --format markdown\n  ```\n- **Remote analysis via oc debug**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --node worker-rt-0 \\\n    --kubeconfig ~/.kube/prod \\\n    --format markdown\n  ```\n- **Collect sosreport via oc debug and analyze locally**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --node worker-rt-0 \\\n    --toolbox-image registry.example.com/support-tools:latest \\\n    --sosreport-arg \"--case-id=01234567\" \\\n    --sosreport-output .work/node-tuning/sosreports \\\n    --format json\n  ```\n- **Offline sosreport analysis**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --sosreport /path/to/sosreport-2025-10-20\n  ```\n- **Automation-friendly JSON**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --sosreport /path/to/sosreport \\\n    --format json --output .work/node-tuning/node-analysis.json\n  ```\n\n### Implementation Steps\n1. **Select data source**\n   - Provide `--node <name>` (with optional `--kubeconfig` / `--oc-binary`). By default the helper runs `sosreport` remotely from inside the RHCOS toolbox container (`registry.redhat.io/rhel9/support-tools`). Override the image with `--toolbox-image`, extend the sosreport command with `--sosreport-arg`, or disable the curated OpenShift flags via `--skip-default-sosreport-flags`. Pass `--no-collect-sosreport` to fall back to the direct `/proc` snapshot mode.\n   - Provide `--sosreport <dir>` for archived diagnostics; detection finds embedded `proc/` and `sys/`.\n   - Omit both switches to query the live filesystem (defaults to `/proc` and `/sys`).\n   - Override paths with `--proc-root` or `--sys-root` when the layout differs.\n2. **Run analysis**\n   - The script parses `cpuinfo`, kernel cmdline parameters (`isolcpus`, `nohz_full`, `tuned.non_isolcpus`), default IRQ affinities, huge page counters, sysctl values (net, vm, kernel), transparent hugepage settings, `netstat`/`sockstat` counters, and `ps` snapshots (when available in sosreport).\n3. **Review the report**\n   - Markdown output groups findings by section (System Overview, CPU & Isolation, Huge Pages, Sysctl Highlights, Network Signals, IRQ Affinity, Process Snapshot) and lists recommendations.\n   - JSON output contains the same information in structured form for pipelines or dashboards.\n4. **Act on recommendations**\n   - Apply Tuned profiles, MachineConfig updates, or manual sysctl/irqbalance adjustments.\n   - Feed actionable items back into `/node-tuning:generate-tuned-profile` to codify desired state.\n\n### Error Handling\n- Missing `proc/` or `sys/` directories trigger descriptive errors.\n- Unreadable files are skipped gracefully and noted in observations where relevant.\n- Non-numeric sysctl values are flagged for manual investigation.\n\n### Example Output (Markdown excerpt)\n```\n# Node Tuning Analysis\n\n## System Overview\n- Hostname: worker-rt-1\n- Kernel: 4.18.0-477.el8\n- NUMA nodes: 2\n- Kernel cmdline: `BOOT_IMAGE=... isolcpus=2-15 tuned.non_isolcpus=0-1`\n\n## CPU & Isolation\n- Logical CPUs: 32\n- Physical cores: 16 across 2 socket(s)\n- SMT detected: yes\n- Isolated CPUs: 2-15\n...\n\n## Recommended Actions\n- Configure net.core.netdev_max_backlog (>=32768) to accommodate bursty NIC traffic.\n- Transparent Hugepages are not disabled (`[never]` not selected). Consider setting to `never` for latency-sensitive workloads.\n- 4 IRQs overlap isolated CPUs. Relocate interrupt affinities using tuned profiles or irqbalance.\n```\n\n### Follow-up Automation Ideas\n- Persist JSON results in `.work/node-tuning/<host>/analysis.json` for historical tracing.\n- Gate upgrades by comparing recommendations across nodes.\n- Integrate with CI jobs that validate cluster tuning post-change."
              }
            ]
          },
          {
            "name": "origin",
            "description": "Helpers for openshift/origin development.",
            "source": "./plugins/origin",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install origin@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/two-node-origin-pr-helper",
                "description": "Expert review tool for PRs that add or modify Two Node (Fencing or Arbiter) tests under test/extended/two_node/ in openshift/origin.",
                "path": "plugins/origin/commands/two-node-origin-pr-helper.md",
                "frontmatter": {
                  "description": "Expert review tool for PRs that add or modify Two Node (Fencing or Arbiter) tests under test/extended/two_node/ in openshift/origin.",
                  "argument-hint": "[--url PR_URL] [<pr>] [--depth quick|full]"
                },
                "content": "## Name\n\n/origin:two-node-origin-pr-helper  Review Two Node (Fencing/Arbiter) tests in openshift/origin.\n\n## Synopsis\n```\n/origin:two-node-origin-pr-helper [--url PR_URL] [<pr>] [--depth quick|full]\n```\n## Description\n\nThe /origin:two-node-origin-pr-helper command is an expert review tool for PRs that add or modify\nTwo Node (Fencing or Arbiter) tests under test/extended/two_node/ in openshift/origin.\n\nIt:\n\n- Discovers changed Two Node test files from the current branch.\n- Analyzes Ginkgo Describe / Context / It blocks, suite tags, and [Serial] markers.\n- Reviews test logic, structure, cleanup, and determinism.\n- Suggests reuse of existing Origin and Kubernetes helpers instead of ad-hoc code.\n- Recommends suite + [Serial] tagging and CI coverage.\n- Generates ready-to-paste PR description text for the Origin PR.\n- Suggests CI lane characteristics for openshift/release (without generating full PR text).\n\nUse this command when creating or reviewing Origin PRs that touch the Two Node test suite and you\nwant a focused, reproducible review of test design, helper usage, and CI integration.\n\nThis is a specialized Origin review helper focused on Two Node tests and is intended as a building\nblock toward a future generic Origin review command.\n\n## Implementation\n\nThe command should behave as follows.\n\n### 1. Argument handling\n\nParse arguments from the invocation:\n\n- --url:\n  - Optional full PR URL (example: <https://github.com/openshift/origin/pull/30510>)\n  - When provided, this takes precedence over any local git information.\n\n- <pr> (optional positional):\n  - Optional PR number (example: 30510)\n\n- --depth:\n  - quick: short, high-level summary\n  - full: detailed four-section output (default)\n\nDefault behavior:\n\n- If --url is provided, use that PR.\n- Else if <pr> is provided, use that PR in the current repo.\n- Else infer the PR from the current git repository remote and branch name.\n- Fail with a clear error message if the PR cannot be determined.\n\n### 2. Automatically discover relevant changes\n\nAssume the command is run inside a local checkout of the repo.\n\n- Determine changed files using git diff.\n- Filter to Go files under test/extended/two_node/.\n- Parse:\n  - Ginkgo Describe / Context / It blocks\n  - Suite tags\n  - [Serial] markers\n  - Helper imports\n\n### 3. Review test design and correctness\n\nFor each test:\n\n- Validate alignment between intent and implementation.\n- Validate degraded vs non-degraded behavior.\n- Validate fencing vs arbiter semantics.\n- Validate quorum, failover, and recovery expectations.\n\nDo not assume helper existence. Infer from imports and logic only.\n\n### 4. Suggest reuse of utilities and helpers\n\nLook for re-implemented logic where helpers already exist.\n\nExamples:\n\n- Origin utilities under github.com/openshift/origin/test/extended/util\n- Kubernetes helpers under k8s.io/apimachinery and k8s.io/utils\n\nCall out:\n\n- Correct helper usage\n- Missed reuse opportunities\n- Duplication that should become shared Two Node helpers\n\n### 5. Evaluate structure and readability\n\nReview:\n\n- Describe / Context / It hierarchy\n- By(...) usage\n- Assertion clarity\n- Avoidance of time.Sleep in favor of polling\n\n### 6. Recommend suite and Serial annotations\n\n- Prefer [Suite:openshift/two-node] for Two Node tests.\n- Recommend [Serial] for:\n  - Cluster-scoped mutations\n  - Reboots\n  - Degradation or fencing actions\n\n- Recommend parallel for isolated, namespaced tests.\n\nAlways explain why.\n\n### 7. Propose CI lane coverage\n\n- Determine if existing CI already covers the tests.\n- If not, propose:\n  - Topology\n  - TEST_SUITE\n  - Feature gates\n  - Blocking vs periodic vs optional\n\nDo not hard-code lane names.\n\n### 8. Generate ready-to-paste text\n\nProduce:\n\n- Origin PR summary text\n- Optional CI lane summary text (not a full release PR)\n\nThe command is static and requires no cluster access.\n\n---\n\n## Expected input\n\n/origin:two-node-origin-pr-helper --depth full  \n/origin:two-node-origin-pr-helper 30510 --depth full  \n/origin:two-node-origin-pr-helper --url <https://github.com/openshift/origin/pull/30510> --depth quick  \n\n---\n\n## Output structure\n\nAlways respond in four sections:\n\n1. Summary of changes  \n2. Review of tests (design, logic, reuse)  \n3. Suite, Serial, and CI recommendations  \n4. Ready-to-paste text  \n\nRespect --depth only:\n\n- quick  compact output\n- full  detailed output\n\n---\n\n## Example 1  Degraded Two Node Fencing tests\n\n/origin:two-node-origin-pr-helper 30510 --depth full\n\n---\n\n## Example 2  Two Node Arbiter recovery tests\n\n/origin:two-node-origin-pr-helper --url <https://github.com/openshift/origin/pull/XXXXX> --depth quick"
              }
            ],
            "skills": []
          },
          {
            "name": "container-image",
            "description": "Container image inspection and analysis using skopeo and podman",
            "source": "./plugins/container-image",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install container-image@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/compare",
                "description": "Compare two container images to identify differences",
                "path": "plugins/container-image/commands/compare.md",
                "frontmatter": {
                  "description": "Compare two container images to identify differences",
                  "argument-hint": "<image1> <image2>"
                },
                "content": "## Name\ncontainer-image:compare\n\n## Synopsis\n```\n/container-image:compare <image1> <image2>\n```\n\n## Description\n\nThe `container-image:compare` command compares two container images and identifies their differences. This is useful for understanding what changed between image versions, comparing images from different registries, or verifying image rebuilds.\n\nThe command analyzes and compares:\n- Image metadata (digests, creation dates)\n- Layer differences (added, removed, modified)\n- Size differences\n- Configuration changes (environment variables, labels, entrypoints)\n- Platform/architecture support\n- Security and vulnerability differences (if scanning tools available)\n\nThis command is useful for:\n- Understanding changes between image versions\n- Verifying image rebuilds match expectations\n- Comparing images across registries (e.g., production vs staging)\n- Identifying what layers changed in an update\n- Troubleshooting deployment issues\n- Security auditing and change tracking\n\n## Prerequisites\n\n**Required Tools:**\n\n1. **skopeo** - For image inspection and comparison\n   - Check if installed: `which skopeo`\n   - Installation:\n     - RHEL/Fedora: `sudo dnf install skopeo`\n     - Ubuntu/Debian: `sudo apt-get install skopeo`\n     - macOS: `brew install skopeo`\n   - Documentation: https://github.com/containers/skopeo\n\n**Optional Tools:**\n\n2. **podman** - For additional image analysis\n   - Useful for layer-by-layer comparison\n   - Installation: See `/container-image:inspect` prerequisites\n\n3. **dive** - For detailed layer analysis\n   - Check if installed: `which dive`\n   - Installation: https://github.com/wagoodman/dive\n   - Provides interactive layer comparison\n\n**Registry Authentication:**\n\nFor private registries:\n```bash\nskopeo login registry.example.com\n```\n\n## Implementation\n\nThe command performs the following comparison:\n\n1. **Check Tool Availability**:\n   - Verify `skopeo` is installed\n   - Check for optional tools (`podman`, `dive`)\n\n2. **Inspect Both Images**:\n   ```bash\n   skopeo inspect docker://<image1>\n   skopeo inspect docker://<image2>\n   ```\n\n3. **Compare Basic Metadata**:\n   - Digests (are they the same image?)\n   - Creation timestamps\n   - Architecture and OS\n   - Manifest type (single vs manifest list)\n\n4. **Analyze Layer Differences**:\n   - Extract layer digests from both images\n   - Identify:\n     - **Common layers**: Layers shared between images\n     - **Added layers**: New layers in image2\n     - **Removed layers**: Layers from image1 not in image2\n     - **Modified layers**: Layers with same position but different content\n   - Calculate size differences\n\n5. **Compare Configuration**:\n   - Environment variables (added, removed, changed)\n   - Labels and annotations\n   - Exposed ports\n   - Entrypoint and command\n   - Working directory\n   - User/UID\n   - Volume mount points\n\n6. **Calculate Size Impact**:\n   - Total size difference\n   - Size added by new layers\n   - Size saved by removed layers\n\n7. **Present Structured Comparison**:\n   - Summary of differences\n   - Detailed breakdown by category\n   - Highlight significant changes\n   - Provide recommendations\n\n## Return Value\n\nThe command outputs a structured comparison report:\n\n```\n================================================================================\nCONTAINER IMAGE COMPARISON\n================================================================================\nImage 1: quay.io/openshift-release-dev/ocp-release:4.16.0\nImage 2: quay.io/openshift-release-dev/ocp-release:4.17.0\n\nCOMPARISON SUMMARY:\n  Images are:     DIFFERENT\n  Digest match:   NO\n  Architecture:   Both linux/amd64\n\nMETADATA COMPARISON:\n  Attribute        Image 1                          Image 2                          Change\n  \n  Digest           sha256:abc123...                 sha256:def456...                 CHANGED\n  Created          2023-11-15T10:30:45Z             2024-01-15T10:30:45Z             +61 days\n  Size             1.15 GB                          1.22 GB                          +70 MB\n\nLAYER ANALYSIS:\n  Total Layers (Image 1):  15\n  Total Layers (Image 2):  17\n\n  Common Layers:    12 layers (850 MB)\n  Added Layers:     5 layers (220 MB)\n  Removed Layers:   3 layers (150 MB)\n\n  Layer Breakdown:\n   Layer 1-8:     IDENTICAL (base layers)\n  + Layer 9:       ADDED in Image 2 (45 MB)    - New component added\n  - Layer 10:      REMOVED from Image 1 (30 MB) - Old dependency removed\n   Layer 11-15:   IDENTICAL\n  + Layer 16-17:   ADDED in Image 2 (25 MB)    - Updates\n\nCONFIGURATION DIFFERENCES:\n\n  Environment Variables:\n    + OPENSHIFT_VERSION=4.17.0  (was: 4.16.0)\n    + NEW_FEATURE_FLAG=enabled  (added)\n    - DEPRECATED_FLAG=true      (removed)\n\n  Labels:\n    + io.openshift.release=4.17.0  (was: 4.16.0)\n    + io.openshift.build-date=2024-01-15  (was: 2023-11-15)\n\n  Exposed Ports:\n     8080/tcp  (unchanged)\n     8443/tcp  (unchanged)\n\n  Entrypoint:\n     [\"/usr/bin/entrypoint.sh\"]  (unchanged)\n\n  Command:\n    - [\"--legacy-mode\"]  (removed)\n    + [\"--v2-mode\"]      (added)\n\nSIGNIFICANT CHANGES:\n   Version upgrade: 4.16.0  4.17.0\n   Size increase: +70 MB (+6%)\n   5 new layers added\n   3 old layers removed\n   Command-line arguments changed\n   New feature flag enabled\n\nRECOMMENDATIONS:\n   Review changelog for 4.16.0  4.17.0 upgrade\n   Test with new command-line arguments (--v2-mode)\n   Verify NEW_FEATURE_FLAG behavior in your environment\n   Consider size impact (+70 MB) in constrained environments\n================================================================================\n```\n\n**For Identical Images:**\n```\n================================================================================\nCONTAINER IMAGE COMPARISON\n================================================================================\nImage 1: quay.io/myapp:v1.0.0\nImage 2: registry.example.com/myapp:v1.0.0\n\nCOMPARISON SUMMARY:\n  Images are:     IDENTICAL\n  Digest match:   YES (sha256:abc123...)\n\nThese images are the same, just referenced from different registries.\nNo differences found.\n================================================================================\n```\n\n## Examples\n\n1. **Compare two versions of the same image**:\n   ```\n   /container-image:compare quay.io/openshift-release-dev/ocp-release:4.16.0 quay.io/openshift-release-dev/ocp-release:4.17.0\n   ```\n   Shows what changed between OpenShift 4.16 and 4.17.\n\n2. **Compare production vs staging**:\n   ```\n   /container-image:compare registry.prod.example.com/myapp:latest registry.staging.example.com/myapp:latest\n   ```\n   Verifies staging matches production.\n\n3. **Compare images across registries**:\n   ```\n   /container-image:compare docker.io/library/nginx:1.25 quay.io/nginx/nginx:1.25\n   ```\n   Checks if images from different registries are identical.\n\n4. **Verify image rebuild**:\n   ```\n   /container-image:compare myapp:v1.0.0-original myapp:v1.0.0-rebuilt\n   ```\n   Confirms rebuild produced the same image.\n\n5. **Compare by digest**:\n   ```\n   /container-image:compare quay.io/myapp@sha256:abc123... quay.io/myapp@sha256:def456...\n   ```\n   Compares specific image versions by digest.\n\n## Error Handling\n\n- **Image not found**: Verify both image references are correct\n- **Authentication required**: Ensure you're logged into both registries\n- **Network errors**: Check connectivity to both registries\n- **Tool not available**: Provide installation instructions for `skopeo`\n- **Different architectures**: Note when comparing images for different platforms\n\n## Notes\n\n- **Digest Comparison**: If digests match, images are identical\n- **Layer Sharing**: Base layers are often shared between versions\n- **Size Calculation**: Sizes shown are compressed (as stored in registry)\n- **Semantic Versioning**: Helps identify major vs minor changes\n- **Build Reproducibility**: Identical source should produce identical digests\n- **Registry Metadata**: Some metadata may differ even if image content is identical\n\n## Advanced Usage\n\n**Compare Specific Architectures:**\n\nFor manifest lists, you can compare specific platform variants:\n```bash\n# Compare amd64 variants\n/container-image:compare quay.io/myapp:v1@sha256:<amd64-digest-v1> quay.io/myapp:v2@sha256:<amd64-digest-v2>\n```\n\n**Layer-by-Layer Analysis:**\n\nIf `dive` is installed, the command can provide interactive layer comparison:\n```bash\ndive <image1> --compare <image2>\n```\n\n## Use Cases\n\n1. **Version Upgrades**: Understand what changed before upgrading\n2. **Security Auditing**: Track changes to identify security implications\n3. **Deployment Verification**: Confirm correct image is deployed\n4. **Registry Migration**: Verify images copied between registries\n5. **Build Debugging**: Identify why builds differ\n6. **Compliance**: Document and track image changes\n\n## Arguments\n\n- **$1** (image1): Required. First image reference.\n  - Format: `[registry/]repository[:tag|@digest]`\n\n- **$2** (image2): Required. Second image reference.\n  - Format: `[registry/]repository[:tag|@digest]`\n\n**Note**: Images can be from the same or different registries."
              },
              {
                "name": "/inspect",
                "description": "Inspect and provide detailed breakdown of a container image",
                "path": "plugins/container-image/commands/inspect.md",
                "frontmatter": {
                  "description": "Inspect and provide detailed breakdown of a container image",
                  "argument-hint": "<image>"
                },
                "content": "## Name\ncontainer-image:inspect\n\n## Synopsis\n```\n/container-image:inspect <image>\n```\n\n## Description\n\nThe `container-image:inspect` command provides a comprehensive breakdown of a container image using `skopeo` and `podman`. It analyzes the image metadata, configuration, and layers to give you detailed information about the image structure, size, architecture, and contents.\n\nThis command is useful for:\n- Understanding image composition and layers\n- Verifying image architecture and OS\n- Checking image size and disk usage\n- Inspecting image labels and annotations\n- Validating image configuration\n- Troubleshooting image-related issues\n- Verifying multi-architecture image support\n- Checking which platforms are available for an image\n- Comparing platform-specific image differences\n- Planning multi-arch image builds\n\nThe command works with images from any registry (quay.io, docker.io, registry.redhat.io, etc.) and automatically detects whether an image is a manifest list (multi-architecture) or a single image, providing detailed analysis for both cases.\n\n## Prerequisites\n\n**Required Tools:**\n\n1. **skopeo** - For image inspection without pulling\n   - Check if installed: `which skopeo`\n   - Installation:\n     - RHEL/Fedora: `sudo dnf install skopeo`\n     - Ubuntu/Debian: `sudo apt-get install skopeo`\n     - macOS: `brew install skopeo`\n   - Documentation: https://github.com/containers/skopeo\n\n2. **podman** (Optional) - For additional image analysis\n   - Check if installed: `which podman`\n   - Installation:\n     - RHEL/Fedora: `sudo dnf install podman`\n     - Ubuntu/Debian: `sudo apt-get install podman`\n     - macOS: `brew install podman`\n   - Documentation: https://podman.io/\n\n**Registry Authentication:**\n\nFor private registries, ensure you're authenticated:\n```bash\n# Using skopeo\nskopeo login registry.example.com\n\n# Using podman\npodman login registry.example.com\n```\n\n## Implementation\n\nThe command performs the following analysis steps:\n\n1. **Check Tool Availability**:\n   - Verify `skopeo` is installed\n   - Check for `podman` (optional but recommended)\n   - If tools are missing, provide installation instructions\n\n2. **Inspect Image Metadata with skopeo**:\n   ```bash\n   skopeo inspect docker://<image>\n   ```\n\n   This provides:\n   - Image digest and tags\n   - Architecture and OS\n   - Layer information\n   - Creation timestamp\n   - Labels and annotations\n   - Environment variables\n   - Exposed ports\n   - Entrypoint and command\n\n3. **Determine Image Type**:\n   - Check if the image is a **manifest list** (multi-arch) or a **single image**\n   - Fetch raw manifest to determine type:\n     ```bash\n     skopeo inspect --raw docker://<image>\n     ```\n   - Parse `schemaVersion` and `mediaType` to identify:\n     - **Manifest List** (OCI Index): `application/vnd.oci.image.index.v1+json`\n     - **Manifest List** (Docker): `application/vnd.docker.distribution.manifest.list.v2+json`\n     - **Single Image** (OCI): `application/vnd.oci.image.manifest.v1+json`\n     - **Single Image** (Docker): `application/vnd.docker.distribution.manifest.v2+json`\n\n4. **Extract Manifest List Details** (if applicable):\n   - For manifest lists, extract platform information for each variant:\n     - Architecture (amd64, arm64, ppc64le, s390x, etc.)\n     - OS (linux, windows)\n     - Variant (v7, v8 for ARM)\n     - Digest of platform-specific image\n     - Size of platform-specific image\n   - Optionally inspect each platform variant:\n     ```bash\n     skopeo inspect docker://<image>@<platform-digest>\n     ```\n   - Compare platform differences:\n     - Image sizes across platforms\n     - Layer counts\n     - Creation timestamps\n     - Configuration differences\n\n5. **Analyze Image Layers**:\n   - List all layers with their sizes\n   - Calculate total image size\n   - Identify the largest layers\n   - Show layer history (if available)\n\n6. **Extract Configuration Details**:\n   - Operating system and distribution\n   - Architecture (amd64, arm64, ppc64le, s390x, etc.)\n   - Environment variables\n   - Working directory\n   - User/UID\n   - Exposed ports\n   - Volume mount points\n   - Labels (including OpenShift/Kubernetes metadata)\n\n7. **Infer Image Purpose**:\n   - Analyze image metadata to determine the likely purpose:\n     - Image name and repository patterns (e.g., \"nginx\", \"postgres\", \"ocp-release\")\n     - Labels (especially `io.openshift.*`, `io.k8s.*`, `org.opencontainers.*`)\n     - Entrypoint and command (what executable is being run)\n     - Exposed ports (common service ports)\n     - Environment variables (framework indicators, version info)\n   - Provide context about:\n     - What the image is (e.g., \"web server\", \"database\", \"operator\", \"release payload\")\n     - Common use cases\n     - Notable characteristics based on configuration\n\n8. **Present Organized Summary**:\n   - Image identity (digest, tags)\n   - Inferred purpose and context\n   - Basic information (OS, architecture, created date)\n   - Size breakdown\n   - Configuration summary\n   - Manifest list details (if applicable)\n   - Notable labels and annotations\n\n## Return Value\n\nThe command outputs a structured breakdown of the image:\n\n```\n================================================================================\nCONTAINER IMAGE INSPECTION\n================================================================================\nImage: quay.io/openshift-release-dev/ocp-release:4.20.0-multi\n\nIMAGE PURPOSE:\n  This is an OpenShift release image containing the cluster-version-operator\n  for OpenShift 4.20.0. It's part of the OpenShift release payload used to\n  manage cluster upgrades and version management.\n\nBASIC INFORMATION:\n  Manifest Digest: sha256:4f1e772349a20f2eb69e8cf70d73b4fcc299c15cb6e4f027696eb469e66d4080\n  Type:            Manifest List (Multi-Architecture)\n  Manifest Type:   Docker Distribution Manifest List v2\n  Created:         2025-10-16T13:35:26Z\n\nMANIFEST LIST DETAILS:\n  This is a multi-architecture manifest list containing 4 platform variants.\n\n  AVAILABLE PLATFORMS (4):\n  --------------------------------------------------------------------------------\n  1. linux/amd64\n     Digest:  sha256:b4bd68afe0fb47bf9876f51e33d88e9dd218fed2dcf41b025740591746dda5c9\n     Size:    167.6 MB (175,762,648 bytes)\n     Layers:  6\n     Created: 2025-10-16T13:35:26Z\n\n  2. linux/arm64\n     Digest:  sha256:eec6b0e6ff1c4cf5edc158c41a171ac8b02d7e0389715b663528a4ec0931b1f2\n     Size:    161.6 MB (169,501,175 bytes)\n     Layers:  6\n     Created: 2025-10-16T13:35:26Z\n\n  3. linux/ppc64le\n     Digest:  sha256:4bb9eb125d4d35c100699617ec8278691a9cee771ebacb113173b75f0707df56\n     Size:    174.4 MB (182,863,818 bytes)\n     Layers:  6\n     Created: 2025-10-16T13:35:26Z\n\n  4. linux/s390x\n     Digest:  sha256:5e852c796f2d3b83b3bd4506973a455a521b6933e3944740b32c1ed483b2174e\n     Size:    163.2 MB (171,055,271 bytes)\n     Layers:  6\n     Created: 2025-10-16T13:35:26Z\n\n  PLATFORM COMPARISON:\n    Size Range:      161.6 MB - 174.4 MB (arm64 smallest, ppc64le largest)\n    Size Variance:   ~12.8 MB difference between smallest and largest\n    Architectures:   4 platforms (amd64, arm64, ppc64le, s390x)\n    OS:              linux (all)\n    Layer Count:     6 (all platforms)\n    Build Time:      All platforms built simultaneously\n\n  USAGE:\n    To pull a specific platform:\n      podman pull --platform=linux/amd64 quay.io/openshift-release-dev/ocp-release:4.20.0-multi\n      podman pull quay.io/openshift-release-dev/ocp-release@sha256:b4bd68afe0fb...  # amd64\n\nCONFIGURATION (amd64 example):\n  User:           <default>\n  WorkingDir:     <default>\n  Entrypoint:     [\"/usr/bin/cluster-version-operator\"]\n  Cmd:            <none>\n  Env:\n    - PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    - BUILD_VERSION=v4.20.0\n    - OS_GIT_VERSION=4.20.0-202509230726.p2.g9de00ba.assembly.stream.el9-9de00ba\n\nEXPOSED PORTS:\n  <none>\n\nLABELS:\n  io.openshift.release: 4.20.0\n  io.openshift.release.base-image-digest: sha256:6f58f521f51ae43617d2dead1efbe9690b605d646565892bb0f8c6030a742ba7\n\nVOLUMES:\n  <none>\n\nLAYER DETAILS (amd64):\n  Total Layers: 6\n  Total Size:   167.6 MB (compressed)\n================================================================================\n```\n\n## Examples\n\n1. **Inspect a public image**:\n   ```\n   /container-image:inspect quay.io/openshift-release-dev/ocp-release:4.17.0-x86_64\n   ```\n   Provides full breakdown of the OpenShift release image.\n\n2. **Inspect a manifest list**:\n   ```\n   /container-image:inspect registry.redhat.io/ubi9/ubi:latest\n   ```\n   Shows available architectures and platform-specific details.\n\n3. **Inspect with specific tag**:\n   ```\n   /container-image:inspect docker.io/library/nginx:1.25\n   ```\n   Analyzes the nginx image with tag 1.25.\n\n4. **Inspect by digest**:\n   ```\n   /container-image:inspect quay.io/prometheus/prometheus@sha256:abc123...\n   ```\n   Inspects a specific image version by its digest.\n\n5. **Inspect a private registry image**:\n   ```\n   /container-image:inspect registry.example.com/myorg/myapp:v1.0.0\n   ```\n   Analyzes an image from a private registry (requires authentication).\n\n## Error Handling\n\n- **Image not found**: If the image doesn't exist or the name is incorrect:\n  - Verify the image name and tag\n  - Check registry accessibility\n  - Ensure authentication is set up for private registries\n\n- **Tool not available**: If `skopeo` is not installed:\n  - Display installation instructions for the user's platform\n  - Suggest using `podman inspect` as an alternative (if podman is available)\n\n- **Authentication errors**: If registry requires authentication:\n  - Prompt user to run `skopeo login <registry>` or `podman login <registry>`\n  - Provide documentation link for registry authentication\n\n- **Network errors**: If registry is unreachable:\n  - Check internet connectivity\n  - Verify registry URL is correct\n  - Check for proxy/firewall issues\n\n## Notes\n\n- **No Image Pull Required**: `skopeo inspect` fetches metadata without downloading the entire image\n- **Manifest Lists**: For multi-arch images, the command automatically detects and shows detailed platform information including per-platform digests, sizes, and configurations\n- **Manifest List vs Single Image**: The command clearly distinguishes between manifest lists and single-architecture images\n- **Platform Selection**: Container runtimes automatically select the correct platform from a manifest list\n- **Digest Pinning**: Always displays the image digest for reproducible deployments\n- **Label Standards**: Highlights important labels like OpenShift/Kubernetes metadata\n- **Size Accuracy**: Layer sizes are compressed sizes as stored in the registry\n- **Size Variations**: Platform-specific images may have different sizes due to architecture differences\n- **OCI vs Docker**: Supports both OCI and Docker manifest formats\n- **Variant Field**: ARM images may have variants (v7, v8) for different ARM versions\n- **Registry Support**: Works with any OCI-compliant registry\n\n## Arguments\n\n- **$1** (image): Required. The full image reference including registry, repository, and tag/digest.\n  - Format: `[registry/]repository[:tag|@digest]`\n  - Examples:\n    - `quay.io/openshift/origin-node:latest`\n    - `docker.io/library/alpine:3.18`\n    - `registry.redhat.io/ubi9/ubi@sha256:abc123...`"
              },
              {
                "name": "/tags",
                "description": "List and analyze available tags for a container image repository",
                "path": "plugins/container-image/commands/tags.md",
                "frontmatter": {
                  "description": "List and analyze available tags for a container image repository",
                  "argument-hint": "<repository>"
                },
                "content": "## Name\ncontainer-image:tags\n\n## Synopsis\n```\n/container-image:tags <repository>\n```\n\n## Description\n\nThe `container-image:tags` command lists and analyzes all available tags for a container image repository. It provides detailed information about each tag including creation date, size, architecture support, and digest.\n\nThis command helps you:\n- Discover available image versions\n- Identify the latest stable releases\n- Find images for specific architectures\n- Track image update frequency\n- Identify deprecated or outdated tags\n- Plan image upgrades\n- Understand tagging conventions\n\nThe command works with any OCI-compliant registry and can filter, sort, and analyze tags based on various criteria.\n\n## Prerequisites\n\n**Required Tools:**\n\n1. **skopeo** - For listing and inspecting tags\n   - Check if installed: `which skopeo`\n   - Installation:\n     - RHEL/Fedora: `sudo dnf install skopeo`\n     - Ubuntu/Debian: `sudo apt-get install skopeo`\n     - macOS: `brew install skopeo`\n   - Documentation: https://github.com/containers/skopeo\n\n**Registry Authentication:**\n\nFor private registries:\n```bash\nskopeo login registry.example.com\n```\n\n## Implementation\n\nThe command performs the following analysis:\n\n1. **Check Tool Availability**:\n   - Verify `skopeo` is installed\n   - If missing, provide installation instructions\n\n2. **List All Tags**:\n   ```bash\n   skopeo list-tags docker://<repository>\n   ```\n\n   This returns all available tags for the repository.\n\n3. **Inspect Each Tag** (for detailed analysis):\n   For each tag (or a sample of tags for large repositories):\n   ```bash\n   skopeo inspect docker://<repository>:<tag>\n   ```\n\n   Extract:\n   - Image digest\n   - Creation date\n   - Size\n   - Architecture(s)\n   - Labels\n   - Manifest type\n\n4. **Categorize Tags**:\n   - **Version tags**: Semantic versions (v1.0.0, 2.1.3)\n   - **Latest tags**: Tags like `latest`, `stable`, `production`\n   - **Date-based tags**: Tags with dates (20240115, 2024-01-15)\n   - **Branch tags**: Development branches (main, develop)\n   - **SHA tags**: Git commit SHAs\n   - **Custom tags**: Other tagging schemes\n\n5. **Sort and Filter**:\n   - Sort by creation date (newest first)\n   - Sort by semantic version\n   - Filter by pattern (e.g., only `v4.*` tags)\n   - Filter by architecture support\n   - Show only recent tags (e.g., last 30 days)\n\n6. **Identify Key Tags**:\n   - Current `latest` tag\n   - Most recent version tag\n   - Long-term support (LTS) tags\n   - Deprecated tags\n   - Duplicate tags (same digest, different names)\n\n7. **Present Organized Analysis**:\n   - Summary of tag categories\n   - Detailed tag list with metadata\n   - Recommendations for tag selection\n   - Notable patterns or issues\n\n## Return Value\n\nThe command outputs a structured tag listing:\n\n```\n================================================================================\nCONTAINER IMAGE TAGS\n================================================================================\nRepository: quay.io/openshift-release-dev/ocp-release\n\nTotal Tags: 487\n\nTAG SUMMARY:\n  Version Tags:     312  (e.g., 4.17.0, 4.16.1)\n  Date Tags:        150  (e.g., 2024-01-15)\n  Latest Tags:      3    (latest, stable, production)\n  Other Tags:       22\n\nRECENT TAGS (Last 30 days):\n--------------------------------------------------------------------------------\nTAG                          CREATED              SIZE      ARCH        DIGEST\n4.17.0                       2024-01-15 10:30     1.2 GB    multi       sha256:abc123...\n4.17.0-rc.1                  2024-01-10 08:15     1.2 GB    multi       sha256:def456...\n4.16.2                       2024-01-08 14:22     1.1 GB    multi       sha256:ghi789...\nlatest                       2024-01-15 10:30     1.2 GB    multi       sha256:abc123...\nstable                       2024-01-08 14:22     1.1 GB    multi       sha256:ghi789...\n\nVERSION TAGS (Semantic):\n--------------------------------------------------------------------------------\n4.17.0         2024-01-15  1.2 GB  multi  sha256:abc123...  [LATEST]\n4.17.0-rc.1    2024-01-10  1.2 GB  multi  sha256:def456...\n4.16.2         2024-01-08  1.1 GB  multi  sha256:ghi789...\n4.16.1         2023-12-20  1.1 GB  multi  sha256:jkl012...\n4.16.0         2023-12-01  1.1 GB  multi  sha256:mno345...\n4.15.18        2023-11-28  1.0 GB  multi  sha256:pqr678...\n...\n\nSPECIAL TAGS:\n--------------------------------------------------------------------------------\nlatest     4.17.0 (sha256:abc123...)\nstable     4.16.2 (sha256:ghi789...)\nlts        4.15.18 (sha256:pqr678...)\n\nARCHITECTURE SUPPORT:\n  Multi-arch tags: 465 (linux/amd64, linux/arm64, linux/ppc64le, linux/s390x)\n  Single-arch:     22  (linux/amd64 only)\n\nDUPLICATE TAGS (same image, multiple tags):\n  4.17.0 = latest = 2024-01-15 (sha256:abc123...)\n  4.16.2 = stable (sha256:ghi789...)\n\nTAG PATTERNS:\n   Semantic versioning (4.x.y)\n   Release candidates (-rc.x)\n   Date-based snapshots (YYYY-MM-DD)\n   Architecture-specific suffixes (-amd64, -arm64)\n\nRECOMMENDATIONS:\n   For production: Use stable (4.16.2) or specific version tag\n   For testing: Use latest (4.17.0)\n   For LTS: Use lts (4.15.18)\n   Avoid: Using generic tags like 'latest' in production\n   Pin by digest: Use @sha256:abc123... for reproducibility\n\nNOTABLE:\n   3 tags updated in the last 7 days\n   15 release candidates available\n   Average tag age: 45 days\n   Update frequency: ~2 tags per week\n================================================================================\n```\n\n**For Small Repositories:**\n```\n================================================================================\nCONTAINER IMAGE TAGS\n================================================================================\nRepository: docker.io/library/alpine\n\nTotal Tags: 47\n\nALL TAGS:\n--------------------------------------------------------------------------------\nTAG              CREATED              SIZE      ARCH        DIGEST\nlatest           2024-01-20 12:00     7.3 MB    multi       sha256:abc123...\n3.19             2024-01-20 12:00     7.3 MB    multi       sha256:abc123...\n3.18             2023-11-15 09:30     7.0 MB    multi       sha256:def456...\n3.17             2023-08-10 14:15     6.8 MB    multi       sha256:ghi789...\nedge             2024-01-22 08:00     7.5 MB    multi       sha256:jkl012...\n...\n\nRECOMMENDATIONS:\n   For production: Use 3.19 (latest stable)\n   For edge features: Use edge\n   For compatibility: Use 3.18 or 3.17\n================================================================================\n```\n\n## Examples\n\n1. **List tags for OpenShift release images**:\n   ```\n   /container-image:tags quay.io/openshift-release-dev/ocp-release\n   ```\n   Shows all available OpenShift release versions.\n\n2. **Check available UBI tags**:\n   ```\n   /container-image:tags registry.redhat.io/ubi9/ubi\n   ```\n   Lists all UBI 9 image tags.\n\n3. **Explore nginx versions**:\n   ```\n   /container-image:tags docker.io/library/nginx\n   ```\n   Shows available nginx versions and variants.\n\n4. **Check private repository tags**:\n   ```\n   /container-image:tags registry.example.com/myorg/myapp\n   ```\n   Lists tags from a private registry (requires authentication).\n\n5. **Analyze Prometheus tags**:\n   ```\n   /container-image:tags quay.io/prometheus/prometheus\n   ```\n   Shows Prometheus versions and release patterns.\n\n## Advanced Options\n\nThe command can support optional filters and sorting:\n\n**Filter by Pattern:**\n```\n/container-image:tags quay.io/openshift-release-dev/ocp-release --filter \"4.17.*\"\n```\nShows only 4.17.x tags.\n\n**Limit Results:**\n```\n/container-image:tags docker.io/library/alpine --limit 10\n```\nShows only the 10 most recent tags.\n\n**Sort Options:**\n```\n/container-image:tags quay.io/myapp --sort version   # Semantic version sort\n/container-image:tags quay.io/myapp --sort date      # Creation date sort\n/container-image:tags quay.io/myapp --sort size      # Size sort\n```\n\n**Architecture Filter:**\n```\n/container-image:tags registry.example.com/myapp --arch arm64\n```\nShows only tags that support arm64.\n\n## Error Handling\n\n- **Repository not found**: Verify repository name and registry\n- **Authentication required**: Guide user to login with `skopeo login`\n- **Network errors**: Check connectivity and registry availability\n- **Tool not available**: Provide installation instructions for `skopeo`\n- **Rate limiting**: Handle registry rate limits gracefully\n- **Large repositories**: For repositories with 1000+ tags, sample or paginate results\n\n## Notes\n\n- **Tag Mutability**: Tags (except digests) can be reassigned to different images\n- **Latest Tag**: \"latest\" doesn't always mean newest; it's just a convention\n- **Digest Pinning**: For reproducible deployments, always use digest (@sha256:...)\n- **Semantic Versioning**: Many projects follow semver (MAJOR.MINOR.PATCH)\n- **Multi-arch Support**: Check which tags support your target architecture\n- **Deprecation**: Older tags may be removed; check registry retention policies\n\n## Performance Considerations\n\nFor repositories with many tags:\n- The command samples tags rather than inspecting all\n- Full inspection can be requested with `--full` flag\n- Results can be cached for repeated queries\n- Pagination is used for very large tag lists\n\n## Use Cases\n\n1. **Version Discovery**: Find the latest stable version before deployment\n2. **Update Planning**: Identify available updates for current images\n3. **Architecture Planning**: Verify multi-arch support before migration\n4. **Cleanup Planning**: Identify old/unused tags for cleanup\n5. **Compliance**: Document available versions for audit trails\n6. **CI/CD Integration**: Automate image version selection\n7. **Troubleshooting**: Compare production tag with available versions\n\n## Arguments\n\n- **$1** (repository): Required. The repository path (without tag).\n  - Format: `[registry/]repository`\n  - Examples:\n    - `quay.io/openshift-release-dev/ocp-release`\n    - `docker.io/library/nginx`\n    - `registry.redhat.io/ubi9/ubi`\n    - `registry.example.com/myorg/myapp`\n\n**Note**: Do NOT include the tag (`:tagname`) in the repository argument."
              }
            ],
            "skills": []
          },
          {
            "name": "node",
            "description": "Kubernetes and OpenShift node health monitoring and diagnostics",
            "source": "./plugins/node",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install node@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-node-tuning",
                "description": "Analyze kernel/sysctl tuning from a live node or sosreport snapshot and propose NTO recommendations",
                "path": "plugins/node-tuning/commands/analyze-node-tuning.md",
                "frontmatter": {
                  "description": "Analyze kernel/sysctl tuning from a live node or sosreport snapshot and propose NTO recommendations",
                  "argument-hint": "[--sosreport PATH] [--format json|markdown] [--max-irq-samples N]"
                },
                "content": "## Name\nnode-tuning:analyze-node-tuning\n\n## Synopsis\n```text\n/node-tuning:analyze-node-tuning [--sosreport PATH] [--collect-sosreport|--no-collect-sosreport] [--sosreport-output PATH] [--node NODE] [--kubeconfig PATH] [--oc-binary PATH] [--format json|markdown] [--max-irq-samples N] [--keep-snapshot]\n```\n\n## Description\nThe `node-tuning:analyze-node-tuning` command inspects kernel tuning signals gathered from either a live OpenShift node (`/proc`, `/sys`), an `oc debug node/<name>` snapshot captured via KUBECONFIG, or an extracted sosreport directory. It parses CPU isolation parameters, IRQ affinity, huge page allocation, critical sysctl settings, and networking counters before compiling actionable recommendations that can be enforced through Tuned profiles or MachineConfig updates.\n\nUse this command when you need to:\n- Audit a node for tuning regressions after upgrades or configuration changes.\n- Translate findings into remediation steps for the Node Tuning Operator.\n- Produce JSON or Markdown reports suitable for incident response, CI gates, or documentation.\n\n## Implementation\n1. **Establish data source**\n   - Live (local) analysis: the helper script defaults to `/proc` and `/sys`. Ensure the command runs on the target node (or within an SSH session / debug pod).\n   - Remote analysis via `oc debug`: provide `--node <name>` (plus optional `--kubeconfig` and `--oc-binary`). The helper defaults to entering the RHCOS `toolbox` (backed by the `registry.redhat.io/rhel9/support-tools` image) via `oc debug node/<name>`, running `sosreport --batch --quiet -e openshift -e openshift_ovn -e openvswitch -e podman -e crio -k crio.all=on -k crio.logs=on -k podman.all=on -k podman.logs=on -k networking.ethtool-namespaces=off --all-logs --plugin-timeout=600`, streaming the archive locally (respecting `--sosreport-output` when set), and analyzing the extracted data. Use `--toolbox-image` (or `TOOLBOX_IMAGE`) to point at a mirrored support-tools image, `--sosreport-arg` to append extra flags (repeat per flag), or `--skip-default-sosreport-flags` to take full control. Host HTTP(S) proxy variables are forwarded when present but entirely optional. Add `--no-collect-sosreport` to skip sosreport generation entirely, and `--keep-snapshot` if you want to retain the downloaded files.\n   - Offline analysis: provide `--sosreport /path/to/sosreport-<timestamp>` pointing to an extracted sosreport directory; the script auto-discovers embedded `proc/` and `sys/` trees.\n   - Override non-standard layouts with `--proc-root` or `--sys-root` as needed.\n\n2. **Prepare workspace**\n   - Create `.work/node-tuning/<hostname>/` to store generated reports (remote snapshots and sosreport captures may reuse this path or default to a temporary directory).\n   - Decide whether you want Markdown (human-readable) or JSON (automation-ready) output. Set `--format json` and `--output` for machine consumption.\n\n3. **Invoke the analysis helper**\n   ```bash\n   python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n     --sosreport \"$SOS_DIR\" \\\n     --format markdown \\\n     --max-irq-samples 10 \\\n     --output \".work/node-tuning/${HOSTNAME}/analysis.md\"\n   ```\n   - Omit `--sosreport` and `--node` to evaluate the local environment.\n   - Lower `--max-irq-samples` to cap the number of IRQ affinity overlaps listed in the report.\n\n4. **Interpret results**\n   - **System Overview**: Validates kernel release, NUMA nodes, and kernel cmdline flags (isolcpus, nohz_full, tuned.non_isolcpus).\n   - **CPU & Isolation**: Highlights SMT detection, isolated CPU masks, and mismatches between default IRQ affinity and isolated cores.\n   - **Huge Pages**: Summarizes global and per-NUMA huge page pools, reserved counts, and sysctl targets.\n   - **Sysctl Highlights**: Surfaces values for tuning-critical keys (e.g., `net.core.netdev_max_backlog`, `vm.swappiness`, THP state) with recommendations when thresholds are missed.\n   - **Network Signals**: Examines `TcpExt` counters and sockstat data for backlog drops, syncookie failures, or orphaned sockets.\n   - **IRQ Affinity**: Lists IRQs overlapping isolated CPUs so you can adjust tuned profiles or irqbalance policies.\n   - **Process Snapshot**: When available in sosreport snapshots, shows top CPU consumers and flags irqbalance presence.\n\n5. **Apply remediation**\n   - Feed the recommendations into `/node-tuning:generate-tuned-profile` or MachineConfig workflows.\n   - For immediate live tuning, adjust sysctls or interrupt affinities manually, then rerun the analysis to confirm remediation.\n\n## Return Value\n- **Success**: Returns a Markdown or JSON report summarizing findings and recommended actions.\n- **Failure**: Reports descriptive errors (e.g., missing `proc/` or `sys/` directories, unreadable sosreport path) and exits non-zero.\n\n## Examples\n\n1. **Analyze a live node and print Markdown**\n   ```text\n   /node-tuning:analyze-node-tuning --format markdown\n   ```\n\n2. **Capture `/proc` and `/sys` via `oc debug` (sosreport by default) and analyze remotely**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --node worker-rt-0 \\\n     --kubeconfig ~/.kube/prod \\\n     --format markdown\n   ```\n\n3. **Collect a sosreport via `oc debug` (custom image + flags) and analyze it locally**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --node worker-rt-0 \\\n     --toolbox-image registry.example.com/support-tools:latest \\\n     --sosreport-arg \"--case-id=01234567\" \\\n     --sosreport-output .work/node-tuning/sosreports \\\n     --format json\n   ```\n\n4. **Inspect an extracted sosreport and save JSON to disk**\n   ```text\n   /node-tuning:analyze-node-tuning \\\n     --sosreport ~/Downloads/sosreport-worker-001 \\\n     --format json \\\n     --max-irq-samples 20\n   ```\n\n5. **Limit the recommendation set to a handful of IRQ overlaps**\n   ```text\n   /node-tuning:analyze-node-tuning --sosreport /tmp/sosreport --max-irq-samples 5\n   ```\n\n## Arguments:\n- **--sosreport**: Path to an extracted sosreport directory to analyze instead of the live filesystem.\n- **--format**: Output format (`markdown` default or `json` for structured data).\n- **--output**: Optional file path where the helper writes the report.\n- **--max-irq-samples**: Maximum number of IRQ affinity overlaps to include in the output (default 15).\n- **--proc-root**: Override path to the procfs tree when auto-detection is insufficient.\n- **--sys-root**: Override path to the sysfs tree when auto-detection is insufficient.\n- **--node**: OpenShift node name to analyze via `oc debug node/<name>` when direct access is not possible.\n- **--kubeconfig**: Path to the kubeconfig file used for `oc debug`; relies on the current oc context when omitted.\n- **--oc-binary**: Path to the `oc` binary (defaults to `$OC_BIN` or `oc`).\n- **--keep-snapshot**: Preserve the temporary directory produced from `oc debug` (snapshots or sosreports) for later inspection.\n- **--collect-sosreport**: Trigger `sosreport` via `oc debug node/<name>`, download the archive, and analyze the extracted contents automatically (default behavior whenever `--node` is supplied and no other source is chosen).\n- **--no-collect-sosreport**: Disable the default sosreport workflow when `--node` is supplied, falling back to the raw `/proc`/`/sys` snapshot.\n- **--sosreport-output**: Directory where downloaded sosreport archives and their extraction should be placed (defaults to a temporary directory).\n- **--toolbox-image**: Override the container image that toolbox pulls when collecting sosreport (defaults to `registry.redhat.io/rhel9/support-tools:latest` or `TOOLBOX_IMAGE` env).\n- **--sosreport-arg**: Append an additional argument to the sosreport command (repeatable).\n- **--skip-default-sosreport-flags**: Do not include the default OpenShift-focused sosreport plugins/collectors; only use values supplied via `--sosreport-arg`."
              },
              {
                "name": "/generate-tuned-profile",
                "description": "Generate a Tuned (tuned.openshift.io/v1) profile manifest for the Node Tuning Operator",
                "path": "plugins/node-tuning/commands/generate-tuned-profile.md",
                "frontmatter": {
                  "description": "Generate a Tuned (tuned.openshift.io/v1) profile manifest for the Node Tuning Operator",
                  "argument-hint": "[profile-name] [--summary ...] [--sysctl ...] [options]"
                },
                "content": "## Name\nnode-tuning:generate-tuned-profile\n\n## Synopsis\n```text\n/node-tuning:generate-tuned-profile [profile-name] [--summary TEXT] [--include VALUE ...] [--sysctl KEY=VALUE ...] [--match-label KEY[=VALUE] ...] [options]\n```\n\n## Description\nThe `node-tuning:generate-tuned-profile` command streamlines creation of `tuned.openshift.io/v1` manifests for the OpenShift Node Tuning Operator. It captures the desired Tuned profile metadata, tuned daemon configuration blocks (e.g. `[sysctl]`, `[variables]`, `[bootloader]`), and recommendation rules, then invokes the helper script at `plugins/node-tuning/skills/scripts/generate_tuned_profile.py` to render a ready-to-apply YAML file.\n\nUse this command whenever you need to:\n- Bootstrap a new Tuned custom profile targeting selected nodes or machine config pools\n- Generate manifests that can be version-controlled alongside other automation\n- Iterate on sysctl, bootloader, or service parameters without hand-editing multi-line YAML\n\nThe generated manifest follows the structure expected by the cluster Node Tuning Operator:\n```\napiVersion: tuned.openshift.io/v1\nkind: Tuned\nmetadata:\n  name: <profile-name>\n  namespace: openshift-cluster-node-tuning-operator\nspec:\n  profile:\n  - data: |\n      [main]\n      summary=...\n      include=...\n      ...\n    name: <profile-name>\n  recommend:\n  - machineConfigLabels: {...}\n    match:\n    - label: ...\n      value: ...\n    priority: <priority>\n    profile: <profile-name>\n```\n\n## Implementation\n1. **Collect inputs**\n   - Confirm Python 3.8+ is available (`python3 --version`).\n   - Gather the Tuned profile name, summary, optional include chain, sysctl values, variables, and any additional section lines (e.g. `[bootloader]`, `[service]`).\n   - Determine targeting rules: either `--match-label` entries (node labels) or `--machine-config-label` entries (MachineConfigPool selectors).\n   - Decide whether an accompanying MachineConfigPool (MCP) workflow is required for kernel boot arguments (see **Advanced Workflow** below).\n   - Use the helper's `--list-nodes` and `--label-node` flags when you need to inspect or label nodes prior to manifest generation.\n\n2. **Build execution workspace**\n   - Create or reuse `.work/node-tuning/<profile-name>/`.\n   - Decide on the manifest filename (default `tuned.yaml` inside the workspace) or provide `--output` to override.\n\n3. **Invoke the generator script**\n   - Run the helper with the collected switches:\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --profile-name \"$PROFILE_NAME\" \\\n       --summary \"$SUMMARY\" \\\n       --include openshift-node \\\n       --sysctl net.core.netdev_max_backlog=16384 \\\n       --variable isolated_cores=1 \\\n       --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n       --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n       --match-label tuned.openshift.io/elasticsearch=\"\" \\\n       --priority 25 \\\n       --output \".work/node-tuning/$PROFILE_NAME/tuned.yaml\"\n     ```\n   - Use `--dry-run` to print the manifest to stdout before writing, if desired.\n\n4. **Validate output**\n   - Inspect the generated YAML (`yq e . .work/node-tuning/$PROFILE_NAME/tuned.yaml` or open in an editor).\n   - Optionally run `oc apply --server-dry-run=client -f .work/node-tuning/$PROFILE_NAME/tuned.yaml` to confirm schema compatibility.\n\n5. **Apply or distribute**\n   - Apply to a cluster with `oc apply -f .work/node-tuning/$PROFILE_NAME/tuned.yaml`.\n   - Commit the manifest to Git or attach to automated pipelines as needed.\n\n## Advanced Workflow: Huge Pages with a Dedicated MachineConfigPool\nUse this workflow when enabling huge pages or other kernel boot parameters that require coordinating the Node Tuning Operator with the Machine Config Operator while minimizing reboots.\n\n1. **Label target nodes**\n   - Preview candidates: `python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py --list-nodes --node-selector \"node-role.kubernetes.io/worker\" --skip-manifest`.\n   - Label workers with the helper (repeat per node):\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --label-node ip-10-0-1-23.ec2.internal:node-role.kubernetes.io/worker-hp= \\\n       --overwrite-labels \\\n       --skip-manifest\n     ```\n   - Alternatively run `oc label node <node> node-role.kubernetes.io/worker-hp=` directly if you prefer the CLI.\n\n2. **Generate the Tuned manifest**\n   - Include bootloader arguments via the helper script:\n     ```text\n     bash\n     python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n       --profile-name \"openshift-node-hugepages\" \\\n       --summary \"Boot time configuration for hugepages\" \\\n       --include openshift-node \\\n       --section bootloader:cmdline_openshift_node_hugepages=\"hugepagesz=2M hugepages=50\" \\\n       --machine-config-label machineconfiguration.openshift.io/role=worker-hp \\\n       --priority 30 \\\n       --output .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml\n     ```\n   - Review the `[bootloader]` section to ensure the kernel arguments match the desired configuration (e.g. `kernel-rt`, huge pages, additional sysctls).\n\n3. **Author the MachineConfigPool manifest**\n   - Create `.work/node-tuning/openshift-node-hugepages/hugepages-mcp.yaml` with:\n     ```yaml\n     apiVersion: machineconfiguration.openshift.io/v1\n     kind: MachineConfigPool\n     metadata:\n       name: worker-hp\n       labels:\n         worker-hp: \"\"\n     spec:\n       machineConfigSelector:\n         matchExpressions:\n           - key: machineconfiguration.openshift.io/role\n             operator: In\n             values:\n               - worker\n               - worker-hp\n       nodeSelector:\n         matchLabels:\n           node-role.kubernetes.io/worker-hp: \"\"\n     ```\n\n4. **Apply manifests (optional `--dry-run`)**\n   - `oc apply -f .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml`\n   - `oc apply -f .work/node-tuning/openshift-node-hugepages/hugepages-mcp.yaml`\n   - Watch progress: `oc get mcp worker-hp -w`\n\n5. **Verify results**\n   - Confirm huge page allocation after the reboot: `oc get node <node> -o jsonpath=\"{.status.allocatable.hugepages-2Mi}\"`\n   - Inspect kernel arguments: `oc debug node/<node> -q -- chroot /host cat /proc/cmdline`\n\n## Return Value\n- **Success**: Path to the generated manifest and the profile name are returned to the caller.\n- **Failure**: Script exits non-zero with stderr diagnostics (e.g. invalid `KEY=VALUE` pair, missing labels, unwritable output path).\n\n## Examples\n\n1. **Realtime worker profile targeting worker-rt MCP**\n   ```text\n   /node-tuning:generate-tuned-profile openshift-realtime \\\n     --summary \"Custom realtime tuned profile\" \\\n     --include openshift-node --include realtime \\\n     --variable isolated_cores=1 \\\n     --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n     --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n     --output .work/node-tuning/openshift-realtime/realtime.yaml\n   ```\n\n2. **Sysctl-only profile matched by node label**\n   ```text\n   /node-tuning:generate-tuned-profile custom-net-tuned \\\n     --summary \"Increase conntrack table\" \\\n     --sysctl net.netfilter.nf_conntrack_max=262144 \\\n     --match-label tuned.openshift.io/custom-net \\\n     --priority 18\n   ```\n\n3. **Preview manifest without writing to disk**\n   ```text\n   /node-tuning:generate-tuned-profile pidmax-test \\\n     --summary \"Raise pid max\" \\\n     --sysctl kernel.pid_max=131072 \\\n     --match-label tuned.openshift.io/pidmax=\"\" \\\n     --dry-run\n   ```\n\n## Arguments:\n- **$1** (`profile-name`): Name for the Tuned profile and manifest resource.\n- **--summary**: Required summary string placed in the `[main]` section.\n- **--include**: Optional include chain entries (multiple allowed).\n- **--main-option**: Additional `[main]` section key/value pairs (`KEY=VALUE`).\n- **--variable**: Add entries to the `[variables]` section (`KEY=VALUE`).\n- **--sysctl**: Add sysctl settings to the `[sysctl]` section (`KEY=VALUE`).\n- **--section**: Add lines to arbitrary sections using `SECTION:KEY=VALUE`.\n- **--machine-config-label**: MachineConfigPool selector labels (`key=value`) applied under `machineConfigLabels`.\n- **--match-label**: Node selector labels for the `recommend[].match[]` block; omit `=value` to match existence only.\n- **--priority**: Recommendation priority (integer, default 20).\n- **--namespace**: Override the manifest namespace (default `openshift-cluster-node-tuning-operator`).\n- **--output**: Destination file path; defaults to `<profile-name>.yaml` in the current directory.\n- **--dry-run**: Print manifest to stdout instead of writing to a file.\n- **--skip-manifest**: Skip manifest generation; useful when only listing or labeling nodes.\n- **--list-nodes**: List nodes via `oc get nodes` (works with `--node-selector`).\n- **--node-selector**: Label selector applied when `--list-nodes` is used.\n- **--label-node**: Apply labels to nodes using `NODE:KEY[=VALUE]` notation; repeatable.\n- **--overwrite-labels**: Allow overwriting existing labels when labeling nodes.\n- **--oc-binary**: Path to the `oc` executable (defaults to `$OC_BIN` or `oc`)."
              },
              {
                "name": "/cluster-node-health-check",
                "description": "Perform comprehensive health check on cluster nodes and report kubelet, CRI-O, and node-level issues",
                "path": "plugins/node/commands/cluster-node-health-check.md",
                "frontmatter": {
                  "description": "Perform comprehensive health check on cluster nodes and report kubelet, CRI-O, and node-level issues",
                  "argument-hint": "[--node <node-name>] [--verbose] [--output-format json|text]"
                },
                "content": "## Name\nnode:cluster-node-health-check\n\n## Synopsis\n\n```\n/node:cluster-node-health-check [--node <node-name>] [--verbose] [--output-format json|text]\n```\n\n## Description\n\nThe `/node:cluster-node-health-check` command performs an extensive diagnostic of Kubernetes/OpenShift cluster nodes to assess their operational health, stability, and performance. It automates the validation of node-level components including kubelet, CRI-O container runtime, system resources, and node conditions to ensure nodes are functioning as expected.\n\nThe command runs a comprehensive set of health checks covering node status, kubelet health, container runtime (CRI-O) operations, resource utilization, system daemons, and kernel parameters. It also detects degraded states, disk/memory pressure, network issues, and recent warning events.\n\nSpecifically, it performs the following:\n\n- Detects and validates the availability of oc or kubectl CLI tools and verifies cluster connectivity.\n- Checks all node statuses (Ready, MemoryPressure, DiskPressure, PIDPressure, NetworkUnavailable) and reports any abnormal conditions with detailed reasons and messages.\n- Validates kubelet service health on each node, detecting service failures, high restart counts, and configuration issues.\n- Performs CRI-O runtime health checks to ensure container operations are functioning correctly.\n- Inspects resource utilization including CPU, memory, disk space, and process/pod counts against allocatable resources.\n- Evaluates system daemon health (systemd services) critical for node operations.\n- Examines kernel parameters and system tunables relevant to Kubernetes operations.\n- Scans for recent warning events at the node level and for pods running on nodes.\n- Reviews certificate validity for kubelet client certificates.\n- Identifies node taints, labels, and scheduling constraints that may affect workload placement.\n- Generates a clear, color-coded summary report and optionally exports findings in JSON format for automation or CI integration.\n\n## Prerequisites\n\nBefore using this command, ensure you have:\n\n1. **Kubernetes/OpenShift CLI**: Either `oc` (OpenShift) or `kubectl` (Kubernetes)\n   - Install `oc` from: <https://mirror.openshift.com/pub/openshift-v4/clients/ocp/>\n   - Or install `kubectl` from: <https://kubernetes.io/docs/tasks/tools/>\n   - Verify with: `oc version` or `kubectl version`\n\n2. **Active cluster connection**: Must be connected to a running cluster\n   - Verify with: `oc whoami` or `kubectl cluster-info`\n   - Ensure KUBECONFIG is set if needed\n\n3. **Sufficient permissions**: Must have read access to cluster resources\n   - Cluster-admin or monitoring role recommended for comprehensive checks\n   - Minimum: ability to view nodes, pods, and node metrics\n   - For node debugging (accessing journalctl, crictl): ability to create debug pods or ssh access\n\n## Arguments\n\n- **--node** (optional): Name of a specific node to check. If not provided, checks all nodes in the cluster. Example: `--node ip-10-0-1-23.ec2.internal`\n\n- **--verbose** (optional): Enable detailed output with additional context\n  - Shows resource-level details\n  - Includes warning conditions\n  - Provides remediation suggestions\n\n- **--output-format** (optional): Output format for results\n  - `text` (default): Human-readable text format\n  - `json`: Machine-readable JSON format for automation\n\n## Implementation\n\nThe command performs the following health checks:\n\n### 1. Determine CLI Tool and Verify Connectivity\n\nDetect which Kubernetes CLI is available and verify cluster connection:\n\n```bash\nif command -v oc &> /dev/null; then\n    CLI=\"oc\"\nelif command -v kubectl &> /dev/null; then\n    CLI=\"kubectl\"\nelse\n    echo \"Error: Neither 'oc' nor 'kubectl' CLI found. Please install one of them.\"\n    exit 1\nfi\n\n# Verify cluster connectivity\nif ! $CLI cluster-info &> /dev/null; then\n    echo \"Error: Not connected to a cluster. Please configure your KUBECONFIG.\"\n    exit 1\nfi\n```\n\n### 2. Initialize Health Check Report\n\nCreate a report structure to collect findings:\n\n```bash\nNODE_FILTER=${NODE_FILTER:-\"\"}\nVERBOSE=${VERBOSE:-false}\nOUTPUT_FORMAT=${OUTPUT_FORMAT:-\"text\"}\n\nREPORT_FILE=\".work/node-health-check/report-$(date +%Y%m%d-%H%M%S).txt\"\nmkdir -p .work/node-health-check\n\n# Initialize counters\nCRITICAL_ISSUES=0\nWARNING_ISSUES=0\nINFO_MESSAGES=0\n```\n\n### 3. Check Node Status and Conditions\n\nVerify node health and readiness:\n\n```bash\necho \"Checking Node Status...\"\n\n# Get all nodes or specific node\nif [ -n \"$NODE_FILTER\" ]; then\n    NODES=$NODE_FILTER\nelse\n    NODES=$($CLI get nodes -o jsonpath='{.items[*].metadata.name}')\nfi\n\nfor node in $NODES; do\n    echo \"  Checking node: $node\"\n\n    # Check if node exists\n    if ! $CLI get node \"$node\" &> /dev/null; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Node '$node' not found\"\n        continue\n    fi\n\n    # Get node conditions\n    NODE_READY=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"Ready\") | .status')\n    NODE_MEMORY_PRESSURE=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"MemoryPressure\") | .status')\n    NODE_DISK_PRESSURE=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"DiskPressure\") | .status')\n    NODE_PID_PRESSURE=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"PIDPressure\") | .status')\n    NODE_NETWORK=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"NetworkUnavailable\") | .status // \"False\"')\n\n    if [ \"$NODE_READY\" != \"True\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Node $node is not Ready\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"Ready\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n    fi\n\n    if [ \"$NODE_MEMORY_PRESSURE\" == \"True\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Node $node has MemoryPressure\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"MemoryPressure\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n    fi\n\n    if [ \"$NODE_DISK_PRESSURE\" == \"True\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Node $node has DiskPressure\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"DiskPressure\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n    fi\n\n    if [ \"$NODE_PID_PRESSURE\" == \"True\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: Node $node has PIDPressure\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"PIDPressure\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n    fi\n\n    if [ \"$NODE_NETWORK\" == \"True\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Node $node has NetworkUnavailable\"\n        $CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"NetworkUnavailable\") | \"    Reason: \\(.reason)\\n    Message: \\(.message)\"'\n    fi\n\n    # Check node version and kubelet version\n    KUBELET_VERSION=$($CLI get node \"$node\" -o json | jq -r '.status.nodeInfo.kubeletVersion')\n    echo \"      Kubelet version: $KUBELET_VERSION\"\n\n    # Check for taints\n    TAINTS=$($CLI get node \"$node\" -o json | jq -r '.spec.taints // [] | length')\n    if [ \"$TAINTS\" -gt 0 ]; then\n        echo \"      Node has $TAINTS taint(s)\"\n        $CLI get node \"$node\" -o json | jq -r '.spec.taints[] | \"      - \\(.key)=\\(.value):\\(.effect)\"'\n    fi\ndone\n```\n\n### 4. Check Kubelet Service Health\n\nExamine kubelet service status on each node using debug pods:\n\n```bash\necho \"Checking Kubelet Service Health...\"\n\nfor node in $NODES; do\n    echo \"  Checking kubelet on node: $node\"\n\n    # Use debug pod to check kubelet service\n    KUBELET_STATUS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl is-active kubelet 2>/dev/null || echo \"failed\")\n\n    if [ \"$KUBELET_STATUS\" != \"active\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: Kubelet service is not active on node $node (Status: $KUBELET_STATUS)\"\n\n        # Get kubelet logs for troubleshooting\n        if [ \"$VERBOSE\" = true ]; then\n            echo \"    Recent kubelet logs:\"\n            $CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host journalctl -u kubelet -n 20 --no-pager 2>/dev/null\n        fi\n    else\n        # Check for kubelet restarts\n        KUBELET_RESTART_COUNT=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl show kubelet -p NRestarts --value 2>/dev/null || echo \"0\")\n\n        if [ \"$KUBELET_RESTART_COUNT\" -gt 3 ]; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: Kubelet has restarted $KUBELET_RESTART_COUNT times on node $node\"\n        fi\n    fi\n\n    # Check kubelet certificate expiration\n    CERT_EXPIRY=$($CLI get node \"$node\" -o json | jq -r '.status.conditions[] | select(.type==\"Ready\") | .message' | grep -i \"certificate\" || echo \"\")\n    if [ -n \"$CERT_EXPIRY\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: Certificate issue on node $node: $CERT_EXPIRY\"\n    fi\ndone\n```\n\n### 5. Check CRI-O Container Runtime Health\n\nVerify CRI-O runtime health and operations:\n\n```bash\necho \"Checking CRI-O Container Runtime...\"\n\nfor node in $NODES; do\n    echo \"  Checking CRI-O on node: $node\"\n\n    # Check crio service status\n    CRIO_STATUS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl is-active crio 2>/dev/null || echo \"failed\")\n\n    if [ \"$CRIO_STATUS\" != \"active\" ]; then\n        CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n        echo \" CRITICAL: CRI-O service is not active on node $node (Status: $CRIO_STATUS)\"\n\n        if [ \"$VERBOSE\" = true ]; then\n            echo \"    Recent CRI-O logs:\"\n            $CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host journalctl -u crio -n 20 --no-pager 2>/dev/null\n        fi\n    else\n        # Check CRI-O version\n        CRIO_VERSION=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host crictl version -o json 2>/dev/null | jq -r '.runtimeVersion // \"unknown\"')\n        echo \"      CRI-O version: $CRIO_VERSION\"\n\n        # Check for container runtime errors\n        RUNTIME_ERRORS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host journalctl -u crio --since \"1 hour ago\" -p err --no-pager 2>/dev/null | wc -l)\n\n        if [ \"$RUNTIME_ERRORS\" -gt 10 ]; then\n            WARNING_ISSUES=$((WARNING_ISSUES + 1))\n            echo \"  WARNING: CRI-O has $RUNTIME_ERRORS errors in the last hour on node $node\"\n        fi\n    fi\ndone\n```\n\n### 6. Check Node Resource Utilization\n\nVerify resource usage against allocatable capacity:\n\n```bash\necho \"Checking Node Resource Utilization...\"\n\nfor node in $NODES; do\n    echo \"  Checking resources on node: $node\"\n\n    # Get allocatable and capacity\n    CPU_CAPACITY=$($CLI get node \"$node\" -o json | jq -r '.status.capacity.cpu')\n    CPU_ALLOCATABLE=$($CLI get node \"$node\" -o json | jq -r '.status.allocatable.cpu')\n    MEMORY_CAPACITY=$($CLI get node \"$node\" -o json | jq -r '.status.capacity.memory')\n    MEMORY_ALLOCATABLE=$($CLI get node \"$node\" -o json | jq -r '.status.allocatable.memory')\n    PODS_CAPACITY=$($CLI get node \"$node\" -o json | jq -r '.status.capacity.pods')\n\n    # Get current pod count\n    POD_COUNT=$($CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\" --no-headers 2>/dev/null | wc -l)\n\n    echo \"    CPU: $CPU_ALLOCATABLE/$CPU_CAPACITY allocatable\"\n    echo \"    Memory: $MEMORY_ALLOCATABLE/$MEMORY_CAPACITY allocatable\"\n    echo \"    Pods: $POD_COUNT/$PODS_CAPACITY\"\n\n    # Check if pod count is near capacity\n    if [ \"$POD_COUNT\" -ge \"$((PODS_CAPACITY * 90 / 100))\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: Node $node is running $POD_COUNT pods (near capacity of $PODS_CAPACITY)\"\n    fi\n\n    # Check disk usage\n    if [ \"$VERBOSE\" = true ]; then\n        echo \"    Disk usage:\"\n        $CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host df -h / /var /var/lib/kubelet /var/lib/containers 2>/dev/null | grep -v \"Filesystem\"\n    fi\n\n    # Check ephemeral storage pressure\n    EPHEMERAL_STORAGE=$($CLI get node \"$node\" -o json | jq -r '.status.allocatable.\"ephemeral-storage\" // \"unknown\"')\n    if [ \"$EPHEMERAL_STORAGE\" != \"unknown\" ]; then\n        echo \"    Ephemeral Storage: $EPHEMERAL_STORAGE allocatable\"\n    fi\ndone\n```\n\n### 7. Check System Daemons and Services\n\nValidate critical system services:\n\n```bash\necho \"Checking System Daemons...\"\n\nCRITICAL_SERVICES=\"kubelet crio\"\n\nfor node in $NODES; do\n    echo \"  Checking system services on node: $node\"\n\n    for service in $CRITICAL_SERVICES; do\n        SERVICE_STATUS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl is-active \"$service\" 2>/dev/null || echo \"failed\")\n\n        if [ \"$SERVICE_STATUS\" != \"active\" ]; then\n            CRITICAL_ISSUES=$((CRITICAL_ISSUES + 1))\n            echo \" CRITICAL: Service $service is not active on node $node\"\n        fi\n    done\n\n    # Check for failed systemd units\n    FAILED_UNITS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl list-units --state=failed --no-pager --no-legend 2>/dev/null | wc -l)\n\n    if [ \"$FAILED_UNITS\" -gt 0 ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: Node $node has $FAILED_UNITS failed systemd unit(s)\"\n\n        if [ \"$VERBOSE\" = true ]; then\n            $CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host systemctl list-units --state=failed --no-pager 2>/dev/null\n        fi\n    fi\ndone\n```\n\n### 8. Check Kernel Parameters and System Tunables\n\nVerify important kernel parameters for Kubernetes:\n\n```bash\necho \"Checking Kernel Parameters...\"\n\nfor node in $NODES; do\n    if [ \"$VERBOSE\" = true ]; then\n        echo \"  Checking kernel parameters on node: $node\"\n\n        # Check key sysctl parameters\n        echo \"    Key sysctl parameters:\"\n        $CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host sysctl -a 2>/dev/null | grep -E \"(vm.overcommit_memory|vm.panic_on_oom|kernel.panic|kernel.panic_on_oops|net.ipv4.ip_forward)\" || true\n\n        # Check SELinux status\n        SELINUX_STATUS=$($CLI debug node/\"$node\" --image=registry.access.redhat.com/ubi9/ubi-minimal -- chroot /host getenforce 2>/dev/null || echo \"unknown\")\n        echo \"    SELinux: $SELINUX_STATUS\"\n    fi\ndone\n```\n\n### 9. Check Recent Node Events\n\nLook for recent warning/error events:\n\n```bash\necho \"Checking Recent Node Events...\"\n\nfor node in $NODES; do\n    # Get recent warning events for the node\n    NODE_EVENTS=$($CLI get events --all-namespaces --field-selector involvedObject.name=\"$node\",involvedObject.kind=Node,type=Warning --sort-by='.lastTimestamp' 2>/dev/null | tail -10)\n\n    if [ -n \"$NODE_EVENTS\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  Recent Warning Events for Node $node:\"\n        echo \"$NODE_EVENTS\"\n    fi\n\n    # Get recent pod events on this node\n    POD_EVENTS=$($CLI get events --all-namespaces --field-selector spec.nodeName=\"$node\",type=Warning --sort-by='.lastTimestamp' 2>/dev/null | tail -10)\n\n    if [ -n \"$POD_EVENTS\" ] && [ \"$VERBOSE\" = true ]; then\n        echo \"  Recent Pod Warning Events on Node $node:\"\n        echo \"$POD_EVENTS\"\n    fi\ndone\n```\n\n### 10. Check Pod Status on Nodes\n\nVerify pods running on each node:\n\n```bash\necho \"Checking Pods on Nodes...\"\n\nfor node in $NODES; do\n    echo \"  Checking pods on node: $node\"\n\n    # Count pods by phase\n    RUNNING_PODS=$($CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\",status.phase=Running --no-headers 2>/dev/null | wc -l)\n    PENDING_PODS=$($CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\",status.phase=Pending --no-headers 2>/dev/null | wc -l)\n    FAILED_PODS=$($CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\",status.phase=Failed --no-headers 2>/dev/null | wc -l)\n\n    echo \"    Running: $RUNNING_PODS, Pending: $PENDING_PODS, Failed: $FAILED_PODS\"\n\n    if [ \"$FAILED_PODS\" -gt 0 ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + 1))\n        echo \"  WARNING: Node $node has $FAILED_PODS failed pod(s)\"\n\n        if [ \"$VERBOSE\" = true ]; then\n            $CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\",status.phase=Failed --no-headers\n        fi\n    fi\n\n    # Check for pods with high restart counts\n    HIGH_RESTART_PODS=$($CLI get pods --all-namespaces --field-selector spec.nodeName=\"$node\" -o json 2>/dev/null | jq -r '.items[] | select(.status.containerStatuses[]? | .restartCount > 5) | \"\\(.metadata.namespace)/\\(.metadata.name) [Restarts: \\(.status.containerStatuses[0].restartCount)]\"')\n\n    if [ -n \"$HIGH_RESTART_PODS\" ]; then\n        WARNING_ISSUES=$((WARNING_ISSUES + $(echo \"$HIGH_RESTART_PODS\" | wc -l)))\n        echo \"  WARNING: Pods with high restart count (>5) on node $node:\"\n        echo \"$HIGH_RESTART_PODS\" | while read pod; do\n            echo \"    - $pod\"\n        done\n    fi\ndone\n```\n\n### 11. Check Node Labels and Roles\n\nVerify node labels and role assignments:\n\n```bash\necho \"Checking Node Labels and Roles...\"\n\nfor node in $NODES; do\n    if [ \"$VERBOSE\" = true ]; then\n        echo \"  Node: $node\"\n\n        # Get node roles\n        ROLES=$($CLI get node \"$node\" -o json | jq -r '.metadata.labels | to_entries[] | select(.key | startswith(\"node-role.kubernetes.io/\")) | .key' | sed 's/node-role.kubernetes.io\\///' | tr '\\n' ',' | sed 's/,$//')\n        echo \"    Roles: ${ROLES:-none}\"\n\n        # Check for custom labels\n        CUSTOM_LABELS=$($CLI get node \"$node\" -o json | jq -r '.metadata.labels | to_entries[] | select(.key | startswith(\"node-role.kubernetes.io/\") | not) | \"\\(.key)=\\(.value)\"' | head -5)\n        if [ -n \"$CUSTOM_LABELS\" ]; then\n            echo \"    Custom labels:\"\n            echo \"$CUSTOM_LABELS\" | while read label; do\n                echo \"      - $label\"\n            done\n        fi\n    fi\ndone\n```\n\n### 12. Generate Summary Report\n\nCreate a summary of findings:\n\n```bash\necho \"\"\necho \"===============================================\"\necho \"Node Health Check Summary\"\necho \"===============================================\"\necho \"Check Time: $(date)\"\necho \"Nodes Checked: $(echo $NODES | wc -w)\"\necho \"\"\necho \"Results:\"\necho \"  Critical Issues: $CRITICAL_ISSUES\"\necho \"  Warnings: $WARNING_ISSUES\"\necho \"\"\n\nif [ $CRITICAL_ISSUES -eq 0 ] && [ $WARNING_ISSUES -eq 0 ]; then\n    echo \" OVERALL STATUS: HEALTHY - No issues detected\"\n    exit 0\nelif [ $CRITICAL_ISSUES -gt 0 ]; then\n    echo \" OVERALL STATUS: CRITICAL - Immediate attention required\"\n    exit 1\nelse\n    echo \"  OVERALL STATUS: WARNING - Monitoring recommended\"\n    exit 0\nfi\n```\n\n### 13. Optional: Export to JSON Format\n\nIf `--output-format json` is specified, export findings as JSON:\n\n```json\n{\n  \"cluster\": {\n    \"checkTime\": \"2025-11-27T12:00:00Z\",\n    \"nodesChecked\": 3\n  },\n  \"summary\": {\n    \"criticalIssues\": 0,\n    \"warnings\": 2,\n    \"overallStatus\": \"WARNING\"\n  },\n  \"nodes\": [\n    {\n      \"name\": \"worker-0\",\n      \"status\": {\n        \"ready\": true,\n        \"memoryPressure\": false,\n        \"diskPressure\": false,\n        \"pidPressure\": false,\n        \"networkUnavailable\": false\n      },\n      \"kubelet\": {\n        \"version\": \"v1.28.5\",\n        \"status\": \"active\",\n        \"restartCount\": 0\n      },\n      \"crio\": {\n        \"version\": \"1.28.2\",\n        \"status\": \"active\",\n        \"errorCount\": 0\n      },\n      \"resources\": {\n        \"cpu\": {\n          \"capacity\": \"4\",\n          \"allocatable\": \"3800m\"\n        },\n        \"memory\": {\n          \"capacity\": \"16Gi\",\n          \"allocatable\": \"15Gi\"\n        },\n        \"pods\": {\n          \"running\": 25,\n          \"capacity\": 110\n        }\n      },\n      \"issues\": []\n    }\n  ]\n}\n```\n\n## Examples\n\n### Example 1: Check all nodes in the cluster\n```bash\n/node:cluster-node-health-check\n```\n\nOutput, for healthy cluster:\n```text\nNode Health Check Summary\n================================================================================\n\nOVERALL STATUS:  HEALTHY\n\nNODE STATUS:\n worker-0: Ready (Kubelet v1.28.5, CRI-O 1.28.2)\n   - CPU: 3800m/4 allocatable, Memory: 15Gi/16Gi\n   - Pods: 25/110\n worker-1: Ready (Kubelet v1.28.5, CRI-O 1.28.2)\n   - CPU: 3800m/4 allocatable, Memory: 15Gi/16Gi\n   - Pods: 28/110\n worker-2: Ready (Kubelet v1.28.5, CRI-O 1.28.2)\n   - CPU: 3800m/4 allocatable, Memory: 15Gi/16Gi\n   - Pods: 22/110\n\nNo critical issues found. All nodes are operating normally.\n\nDIAGNOSTIC COMMANDS:\nRun these commands for detailed information:\n\nkubectl get nodes -o wide\nkubectl describe node <node-name>\nkubectl top nodes\n```\n\nOutput, with issues:\n```text\nNode Health Check Summary\n================================================================================\n\nOVERALL STATUS:   WARNING\n\nNODE STATUS:\n worker-0: Ready\n  worker-1: Ready (with warnings)\n worker-2: Not Ready\n\nISSUES FOUND:\n\n[CRITICAL] Node Not Ready\n- Node: worker-2\n- Condition: Ready=False\n- Reason: KubeletNotReady\n- Message: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\n- Impact: Node cannot schedule new pods\n- Recommended Action:\n  kubectl describe node worker-2\n  kubectl debug node/worker-2 -- chroot /host journalctl -u kubelet -n 50\n\n[WARNING] High Pod Restart Count\n- Node: worker-1\n- Pod: openshift-monitoring/prometheus-k8s-0 [Restarts: 8]\n- Impact: May indicate application or resource issues\n- Recommended Action:\n  kubectl logs -n openshift-monitoring prometheus-k8s-0 --previous\n  kubectl describe pod -n openshift-monitoring prometheus-k8s-0\n\n[WARNING] Disk Pressure\n- Node: worker-1\n- Condition: DiskPressure=True\n- Message: ephemeral-storage usage(92%) exceeds the threshold(85%)\n- Impact: Pods may be evicted to free disk space\n- Recommended Action:\n  kubectl debug node/worker-1 -- chroot /host df -h\n  kubectl get pods -A --field-selector spec.nodeName=worker-1 -o json | jq '.items[] | {name:.metadata.name, ephemeralStorage:.spec.containers[].resources.requests.\"ephemeral-storage\"}'\n\nDIAGNOSTIC COMMANDS:\n\n1. Check node details:\n   kubectl get nodes -o wide\n   kubectl describe node worker-2\n\n2. Check kubelet logs:\n   kubectl debug node/worker-2 -- chroot /host journalctl -u kubelet -n 100\n\n3. Check CRI-O logs:\n   kubectl debug node/worker-2 -- chroot /host journalctl -u crio -n 100\n\n4. Check node resource usage:\n   kubectl top nodes\n```\n\n### Example 2: Check a specific node\n```bash\n/node:cluster-node-health-check --node worker-1\n```\n\n### Example 3: Verbose health check with detailed output\n```bash\n/node:cluster-node-health-check --verbose\n```\n\n### Example 4: JSON output for automation\n```bash\n/node:cluster-node-health-check --output-format json\n```\n\n## Return Value\n\nThe command returns a structured health report containing:\n\n- **OVERALL STATUS**: Health summary (Healthy  / Warning  / Critical )\n- **NODE STATUS**: Status of each checked node with visual indicators\n- **ISSUES FOUND**: Detailed list of problems with:\n  - Severity level (Critical/Warning/Info)\n  - Node location\n  - Impact assessment\n  - Recommended actions\n- **DIAGNOSTIC COMMANDS**: kubectl commands for further investigation\n\n## Common Issues and Remediation\n\n### Node Not Ready\n\n**Symptoms**: Node showing Ready=False\n\n**Investigation**:\n```bash\nkubectl describe node <node-name>\nkubectl debug node/<node-name> -- chroot /host journalctl -u kubelet -n 100\n```\n\n**Remediation**: Common causes include:\n- Kubelet service failure\n- Container runtime (CRI-O) issues\n- Network plugin not initialized\n- Certificate expiration\n\n### Kubelet Service Failures\n\n**Symptoms**: Kubelet service not active or frequently restarting\n\n**Investigation**:\n```bash\nkubectl debug node/<node-name> -- chroot /host systemctl status kubelet\nkubectl debug node/<node-name> -- chroot /host journalctl -u kubelet --since \"1 hour ago\"\n```\n\n**Remediation**: Check kubelet logs for configuration errors, certificate issues, or API server connectivity problems\n\n### CRI-O Runtime Issues\n\n**Symptoms**: Pods failing to start, container runtime errors\n\n**Investigation**:\n```bash\nkubectl debug node/<node-name> -- chroot /host systemctl status crio\nkubectl debug node/<node-name> -- chroot /host journalctl -u crio -p err --since \"1 hour ago\"\nkubectl debug node/<node-name> -- chroot /host crictl ps -a\n```\n\n**Remediation**: Check for CRI-O configuration issues, storage problems, or network misconfigurations\n\n### Memory/Disk Pressure\n\n**Symptoms**: MemoryPressure=True or DiskPressure=True\n\n**Investigation**:\n```bash\nkubectl describe node <node-name>\nkubectl debug node/<node-name> -- chroot /host df -h\nkubectl debug node/<node-name> -- chroot /host free -h\nkubectl top node <node-name>\nkubectl top pods --all-namespaces --field-selector spec.nodeName=<node-name>\n```\n\n**Remediation**:\n- Increase node resources\n- Clean up unused images: `crictl rmi --prune`\n- Evict or delete unnecessary pods\n- Check for pod resource limits\n\n### Network Unavailable\n\n**Symptoms**: NetworkUnavailable=True\n\n**Investigation**:\n```bash\nkubectl describe node <node-name>\nkubectl get pods -n openshift-multus -o wide\nkubectl get pods -n openshift-sdn -o wide  # or openshift-ovn-kubernetes\n```\n\n**Remediation**: Check CNI plugin status, network operator logs, and node network configuration\n\n### Certificate Expiration\n\n**Symptoms**: Certificate warnings in kubelet logs\n\n**Investigation**:\n```bash\nkubectl debug node/<node-name> -- chroot /host openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -noout -dates\n```\n\n**Remediation**: Rotate kubelet certificates (automatic in most cases, manual intervention may be needed for expired certs)\n\n## Security Considerations\n\n- **Read-only access**: This command primarily reads cluster state, but uses debug pods for node-level inspection\n- **Debug pods**: Creates temporary debug pods with host access for system-level checks\n- **Sensitive data**: Node logs and system information may contain sensitive data\n- **RBAC requirements**: Ensure user has appropriate permissions for nodes, pods, and debug pod creation\n\n## Notes\n\n- This command requires appropriate RBAC permissions to view nodes, pods, and create debug pods\n- Debug pods are automatically cleaned up after inspection\n- Some checks require debug pod creation which may not work in all cluster configurations\n- For OpenShift clusters, some checks leverage OpenShift-specific features\n- The command provides diagnostic guidance but does not automatically remediate issues\n- Regular health checks help catch node issues before they impact workloads\n- Node-level issues can cascade to affect pod scheduling and cluster capacity"
              }
            ],
            "skills": [
              {
                "name": "Node Tuning Helper Scripts",
                "description": "Generate tuned manifests and evaluate node tuning snapshots",
                "path": "plugins/node-tuning/skills/scripts/SKILL.md",
                "frontmatter": {
                  "name": "Node Tuning Helper Scripts",
                  "description": "Generate tuned manifests and evaluate node tuning snapshots"
                },
                "content": "# Node Tuning Helper Scripts\n\nDetailed instructions for invoking the helper utilities that back `/node-tuning` commands:\n- `generate_tuned_profile.py` renders Tuned manifests (`tuned.openshift.io/v1`).\n- `analyze_node_tuning.py` inspects live nodes or sosreports for tuning gaps.\n\n## When to Use These Scripts\n- Translate structured command inputs into Tuned manifests for the Node Tuning Operator.\n- Iterate on generated YAML outside the assistant or integrate the generator into automation.\n- Analyze CPU isolation, IRQ affinity, huge pages, sysctl values, and networking counters from live clusters or archived sosreports.\n\n## Prerequisites\n- Python 3.8 or newer (`python3 --version`).\n- Repository checkout so the scripts under `plugins/node-tuning/skills/scripts/` are accessible.\n- Optional: `oc` CLI when validating or applying manifests.\n- Optional: Extracted sosreport directory when running the analysis script offline.\n- Optional (remote analysis): `oc` CLI access plus a valid `KUBECONFIG` when capturing `/proc`/`/sys` or sosreport via `oc debug node/<name>`. The sosreport workflow pulls the `registry.redhat.io/rhel9/support-tools` image (override with `--toolbox-image` or `TOOLBOX_IMAGE`) and requires registry access. HTTP(S) proxy env vars from the host are forwarded automatically when present, but using a proxy is optional.\n\n---\n\n## Script: `generate_tuned_profile.py`\n\n### Implementation Steps\n1. **Collect Inputs**\n   - `--profile-name`: Tuned resource name.\n   - `--summary`: `[main]` section summary.\n   - Repeatable options: `--include`, `--main-option`, `--variable`, `--sysctl`, `--section` (`SECTION:KEY=VALUE`).\n   - Target selectors: `--machine-config-label key=value`, `--match-label key[=value]`.\n   - Optional: `--priority` (default 20), `--namespace`, `--output`, `--dry-run`.\n   - Use `--list-nodes`/`--node-selector` to inspect nodes and `--label-node NODE:KEY[=VALUE]` (plus `--overwrite-labels`) to tag machines.\n\n2. **Inspect or Label Nodes (optional)**\n   ```bash\n   # List all worker nodes\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py --list-nodes --node-selector \"node-role.kubernetes.io/worker\" --skip-manifest\n\n   # Label a specific node for the worker-hp pool\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n     --label-node ip-10-0-1-23.ec2.internal:node-role.kubernetes.io/worker-hp= \\\n     --overwrite-labels \\\n     --skip-manifest\n   ```\n\n3. **Render the Manifest**\n   ```bash\n   python3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n     --profile-name \"$PROFILE\" \\\n     --summary \"$SUMMARY\" \\\n     --sysctl net.core.netdev_max_backlog=16384 \\\n     --match-label tuned.openshift.io/custom-net \\\n     --output .work/node-tuning/$PROFILE/tuned.yaml\n   ```\n   - Omit `--output` to write `<profile-name>.yaml` in the current directory.\n   - Add `--dry-run` to print the manifest to stdout.\n\n4. **Review Output**\n   - Inspect the generated YAML for accuracy.\n   - Optionally format with `yq` or open in an editor for readability.\n\n5. **Validate and Apply**\n   - Dry-run: `oc apply --server-dry-run=client -f <manifest>`.\n   - Apply: `oc apply -f <manifest>`.\n\n### Error Handling\n- Missing required options raise `ValueError` with descriptive messages.\n- The script exits non-zero when no target selectors (`--machine-config-label` or `--match-label`) are supplied.\n- Invalid key/value or section inputs identify the failing argument explicitly.\n\n### Examples\n```bash\npython3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n  --profile-name realtime-worker \\\n  --summary \"Realtime tuned profile\" \\\n  --include openshift-node --include realtime \\\n  --variable isolated_cores=1 \\\n  --section bootloader:cmdline_ocp_realtime=+systemd.cpu_affinity=${not_isolated_cores_expanded} \\\n  --machine-config-label machineconfiguration.openshift.io/role=worker-rt \\\n  --priority 25 \\\n  --output .work/node-tuning/realtime-worker/tuned.yaml\n```\n```bash\npython3 plugins/node-tuning/skills/scripts/generate_tuned_profile.py \\\n  --profile-name openshift-node-hugepages \\\n  --summary \"Boot time configuration for hugepages\" \\\n  --include openshift-node \\\n  --section bootloader:cmdline_openshift_node_hugepages=\"hugepagesz=2M hugepages=50\" \\\n  --machine-config-label machineconfiguration.openshift.io/role=worker-hp \\\n  --priority 30 \\\n  --output .work/node-tuning/openshift-node-hugepages/hugepages-tuned-boottime.yaml\n```\n\n---\n\n## Script: `analyze_node_tuning.py`\n\n### Purpose\nInspect either a live node (`/proc`, `/sys`) or an extracted sosreport snapshot for tuning signals (CPU isolation, IRQ affinity, huge pages, sysctl state, networking counters) and emit actionable recommendations.\n\n### Usage Patterns\n- **Live node analysis**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py --format markdown\n  ```\n- **Remote analysis via oc debug**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --node worker-rt-0 \\\n    --kubeconfig ~/.kube/prod \\\n    --format markdown\n  ```\n- **Collect sosreport via oc debug and analyze locally**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --node worker-rt-0 \\\n    --toolbox-image registry.example.com/support-tools:latest \\\n    --sosreport-arg \"--case-id=01234567\" \\\n    --sosreport-output .work/node-tuning/sosreports \\\n    --format json\n  ```\n- **Offline sosreport analysis**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --sosreport /path/to/sosreport-2025-10-20\n  ```\n- **Automation-friendly JSON**\n  ```bash\n  python3 plugins/node-tuning/skills/scripts/analyze_node_tuning.py \\\n    --sosreport /path/to/sosreport \\\n    --format json --output .work/node-tuning/node-analysis.json\n  ```\n\n### Implementation Steps\n1. **Select data source**\n   - Provide `--node <name>` (with optional `--kubeconfig` / `--oc-binary`). By default the helper runs `sosreport` remotely from inside the RHCOS toolbox container (`registry.redhat.io/rhel9/support-tools`). Override the image with `--toolbox-image`, extend the sosreport command with `--sosreport-arg`, or disable the curated OpenShift flags via `--skip-default-sosreport-flags`. Pass `--no-collect-sosreport` to fall back to the direct `/proc` snapshot mode.\n   - Provide `--sosreport <dir>` for archived diagnostics; detection finds embedded `proc/` and `sys/`.\n   - Omit both switches to query the live filesystem (defaults to `/proc` and `/sys`).\n   - Override paths with `--proc-root` or `--sys-root` when the layout differs.\n2. **Run analysis**\n   - The script parses `cpuinfo`, kernel cmdline parameters (`isolcpus`, `nohz_full`, `tuned.non_isolcpus`), default IRQ affinities, huge page counters, sysctl values (net, vm, kernel), transparent hugepage settings, `netstat`/`sockstat` counters, and `ps` snapshots (when available in sosreport).\n3. **Review the report**\n   - Markdown output groups findings by section (System Overview, CPU & Isolation, Huge Pages, Sysctl Highlights, Network Signals, IRQ Affinity, Process Snapshot) and lists recommendations.\n   - JSON output contains the same information in structured form for pipelines or dashboards.\n4. **Act on recommendations**\n   - Apply Tuned profiles, MachineConfig updates, or manual sysctl/irqbalance adjustments.\n   - Feed actionable items back into `/node-tuning:generate-tuned-profile` to codify desired state.\n\n### Error Handling\n- Missing `proc/` or `sys/` directories trigger descriptive errors.\n- Unreadable files are skipped gracefully and noted in observations where relevant.\n- Non-numeric sysctl values are flagged for manual investigation.\n\n### Example Output (Markdown excerpt)\n```\n# Node Tuning Analysis\n\n## System Overview\n- Hostname: worker-rt-1\n- Kernel: 4.18.0-477.el8\n- NUMA nodes: 2\n- Kernel cmdline: `BOOT_IMAGE=... isolcpus=2-15 tuned.non_isolcpus=0-1`\n\n## CPU & Isolation\n- Logical CPUs: 32\n- Physical cores: 16 across 2 socket(s)\n- SMT detected: yes\n- Isolated CPUs: 2-15\n...\n\n## Recommended Actions\n- Configure net.core.netdev_max_backlog (>=32768) to accommodate bursty NIC traffic.\n- Transparent Hugepages are not disabled (`[never]` not selected). Consider setting to `never` for latency-sensitive workloads.\n- 4 IRQs overlap isolated CPUs. Relocate interrupt affinities using tuned profiles or irqbalance.\n```\n\n### Follow-up Automation Ideas\n- Persist JSON results in `.work/node-tuning/<host>/analysis.json` for historical tracing.\n- Gate upgrades by comparing recommendations across nodes.\n- Integrate with CI jobs that validate cluster tuning post-change."
              }
            ]
          },
          {
            "name": "bigquery",
            "description": "BigQuery analysis utilities",
            "source": "./plugins/bigquery",
            "category": null,
            "version": "0.0.2",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install bigquery@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/analyze-usage",
                "description": "Analyze BigQuery usage and costs for a project",
                "path": "plugins/bigquery/commands/analyze-usage.md",
                "frontmatter": {
                  "description": "Analyze BigQuery usage and costs for a project",
                  "argument-hint": "<project-id> <timeframe>"
                },
                "content": "## Name\nbigquery:analyze-usage\n\n## Synopsis\n```\n/bigquery:analyze-usage <project-id> <timeframe>\n/bigquery:analyze-usage openshift-ci-data-analysis \"24 hours\"\n/bigquery:analyze-usage my-project \"7 days\"\n```\n\n## Description\n\nThe `analyze-usage` command provides comprehensive analysis of BigQuery usage patterns, costs, and query performance for a given project. It identifies expensive queries, heavy users, and provides actionable optimization recommendations.\n\nThis command helps answer questions like:\n- Which users or service accounts are consuming the most data?\n- What are the most expensive queries?\n- Which query patterns are running most frequently?\n- How can we reduce BigQuery costs?\n- Are we over any usage thresholds?\n\nThe analysis includes:\n- Total usage summary (queries, data scanned, estimated costs)\n- Usage breakdown by user/service account\n- Per-user deep dive analysis for top 2-3 users\n- Top individual queries by cost\n- Query pattern analysis to identify optimization opportunities\n- Specific, actionable optimization recommendations\n- Optional markdown report generation\n\n## Implementation\n\nThis command uses the `bigquery:analyze-usage` skill to perform the analysis.\n\n### Prerequisites\n- Google Cloud SDK (`bq` command-line tool) must be installed\n- User must have BigQuery read access to the project\n- User must be authenticated (`gcloud auth login`)\n\n### Steps\n\n1. **Parse and Validate Arguments**:\n   - If project-id is missing: Use AskUserQuestion to prompt for it\n   - If timeframe is missing: Use AskUserQuestion to prompt for it (options: \"1 hour\", \"6 hours\", \"24 hours\", \"7 days\", \"30 days\")\n   - Parse timeframe into hours (e.g., \"24 hours\"  24, \"7 days\"  168)\n\n2. **Invoke the analyze-usage Skill**:\n   ```\n   Use the Skill tool to invoke \"bigquery:analyze-usage\"\n   ```\n   The skill will handle all the data collection and analysis.\n\n3. **Present Results**:\n   The skill returns a comprehensive report. Present it to the user in a clear, readable format with:\n   - Executive summary at the top\n   - Tables for user usage and top queries\n   - Per-user deep dive for top 2-3 users showing their specific query patterns\n   - Detailed query pattern analysis\n   - Prioritized optimization recommendations\n\n4. **Offer to Save Report**:\n   After presenting the analysis, ask the user if they want to save it to a markdown file:\n   - Suggest filename: `bigquery-usage-<project-id>-<timestamp>.md`\n   - If user agrees, use Write tool to save the formatted report\n   - Include timestamp and all analysis details in the file\n\n## Return Value\n- **Success**: Comprehensive usage analysis report\n- **Error**: Authentication errors, missing permissions, invalid project, or bq tool not found\n\n**Important for Claude**:\n1. **REQUIRED**: You MUST invoke the `bigquery:analyze-usage` skill using the Skill tool\n2. Always validate both arguments before proceeding\n3. If arguments are missing, use AskUserQuestion to collect them\n4. Present the report in a clean, readable format\n5. Always offer to save the report to a markdown file at the end\n6. Handle errors gracefully with helpful suggestions\n\n## Examples\n\n1. **Analyze last 24 hours for a project**:\n   ```\n   /bigquery:analyze-usage openshift-ci-data-analysis \"24 hours\"\n   ```\n   Returns comprehensive report showing:\n   - Total of 9.76 TB scanned\n   - Top users: openshift-ci-data-writer (4.12 TB), job-run-big-query-writer (3.89 TB)\n   - Most expensive query pattern: TestRuns_Summary_Last200Runs (3.05 TB)\n   - Specific optimization recommendations\n   - Offer to save to `bigquery-usage-openshift-ci-data-analysis-20251202.md`\n\n2. **Analyze last 7 days**:\n   ```\n   /bigquery:analyze-usage my-project \"7 days\"\n   ```\n   Provides weekly usage analysis with trends and patterns.\n\n3. **Missing arguments - prompts user**:\n   ```\n   /bigquery:analyze-usage\n   ```\n   Claude asks:\n   - \"Which project would you like to analyze?\"\n   - \"What timeframe should I analyze?\" (with options)\n\n4. **Partial arguments**:\n   ```\n   /bigquery:analyze-usage openshift-ci-data-analysis\n   ```\n   Claude asks: \"What timeframe should I analyze?\" (with options: 1 hour, 6 hours, 24 hours, 7 days, 30 days)\n\n## Arguments\n\n- **project-id** (required): The GCP project ID to analyze\n  - Example: `openshift-ci-data-analysis`\n  - Must be a valid BigQuery project the user has access to\n\n- **timeframe** (required): Time period for analysis\n  - Supported formats:\n    - \"1 hour\", \"6 hours\", \"24 hours\"\n    - \"1 day\", \"7 days\", \"30 days\"\n    - Or just numbers for hours: \"24\", \"168\"\n  - Default if not specified: Prompt user with options\n\n## Timeframe Conversion\n- \"1 hour\"  1 hour\n- \"6 hours\"  6 hours\n- \"24 hours\" or \"1 day\"  24 hours\n- \"7 days\"  168 hours (7  24)\n- \"30 days\"  720 hours (30  24)\n- Numeric values are treated as hours\n\n## Report Contents\n\nThe generated report includes:\n\n### 1. Executive Summary\n- Analysis timeframe\n- Total queries executed\n- Total data scanned (in TB/GB)\n- Estimated cost (using $6.25/TB on-demand pricing)\n- Key findings (top 3 issues)\n\n### 2. Usage by User/Service Account\nTable showing top 10-20 users:\n- User email or service account\n- Number of queries\n- Total data scanned\n- Estimated cost\n- Average data per query\n\n### 3. Top Query Patterns\nDetailed breakdown of top 5-10 query patterns:\n- Query preview (first 200 chars)\n- Execution count\n- Total data scanned across all executions\n- Average data per execution\n- Sample user who ran it\n- Specific optimization recommendation\n\n### 4. Per-User Analysis (Top 2-3 Users)\nFor each top user by data scanned:\n- User email and total usage summary\n- What this user/service account does (inferred from queries)\n- Breakdown of primary query types with:\n  - Data scanned per pattern\n  - Execution count\n  - Behavior patterns (automation, time windows, etc.)\n  - Specific optimization recommendations for this user\n\n### 5. Top Individual Queries\nTable of top 20 queries by bytes scanned:\n- Timestamp\n- User\n- Data scanned\n- Query preview\n- Job ID\n\n### 6. Optimization Recommendations\nPrioritized list of actions:\n- What to optimize\n- Why it's expensive\n- How to fix it\n- Estimated savings\n- Implementation difficulty\n\n## Notes\n\n- **Pricing**: Cost estimates use on-demand pricing ($6.25/TB). Projects with flat-rate pricing will have different actual costs.\n- **Region**: Queries assume `region-us` INFORMATION_SCHEMA. May need adjustment for other regions.\n- **Authentication**: User must be authenticated via `gcloud auth login`\n- **Permissions**: User needs `bigquery.jobs.list` permission at minimum\n- **Performance**: Analysis queries are read-only and lightweight (use INFORMATION_SCHEMA)\n- **Timeframes**: Longer timeframes (30 days) may take longer to analyze due to more data\n\n## Troubleshooting\n\n**\"bq command not found\"**\n- Install Google Cloud SDK: `brew install google-cloud-sdk` (macOS) or visit cloud.google.com/sdk\n\n**\"Access Denied\" errors**\n- Run `gcloud auth login` to authenticate\n- Verify you have access to the project: `bq ls --project_id=<project-id>`\n- Check you have BigQuery Job User role or higher\n\n**\"Invalid project ID\"**\n- Verify project ID is correct (not project name)\n- Check project exists: `gcloud projects list`\n\n**No data returned**\n- Verify queries have run in the specified timeframe\n- Check the region (try `region-us`, `US`, `EU`, etc.)\n- Ensure you're querying the right project"
              }
            ],
            "skills": [
              {
                "name": "Analyze BigQuery Usage",
                "description": "Comprehensive analysis of BigQuery usage patterns, costs, and query performance",
                "path": "plugins/bigquery/skills/analyze-usage/SKILL.md",
                "frontmatter": {
                  "name": "Analyze BigQuery Usage",
                  "description": "Comprehensive analysis of BigQuery usage patterns, costs, and query performance"
                },
                "content": "# Analyze BigQuery Usage\n\nThis skill performs comprehensive analysis of BigQuery usage patterns, costs, and query performance for a given project. It identifies expensive queries, heavy users, and provides actionable optimization recommendations.\n\n## When to Use This Skill\n\nThis skill is automatically invoked by the `/bigquery:analyze-usage` command to perform usage analysis.\n\n## Prerequisites\n\n- Google Cloud SDK (`bq` command-line tool) must be installed\n- User must have BigQuery read access to the project\n- User must be authenticated (`gcloud auth login`)\n- User needs `bigquery.jobs.list` permission at minimum\n\n## Parameters\n\nWhen invoked, this skill expects:\n- **Project ID**: The GCP project ID to analyze (required)\n- **Timeframe**: Time period for analysis in hours (e.g., 24, 168 for 7 days)\n\n## Analysis Workflow\n\n### 1. Validate Prerequisites\n\nFirst, verify the environment is ready:\n- Check if `bq` command is available\n- Verify project access\n- Parse timeframe into hours\n\n### 2. Collect Usage Data\n\nExecute the following BigQuery queries against INFORMATION_SCHEMA:\n\n#### Total Usage Summary\n```sql\nSELECT\n  COUNT(*) as total_queries,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12), 2) as total_tb_scanned,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12) * 6.25, 2) as estimated_cost_usd\nFROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\nWHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @hours HOUR)\n  AND job_type = 'QUERY'\n  AND state = 'DONE'\n  AND statement_type != 'SCRIPT'\n```\n\n#### Usage by User/Service Account\n```sql\nSELECT\n  user_email,\n  COUNT(*) as query_count,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12), 2) as total_tb_scanned,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12) * 6.25, 2) as estimated_cost_usd,\n  ROUND(AVG(total_bytes_processed) / POW(10, 9), 2) as avg_gb_per_query\nFROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\nWHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @hours HOUR)\n  AND job_type = 'QUERY'\n  AND state = 'DONE'\n  AND statement_type != 'SCRIPT'\nGROUP BY user_email\nORDER BY total_tb_scanned DESC\nLIMIT 20\n```\n\n#### Top Individual Queries by Cost\n```sql\nSELECT\n  creation_time,\n  user_email,\n  job_id,\n  ROUND(total_bytes_processed / POW(10, 12), 3) as tb_scanned,\n  ROUND(total_bytes_processed / POW(10, 12) * 6.25, 2) as cost_usd,\n  SUBSTR(query, 1, 200) as query_preview\nFROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\nWHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @hours HOUR)\n  AND job_type = 'QUERY'\n  AND state = 'DONE'\n  AND statement_type != 'SCRIPT'\n  AND total_bytes_processed > 0\nORDER BY total_bytes_processed DESC\nLIMIT 20\n```\n\n#### Query Pattern Analysis\n```sql\nSELECT\n  SUBSTR(query, 1, 200) as query_pattern,\n  COUNT(*) as execution_count,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12), 3) as total_tb_scanned,\n  ROUND(AVG(total_bytes_processed) / POW(10, 9), 2) as avg_gb_per_execution,\n  ANY_VALUE(user_email) as sample_user\nFROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\nWHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @hours HOUR)\n  AND job_type = 'QUERY'\n  AND state = 'DONE'\n  AND statement_type != 'SCRIPT'\n  AND total_bytes_processed > 0\nGROUP BY query_pattern\nHAVING execution_count > 10\nORDER BY total_tb_scanned DESC\nLIMIT 15\n```\n\n### 3. Per-User Deep Dive Analysis\n\nFor the top 2-3 users by data scanned, perform detailed query pattern analysis:\n\n```sql\nSELECT\n  SUBSTR(query, 1, 300) as query_pattern,\n  COUNT(*) as execution_count,\n  ROUND(SUM(total_bytes_processed) / POW(10, 12), 3) as total_tb_scanned,\n  ROUND(AVG(total_bytes_processed) / POW(10, 9), 2) as avg_gb_per_execution,\n  MIN(creation_time) as first_execution,\n  MAX(creation_time) as last_execution\nFROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\nWHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @hours HOUR)\n  AND job_type = 'QUERY'\n  AND state = 'DONE'\n  AND statement_type != 'SCRIPT'\n  AND user_email = @user_email\n  AND total_bytes_processed > 0\nGROUP BY query_pattern\nORDER BY total_tb_scanned DESC\nLIMIT 20\n```\n\nThis reveals:\n- What each heavy user is querying\n- Patterns in their query behavior\n- Opportunities for user-specific optimizations\n- Whether queries are automated (service accounts) or manual (humans)\n\n### 4. Analyze Results and Identify Patterns\n\nLook for common issues:\n\n**Query Anti-Patterns:**\n- `SELECT *` on large tables\n- Full table scans without WHERE clauses\n- High-frequency queries that could be cached\n- Queries scanning data unnecessarily (e.g., counting rows by scanning 40GB)\n- Missing partitioning filters\n- Repeated identical queries\n\n**User Behavior Patterns:**\n- Service accounts with high query volume (automation candidates)\n- Deduplication checks that scan large amounts of data\n- Dashboard queries hitting raw tables instead of materialized views\n- Scheduled queries running too frequently\n\n**Cost Drivers:**\n- Single expensive queries\n- High-volume low-cost queries (death by a thousand cuts)\n- Inefficient aggregations\n- Missing indexes/clustering\n\n### 5. Generate Optimization Recommendations\n\nFor each issue found, provide:\n\n1. **What**: Describe the issue clearly\n2. **Why**: Explain why it's expensive\n3. **How**: Provide specific fix instructions\n4. **Savings**: Estimate potential cost reduction\n5. **Difficulty**: Rate implementation effort (easy/medium/hard)\n6. **Priority**: Based on impact and ease\n\n**Prioritization Framework:**\n- Priority 1: High impact, easy wins (>$5/day saved, easy implementation)\n- Priority 2: Medium impact (>$2/day saved)\n- Priority 3: Architectural improvements (long-term benefits)\n\n### 6. Format Comprehensive Report\n\nStructure the output as:\n\n```markdown\n# BigQuery Usage Analysis Report\n**Project:** <project-id>\n**Analysis Period:** Last <timeframe>\n**Generated:** <timestamp>\n\n## Executive Summary\n- Total Queries Executed: <count>\n- Total Data Scanned: <TB>\n- Estimated Cost: $<amount>\n\n### Key Findings\n1. <Top issue with data/cost>\n2. <Second major issue>\n3. <Third issue>\n\n## Usage by User/Service Account\n<Table with top 10-20 users>\n\n## Top Query Patterns\n<Detailed breakdown of top patterns with recommendations>\n\n## Per-User Analysis\n\n### Top User 1: <user_email> (<TB> scanned, $<cost>)\n<Summary of what this user does>\n\n**Primary Query Types:**\n1. <Pattern description> (<data scanned>)\n   - Execution count\n   - Average per query\n   - Specific optimization recommendation\n\n### Top User 2: <user_email> (<TB> scanned, $<cost>)\n<Summary of what this user does>\n\n**Primary Query Types:**\n1. <Pattern description> (<data scanned>)\n   - Execution count\n   - Average per query\n   - Specific optimization recommendation\n\n## Top Individual Queries\n<Table of most expensive queries>\n\n## Optimization Recommendations\n\n### Priority 1: High Impact, Easy Wins\n1. **<Recommendation title>**\n   - Issue: <description>\n   - Fix: <specific steps>\n   - Estimated Savings: <$/day or %>\n   - Difficulty: Easy/Medium/Hard\n\n### Priority 2: Medium Impact\n...\n\n### Priority 3: Architectural Improvements\n...\n\n## Cost Breakdown Summary\n- Service Accounts: $<amount> (<percentage>)\n- Human Users: $<amount> (<percentage>)\n```\n\n### 7. Offer to Save Report\n\nAfter presenting the analysis, ask the user if they want to save it to a markdown file:\n- Suggest filename: `bigquery-usage-<project-id>-<YYYYMMDD>.md`\n- Include all analysis details\n- Format with proper markdown tables and sections\n\n## Error Handling\n\nHandle common issues gracefully:\n\n**\"bq command not found\"**\n- Provide installation instructions for user's platform\n- macOS: `brew install google-cloud-sdk`\n- Linux: Point to cloud.google.com/sdk\n- Verify PATH configuration\n\n**\"Access Denied\" errors**\n- Guide user through `gcloud auth login`\n- Verify project access: `bq ls --project_id=<project-id>`\n- Check IAM permissions (need BigQuery Job User role minimum)\n\n**\"Invalid project ID\"**\n- Verify project ID vs project name\n- List available projects: `gcloud projects list`\n- Check for typos\n\n**No data returned**\n- Verify queries have run in the specified timeframe\n- Check region (try `region-us`, `US`, `EU`, etc.)\n- Ensure querying correct project\n\n**Query execution errors**\n- Check INFORMATION_SCHEMA availability\n- Verify region-specific schema locations\n- Adjust queries for project's BigQuery setup\n\n## Implementation Notes\n\n### Cost Calculation\n- Use on-demand pricing: $6.25/TB (as of 2024-2025)\n- Note in report if project may have flat-rate pricing\n- Savings estimates are based on on-demand pricing\n\n### Region Handling\n- Default to `region-us` INFORMATION_SCHEMA\n- If queries fail, try `US` or `EU`\n- Consider making region configurable in future\n\n### Performance Considerations\n- All analysis queries are read-only\n- Use INFORMATION_SCHEMA (metadata only, very efficient)\n- Queries should complete in seconds\n- Longer timeframes (30 days) may take longer but still fast\n\n### Query Optimization\n- Use `--format=json` for easy parsing\n- Use `--use_legacy_sql=false` for standard SQL\n- Limit result sets appropriately\n- Filter to completed queries only (state = 'DONE')\n\n## Example Output Insights\n\n**Good User Analysis Example:**\n\n```\n### openshift-ci-data-writer (4.07 TB, $25.41)\nThis service account runs test run analysis and backend disruption monitoring.\n\n**Primary Query Types:**\n1. **TestRuns_Summary_Last200Runs** (3.02 TB - 74% of usage)\n   - 96 executions using SELECT *\n   - 31.41 GB per query\n   - **Recommendation:** Replace SELECT * with specific columns.\n     Estimated savings: $9-15/day\n\n2. **BackendDisruption Lookups** (0.68 TB - 17% of usage)\n   - 4,165 queries checking for job run names\n   - **Recommendation:** Add clustering on JobName + JobRunStartTime.\n     Implement result caching.\n```\n\nThis level of detail helps users understand exactly what's driving their costs and how to fix it.\n\n## Success Criteria\n\nA successful analysis should:\n1. Identify all queries scanning >100GB per execution\n2. Find high-frequency query patterns (>100 executions)\n3. Provide at least 3 actionable optimization recommendations\n4. Include cost estimates for top recommendations\n5. Offer clear next steps for the user\n6. Be formatted clearly and professionally\n\n## Future Enhancements\n\nConsider adding:\n- Trend analysis (compare current period to previous periods)\n- Query performance metrics (execution time, slot usage)\n- Automatic detection of partitioning opportunities\n- Cost anomaly detection (unusual spikes)\n- Integration with BigQuery Reservations data\n- Historical cost tracking over time"
              }
            ]
          },
          {
            "name": "workspaces",
            "description": "Manage isolated git worktree workspaces for multi-repo development",
            "source": "./plugins/workspaces",
            "category": null,
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add openshift-eng/ai-helpers",
              "/plugin install workspaces@ai-helpers"
            ],
            "signals": {
              "stars": 34,
              "forks": 197,
              "pushed_at": "2026-01-12T15:53:18Z",
              "created_at": "2025-10-10T07:55:31Z",
              "license": "Apache-2.0"
            },
            "commands": [
              {
                "name": "/create",
                "description": "Create a workspace with git worktrees for multi-repository development",
                "path": "plugins/workspaces/commands/create.md",
                "frontmatter": {
                  "description": "Create a workspace with git worktrees for multi-repository development",
                  "argument-hint": "<short-description> <repo1|url> [repo2...]"
                },
                "content": "## Name\nworkspaces:create\n\n## Synopsis\n```\n/workspaces:create <short-description> <repo1|url> [repo2...]\n```\n\n## Description\nThe `workspaces:create` command creates a dedicated workspace directory with git worktrees for multi-repository development. It intelligently parses the workspace description to identify repositories, PR URLs, and JIRA keys, then sets up isolated git worktrees on a new feature branch or checks out a PR for review.\n\nThis command streamlines working on tasks that span multiple repositories by keeping all related code changes synchronized in a single workspace.\n\n## Implementation\n\n### Workflow\n\n```\n- [ ] Step 1: Gather info and check configuration\n- [ ] Step 2: Parse workspace description\n- [ ] Step 3: Ask user to confirm\n- [ ] Step 4: Validate and execute\n```\n\n---\n\n## Step 1: Gather info and check configuration\n\nRun the gather script (which includes preflight check):\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/create/gather.sh\n```\n\n**If the output contains `STATUS: NOT_CONFIGURED`**:\n\n1. Use AskUserQuestion to ask the user for:\n   - **Git repositories root**: Full path to where git repos are cloned (e.g., `/home/user/git-repos` or `~/work/repos`)\n   - **Workspaces root**: Full path to where workspaces will be created (e.g., `/home/user/workspaces` or `~/dev/workspaces`)\n\n2. Run gather again with the user's paths:\n   ```bash\n   ${CLAUDE_PLUGIN_ROOT}/commands/create/gather.sh --repos-root <REPOS_PATH> --workspaces-root <WORKSPACES_PATH>\n   ```\n\n3. Verify the output shows `CONFIGURATION SAVED` before proceeding to Step 2\n\n**If TEMPLATE is MISSING**: Abort with error.\n\n---\n\n## Step 2: Parse workspace description\n\n**Check for custom rules**: From Step 1 output, check if `=== CUSTOM_PROMPT ===` section is present:\n\n- **If present**: Apply the aliases and auto-detect rules defined in that section\n- **If not present**: Use generic parsing without aliases or auto-detect\n\n**Extract from workspace description:**\n- **GitHub PR URLs**: `github.com/{org}/{repo}/pull/{number}`\n- **Jira keys**: Pattern `[A-Z]+-[0-9]+` (e.g., TEAM-1234, PROJECT-567)\n- **Repository names**: Match against repos listed in Step 1 REPOS section\n\n**Apply custom rules** (if custom-prompt.md was found):\n- Expand any aliases defined in the file\n- Apply auto-detect rules to add dependent repositories\n\n**Workspace directory name priority**:\n1. Jira key (if found in description)\n2. `review-{repo}-{pr}` (if PR URL found)\n3. `feature-{summary}` (derive from description)\n\n---\n\n## Step 3: Ask user to confirm\n\nUse AskUserQuestion for:\n- **Repositories**: 1-3 specific combinations\n- **Workspace name**: 1-3 actual suggestions\n\n**If user picks \"Other\" for repositories**: Ask a second confirmation to verify their custom repo list before proceeding.\n\n---\n\n## Step 4: Execute workspace creation\n\n**Use the workspace NAME (not full path)**.\n\nFor feature workspaces:\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/create/execute.sh {WORKSPACE_NAME} {BRANCH} feature {REPO1} {REPO2} ...\n```\n\nFor PR review workspaces:\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/create/execute.sh {WORKSPACE_NAME} {BRANCH} review:{PR_NUM} {REPO1} {REPO2} ...\n```\n\n**Example**: Use `feature-azure-template-field` (not `/home/user/workspaces/feature-azure-template-field`).\n\nThe script will create the workspace with worktrees for each repository. If any issues are found, the script will abort with an error message.\n\nReport the workspace location and worktrees created to the user."
              },
              {
                "name": "/delete",
                "description": "Delete a workspace and its git worktrees",
                "path": "plugins/workspaces/commands/delete.md",
                "frontmatter": {
                  "description": "Delete a workspace and its git worktrees",
                  "argument-hint": "<workspace-name>"
                },
                "content": "## Name\nworkspaces:delete\n\n## Synopsis\n```\n/workspaces:delete <workspace-name>\n```\n\n## Description\nThe `workspaces:delete` command removes a workspace directory and its associated git worktrees. Before deletion, it checks the status of all repositories in the workspace to detect uncommitted changes or unpushed commits, prompting the user for confirmation when issues are found.\n\nThis command helps clean up task workspaces created by `/workspaces:create` while ensuring no work is accidentally lost.\n\n## Implementation\n\n### Workflow\n\n```\n- [ ] Step 1: Identify workspace and check configuration\n- [ ] Step 2: Execute deletion\n```\n\n---\n\n## Step 1: Identify workspace and check configuration\n\nGet available workspaces (includes configuration check):\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/delete/list.sh\n```\n\n**If the output contains `STATUS: NOT_CONFIGURED`**:\n\n1. Use AskUserQuestion to ask the user for:\n   - **Git repositories root**: Full path to where git repos are cloned (e.g., `/home/user/git-repos` or `~/work/repos`)\n   - **Workspaces root**: Full path to where workspaces will be created (e.g., `/home/user/workspaces` or `~/dev/workspaces`)\n\n2. Run list again with the user's paths:\n   ```bash\n   ${CLAUDE_PLUGIN_ROOT}/commands/delete/list.sh --repos-root <REPOS_PATH> --workspaces-root <WORKSPACES_PATH>\n   ```\n\n3. Verify the output shows `CONFIGURATION SAVED` before proceeding to Step 2\n\nMatch user's input against the workspace list:\n- **Exact match found**: Proceed to Step 2\n- **Partial matches found**: Use AskUserQuestion to let user select from matches\n- **No matches**: Use AskUserQuestion to let user select from all workspaces or cancel\n\n---\n\n## Step 2: Execute deletion\n\n**Use the workspace NAME (not full path)** from Step 1.\n\n**Example**: If the workspace is at `/home/user/workspaces/feature-azure-template-field`, use `feature-azure-template-field` (not the full path).\n\n### Step 2.1: Attempt deletion\n\nExecute the deletion script:\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/delete/execute.sh {WORKSPACE_NAME}\n```\n\nThe script will:\n1. **Check status** of all repositories in the workspace\n2. **Display** the status to the user\n3. **Proceed automatically** if workspace is clean\n4. **Request user input** if there are uncommitted changes or unpushed commits\n\n### Step 2.2: Handle the result\n\n**Case A: Exit code 0** (success)\n\nThe workspace was clean and has been deleted successfully. Report completion to user.\n\n---\n\n**Case B: Exit code 2** (user input required)\n\nThe workspace has uncommitted changes or unpushed commits. The status has already been displayed to the user.\n\nUse AskUserQuestion with these options:\n1. **\"Keep branches (Recommended)\"** - Description: \"Delete workspace but keep git branches. You can still access unpushed commits from the main repository.\"\n2. **\"Delete branches\"** - Description: \"Delete workspace AND branches. Warning: uncommitted changes and unpushed commits will be lost.\"\n3. **\"Abort\"** - Description: \"Cancel the deletion operation.\"\n\nThen execute based on user's choice:\n\n**If user chose \"Keep branches\"**:\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/delete/execute.sh {WORKSPACE_NAME} --keep-branches\n```\n\n**If user chose \"Delete branches\"**:\n```bash\n${CLAUDE_PLUGIN_ROOT}/commands/delete/execute.sh {WORKSPACE_NAME} --delete-branches\n```\n\n**If user chose \"Abort\"**:\n- Do not execute deletion\n- Inform user: \"Deletion cancelled. Workspace remains intact.\"\n\n---\n\n**Case C: Exit code 1** (error)\n\nAn error occurred (e.g., workspace not found, configuration issue). Report the error to the user."
              }
            ],
            "skills": []
          }
        ]
      }
    }
  ]
}