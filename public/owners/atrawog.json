{
  "owner": {
    "id": "atrawog",
    "display_name": "Andreas Trawoeger",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/927669?u=a11f5491b062a128c80a04084aeed9a936ced0df&v=4",
    "url": "https://github.com/atrawog",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 113,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "atrawog/bazzite-ai-plugins",
      "url": "https://github.com/atrawog/bazzite-ai-plugins",
      "description": "Claude Code plugins for Bazzite AI",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T13:36:51Z",
        "created_at": "2025-12-26T20:21:42Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2650
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 3754
        },
        {
          "path": "bazzite-ai-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 240
        },
        {
          "path": "bazzite-ai-dev/.mcp.json",
          "type": "blob",
          "size": 532
        },
        {
          "path": "bazzite-ai-dev/README.md",
          "type": "blob",
          "size": 4391
        },
        {
          "path": "bazzite-ai-dev/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/agents/architecture-advisor.md",
          "type": "blob",
          "size": 8118
        },
        {
          "path": "bazzite-ai-dev/agents/buildcache-validator.md",
          "type": "blob",
          "size": 11320
        },
        {
          "path": "bazzite-ai-dev/agents/code-research.md",
          "type": "blob",
          "size": 5161
        },
        {
          "path": "bazzite-ai-dev/agents/config-integrity-enforcer.md",
          "type": "blob",
          "size": 5620
        },
        {
          "path": "bazzite-ai-dev/agents/documentation-validator.md",
          "type": "blob",
          "size": 7695
        },
        {
          "path": "bazzite-ai-dev/agents/github-actions.md",
          "type": "blob",
          "size": 6908
        },
        {
          "path": "bazzite-ai-dev/agents/justfile-validator.md",
          "type": "blob",
          "size": 15557
        },
        {
          "path": "bazzite-ai-dev/agents/overlay-testing-enforcer.md",
          "type": "blob",
          "size": 9986
        },
        {
          "path": "bazzite-ai-dev/agents/pixi-lock-enforcer.md",
          "type": "blob",
          "size": 5202
        },
        {
          "path": "bazzite-ai-dev/agents/policy-enforcer.md",
          "type": "blob",
          "size": 24148
        },
        {
          "path": "bazzite-ai-dev/agents/pre-commit-guardian.md",
          "type": "blob",
          "size": 6596
        },
        {
          "path": "bazzite-ai-dev/agents/root-cause-analyzer.md",
          "type": "blob",
          "size": 8661
        },
        {
          "path": "bazzite-ai-dev/agents/sudo-usage-enforcer.md",
          "type": "blob",
          "size": 6257
        },
        {
          "path": "bazzite-ai-dev/agents/testing-validator.md",
          "type": "blob",
          "size": 11038
        },
        {
          "path": "bazzite-ai-dev/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/build",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/build/SKILL.md",
          "type": "blob",
          "size": 6802
        },
        {
          "path": "bazzite-ai-dev/skills/clean",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/clean/SKILL.md",
          "type": "blob",
          "size": 7711
        },
        {
          "path": "bazzite-ai-dev/skills/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/test/SKILL.md",
          "type": "blob",
          "size": 6287
        },
        {
          "path": "bazzite-ai-dev/skills/test/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/test/references/overlay-architecture.md",
          "type": "blob",
          "size": 2611
        },
        {
          "path": "bazzite-ai-dev/skills/test/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-dev/skills/test/scripts/check-overlay-status.sh",
          "type": "blob",
          "size": 954
        },
        {
          "path": "bazzite-ai-jupyter",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 303
        },
        {
          "path": "bazzite-ai-jupyter/.mcp.json",
          "type": "blob",
          "size": 112
        },
        {
          "path": "bazzite-ai-jupyter/README.md",
          "type": "blob",
          "size": 4380
        },
        {
          "path": "bazzite-ai-jupyter/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/dpo",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/dpo/SKILL.md",
          "type": "blob",
          "size": 9268
        },
        {
          "path": "bazzite-ai-jupyter/skills/evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/evaluation/SKILL.md",
          "type": "blob",
          "size": 7844
        },
        {
          "path": "bazzite-ai-jupyter/skills/finetuning",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/finetuning/SKILL.md",
          "type": "blob",
          "size": 10781
        },
        {
          "path": "bazzite-ai-jupyter/skills/grpo",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/grpo/SKILL.md",
          "type": "blob",
          "size": 11569
        },
        {
          "path": "bazzite-ai-jupyter/skills/inference",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/inference/SKILL.md",
          "type": "blob",
          "size": 11874
        },
        {
          "path": "bazzite-ai-jupyter/skills/langchain",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/langchain/SKILL.md",
          "type": "blob",
          "size": 7737
        },
        {
          "path": "bazzite-ai-jupyter/skills/peft",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/peft/SKILL.md",
          "type": "blob",
          "size": 10995
        },
        {
          "path": "bazzite-ai-jupyter/skills/qlora",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/qlora/SKILL.md",
          "type": "blob",
          "size": 17326
        },
        {
          "path": "bazzite-ai-jupyter/skills/quantization",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/quantization/SKILL.md",
          "type": "blob",
          "size": 8666
        },
        {
          "path": "bazzite-ai-jupyter/skills/rag",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/rag/SKILL.md",
          "type": "blob",
          "size": 7381
        },
        {
          "path": "bazzite-ai-jupyter/skills/reward",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/reward/SKILL.md",
          "type": "blob",
          "size": 8761
        },
        {
          "path": "bazzite-ai-jupyter/skills/rloo",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/rloo/SKILL.md",
          "type": "blob",
          "size": 9891
        },
        {
          "path": "bazzite-ai-jupyter/skills/sft",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/sft/SKILL.md",
          "type": "blob",
          "size": 10841
        },
        {
          "path": "bazzite-ai-jupyter/skills/transformers",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/transformers/SKILL.md",
          "type": "blob",
          "size": 9851
        },
        {
          "path": "bazzite-ai-jupyter/skills/vision",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-jupyter/skills/vision/SKILL.md",
          "type": "blob",
          "size": 14999
        },
        {
          "path": "bazzite-ai-ollama",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 248
        },
        {
          "path": "bazzite-ai-ollama/README.md",
          "type": "blob",
          "size": 1363
        },
        {
          "path": "bazzite-ai-ollama/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/api/SKILL.md",
          "type": "blob",
          "size": 6118
        },
        {
          "path": "bazzite-ai-ollama/skills/gpu",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/gpu/SKILL.md",
          "type": "blob",
          "size": 9481
        },
        {
          "path": "bazzite-ai-ollama/skills/huggingface",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/huggingface/SKILL.md",
          "type": "blob",
          "size": 7013
        },
        {
          "path": "bazzite-ai-ollama/skills/openai",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/openai/SKILL.md",
          "type": "blob",
          "size": 6008
        },
        {
          "path": "bazzite-ai-ollama/skills/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai-ollama/skills/python/SKILL.md",
          "type": "blob",
          "size": 6437
        },
        {
          "path": "bazzite-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 226
        },
        {
          "path": "bazzite-ai/README.md",
          "type": "blob",
          "size": 2065
        },
        {
          "path": "bazzite-ai/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/apptainer",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/apptainer/SKILL.md",
          "type": "blob",
          "size": 7468
        },
        {
          "path": "bazzite-ai/skills/bootc",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/bootc/SKILL.md",
          "type": "blob",
          "size": 7353
        },
        {
          "path": "bazzite-ai/skills/comfyui",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/comfyui/SKILL.md",
          "type": "blob",
          "size": 10738
        },
        {
          "path": "bazzite-ai/skills/config",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/config/SKILL.md",
          "type": "blob",
          "size": 6729
        },
        {
          "path": "bazzite-ai/skills/config/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/config/references/service-targets.md",
          "type": "blob",
          "size": 2931
        },
        {
          "path": "bazzite-ai/skills/fiftyone",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/fiftyone/SKILL.md",
          "type": "blob",
          "size": 8368
        },
        {
          "path": "bazzite-ai/skills/install",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/install/SKILL.md",
          "type": "blob",
          "size": 6690
        },
        {
          "path": "bazzite-ai/skills/jellyfin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/jellyfin/SKILL.md",
          "type": "blob",
          "size": 7889
        },
        {
          "path": "bazzite-ai/skills/jupyter",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/jupyter/SKILL.md",
          "type": "blob",
          "size": 8675
        },
        {
          "path": "bazzite-ai/skills/localai",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/localai/SKILL.md",
          "type": "blob",
          "size": 8957
        },
        {
          "path": "bazzite-ai/skills/ollama",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/ollama/SKILL.md",
          "type": "blob",
          "size": 8206
        },
        {
          "path": "bazzite-ai/skills/openwebui",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/openwebui/SKILL.md",
          "type": "blob",
          "size": 8318
        },
        {
          "path": "bazzite-ai/skills/pods",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/pods/SKILL.md",
          "type": "blob",
          "size": 3010
        },
        {
          "path": "bazzite-ai/skills/runners",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/runners/SKILL.md",
          "type": "blob",
          "size": 7528
        },
        {
          "path": "bazzite-ai/skills/tailscale",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/tailscale/SKILL.md",
          "type": "blob",
          "size": 6834
        },
        {
          "path": "bazzite-ai/skills/test",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/test/SKILL.md",
          "type": "blob",
          "size": 5078
        },
        {
          "path": "bazzite-ai/skills/vm",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite-ai/skills/vm/SKILL.md",
          "type": "blob",
          "size": 7671
        },
        {
          "path": "bazzite",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 228
        },
        {
          "path": "bazzite/README.md",
          "type": "blob",
          "size": 2488
        },
        {
          "path": "bazzite/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/apps",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/apps/SKILL.md",
          "type": "blob",
          "size": 5626
        },
        {
          "path": "bazzite/skills/audio",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/audio/SKILL.md",
          "type": "blob",
          "size": 4592
        },
        {
          "path": "bazzite/skills/boot",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/boot/SKILL.md",
          "type": "blob",
          "size": 3885
        },
        {
          "path": "bazzite/skills/desktop",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/desktop/SKILL.md",
          "type": "blob",
          "size": 4132
        },
        {
          "path": "bazzite/skills/distrobox",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/distrobox/SKILL.md",
          "type": "blob",
          "size": 4616
        },
        {
          "path": "bazzite/skills/gaming",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/gaming/SKILL.md",
          "type": "blob",
          "size": 5678
        },
        {
          "path": "bazzite/skills/gpu",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/gpu/SKILL.md",
          "type": "blob",
          "size": 5296
        },
        {
          "path": "bazzite/skills/network",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/network/SKILL.md",
          "type": "blob",
          "size": 4323
        },
        {
          "path": "bazzite/skills/security",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/security/SKILL.md",
          "type": "blob",
          "size": 4130
        },
        {
          "path": "bazzite/skills/storage",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/storage/SKILL.md",
          "type": "blob",
          "size": 5388
        },
        {
          "path": "bazzite/skills/system",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/system/SKILL.md",
          "type": "blob",
          "size": 4218
        },
        {
          "path": "bazzite/skills/virtualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "bazzite/skills/virtualization/SKILL.md",
          "type": "blob",
          "size": 5304
        }
      ],
      "marketplace": {
        "name": "bazzite-ai-plugins",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "atrawog"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "bazzite",
            "description": "Skills for using Bazzite OS features via ujust commands",
            "source": "./bazzite",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "atrawog"
            },
            "install_commands": [
              "/plugin marketplace add atrawog/bazzite-ai-plugins",
              "/plugin install bazzite@bazzite-ai-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T13:36:51Z",
              "created_at": "2025-12-26T20:21:42Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "build",
                "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n",
                "path": "bazzite-ai-dev/skills/build/SKILL.md",
                "frontmatter": {
                  "name": "build",
                  "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n"
                },
                "content": "# Build - Unified Build System\n\n## Overview\n\nThe `build` command provides a unified interface for all bazzite-ai build operations:\n\n- OS container images\n- Pod container variants\n- VM images (QCOW2/RAW)\n- Live ISO installers\n- Push to registry\n- Sign with cosign\n\n**Smart Caching:** Automatically detects git branch and uses matching cache tag, ensuring local builds are compatible with GitHub Actions builds.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Build OS | `just build os` | Build OS container image |\n| Build pod | `just build pod nvidia` | Build specific pod variant |\n| Build all pods | `just build pod all` | Build all pod variants |\n| Build ISO | `just build iso` | Build live ISO installer |\n| Build QCOW2 | `just build qcow2` | Build QCOW2 VM image |\n| Build RAW | `just build raw` | Build RAW VM image |\n| Generate lock | `just build pixi python` | Generate pixi.lock |\n| Push OS | `just build push os` | Push OS image to registry |\n| Push pod | `just build push pod nvidia` | Push pod to registry |\n| Sign OS | `just build sign os` | Sign OS image with cosign |\n| Sign pod | `just build sign pod nvidia` | Sign pod with cosign |\n| Show status | `just build status` | Show cache/build status |\n\n## Pod Variants\n\n| Variant | Image Name | Description |\n|---------|------------|-------------|\n| `base` | `bazzite-ai-pod` | CPU-only development |\n| `nvidia` | `bazzite-ai-pod-nvidia` | GPU compute with CUDA |\n| `nvidia-python` | `bazzite-ai-pod-nvidia-python` | NVIDIA + ML packages |\n| `jupyter` | `bazzite-ai-pod-jupyter` | JupyterLab + ML stack |\n| `ollama` | `bazzite-ai-pod-ollama` | LLM inference |\n| `comfyui` | `bazzite-ai-pod-comfyui` | Stable Diffusion UI |\n| `devops` | `bazzite-ai-pod-devops` | AWS/kubectl/Helm tools |\n| `githubrunner` | `bazzite-ai-pod-githubrunner` | CI/CD pipeline |\n\n## Smart Cache Strategy\n\nThe build system automatically detects your git branch and uses the appropriate cache tag to maximize cache reuse between local and CI builds:\n\n| Branch | Cache Tag | Build Tag |\n|--------|-----------|-----------|\n| `main` | `stable` | `stable` |\n| `testing` | `testing` | `testing` |\n| Other | `{branch}` | `{branch}` |\n\nThis ensures that when you build locally on the `testing` branch, you pull cache layers from the `:testing` images pushed by GitHub Actions.\n\n## Environment Variables\n\nFor CI integration, the following environment variables are supported:\n\n| Variable | Purpose |\n|----------|---------|\n| `COSIGN_PRIVATE_KEY` | Private key for signing with cosign |\n| `BUILD_LABELS` | Space-separated OCI labels to apply during build |\n| `BUILD_TAGS` | Space-separated tags to apply (overrides default) |\n| `BASE_IMAGE` | Override base image for pod builds (for digest pinning) |\n\n## Common Workflows\n\n### Build OS Image\n\n```bash\n# Build with branch-appropriate tag\njust build os\n\n# Build with custom tag\njust build os custom-tag\n```\n\n### Build Pods\n\n```bash\n# Interactive selection\njust build pod\n\n# Specific variant\njust build pod nvidia\n\n# All variants\njust build pod all\n```\n\n### Build VM/ISO\n\n```bash\n# Build QCOW2 VM image\njust build qcow2\n\n# Build live ISO\njust build iso\n\n# Build RAW image\njust build raw\n```\n\n### Push to Registry\n\n```bash\n# Push OS image\njust build push os\n\n# Push specific pod\njust build push pod nvidia\n\n# Push all pods\njust build push pod all\n```\n\n### Sign Images\n\n```bash\n# Sign OS image (requires COSIGN_PRIVATE_KEY env var)\nCOSIGN_PRIVATE_KEY=$KEY just build sign os\n\n# Sign pod\nCOSIGN_PRIVATE_KEY=$KEY just build sign pod nvidia\n```\n\n### Generate Pixi Locks\n\n```bash\n# Python variant\njust build pixi python\n\n# Jupyter variant\njust build pixi jupyter\n\n# All variants\njust build pixi all\n```\n\n## CI Integration\n\nThe build commands are designed for GitHub Actions integration:\n\n```yaml\n# Build, push, and sign in CI\n- name: Build and push OS\n  env:\n    BUILD_LABELS: ${{ steps.metadata.outputs.labels }}\n    COSIGN_PRIVATE_KEY: ${{ secrets.SIGNING_SECRET }}\n  run: |\n    just build os $TAG\n    just build push os $TAG\n    just build sign os $TAG\n\n# Build pod with base image digest\n- name: Build nvidia pod\n  env:\n    BASE_IMAGE: ghcr.io/owner/bazzite-ai-pod@${{ needs.base.outputs.digest }}\n  run: just build pod nvidia $TAG\n```\n\n## Output Images\n\nImages are tagged with the registry prefix:\n\n```\nghcr.io/atrawog/bazzite-ai:{tag}           # OS image\nghcr.io/atrawog/bazzite-ai-pod:{tag}       # Base pod\nghcr.io/atrawog/bazzite-ai-pod-nvidia:{tag} # NVIDIA pod\nghcr.io/atrawog/bazzite-ai-pod-comfyui:{tag} # ComfyUI pod\n```\n\n## Requirements\n\n- Podman installed and configured\n- Git repository cloned\n- Sufficient disk space (~10GB for OS, ~20GB for ISO)\n- Network access (pulls base images)\n- cosign installed (for signing)\n- Registry authentication (for push)\n\n## Troubleshooting\n\n### Build Fails with Cache Error\n\n**Symptom:** Cache layer not found\n\n**Cause:** Remote image not yet pushed for this branch\n\n**Fix:**\n\n```bash\n# Build without cache (first build on new branch)\n# Or check status to see cache state\njust build status\n```\n\n### Pod Build Fails with Base Image Missing\n\n**Symptom:** Cannot find base pod image\n\n**Cause:** Parent variant not built\n\n**Fix:**\n\n```bash\n# Build in order (base -> nvidia -> jupyter)\njust build pod base\njust build pod nvidia\njust build pod jupyter\n```\n\n### Push Fails with Authentication Error\n\n**Symptom:** unauthorized: authentication required\n\n**Cause:** Not logged into registry\n\n**Fix:**\n\n```bash\n# Login to GitHub Container Registry\npodman login ghcr.io\n```\n\n### Sign Fails\n\n**Symptom:** cosign not found or key not set\n\n**Cause:** cosign not installed or COSIGN_PRIVATE_KEY not set\n\n**Fix:**\n\n```bash\n# Check cosign is installed\nwhich cosign\n\n# Set signing key\nexport COSIGN_PRIVATE_KEY=\"$(cat cosign.key)\"\n```\n\n### CUDA Test Fails\n\n**Symptom:** nvidia-smi not found\n\n**Cause:** No GPU available or CDI not configured\n\n**Fix:**\n\n```bash\n# Verify GPU on host\nnvidia-smi\n\n# Check CDI configuration\nls /etc/cdi/\n```\n\n## Cross-References\n\n- **Related Skills:** `clean` (cleanup build artifacts)\n- **System Commands:** `ujust jupyter`, `ujust ollama` (use built pods)\n- **Documentation:** See `Containerfile` for image layers\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"build os\", \"build image\", \"build container\"\n- \"build pod\", \"build nvidia\", \"build jupyter\", \"build comfyui\"\n- \"build iso\", \"build qcow2\", \"build vm\"\n- \"push os\", \"push pod\", \"push to registry\"\n- \"sign image\", \"cosign\", \"sign pod\"\n- \"pixi lock\", \"generate lock\"\n- \"just build\" (any build command)"
              },
              {
                "name": "clean",
                "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n",
                "path": "bazzite-ai-dev/skills/clean/SKILL.md",
                "frontmatter": {
                  "name": "clean",
                  "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n"
                },
                "content": "# Clean - Cleanup & Maintenance\n\n## Overview\n\nThe `clean` development commands remove build artifacts, caches, containers, and other temporary files to recover disk space and reset the development environment.\n\n**Key Concept:** This is a **development command** - run with `just` from the repository root, not `ujust`. It provides both interactive menu and non-interactive modes.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Interactive menu | `just clean` | Show cleanup options |\n| Status report | `just clean status` | Show what would be cleaned |\n| Safe cleanup | `just clean all` | Safe cleanup (preserves running containers) |\n| Nuclear cleanup | `just clean nuke` | NUCLEAR: destroy everything (requires NUKE confirmation) |\n| Podman prune | `just clean podman` | Full podman system prune |\n| Images | `just clean images` | Dangling images only |\n| All images | `just clean images all` | All unused images |\n| Build cache | `just clean images build-cache` | Podman builder cache |\n| Containers | `just clean containers` | Stopped containers |\n| Runners | `just clean runners` | Stop/restart GitHub runners |\n| VMs | `just clean vm` | VM images (libvirt + cache) |\n| System | `just clean system` | Tmp files + journal |\n| Logs | `just clean logs` | Remove *.log files |\n| Docs | `just clean docs` | Remove site/ directory |\n| Output | `just clean output` | Remove output/ contents |\n| Cache menu | `just clean cache` | Cache cleanup submenu |\n| Pixi cache | `just clean cache pixi` | .pixi/ + ~/.cache/rattler |\n| Venv | `just clean cache venv` | venv/ directory |\n| Chunkhound | `just clean cache chunkhound` | .chunkhound/ directory |\n| Pip | `just clean cache pip` | ~/.cache/pip/ |\n| Pre-commit | `just clean cache precommit` | ~/.cache/pre-commit/ |\n| GitHub CLI | `just clean cache gh` | ~/.cache/gh/ |\n\n## Safe vs Nuclear Cleanup\n\n### Safe Cleanup (`just clean all`)\n\nSafe cleanup that preserves running containers and configurations:\n\n1. Stop GitHub runners\n2. Remove runner containers\n3. Remove stopped containers\n4. Remove buildah working containers\n5. Clean /var/tmp (buildah artifacts)\n6. Podman system prune\n7. Clean builder cache\n8. Prune unused images\n9. Remove build logs\n10. Remove docs output\n11. Remove build output\n12. Clean all caches\n13. Vacuum journal logs\n14. Prune volumes\n15. Restart GitHub runners\n\n**Use when:** You want to free disk space but keep your pod configurations intact.\n\n### Nuclear Cleanup (`just clean nuke`)\n\n**DESTROYS EVERYTHING** - requires typing 'NUKE' to confirm:\n\n- Removes ALL containers (running and stopped)\n- Removes ALL images\n- Removes ALL volumes\n- Removes ALL pod configurations\n- Removes ALL cached data\n- Cleans system caches\n\n**Use when:** You want a completely fresh start or are troubleshooting persistent issues.\n\n**Warning:** This will delete:\n\n- All pod configurations (you'll need to reconfigure)\n- All downloaded container images (will need to re-pull)\n- All model data if stored in containers\n- All runner configurations\n\n## Parameters\n\n```bash\njust clean [ACTION] [SUBOPTION]\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | See quick reference | Cleanup action |\n| `SUBOPTION` | Varies by action | Sub-action for nested menus |\n\n## Cleanup Actions\n\n### status\n\nShow what would be cleaned (dry-run):\n\n```bash\njust clean status\n```\n\n**Reports:**\n\n- Podman images/containers\n- System files (/var/tmp, journal)\n- Build artifacts (logs, docs, output)\n- Caches (pixi, venv, pip, etc.)\n\n### all\n\nSafe cleanup (15 steps):\n\n```bash\njust clean all\n```\n\n### nuke\n\nNuclear option (requires NUKE confirmation):\n\n```bash\njust clean nuke\n# Type 'NUKE' when prompted to confirm\n```\n\n### podman\n\nFull podman system prune:\n\n```bash\njust clean podman\n```\n\n**Removes:**\n\n- All unused images\n- Stopped containers\n- Unused volumes\n- Builder cache\n\n### images\n\nClean podman images:\n\n```bash\njust clean images              # Dangling only\njust clean images all          # All unused\njust clean images build-cache  # Builder cache\n```\n\n### containers\n\nRemove stopped containers:\n\n```bash\njust clean containers\n```\n\n### runners\n\nManage GitHub runners:\n\n```bash\njust clean runners stop   # Stop runners\njust clean runners start  # Start runners\n```\n\n### vm\n\nClean VM images:\n\n```bash\njust clean vm            # Interactive\njust clean vm libvirt    # Libvirt VMs\njust clean vm cache      # VM cache\n```\n\n### system\n\nSystem cleanup:\n\n```bash\njust clean system        # Interactive\njust clean system tmp    # Clean /var/tmp\njust clean system journal # Vacuum journal logs\n```\n\n### cache\n\nClean development caches:\n\n```bash\njust clean cache          # Interactive\njust clean cache pixi     # .pixi/ + ~/.cache/rattler\njust clean cache venv     # venv/\njust clean cache chunkhound # .chunkhound/\njust clean cache pip      # ~/.cache/pip/\njust clean cache precommit # ~/.cache/pre-commit/\njust clean cache gh       # ~/.cache/gh/\n```\n\n## Common Workflows\n\n### Check Before Cleanup\n\n```bash\n# See what would be cleaned\njust clean status\n\n# Then decide what to clean\njust clean podman\n```\n\n### Recover Disk Space\n\n```bash\n# Safe cleanup\njust clean all\n\n# Or targeted cleanup\njust clean images all\njust clean cache pixi\njust clean output\n```\n\n### Reset Build Environment\n\n```bash\n# Clean all caches and build artifacts\njust clean cache all\njust clean output\njust clean docs\n\n# Reinstall dependencies\njust docs-install\n```\n\n### Before Major Rebuild\n\n```bash\n# Clean containers and images\njust clean podman\n\n# Then rebuild\njust build os\n```\n\n### Complete Fresh Start\n\n```bash\n# Nuclear option - destroys everything\njust clean nuke\n# Type 'NUKE' to confirm\n\n# Reconfigure everything from scratch\nujust jupyter config\nujust ollama config\n```\n\n## Disk Space Targets\n\n| Target | Typical Size | Command |\n|--------|--------------|---------|\n| Podman images | 10-50GB | `clean podman` |\n| Builder cache | 1-10GB | `clean images build-cache` |\n| /var/tmp | 1-5GB | `clean system tmp` |\n| Journal logs | 100MB-1GB | `clean system journal` |\n| Pixi cache | 1-5GB | `clean cache pixi` |\n| Output/ | 1-20GB | `clean output` |\n\n## Troubleshooting\n\n### Cleanup Fails with Permission Error\n\n**Symptom:** Cannot remove files in output/ or /var/tmp\n\n**Fix:**\n\n```bash\n# Fix permissions\nsudo chown -R $USER:$USER output/\n\n# For /var/tmp\nsudo rm -rf /var/tmp/buildah*\n```\n\n### Podman Prune Doesn't Free Space\n\n**Symptom:** Images still present after prune\n\n**Cause:** Containers referencing images\n\n**Fix:**\n\n```bash\n# Stop and remove all containers first\njust clean containers\njust clean runners stop\n\n# Then prune\njust clean podman\n```\n\n### GitHub Runners Won't Restart\n\n**Symptom:** Runners fail to start after cleanup\n\n**Cause:** Configuration lost or token expired\n\n**Fix:**\n\n```bash\n# Re-authenticate\njust gh-login\n\n# Reconfigure runners\nujust runners config <REPO_URL> 1\n```\n\n## Cross-References\n\n- **Related Skills:** `pods` (build pods), `vms` (build VMs), `docs` (build docs)\n- **GitHub Runners:** `ujust runners` (runner management)\n- **Disk Analysis:** `just clean status`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"clean up\", \"cleanup\", \"free disk space\"\n- \"remove containers\", \"prune images\"\n- \"clean cache\", \"clear cache\"\n- \"just clean\", \"clean podman\"\n- \"disk full\", \"out of space\"\n- \"reset environment\", \"fresh start\"\n- \"nuclear cleanup\", \"destroy everything\""
              },
              {
                "name": "test",
                "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n",
                "path": "bazzite-ai-dev/skills/test/SKILL.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n"
                },
                "content": "# Test - Overlay Testing Management\n\n## Overview\n\nThe `test` command manages overlay testing sessions for bazzite-ai development. It creates symlinks from the repository to `/usr/share/bazzite-ai/just/`, allowing live editing of justfiles without rebuilding the OS image.\n\n**Key Concept:** On immutable OSTree systems (Bazzite-AI, Silverblue), `/usr` is read-only. Overlay mode temporarily unlocks it. On traditional systems (Fedora, CentOS), symlinks provide the same live-editing capability.\n\n**Command Prefix:**\n- `just test` - Development mode (from repository root, any Linux system)\n- `ujust test` - Installed mode (on bazzite-ai system with test.just installed)\n\nThe Quick Reference shows `ujust` commands (installed mode). The Common Workflows section shows `just` commands (development mode from repo root).\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Enable overlay | `ujust test overlay enable` | Bootstrap overlay testing session |\n| Check status | `ujust test overlay check` | Show current overlay/symlink status |\n| Refresh | `ujust test overlay refresh` | Regenerate 60-custom.just after changes |\n| VM testing | `ujust test vm` | VM testing submenu |\n| Install testing | `ujust test install` | Test install commands |\n| Install all | `ujust test install all` | Test all install commands |\n| System info | `ujust test info` | Show detailed system info |\n| Help | `ujust test help` | Show usage help |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust test ACTION=\"\" SUBACTION=\"\" ARGS...\n\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | `overlay`, `vm`, `install`, `info`, `help` | Primary action |\n| `SUBACTION` | `enable`, `check`, `refresh` (for overlay) | Subaction |\n| `ARGS` | varies | Additional arguments for vm/install |\n\n### Rule of Intent\n\nWhen `ACTION` is provided, the command runs non-interactively. Without it, an interactive menu appears.\n\n## Overlay Subcommands\n\n### Enable Overlay\n\n```bash\nujust test overlay enable\n\n```\n\n1. Activates overlay mode (OSTree) or creates symlinks (traditional)\n2. Detects repository location automatically\n3. Sets up symlinks to `/usr/share/bazzite-ai/just/`\n4. Generates `60-custom.just` import file\n5. Requires sudo (handles internally)\n\n### Check Status\n\n```bash\nujust test overlay check\n\n```\n\nShows current status:\n\n- **Immutable OS**: Whether overlay mode is active\n\n- **Traditional OS**: Whether symlinks are configured\n\n- Target repository path\n\n### Refresh Overlay\n\n```bash\nujust test overlay refresh\n\n```\n\nUse after:\n\n- Adding new `.just` files\n\n- Removing `.just` files\n\n- Modifying the generator script\n\nRegenerates `60-custom.just` without full restart.\n\n## VM Testing\n\n```bash\nujust test vm              # Interactive VM test menu\nujust test vm list         # List available VM tests\nujust test vm <name>       # Run specific VM test\n\n```\n\nDelegates to the VM testing harness for testing in virtual machines.\n\n## Install Testing\n\n```bash\nujust test install         # Interactive install test menu\nujust test install all     # Test all install commands\nujust test install <name>  # Test specific install command\n\n```\n\nTests install commands for validation.\n\n## Common Workflows\n\n### Initial Development Setup\n\n```bash\n# 1. Clone repository\ngit clone <repo-url> && cd bazzite-ai\n\n# 2. Enable overlay testing (one-time)\njust test overlay enable\n\n# 3. Make changes to justfiles\nvim just/bazzite-ai/my-feature.just\n\n# 4. Test immediately with ujust\nujust my-feature\n\n# 5. If adding new files, refresh\njust test overlay refresh\n\n```\n\n### After Reboot (Immutable OS Only)\n\n```bash\n# Overlay resets on reboot - re-enable\njust test overlay enable\n\n# Your git commits persist, overlay changes don't\n\n```\n\n### Testing a New Command\n\n```bash\n# 1. Create/edit the justfile\nvim just/bazzite-ai/new-command.just\n\n# 2. Refresh to pick up new file\njust test overlay refresh\n\n# 3. Test the command\nujust new-command\n\n```\n\n## OS Type Detection\n\n| OS Type | Detection | Overlay Method |\n|---------|-----------|----------------|\n| Immutable (OSTree) | `/run/ostree-booted` exists | `rpm-ostree` overlay |\n| Traditional | No OSTree marker | Symlinks only |\n\n## Troubleshooting\n\n### Overlay Not Active After Enable\n\n**Symptom:** `ujust test overlay check` shows \"Normal immutable mode\"\n\n**Cause:** Overlay activation failed or needs reboot\n\n**Fix:**\n\n```bash\n# Check if rpm-ostree unlock succeeded\nsudo rpm-ostree status | grep -i unlock\n\n# If not, try manual unlock\nsudo rpm-ostree usroverlay\n\n```\n\n### Symlinks Not Working\n\n**Symptom:** Changes to justfiles not reflected in `ujust` output\n\n**Cause:** Symlinks not properly created or 60-custom.just not regenerated\n\n**Fix:**\n\n```bash\n# Check symlink status\nls -la /usr/share/bazzite-ai/just/\n\n# Re-enable overlay\njust test overlay enable\n\n# Refresh imports\njust test overlay refresh\n\n```\n\n### Command Not Found After Adding File\n\n**Symptom:** New recipe not available in `ujust --list`\n\n**Cause:** 60-custom.just needs regeneration\n\n**Fix:**\n\n```bash\njust test overlay refresh\n\n```\n\n### Permission Denied\n\n**Symptom:** `sudo: a terminal is required`\n\n**Cause:** Running in non-interactive mode without passwordless sudo\n\n**Fix:**\n\n```bash\n# Enable passwordless sudo first\nujust config passwordless-sudo enable\n\n# Then retry\njust test overlay enable\n\n```\n\n## Cross-References\n\n- **Related Skills:** `install` (for testing install commands), `vm` (for VM testing)\n\n- **Configuration:** `ujust config passwordless-sudo enable` for sudo access\n\n- **Documentation:** [Overlay Testing Architecture](./references/overlay-architecture.md)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"enable overlay\", \"start testing session\", \"development mode\"\n\n- \"test my changes\", \"live reload justfiles\"\n\n- \"overlay not working\", \"symlinks not configured\"\n\n- \"refresh overlay\", \"pick up new files\"\n\n- \"VM testing\", \"test in VM\"\n\n- \"test install commands\""
              },
              {
                "name": "dpo",
                "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n",
                "path": "bazzite-ai-jupyter/skills/dpo/SKILL.md",
                "frontmatter": {
                  "name": "dpo",
                  "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n"
                },
                "content": "# Direct Preference Optimization (DPO)\n\n## Overview\n\nDPO learns from preference pairs (chosen vs rejected responses) without training an explicit reward model. It directly optimizes the policy using the Bradley-Terry preference model, making it simpler than RLHF while achieving comparable results. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `DPOTrainer` | Trainer for preference optimization |\n| `DPOConfig` | Training hyperparameters |\n| `beta` | Temperature for implicit reward (0.1 typical) |\n| `learning_rate` | 5e-6 (most conservative of RL methods) |\n| `ref_model` | Reference model for KL constraint |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## DPO Concepts\n\n### How DPO Works\n\n1. Given prompt + chosen response + rejected response\n2. Compute log-probabilities under policy and reference\n3. Optimize policy to increase P(chosen) / P(rejected) ratio\n4. Beta controls how strongly to enforce preference\n\n### Key Differences from RLHF\n\n| Aspect | DPO | RLHF |\n|--------|-----|------|\n| Reward Model | Implicit | Explicit |\n| Training | Single stage | Multi-stage |\n| Complexity | Simpler | More complex |\n| Compute | Lower | Higher |\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is when a function calls itself with a simpler version of the problem, including a base case to stop.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### From Comparison Data\n\n```python\ndef format_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"question\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"better_response\"],\n        \"rejected\": sample[\"worse_response\"],\n    }\n\ndataset = raw_dataset.map(format_preferences)\n```\n\n### Thinking Quality Preference Pairs\n\nFor thinking models, create preference pairs based on reasoning quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller, similar pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\nWhat's needed for it to work? A base case to stop the recursion.\n</think>\n\nRecursion is a programming technique where a function calls itself to solve a problem by breaking it into smaller, similar subproblems. For example, calculating factorial: n! = n * (n-1)!. Every recursive solution needs a base case to prevent infinite loops.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\nI can verify: 42 - 15 = 27. Correct!\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n    {\n        \"prompt\": \"Explain the difference between TCP and UDP.\",\n        \"chosen\": \"\"\"<think>\nWhat are TCP and UDP? They're network transport protocols.\nWhat's the key difference? TCP is connection-oriented, UDP is connectionless.\nWhat does that mean practically?\n- TCP: Reliable, ordered delivery with acknowledgments\n- UDP: Fast, no guarantees, better for streaming\nWhen would you use each?\n- TCP: File transfer, web browsing, email\n- UDP: Video streaming, gaming, DNS\n</think>\n\nTCP is a connection-oriented protocol that guarantees reliable, ordered delivery through acknowledgments and retransmission. UDP is connectionless, offering faster but unreliable delivery without guarantees. Use TCP for reliability (file transfers, web), UDP for speed (streaming, gaming).\"\"\",\n        \"rejected\": \"TCP is reliable, UDP is not.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n\ndef format_thinking_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"chosen\"],\n        \"rejected\": sample[\"rejected\"],\n    }\n\ndataset = dataset.map(format_thinking_preferences)\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for DPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## DPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import DPOConfig\n\ndpo_config = DPOConfig(\n    output_dir=\"./dpo_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=5e-6,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    beta=0.1,\n    max_length=512,\n    max_prompt_length=256,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.1-0.5 | Implicit reward temperature |\n| `learning_rate` | 1e-6 to 5e-6 | Lower than SFT |\n| `max_length` | 512-1024 | Max combined length |\n| `max_prompt_length` | 256-512 | Max prompt length |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import DPOTrainer\n\ntrainer = DPOTrainer(\n    model=model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n### With Reference Model\n\n```python\n# For stronger KL constraint\nref_model, _ = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n```\n\n## Beta Selection Guide\n\n| Beta | Use Case |\n|------|----------|\n| 0.01 | Weak preference signal |\n| 0.1 | Standard (recommended) |\n| 0.3 | Strong preference enforcement |\n| 0.5+ | Very strong (may overfit) |\n\n## Troubleshooting\n\n### Chosen/Rejected Scores Similar\n\n**Symptom:** Model doesn't distinguish preferences\n\n**Fix:**\n- Increase `beta` for stronger signal\n- Train longer\n- Check data quality (clear preference differences)\n\n### Overfitting to Preferences\n\n**Symptom:** Model only outputs chosen-style responses\n\n**Fix:**\n- Lower `beta`\n- Use reference model\n- Add regularization\n\n### Low Accuracy\n\n**Symptom:** DPO accuracy metric stays low\n\n**Fix:**\n- Ensure chosen is genuinely better than rejected\n- Increase training steps\n- Check prompt formatting\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Set `ref_model=None` (uses implicit reference)\n- Reduce `max_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nDPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- You have preference data (chosen vs rejected)\n- Simpler pipeline than RLHF desired\n- No reward model available\n- Post-SFT alignment\n- Human preference learning\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before DPO\n- `bazzite-ai-jupyter:grpo` - Alternative with explicit rewards\n- `bazzite-ai-jupyter:rloo` - Alternative RL with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models (alternative to DPO)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "evaluation",
                "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n",
                "path": "bazzite-ai-jupyter/skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n"
                },
                "content": "# LLM Evaluation with Evidently.ai\n\n## Overview\n\nEvidently.ai provides tools for evaluating LLM outputs using descriptors (row-level metrics) and reports. It supports automated prompt optimization and LLM-as-a-Judge patterns for quality assessment.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `Dataset` | Wrapper for evaluation data |\n| `Descriptor` | Row-level score or label |\n| `Report` | Aggregate metrics |\n| `TextEvals` | Text quality metrics |\n| `LLMJudge` | LLM-based evaluation |\n| `PromptOptimizer` | Automated prompt tuning |\n\n## Basic Setup\n\n```python\nimport pandas as pd\nfrom evidently import Dataset, DataDefinition\nfrom evidently.descriptors import TextLength, Sentiment, WordCount\n\n# Sample data\ndata = [\n    {\"question\": \"What is Python?\", \"answer\": \"Python is a programming language.\"},\n    {\"question\": \"Explain AI.\", \"answer\": \"AI is artificial intelligence.\"},\n]\n\ndf = pd.DataFrame(data)\n\n# Define data structure\ndefinition = DataDefinition(text_columns=[\"question\", \"answer\"])\n\n# Create Evidently Dataset\neval_dataset = Dataset.from_pandas(df, data_definition=definition)\n```\n\n## Text Descriptors\n\n### Basic Metrics\n\n```python\nfrom evidently.descriptors import TextLength, WordCount, Sentiment\n\n# Add descriptors\neval_dataset.add_descriptors(descriptors=[\n    TextLength(column=\"answer\"),\n    WordCount(column=\"answer\"),\n    Sentiment(column=\"answer\")\n])\n\n# View results\neval_dataset.as_dataframe()\n```\n\n### Available Descriptors\n\n| Descriptor | Description |\n|------------|-------------|\n| `TextLength` | Character count |\n| `WordCount` | Word count |\n| `Sentiment` | Sentiment score (-1 to 1) |\n| `RegexMatch` | Regex pattern matching |\n| `Contains` | Substring presence |\n| `IsValidJSON` | JSON validity check |\n| `IsValidPython` | Python syntax check |\n\n## LLM-as-a-Judge\n\n### Binary Classification\n\n```python\nimport os\nfrom evidently.descriptors import LLMJudge\nfrom evidently.llm import OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Configure Ollama as provider\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Create judge\njudge = LLMJudge(\n    provider=provider,\n    template=\"Is this answer helpful? Answer YES or NO.\\n\\nQuestion: {question}\\nAnswer: {answer}\",\n    include_reasoning=True\n)\n\neval_dataset.add_descriptors(descriptors=[judge])\n```\n\n### Multi-Class Classification\n\n```python\nfrom evidently.descriptors import LLMJudge\n\njudge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Classify this query into one category: BOOKING, CANCELLATION, GENERAL.\n\nQuery: {query}\n\nCategory:\"\"\",\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"],\n    include_reasoning=True\n)\n```\n\n### Quality Scoring\n\n```python\nfrom evidently.descriptors import LLMJudge\n\nquality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Rate this code review on a scale of 1-5.\n\nCode Review: {review}\n\nScore (1-5):\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Prompt Optimization\n\n### Setup Optimizer\n\n```python\nfrom evidently.llm import PromptOptimizer, OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\noptimizer = PromptOptimizer(\n    provider=provider,\n    max_iterations=10\n)\n```\n\n### Binary Classification Optimization\n\n```python\n# Initial prompt template\ninitial_prompt = \"\"\"Classify if this code review is good or bad.\n\nReview: {review}\n\nAnswer (GOOD or BAD):\"\"\"\n\n# Define judge for evaluation\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"GOOD\", \"BAD\"]\n)\n\n# Run optimization\nbest_prompt = optimizer.optimize(\n    dataset=eval_dataset,\n    initial_template=initial_prompt,\n    target_column=\"label\",  # Ground truth column\n    judge=judge\n)\n\nprint(\"Best prompt found:\")\nprint(best_prompt)\n```\n\n### Multi-Class Optimization\n\n```python\ninitial_prompt = \"\"\"Classify this query.\n\nQuery: {query}\n\nCategory (BOOKING/CANCELLATION/GENERAL):\"\"\"\n\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"]\n)\n\nbest_prompt = optimizer.optimize(\n    dataset=dataset,\n    initial_template=initial_prompt,\n    target_column=\"category\",\n    judge=judge\n)\n```\n\n## Reports\n\n### Generate Report\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import TextDescriptorsDriftMetric\n\nreport = Report(metrics=[\n    TextDescriptorsDriftMetric(column=\"answer\")\n])\n\nreport.run(reference_data=reference_dataset, current_data=current_dataset)\nreport.show()\n```\n\n### Save Report\n\n```python\nreport.save_html(\"evaluation_report.html\")\nreport.save_json(\"evaluation_report.json\")\n```\n\n## Common Patterns\n\n### Evaluate RAG Quality\n\n```python\nfrom evidently.descriptors import LLMJudge, TextLength, Contains\n\n# Relevance judge\nrelevance_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer relevant to the question?\n\nQuestion: {question}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\n# Factuality judge\nfactuality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer factually accurate based on the context?\n\nContext: {context}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\neval_dataset.add_descriptors([\n    relevance_judge,\n    factuality_judge,\n    TextLength(column=\"answer\")\n])\n```\n\n### Compare Models\n\n```python\n# Evaluate model A\nmodel_a_dataset = run_inference(model_a, test_data)\nmodel_a_dataset.add_descriptors([quality_judge])\n\n# Evaluate model B\nmodel_b_dataset = run_inference(model_b, test_data)\nmodel_b_dataset.add_descriptors([quality_judge])\n\n# Compare\nprint(\"Model A average score:\", model_a_dataset.as_dataframe()[\"quality\"].mean())\nprint(\"Model B average score:\", model_b_dataset.as_dataframe()[\"quality\"].mean())\n```\n\n## Troubleshooting\n\n### Slow Evaluation\n\n**Symptom:** Evaluation takes too long\n\n**Fix:**\n\n- Reduce dataset size for initial testing\n- Use smaller/faster judge model\n- Batch requests where possible\n\n### Inconsistent Judgments\n\n**Symptom:** LLM judge gives inconsistent scores\n\n**Fix:**\n\n- Lower temperature (0.0-0.3)\n- Make prompt more specific\n- Add examples to prompt\n- Use structured output options\n\n### Optimization Not Improving\n\n**Symptom:** Prompt optimization stuck\n\n**Fix:**\n\n- Increase `max_iterations`\n- Try different initial prompts\n- Check ground truth labels are correct\n- Use more training examples\n\n## When to Use This Skill\n\nUse when:\n\n- Measuring LLM output quality\n- Comparing different prompts\n- Automating prompt engineering\n- Building evaluation pipelines\n- Monitoring LLM performance over time\n\n## Evaluating Thinking Models\n\nFor thinking models (Qwen3-Thinking), evaluate both thinking quality and response quality:\n\n```python\nthinking_quality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Evaluate the quality of reasoning in this response.\n\nQuestion: {question}\nResponse: {response}\n\nScore the THINKING quality (1-5):\n1 = No reasoning shown\n2 = Minimal reasoning\n3 = Some step-by-step thinking\n4 = Good reasoning with self-questioning\n5 = Excellent thorough reasoning\n\nScore:\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain for LLM calls\n- `bazzite-ai-jupyter:rag` - RAG evaluation patterns\n- `bazzite-ai-jupyter:sft` - Training thinking models\n- `bazzite-ai-jupyter:inference` - Thinking model parsing\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility"
              },
              {
                "name": "finetuning",
                "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n",
                "path": "bazzite-ai-jupyter/skills/finetuning/SKILL.md",
                "frontmatter": {
                  "name": "finetuning",
                  "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n"
                },
                "content": "# Model Fine-Tuning\n\n## Overview\n\nFine-tuning adapts a pre-trained LLM to specific tasks by training on task-specific data. This skill covers both manual PyTorch training and HuggingFace's high-level Trainer API.\n\n**Recommended**: For 2x faster training with less memory, use **Unsloth** (see `bazzite-ai-jupyter:sft`).\n\n## Quick Reference\n\n| Approach | Use Case | Speed |\n|----------|----------|-------|\n| **Unsloth + SFTTrainer** | **Recommended default** | **2x faster** |\n| PyTorch Manual | Full control, custom training | Baseline |\n| HuggingFace Trainer | Standard training, less code | Fast |\n| SFTTrainer | Instruction/chat fine-tuning | Fast |\n\n## Method Comparison\n\n| Method | Learning Rate | Use Case |\n|--------|---------------|----------|\n| SFT | 2e-4 | Instruction tuning (first step) |\n| GRPO | 1e-5 | RL with rewards |\n| DPO | 5e-6 | Preference learning |\n| RLOO | 1e-5 | RL with lower variance |\n| Reward | 1e-5 | Reward model training |\n\n## Unsloth Quickstart (Recommended)\n\n```python\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Apply LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train\ntrainer = SFTTrainer(\n    model=model, tokenizer=tokenizer, train_dataset=dataset,\n    args=SFTConfig(\n        output_dir=\"./output\",\n        max_steps=100,\n        learning_rate=2e-4,\n        bf16=is_bf16_supported(),\n        optim=\"adamw_8bit\",\n    ),\n)\ntrainer.train()\n```\n\nSee `bazzite-ai-jupyter:sft` for complete Unsloth patterns.\n\n## Dataset Preparation\n\n### Load from HuggingFace Hub\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"test\"]\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n```\n\n### Data Format\n\n```python\n# Example conversation format\nexample = train_data[0]\nprint(example[\"text\"])\n\n# Output:\n# ### Human: What is Python?\n# ### Assistant: Python is a programming language...\n```\n\n### Create Prompt Template\n\n```python\ndef build_prompt(instruction, response=None):\n    prompt = f\"### Human: {instruction}\\n### Assistant:\"\n    if response:\n        prompt += f\" {response}\"\n    return prompt\n\n# For training\ntrain_prompt = build_prompt(\"What is AI?\", \"AI is artificial intelligence.\")\n\n# For inference\ninference_prompt = build_prompt(\"What is AI?\")\n```\n\n## Tokenization\n\n### Setup Tokenizer\n\n```python\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Ensure pad token exists\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Tokenize Dataset\n\n```python\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\ntokenized_train = train_data.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_data.column_names\n)\n\ntokenized_train.set_format(\"torch\")\n```\n\n## PyTorch Training (Manual)\n\n### Setup Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n```\n\n### Training Configuration\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass TrainConfig:\n    batch_size: int = 4\n    learning_rate: float = 2e-5\n    num_epochs: int = 3\n    max_length: int = 512\n    warmup_steps: int = 100\n    weight_decay: float = 0.01\n    output_dir: str = \"./checkpoints\"\n\ncfg = TrainConfig()\n```\n\n### DataLoader\n\n```python\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    tokenized_train,\n    batch_size=cfg.batch_size,\n    shuffle=True\n)\n```\n\n### Optimizer and Scheduler\n\n```python\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.learning_rate,\n    weight_decay=cfg.weight_decay\n)\n\ntotal_steps = len(train_loader) * cfg.num_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=cfg.warmup_steps,\n    num_training_steps=total_steps\n)\n```\n\n### Training Loop\n\n```python\nfrom tqdm.auto import tqdm\n\nmodel.train()\ndevice = next(model.parameters()).device\n\nfor epoch in range(cfg.num_epochs):\n    total_loss = 0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in progress:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = input_ids.clone()\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        progress.set_postfix({\"loss\": loss.item()})\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n\n    # Save checkpoint\n    model.save_pretrained(f\"{cfg.output_dir}/epoch_{epoch+1}\")\n```\n\n## HuggingFace Trainer\n\n### TrainingArguments\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True,\n    fp16=True,  # Mixed precision\n)\n```\n\n### Create Trainer\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n)\n```\n\n### Train and Evaluate\n\n```python\n# Train\ntrain_result = trainer.train()\n\n# Save\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\n\n# Evaluate\nmetrics = trainer.evaluate()\nprint(metrics)\n```\n\n## SFTTrainer (Instruction Tuning)\n\n### Setup\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-5,\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    packing=False,  # Don't pack multiple samples\n)\n```\n\n### Train with SFTTrainer\n\n```python\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_data,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",  # Column with training text\n)\n\ntrainer.train()\ntrainer.save_model(\"./sft_model\")\n```\n\n## Evaluation\n\n### Evaluation Function\n\n```python\ndef evaluate(model, dataloader):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = input_ids.clone()\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            total_loss += outputs.loss.item()\n\n    return total_loss / len(dataloader)\n```\n\n### Perplexity\n\n```python\nimport math\n\neval_loss = evaluate(model, val_loader)\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity:.2f}\")\n```\n\n## Inference with Fine-Tuned Model\n\n```python\ndef generate_response(model, tokenizer, prompt, max_new_tokens=128):\n    model.eval()\n    device = next(model.parameters()).device\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test\nprompt = build_prompt(\"What is machine learning?\")\nresponse = generate_response(model, tokenizer, prompt)\nprint(response)\n```\n\n## Checkpointing\n\n### Save Checkpoint\n\n```python\n# Save model and tokenizer\nmodel.save_pretrained(\"./checkpoint\")\ntokenizer.save_pretrained(\"./checkpoint\")\n```\n\n### Load Checkpoint\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"./checkpoint\")\n```\n\n### Resume Training\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n)\n\ntrainer.train(resume_from_checkpoint=\"./checkpoint\")\n```\n\n## Hyperparameters Guide\n\n| Parameter | Typical Values | Notes |\n|-----------|----------------|-------|\n| `learning_rate` | 1e-5 to 5e-5 | Lower for larger models |\n| `batch_size` | 4, 8, 16 | Limited by GPU memory |\n| `epochs` | 1-5 | More for smaller datasets |\n| `warmup_steps` | 5-10% of total | Stabilizes early training |\n| `weight_decay` | 0.01-0.1 | Regularization |\n| `max_length` | 512, 1024, 2048 | Context window |\n\n## When to Use This Skill\n\nUse when:\n\n- Adapting LLM to specific domain/task\n- Improving model performance on your data\n- Creating instruction-following models\n- Need full control over training process\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Unsloth-optimized SFT (recommended)\n- `bazzite-ai-jupyter:grpo` - RL with reward functions\n- `bazzite-ai-jupyter:dpo` - Preference learning\n- `bazzite-ai-jupyter:rloo` - RL with lower variance\n- `bazzite-ai-jupyter:quantization` - Memory-efficient training\n- `bazzite-ai-jupyter:peft` - Parameter-efficient fine-tuning\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Architecture understanding"
              },
              {
                "name": "grpo",
                "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n",
                "path": "bazzite-ai-jupyter/skills/grpo/SKILL.md",
                "frontmatter": {
                  "name": "grpo",
                  "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n"
                },
                "content": "# Group Relative Policy Optimization (GRPO)\n\n## Overview\n\nGRPO is a reinforcement learning method for LLM alignment. It generates multiple completions per prompt, scores them with a reward function, and optimizes the policy to favor higher-reward responses using relative policy gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `GRPOTrainer` | RL trainer for policy optimization |\n| `GRPOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `beta` | KL penalty coefficient (0.1 typical) |\n| `num_generations` | Completions per prompt (2-4) |\n| `learning_rate` | 1e-5 (10x lower than SFT) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import GRPOConfig, GRPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Setting `ACCELERATE_MIXED_PRECISION` after imports may cause training issues.\n\n## GRPO Concepts\n\n### How GRPO Works\n\n1. Generate multiple completions for each prompt\n2. Score completions with reward function(s)\n3. Compute relative advantages within each group\n4. Update policy to favor higher-reward completions\n5. Apply KL penalty to prevent divergence from reference\n\n### Key Differences from PPO\n\n| Aspect | GRPO | PPO |\n|--------|------|-----|\n| Baseline | Group relative | Value function |\n| Critic | Not needed | Required |\n| Memory | Lower | Higher |\n| Stability | Good | Can be unstable |\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for GRPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Dataset Format\n\n```python\n# GRPO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"What is recursion?\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response length.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        if length < 5:\n            rewards.append(-1.0)\n        elif length < 50:\n            rewards.append(1.0)\n        else:\n            rewards.append(0.5)\n    return rewards\n```\n\n### LLM-as-Judge Reward\n\n```python\ndef llm_judge_reward(completions, prompts):\n    \"\"\"Use another LLM to score responses.\"\"\"\n    rewards = []\n    for prompt, completion in zip(prompts, completions):\n        score = judge_model.evaluate(prompt, completion)\n        rewards.append(score)\n    return rewards\n```\n\n### Rule-Based Reward\n\n```python\ndef format_reward(completions, prompts=None):\n    \"\"\"Reward proper formatting.\"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n        if completion.endswith(\".\"):\n            score += 0.5\n        if not completion.startswith(\" \"):\n            score += 0.5\n        rewards.append(score)\n    return rewards\n```\n\n### Composite Rewards\n\n```python\ndef combined_reward(completions, prompts):\n    \"\"\"Combine multiple reward signals.\"\"\"\n    length_scores = length_reward(completions)\n    format_scores = format_reward(completions)\n    return [0.5 * l + 0.5 * f for l, f in zip(length_scores, format_scores)]\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n### Multi-Objective Thinking Reward (Token-Based)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef comprehensive_thinking_reward(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Evaluate multiple aspects of thinking quality using token IDs.\n\n    Scoring breakdown:\n    - Has </think> token: +0.3\n    - Thinking depth (20+ tokens): +0.3\n    - Structured sentences: +0.2\n    - Self-questioning: +0.1\n    - Step-by-step reasoning: +0.1\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        score = 0.0\n\n        # Token-based boundary detection\n        if THINK_END_TOKEN_ID in comp_ids:\n            score += 0.3  # Has proper </think> token\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count\n\n            # Extract thinking content for text analysis\n            thinking = completion.split('</think>')[0]\n\n            # Depth (token count from IDs)\n            if thinking_length >= 20:\n                score += 0.3\n            elif thinking_length >= 10:\n                score += 0.2\n\n            # Structure (sentences in text)\n            sentences = thinking.count('.') + thinking.count('!')\n            if sentences >= 2:\n                score += 0.2\n\n            # Self-questioning\n            if '?' in thinking:\n                score += 0.1\n\n            # Step-by-step reasoning\n            if any(w in thinking.lower() for w in ['first', 'then', 'next', 'finally']):\n                score += 0.1\n        else:\n            score = -0.5  # Penalize missing </think> token\n\n        rewards.append(score)\n\n    return rewards\n```\n\n## GRPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import GRPOConfig\n\ngrpo_config = GRPOConfig(\n    output_dir=\"./grpo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_completion_length=128,\n    num_generations=4,\n    beta=0.1,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.01-0.1 | KL penalty strength |\n| `num_generations` | 2-8 | Completions per prompt |\n| `max_completion_length` | 64-256 | Generation length |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n\n## Training\n\n### Basic Training Loop\n\n```python\nfrom trl import GRPOTrainer\n\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=length_reward,\n)\n\ntrainer.train()\n```\n\n### Multiple Reward Functions\n\n```python\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=[length_reward, format_reward],\n    reward_weights=[0.5, 0.5],\n)\n```\n\n## Troubleshooting\n\n### Reward Hacking\n\n**Symptom:** Model exploits reward function (e.g., always outputs same length)\n\n**Fix:**\n- Add diversity penalties\n- Use multiple reward signals\n- Cap maximum reward\n\n### KL Divergence Too High\n\n**Symptom:** Policy diverges too far from reference\n\n**Fix:**\n- Increase `beta` (stronger KL penalty)\n- Reduce `learning_rate`\n- Fewer training steps\n\n### Training Instability\n\n**Symptom:** Loss spikes or NaN\n\n**Fix:**\n- Lower `learning_rate` to 5e-6\n- Reduce `num_generations` to 2\n- Check reward scale (should be roughly -1 to 1)\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2\n- Use gradient checkpointing\n- Reduce `max_completion_length`\n\n## Kernel Shutdown (Jupyter)\n\nGRPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Aligning models with human preferences\n- Optimizing for specific behaviors\n- Post-SFT refinement\n- Building reward-driven systems\n- Simpler alternative to PPO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before GRPO\n- `bazzite-ai-jupyter:dpo` - Simpler preference learning (no reward model)\n- `bazzite-ai-jupyter:rloo` - Alternative RL method with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models for GRPO\n- `bazzite-ai-jupyter:peft` - LoRA for efficient RL\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-ollama:api` - Reward model inference"
              },
              {
                "name": "inference",
                "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n",
                "path": "bazzite-ai-jupyter/skills/inference/SKILL.md",
                "frontmatter": {
                  "name": "inference",
                  "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n"
                },
                "content": "# Fast Inference\n\n## Overview\n\nUnsloth provides optimized inference through the vLLM backend, enabling 2x faster generation compared to standard HuggingFace inference. This skill covers fast inference setup, thinking model output parsing, and memory management.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `fast_inference=True` | Enable vLLM backend for 2x speedup |\n| `model.fast_generate()` | vLLM-accelerated generation |\n| `SamplingParams` | Control generation (temperature, top_p, etc.) |\n| `FastLanguageModel.for_inference()` | Merge LoRA adapters for inference |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\nimport torch\nimport vllm\nfrom vllm import SamplingParams\n```\n\n## Environment Verification\n\nBefore inference, verify your environment is correctly configured:\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nimport vllm\n\n# Check versions\nprint(f\"unsloth: {unsloth.__version__}\")\nprint(f\"vLLM: {vllm.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n```\n\n## Standard Inference (No vLLM)\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Prepare for inference (merges LoRA adapters if present)\nFastLanguageModel.for_inference(model)\n```\n\n### Generate Response\n\n```python\nmessages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode only new tokens\ninput_length = inputs[\"input_ids\"].shape[1]\nresponse = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\nprint(response)\n```\n\n## Fast Inference (vLLM Backend)\n\n### Load Model with Fast Inference\n\n```python\nfrom unsloth import FastLanguageModel\nfrom vllm import SamplingParams\n\nMODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    MODEL_NAME,\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,  # Enable vLLM backend\n)\n```\n\n### Fast Generate\n\n```python\nFastLanguageModel.for_inference(model)\n\nmessages = [{\"role\": \"user\", \"content\": \"What is 15 + 27? Show your thinking.\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nsampling_params = SamplingParams(\n    temperature=0.6,      # Recommended for thinking models\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,      # Increased for thinking + response\n)\n\n# Use fast_generate instead of generate\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\n\n# Extract output\nraw_output = outputs[0].outputs[0].text\noutput_token_ids = outputs[0].outputs[0].token_ids\nprint(raw_output)\n```\n\n### Sampling Parameters\n\n```python\nfrom vllm import SamplingParams\n\n# Conservative (factual responses)\nconservative = SamplingParams(\n    temperature=0.3,\n    top_p=0.9,\n    max_tokens=512,\n)\n\n# Balanced (general use)\nbalanced = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=1024,\n)\n\n# Creative (diverse outputs)\ncreative = SamplingParams(\n    temperature=0.9,\n    top_p=0.95,\n    top_k=50,\n    max_tokens=2048,\n)\n\n# Thinking models (allow long reasoning)\nthinking = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,  # Extra space for <think> content\n)\n```\n\n## Thinking Model Output Parsing\n\nQwen3-Thinking models use `<think>...</think>` tags to separate reasoning from final responses. Use token-based parsing for accuracy.\n\n### Token-Based Parsing (Recommended)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"\n    Parse thinking model output using token ID boundary.\n\n    With Thinking models + add_generation_prompt=True:\n    - Template adds <think> to prompt\n    - Model output starts with thinking content\n    - Model outputs </think> (token 151668) when done\n    - Final response follows </think>\n\n    Args:\n        token_ids: Output token IDs from generation\n        tokenizer: Model tokenizer\n\n    Returns:\n        tuple: (thinking_content, response_content)\n    \"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking_tokens = token_list[:end_idx]\n        response_tokens = token_list[end_idx + 1:]\n\n        thinking = tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n    else:\n        # No </think> found - model may still be thinking\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True).strip()\n        response = \"(Model did not complete thinking - increase max_tokens)\"\n\n    return thinking, response\n```\n\n### Usage Example\n\n```python\n# Generate with fast_inference\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\noutput_token_ids = outputs[0].outputs[0].token_ids\n\n# Parse thinking and response\nthinking, response = parse_thinking_response(output_token_ids, tokenizer)\n\nprint(\"=== THINKING ===\")\nprint(thinking)\nprint(\"\\n=== RESPONSE ===\")\nprint(response)\n```\n\n### Verification\n\n```python\n# Verify parsing worked correctly\nthink_tag_found = THINK_END_TOKEN_ID in list(output_token_ids)\nhas_thinking = bool(thinking) and \"did not complete\" not in response\nhas_response = bool(response) and \"did not complete\" not in response\n\nprint(f\"</think> token found: {'Yes' if think_tag_found else 'No'}\")\nprint(f\"Thinking extracted: {'Yes' if has_thinking else 'No'}\")\nprint(f\"Response extracted: {'Yes' if has_response else 'No'}\")\n\nif not think_tag_found:\n    print(\"Tip: Increase max_tokens in SamplingParams\")\n```\n\n## Batch Inference\n\n### Multiple Prompts\n\n```python\nprompts = [\n    \"What is recursion?\",\n    \"Explain machine learning in simple terms.\",\n    \"What is the difference between Python and JavaScript?\",\n]\n\n# Format all prompts\nformatted_prompts = [\n    tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": p}],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    for p in prompts\n]\n\n# Batch generate (vLLM handles parallelization)\nsampling_params = SamplingParams(temperature=0.6, max_tokens=512)\noutputs = model.fast_generate(formatted_prompts, sampling_params=sampling_params)\n\n# Process results\nfor i, output in enumerate(outputs):\n    print(f\"\\n=== Prompt {i+1} ===\")\n    print(f\"Q: {prompts[i]}\")\n    print(f\"A: {output.outputs[0].text}\")\n```\n\n## Memory Management\n\n### GPU Memory Monitoring\n\n```python\nimport subprocess\n\ndef measure_gpu_memory():\n    \"\"\"Measure current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\n# Usage\nprint(f\"GPU memory used: {measure_gpu_memory()} MB\")\n```\n\n### Memory Cleanup\n\n```python\nimport gc\nimport torch\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage after inference\ncleanup_memory()\nprint(f\"GPU memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Jupyter Kernel Shutdown (Critical for vLLM)\n\n**vLLM does NOT release GPU memory within a Jupyter session.** Kernel restart is required between model tests:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of notebooks that use `fast_inference=True`. Without kernel shutdown, loading a different model will fail with OOM.\n\n**Notebook pattern**: All finetuning notebooks end with a shutdown cell.\n\n## Model Loading Patterns\n\n### Pre-Quantized Models (Recommended)\n\n```python\n# Fast loading with pre-quantized models\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # Pre-quantized\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,\n)\n```\n\n### On-Demand Quantization\n\n```python\n# Quantize during loading (slower initial load)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize on load\n    fast_inference=True,\n)\n```\n\n### Post-Training Inference\n\n```python\n# After SFT/GRPO/DPO training\nFastLanguageModel.for_inference(model)  # Merge LoRA adapters\n\n# Then generate as normal\noutputs = model.generate(**inputs, max_new_tokens=512)\n```\n\n## Supported Models\n\n| Model | Path | Parameters | Use Case |\n|-------|------|------------|----------|\n| Qwen3-4B-Thinking | `unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit` | 4B | Reasoning, chain-of-thought |\n| Ministral-3B-Reasoning | `unsloth/Ministral-3-3B-Reasoning-2512` | 3B | Fast reasoning |\n| Qwen3-4B | `unsloth/Qwen3-4B-unsloth-bnb-4bit` | 4B | General instruction following |\n| Llama-3.2-3B | `unsloth/Llama-3.2-3B-Instruct-bnb-4bit` | 3B | General instruction following |\n\n## Troubleshooting\n\n### vLLM Not Available\n\n**Symptom:** `fast_inference=True` fails or falls back to standard inference\n\n**Fix:**\n```python\n# Check vLLM installation\nimport inspect\nsig = inspect.signature(FastLanguageModel.from_pretrained)\nif 'fast_inference' in sig.parameters:\n    print(\"fast_inference parameter available\")\nelse:\n    print(\"vLLM not available - using standard inference\")\n```\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory during inference\n\n**Fix:**\n- Use 4-bit quantization (`load_in_4bit=True`)\n- Reduce `max_seq_length`\n- Reduce `max_tokens` in SamplingParams\n- Use `cleanup_memory()` between batches\n\n### Incomplete Thinking\n\n**Symptom:** `</think>` token not found in output\n\n**Fix:**\n- Increase `max_tokens` in SamplingParams (try 2048+)\n- Check that model is a Thinking variant\n- Verify `add_generation_prompt=True` in chat template\n\n### GPU Memory Not Released\n\n**Symptom:** Memory stays high after inference\n\n**Fix:**\n- Call `cleanup_memory()`\n- Restart Jupyter kernel between model tests\n- Use `del model` then `cleanup_memory()`\n\n## When to Use This Skill\n\nUse when:\n- Running inference on fine-tuned models\n- Need fast batch inference\n- Working with thinking/reasoning models\n- Optimizing inference latency\n- Parsing chain-of-thought outputs\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Supervised fine-tuning (train before inference)\n- `bazzite-ai-jupyter:peft` - LoRA adapter loading\n- `bazzite-ai-jupyter:quantization` - Quantization options\n- `bazzite-ai-jupyter:transformers` - Transformer architecture background\n- `bazzite-ai-ollama:api` - Ollama deployment for production"
              },
              {
                "name": "langchain",
                "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n",
                "path": "bazzite-ai-jupyter/skills/langchain/SKILL.md",
                "frontmatter": {
                  "name": "langchain",
                  "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n"
                },
                "content": "# LangChain Framework\n\n## Overview\n\nLangChain is a framework for building LLM applications. It provides abstractions for prompts, models, chains, and output parsing that work with both local models (HuggingFace, Ollama) and cloud APIs (OpenAI, Anthropic).\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `ChatOpenAI` | Connect to Ollama (OpenAI-compatible) |\n| `HuggingFacePipeline` | Wrap local HuggingFace models |\n| `ChatHuggingFace` | Chat interface for HF models |\n| `PromptTemplate` | Single-string prompt formatting |\n| `ChatPromptTemplate` | Multi-message prompt formatting |\n| `PydanticOutputParser` | Structured output parsing |\n\n## Model Wrappers\n\n### Ollama via OpenAI-Compatible API\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",  # Required by library, ignored by Ollama\n    model=MODEL,\n    temperature=0.7,\n    max_tokens=150\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n### HuggingFace Local Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n\nHF_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n\n# 4-bit quantization for memory efficiency\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\n# Create pipeline\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=150,\n    return_full_text=False\n)\n\n# Wrap for LangChain\nllm = HuggingFacePipeline(pipeline=text_pipeline)\nchat_llm = ChatHuggingFace(llm=llm)\n```\n\n## LLM Methods\n\n### invoke() - Single Input\n\n```python\nresponse = llm.invoke(\"Tell me a fact about Mars.\")\nprint(response)\n```\n\n### batch() - Multiple Inputs\n\n```python\nprompts = [\"Tell me a joke\", \"Translate to German: Hello!\"]\nresults = llm.batch(prompts)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {result}\\n\")\n```\n\n### generate() - With Metadata\n\n```python\nresults = llm.generate([\"Where should I go for a Safari?\"])\n\nfor gen in results.generations:\n    print(gen[0].text)\n\n# Access token counts\nprint(results.llm_output)\n```\n\n### stream() - Token Streaming\n\n```python\nfor chunk in llm.stream(\"Tell me a story about a cat.\"):\n    print(chunk, end=\"\", flush=True)\n```\n\n## Prompt Templates\n\n### Basic PromptTemplate\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Explain {topic} in simple terms.\"\n)\n\nformatted = template.format(topic=\"quantum computing\")\nresponse = llm.invoke(formatted)\n```\n\n### ChatPromptTemplate\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful legal translator.\"),\n    (\"human\", \"Simplify this legal text: {legal_text}\")\n])\n\nmessages = chat_prompt.format_messages(legal_text=\"...\")\nresponse = chat_llm.invoke(messages)\n```\n\n## Few-Shot Learning\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define examples\nexamples = [\n    {\"input\": \"Legal term 1\", \"output\": \"Plain explanation 1\"},\n    {\"input\": \"Legal term 2\", \"output\": \"Plain explanation 2\"}\n]\n\n# Build few-shot prompt\nmessages = [\n    (\"system\", \"Translate legal terms to plain language.\")\n]\nfor ex in examples:\n    messages.append((\"human\", ex[\"input\"]))\n    messages.append((\"assistant\", ex[\"output\"]))\nmessages.append((\"human\", \"{new_input}\"))\n\nfew_shot_prompt = ChatPromptTemplate.from_messages(messages)\n```\n\n## Output Parsing\n\n### Pydantic Parser\n\n```python\nfrom pydantic import BaseModel, Field\nfrom langchain.output_parsers import PydanticOutputParser\n\nclass LegalClause(BaseModel):\n    parties: list[str] = Field(description=\"Parties involved\")\n    obligations: str = Field(description=\"Main obligations\")\n    conditions: str = Field(description=\"Key conditions\")\n\nparser = PydanticOutputParser(pydantic_object=LegalClause)\n\nprompt = PromptTemplate(\n    input_variables=[\"clause\"],\n    template=\"Parse this legal clause:\\n{clause}\\n\\n{format_instructions}\",\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\nformatted = prompt.format(clause=\"...\")\nresponse = llm.invoke(formatted)\nparsed = parser.parse(response)\n\nprint(parsed.parties)\nprint(parsed.obligations)\n```\n\n## Chaining\n\n### Sequential Chain (Pipe Syntax)\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\n# Define chains\ntemplate1 = \"Give a bullet point outline for a blog about {topic}\"\ntemplate2 = \"Write a blog post from this outline:\\n{outline}\"\n\nchain1 = PromptTemplate.from_template(template1) | llm\nchain2 = PromptTemplate.from_template(template2) | llm\n\n# Compose\nfull_chain = chain1 | chain2\n\nresult = full_chain.invoke({\"topic\": \"AI\"})\n```\n\n### Multi-Step Processing\n\n```python\ntemplate1 = \"Summarize this review:\\n{review}\"\ntemplate2 = \"Identify weaknesses:\\n{summary}\"\ntemplate3 = \"Create improvement plan:\\n{weaknesses}\"\n\nchain_1 = PromptTemplate.from_template(template1) | llm\nchain_2 = PromptTemplate.from_template(template2) | llm\nchain_3 = PromptTemplate.from_template(template3) | llm\n\nfull_chain = chain_1 | chain_2 | chain_3\nresult = full_chain.invoke(employee_review)\n```\n\n### Router Chain\n\n```python\nfrom langchain.chains.router import MultiPromptChain\n\nbeginner_template = \"Explain {input} simply for a child.\"\nexpert_template = \"Explain {input} technically for an expert.\"\n\nprompt_infos = [\n    {\"name\": \"beginner\", \"description\": \"For simple questions\", \"prompt_template\": beginner_template},\n    {\"name\": \"expert\", \"description\": \"For technical questions\", \"prompt_template\": expert_template}\n]\n\nchain = MultiPromptChain.from_prompts(llm, prompt_infos, verbose=True)\nresult = chain.invoke(\"How do Feynman diagrams work?\")\n```\n\n## Caching\n\n```python\nimport langchain\nfrom langchain.cache import SQLiteCache\n\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n\n# First call - hits LLM\nresponse1 = llm.invoke(\"What is Python?\")\n\n# Second call - uses cache (instant)\nresponse2 = llm.invoke(\"What is Python?\")\n```\n\n## Messages\n\n```python\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is 2+2?\"),\n    AIMessage(content=\"4\"),\n    HumanMessage(content=\"And times 3?\")\n]\n\nresponse = chat_llm.invoke(messages)\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Building LLM applications with structured workflows\n- Need prompt templating and variable substitution\n- Chaining multiple LLM calls together\n- Parsing structured output from LLMs\n- Working with both local and cloud models\n\n## Cross-References\n\n- `bazzite-ai-jupyter:rag` - RAG pipelines using LangChain\n- `bazzite-ai-jupyter:evaluation` - LLM evaluation\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility\n- `bazzite-ai-ollama:python` - Native Ollama Python library"
              },
              {
                "name": "peft",
                "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n",
                "path": "bazzite-ai-jupyter/skills/peft/SKILL.md",
                "frontmatter": {
                  "name": "peft",
                  "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n"
                },
                "content": "# Parameter-Efficient Fine-Tuning (PEFT)\n\n## Overview\n\nPEFT methods like LoRA train only a small number of adapter parameters instead of the full model, reducing memory by 10-100x while maintaining quality.\n\n## Quick Reference\n\n| Method | Memory | Speed | Quality |\n|--------|--------|-------|---------|\n| Full Fine-tune | High | Slow | Best |\n| LoRA | Low | Fast | Very Good |\n| QLoRA | Very Low | Fast | Good |\n| Unsloth | Very Low | 2x Faster | Good |\n\n## LoRA Concepts\n\n### How LoRA Works\n\n```\nOriginal weight matrix W (frozen):     d x k\nLoRA adapters A and B:                 d x r, r x k (where r << min(d,k))\n\nForward pass:\n  output = x @ W + x @ A @ B * (alpha / r)\n\nTrainable params: 2 * r * d  (instead of d * k)\n```\n\n### Memory Savings\n\n```python\ndef lora_savings(d, k, r):\n    original = d * k\n    lora = 2 * r * max(d, k)\n    reduction = (1 - lora / original) * 100\n    return reduction\n\n# Example: 4096 x 4096 matrix with rank 8\nprint(f\"Memory reduction: {lora_savings(4096, 4096, 8):.1f}%\")\n# Output: ~99.6% reduction\n```\n\n## Basic LoRA Setup\n\n### Configure LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=8,                          # Rank (capacity)\n    lora_alpha=16,                # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers\n    lora_dropout=0.05,            # Regularization\n    bias=\"none\",                  # Don't train biases\n    task_type=TaskType.CAUSAL_LM  # Task type\n)\n```\n\n### Apply to Model\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 4,194,304 || all params: 1,100,048,384 || trainable%: 0.38%\n```\n\n## LoRA Parameters\n\n### Key Parameters\n\n| Parameter | Values | Effect |\n|-----------|--------|--------|\n| `r` | 4, 8, 16, 32 | Adapter capacity |\n| `lora_alpha` | r to 2*r | Scaling (higher = stronger) |\n| `target_modules` | List | Which layers to adapt |\n| `lora_dropout` | 0.0-0.1 | Regularization |\n\n### Target Modules\n\n```python\n# Common target modules for different models\n\n# LLaMA / Mistral / TinyLlama\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# GPT-2\ntarget_modules = [\"c_attn\", \"c_proj\"]\n\n# BLOOM\ntarget_modules = [\"query_key_value\", \"dense\"]\n\n# All linear layers (most aggressive)\ntarget_modules = \"all-linear\"\n```\n\n### Rank Selection Guide\n\n| Rank (r) | Use Case |\n|----------|----------|\n| 4 | Simple tasks, small datasets |\n| 8 | General purpose (recommended) |\n| 16 | Complex tasks, more capacity |\n| 32+ | Near full fine-tune quality |\n\n## QLoRA (Quantized LoRA)\n\n### Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training (important!)\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n```\n\n## Training with PEFT\n\n### Using SFTTrainer\n\n```python\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\nsft_config = SFTConfig(\n    output_dir=\"./lora_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-4,  # Higher LR for LoRA\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    gradient_accumulation_steps=4,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset[\"train\"],\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    peft_config=lora_config,  # Pass LoRA config\n)\n\ntrainer.train()\n```\n\n## Unsloth (2x Faster Training)\n\n### Setup\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/tinyllama-chat-bnb-4bit\",  # Pre-quantized\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\n# Add LoRA with Unsloth\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=42,\n)\n```\n\n### Train with Unsloth\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./unsloth_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=42,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n## Save and Load Adapters\n\n### Save Adapters Only\n\n```python\n# Save just the LoRA weights (small!)\nmodel.save_pretrained(\"./lora_adapters\")\n```\n\n### Load Adapters\n\n```python\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n```\n\n### Merge Adapters into Base Model\n\n```python\n# Merge LoRA weights into base model (for deployment)\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged_model\")\n```\n\n## Inference with Adapters\n\n```python\nfrom peft import PeftModel\n\n# Load base + adapters\nbase_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n\n# Generate\nmodel.eval()\ninputs = tokenizer(\"What is Python?\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap them at inference time without reloading the base model.\n\n### Train Multiple Adapters\n\n```python\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, SFTConfig\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual responses\n    \"creative\": creative_data,     # Imaginative, expressive responses\n    \"code\": code_data,             # Code-focused analysis\n}\n\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(\n        model, r=16, lora_alpha=16,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n    )\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\nfrom unsloth import FastLanguageModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ndef load_and_generate(adapter_path, prompt):\n    \"\"\"Load adapter and generate response.\"\"\"\n    # Hot-swap adapter onto base model\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(adapted_model.device)\n\n    outputs = adapted_model.generate(input_ids=inputs, max_new_tokens=128)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Use different adapters for different tasks\ntechnical_response = load_and_generate(\"./adapters/technical\", \"Explain TCP vs UDP\")\ncreative_response = load_and_generate(\"./adapters/creative\", \"Write a haiku about coding\")\ncode_response = load_and_generate(\"./adapters/code\", \"Explain Python decorators\")\n```\n\n### Adapter Storage Efficiency\n\n| Component | Size |\n|-----------|------|\n| Base model (4-bit) | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters total | ~1.3GB |\n\n**Multi-adapter approach**: 8GB + 1.3GB = 9.3GB total\n**vs 10 full models**: 80GB total\n\n## Comparison: Full vs LoRA vs QLoRA\n\n| Aspect | Full Fine-tune | LoRA | QLoRA |\n|--------|----------------|------|-------|\n| Trainable % | 100% | ~0.1-1% | ~0.1-1% |\n| Memory | 4x model | ~1.2x model | ~0.5x model |\n| Training speed | Slow | Fast | Fast |\n| Quality | Best | Very Good | Good |\n| 7B model | 28GB+ | ~16GB | ~6GB |\n\n## Troubleshooting\n\n### Out of Memory\n\n**Fix:**\n\n```python\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Use smaller batch with accumulation\nper_device_train_batch_size=1\ngradient_accumulation_steps=8\n```\n\n### Poor Quality\n\n**Fix:**\n\n- Increase `r` (rank)\n- Add more target modules\n- Train longer\n- Check data quality\n\n### NaN Loss\n\n**Fix:**\n\n- Lower learning rate\n- Use gradient clipping\n- Check for data issues\n\n## When to Use This Skill\n\nUse when:\n\n- GPU memory is limited\n- Fine-tuning large models (7B+)\n- Need fast training iterations\n- Want to swap adapters for different tasks\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning basics\n- `bazzite-ai-jupyter:quantization` - Quantization for QLoRA\n- `bazzite-ai-jupyter:sft` - SFT training with LoRA\n- `bazzite-ai-jupyter:inference` - Fast inference with adapters\n- `bazzite-ai-jupyter:transformers` - Target module selection"
              },
              {
                "name": "qlora",
                "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n",
                "path": "bazzite-ai-jupyter/skills/qlora/SKILL.md",
                "frontmatter": {
                  "name": "qlora",
                  "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n"
                },
                "content": "# Advanced QLoRA Experiments\n\n## Overview\n\nThis skill covers advanced QLoRA experimentation patterns for optimizing fine-tuning performance. Learn how to select the best LoRA rank, alpha scaling, target modules, and quantization settings for your specific use case.\n\n## Quick Reference\n\n| Topic | Key Finding |\n|-------|-------------|\n| **Rank (r)** | r=16 is optimal balance; r=8 for memory constrained |\n| **Alpha** | alpha=r (1.0x scaling) is standard; alpha=2r for aggressive |\n| **Target Modules** | all_linear for general; mlp_only for knowledge injection |\n| **Quantization** | 4-bit NF4 matches BF16 quality with 11-15% memory savings |\n| **Continual Learning** | Sequential training adds knowledge without forgetting |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n```\n\n## Alpha Scaling\n\n### Formula\n\nThe effective LoRA scaling factor is:\n\n```\nscaling_factor = alpha / r\n```\n\nThis acts as a learning rate multiplier for adapter weights.\n\n### Alpha Comparison Code\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import TrainerCallback\n\nALPHAS = [8, 16, 32, 64]\nFIXED_RANK = 16\nresults = []\n\nfor alpha in ALPHAS:\n    scaling_factor = alpha / FIXED_RANK\n    print(f\"\\n=== Testing alpha={alpha} (scaling={scaling_factor}x) ===\")\n\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA with specific alpha\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=FIXED_RANK,\n        lora_alpha=alpha,  # Variable alpha\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Train and record results\n    trainer = SFTTrainer(model=model, tokenizer=tokenizer, ...)\n    stats = trainer.train()\n\n    results.append({\n        \"alpha\": alpha,\n        \"scaling\": scaling_factor,\n        \"final_loss\": stats.metrics[\"train_loss\"]\n    })\n```\n\n### Alpha Scaling Results\n\n| Alpha | Scaling | Final Loss | Behavior |\n|-------|---------|------------|----------|\n| 8 | 0.5x | ~3.02 | Conservative, slower convergence |\n| 16 | 1.0x | ~2.94 | Standard, balanced |\n| 32 | 2.0x | ~2.80 | Aggressive, faster convergence |\n| 64 | 4.0x | ~2.60 | Very aggressive, risk of instability |\n\n### Recommendations\n\n- **Standard**: `alpha = r` (1.0x scaling)\n- **Aggressive training**: `alpha = 2r` with reduced learning rate\n- **Stability priority**: `alpha = r/2` (0.5x scaling)\n\n## LoRA Rank Comparison\n\n### Rank Selection Code\n\n```python\nRANKS = [4, 8, 16, 32, 64]\n\nfor rank in RANKS:\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=rank,\n        lora_alpha=rank,  # Keep alpha = r for fair comparison\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Count parameters\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    pct = 100 * trainable / total\n\n    print(f\"r={rank}: {trainable:,} trainable ({pct:.2f}%)\")\n```\n\n### Rank Comparison Results (Qwen3-4B)\n\n| Rank | Trainable Params | % of Total | Memory | Best For |\n|------|------------------|------------|--------|----------|\n| 4 | ~8M | 0.3% | Lowest | Quick experiments |\n| 8 | ~16M | 0.6% | Low | Memory constrained |\n| **16** | ~33M | 1.3% | Medium | **General use (default)** |\n| 32 | ~66M | 2.6% | High | Complex tasks |\n| 64 | ~132M | 5.2% | Highest | Maximum capacity |\n\n### Rank Selection Guidelines\n\n```python\ndef recommend_rank(gpu_vram_gb, task_complexity, dataset_size):\n    \"\"\"Recommend LoRA rank based on constraints.\"\"\"\n\n    # Memory constraints\n    if gpu_vram_gb < 8:\n        max_rank = 8\n    elif gpu_vram_gb < 12:\n        max_rank = 16\n    elif gpu_vram_gb < 24:\n        max_rank = 32\n    else:\n        max_rank = 64\n\n    # Task complexity adjustment\n    if task_complexity == \"simple\":\n        suggested = 8\n    elif task_complexity == \"medium\":\n        suggested = 16\n    elif task_complexity == \"complex\":\n        suggested = 32\n    else:\n        suggested = 16\n\n    # Dataset size adjustment\n    if dataset_size < 1000:\n        suggested = min(suggested, 16)  # Avoid overfitting\n    elif dataset_size > 10000:\n        suggested = max(suggested, 16)  # Can use higher rank\n\n    return min(suggested, max_rank)\n```\n\n## Target Module Selection\n\n### Available Configurations\n\n```python\nTARGET_CONFIGS = {\n    \"qv_only\": {\n        \"modules\": [\"q_proj\", \"v_proj\"],\n        \"params\": \"~9M\",\n        \"description\": \"Query + Value only (minimal, original LoRA paper)\"\n    },\n    \"attention_only\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"params\": \"~18M\",\n        \"description\": \"All attention layers\"\n    },\n    \"mlp_only\": {\n        \"modules\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~15M\",\n        \"description\": \"MLP/FFN layers only\"\n    },\n    \"all_linear\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~33M\",\n        \"description\": \"All linear layers (maximum capacity)\"\n    },\n}\n```\n\n### Module Function Analysis\n\n**Attention Layers (q, k, v, o):**\n- Control how model attends to input\n- Affect reasoning patterns and style\n- Best for: Format adaptation, thinking pattern changes\n\n**MLP Layers (gate, up, down):**\n- Store factual knowledge\n- Process and transform representations\n- Best for: Knowledge injection, domain adaptation\n\n### Use Case Recommendations\n\n| Use Case | Config | Rationale |\n|----------|--------|-----------|\n| Minimal fine-tuning | `qv_only` | Fastest, smallest adapters |\n| Style/format change | `attention_only` | Changes reasoning patterns |\n| Knowledge injection | `mlp_only` | Updates knowledge only |\n| **General fine-tuning** | `all_linear` | **Maximum flexibility (default)** |\n| Preserve reasoning | `mlp_only` | Keeps thinking style |\n\n### Target Module Selection Code\n\n```python\ndef get_target_modules(use_case):\n    \"\"\"Select target modules based on use case.\"\"\"\n\n    configs = {\n        \"minimal\": [\"q_proj\", \"v_proj\"],\n        \"style\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"knowledge\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"full\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                 \"gate_proj\", \"up_proj\", \"down_proj\"],\n    }\n\n    return configs.get(use_case, configs[\"full\"])\n\n# Usage\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=get_target_modules(\"full\"),\n    ...\n)\n```\n\n## Continual Learning\n\nSequential training adds new knowledge without catastrophic forgetting.\n\n### Sequential Training Pattern\n\n```python\nTRAINING_STAGES = [\n    (\"medical\", medical_dataset),\n    (\"legal\", legal_dataset),\n    (\"technical\", technical_dataset),\n]\n\n# Load model ONCE\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Apply LoRA ONCE\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train sequentially\nfor stage_idx, (domain_name, domain_data) in enumerate(TRAINING_STAGES):\n    print(f\"\\n=== Stage {stage_idx + 1}: Training on {domain_name} ===\")\n\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=domain_data,\n        args=SFTConfig(\n            output_dir=f\"./continual_{domain_name}\",\n            max_steps=5,\n            learning_rate=2e-4,\n            ...\n        ),\n    )\n    trainer.train()\n\n    # Save checkpoint\n    model.save_pretrained(f\"./checkpoint_stage_{stage_idx}\")\n\n    # Test retention on ALL previous domains\n    test_retention(model, tokenizer, TRAINING_STAGES[:stage_idx+1])\n```\n\n### Retention Testing\n\n```python\ndef test_retention(model, tokenizer, trained_domains):\n    \"\"\"Verify model retains knowledge from previous domains.\"\"\"\n\n    RETENTION_TESTS = {\n        \"medical\": \"What is hypertension and how is it treated?\",\n        \"legal\": \"Explain the concept of due process.\",\n        \"technical\": \"What is a REST API?\",\n    }\n\n    FastLanguageModel.for_inference(model)\n\n    print(\"\\n--- Retention Test ---\")\n    for domain_name, _ in trained_domains:\n        prompt = RETENTION_TESTS[domain_name]\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(model.device)\n\n        outputs = model.generate(input_ids=inputs, max_new_tokens=100)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Check response quality\n        has_content = len(response.split()) > 10\n        print(f\"{domain_name}: {'PASS' if has_content else 'FAIL'}\")\n```\n\n### Continual Learning Benefits\n\n- **No catastrophic forgetting**: Base weights frozen, adapters accumulate knowledge\n- **Incremental updates**: Add new domains without full retraining\n- **Curriculum learning**: Simple  complex topic progression\n- **Personalization**: Adapt over time with user feedback\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap at inference time.\n\n### Training Multiple Adapters\n\n```python\nfrom peft import PeftModel\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual\n    \"creative\": creative_data,     # Imaginative, expressive\n    \"code\": code_data,             # Code-focused\n}\n\n# Train separate adapters\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load base model fresh\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(model, r=16, lora_alpha=16, ...)\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n    print(f\"Saved {task_name} adapter\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Function to swap adapters\ndef load_adapter(base_model, adapter_path):\n    \"\"\"Load specific adapter onto base model.\"\"\"\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n    return adapted_model\n\n# Usage\ntechnical_model = load_adapter(base_model, \"./adapters/technical\")\nresponse = generate(technical_model, \"Explain TCP vs UDP\")\n\ncreative_model = load_adapter(base_model, \"./adapters/creative\")\nresponse = generate(creative_model, \"Write a haiku about coding\")\n```\n\n### Adapter Storage\n\n| Component | Size |\n|-----------|------|\n| Base model | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters | ~1.3GB total |\n\nMulti-adapter approach: 8GB + 1.3GB = 9.3GB total\nvs. 10 full models = 80GB\n\n## Quantization Comparison\n\n### 4-bit vs BF16 Code\n\n```python\nQUANT_CONFIGS = {\n    \"4bit_nf4\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        \"load_in_4bit\": True,\n    },\n    \"bf16\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507\",\n        \"load_in_4bit\": False,\n    },\n}\n\nresults = []\n\nfor config_name, config in QUANT_CONFIGS.items():\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        config[\"model_name\"],\n        max_seq_length=512,\n        load_in_4bit=config.get(\"load_in_4bit\", False),\n    )\n\n    # Measure memory\n    memory_before = measure_gpu_memory()\n\n    # Train\n    trainer = SFTTrainer(model=model, ...)\n    stats = trainer.train()\n\n    memory_after = measure_gpu_memory()\n\n    results.append({\n        \"config\": config_name,\n        \"memory_mb\": memory_after,\n        \"final_loss\": stats.metrics[\"train_loss\"],\n    })\n```\n\n### Quantization Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves identical final loss with 11-15% memory savings.\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n## Utility Functions\n\n### Loss History Callback\n\n```python\nfrom transformers import TrainerCallback\n\nclass LossHistoryCallback(TrainerCallback):\n    \"\"\"Track loss during training for comparison.\"\"\"\n\n    def __init__(self):\n        self.losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and 'loss' in logs:\n            self.losses.append({\n                'step': state.global_step,\n                'loss': logs['loss']\n            })\n\n# Usage\nloss_callback = LossHistoryCallback()\ntrainer = SFTTrainer(..., callbacks=[loss_callback])\ntrainer.train()\n\n# Access loss history\nfor entry in loss_callback.losses:\n    print(f\"Step {entry['step']}: Loss {entry['loss']:.4f}\")\n```\n\n### GPU Memory Measurement\n\n```python\nimport subprocess\nimport gc\nimport torch\n\ndef measure_gpu_memory():\n    \"\"\"Get current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage\nprint(f\"Memory before: {measure_gpu_memory()} MB\")\ncleanup_memory()\nprint(f\"Memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Parameter Counting\n\n```python\ndef count_parameters(model):\n    \"\"\"Count trainable and total parameters.\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return {\n        \"trainable\": trainable,\n        \"total\": total,\n        \"trainable_formatted\": f\"{trainable:,}\",\n        \"total_formatted\": f\"{total:,}\",\n        \"percentage\": f\"{100 * trainable / total:.2f}%\"\n    }\n\n# Usage\nparams = count_parameters(model)\nprint(f\"Trainable: {params['trainable_formatted']} ({params['percentage']})\")\n```\n\n## Decision Tree\n\n```\nWhat's your priority?\n\n Memory constrained (<12GB VRAM)\n    Use r=8 or r=4\n    Use 4-bit quantization\n    Use qv_only or attention_only modules\n\n Maximum quality\n    Use r=32\n    Use BF16 if VRAM allows\n    Use all_linear modules\n\n Knowledge injection only\n    Use mlp_only modules\n    Preserves reasoning style\n\n Multiple tasks\n    Train separate adapters\n    Hot-swap at inference\n\n Incremental updates\n     Sequential training\n     Test retention after each stage\n```\n\n## Kernel Shutdown (Jupyter)\n\nQLoRA experiments require loading/unloading multiple models. Shutdown kernel between experiments to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Each experiment in the loop should clean up memory with `del model` and `gc.collect()`, but kernel shutdown is required between different experiment notebooks.\n\n## When to Use This Skill\n\nUse when:\n- Optimizing LoRA hyperparameters\n- Memory-constrained training\n- Building multi-task systems\n- Incrementally updating models\n- Comparing quantization approaches\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - Basic LoRA setup\n- `bazzite-ai-jupyter:quantization` - Quantization fundamentals\n- `bazzite-ai-jupyter:sft` - Training with SFTTrainer\n- `bazzite-ai-jupyter:inference` - Fast inference patterns"
              },
              {
                "name": "quantization",
                "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n",
                "path": "bazzite-ai-jupyter/skills/quantization/SKILL.md",
                "frontmatter": {
                  "name": "quantization",
                  "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n"
                },
                "content": "# Model Quantization\n\n## Overview\n\nQuantization reduces model precision to save memory and speed up inference. A 7B model at FP32 requires ~28GB, but at 4-bit only ~4GB.\n\n## Quick Reference\n\n| Precision | Bits | Memory | Quality | Speed |\n|-----------|------|--------|---------|-------|\n| FP32 | 32 | 4x | Best | Slowest |\n| FP16 | 16 | 2x | Excellent | Fast |\n| BF16 | 16 | 2x | Excellent | Fast |\n| INT8 | 8 | 1x | Good | Faster |\n| INT4 | 4 | 0.5x | Acceptable | Fastest |\n\n## Memory Estimation\n\n```python\ndef estimate_memory(params_billions, precision_bits):\n    \"\"\"Estimate model memory in GB.\"\"\"\n    bytes_per_param = precision_bits / 8\n    return params_billions * bytes_per_param\n\n# Example: 7B model\nmodel_size = 7  # billion parameters\n\nprint(f\"FP32: {estimate_memory(7, 32):.1f} GB\")  # 28 GB\nprint(f\"FP16: {estimate_memory(7, 16):.1f} GB\")  # 14 GB\nprint(f\"INT8: {estimate_memory(7, 8):.1f} GB\")   # 7 GB\nprint(f\"INT4: {estimate_memory(7, 4):.1f} GB\")   # 3.5 GB\n```\n\n## Measure Model Size\n\n```python\ndef get_model_size(model):\n    \"\"\"Get model size in GB including buffers.\"\"\"\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    total = (param_size + buffer_size) / 1024**3\n    return total\n\nprint(f\"Model size: {get_model_size(model):.2f} GB\")\n```\n\n## Load Model at Different Precisions\n\n### FP32 (Default)\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel_32bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nprint(f\"FP32 size: {get_model_size(model_32bit):.2f} GB\")\n```\n\n### FP16 / BF16\n\n```python\nimport torch\n\nmodel_16bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    torch_dtype=torch.float16,  # or torch.bfloat16\n    device_map=\"auto\"\n)\n\nprint(f\"FP16 size: {get_model_size(model_16bit):.2f} GB\")\n```\n\n### 8-bit Quantization\n\n```python\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"8-bit size: {get_model_size(model_8bit):.2f} GB\")\n```\n\n### 4-bit Quantization (Recommended)\n\n```python\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True  # Nested quantization\n)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"4-bit size: {get_model_size(model_4bit):.2f} GB\")\n```\n\n## BitsAndBytesConfig Options\n\n### 4-bit Configuration\n\n```python\nfrom transformers import BitsAndBytesConfig\nimport torch\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n\n    # Quantization type\n    bnb_4bit_quant_type=\"nf4\",  # \"nf4\" or \"fp4\"\n\n    # Compute dtype for dequantized weights\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\n    # Double quantization (saves more memory)\n    bnb_4bit_use_double_quant=True,\n)\n```\n\n### Options Explained\n\n| Option | Values | Effect |\n|--------|--------|--------|\n| `load_in_4bit` | True/False | Enable 4-bit |\n| `bnb_4bit_quant_type` | \"nf4\", \"fp4\" | nf4 better for LLMs |\n| `bnb_4bit_compute_dtype` | float16, bfloat16 | Computation precision |\n| `bnb_4bit_use_double_quant` | True/False | Quantize quantization constants |\n\n## Compare Precision Performance\n\n```python\nfrom transformers import pipeline\nimport time\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Test message\nmessages = [{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}]\n\ndef benchmark(model, tokenizer, name):\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n    start = time.time()\n    output = pipe(messages, max_new_tokens=100, return_full_text=False)\n    elapsed = time.time() - start\n\n    print(f\"{name}:\")\n    print(f\"  Time: {elapsed:.2f}s\")\n    print(f\"  Size: {get_model_size(model):.2f} GB\")\n    print(f\"  Output: {output[0]['generated_text'][:50]}...\")\n    print()\n\n# Benchmark each precision\nbenchmark(model_32bit, tokenizer, \"FP32\")\nbenchmark(model_16bit, tokenizer, \"FP16\")\nbenchmark(model_8bit, tokenizer, \"8-bit\")\nbenchmark(model_4bit, tokenizer, \"4-bit\")\n```\n\n## Quantization for Training\n\n### QLoRA Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit base model\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## Precision Comparison\n\n| Precision | Memory | Quality | Training | Best For |\n|-----------|--------|---------|----------|----------|\n| FP32 | 4x | Perfect | Yes | Research, baselines |\n| FP16 | 2x | Excellent | Yes | Standard training |\n| BF16 | 2x | Excellent | Yes | Large models |\n| INT8 | 1x | Good | Limited | Inference |\n| INT4 | 0.5x | Acceptable | QLoRA | Memory-constrained |\n\n## FP16 vs BF16\n\n| Aspect | FP16 | BF16 |\n|--------|------|------|\n| Range | Smaller | Larger (like FP32) |\n| Precision | Higher | Lower |\n| Overflow risk | Higher | Lower |\n| Hardware | All GPUs | Ampere+ |\n| Best for | Inference | Training |\n\n## 4-bit NF4 vs BF16 Comparison (Tested)\n\nBased on experiments with Qwen3-4B-Thinking models:\n\n### Comparison Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves **identical final loss** with 11-15% memory savings.\n\n### Pre-Quantized Models (Recommended)\n\nUse pre-quantized models for faster loading:\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Pre-quantized (fast loading)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # -bnb-4bit suffix\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# vs. On-demand quantization (slower)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize during load\n)\n```\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n### Quality Preservation\n\n4-bit NF4 preserves:\n- Training convergence (identical final loss)\n- Thinking tag structure (`<think>...</think>`)\n- Response quality and coherence\n- Model reasoning capabilities\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA OOM error\n\n**Fix:**\n\n```python\n# Use 4-bit quantization\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True\n)\n```\n\n### Quality Degradation\n\n**Symptom:** Poor model outputs after quantization\n\n**Fix:**\n\n- Use nf4 instead of fp4\n- Try 8-bit instead of 4-bit\n- Increase LoRA rank if fine-tuning\n\n### Slow Loading\n\n**Symptom:** Model takes long to load\n\n**Fix:**\n\n- Quantization happens at load time\n- Use `device_map=\"auto\"` for multi-GPU\n\n## When to Use This Skill\n\nUse when:\n\n- Model doesn't fit in GPU memory\n- Need faster inference\n- Training with limited resources (QLoRA)\n- Deploying to edge devices\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:peft` - LoRA with quantization (QLoRA)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning\n- `bazzite-ai-jupyter:sft` - SFT training with quantization\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Model architecture"
              },
              {
                "name": "rag",
                "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n",
                "path": "bazzite-ai-jupyter/skills/rag/SKILL.md",
                "frontmatter": {
                  "name": "rag",
                  "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n"
                },
                "content": "# Retrieval-Augmented Generation (RAG)\n\n## Overview\n\nRAG enhances LLM responses by retrieving relevant context from a knowledge base before generation. This grounds responses in specific documents and reduces hallucination.\n\n## Quick Reference\n\n| Step | Component |\n|------|-----------|\n| 1. Chunk | Split documents into segments |\n| 2. Embed | Convert chunks to vectors |\n| 3. Store | Save in vector database |\n| 4. Retrieve | Find relevant chunks |\n| 5. Generate | LLM answers with context |\n\n## Basic RAG Pipeline\n\n### 1. Document Chunking\n\n```python\nimport textwrap\n\ndocument = \"\"\"\nYour long document text here...\nMultiple paragraphs of content...\n\"\"\"\n\n# Chunk into segments of max 1000 characters\nchunks = textwrap.wrap(document, width=1000)\n\nprint(f\"Created {len(chunks)} chunks\")\n```\n\n### 2. Generate Embeddings\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nEMBED_MODEL = \"llama3.2:latest\"\n\nclient = OpenAI(base_url=f\"{OLLAMA_HOST}/v1\", api_key=\"ollama\")\n\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=EMBED_MODEL,\n        input=text\n    )\n    return response.data[0].embedding\n\n# Embed all chunks\nembeddings = [get_embedding(chunk) for chunk in chunks]\nprint(f\"Embedding dimensions: {len(embeddings[0])}\")\n```\n\n### 3. Create Vector Database (Pandas)\n\n```python\nimport pandas as pd\nimport numpy as np\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": [np.array(e) for e in embeddings]\n})\n```\n\n### 4. Similarity Search\n\n```python\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef search(query, n_results=5):\n    query_embedding = get_embedding(query)\n\n    similarities = vector_db[\"embeddings\"].apply(\n        lambda x: cosine_similarity(query_embedding, x)\n    )\n\n    top_indices = similarities.nlargest(n_results).index\n    return vector_db.loc[top_indices, \"text\"].tolist()\n\n# Find relevant chunks\nrelevant = search(\"What are the symptoms?\", n_results=3)\n```\n\n### 5. Generate with Context\n\n```python\nLLM_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\ndef rag_query(question, n_docs=5):\n    # Retrieve context\n    context_chunks = search(question, n_results=n_docs)\n    context = \"\\n\\n\".join(context_chunks)\n\n    # Build prompt\n    messages = [\n        {\"role\": \"system\", \"content\": f\"Answer based on this context:\\n\\n{context}\"},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\n    # Generate\n    response = client.chat.completions.create(\n        model=LLM_MODEL,\n        messages=messages,\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content\n\nanswer = rag_query(\"What are the main symptoms of Omicron?\")\n```\n\n## LangChain RAG with ChromaDB\n\n### Setup\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# LLM for generation\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n```\n\n### Create Vector Store\n\n```python\nimport textwrap\n\n# Chunk document\ndocument = \"Your document text...\"\nchunks = textwrap.wrap(document, width=1000)\n\n# Create ChromaDB store\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n```\n\n### Build RAG Chain\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_classic.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\n# Prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer based on this context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{input}\")\n])\n\n# Create chains\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\n```\n\n### Conversational RAG\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nchat_history = []\n\ndef chat(question):\n    result = rag_chain.invoke({\n        \"input\": question,\n        \"chat_history\": chat_history\n    })\n\n    # Update history\n    chat_history.append(HumanMessage(content=question))\n    chat_history.append(AIMessage(content=result[\"answer\"]))\n\n    return result[\"answer\"]\n\n# Multi-turn conversation\nprint(chat(\"What is Omicron?\"))\nprint(chat(\"What are its symptoms?\"))\nprint(chat(\"How does it compare to Delta?\"))\n```\n\n## Chunking Strategies\n\n### Fixed Size\n\n```python\ndef fixed_chunks(text, size=1000):\n    return textwrap.wrap(text, width=size)\n```\n\n### Sentence-Based\n\n```python\nimport re\n\ndef sentence_chunks(text, max_sentences=5):\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    chunks = []\n    current = []\n\n    for sent in sentences:\n        current.append(sent)\n        if len(current) >= max_sentences:\n            chunks.append(\" \".join(current))\n            current = []\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n```\n\n### Overlap Chunks\n\n```python\ndef overlap_chunks(text, size=1000, overlap=200):\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + size\n        chunks.append(text[start:end])\n        start = end - overlap\n\n    return chunks\n```\n\n## Vector Store Options\n\n### Pandas DataFrame (Simple)\n\n```python\nimport pandas as pd\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": embeddings\n})\n```\n\n### ChromaDB (Persistent)\n\n```python\nfrom langchain_community.vectorstores import Chroma\n\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n```\n\n### FAISS (Fast)\n\n```python\nfrom langchain_community.vectorstores import FAISS\n\nvectorstore = FAISS.from_texts(chunks, embeddings)\nvectorstore.save_local(\"./faiss_index\")\n```\n\n## Troubleshooting\n\n### Poor Retrieval Quality\n\n**Symptom:** Retrieved chunks not relevant\n\n**Fix:**\n\n- Increase chunk overlap\n- Use smaller chunk sizes\n- Try different embedding models\n- Increase `k` in retriever\n\n### Slow Embedding\n\n**Symptom:** Takes long to embed documents\n\n**Fix:**\n\n- Batch embeddings\n- Use smaller embedding model\n- Cache embeddings to disk\n\n### Out of Context\n\n**Symptom:** LLM ignores retrieved context\n\n**Fix:**\n\n- Increase `max_tokens`\n- Use explicit system prompt\n- Reduce number of retrieved chunks\n\n## When to Use This Skill\n\nUse when:\n\n- LLM needs to answer from specific documents\n- Reducing hallucination is critical\n- Building Q&A systems over documents\n- Need up-to-date information not in training data\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain fundamentals\n- `bazzite-ai-jupyter:evaluation` - Evaluate RAG quality\n- `bazzite-ai-ollama:python` - Ollama embeddings API"
              },
              {
                "name": "reward",
                "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n",
                "path": "bazzite-ai-jupyter/skills/reward/SKILL.md",
                "frontmatter": {
                  "name": "reward",
                  "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n"
                },
                "content": "# Reward Model Training\n\n## Overview\n\nReward models learn to score responses based on human preferences. They're used in RLHF pipelines (PPO, GRPO, RLOO) to provide reward signals for policy optimization. The model outputs a scalar reward for each response. This skill includes patterns for scoring thinking/reasoning quality.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RewardTrainer` | Trainer for reward model |\n| `RewardConfig` | Training hyperparameters |\n| `AutoModelForSequenceClassification` | Model with `num_labels=1` |\n| `task_type=\"SEQ_CLS\"` | LoRA task type for reward models |\n| Preference pairs | Training data format |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# Standard transformers for reward models (not Unsloth)\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import RewardTrainer, RewardConfig\nfrom datasets import Dataset\nimport torch\n```\n\n## Reward Model Concepts\n\n### How Reward Models Work\n\n1. Take prompt + response as input\n2. Output scalar reward score\n3. Trained on preference pairs (chosen > rejected)\n4. Used to guide RL policy optimization\n\n### Architecture\n\n```\nInput: [prompt + response]\n  \nBase LLM (frozen or LoRA)\n  \nClassification Head (Linear  Scalar)\n  \nOutput: Reward score (float)\n```\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is a function calling itself with a base case.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### Preprocessing\n\n```python\ndef format_for_reward(sample):\n    prompt = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n        tokenize=False, add_generation_prompt=True\n    )\n    return {\n        \"input_ids_chosen\": tokenizer(prompt + sample[\"chosen\"])[\"input_ids\"],\n        \"input_ids_rejected\": tokenizer(prompt + sample[\"rejected\"])[\"input_ids\"],\n    }\n```\n\n### Thinking Quality Preference Dataset\n\nTrain reward model to score thinking quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\n</think>\n\nRecursion is a technique where a function calls itself with a simpler version of the problem.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n```\n\n## Setup\n\n### Load Reward Model\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\n# Load as sequence classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Non-quantized base\n    num_labels=1,  # Single scalar reward output\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n\n# Setup pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## RewardTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RewardConfig\n\nreward_config = RewardConfig(\n    output_dir=\"./reward_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_length=512,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 1e-5 to 5e-5 | Training speed |\n| `max_length` | 512-1024 | Input truncation |\n| `center_rewards_coefficient` | 0.0-0.1 | Reward centering |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RewardTrainer\n\ntrainer = RewardTrainer(\n    model=model,\n    args=reward_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n## Using the Reward Model\n\n### Score Responses\n\n```python\ndef get_reward(prompt, response):\n    text = prompt + response\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        reward = outputs.logits[0, 0].item()\n\n    return reward\n\n# Example\nscore = get_reward(\"What is Python?\", \"A programming language.\")\nprint(f\"Reward: {score:.3f}\")\n```\n\n### Batch Scoring\n\n```python\ndef get_rewards_batch(prompts, responses):\n    texts = [p + r for p, r in zip(prompts, responses)]\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        rewards = outputs.logits[:, 0].tolist()\n\n    return rewards\n```\n\n### In GRPO/RLOO\n\n```python\ndef reward_fn(completions, prompts):\n    return get_rewards_batch(prompts, completions)\n\ngrpo_trainer = GRPOTrainer(\n    model=policy_model,\n    args=grpo_config,\n    train_dataset=dataset,\n    reward_funcs=reward_fn,\n)\n```\n\n## Reward Scaling\n\n### Normalize Rewards\n\n```python\ndef normalized_reward(completions, prompts):\n    raw_rewards = get_rewards_batch(prompts, completions)\n    mean = sum(raw_rewards) / len(raw_rewards)\n    std = (sum((r - mean) ** 2 for r in raw_rewards) / len(raw_rewards)) ** 0.5\n    return [(r - mean) / (std + 1e-8) for r in raw_rewards]\n```\n\n### Clip Rewards\n\n```python\ndef clipped_reward(completions, prompts):\n    rewards = get_rewards_batch(prompts, completions)\n    return [max(-1.0, min(1.0, r)) for r in rewards]\n```\n\n## Troubleshooting\n\n### Poor Discrimination\n\n**Symptom:** Similar scores for chosen and rejected\n\n**Fix:**\n- More training steps\n- Higher learning rate\n- Check data quality\n\n### Reward Hacking\n\n**Symptom:** RL model exploits reward model\n\n**Fix:**\n- Add diversity in training data\n- Ensemble multiple reward models\n- Regularization during RL\n\n### Overconfident Scores\n\n**Symptom:** Extreme reward values\n\n**Fix:**\n- Use `center_rewards_coefficient`\n- Normalize outputs\n- Clip reward range\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Use LoRA instead of full fine-tuning\n- Reduce `max_length`\n- Smaller batch size\n\n## Kernel Shutdown (Jupyter)\n\nReward model training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Building RLHF pipelines\n- Need explicit reward signal\n- Have preference data\n- Want interpretable scoring\n- Planning to use GRPO or RLOO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:grpo` - Uses reward models for RL\n- `bazzite-ai-jupyter:rloo` - Uses reward models for RL\n- `bazzite-ai-jupyter:dpo` - Alternative that doesn't need reward model\n- `bazzite-ai-jupyter:peft` - LoRA for efficient reward training\n- `bazzite-ai-jupyter:sft` - Pre-training before reward modeling\n- `bazzite-ai-jupyter:inference` - Inference for reward scoring"
              },
              {
                "name": "rloo",
                "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n",
                "path": "bazzite-ai-jupyter/skills/rloo/SKILL.md",
                "frontmatter": {
                  "name": "rloo",
                  "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n"
                },
                "content": "# Reinforcement Learning with Leave-One-Out (RLOO)\n\n## Overview\n\nRLOO is a reinforcement learning method that uses leave-one-out baseline estimation for variance reduction. Like GRPO, it generates multiple completions per prompt but uses a different baseline computation that can provide more stable gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RLOOTrainer` | RL trainer with RLOO baseline |\n| `RLOOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `num_generations` | Completions per prompt (4 typical) |\n| `kl_coef` | KL penalty coefficient (0.05, lower than GRPO) |\n| `learning_rate` | 1e-5 (same as GRPO) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import RLOOConfig, RLOOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## RLOO Concepts\n\n### How RLOO Works\n\n1. Generate K completions for each prompt\n2. Score all completions with reward function\n3. For each completion, compute baseline as mean of other K-1 rewards\n4. Advantage = reward - leave-one-out baseline\n5. Update policy using advantages\n\n### Leave-One-Out Baseline\n\n```\nFor completion i:\n  baseline_i = mean(rewards except reward_i)\n  advantage_i = reward_i - baseline_i\n\nThis reduces variance compared to:\n  - Single-sample estimates (high variance)\n  - Fixed baselines (may be inaccurate)\n```\n\n### Comparison with GRPO\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| Compute | Similar | Similar |\n| Stability | Often better | Good |\n\n## Dataset Format\n\n```python\n# RLOO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"Explain recursion.\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for RLOO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## RLOOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RLOOConfig\n\nrloo_config = RLOOConfig(\n    output_dir=\"./rloo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    num_generations=4,\n    max_completion_length=128,\n    kl_coef=0.05,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `num_generations` | 4-8 | Completions per prompt |\n| `kl_coef` | 0.01-0.1 | KL penalty strength |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n| `max_completion_length` | 64-256 | Generation length |\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response quality heuristics.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        score = 0.0\n\n        # Prefer medium length\n        if 10 <= length <= 50:\n            score += 1.0\n        elif length < 10:\n            score -= 0.5\n\n        # Prefer complete sentences\n        if completion.strip().endswith(\".\"):\n            score += 0.5\n\n        rewards.append(score)\n    return rewards\n```\n\n### Using Trained Reward Model\n\n```python\ndef trained_reward(completions, prompts):\n    \"\"\"Use trained reward model.\"\"\"\n    return reward_model.get_rewards(prompts, completions)\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing (same pattern as GRPO):\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RLOOTrainer\n\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=length_reward,\n)\n\ntrainer.train()\n```\n\n### With Reward Model Instance\n\n```python\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=trained_reward_model,\n)\n```\n\n## num_generations Selection\n\n| K | Use Case |\n|---|----------|\n| 2 | Minimum (limited variance reduction) |\n| 4 | Standard (recommended) |\n| 8 | Better baseline estimation (more compute) |\n| 16+ | Diminishing returns |\n\n**Trade-off:** Higher K = better baseline but more memory/compute\n\n## Troubleshooting\n\n### High Variance\n\n**Symptom:** Unstable training, jumpy rewards\n\n**Fix:**\n- Increase `num_generations` to 6-8\n- Lower `learning_rate`\n- Increase `kl_coef`\n\n### KL Divergence Explosion\n\n**Symptom:** Model output degrades quickly\n\n**Fix:**\n- Increase `kl_coef` to 0.1\n- Reduce `learning_rate`\n- More frequent evaluation\n\n### Reward Collapse\n\n**Symptom:** All generations get similar rewards\n\n**Fix:**\n- Check reward function diversity\n- Increase `temperature` during generation\n- More diverse prompts\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2-4\n- Reduce `max_completion_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nRLOO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Want lower variance than GRPO\n- Have compute for multiple generations\n- Building RLHF pipelines\n- Need stable RL training\n- Policy optimization from rewards\n\n## RLOO vs GRPO Comparison\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| KL penalty (beta) | 0.05 | 0.1 |\n| num_generations | 4 | 2 |\n| batch_size | 4 | 2 |\n| Stability | Often better | Good |\n| Use when | Need stable training | Faster iteration |\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before RLOO\n- `bazzite-ai-jupyter:grpo` - Alternative RL method (higher variance)\n- `bazzite-ai-jupyter:reward` - Training reward models for RLOO\n- `bazzite-ai-jupyter:dpo` - Simpler alternative (no RL)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "sft",
                "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n",
                "path": "bazzite-ai-jupyter/skills/sft/SKILL.md",
                "frontmatter": {
                  "name": "sft",
                  "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n"
                },
                "content": "# Supervised Fine-Tuning (SFT)\n\n## Overview\n\nSFT adapts a pre-trained LLM to follow instructions by training on instruction-response pairs. Unsloth provides an optimized SFTTrainer for 2x faster training with reduced memory usage. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastLanguageModel` | Load model with Unsloth optimizations |\n| `SFTTrainer` | Trainer for instruction tuning |\n| `SFTConfig` | Training hyperparameters |\n| `dataset_text_field` | Column containing formatted text |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then other imports\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Importing TRL before Unsloth will disable optimizations and may cause errors.\n\n## Dataset Formats\n\n### Instruction-Response Format\n\n```python\ndataset = [\n    {\"instruction\": \"What is Python?\", \"response\": \"A programming language.\"},\n    {\"instruction\": \"Explain ML.\", \"response\": \"Machine learning is...\"},\n]\n```\n\n### Chat/Conversation Format\n\n```python\ndataset = [\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"What is Python?\"},\n        {\"role\": \"assistant\", \"content\": \"A programming language.\"}\n    ]},\n]\n```\n\n### Using Chat Templates\n\n```python\ndef format_conversation(sample):\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\ndataset = dataset.map(format_conversation)\n```\n\n### Thinking Model Format\n\nFor models like Qwen3-Thinking, include `<think>` tags in the assistant response. Use **self-questioning internal dialogue** style:\n\n```python\ndef format_thinking_conversation(sample):\n    \"\"\"Format with thinking/reasoning tags.\"\"\"\n    # Combine thinking and response with tags\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\n# Sample dataset with self-questioning thinking style\nthinking_data = [\n    {\n        \"instruction\": \"What is machine learning?\",\n        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n    },\n    {\n        \"instruction\": \"Explain Python in one sentence.\",\n        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n    },\n    {\n        \"instruction\": \"What is a neural network?\",\n        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_data)\ndataset = dataset.map(format_thinking_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n```\n\n**Thinking Style Patterns:**\n- \"What is the user asking here?\"\n- \"Let me think about the key concepts...\"\n- \"How should I structure this explanation?\"\n- \"What's most important about X?\"\n\n## Unsloth SFT Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Training Configuration\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=512,\n)\n```\n\n## SFTTrainer Usage\n\n### Basic Training\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n### With Custom Formatting\n\n```python\ndef formatting_func(examples):\n    texts = []\n    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n        texts.append(text)\n    return texts\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    formatting_func=formatting_func,\n    args=sft_config,\n)\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 2e-4 to 2e-5 | Training speed vs stability |\n| `per_device_train_batch_size` | 1-4 | Memory usage |\n| `gradient_accumulation_steps` | 2-8 | Effective batch size |\n| `max_seq_length` | 512-2048 | Context window |\n| `optim` | \"adamw_8bit\" | Memory-efficient optimizer |\n\n## Save and Load\n\n### Save Model\n\n```python\n# Save LoRA adapters only (small)\nmodel.save_pretrained(\"./sft_lora\")\n\n# Save merged model (full size)\nmodel.save_pretrained_merged(\"./sft_merged\", tokenizer)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"./sft_lora\")\nFastLanguageModel.for_inference(model)\n```\n\n### Thinking Model Inference\n\nParse thinking content from model output using token IDs:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking\n\ndef generate_with_thinking(model, tokenizer, prompt):\n    \"\"\"Generate and parse thinking model output.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Setup pad token if needed\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    # Extract only generated tokens\n    input_length = inputs.shape[1]\n    generated_ids = outputs[0][input_length:].tolist()\n\n    # Parse thinking and response\n    if THINK_END_TOKEN_ID in generated_ids:\n        end_idx = generated_ids.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(generated_ids[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(generated_ids[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(generated_ids, skip_special_tokens=True)\n        response = \"(incomplete - increase max_new_tokens)\"\n\n    return thinking.strip(), response.strip()\n\n# Usage\nFastLanguageModel.for_inference(model)\nthinking, response = generate_with_thinking(model, tokenizer, \"What is 15 + 27?\")\nprint(f\"Thinking: {thinking}\")\nprint(f\"Response: {response}\")\n```\n\n## Ollama Integration\n\n### Export to GGUF\n\n```python\n# Export to GGUF for Ollama\nmodel.save_pretrained_gguf(\n    \"model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n```\n\n### Deploy to Ollama\n\n```bash\nollama create mymodel -f Modelfile\nollama run mymodel\n```\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory error\n\n**Fix:**\n- Use `gradient_checkpointing=\"unsloth\"`\n- Reduce `per_device_train_batch_size` to 1\n- Use 4-bit quantization (`load_in_4bit=True`)\n\n### NaN Loss\n\n**Symptom:** Loss becomes NaN during training\n\n**Fix:**\n- Lower `learning_rate` to 1e-5\n- Check data quality (no empty samples)\n- Use gradient clipping\n\n### Slow Training\n\n**Symptom:** Training slower than expected\n\n**Fix:**\n- Ensure Unsloth is imported FIRST (before TRL)\n- Use `bf16=True` if supported\n- Enable `use_gradient_checkpointing=\"unsloth\"`\n\n## Kernel Shutdown (Jupyter)\n\nSFT training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Creating instruction-following models\n- Fine-tuning for chat/conversation\n- Adapting to domain-specific tasks\n- Building custom assistants\n- First step before preference optimization (DPO/GRPO)\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - General fine-tuning concepts\n- `bazzite-ai-jupyter:dpo` - Direct Preference Optimization after SFT\n- `bazzite-ai-jupyter:grpo` - GRPO reinforcement learning after SFT\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-jupyter:vision` - Vision model fine-tuning\n- `bazzite-ai-ollama:api` - Ollama deployment"
              },
              {
                "name": "transformers",
                "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n",
                "path": "bazzite-ai-jupyter/skills/transformers/SKILL.md",
                "frontmatter": {
                  "name": "transformers",
                  "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n"
                },
                "content": "# Transformer Architecture\n\n## Overview\n\nThe Transformer architecture is the foundation of modern LLMs. Understanding its components helps with fine-tuning decisions, model selection, and debugging performance issues.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| Self-Attention | Learn relationships between tokens |\n| Multi-Head Attention | Multiple attention perspectives |\n| Feed-Forward Network | Transform representations |\n| Layer Normalization | Stabilize training |\n| Residual Connections | Enable deep networks |\n\n## Self-Attention Mechanism\n\n### Concept\n\nSelf-attention allows each token to attend to all other tokens in a sequence, learning contextual relationships.\n\n```\n\"The cat sat on the mat\"\n       \n  Each word attends to every other word\n       \n  Contextual representations\n```\n\n### Implementation\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n# Example tokens\ntokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\nseq_length = len(tokens)\nembed_dim = 8\n\n# Random embeddings (in practice, learned)\nembeddings = torch.randn(seq_length, embed_dim)\n\n# Query, Key, Value weight matrices\nW_q = torch.randn(embed_dim, embed_dim)\nW_k = torch.randn(embed_dim, embed_dim)\nW_v = torch.randn(embed_dim, embed_dim)\n\n# Compute Q, K, V\nQ = embeddings @ W_q  # Queries: what am I looking for?\nK = embeddings @ W_k  # Keys: what do I contain?\nV = embeddings @ W_v  # Values: what information do I provide?\n\n# Attention scores\nscores = Q @ K.T / (embed_dim ** 0.5)  # Scale by sqrt(d_k)\n\n# Softmax for attention weights\nattention_weights = F.softmax(scores, dim=-1)\n\n# Weighted sum of values\noutput = attention_weights @ V\n\nprint(f\"Input shape: {embeddings.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attention_weights.shape}\")\n```\n\n### Attention Formula\n\n```\nAttention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n```\n\nWhere:\n\n- Q = Query matrix\n- K = Key matrix\n- V = Value matrix\n- d_k = Key dimension (for scaling)\n\n## Multi-Head Attention\n\n### Concept\n\nMultiple attention heads learn different aspects of relationships (syntax, semantics, etc.).\n\n```python\nnum_heads = 4\nhead_dim = embed_dim // num_heads\n\n# Split into heads\ndef split_heads(x, num_heads):\n    batch_size, seq_len, embed_dim = x.shape\n    head_dim = embed_dim // num_heads\n    return x.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n\n# Compute attention for each head\nheads = []\nfor h in range(num_heads):\n    W_q_h = torch.randn(embed_dim, head_dim)\n    W_k_h = torch.randn(embed_dim, head_dim)\n    W_v_h = torch.randn(embed_dim, head_dim)\n\n    Q_h = embeddings @ W_q_h\n    K_h = embeddings @ W_k_h\n    V_h = embeddings @ W_v_h\n\n    scores_h = Q_h @ K_h.T / (head_dim ** 0.5)\n    attn_h = F.softmax(scores_h, dim=-1)\n    head_output = attn_h @ V_h\n    heads.append(head_output)\n\n# Concatenate heads\nmulti_head_output = torch.cat(heads, dim=-1)\n\n# Project back to embed_dim\nW_o = torch.randn(embed_dim, embed_dim)\nfinal_output = multi_head_output @ W_o\n\nprint(f\"Multi-head output shape: {final_output.shape}\")\n```\n\n## Feed-Forward Network\n\n### Concept\n\nTwo linear layers with activation, applied to each position independently.\n\n```python\nimport torch.nn as nn\n\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n        self.activation = nn.GELU()  # or ReLU\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n\nffn = FeedForward(embed_dim=512)\nx = torch.randn(1, 10, 512)  # (batch, seq_len, embed_dim)\noutput = ffn(x)\n\nprint(f\"FFN output shape: {output.shape}\")\n```\n\n### Formula\n\n```\nFFN(x) = GELU(xW_1 + b_1)W_2 + b_2\n```\n\n## Layer Normalization\n\n### Concept\n\nNormalizes across the embedding dimension to stabilize training.\n\n```python\nclass LayerNorm(nn.Module):\n    def __init__(self, embed_dim, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(embed_dim))\n        self.beta = nn.Parameter(torch.zeros(embed_dim))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nlayer_norm = nn.LayerNorm(embed_dim)\nnormalized = layer_norm(embeddings)\n```\n\n## Residual Connections\n\n### Concept\n\nSkip connections that add input to output, enabling gradient flow in deep networks.\n\n```python\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # Self-attention with residual\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)  # Residual connection\n\n        # FFN with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)  # Residual connection\n\n        return x\n```\n\n## Complete Transformer Layer\n\n```python\nclass TransformerLayer(nn.Module):\n    def __init__(self, embed_dim=512, num_heads=8, hidden_dim=2048, dropout=0.1):\n        super().__init__()\n\n        # Multi-head attention\n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n\n        # Feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n        # Layer norms\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention block\n        attn_out, attn_weights = self.self_attn(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout(attn_out))\n\n        # FFN block\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n\n        return x, attn_weights\n\n# Example usage\nlayer = TransformerLayer()\nx = torch.randn(10, 1, 512)  # (seq_len, batch, embed_dim)\noutput, weights = layer(x)\nprint(f\"Output shape: {output.shape}\")\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `embed_dim` | 768, 1024, 4096 | Model capacity |\n| `num_heads` | 8, 12, 16 | Attention perspectives |\n| `num_layers` | 12, 24, 32 | Model depth |\n| `hidden_dim` | 4 * embed_dim | FFN capacity |\n| `dropout` | 0.1 | Regularization |\n\n## Thinking Model Special Tokens\n\nQwen3-Thinking models use special tokens for chain-of-thought reasoning.\n\n### Token IDs\n\n| Token | ID | Purpose |\n|-------|----| --------|\n| `<think>` | 151667 | Start of thinking block |\n| `</think>` | 151668 | End of thinking block |\n\n### Parsing Thinking Output\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> for Qwen3-Thinking\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"Parse thinking model output using token ID boundary.\"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(token_list[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(token_list[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True)\n        response = \"(incomplete - increase max_tokens)\"\n\n    return thinking.strip(), response.strip()\n```\n\n### Chat Template with Thinking\n\n```python\n# Format training data with thinking tags\ndef format_thinking_sample(sample):\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n```\n\n## Model Size Estimation\n\n```python\ndef estimate_params(vocab_size, embed_dim, num_layers, hidden_dim, num_heads):\n    # Embedding\n    embedding_params = vocab_size * embed_dim\n\n    # Per layer\n    attn_params = 4 * embed_dim * embed_dim  # Q, K, V, O projections\n    ffn_params = 2 * embed_dim * hidden_dim  # Two linear layers\n    norm_params = 4 * embed_dim  # Two layer norms\n\n    layer_params = attn_params + ffn_params + norm_params\n    total_layer_params = num_layers * layer_params\n\n    # Output head\n    output_params = embed_dim * vocab_size\n\n    total = embedding_params + total_layer_params + output_params\n    return total / 1e9  # Billions\n\n# Example: LLaMA-7B-like\nparams_b = estimate_params(\n    vocab_size=32000,\n    embed_dim=4096,\n    num_layers=32,\n    hidden_dim=11008,\n    num_heads=32\n)\nprint(f\"Estimated parameters: {params_b:.1f}B\")\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Understanding model architecture for fine-tuning\n- Debugging attention patterns\n- Selecting target modules for LoRA\n- Estimating model size and memory\n- Building custom transformer components\n\n## Cross-References\n\n- `bazzite-ai-jupyter:finetuning` - Fine-tuning transformers\n- `bazzite-ai-jupyter:sft` - SFT with thinking models\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:peft` - Parameter-efficient tuning\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              },
              {
                "name": "vision",
                "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n",
                "path": "bazzite-ai-jupyter/skills/vision/SKILL.md",
                "frontmatter": {
                  "name": "vision",
                  "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n"
                },
                "content": "# Vision Model Fine-Tuning\n\n## Overview\n\nUnsloth provides `FastVisionModel` for fine-tuning vision-language models (VLMs) like Pixtral and Ministral with 2x faster training. This skill covers vision model loading, dataset preparation with images, and vision-specific LoRA configuration.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastVisionModel` | Load vision models with Unsloth optimizations |\n| `UnslothVisionDataCollator` | Handle image+text modality in batches |\n| `finetune_vision_layers` | Enable training of vision encoder |\n| `finetune_language_layers` | Enable training of language model |\n| `skip_prepare_dataset=True` | Required for vision datasets |\n| `dataset_text_field=\"\"` | Empty string for vision (not a field name) |\n| List dataset format | Use `[convert(s) for s in dataset]`, not `.map()` |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\n\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nimport torch\n```\n\n## Supported Vision Models\n\n| Model | Path | Parameters | Best For |\n|-------|------|------------|----------|\n| Pixtral-12B | `unsloth/pixtral-12b-2409-bnb-4bit` | 12.7B | High-quality vision tasks |\n| Ministral-8B-Vision | `unsloth/Ministral-8B-Vision-2507-bnb-4bit` | 8B | Balanced quality/speed |\n| Llama-3.2-11B-Vision | `unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit` | 11B | General vision tasks |\n\n## Load Vision Model\n\n```python\nfrom unsloth import FastVisionModel, is_bf16_supported\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\n\nprint(f\"Model loaded: {type(model).__name__}\")\nprint(f\"Tokenizer: {type(tokenizer).__name__}\")\n```\n\n## Vision-Specific LoRA Configuration\n\nVision models require special LoRA flags to enable training of vision encoder layers:\n\n```python\nmodel = FastVisionModel.get_peft_model(\n    model,\n    # Vision-specific flags\n    finetune_vision_layers=True,      # Train vision encoder\n    finetune_language_layers=True,    # Train language model\n    finetune_attention_modules=True,  # Train attention layers\n    finetune_mlp_modules=True,        # Train MLP/FFN layers\n\n    # Standard LoRA parameters\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Check trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n```\n\n### LoRA Flag Combinations\n\n| Use Case | vision_layers | language_layers | attention | mlp |\n|----------|--------------|-----------------|-----------|-----|\n| Full fine-tune | True | True | True | True |\n| Vision only | True | False | True | True |\n| Language only | False | True | True | True |\n| Minimal | False | True | True | False |\n\n## Dataset Format\n\nVision datasets require messages with multi-modal content containing both text and images.\n\n### Image + Text Format\n\n```python\nfrom datasets import Dataset\nfrom PIL import Image\n\n# Sample dataset structure\nsamples = [\n    {\n        \"image\": Image.open(\"equation1.png\"),\n        \"instruction\": \"Convert this equation to LaTeX.\",\n        \"response\": \"\\\\frac{d}{dx} x^2 = 2x\"\n    },\n    {\n        \"image\": Image.open(\"equation2.png\"),\n        \"instruction\": \"What does this equation represent?\",\n        \"response\": \"This is the quadratic formula: x = \\\\frac{-b \\\\pm \\\\sqrt{b^2-4ac}}{2a}\"\n    },\n]\n\ndataset = Dataset.from_list(samples)\n```\n\n### Converting to Chat Format\n\n```python\ndef convert_to_vision_conversation(sample):\n    \"\"\"Convert sample to vision chat format with image content.\"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"instruction\"]},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"response\"]}\n            ]\n        }\n    ]\n    return {\"messages\": messages}\n\n# Apply conversion\nconverted_dataset = dataset.map(convert_to_vision_conversation)\n```\n\n### Using HuggingFace Datasets\n\n**Important**: Use list comprehension, NOT `.map()` for vision datasets:\n\n```python\nfrom datasets import load_dataset\n\n# Load LaTeX OCR dataset from HuggingFace (via Unsloth mirror)\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:100]\")\n\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    \"\"\"Format sample for vision training.\"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: Use list comprehension, NOT .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\n```\n\n**Why list format?** Vision datasets with PIL images work more reliably as plain Python lists than HuggingFace Dataset objects with `.map()`.\n\n## Vision Data Collator\n\nThe `UnslothVisionDataCollator` handles image+text batching:\n\n```python\nfrom unsloth.trainer import UnslothVisionDataCollator\n\ndata_collator = UnslothVisionDataCollator(model, tokenizer)\n```\n\n## Training Configuration\n\nVision training requires specific SFTConfig settings:\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./vision_output\",\n    per_device_train_batch_size=1,      # Keep low for large vision models\n    gradient_accumulation_steps=4,       # Effective batch size = 4\n    max_steps=100,                       # Or num_train_epochs=1\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n\n    # Precision settings\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n\n    # Optimizer\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n\n    # Sequence length\n    max_seq_length=1024,\n\n    # CRITICAL for vision - all 3 are required\n    remove_unused_columns=False,         # Keep image column\n    dataset_text_field=\"\",               # Empty string (NOT a field name)\n    dataset_kwargs={\"skip_prepare_dataset\": True},  # Required for vision\n\n    # Other\n    seed=3407,\n    report_to=\"none\",\n)\n```\n\n**Critical settings explained:**\n- `remove_unused_columns=False`: Preserves image column during training\n- `dataset_text_field=\"\"`: Empty string tells TRL to use the messages format\n- `skip_prepare_dataset=True`: Prevents TRL from processing vision data incorrectly\n\n## SFTTrainer for Vision\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\n# Train\ntrainer_stats = trainer.train()\n\nprint(f\"Training completed!\")\nprint(f\"Final loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Complete Training Example\n\nThis example matches the tested notebook pattern:\n\n```python\n# 1. Environment Setup\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# 2. Imports (unsloth FIRST)\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\n# 3. Load model\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\nprint(f\"Model loaded: {type(model).__name__}\")\n\n# 4. Apply LoRA\nmodel = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers=True,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n)\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"LoRA applied ({trainable:,} trainable params)\")\n\n# 5. Prepare dataset (use LIST, not .map())\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:50]\")\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: List comprehension, not .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\nprint(f\"Dataset loaded ({len(converted_dataset)} samples)\")\n\n# 6. Configure training\nsft_config = SFTConfig(\n    output_dir=\"./vision_lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=50,\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=1024,\n    # CRITICAL for vision - all 3 required\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    seed=3407,\n    report_to=\"none\",\n)\n\n# 7. Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\ntrainer_stats = trainer.train()\nprint(f\"Training complete! Loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Inference with Vision Models\n\n### Prepare for Inference\n\n```python\nFastVisionModel.for_inference(model)\n```\n\n### Generate from Image\n\n```python\nfrom PIL import Image\n\n# Load test image\ntest_image = Image.open(\"test_equation.png\")\n\n# Format as conversation\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Convert this to LaTeX:\"},\n            {\"type\": \"image\", \"image\": test_image}\n        ]\n    }\n]\n\n# Apply chat template\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs,\n    max_new_tokens=256,\n    temperature=0.1,      # Low for accurate transcription\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n\n### Batch Inference\n\n```python\nfrom PIL import Image\n\nimages = [Image.open(f\"image_{i}.png\") for i in range(3)]\nprompts = [\"Describe this image.\", \"What objects are in this image?\", \"Transcribe the text.\"]\n\nfor img, prompt in zip(images, prompts):\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\"type\": \"image\", \"image\": img}\n        ]}\n    ]\n\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    outputs = model.generate(input_ids=inputs, max_new_tokens=128)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Save and Load\n\n### Save LoRA Adapter\n\n```python\n# Save only LoRA weights (~66MB for Pixtral)\nmodel.save_pretrained(\"./vision_lora\")\ntokenizer.save_pretrained(\"./vision_lora\")\n```\n\n### Save Merged Model\n\n```python\n# Save full merged model (large)\nmodel.save_pretrained_merged(\n    \"./vision_merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",\n)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastVisionModel\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"./vision_lora\",\n    load_in_4bit=True,\n)\nFastVisionModel.for_inference(model)\n```\n\n## Memory Requirements\n\n| Model | 4-bit VRAM | Training VRAM |\n|-------|------------|---------------|\n| Pixtral-12B | ~8GB | ~12GB |\n| Ministral-8B-Vision | ~6GB | ~10GB |\n| Llama-3.2-11B-Vision | ~7GB | ~11GB |\n\n## Troubleshooting\n\n### Image Not Processed\n\n**Symptom:** Model ignores image content\n\n**Fix:**\n- Ensure `remove_unused_columns=False` in SFTConfig\n- Use `skip_prepare_dataset=True` in dataset_kwargs\n- Verify image is PIL.Image object, not path string\n\n### Out of Memory\n\n**Symptom:** CUDA OOM during vision training\n\n**Fix:**\n- Reduce `per_device_train_batch_size` to 1\n- Increase `gradient_accumulation_steps`\n- Use smaller model (Ministral-8B instead of Pixtral-12B)\n- Enable gradient checkpointing\n\n### Poor Generation Quality\n\n**Symptom:** Model outputs nonsense for images\n\n**Fix:**\n- Increase training steps (50-100+)\n- Check dataset quality (image-text alignment)\n- Use lower learning rate (1e-4)\n- Ensure vision layers are being trained (`finetune_vision_layers=True`)\n\n### Data Collator Error\n\n**Symptom:** `KeyError` or shape mismatch in data collator\n\n**Fix:**\n- Use `UnslothVisionDataCollator(model, tokenizer)`\n- Ensure dataset has \"messages\" field with correct structure\n- Check that images are valid PIL.Image objects\n\n## Kernel Shutdown (Jupyter)\n\nVision models use significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## Use Cases\n\n- **OCR/Document Processing**: LaTeX equation recognition, receipt scanning\n- **Image Captioning**: Generate descriptions for images\n- **Visual QA**: Answer questions about image content\n- **Chart/Graph Analysis**: Extract data from visualizations\n- **Medical Imaging**: X-ray, scan analysis (with appropriate data)\n\n## When to Use This Skill\n\nUse when:\n- Fine-tuning models to understand images\n- Building OCR or document processing pipelines\n- Creating image captioning systems\n- Developing visual question-answering applications\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Standard SFT for text-only models\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              },
              {
                "name": "api",
                "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/api/SKILL.md",
                "frontmatter": {
                  "name": "api",
                  "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n"
                },
                "content": "# Ollama REST API\n\n## Overview\n\nThe Ollama REST API provides direct HTTP access to all Ollama functionality. Use the `requests` library for maximum control over API interactions.\n\n**Default Endpoint:** `http://localhost:11434` (or `http://ollama:11434` in containers)\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/api/tags` | GET | List available models |\n| `/api/show` | POST | Show model details |\n| `/api/ps` | GET | List running models |\n| `/api/generate` | POST | Generate text |\n| `/api/chat` | POST | Chat completion |\n| `/api/embed` | POST | Generate embeddings |\n| `/api/copy` | POST | Copy a model |\n| `/api/delete` | DELETE | Delete a model |\n\n## Setup\n\n```python\nimport os\nimport requests\nimport json\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n```\n\n## List Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/tags\")\nmodels = response.json()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/show\",\n    json={\"model\": \"llama3.2:latest\"}\n)\nmodel_info = response.json()\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Why is the sky blue?\",\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Count from 1 to 5.\",\n        \"stream\": True\n    },\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line:\n        chunk = json.loads(line)\n        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n        if chunk.get(\"done\"):\n            break\n```\n\n## Chat Completion\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/chat\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ],\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"message\"][\"content\"])\n```\n\n## Generate Embeddings\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/embed\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"input\": \"Ollama makes running LLMs locally easy.\"\n    }\n)\nresult = response.json()\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\n```\n\n## Copy Model\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/copy\",\n    json={\n        \"source\": \"llama3.2:latest\",\n        \"destination\": \"llama3.2-backup:latest\"\n    }\n)\nif response.status_code == 200:\n    print(\"Copy successful!\")\n```\n\n## Delete Model\n\n```python\nresponse = requests.delete(\n    f\"{OLLAMA_HOST}/api/delete\",\n    json={\"model\": \"llama3.2-backup:latest\"}\n)\nif response.status_code == 200:\n    print(\"Delete successful!\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = requests.post(\n        f\"{OLLAMA_HOST}/api/generate\",\n        json={\"model\": \"nonexistent\", \"prompt\": \"Hello\"},\n        timeout=30\n    )\n    if response.status_code != 200:\n        print(f\"Error: {response.status_code} - {response.text}\")\n    else:\n        result = response.json()\n        if \"error\" in result:\n            print(f\"API Error: {result['error']}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\nThe generate endpoint returns useful metrics:\n\n```python\nresult = response.json()\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## When to Use This Skill\n\nUse when:\n- You need direct control over HTTP requests\n- Debugging API interactions\n- Building custom integrations\n- Working with streaming responses\n- Checking raw API responses\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Higher-level Python library\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              },
              {
                "name": "gpu",
                "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n",
                "path": "bazzite-ai-ollama/skills/gpu/SKILL.md",
                "frontmatter": {
                  "name": "gpu",
                  "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n"
                },
                "content": "# GPU Monitoring for Ollama\n\n## Overview\n\nMonitor GPU usage and performance when running Ollama with GPU acceleration. This skill covers checking GPU status, VRAM usage, models loaded in GPU memory, and inference performance metrics.\n\n## Quick Reference\n\n| Check | Method |\n|-------|--------|\n| GPU status | `nvidia-smi` / `rocm-smi` |\n| Models in memory | `GET /api/ps` |\n| Inference metrics | Response metadata |\n| VRAM usage | Both nvidia-smi and /api/ps |\n\n## GPU Status Check\n\n### NVIDIA\n\n```python\nimport subprocess\n\ndef check_nvidia_gpu():\n    \"\"\"Check NVIDIA GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.used,memory.total,utilization.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            lines = result.stdout.strip().split(\"\\n\")\n            for i, line in enumerate(lines):\n                parts = line.split(\", \")\n                if len(parts) >= 4:\n                    name, mem_used, mem_total, util = parts\n                    print(f\"GPU {i}: {name}\")\n                    print(f\"  Memory: {mem_used} MB / {mem_total} MB\")\n                    print(f\"  Utilization: {util}%\")\n    except FileNotFoundError:\n        print(\"nvidia-smi not found - NVIDIA GPU may not be available\")\n    except subprocess.TimeoutExpired:\n        print(\"nvidia-smi timed out\")\n\ncheck_nvidia_gpu()\n```\n\n### AMD\n\n```python\nimport subprocess\n\ndef check_amd_gpu():\n    \"\"\"Check AMD GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"rocm-smi\", \"--showmeminfo\", \"vram\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        print(result.stdout)\n    except FileNotFoundError:\n        print(\"rocm-smi not found - AMD GPU may not be available\")\n\ncheck_amd_gpu()\n```\n\n## Models Loaded in GPU Memory\n\n```python\nimport os\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nif running.get(\"models\"):\n    print(\"=== Models Loaded in GPU Memory ===\")\n    for model in running[\"models\"]:\n        name = model.get(\"name\", \"Unknown\")\n        size = model.get(\"size\", 0) / (1024**3)\n        vram = model.get(\"size_vram\", 0) / (1024**3)\n        expires = model.get(\"expires_at\", \"N/A\")\n        print(f\"  - {name}\")\n        print(f\"    Total Size: {size:.2f} GB\")\n        print(f\"    VRAM Usage: {vram:.2f} GB\")\n        print(f\"    Expires: {expires}\")\nelse:\n    print(\"No models currently loaded in memory\")\n```\n\n## Inference Performance Metrics\n\n```python\nimport os\nimport time\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\n# Run inference\nstart_time = time.perf_counter()\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a haiku about computers.\",\n        \"stream\": False\n    }\n)\nend_time = time.perf_counter()\n\nresult = response.json()\n\nprint(f\"Response: {result['response']}\")\nprint()\nprint(\"=== Inference Metrics ===\")\nprint(f\"Wall clock time: {end_time - start_time:.2f}s\")\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens generated): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## GPU Usage During Inference\n\n```python\nimport os\nimport subprocess\nimport requests\nimport threading\nimport time\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\ndef monitor_gpu(stop_event, readings):\n    \"\"\"Monitor GPU usage in background.\"\"\"\n    while not stop_event.is_set():\n        try:\n            result = subprocess.run(\n                [\"nvidia-smi\",\n                 \"--query-gpu=utilization.gpu,memory.used\",\n                 \"--format=csv,noheader,nounits\"],\n                capture_output=True,\n                text=True,\n                timeout=1\n            )\n            if result.returncode == 0:\n                parts = result.stdout.strip().split(\", \")\n                if len(parts) >= 2:\n                    readings.append({\n                        \"util\": int(parts[0]),\n                        \"mem\": int(parts[1])\n                    })\n        except:\n            pass\n        time.sleep(0.5)\n\n# Start monitoring\nstop_event = threading.Event()\nreadings = []\nmonitor_thread = threading.Thread(target=monitor_gpu, args=(stop_event, readings))\nmonitor_thread.start()\n\n# Run inference\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a short story about AI.\",\n        \"stream\": False\n    }\n)\n\n# Stop monitoring\nstop_event.set()\nmonitor_thread.join()\n\n# Report\nif readings:\n    avg_util = sum(r[\"util\"] for r in readings) / len(readings)\n    max_mem = max(r[\"mem\"] for r in readings)\n    print(f\"Average GPU utilization: {avg_util:.1f}%\")\n    print(f\"Peak memory usage: {max_mem} MB\")\n```\n\n## Complete Health Check\n\n```python\nimport os\nimport subprocess\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\nDEFAULT_MODEL = \"llama3.2:latest\"\n\ndef complete_gpu_health_check():\n    \"\"\"Complete GPU and Ollama health check.\"\"\"\n    print(\"=== GPU Health Check ===\")\n    print()\n\n    # 1. Check GPU hardware\n    print(\"1. GPU Hardware:\")\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.total\",\n             \"--format=csv,noheader\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            print(f\"   {result.stdout.strip()}\")\n        else:\n            print(\"   nvidia-smi failed\")\n    except FileNotFoundError:\n        print(\"   NVIDIA GPU not detected\")\n\n    # 2. Check Ollama server\n    print()\n    print(\"2. Ollama Server:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            print(\"   Server is running\")\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            if DEFAULT_MODEL in model_names:\n                print(f\"   Model '{DEFAULT_MODEL}' available\")\n            else:\n                print(f\"   Model '{DEFAULT_MODEL}' NOT available\")\n        else:\n            print(f\"   Server error: {response.status_code}\")\n    except requests.exceptions.ConnectionError:\n        print(\"   Cannot connect to server\")\n\n    # 3. Check models in GPU memory\n    print()\n    print(\"3. Models in GPU Memory:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n        running = response.json()\n        if running.get(\"models\"):\n            for model in running[\"models\"]:\n                vram = model.get(\"size_vram\", 0) / (1024**3)\n                print(f\"   {model['name']}: {vram:.2f} GB VRAM\")\n        else:\n            print(\"   No models loaded\")\n    except:\n        print(\"   Cannot check running models\")\n\ncomplete_gpu_health_check()\n```\n\n## Model Size Guide\n\n| Model | Parameters | VRAM Needed | Tokens/sec (typical) |\n|-------|------------|-------------|----------------------|\n| phi3 | 3B | 4GB | 60-80 |\n| llama3.2 | 8B | 8GB | 40-60 |\n| mistral | 7B | 8GB | 40-60 |\n| codellama | 7B | 8GB | 40-60 |\n| llama3.2:70b | 70B | 48GB+ | 10-20 |\n\n## Troubleshooting\n\n### GPU Not Used\n\n**Symptom:** Low tokens/second, nvidia-smi shows 0% utilization\n\n**Check:**\n\n```bash\n# Check GPU inside container (adjust container name as needed)\ndocker exec -it ollama nvidia-smi\n# or\npodman exec -it ollama nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Restart Ollama container with GPU access\n# Refer to bazzite-ai-pod-ollama documentation for container setup\n```\n\n### Out of Memory\n\n**Symptom:** \"out of memory\" error during model loading\n\n**Fix:**\n\n```python\n# Use smaller/quantized model via API\nimport requests\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/pull\",\n    json={\"name\": \"llama3.2:7b-q4_0\"},\n    stream=True\n)\nfor line in response.iter_lines():\n    if line:\n        print(line.decode())\n```\n\n### Slow Inference\n\n**Symptom:** Very low tokens/second\n\n**Possible causes:**\n1. Model too large for VRAM (using CPU fallback)\n2. Wrong GPU type configured\n3. Driver issues\n\n**Check:**\n\n```python\n# Check VRAM usage vs model size\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n# If size_vram << size, model is partially on CPU\n```\n\n## When to Use This Skill\n\nUse when:\n- Debugging slow inference\n- Checking if GPU is being utilized\n- Monitoring VRAM usage\n- Benchmarking different models\n- Troubleshooting GPU issues\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - API for running inference\n- `bazzite-ai-ollama:python` - Python library for inference"
              },
              {
                "name": "huggingface",
                "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n",
                "path": "bazzite-ai-ollama/skills/huggingface/SKILL.md",
                "frontmatter": {
                  "name": "huggingface",
                  "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n"
                },
                "content": "# HuggingFace Model Import\n\n## Overview\n\nOllama can directly pull GGUF models from HuggingFace using the `hf.co/` prefix. This enables access to thousands of quantized models beyond the official Ollama library.\n\n## Quick Reference\n\n| Action | Syntax |\n|--------|--------|\n| Pull model | `hf.co/{org}/{repo}:{quantization}` |\n| List models | `ollama.list()` |\n| Use model | Same as any Ollama model |\n| Delete model | `ollama.delete(\"hf.co/...\")` |\n\n## Model Naming Format\n\n```\nhf.co/{organization}/{repository}-GGUF:{quantization}\n```\n\n**Examples:**\n\n```\nhf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\nhf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M\nhf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M\n```\n\n## Common Quantizations\n\n| Quantization | Size | Quality | Use Case |\n|--------------|------|---------|----------|\n| Q2_K | Smallest | Lowest | Testing only |\n| Q4_K_M | Medium | Good | Recommended default |\n| Q5_K_M | Larger | Better | Quality-focused |\n| Q6_K | Large | High | Near-original quality |\n| Q8_0 | Largest | Highest | Maximum quality |\n\n## Pull Model from HuggingFace\n\n### With Progress Tracking\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprint(f\"Pulling {HF_MODEL}...\")\n\nlast_status = \"\"\nfor progress in ollama.pull(HF_MODEL, stream=True):\n    status = progress.get(\"status\", \"\")\n    digest = progress.get(\"digest\", \"\")\n    total = progress.get(\"total\")\n\n    # Only print when status changes\n    if status != last_status:\n        if status == \"pulling manifest\":\n            print(f\"  {status}\")\n        elif status.startswith(\"pulling\") and digest:\n            short_digest = digest.split(\":\")[-1][:12] if \":\" in digest else digest[:12]\n            size_mb = (total / 1024 / 1024) if total else 0\n            if size_mb > 100:\n                print(f\"  pulling {short_digest}... ({size_mb:.0f} MB)\")\n        elif status in [\"verifying sha256 digest\", \"writing manifest\", \"success\"]:\n            print(f\"  {status}\")\n\n        last_status = status\n\nprint(\"Model pulled successfully!\")\n```\n\n### Simple Pull\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Non-streaming (blocks until complete)\nollama.pull(HF_MODEL)\nprint(\"Model pulled!\")\n```\n\n## Verify Installation\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodels = ollama.list()\nmodel_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n\n# Check for the HF model\nhf_model_installed = any(\n    \"Nous-Hermes\" in name or HF_MODEL in name\n    for name in model_names\n)\n\nif hf_model_installed:\n    print(\"Model is installed!\")\n    for name in model_names:\n        if \"Nous-Hermes\" in name or \"hf.co\" in name:\n            print(f\"  Name: {name}\")\nelse:\n    print(\"Model not found\")\n```\n\n## Show Model Details\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodel_info = ollama.show(HF_MODEL)\n\nprint(f\"Model: {HF_MODEL}\")\nif \"details\" in model_info:\n    details = model_info[\"details\"]\n    print(f\"Family: {details.get('family', 'N/A')}\")\n    print(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\n    print(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## Use Imported Model\n\n### Generate Text\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nresult = ollama.generate(\n    model=HF_MODEL,\n    prompt=\"What is the capital of France?\"\n)\nprint(result[\"response\"])\n```\n\n### Chat Completion\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Nous-Hermes-2 uses ChatML format natively\nresponse = ollama.chat(\n    model=HF_MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Hermes 2, a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in two sentences.\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n## Delete Imported Model\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nollama.delete(HF_MODEL)\nprint(\"Model deleted!\")\n```\n\n## Popular HuggingFace Models\n\n### General Purpose\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Nous-Hermes-2-Mistral | `hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M` | 4.4 GB |\n| Llama-2-7B-Chat | `hf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M` | 4.1 GB |\n| Mistral-7B-Instruct | `hf.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF:Q4_K_M` | 4.4 GB |\n\n### Code Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| CodeLlama-7B | `hf.co/TheBloke/CodeLlama-7B-Instruct-GGUF:Q4_K_M` | 4.1 GB |\n| Phind-CodeLlama | `hf.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF:Q4_K_M` | 20 GB |\n| WizardCoder | `hf.co/TheBloke/WizardCoder-Python-7B-V1.0-GGUF:Q4_K_M` | 4.1 GB |\n\n### Small/Fast Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Phi-3-mini | `hf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M` | 2.4 GB |\n| TinyLlama | `hf.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q4_K_M` | 0.7 GB |\n\n## Finding Models on HuggingFace\n\n1. Go to [huggingface.co/models](https://huggingface.co/models)\n2. Filter by:\n   - **Library:** GGUF\n   - **Task:** Text Generation\n3. Look for models with `-GGUF` suffix\n4. Check the \"Files\" tab for available quantizations\n\n## Troubleshooting\n\n### Model Not Found\n\n**Symptom:** Error pulling model\n\n**Check:**\n- Repository exists on HuggingFace\n- Repository has GGUF files\n- Quantization tag is correct\n\n```python\n# Verify HuggingFace URL\n# https://huggingface.co/{org}/{repo}/tree/main\n```\n\n### Download Fails\n\n**Symptom:** Download interrupted or fails\n\n**Fix:**\n- Check internet connection\n- Try again (Ollama resumes partial downloads)\n- Check disk space\n\n### Wrong Prompt Format\n\n**Symptom:** Model gives poor responses\n\n**Fix:**\n- Check model card for correct prompt template\n- Some models require specific formats (ChatML, Alpaca, etc.)\n\n```python\n# ChatML format example (Nous-Hermes-2)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\n# The ollama library handles format conversion automatically\n```\n\n## When to Use This Skill\n\nUse when:\n- You need a model not in the official Ollama library\n- Testing specific model variants\n- Using specialized/fine-tuned models\n- Comparing different quantizations\n\n## Resources\n\n- [Ollama Import Docs](https://docs.ollama.com/import)\n- [HuggingFace Ollama Integration](https://huggingface.co/docs/hub/ollama)\n- [TheBloke's GGUF Models](https://huggingface.co/TheBloke)\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Using imported models\n- `bazzite-ai-ollama:api` - REST API for model management"
              },
              {
                "name": "openai",
                "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n",
                "path": "bazzite-ai-ollama/skills/openai/SKILL.md",
                "frontmatter": {
                  "name": "openai",
                  "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n"
                },
                "content": "# Ollama OpenAI Compatibility\n\n## Overview\n\nOllama provides an OpenAI-compatible API at `/v1/*` endpoints. This allows using the official `openai` Python library with Ollama, enabling:\n\n- **Migration** - Drop-in replacement for OpenAI API\n- **Tool ecosystem** - Works with LangChain, LlamaIndex, etc.\n- **Familiar interface** - Standard OpenAI patterns\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/v1/models` | GET | List models |\n| `/v1/completions` | POST | Text generation |\n| `/v1/chat/completions` | POST | Chat completion |\n| `/v1/embeddings` | POST | Generate embeddings |\n\n### Limitations\n\nThe OpenAI compatibility layer does **not** support:\n\n- Show model details (`/api/show`)\n- List running models (`/api/ps`)\n- Copy model (`/api/copy`)\n- Delete model (`/api/delete`)\n\nUse `bazzite-ai-ollama:api` or `bazzite-ai-ollama:python` for these operations.\n\n## Setup\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nclient = OpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\"  # Required by library but ignored by Ollama\n)\n```\n\n## List Models\n\n```python\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"  - {model.id}\")\n```\n\n## Text Completions\n\n```python\nresponse = client.completions.create(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\",\n    max_tokens=100\n)\n\nprint(response.choices[0].text)\nprint(f\"Tokens used: {response.usage.completion_tokens}\")\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n    ],\n    temperature=0.7,\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\nprint(f\"Tokens used: {response.usage.total_tokens}\")\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}\n]\n\n# Turn 1\nmessages.append({\"role\": \"user\", \"content\": \"What is 2 + 2?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nassistant_msg = response.choices[0].message.content\nmessages.append({\"role\": \"assistant\", \"content\": assistant_msg})\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {assistant_msg}\")\n\n# Turn 2\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response.choices[0].message.content}\")\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = client.chat.completions.create(\n        model=\"invalid-model\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n```\n\n## Migration from OpenAI\n\n### Before (OpenAI)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()  # Uses OPENAI_API_KEY\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### After (Ollama)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",  # Change model name\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain Integration\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n## LlamaIndex Integration\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(\n    api_base=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.complete(\"What is Python?\")\nprint(response.text)\n```\n\n## Connection Health Check\n\n```python\nimport requests\n\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Migrating from OpenAI to local LLMs\n- Using LangChain, LlamaIndex, or other OpenAI-based tools\n- You prefer the OpenAI client interface\n- Building applications that may switch between OpenAI and Ollama\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Native Ollama library (more features)\n- `bazzite-ai-ollama:api` - Direct REST API access"
              },
              {
                "name": "python",
                "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/python/SKILL.md",
                "frontmatter": {
                  "name": "python",
                  "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n"
                },
                "content": "# Ollama Python Library\n\n## Overview\n\nThe official `ollama` Python library provides a clean, Pythonic interface to all Ollama functionality. It automatically connects to the Ollama server and handles serialization.\n\n## Quick Reference\n\n| Function | Purpose |\n|----------|---------|\n| `ollama.list()` | List available models |\n| `ollama.show()` | Show model details |\n| `ollama.ps()` | List running models |\n| `ollama.generate()` | Generate text |\n| `ollama.chat()` | Chat completion |\n| `ollama.embed()` | Generate embeddings |\n| `ollama.copy()` | Copy a model |\n| `ollama.delete()` | Delete a model |\n| `ollama.pull()` | Pull a model |\n\n## Setup\n\n```python\nimport ollama\n\n# The library automatically uses OLLAMA_HOST environment variable\n# Default: http://localhost:11434\n```\n\n## List Models\n\n```python\nmodels = ollama.list()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['model']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nmodel_info = ollama.show(\"llama3.2:latest\")\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nrunning = ollama.ps()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresult = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\"\n)\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nstream = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Count from 1 to 5.\",\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"response\"], end=\"\", flush=True)\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is Python?\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n]\n\n# First turn\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {response['message']['content']}\")\n\n# Continue conversation\nmessages.append(response[\"message\"])\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response['message']['content']}\")\n```\n\n### Streaming Chat\n\n```python\nstream = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresult = ollama.embed(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\nprint(f\"First 5 values: {embeddings[:5]}\")\n```\n\n## Model Management\n\n### Copy Model\n\n```python\nollama.copy(source=\"llama3.2:latest\", destination=\"llama3.2-backup:latest\")\nprint(\"Copy successful!\")\n```\n\n### Delete Model\n\n```python\nollama.delete(\"llama3.2-backup:latest\")\nprint(\"Delete successful!\")\n```\n\n### Pull Model\n\n```python\n# Non-streaming\nollama.pull(\"llama3.2:latest\")\n\n# With progress\nfor progress in ollama.pull(\"llama3.2:latest\", stream=True):\n    status = progress.get(\"status\", \"\")\n    print(status)\n```\n\n## Error Handling\n\n```python\ntry:\n    result = ollama.generate(\n        model=\"nonexistent-model\",\n        prompt=\"Hello\"\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}: {e}\")\n\n# Connection check\ntry:\n    models = ollama.list()\n    print(\"Ollama server is running!\")\nexcept Exception as e:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        models = ollama.list()\n        model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n        return True, model in model_names\n    except Exception:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\n```python\nresult = ollama.generate(model=\"llama3.2:latest\", prompt=\"Hello!\")\n\nprint(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.2f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## Common Patterns\n\n### Conversation Class\n\n```python\nclass Conversation:\n    def __init__(self, model=\"llama3.2:latest\", system_prompt=None):\n        self.model = model\n        self.messages = []\n        if system_prompt:\n            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    def chat(self, user_message):\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n        response = ollama.chat(model=self.model, messages=self.messages)\n        assistant_message = response[\"message\"]\n        self.messages.append(assistant_message)\n        return assistant_message[\"content\"]\n\n# Usage\nconv = Conversation(system_prompt=\"You are a helpful assistant.\")\nprint(conv.chat(\"What is Python?\"))\nprint(conv.chat(\"What are its main features?\"))\n```\n\n## When to Use This Skill\n\nUse when:\n\n- You want a clean, Pythonic interface\n- Building Python applications\n- Need IDE autocompletion support\n- Working with multi-turn conversations\n- Prefer not to handle HTTP directly\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - Direct REST API access\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              },
              {
                "name": "apptainer",
                "description": "Apptainer (Singularity) container management for HPC workloads. Build SIF\nimages, run containers with GPU passthrough. Use when users need HPC-compatible\ncontainerization or need to pull/run Apptainer images.\n",
                "path": "bazzite-ai/skills/apptainer/SKILL.md",
                "frontmatter": {
                  "name": "apptainer",
                  "description": "Apptainer (Singularity) container management for HPC workloads. Build SIF\nimages, run containers with GPU passthrough. Use when users need HPC-compatible\ncontainerization or need to pull/run Apptainer images.\n"
                },
                "content": "# Apptainer - HPC Container Management\n\n## Overview\n\nThe `apptainer` command manages Apptainer (formerly Singularity) containers for HPC-compatible workloads. It provides SIF image management with automatic GPU detection.\n\n**Key Concept:** Apptainer is the HPC standard. Unlike Docker/Podman, containers run as the user (no root). SIF files are single-file images.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Pull | `ujust apptainer pull [--image=...] [--tag=...]` | Download image to SIF |\n| Run | `ujust apptainer run [--image=...] [-- CMD...]` | Run container |\n| Shell | `ujust apptainer shell [--image=...]` | Interactive shell |\n| Exec | `ujust apptainer exec [--image=...] [-- CMD...]` | Execute command |\n| Build | `ujust apptainer build [--image=...] [--tag=...]` | Build from definition |\n| Inspect | `ujust apptainer inspect [--image=...]` | Show metadata |\n| GPU | `ujust apptainer gpu` | Test GPU support |\n| Cache | `ujust apptainer cache [--tag=clean\\|list]` | Manage cache |\n| Help | `ujust apptainer help` | Show help |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: pull, run, shell, exec, build, inspect, gpu, cache |\n| image | `--image` | `-i` | `\"\"` | SIF file path, image name, or DEF file |\n| tag | `--tag` | `-t` | `\"\"` | Image tag, output file, or cache subaction |\n| cmd | (variadic) | - | `\"\"` | Command to execute (use `--` separator) |\n\n## Pull Images\n\n### bazzite-ai Pod Images\n\n```bash\n# Pull nvidia-python (long form)\nujust apptainer pull --image=nvidia-python\n\n# Pull with tag (long form)\nujust apptainer pull --image=nvidia-python --tag=testing\n\n# Pull nvidia-python (short form)\nujust apptainer pull -i nvidia-python\n\n# Pull with tag (short form)\nujust apptainer pull -i nvidia-python -t testing\n\n# Pull jupyter\nujust apptainer pull --image=jupyter --tag=stable\n```\n\n### External Images\n\n```bash\n# Docker Hub\nujust apptainer pull --image=docker://ubuntu:22.04\n\n# NVIDIA NGC\nujust apptainer pull --image=docker://nvcr.io/nvidia/pytorch:latest\n\n# Sylabs Cloud\nujust apptainer pull --image=library://sylabsed/examples/lolcow\n```\n\n### Pull Output\n\nImages are saved as SIF files:\n\n```\n~/.local/share/apptainer/bazzite-ai-pod-nvidia-python.sif\n```\n\n## Run Containers\n\n### Run with Default Command\n\n```bash\n# Run nvidia-python (long form)\nujust apptainer run --image=nvidia-python\n\n# Run nvidia-python (short form)\nujust apptainer run -i nvidia-python\n\n# Run specific SIF file\nujust apptainer run --image=./my-container.sif\n```\n\n### Run with Command\n\n```bash\n# Run Python in container (use -- separator for commands)\nujust apptainer run --image=nvidia-python -- python\n\n# Run script\nujust apptainer run --image=nvidia-python -- python script.py\n\n# Short form\nujust apptainer run -i nvidia-python -- python train.py\n```\n\n### GPU Auto-Detection\n\nGPU flags are auto-detected:\n\n- NVIDIA: Adds `--nv`\n- AMD: Adds `--rocm`\n\n```bash\n# GPU is automatically enabled\nujust apptainer run --image=nvidia-python -- python -c \"import torch; print(torch.cuda.is_available())\"\n```\n\n## Interactive Shell\n\n```bash\n# Shell into container (long form)\nujust apptainer shell --image=nvidia-python\n\n# Shell into container (short form)\nujust apptainer shell -i nvidia-python\n\n# Now inside container\npython --version\nnvidia-smi\nexit\n```\n\n## Execute Commands\n\n```bash\n# Execute single command (use -- separator)\nujust apptainer exec --image=nvidia-python -- pip list\n\n# Execute Python one-liner\nujust apptainer exec -i nvidia-python -- python -c 'print(1+1)'\n```\n\n## Build from Definition\n\n### Definition File Example\n\n```def\nBootstrap: docker\nFrom: ubuntu:22.04\n\n%post\n    apt-get update\n    apt-get install -y python3 python3-pip\n\n%runscript\n    python3 \"$@\"\n```\n\n### Build\n\n```bash\n# Build SIF from definition (image=DEF, tag=OUTPUT)\nujust apptainer build --image=mydef.def --tag=myimage.sif\n\n# Build to default location\nujust apptainer build --image=mydef.def\n\n# Short form\nujust apptainer build -i mydef.def -t myimage.sif\n```\n\n## GPU Support\n\n### Test GPU\n\n```bash\n# Detect and test GPU\nujust apptainer gpu\n```\n\n### GPU Flags\n\n| GPU | Flag | Auto-Detection |\n|-----|------|----------------|\n| NVIDIA | `--nv` | Yes |\n| AMD | `--rocm` | Yes |\n| Intel | (none yet) | No |\n\n### Manual GPU Override\n\n```bash\n# Direct apptainer command with GPU\napptainer run --nv nvidia-python.sif nvidia-smi\n```\n\n## Cache Management\n\n### List Cache\n\n```bash\n# Long form\nujust apptainer cache --tag=list\n\n# Or\nujust apptainer cache list\n```\n\n### Clean Cache\n\n```bash\n# Long form\nujust apptainer cache --tag=clean\n\n# Or\nujust apptainer cache clean\n```\n\nCache is stored in `~/.apptainer/cache/`.\n\n## Common Workflows\n\n### HPC Development\n\n```bash\n# Pull HPC-ready image\nujust apptainer pull --image=nvidia-python\n\n# Test GPU\nujust apptainer gpu\n\n# Development shell\nujust apptainer shell --image=nvidia-python\n\n# Run production workload\nujust apptainer run --image=nvidia-python -- python train.py\n```\n\n### Use NGC Images\n\n```bash\n# Pull NVIDIA PyTorch\nujust apptainer pull --image=docker://nvcr.io/nvidia/pytorch:23.10-py3\n\n# Run training\nujust apptainer run --image=pytorch_23.10-py3.sif -- python train.py\n```\n\n### Build Custom Image\n\n```bash\n# Create definition file\ncat > myenv.def << 'EOF'\nBootstrap: docker\nFrom: python:3.11\n\n%post\n    pip install numpy pandas scikit-learn\n\n%runscript\n    python \"$@\"\nEOF\n\n# Build\nujust apptainer build --image=myenv.def --tag=myenv.sif\n\n# Test\nujust apptainer run --image=myenv.sif -- python -c \"import numpy; print(numpy.__version__)\"\n```\n\n## Apptainer vs Docker/Podman\n\n| Feature | Apptainer | Docker/Podman |\n|---------|-----------|---------------|\n| Root required | No | Sometimes |\n| Single file | Yes (SIF) | No (layers) |\n| HPC compatible | Yes | Limited |\n| GPU support | --nv, --rocm | nvidia-docker |\n| Security model | User namespace | Container namespace |\n\n**Use Apptainer when:**\n\n- Running on HPC clusters\n- Need single-file portability\n- Can't run as root\n- Need reproducibility\n\n## Troubleshooting\n\n### Pull Failed\n\n**Check:**\n\n```bash\n# Test network\ncurl -I https://ghcr.io\n\n# Check registry auth\napptainer remote list\n```\n\n**Fix:**\n\n```bash\n# Login to registry\napptainer remote login docker://ghcr.io\n```\n\n### GPU Not Available\n\n**Check:**\n\n```bash\nujust apptainer gpu\nnvidia-smi  # or rocm-smi\n```\n\n**Fix:**\n\n```bash\n# Ensure drivers installed\n# For NVIDIA:\nnvidia-smi\n# For AMD:\nrocm-smi\n```\n\n### SIF File Corrupted\n\n**Fix:**\n\n```bash\n# Remove and re-pull\nrm ~/.local/share/apptainer/*.sif\nujust apptainer pull --image=nvidia-python\n```\n\n### Cache Too Large\n\n**Check:**\n\n```bash\ndu -sh ~/.apptainer/cache/\n```\n\n**Fix:**\n\n```bash\nujust apptainer cache --tag=clean\n```\n\n## Cross-References\n\n- **Related Skills:** `pod` (build OCI images), `jupyter` (uses containers)\n- **GPU Setup:** `ujust config gpu setup`\n- **Apptainer Docs:** <https://apptainer.org/docs/>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"apptainer\", \"singularity\", \"HPC container\"\n- \"SIF file\", \"pull image\", \"build container\"\n- \"apptainer GPU\", \"run with GPU\"\n- \"HPC workload\", \"cluster container\""
              },
              {
                "name": "bootc",
                "description": "bootc VM management via bcvk (bootc virtualization kit). Run bootable\ncontainers as VMs for testing. Supports ephemeral (quick test) and\npersistent modes. Use when users need to test bootable container images\nas virtual machines.\n",
                "path": "bazzite-ai/skills/bootc/SKILL.md",
                "frontmatter": {
                  "name": "bootc",
                  "description": "bootc VM management via bcvk (bootc virtualization kit). Run bootable\ncontainers as VMs for testing. Supports ephemeral (quick test) and\npersistent modes. Use when users need to test bootable container images\nas virtual machines.\n"
                },
                "content": "# Bootc - bootc-based VM Management\n\n## Overview\n\nThe `bootc` command manages bootable container VMs using bcvk (bootc virtualization kit). It converts OCI container images into bootable VMs for testing.\n\n**Key Concept:** Unlike traditional VMs, bootc VMs are created directly from container images. This enables testing bootable containers without building disk images first.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Test | `ujust test bootc [--image=...] [--cpus=...]` | Ephemeral VM (deleted on exit) |\n| Add | `ujust bootc add [NAME] [--image=...] [--cpus=...]` | Create persistent VM |\n| List | `ujust bootc list` | List all VMs |\n| Status | `ujust bootc status [NAME]` | Show VM status |\n| SSH | `ujust bootc ssh [NAME] [--ssh-user=...]` | Connect to VM |\n| Start | `ujust bootc start [NAME]` | Start VM |\n| Stop | `ujust bootc stop [NAME]` | Stop VM |\n| Delete | `ujust bootc delete [NAME]` | Remove VM |\n| Export | `ujust bootc export [--image=...] [--format=...]` | Export to disk image |\n| Images | `ujust bootc images` | List available images |\n| Help | `ujust bootc help` | Show help |\n\n## Prerequisites\n\n```bash\n# Install bcvk\nujust install bcvk\n\n# Verify installation\nbcvk --version\n```\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: add, list, status, ssh, etc. |\n| vm_name | (positional) | - | `bazzite-bootc` | VM name |\n| image | `--image` | `-i` | (varies) | Container image to boot |\n| cpus | `--cpus` | - | `2` | Number of CPUs |\n| ram | `--ram` | - | `4096` | Memory in MB |\n| disk_size | `--disk-size` | - | `20G` | Disk size |\n| format | `--format` | `-f` | `qcow2` | Export format (qcow2, raw) |\n| ssh_port | `--ssh-port` | - | `2222` | SSH port |\n| ssh_user | `--ssh-user` | - | `root` | SSH user |\n\n## Ephemeral Testing\n\nQuick test that auto-deletes VM on exit:\n\n```bash\n# Test default bazzite-ai image\nujust test bootc\n\n# Test specific image (long form)\nujust test bootc --image=ghcr.io/org/image:tag\n\n# Test specific image (short form)\nujust test bootc -i ghcr.io/org/image:tag\n\n# Test with more resources\nujust test bootc --image=myimage --cpus=4 --ram=8192\n\n# Short form\nujust test bootc -i myimage --cpus=4 --ram=8192\n```\n\nEphemeral mode:\n\n- Creates temporary VM\n- Boots to console\n- VM deleted when console exits\n\n## Persistent VMs\n\nCreate VMs that persist across sessions:\n\n```bash\n# Create VM with default image\nujust bootc add dev\n\n# Create with specific image (long form)\nujust bootc add testing --image=ghcr.io/org/image:testing\n\n# Create with specific image (short form)\nujust bootc add testing -i ghcr.io/org/image:testing\n\n# Custom resources\nujust bootc add heavy --cpus=8 --ram=16384 --disk-size=100G\n```\n\n### Manage Persistent VMs\n\n```bash\n# Start VM\nujust bootc start dev\n\n# Stop VM\nujust bootc stop dev\n\n# Delete VM\nujust bootc delete dev\n```\n\n## Connecting to VMs\n\n### SSH Connection\n\n```bash\n# Connect to VM\nujust bootc ssh dev\n\n# Run command (use -- separator)\nujust bootc ssh dev -- systemctl status\n\n# Different user\nujust bootc ssh dev --ssh-user=admin\n```\n\nDefault: `ssh -p 2222 root@localhost`\n\n### List VMs\n\n```bash\nujust bootc list\n```\n\nOutput:\n\n```\nNAME         STATE    IMAGE\ndev          running  ghcr.io/org/image:latest\ntesting      stopped  ghcr.io/org/image:testing\n```\n\n### Check Status\n\n```bash\nujust bootc status dev\n```\n\n## Export Disk Images\n\nConvert bootable container to disk image:\n\n```bash\n# Export to QCOW2 (long form)\nujust bootc export --image=ghcr.io/org/image:tag\n\n# Export to QCOW2 (short form)\nujust bootc export -i ghcr.io/org/image:tag\n\n# Export to raw (long form)\nujust bootc export --image=ghcr.io/org/image:tag --format=raw\n\n# Export to raw (short form)\nujust bootc export -i ghcr.io/org/image:tag -f raw\n```\n\nSupported formats:\n\n- `qcow2` - QEMU disk image\n- `raw` - Raw disk image\n\n## Common Workflows\n\n### Quick Test New Image\n\n```bash\n# Test ephemeral (no cleanup needed)\nujust test bootc --image=ghcr.io/myorg/myimage:dev\n# Exit console to destroy VM\n\n# Short form\nujust test bootc -i ghcr.io/myorg/myimage:dev\n```\n\n### Development Environment\n\n```bash\n# Create persistent VM (long form)\nujust bootc add dev --image=ghcr.io/myorg/myimage:latest\n\n# Or short form\nujust bootc add dev -i ghcr.io/myorg/myimage:latest\n\n# Start it\nujust bootc start dev\n\n# SSH in\nujust bootc ssh dev\n\n# Make changes, test...\n\n# Stop when done\nujust bootc stop dev\n```\n\n### Test Before Release\n\n```bash\n# Test testing branch\nujust test bootc --image=ghcr.io/myorg/myimage:testing\n\n# If good, test stable\nujust test bootc --image=ghcr.io/myorg/myimage:stable\n```\n\n### Create Installation Media\n\n```bash\n# Export to QCOW2 for cloud (long form)\nujust bootc export --image=ghcr.io/myorg/myimage:stable --format=qcow2\n\n# Export to QCOW2 for cloud (short form)\nujust bootc export -i ghcr.io/myorg/myimage:stable -f qcow2\n\n# Export to raw for disk imaging\nujust bootc export -i ghcr.io/myorg/myimage:stable -f raw\n```\n\n## bcvk vs vm Command\n\n| Feature | `ujust bootc` (bcvk) | `ujust vm` (libvirt) |\n|---------|----------------------|----------------------|\n| Image source | Container images | QCOW2 files |\n| Ephemeral mode | Yes | No |\n| Export formats | qcow2/raw | N/A |\n| SSH port | 2222 (fixed) | 4444 (configurable) |\n| Home sharing | No | Yes (virtiofs) |\n| Boot time | Faster | Slower |\n| Use case | Testing containers | Full VMs |\n\n**Use `bootc` when:**\n\n- Testing bootable container images\n- Quick ephemeral tests\n- Building disk images from containers\n\n**Use `vm` when:**\n\n- Need persistent VMs with home sharing\n- Need configurable ports\n- Need full libvirt features\n\n## Troubleshooting\n\n### bcvk Not Found\n\n**Fix:**\n\n```bash\nujust install bcvk\n```\n\n### VM Won't Start\n\n**Check:**\n\n```bash\nujust bootc status dev\nujust bootc list\n```\n\n**Common causes:**\n\n- Image not pulled\n- Resource conflict\n- Disk full\n\n**Fix:**\n\n```bash\nujust bootc delete dev\nujust bootc add dev\n```\n\n### SSH Connection Failed\n\n**Check:**\n\n```bash\nssh -p 2222 root@localhost\n```\n\n**Common causes:**\n\n- VM still booting\n- Port conflict (2222 used)\n- SSH not started\n\n**Fix:**\n\n```bash\n# Wait for boot\nsleep 30\nujust bootc ssh dev\n\n# Or check console\nujust test bootc  # Watch boot process\n```\n\n### Image Pull Failed\n\n**Check:**\n\n```bash\npodman pull ghcr.io/org/image:tag\n```\n\n**Common causes:**\n\n- Network issue\n- Auth required\n- Image doesn't exist\n\n**Fix:**\n\n```bash\n# Login to registry\npodman login ghcr.io\n\n# Pull manually\npodman pull ghcr.io/org/image:tag\n\n# Retry\nujust bootc add dev --image=ghcr.io/org/image:tag\n```\n\n## Cross-References\n\n- **Related Skills:** `vm` (traditional VMs), `install` (bcvk installation)\n- **Installation:** `ujust install bcvk`\n- **bcvk Docs:** <https://github.com/containers/bcvk>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"bootc VM\", \"bootable container\", \"test container as VM\"\n- \"bcvk\", \"bootc virtualization\"\n- \"ephemeral VM\", \"quick test VM\"\n- \"export to qcow2\", \"create ISO from container\""
              },
              {
                "name": "comfyui",
                "description": "ComfyUI node-based Stable Diffusion interface. GPU-accelerated image\ngeneration with custom node support and CivitAI model downloads.\nUse 'ujust comfyui' for configuration, lifecycle management, and\nmodel/node operations.\n",
                "path": "bazzite-ai/skills/comfyui/SKILL.md",
                "frontmatter": {
                  "name": "comfyui",
                  "description": "ComfyUI node-based Stable Diffusion interface. GPU-accelerated image\ngeneration with custom node support and CivitAI model downloads.\nUse 'ujust comfyui' for configuration, lifecycle management, and\nmodel/node operations.\n"
                },
                "content": "# ComfyUI - Stable Diffusion Interface\n\n## Overview\n\nComfyUI is a powerful node-based Stable Diffusion interface for AI image generation. The `comfyui` command manages the ComfyUI container, including configuration, lifecycle management, model downloads, and custom node management.\n\n**Key Concept:** This is a **system command** - run with `ujust` from anywhere on the system. ComfyUI runs as a Podman Quadlet service. By default, data is ephemeral (stored inside the container). Configure volume mounts for persistent storage.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust comfyui config [--models-dir=...] [--output-dir=...] [--port=...]` | Configure ComfyUI |\n| Start | `ujust comfyui start` | Start ComfyUI server |\n| Stop | `ujust comfyui stop` | Stop ComfyUI server |\n| Restart | `ujust comfyui restart` | Restart ComfyUI server |\n| Status | `ujust comfyui status` | Show status and model counts |\n| Logs | `ujust comfyui logs [--lines=...]` | View service logs |\n| Open | `ujust comfyui open` | Open UI in browser |\n| Shell | `ujust comfyui shell [-- CMD...]` | Open shell in container |\n| Download model | `ujust comfyui download --model-url=<url> --model-type=<type>` | Download from CivitAI |\n| List models | `ujust comfyui models` | List installed models |\n| Install node | `ujust comfyui node-install --node-url=<url>` | Install custom node |\n| List nodes | `ujust comfyui node-list` | List custom nodes |\n| Update nodes | `ujust comfyui node-update` | Update all nodes |\n| Delete | `ujust comfyui delete` | Remove ComfyUI and images |\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `8188` | Web UI port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU: nvidia/amd/intel/auto |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Models Dir | `--models-dir` | - | (empty) | Path for SD models |\n| Output Dir | `--output-dir` | - | (empty) | Path for generated images |\n| Input Dir | `--input-dir` | - | (empty) | Path for input images |\n| Nodes Dir | `--nodes-dir` | - | (empty) | Path for custom nodes |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n| Instance | `--instance` | `-n` | `1` | Instance number |\n\n**Important:** All directory parameters default to empty. When empty, data is stored inside the container and will be **lost when the container is recreated**. For persistent storage, provide explicit paths.\n\n### Configuration Examples\n\n```bash\n# Ephemeral mode - no persistent storage (data lost on container recreation)\nujust comfyui config\n\n# Persist models only (most common)\nujust comfyui config --models-dir=/data/models\n\n# Persist models and output\nujust comfyui config --models-dir=/data/models --output-dir=/data/output\n\n# Persist models and custom_nodes\nujust comfyui config --models-dir=/data/models --nodes-dir=/data/nodes\n\n# All directories with custom port and GPU\nujust comfyui config --models-dir=/data/models --output-dir=/data/output \\\n  --input-dir=/data/input --nodes-dir=/data/nodes --port=8189 --gpu-type=nvidia\n\n# With short forms\nujust comfyui config -p 8189 -g nvidia --models-dir=/data/models\n\n# Network-wide access\nujust comfyui config --bind=0.0.0.0\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed:\n\n```bash\n# Initially configured with defaults\nujust comfyui config\n\n# Later, add models directory (other settings preserved)\nujust comfyui config --models-dir=/data/models\n```\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust comfyui shell\n\n# Run specific command (use -- separator)\nujust comfyui shell -- pip list\nujust comfyui shell -- nvidia-smi\n```\n\n## Model Downloads\n\n### download\n\n```bash\nujust comfyui download --model-url=<URL> --model-type=<TYPE>\n```\n\n| Parameter | Flag | Description |\n|-----------|------|-------------|\n| URL | `--model-url` | CivitAI URL, model ID, or direct download URL |\n| Type | `--model-type` | Model type (see below) |\n\n**Requires:** `--models-dir` must be configured (not ephemeral)\n\n**Model Types:**\n\n| Type | Directory | Description |\n|------|-----------|-------------|\n| `checkpoint` | checkpoints/ | Main SD models |\n| `lora` | loras/ | LoRA adapters |\n| `vae` | vae/ | VAE models |\n| `embedding` | embeddings/ | Textual inversions |\n| `controlnet` | controlnet/ | ControlNet models |\n| `upscale` | upscale_models/ | Upscaler models |\n\n### Download Examples\n\n```bash\n# By CivitAI URL\nujust comfyui download --model-url=https://civitai.com/models/101055 --model-type=checkpoint\n\n# By model ID\nujust comfyui download --model-url=101055 --model-type=checkpoint\n\n# LoRA model\nujust comfyui download --model-url=123456 --model-type=lora\n\n# Direct URL\nujust comfyui download --model-url=https://example.com/model.safetensors --model-type=vae\n```\n\n## Custom Nodes\n\n### node-install\n\n```bash\nujust comfyui node-install --node-url=<GIT_URL>\n```\n\n**Requires:** `--nodes-dir` must be configured (not ephemeral)\n\n| Parameter | Flag | Description |\n|-----------|------|-------------|\n| GIT_URL | `--node-url` | Git repository URL for custom node |\n\n### Popular Custom Nodes\n\n```bash\n# ComfyUI-Manager (recommended)\nujust comfyui node-install --node-url=https://github.com/ltdrdata/ComfyUI-Manager\n\n# Impact Pack\nujust comfyui node-install --node-url=https://github.com/ltdrdata/ComfyUI-Impact-Pack\n\n# ControlNet Aux\nujust comfyui node-install --node-url=https://github.com/Fannovel16/comfyui_controlnet_aux\n\n# List installed nodes\nujust comfyui node-list\n\n# Update all nodes\nujust comfyui node-update\n```\n\n## Data Storage\n\n### Ephemeral Mode (Default)\n\nWhen no directories are configured, ComfyUI uses internal container directories:\n\n- Data is stored inside the container\n- **All data is lost** when container is recreated\n- Suitable for testing or temporary use\n\n### Persistent Mode\n\nWhen directories are configured, they are mounted into the container:\n\n```\n/path/to/models/           # Your MODELS_DIR\n checkpoints/           # Main SD models (.safetensors, .ckpt)\n loras/                 # LoRA adapters\n vae/                   # VAE models\n embeddings/            # Textual inversions\n controlnet/            # ControlNet models\n upscale_models/        # Upscaler models\n\n/path/to/output/           # Your OUTPUT_DIR - generated images\n/path/to/input/            # Your INPUT_DIR - input images for img2img\n/path/to/custom_nodes/     # Your CUSTOM_NODES_DIR - node extensions\n```\n\n## Common Workflows\n\n### Initial Setup (Persistent)\n\n```bash\n# 1. Configure with persistent models directory\nujust comfyui config --models-dir=/data/comfyui/models\n\n# 2. Download a checkpoint model\nujust comfyui download --model-url=https://civitai.com/models/101055 --model-type=checkpoint\n\n# 3. Start ComfyUI\nujust comfyui start\n\n# 4. Open in browser\nujust comfyui open\n```\n\n### Quick Test (Ephemeral)\n\n```bash\n# 1. Configure with defaults (ephemeral)\nujust comfyui config\n\n# 2. Start ComfyUI\nujust comfyui start\n\n# 3. Open in browser\nujust comfyui open\n\n# Note: Download models via the UI - they will be lost on container recreation\n```\n\n### Daily Usage\n\n```bash\n# Start ComfyUI\nujust comfyui start\n\n# Open in browser\nujust comfyui open\n\n# View logs\nujust comfyui logs\n\n# Stop when done\nujust comfyui stop\n```\n\n## GPU Support\n\nComfyUI automatically detects and configures GPU acceleration:\n\n| GPU | Configuration | Performance |\n|-----|---------------|-------------|\n| **NVIDIA** | CDI device passthrough | Full CUDA acceleration |\n| **AMD** | /dev/dri + /dev/kfd | ROCm acceleration |\n| **Intel** | /dev/dri | oneAPI acceleration |\n| **CPU** | Fallback mode | Very slow (not recommended) |\n\n### NVIDIA Setup\n\nIf NVIDIA GPU is not detected:\n\n```bash\n# Generate CDI specification\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n\n# Reconfigure ComfyUI\nujust comfyui delete\nujust comfyui config /data/models\n```\n\n## Troubleshooting\n\n### Model/Node Commands Fail\n\n**Symptom:** \"No MODELS_DIR configured\" or \"No CUSTOM_NODES_DIR configured\"\n\n**Cause:** Using ephemeral mode (no directories configured)\n\n**Fix:** Reconfigure with persistent directories:\n\n```bash\n# Add models directory\nujust comfyui config --models-dir=/path/to/models\n\n# Or add both models and custom_nodes\nujust comfyui config --models-dir=/path/to/models --nodes-dir=/path/to/nodes\n```\n\n### Model Not Appearing\n\n**Symptom:** Downloaded model not visible in ComfyUI\n\n**Fix:**\n\n```bash\n# Restart ComfyUI to reload models\nujust comfyui restart\n\n# Verify model is in correct directory\nls /path/to/your/models/checkpoints/\n```\n\n### CivitAI Download Fails\n\n**Symptom:** Cannot download from CivitAI\n\n**Cause:** Model requires authentication or is restricted\n\n**Fix:**\n\n```bash\n# Download manually and place in appropriate directory\nmv ~/Downloads/model.safetensors /path/to/models/checkpoints/\n```\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory error\n\n**Fix:** Check logs and consider using smaller models or lower precision:\n\n```bash\nujust comfyui logs\n```\n\n### Service Won't Start\n\n**Symptom:** ComfyUI fails to start\n\n**Fix:**\n\n```bash\n# Check logs for errors\nujust comfyui logs\n\n# Verify GPU access\nnvidia-smi\n\n# Delete and reconfigure\nujust comfyui delete\nujust comfyui config --models-dir=/data/models\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Instance config | Settings | `~/.config/comfyui/1.env` |\n| Quadlet file | Service definition | `~/.config/containers/systemd/comfyui-1.container` |\n\n## Cross-References\n\n- **Related Skills:** `ollama` (LLM inference), `jupyter` (notebooks)\n- **Pod Building:** `just build pod comfyui`\n- **ComfyUI Docs:** <https://github.com/comfyanonymous/ComfyUI>\n- **ComfyUI-Manager:** <https://github.com/ltdrdata/ComfyUI-Manager>\n- **CivitAI:** <https://civitai.com/>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"comfyui\", \"stable diffusion\", \"image generation\"\n- \"download model\", \"civitai\", \"checkpoint\", \"lora\"\n- \"custom nodes\", \"comfyui manager\"\n- \"ujust comfyui\", \"start comfyui\", \"configure comfyui\"\n- \"gpu image generation\", \"ai art\""
              },
              {
                "name": "config",
                "description": "Unified system configuration dispatcher for bazzite-ai. Manages services\n(Docker, Cockpit, SSH), desktop settings (gamemode, Steam), security\n(passwordless sudo), and development environment (GPU containers). Use\nwhen users need to enable/disable system features or check configuration status.\n",
                "path": "bazzite-ai/skills/config/SKILL.md",
                "frontmatter": {
                  "name": "config",
                  "description": "Unified system configuration dispatcher for bazzite-ai. Manages services\n(Docker, Cockpit, SSH), desktop settings (gamemode, Steam), security\n(passwordless sudo), and development environment (GPU containers). Use\nwhen users need to enable/disable system features or check configuration status.\n"
                },
                "content": "# Config - System Configuration Dispatcher\n\n## Overview\n\nThe `config` command is a unified dispatcher for system configuration tasks. It replaces scattered `toggle-*`, `setup-*`, and `config-*` commands with a single interface.\n\n**Key Concept:** All configuration targets support consistent actions: `enable`, `disable`, `status`, and `help`.\n\n## Quick Reference\n\n| Category | Targets |\n|----------|---------|\n| **Services** | `docker`, `cockpit`, `syncthing`, `libvirtd`, `sshd` |\n| **Desktop** | `gamemode`, `steam-autostart`, `shell` |\n| **Security** | `passwordless-sudo` |\n| **Apps** | `winboat` |\n| **Development** | `gpu`, `dev-environment` |\n\n## Parameters\n\n### Command Pattern\n\n```bash\nujust config TARGET=\"\" ACTION=\"\" ARGS...\n\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `TARGET` | See targets below | Configuration target |\n| `ACTION` | `enable`, `disable`, `status`, `help` | Action to perform |\n| `ARGS` | varies | Additional arguments |\n\nWithout `TARGET`, shows interactive picker.\n\n## Service Targets\n\n### Docker\n\n```bash\nujust config docker status        # Show Docker service status\nujust config docker enable        # Enable Docker daemon\nujust config docker disable       # Disable Docker daemon\nujust config docker enable-socket # Enable socket activation only\n\n```\n\n### Cockpit\n\n```bash\nujust config cockpit status       # Show Cockpit status\nujust config cockpit enable       # Enable web console\nujust config cockpit disable      # Disable web console\n\n```\n\nAccess at: `[https://localhost](https://localhost):9090`\n\n### Syncthing\n\n```bash\nujust config syncthing status     # Show Syncthing status\nujust config syncthing enable     # Enable file sync\nujust config syncthing disable    # Disable file sync\n\n```\n\n### Libvirtd\n\n```bash\nujust config libvirtd status      # Show libvirt status\nujust config libvirtd enable      # Enable virtualization\nujust config libvirtd disable     # Disable virtualization\n\n```\n\n### SSH Server\n\n```bash\nujust config sshd status          # Show SSH server status\nujust config sshd enable          # Enable SSH server\nujust config sshd disable         # Disable SSH server\n\n```\n\n## Desktop Targets\n\n### Gamemode\n\n```bash\nujust config gamemode status      # Show current session type\nujust config gamemode gamemode    # Set to Game Mode session\nujust config gamemode desktop     # Set to Desktop session\n\n```\n\n### Steam Autostart\n\n```bash\nujust config steam-autostart status   # Show autostart status\nujust config steam-autostart enable   # Enable Steam autostart\nujust config steam-autostart disable  # Disable Steam autostart\n\n```\n\n### Shell Configuration\n\nManages shell configuration files by synchronizing them with system skeleton defaults in `/etc/skel`.\n\n```bash\nujust config shell status   # Check if configs match skeleton\nujust config shell update   # Update all configs from /etc/skel (with backup)\n\n```\n\n**Managed files:**\n\n| File | Purpose |\n|------|---------|\n| `~/.bashrc` | Bash shell configuration |\n| `~/.zshrc` | Zsh shell configuration |\n| `~/.config/starship.toml` | Starship prompt config |\n| `~/.config/ghostty/` | Ghostty terminal config |\n\n**Backup location:** `~/.config-backup-shell-YYYYMMDD_HHMMSS/`\n\n## Security Targets\n\n### Passwordless Sudo\n\n```bash\nujust config passwordless-sudo status   # Show sudo config\nujust config passwordless-sudo enable   # Enable passwordless sudo\nujust config passwordless-sudo disable  # Disable passwordless sudo\n\n```\n\n**Warning:** Enabling passwordless sudo reduces security. Useful for development/automation.\n\n## Application Targets\n\n### WinBoat\n\n```bash\nujust config winboat launch              # Launch Windows app\nujust config winboat info                # Show WinBoat info\n\n```\n\n## Development Targets\n\n### GPU Containers\n\n```bash\nujust config gpu status       # Show GPU container support\nujust config gpu setup        # Setup GPU passthrough\n\n```\n\nConfigures:\n\n- NVIDIA Container Toolkit\n\n- AMD ROCm container support\n\n- Intel oneAPI container support\n\n### Dev Environment\n\n```bash\nujust config dev-environment verify      # Verify dev tools installed\n\n```\n\nChecks for required development tools and reports missing items.\n\n## Common Workflows\n\n### Setup Development Environment\n\n```bash\n# Enable passwordless sudo for automation\nujust config passwordless-sudo enable\n\n# Enable Docker for container development\nujust config docker enable\n\n# Setup GPU container support\nujust config gpu setup\n\n# Verify everything is ready\nujust config dev-environment verify\n\n```\n\n### Enable Remote Access\n\n```bash\n# Enable SSH server\nujust config sshd enable\n\n# Enable web console (Cockpit)\nujust config cockpit enable\n\n# Check both are running\nujust config sshd status\nujust config cockpit status\n\n```\n\n### Gaming Setup\n\n```bash\n# Set to Game Mode session\nujust config gamemode gamemode\n\n# Enable Steam autostart\nujust config steam-autostart enable\n\n```\n\n### Return to Desktop\n\n```bash\n# Set to Desktop session\nujust config gamemode desktop\n\n# Disable Steam autostart\nujust config steam-autostart disable\n\n```\n\n## Non-Interactive Usage\n\nAll commands work without TTY:\n\n```bash\n# CI/automation-friendly\nujust config docker enable\nujust config passwordless-sudo enable\n\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n**Symptom:** `ujust config <service> enable` completes but service not running\n\n**Fix:**\n\n```bash\n# Check service status\nsystemctl status <service>\n\n# Check logs\njournalctl -u <service> -n 50\n\n# Try manual start\nsudo systemctl start <service>\n\n```\n\n### GPU Containers Not Working\n\n**Symptom:** Containers can't access GPU\n\n**Cause:** GPU container toolkit not configured\n\n**Fix:**\n\n```bash\nujust config gpu setup\n# May require reboot\n\n```\n\n## Cross-References\n\n- **Related Skills:** `install` (for installing tools), `test` (for development)\n\n- **Services:** `jupyter`, `ollama`, `runners` (managed services with lifecycle)\n\n- **Documentation:** [Service Targets](./references/service-targets.md)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"enable Docker\", \"disable SSH\", \"configure cockpit\"\n\n- \"gamemode\", \"Game Mode session\", \"desktop mode\"\n\n- \"passwordless sudo\", \"sudo without password\"\n\n- \"GPU containers\", \"container GPU access\"\n\n- \"reset shell config\", \"restore bashrc\", \"default zshrc\"\n\n- \"starship not working\", \"prompt broken\", \"shell configuration\"\n\n- \"sync shell from skeleton\", \"ghostty config\""
              },
              {
                "name": "fiftyone",
                "description": "FiftyOne dataset visualization and curation tool via Podman Quadlet.\nMulti-container architecture with MongoDB sidecar for dataset persistence.\nGPU-accelerated for ML workflows. Use when users need to configure, start,\nor manage FiftyOne for dataset analysis.\n",
                "path": "bazzite-ai/skills/fiftyone/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone",
                  "description": "FiftyOne dataset visualization and curation tool via Podman Quadlet.\nMulti-container architecture with MongoDB sidecar for dataset persistence.\nGPU-accelerated for ML workflows. Use when users need to configure, start,\nor manage FiftyOne for dataset analysis.\n"
                },
                "content": "# FiftyOne - Dataset Visualization & Curation\n\n## Overview\n\nThe `fiftyone` command manages FiftyOne dataset visualization using Podman Quadlet containers. It includes a MongoDB sidecar for persistent dataset storage.\n\n**Key Concept:** FiftyOne runs as a multi-container application with a MongoDB sidecar. The main FiftyOne container handles the web UI and processing, while MongoDB stores dataset metadata.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust fiftyone config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust fiftyone start [--instance=N\\|all]` | Start FiftyOne + MongoDB |\n| Stop | `ujust fiftyone stop [--instance=N\\|all]` | Stop FiftyOne + MongoDB |\n| Restart | `ujust fiftyone restart [--instance=N\\|all]` | Restart all containers |\n| Logs | `ujust fiftyone logs [--instance=N] [--lines=...]` | View interleaved logs |\n| Status | `ujust fiftyone status [--instance=N]` | Show status (all instances) |\n| URL | `ujust fiftyone url [--instance=N]` | Show access URL |\n| Shell | `ujust fiftyone shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Plugins | `ujust fiftyone plugins [-- CMD...]` | Manage FiftyOne plugins |\n| Delete | `ujust fiftyone delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: config, start, stop, etc. |\n| config_dir | `--config-dir` | `-c` | `~/.config/fiftyone/{N}` | Configuration directory |\n| workspace_dir | `--workspace-dir` | `-w` | `\"\"` | Optional mount to /workspace |\n| bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| port | `--port` | `-p` | `5151` | Web UI port |\n| image | `--image` | `-i` | `docker.io/voxel51/fiftyone` | Container image |\n| tag | `--tag` | `-t` | `latest` | Image tag |\n| gpu_type | `--gpu-type` | `-g` | `auto` | GPU type (auto/nvidia/amd/intel/none) |\n| lines | `--lines` | `-l` | `50` | Log lines to show |\n| instance | `--instance` | `-n` | `1` | Instance number |\n\n## Configuration\n\n```bash\n# Default configuration (port 5151, localhost only)\nujust fiftyone config\n\n# Custom port (long form)\nujust fiftyone config --port=5152\n\n# Custom port (short form)\nujust fiftyone config -p 5152\n\n# Network-wide access\nujust fiftyone config --bind=0.0.0.0\n\n# With workspace mount\nujust fiftyone config --workspace-dir=/data/datasets\n\n# Combine parameters (long form)\nujust fiftyone config --port=5152 --bind=0.0.0.0 --workspace-dir=/data\n\n# Combine parameters (short form)\nujust fiftyone config -p 5152 -b 0.0.0.0 -w /data\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n## Lifecycle Commands\n\n### Start/Stop/Restart\n\n```bash\n# Start FiftyOne (includes MongoDB sidecar)\nujust fiftyone start\n\n# Start specific instance (long form)\nujust fiftyone start --instance=1\n\n# Start specific instance (short form)\nujust fiftyone start -n 1\n\n# Start all instances\nujust fiftyone start --instance=all\n\n# Stop FiftyOne + MongoDB\nujust fiftyone stop --instance=1\n\n# Restart all containers\nujust fiftyone restart\n```\n\n### View Logs\n\nFiftyOne shows interleaved logs from both the main container and MongoDB sidecar:\n\n```bash\n# Follow logs (default 50 lines)\nujust fiftyone logs\n\n# More lines (long form)\nujust fiftyone logs --lines=100\n\n# More lines (short form)\nujust fiftyone logs -l 100\n\n# Specific instance\nujust fiftyone logs -n 1 -l 100\n```\n\nLog output format:\n\n```\n[fiftyone-mongodb] 2024-01-09 10:00:01 MongoDB started\n[fiftyone] 2024-01-09 10:00:02 Connecting to database...\n[fiftyone] 2024-01-09 10:00:03 FiftyOne App ready on port 5151\n```\n\n### Get URL\n\n```bash\nujust fiftyone url\n# Output: http://localhost:5151\n\n# Specific instance\nujust fiftyone url --instance=2\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust fiftyone shell\n\n# Run specific command (use -- separator)\nujust fiftyone shell -- fiftyone --version\nujust fiftyone shell -- pip list\n\n# Specific instance\nujust fiftyone shell --instance=2 -- python -c \"import fiftyone as fo; print(fo.__version__)\"\n\n# Short form\nujust fiftyone shell -n 2 -- ls -la\n```\n\n## Plugin Management\n\n```bash\n# List installed plugins\nujust fiftyone plugins -- list\n\n# Install a plugin\nujust fiftyone plugins -- install <plugin-name>\n\n# Update plugins\nujust fiftyone plugins -- update\n```\n\n## Multi-Container Architecture\n\nFiftyOne runs with a MongoDB sidecar:\n\n```\n+-------------------+        +-------------------+\n|    FiftyOne       |        |     MongoDB       |\n|   (fiftyone-1)    | -----> | (fiftyone-mongodb-1) |\n|   Port 5151       |        |   Port 27017      |\n+-------------------+        +-------------------+\n         |                            |\n         +---- bazzite-ai network ----+\n```\n\n**Container Names:**\n- `fiftyone-{N}` - Main FiftyOne container\n- `fiftyone-mongodb-{N}` - MongoDB sidecar\n\n**Lifecycle:**\n- `start` starts MongoDB first, then FiftyOne\n- `stop` stops FiftyOne first, then MongoDB\n- `logs` shows interleaved output from both\n\n## Port Allocation\n\n| Instance | FiftyOne Port | MongoDB Port |\n|----------|---------------|--------------|\n| 1 | 5151 | 27017 |\n| 2 | 5152 | 27018 |\n| N | 5150+N | 27016+N |\n\n## GPU Support\n\nFiftyOne supports GPU acceleration for ML model inference:\n\n```bash\n# Auto-detect GPU (default)\nujust fiftyone config\n\n# Explicit NVIDIA (long form)\nujust fiftyone config --gpu-type=nvidia\n\n# Explicit NVIDIA (short form)\nujust fiftyone config -g nvidia\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Instance config | Per-instance settings | `~/.config/fiftyone/instance-{N}.env` |\n| Quadlet unit (main) | Service definition | `~/.config/containers/systemd/fiftyone-{N}.container` |\n| Quadlet unit (MongoDB) | Sidecar definition | `~/.config/containers/systemd/fiftyone-mongodb-{N}.container` |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure FiftyOne with dataset directory\nujust fiftyone config --workspace-dir=/data/datasets\n\n# 2. Start FiftyOne\nujust fiftyone start\n\n# 3. Get URL\nujust fiftyone url\n\n# 4. Open in browser\n# http://localhost:5151\n```\n\n### Dataset Analysis\n\n```bash\n# Start FiftyOne\nujust fiftyone start\n\n# Open shell for interactive work\nujust fiftyone shell\n\n# Inside container:\n# import fiftyone as fo\n# dataset = fo.load_dataset(\"my_dataset\")\n# session = fo.launch_app(dataset)\n```\n\n### Network Access\n\n```bash\n# Configure for network access\nujust fiftyone config --bind=0.0.0.0\n\n# Restart to apply\nujust fiftyone restart\n\n# Access from other machines\n# http://<hostname>:5151\n```\n\n## Troubleshooting\n\n### FiftyOne Won't Start\n\n**Check:**\n\n```bash\nujust fiftyone status\nujust fiftyone logs --lines=50\n```\n\n**Common causes:**\n\n- Port 5151 already in use\n- MongoDB failed to start\n- GPU driver issues\n\n**Fix:**\n\n```bash\n# Delete and reconfigure\nujust fiftyone delete\nujust fiftyone config --port=5152\nujust fiftyone start\n```\n\n### MongoDB Connection Failed\n\n**Symptom:** FiftyOne logs show \"Connection refused\" to MongoDB\n\n**Check:**\n\n```bash\n# Check MongoDB container\npodman ps | grep fiftyone-mongodb\nujust fiftyone logs | grep mongodb\n```\n\n**Fix:**\n\n```bash\n# Restart both containers\nujust fiftyone restart\n```\n\n### Datasets Not Persisting\n\n**Symptom:** Datasets disappear after restart\n\n**Check:**\n\n- Verify config_dir is properly set\n- Check MongoDB volume mounts\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit config directory\nujust fiftyone config --config-dir=/data/fiftyone\n```\n\n## Cross-References\n\n- **Related Skills:** `jupyter` (ML notebooks), `ollama` (LLM inference)\n- **FiftyOne Docs:** <https://docs.voxel51.com/>\n- **GPU Setup:** `ujust config gpu setup`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"fiftyone\", \"dataset visualization\", \"dataset curation\"\n- \"ML datasets\", \"computer vision datasets\"\n- \"data labeling\", \"annotation tool\"\n- \"start fiftyone\", \"configure fiftyone\""
              },
              {
                "name": "install",
                "description": "Development tool installation dispatcher for bazzite-ai. Installs Claude Code,\npixi, chunkhound, bcvk, linters, flatpaks, and more. Use when users need to\ninstall standalone developer tools (not services with lifecycle management).\n",
                "path": "bazzite-ai/skills/install/SKILL.md",
                "frontmatter": {
                  "name": "install",
                  "description": "Development tool installation dispatcher for bazzite-ai. Installs Claude Code,\npixi, chunkhound, bcvk, linters, flatpaks, and more. Use when users need to\ninstall standalone developer tools (not services with lifecycle management).\n"
                },
                "content": "# Install - Development Tool Installer\n\n## Overview\n\nThe `install` command is a unified dispatcher for installing standalone development tools. For services with lifecycle management (start/stop/logs), use their dedicated commands.\n\n**Key Concept:** This is for standalone tools only. Use `ujust jupyter install`, `ujust runners install`, or `ujust jellyfin install` for managed services.\n\n## Quick Reference\n\n### Development Tools\n\n| Program | Command | Description |\n|---------|---------|-------------|\n| Claude Code | `ujust install claude-code-npm` | Claude Code AI CLI (npm) |\n| Pixi | `ujust install pixi` | Conda-compatible package manager |\n| Chunkhound | `ujust install chunkhound` | Semantic code search MCP |\n| Devcontainers CLI | `ujust install devcontainers-cli` | Dev Container CLI |\n| TweakCC | `ujust install tweakcc` | Claude Code customization |\n| ccstatusline | `ujust install ccstatusline` | Claude Code statusline widget |\n| Chrome Extension Fix | `ujust install chrome-extension-fix` | Fix Claude extension |\n| GitHub MCP | `ujust install github-mcp-server` | GitHub MCP server |\n| Chrome DevTools MCP | `ujust install chrome-devtools-mcp` | Chrome DevTools MCP |\n| Kind | `ujust install kind [VERSION]` | Kubernetes in Docker |\n| Minikube | `ujust install minikube [VERSION]` | Local Kubernetes |\n| bcvk | `ujust install bcvk` | bootc virtualization kit |\n| Linters | `ujust install linters` | Code linting tools |\n| Homebrew | `ujust install homebrew` | Homebrew package manager |\n| AppImage Manager | `ujust install appimage-manager` | Gear Lever AppImage manager |\n| Gemini CLI | `ujust install gemini-cli` | Google Gemini CLI |\n| Firebase CLI | `ujust install firebase-cli` | Firebase project management |\n| Wrangler | `ujust install wrangler` | Cloudflare Workers CLI |\n\n### Meta-Installers\n\n| Program | Command | Description |\n|---------|---------|-------------|\n| Dev Tools | `ujust install dev-tools [COMPONENT]` | Install tool groups |\n| Kubernetes | `ujust install kubernetes-tools` | Kind + Minikube |\n\n### Flatpak Categories\n\n| Category | Command | Description |\n|----------|---------|-------------|\n| Dev | `ujust install flatpaks-dev` | Development flatpaks |\n| Media | `ujust install flatpaks-media` | Media & graphics |\n| Gaming | `ujust install flatpaks-gaming` | Gaming tools |\n| Communication | `ujust install flatpaks-communication` | Chat apps |\n| Productivity | `ujust install flatpaks-productivity` | Office tools |\n| Utilities | `ujust install flatpaks-utilities` | Utility apps |\n| Experimental | `ujust install flatpaks-experimental` | Experimental apps |\n| Streaming | `ujust install flatpaks-streaming` | Streaming clients |\n| All | `ujust install flatpaks-all` | All flatpaks |\n\n## Common Installations\n\n### AI Development Setup\n\n```bash\n# Install Claude Code\nujust install claude-code-npm\n\n# Install Chunkhound for code search\nujust install chunkhound\n\n# Install GitHub MCP server\nujust install github-mcp-server\n\n# Install Chrome DevTools MCP (for browser automation)\nujust install chrome-devtools-mcp\n\n```\n\n### Python/ML Development\n\n```bash\n# Install Pixi (Conda-compatible, faster)\nujust install pixi\n\n# Install development flatpaks\nujust install flatpaks-dev\n\n```\n\n### Kubernetes Development\n\n```bash\n# Install both Kind and Minikube\nujust install kubernetes-tools\n\n# Or individually\nujust install kind\nujust install minikube\n\n```\n\n### VM Testing\n\n```bash\n# Install bcvk for bootc VM testing\nujust install bcvk\n\n```\n\n## Dev Tools Meta-Installer\n\nInstall groups of tools at once:\n\n```bash\n# Quick essentials\nujust install dev-tools quick\n\n# Core development tools\nujust install dev-tools core\n\n# Claude Code ecosystem\nujust install dev-tools claude\n\n# Code quality tools\nujust install dev-tools quality\n\n# Extra utilities\nujust install dev-tools extras\n\n# Google tools (Gemini, Firebase)\nujust install dev-tools google\n\n# Full development environment\nujust install dev-tools environment\n\n```\n\n### Component Groups\n\n| Component | Includes |\n|-----------|----------|\n| `quick` | claude-code-npm, pixi |\n| `core` | quick + homebrew, linters |\n| `claude` | chunkhound, github-mcp, tweakcc, ccstatusline |\n| `quality` | linters, devcontainers-cli |\n| `extras` | bcvk, appimage-manager |\n| `google` | gemini-cli, firebase-cli, wrangler |\n| `environment` | All of the above |\n\n## Services vs Install\n\n| For This | Use This |\n|----------|----------|\n| JupyterLab | `ujust jupyter install` |\n| GitHub Runners | `ujust runners install` |\n| Jellyfin | `ujust jellyfin install` |\n| Ollama | `ujust ollama install` |\n| Standalone tools | `ujust install <tool>` |\n\nServices have lifecycle commands (start/stop/logs). Standalone tools are just installed.\n\n## Flatpak Details\n\n### Development\n\n```bash\nujust install flatpaks-dev\n# Includes: VS Code, PyCharm, etc.\n\n```\n\n### Media\n\n```bash\nujust install flatpaks-media\n# Includes: GIMP, Inkscape, Kdenlive, etc.\n\n```\n\n### Gaming\n\n```bash\nujust install flatpaks-gaming\n# Includes: Lutris, Heroic, ProtonUp-Qt, etc.\n\n```\n\n### All Flatpaks\n\n```bash\nujust install flatpaks-all\n# Installs all categories\n\n```\n\n## Troubleshooting\n\n### Installation Failed\n\n**Check:**\n\n```bash\n# For npm-based tools\nnpm --version\n\n# For Homebrew tools\nbrew --version\n\n# For Flatpaks\nflatpak --version\n\n```\n\n### Claude Code Not Found After Install\n\n**Cause:** Shell not reloaded\n\n**Fix:**\n\n```bash\nexec $SHELL\n# Or\nsource ~/.bashrc\n\n```\n\n### Pixi Not Found\n\n**Fix:**\n\n```bash\n# Add to PATH\nexport PATH=\"$HOME/.pixi/bin:$PATH\"\n\n# Or reload shell\nexec $SHELL\n\n```\n\n### Flatpak Install Fails\n\n**Check:**\n\n```bash\n# Verify Flathub remote\nflatpak remote-list\n\n```\n\n**Fix:**\n\n```bash\n# Add Flathub if missing\nflatpak remote-add --if-not-exists flathub [https://flathub.org/repo/flathub.flatpakrepo]([https://flathub.org/repo/flathub.flatpakrepo](https://flathub.org/repo/flathub.flatpakrepo))\n```\n\n## Cross-References\n\n- **Services:** `jupyter`, `runners`, `jellyfin`, `ollama` (have lifecycle commands)\n\n- **Configuration:** `configure` (for enabling system services)\n\n- **VM Tools:** `vm`, `bootc` (after installing bcvk)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install claude code\", \"setup claude\", \"claude cli\"\n\n- \"install pixi\", \"conda alternative\"\n\n- \"install kubernetes\", \"setup kind\", \"minikube\"\n\n- \"install flatpaks\", \"flatpak applications\"\n\n- \"install development tools\", \"dev environment\"\n\n- \"install bcvk\", \"bootc tools\""
              },
              {
                "name": "jellyfin",
                "description": "Jellyfin media server management via Podman Quadlet. Supports multi-instance\ndeployment, hardware transcoding (NVIDIA/AMD/Intel), and FUSE filesystem\nmounts. Use when users need to set up or manage Jellyfin media servers.\n",
                "path": "bazzite-ai/skills/jellyfin/SKILL.md",
                "frontmatter": {
                  "name": "jellyfin",
                  "description": "Jellyfin media server management via Podman Quadlet. Supports multi-instance\ndeployment, hardware transcoding (NVIDIA/AMD/Intel), and FUSE filesystem\nmounts. Use when users need to set up or manage Jellyfin media servers.\n"
                },
                "content": "# Jellyfin - Media Server Management\n\n## Overview\n\nThe `jellyfin` command manages Jellyfin media server instances using Podman Quadlet containers. It supports hardware transcoding and FUSE filesystem compatibility for network mounts.\n\n**Key Concept:** Multi-instance support allows running multiple media libraries. FUSE compatibility enables rclone/sshfs mounts for cloud or remote storage.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust jellyfin config --config-dir=<CONFIG> --cache-dir=<CACHE> --media-dir=<MEDIA>` | Configure instance |\n| Start | `ujust jellyfin start [--instance=N\\|all]` | Start instance(s) |\n| Stop | `ujust jellyfin stop [--instance=N\\|all]` | Stop instance(s) |\n| Restart | `ujust jellyfin restart [--instance=N\\|all]` | Restart instance(s) |\n| Logs | `ujust jellyfin logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust jellyfin list` | List all instances |\n| Status | `ujust jellyfin status [--instance=N]` | Show instance status |\n| URL | `ujust jellyfin url [--instance=N]` | Show access URL |\n| Shell | `ujust jellyfin shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Delete | `ujust jellyfin delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Required | Description |\n|-----------|-----------|-------|----------|-------------|\n| Config Dir | `--config-dir` | `-c` | Yes | Configuration directory |\n| Cache Dir | `--cache-dir` | - | Yes | Cache directory (transcoding) |\n| Media Dir | `--media-dir` | - | Yes | Media library path |\n| Instance | `--instance` | `-n` | No | Instance number (default: 1) |\n| GPU Type | `--gpu-type` | `-g` | No | GPU: nvidia, amd, intel, auto |\n| Image | `--image` | `-i` | No | Container image |\n| Tag | `--tag` | `-t` | No | Image tag (default: stable) |\n| Workspace | `--workspace-dir` | `-w` | No | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | No | Bind address |\n| Port | `--port` | `-p` | No | Service port |\n| Lines | `--lines` | `-l` | No | Log lines to show |\n\n### Configuration Examples\n\n```bash\n# Basic installation (long form)\nujust jellyfin config --config-dir=~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media\n\n# With NVIDIA GPU for transcoding\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media --gpu-type=nvidia\n\n# Second instance for different library\nujust jellyfin config -c ~/jellyfin2/config --cache-dir=~/jellyfin2/cache --media-dir=~/videos --instance=2\n\n# With short forms\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media -n 1 -g nvidia\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust jellyfin shell\n\n# Run specific command (use -- separator)\nujust jellyfin shell -- df -h\n\n# Shell in specific instance\nujust jellyfin shell --instance=2 -- ls /media\n```\n\n## Lifecycle Commands\n\n### Start/Stop\n\n```bash\n# Single instance\nujust jellyfin start --instance=1\nujust jellyfin stop --instance=1\n\n# Short form\nujust jellyfin start -n 1\nujust jellyfin stop -n 1\n\n# All instances\nujust jellyfin start --instance=all\nujust jellyfin stop --instance=all\n```\n\n### View Logs\n\n```bash\n# Follow logs\nujust jellyfin logs\n\n# Specific instance with line count\nujust jellyfin logs --instance=1 --lines=100\n\n# Short form\nujust jellyfin logs -n 1 -l 100\n```\n\n### Get URL\n\n```bash\nujust jellyfin url\n# Output: http://localhost:8096\n\n# Specific instance\nujust jellyfin url --instance=2\n```\n\n## Port Allocation\n\n| Instance | Port |\n|----------|------|\n| 1 | 8096 |\n| 2 | 8097 |\n| 3 | 8098 |\n| N | 8095+N |\n\n## Hardware Transcoding\n\n### GPU Types\n\n| GPU | Flag Value | Transcoding |\n|-----|------------|-------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | NVENC/NVDEC |\n| AMD | `--gpu-type=amd` or `-g amd` | VAAPI |\n| Intel | `--gpu-type=intel` or `-g intel` | QuickSync |\n\n### Enable GPU\n\n```bash\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media --gpu-type=nvidia\n```\n\n### Verify GPU\n\n```bash\n# Check inside container\nujust jellyfin shell -- nvidia-smi  # or vainfo for AMD/Intel\n```\n\n## FUSE Filesystem Support\n\nJellyfin containers support FUSE mounts (rclone, sshfs) for remote storage.\n\n### Mount Before Starting\n\n```bash\n# Mount cloud storage\nrclone mount gdrive:media ~/media --daemon\n\n# Then start Jellyfin\nujust jellyfin start 1\n```\n\n### Why Host Networking?\n\nJellyfin uses host networking for:\n\n- DLNA discovery\n- mDNS/Bonjour\n- Chromecast\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/jellyfin-1.container` |\n| Instance config | Settings | `~/.config/jellyfin/instance-1.env` |\n| Jellyfin data | Libraries, users | `<CONFIG>/` |\n| Transcoding cache | Temp files | `<CACHE>/` |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Create directories\nmkdir -p ~/jellyfin/{config,cache}\n\n# 2. Configure Jellyfin\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media --gpu-type=nvidia\n\n# 3. Start it\nujust jellyfin start\n\n# 4. Access web UI\nujust jellyfin url\n# Open http://localhost:8096\n```\n\n### Multiple Libraries\n\n```bash\n# Movies library\nujust jellyfin config -c ~/jellyfin-movies/config --cache-dir=~/jellyfin-movies/cache --media-dir=~/movies -n 1\n\n# TV library\nujust jellyfin config -c ~/jellyfin-tv/config --cache-dir=~/jellyfin-tv/cache --media-dir=~/tv -n 2\n\n# Start both\nujust jellyfin start --instance=all\n```\n\n### Cloud Storage\n\n```bash\n# 1. Mount cloud storage\nrclone mount gdrive:media ~/cloud-media --daemon --vfs-cache-mode writes\n\n# 2. Configure Jellyfin pointing to mount\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/cloud-media\n\n# 3. Start\nujust jellyfin start\n```\n\n## Initial Configuration\n\nFirst-time setup via web UI:\n\n1. Open `http://localhost:8096`\n2. Create admin user\n3. Add media libraries\n4. Configure transcoding (if GPU)\n5. Set up remote access\n\n## Troubleshooting\n\n### Jellyfin Won't Start\n\n**Check:**\n\n```bash\nujust jellyfin status\nujust jellyfin logs --lines=50\n```\n\n**Common causes:**\n\n- Port conflict (8096 in use)\n- Invalid paths\n- GPU driver issues\n\n### Transcoding Fails\n\n**Check:**\n\n```bash\n# View logs for transcoding errors\nujust jellyfin logs | grep -i transcode\n```\n\n**Common causes:**\n\n- GPU not passed through\n- Missing codec support\n\n**Fix:**\n\n```bash\n# Reconfigure with GPU\nujust jellyfin delete\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media --gpu-type=nvidia\n```\n\n### Media Not Found\n\n**Check:**\n\n- Media directory exists\n- Correct path in config\n- Permissions\n\n**Fix:**\n\n```bash\n# Verify path\nls ~/media\n\n# Reconfigure with correct path\nujust jellyfin delete\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=/correct/path\n```\n\n### DLNA Not Working\n\n**Cause:** Network isolation\n\nJellyfin uses host networking, but ensure:\n\n- Firewall allows mDNS (5353/udp)\n- Same network as clients\n\n## Cross-References\n\n- **Related Skills:** `configure gpu` (GPU setup)\n- **Jellyfin Docs:** <https://jellyfin.org/docs/>\n- **Web UI:** [http://localhost:8096](http://localhost:8096)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install jellyfin\", \"setup media server\"\n- \"jellyfin not working\", \"jellyfin transcoding\"\n- \"jellyfin GPU\", \"hardware transcoding\"\n- \"multiple jellyfin\", \"jellyfin instances\""
              },
              {
                "name": "jupyter",
                "description": "JupyterLab ML/AI development environment management via Podman Quadlet.\nSupports multi-instance deployment, GPU acceleration (NVIDIA/AMD/Intel),\ntoken authentication, and per-instance configuration. Use when users need\nto configure, start, stop, or manage JupyterLab containers for ML development.\n",
                "path": "bazzite-ai/skills/jupyter/SKILL.md",
                "frontmatter": {
                  "name": "jupyter",
                  "description": "JupyterLab ML/AI development environment management via Podman Quadlet.\nSupports multi-instance deployment, GPU acceleration (NVIDIA/AMD/Intel),\ntoken authentication, and per-instance configuration. Use when users need\nto configure, start, stop, or manage JupyterLab containers for ML development.\n"
                },
                "content": "# Jupyter - ML/AI Development Environment\n\n## Overview\n\nThe `jupyter` command manages JupyterLab instances for ML/AI development using Podman Quadlet containers. Each instance runs as a systemd user service with optional GPU acceleration.\n\n**Key Concept:** Multi-instance support allows running multiple isolated JupyterLab environments simultaneously, each on different ports with different GPU configurations.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust jupyter config [--instance=N] [--port=...] [--gpu-type=...]` | Configure instance N |\n| Start | `ujust jupyter start [--instance=N\\|all]` | Start instance(s) |\n| Stop | `ujust jupyter stop [--instance=N\\|all]` | Stop instance(s) |\n| Restart | `ujust jupyter restart [--instance=N\\|all]` | Restart instance(s) |\n| Logs | `ujust jupyter logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust jupyter list` | List all instances |\n| Status | `ujust jupyter status [--instance=N]` | Show instance status |\n| URL | `ujust jupyter url [--instance=N]` | Show access URL |\n| Shell | `ujust jupyter shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Token enable | `ujust jupyter token-enable [--instance=N]` | Enable token auth |\n| Token show | `ujust jupyter token-show [--instance=N]` | Show token |\n| Token disable | `ujust jupyter token-disable [--instance=N]` | Disable token auth |\n| Token regenerate | `ujust jupyter token-regenerate [--instance=N]` | Generate new token |\n| Delete | `ujust jupyter delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Instance | `--instance` | `-n` | `1` | Instance number (1, 2, 3...) |\n| Port | `--port` | `-p` | `8888` | Web UI port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type: `nvidia`, `amd`, `intel`, `none`, `auto` |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n### Instance Numbering\n\n- Instance 1: Port 8888 (default)\n- Instance 2: Port 8889\n- Instance N: Port 8887+N\n\n## Configuration Examples\n\n```bash\n# Default: Instance 1, port 8888, auto-detect GPU\nujust jupyter config\n\n# Instance 2 with custom port and NVIDIA GPU (long form)\nujust jupyter config --instance=2 --port=8889 --gpu-type=nvidia\n\n# Instance 2 with custom port and NVIDIA GPU (short form)\nujust jupyter config -n 2 -p 8889 -g nvidia\n\n# Instance 3 with AMD GPU\nujust jupyter config -n 3 -p 8890 -g amd\n\n# No GPU acceleration\nujust jupyter config --gpu-type=none\n\n# With workspace mount\nujust jupyter config --gpu-type=nvidia --workspace-dir=/home/user/projects\n\n# Network-wide access\nujust jupyter config --bind=0.0.0.0\n\n# Combine multiple options\nujust jupyter config -n 2 -p 8889 -g nvidia -b 0.0.0.0 -w /home/user/projects\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust jupyter shell\n\n# Run specific command (use -- separator)\nujust jupyter shell -- pip list\n\n# Shell in specific instance\nujust jupyter shell --instance=2 -- nvidia-smi\n\n# Short form\nujust jupyter shell -n 2 -- nvidia-smi\n```\n\n## Lifecycle Commands\n\n### Start/Stop/Restart\n\n```bash\n# Single instance (long form)\nujust jupyter start --instance=1\nujust jupyter stop --instance=1\nujust jupyter restart --instance=1\n\n# Single instance (short form)\nujust jupyter start -n 1\nujust jupyter stop -n 1\nujust jupyter restart -n 1\n\n# All instances\nujust jupyter start --instance=all\nujust jupyter stop --instance=all\nujust jupyter restart --instance=all\n```\n\n### View Logs\n\n```bash\n# Follow logs (instance 1 default)\nujust jupyter logs\n\n# Specific instance\nujust jupyter logs --instance=1\n\n# Last N lines (long form)\nujust jupyter logs --lines=100\n\n# Last N lines (short form)\nujust jupyter logs -l 100 -n 2\n```\n\n### Get Access URL\n\n```bash\nujust jupyter url\n# Output: http://localhost:8888\n\n# Specific instance\nujust jupyter url --instance=2\n```\n\n## Token Authentication\n\nBy default, JupyterLab requires no token for local development. Enable token auth for remote access or shared environments.\n\n```bash\n# Enable token (generates random token) - instance 1 default\nujust jupyter token-enable\n\n# Enable token for specific instance\nujust jupyter token-enable --instance=2\n\n# Show current token\nujust jupyter token-show --instance=1\n\n# Disable token (password-less access)\nujust jupyter token-disable\n\n# Generate new token\nujust jupyter token-regenerate --instance=1\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/jupyter-1.container` |\n| Instance config | Per-instance settings | `~/.config/jupyter/instance-1.env` |\n\n## Volume Mounts\n\n| Container Path | Host Path | Purpose |\n|----------------|-----------|---------|\n| `/workspace` | `$HOME` | User home directory |\n| `/home/jovyan/.jupyter` | `~/.jupyter` | Jupyter config |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure JupyterLab with GPU support\nujust jupyter config --gpu-type=nvidia\n\n# 2. Start the instance\nujust jupyter start\n\n# 3. Get the URL\nujust jupyter url\n\n# 4. Open in browser\n# http://localhost:8888\n```\n\n### Multiple Environments\n\n```bash\n# PyTorch environment (instance 1)\nujust jupyter config --instance=1 --gpu-type=nvidia\n\n# TensorFlow environment (instance 2)\nujust jupyter config -n 2 -p 8889 -g nvidia\n\n# CPU-only data science (instance 3)\nujust jupyter config -n 3 -p 8890 -g none\n\n# Start all\nujust jupyter start --instance=all\n\n# List all\nujust jupyter list\n```\n\n### Remote Access\n\n```bash\n# Enable token for security\nujust jupyter token-enable\n\n# Get token\nujust jupyter token-show\n# Use: http://your-ip:8888/?token=<token>\n```\n\n## GPU Support\n\n### Automatic Detection\n\n```bash\nujust jupyter config  # Auto-detects GPU type\n```\n\n### Manual Selection\n\n| GPU Type | Flag Value | Requirements |\n|----------|------------|--------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | NVIDIA drivers + nvidia-container-toolkit |\n| AMD | `--gpu-type=amd` or `-g amd` | ROCm drivers |\n| Intel | `--gpu-type=intel` or `-g intel` | oneAPI runtime |\n| None | `--gpu-type=none` or `-g none` | CPU only |\n\n### Verify GPU Access\n\n```bash\nujust jupyter shell -- nvidia-smi  # NVIDIA\nujust jupyter shell -- rocm-smi    # AMD\n```\n\n## Troubleshooting\n\n### Instance Won't Start\n\n**Symptom:** `ujust jupyter start` fails\n\n**Check:**\n\n```bash\n# Check service status\nsystemctl --user status jupyter-1\n\n# Check logs\nujust jupyter logs --lines=50\n```\n\n**Common causes:**\n\n- Port already in use\n- GPU not available\n- Image not pulled\n\n### GPU Not Detected\n\n**Symptom:** No GPU acceleration in notebooks\n\n**Check:**\n\n```bash\n# Verify GPU config\nujust jupyter status\n\n# Test inside container\nujust jupyter shell -- nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit GPU type\nujust jupyter delete\nujust jupyter config --gpu-type=nvidia\n```\n\n### Token Issues\n\n**Symptom:** Can't access Jupyter, token required\n\n**Fix:**\n\n```bash\n# Show current token\nujust jupyter token-show\n\n# Or disable token for local use\nujust jupyter token-disable\n```\n\n### Port Conflict\n\n**Symptom:** \"Address already in use\"\n\n**Fix:**\n\n```bash\n# Find what's using the port\nlsof -i :8888\n\n# Use different port\nujust jupyter config --port=8889\n```\n\n## Cross-References\n\n- **Related Skills:** `pod` (build images), `configure gpu` (GPU setup)\n- **GPU Setup:** `ujust config gpu setup`\n- **Documentation:** [Podman Quadlet Docs](https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install jupyter\", \"setup jupyterlab\", \"ML development\"\n- \"start jupyter\", \"stop jupyter\", \"restart jupyter\"\n- \"jupyter not working\", \"jupyter won't start\"\n- \"jupyter token\", \"jupyter password\", \"jupyter authentication\"\n- \"jupyter GPU\", \"jupyter nvidia\", \"jupyter cuda\"\n- \"multiple jupyter\", \"second jupyter instance\""
              },
              {
                "name": "localai",
                "description": "LocalAI local inference API management via Podman Quadlet. Provides an\nOpenAI-compatible API for local model inference with GPU acceleration.\nUse when users need to configure, start, or manage the LocalAI service.\n",
                "path": "bazzite-ai/skills/localai/SKILL.md",
                "frontmatter": {
                  "name": "localai",
                  "description": "LocalAI local inference API management via Podman Quadlet. Provides an\nOpenAI-compatible API for local model inference with GPU acceleration.\nUse when users need to configure, start, or manage the LocalAI service.\n"
                },
                "content": "# LocalAI - Local AI Inference API\n\n## Overview\n\nThe `localai` command manages the LocalAI service using Podman Quadlet containers. It provides an OpenAI-compatible API for running AI models locally with GPU acceleration.\n\n**Key Features:**\n\n- OpenAI-compatible API endpoints\n- GPU-specific container images (auto-selected)\n- Multiple GPU support (NVIDIA, AMD, Intel)\n- Cross-pod DNS via `bazzite-ai` network\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust localai config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust localai start [--instance=...]` | Start service |\n| Stop | `ujust localai stop [--instance=...]` | Stop service |\n| Restart | `ujust localai restart [--instance=...]` | Restart service |\n| Logs | `ujust localai logs [--lines=...]` | View logs |\n| Status | `ujust localai status [--instance=...]` | Show status |\n| URL | `ujust localai url [--instance=...]` | Show API URL |\n| List | `ujust localai list` | List instances |\n| Shell | `ujust localai shell [-- CMD...]` | Container shell |\n| Delete | `ujust localai delete [--instance=...]` | Remove service |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `8080` | Host port for API |\n| Image | `--image` | `-i` | (auto by GPU) | Container image |\n| Tag | `--tag` | `-t` | `latest` | Image tag |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Config Dir | `--config-dir` | `-c` | `~/.config/localai/1` | Config/models directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Workspace mount |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type |\n| Instance | `--instance` | `-n` | `1` | Instance number or `all` |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n## GPU-Specific Images\n\nLocalAI uses different container images optimized for each GPU type:\n\n| GPU Type | Image | Auto-Selected? |\n|----------|-------|----------------|\n| CPU (none) | `localai/localai:latest` | Yes |\n| NVIDIA | `localai/localai:latest-gpu-nvidia-cuda-12` | Yes |\n| AMD | `localai/localai:latest-gpu-hipblas` | Yes |\n| Intel | `localai/localai:latest-gpu-intel` | Yes |\n\nThe appropriate image is automatically selected based on detected GPU hardware.\n\n## Configuration\n\n```bash\n# Default configuration (auto-detects GPU, port 8080)\nujust localai config\n\n# Custom port (long form)\nujust localai config --port=8081\n\n# Custom port (short form)\nujust localai config -p 8081\n\n# Network-wide access\nujust localai config --bind=0.0.0.0\n\n# Force CPU image (ignore GPU)\nujust localai config --image=localai/localai:latest\n\n# Combine parameters (long form)\nujust localai config --port=8081 --bind=0.0.0.0\n\n# Combine parameters (short form)\nujust localai config -p 8081 -b 0.0.0.0\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured updates the existing settings:\n\n```bash\n# Change only the bind address\nujust localai config --bind=0.0.0.0\n\n# Update port without affecting other settings\nujust localai config --port=8082\n```\n\n## Lifecycle Management\n\n```bash\n# Start LocalAI\nujust localai start\n\n# Stop service\nujust localai stop\n\n# Restart (apply config changes)\nujust localai restart\n\n# View logs (default 50 lines)\nujust localai logs\n\n# View more logs (long form)\nujust localai logs --lines=200\n\n# View more logs (short form)\nujust localai logs -l 200\n\n# Check status\nujust localai status\n\n# Show API URL\nujust localai url\n```\n\n## Multi-Instance Support\n\n```bash\n# Start all instances (long form)\nujust localai start --instance=all\n\n# Start all instances (short form)\nujust localai start -n all\n\n# Stop specific instance\nujust localai stop --instance=2\n\n# Delete all instances\nujust localai delete --instance=all\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust localai shell\n\n# Run specific command (use -- separator)\nujust localai shell -- ls -la /models\nujust localai shell -- nvidia-smi\n```\n\n## Network Architecture\n\nLocalAI uses the `bazzite-ai` bridge network for cross-container DNS:\n\n```\n+-------------------+     DNS      +-------------------+\n|   Open WebUI      | -----------> |     LocalAI       |\n|   (openwebui)     |              |    (localai)      |\n|   Port 3000       |              |   Port 8080       |\n+-------------------+              +-------------------+\n         |                                  |\n         +------ bazzite-ai network --------+\n                         |\n+-------------------+    |    +-------------------+\n|     Ollama        |----+----+     Jupyter       |\n|    (ollama)       |         |    (jupyter)      |\n|   Port 11434      |         |   Port 8888       |\n+-------------------+         +-------------------+\n```\n\n**Cross-Pod DNS:**\n\n- LocalAI accessible as `http://localai:8080` from other containers\n- Can replace Ollama as backend for OpenWebUI\n\n## API Endpoints (OpenAI-Compatible)\n\n| Endpoint | Description |\n|----------|-------------|\n| `/v1/models` | List available models |\n| `/v1/chat/completions` | Chat completions |\n| `/v1/completions` | Text completions |\n| `/v1/embeddings` | Generate embeddings |\n| `/v1/images/generations` | Image generation |\n| `/v1/audio/transcriptions` | Speech-to-text |\n\n### Example API Usage\n\n```bash\n# List models\ncurl http://localhost:8080/v1/models\n\n# Chat completion\ncurl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Model Storage\n\n| Path | Description |\n|------|-------------|\n| `~/.config/localai/<INSTANCE>/models` | Model files |\n\nModels persist across container restarts. Each instance has isolated storage.\n\n### Loading Models\n\nPlace model files (GGUF, GGML) in the models directory:\n\n```bash\n# Copy a model\ncp my-model.gguf ~/.config/localai/1/models/\n\n# Or download directly\ncurl -L -o ~/.config/localai/1/models/model.gguf \\\n  https://huggingface.co/.../model.gguf\n```\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure LocalAI (auto-detects GPU)\nujust localai config\n\n# 2. Start the service\nujust localai start\n\n# 3. Check the API\nujust localai url\n# Output: http://127.0.0.1:8080\n\n# 4. Test the API\ncurl http://localhost:8080/v1/models\n```\n\n### Use with OpenWebUI\n\nOpenWebUI can use LocalAI as an OpenAI-compatible backend:\n\n```bash\n# Start LocalAI\nujust localai start\n\n# In OpenWebUI settings, add connection:\n# URL: http://localai:8080/v1  (cross-pod DNS)\n# Or: http://host.containers.internal:8080/v1  (from host)\n```\n\n### Remote Access Setup\n\n```bash\n# Configure for network access\nujust localai config --bind=0.0.0.0\n\n# Start the service\nujust localai start\n\n# Or use Tailscale for secure access\nujust tailscale serve --service=localai\n```\n\n## GPU Support\n\nGPU is automatically detected and the appropriate image is selected:\n\n| GPU Type | Detection | Device Passthrough |\n|----------|-----------|-------------------|\n| NVIDIA | `nvidia-smi` | CDI (`nvidia.com/gpu=all`) |\n| AMD | lspci | `/dev/dri` + `/dev/kfd` |\n| Intel | lspci | `/dev/dri` |\n\n### Check GPU in Container\n\n```bash\n# NVIDIA\nujust localai shell -- nvidia-smi\n\n# Check GPU environment\nujust localai shell -- env | grep -i gpu\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n```bash\n# Check status\nujust localai status\n\n# View logs\nujust localai logs --lines=100\n\n# Check image was pulled\npodman images | grep localai\n```\n\n**Common causes:**\n\n- Port 8080 already in use\n- Container image not pulled\n- GPU driver issues\n\n### GPU Not Detected\n\n**NVIDIA:**\n\n```bash\n# Check CDI configuration\nnvidia-ctk cdi list\n\n# Regenerate CDI spec\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n```\n\n**AMD:**\n\n```bash\n# Check /dev/kfd exists\nls -la /dev/kfd\n\n# Check ROCm\nrocminfo\n```\n\n### API Errors\n\n```bash\n# Test API endpoint\ncurl http://localhost:8080/v1/models\n\n# Check logs for errors\nujust localai logs --lines=100\n```\n\n### Clear Data and Start Fresh\n\n```bash\n# Delete everything\nujust localai delete --instance=all\n\n# Reconfigure\nujust localai config\nujust localai start\n```\n\n## Cross-References\n\n- **Network peers:** ollama, openwebui, jupyter, comfyui (all use bazzite-ai network)\n- **Alternative:** `ollama` (simpler model management, different API)\n- **Client:** `openwebui` (can use LocalAI as backend)\n- **Docs:** [LocalAI Documentation](https://localai.io/)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install localai\", \"setup local inference\", \"openai-compatible api\"\n- \"configure localai\", \"change port\", \"gpu acceleration\"\n- \"localai not working\", \"api error\", \"model loading\"\n- \"localai logs\", \"debug localai\"\n- \"delete localai\", \"uninstall\""
              },
              {
                "name": "ollama",
                "description": "Ollama LLM inference server management via Podman Quadlet. Single-instance\ndesign with GPU acceleration for running local LLMs. Use when users need\nto configure Ollama, pull models, run inference, or manage the Ollama server.\n",
                "path": "bazzite-ai/skills/ollama/SKILL.md",
                "frontmatter": {
                  "name": "ollama",
                  "description": "Ollama LLM inference server management via Podman Quadlet. Single-instance\ndesign with GPU acceleration for running local LLMs. Use when users need\nto configure Ollama, pull models, run inference, or manage the Ollama server.\n"
                },
                "content": "# Ollama - Local LLM Inference Server\n\n## Overview\n\nThe `ollama` command manages the Ollama LLM inference server using Podman Quadlet containers. It provides a single-instance server for running local LLMs with GPU acceleration.\n\n**Key Concept:** Unlike Jupyter, Ollama uses a single-instance design because GPU memory is shared across all loaded models. The API is accessible at port 11434.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust ollama config [--port=...] [--gpu-type=...]` | Configure server |\n| Start | `ujust ollama start` | Start server |\n| Stop | `ujust ollama stop` | Stop server |\n| Restart | `ujust ollama restart` | Restart server |\n| Logs | `ujust ollama logs [--lines=...]` | View logs |\n| Status | `ujust ollama status` | Show server status |\n| Pull | `ujust ollama pull --model=<MODEL>` | Download a model |\n| List | `ujust ollama list` | List installed models |\n| Run | `ujust ollama run --model=<MODEL> [--prompt=...]` | Run model |\n| Shell | `ujust ollama shell [-- CMD...]` | Open container shell |\n| Delete | `ujust ollama delete` | Remove server and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `11434` | API port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type: `nvidia`, `amd`, `intel`, `none`, `auto` |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Config Dir | `--config-dir` | `-c` | `~/.config/ollama/1` | Config/data directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n| Model | `--model` | `-m` | `qwen3:4b` | Model for pull/run actions |\n| Prompt | `--prompt` | - | `say hi` | Prompt for run action |\n| Context Length | `--context-length` | - | `8192` | Context window size |\n| Instance | `--instance` | `-n` | `1` | Instance number |\n\n## Configuration\n\n```bash\n# Default: Port 11434, auto-detect GPU\nujust ollama config\n\n# Custom port with NVIDIA GPU (long form)\nujust ollama config --port=11435 --gpu-type=nvidia\n\n# Custom port with NVIDIA GPU (short form)\nujust ollama config -p 11435 -g nvidia\n\n# CPU only\nujust ollama config --gpu-type=none\n\n# With workspace mount\nujust ollama config --gpu-type=nvidia --workspace-dir=/home/user/projects\n\n# Custom context length\nujust ollama config --context-length=16384\n\n# Network-wide access\nujust ollama config --bind=0.0.0.0\n\n# Combine multiple options\nujust ollama config -p 11435 -g nvidia -b 0.0.0.0 --context-length=16384\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust ollama shell\n\n# Run specific command (use -- separator)\nujust ollama shell -- nvidia-smi\nujust ollama shell -- df -h\nujust ollama shell -- ls -la /root/.ollama\n```\n\n## Model Management\n\n### Pull Models\n\n```bash\n# Download popular models (long form)\nujust ollama pull --model=llama3.2\nujust ollama pull --model=codellama\nujust ollama pull --model=mistral\nujust ollama pull --model=phi3\n\n# Short form\nujust ollama pull -m llama3.2\nujust ollama pull -m codellama\n\n# Specific versions\nujust ollama pull -m llama3.2:7b\nujust ollama pull -m llama3.2:70b\n```\n\n### List Models\n\n```bash\nujust ollama list\n```\n\nOutput:\n\n```\nNAME              SIZE      MODIFIED\nllama3.2:latest   4.7 GB    2 hours ago\ncodellama:latest  3.8 GB    1 day ago\n```\n\n### Run Models\n\n```bash\n# Interactive chat (long form)\nujust ollama run --model=llama3.2\n\n# Interactive chat (short form)\nujust ollama run -m llama3.2\n\n# Single prompt\nujust ollama run -m llama3.2 --prompt=\"Explain quantum computing\"\n\n# Code generation\nujust ollama run -m codellama --prompt=\"Write a Python function to sort a list\"\n```\n\n## API Access\n\n### Default Endpoint\n\n```\nhttp://localhost:11434\n```\n\n### API Examples\n\n```bash\n# Generate completion\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Hello, how are you?\"\n}'\n\n# Chat\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n}'\n\n# List models\ncurl http://localhost:11434/api/tags\n```\n\n### Integration with Tools\n\n```bash\n# Claude Code with Ollama\nexport OLLAMA_HOST=http://localhost:11434\n\n# LangChain\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"llama3.2\", base_url=\"http://localhost:11434\")\n```\n\n## Volume Mounts\n\n| Container Path | Host Path | Purpose |\n|----------------|-----------|---------|\n| `/root/.ollama` | `~/.ollama` | Model storage |\n\nModels are persisted in `~/.ollama` and survive container restarts.\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure Ollama with GPU\nujust ollama config --gpu-type=nvidia\n\n# 2. Start the server\nujust ollama start\n\n# 3. Pull a model\nujust ollama pull -m llama3.2\n\n# 4. Test it\nujust ollama run -m llama3.2 --prompt=\"Hello!\"\n```\n\n### Development with Local LLM\n\n```bash\n# Start Ollama\nujust ollama start\n\n# In your code, use:\n# OLLAMA_HOST=http://localhost:11434\n```\n\n### Model Comparison\n\n```bash\n# Pull multiple models\nujust ollama pull -m llama3.2\nujust ollama pull -m mistral\nujust ollama pull -m phi3\n\n# Compare responses\nujust ollama run -m llama3.2 --prompt=\"Explain REST APIs\"\nujust ollama run -m mistral --prompt=\"Explain REST APIs\"\nujust ollama run -m phi3 --prompt=\"Explain REST APIs\"\n```\n\n## GPU Support\n\n### Automatic Detection\n\n```bash\nujust ollama config  # Auto-detects GPU\n```\n\n### Manual Selection\n\n| GPU Type | Flag Value | VRAM Usage |\n|----------|------------|------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | Full GPU acceleration |\n| AMD | `--gpu-type=amd` or `-g amd` | ROCm acceleration |\n| Intel | `--gpu-type=intel` or `-g intel` | oneAPI acceleration |\n| None | `--gpu-type=none` or `-g none` | CPU only (slower) |\n\n### Check GPU Status\n\n```bash\nujust ollama shell -- nvidia-smi  # NVIDIA\nujust ollama shell -- rocm-smi    # AMD\n```\n\n## Model Size Guide\n\n| Model | Parameters | VRAM Needed | Quality |\n|-------|------------|-------------|---------|\n| phi3 | 3B | 4GB | Fast, basic |\n| llama3.2 | 8B | 8GB | Balanced |\n| mistral | 7B | 8GB | Good coding |\n| codellama | 7B | 8GB | Code-focused |\n| llama3.2:70b | 70B | 48GB+ | Best quality |\n\n## Troubleshooting\n\n### Server Won't Start\n\n**Check:**\n\n```bash\nsystemctl --user status ollama\nujust ollama logs --lines=50\n```\n\n**Common causes:**\n\n- Port 11434 already in use\n- GPU driver issues\n- Image not pulled\n\n### Model Loading Fails\n\n**Symptom:** \"out of memory\" or slow loading\n\n**Cause:** Model too large for GPU VRAM\n\n**Fix:**\n\n```bash\n# Use smaller model\nujust ollama pull -m phi3  # Only 4GB VRAM\n\n# Or use quantized version\nujust ollama pull -m llama3.2:7b-q4_0\n```\n\n### GPU Not Used\n\n**Symptom:** Inference very slow\n\n**Check:**\n\n```bash\nujust ollama status\nujust ollama shell -- nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit GPU\nujust ollama delete\nujust ollama config --gpu-type=nvidia\n```\n\n### API Not Responding\n\n**Symptom:** `curl localhost:11434` fails\n\n**Check:**\n\n```bash\nujust ollama status\nujust ollama logs\n```\n\n**Fix:**\n\n```bash\nujust ollama restart\n```\n\n## Cross-References\n\n- **Related Skills:** `configure gpu` (GPU setup), `jupyter` (ML development)\n- **API Docs:** [https://ollama.ai/docs](https://ollama.ai/docs)\n- **Model Library:** [https://ollama.ai/library](https://ollama.ai/library)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install ollama\", \"setup local LLM\", \"run LLM locally\"\n- \"pull model\", \"download llama\", \"get mistral\"\n- \"ollama not working\", \"model won't load\"\n- \"ollama GPU\", \"ollama cuda\", \"ollama slow\"\n- \"ollama API\", \"integrate with ollama\""
              },
              {
                "name": "openwebui",
                "description": "Open WebUI AI chat interface management via Podman Quadlet. Provides a web UI\nfor interacting with Ollama models. Use when users need to configure, start,\nor manage the Open WebUI service.\n",
                "path": "bazzite-ai/skills/openwebui/SKILL.md",
                "frontmatter": {
                  "name": "openwebui",
                  "description": "Open WebUI AI chat interface management via Podman Quadlet. Provides a web UI\nfor interacting with Ollama models. Use when users need to configure, start,\nor manage the Open WebUI service.\n"
                },
                "content": "# Open WebUI - AI Chat Interface\n\n## Overview\n\nThe `openwebui` command manages the Open WebUI service using Podman Quadlet containers. It provides a web-based chat interface for interacting with Ollama LLM models.\n\n**Key Concept:** Open WebUI connects to Ollama via the `bazzite-ai` network using DNS (`http://ollama:11434`). Ensure Ollama is running before using Open WebUI.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust openwebui config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust openwebui start [--instance=...]` | Start service |\n| Stop | `ujust openwebui stop [--instance=...]` | Stop service |\n| Restart | `ujust openwebui restart [--instance=...]` | Restart service |\n| Logs | `ujust openwebui logs [--lines=...]` | View logs |\n| Status | `ujust openwebui status [--instance=...]` | Show status |\n| URL | `ujust openwebui url [--instance=...]` | Show access URL |\n| List | `ujust openwebui list` | List instances |\n| Shell | `ujust openwebui shell [-- CMD...]` | Container shell |\n| Delete | `ujust openwebui delete [--instance=...]` | Remove service |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `3000` | Host port for web UI |\n| Image | `--image` | `-i` | `ghcr.io/open-webui/open-webui:main` | Container image |\n| Tag | `--tag` | `-t` | `main` | Image tag |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Config Dir | `--config-dir` | `-c` | `~/.config/openwebui/1` | Config/data directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Workspace mount |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type |\n| Instance | `--instance` | `-n` | `1` | Instance number or `all` |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n## Configuration\n\n```bash\n# Default configuration (port 3000, localhost only)\nujust openwebui config\n\n# Custom port (long form)\nujust openwebui config --port=3001\n\n# Custom port (short form)\nujust openwebui config -p 3001\n\n# Network-wide access\nujust openwebui config --bind=0.0.0.0\n\n# Combine parameters (long form)\nujust openwebui config --port=3001 --bind=0.0.0.0\n\n# Combine parameters (short form)\nujust openwebui config -p 3001 -b 0.0.0.0\n\n# GPU-optimized image\nujust openwebui config --image=ghcr.io/open-webui/open-webui:cuda\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured updates the existing settings:\n\n```bash\n# Change only the bind address\nujust openwebui config --bind=0.0.0.0\n\n# Update port without affecting other settings\nujust openwebui config --port=3002\n```\n\n## Container Images\n\n| Image | Description |\n|-------|-------------|\n| `ghcr.io/open-webui/open-webui:main` | Standard image (default) |\n| `ghcr.io/open-webui/open-webui:cuda` | NVIDIA CUDA optimized |\n| `ghcr.io/open-webui/open-webui:ollama` | Bundled with Ollama (not recommended) |\n\n**Note:** GPU is auto-detected and attached regardless of image choice.\n\n## Lifecycle Management\n\n```bash\n# Start Open WebUI\nujust openwebui start\n\n# Stop service\nujust openwebui stop\n\n# Restart (apply config changes)\nujust openwebui restart\n\n# View logs (default 50 lines)\nujust openwebui logs\n\n# View more logs (long form)\nujust openwebui logs --lines=200\n\n# View more logs (short form)\nujust openwebui logs -l 200\n\n# Check status\nujust openwebui status\n\n# Show access URL\nujust openwebui url\n```\n\n## Multi-Instance Support\n\n```bash\n# Start all instances (long form)\nujust openwebui start --instance=all\n\n# Start all instances (short form)\nujust openwebui start -n all\n\n# Stop specific instance\nujust openwebui stop --instance=2\n\n# Delete all instances\nujust openwebui delete --instance=all\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust openwebui shell\n\n# Run specific command (use -- separator)\nujust openwebui shell -- ls -la /app/backend/data\nujust openwebui shell -- cat /app/backend/data/config.json\n```\n\n## Network Architecture\n\nOpen WebUI uses the `bazzite-ai` bridge network for cross-container DNS:\n\n```\n+-------------------+     DNS      +-------------------+\n|   Open WebUI      | -----------> |      Ollama       |\n|   (openwebui)     |              |    (ollama)       |\n|   Port 3000       |              |   Port 11434      |\n+-------------------+              +-------------------+\n         |                                  |\n         +------ bazzite-ai network --------+\n```\n\n**Environment Variables (injected automatically):**\n\n```\nOLLAMA_BASE_URL=http://ollama:11434\nOLLAMA_HOST=http://ollama:11434\nJUPYTER_HOST=http://jupyter:8888\nCOMFYUI_HOST=http://comfyui:8188\n```\n\n## Network Binding\n\n| Bind Address | Access | Use Case |\n|--------------|--------|----------|\n| `127.0.0.1` | Localhost only | Default, secure |\n| `0.0.0.0` | All interfaces | Network access, Tailscale |\n\n**Security Note:** Using `--bind=0.0.0.0` exposes the service to your network. Consider using Tailscale for secure remote access:\n\n```bash\n# Expose via Tailscale (secure)\nujust tailscale serve --service=openwebui\n```\n\n## Data Persistence\n\n| Path | Description |\n|------|-------------|\n| `~/.config/openwebui/<INSTANCE>/data` | Users, chats, settings |\n\nData persists across container restarts. Each instance has isolated data.\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Ensure Ollama is running\nujust ollama start\n\n# 2. Configure Open WebUI\nujust openwebui config\n\n# 3. Start the service\nujust openwebui start\n\n# 4. Access the web UI\nujust openwebui url\n# Output: http://127.0.0.1:3000\n```\n\n### Remote Access Setup\n\n```bash\n# Configure for network access\nujust openwebui config --bind=0.0.0.0\n\n# Start the service\nujust openwebui start\n\n# Or use Tailscale for secure access\nujust tailscale serve --service=openwebui\n```\n\n### Upgrade Container Image\n\n```bash\n# Stop service\nujust openwebui stop\n\n# Update to new image\nujust openwebui config --image=ghcr.io/open-webui/open-webui:main\n\n# Restart\nujust openwebui start\n```\n\n## GPU Support\n\nGPU is automatically detected and attached:\n\n| GPU Type | Detection | Quadlet Config |\n|----------|-----------|----------------|\n| NVIDIA | `nvidia-smi` | `AddDevice=nvidia.com/gpu=all` |\n| AMD | lspci | `AddDevice=/dev/dri` |\n| Intel | lspci | `AddDevice=/dev/dri` |\n\nCheck GPU status:\n\n```bash\nujust openwebui shell -- nvidia-smi\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n```bash\n# Check status\nujust openwebui status\n\n# View logs\nujust openwebui logs --lines=100\n\n# Check if Ollama is running\nujust ollama status\n```\n\n**Common causes:**\n\n- Port 3000 already in use\n- Ollama not running\n- Container image not pulled\n\n### Can't Connect to Ollama\n\n**Symptom:** \"No models available\" in web UI\n\n**Check:**\n\n```bash\n# Verify Ollama is running\nujust ollama status\n\n# Test Ollama connection from Open WebUI container\nujust openwebui shell -- curl http://ollama:11434/api/tags\n```\n\n**Fix:**\n\n```bash\n# Start Ollama first\nujust ollama start\n\n# Restart Open WebUI\nujust openwebui restart\n```\n\n### Web UI Not Accessible\n\n**Symptom:** Browser can't connect to `http://localhost:3000`\n\n**Check:**\n\n```bash\nujust openwebui status\nujust openwebui url\n```\n\n**Fix:**\n\n```bash\n# If using wrong bind address\nujust openwebui config --bind=127.0.0.1\nujust openwebui restart\n```\n\n### Clear Data and Start Fresh\n\n```bash\n# Delete everything\nujust openwebui delete --instance=all\n\n# Reconfigure\nujust openwebui config\nujust openwebui start\n```\n\n## Cross-References\n\n- **Required:** `ollama` (Ollama must be running for models)\n- **Related:** `jupyter` (ML development), `comfyui` (image generation)\n- **Network:** Uses `bazzite-ai` network (shared with ollama, jupyter, comfyui)\n- **Docs:** [Open WebUI GitHub](https://github.com/open-webui/open-webui)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install open webui\", \"setup chat interface\", \"web ui for ollama\"\n- \"configure openwebui\", \"change port\", \"network access\"\n- \"open webui not working\", \"can't see models\", \"connection error\"\n- \"open webui logs\", \"debug open webui\"\n- \"delete open webui\", \"uninstall\""
              },
              {
                "name": "pods",
                "description": "Aggregate management for all AI pod services. Provides status overview\nand bulk operations across all pod containers (ollama, jupyter, comfyui,\nopenwebui, localai, fiftyone, jellyfin, runners).\n",
                "path": "bazzite-ai/skills/pods/SKILL.md",
                "frontmatter": {
                  "name": "pods",
                  "description": "Aggregate management for all AI pod services. Provides status overview\nand bulk operations across all pod containers (ollama, jupyter, comfyui,\nopenwebui, localai, fiftyone, jellyfin, runners).\n"
                },
                "content": "# Pods - Aggregate Pod Management\n\n## Overview\n\nThe `pods` command provides aggregate management for all AI pod services. It shows combined status and enables bulk operations across all running pod containers.\n\n**Key Concept:** This is a meta-command for managing multiple pods at once. For individual pod management, use the specific service command (e.g., `ujust ollama`, `ujust jupyter`).\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Status | `ujust pods status` | Show status of all pods |\n| Purge | `ujust pods purge` | Remove all pod containers |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust pods ACTION=\"\"\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | `status`, `purge` | Action to perform |\n\nWithout `ACTION`, shows interactive menu (requires TTY).\n\n## Commands\n\n### Status\n\n```bash\nujust pods status\n```\n\nShows status of all pod services:\n\n- Container running state\n- Port bindings\n- GPU attachment\n- Resource usage\n\n**Output includes:**\n\n- Ollama\n- Jupyter\n- ComfyUI\n- Open WebUI\n- LocalAI\n- FiftyOne\n- Jellyfin\n- GitHub Runners\n\n### Purge\n\n```bash\nujust pods purge\n```\n\nRemoves all pod containers and their configurations:\n\n1. Stops all running pods\n2. Removes all pod containers\n3. Cleans up Quadlet configs\n4. Reloads systemd\n\n**Warning:** This removes ALL pod containers. Data in workspace directories is preserved.\n\n## Common Workflows\n\n### Check All Services\n\n```bash\n# Quick status overview\nujust pods status\n```\n\n### Clean Restart\n\n```bash\n# Remove all pods\nujust pods purge\n\n# Reconfigure and start individual services\nujust ollama config\nujust ollama start\n```\n\n### Before System Update\n\n```bash\n# Check what's running\nujust pods status\n\n# Stop all if needed\nujust pods purge\n```\n\n## Non-Interactive Usage\n\nAll commands work without TTY:\n\n```bash\n# CI/automation-friendly\nujust pods status\nujust pods purge\n```\n\n## Troubleshooting\n\n### Status Shows Stale Containers\n\n**Symptom:** Status shows containers that don't exist\n\n**Cause:** Quadlet configs out of sync\n\n**Fix:**\n\n```bash\nsystemctl --user daemon-reload\nujust pods status\n```\n\n### Purge Doesn't Remove All\n\n**Symptom:** Some containers remain after purge\n\n**Cause:** Containers created outside Quadlet\n\n**Fix:**\n\n```bash\n# Manual cleanup\npodman ps -a\npodman rm -f <container-id>\n```\n\n## Cross-References\n\n- **Individual Services:** `ollama`, `jupyter`, `comfyui`, `openwebui`, `localai`, `fiftyone`, `jellyfin`, `runners` skills\n- **Testing:** `test pods` for lifecycle testing\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"all pods status\", \"check all services\"\n- \"remove all containers\", \"clean up pods\"\n- \"what's running\", \"show all services\"\n- \"purge containers\", \"reset pods\""
              },
              {
                "name": "runners",
                "description": "Self-hosted GitHub Actions runner management via Podman Quadlet. Supports\nmulti-instance pools with ephemeral storage, automatic token generation,\nand rolling updates. Use when users need to set up CI/CD runners for\ntheir GitHub repositories.\n",
                "path": "bazzite-ai/skills/runners/SKILL.md",
                "frontmatter": {
                  "name": "runners",
                  "description": "Self-hosted GitHub Actions runner management via Podman Quadlet. Supports\nmulti-instance pools with ephemeral storage, automatic token generation,\nand rolling updates. Use when users need to set up CI/CD runners for\ntheir GitHub repositories.\n"
                },
                "content": "# Runners - GitHub Actions Self-Hosted Runners\n\n## Overview\n\nThe `runners` command manages self-hosted GitHub Actions runners using Podman Quadlet containers. It supports multi-instance pools with ephemeral storage for clean builds.\n\n**Key Concept:** Each runner instance connects to a GitHub repository and picks up workflow jobs. Ephemeral storage ensures each job starts with a clean state.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust runners config --repo-url=<URL> --instance=<N>` | Configure runner N for repo |\n| Start | `ujust runners start [--instance=N\\|all]` | Start runner(s) |\n| Stop | `ujust runners stop [--instance=N\\|all]` | Stop runner(s) |\n| Restart | `ujust runners restart [--instance=N\\|all]` | Restart runner(s) |\n| Update | `ujust runners update [--instance=N\\|all]` | Update to latest image |\n| Rolling update | `ujust runners rolling-update` | Update with zero downtime |\n| Sync | `ujust runners sync [--instance=N]` | Sync config from source |\n| Logs | `ujust runners logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust runners list` | List all runners |\n| Shell | `ujust runners shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Delete | `ujust runners delete [--instance=N\\|all]` | Remove runner(s) and images |\n\n## Prerequisites\n\n```bash\n# 1. Authenticate GitHub CLI\ngh auth login\n\n# 2. Verify authentication\ngh auth status\n```\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Required | Description |\n|-----------|-----------|-------|----------|-------------|\n| Repo URL | `--repo-url` | `-r` | Yes | GitHub repository URL |\n| Instance | `--instance` | `-n` | Yes | Instance number (1, 2, 3...) |\n| Image | `--image` | `-i` | No | Container image |\n| Tag | `--tag` | `-t` | No | Image tag (default: stable) |\n| Workspace | `--workspace-dir` | `-w` | No | Optional mount to /workspace |\n| Lines | `--lines` | `-l` | No | Log lines to show |\n\n### Configuration Examples\n\n```bash\n# Basic runner (long form)\nujust runners config --repo-url=https://github.com/owner/repo --instance=1\n\n# Basic runner (short form)\nujust runners config -r https://github.com/owner/repo -n 1\n\n# Runner with testing tag\nujust runners config -r https://github.com/owner/repo -n 1 --tag=testing\n\n# Runner with workspace mount\nujust runners config -r https://github.com/owner/repo -n 1 --workspace-dir=/home/user\n```\n\n### Install Multiple Runners\n\n```bash\n# Runner pool for a repository\nujust runners config -r https://github.com/owner/repo -n 1\nujust runners config -r https://github.com/owner/repo -n 2\nujust runners config -r https://github.com/owner/repo -n 3\n\n# Start all\nujust runners start --instance=all\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust runners shell\n\n# Run specific command (use -- separator)\nujust runners shell -- df -h\n\n# Shell in specific instance\nujust runners shell --instance=2 -- cat /config/runner.env\n```\n\n## Lifecycle Commands\n\n### Start/Stop\n\n```bash\n# Single runner\nujust runners start --instance=1\nujust runners stop --instance=1\n\n# Short form\nujust runners start -n 1\nujust runners stop -n 1\n\n# All runners\nujust runners start --instance=all\nujust runners stop --instance=all\n```\n\n### Updates\n\n```bash\n# Fast update (stops runner briefly)\nujust runners update --instance=1\n\n# Rolling update (zero-downtime)\nujust runners rolling-update\n```\n\nRolling update:\n\n1. Stops runner 1\n2. Updates runner 1\n3. Starts runner 1\n4. Waits for healthy state\n5. Repeats for runner 2, 3, ...\n\n### View Logs\n\n```bash\n# Follow logs\nujust runners logs\n\n# Specific instance with line count\nujust runners logs --instance=1 --lines=100\n\n# Short form\nujust runners logs -n 1 -l 100\n```\n\n## Token Management\n\nTokens are **automatically generated** via GitHub API - no manual copying required!\n\n### How It Works\n\n1. Config command calls GitHub API\n2. Generates registration token\n3. Configures runner with token\n4. Token auto-refreshes on restart\n\n### Requirements\n\n- GitHub CLI authenticated (`gh auth login`)\n- Admin access to repository\n\n## Architecture\n\n### Ephemeral Storage\n\nEach runner has ephemeral storage:\n\n- Clean state on every restart\n- No stale artifacts between jobs\n- Prevents cache bloat\n\n### Host Image Cache\n\nRunners access host container cache (read-only):\n\n- Fast container image pulls\n- Shared cache across runners\n- No duplicate downloads\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/github-runner-1.container` |\n| Runner config | Per-runner settings | `~/.config/github-runner/runner-1.env` |\n\n## Common Workflows\n\n### Setup CI for Repository\n\n```bash\n# 1. Authenticate GitHub\ngh auth login\n\n# 2. Configure runner\nujust runners config -r https://github.com/myorg/myrepo -n 1\n\n# 3. Start runner\nujust runners start\n\n# 4. Verify in GitHub\n# Settings  Actions  Runners\n```\n\n### Scale Up Runner Pool\n\n```bash\n# Add more runners\nujust runners config -r https://github.com/myorg/myrepo -n 2\nujust runners config -r https://github.com/myorg/myrepo -n 3\n\n# Start all\nujust runners start --instance=all\n\n# List pool\nujust runners list\n```\n\n### Update All Runners\n\n```bash\n# Option 1: Fast update (brief downtime)\nujust runners stop --instance=all\nujust runners update --instance=all\nujust runners start --instance=all\n\n# Option 2: Rolling update (zero downtime)\nujust runners rolling-update\n```\n\n### Clean Reinstall\n\n```bash\n# Delete runner\nujust runners delete --instance=1\n\n# Reconfigure\nujust runners config -r https://github.com/myorg/myrepo -n 1\nujust runners start\n```\n\n## Workflow Labels\n\nRunners automatically get these labels:\n\n- `self-hosted`\n- `linux`\n- `x64`\n- `bazzite-ai`\n\nUse in workflow:\n\n```yaml\nruns-on: [self-hosted, bazzite-ai]\n```\n\n## Troubleshooting\n\n### Runner Not Appearing in GitHub\n\n**Check:**\n\n```bash\nujust runners status\nujust runners logs --lines=50\n```\n\n**Common causes:**\n\n- GitHub CLI not authenticated\n- Token generation failed\n- Network issues\n\n**Fix:**\n\n```bash\n# Re-authenticate\ngh auth login\n\n# Reconfigure runner\nujust runners delete --instance=1\nujust runners config -r https://github.com/owner/repo -n 1\n```\n\n### Jobs Not Running\n\n**Symptom:** Runner shows \"Idle\" but jobs queue\n\n**Check:**\n\n```bash\nujust runners logs\n```\n\n**Common causes:**\n\n- Labels don't match workflow\n- Runner offline\n- Repository permissions\n\n### Runner Keeps Restarting\n\n**Check:**\n\n```bash\nsystemctl --user status github-runner-1\nujust runners logs --lines=100\n```\n\n**Common causes:**\n\n- Token expired (auto-fixes on restart)\n- Image issues\n- Resource exhaustion\n\n## Cross-References\n\n- **Related Skills:** `pod` (build images)\n- **GitHub Docs:** Actions  Self-hosted runners\n- **Authentication:** `gh auth login`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"setup github runner\", \"self-hosted runner\", \"CI runner\"\n- \"install runner\", \"add runner\", \"more runners\"\n- \"runner not working\", \"runner offline\"\n- \"update runner\", \"rolling update\"\n- \"runner logs\", \"runner status\""
              },
              {
                "name": "tailscale",
                "description": "Tailscale Serve management for exposing local services to your tailnet.\nAuto-detects running bazzite-ai services and creates persistent HTTPS\nendpoints. Use when users need to expose Jupyter, Ollama, ComfyUI or\nother services to their Tailscale network.\n",
                "path": "bazzite-ai/skills/tailscale/SKILL.md",
                "frontmatter": {
                  "name": "tailscale",
                  "description": "Tailscale Serve management for exposing local services to your tailnet.\nAuto-detects running bazzite-ai services and creates persistent HTTPS\nendpoints. Use when users need to expose Jupyter, Ollama, ComfyUI or\nother services to their Tailscale network.\n"
                },
                "content": "# Tailscale - Service Exposure via Tailnet\n\n## Overview\n\nThe `tailscale` command manages Tailscale Serve to expose local bazzite-ai services to your tailnet. It provides HTTPS endpoints with auto-provisioned certificates.\n\n**Key Concept:** Tailscale Serve exposes local services only to your tailnet (not the public internet). HTTPS certificates are automatically provisioned and managed by Tailscale.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Serve | `ujust tailscale serve [--service=...] [--port=...]` | Expose service to tailnet |\n| Unserve | `ujust tailscale unserve [--service=...\\|all]` | Stop exposing service |\n| Status | `ujust tailscale status` | Show current serve configuration |\n| List | `ujust tailscale list` | List available services |\n| Help | `ujust tailscale help` | Show help |\n\n## Prerequisites\n\n```bash\n# Tailscale must be installed and logged in\nsudo dnf install tailscale\nsudo systemctl enable --now tailscaled\ntailscale up\n```\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: serve, unserve, status, list |\n| service | `--service` | `-s` | `\"\"` | Service name or port number |\n| port | `--port` | `-p` | `\"\"` | Tailscale HTTPS port to expose on |\n\n## Known Services\n\n| Service | Default Port | Description |\n|---------|--------------|-------------|\n| `jupyter` | 8888 | JupyterLab notebooks |\n| `ollama` | 11434 | Ollama LLM API |\n| `comfyui` | 8188 | ComfyUI Stable Diffusion |\n| `openwebui` | 3000 | Open WebUI chat interface |\n| `fiftyone` | 5151 | FiftyOne dataset visualization |\n\n## Serve Commands\n\n### Serve by Service Name\n\n```bash\n# Auto-detect port for known services (long form)\nujust tailscale serve --service=jupyter\n\n# Auto-detect port for known services (short form)\nujust tailscale serve -s jupyter\n\n# Serve ollama\nujust tailscale serve -s ollama\n\n# Serve comfyui\nujust tailscale serve -s comfyui\n\n# Serve openwebui\nujust tailscale serve -s openwebui\n```\n\n### Serve by Port Number\n\n```bash\n# Serve arbitrary port (long form)\nujust tailscale serve --service=8080\n\n# Serve arbitrary port (short form)\nujust tailscale serve -s 8080\n\n# Or just the port\nujust tailscale serve 8080\n```\n\n### Serve with Custom Tailscale Port\n\n```bash\n# Expose jupyter on tailscale port 443 (long form)\nujust tailscale serve --service=jupyter --port=443\n\n# Expose jupyter on tailscale port 443 (short form)\nujust tailscale serve -s jupyter -p 443\n\n# Expose on multiple tailscale ports\nujust tailscale serve -s jupyter -p 8888\nujust tailscale serve -s ollama -p 11434\n```\n\n## Unserve Commands\n\n```bash\n# Stop serving a specific service (long form)\nujust tailscale unserve --service=jupyter\n\n# Stop serving a specific service (short form)\nujust tailscale unserve -s jupyter\n\n# Stop serving by port\nujust tailscale unserve -s 8888\n\n# Stop all serves\nujust tailscale unserve all\n```\n\n## Status Commands\n\n```bash\n# Show current serve configuration\nujust tailscale status\n\n# List available bazzite-ai services\nujust tailscale list\n```\n\n## Common Workflows\n\n### Expose JupyterLab\n\n```bash\n# 1. Ensure Jupyter is running\nujust jupyter start\n\n# 2. Expose to tailnet\nujust tailscale serve -s jupyter\n\n# 3. Access from any tailnet device\n# https://<hostname>.<tailnet-name>.ts.net:8888\n```\n\n### Expose Multiple Services\n\n```bash\n# Start services\nujust jupyter start\nujust ollama start\nujust comfyui start\n\n# Expose all\nujust tailscale serve -s jupyter\nujust tailscale serve -s ollama\nujust tailscale serve -s comfyui\n\n# Check status\nujust tailscale status\n```\n\n### Remote AI Development\n\n```bash\n# On your server\nujust jupyter config --bind=127.0.0.1  # Only localhost\nujust jupyter start\nujust tailscale serve -s jupyter\n\n# On your laptop (connected to same tailnet)\n# Access: https://<server>.<tailnet>.ts.net:8888\n```\n\n### Clean Up All Serves\n\n```bash\n# Stop all tailscale serves\nujust tailscale unserve all\n\n# Verify\nujust tailscale status\n```\n\n## Features\n\n### Auto-Detection\n\nWhen you serve a known service, the command auto-detects:\n\n- Whether the service is running\n- The correct local port\n- Appropriate HTTPS configuration\n\n### Persistent Serves\n\nServes persist across reboots. Tailscale remembers your configuration.\n\n### HTTPS Certificates\n\nTailscale automatically:\n\n- Provisions certificates\n- Handles renewals\n- Terminates TLS at edge\n\n### Tailnet-Only\n\nUnlike Tailscale Funnel, Serve only exposes to your tailnet:\n\n- No public internet exposure\n- Access limited to your devices\n- Requires Tailscale authentication\n\n## Troubleshooting\n\n### Service Not Found\n\n**Symptom:** \"Service not found\" error\n\n**Check:**\n\n```bash\n# Verify service is running\nujust jupyter status\nsystemctl --user status jupyter-1\n```\n\n**Fix:**\n\n```bash\n# Start the service first\nujust jupyter start\n# Then serve\nujust tailscale serve -s jupyter\n```\n\n### Tailscale Not Running\n\n**Symptom:** \"Tailscale not running or not logged in\"\n\n**Fix:**\n\n```bash\n# Start tailscaled\nsudo systemctl start tailscaled\n\n# Login to Tailscale\ntailscale up\n```\n\n### Cannot Access from Other Device\n\n**Check:**\n\n```bash\n# Verify serve is active\nujust tailscale status\n\n# Check Tailscale connection\ntailscale status\n```\n\n**Common causes:**\n\n- Not on same tailnet\n- Firewall blocking\n- Service not bound to localhost\n\n**Fix:**\n\n```bash\n# Ensure service binds to localhost (required for Serve)\nujust jupyter config --bind=127.0.0.1\nujust jupyter restart\nujust tailscale serve -s jupyter\n```\n\n### Port Conflict\n\n**Symptom:** \"Port already in use on tailscale\"\n\n**Fix:**\n\n```bash\n# Use different tailscale port\nujust tailscale serve -s jupyter -p 8889\n```\n\n## Security Considerations\n\n**Tailscale Serve is secure by design:**\n\n- Only accessible from your tailnet\n- Requires Tailscale authentication\n- Uses WireGuard encryption\n- HTTPS with auto-managed certificates\n\n**Best practices:**\n\n1. Keep services bound to localhost (`127.0.0.1`)\n2. Use strong Tailscale ACLs\n3. Review serves periodically with `status`\n4. Remove unused serves with `unserve`\n\n## Cross-References\n\n- **Related Skills:** `jupyter`, `ollama`, `comfyui`, `openwebui`\n- **Prerequisites:** `ujust config tailscale enable`\n- **Tailscale Docs:** <https://tailscale.com/kb/1242/tailscale-serve>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"expose to tailnet\", \"tailscale serve\"\n- \"access jupyter remotely\", \"remote access\"\n- \"share service with tailscale\"\n- \"tailscale status\", \"stop tailscale serve\""
              },
              {
                "name": "test",
                "description": "Runtime verification tests for bazzite-ai installation. Tests GPU detection,\nCUDA, PyTorch, service health, network connectivity, and pod lifecycles.\nUse when users need to verify their bazzite-ai installation works correctly.\n",
                "path": "bazzite-ai/skills/test/SKILL.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Runtime verification tests for bazzite-ai installation. Tests GPU detection,\nCUDA, PyTorch, service health, network connectivity, and pod lifecycles.\nUse when users need to verify their bazzite-ai installation works correctly.\n"
                },
                "content": "# Test - Runtime Verification\n\n## Overview\n\nThe `test` command provides comprehensive runtime verification for bazzite-ai installations. It tests GPU detection, CUDA/PyTorch functionality, service health, network connectivity, and pod container lifecycles.\n\n**Key Concept:** Tests run on the LOCAL system to verify actual functionality, not just syntax.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Quick test | `ujust test quick` | GPU + service status (~30s) |\n| GPU test | `ujust test gpu` | GPU detection and CDI check |\n| CUDA test | `ujust test cuda` | CUDA tests in nvidia container |\n| PyTorch test | `ujust test pytorch` | PyTorch GPU access test |\n| All tests | `ujust test all` | Full test suite (~2min) |\n| Help | `ujust test help` | Show all options |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust test ACTION=\"\"\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | See actions below | Test to run |\n\nWithout `ACTION`, shows interactive menu (requires TTY).\n\n### Test Options\n\n```bash\nujust test [ACTION] [--instance=N] [--image=IMAGE] [--cpus=N] [--ram=MB] [--ssh-port=PORT]\n```\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `--instance, -n` | `90` | Pod instance number for test pods |\n| `--image, -i` | (default) | Container image for bootc testing |\n| `--cpus` | `4` | CPUs for bootc VM |\n| `--ram` | `8192` | RAM in MB for bootc VM |\n| `--ssh-port` | `2222` | SSH port for bootc VM |\n\n## Available Tests\n\n### Quick Tests\n\n```bash\nujust test quick      # GPU + service status (~30s)\nujust test status     # Test status summary\n```\n\n### GPU Tests\n\n```bash\nujust test gpu        # GPU detection and CDI check\nujust test cuda       # CUDA tests in nvidia container\nujust test pytorch    # PyTorch GPU access test\n```\n\n### Service Tests\n\n```bash\nujust test ollama     # Ollama health + quick inference\nujust test jupyter    # Jupyter service health\nujust test comfyui    # ComfyUI service health\nujust test openwebui  # Open WebUI service health\nujust test services   # All installed services status\n```\n\n### Infrastructure Tests\n\n```bash\nujust test config     # Configuration dispatcher test\nujust test network    # Registry connectivity test\nujust test apptainer  # Apptainer GPU detection\n```\n\n### VM Tests\n\n```bash\nujust test bootc                    # Ephemeral bootc VM (auto-cleanup)\nujust test bootc --image=stable     # Test specific image\n```\n\n### Pod Lifecycle Tests\n\n```bash\nujust test pods config --instance=91   # Configure test pods\nujust test pods start --instance=91    # Start test pods\nujust test pods status --instance=91   # Check test pods status\nujust test pods stop --instance=91     # Stop test pods\nujust test pods delete --instance=91   # Delete test pod configs\nujust test pods all --instance=91      # Full lifecycle test\n```\n\n## Common Workflows\n\n### Verify New Installation\n\n```bash\n# Quick verification\nujust test quick\n\n# If issues found, run full suite\nujust test all\n```\n\n### Verify GPU Support\n\n```bash\n# Check GPU detection\nujust test gpu\n\n# Test CUDA if NVIDIA\nujust test cuda\n\n# Test PyTorch GPU access\nujust test pytorch\n```\n\n### Verify Services\n\n```bash\n# Test all services\nujust test services\n\n# Or individual services\nujust test ollama\nujust test jupyter\n```\n\n### Test Pod Lifecycle\n\n```bash\n# Full lifecycle test with isolated instance\nujust test pods all --instance=91\n```\n\n## Non-Interactive Usage\n\nAll tests work without TTY:\n\n```bash\n# CI/automation-friendly\nujust test quick\nujust test gpu\nujust test all\n```\n\n## Troubleshooting\n\n### GPU Not Detected\n\n**Symptom:** `ujust test gpu` shows no GPU\n\n**Cause:** GPU drivers not loaded or CDI not configured\n\n**Fix:**\n\n```bash\n# Check NVIDIA driver\nnvidia-smi\n\n# Check CDI\nls /etc/cdi/\n\n# Setup GPU container support\nujust config gpu setup\n```\n\n### CUDA Test Fails\n\n**Symptom:** `ujust test cuda` fails\n\n**Cause:** NVIDIA container toolkit not configured\n\n**Fix:**\n\n```bash\nujust config gpu setup\n# May require reboot\n```\n\n### Service Test Fails\n\n**Symptom:** Service test shows unhealthy\n\n**Cause:** Service not running or misconfigured\n\n**Fix:**\n\n```bash\n# Check specific service\nujust <service> status\n\n# View logs\nujust <service> logs\n\n# Restart service\nujust <service> restart\n```\n\n## Cross-References\n\n- **GPU Setup:** `config` skill (ujust config gpu setup)\n- **Service Management:** Individual service skills (ollama, jupyter, etc.)\n- **Pod Management:** `pods` skill for aggregate operations\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"verify installation\", \"test bazzite-ai\", \"check if working\"\n- \"GPU test\", \"CUDA test\", \"PyTorch test\"\n- \"service health\", \"check services\"\n- \"network connectivity\", \"registry access\"\n- \"test pods\", \"lifecycle test\""
              },
              {
                "name": "vm",
                "description": "QCOW2 virtual machine management using libvirt. Creates VMs from pre-built\nimages downloaded from R2 CDN with cloud-init customization. Supports SSH,\nVNC, and virtiofs home directory sharing. Use when users need to create,\nmanage, or connect to bazzite-ai VMs.\n",
                "path": "bazzite-ai/skills/vm/SKILL.md",
                "frontmatter": {
                  "name": "vm",
                  "description": "QCOW2 virtual machine management using libvirt. Creates VMs from pre-built\nimages downloaded from R2 CDN with cloud-init customization. Supports SSH,\nVNC, and virtiofs home directory sharing. Use when users need to create,\nmanage, or connect to bazzite-ai VMs.\n"
                },
                "content": "# VM - QCOW2 Virtual Machine Management\n\n## Overview\n\nThe `vm` command manages bazzite-ai virtual machines using libvirt. VMs are created from pre-built QCOW2 images downloaded from R2 CDN, customized via cloud-init.\n\n**Key Concept:** VMs run in user session (qemu:///session), not requiring root. Home directory is shared via virtiofs at `/workspace` in the VM.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Add VM | `ujust vm add [NAME] [--cpus=...] [--ram=...]` | Download + create VM |\n| Update VM | `ujust vm update [NAME] [--what=...]` | Update VM config |\n| Delete VM | `ujust vm delete [NAME]` | Remove VM |\n| Download | `ujust vm download [--branch=...]` | Download QCOW2 image |\n| Seed | `ujust vm seed [NAME] [--username=...]` | Create cloud-init ISO |\n| Create | `ujust vm create [NAME] [--cpus=...] [--ram=...]` | Create VM from image |\n| Start | `ujust vm start [NAME]` | Start VM |\n| Stop | `ujust vm stop [NAME]` | Stop VM |\n| SSH | `ujust vm ssh [NAME] [--ssh-user=...]` | SSH to VM |\n| VNC | `ujust vm vnc [NAME]` | Open VNC viewer |\n| Status | `ujust vm status [NAME]` | Show VM status |\n| Help | `ujust vm help` | Show help |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: add, update, delete, download, etc. |\n| vm_name | (positional) | - | `bazzite-ai` | VM name |\n| url | `--url` | - | R2 CDN URL | QCOW2 image URL |\n| cpus | `--cpus` | - | `4` | Number of CPUs |\n| ram | `--ram` | - | `8192` | Memory in MB |\n| disk_size | `--disk-size` | - | `100G` | Disk size |\n| username | `--username` | `-u` | `$USER` | VM username |\n| password | `--password` | - | (empty) | VM password |\n| autologin | `--autologin` | - | `true` | Enable autologin |\n| ssh_port | `--ssh-port` | - | `4444` | SSH port forwarding |\n| vnc_port | `--vnc-port` | - | `5900` | VNC port |\n| ssh_user | `--ssh-user` | - | `$USER` | SSH user for connection |\n| share_dir | `--share-dir` | - | `$HOME` | Directory to share |\n| branch | `--branch` | `-b` | `stable` | Image branch (stable/testing) |\n| what | `--what` | - | - | Update target (for update action) |\n\n## Add VM (Full Workflow)\n\n```bash\n# Default: bazzite-ai VM with auto-detect settings\nujust vm add\n\n# Named VM with custom config (long form)\nujust vm add myvm --cpus=8 --ram=16384 --disk-size=200G\n\n# Testing branch image\nujust vm add testing-vm --branch=testing\n\n# Short form for branch\nujust vm add testing-vm -b testing\n\n# Different SSH port\nujust vm add dev-vm --ssh-port=4445\n\n# No home sharing\nujust vm add isolated --share-dir=''\n```\n\nThe `add` command:\n\n1. Downloads QCOW2 image (cached)\n2. Creates cloud-init seed ISO\n3. Creates libvirt VM\n4. Configures port forwarding\n\n## Individual Steps\n\n### Download QCOW2\n\n```bash\n# Stable image (default)\nujust vm download\n\n# Testing branch (long form)\nujust vm download --branch=testing\n\n# Testing branch (short form)\nujust vm download -b testing\n\n# Custom URL\nujust vm download --url=https://example.com/custom.qcow2\n```\n\n### Create Seed ISO\n\n```bash\n# Long form\nujust vm seed myvm --username=developer --password=secret\n\n# Short form for username\nujust vm seed myvm -u developer --password=secret\n```\n\n### Create VM\n\n```bash\nujust vm create myvm --cpus=4 --ram=8192\n```\n\n## VM Lifecycle\n\n### Start VM\n\n```bash\nujust vm start              # Default VM\nujust vm start myvm         # Named VM\n```\n\nAuto-adds VM if it doesn't exist.\n\n### Stop VM\n\n```bash\nujust vm stop              # Graceful shutdown\nujust vm stop myvm         # Named VM\n```\n\n### Delete VM\n\n```bash\nujust vm delete myvm        # Remove VM and disk\n```\n\n## Connecting to VM\n\n### SSH Connection\n\n```bash\n# Connect to default VM\nujust vm ssh\n\n# Named VM\nujust vm ssh myvm\n\n# Different user\nujust vm ssh myvm --ssh-user=root\n\n# Run command (use -- separator)\nujust vm ssh myvm -- ls -la\n```\n\nDefault SSH: `ssh -p 4444 localhost`\n\n### VNC Connection\n\n```bash\nujust vm vnc              # Opens VNC viewer\nujust vm vnc myvm\n```\n\nDefault VNC: Port 5900\n\n## Home Directory Sharing\n\nBy default, your home directory is shared to the VM at `/workspace` via virtiofs.\n\n```bash\n# Default: $HOME -> /workspace\nujust vm add\n\n# Disable sharing\nujust vm add isolated --share-dir=''\n\n# Share specific directory\nujust vm add project --share-dir=/path/to/project\n```\n\nInside VM:\n\n```bash\nls /workspace  # Your home directory\n```\n\n## Image Branches\n\n| Branch | Tag | Description |\n|--------|-----|-------------|\n| `stable` | `:stable` | Production, tested |\n| `testing` | `:testing` | Latest features |\n\n```bash\n# Long form\nujust vm download --branch=stable\nujust vm download --branch=testing\n\n# Short form\nujust vm download -b stable\nujust vm download -b testing\n```\n\n## Storage Locations\n\n| Item | Location |\n|------|----------|\n| Download cache | `~/.local/share/bazzite-ai/vm/cache/` |\n| VM disks | `~/.local/share/libvirt/images/` |\n| VM config | `~/.local/share/bazzite-ai/vm/<name>.conf` |\n| Seed ISO | `~/.local/share/bazzite-ai/vm/<name>-seed.iso` |\n\n## Common Workflows\n\n### Quick Test VM\n\n```bash\n# Add and start default VM\nujust vm add\nujust vm start\nujust vm ssh\n```\n\n### Development Environment\n\n```bash\n# Create dev VM with more resources\nujust vm add dev --cpus=8 --ram=16384 --disk-size=200G\n\n# Start it\nujust vm start dev\n\n# SSH in\nujust vm ssh dev\n\n# Your home is at /workspace\n```\n\n### Testing Branch\n\n```bash\n# Test latest features (long form)\nujust vm add testing-vm --branch=testing\n\n# Or short form\nujust vm add testing-vm -b testing\n\nujust vm start testing-vm\nujust vm ssh testing-vm\n```\n\n### Multiple VMs\n\n```bash\n# Create VMs on different ports\nujust vm add dev1 --ssh-port=4444\nujust vm add dev2 --ssh-port=4445\nujust vm add dev3 --ssh-port=4446\n\n# Start all (not a built-in command, use loop)\nfor vm in dev1 dev2 dev3; do ujust vm start $vm; done\n```\n\n## Troubleshooting\n\n### VM Won't Start\n\n**Check:**\n\n```bash\nujust vm status myvm\nvirsh --connect qemu:///session list --all\n```\n\n**Common causes:**\n\n- Disk image not found\n- Port conflict\n- Virtiofs path issue\n\n**Fix:**\n\n```bash\nujust vm delete myvm\nujust vm add myvm\n```\n\n### SSH Connection Refused\n\n**Check:**\n\n```bash\nssh -p 4444 localhost\n```\n\n**Common causes:**\n\n- VM not fully booted\n- Wrong SSH port\n- SSH not started in VM\n\n**Fix:**\n\n```bash\n# Wait longer after start\nsleep 30\nujust vm ssh myvm\n\n# Check VM console via VNC\nujust vm vnc myvm\n```\n\n### Virtiofs Not Working\n\n**Symptom:** `/workspace` empty or not mounted\n\n**Cause:** SHARE_DIR path issue (symlinks)\n\n**Fix:**\n\n```bash\n# Delete and recreate with canonical path\nujust vm delete myvm\nujust vm add myvm --share-dir=$(readlink -f $HOME)\n```\n\n### Out of Disk Space\n\n**Check:**\n\n```bash\nqemu-img info ~/.local/share/libvirt/images/myvm.qcow2\n```\n\n**Fix:**\n\n```bash\n# Create new VM with larger disk\nujust vm delete myvm\nujust vm add myvm --disk-size=200G\n```\n\n## Cross-References\n\n- **Related Skills:** `bootc` (alternative: bootc-based VMs)\n- **Prerequisites:** `ujust config libvirtd enable`\n- **bcvk alternative:** `ujust install bcvk` + `ujust bootc`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"create VM\", \"add VM\", \"start VM\"\n- \"ssh to VM\", \"connect to VM\"\n- \"download qcow2\", \"VM image\"\n- \"VM not starting\", \"VM connection failed\"\n- \"share directory with VM\", \"virtiofs\""
              },
              {
                "name": "apps",
                "description": "Third-party application installation for Bazzite. CoolerControl, DisplayLink,\nJetBrains Toolbox, OpenRazer, tablet drivers, scrcpy, and more. Use when users\nneed to install hardware-specific or specialized applications.\n",
                "path": "bazzite/skills/apps/SKILL.md",
                "frontmatter": {
                  "name": "apps",
                  "description": "Third-party application installation for Bazzite. CoolerControl, DisplayLink,\nJetBrains Toolbox, OpenRazer, tablet drivers, scrcpy, and more. Use when users\nneed to install hardware-specific or specialized applications.\n"
                },
                "content": "# Apps - Third-Party Application Installation\n\n## Overview\n\nInstall specialized applications and hardware drivers on Bazzite that aren't in Flatpak or require system integration.\n\n## Quick Reference\n\n### Hardware Control\n\n| Command | Description |\n|---------|-------------|\n| `ujust install-coolercontrol` | Fan/pump control software |\n| `ujust install-openrazer` | Razer gaming hardware drivers |\n| `ujust install-openrgb` | RGB lighting control |\n| `ujust install-opentabletdriver` | Drawing tablet drivers |\n| `ujust remove-opentabletdriver` | Remove tablet drivers |\n\n### Connectivity\n\n| Command | Description |\n|---------|-------------|\n| `ujust install-displaylink` | Laptop dock video output |\n| `ujust install-scrcpy` | Android device mirroring |\n| `ujust install-resilio-sync` | BitTorrent file sync |\n\n### Development\n\n| Command | Description |\n|---------|-------------|\n| `ujust install-jetbrains-toolbox` | JetBrains IDE manager |\n\n### Utilities\n\n| Command | Description |\n|---------|-------------|\n| `ujust setup-cdemu` | CD/DVD emulation |\n| `ujust pick` | Interactive ujust picker |\n\n## Hardware Control\n\n### CoolerControl\n\n```bash\n# Install CoolerControl for fan/pump management\nujust install-coolercontrol\n```\n\n**Features:**\n- Fan curve customization\n- Pump speed control\n- Temperature monitoring\n- Profile management\n\n**Supports:**\n- AIO coolers\n- Case fans\n- GPU fans (some)\n\n### OpenRazer\n\n```bash\n# Install OpenRazer for Razer hardware\nujust install-openrazer\n```\n\n**Supports:**\n- Razer keyboards\n- Razer mice\n- Razer headsets\n- RGB effects\n\n**Companion apps:**\n- Polychromatic (GUI)\n- RazerGenie\n- OpenRGB\n\n### OpenRGB\n\n```bash\n# Install OpenRGB for RGB lighting\nujust install-openrgb\n```\n\n**Features:**\n- Universal RGB control\n- Multiple brand support\n- Custom effects\n- Profile sync\n\n### Drawing Tablets\n\n```bash\n# Install OpenTabletDriver\nujust install-opentabletdriver\n\n# Remove if needed\nujust remove-opentabletdriver\n```\n\n**Supports:**\n- Wacom tablets\n- Huion tablets\n- XP-Pen tablets\n- Generic tablets\n\n## Connectivity\n\n### DisplayLink\n\n```bash\n# Install DisplayLink for docking stations\nujust install-displaylink\n```\n\n**Use for:**\n- USB-C docks with video\n- DisplayLink-based docks\n- External monitors via USB\n\n### scrcpy\n\n```bash\n# Install scrcpy for Android mirroring\nujust install-scrcpy\n```\n\n**Features:**\n- Mirror Android screen\n- Control device from PC\n- Low latency\n- No root required\n\n**Usage:**\n\n```bash\n# Connect via USB\nscrcpy\n\n# Wireless (after enabling)\nscrcpy --tcpip=192.168.x.x\n```\n\n### Resilio Sync\n\n```bash\n# Install Resilio Sync\nujust install-resilio-sync\n```\n\n**Features:**\n- BitTorrent-based sync\n- P2P file sharing\n- Encrypted transfers\n- Cross-platform\n\n## Development\n\n### JetBrains Toolbox\n\n```bash\n# Install JetBrains Toolbox\nujust install-jetbrains-toolbox\n```\n\n**Manages:**\n- IntelliJ IDEA\n- PyCharm\n- WebStorm\n- All JetBrains IDEs\n\n**After install:**\n- Launch Toolbox\n- Log in to JetBrains account\n- Install desired IDEs\n\n## Utilities\n\n### CD/DVD Emulation\n\n```bash\n# Setup CDEmu for virtual drives\nujust setup-cdemu\n```\n\n**Features:**\n- Mount ISO files\n- Virtual CD/DVD drives\n- Legacy software support\n\n**Usage:**\n\n```bash\n# Load ISO\ncdemu load 0 /path/to/image.iso\n\n# Unload\ncdemu unload 0\n```\n\n### Interactive ujust Picker\n\n```bash\n# Browse all ujust commands interactively\nujust pick\n```\n\n**Features:**\n- Search commands\n- Category browsing\n- Command descriptions\n- Direct execution\n\n## Common Workflows\n\n### Gaming Setup\n\n```bash\n# RGB control\nujust install-openrgb\n\n# Razer hardware\nujust install-openrazer\n\n# Fan control\nujust install-coolercontrol\n```\n\n### Digital Art Setup\n\n```bash\n# Tablet driver\nujust install-opentabletdriver\n\n# Android tablet as input (via scrcpy)\nujust install-scrcpy\n```\n\n### Development Setup\n\n```bash\n# JetBrains IDEs\nujust install-jetbrains-toolbox\n\n# For AI development, see bazzite-ai:install\n```\n\n### Docking Station\n\n```bash\n# DisplayLink for monitors\nujust install-displaylink\n\n# May need reboot\nsystemctl reboot\n```\n\n## Troubleshooting\n\n### CoolerControl Not Detecting Fans\n\n**Check sensors:**\n\n```bash\nsensors-detect\nsensors\n```\n\n**Verify service:**\n\n```bash\nsystemctl status coolercontrold\n```\n\n### OpenRazer Device Not Found\n\n**Check connection:**\n\n```bash\n# List USB devices\nlsusb | grep -i razer\n```\n\n**Check daemon:**\n\n```bash\nsystemctl --user status openrazer-daemon\n```\n\n### DisplayLink Not Working\n\n**Check module:**\n\n```bash\nlsmod | grep evdi\n```\n\n**Reconnect dock and check:**\n\n```bash\ndmesg | tail -20\n```\n\n### Tablet Not Responding\n\n**Check device:**\n\n```bash\n# List tablets\notd list\n```\n\n**Check daemon:**\n\n```bash\nsystemctl --user status opentabletdriver\n```\n\n## Cross-References\n\n- **bazzite-ai:install** - Development tools (Claude Code, pixi)\n- **bazzite:gaming** - Gaming-specific apps (Decky, EmuDeck)\n- **bazzite:distrobox** - Apps requiring containers\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"CoolerControl\", \"fan control\", \"pump speed\"\n- \"Razer\", \"OpenRazer\", \"Razer keyboard\", \"Razer mouse\"\n- \"RGB lighting\", \"OpenRGB\", \"LED control\"\n- \"drawing tablet\", \"Wacom\", \"Huion\", \"tablet driver\"\n- \"DisplayLink\", \"dock\", \"USB-C dock\", \"external monitors\"\n- \"scrcpy\", \"Android mirror\", \"phone screen\"\n- \"JetBrains\", \"IntelliJ\", \"PyCharm\", \"IDE\"\n- \"CD emulation\", \"mount ISO\", \"virtual drive\"\n- \"ujust picker\", \"browse commands\""
              },
              {
                "name": "audio",
                "description": "Audio configuration for Bazzite. Virtual audio channels for Game/Voice/Browser/Music,\n7.1 surround for headphones, Bluetooth headset profiles, and PipeWire management.\nUse when users need to configure audio on Bazzite.\n",
                "path": "bazzite/skills/audio/SKILL.md",
                "frontmatter": {
                  "name": "audio",
                  "description": "Audio configuration for Bazzite. Virtual audio channels for Game/Voice/Browser/Music,\n7.1 surround for headphones, Bluetooth headset profiles, and PipeWire management.\nUse when users need to configure audio on Bazzite.\n"
                },
                "content": "# Audio - Bazzite Audio Configuration\n\n## Overview\n\nBazzite uses PipeWire for audio. This skill covers virtual audio channels, surround sound emulation, Bluetooth audio, and PipeWire management.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust setup-virtual-channels` | Create Game/Voice/Browser/Music sinks |\n| `ujust setup-virtual-surround` | Setup 7.1 surround for headphones |\n| `ujust toggle-bt-mic` | Toggle Bluetooth headset profile fix |\n| `ujust restart-pipewire` | Restart PipeWire service |\n\n## Virtual Audio Channels\n\n### Setup Virtual Channels\n\n```bash\n# Create virtual audio sinks\nujust setup-virtual-channels\n```\n\n**Creates sinks:**\n- **Game** - Game audio\n- **Voice** - Discord, voice chat\n- **Browser** - Web browser audio\n- **Music** - Music players\n\n**Use case:** Route different apps to different channels for:\n- Separate volume control\n- Stream audio isolation\n- Recording specific sources\n\n### Using Virtual Channels\n\nAfter setup, select sinks in PipeWire/PulseAudio-compatible apps:\n\n1. Open app settings\n2. Select output device\n3. Choose Game/Voice/Browser/Music\n\nIn `pavucontrol`:\n1. Go to \"Playback\" tab\n2. Click app dropdown\n3. Select virtual sink\n\n## Surround Sound\n\n### Virtual 7.1 Surround\n\n```bash\n# Setup 7.1 surround for headphones\nujust setup-virtual-surround\n```\n\nCreates a virtual 7.1 surround sink that:\n- Takes stereo headphone output\n- Uses HRTF spatializer\n- Simulates surround positioning\n\n**Best for:**\n- Gaming with positional audio\n- Movies with surround tracks\n- Stereo headphones\n\n## Bluetooth Audio\n\n### Toggle BT Mic Fix\n\n```bash\n# Toggle Bluetooth headset profile mitigation\nujust toggle-bt-mic\n```\n\nFixes issues with Bluetooth headsets switching profiles when:\n- Mic is enabled/disabled\n- Switching between A2DP and HSP/HFP\n- Audio quality drops unexpectedly\n\n## PipeWire Management\n\n### Restart PipeWire\n\n```bash\n# Restart PipeWire and related services\nujust restart-pipewire\n```\n\nRestarts:\n- pipewire\n- pipewire-pulse\n- wireplumber\n\nUse when:\n- Audio stops working\n- Bluetooth audio issues\n- After configuration changes\n\n## Common Workflows\n\n### Streaming Setup\n\n```bash\n# Create virtual channels\nujust setup-virtual-channels\n\n# In OBS:\n# - Capture \"Game\" sink for game audio\n# - Capture \"Voice\" sink for Discord\n# - Exclude browser/music from stream\n```\n\n### Gaming Audio\n\n```bash\n# Enable 7.1 surround for headphones\nujust setup-virtual-surround\n\n# In game settings:\n# - Select 7.1 surround output\n# - Enable spatial audio\n```\n\n### Bluetooth Troubleshooting\n\n```bash\n# If BT audio drops or switches profiles\nujust toggle-bt-mic\n\n# Restart audio stack\nujust restart-pipewire\n```\n\n## Advanced Configuration\n\n### PipeWire Config Location\n\n```\n~/.config/pipewire/\n~/.config/wireplumber/\n```\n\n### Check Audio Devices\n\n```bash\n# List sinks\npactl list sinks short\n\n# List sources\npactl list sources short\n\n# PipeWire info\npw-cli info\n```\n\n### Volume Control\n\n```bash\n# GUI volume control\npavucontrol\n\n# CLI volume control\npactl set-sink-volume @DEFAULT_SINK@ 50%\n```\n\n## Troubleshooting\n\n### No Audio\n\n**Check PipeWire status:**\n\n```bash\nsystemctl --user status pipewire\nsystemctl --user status pipewire-pulse\n```\n\n**Restart:**\n\n```bash\nujust restart-pipewire\n```\n\n### Virtual Channels Not Showing\n\n**Verify sinks:**\n\n```bash\npactl list sinks short | grep -E \"Game|Voice|Browser|Music\"\n```\n\n**Recreate:**\n\n```bash\nujust setup-virtual-channels\n```\n\n### Bluetooth Audio Choppy\n\n**Check codec:**\n\n```bash\npactl list cards | grep -A10 \"bluez\"\n```\n\n**Switch to SBC-XQ or AAC if available:**\nUse `pavucontrol` > Configuration tab\n\n### Surround Not Working\n\n**Check sink:**\n\n```bash\npactl list sinks short | grep surround\n```\n\n**Verify game audio settings:**\n- Game must output 5.1/7.1\n- Virtual sink must be selected\n\n## Cross-References\n\n- **bazzite:gaming** - Gaming audio setup\n- **bazzite:network** - Bluetooth considerations\n- **bazzite-ai:configure** - Service configuration\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"audio channels\", \"virtual sinks\", \"separate audio\"\n- \"surround sound\", \"7.1 headphones\", \"spatial audio\"\n- \"Bluetooth mic\", \"BT audio\", \"headset profile\"\n- \"restart audio\", \"PipeWire restart\", \"audio not working\"\n- \"Game audio\", \"Voice chat audio\", \"streaming audio\"\n- \"audio routing\", \"OBS audio\", \"Discord audio\""
              },
              {
                "name": "boot",
                "description": "Boot configuration for Bazzite OS. BIOS/UEFI access, GRUB menu settings,\nsecure boot key enrollment, and Windows dual-boot setup. Use when users\nneed to configure boot options or access BIOS settings.\n",
                "path": "bazzite/skills/boot/SKILL.md",
                "frontmatter": {
                  "name": "boot",
                  "description": "Boot configuration for Bazzite OS. BIOS/UEFI access, GRUB menu settings,\nsecure boot key enrollment, and Windows dual-boot setup. Use when users\nneed to configure boot options or access BIOS settings.\n"
                },
                "content": "# Boot - Bazzite Boot Configuration\n\n## Overview\n\nThe boot skill covers BIOS/UEFI access, GRUB configuration, secure boot key management, and Windows dual-boot setup.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust bios` | Reboot to BIOS/UEFI setup |\n| `ujust bios-info` | Display BIOS information |\n| `ujust regenerate-grub` | Regenerate GRUB config |\n| `ujust config-grub` | Configure GRUB menu visibility |\n| `ujust enroll-secure-boot-key` | Enroll NVIDIA/KMOD secure boot key |\n| `ujust setup-boot-windows-steam` | Add Windows to Steam boot options |\n\n## BIOS/UEFI\n\n### Reboot to BIOS\n\n```bash\n# Reboot directly to BIOS/UEFI setup\nujust bios\n```\n\nSystem will reboot and enter BIOS setup automatically.\n\n### View BIOS Info\n\n```bash\n# Display BIOS information\nujust bios-info\n```\n\nUses `dmidecode` to show BIOS vendor, version, and date.\n\n## GRUB Configuration\n\n### Regenerate GRUB\n\n```bash\n# Regenerate GRUB configuration\nujust regenerate-grub\n```\n\nUseful for:\n- Dual-boot setup changes\n- New kernel installations\n- Boot parameter changes\n\n### GRUB Menu Visibility\n\n```bash\n# Interactive: choose hide/unhide/show\nujust config-grub\n\n# Non-interactive options\nujust config-grub hide     # Hide GRUB menu (instant boot)\nujust config-grub unhide   # Show GRUB menu briefly\nujust config-grub show     # Always show GRUB menu\n```\n\n## Secure Boot\n\n### Enroll Secure Boot Key\n\n```bash\n# Enroll NVIDIA driver and KMOD signing key\nujust enroll-secure-boot-key\n```\n\nRequired for:\n- NVIDIA proprietary drivers with Secure Boot enabled\n- Custom kernel modules\n- Third-party drivers\n\n**Process:**\n1. Generates MOK (Machine Owner Key)\n2. Prompts for password\n3. Reboots to MOK Manager\n4. Enter password to enroll key\n\n## Windows Dual-Boot\n\n### Add Windows to Steam\n\n```bash\n# Add Windows boot option to Steam non-Steam games\nujust setup-boot-windows-steam\n```\n\nAllows booting to Windows directly from Steam's game library.\n\n**Requirements:**\n- Windows installed on separate partition/drive\n- GRUB detecting Windows entry\n- Steam installed\n\n## Common Workflows\n\n### Dual-Boot Setup\n\n```bash\n# Regenerate GRUB to detect Windows\nujust regenerate-grub\n\n# Show GRUB menu on boot\nujust config-grub show\n\n# Add Windows to Steam\nujust setup-boot-windows-steam\n```\n\n### Secure Boot with NVIDIA\n\n```bash\n# Enroll the signing key\nujust enroll-secure-boot-key\n\n# Follow prompts, set password\n# Reboot and enroll in MOK Manager\n```\n\n### Hide Boot Menu\n\n```bash\n# For single-boot systems\nujust config-grub hide\n```\n\n## Troubleshooting\n\n### GRUB Not Showing Windows\n\n**Fix:**\n\n```bash\n# Regenerate GRUB config\nujust regenerate-grub\n\n# Check os-prober\nsudo os-prober\n```\n\n### Secure Boot Key Enrollment Fails\n\n**Check:**\n- Secure Boot enabled in BIOS\n- No pending updates\n- Reboot completely (not just suspend)\n\n**Retry:**\n\n```bash\nujust enroll-secure-boot-key\n```\n\n### Can't Boot After GRUB Change\n\n**From GRUB menu:**\n1. Press `e` to edit entry\n2. Modify boot parameters if needed\n3. Press `F10` to boot\n\n**Recover:**\n\n```bash\n# Boot from live USB\n# Mount system partition\n# Regenerate GRUB\n```\n\n## Cross-References\n\n- **bazzite:system** - System maintenance\n- **bazzite:gpu** - NVIDIA driver configuration\n- **bazzite:security** - LUKS/TPM unlock\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"reboot to BIOS\", \"enter UEFI\", \"BIOS setup\"\n- \"BIOS version\", \"BIOS info\", \"dmidecode\"\n- \"GRUB menu\", \"boot menu\", \"regenerate grub\"\n- \"hide boot menu\", \"show grub\", \"grub timeout\"\n- \"secure boot\", \"MOK\", \"enroll key\", \"signing key\"\n- \"dual boot\", \"Windows boot\", \"boot to Windows\"\n- \"Windows from Steam\", \"game mode Windows\""
              },
              {
                "name": "desktop",
                "description": "Desktop customization for Bazzite. GTK theme restoration, terminal transparency,\nand MOTD settings. Use when users need to customize their desktop appearance.\n",
                "path": "bazzite/skills/desktop/SKILL.md",
                "frontmatter": {
                  "name": "desktop",
                  "description": "Desktop customization for Bazzite. GTK theme restoration, terminal transparency,\nand MOTD settings. Use when users need to customize their desktop appearance.\n"
                },
                "content": "# Desktop - Bazzite Desktop Customization\n\n## Overview\n\nDesktop appearance customization for Bazzite including GTK themes, terminal transparency, and message of the day settings.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust restore-bazzite-breeze-gtk-theme` | Restore Bazzite GTK4 theme |\n| `ujust ptyxis-transparency` | Set terminal transparency |\n| `ujust toggle-user-motd` | Toggle terminal MOTD |\n\n## GTK Theme\n\n### Restore Bazzite Theme\n\n```bash\n# Restore default Bazzite Breeze GTK4 theme\nujust restore-bazzite-breeze-gtk-theme\n```\n\n**Restores:**\n- Bazzite Breeze GTK4 colors\n- Window decorations\n- Widget styling\n- Default Bazzite appearance\n\n**Use when:**\n- Theme got corrupted\n- Changed themes and want to revert\n- Fresh desktop appearance needed\n\n## Terminal Transparency\n\n### Set Transparency\n\n```bash\n# Set Ptyxis terminal transparency (0-1)\nujust ptyxis-transparency 0.8   # 80% opaque\nujust ptyxis-transparency 0.5   # 50% opaque\nujust ptyxis-transparency 1.0   # Fully opaque (no transparency)\nujust ptyxis-transparency 0.0   # Fully transparent\n```\n\n**Values:**\n- `1.0` = Fully opaque (solid)\n- `0.0` = Fully transparent\n- `0.8` = Recommended for readability\n\n**Note:** Ptyxis is the default terminal on Bazzite GNOME.\n\n## Message of the Day\n\n### Toggle MOTD\n\n```bash\n# Toggle user MOTD display on terminal\nujust toggle-user-motd\n```\n\n**MOTD (Message of the Day):**\n- Shows system info on terminal open\n- Welcome message\n- Tips and notifications\n\n**Toggle:**\n- Enabled  Disabled\n- Disabled  Enabled\n\n## Common Workflows\n\n### Clean Desktop Reset\n\n```bash\n# Restore default theme\nujust restore-bazzite-breeze-gtk-theme\n\n# Reset terminal transparency\nujust ptyxis-transparency 1.0\n```\n\n### Aesthetic Terminal\n\n```bash\n# Light transparency\nujust ptyxis-transparency 0.85\n\n# Enable MOTD for info\nujust toggle-user-motd\n```\n\n### Minimal Setup\n\n```bash\n# Disable MOTD\nujust toggle-user-motd\n\n# Full opacity\nujust ptyxis-transparency 1.0\n```\n\n## Manual Customization\n\n### GTK Themes\n\n```bash\n# List available themes\nls /usr/share/themes/\n\n# Set theme (GNOME)\ngsettings set org.gnome.desktop.interface gtk-theme \"Adwaita\"\n\n# Set icon theme\ngsettings set org.gnome.desktop.interface icon-theme \"Adwaita\"\n```\n\n### Cursor Theme\n\n```bash\n# List cursors\nls /usr/share/icons/*/cursors\n\n# Set cursor theme\ngsettings set org.gnome.desktop.interface cursor-theme \"Adwaita\"\n```\n\n### Font Settings\n\n```bash\n# Set interface font\ngsettings set org.gnome.desktop.interface font-name \"Cantarell 11\"\n\n# Set monospace font\ngsettings set org.gnome.desktop.interface monospace-font-name \"Source Code Pro 10\"\n```\n\n## Troubleshooting\n\n### Theme Not Applying\n\n**GTK4 apps:**\n\n```bash\n# Restart GTK4 apps or:\n# Log out and log back in\n```\n\n**Check theme exists:**\n\n```bash\nls /usr/share/themes/ | grep -i breeze\n```\n\n### Transparency Not Working\n\n**Check compositor:**\n\n```bash\n# Wayland sessions have transparency support\necho $XDG_SESSION_TYPE\n```\n\n**Ptyxis specific:**\n\n```bash\n# Check Ptyxis is running\npgrep ptyxis\n```\n\n### MOTD Still Showing\n\n**Check config:**\n\n```bash\n# MOTD config location\ncat ~/.config/motd-disabled 2>/dev/null\n```\n\n**Manual disable:**\n\n```bash\ntouch ~/.config/motd-disabled\n```\n\n## Desktop Environments\n\n### Bazzite GNOME\n\nDefault desktop with:\n- Ptyxis terminal\n- Nautilus file manager\n- GNOME extensions\n\n### Bazzite KDE\n\nAlternative with:\n- Konsole terminal\n- Dolphin file manager\n- KDE Plasma customization\n\n**Note:** Some commands may differ on KDE.\n\n## Cross-References\n\n- **bazzite-ai:shell** - Shell customization\n- **bazzite:gaming** - Game Mode appearance\n- **bazzite:system** - System cleanup\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"GTK theme\", \"restore theme\", \"Bazzite theme\", \"Breeze\"\n- \"terminal transparency\", \"Ptyxis\", \"transparent terminal\"\n- \"MOTD\", \"message of the day\", \"terminal welcome\"\n- \"desktop appearance\", \"customize desktop\""
              },
              {
                "name": "distrobox",
                "description": "Distrobox container management for Bazzite. Create containers from manifests,\ncustom containers, app-specific containers (brew), and DaVinci Resolve installation.\nUse when users need to work with distrobox containers.\n",
                "path": "bazzite/skills/distrobox/SKILL.md",
                "frontmatter": {
                  "name": "distrobox",
                  "description": "Distrobox container management for Bazzite. Create containers from manifests,\ncustom containers, app-specific containers (brew), and DaVinci Resolve installation.\nUse when users need to work with distrobox containers.\n"
                },
                "content": "# Distrobox - Container Management\n\n## Overview\n\nDistrobox lets you run any Linux distribution inside containers with seamless host integration. This skill covers creating and managing distrobox containers on Bazzite.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust distrobox-assemble` | Create containers from distrobox.ini |\n| `ujust assemble` | Alias for distrobox-assemble |\n| `ujust distrobox-new` | Create custom distrobox container |\n| `ujust distrobox` | Alias for distrobox-new |\n| `ujust setup-distrobox-app` | Install app containers (brew) |\n| `ujust install-resolve` | Install DaVinci Resolve |\n| `ujust install-davinci` | Alias for install-resolve |\n| `ujust install-davinci-resolve` | Alias for install-resolve |\n| `ujust install-resolve-studio` | Install DaVinci Resolve Studio |\n\n## Container Creation\n\n### From Manifest\n\n```bash\n# Create containers defined in distrobox.ini\nujust distrobox-assemble\n```\n\nReads `~/.config/distrobox/distrobox.ini` and creates all defined containers.\n\n**Example distrobox.ini:**\n\n```ini\n[ubuntu]\nimage=ubuntu:22.04\npull=true\ninit=true\n\n[fedora]\nimage=fedora:39\npull=true\n```\n\n### Custom Container\n\n```bash\n# Interactive container creation\nujust distrobox-new\n```\n\nPrompts for:\n- Container name\n- Base image\n- Additional options\n\n### Manual Creation\n\n```bash\n# Direct distrobox command\ndistrobox create --name mybox --image ubuntu:22.04\ndistrobox enter mybox\n```\n\n## App Containers\n\n### Homebrew Container\n\n```bash\n# Setup brew in a container\nujust setup-distrobox-app brew\n```\n\nCreates a dedicated container with Homebrew installed.\n\n**Usage after setup:**\n\n```bash\ndistrobox enter brew\nbrew install <package>\n```\n\n## DaVinci Resolve\n\n### Free Version\n\n```bash\n# Install DaVinci Resolve in container\nujust install-resolve\n\n# Aliases\nujust install-davinci\nujust install-davinci-resolve\n```\n\n### Studio Version\n\n```bash\n# Install DaVinci Resolve Studio\nujust install-resolve-studio\n```\n\nRequires license/dongle for Studio features.\n\n**Process:**\n1. Downloads Resolve installer\n2. Creates Fedora-based container\n3. Installs dependencies\n4. Installs Resolve\n5. Creates desktop entry\n\n## Common Workflows\n\n### Development Environment\n\n```bash\n# Create Ubuntu dev container\ndistrobox create --name dev --image ubuntu:22.04\ndistrobox enter dev\n\n# Inside container\nsudo apt update\nsudo apt install build-essential python3-pip\n```\n\n### Multiple Distros\n\n```bash\n# Create distrobox.ini\ncat > ~/.config/distrobox/distrobox.ini << EOF\n[arch]\nimage=archlinux:latest\npull=true\n\n[debian]\nimage=debian:bookworm\npull=true\nEOF\n\n# Create all containers\nujust distrobox-assemble\n```\n\n### Video Editing Setup\n\n```bash\n# Install DaVinci Resolve\nujust install-resolve\n\n# Launch from applications menu or:\ndistrobox enter resolve\nresolve\n```\n\n## Container Management\n\n### List Containers\n\n```bash\ndistrobox list\n```\n\n### Enter Container\n\n```bash\ndistrobox enter <name>\n```\n\n### Stop Container\n\n```bash\ndistrobox stop <name>\n```\n\n### Remove Container\n\n```bash\ndistrobox rm <name>\n```\n\n### Export Application\n\n```bash\n# Export app to host\ndistrobox enter <name>\ndistrobox-export --app <application>\n```\n\n## Troubleshooting\n\n### Container Won't Start\n\n**Check:**\n\n```bash\n# Container status\npodman ps -a\n\n# Logs\npodman logs <container-id>\n```\n\n**Fix:**\n\n```bash\n# Recreate container\ndistrobox rm <name>\ndistrobox create --name <name> --image <image>\n```\n\n### Resolve Won't Launch\n\n**Check NVIDIA drivers:**\n\n```bash\nnvidia-smi\n```\n\n**Check GPU access in container:**\n\n```bash\ndistrobox enter resolve\nnvidia-smi\n```\n\n### GUI Apps Not Working\n\n**Verify Wayland/X11:**\n\n```bash\necho $XDG_SESSION_TYPE\necho $DISPLAY\n```\n\n**Try X11 forwarding:**\n\n```bash\ndistrobox create --name <name> --image <image> --additional-flags \"--env DISPLAY=$DISPLAY\"\n```\n\n## Cross-References\n\n- **bazzite:gpu** - GPU driver configuration\n- **bazzite-ai:configure** - Docker/Podman configuration\n- **bazzite:apps** - Third-party application installation\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"distrobox\", \"create container\", \"linux container\"\n- \"ubuntu on bazzite\", \"arch container\", \"debian box\"\n- \"distrobox.ini\", \"assemble containers\", \"manifest\"\n- \"brew on bazzite\", \"homebrew\", \"brew container\"\n- \"DaVinci Resolve\", \"video editing\", \"Resolve Studio\"\n- \"run other distro\", \"different distribution\""
              },
              {
                "name": "gaming",
                "description": "Gaming ecosystem for Bazzite. Steam fixes, Proton troubleshooting, EmuDeck emulation,\nDecky Loader plugins, Sunshine game streaming, frame generation, and media apps.\nUse when users need help with gaming on Bazzite.\n",
                "path": "bazzite/skills/gaming/SKILL.md",
                "frontmatter": {
                  "name": "gaming",
                  "description": "Gaming ecosystem for Bazzite. Steam fixes, Proton troubleshooting, EmuDeck emulation,\nDecky Loader plugins, Sunshine game streaming, frame generation, and media apps.\nUse when users need help with gaming on Bazzite.\n"
                },
                "content": "# Gaming - Bazzite Gaming Ecosystem\n\n## Overview\n\nBazzite is a gaming-focused OS with extensive Steam, emulation, and streaming support. This skill covers the gaming ecosystem.\n\n## Quick Reference\n\n### Steam & Proton\n\n| Command | Description |\n|---------|-------------|\n| `ujust fix-gmod` | Patch GMod 64-bit for Linux |\n| `ujust fix-proton-hang` | Kill hung wine/proton processes |\n| `ujust fix-reset-steam` | Reset Steam (keeps games/saves) |\n| `ujust steam-icons` | Manage Steam shortcuts on desktop |\n\n### Streaming & Decky\n\n| Command | Description |\n|---------|-------------|\n| `ujust setup-sunshine` | Setup Sunshine streaming server |\n| `ujust setup-decky` | Install/uninstall Decky Loader |\n| `ujust install-decky-plugins` | Install Decky plugins |\n\n### Emulation & Tools\n\n| Command | Description |\n|---------|-------------|\n| `ujust install-emudeck` | Install EmuDeck |\n| `ujust get-emudeck` | Alias for install-emudeck |\n| `ujust install-boxtron` | DOS games via Steam |\n| `ujust install-steamcmd` | Install SteamCMD |\n| `ujust get-lsfg` | Lossless Scaling frame gen layer |\n| `ujust get-media-app` | Add streaming services to Steam |\n\n## Steam Fixes\n\n### GMod 64-bit Patch\n\n```bash\n# Patch Garry's Mod 64-bit beta for Linux\nujust fix-gmod\n```\n\nFixes compatibility issues with the 64-bit branch.\n\n### Kill Hung Proton\n\n```bash\n# Force-kill stuck wine/proton processes\nujust fix-proton-hang\n```\n\nUse when a game won't close or Steam shows a game as \"running\".\n\n### Reset Steam\n\n```bash\n# Reset Steam folder to fresh state\nujust fix-reset-steam\n```\n\n**Preserves:**\n- Game installations\n- Save files\n- Screenshots\n\n**Resets:**\n- Steam configuration\n- Compatibility settings\n- Shader cache\n\n### Steam Shortcuts\n\n```bash\n# Manage Steam game shortcuts on desktop\nujust steam-icons\n```\n\nCreates/removes desktop shortcuts for Steam games.\n\n## Game Streaming\n\n### Sunshine Server\n\n```bash\n# Setup Sunshine (Moonlight protocol)\nujust setup-sunshine\n```\n\n**Features:**\n- Host games for Moonlight clients\n- Stream to phones, tablets, other PCs\n- Hardware encoding (NVENC, VAAPI, QSV)\n\n**After setup:**\n- Access web UI at `https://localhost:47990`\n- Pair with Moonlight client\n- Configure apps and streaming settings\n\n## Decky Loader\n\n### Install/Uninstall\n\n```bash\n# Install or uninstall Decky Loader\nujust setup-decky\n```\n\nDecky Loader adds plugins to Steam's Game Mode.\n\n### Install Plugins\n\n```bash\n# Install recommended plugins\nujust install-decky-plugins\n```\n\n**Installs:**\n- Bazzite Buddy - Bazzite-specific features\n- FrameGen - Frame generation\n- LSFG-VK - Lossless Scaling Vulkan\n\n## Emulation\n\n### EmuDeck\n\n```bash\n# Install EmuDeck for emulation\nujust install-emudeck\n```\n\nEmuDeck configures:\n- RetroArch cores\n- Standalone emulators\n- Steam ROM Manager\n- Controller mappings\n\n### Boxtron (DOS)\n\n```bash\n# Install Boxtron for DOS games via Steam\nujust install-boxtron\n```\n\nEnables DOSBox integration for Steam DOS games.\n\n### SteamCMD\n\n```bash\n# Install SteamCMD\nujust install-steamcmd\n```\n\nCommand-line Steam client for:\n- Dedicated servers\n- Game downloads\n- Automation\n\n## Frame Generation\n\n### Lossless Scaling Layer\n\n```bash\n# Install/uninstall LSFG Vulkan layer\nujust get-lsfg\n```\n\nAdds frame generation to games via Vulkan layer.\n\n## Media Apps\n\n### Streaming Services\n\n```bash\n# Add streaming services to Steam\nujust get-media-app\n```\n\n**Adds:**\n- YouTube\n- Netflix\n- Twitch\n- Prime Video\n- Other streaming services\n\nShows as non-Steam games in library for Game Mode access.\n\n## Common Workflows\n\n### Fresh Gaming Setup\n\n```bash\n# Install Decky with plugins\nujust setup-decky\nujust install-decky-plugins\n\n# Install EmuDeck for emulation\nujust install-emudeck\n\n# Add streaming apps\nujust get-media-app\n```\n\n### Game Streaming Host\n\n```bash\n# Setup Sunshine\nujust setup-sunshine\n\n# On client devices, use Moonlight app\n# Pair using PIN from Sunshine web UI\n```\n\n### Steam Troubleshooting\n\n```bash\n# Game won't close\nujust fix-proton-hang\n\n# Major Steam issues\nujust fix-reset-steam\n```\n\n## Troubleshooting\n\n### Proton Game Won't Launch\n\n**Check Proton version:**\n\n```bash\n# Try different Proton version in Steam\n# Right-click game > Properties > Compatibility\n# Select specific Proton version\n```\n\n**Check logs:**\n\n```bash\n# Enable Proton logging\nPROTON_LOG=1 steam steam://rungameid/<appid>\n```\n\n### Sunshine Not Streaming\n\n**Check service:**\n\n```bash\nsystemctl --user status sunshine\n```\n\n**Check ports:**\n- TCP: 47984, 47989, 47990, 48010\n- UDP: 47998-48000, 48002, 48010\n\n### Decky Not Loading\n\n**Reinstall:**\n\n```bash\nujust setup-decky\n```\n\n**Check logs:**\n\n```bash\njournalctl --user -u decky -n 50\n```\n\n### EmuDeck Not Finding ROMs\n\n**Check ROM paths:**\n- Default: `~/Emulation/roms/<system>/`\n- Ensure correct folder structure\n- Run Steam ROM Manager to refresh\n\n## Cross-References\n\n- **bazzite:gpu** - GPU driver configuration\n- **bazzite:audio** - Audio setup for gaming\n- **bazzite-ai:configure** - Steam autostart, gamemode\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"Steam not working\", \"game won't launch\", \"Proton hang\"\n- \"reset Steam\", \"Steam problems\", \"game stuck\"\n- \"Sunshine\", \"game streaming\", \"Moonlight\", \"remote play\"\n- \"Decky\", \"Steam Deck plugins\", \"Game Mode plugins\"\n- \"EmuDeck\", \"emulation\", \"retro games\", \"ROMs\"\n- \"frame generation\", \"LSFG\", \"Lossless Scaling\"\n- \"Netflix on Steam\", \"streaming apps\", \"media in Game Mode\"\n- \"GMod Linux\", \"Garry's Mod fix\""
              },
              {
                "name": "gpu",
                "description": "GPU driver configuration for Bazzite. NVIDIA proprietary drivers, Optimus laptops,\nNVK (open-source NVIDIA), GPU switching, Broadcom WiFi, and Mesa testing builds.\nUse when users need to configure graphics drivers.\n",
                "path": "bazzite/skills/gpu/SKILL.md",
                "frontmatter": {
                  "name": "gpu",
                  "description": "GPU driver configuration for Bazzite. NVIDIA proprietary drivers, Optimus laptops,\nNVK (open-source NVIDIA), GPU switching, Broadcom WiFi, and Mesa testing builds.\nUse when users need to configure graphics drivers.\n"
                },
                "content": "# GPU - Bazzite GPU Configuration\n\n## Overview\n\nBazzite supports NVIDIA, AMD, and Intel GPUs. This skill covers NVIDIA driver configuration, Optimus laptops, GPU switching, and related drivers.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust config-nvidia` | Configure NVIDIA drivers |\n| `ujust nvidia` | Alias for configure-nvidia |\n| `ujust toggle-nvk` | Switch between NVIDIA/NVK images |\n| `ujust config-nvidia-optimus` | Configure Optimus power management |\n| `ujust config-broadcom-wl` | Enable/disable Broadcom WiFi driver |\n| `ujust enable-supergfxctl` | Enable GPU switcher for hybrid laptops |\n| `ujust _mesa-git` | Mesa testing builds |\n\n## NVIDIA Configuration\n\n### Configure NVIDIA\n\n```bash\n# Interactive NVIDIA configuration\nujust config-nvidia\n\n# Same command\nujust nvidia\n```\n\n**Options:**\n- `kargs` - Set kernel arguments\n- `test-cuda` - Test CUDA functionality\n- `firefox-vaapi` - Enable Firefox hardware acceleration\n\n### Kernel Arguments\n\n```bash\nujust config-nvidia kargs\n```\n\nSets recommended kernel parameters for NVIDIA:\n- `nvidia_drm.modeset=1`\n- `nvidia_drm.fbdev=1`\n\n### Test CUDA\n\n```bash\nujust config-nvidia test-cuda\n```\n\nRuns CUDA sample to verify GPU compute.\n\n### Firefox VA-API\n\n```bash\nujust config-nvidia firefox-vaapi\n```\n\nEnables hardware video acceleration in Firefox.\n\n## NVK (Open-Source)\n\n### Toggle NVK\n\n```bash\n# Switch between NVIDIA proprietary and NVK\nujust toggle-nvk\n```\n\n**NVK:**\n- Mesa's open-source Vulkan driver for NVIDIA\n- Requires newer GPUs (Turing+)\n- Part of nvidia-open images\n\n**NVIDIA:**\n- Proprietary drivers\n- CUDA support\n- Better compatibility for older GPUs\n\n**Reboot required after switching.**\n\n## Optimus Laptops\n\n### Configure Optimus\n\n```bash\n# Configure NVIDIA Optimus power management\nujust config-nvidia-optimus\n```\n\n**Options:**\n- `power-management` - Power state management\n\n### Enable GPU Switcher\n\n```bash\n# Enable supergfxctl for GPU switching\nujust enable-supergfxctl\n```\n\n**supergfxctl** allows:\n- Switching between iGPU and dGPU\n- Power management modes\n- Profile selection\n\n**Modes:**\n- Integrated - Intel/AMD iGPU only (power saving)\n- Hybrid - Both GPUs, NVIDIA on-demand\n- Dedicated - NVIDIA only (performance)\n\n## Broadcom WiFi\n\n### Configure Broadcom\n\n```bash\n# Enable/disable Broadcom WL driver\nujust config-broadcom-wl\n```\n\nRequired for certain Broadcom wireless chips that don't work with open-source drivers.\n\n**Options:**\n- `enable` - Enable Broadcom WL driver\n- `disable` - Disable and use open-source\n\n## Mesa Testing\n\n### Mesa Git Builds\n\n```bash\n# Manage Mesa Git builds\nujust _mesa-git\n```\n\n**Options:**\n- Download latest Mesa Git\n- Install for testing\n- Cleanup old builds\n\n**Warning:** For testing only. May cause instability.\n\n## Common Workflows\n\n### Fresh NVIDIA Setup\n\n```bash\n# Configure kernel args\nujust config-nvidia kargs\n\n# Reboot\nsystemctl reboot\n\n# Test CUDA\nujust config-nvidia test-cuda\n\n# Enable Firefox HW accel\nujust config-nvidia firefox-vaapi\n```\n\n### Laptop Power Saving\n\n```bash\n# Enable GPU switcher\nujust enable-supergfxctl\n\n# Use supergfxctl to select mode\nsupergfxctl -m integrated\n```\n\n### Try NVK Driver\n\n```bash\n# Switch to NVK\nujust toggle-nvk\n\n# Reboot\nsystemctl reboot\n\n# Verify\nvulkaninfo | grep driverName\n```\n\n## Verification\n\n### Check NVIDIA Driver\n\n```bash\n# Driver version\nnvidia-smi\n\n# Module loaded\nlsmod | grep nvidia\n\n# Vulkan info\nvulkaninfo | head -20\n```\n\n### Check GPU in Use\n\n```bash\n# Current GPU\nglxinfo | grep \"OpenGL renderer\"\n\n# For Vulkan\nvulkaninfo | grep deviceName\n```\n\n### Check Power Mode\n\n```bash\n# With supergfxctl\nsupergfxctl -g\n\n# NVIDIA power state\ncat /sys/bus/pci/devices/0000:01:00.0/power/runtime_status\n```\n\n## Troubleshooting\n\n### NVIDIA Driver Not Loading\n\n**Check secure boot:**\n\n```bash\n# If secure boot enabled, enroll key\nujust enroll-secure-boot-key\n```\n\n**Check kernel args:**\n\n```bash\nrpm-ostree kargs\n```\n\n**Reinstall:**\n\n```bash\nujust config-nvidia kargs\nsystemctl reboot\n```\n\n### Black Screen After NVK Switch\n\n**Boot to previous deployment:**\n1. At GRUB, select previous boot entry\n2. Once booted:\n\n```bash\nujust toggle-nvk\nsystemctl reboot\n```\n\n### Optimus Not Switching\n\n**Check supergfxctl:**\n\n```bash\nsystemctl status supergfxd\nsupergfxctl -g\n```\n\n**Manual switch:**\n\n```bash\nsupergfxctl -m <mode>\n# hybrid, integrated, or dedicated\n```\n\n### CUDA Not Working\n\n**Check installation:**\n\n```bash\nnvidia-smi\nujust config-nvidia test-cuda\n```\n\n**Reinstall CUDA toolkit if needed.**\n\n## Cross-References\n\n- **bazzite:boot** - Secure boot key enrollment\n- **bazzite:gaming** - Gaming performance\n- **bazzite-ai:configure** - GPU containers\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"NVIDIA driver\", \"configure nvidia\", \"nvidia setup\"\n- \"NVK\", \"nouveau\", \"open source nvidia\"\n- \"Optimus\", \"laptop GPU\", \"hybrid graphics\"\n- \"GPU switching\", \"supergfxctl\", \"dedicated GPU\"\n- \"Broadcom WiFi\", \"wireless driver\"\n- \"CUDA not working\", \"nvidia-smi\", \"GPU compute\"\n- \"Firefox video\", \"hardware acceleration\", \"VA-API\""
              },
              {
                "name": "network",
                "description": "Network configuration for Bazzite. iwd WiFi backend, Wake-on-LAN, and Tailscale VPN.\nUse when users need to configure network services. For SSH, see bazzite-ai:config.\n",
                "path": "bazzite/skills/network/SKILL.md",
                "frontmatter": {
                  "name": "network",
                  "description": "Network configuration for Bazzite. iwd WiFi backend, Wake-on-LAN, and Tailscale VPN.\nUse when users need to configure network services. For SSH, see bazzite-ai:config.\n"
                },
                "content": "# Network - Bazzite Network Configuration\n\n## Overview\n\nBazzite network configuration including alternative WiFi backends, Wake-on-LAN for remote power control, and Tailscale for VPN/mesh networking.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust toggle-iwd` | Enable/disable iwd as WiFi backend |\n| `ujust toggle-wol` | Toggle Wake-on-LAN |\n| `ujust enable-tailscale` | Enable Tailscale service |\n\n## WiFi Backend\n\n### Toggle iwd\n\n```bash\n# Switch between iwd and wpa_supplicant\nujust toggle-iwd\n```\n\n**iwd (Intel Wireless Daemon):**\n- Faster connection times\n- Lower resource usage\n- Better power efficiency\n- Modern replacement for wpa_supplicant\n\n**wpa_supplicant:**\n- Default on most systems\n- Broader compatibility\n- Required for some enterprise networks\n\n**After switching:** Reconnect to WiFi networks.\n\n## Wake-on-LAN\n\n### Toggle WOL\n\n```bash\n# Interactive WOL toggle\nujust toggle-wol\n\n# Non-interactive\nujust toggle-wol enable\nujust toggle-wol disable\nujust toggle-wol force-enable\n```\n\n**Options:**\n- `enable` - Enable WOL\n- `disable` - Disable WOL\n- `force-enable` - Force enable (overrides power settings)\n\n### Using WOL\n\n**On target machine:**\n\n```bash\n# Get MAC address\nip link show | grep ether\n```\n\n**From remote machine:**\n\n```bash\n# Wake the target\nwakeonlan <MAC_ADDRESS>\n# or\nwol <MAC_ADDRESS>\n```\n\n**Requirements:**\n- Wired Ethernet connection\n- BIOS WOL support enabled\n- Both machines on same network (or port forwarding)\n\n## Tailscale VPN\n\n### Enable Tailscale\n\n```bash\n# Enable Tailscale service\nujust enable-tailscale\n```\n\n**After enabling:**\n\n```bash\n# Authenticate\ntailscale up\n\n# Check status\ntailscale status\n\n# Get IP\ntailscale ip\n```\n\n**Features:**\n- Zero-config VPN\n- Mesh networking\n- Access machines anywhere\n- MagicDNS for hostnames\n\n### Tailscale Usage\n\n```bash\n# Connect to Tailscale network\ntailscale up\n\n# Exit node (route all traffic)\ntailscale up --exit-node=<node>\n\n# Disconnect\ntailscale down\n\n# Status\ntailscale status\n```\n\n## Common Workflows\n\n### Remote Access Setup\n\n```bash\n# Enable Tailscale\nujust enable-tailscale\ntailscale up\n\n# Enable Wake-on-LAN for remote power\nujust toggle-wol enable\n\n# Enable SSH (via bazzite-ai)\nujust config sshd enable\n```\n\n### Better WiFi Performance\n\n```bash\n# Switch to iwd\nujust toggle-iwd\n\n# Reconnect to WiFi\nnmcli device wifi list\nnmcli device wifi connect \"<SSID>\" password \"<password>\"\n```\n\n### Home Server Access\n\n```bash\n# On server: Enable Tailscale\nujust enable-tailscale\ntailscale up\n\n# On client: Connect\ntailscale up\n\n# Access server via Tailscale IP or MagicDNS name\nssh user@<server-tailscale-ip>\nssh user@<server-name>  # with MagicDNS\n```\n\n## Network Troubleshooting\n\n### Check Network Status\n\n```bash\n# NetworkManager status\nnmcli general status\n\n# List connections\nnmcli connection show\n\n# Current IP\nip addr show\n\n# WiFi networks\nnmcli device wifi list\n```\n\n### WiFi Issues\n\n**Reconnect:**\n\n```bash\nnmcli device wifi connect \"<SSID>\" password \"<password>\"\n```\n\n**Forget and reconnect:**\n\n```bash\nnmcli connection delete \"<SSID>\"\nnmcli device wifi connect \"<SSID>\" password \"<password>\"\n```\n\n### Tailscale Issues\n\n**Check service:**\n\n```bash\nsystemctl status tailscaled\n```\n\n**Re-authenticate:**\n\n```bash\ntailscale logout\ntailscale up\n```\n\n**Check connectivity:**\n\n```bash\ntailscale netcheck\ntailscale ping <node>\n```\n\n### WOL Not Working\n\n**Check BIOS:**\n- Enable \"Wake on LAN\" in BIOS/UEFI\n\n**Check interface:**\n\n```bash\n# Verify WOL enabled\nethtool <interface> | grep Wake-on\n# Should show: Wake-on: g\n```\n\n**Enable manually:**\n\n```bash\nsudo ethtool -s <interface> wol g\n```\n\n## Cross-References\n\n- **bazzite-ai:configure** - SSH server configuration\n- **bazzite:security** - VPN security considerations\n- **bazzite:system** - Network diagnostics\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"iwd\", \"wpa_supplicant\", \"WiFi backend\", \"faster WiFi\"\n- \"Wake on LAN\", \"WOL\", \"remote power on\", \"wake computer\"\n- \"Tailscale\", \"VPN\", \"mesh network\", \"remote access\"\n- \"WiFi not connecting\", \"network issues\"\n\n**For SSH configuration, use:** `/bazzite-ai:configure`"
              },
              {
                "name": "security",
                "description": "Security configuration for Bazzite. LUKS disk encryption with TPM auto-unlock,\nsecure boot key management, and sudo password feedback. Use when users need\nto configure security features.\n",
                "path": "bazzite/skills/security/SKILL.md",
                "frontmatter": {
                  "name": "security",
                  "description": "Security configuration for Bazzite. LUKS disk encryption with TPM auto-unlock,\nsecure boot key management, and sudo password feedback. Use when users need\nto configure security features.\n"
                },
                "content": "# Security - Bazzite Security Configuration\n\n## Overview\n\nBazzite security features including LUKS disk encryption with TPM auto-unlock, and sudo password visibility settings.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust setup-luks-tpm-unlock` | Enable TPM auto-unlock for LUKS |\n| `ujust remove-luks-tpm-unlock` | Remove TPM auto-unlock |\n| `ujust toggle-password-feedback` | Toggle sudo asterisk feedback |\n\n## LUKS TPM Unlock\n\n### Setup TPM Auto-Unlock\n\n```bash\n# Enable automatic LUKS unlock via TPM\nujust setup-luks-tpm-unlock\n```\n\n**What it does:**\n- Binds LUKS encryption to TPM 2.0\n- System unlocks automatically at boot\n- No password prompt needed\n\n**Requirements:**\n- TPM 2.0 chip\n- LUKS-encrypted root partition\n- Secure Boot recommended\n\n**Process:**\n1. Verifies TPM availability\n2. Creates TPM binding\n3. Updates initramfs\n4. Tests unlock\n\n### Remove TPM Unlock\n\n```bash\n# Remove TPM auto-unlock\nujust remove-luks-tpm-unlock\n```\n\nReturns to password-based unlock at boot.\n\n**Use when:**\n- Selling/giving away machine\n- Security concerns\n- TPM issues\n\n## Sudo Password Feedback\n\n### Toggle Asterisks\n\n```bash\n# Toggle sudo password asterisk feedback\nujust toggle-password-feedback\n```\n\n**With feedback:**\n```\n[sudo] password for user: ****\n```\n\n**Without feedback (default):**\n```\n[sudo] password for user:\n```\n\n**Security note:** Asterisks reveal password length. Default (no feedback) is more secure.\n\n## Common Workflows\n\n### Secure Boot Setup\n\n```bash\n# 1. Enroll secure boot key (for NVIDIA)\nujust enroll-secure-boot-key\n\n# 2. Setup TPM unlock\nujust setup-luks-tpm-unlock\n\n# Reboot to test\nsystemctl reboot\n```\n\n### Disable Before Selling\n\n```bash\n# Remove TPM binding\nujust remove-luks-tpm-unlock\n\n# Clear TPM (in BIOS/UEFI)\n# Factory reset recommended\n```\n\n## TPM Status\n\n### Check TPM Availability\n\n```bash\n# TPM version and status\ntpm2_getcap properties-fixed | head -20\n\n# TPM PCR values\ntpm2_pcrread\n```\n\n### Check LUKS Binding\n\n```bash\n# List LUKS tokens\ncryptsetup luksDump /dev/<device> | grep Token\n\n# Check systemd-cryptenroll\nsystemd-cryptenroll --tpm2-device=list\n```\n\n## Troubleshooting\n\n### TPM Unlock Fails\n\n**Common causes:**\n- BIOS update changed PCR values\n- Secure Boot state changed\n- Hardware change detected\n\n**Fix:**\n\n```bash\n# Re-enroll TPM\nujust remove-luks-tpm-unlock\nujust setup-luks-tpm-unlock\n```\n\n### TPM Not Found\n\n**Check:**\n\n```bash\n# Verify TPM device\nls /dev/tpm*\n\n# TPM status\ntpm2_getcap properties-fixed\n```\n\n**Enable in BIOS:**\n- Find TPM/Security settings\n- Enable TPM 2.0\n\n### After BIOS Update\n\nTPM PCR values change after BIOS updates, breaking auto-unlock.\n\n**Fix:**\n\n```bash\n# Boot with password\n# Then re-enroll\nujust remove-luks-tpm-unlock\nujust setup-luks-tpm-unlock\n```\n\n### Sudo Password Not Showing\n\n**If you want asterisks:**\n\n```bash\nujust toggle-password-feedback\n```\n\n**Manual fix:**\n\n```bash\n# Edit sudoers\nsudo visudo\n\n# Add line:\n# Defaults pwfeedback\n```\n\n## Security Best Practices\n\n### For TPM Unlock\n\n1. **Enable Secure Boot** - Prevents boot tampering\n2. **Set BIOS password** - Prevents Secure Boot changes\n3. **Keep backup passphrase** - For recovery\n4. **Re-enroll after BIOS updates**\n\n### For General Security\n\n1. **Use strong passwords**\n2. **Enable automatic updates** (`ujust toggle-updates`)\n3. **Consider password feedback OFF** (hides length)\n4. **Check SSH settings** (`ujust config sshd status`)\n\n## Cross-References\n\n- **bazzite:boot** - Secure boot key enrollment\n- **bazzite:storage** - LUKS volume management\n- **bazzite-ai:configure** - SSH and service security\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"LUKS unlock\", \"disk encryption\", \"TPM unlock\"\n- \"auto unlock\", \"boot without password\", \"encrypted boot\"\n- \"remove TPM\", \"disable auto unlock\"\n- \"sudo password\", \"asterisks\", \"password feedback\"\n- \"security settings\", \"secure boot\", \"TPM\""
              },
              {
                "name": "storage",
                "description": "Storage management for Bazzite. Automounting drives (BTRFS/EXT4, Framework, SteamOS),\nBTRFS deduplication, rmlint disk trimming, and snapper snapshots. Use when users\nneed to configure disk and partition management.\n",
                "path": "bazzite/skills/storage/SKILL.md",
                "frontmatter": {
                  "name": "storage",
                  "description": "Storage management for Bazzite. Automounting drives (BTRFS/EXT4, Framework, SteamOS),\nBTRFS deduplication, rmlint disk trimming, and snapper snapshots. Use when users\nneed to configure disk and partition management.\n"
                },
                "content": "# Storage - Bazzite Storage Management\n\n## Overview\n\nBazzite supports automatic mounting of drives, BTRFS features like deduplication and snapshots, and disk optimization with rmlint.\n\n## Quick Reference\n\n### Automounting\n\n| Command | Description |\n|---------|-------------|\n| `ujust enable-automounting` | Enable BTRFS/EXT4 automount |\n| `ujust disable-automounting` | Disable BTRFS/EXT4 automount |\n| `ujust enable-framework-automount` | Enable Framework laptop automount |\n| `ujust disable-framework-automount` | Disable Framework automount |\n| `ujust enable-steamos-automount` | Enable SteamOS automount |\n| `ujust disable-steamos-automount` | Disable SteamOS automount |\n| `ujust enable-automount-all` | Enable all automounting |\n| `ujust disable-automount-all` | Disable all automounting |\n\n### BTRFS Features\n\n| Command | Description |\n|---------|-------------|\n| `ujust enable-deduplication` | Enable BTRFS dedup for /var/home |\n| `ujust enable-rmlint` | Enable/disable rmlint trim |\n| `ujust config-snapshots` | Enable/disable snapper snapshots |\n\n## Automounting\n\n### Standard Automounting\n\n```bash\n# Enable automounting for BTRFS/EXT4 labeled partitions\nujust enable-automounting\n\n# Disable\nujust disable-automounting\n```\n\nMounts drives with recognized labels at:\n- `/run/media/$USER/<label>`\n\n### Framework Laptop\n\n```bash\n# Enable Framework-specific automounting\nujust enable-framework-automount\n\n# Disable\nujust disable-framework-automount\n```\n\nHandles Framework expansion cards and storage modules.\n\n### SteamOS Mounts\n\n```bash\n# Enable SteamOS-style automounting\nujust enable-steamos-automount\n\n# Disable\nujust disable-steamos-automount\n```\n\nCompatibility with SteamOS mount paths.\n\n### All Automounting\n\n```bash\n# Enable everything\nujust enable-automount-all\n\n# Disable everything\nujust disable-automount-all\n```\n\n## BTRFS Deduplication\n\n### Enable Deduplication\n\n```bash\n# Enable BTRFS deduplication for /var/home\nujust enable-deduplication\n```\n\n**Benefits:**\n- Saves space with duplicate files\n- Runs in background\n- Minimal performance impact\n\n**Note:** Only for BTRFS partitions.\n\n## Disk Trimming\n\n### Enable rmlint\n\n```bash\n# Enable/disable rmlint trim feature\nujust enable-rmlint\n```\n\nrmlint finds and removes:\n- Duplicate files\n- Empty directories\n- Broken symlinks\n- Other disk clutter\n\n## Snapshots\n\n### Configure Snapper\n\n```bash\n# Enable/disable snapper snapshots for /var/home\nujust config-snapshots\n```\n\n**Snapper:**\n- Creates automatic snapshots\n- Allows rollback of /var/home\n- Timeline-based retention\n\n**Warning:** Snapshots use disk space.\n\n## Common Workflows\n\n### Add External Drive\n\n```bash\n# Enable automounting\nujust enable-automounting\n\n# Plug in drive - mounts automatically\n# Access at /run/media/$USER/<label>\n```\n\n### Space Optimization\n\n```bash\n# Enable deduplication\nujust enable-deduplication\n\n# Enable rmlint for cleanup\nujust enable-rmlint\n```\n\n### Backup with Snapshots\n\n```bash\n# Enable snapshots\nujust config-snapshots\n\n# View snapshots\nsnapper list\n\n# Rollback\nsnapper rollback <number>\n```\n\n### Framework Setup\n\n```bash\n# Enable Framework automounting\nujust enable-framework-automount\n\n# Expansion cards auto-mount when inserted\n```\n\n## Manual Mount Management\n\n### Mount Manually\n\n```bash\n# Create mount point\nsudo mkdir -p /mnt/data\n\n# Mount\nsudo mount /dev/sdb1 /mnt/data\n\n# Mount BTRFS with options\nsudo mount -o compress=zstd /dev/sdb1 /mnt/data\n```\n\n### Add to fstab\n\n```bash\n# Get UUID\nlsblk -o NAME,UUID,FSTYPE\n\n# Edit fstab\nsudo nano /etc/fstab\n\n# Add line:\n# UUID=<uuid> /mnt/data btrfs defaults,compress=zstd 0 0\n```\n\n## Verification\n\n### Check Mounts\n\n```bash\n# List mounts\nmount | grep -E \"/dev/sd|/dev/nvme\"\n\n# List block devices\nlsblk\n\n# Check BTRFS usage\nbtrfs filesystem usage /var/home\n```\n\n### Check Deduplication\n\n```bash\n# Check dedup status\nbtrfs filesystem df /var/home\n```\n\n### Check Snapshots\n\n```bash\n# List snapshots\nsnapper list\n\n# Snapshot details\nsnapper status <number>\n```\n\n## Troubleshooting\n\n### Drive Not Automounting\n\n**Check label:**\n\n```bash\nlsblk -o NAME,LABEL,FSTYPE\n```\n\n**Check automount status:**\n\n```bash\n# Is automounting enabled?\ngsettings get org.gnome.desktop.media-handling automount\n```\n\n**Manual mount to test:**\n\n```bash\nsudo mount /dev/sdb1 /mnt/test\n```\n\n### Deduplication Not Working\n\n**Verify BTRFS:**\n\n```bash\n# Must be BTRFS\ndf -T /var/home | grep btrfs\n```\n\n**Check status:**\n\n```bash\nbtrfs filesystem du -s /var/home\n```\n\n### Snapshots Filling Disk\n\n**List and clean:**\n\n```bash\n# List snapshots\nsnapper list\n\n# Delete old snapshots\nsnapper delete <number>\n\n# Or disable entirely\nujust config-snapshots\n```\n\n## Cross-References\n\n- **bazzite:system** - System cleanup\n- **bazzite:security** - LUKS encryption\n- **bazzite-ai:configure** - Docker/Podman storage\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"mount drive\", \"automount\", \"external drive\"\n- \"Framework storage\", \"expansion card\", \"SteamOS mount\"\n- \"BTRFS dedup\", \"deduplication\", \"save space\"\n- \"disk trim\", \"rmlint\", \"cleanup duplicates\"\n- \"snapshots\", \"snapper\", \"backup home\", \"rollback\"\n- \"partition mount\", \"fstab\", \"mount on boot\""
              },
              {
                "name": "system",
                "description": "System maintenance for Bazzite OS. Updates via topgrade, cleanup of podman/flatpaks,\nviewing logs and changelogs, diagnostics, and power measurements. Use when users\nneed to update, clean, or diagnose their Bazzite system.\n",
                "path": "bazzite/skills/system/SKILL.md",
                "frontmatter": {
                  "name": "system",
                  "description": "System maintenance for Bazzite OS. Updates via topgrade, cleanup of podman/flatpaks,\nviewing logs and changelogs, diagnostics, and power measurements. Use when users\nneed to update, clean, or diagnose their Bazzite system.\n"
                },
                "content": "# System - Bazzite System Maintenance\n\n## Overview\n\nThe system skill covers core Bazzite maintenance tasks: updates, cleanup, logging, diagnostics, and benchmarking.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust update` | Update system via topgrade |\n| `ujust upgrade` | Alias for update |\n| `ujust changelogs` | View stable release notes |\n| `ujust changelogs-testing` | View pre-release notes |\n| `ujust toggle-updates` | Enable/disable automatic updates |\n| `ujust clean-system` | Cleanup podman, flatpaks, rpm-ostree |\n| `ujust logs-this-boot` | Current boot journal logs |\n| `ujust logs-last-boot` | Previous boot journal logs |\n| `ujust get-logs` | Upload logs to pastebin |\n| `ujust device-info` | Upload device info to pastebin |\n| `ujust check-idle-power-draw` | Measure idle power with powerstat |\n| `ujust check-local-overrides` | Compare /usr/etc vs /etc |\n| `ujust benchmark` | 1-minute stress test |\n| `ujust bazzite-cli` | Toggle Bluefin CLI experience |\n\n## Updates\n\n### Update System\n\n```bash\n# Full system update (flatpaks, containers, rpm-ostree)\nujust update\n\n# Same as update\nujust upgrade\n```\n\nUses `topgrade` to update:\n- Flatpak applications\n- Podman containers\n- rpm-ostree packages\n- System components\n\n### View Changelogs\n\n```bash\n# Stable release notes\nujust changelogs\n\n# Pre-release/testing notes\nujust changelogs-testing\n```\n\n### Automatic Updates\n\n```bash\n# Toggle uupd.timer (automatic updates)\nujust toggle-updates\n```\n\n## Cleanup\n\n```bash\n# Clean podman images, flatpaks, rpm-ostree content\nujust clean-system\n```\n\nRemoves:\n- Unused podman images\n- Orphaned flatpak runtimes\n- Old rpm-ostree deployments\n\n## Logging\n\n### View Logs\n\n```bash\n# Current boot\nujust logs-this-boot\n\n# Previous boot (useful after crash)\nujust logs-last-boot\n```\n\n### Share Logs\n\n```bash\n# Upload system logs to pastebin for support\nujust get-logs\n\n# Upload device info to pastebin\nujust device-info\n```\n\nReturns a pastebin URL to share with support.\n\n## Diagnostics\n\n### Power Measurement\n\n```bash\n# Measure idle power draw\nujust check-idle-power-draw\n```\n\nUses `powerstat` to measure system power consumption.\n\n### Local Overrides\n\n```bash\n# Compare /usr/etc vs /etc\nujust check-local-overrides\n```\n\nShows files in /etc that override /usr/etc defaults.\n\n### Benchmarking\n\n```bash\n# 1-minute stress test\nujust benchmark\n```\n\nUses `stress-ng` to benchmark CPU, memory, and I/O.\n\n## CLI Experience\n\n```bash\n# Toggle Bluefin-style CLI (bling)\nujust bazzite-cli\n```\n\nEnables/disables enhanced CLI features from Bluefin.\n\n## Common Workflows\n\n### Weekly Maintenance\n\n```bash\n# Update everything\nujust update\n\n# Clean unused resources\nujust clean-system\n```\n\n### Troubleshooting Crashes\n\n```bash\n# Check previous boot logs\nujust logs-last-boot\n\n# Share logs for support\nujust get-logs\n```\n\n### Performance Testing\n\n```bash\n# Run benchmark\nujust benchmark\n\n# Check power draw\nujust check-idle-power-draw\n```\n\n## Troubleshooting\n\n### Update Fails\n\n**Check:** Network connectivity, disk space\n\n```bash\n# Manual rpm-ostree update\nrpm-ostree upgrade\n\n# Check for pending changes\nrpm-ostree status\n```\n\n### Logs Too Long\n\n**Use journalctl filters:**\n\n```bash\n# Last 100 lines\njournalctl -n 100\n\n# Since specific time\njournalctl --since \"1 hour ago\"\n\n# Specific unit\njournalctl -u <service-name>\n```\n\n## Cross-References\n\n- **bazzite-ai:configure** - Service configuration\n- **bazzite:boot** - Boot and GRUB settings\n- **bazzite:storage** - Disk management and snapshots\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"update bazzite\", \"upgrade system\", \"system update\"\n- \"view changelog\", \"release notes\", \"what's new\"\n- \"clean up system\", \"free disk space\", \"remove unused\"\n- \"view logs\", \"system logs\", \"check journal\"\n- \"share logs\", \"upload logs\", \"support pastebin\"\n- \"power consumption\", \"idle power\", \"battery\"\n- \"benchmark\", \"stress test\", \"performance\"\n- \"automatic updates\", \"disable updates\""
              },
              {
                "name": "virtualization",
                "description": "GPU passthrough and virtualization for Bazzite. KVM/VFIO setup, Looking Glass (kvmfr),\nUSB hotplug for VMs, and libvirt configuration. Use when users need GPU passthrough\nor advanced virtualization features.\n",
                "path": "bazzite/skills/virtualization/SKILL.md",
                "frontmatter": {
                  "name": "virtualization",
                  "description": "GPU passthrough and virtualization for Bazzite. KVM/VFIO setup, Looking Glass (kvmfr),\nUSB hotplug for VMs, and libvirt configuration. Use when users need GPU passthrough\nor advanced virtualization features.\n"
                },
                "content": "# Virtualization - Bazzite GPU Passthrough & KVM\n\n## Overview\n\nAdvanced virtualization features for Bazzite including KVM, VFIO GPU passthrough, Looking Glass (kvmfr), and USB hotplug for VMs.\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `ujust setup-virtualization` | Main virtualization setup |\n| `ujust setup-virtualization virt-on` | Enable KVM/libvirt |\n| `ujust setup-virtualization virt-off` | Disable KVM/libvirt |\n| `ujust setup-virtualization vfio-on` | Enable VFIO passthrough |\n| `ujust setup-virtualization vfio-off` | Disable VFIO passthrough |\n| `ujust setup-virtualization kvmfr` | Setup Looking Glass |\n| `ujust setup-virtualization usbhp-on` | Enable USB hotplug |\n| `ujust setup-virtualization usbhp-off` | Disable USB hotplug |\n\n## KVM/Libvirt\n\n### Enable Virtualization\n\n```bash\n# Enable KVM and libvirt\nujust setup-virtualization virt-on\n```\n\n**Enables:**\n- libvirtd service\n- User permissions for VMs\n- Default network\n- QEMU/KVM backend\n\n### Disable Virtualization\n\n```bash\n# Disable KVM and libvirt\nujust setup-virtualization virt-off\n```\n\n## VFIO GPU Passthrough\n\n### Enable VFIO\n\n```bash\n# Enable VFIO for GPU passthrough\nujust setup-virtualization vfio-on\n```\n\n**What VFIO does:**\n- Isolates GPU from host\n- Passes GPU directly to VM\n- Near-native GPU performance in VM\n\n**Requirements:**\n- Two GPUs (or iGPU + dGPU)\n- IOMMU support (VT-d or AMD-Vi)\n- Supported GPU\n\n### Disable VFIO\n\n```bash\n# Disable VFIO passthrough\nujust setup-virtualization vfio-off\n```\n\nReturns GPU to host control.\n\n## Looking Glass (kvmfr)\n\n### Setup kvmfr\n\n```bash\n# Setup Looking Glass shared memory\nujust setup-virtualization kvmfr\n```\n\n**Looking Glass:**\n- Zero-copy GPU framebuffer sharing\n- Near-zero latency display\n- No GPU encoding needed\n- Mouse/keyboard passthrough\n\n**Requirements:**\n- Windows VM with GPU passthrough\n- Looking Glass host app\n- IVSHMEM device in VM\n\n### Using Looking Glass\n\n1. **Host:** Run `looking-glass-client`\n2. **VM:** Run Looking Glass host service\n3. Connect via shared memory\n\n## USB Hotplug\n\n### Enable USB Hotplug\n\n```bash\n# Enable USB device hotplug for VMs\nujust setup-virtualization usbhp-on\n```\n\nAllows:\n- Hot-add USB devices to running VMs\n- Dynamic USB device assignment\n- No VM restart needed\n\n### Disable USB Hotplug\n\n```bash\nujust setup-virtualization usbhp-off\n```\n\n## Common Workflows\n\n### Basic VM Setup\n\n```bash\n# Enable virtualization\nujust setup-virtualization virt-on\n\n# Use virt-manager for GUI\nvirt-manager\n```\n\n### Gaming VM with GPU Passthrough\n\n```bash\n# 1. Enable VFIO\nujust setup-virtualization vfio-on\n\n# 2. Reboot (GPU now isolated)\nsystemctl reboot\n\n# 3. Create VM with GPU\n# In virt-manager: Add PCI device (GPU)\n\n# 4. Optional: Setup Looking Glass\nujust setup-virtualization kvmfr\n```\n\n### Dynamic USB Access\n\n```bash\n# Enable USB hotplug\nujust setup-virtualization usbhp-on\n\n# In running VM:\n# - Right-click VM in virt-manager\n# - Add Hardware > USB Host Device\n# - Select device\n```\n\n## IOMMU Groups\n\n### Check IOMMU\n\n```bash\n# Verify IOMMU enabled\ndmesg | grep -i iommu\n\n# List IOMMU groups\nfor d in /sys/kernel/iommu_groups/*/devices/*; do\n  n=${d#*/iommu_groups/*}; n=${n%%/*}\n  printf 'IOMMU Group %s ' \"$n\"\n  lspci -nns \"${d##*/}\"\ndone\n```\n\n### GPU IOMMU Group\n\n```bash\n# Find GPU group\nlspci -nn | grep -i nvidia\n# or\nlspci -nn | grep -i amd\n```\n\nIdeal: GPU alone in IOMMU group. If not, may need ACS override patch.\n\n## VM Management\n\n### Virsh Commands\n\n```bash\n# List VMs\nvirsh list --all\n\n# Start VM\nvirsh start <vm-name>\n\n# Shutdown VM\nvirsh shutdown <vm-name>\n\n# Force off\nvirsh destroy <vm-name>\n```\n\n### GUI Management\n\n```bash\n# virt-manager (GUI)\nvirt-manager\n\n# GNOME Boxes (simpler)\ngnome-boxes\n```\n\n## Troubleshooting\n\n### VFIO Not Binding GPU\n\n**Check IOMMU:**\n\n```bash\ndmesg | grep -i iommu\n# Should show \"IOMMU enabled\"\n```\n\n**Enable in BIOS:**\n- Intel: VT-d\n- AMD: AMD-Vi / IOMMU\n\n**Check binding:**\n\n```bash\nlspci -nnk | grep -A3 \"VGA\\|Audio\"\n# Kernel driver should be vfio-pci\n```\n\n### Looking Glass Black Screen\n\n**Check IVSHMEM:**\n\n```bash\n# In VM, verify IVSHMEM device exists\n# Check Looking Glass host logs\n```\n\n**Verify shared memory:**\n\n```bash\nls -la /dev/shm/looking-glass\n```\n\n### USB Device Not Passing Through\n\n**Check permissions:**\n\n```bash\n# User in libvirt group?\ngroups $USER\n```\n\n**Check device:**\n\n```bash\nlsusb\n# Identify device ID\n```\n\n### VM Won't Start After VFIO\n\n**GPU still attached to host:**\n\n```bash\n# Verify VFIO binding\nlspci -nnk | grep -A3 \"VGA\"\n# Should show: Kernel driver in use: vfio-pci\n```\n\n**Reboot may be needed after vfio-on.**\n\n## Cross-References\n\n- **bazzite-ai:vm** - QCOW2 VM management\n- **bazzite:gpu** - GPU driver configuration\n- **bazzite-ai:configure** - libvirtd service\n\n## When to Use This Skill\n\nUse when the user asks about:\n- \"GPU passthrough\", \"VFIO\", \"pass GPU to VM\"\n- \"Looking Glass\", \"kvmfr\", \"VM display\"\n- \"KVM\", \"libvirt\", \"virtualization\"\n- \"USB hotplug\", \"pass USB to VM\"\n- \"gaming VM\", \"Windows VM\", \"VM performance\"\n- \"IOMMU\", \"VT-d\", \"AMD-Vi\""
              }
            ]
          },
          {
            "name": "bazzite-ai",
            "description": "Skills for using Bazzite AI OS features via ujust commands",
            "source": "./bazzite-ai",
            "category": "productivity",
            "version": "1.0.0",
            "author": {
              "name": "atrawog"
            },
            "install_commands": [
              "/plugin marketplace add atrawog/bazzite-ai-plugins",
              "/plugin install bazzite-ai@bazzite-ai-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T13:36:51Z",
              "created_at": "2025-12-26T20:21:42Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "build",
                "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n",
                "path": "bazzite-ai-dev/skills/build/SKILL.md",
                "frontmatter": {
                  "name": "build",
                  "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n"
                },
                "content": "# Build - Unified Build System\n\n## Overview\n\nThe `build` command provides a unified interface for all bazzite-ai build operations:\n\n- OS container images\n- Pod container variants\n- VM images (QCOW2/RAW)\n- Live ISO installers\n- Push to registry\n- Sign with cosign\n\n**Smart Caching:** Automatically detects git branch and uses matching cache tag, ensuring local builds are compatible with GitHub Actions builds.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Build OS | `just build os` | Build OS container image |\n| Build pod | `just build pod nvidia` | Build specific pod variant |\n| Build all pods | `just build pod all` | Build all pod variants |\n| Build ISO | `just build iso` | Build live ISO installer |\n| Build QCOW2 | `just build qcow2` | Build QCOW2 VM image |\n| Build RAW | `just build raw` | Build RAW VM image |\n| Generate lock | `just build pixi python` | Generate pixi.lock |\n| Push OS | `just build push os` | Push OS image to registry |\n| Push pod | `just build push pod nvidia` | Push pod to registry |\n| Sign OS | `just build sign os` | Sign OS image with cosign |\n| Sign pod | `just build sign pod nvidia` | Sign pod with cosign |\n| Show status | `just build status` | Show cache/build status |\n\n## Pod Variants\n\n| Variant | Image Name | Description |\n|---------|------------|-------------|\n| `base` | `bazzite-ai-pod` | CPU-only development |\n| `nvidia` | `bazzite-ai-pod-nvidia` | GPU compute with CUDA |\n| `nvidia-python` | `bazzite-ai-pod-nvidia-python` | NVIDIA + ML packages |\n| `jupyter` | `bazzite-ai-pod-jupyter` | JupyterLab + ML stack |\n| `ollama` | `bazzite-ai-pod-ollama` | LLM inference |\n| `comfyui` | `bazzite-ai-pod-comfyui` | Stable Diffusion UI |\n| `devops` | `bazzite-ai-pod-devops` | AWS/kubectl/Helm tools |\n| `githubrunner` | `bazzite-ai-pod-githubrunner` | CI/CD pipeline |\n\n## Smart Cache Strategy\n\nThe build system automatically detects your git branch and uses the appropriate cache tag to maximize cache reuse between local and CI builds:\n\n| Branch | Cache Tag | Build Tag |\n|--------|-----------|-----------|\n| `main` | `stable` | `stable` |\n| `testing` | `testing` | `testing` |\n| Other | `{branch}` | `{branch}` |\n\nThis ensures that when you build locally on the `testing` branch, you pull cache layers from the `:testing` images pushed by GitHub Actions.\n\n## Environment Variables\n\nFor CI integration, the following environment variables are supported:\n\n| Variable | Purpose |\n|----------|---------|\n| `COSIGN_PRIVATE_KEY` | Private key for signing with cosign |\n| `BUILD_LABELS` | Space-separated OCI labels to apply during build |\n| `BUILD_TAGS` | Space-separated tags to apply (overrides default) |\n| `BASE_IMAGE` | Override base image for pod builds (for digest pinning) |\n\n## Common Workflows\n\n### Build OS Image\n\n```bash\n# Build with branch-appropriate tag\njust build os\n\n# Build with custom tag\njust build os custom-tag\n```\n\n### Build Pods\n\n```bash\n# Interactive selection\njust build pod\n\n# Specific variant\njust build pod nvidia\n\n# All variants\njust build pod all\n```\n\n### Build VM/ISO\n\n```bash\n# Build QCOW2 VM image\njust build qcow2\n\n# Build live ISO\njust build iso\n\n# Build RAW image\njust build raw\n```\n\n### Push to Registry\n\n```bash\n# Push OS image\njust build push os\n\n# Push specific pod\njust build push pod nvidia\n\n# Push all pods\njust build push pod all\n```\n\n### Sign Images\n\n```bash\n# Sign OS image (requires COSIGN_PRIVATE_KEY env var)\nCOSIGN_PRIVATE_KEY=$KEY just build sign os\n\n# Sign pod\nCOSIGN_PRIVATE_KEY=$KEY just build sign pod nvidia\n```\n\n### Generate Pixi Locks\n\n```bash\n# Python variant\njust build pixi python\n\n# Jupyter variant\njust build pixi jupyter\n\n# All variants\njust build pixi all\n```\n\n## CI Integration\n\nThe build commands are designed for GitHub Actions integration:\n\n```yaml\n# Build, push, and sign in CI\n- name: Build and push OS\n  env:\n    BUILD_LABELS: ${{ steps.metadata.outputs.labels }}\n    COSIGN_PRIVATE_KEY: ${{ secrets.SIGNING_SECRET }}\n  run: |\n    just build os $TAG\n    just build push os $TAG\n    just build sign os $TAG\n\n# Build pod with base image digest\n- name: Build nvidia pod\n  env:\n    BASE_IMAGE: ghcr.io/owner/bazzite-ai-pod@${{ needs.base.outputs.digest }}\n  run: just build pod nvidia $TAG\n```\n\n## Output Images\n\nImages are tagged with the registry prefix:\n\n```\nghcr.io/atrawog/bazzite-ai:{tag}           # OS image\nghcr.io/atrawog/bazzite-ai-pod:{tag}       # Base pod\nghcr.io/atrawog/bazzite-ai-pod-nvidia:{tag} # NVIDIA pod\nghcr.io/atrawog/bazzite-ai-pod-comfyui:{tag} # ComfyUI pod\n```\n\n## Requirements\n\n- Podman installed and configured\n- Git repository cloned\n- Sufficient disk space (~10GB for OS, ~20GB for ISO)\n- Network access (pulls base images)\n- cosign installed (for signing)\n- Registry authentication (for push)\n\n## Troubleshooting\n\n### Build Fails with Cache Error\n\n**Symptom:** Cache layer not found\n\n**Cause:** Remote image not yet pushed for this branch\n\n**Fix:**\n\n```bash\n# Build without cache (first build on new branch)\n# Or check status to see cache state\njust build status\n```\n\n### Pod Build Fails with Base Image Missing\n\n**Symptom:** Cannot find base pod image\n\n**Cause:** Parent variant not built\n\n**Fix:**\n\n```bash\n# Build in order (base -> nvidia -> jupyter)\njust build pod base\njust build pod nvidia\njust build pod jupyter\n```\n\n### Push Fails with Authentication Error\n\n**Symptom:** unauthorized: authentication required\n\n**Cause:** Not logged into registry\n\n**Fix:**\n\n```bash\n# Login to GitHub Container Registry\npodman login ghcr.io\n```\n\n### Sign Fails\n\n**Symptom:** cosign not found or key not set\n\n**Cause:** cosign not installed or COSIGN_PRIVATE_KEY not set\n\n**Fix:**\n\n```bash\n# Check cosign is installed\nwhich cosign\n\n# Set signing key\nexport COSIGN_PRIVATE_KEY=\"$(cat cosign.key)\"\n```\n\n### CUDA Test Fails\n\n**Symptom:** nvidia-smi not found\n\n**Cause:** No GPU available or CDI not configured\n\n**Fix:**\n\n```bash\n# Verify GPU on host\nnvidia-smi\n\n# Check CDI configuration\nls /etc/cdi/\n```\n\n## Cross-References\n\n- **Related Skills:** `clean` (cleanup build artifacts)\n- **System Commands:** `ujust jupyter`, `ujust ollama` (use built pods)\n- **Documentation:** See `Containerfile` for image layers\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"build os\", \"build image\", \"build container\"\n- \"build pod\", \"build nvidia\", \"build jupyter\", \"build comfyui\"\n- \"build iso\", \"build qcow2\", \"build vm\"\n- \"push os\", \"push pod\", \"push to registry\"\n- \"sign image\", \"cosign\", \"sign pod\"\n- \"pixi lock\", \"generate lock\"\n- \"just build\" (any build command)"
              },
              {
                "name": "clean",
                "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n",
                "path": "bazzite-ai-dev/skills/clean/SKILL.md",
                "frontmatter": {
                  "name": "clean",
                  "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n"
                },
                "content": "# Clean - Cleanup & Maintenance\n\n## Overview\n\nThe `clean` development commands remove build artifacts, caches, containers, and other temporary files to recover disk space and reset the development environment.\n\n**Key Concept:** This is a **development command** - run with `just` from the repository root, not `ujust`. It provides both interactive menu and non-interactive modes.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Interactive menu | `just clean` | Show cleanup options |\n| Status report | `just clean status` | Show what would be cleaned |\n| Safe cleanup | `just clean all` | Safe cleanup (preserves running containers) |\n| Nuclear cleanup | `just clean nuke` | NUCLEAR: destroy everything (requires NUKE confirmation) |\n| Podman prune | `just clean podman` | Full podman system prune |\n| Images | `just clean images` | Dangling images only |\n| All images | `just clean images all` | All unused images |\n| Build cache | `just clean images build-cache` | Podman builder cache |\n| Containers | `just clean containers` | Stopped containers |\n| Runners | `just clean runners` | Stop/restart GitHub runners |\n| VMs | `just clean vm` | VM images (libvirt + cache) |\n| System | `just clean system` | Tmp files + journal |\n| Logs | `just clean logs` | Remove *.log files |\n| Docs | `just clean docs` | Remove site/ directory |\n| Output | `just clean output` | Remove output/ contents |\n| Cache menu | `just clean cache` | Cache cleanup submenu |\n| Pixi cache | `just clean cache pixi` | .pixi/ + ~/.cache/rattler |\n| Venv | `just clean cache venv` | venv/ directory |\n| Chunkhound | `just clean cache chunkhound` | .chunkhound/ directory |\n| Pip | `just clean cache pip` | ~/.cache/pip/ |\n| Pre-commit | `just clean cache precommit` | ~/.cache/pre-commit/ |\n| GitHub CLI | `just clean cache gh` | ~/.cache/gh/ |\n\n## Safe vs Nuclear Cleanup\n\n### Safe Cleanup (`just clean all`)\n\nSafe cleanup that preserves running containers and configurations:\n\n1. Stop GitHub runners\n2. Remove runner containers\n3. Remove stopped containers\n4. Remove buildah working containers\n5. Clean /var/tmp (buildah artifacts)\n6. Podman system prune\n7. Clean builder cache\n8. Prune unused images\n9. Remove build logs\n10. Remove docs output\n11. Remove build output\n12. Clean all caches\n13. Vacuum journal logs\n14. Prune volumes\n15. Restart GitHub runners\n\n**Use when:** You want to free disk space but keep your pod configurations intact.\n\n### Nuclear Cleanup (`just clean nuke`)\n\n**DESTROYS EVERYTHING** - requires typing 'NUKE' to confirm:\n\n- Removes ALL containers (running and stopped)\n- Removes ALL images\n- Removes ALL volumes\n- Removes ALL pod configurations\n- Removes ALL cached data\n- Cleans system caches\n\n**Use when:** You want a completely fresh start or are troubleshooting persistent issues.\n\n**Warning:** This will delete:\n\n- All pod configurations (you'll need to reconfigure)\n- All downloaded container images (will need to re-pull)\n- All model data if stored in containers\n- All runner configurations\n\n## Parameters\n\n```bash\njust clean [ACTION] [SUBOPTION]\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | See quick reference | Cleanup action |\n| `SUBOPTION` | Varies by action | Sub-action for nested menus |\n\n## Cleanup Actions\n\n### status\n\nShow what would be cleaned (dry-run):\n\n```bash\njust clean status\n```\n\n**Reports:**\n\n- Podman images/containers\n- System files (/var/tmp, journal)\n- Build artifacts (logs, docs, output)\n- Caches (pixi, venv, pip, etc.)\n\n### all\n\nSafe cleanup (15 steps):\n\n```bash\njust clean all\n```\n\n### nuke\n\nNuclear option (requires NUKE confirmation):\n\n```bash\njust clean nuke\n# Type 'NUKE' when prompted to confirm\n```\n\n### podman\n\nFull podman system prune:\n\n```bash\njust clean podman\n```\n\n**Removes:**\n\n- All unused images\n- Stopped containers\n- Unused volumes\n- Builder cache\n\n### images\n\nClean podman images:\n\n```bash\njust clean images              # Dangling only\njust clean images all          # All unused\njust clean images build-cache  # Builder cache\n```\n\n### containers\n\nRemove stopped containers:\n\n```bash\njust clean containers\n```\n\n### runners\n\nManage GitHub runners:\n\n```bash\njust clean runners stop   # Stop runners\njust clean runners start  # Start runners\n```\n\n### vm\n\nClean VM images:\n\n```bash\njust clean vm            # Interactive\njust clean vm libvirt    # Libvirt VMs\njust clean vm cache      # VM cache\n```\n\n### system\n\nSystem cleanup:\n\n```bash\njust clean system        # Interactive\njust clean system tmp    # Clean /var/tmp\njust clean system journal # Vacuum journal logs\n```\n\n### cache\n\nClean development caches:\n\n```bash\njust clean cache          # Interactive\njust clean cache pixi     # .pixi/ + ~/.cache/rattler\njust clean cache venv     # venv/\njust clean cache chunkhound # .chunkhound/\njust clean cache pip      # ~/.cache/pip/\njust clean cache precommit # ~/.cache/pre-commit/\njust clean cache gh       # ~/.cache/gh/\n```\n\n## Common Workflows\n\n### Check Before Cleanup\n\n```bash\n# See what would be cleaned\njust clean status\n\n# Then decide what to clean\njust clean podman\n```\n\n### Recover Disk Space\n\n```bash\n# Safe cleanup\njust clean all\n\n# Or targeted cleanup\njust clean images all\njust clean cache pixi\njust clean output\n```\n\n### Reset Build Environment\n\n```bash\n# Clean all caches and build artifacts\njust clean cache all\njust clean output\njust clean docs\n\n# Reinstall dependencies\njust docs-install\n```\n\n### Before Major Rebuild\n\n```bash\n# Clean containers and images\njust clean podman\n\n# Then rebuild\njust build os\n```\n\n### Complete Fresh Start\n\n```bash\n# Nuclear option - destroys everything\njust clean nuke\n# Type 'NUKE' to confirm\n\n# Reconfigure everything from scratch\nujust jupyter config\nujust ollama config\n```\n\n## Disk Space Targets\n\n| Target | Typical Size | Command |\n|--------|--------------|---------|\n| Podman images | 10-50GB | `clean podman` |\n| Builder cache | 1-10GB | `clean images build-cache` |\n| /var/tmp | 1-5GB | `clean system tmp` |\n| Journal logs | 100MB-1GB | `clean system journal` |\n| Pixi cache | 1-5GB | `clean cache pixi` |\n| Output/ | 1-20GB | `clean output` |\n\n## Troubleshooting\n\n### Cleanup Fails with Permission Error\n\n**Symptom:** Cannot remove files in output/ or /var/tmp\n\n**Fix:**\n\n```bash\n# Fix permissions\nsudo chown -R $USER:$USER output/\n\n# For /var/tmp\nsudo rm -rf /var/tmp/buildah*\n```\n\n### Podman Prune Doesn't Free Space\n\n**Symptom:** Images still present after prune\n\n**Cause:** Containers referencing images\n\n**Fix:**\n\n```bash\n# Stop and remove all containers first\njust clean containers\njust clean runners stop\n\n# Then prune\njust clean podman\n```\n\n### GitHub Runners Won't Restart\n\n**Symptom:** Runners fail to start after cleanup\n\n**Cause:** Configuration lost or token expired\n\n**Fix:**\n\n```bash\n# Re-authenticate\njust gh-login\n\n# Reconfigure runners\nujust runners config <REPO_URL> 1\n```\n\n## Cross-References\n\n- **Related Skills:** `pods` (build pods), `vms` (build VMs), `docs` (build docs)\n- **GitHub Runners:** `ujust runners` (runner management)\n- **Disk Analysis:** `just clean status`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"clean up\", \"cleanup\", \"free disk space\"\n- \"remove containers\", \"prune images\"\n- \"clean cache\", \"clear cache\"\n- \"just clean\", \"clean podman\"\n- \"disk full\", \"out of space\"\n- \"reset environment\", \"fresh start\"\n- \"nuclear cleanup\", \"destroy everything\""
              },
              {
                "name": "test",
                "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n",
                "path": "bazzite-ai-dev/skills/test/SKILL.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n"
                },
                "content": "# Test - Overlay Testing Management\n\n## Overview\n\nThe `test` command manages overlay testing sessions for bazzite-ai development. It creates symlinks from the repository to `/usr/share/bazzite-ai/just/`, allowing live editing of justfiles without rebuilding the OS image.\n\n**Key Concept:** On immutable OSTree systems (Bazzite-AI, Silverblue), `/usr` is read-only. Overlay mode temporarily unlocks it. On traditional systems (Fedora, CentOS), symlinks provide the same live-editing capability.\n\n**Command Prefix:**\n- `just test` - Development mode (from repository root, any Linux system)\n- `ujust test` - Installed mode (on bazzite-ai system with test.just installed)\n\nThe Quick Reference shows `ujust` commands (installed mode). The Common Workflows section shows `just` commands (development mode from repo root).\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Enable overlay | `ujust test overlay enable` | Bootstrap overlay testing session |\n| Check status | `ujust test overlay check` | Show current overlay/symlink status |\n| Refresh | `ujust test overlay refresh` | Regenerate 60-custom.just after changes |\n| VM testing | `ujust test vm` | VM testing submenu |\n| Install testing | `ujust test install` | Test install commands |\n| Install all | `ujust test install all` | Test all install commands |\n| System info | `ujust test info` | Show detailed system info |\n| Help | `ujust test help` | Show usage help |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust test ACTION=\"\" SUBACTION=\"\" ARGS...\n\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | `overlay`, `vm`, `install`, `info`, `help` | Primary action |\n| `SUBACTION` | `enable`, `check`, `refresh` (for overlay) | Subaction |\n| `ARGS` | varies | Additional arguments for vm/install |\n\n### Rule of Intent\n\nWhen `ACTION` is provided, the command runs non-interactively. Without it, an interactive menu appears.\n\n## Overlay Subcommands\n\n### Enable Overlay\n\n```bash\nujust test overlay enable\n\n```\n\n1. Activates overlay mode (OSTree) or creates symlinks (traditional)\n2. Detects repository location automatically\n3. Sets up symlinks to `/usr/share/bazzite-ai/just/`\n4. Generates `60-custom.just` import file\n5. Requires sudo (handles internally)\n\n### Check Status\n\n```bash\nujust test overlay check\n\n```\n\nShows current status:\n\n- **Immutable OS**: Whether overlay mode is active\n\n- **Traditional OS**: Whether symlinks are configured\n\n- Target repository path\n\n### Refresh Overlay\n\n```bash\nujust test overlay refresh\n\n```\n\nUse after:\n\n- Adding new `.just` files\n\n- Removing `.just` files\n\n- Modifying the generator script\n\nRegenerates `60-custom.just` without full restart.\n\n## VM Testing\n\n```bash\nujust test vm              # Interactive VM test menu\nujust test vm list         # List available VM tests\nujust test vm <name>       # Run specific VM test\n\n```\n\nDelegates to the VM testing harness for testing in virtual machines.\n\n## Install Testing\n\n```bash\nujust test install         # Interactive install test menu\nujust test install all     # Test all install commands\nujust test install <name>  # Test specific install command\n\n```\n\nTests install commands for validation.\n\n## Common Workflows\n\n### Initial Development Setup\n\n```bash\n# 1. Clone repository\ngit clone <repo-url> && cd bazzite-ai\n\n# 2. Enable overlay testing (one-time)\njust test overlay enable\n\n# 3. Make changes to justfiles\nvim just/bazzite-ai/my-feature.just\n\n# 4. Test immediately with ujust\nujust my-feature\n\n# 5. If adding new files, refresh\njust test overlay refresh\n\n```\n\n### After Reboot (Immutable OS Only)\n\n```bash\n# Overlay resets on reboot - re-enable\njust test overlay enable\n\n# Your git commits persist, overlay changes don't\n\n```\n\n### Testing a New Command\n\n```bash\n# 1. Create/edit the justfile\nvim just/bazzite-ai/new-command.just\n\n# 2. Refresh to pick up new file\njust test overlay refresh\n\n# 3. Test the command\nujust new-command\n\n```\n\n## OS Type Detection\n\n| OS Type | Detection | Overlay Method |\n|---------|-----------|----------------|\n| Immutable (OSTree) | `/run/ostree-booted` exists | `rpm-ostree` overlay |\n| Traditional | No OSTree marker | Symlinks only |\n\n## Troubleshooting\n\n### Overlay Not Active After Enable\n\n**Symptom:** `ujust test overlay check` shows \"Normal immutable mode\"\n\n**Cause:** Overlay activation failed or needs reboot\n\n**Fix:**\n\n```bash\n# Check if rpm-ostree unlock succeeded\nsudo rpm-ostree status | grep -i unlock\n\n# If not, try manual unlock\nsudo rpm-ostree usroverlay\n\n```\n\n### Symlinks Not Working\n\n**Symptom:** Changes to justfiles not reflected in `ujust` output\n\n**Cause:** Symlinks not properly created or 60-custom.just not regenerated\n\n**Fix:**\n\n```bash\n# Check symlink status\nls -la /usr/share/bazzite-ai/just/\n\n# Re-enable overlay\njust test overlay enable\n\n# Refresh imports\njust test overlay refresh\n\n```\n\n### Command Not Found After Adding File\n\n**Symptom:** New recipe not available in `ujust --list`\n\n**Cause:** 60-custom.just needs regeneration\n\n**Fix:**\n\n```bash\njust test overlay refresh\n\n```\n\n### Permission Denied\n\n**Symptom:** `sudo: a terminal is required`\n\n**Cause:** Running in non-interactive mode without passwordless sudo\n\n**Fix:**\n\n```bash\n# Enable passwordless sudo first\nujust config passwordless-sudo enable\n\n# Then retry\njust test overlay enable\n\n```\n\n## Cross-References\n\n- **Related Skills:** `install` (for testing install commands), `vm` (for VM testing)\n\n- **Configuration:** `ujust config passwordless-sudo enable` for sudo access\n\n- **Documentation:** [Overlay Testing Architecture](./references/overlay-architecture.md)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"enable overlay\", \"start testing session\", \"development mode\"\n\n- \"test my changes\", \"live reload justfiles\"\n\n- \"overlay not working\", \"symlinks not configured\"\n\n- \"refresh overlay\", \"pick up new files\"\n\n- \"VM testing\", \"test in VM\"\n\n- \"test install commands\""
              },
              {
                "name": "dpo",
                "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n",
                "path": "bazzite-ai-jupyter/skills/dpo/SKILL.md",
                "frontmatter": {
                  "name": "dpo",
                  "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n"
                },
                "content": "# Direct Preference Optimization (DPO)\n\n## Overview\n\nDPO learns from preference pairs (chosen vs rejected responses) without training an explicit reward model. It directly optimizes the policy using the Bradley-Terry preference model, making it simpler than RLHF while achieving comparable results. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `DPOTrainer` | Trainer for preference optimization |\n| `DPOConfig` | Training hyperparameters |\n| `beta` | Temperature for implicit reward (0.1 typical) |\n| `learning_rate` | 5e-6 (most conservative of RL methods) |\n| `ref_model` | Reference model for KL constraint |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## DPO Concepts\n\n### How DPO Works\n\n1. Given prompt + chosen response + rejected response\n2. Compute log-probabilities under policy and reference\n3. Optimize policy to increase P(chosen) / P(rejected) ratio\n4. Beta controls how strongly to enforce preference\n\n### Key Differences from RLHF\n\n| Aspect | DPO | RLHF |\n|--------|-----|------|\n| Reward Model | Implicit | Explicit |\n| Training | Single stage | Multi-stage |\n| Complexity | Simpler | More complex |\n| Compute | Lower | Higher |\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is when a function calls itself with a simpler version of the problem, including a base case to stop.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### From Comparison Data\n\n```python\ndef format_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"question\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"better_response\"],\n        \"rejected\": sample[\"worse_response\"],\n    }\n\ndataset = raw_dataset.map(format_preferences)\n```\n\n### Thinking Quality Preference Pairs\n\nFor thinking models, create preference pairs based on reasoning quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller, similar pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\nWhat's needed for it to work? A base case to stop the recursion.\n</think>\n\nRecursion is a programming technique where a function calls itself to solve a problem by breaking it into smaller, similar subproblems. For example, calculating factorial: n! = n * (n-1)!. Every recursive solution needs a base case to prevent infinite loops.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\nI can verify: 42 - 15 = 27. Correct!\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n    {\n        \"prompt\": \"Explain the difference between TCP and UDP.\",\n        \"chosen\": \"\"\"<think>\nWhat are TCP and UDP? They're network transport protocols.\nWhat's the key difference? TCP is connection-oriented, UDP is connectionless.\nWhat does that mean practically?\n- TCP: Reliable, ordered delivery with acknowledgments\n- UDP: Fast, no guarantees, better for streaming\nWhen would you use each?\n- TCP: File transfer, web browsing, email\n- UDP: Video streaming, gaming, DNS\n</think>\n\nTCP is a connection-oriented protocol that guarantees reliable, ordered delivery through acknowledgments and retransmission. UDP is connectionless, offering faster but unreliable delivery without guarantees. Use TCP for reliability (file transfers, web), UDP for speed (streaming, gaming).\"\"\",\n        \"rejected\": \"TCP is reliable, UDP is not.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n\ndef format_thinking_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"chosen\"],\n        \"rejected\": sample[\"rejected\"],\n    }\n\ndataset = dataset.map(format_thinking_preferences)\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for DPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## DPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import DPOConfig\n\ndpo_config = DPOConfig(\n    output_dir=\"./dpo_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=5e-6,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    beta=0.1,\n    max_length=512,\n    max_prompt_length=256,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.1-0.5 | Implicit reward temperature |\n| `learning_rate` | 1e-6 to 5e-6 | Lower than SFT |\n| `max_length` | 512-1024 | Max combined length |\n| `max_prompt_length` | 256-512 | Max prompt length |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import DPOTrainer\n\ntrainer = DPOTrainer(\n    model=model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n### With Reference Model\n\n```python\n# For stronger KL constraint\nref_model, _ = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n```\n\n## Beta Selection Guide\n\n| Beta | Use Case |\n|------|----------|\n| 0.01 | Weak preference signal |\n| 0.1 | Standard (recommended) |\n| 0.3 | Strong preference enforcement |\n| 0.5+ | Very strong (may overfit) |\n\n## Troubleshooting\n\n### Chosen/Rejected Scores Similar\n\n**Symptom:** Model doesn't distinguish preferences\n\n**Fix:**\n- Increase `beta` for stronger signal\n- Train longer\n- Check data quality (clear preference differences)\n\n### Overfitting to Preferences\n\n**Symptom:** Model only outputs chosen-style responses\n\n**Fix:**\n- Lower `beta`\n- Use reference model\n- Add regularization\n\n### Low Accuracy\n\n**Symptom:** DPO accuracy metric stays low\n\n**Fix:**\n- Ensure chosen is genuinely better than rejected\n- Increase training steps\n- Check prompt formatting\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Set `ref_model=None` (uses implicit reference)\n- Reduce `max_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nDPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- You have preference data (chosen vs rejected)\n- Simpler pipeline than RLHF desired\n- No reward model available\n- Post-SFT alignment\n- Human preference learning\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before DPO\n- `bazzite-ai-jupyter:grpo` - Alternative with explicit rewards\n- `bazzite-ai-jupyter:rloo` - Alternative RL with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models (alternative to DPO)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "evaluation",
                "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n",
                "path": "bazzite-ai-jupyter/skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n"
                },
                "content": "# LLM Evaluation with Evidently.ai\n\n## Overview\n\nEvidently.ai provides tools for evaluating LLM outputs using descriptors (row-level metrics) and reports. It supports automated prompt optimization and LLM-as-a-Judge patterns for quality assessment.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `Dataset` | Wrapper for evaluation data |\n| `Descriptor` | Row-level score or label |\n| `Report` | Aggregate metrics |\n| `TextEvals` | Text quality metrics |\n| `LLMJudge` | LLM-based evaluation |\n| `PromptOptimizer` | Automated prompt tuning |\n\n## Basic Setup\n\n```python\nimport pandas as pd\nfrom evidently import Dataset, DataDefinition\nfrom evidently.descriptors import TextLength, Sentiment, WordCount\n\n# Sample data\ndata = [\n    {\"question\": \"What is Python?\", \"answer\": \"Python is a programming language.\"},\n    {\"question\": \"Explain AI.\", \"answer\": \"AI is artificial intelligence.\"},\n]\n\ndf = pd.DataFrame(data)\n\n# Define data structure\ndefinition = DataDefinition(text_columns=[\"question\", \"answer\"])\n\n# Create Evidently Dataset\neval_dataset = Dataset.from_pandas(df, data_definition=definition)\n```\n\n## Text Descriptors\n\n### Basic Metrics\n\n```python\nfrom evidently.descriptors import TextLength, WordCount, Sentiment\n\n# Add descriptors\neval_dataset.add_descriptors(descriptors=[\n    TextLength(column=\"answer\"),\n    WordCount(column=\"answer\"),\n    Sentiment(column=\"answer\")\n])\n\n# View results\neval_dataset.as_dataframe()\n```\n\n### Available Descriptors\n\n| Descriptor | Description |\n|------------|-------------|\n| `TextLength` | Character count |\n| `WordCount` | Word count |\n| `Sentiment` | Sentiment score (-1 to 1) |\n| `RegexMatch` | Regex pattern matching |\n| `Contains` | Substring presence |\n| `IsValidJSON` | JSON validity check |\n| `IsValidPython` | Python syntax check |\n\n## LLM-as-a-Judge\n\n### Binary Classification\n\n```python\nimport os\nfrom evidently.descriptors import LLMJudge\nfrom evidently.llm import OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Configure Ollama as provider\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Create judge\njudge = LLMJudge(\n    provider=provider,\n    template=\"Is this answer helpful? Answer YES or NO.\\n\\nQuestion: {question}\\nAnswer: {answer}\",\n    include_reasoning=True\n)\n\neval_dataset.add_descriptors(descriptors=[judge])\n```\n\n### Multi-Class Classification\n\n```python\nfrom evidently.descriptors import LLMJudge\n\njudge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Classify this query into one category: BOOKING, CANCELLATION, GENERAL.\n\nQuery: {query}\n\nCategory:\"\"\",\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"],\n    include_reasoning=True\n)\n```\n\n### Quality Scoring\n\n```python\nfrom evidently.descriptors import LLMJudge\n\nquality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Rate this code review on a scale of 1-5.\n\nCode Review: {review}\n\nScore (1-5):\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Prompt Optimization\n\n### Setup Optimizer\n\n```python\nfrom evidently.llm import PromptOptimizer, OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\noptimizer = PromptOptimizer(\n    provider=provider,\n    max_iterations=10\n)\n```\n\n### Binary Classification Optimization\n\n```python\n# Initial prompt template\ninitial_prompt = \"\"\"Classify if this code review is good or bad.\n\nReview: {review}\n\nAnswer (GOOD or BAD):\"\"\"\n\n# Define judge for evaluation\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"GOOD\", \"BAD\"]\n)\n\n# Run optimization\nbest_prompt = optimizer.optimize(\n    dataset=eval_dataset,\n    initial_template=initial_prompt,\n    target_column=\"label\",  # Ground truth column\n    judge=judge\n)\n\nprint(\"Best prompt found:\")\nprint(best_prompt)\n```\n\n### Multi-Class Optimization\n\n```python\ninitial_prompt = \"\"\"Classify this query.\n\nQuery: {query}\n\nCategory (BOOKING/CANCELLATION/GENERAL):\"\"\"\n\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"]\n)\n\nbest_prompt = optimizer.optimize(\n    dataset=dataset,\n    initial_template=initial_prompt,\n    target_column=\"category\",\n    judge=judge\n)\n```\n\n## Reports\n\n### Generate Report\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import TextDescriptorsDriftMetric\n\nreport = Report(metrics=[\n    TextDescriptorsDriftMetric(column=\"answer\")\n])\n\nreport.run(reference_data=reference_dataset, current_data=current_dataset)\nreport.show()\n```\n\n### Save Report\n\n```python\nreport.save_html(\"evaluation_report.html\")\nreport.save_json(\"evaluation_report.json\")\n```\n\n## Common Patterns\n\n### Evaluate RAG Quality\n\n```python\nfrom evidently.descriptors import LLMJudge, TextLength, Contains\n\n# Relevance judge\nrelevance_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer relevant to the question?\n\nQuestion: {question}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\n# Factuality judge\nfactuality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer factually accurate based on the context?\n\nContext: {context}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\neval_dataset.add_descriptors([\n    relevance_judge,\n    factuality_judge,\n    TextLength(column=\"answer\")\n])\n```\n\n### Compare Models\n\n```python\n# Evaluate model A\nmodel_a_dataset = run_inference(model_a, test_data)\nmodel_a_dataset.add_descriptors([quality_judge])\n\n# Evaluate model B\nmodel_b_dataset = run_inference(model_b, test_data)\nmodel_b_dataset.add_descriptors([quality_judge])\n\n# Compare\nprint(\"Model A average score:\", model_a_dataset.as_dataframe()[\"quality\"].mean())\nprint(\"Model B average score:\", model_b_dataset.as_dataframe()[\"quality\"].mean())\n```\n\n## Troubleshooting\n\n### Slow Evaluation\n\n**Symptom:** Evaluation takes too long\n\n**Fix:**\n\n- Reduce dataset size for initial testing\n- Use smaller/faster judge model\n- Batch requests where possible\n\n### Inconsistent Judgments\n\n**Symptom:** LLM judge gives inconsistent scores\n\n**Fix:**\n\n- Lower temperature (0.0-0.3)\n- Make prompt more specific\n- Add examples to prompt\n- Use structured output options\n\n### Optimization Not Improving\n\n**Symptom:** Prompt optimization stuck\n\n**Fix:**\n\n- Increase `max_iterations`\n- Try different initial prompts\n- Check ground truth labels are correct\n- Use more training examples\n\n## When to Use This Skill\n\nUse when:\n\n- Measuring LLM output quality\n- Comparing different prompts\n- Automating prompt engineering\n- Building evaluation pipelines\n- Monitoring LLM performance over time\n\n## Evaluating Thinking Models\n\nFor thinking models (Qwen3-Thinking), evaluate both thinking quality and response quality:\n\n```python\nthinking_quality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Evaluate the quality of reasoning in this response.\n\nQuestion: {question}\nResponse: {response}\n\nScore the THINKING quality (1-5):\n1 = No reasoning shown\n2 = Minimal reasoning\n3 = Some step-by-step thinking\n4 = Good reasoning with self-questioning\n5 = Excellent thorough reasoning\n\nScore:\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain for LLM calls\n- `bazzite-ai-jupyter:rag` - RAG evaluation patterns\n- `bazzite-ai-jupyter:sft` - Training thinking models\n- `bazzite-ai-jupyter:inference` - Thinking model parsing\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility"
              },
              {
                "name": "finetuning",
                "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n",
                "path": "bazzite-ai-jupyter/skills/finetuning/SKILL.md",
                "frontmatter": {
                  "name": "finetuning",
                  "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n"
                },
                "content": "# Model Fine-Tuning\n\n## Overview\n\nFine-tuning adapts a pre-trained LLM to specific tasks by training on task-specific data. This skill covers both manual PyTorch training and HuggingFace's high-level Trainer API.\n\n**Recommended**: For 2x faster training with less memory, use **Unsloth** (see `bazzite-ai-jupyter:sft`).\n\n## Quick Reference\n\n| Approach | Use Case | Speed |\n|----------|----------|-------|\n| **Unsloth + SFTTrainer** | **Recommended default** | **2x faster** |\n| PyTorch Manual | Full control, custom training | Baseline |\n| HuggingFace Trainer | Standard training, less code | Fast |\n| SFTTrainer | Instruction/chat fine-tuning | Fast |\n\n## Method Comparison\n\n| Method | Learning Rate | Use Case |\n|--------|---------------|----------|\n| SFT | 2e-4 | Instruction tuning (first step) |\n| GRPO | 1e-5 | RL with rewards |\n| DPO | 5e-6 | Preference learning |\n| RLOO | 1e-5 | RL with lower variance |\n| Reward | 1e-5 | Reward model training |\n\n## Unsloth Quickstart (Recommended)\n\n```python\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Apply LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train\ntrainer = SFTTrainer(\n    model=model, tokenizer=tokenizer, train_dataset=dataset,\n    args=SFTConfig(\n        output_dir=\"./output\",\n        max_steps=100,\n        learning_rate=2e-4,\n        bf16=is_bf16_supported(),\n        optim=\"adamw_8bit\",\n    ),\n)\ntrainer.train()\n```\n\nSee `bazzite-ai-jupyter:sft` for complete Unsloth patterns.\n\n## Dataset Preparation\n\n### Load from HuggingFace Hub\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"test\"]\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n```\n\n### Data Format\n\n```python\n# Example conversation format\nexample = train_data[0]\nprint(example[\"text\"])\n\n# Output:\n# ### Human: What is Python?\n# ### Assistant: Python is a programming language...\n```\n\n### Create Prompt Template\n\n```python\ndef build_prompt(instruction, response=None):\n    prompt = f\"### Human: {instruction}\\n### Assistant:\"\n    if response:\n        prompt += f\" {response}\"\n    return prompt\n\n# For training\ntrain_prompt = build_prompt(\"What is AI?\", \"AI is artificial intelligence.\")\n\n# For inference\ninference_prompt = build_prompt(\"What is AI?\")\n```\n\n## Tokenization\n\n### Setup Tokenizer\n\n```python\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Ensure pad token exists\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Tokenize Dataset\n\n```python\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\ntokenized_train = train_data.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_data.column_names\n)\n\ntokenized_train.set_format(\"torch\")\n```\n\n## PyTorch Training (Manual)\n\n### Setup Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n```\n\n### Training Configuration\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass TrainConfig:\n    batch_size: int = 4\n    learning_rate: float = 2e-5\n    num_epochs: int = 3\n    max_length: int = 512\n    warmup_steps: int = 100\n    weight_decay: float = 0.01\n    output_dir: str = \"./checkpoints\"\n\ncfg = TrainConfig()\n```\n\n### DataLoader\n\n```python\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    tokenized_train,\n    batch_size=cfg.batch_size,\n    shuffle=True\n)\n```\n\n### Optimizer and Scheduler\n\n```python\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.learning_rate,\n    weight_decay=cfg.weight_decay\n)\n\ntotal_steps = len(train_loader) * cfg.num_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=cfg.warmup_steps,\n    num_training_steps=total_steps\n)\n```\n\n### Training Loop\n\n```python\nfrom tqdm.auto import tqdm\n\nmodel.train()\ndevice = next(model.parameters()).device\n\nfor epoch in range(cfg.num_epochs):\n    total_loss = 0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in progress:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = input_ids.clone()\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        progress.set_postfix({\"loss\": loss.item()})\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n\n    # Save checkpoint\n    model.save_pretrained(f\"{cfg.output_dir}/epoch_{epoch+1}\")\n```\n\n## HuggingFace Trainer\n\n### TrainingArguments\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True,\n    fp16=True,  # Mixed precision\n)\n```\n\n### Create Trainer\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n)\n```\n\n### Train and Evaluate\n\n```python\n# Train\ntrain_result = trainer.train()\n\n# Save\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\n\n# Evaluate\nmetrics = trainer.evaluate()\nprint(metrics)\n```\n\n## SFTTrainer (Instruction Tuning)\n\n### Setup\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-5,\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    packing=False,  # Don't pack multiple samples\n)\n```\n\n### Train with SFTTrainer\n\n```python\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_data,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",  # Column with training text\n)\n\ntrainer.train()\ntrainer.save_model(\"./sft_model\")\n```\n\n## Evaluation\n\n### Evaluation Function\n\n```python\ndef evaluate(model, dataloader):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = input_ids.clone()\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            total_loss += outputs.loss.item()\n\n    return total_loss / len(dataloader)\n```\n\n### Perplexity\n\n```python\nimport math\n\neval_loss = evaluate(model, val_loader)\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity:.2f}\")\n```\n\n## Inference with Fine-Tuned Model\n\n```python\ndef generate_response(model, tokenizer, prompt, max_new_tokens=128):\n    model.eval()\n    device = next(model.parameters()).device\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test\nprompt = build_prompt(\"What is machine learning?\")\nresponse = generate_response(model, tokenizer, prompt)\nprint(response)\n```\n\n## Checkpointing\n\n### Save Checkpoint\n\n```python\n# Save model and tokenizer\nmodel.save_pretrained(\"./checkpoint\")\ntokenizer.save_pretrained(\"./checkpoint\")\n```\n\n### Load Checkpoint\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"./checkpoint\")\n```\n\n### Resume Training\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n)\n\ntrainer.train(resume_from_checkpoint=\"./checkpoint\")\n```\n\n## Hyperparameters Guide\n\n| Parameter | Typical Values | Notes |\n|-----------|----------------|-------|\n| `learning_rate` | 1e-5 to 5e-5 | Lower for larger models |\n| `batch_size` | 4, 8, 16 | Limited by GPU memory |\n| `epochs` | 1-5 | More for smaller datasets |\n| `warmup_steps` | 5-10% of total | Stabilizes early training |\n| `weight_decay` | 0.01-0.1 | Regularization |\n| `max_length` | 512, 1024, 2048 | Context window |\n\n## When to Use This Skill\n\nUse when:\n\n- Adapting LLM to specific domain/task\n- Improving model performance on your data\n- Creating instruction-following models\n- Need full control over training process\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Unsloth-optimized SFT (recommended)\n- `bazzite-ai-jupyter:grpo` - RL with reward functions\n- `bazzite-ai-jupyter:dpo` - Preference learning\n- `bazzite-ai-jupyter:rloo` - RL with lower variance\n- `bazzite-ai-jupyter:quantization` - Memory-efficient training\n- `bazzite-ai-jupyter:peft` - Parameter-efficient fine-tuning\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Architecture understanding"
              },
              {
                "name": "grpo",
                "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n",
                "path": "bazzite-ai-jupyter/skills/grpo/SKILL.md",
                "frontmatter": {
                  "name": "grpo",
                  "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n"
                },
                "content": "# Group Relative Policy Optimization (GRPO)\n\n## Overview\n\nGRPO is a reinforcement learning method for LLM alignment. It generates multiple completions per prompt, scores them with a reward function, and optimizes the policy to favor higher-reward responses using relative policy gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `GRPOTrainer` | RL trainer for policy optimization |\n| `GRPOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `beta` | KL penalty coefficient (0.1 typical) |\n| `num_generations` | Completions per prompt (2-4) |\n| `learning_rate` | 1e-5 (10x lower than SFT) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import GRPOConfig, GRPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Setting `ACCELERATE_MIXED_PRECISION` after imports may cause training issues.\n\n## GRPO Concepts\n\n### How GRPO Works\n\n1. Generate multiple completions for each prompt\n2. Score completions with reward function(s)\n3. Compute relative advantages within each group\n4. Update policy to favor higher-reward completions\n5. Apply KL penalty to prevent divergence from reference\n\n### Key Differences from PPO\n\n| Aspect | GRPO | PPO |\n|--------|------|-----|\n| Baseline | Group relative | Value function |\n| Critic | Not needed | Required |\n| Memory | Lower | Higher |\n| Stability | Good | Can be unstable |\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for GRPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Dataset Format\n\n```python\n# GRPO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"What is recursion?\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response length.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        if length < 5:\n            rewards.append(-1.0)\n        elif length < 50:\n            rewards.append(1.0)\n        else:\n            rewards.append(0.5)\n    return rewards\n```\n\n### LLM-as-Judge Reward\n\n```python\ndef llm_judge_reward(completions, prompts):\n    \"\"\"Use another LLM to score responses.\"\"\"\n    rewards = []\n    for prompt, completion in zip(prompts, completions):\n        score = judge_model.evaluate(prompt, completion)\n        rewards.append(score)\n    return rewards\n```\n\n### Rule-Based Reward\n\n```python\ndef format_reward(completions, prompts=None):\n    \"\"\"Reward proper formatting.\"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n        if completion.endswith(\".\"):\n            score += 0.5\n        if not completion.startswith(\" \"):\n            score += 0.5\n        rewards.append(score)\n    return rewards\n```\n\n### Composite Rewards\n\n```python\ndef combined_reward(completions, prompts):\n    \"\"\"Combine multiple reward signals.\"\"\"\n    length_scores = length_reward(completions)\n    format_scores = format_reward(completions)\n    return [0.5 * l + 0.5 * f for l, f in zip(length_scores, format_scores)]\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n### Multi-Objective Thinking Reward (Token-Based)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef comprehensive_thinking_reward(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Evaluate multiple aspects of thinking quality using token IDs.\n\n    Scoring breakdown:\n    - Has </think> token: +0.3\n    - Thinking depth (20+ tokens): +0.3\n    - Structured sentences: +0.2\n    - Self-questioning: +0.1\n    - Step-by-step reasoning: +0.1\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        score = 0.0\n\n        # Token-based boundary detection\n        if THINK_END_TOKEN_ID in comp_ids:\n            score += 0.3  # Has proper </think> token\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count\n\n            # Extract thinking content for text analysis\n            thinking = completion.split('</think>')[0]\n\n            # Depth (token count from IDs)\n            if thinking_length >= 20:\n                score += 0.3\n            elif thinking_length >= 10:\n                score += 0.2\n\n            # Structure (sentences in text)\n            sentences = thinking.count('.') + thinking.count('!')\n            if sentences >= 2:\n                score += 0.2\n\n            # Self-questioning\n            if '?' in thinking:\n                score += 0.1\n\n            # Step-by-step reasoning\n            if any(w in thinking.lower() for w in ['first', 'then', 'next', 'finally']):\n                score += 0.1\n        else:\n            score = -0.5  # Penalize missing </think> token\n\n        rewards.append(score)\n\n    return rewards\n```\n\n## GRPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import GRPOConfig\n\ngrpo_config = GRPOConfig(\n    output_dir=\"./grpo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_completion_length=128,\n    num_generations=4,\n    beta=0.1,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.01-0.1 | KL penalty strength |\n| `num_generations` | 2-8 | Completions per prompt |\n| `max_completion_length` | 64-256 | Generation length |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n\n## Training\n\n### Basic Training Loop\n\n```python\nfrom trl import GRPOTrainer\n\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=length_reward,\n)\n\ntrainer.train()\n```\n\n### Multiple Reward Functions\n\n```python\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=[length_reward, format_reward],\n    reward_weights=[0.5, 0.5],\n)\n```\n\n## Troubleshooting\n\n### Reward Hacking\n\n**Symptom:** Model exploits reward function (e.g., always outputs same length)\n\n**Fix:**\n- Add diversity penalties\n- Use multiple reward signals\n- Cap maximum reward\n\n### KL Divergence Too High\n\n**Symptom:** Policy diverges too far from reference\n\n**Fix:**\n- Increase `beta` (stronger KL penalty)\n- Reduce `learning_rate`\n- Fewer training steps\n\n### Training Instability\n\n**Symptom:** Loss spikes or NaN\n\n**Fix:**\n- Lower `learning_rate` to 5e-6\n- Reduce `num_generations` to 2\n- Check reward scale (should be roughly -1 to 1)\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2\n- Use gradient checkpointing\n- Reduce `max_completion_length`\n\n## Kernel Shutdown (Jupyter)\n\nGRPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Aligning models with human preferences\n- Optimizing for specific behaviors\n- Post-SFT refinement\n- Building reward-driven systems\n- Simpler alternative to PPO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before GRPO\n- `bazzite-ai-jupyter:dpo` - Simpler preference learning (no reward model)\n- `bazzite-ai-jupyter:rloo` - Alternative RL method with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models for GRPO\n- `bazzite-ai-jupyter:peft` - LoRA for efficient RL\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-ollama:api` - Reward model inference"
              },
              {
                "name": "inference",
                "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n",
                "path": "bazzite-ai-jupyter/skills/inference/SKILL.md",
                "frontmatter": {
                  "name": "inference",
                  "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n"
                },
                "content": "# Fast Inference\n\n## Overview\n\nUnsloth provides optimized inference through the vLLM backend, enabling 2x faster generation compared to standard HuggingFace inference. This skill covers fast inference setup, thinking model output parsing, and memory management.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `fast_inference=True` | Enable vLLM backend for 2x speedup |\n| `model.fast_generate()` | vLLM-accelerated generation |\n| `SamplingParams` | Control generation (temperature, top_p, etc.) |\n| `FastLanguageModel.for_inference()` | Merge LoRA adapters for inference |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\nimport torch\nimport vllm\nfrom vllm import SamplingParams\n```\n\n## Environment Verification\n\nBefore inference, verify your environment is correctly configured:\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nimport vllm\n\n# Check versions\nprint(f\"unsloth: {unsloth.__version__}\")\nprint(f\"vLLM: {vllm.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n```\n\n## Standard Inference (No vLLM)\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Prepare for inference (merges LoRA adapters if present)\nFastLanguageModel.for_inference(model)\n```\n\n### Generate Response\n\n```python\nmessages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode only new tokens\ninput_length = inputs[\"input_ids\"].shape[1]\nresponse = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\nprint(response)\n```\n\n## Fast Inference (vLLM Backend)\n\n### Load Model with Fast Inference\n\n```python\nfrom unsloth import FastLanguageModel\nfrom vllm import SamplingParams\n\nMODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    MODEL_NAME,\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,  # Enable vLLM backend\n)\n```\n\n### Fast Generate\n\n```python\nFastLanguageModel.for_inference(model)\n\nmessages = [{\"role\": \"user\", \"content\": \"What is 15 + 27? Show your thinking.\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nsampling_params = SamplingParams(\n    temperature=0.6,      # Recommended for thinking models\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,      # Increased for thinking + response\n)\n\n# Use fast_generate instead of generate\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\n\n# Extract output\nraw_output = outputs[0].outputs[0].text\noutput_token_ids = outputs[0].outputs[0].token_ids\nprint(raw_output)\n```\n\n### Sampling Parameters\n\n```python\nfrom vllm import SamplingParams\n\n# Conservative (factual responses)\nconservative = SamplingParams(\n    temperature=0.3,\n    top_p=0.9,\n    max_tokens=512,\n)\n\n# Balanced (general use)\nbalanced = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=1024,\n)\n\n# Creative (diverse outputs)\ncreative = SamplingParams(\n    temperature=0.9,\n    top_p=0.95,\n    top_k=50,\n    max_tokens=2048,\n)\n\n# Thinking models (allow long reasoning)\nthinking = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,  # Extra space for <think> content\n)\n```\n\n## Thinking Model Output Parsing\n\nQwen3-Thinking models use `<think>...</think>` tags to separate reasoning from final responses. Use token-based parsing for accuracy.\n\n### Token-Based Parsing (Recommended)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"\n    Parse thinking model output using token ID boundary.\n\n    With Thinking models + add_generation_prompt=True:\n    - Template adds <think> to prompt\n    - Model output starts with thinking content\n    - Model outputs </think> (token 151668) when done\n    - Final response follows </think>\n\n    Args:\n        token_ids: Output token IDs from generation\n        tokenizer: Model tokenizer\n\n    Returns:\n        tuple: (thinking_content, response_content)\n    \"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking_tokens = token_list[:end_idx]\n        response_tokens = token_list[end_idx + 1:]\n\n        thinking = tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n    else:\n        # No </think> found - model may still be thinking\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True).strip()\n        response = \"(Model did not complete thinking - increase max_tokens)\"\n\n    return thinking, response\n```\n\n### Usage Example\n\n```python\n# Generate with fast_inference\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\noutput_token_ids = outputs[0].outputs[0].token_ids\n\n# Parse thinking and response\nthinking, response = parse_thinking_response(output_token_ids, tokenizer)\n\nprint(\"=== THINKING ===\")\nprint(thinking)\nprint(\"\\n=== RESPONSE ===\")\nprint(response)\n```\n\n### Verification\n\n```python\n# Verify parsing worked correctly\nthink_tag_found = THINK_END_TOKEN_ID in list(output_token_ids)\nhas_thinking = bool(thinking) and \"did not complete\" not in response\nhas_response = bool(response) and \"did not complete\" not in response\n\nprint(f\"</think> token found: {'Yes' if think_tag_found else 'No'}\")\nprint(f\"Thinking extracted: {'Yes' if has_thinking else 'No'}\")\nprint(f\"Response extracted: {'Yes' if has_response else 'No'}\")\n\nif not think_tag_found:\n    print(\"Tip: Increase max_tokens in SamplingParams\")\n```\n\n## Batch Inference\n\n### Multiple Prompts\n\n```python\nprompts = [\n    \"What is recursion?\",\n    \"Explain machine learning in simple terms.\",\n    \"What is the difference between Python and JavaScript?\",\n]\n\n# Format all prompts\nformatted_prompts = [\n    tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": p}],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    for p in prompts\n]\n\n# Batch generate (vLLM handles parallelization)\nsampling_params = SamplingParams(temperature=0.6, max_tokens=512)\noutputs = model.fast_generate(formatted_prompts, sampling_params=sampling_params)\n\n# Process results\nfor i, output in enumerate(outputs):\n    print(f\"\\n=== Prompt {i+1} ===\")\n    print(f\"Q: {prompts[i]}\")\n    print(f\"A: {output.outputs[0].text}\")\n```\n\n## Memory Management\n\n### GPU Memory Monitoring\n\n```python\nimport subprocess\n\ndef measure_gpu_memory():\n    \"\"\"Measure current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\n# Usage\nprint(f\"GPU memory used: {measure_gpu_memory()} MB\")\n```\n\n### Memory Cleanup\n\n```python\nimport gc\nimport torch\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage after inference\ncleanup_memory()\nprint(f\"GPU memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Jupyter Kernel Shutdown (Critical for vLLM)\n\n**vLLM does NOT release GPU memory within a Jupyter session.** Kernel restart is required between model tests:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of notebooks that use `fast_inference=True`. Without kernel shutdown, loading a different model will fail with OOM.\n\n**Notebook pattern**: All finetuning notebooks end with a shutdown cell.\n\n## Model Loading Patterns\n\n### Pre-Quantized Models (Recommended)\n\n```python\n# Fast loading with pre-quantized models\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # Pre-quantized\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,\n)\n```\n\n### On-Demand Quantization\n\n```python\n# Quantize during loading (slower initial load)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize on load\n    fast_inference=True,\n)\n```\n\n### Post-Training Inference\n\n```python\n# After SFT/GRPO/DPO training\nFastLanguageModel.for_inference(model)  # Merge LoRA adapters\n\n# Then generate as normal\noutputs = model.generate(**inputs, max_new_tokens=512)\n```\n\n## Supported Models\n\n| Model | Path | Parameters | Use Case |\n|-------|------|------------|----------|\n| Qwen3-4B-Thinking | `unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit` | 4B | Reasoning, chain-of-thought |\n| Ministral-3B-Reasoning | `unsloth/Ministral-3-3B-Reasoning-2512` | 3B | Fast reasoning |\n| Qwen3-4B | `unsloth/Qwen3-4B-unsloth-bnb-4bit` | 4B | General instruction following |\n| Llama-3.2-3B | `unsloth/Llama-3.2-3B-Instruct-bnb-4bit` | 3B | General instruction following |\n\n## Troubleshooting\n\n### vLLM Not Available\n\n**Symptom:** `fast_inference=True` fails or falls back to standard inference\n\n**Fix:**\n```python\n# Check vLLM installation\nimport inspect\nsig = inspect.signature(FastLanguageModel.from_pretrained)\nif 'fast_inference' in sig.parameters:\n    print(\"fast_inference parameter available\")\nelse:\n    print(\"vLLM not available - using standard inference\")\n```\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory during inference\n\n**Fix:**\n- Use 4-bit quantization (`load_in_4bit=True`)\n- Reduce `max_seq_length`\n- Reduce `max_tokens` in SamplingParams\n- Use `cleanup_memory()` between batches\n\n### Incomplete Thinking\n\n**Symptom:** `</think>` token not found in output\n\n**Fix:**\n- Increase `max_tokens` in SamplingParams (try 2048+)\n- Check that model is a Thinking variant\n- Verify `add_generation_prompt=True` in chat template\n\n### GPU Memory Not Released\n\n**Symptom:** Memory stays high after inference\n\n**Fix:**\n- Call `cleanup_memory()`\n- Restart Jupyter kernel between model tests\n- Use `del model` then `cleanup_memory()`\n\n## When to Use This Skill\n\nUse when:\n- Running inference on fine-tuned models\n- Need fast batch inference\n- Working with thinking/reasoning models\n- Optimizing inference latency\n- Parsing chain-of-thought outputs\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Supervised fine-tuning (train before inference)\n- `bazzite-ai-jupyter:peft` - LoRA adapter loading\n- `bazzite-ai-jupyter:quantization` - Quantization options\n- `bazzite-ai-jupyter:transformers` - Transformer architecture background\n- `bazzite-ai-ollama:api` - Ollama deployment for production"
              },
              {
                "name": "langchain",
                "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n",
                "path": "bazzite-ai-jupyter/skills/langchain/SKILL.md",
                "frontmatter": {
                  "name": "langchain",
                  "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n"
                },
                "content": "# LangChain Framework\n\n## Overview\n\nLangChain is a framework for building LLM applications. It provides abstractions for prompts, models, chains, and output parsing that work with both local models (HuggingFace, Ollama) and cloud APIs (OpenAI, Anthropic).\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `ChatOpenAI` | Connect to Ollama (OpenAI-compatible) |\n| `HuggingFacePipeline` | Wrap local HuggingFace models |\n| `ChatHuggingFace` | Chat interface for HF models |\n| `PromptTemplate` | Single-string prompt formatting |\n| `ChatPromptTemplate` | Multi-message prompt formatting |\n| `PydanticOutputParser` | Structured output parsing |\n\n## Model Wrappers\n\n### Ollama via OpenAI-Compatible API\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",  # Required by library, ignored by Ollama\n    model=MODEL,\n    temperature=0.7,\n    max_tokens=150\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n### HuggingFace Local Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n\nHF_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n\n# 4-bit quantization for memory efficiency\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\n# Create pipeline\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=150,\n    return_full_text=False\n)\n\n# Wrap for LangChain\nllm = HuggingFacePipeline(pipeline=text_pipeline)\nchat_llm = ChatHuggingFace(llm=llm)\n```\n\n## LLM Methods\n\n### invoke() - Single Input\n\n```python\nresponse = llm.invoke(\"Tell me a fact about Mars.\")\nprint(response)\n```\n\n### batch() - Multiple Inputs\n\n```python\nprompts = [\"Tell me a joke\", \"Translate to German: Hello!\"]\nresults = llm.batch(prompts)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {result}\\n\")\n```\n\n### generate() - With Metadata\n\n```python\nresults = llm.generate([\"Where should I go for a Safari?\"])\n\nfor gen in results.generations:\n    print(gen[0].text)\n\n# Access token counts\nprint(results.llm_output)\n```\n\n### stream() - Token Streaming\n\n```python\nfor chunk in llm.stream(\"Tell me a story about a cat.\"):\n    print(chunk, end=\"\", flush=True)\n```\n\n## Prompt Templates\n\n### Basic PromptTemplate\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Explain {topic} in simple terms.\"\n)\n\nformatted = template.format(topic=\"quantum computing\")\nresponse = llm.invoke(formatted)\n```\n\n### ChatPromptTemplate\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful legal translator.\"),\n    (\"human\", \"Simplify this legal text: {legal_text}\")\n])\n\nmessages = chat_prompt.format_messages(legal_text=\"...\")\nresponse = chat_llm.invoke(messages)\n```\n\n## Few-Shot Learning\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define examples\nexamples = [\n    {\"input\": \"Legal term 1\", \"output\": \"Plain explanation 1\"},\n    {\"input\": \"Legal term 2\", \"output\": \"Plain explanation 2\"}\n]\n\n# Build few-shot prompt\nmessages = [\n    (\"system\", \"Translate legal terms to plain language.\")\n]\nfor ex in examples:\n    messages.append((\"human\", ex[\"input\"]))\n    messages.append((\"assistant\", ex[\"output\"]))\nmessages.append((\"human\", \"{new_input}\"))\n\nfew_shot_prompt = ChatPromptTemplate.from_messages(messages)\n```\n\n## Output Parsing\n\n### Pydantic Parser\n\n```python\nfrom pydantic import BaseModel, Field\nfrom langchain.output_parsers import PydanticOutputParser\n\nclass LegalClause(BaseModel):\n    parties: list[str] = Field(description=\"Parties involved\")\n    obligations: str = Field(description=\"Main obligations\")\n    conditions: str = Field(description=\"Key conditions\")\n\nparser = PydanticOutputParser(pydantic_object=LegalClause)\n\nprompt = PromptTemplate(\n    input_variables=[\"clause\"],\n    template=\"Parse this legal clause:\\n{clause}\\n\\n{format_instructions}\",\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\nformatted = prompt.format(clause=\"...\")\nresponse = llm.invoke(formatted)\nparsed = parser.parse(response)\n\nprint(parsed.parties)\nprint(parsed.obligations)\n```\n\n## Chaining\n\n### Sequential Chain (Pipe Syntax)\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\n# Define chains\ntemplate1 = \"Give a bullet point outline for a blog about {topic}\"\ntemplate2 = \"Write a blog post from this outline:\\n{outline}\"\n\nchain1 = PromptTemplate.from_template(template1) | llm\nchain2 = PromptTemplate.from_template(template2) | llm\n\n# Compose\nfull_chain = chain1 | chain2\n\nresult = full_chain.invoke({\"topic\": \"AI\"})\n```\n\n### Multi-Step Processing\n\n```python\ntemplate1 = \"Summarize this review:\\n{review}\"\ntemplate2 = \"Identify weaknesses:\\n{summary}\"\ntemplate3 = \"Create improvement plan:\\n{weaknesses}\"\n\nchain_1 = PromptTemplate.from_template(template1) | llm\nchain_2 = PromptTemplate.from_template(template2) | llm\nchain_3 = PromptTemplate.from_template(template3) | llm\n\nfull_chain = chain_1 | chain_2 | chain_3\nresult = full_chain.invoke(employee_review)\n```\n\n### Router Chain\n\n```python\nfrom langchain.chains.router import MultiPromptChain\n\nbeginner_template = \"Explain {input} simply for a child.\"\nexpert_template = \"Explain {input} technically for an expert.\"\n\nprompt_infos = [\n    {\"name\": \"beginner\", \"description\": \"For simple questions\", \"prompt_template\": beginner_template},\n    {\"name\": \"expert\", \"description\": \"For technical questions\", \"prompt_template\": expert_template}\n]\n\nchain = MultiPromptChain.from_prompts(llm, prompt_infos, verbose=True)\nresult = chain.invoke(\"How do Feynman diagrams work?\")\n```\n\n## Caching\n\n```python\nimport langchain\nfrom langchain.cache import SQLiteCache\n\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n\n# First call - hits LLM\nresponse1 = llm.invoke(\"What is Python?\")\n\n# Second call - uses cache (instant)\nresponse2 = llm.invoke(\"What is Python?\")\n```\n\n## Messages\n\n```python\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is 2+2?\"),\n    AIMessage(content=\"4\"),\n    HumanMessage(content=\"And times 3?\")\n]\n\nresponse = chat_llm.invoke(messages)\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Building LLM applications with structured workflows\n- Need prompt templating and variable substitution\n- Chaining multiple LLM calls together\n- Parsing structured output from LLMs\n- Working with both local and cloud models\n\n## Cross-References\n\n- `bazzite-ai-jupyter:rag` - RAG pipelines using LangChain\n- `bazzite-ai-jupyter:evaluation` - LLM evaluation\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility\n- `bazzite-ai-ollama:python` - Native Ollama Python library"
              },
              {
                "name": "peft",
                "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n",
                "path": "bazzite-ai-jupyter/skills/peft/SKILL.md",
                "frontmatter": {
                  "name": "peft",
                  "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n"
                },
                "content": "# Parameter-Efficient Fine-Tuning (PEFT)\n\n## Overview\n\nPEFT methods like LoRA train only a small number of adapter parameters instead of the full model, reducing memory by 10-100x while maintaining quality.\n\n## Quick Reference\n\n| Method | Memory | Speed | Quality |\n|--------|--------|-------|---------|\n| Full Fine-tune | High | Slow | Best |\n| LoRA | Low | Fast | Very Good |\n| QLoRA | Very Low | Fast | Good |\n| Unsloth | Very Low | 2x Faster | Good |\n\n## LoRA Concepts\n\n### How LoRA Works\n\n```\nOriginal weight matrix W (frozen):     d x k\nLoRA adapters A and B:                 d x r, r x k (where r << min(d,k))\n\nForward pass:\n  output = x @ W + x @ A @ B * (alpha / r)\n\nTrainable params: 2 * r * d  (instead of d * k)\n```\n\n### Memory Savings\n\n```python\ndef lora_savings(d, k, r):\n    original = d * k\n    lora = 2 * r * max(d, k)\n    reduction = (1 - lora / original) * 100\n    return reduction\n\n# Example: 4096 x 4096 matrix with rank 8\nprint(f\"Memory reduction: {lora_savings(4096, 4096, 8):.1f}%\")\n# Output: ~99.6% reduction\n```\n\n## Basic LoRA Setup\n\n### Configure LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=8,                          # Rank (capacity)\n    lora_alpha=16,                # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers\n    lora_dropout=0.05,            # Regularization\n    bias=\"none\",                  # Don't train biases\n    task_type=TaskType.CAUSAL_LM  # Task type\n)\n```\n\n### Apply to Model\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 4,194,304 || all params: 1,100,048,384 || trainable%: 0.38%\n```\n\n## LoRA Parameters\n\n### Key Parameters\n\n| Parameter | Values | Effect |\n|-----------|--------|--------|\n| `r` | 4, 8, 16, 32 | Adapter capacity |\n| `lora_alpha` | r to 2*r | Scaling (higher = stronger) |\n| `target_modules` | List | Which layers to adapt |\n| `lora_dropout` | 0.0-0.1 | Regularization |\n\n### Target Modules\n\n```python\n# Common target modules for different models\n\n# LLaMA / Mistral / TinyLlama\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# GPT-2\ntarget_modules = [\"c_attn\", \"c_proj\"]\n\n# BLOOM\ntarget_modules = [\"query_key_value\", \"dense\"]\n\n# All linear layers (most aggressive)\ntarget_modules = \"all-linear\"\n```\n\n### Rank Selection Guide\n\n| Rank (r) | Use Case |\n|----------|----------|\n| 4 | Simple tasks, small datasets |\n| 8 | General purpose (recommended) |\n| 16 | Complex tasks, more capacity |\n| 32+ | Near full fine-tune quality |\n\n## QLoRA (Quantized LoRA)\n\n### Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training (important!)\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n```\n\n## Training with PEFT\n\n### Using SFTTrainer\n\n```python\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\nsft_config = SFTConfig(\n    output_dir=\"./lora_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-4,  # Higher LR for LoRA\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    gradient_accumulation_steps=4,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset[\"train\"],\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    peft_config=lora_config,  # Pass LoRA config\n)\n\ntrainer.train()\n```\n\n## Unsloth (2x Faster Training)\n\n### Setup\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/tinyllama-chat-bnb-4bit\",  # Pre-quantized\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\n# Add LoRA with Unsloth\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=42,\n)\n```\n\n### Train with Unsloth\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./unsloth_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=42,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n## Save and Load Adapters\n\n### Save Adapters Only\n\n```python\n# Save just the LoRA weights (small!)\nmodel.save_pretrained(\"./lora_adapters\")\n```\n\n### Load Adapters\n\n```python\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n```\n\n### Merge Adapters into Base Model\n\n```python\n# Merge LoRA weights into base model (for deployment)\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged_model\")\n```\n\n## Inference with Adapters\n\n```python\nfrom peft import PeftModel\n\n# Load base + adapters\nbase_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n\n# Generate\nmodel.eval()\ninputs = tokenizer(\"What is Python?\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap them at inference time without reloading the base model.\n\n### Train Multiple Adapters\n\n```python\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, SFTConfig\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual responses\n    \"creative\": creative_data,     # Imaginative, expressive responses\n    \"code\": code_data,             # Code-focused analysis\n}\n\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(\n        model, r=16, lora_alpha=16,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n    )\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\nfrom unsloth import FastLanguageModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ndef load_and_generate(adapter_path, prompt):\n    \"\"\"Load adapter and generate response.\"\"\"\n    # Hot-swap adapter onto base model\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(adapted_model.device)\n\n    outputs = adapted_model.generate(input_ids=inputs, max_new_tokens=128)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Use different adapters for different tasks\ntechnical_response = load_and_generate(\"./adapters/technical\", \"Explain TCP vs UDP\")\ncreative_response = load_and_generate(\"./adapters/creative\", \"Write a haiku about coding\")\ncode_response = load_and_generate(\"./adapters/code\", \"Explain Python decorators\")\n```\n\n### Adapter Storage Efficiency\n\n| Component | Size |\n|-----------|------|\n| Base model (4-bit) | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters total | ~1.3GB |\n\n**Multi-adapter approach**: 8GB + 1.3GB = 9.3GB total\n**vs 10 full models**: 80GB total\n\n## Comparison: Full vs LoRA vs QLoRA\n\n| Aspect | Full Fine-tune | LoRA | QLoRA |\n|--------|----------------|------|-------|\n| Trainable % | 100% | ~0.1-1% | ~0.1-1% |\n| Memory | 4x model | ~1.2x model | ~0.5x model |\n| Training speed | Slow | Fast | Fast |\n| Quality | Best | Very Good | Good |\n| 7B model | 28GB+ | ~16GB | ~6GB |\n\n## Troubleshooting\n\n### Out of Memory\n\n**Fix:**\n\n```python\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Use smaller batch with accumulation\nper_device_train_batch_size=1\ngradient_accumulation_steps=8\n```\n\n### Poor Quality\n\n**Fix:**\n\n- Increase `r` (rank)\n- Add more target modules\n- Train longer\n- Check data quality\n\n### NaN Loss\n\n**Fix:**\n\n- Lower learning rate\n- Use gradient clipping\n- Check for data issues\n\n## When to Use This Skill\n\nUse when:\n\n- GPU memory is limited\n- Fine-tuning large models (7B+)\n- Need fast training iterations\n- Want to swap adapters for different tasks\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning basics\n- `bazzite-ai-jupyter:quantization` - Quantization for QLoRA\n- `bazzite-ai-jupyter:sft` - SFT training with LoRA\n- `bazzite-ai-jupyter:inference` - Fast inference with adapters\n- `bazzite-ai-jupyter:transformers` - Target module selection"
              },
              {
                "name": "qlora",
                "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n",
                "path": "bazzite-ai-jupyter/skills/qlora/SKILL.md",
                "frontmatter": {
                  "name": "qlora",
                  "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n"
                },
                "content": "# Advanced QLoRA Experiments\n\n## Overview\n\nThis skill covers advanced QLoRA experimentation patterns for optimizing fine-tuning performance. Learn how to select the best LoRA rank, alpha scaling, target modules, and quantization settings for your specific use case.\n\n## Quick Reference\n\n| Topic | Key Finding |\n|-------|-------------|\n| **Rank (r)** | r=16 is optimal balance; r=8 for memory constrained |\n| **Alpha** | alpha=r (1.0x scaling) is standard; alpha=2r for aggressive |\n| **Target Modules** | all_linear for general; mlp_only for knowledge injection |\n| **Quantization** | 4-bit NF4 matches BF16 quality with 11-15% memory savings |\n| **Continual Learning** | Sequential training adds knowledge without forgetting |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n```\n\n## Alpha Scaling\n\n### Formula\n\nThe effective LoRA scaling factor is:\n\n```\nscaling_factor = alpha / r\n```\n\nThis acts as a learning rate multiplier for adapter weights.\n\n### Alpha Comparison Code\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import TrainerCallback\n\nALPHAS = [8, 16, 32, 64]\nFIXED_RANK = 16\nresults = []\n\nfor alpha in ALPHAS:\n    scaling_factor = alpha / FIXED_RANK\n    print(f\"\\n=== Testing alpha={alpha} (scaling={scaling_factor}x) ===\")\n\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA with specific alpha\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=FIXED_RANK,\n        lora_alpha=alpha,  # Variable alpha\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Train and record results\n    trainer = SFTTrainer(model=model, tokenizer=tokenizer, ...)\n    stats = trainer.train()\n\n    results.append({\n        \"alpha\": alpha,\n        \"scaling\": scaling_factor,\n        \"final_loss\": stats.metrics[\"train_loss\"]\n    })\n```\n\n### Alpha Scaling Results\n\n| Alpha | Scaling | Final Loss | Behavior |\n|-------|---------|------------|----------|\n| 8 | 0.5x | ~3.02 | Conservative, slower convergence |\n| 16 | 1.0x | ~2.94 | Standard, balanced |\n| 32 | 2.0x | ~2.80 | Aggressive, faster convergence |\n| 64 | 4.0x | ~2.60 | Very aggressive, risk of instability |\n\n### Recommendations\n\n- **Standard**: `alpha = r` (1.0x scaling)\n- **Aggressive training**: `alpha = 2r` with reduced learning rate\n- **Stability priority**: `alpha = r/2` (0.5x scaling)\n\n## LoRA Rank Comparison\n\n### Rank Selection Code\n\n```python\nRANKS = [4, 8, 16, 32, 64]\n\nfor rank in RANKS:\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=rank,\n        lora_alpha=rank,  # Keep alpha = r for fair comparison\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Count parameters\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    pct = 100 * trainable / total\n\n    print(f\"r={rank}: {trainable:,} trainable ({pct:.2f}%)\")\n```\n\n### Rank Comparison Results (Qwen3-4B)\n\n| Rank | Trainable Params | % of Total | Memory | Best For |\n|------|------------------|------------|--------|----------|\n| 4 | ~8M | 0.3% | Lowest | Quick experiments |\n| 8 | ~16M | 0.6% | Low | Memory constrained |\n| **16** | ~33M | 1.3% | Medium | **General use (default)** |\n| 32 | ~66M | 2.6% | High | Complex tasks |\n| 64 | ~132M | 5.2% | Highest | Maximum capacity |\n\n### Rank Selection Guidelines\n\n```python\ndef recommend_rank(gpu_vram_gb, task_complexity, dataset_size):\n    \"\"\"Recommend LoRA rank based on constraints.\"\"\"\n\n    # Memory constraints\n    if gpu_vram_gb < 8:\n        max_rank = 8\n    elif gpu_vram_gb < 12:\n        max_rank = 16\n    elif gpu_vram_gb < 24:\n        max_rank = 32\n    else:\n        max_rank = 64\n\n    # Task complexity adjustment\n    if task_complexity == \"simple\":\n        suggested = 8\n    elif task_complexity == \"medium\":\n        suggested = 16\n    elif task_complexity == \"complex\":\n        suggested = 32\n    else:\n        suggested = 16\n\n    # Dataset size adjustment\n    if dataset_size < 1000:\n        suggested = min(suggested, 16)  # Avoid overfitting\n    elif dataset_size > 10000:\n        suggested = max(suggested, 16)  # Can use higher rank\n\n    return min(suggested, max_rank)\n```\n\n## Target Module Selection\n\n### Available Configurations\n\n```python\nTARGET_CONFIGS = {\n    \"qv_only\": {\n        \"modules\": [\"q_proj\", \"v_proj\"],\n        \"params\": \"~9M\",\n        \"description\": \"Query + Value only (minimal, original LoRA paper)\"\n    },\n    \"attention_only\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"params\": \"~18M\",\n        \"description\": \"All attention layers\"\n    },\n    \"mlp_only\": {\n        \"modules\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~15M\",\n        \"description\": \"MLP/FFN layers only\"\n    },\n    \"all_linear\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~33M\",\n        \"description\": \"All linear layers (maximum capacity)\"\n    },\n}\n```\n\n### Module Function Analysis\n\n**Attention Layers (q, k, v, o):**\n- Control how model attends to input\n- Affect reasoning patterns and style\n- Best for: Format adaptation, thinking pattern changes\n\n**MLP Layers (gate, up, down):**\n- Store factual knowledge\n- Process and transform representations\n- Best for: Knowledge injection, domain adaptation\n\n### Use Case Recommendations\n\n| Use Case | Config | Rationale |\n|----------|--------|-----------|\n| Minimal fine-tuning | `qv_only` | Fastest, smallest adapters |\n| Style/format change | `attention_only` | Changes reasoning patterns |\n| Knowledge injection | `mlp_only` | Updates knowledge only |\n| **General fine-tuning** | `all_linear` | **Maximum flexibility (default)** |\n| Preserve reasoning | `mlp_only` | Keeps thinking style |\n\n### Target Module Selection Code\n\n```python\ndef get_target_modules(use_case):\n    \"\"\"Select target modules based on use case.\"\"\"\n\n    configs = {\n        \"minimal\": [\"q_proj\", \"v_proj\"],\n        \"style\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"knowledge\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"full\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                 \"gate_proj\", \"up_proj\", \"down_proj\"],\n    }\n\n    return configs.get(use_case, configs[\"full\"])\n\n# Usage\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=get_target_modules(\"full\"),\n    ...\n)\n```\n\n## Continual Learning\n\nSequential training adds new knowledge without catastrophic forgetting.\n\n### Sequential Training Pattern\n\n```python\nTRAINING_STAGES = [\n    (\"medical\", medical_dataset),\n    (\"legal\", legal_dataset),\n    (\"technical\", technical_dataset),\n]\n\n# Load model ONCE\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Apply LoRA ONCE\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train sequentially\nfor stage_idx, (domain_name, domain_data) in enumerate(TRAINING_STAGES):\n    print(f\"\\n=== Stage {stage_idx + 1}: Training on {domain_name} ===\")\n\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=domain_data,\n        args=SFTConfig(\n            output_dir=f\"./continual_{domain_name}\",\n            max_steps=5,\n            learning_rate=2e-4,\n            ...\n        ),\n    )\n    trainer.train()\n\n    # Save checkpoint\n    model.save_pretrained(f\"./checkpoint_stage_{stage_idx}\")\n\n    # Test retention on ALL previous domains\n    test_retention(model, tokenizer, TRAINING_STAGES[:stage_idx+1])\n```\n\n### Retention Testing\n\n```python\ndef test_retention(model, tokenizer, trained_domains):\n    \"\"\"Verify model retains knowledge from previous domains.\"\"\"\n\n    RETENTION_TESTS = {\n        \"medical\": \"What is hypertension and how is it treated?\",\n        \"legal\": \"Explain the concept of due process.\",\n        \"technical\": \"What is a REST API?\",\n    }\n\n    FastLanguageModel.for_inference(model)\n\n    print(\"\\n--- Retention Test ---\")\n    for domain_name, _ in trained_domains:\n        prompt = RETENTION_TESTS[domain_name]\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(model.device)\n\n        outputs = model.generate(input_ids=inputs, max_new_tokens=100)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Check response quality\n        has_content = len(response.split()) > 10\n        print(f\"{domain_name}: {'PASS' if has_content else 'FAIL'}\")\n```\n\n### Continual Learning Benefits\n\n- **No catastrophic forgetting**: Base weights frozen, adapters accumulate knowledge\n- **Incremental updates**: Add new domains without full retraining\n- **Curriculum learning**: Simple  complex topic progression\n- **Personalization**: Adapt over time with user feedback\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap at inference time.\n\n### Training Multiple Adapters\n\n```python\nfrom peft import PeftModel\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual\n    \"creative\": creative_data,     # Imaginative, expressive\n    \"code\": code_data,             # Code-focused\n}\n\n# Train separate adapters\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load base model fresh\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(model, r=16, lora_alpha=16, ...)\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n    print(f\"Saved {task_name} adapter\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Function to swap adapters\ndef load_adapter(base_model, adapter_path):\n    \"\"\"Load specific adapter onto base model.\"\"\"\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n    return adapted_model\n\n# Usage\ntechnical_model = load_adapter(base_model, \"./adapters/technical\")\nresponse = generate(technical_model, \"Explain TCP vs UDP\")\n\ncreative_model = load_adapter(base_model, \"./adapters/creative\")\nresponse = generate(creative_model, \"Write a haiku about coding\")\n```\n\n### Adapter Storage\n\n| Component | Size |\n|-----------|------|\n| Base model | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters | ~1.3GB total |\n\nMulti-adapter approach: 8GB + 1.3GB = 9.3GB total\nvs. 10 full models = 80GB\n\n## Quantization Comparison\n\n### 4-bit vs BF16 Code\n\n```python\nQUANT_CONFIGS = {\n    \"4bit_nf4\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        \"load_in_4bit\": True,\n    },\n    \"bf16\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507\",\n        \"load_in_4bit\": False,\n    },\n}\n\nresults = []\n\nfor config_name, config in QUANT_CONFIGS.items():\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        config[\"model_name\"],\n        max_seq_length=512,\n        load_in_4bit=config.get(\"load_in_4bit\", False),\n    )\n\n    # Measure memory\n    memory_before = measure_gpu_memory()\n\n    # Train\n    trainer = SFTTrainer(model=model, ...)\n    stats = trainer.train()\n\n    memory_after = measure_gpu_memory()\n\n    results.append({\n        \"config\": config_name,\n        \"memory_mb\": memory_after,\n        \"final_loss\": stats.metrics[\"train_loss\"],\n    })\n```\n\n### Quantization Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves identical final loss with 11-15% memory savings.\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n## Utility Functions\n\n### Loss History Callback\n\n```python\nfrom transformers import TrainerCallback\n\nclass LossHistoryCallback(TrainerCallback):\n    \"\"\"Track loss during training for comparison.\"\"\"\n\n    def __init__(self):\n        self.losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and 'loss' in logs:\n            self.losses.append({\n                'step': state.global_step,\n                'loss': logs['loss']\n            })\n\n# Usage\nloss_callback = LossHistoryCallback()\ntrainer = SFTTrainer(..., callbacks=[loss_callback])\ntrainer.train()\n\n# Access loss history\nfor entry in loss_callback.losses:\n    print(f\"Step {entry['step']}: Loss {entry['loss']:.4f}\")\n```\n\n### GPU Memory Measurement\n\n```python\nimport subprocess\nimport gc\nimport torch\n\ndef measure_gpu_memory():\n    \"\"\"Get current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage\nprint(f\"Memory before: {measure_gpu_memory()} MB\")\ncleanup_memory()\nprint(f\"Memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Parameter Counting\n\n```python\ndef count_parameters(model):\n    \"\"\"Count trainable and total parameters.\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return {\n        \"trainable\": trainable,\n        \"total\": total,\n        \"trainable_formatted\": f\"{trainable:,}\",\n        \"total_formatted\": f\"{total:,}\",\n        \"percentage\": f\"{100 * trainable / total:.2f}%\"\n    }\n\n# Usage\nparams = count_parameters(model)\nprint(f\"Trainable: {params['trainable_formatted']} ({params['percentage']})\")\n```\n\n## Decision Tree\n\n```\nWhat's your priority?\n\n Memory constrained (<12GB VRAM)\n    Use r=8 or r=4\n    Use 4-bit quantization\n    Use qv_only or attention_only modules\n\n Maximum quality\n    Use r=32\n    Use BF16 if VRAM allows\n    Use all_linear modules\n\n Knowledge injection only\n    Use mlp_only modules\n    Preserves reasoning style\n\n Multiple tasks\n    Train separate adapters\n    Hot-swap at inference\n\n Incremental updates\n     Sequential training\n     Test retention after each stage\n```\n\n## Kernel Shutdown (Jupyter)\n\nQLoRA experiments require loading/unloading multiple models. Shutdown kernel between experiments to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Each experiment in the loop should clean up memory with `del model` and `gc.collect()`, but kernel shutdown is required between different experiment notebooks.\n\n## When to Use This Skill\n\nUse when:\n- Optimizing LoRA hyperparameters\n- Memory-constrained training\n- Building multi-task systems\n- Incrementally updating models\n- Comparing quantization approaches\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - Basic LoRA setup\n- `bazzite-ai-jupyter:quantization` - Quantization fundamentals\n- `bazzite-ai-jupyter:sft` - Training with SFTTrainer\n- `bazzite-ai-jupyter:inference` - Fast inference patterns"
              },
              {
                "name": "quantization",
                "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n",
                "path": "bazzite-ai-jupyter/skills/quantization/SKILL.md",
                "frontmatter": {
                  "name": "quantization",
                  "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n"
                },
                "content": "# Model Quantization\n\n## Overview\n\nQuantization reduces model precision to save memory and speed up inference. A 7B model at FP32 requires ~28GB, but at 4-bit only ~4GB.\n\n## Quick Reference\n\n| Precision | Bits | Memory | Quality | Speed |\n|-----------|------|--------|---------|-------|\n| FP32 | 32 | 4x | Best | Slowest |\n| FP16 | 16 | 2x | Excellent | Fast |\n| BF16 | 16 | 2x | Excellent | Fast |\n| INT8 | 8 | 1x | Good | Faster |\n| INT4 | 4 | 0.5x | Acceptable | Fastest |\n\n## Memory Estimation\n\n```python\ndef estimate_memory(params_billions, precision_bits):\n    \"\"\"Estimate model memory in GB.\"\"\"\n    bytes_per_param = precision_bits / 8\n    return params_billions * bytes_per_param\n\n# Example: 7B model\nmodel_size = 7  # billion parameters\n\nprint(f\"FP32: {estimate_memory(7, 32):.1f} GB\")  # 28 GB\nprint(f\"FP16: {estimate_memory(7, 16):.1f} GB\")  # 14 GB\nprint(f\"INT8: {estimate_memory(7, 8):.1f} GB\")   # 7 GB\nprint(f\"INT4: {estimate_memory(7, 4):.1f} GB\")   # 3.5 GB\n```\n\n## Measure Model Size\n\n```python\ndef get_model_size(model):\n    \"\"\"Get model size in GB including buffers.\"\"\"\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    total = (param_size + buffer_size) / 1024**3\n    return total\n\nprint(f\"Model size: {get_model_size(model):.2f} GB\")\n```\n\n## Load Model at Different Precisions\n\n### FP32 (Default)\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel_32bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nprint(f\"FP32 size: {get_model_size(model_32bit):.2f} GB\")\n```\n\n### FP16 / BF16\n\n```python\nimport torch\n\nmodel_16bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    torch_dtype=torch.float16,  # or torch.bfloat16\n    device_map=\"auto\"\n)\n\nprint(f\"FP16 size: {get_model_size(model_16bit):.2f} GB\")\n```\n\n### 8-bit Quantization\n\n```python\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"8-bit size: {get_model_size(model_8bit):.2f} GB\")\n```\n\n### 4-bit Quantization (Recommended)\n\n```python\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True  # Nested quantization\n)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"4-bit size: {get_model_size(model_4bit):.2f} GB\")\n```\n\n## BitsAndBytesConfig Options\n\n### 4-bit Configuration\n\n```python\nfrom transformers import BitsAndBytesConfig\nimport torch\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n\n    # Quantization type\n    bnb_4bit_quant_type=\"nf4\",  # \"nf4\" or \"fp4\"\n\n    # Compute dtype for dequantized weights\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\n    # Double quantization (saves more memory)\n    bnb_4bit_use_double_quant=True,\n)\n```\n\n### Options Explained\n\n| Option | Values | Effect |\n|--------|--------|--------|\n| `load_in_4bit` | True/False | Enable 4-bit |\n| `bnb_4bit_quant_type` | \"nf4\", \"fp4\" | nf4 better for LLMs |\n| `bnb_4bit_compute_dtype` | float16, bfloat16 | Computation precision |\n| `bnb_4bit_use_double_quant` | True/False | Quantize quantization constants |\n\n## Compare Precision Performance\n\n```python\nfrom transformers import pipeline\nimport time\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Test message\nmessages = [{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}]\n\ndef benchmark(model, tokenizer, name):\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n    start = time.time()\n    output = pipe(messages, max_new_tokens=100, return_full_text=False)\n    elapsed = time.time() - start\n\n    print(f\"{name}:\")\n    print(f\"  Time: {elapsed:.2f}s\")\n    print(f\"  Size: {get_model_size(model):.2f} GB\")\n    print(f\"  Output: {output[0]['generated_text'][:50]}...\")\n    print()\n\n# Benchmark each precision\nbenchmark(model_32bit, tokenizer, \"FP32\")\nbenchmark(model_16bit, tokenizer, \"FP16\")\nbenchmark(model_8bit, tokenizer, \"8-bit\")\nbenchmark(model_4bit, tokenizer, \"4-bit\")\n```\n\n## Quantization for Training\n\n### QLoRA Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit base model\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## Precision Comparison\n\n| Precision | Memory | Quality | Training | Best For |\n|-----------|--------|---------|----------|----------|\n| FP32 | 4x | Perfect | Yes | Research, baselines |\n| FP16 | 2x | Excellent | Yes | Standard training |\n| BF16 | 2x | Excellent | Yes | Large models |\n| INT8 | 1x | Good | Limited | Inference |\n| INT4 | 0.5x | Acceptable | QLoRA | Memory-constrained |\n\n## FP16 vs BF16\n\n| Aspect | FP16 | BF16 |\n|--------|------|------|\n| Range | Smaller | Larger (like FP32) |\n| Precision | Higher | Lower |\n| Overflow risk | Higher | Lower |\n| Hardware | All GPUs | Ampere+ |\n| Best for | Inference | Training |\n\n## 4-bit NF4 vs BF16 Comparison (Tested)\n\nBased on experiments with Qwen3-4B-Thinking models:\n\n### Comparison Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves **identical final loss** with 11-15% memory savings.\n\n### Pre-Quantized Models (Recommended)\n\nUse pre-quantized models for faster loading:\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Pre-quantized (fast loading)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # -bnb-4bit suffix\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# vs. On-demand quantization (slower)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize during load\n)\n```\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n### Quality Preservation\n\n4-bit NF4 preserves:\n- Training convergence (identical final loss)\n- Thinking tag structure (`<think>...</think>`)\n- Response quality and coherence\n- Model reasoning capabilities\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA OOM error\n\n**Fix:**\n\n```python\n# Use 4-bit quantization\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True\n)\n```\n\n### Quality Degradation\n\n**Symptom:** Poor model outputs after quantization\n\n**Fix:**\n\n- Use nf4 instead of fp4\n- Try 8-bit instead of 4-bit\n- Increase LoRA rank if fine-tuning\n\n### Slow Loading\n\n**Symptom:** Model takes long to load\n\n**Fix:**\n\n- Quantization happens at load time\n- Use `device_map=\"auto\"` for multi-GPU\n\n## When to Use This Skill\n\nUse when:\n\n- Model doesn't fit in GPU memory\n- Need faster inference\n- Training with limited resources (QLoRA)\n- Deploying to edge devices\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:peft` - LoRA with quantization (QLoRA)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning\n- `bazzite-ai-jupyter:sft` - SFT training with quantization\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Model architecture"
              },
              {
                "name": "rag",
                "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n",
                "path": "bazzite-ai-jupyter/skills/rag/SKILL.md",
                "frontmatter": {
                  "name": "rag",
                  "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n"
                },
                "content": "# Retrieval-Augmented Generation (RAG)\n\n## Overview\n\nRAG enhances LLM responses by retrieving relevant context from a knowledge base before generation. This grounds responses in specific documents and reduces hallucination.\n\n## Quick Reference\n\n| Step | Component |\n|------|-----------|\n| 1. Chunk | Split documents into segments |\n| 2. Embed | Convert chunks to vectors |\n| 3. Store | Save in vector database |\n| 4. Retrieve | Find relevant chunks |\n| 5. Generate | LLM answers with context |\n\n## Basic RAG Pipeline\n\n### 1. Document Chunking\n\n```python\nimport textwrap\n\ndocument = \"\"\"\nYour long document text here...\nMultiple paragraphs of content...\n\"\"\"\n\n# Chunk into segments of max 1000 characters\nchunks = textwrap.wrap(document, width=1000)\n\nprint(f\"Created {len(chunks)} chunks\")\n```\n\n### 2. Generate Embeddings\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nEMBED_MODEL = \"llama3.2:latest\"\n\nclient = OpenAI(base_url=f\"{OLLAMA_HOST}/v1\", api_key=\"ollama\")\n\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=EMBED_MODEL,\n        input=text\n    )\n    return response.data[0].embedding\n\n# Embed all chunks\nembeddings = [get_embedding(chunk) for chunk in chunks]\nprint(f\"Embedding dimensions: {len(embeddings[0])}\")\n```\n\n### 3. Create Vector Database (Pandas)\n\n```python\nimport pandas as pd\nimport numpy as np\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": [np.array(e) for e in embeddings]\n})\n```\n\n### 4. Similarity Search\n\n```python\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef search(query, n_results=5):\n    query_embedding = get_embedding(query)\n\n    similarities = vector_db[\"embeddings\"].apply(\n        lambda x: cosine_similarity(query_embedding, x)\n    )\n\n    top_indices = similarities.nlargest(n_results).index\n    return vector_db.loc[top_indices, \"text\"].tolist()\n\n# Find relevant chunks\nrelevant = search(\"What are the symptoms?\", n_results=3)\n```\n\n### 5. Generate with Context\n\n```python\nLLM_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\ndef rag_query(question, n_docs=5):\n    # Retrieve context\n    context_chunks = search(question, n_results=n_docs)\n    context = \"\\n\\n\".join(context_chunks)\n\n    # Build prompt\n    messages = [\n        {\"role\": \"system\", \"content\": f\"Answer based on this context:\\n\\n{context}\"},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\n    # Generate\n    response = client.chat.completions.create(\n        model=LLM_MODEL,\n        messages=messages,\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content\n\nanswer = rag_query(\"What are the main symptoms of Omicron?\")\n```\n\n## LangChain RAG with ChromaDB\n\n### Setup\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# LLM for generation\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n```\n\n### Create Vector Store\n\n```python\nimport textwrap\n\n# Chunk document\ndocument = \"Your document text...\"\nchunks = textwrap.wrap(document, width=1000)\n\n# Create ChromaDB store\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n```\n\n### Build RAG Chain\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_classic.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\n# Prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer based on this context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{input}\")\n])\n\n# Create chains\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\n```\n\n### Conversational RAG\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nchat_history = []\n\ndef chat(question):\n    result = rag_chain.invoke({\n        \"input\": question,\n        \"chat_history\": chat_history\n    })\n\n    # Update history\n    chat_history.append(HumanMessage(content=question))\n    chat_history.append(AIMessage(content=result[\"answer\"]))\n\n    return result[\"answer\"]\n\n# Multi-turn conversation\nprint(chat(\"What is Omicron?\"))\nprint(chat(\"What are its symptoms?\"))\nprint(chat(\"How does it compare to Delta?\"))\n```\n\n## Chunking Strategies\n\n### Fixed Size\n\n```python\ndef fixed_chunks(text, size=1000):\n    return textwrap.wrap(text, width=size)\n```\n\n### Sentence-Based\n\n```python\nimport re\n\ndef sentence_chunks(text, max_sentences=5):\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    chunks = []\n    current = []\n\n    for sent in sentences:\n        current.append(sent)\n        if len(current) >= max_sentences:\n            chunks.append(\" \".join(current))\n            current = []\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n```\n\n### Overlap Chunks\n\n```python\ndef overlap_chunks(text, size=1000, overlap=200):\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + size\n        chunks.append(text[start:end])\n        start = end - overlap\n\n    return chunks\n```\n\n## Vector Store Options\n\n### Pandas DataFrame (Simple)\n\n```python\nimport pandas as pd\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": embeddings\n})\n```\n\n### ChromaDB (Persistent)\n\n```python\nfrom langchain_community.vectorstores import Chroma\n\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n```\n\n### FAISS (Fast)\n\n```python\nfrom langchain_community.vectorstores import FAISS\n\nvectorstore = FAISS.from_texts(chunks, embeddings)\nvectorstore.save_local(\"./faiss_index\")\n```\n\n## Troubleshooting\n\n### Poor Retrieval Quality\n\n**Symptom:** Retrieved chunks not relevant\n\n**Fix:**\n\n- Increase chunk overlap\n- Use smaller chunk sizes\n- Try different embedding models\n- Increase `k` in retriever\n\n### Slow Embedding\n\n**Symptom:** Takes long to embed documents\n\n**Fix:**\n\n- Batch embeddings\n- Use smaller embedding model\n- Cache embeddings to disk\n\n### Out of Context\n\n**Symptom:** LLM ignores retrieved context\n\n**Fix:**\n\n- Increase `max_tokens`\n- Use explicit system prompt\n- Reduce number of retrieved chunks\n\n## When to Use This Skill\n\nUse when:\n\n- LLM needs to answer from specific documents\n- Reducing hallucination is critical\n- Building Q&A systems over documents\n- Need up-to-date information not in training data\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain fundamentals\n- `bazzite-ai-jupyter:evaluation` - Evaluate RAG quality\n- `bazzite-ai-ollama:python` - Ollama embeddings API"
              },
              {
                "name": "reward",
                "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n",
                "path": "bazzite-ai-jupyter/skills/reward/SKILL.md",
                "frontmatter": {
                  "name": "reward",
                  "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n"
                },
                "content": "# Reward Model Training\n\n## Overview\n\nReward models learn to score responses based on human preferences. They're used in RLHF pipelines (PPO, GRPO, RLOO) to provide reward signals for policy optimization. The model outputs a scalar reward for each response. This skill includes patterns for scoring thinking/reasoning quality.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RewardTrainer` | Trainer for reward model |\n| `RewardConfig` | Training hyperparameters |\n| `AutoModelForSequenceClassification` | Model with `num_labels=1` |\n| `task_type=\"SEQ_CLS\"` | LoRA task type for reward models |\n| Preference pairs | Training data format |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# Standard transformers for reward models (not Unsloth)\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import RewardTrainer, RewardConfig\nfrom datasets import Dataset\nimport torch\n```\n\n## Reward Model Concepts\n\n### How Reward Models Work\n\n1. Take prompt + response as input\n2. Output scalar reward score\n3. Trained on preference pairs (chosen > rejected)\n4. Used to guide RL policy optimization\n\n### Architecture\n\n```\nInput: [prompt + response]\n  \nBase LLM (frozen or LoRA)\n  \nClassification Head (Linear  Scalar)\n  \nOutput: Reward score (float)\n```\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is a function calling itself with a base case.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### Preprocessing\n\n```python\ndef format_for_reward(sample):\n    prompt = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n        tokenize=False, add_generation_prompt=True\n    )\n    return {\n        \"input_ids_chosen\": tokenizer(prompt + sample[\"chosen\"])[\"input_ids\"],\n        \"input_ids_rejected\": tokenizer(prompt + sample[\"rejected\"])[\"input_ids\"],\n    }\n```\n\n### Thinking Quality Preference Dataset\n\nTrain reward model to score thinking quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\n</think>\n\nRecursion is a technique where a function calls itself with a simpler version of the problem.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n```\n\n## Setup\n\n### Load Reward Model\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\n# Load as sequence classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Non-quantized base\n    num_labels=1,  # Single scalar reward output\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n\n# Setup pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## RewardTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RewardConfig\n\nreward_config = RewardConfig(\n    output_dir=\"./reward_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_length=512,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 1e-5 to 5e-5 | Training speed |\n| `max_length` | 512-1024 | Input truncation |\n| `center_rewards_coefficient` | 0.0-0.1 | Reward centering |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RewardTrainer\n\ntrainer = RewardTrainer(\n    model=model,\n    args=reward_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n## Using the Reward Model\n\n### Score Responses\n\n```python\ndef get_reward(prompt, response):\n    text = prompt + response\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        reward = outputs.logits[0, 0].item()\n\n    return reward\n\n# Example\nscore = get_reward(\"What is Python?\", \"A programming language.\")\nprint(f\"Reward: {score:.3f}\")\n```\n\n### Batch Scoring\n\n```python\ndef get_rewards_batch(prompts, responses):\n    texts = [p + r for p, r in zip(prompts, responses)]\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        rewards = outputs.logits[:, 0].tolist()\n\n    return rewards\n```\n\n### In GRPO/RLOO\n\n```python\ndef reward_fn(completions, prompts):\n    return get_rewards_batch(prompts, completions)\n\ngrpo_trainer = GRPOTrainer(\n    model=policy_model,\n    args=grpo_config,\n    train_dataset=dataset,\n    reward_funcs=reward_fn,\n)\n```\n\n## Reward Scaling\n\n### Normalize Rewards\n\n```python\ndef normalized_reward(completions, prompts):\n    raw_rewards = get_rewards_batch(prompts, completions)\n    mean = sum(raw_rewards) / len(raw_rewards)\n    std = (sum((r - mean) ** 2 for r in raw_rewards) / len(raw_rewards)) ** 0.5\n    return [(r - mean) / (std + 1e-8) for r in raw_rewards]\n```\n\n### Clip Rewards\n\n```python\ndef clipped_reward(completions, prompts):\n    rewards = get_rewards_batch(prompts, completions)\n    return [max(-1.0, min(1.0, r)) for r in rewards]\n```\n\n## Troubleshooting\n\n### Poor Discrimination\n\n**Symptom:** Similar scores for chosen and rejected\n\n**Fix:**\n- More training steps\n- Higher learning rate\n- Check data quality\n\n### Reward Hacking\n\n**Symptom:** RL model exploits reward model\n\n**Fix:**\n- Add diversity in training data\n- Ensemble multiple reward models\n- Regularization during RL\n\n### Overconfident Scores\n\n**Symptom:** Extreme reward values\n\n**Fix:**\n- Use `center_rewards_coefficient`\n- Normalize outputs\n- Clip reward range\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Use LoRA instead of full fine-tuning\n- Reduce `max_length`\n- Smaller batch size\n\n## Kernel Shutdown (Jupyter)\n\nReward model training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Building RLHF pipelines\n- Need explicit reward signal\n- Have preference data\n- Want interpretable scoring\n- Planning to use GRPO or RLOO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:grpo` - Uses reward models for RL\n- `bazzite-ai-jupyter:rloo` - Uses reward models for RL\n- `bazzite-ai-jupyter:dpo` - Alternative that doesn't need reward model\n- `bazzite-ai-jupyter:peft` - LoRA for efficient reward training\n- `bazzite-ai-jupyter:sft` - Pre-training before reward modeling\n- `bazzite-ai-jupyter:inference` - Inference for reward scoring"
              },
              {
                "name": "rloo",
                "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n",
                "path": "bazzite-ai-jupyter/skills/rloo/SKILL.md",
                "frontmatter": {
                  "name": "rloo",
                  "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n"
                },
                "content": "# Reinforcement Learning with Leave-One-Out (RLOO)\n\n## Overview\n\nRLOO is a reinforcement learning method that uses leave-one-out baseline estimation for variance reduction. Like GRPO, it generates multiple completions per prompt but uses a different baseline computation that can provide more stable gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RLOOTrainer` | RL trainer with RLOO baseline |\n| `RLOOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `num_generations` | Completions per prompt (4 typical) |\n| `kl_coef` | KL penalty coefficient (0.05, lower than GRPO) |\n| `learning_rate` | 1e-5 (same as GRPO) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import RLOOConfig, RLOOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## RLOO Concepts\n\n### How RLOO Works\n\n1. Generate K completions for each prompt\n2. Score all completions with reward function\n3. For each completion, compute baseline as mean of other K-1 rewards\n4. Advantage = reward - leave-one-out baseline\n5. Update policy using advantages\n\n### Leave-One-Out Baseline\n\n```\nFor completion i:\n  baseline_i = mean(rewards except reward_i)\n  advantage_i = reward_i - baseline_i\n\nThis reduces variance compared to:\n  - Single-sample estimates (high variance)\n  - Fixed baselines (may be inaccurate)\n```\n\n### Comparison with GRPO\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| Compute | Similar | Similar |\n| Stability | Often better | Good |\n\n## Dataset Format\n\n```python\n# RLOO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"Explain recursion.\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for RLOO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## RLOOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RLOOConfig\n\nrloo_config = RLOOConfig(\n    output_dir=\"./rloo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    num_generations=4,\n    max_completion_length=128,\n    kl_coef=0.05,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `num_generations` | 4-8 | Completions per prompt |\n| `kl_coef` | 0.01-0.1 | KL penalty strength |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n| `max_completion_length` | 64-256 | Generation length |\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response quality heuristics.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        score = 0.0\n\n        # Prefer medium length\n        if 10 <= length <= 50:\n            score += 1.0\n        elif length < 10:\n            score -= 0.5\n\n        # Prefer complete sentences\n        if completion.strip().endswith(\".\"):\n            score += 0.5\n\n        rewards.append(score)\n    return rewards\n```\n\n### Using Trained Reward Model\n\n```python\ndef trained_reward(completions, prompts):\n    \"\"\"Use trained reward model.\"\"\"\n    return reward_model.get_rewards(prompts, completions)\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing (same pattern as GRPO):\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RLOOTrainer\n\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=length_reward,\n)\n\ntrainer.train()\n```\n\n### With Reward Model Instance\n\n```python\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=trained_reward_model,\n)\n```\n\n## num_generations Selection\n\n| K | Use Case |\n|---|----------|\n| 2 | Minimum (limited variance reduction) |\n| 4 | Standard (recommended) |\n| 8 | Better baseline estimation (more compute) |\n| 16+ | Diminishing returns |\n\n**Trade-off:** Higher K = better baseline but more memory/compute\n\n## Troubleshooting\n\n### High Variance\n\n**Symptom:** Unstable training, jumpy rewards\n\n**Fix:**\n- Increase `num_generations` to 6-8\n- Lower `learning_rate`\n- Increase `kl_coef`\n\n### KL Divergence Explosion\n\n**Symptom:** Model output degrades quickly\n\n**Fix:**\n- Increase `kl_coef` to 0.1\n- Reduce `learning_rate`\n- More frequent evaluation\n\n### Reward Collapse\n\n**Symptom:** All generations get similar rewards\n\n**Fix:**\n- Check reward function diversity\n- Increase `temperature` during generation\n- More diverse prompts\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2-4\n- Reduce `max_completion_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nRLOO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Want lower variance than GRPO\n- Have compute for multiple generations\n- Building RLHF pipelines\n- Need stable RL training\n- Policy optimization from rewards\n\n## RLOO vs GRPO Comparison\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| KL penalty (beta) | 0.05 | 0.1 |\n| num_generations | 4 | 2 |\n| batch_size | 4 | 2 |\n| Stability | Often better | Good |\n| Use when | Need stable training | Faster iteration |\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before RLOO\n- `bazzite-ai-jupyter:grpo` - Alternative RL method (higher variance)\n- `bazzite-ai-jupyter:reward` - Training reward models for RLOO\n- `bazzite-ai-jupyter:dpo` - Simpler alternative (no RL)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "sft",
                "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n",
                "path": "bazzite-ai-jupyter/skills/sft/SKILL.md",
                "frontmatter": {
                  "name": "sft",
                  "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n"
                },
                "content": "# Supervised Fine-Tuning (SFT)\n\n## Overview\n\nSFT adapts a pre-trained LLM to follow instructions by training on instruction-response pairs. Unsloth provides an optimized SFTTrainer for 2x faster training with reduced memory usage. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastLanguageModel` | Load model with Unsloth optimizations |\n| `SFTTrainer` | Trainer for instruction tuning |\n| `SFTConfig` | Training hyperparameters |\n| `dataset_text_field` | Column containing formatted text |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then other imports\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Importing TRL before Unsloth will disable optimizations and may cause errors.\n\n## Dataset Formats\n\n### Instruction-Response Format\n\n```python\ndataset = [\n    {\"instruction\": \"What is Python?\", \"response\": \"A programming language.\"},\n    {\"instruction\": \"Explain ML.\", \"response\": \"Machine learning is...\"},\n]\n```\n\n### Chat/Conversation Format\n\n```python\ndataset = [\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"What is Python?\"},\n        {\"role\": \"assistant\", \"content\": \"A programming language.\"}\n    ]},\n]\n```\n\n### Using Chat Templates\n\n```python\ndef format_conversation(sample):\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\ndataset = dataset.map(format_conversation)\n```\n\n### Thinking Model Format\n\nFor models like Qwen3-Thinking, include `<think>` tags in the assistant response. Use **self-questioning internal dialogue** style:\n\n```python\ndef format_thinking_conversation(sample):\n    \"\"\"Format with thinking/reasoning tags.\"\"\"\n    # Combine thinking and response with tags\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\n# Sample dataset with self-questioning thinking style\nthinking_data = [\n    {\n        \"instruction\": \"What is machine learning?\",\n        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n    },\n    {\n        \"instruction\": \"Explain Python in one sentence.\",\n        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n    },\n    {\n        \"instruction\": \"What is a neural network?\",\n        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_data)\ndataset = dataset.map(format_thinking_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n```\n\n**Thinking Style Patterns:**\n- \"What is the user asking here?\"\n- \"Let me think about the key concepts...\"\n- \"How should I structure this explanation?\"\n- \"What's most important about X?\"\n\n## Unsloth SFT Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Training Configuration\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=512,\n)\n```\n\n## SFTTrainer Usage\n\n### Basic Training\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n### With Custom Formatting\n\n```python\ndef formatting_func(examples):\n    texts = []\n    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n        texts.append(text)\n    return texts\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    formatting_func=formatting_func,\n    args=sft_config,\n)\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 2e-4 to 2e-5 | Training speed vs stability |\n| `per_device_train_batch_size` | 1-4 | Memory usage |\n| `gradient_accumulation_steps` | 2-8 | Effective batch size |\n| `max_seq_length` | 512-2048 | Context window |\n| `optim` | \"adamw_8bit\" | Memory-efficient optimizer |\n\n## Save and Load\n\n### Save Model\n\n```python\n# Save LoRA adapters only (small)\nmodel.save_pretrained(\"./sft_lora\")\n\n# Save merged model (full size)\nmodel.save_pretrained_merged(\"./sft_merged\", tokenizer)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"./sft_lora\")\nFastLanguageModel.for_inference(model)\n```\n\n### Thinking Model Inference\n\nParse thinking content from model output using token IDs:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking\n\ndef generate_with_thinking(model, tokenizer, prompt):\n    \"\"\"Generate and parse thinking model output.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Setup pad token if needed\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    # Extract only generated tokens\n    input_length = inputs.shape[1]\n    generated_ids = outputs[0][input_length:].tolist()\n\n    # Parse thinking and response\n    if THINK_END_TOKEN_ID in generated_ids:\n        end_idx = generated_ids.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(generated_ids[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(generated_ids[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(generated_ids, skip_special_tokens=True)\n        response = \"(incomplete - increase max_new_tokens)\"\n\n    return thinking.strip(), response.strip()\n\n# Usage\nFastLanguageModel.for_inference(model)\nthinking, response = generate_with_thinking(model, tokenizer, \"What is 15 + 27?\")\nprint(f\"Thinking: {thinking}\")\nprint(f\"Response: {response}\")\n```\n\n## Ollama Integration\n\n### Export to GGUF\n\n```python\n# Export to GGUF for Ollama\nmodel.save_pretrained_gguf(\n    \"model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n```\n\n### Deploy to Ollama\n\n```bash\nollama create mymodel -f Modelfile\nollama run mymodel\n```\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory error\n\n**Fix:**\n- Use `gradient_checkpointing=\"unsloth\"`\n- Reduce `per_device_train_batch_size` to 1\n- Use 4-bit quantization (`load_in_4bit=True`)\n\n### NaN Loss\n\n**Symptom:** Loss becomes NaN during training\n\n**Fix:**\n- Lower `learning_rate` to 1e-5\n- Check data quality (no empty samples)\n- Use gradient clipping\n\n### Slow Training\n\n**Symptom:** Training slower than expected\n\n**Fix:**\n- Ensure Unsloth is imported FIRST (before TRL)\n- Use `bf16=True` if supported\n- Enable `use_gradient_checkpointing=\"unsloth\"`\n\n## Kernel Shutdown (Jupyter)\n\nSFT training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Creating instruction-following models\n- Fine-tuning for chat/conversation\n- Adapting to domain-specific tasks\n- Building custom assistants\n- First step before preference optimization (DPO/GRPO)\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - General fine-tuning concepts\n- `bazzite-ai-jupyter:dpo` - Direct Preference Optimization after SFT\n- `bazzite-ai-jupyter:grpo` - GRPO reinforcement learning after SFT\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-jupyter:vision` - Vision model fine-tuning\n- `bazzite-ai-ollama:api` - Ollama deployment"
              },
              {
                "name": "transformers",
                "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n",
                "path": "bazzite-ai-jupyter/skills/transformers/SKILL.md",
                "frontmatter": {
                  "name": "transformers",
                  "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n"
                },
                "content": "# Transformer Architecture\n\n## Overview\n\nThe Transformer architecture is the foundation of modern LLMs. Understanding its components helps with fine-tuning decisions, model selection, and debugging performance issues.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| Self-Attention | Learn relationships between tokens |\n| Multi-Head Attention | Multiple attention perspectives |\n| Feed-Forward Network | Transform representations |\n| Layer Normalization | Stabilize training |\n| Residual Connections | Enable deep networks |\n\n## Self-Attention Mechanism\n\n### Concept\n\nSelf-attention allows each token to attend to all other tokens in a sequence, learning contextual relationships.\n\n```\n\"The cat sat on the mat\"\n       \n  Each word attends to every other word\n       \n  Contextual representations\n```\n\n### Implementation\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n# Example tokens\ntokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\nseq_length = len(tokens)\nembed_dim = 8\n\n# Random embeddings (in practice, learned)\nembeddings = torch.randn(seq_length, embed_dim)\n\n# Query, Key, Value weight matrices\nW_q = torch.randn(embed_dim, embed_dim)\nW_k = torch.randn(embed_dim, embed_dim)\nW_v = torch.randn(embed_dim, embed_dim)\n\n# Compute Q, K, V\nQ = embeddings @ W_q  # Queries: what am I looking for?\nK = embeddings @ W_k  # Keys: what do I contain?\nV = embeddings @ W_v  # Values: what information do I provide?\n\n# Attention scores\nscores = Q @ K.T / (embed_dim ** 0.5)  # Scale by sqrt(d_k)\n\n# Softmax for attention weights\nattention_weights = F.softmax(scores, dim=-1)\n\n# Weighted sum of values\noutput = attention_weights @ V\n\nprint(f\"Input shape: {embeddings.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attention_weights.shape}\")\n```\n\n### Attention Formula\n\n```\nAttention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n```\n\nWhere:\n\n- Q = Query matrix\n- K = Key matrix\n- V = Value matrix\n- d_k = Key dimension (for scaling)\n\n## Multi-Head Attention\n\n### Concept\n\nMultiple attention heads learn different aspects of relationships (syntax, semantics, etc.).\n\n```python\nnum_heads = 4\nhead_dim = embed_dim // num_heads\n\n# Split into heads\ndef split_heads(x, num_heads):\n    batch_size, seq_len, embed_dim = x.shape\n    head_dim = embed_dim // num_heads\n    return x.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n\n# Compute attention for each head\nheads = []\nfor h in range(num_heads):\n    W_q_h = torch.randn(embed_dim, head_dim)\n    W_k_h = torch.randn(embed_dim, head_dim)\n    W_v_h = torch.randn(embed_dim, head_dim)\n\n    Q_h = embeddings @ W_q_h\n    K_h = embeddings @ W_k_h\n    V_h = embeddings @ W_v_h\n\n    scores_h = Q_h @ K_h.T / (head_dim ** 0.5)\n    attn_h = F.softmax(scores_h, dim=-1)\n    head_output = attn_h @ V_h\n    heads.append(head_output)\n\n# Concatenate heads\nmulti_head_output = torch.cat(heads, dim=-1)\n\n# Project back to embed_dim\nW_o = torch.randn(embed_dim, embed_dim)\nfinal_output = multi_head_output @ W_o\n\nprint(f\"Multi-head output shape: {final_output.shape}\")\n```\n\n## Feed-Forward Network\n\n### Concept\n\nTwo linear layers with activation, applied to each position independently.\n\n```python\nimport torch.nn as nn\n\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n        self.activation = nn.GELU()  # or ReLU\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n\nffn = FeedForward(embed_dim=512)\nx = torch.randn(1, 10, 512)  # (batch, seq_len, embed_dim)\noutput = ffn(x)\n\nprint(f\"FFN output shape: {output.shape}\")\n```\n\n### Formula\n\n```\nFFN(x) = GELU(xW_1 + b_1)W_2 + b_2\n```\n\n## Layer Normalization\n\n### Concept\n\nNormalizes across the embedding dimension to stabilize training.\n\n```python\nclass LayerNorm(nn.Module):\n    def __init__(self, embed_dim, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(embed_dim))\n        self.beta = nn.Parameter(torch.zeros(embed_dim))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nlayer_norm = nn.LayerNorm(embed_dim)\nnormalized = layer_norm(embeddings)\n```\n\n## Residual Connections\n\n### Concept\n\nSkip connections that add input to output, enabling gradient flow in deep networks.\n\n```python\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # Self-attention with residual\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)  # Residual connection\n\n        # FFN with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)  # Residual connection\n\n        return x\n```\n\n## Complete Transformer Layer\n\n```python\nclass TransformerLayer(nn.Module):\n    def __init__(self, embed_dim=512, num_heads=8, hidden_dim=2048, dropout=0.1):\n        super().__init__()\n\n        # Multi-head attention\n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n\n        # Feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n        # Layer norms\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention block\n        attn_out, attn_weights = self.self_attn(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout(attn_out))\n\n        # FFN block\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n\n        return x, attn_weights\n\n# Example usage\nlayer = TransformerLayer()\nx = torch.randn(10, 1, 512)  # (seq_len, batch, embed_dim)\noutput, weights = layer(x)\nprint(f\"Output shape: {output.shape}\")\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `embed_dim` | 768, 1024, 4096 | Model capacity |\n| `num_heads` | 8, 12, 16 | Attention perspectives |\n| `num_layers` | 12, 24, 32 | Model depth |\n| `hidden_dim` | 4 * embed_dim | FFN capacity |\n| `dropout` | 0.1 | Regularization |\n\n## Thinking Model Special Tokens\n\nQwen3-Thinking models use special tokens for chain-of-thought reasoning.\n\n### Token IDs\n\n| Token | ID | Purpose |\n|-------|----| --------|\n| `<think>` | 151667 | Start of thinking block |\n| `</think>` | 151668 | End of thinking block |\n\n### Parsing Thinking Output\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> for Qwen3-Thinking\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"Parse thinking model output using token ID boundary.\"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(token_list[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(token_list[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True)\n        response = \"(incomplete - increase max_tokens)\"\n\n    return thinking.strip(), response.strip()\n```\n\n### Chat Template with Thinking\n\n```python\n# Format training data with thinking tags\ndef format_thinking_sample(sample):\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n```\n\n## Model Size Estimation\n\n```python\ndef estimate_params(vocab_size, embed_dim, num_layers, hidden_dim, num_heads):\n    # Embedding\n    embedding_params = vocab_size * embed_dim\n\n    # Per layer\n    attn_params = 4 * embed_dim * embed_dim  # Q, K, V, O projections\n    ffn_params = 2 * embed_dim * hidden_dim  # Two linear layers\n    norm_params = 4 * embed_dim  # Two layer norms\n\n    layer_params = attn_params + ffn_params + norm_params\n    total_layer_params = num_layers * layer_params\n\n    # Output head\n    output_params = embed_dim * vocab_size\n\n    total = embedding_params + total_layer_params + output_params\n    return total / 1e9  # Billions\n\n# Example: LLaMA-7B-like\nparams_b = estimate_params(\n    vocab_size=32000,\n    embed_dim=4096,\n    num_layers=32,\n    hidden_dim=11008,\n    num_heads=32\n)\nprint(f\"Estimated parameters: {params_b:.1f}B\")\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Understanding model architecture for fine-tuning\n- Debugging attention patterns\n- Selecting target modules for LoRA\n- Estimating model size and memory\n- Building custom transformer components\n\n## Cross-References\n\n- `bazzite-ai-jupyter:finetuning` - Fine-tuning transformers\n- `bazzite-ai-jupyter:sft` - SFT with thinking models\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:peft` - Parameter-efficient tuning\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              },
              {
                "name": "vision",
                "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n",
                "path": "bazzite-ai-jupyter/skills/vision/SKILL.md",
                "frontmatter": {
                  "name": "vision",
                  "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n"
                },
                "content": "# Vision Model Fine-Tuning\n\n## Overview\n\nUnsloth provides `FastVisionModel` for fine-tuning vision-language models (VLMs) like Pixtral and Ministral with 2x faster training. This skill covers vision model loading, dataset preparation with images, and vision-specific LoRA configuration.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastVisionModel` | Load vision models with Unsloth optimizations |\n| `UnslothVisionDataCollator` | Handle image+text modality in batches |\n| `finetune_vision_layers` | Enable training of vision encoder |\n| `finetune_language_layers` | Enable training of language model |\n| `skip_prepare_dataset=True` | Required for vision datasets |\n| `dataset_text_field=\"\"` | Empty string for vision (not a field name) |\n| List dataset format | Use `[convert(s) for s in dataset]`, not `.map()` |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\n\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nimport torch\n```\n\n## Supported Vision Models\n\n| Model | Path | Parameters | Best For |\n|-------|------|------------|----------|\n| Pixtral-12B | `unsloth/pixtral-12b-2409-bnb-4bit` | 12.7B | High-quality vision tasks |\n| Ministral-8B-Vision | `unsloth/Ministral-8B-Vision-2507-bnb-4bit` | 8B | Balanced quality/speed |\n| Llama-3.2-11B-Vision | `unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit` | 11B | General vision tasks |\n\n## Load Vision Model\n\n```python\nfrom unsloth import FastVisionModel, is_bf16_supported\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\n\nprint(f\"Model loaded: {type(model).__name__}\")\nprint(f\"Tokenizer: {type(tokenizer).__name__}\")\n```\n\n## Vision-Specific LoRA Configuration\n\nVision models require special LoRA flags to enable training of vision encoder layers:\n\n```python\nmodel = FastVisionModel.get_peft_model(\n    model,\n    # Vision-specific flags\n    finetune_vision_layers=True,      # Train vision encoder\n    finetune_language_layers=True,    # Train language model\n    finetune_attention_modules=True,  # Train attention layers\n    finetune_mlp_modules=True,        # Train MLP/FFN layers\n\n    # Standard LoRA parameters\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Check trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n```\n\n### LoRA Flag Combinations\n\n| Use Case | vision_layers | language_layers | attention | mlp |\n|----------|--------------|-----------------|-----------|-----|\n| Full fine-tune | True | True | True | True |\n| Vision only | True | False | True | True |\n| Language only | False | True | True | True |\n| Minimal | False | True | True | False |\n\n## Dataset Format\n\nVision datasets require messages with multi-modal content containing both text and images.\n\n### Image + Text Format\n\n```python\nfrom datasets import Dataset\nfrom PIL import Image\n\n# Sample dataset structure\nsamples = [\n    {\n        \"image\": Image.open(\"equation1.png\"),\n        \"instruction\": \"Convert this equation to LaTeX.\",\n        \"response\": \"\\\\frac{d}{dx} x^2 = 2x\"\n    },\n    {\n        \"image\": Image.open(\"equation2.png\"),\n        \"instruction\": \"What does this equation represent?\",\n        \"response\": \"This is the quadratic formula: x = \\\\frac{-b \\\\pm \\\\sqrt{b^2-4ac}}{2a}\"\n    },\n]\n\ndataset = Dataset.from_list(samples)\n```\n\n### Converting to Chat Format\n\n```python\ndef convert_to_vision_conversation(sample):\n    \"\"\"Convert sample to vision chat format with image content.\"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"instruction\"]},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"response\"]}\n            ]\n        }\n    ]\n    return {\"messages\": messages}\n\n# Apply conversion\nconverted_dataset = dataset.map(convert_to_vision_conversation)\n```\n\n### Using HuggingFace Datasets\n\n**Important**: Use list comprehension, NOT `.map()` for vision datasets:\n\n```python\nfrom datasets import load_dataset\n\n# Load LaTeX OCR dataset from HuggingFace (via Unsloth mirror)\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:100]\")\n\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    \"\"\"Format sample for vision training.\"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: Use list comprehension, NOT .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\n```\n\n**Why list format?** Vision datasets with PIL images work more reliably as plain Python lists than HuggingFace Dataset objects with `.map()`.\n\n## Vision Data Collator\n\nThe `UnslothVisionDataCollator` handles image+text batching:\n\n```python\nfrom unsloth.trainer import UnslothVisionDataCollator\n\ndata_collator = UnslothVisionDataCollator(model, tokenizer)\n```\n\n## Training Configuration\n\nVision training requires specific SFTConfig settings:\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./vision_output\",\n    per_device_train_batch_size=1,      # Keep low for large vision models\n    gradient_accumulation_steps=4,       # Effective batch size = 4\n    max_steps=100,                       # Or num_train_epochs=1\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n\n    # Precision settings\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n\n    # Optimizer\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n\n    # Sequence length\n    max_seq_length=1024,\n\n    # CRITICAL for vision - all 3 are required\n    remove_unused_columns=False,         # Keep image column\n    dataset_text_field=\"\",               # Empty string (NOT a field name)\n    dataset_kwargs={\"skip_prepare_dataset\": True},  # Required for vision\n\n    # Other\n    seed=3407,\n    report_to=\"none\",\n)\n```\n\n**Critical settings explained:**\n- `remove_unused_columns=False`: Preserves image column during training\n- `dataset_text_field=\"\"`: Empty string tells TRL to use the messages format\n- `skip_prepare_dataset=True`: Prevents TRL from processing vision data incorrectly\n\n## SFTTrainer for Vision\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\n# Train\ntrainer_stats = trainer.train()\n\nprint(f\"Training completed!\")\nprint(f\"Final loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Complete Training Example\n\nThis example matches the tested notebook pattern:\n\n```python\n# 1. Environment Setup\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# 2. Imports (unsloth FIRST)\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\n# 3. Load model\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\nprint(f\"Model loaded: {type(model).__name__}\")\n\n# 4. Apply LoRA\nmodel = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers=True,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n)\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"LoRA applied ({trainable:,} trainable params)\")\n\n# 5. Prepare dataset (use LIST, not .map())\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:50]\")\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: List comprehension, not .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\nprint(f\"Dataset loaded ({len(converted_dataset)} samples)\")\n\n# 6. Configure training\nsft_config = SFTConfig(\n    output_dir=\"./vision_lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=50,\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=1024,\n    # CRITICAL for vision - all 3 required\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    seed=3407,\n    report_to=\"none\",\n)\n\n# 7. Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\ntrainer_stats = trainer.train()\nprint(f\"Training complete! Loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Inference with Vision Models\n\n### Prepare for Inference\n\n```python\nFastVisionModel.for_inference(model)\n```\n\n### Generate from Image\n\n```python\nfrom PIL import Image\n\n# Load test image\ntest_image = Image.open(\"test_equation.png\")\n\n# Format as conversation\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Convert this to LaTeX:\"},\n            {\"type\": \"image\", \"image\": test_image}\n        ]\n    }\n]\n\n# Apply chat template\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs,\n    max_new_tokens=256,\n    temperature=0.1,      # Low for accurate transcription\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n\n### Batch Inference\n\n```python\nfrom PIL import Image\n\nimages = [Image.open(f\"image_{i}.png\") for i in range(3)]\nprompts = [\"Describe this image.\", \"What objects are in this image?\", \"Transcribe the text.\"]\n\nfor img, prompt in zip(images, prompts):\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\"type\": \"image\", \"image\": img}\n        ]}\n    ]\n\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    outputs = model.generate(input_ids=inputs, max_new_tokens=128)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Save and Load\n\n### Save LoRA Adapter\n\n```python\n# Save only LoRA weights (~66MB for Pixtral)\nmodel.save_pretrained(\"./vision_lora\")\ntokenizer.save_pretrained(\"./vision_lora\")\n```\n\n### Save Merged Model\n\n```python\n# Save full merged model (large)\nmodel.save_pretrained_merged(\n    \"./vision_merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",\n)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastVisionModel\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"./vision_lora\",\n    load_in_4bit=True,\n)\nFastVisionModel.for_inference(model)\n```\n\n## Memory Requirements\n\n| Model | 4-bit VRAM | Training VRAM |\n|-------|------------|---------------|\n| Pixtral-12B | ~8GB | ~12GB |\n| Ministral-8B-Vision | ~6GB | ~10GB |\n| Llama-3.2-11B-Vision | ~7GB | ~11GB |\n\n## Troubleshooting\n\n### Image Not Processed\n\n**Symptom:** Model ignores image content\n\n**Fix:**\n- Ensure `remove_unused_columns=False` in SFTConfig\n- Use `skip_prepare_dataset=True` in dataset_kwargs\n- Verify image is PIL.Image object, not path string\n\n### Out of Memory\n\n**Symptom:** CUDA OOM during vision training\n\n**Fix:**\n- Reduce `per_device_train_batch_size` to 1\n- Increase `gradient_accumulation_steps`\n- Use smaller model (Ministral-8B instead of Pixtral-12B)\n- Enable gradient checkpointing\n\n### Poor Generation Quality\n\n**Symptom:** Model outputs nonsense for images\n\n**Fix:**\n- Increase training steps (50-100+)\n- Check dataset quality (image-text alignment)\n- Use lower learning rate (1e-4)\n- Ensure vision layers are being trained (`finetune_vision_layers=True`)\n\n### Data Collator Error\n\n**Symptom:** `KeyError` or shape mismatch in data collator\n\n**Fix:**\n- Use `UnslothVisionDataCollator(model, tokenizer)`\n- Ensure dataset has \"messages\" field with correct structure\n- Check that images are valid PIL.Image objects\n\n## Kernel Shutdown (Jupyter)\n\nVision models use significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## Use Cases\n\n- **OCR/Document Processing**: LaTeX equation recognition, receipt scanning\n- **Image Captioning**: Generate descriptions for images\n- **Visual QA**: Answer questions about image content\n- **Chart/Graph Analysis**: Extract data from visualizations\n- **Medical Imaging**: X-ray, scan analysis (with appropriate data)\n\n## When to Use This Skill\n\nUse when:\n- Fine-tuning models to understand images\n- Building OCR or document processing pipelines\n- Creating image captioning systems\n- Developing visual question-answering applications\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Standard SFT for text-only models\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              },
              {
                "name": "api",
                "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/api/SKILL.md",
                "frontmatter": {
                  "name": "api",
                  "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n"
                },
                "content": "# Ollama REST API\n\n## Overview\n\nThe Ollama REST API provides direct HTTP access to all Ollama functionality. Use the `requests` library for maximum control over API interactions.\n\n**Default Endpoint:** `http://localhost:11434` (or `http://ollama:11434` in containers)\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/api/tags` | GET | List available models |\n| `/api/show` | POST | Show model details |\n| `/api/ps` | GET | List running models |\n| `/api/generate` | POST | Generate text |\n| `/api/chat` | POST | Chat completion |\n| `/api/embed` | POST | Generate embeddings |\n| `/api/copy` | POST | Copy a model |\n| `/api/delete` | DELETE | Delete a model |\n\n## Setup\n\n```python\nimport os\nimport requests\nimport json\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n```\n\n## List Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/tags\")\nmodels = response.json()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/show\",\n    json={\"model\": \"llama3.2:latest\"}\n)\nmodel_info = response.json()\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Why is the sky blue?\",\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Count from 1 to 5.\",\n        \"stream\": True\n    },\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line:\n        chunk = json.loads(line)\n        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n        if chunk.get(\"done\"):\n            break\n```\n\n## Chat Completion\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/chat\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ],\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"message\"][\"content\"])\n```\n\n## Generate Embeddings\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/embed\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"input\": \"Ollama makes running LLMs locally easy.\"\n    }\n)\nresult = response.json()\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\n```\n\n## Copy Model\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/copy\",\n    json={\n        \"source\": \"llama3.2:latest\",\n        \"destination\": \"llama3.2-backup:latest\"\n    }\n)\nif response.status_code == 200:\n    print(\"Copy successful!\")\n```\n\n## Delete Model\n\n```python\nresponse = requests.delete(\n    f\"{OLLAMA_HOST}/api/delete\",\n    json={\"model\": \"llama3.2-backup:latest\"}\n)\nif response.status_code == 200:\n    print(\"Delete successful!\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = requests.post(\n        f\"{OLLAMA_HOST}/api/generate\",\n        json={\"model\": \"nonexistent\", \"prompt\": \"Hello\"},\n        timeout=30\n    )\n    if response.status_code != 200:\n        print(f\"Error: {response.status_code} - {response.text}\")\n    else:\n        result = response.json()\n        if \"error\" in result:\n            print(f\"API Error: {result['error']}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\nThe generate endpoint returns useful metrics:\n\n```python\nresult = response.json()\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## When to Use This Skill\n\nUse when:\n- You need direct control over HTTP requests\n- Debugging API interactions\n- Building custom integrations\n- Working with streaming responses\n- Checking raw API responses\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Higher-level Python library\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              },
              {
                "name": "gpu",
                "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n",
                "path": "bazzite-ai-ollama/skills/gpu/SKILL.md",
                "frontmatter": {
                  "name": "gpu",
                  "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n"
                },
                "content": "# GPU Monitoring for Ollama\n\n## Overview\n\nMonitor GPU usage and performance when running Ollama with GPU acceleration. This skill covers checking GPU status, VRAM usage, models loaded in GPU memory, and inference performance metrics.\n\n## Quick Reference\n\n| Check | Method |\n|-------|--------|\n| GPU status | `nvidia-smi` / `rocm-smi` |\n| Models in memory | `GET /api/ps` |\n| Inference metrics | Response metadata |\n| VRAM usage | Both nvidia-smi and /api/ps |\n\n## GPU Status Check\n\n### NVIDIA\n\n```python\nimport subprocess\n\ndef check_nvidia_gpu():\n    \"\"\"Check NVIDIA GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.used,memory.total,utilization.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            lines = result.stdout.strip().split(\"\\n\")\n            for i, line in enumerate(lines):\n                parts = line.split(\", \")\n                if len(parts) >= 4:\n                    name, mem_used, mem_total, util = parts\n                    print(f\"GPU {i}: {name}\")\n                    print(f\"  Memory: {mem_used} MB / {mem_total} MB\")\n                    print(f\"  Utilization: {util}%\")\n    except FileNotFoundError:\n        print(\"nvidia-smi not found - NVIDIA GPU may not be available\")\n    except subprocess.TimeoutExpired:\n        print(\"nvidia-smi timed out\")\n\ncheck_nvidia_gpu()\n```\n\n### AMD\n\n```python\nimport subprocess\n\ndef check_amd_gpu():\n    \"\"\"Check AMD GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"rocm-smi\", \"--showmeminfo\", \"vram\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        print(result.stdout)\n    except FileNotFoundError:\n        print(\"rocm-smi not found - AMD GPU may not be available\")\n\ncheck_amd_gpu()\n```\n\n## Models Loaded in GPU Memory\n\n```python\nimport os\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nif running.get(\"models\"):\n    print(\"=== Models Loaded in GPU Memory ===\")\n    for model in running[\"models\"]:\n        name = model.get(\"name\", \"Unknown\")\n        size = model.get(\"size\", 0) / (1024**3)\n        vram = model.get(\"size_vram\", 0) / (1024**3)\n        expires = model.get(\"expires_at\", \"N/A\")\n        print(f\"  - {name}\")\n        print(f\"    Total Size: {size:.2f} GB\")\n        print(f\"    VRAM Usage: {vram:.2f} GB\")\n        print(f\"    Expires: {expires}\")\nelse:\n    print(\"No models currently loaded in memory\")\n```\n\n## Inference Performance Metrics\n\n```python\nimport os\nimport time\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\n# Run inference\nstart_time = time.perf_counter()\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a haiku about computers.\",\n        \"stream\": False\n    }\n)\nend_time = time.perf_counter()\n\nresult = response.json()\n\nprint(f\"Response: {result['response']}\")\nprint()\nprint(\"=== Inference Metrics ===\")\nprint(f\"Wall clock time: {end_time - start_time:.2f}s\")\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens generated): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## GPU Usage During Inference\n\n```python\nimport os\nimport subprocess\nimport requests\nimport threading\nimport time\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\ndef monitor_gpu(stop_event, readings):\n    \"\"\"Monitor GPU usage in background.\"\"\"\n    while not stop_event.is_set():\n        try:\n            result = subprocess.run(\n                [\"nvidia-smi\",\n                 \"--query-gpu=utilization.gpu,memory.used\",\n                 \"--format=csv,noheader,nounits\"],\n                capture_output=True,\n                text=True,\n                timeout=1\n            )\n            if result.returncode == 0:\n                parts = result.stdout.strip().split(\", \")\n                if len(parts) >= 2:\n                    readings.append({\n                        \"util\": int(parts[0]),\n                        \"mem\": int(parts[1])\n                    })\n        except:\n            pass\n        time.sleep(0.5)\n\n# Start monitoring\nstop_event = threading.Event()\nreadings = []\nmonitor_thread = threading.Thread(target=monitor_gpu, args=(stop_event, readings))\nmonitor_thread.start()\n\n# Run inference\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a short story about AI.\",\n        \"stream\": False\n    }\n)\n\n# Stop monitoring\nstop_event.set()\nmonitor_thread.join()\n\n# Report\nif readings:\n    avg_util = sum(r[\"util\"] for r in readings) / len(readings)\n    max_mem = max(r[\"mem\"] for r in readings)\n    print(f\"Average GPU utilization: {avg_util:.1f}%\")\n    print(f\"Peak memory usage: {max_mem} MB\")\n```\n\n## Complete Health Check\n\n```python\nimport os\nimport subprocess\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\nDEFAULT_MODEL = \"llama3.2:latest\"\n\ndef complete_gpu_health_check():\n    \"\"\"Complete GPU and Ollama health check.\"\"\"\n    print(\"=== GPU Health Check ===\")\n    print()\n\n    # 1. Check GPU hardware\n    print(\"1. GPU Hardware:\")\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.total\",\n             \"--format=csv,noheader\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            print(f\"   {result.stdout.strip()}\")\n        else:\n            print(\"   nvidia-smi failed\")\n    except FileNotFoundError:\n        print(\"   NVIDIA GPU not detected\")\n\n    # 2. Check Ollama server\n    print()\n    print(\"2. Ollama Server:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            print(\"   Server is running\")\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            if DEFAULT_MODEL in model_names:\n                print(f\"   Model '{DEFAULT_MODEL}' available\")\n            else:\n                print(f\"   Model '{DEFAULT_MODEL}' NOT available\")\n        else:\n            print(f\"   Server error: {response.status_code}\")\n    except requests.exceptions.ConnectionError:\n        print(\"   Cannot connect to server\")\n\n    # 3. Check models in GPU memory\n    print()\n    print(\"3. Models in GPU Memory:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n        running = response.json()\n        if running.get(\"models\"):\n            for model in running[\"models\"]:\n                vram = model.get(\"size_vram\", 0) / (1024**3)\n                print(f\"   {model['name']}: {vram:.2f} GB VRAM\")\n        else:\n            print(\"   No models loaded\")\n    except:\n        print(\"   Cannot check running models\")\n\ncomplete_gpu_health_check()\n```\n\n## Model Size Guide\n\n| Model | Parameters | VRAM Needed | Tokens/sec (typical) |\n|-------|------------|-------------|----------------------|\n| phi3 | 3B | 4GB | 60-80 |\n| llama3.2 | 8B | 8GB | 40-60 |\n| mistral | 7B | 8GB | 40-60 |\n| codellama | 7B | 8GB | 40-60 |\n| llama3.2:70b | 70B | 48GB+ | 10-20 |\n\n## Troubleshooting\n\n### GPU Not Used\n\n**Symptom:** Low tokens/second, nvidia-smi shows 0% utilization\n\n**Check:**\n\n```bash\n# Check GPU inside container (adjust container name as needed)\ndocker exec -it ollama nvidia-smi\n# or\npodman exec -it ollama nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Restart Ollama container with GPU access\n# Refer to bazzite-ai-pod-ollama documentation for container setup\n```\n\n### Out of Memory\n\n**Symptom:** \"out of memory\" error during model loading\n\n**Fix:**\n\n```python\n# Use smaller/quantized model via API\nimport requests\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/pull\",\n    json={\"name\": \"llama3.2:7b-q4_0\"},\n    stream=True\n)\nfor line in response.iter_lines():\n    if line:\n        print(line.decode())\n```\n\n### Slow Inference\n\n**Symptom:** Very low tokens/second\n\n**Possible causes:**\n1. Model too large for VRAM (using CPU fallback)\n2. Wrong GPU type configured\n3. Driver issues\n\n**Check:**\n\n```python\n# Check VRAM usage vs model size\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n# If size_vram << size, model is partially on CPU\n```\n\n## When to Use This Skill\n\nUse when:\n- Debugging slow inference\n- Checking if GPU is being utilized\n- Monitoring VRAM usage\n- Benchmarking different models\n- Troubleshooting GPU issues\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - API for running inference\n- `bazzite-ai-ollama:python` - Python library for inference"
              },
              {
                "name": "huggingface",
                "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n",
                "path": "bazzite-ai-ollama/skills/huggingface/SKILL.md",
                "frontmatter": {
                  "name": "huggingface",
                  "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n"
                },
                "content": "# HuggingFace Model Import\n\n## Overview\n\nOllama can directly pull GGUF models from HuggingFace using the `hf.co/` prefix. This enables access to thousands of quantized models beyond the official Ollama library.\n\n## Quick Reference\n\n| Action | Syntax |\n|--------|--------|\n| Pull model | `hf.co/{org}/{repo}:{quantization}` |\n| List models | `ollama.list()` |\n| Use model | Same as any Ollama model |\n| Delete model | `ollama.delete(\"hf.co/...\")` |\n\n## Model Naming Format\n\n```\nhf.co/{organization}/{repository}-GGUF:{quantization}\n```\n\n**Examples:**\n\n```\nhf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\nhf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M\nhf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M\n```\n\n## Common Quantizations\n\n| Quantization | Size | Quality | Use Case |\n|--------------|------|---------|----------|\n| Q2_K | Smallest | Lowest | Testing only |\n| Q4_K_M | Medium | Good | Recommended default |\n| Q5_K_M | Larger | Better | Quality-focused |\n| Q6_K | Large | High | Near-original quality |\n| Q8_0 | Largest | Highest | Maximum quality |\n\n## Pull Model from HuggingFace\n\n### With Progress Tracking\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprint(f\"Pulling {HF_MODEL}...\")\n\nlast_status = \"\"\nfor progress in ollama.pull(HF_MODEL, stream=True):\n    status = progress.get(\"status\", \"\")\n    digest = progress.get(\"digest\", \"\")\n    total = progress.get(\"total\")\n\n    # Only print when status changes\n    if status != last_status:\n        if status == \"pulling manifest\":\n            print(f\"  {status}\")\n        elif status.startswith(\"pulling\") and digest:\n            short_digest = digest.split(\":\")[-1][:12] if \":\" in digest else digest[:12]\n            size_mb = (total / 1024 / 1024) if total else 0\n            if size_mb > 100:\n                print(f\"  pulling {short_digest}... ({size_mb:.0f} MB)\")\n        elif status in [\"verifying sha256 digest\", \"writing manifest\", \"success\"]:\n            print(f\"  {status}\")\n\n        last_status = status\n\nprint(\"Model pulled successfully!\")\n```\n\n### Simple Pull\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Non-streaming (blocks until complete)\nollama.pull(HF_MODEL)\nprint(\"Model pulled!\")\n```\n\n## Verify Installation\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodels = ollama.list()\nmodel_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n\n# Check for the HF model\nhf_model_installed = any(\n    \"Nous-Hermes\" in name or HF_MODEL in name\n    for name in model_names\n)\n\nif hf_model_installed:\n    print(\"Model is installed!\")\n    for name in model_names:\n        if \"Nous-Hermes\" in name or \"hf.co\" in name:\n            print(f\"  Name: {name}\")\nelse:\n    print(\"Model not found\")\n```\n\n## Show Model Details\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodel_info = ollama.show(HF_MODEL)\n\nprint(f\"Model: {HF_MODEL}\")\nif \"details\" in model_info:\n    details = model_info[\"details\"]\n    print(f\"Family: {details.get('family', 'N/A')}\")\n    print(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\n    print(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## Use Imported Model\n\n### Generate Text\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nresult = ollama.generate(\n    model=HF_MODEL,\n    prompt=\"What is the capital of France?\"\n)\nprint(result[\"response\"])\n```\n\n### Chat Completion\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Nous-Hermes-2 uses ChatML format natively\nresponse = ollama.chat(\n    model=HF_MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Hermes 2, a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in two sentences.\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n## Delete Imported Model\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nollama.delete(HF_MODEL)\nprint(\"Model deleted!\")\n```\n\n## Popular HuggingFace Models\n\n### General Purpose\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Nous-Hermes-2-Mistral | `hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M` | 4.4 GB |\n| Llama-2-7B-Chat | `hf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M` | 4.1 GB |\n| Mistral-7B-Instruct | `hf.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF:Q4_K_M` | 4.4 GB |\n\n### Code Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| CodeLlama-7B | `hf.co/TheBloke/CodeLlama-7B-Instruct-GGUF:Q4_K_M` | 4.1 GB |\n| Phind-CodeLlama | `hf.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF:Q4_K_M` | 20 GB |\n| WizardCoder | `hf.co/TheBloke/WizardCoder-Python-7B-V1.0-GGUF:Q4_K_M` | 4.1 GB |\n\n### Small/Fast Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Phi-3-mini | `hf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M` | 2.4 GB |\n| TinyLlama | `hf.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q4_K_M` | 0.7 GB |\n\n## Finding Models on HuggingFace\n\n1. Go to [huggingface.co/models](https://huggingface.co/models)\n2. Filter by:\n   - **Library:** GGUF\n   - **Task:** Text Generation\n3. Look for models with `-GGUF` suffix\n4. Check the \"Files\" tab for available quantizations\n\n## Troubleshooting\n\n### Model Not Found\n\n**Symptom:** Error pulling model\n\n**Check:**\n- Repository exists on HuggingFace\n- Repository has GGUF files\n- Quantization tag is correct\n\n```python\n# Verify HuggingFace URL\n# https://huggingface.co/{org}/{repo}/tree/main\n```\n\n### Download Fails\n\n**Symptom:** Download interrupted or fails\n\n**Fix:**\n- Check internet connection\n- Try again (Ollama resumes partial downloads)\n- Check disk space\n\n### Wrong Prompt Format\n\n**Symptom:** Model gives poor responses\n\n**Fix:**\n- Check model card for correct prompt template\n- Some models require specific formats (ChatML, Alpaca, etc.)\n\n```python\n# ChatML format example (Nous-Hermes-2)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\n# The ollama library handles format conversion automatically\n```\n\n## When to Use This Skill\n\nUse when:\n- You need a model not in the official Ollama library\n- Testing specific model variants\n- Using specialized/fine-tuned models\n- Comparing different quantizations\n\n## Resources\n\n- [Ollama Import Docs](https://docs.ollama.com/import)\n- [HuggingFace Ollama Integration](https://huggingface.co/docs/hub/ollama)\n- [TheBloke's GGUF Models](https://huggingface.co/TheBloke)\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Using imported models\n- `bazzite-ai-ollama:api` - REST API for model management"
              },
              {
                "name": "openai",
                "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n",
                "path": "bazzite-ai-ollama/skills/openai/SKILL.md",
                "frontmatter": {
                  "name": "openai",
                  "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n"
                },
                "content": "# Ollama OpenAI Compatibility\n\n## Overview\n\nOllama provides an OpenAI-compatible API at `/v1/*` endpoints. This allows using the official `openai` Python library with Ollama, enabling:\n\n- **Migration** - Drop-in replacement for OpenAI API\n- **Tool ecosystem** - Works with LangChain, LlamaIndex, etc.\n- **Familiar interface** - Standard OpenAI patterns\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/v1/models` | GET | List models |\n| `/v1/completions` | POST | Text generation |\n| `/v1/chat/completions` | POST | Chat completion |\n| `/v1/embeddings` | POST | Generate embeddings |\n\n### Limitations\n\nThe OpenAI compatibility layer does **not** support:\n\n- Show model details (`/api/show`)\n- List running models (`/api/ps`)\n- Copy model (`/api/copy`)\n- Delete model (`/api/delete`)\n\nUse `bazzite-ai-ollama:api` or `bazzite-ai-ollama:python` for these operations.\n\n## Setup\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nclient = OpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\"  # Required by library but ignored by Ollama\n)\n```\n\n## List Models\n\n```python\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"  - {model.id}\")\n```\n\n## Text Completions\n\n```python\nresponse = client.completions.create(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\",\n    max_tokens=100\n)\n\nprint(response.choices[0].text)\nprint(f\"Tokens used: {response.usage.completion_tokens}\")\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n    ],\n    temperature=0.7,\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\nprint(f\"Tokens used: {response.usage.total_tokens}\")\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}\n]\n\n# Turn 1\nmessages.append({\"role\": \"user\", \"content\": \"What is 2 + 2?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nassistant_msg = response.choices[0].message.content\nmessages.append({\"role\": \"assistant\", \"content\": assistant_msg})\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {assistant_msg}\")\n\n# Turn 2\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response.choices[0].message.content}\")\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = client.chat.completions.create(\n        model=\"invalid-model\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n```\n\n## Migration from OpenAI\n\n### Before (OpenAI)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()  # Uses OPENAI_API_KEY\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### After (Ollama)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",  # Change model name\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain Integration\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n## LlamaIndex Integration\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(\n    api_base=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.complete(\"What is Python?\")\nprint(response.text)\n```\n\n## Connection Health Check\n\n```python\nimport requests\n\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Migrating from OpenAI to local LLMs\n- Using LangChain, LlamaIndex, or other OpenAI-based tools\n- You prefer the OpenAI client interface\n- Building applications that may switch between OpenAI and Ollama\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Native Ollama library (more features)\n- `bazzite-ai-ollama:api` - Direct REST API access"
              },
              {
                "name": "python",
                "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/python/SKILL.md",
                "frontmatter": {
                  "name": "python",
                  "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n"
                },
                "content": "# Ollama Python Library\n\n## Overview\n\nThe official `ollama` Python library provides a clean, Pythonic interface to all Ollama functionality. It automatically connects to the Ollama server and handles serialization.\n\n## Quick Reference\n\n| Function | Purpose |\n|----------|---------|\n| `ollama.list()` | List available models |\n| `ollama.show()` | Show model details |\n| `ollama.ps()` | List running models |\n| `ollama.generate()` | Generate text |\n| `ollama.chat()` | Chat completion |\n| `ollama.embed()` | Generate embeddings |\n| `ollama.copy()` | Copy a model |\n| `ollama.delete()` | Delete a model |\n| `ollama.pull()` | Pull a model |\n\n## Setup\n\n```python\nimport ollama\n\n# The library automatically uses OLLAMA_HOST environment variable\n# Default: http://localhost:11434\n```\n\n## List Models\n\n```python\nmodels = ollama.list()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['model']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nmodel_info = ollama.show(\"llama3.2:latest\")\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nrunning = ollama.ps()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresult = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\"\n)\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nstream = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Count from 1 to 5.\",\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"response\"], end=\"\", flush=True)\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is Python?\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n]\n\n# First turn\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {response['message']['content']}\")\n\n# Continue conversation\nmessages.append(response[\"message\"])\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response['message']['content']}\")\n```\n\n### Streaming Chat\n\n```python\nstream = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresult = ollama.embed(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\nprint(f\"First 5 values: {embeddings[:5]}\")\n```\n\n## Model Management\n\n### Copy Model\n\n```python\nollama.copy(source=\"llama3.2:latest\", destination=\"llama3.2-backup:latest\")\nprint(\"Copy successful!\")\n```\n\n### Delete Model\n\n```python\nollama.delete(\"llama3.2-backup:latest\")\nprint(\"Delete successful!\")\n```\n\n### Pull Model\n\n```python\n# Non-streaming\nollama.pull(\"llama3.2:latest\")\n\n# With progress\nfor progress in ollama.pull(\"llama3.2:latest\", stream=True):\n    status = progress.get(\"status\", \"\")\n    print(status)\n```\n\n## Error Handling\n\n```python\ntry:\n    result = ollama.generate(\n        model=\"nonexistent-model\",\n        prompt=\"Hello\"\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}: {e}\")\n\n# Connection check\ntry:\n    models = ollama.list()\n    print(\"Ollama server is running!\")\nexcept Exception as e:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        models = ollama.list()\n        model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n        return True, model in model_names\n    except Exception:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\n```python\nresult = ollama.generate(model=\"llama3.2:latest\", prompt=\"Hello!\")\n\nprint(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.2f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## Common Patterns\n\n### Conversation Class\n\n```python\nclass Conversation:\n    def __init__(self, model=\"llama3.2:latest\", system_prompt=None):\n        self.model = model\n        self.messages = []\n        if system_prompt:\n            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    def chat(self, user_message):\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n        response = ollama.chat(model=self.model, messages=self.messages)\n        assistant_message = response[\"message\"]\n        self.messages.append(assistant_message)\n        return assistant_message[\"content\"]\n\n# Usage\nconv = Conversation(system_prompt=\"You are a helpful assistant.\")\nprint(conv.chat(\"What is Python?\"))\nprint(conv.chat(\"What are its main features?\"))\n```\n\n## When to Use This Skill\n\nUse when:\n\n- You want a clean, Pythonic interface\n- Building Python applications\n- Need IDE autocompletion support\n- Working with multi-turn conversations\n- Prefer not to handle HTTP directly\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - Direct REST API access\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              },
              {
                "name": "apptainer",
                "description": "Apptainer (Singularity) container management for HPC workloads. Build SIF\nimages, run containers with GPU passthrough. Use when users need HPC-compatible\ncontainerization or need to pull/run Apptainer images.\n",
                "path": "bazzite-ai/skills/apptainer/SKILL.md",
                "frontmatter": {
                  "name": "apptainer",
                  "description": "Apptainer (Singularity) container management for HPC workloads. Build SIF\nimages, run containers with GPU passthrough. Use when users need HPC-compatible\ncontainerization or need to pull/run Apptainer images.\n"
                },
                "content": "# Apptainer - HPC Container Management\n\n## Overview\n\nThe `apptainer` command manages Apptainer (formerly Singularity) containers for HPC-compatible workloads. It provides SIF image management with automatic GPU detection.\n\n**Key Concept:** Apptainer is the HPC standard. Unlike Docker/Podman, containers run as the user (no root). SIF files are single-file images.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Pull | `ujust apptainer pull [--image=...] [--tag=...]` | Download image to SIF |\n| Run | `ujust apptainer run [--image=...] [-- CMD...]` | Run container |\n| Shell | `ujust apptainer shell [--image=...]` | Interactive shell |\n| Exec | `ujust apptainer exec [--image=...] [-- CMD...]` | Execute command |\n| Build | `ujust apptainer build [--image=...] [--tag=...]` | Build from definition |\n| Inspect | `ujust apptainer inspect [--image=...]` | Show metadata |\n| GPU | `ujust apptainer gpu` | Test GPU support |\n| Cache | `ujust apptainer cache [--tag=clean\\|list]` | Manage cache |\n| Help | `ujust apptainer help` | Show help |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: pull, run, shell, exec, build, inspect, gpu, cache |\n| image | `--image` | `-i` | `\"\"` | SIF file path, image name, or DEF file |\n| tag | `--tag` | `-t` | `\"\"` | Image tag, output file, or cache subaction |\n| cmd | (variadic) | - | `\"\"` | Command to execute (use `--` separator) |\n\n## Pull Images\n\n### bazzite-ai Pod Images\n\n```bash\n# Pull nvidia-python (long form)\nujust apptainer pull --image=nvidia-python\n\n# Pull with tag (long form)\nujust apptainer pull --image=nvidia-python --tag=testing\n\n# Pull nvidia-python (short form)\nujust apptainer pull -i nvidia-python\n\n# Pull with tag (short form)\nujust apptainer pull -i nvidia-python -t testing\n\n# Pull jupyter\nujust apptainer pull --image=jupyter --tag=stable\n```\n\n### External Images\n\n```bash\n# Docker Hub\nujust apptainer pull --image=docker://ubuntu:22.04\n\n# NVIDIA NGC\nujust apptainer pull --image=docker://nvcr.io/nvidia/pytorch:latest\n\n# Sylabs Cloud\nujust apptainer pull --image=library://sylabsed/examples/lolcow\n```\n\n### Pull Output\n\nImages are saved as SIF files:\n\n```\n~/.local/share/apptainer/bazzite-ai-pod-nvidia-python.sif\n```\n\n## Run Containers\n\n### Run with Default Command\n\n```bash\n# Run nvidia-python (long form)\nujust apptainer run --image=nvidia-python\n\n# Run nvidia-python (short form)\nujust apptainer run -i nvidia-python\n\n# Run specific SIF file\nujust apptainer run --image=./my-container.sif\n```\n\n### Run with Command\n\n```bash\n# Run Python in container (use -- separator for commands)\nujust apptainer run --image=nvidia-python -- python\n\n# Run script\nujust apptainer run --image=nvidia-python -- python script.py\n\n# Short form\nujust apptainer run -i nvidia-python -- python train.py\n```\n\n### GPU Auto-Detection\n\nGPU flags are auto-detected:\n\n- NVIDIA: Adds `--nv`\n- AMD: Adds `--rocm`\n\n```bash\n# GPU is automatically enabled\nujust apptainer run --image=nvidia-python -- python -c \"import torch; print(torch.cuda.is_available())\"\n```\n\n## Interactive Shell\n\n```bash\n# Shell into container (long form)\nujust apptainer shell --image=nvidia-python\n\n# Shell into container (short form)\nujust apptainer shell -i nvidia-python\n\n# Now inside container\npython --version\nnvidia-smi\nexit\n```\n\n## Execute Commands\n\n```bash\n# Execute single command (use -- separator)\nujust apptainer exec --image=nvidia-python -- pip list\n\n# Execute Python one-liner\nujust apptainer exec -i nvidia-python -- python -c 'print(1+1)'\n```\n\n## Build from Definition\n\n### Definition File Example\n\n```def\nBootstrap: docker\nFrom: ubuntu:22.04\n\n%post\n    apt-get update\n    apt-get install -y python3 python3-pip\n\n%runscript\n    python3 \"$@\"\n```\n\n### Build\n\n```bash\n# Build SIF from definition (image=DEF, tag=OUTPUT)\nujust apptainer build --image=mydef.def --tag=myimage.sif\n\n# Build to default location\nujust apptainer build --image=mydef.def\n\n# Short form\nujust apptainer build -i mydef.def -t myimage.sif\n```\n\n## GPU Support\n\n### Test GPU\n\n```bash\n# Detect and test GPU\nujust apptainer gpu\n```\n\n### GPU Flags\n\n| GPU | Flag | Auto-Detection |\n|-----|------|----------------|\n| NVIDIA | `--nv` | Yes |\n| AMD | `--rocm` | Yes |\n| Intel | (none yet) | No |\n\n### Manual GPU Override\n\n```bash\n# Direct apptainer command with GPU\napptainer run --nv nvidia-python.sif nvidia-smi\n```\n\n## Cache Management\n\n### List Cache\n\n```bash\n# Long form\nujust apptainer cache --tag=list\n\n# Or\nujust apptainer cache list\n```\n\n### Clean Cache\n\n```bash\n# Long form\nujust apptainer cache --tag=clean\n\n# Or\nujust apptainer cache clean\n```\n\nCache is stored in `~/.apptainer/cache/`.\n\n## Common Workflows\n\n### HPC Development\n\n```bash\n# Pull HPC-ready image\nujust apptainer pull --image=nvidia-python\n\n# Test GPU\nujust apptainer gpu\n\n# Development shell\nujust apptainer shell --image=nvidia-python\n\n# Run production workload\nujust apptainer run --image=nvidia-python -- python train.py\n```\n\n### Use NGC Images\n\n```bash\n# Pull NVIDIA PyTorch\nujust apptainer pull --image=docker://nvcr.io/nvidia/pytorch:23.10-py3\n\n# Run training\nujust apptainer run --image=pytorch_23.10-py3.sif -- python train.py\n```\n\n### Build Custom Image\n\n```bash\n# Create definition file\ncat > myenv.def << 'EOF'\nBootstrap: docker\nFrom: python:3.11\n\n%post\n    pip install numpy pandas scikit-learn\n\n%runscript\n    python \"$@\"\nEOF\n\n# Build\nujust apptainer build --image=myenv.def --tag=myenv.sif\n\n# Test\nujust apptainer run --image=myenv.sif -- python -c \"import numpy; print(numpy.__version__)\"\n```\n\n## Apptainer vs Docker/Podman\n\n| Feature | Apptainer | Docker/Podman |\n|---------|-----------|---------------|\n| Root required | No | Sometimes |\n| Single file | Yes (SIF) | No (layers) |\n| HPC compatible | Yes | Limited |\n| GPU support | --nv, --rocm | nvidia-docker |\n| Security model | User namespace | Container namespace |\n\n**Use Apptainer when:**\n\n- Running on HPC clusters\n- Need single-file portability\n- Can't run as root\n- Need reproducibility\n\n## Troubleshooting\n\n### Pull Failed\n\n**Check:**\n\n```bash\n# Test network\ncurl -I https://ghcr.io\n\n# Check registry auth\napptainer remote list\n```\n\n**Fix:**\n\n```bash\n# Login to registry\napptainer remote login docker://ghcr.io\n```\n\n### GPU Not Available\n\n**Check:**\n\n```bash\nujust apptainer gpu\nnvidia-smi  # or rocm-smi\n```\n\n**Fix:**\n\n```bash\n# Ensure drivers installed\n# For NVIDIA:\nnvidia-smi\n# For AMD:\nrocm-smi\n```\n\n### SIF File Corrupted\n\n**Fix:**\n\n```bash\n# Remove and re-pull\nrm ~/.local/share/apptainer/*.sif\nujust apptainer pull --image=nvidia-python\n```\n\n### Cache Too Large\n\n**Check:**\n\n```bash\ndu -sh ~/.apptainer/cache/\n```\n\n**Fix:**\n\n```bash\nujust apptainer cache --tag=clean\n```\n\n## Cross-References\n\n- **Related Skills:** `pod` (build OCI images), `jupyter` (uses containers)\n- **GPU Setup:** `ujust config gpu setup`\n- **Apptainer Docs:** <https://apptainer.org/docs/>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"apptainer\", \"singularity\", \"HPC container\"\n- \"SIF file\", \"pull image\", \"build container\"\n- \"apptainer GPU\", \"run with GPU\"\n- \"HPC workload\", \"cluster container\""
              },
              {
                "name": "bootc",
                "description": "bootc VM management via bcvk (bootc virtualization kit). Run bootable\ncontainers as VMs for testing. Supports ephemeral (quick test) and\npersistent modes. Use when users need to test bootable container images\nas virtual machines.\n",
                "path": "bazzite-ai/skills/bootc/SKILL.md",
                "frontmatter": {
                  "name": "bootc",
                  "description": "bootc VM management via bcvk (bootc virtualization kit). Run bootable\ncontainers as VMs for testing. Supports ephemeral (quick test) and\npersistent modes. Use when users need to test bootable container images\nas virtual machines.\n"
                },
                "content": "# Bootc - bootc-based VM Management\n\n## Overview\n\nThe `bootc` command manages bootable container VMs using bcvk (bootc virtualization kit). It converts OCI container images into bootable VMs for testing.\n\n**Key Concept:** Unlike traditional VMs, bootc VMs are created directly from container images. This enables testing bootable containers without building disk images first.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Test | `ujust test bootc [--image=...] [--cpus=...]` | Ephemeral VM (deleted on exit) |\n| Add | `ujust bootc add [NAME] [--image=...] [--cpus=...]` | Create persistent VM |\n| List | `ujust bootc list` | List all VMs |\n| Status | `ujust bootc status [NAME]` | Show VM status |\n| SSH | `ujust bootc ssh [NAME] [--ssh-user=...]` | Connect to VM |\n| Start | `ujust bootc start [NAME]` | Start VM |\n| Stop | `ujust bootc stop [NAME]` | Stop VM |\n| Delete | `ujust bootc delete [NAME]` | Remove VM |\n| Export | `ujust bootc export [--image=...] [--format=...]` | Export to disk image |\n| Images | `ujust bootc images` | List available images |\n| Help | `ujust bootc help` | Show help |\n\n## Prerequisites\n\n```bash\n# Install bcvk\nujust install bcvk\n\n# Verify installation\nbcvk --version\n```\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: add, list, status, ssh, etc. |\n| vm_name | (positional) | - | `bazzite-bootc` | VM name |\n| image | `--image` | `-i` | (varies) | Container image to boot |\n| cpus | `--cpus` | - | `2` | Number of CPUs |\n| ram | `--ram` | - | `4096` | Memory in MB |\n| disk_size | `--disk-size` | - | `20G` | Disk size |\n| format | `--format` | `-f` | `qcow2` | Export format (qcow2, raw) |\n| ssh_port | `--ssh-port` | - | `2222` | SSH port |\n| ssh_user | `--ssh-user` | - | `root` | SSH user |\n\n## Ephemeral Testing\n\nQuick test that auto-deletes VM on exit:\n\n```bash\n# Test default bazzite-ai image\nujust test bootc\n\n# Test specific image (long form)\nujust test bootc --image=ghcr.io/org/image:tag\n\n# Test specific image (short form)\nujust test bootc -i ghcr.io/org/image:tag\n\n# Test with more resources\nujust test bootc --image=myimage --cpus=4 --ram=8192\n\n# Short form\nujust test bootc -i myimage --cpus=4 --ram=8192\n```\n\nEphemeral mode:\n\n- Creates temporary VM\n- Boots to console\n- VM deleted when console exits\n\n## Persistent VMs\n\nCreate VMs that persist across sessions:\n\n```bash\n# Create VM with default image\nujust bootc add dev\n\n# Create with specific image (long form)\nujust bootc add testing --image=ghcr.io/org/image:testing\n\n# Create with specific image (short form)\nujust bootc add testing -i ghcr.io/org/image:testing\n\n# Custom resources\nujust bootc add heavy --cpus=8 --ram=16384 --disk-size=100G\n```\n\n### Manage Persistent VMs\n\n```bash\n# Start VM\nujust bootc start dev\n\n# Stop VM\nujust bootc stop dev\n\n# Delete VM\nujust bootc delete dev\n```\n\n## Connecting to VMs\n\n### SSH Connection\n\n```bash\n# Connect to VM\nujust bootc ssh dev\n\n# Run command (use -- separator)\nujust bootc ssh dev -- systemctl status\n\n# Different user\nujust bootc ssh dev --ssh-user=admin\n```\n\nDefault: `ssh -p 2222 root@localhost`\n\n### List VMs\n\n```bash\nujust bootc list\n```\n\nOutput:\n\n```\nNAME         STATE    IMAGE\ndev          running  ghcr.io/org/image:latest\ntesting      stopped  ghcr.io/org/image:testing\n```\n\n### Check Status\n\n```bash\nujust bootc status dev\n```\n\n## Export Disk Images\n\nConvert bootable container to disk image:\n\n```bash\n# Export to QCOW2 (long form)\nujust bootc export --image=ghcr.io/org/image:tag\n\n# Export to QCOW2 (short form)\nujust bootc export -i ghcr.io/org/image:tag\n\n# Export to raw (long form)\nujust bootc export --image=ghcr.io/org/image:tag --format=raw\n\n# Export to raw (short form)\nujust bootc export -i ghcr.io/org/image:tag -f raw\n```\n\nSupported formats:\n\n- `qcow2` - QEMU disk image\n- `raw` - Raw disk image\n\n## Common Workflows\n\n### Quick Test New Image\n\n```bash\n# Test ephemeral (no cleanup needed)\nujust test bootc --image=ghcr.io/myorg/myimage:dev\n# Exit console to destroy VM\n\n# Short form\nujust test bootc -i ghcr.io/myorg/myimage:dev\n```\n\n### Development Environment\n\n```bash\n# Create persistent VM (long form)\nujust bootc add dev --image=ghcr.io/myorg/myimage:latest\n\n# Or short form\nujust bootc add dev -i ghcr.io/myorg/myimage:latest\n\n# Start it\nujust bootc start dev\n\n# SSH in\nujust bootc ssh dev\n\n# Make changes, test...\n\n# Stop when done\nujust bootc stop dev\n```\n\n### Test Before Release\n\n```bash\n# Test testing branch\nujust test bootc --image=ghcr.io/myorg/myimage:testing\n\n# If good, test stable\nujust test bootc --image=ghcr.io/myorg/myimage:stable\n```\n\n### Create Installation Media\n\n```bash\n# Export to QCOW2 for cloud (long form)\nujust bootc export --image=ghcr.io/myorg/myimage:stable --format=qcow2\n\n# Export to QCOW2 for cloud (short form)\nujust bootc export -i ghcr.io/myorg/myimage:stable -f qcow2\n\n# Export to raw for disk imaging\nujust bootc export -i ghcr.io/myorg/myimage:stable -f raw\n```\n\n## bcvk vs vm Command\n\n| Feature | `ujust bootc` (bcvk) | `ujust vm` (libvirt) |\n|---------|----------------------|----------------------|\n| Image source | Container images | QCOW2 files |\n| Ephemeral mode | Yes | No |\n| Export formats | qcow2/raw | N/A |\n| SSH port | 2222 (fixed) | 4444 (configurable) |\n| Home sharing | No | Yes (virtiofs) |\n| Boot time | Faster | Slower |\n| Use case | Testing containers | Full VMs |\n\n**Use `bootc` when:**\n\n- Testing bootable container images\n- Quick ephemeral tests\n- Building disk images from containers\n\n**Use `vm` when:**\n\n- Need persistent VMs with home sharing\n- Need configurable ports\n- Need full libvirt features\n\n## Troubleshooting\n\n### bcvk Not Found\n\n**Fix:**\n\n```bash\nujust install bcvk\n```\n\n### VM Won't Start\n\n**Check:**\n\n```bash\nujust bootc status dev\nujust bootc list\n```\n\n**Common causes:**\n\n- Image not pulled\n- Resource conflict\n- Disk full\n\n**Fix:**\n\n```bash\nujust bootc delete dev\nujust bootc add dev\n```\n\n### SSH Connection Failed\n\n**Check:**\n\n```bash\nssh -p 2222 root@localhost\n```\n\n**Common causes:**\n\n- VM still booting\n- Port conflict (2222 used)\n- SSH not started\n\n**Fix:**\n\n```bash\n# Wait for boot\nsleep 30\nujust bootc ssh dev\n\n# Or check console\nujust test bootc  # Watch boot process\n```\n\n### Image Pull Failed\n\n**Check:**\n\n```bash\npodman pull ghcr.io/org/image:tag\n```\n\n**Common causes:**\n\n- Network issue\n- Auth required\n- Image doesn't exist\n\n**Fix:**\n\n```bash\n# Login to registry\npodman login ghcr.io\n\n# Pull manually\npodman pull ghcr.io/org/image:tag\n\n# Retry\nujust bootc add dev --image=ghcr.io/org/image:tag\n```\n\n## Cross-References\n\n- **Related Skills:** `vm` (traditional VMs), `install` (bcvk installation)\n- **Installation:** `ujust install bcvk`\n- **bcvk Docs:** <https://github.com/containers/bcvk>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"bootc VM\", \"bootable container\", \"test container as VM\"\n- \"bcvk\", \"bootc virtualization\"\n- \"ephemeral VM\", \"quick test VM\"\n- \"export to qcow2\", \"create ISO from container\""
              },
              {
                "name": "comfyui",
                "description": "ComfyUI node-based Stable Diffusion interface. GPU-accelerated image\ngeneration with custom node support and CivitAI model downloads.\nUse 'ujust comfyui' for configuration, lifecycle management, and\nmodel/node operations.\n",
                "path": "bazzite-ai/skills/comfyui/SKILL.md",
                "frontmatter": {
                  "name": "comfyui",
                  "description": "ComfyUI node-based Stable Diffusion interface. GPU-accelerated image\ngeneration with custom node support and CivitAI model downloads.\nUse 'ujust comfyui' for configuration, lifecycle management, and\nmodel/node operations.\n"
                },
                "content": "# ComfyUI - Stable Diffusion Interface\n\n## Overview\n\nComfyUI is a powerful node-based Stable Diffusion interface for AI image generation. The `comfyui` command manages the ComfyUI container, including configuration, lifecycle management, model downloads, and custom node management.\n\n**Key Concept:** This is a **system command** - run with `ujust` from anywhere on the system. ComfyUI runs as a Podman Quadlet service. By default, data is ephemeral (stored inside the container). Configure volume mounts for persistent storage.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust comfyui config [--models-dir=...] [--output-dir=...] [--port=...]` | Configure ComfyUI |\n| Start | `ujust comfyui start` | Start ComfyUI server |\n| Stop | `ujust comfyui stop` | Stop ComfyUI server |\n| Restart | `ujust comfyui restart` | Restart ComfyUI server |\n| Status | `ujust comfyui status` | Show status and model counts |\n| Logs | `ujust comfyui logs [--lines=...]` | View service logs |\n| Open | `ujust comfyui open` | Open UI in browser |\n| Shell | `ujust comfyui shell [-- CMD...]` | Open shell in container |\n| Download model | `ujust comfyui download --model-url=<url> --model-type=<type>` | Download from CivitAI |\n| List models | `ujust comfyui models` | List installed models |\n| Install node | `ujust comfyui node-install --node-url=<url>` | Install custom node |\n| List nodes | `ujust comfyui node-list` | List custom nodes |\n| Update nodes | `ujust comfyui node-update` | Update all nodes |\n| Delete | `ujust comfyui delete` | Remove ComfyUI and images |\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `8188` | Web UI port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU: nvidia/amd/intel/auto |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Models Dir | `--models-dir` | - | (empty) | Path for SD models |\n| Output Dir | `--output-dir` | - | (empty) | Path for generated images |\n| Input Dir | `--input-dir` | - | (empty) | Path for input images |\n| Nodes Dir | `--nodes-dir` | - | (empty) | Path for custom nodes |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n| Instance | `--instance` | `-n` | `1` | Instance number |\n\n**Important:** All directory parameters default to empty. When empty, data is stored inside the container and will be **lost when the container is recreated**. For persistent storage, provide explicit paths.\n\n### Configuration Examples\n\n```bash\n# Ephemeral mode - no persistent storage (data lost on container recreation)\nujust comfyui config\n\n# Persist models only (most common)\nujust comfyui config --models-dir=/data/models\n\n# Persist models and output\nujust comfyui config --models-dir=/data/models --output-dir=/data/output\n\n# Persist models and custom_nodes\nujust comfyui config --models-dir=/data/models --nodes-dir=/data/nodes\n\n# All directories with custom port and GPU\nujust comfyui config --models-dir=/data/models --output-dir=/data/output \\\n  --input-dir=/data/input --nodes-dir=/data/nodes --port=8189 --gpu-type=nvidia\n\n# With short forms\nujust comfyui config -p 8189 -g nvidia --models-dir=/data/models\n\n# Network-wide access\nujust comfyui config --bind=0.0.0.0\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed:\n\n```bash\n# Initially configured with defaults\nujust comfyui config\n\n# Later, add models directory (other settings preserved)\nujust comfyui config --models-dir=/data/models\n```\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust comfyui shell\n\n# Run specific command (use -- separator)\nujust comfyui shell -- pip list\nujust comfyui shell -- nvidia-smi\n```\n\n## Model Downloads\n\n### download\n\n```bash\nujust comfyui download --model-url=<URL> --model-type=<TYPE>\n```\n\n| Parameter | Flag | Description |\n|-----------|------|-------------|\n| URL | `--model-url` | CivitAI URL, model ID, or direct download URL |\n| Type | `--model-type` | Model type (see below) |\n\n**Requires:** `--models-dir` must be configured (not ephemeral)\n\n**Model Types:**\n\n| Type | Directory | Description |\n|------|-----------|-------------|\n| `checkpoint` | checkpoints/ | Main SD models |\n| `lora` | loras/ | LoRA adapters |\n| `vae` | vae/ | VAE models |\n| `embedding` | embeddings/ | Textual inversions |\n| `controlnet` | controlnet/ | ControlNet models |\n| `upscale` | upscale_models/ | Upscaler models |\n\n### Download Examples\n\n```bash\n# By CivitAI URL\nujust comfyui download --model-url=https://civitai.com/models/101055 --model-type=checkpoint\n\n# By model ID\nujust comfyui download --model-url=101055 --model-type=checkpoint\n\n# LoRA model\nujust comfyui download --model-url=123456 --model-type=lora\n\n# Direct URL\nujust comfyui download --model-url=https://example.com/model.safetensors --model-type=vae\n```\n\n## Custom Nodes\n\n### node-install\n\n```bash\nujust comfyui node-install --node-url=<GIT_URL>\n```\n\n**Requires:** `--nodes-dir` must be configured (not ephemeral)\n\n| Parameter | Flag | Description |\n|-----------|------|-------------|\n| GIT_URL | `--node-url` | Git repository URL for custom node |\n\n### Popular Custom Nodes\n\n```bash\n# ComfyUI-Manager (recommended)\nujust comfyui node-install --node-url=https://github.com/ltdrdata/ComfyUI-Manager\n\n# Impact Pack\nujust comfyui node-install --node-url=https://github.com/ltdrdata/ComfyUI-Impact-Pack\n\n# ControlNet Aux\nujust comfyui node-install --node-url=https://github.com/Fannovel16/comfyui_controlnet_aux\n\n# List installed nodes\nujust comfyui node-list\n\n# Update all nodes\nujust comfyui node-update\n```\n\n## Data Storage\n\n### Ephemeral Mode (Default)\n\nWhen no directories are configured, ComfyUI uses internal container directories:\n\n- Data is stored inside the container\n- **All data is lost** when container is recreated\n- Suitable for testing or temporary use\n\n### Persistent Mode\n\nWhen directories are configured, they are mounted into the container:\n\n```\n/path/to/models/           # Your MODELS_DIR\n checkpoints/           # Main SD models (.safetensors, .ckpt)\n loras/                 # LoRA adapters\n vae/                   # VAE models\n embeddings/            # Textual inversions\n controlnet/            # ControlNet models\n upscale_models/        # Upscaler models\n\n/path/to/output/           # Your OUTPUT_DIR - generated images\n/path/to/input/            # Your INPUT_DIR - input images for img2img\n/path/to/custom_nodes/     # Your CUSTOM_NODES_DIR - node extensions\n```\n\n## Common Workflows\n\n### Initial Setup (Persistent)\n\n```bash\n# 1. Configure with persistent models directory\nujust comfyui config --models-dir=/data/comfyui/models\n\n# 2. Download a checkpoint model\nujust comfyui download --model-url=https://civitai.com/models/101055 --model-type=checkpoint\n\n# 3. Start ComfyUI\nujust comfyui start\n\n# 4. Open in browser\nujust comfyui open\n```\n\n### Quick Test (Ephemeral)\n\n```bash\n# 1. Configure with defaults (ephemeral)\nujust comfyui config\n\n# 2. Start ComfyUI\nujust comfyui start\n\n# 3. Open in browser\nujust comfyui open\n\n# Note: Download models via the UI - they will be lost on container recreation\n```\n\n### Daily Usage\n\n```bash\n# Start ComfyUI\nujust comfyui start\n\n# Open in browser\nujust comfyui open\n\n# View logs\nujust comfyui logs\n\n# Stop when done\nujust comfyui stop\n```\n\n## GPU Support\n\nComfyUI automatically detects and configures GPU acceleration:\n\n| GPU | Configuration | Performance |\n|-----|---------------|-------------|\n| **NVIDIA** | CDI device passthrough | Full CUDA acceleration |\n| **AMD** | /dev/dri + /dev/kfd | ROCm acceleration |\n| **Intel** | /dev/dri | oneAPI acceleration |\n| **CPU** | Fallback mode | Very slow (not recommended) |\n\n### NVIDIA Setup\n\nIf NVIDIA GPU is not detected:\n\n```bash\n# Generate CDI specification\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n\n# Reconfigure ComfyUI\nujust comfyui delete\nujust comfyui config /data/models\n```\n\n## Troubleshooting\n\n### Model/Node Commands Fail\n\n**Symptom:** \"No MODELS_DIR configured\" or \"No CUSTOM_NODES_DIR configured\"\n\n**Cause:** Using ephemeral mode (no directories configured)\n\n**Fix:** Reconfigure with persistent directories:\n\n```bash\n# Add models directory\nujust comfyui config --models-dir=/path/to/models\n\n# Or add both models and custom_nodes\nujust comfyui config --models-dir=/path/to/models --nodes-dir=/path/to/nodes\n```\n\n### Model Not Appearing\n\n**Symptom:** Downloaded model not visible in ComfyUI\n\n**Fix:**\n\n```bash\n# Restart ComfyUI to reload models\nujust comfyui restart\n\n# Verify model is in correct directory\nls /path/to/your/models/checkpoints/\n```\n\n### CivitAI Download Fails\n\n**Symptom:** Cannot download from CivitAI\n\n**Cause:** Model requires authentication or is restricted\n\n**Fix:**\n\n```bash\n# Download manually and place in appropriate directory\nmv ~/Downloads/model.safetensors /path/to/models/checkpoints/\n```\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory error\n\n**Fix:** Check logs and consider using smaller models or lower precision:\n\n```bash\nujust comfyui logs\n```\n\n### Service Won't Start\n\n**Symptom:** ComfyUI fails to start\n\n**Fix:**\n\n```bash\n# Check logs for errors\nujust comfyui logs\n\n# Verify GPU access\nnvidia-smi\n\n# Delete and reconfigure\nujust comfyui delete\nujust comfyui config --models-dir=/data/models\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Instance config | Settings | `~/.config/comfyui/1.env` |\n| Quadlet file | Service definition | `~/.config/containers/systemd/comfyui-1.container` |\n\n## Cross-References\n\n- **Related Skills:** `ollama` (LLM inference), `jupyter` (notebooks)\n- **Pod Building:** `just build pod comfyui`\n- **ComfyUI Docs:** <https://github.com/comfyanonymous/ComfyUI>\n- **ComfyUI-Manager:** <https://github.com/ltdrdata/ComfyUI-Manager>\n- **CivitAI:** <https://civitai.com/>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"comfyui\", \"stable diffusion\", \"image generation\"\n- \"download model\", \"civitai\", \"checkpoint\", \"lora\"\n- \"custom nodes\", \"comfyui manager\"\n- \"ujust comfyui\", \"start comfyui\", \"configure comfyui\"\n- \"gpu image generation\", \"ai art\""
              },
              {
                "name": "config",
                "description": "Unified system configuration dispatcher for bazzite-ai. Manages services\n(Docker, Cockpit, SSH), desktop settings (gamemode, Steam), security\n(passwordless sudo), and development environment (GPU containers). Use\nwhen users need to enable/disable system features or check configuration status.\n",
                "path": "bazzite-ai/skills/config/SKILL.md",
                "frontmatter": {
                  "name": "config",
                  "description": "Unified system configuration dispatcher for bazzite-ai. Manages services\n(Docker, Cockpit, SSH), desktop settings (gamemode, Steam), security\n(passwordless sudo), and development environment (GPU containers). Use\nwhen users need to enable/disable system features or check configuration status.\n"
                },
                "content": "# Config - System Configuration Dispatcher\n\n## Overview\n\nThe `config` command is a unified dispatcher for system configuration tasks. It replaces scattered `toggle-*`, `setup-*`, and `config-*` commands with a single interface.\n\n**Key Concept:** All configuration targets support consistent actions: `enable`, `disable`, `status`, and `help`.\n\n## Quick Reference\n\n| Category | Targets |\n|----------|---------|\n| **Services** | `docker`, `cockpit`, `syncthing`, `libvirtd`, `sshd` |\n| **Desktop** | `gamemode`, `steam-autostart`, `shell` |\n| **Security** | `passwordless-sudo` |\n| **Apps** | `winboat` |\n| **Development** | `gpu`, `dev-environment` |\n\n## Parameters\n\n### Command Pattern\n\n```bash\nujust config TARGET=\"\" ACTION=\"\" ARGS...\n\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `TARGET` | See targets below | Configuration target |\n| `ACTION` | `enable`, `disable`, `status`, `help` | Action to perform |\n| `ARGS` | varies | Additional arguments |\n\nWithout `TARGET`, shows interactive picker.\n\n## Service Targets\n\n### Docker\n\n```bash\nujust config docker status        # Show Docker service status\nujust config docker enable        # Enable Docker daemon\nujust config docker disable       # Disable Docker daemon\nujust config docker enable-socket # Enable socket activation only\n\n```\n\n### Cockpit\n\n```bash\nujust config cockpit status       # Show Cockpit status\nujust config cockpit enable       # Enable web console\nujust config cockpit disable      # Disable web console\n\n```\n\nAccess at: `[https://localhost](https://localhost):9090`\n\n### Syncthing\n\n```bash\nujust config syncthing status     # Show Syncthing status\nujust config syncthing enable     # Enable file sync\nujust config syncthing disable    # Disable file sync\n\n```\n\n### Libvirtd\n\n```bash\nujust config libvirtd status      # Show libvirt status\nujust config libvirtd enable      # Enable virtualization\nujust config libvirtd disable     # Disable virtualization\n\n```\n\n### SSH Server\n\n```bash\nujust config sshd status          # Show SSH server status\nujust config sshd enable          # Enable SSH server\nujust config sshd disable         # Disable SSH server\n\n```\n\n## Desktop Targets\n\n### Gamemode\n\n```bash\nujust config gamemode status      # Show current session type\nujust config gamemode gamemode    # Set to Game Mode session\nujust config gamemode desktop     # Set to Desktop session\n\n```\n\n### Steam Autostart\n\n```bash\nujust config steam-autostart status   # Show autostart status\nujust config steam-autostart enable   # Enable Steam autostart\nujust config steam-autostart disable  # Disable Steam autostart\n\n```\n\n### Shell Configuration\n\nManages shell configuration files by synchronizing them with system skeleton defaults in `/etc/skel`.\n\n```bash\nujust config shell status   # Check if configs match skeleton\nujust config shell update   # Update all configs from /etc/skel (with backup)\n\n```\n\n**Managed files:**\n\n| File | Purpose |\n|------|---------|\n| `~/.bashrc` | Bash shell configuration |\n| `~/.zshrc` | Zsh shell configuration |\n| `~/.config/starship.toml` | Starship prompt config |\n| `~/.config/ghostty/` | Ghostty terminal config |\n\n**Backup location:** `~/.config-backup-shell-YYYYMMDD_HHMMSS/`\n\n## Security Targets\n\n### Passwordless Sudo\n\n```bash\nujust config passwordless-sudo status   # Show sudo config\nujust config passwordless-sudo enable   # Enable passwordless sudo\nujust config passwordless-sudo disable  # Disable passwordless sudo\n\n```\n\n**Warning:** Enabling passwordless sudo reduces security. Useful for development/automation.\n\n## Application Targets\n\n### WinBoat\n\n```bash\nujust config winboat launch              # Launch Windows app\nujust config winboat info                # Show WinBoat info\n\n```\n\n## Development Targets\n\n### GPU Containers\n\n```bash\nujust config gpu status       # Show GPU container support\nujust config gpu setup        # Setup GPU passthrough\n\n```\n\nConfigures:\n\n- NVIDIA Container Toolkit\n\n- AMD ROCm container support\n\n- Intel oneAPI container support\n\n### Dev Environment\n\n```bash\nujust config dev-environment verify      # Verify dev tools installed\n\n```\n\nChecks for required development tools and reports missing items.\n\n## Common Workflows\n\n### Setup Development Environment\n\n```bash\n# Enable passwordless sudo for automation\nujust config passwordless-sudo enable\n\n# Enable Docker for container development\nujust config docker enable\n\n# Setup GPU container support\nujust config gpu setup\n\n# Verify everything is ready\nujust config dev-environment verify\n\n```\n\n### Enable Remote Access\n\n```bash\n# Enable SSH server\nujust config sshd enable\n\n# Enable web console (Cockpit)\nujust config cockpit enable\n\n# Check both are running\nujust config sshd status\nujust config cockpit status\n\n```\n\n### Gaming Setup\n\n```bash\n# Set to Game Mode session\nujust config gamemode gamemode\n\n# Enable Steam autostart\nujust config steam-autostart enable\n\n```\n\n### Return to Desktop\n\n```bash\n# Set to Desktop session\nujust config gamemode desktop\n\n# Disable Steam autostart\nujust config steam-autostart disable\n\n```\n\n## Non-Interactive Usage\n\nAll commands work without TTY:\n\n```bash\n# CI/automation-friendly\nujust config docker enable\nujust config passwordless-sudo enable\n\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n**Symptom:** `ujust config <service> enable` completes but service not running\n\n**Fix:**\n\n```bash\n# Check service status\nsystemctl status <service>\n\n# Check logs\njournalctl -u <service> -n 50\n\n# Try manual start\nsudo systemctl start <service>\n\n```\n\n### GPU Containers Not Working\n\n**Symptom:** Containers can't access GPU\n\n**Cause:** GPU container toolkit not configured\n\n**Fix:**\n\n```bash\nujust config gpu setup\n# May require reboot\n\n```\n\n## Cross-References\n\n- **Related Skills:** `install` (for installing tools), `test` (for development)\n\n- **Services:** `jupyter`, `ollama`, `runners` (managed services with lifecycle)\n\n- **Documentation:** [Service Targets](./references/service-targets.md)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"enable Docker\", \"disable SSH\", \"configure cockpit\"\n\n- \"gamemode\", \"Game Mode session\", \"desktop mode\"\n\n- \"passwordless sudo\", \"sudo without password\"\n\n- \"GPU containers\", \"container GPU access\"\n\n- \"reset shell config\", \"restore bashrc\", \"default zshrc\"\n\n- \"starship not working\", \"prompt broken\", \"shell configuration\"\n\n- \"sync shell from skeleton\", \"ghostty config\""
              },
              {
                "name": "fiftyone",
                "description": "FiftyOne dataset visualization and curation tool via Podman Quadlet.\nMulti-container architecture with MongoDB sidecar for dataset persistence.\nGPU-accelerated for ML workflows. Use when users need to configure, start,\nor manage FiftyOne for dataset analysis.\n",
                "path": "bazzite-ai/skills/fiftyone/SKILL.md",
                "frontmatter": {
                  "name": "fiftyone",
                  "description": "FiftyOne dataset visualization and curation tool via Podman Quadlet.\nMulti-container architecture with MongoDB sidecar for dataset persistence.\nGPU-accelerated for ML workflows. Use when users need to configure, start,\nor manage FiftyOne for dataset analysis.\n"
                },
                "content": "# FiftyOne - Dataset Visualization & Curation\n\n## Overview\n\nThe `fiftyone` command manages FiftyOne dataset visualization using Podman Quadlet containers. It includes a MongoDB sidecar for persistent dataset storage.\n\n**Key Concept:** FiftyOne runs as a multi-container application with a MongoDB sidecar. The main FiftyOne container handles the web UI and processing, while MongoDB stores dataset metadata.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust fiftyone config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust fiftyone start [--instance=N\\|all]` | Start FiftyOne + MongoDB |\n| Stop | `ujust fiftyone stop [--instance=N\\|all]` | Stop FiftyOne + MongoDB |\n| Restart | `ujust fiftyone restart [--instance=N\\|all]` | Restart all containers |\n| Logs | `ujust fiftyone logs [--instance=N] [--lines=...]` | View interleaved logs |\n| Status | `ujust fiftyone status [--instance=N]` | Show status (all instances) |\n| URL | `ujust fiftyone url [--instance=N]` | Show access URL |\n| Shell | `ujust fiftyone shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Plugins | `ujust fiftyone plugins [-- CMD...]` | Manage FiftyOne plugins |\n| Delete | `ujust fiftyone delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: config, start, stop, etc. |\n| config_dir | `--config-dir` | `-c` | `~/.config/fiftyone/{N}` | Configuration directory |\n| workspace_dir | `--workspace-dir` | `-w` | `\"\"` | Optional mount to /workspace |\n| bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| port | `--port` | `-p` | `5151` | Web UI port |\n| image | `--image` | `-i` | `docker.io/voxel51/fiftyone` | Container image |\n| tag | `--tag` | `-t` | `latest` | Image tag |\n| gpu_type | `--gpu-type` | `-g` | `auto` | GPU type (auto/nvidia/amd/intel/none) |\n| lines | `--lines` | `-l` | `50` | Log lines to show |\n| instance | `--instance` | `-n` | `1` | Instance number |\n\n## Configuration\n\n```bash\n# Default configuration (port 5151, localhost only)\nujust fiftyone config\n\n# Custom port (long form)\nujust fiftyone config --port=5152\n\n# Custom port (short form)\nujust fiftyone config -p 5152\n\n# Network-wide access\nujust fiftyone config --bind=0.0.0.0\n\n# With workspace mount\nujust fiftyone config --workspace-dir=/data/datasets\n\n# Combine parameters (long form)\nujust fiftyone config --port=5152 --bind=0.0.0.0 --workspace-dir=/data\n\n# Combine parameters (short form)\nujust fiftyone config -p 5152 -b 0.0.0.0 -w /data\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n## Lifecycle Commands\n\n### Start/Stop/Restart\n\n```bash\n# Start FiftyOne (includes MongoDB sidecar)\nujust fiftyone start\n\n# Start specific instance (long form)\nujust fiftyone start --instance=1\n\n# Start specific instance (short form)\nujust fiftyone start -n 1\n\n# Start all instances\nujust fiftyone start --instance=all\n\n# Stop FiftyOne + MongoDB\nujust fiftyone stop --instance=1\n\n# Restart all containers\nujust fiftyone restart\n```\n\n### View Logs\n\nFiftyOne shows interleaved logs from both the main container and MongoDB sidecar:\n\n```bash\n# Follow logs (default 50 lines)\nujust fiftyone logs\n\n# More lines (long form)\nujust fiftyone logs --lines=100\n\n# More lines (short form)\nujust fiftyone logs -l 100\n\n# Specific instance\nujust fiftyone logs -n 1 -l 100\n```\n\nLog output format:\n\n```\n[fiftyone-mongodb] 2024-01-09 10:00:01 MongoDB started\n[fiftyone] 2024-01-09 10:00:02 Connecting to database...\n[fiftyone] 2024-01-09 10:00:03 FiftyOne App ready on port 5151\n```\n\n### Get URL\n\n```bash\nujust fiftyone url\n# Output: http://localhost:5151\n\n# Specific instance\nujust fiftyone url --instance=2\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust fiftyone shell\n\n# Run specific command (use -- separator)\nujust fiftyone shell -- fiftyone --version\nujust fiftyone shell -- pip list\n\n# Specific instance\nujust fiftyone shell --instance=2 -- python -c \"import fiftyone as fo; print(fo.__version__)\"\n\n# Short form\nujust fiftyone shell -n 2 -- ls -la\n```\n\n## Plugin Management\n\n```bash\n# List installed plugins\nujust fiftyone plugins -- list\n\n# Install a plugin\nujust fiftyone plugins -- install <plugin-name>\n\n# Update plugins\nujust fiftyone plugins -- update\n```\n\n## Multi-Container Architecture\n\nFiftyOne runs with a MongoDB sidecar:\n\n```\n+-------------------+        +-------------------+\n|    FiftyOne       |        |     MongoDB       |\n|   (fiftyone-1)    | -----> | (fiftyone-mongodb-1) |\n|   Port 5151       |        |   Port 27017      |\n+-------------------+        +-------------------+\n         |                            |\n         +---- bazzite-ai network ----+\n```\n\n**Container Names:**\n- `fiftyone-{N}` - Main FiftyOne container\n- `fiftyone-mongodb-{N}` - MongoDB sidecar\n\n**Lifecycle:**\n- `start` starts MongoDB first, then FiftyOne\n- `stop` stops FiftyOne first, then MongoDB\n- `logs` shows interleaved output from both\n\n## Port Allocation\n\n| Instance | FiftyOne Port | MongoDB Port |\n|----------|---------------|--------------|\n| 1 | 5151 | 27017 |\n| 2 | 5152 | 27018 |\n| N | 5150+N | 27016+N |\n\n## GPU Support\n\nFiftyOne supports GPU acceleration for ML model inference:\n\n```bash\n# Auto-detect GPU (default)\nujust fiftyone config\n\n# Explicit NVIDIA (long form)\nujust fiftyone config --gpu-type=nvidia\n\n# Explicit NVIDIA (short form)\nujust fiftyone config -g nvidia\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Instance config | Per-instance settings | `~/.config/fiftyone/instance-{N}.env` |\n| Quadlet unit (main) | Service definition | `~/.config/containers/systemd/fiftyone-{N}.container` |\n| Quadlet unit (MongoDB) | Sidecar definition | `~/.config/containers/systemd/fiftyone-mongodb-{N}.container` |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure FiftyOne with dataset directory\nujust fiftyone config --workspace-dir=/data/datasets\n\n# 2. Start FiftyOne\nujust fiftyone start\n\n# 3. Get URL\nujust fiftyone url\n\n# 4. Open in browser\n# http://localhost:5151\n```\n\n### Dataset Analysis\n\n```bash\n# Start FiftyOne\nujust fiftyone start\n\n# Open shell for interactive work\nujust fiftyone shell\n\n# Inside container:\n# import fiftyone as fo\n# dataset = fo.load_dataset(\"my_dataset\")\n# session = fo.launch_app(dataset)\n```\n\n### Network Access\n\n```bash\n# Configure for network access\nujust fiftyone config --bind=0.0.0.0\n\n# Restart to apply\nujust fiftyone restart\n\n# Access from other machines\n# http://<hostname>:5151\n```\n\n## Troubleshooting\n\n### FiftyOne Won't Start\n\n**Check:**\n\n```bash\nujust fiftyone status\nujust fiftyone logs --lines=50\n```\n\n**Common causes:**\n\n- Port 5151 already in use\n- MongoDB failed to start\n- GPU driver issues\n\n**Fix:**\n\n```bash\n# Delete and reconfigure\nujust fiftyone delete\nujust fiftyone config --port=5152\nujust fiftyone start\n```\n\n### MongoDB Connection Failed\n\n**Symptom:** FiftyOne logs show \"Connection refused\" to MongoDB\n\n**Check:**\n\n```bash\n# Check MongoDB container\npodman ps | grep fiftyone-mongodb\nujust fiftyone logs | grep mongodb\n```\n\n**Fix:**\n\n```bash\n# Restart both containers\nujust fiftyone restart\n```\n\n### Datasets Not Persisting\n\n**Symptom:** Datasets disappear after restart\n\n**Check:**\n\n- Verify config_dir is properly set\n- Check MongoDB volume mounts\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit config directory\nujust fiftyone config --config-dir=/data/fiftyone\n```\n\n## Cross-References\n\n- **Related Skills:** `jupyter` (ML notebooks), `ollama` (LLM inference)\n- **FiftyOne Docs:** <https://docs.voxel51.com/>\n- **GPU Setup:** `ujust config gpu setup`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"fiftyone\", \"dataset visualization\", \"dataset curation\"\n- \"ML datasets\", \"computer vision datasets\"\n- \"data labeling\", \"annotation tool\"\n- \"start fiftyone\", \"configure fiftyone\""
              },
              {
                "name": "install",
                "description": "Development tool installation dispatcher for bazzite-ai. Installs Claude Code,\npixi, chunkhound, bcvk, linters, flatpaks, and more. Use when users need to\ninstall standalone developer tools (not services with lifecycle management).\n",
                "path": "bazzite-ai/skills/install/SKILL.md",
                "frontmatter": {
                  "name": "install",
                  "description": "Development tool installation dispatcher for bazzite-ai. Installs Claude Code,\npixi, chunkhound, bcvk, linters, flatpaks, and more. Use when users need to\ninstall standalone developer tools (not services with lifecycle management).\n"
                },
                "content": "# Install - Development Tool Installer\n\n## Overview\n\nThe `install` command is a unified dispatcher for installing standalone development tools. For services with lifecycle management (start/stop/logs), use their dedicated commands.\n\n**Key Concept:** This is for standalone tools only. Use `ujust jupyter install`, `ujust runners install`, or `ujust jellyfin install` for managed services.\n\n## Quick Reference\n\n### Development Tools\n\n| Program | Command | Description |\n|---------|---------|-------------|\n| Claude Code | `ujust install claude-code-npm` | Claude Code AI CLI (npm) |\n| Pixi | `ujust install pixi` | Conda-compatible package manager |\n| Chunkhound | `ujust install chunkhound` | Semantic code search MCP |\n| Devcontainers CLI | `ujust install devcontainers-cli` | Dev Container CLI |\n| TweakCC | `ujust install tweakcc` | Claude Code customization |\n| ccstatusline | `ujust install ccstatusline` | Claude Code statusline widget |\n| Chrome Extension Fix | `ujust install chrome-extension-fix` | Fix Claude extension |\n| GitHub MCP | `ujust install github-mcp-server` | GitHub MCP server |\n| Chrome DevTools MCP | `ujust install chrome-devtools-mcp` | Chrome DevTools MCP |\n| Kind | `ujust install kind [VERSION]` | Kubernetes in Docker |\n| Minikube | `ujust install minikube [VERSION]` | Local Kubernetes |\n| bcvk | `ujust install bcvk` | bootc virtualization kit |\n| Linters | `ujust install linters` | Code linting tools |\n| Homebrew | `ujust install homebrew` | Homebrew package manager |\n| AppImage Manager | `ujust install appimage-manager` | Gear Lever AppImage manager |\n| Gemini CLI | `ujust install gemini-cli` | Google Gemini CLI |\n| Firebase CLI | `ujust install firebase-cli` | Firebase project management |\n| Wrangler | `ujust install wrangler` | Cloudflare Workers CLI |\n\n### Meta-Installers\n\n| Program | Command | Description |\n|---------|---------|-------------|\n| Dev Tools | `ujust install dev-tools [COMPONENT]` | Install tool groups |\n| Kubernetes | `ujust install kubernetes-tools` | Kind + Minikube |\n\n### Flatpak Categories\n\n| Category | Command | Description |\n|----------|---------|-------------|\n| Dev | `ujust install flatpaks-dev` | Development flatpaks |\n| Media | `ujust install flatpaks-media` | Media & graphics |\n| Gaming | `ujust install flatpaks-gaming` | Gaming tools |\n| Communication | `ujust install flatpaks-communication` | Chat apps |\n| Productivity | `ujust install flatpaks-productivity` | Office tools |\n| Utilities | `ujust install flatpaks-utilities` | Utility apps |\n| Experimental | `ujust install flatpaks-experimental` | Experimental apps |\n| Streaming | `ujust install flatpaks-streaming` | Streaming clients |\n| All | `ujust install flatpaks-all` | All flatpaks |\n\n## Common Installations\n\n### AI Development Setup\n\n```bash\n# Install Claude Code\nujust install claude-code-npm\n\n# Install Chunkhound for code search\nujust install chunkhound\n\n# Install GitHub MCP server\nujust install github-mcp-server\n\n# Install Chrome DevTools MCP (for browser automation)\nujust install chrome-devtools-mcp\n\n```\n\n### Python/ML Development\n\n```bash\n# Install Pixi (Conda-compatible, faster)\nujust install pixi\n\n# Install development flatpaks\nujust install flatpaks-dev\n\n```\n\n### Kubernetes Development\n\n```bash\n# Install both Kind and Minikube\nujust install kubernetes-tools\n\n# Or individually\nujust install kind\nujust install minikube\n\n```\n\n### VM Testing\n\n```bash\n# Install bcvk for bootc VM testing\nujust install bcvk\n\n```\n\n## Dev Tools Meta-Installer\n\nInstall groups of tools at once:\n\n```bash\n# Quick essentials\nujust install dev-tools quick\n\n# Core development tools\nujust install dev-tools core\n\n# Claude Code ecosystem\nujust install dev-tools claude\n\n# Code quality tools\nujust install dev-tools quality\n\n# Extra utilities\nujust install dev-tools extras\n\n# Google tools (Gemini, Firebase)\nujust install dev-tools google\n\n# Full development environment\nujust install dev-tools environment\n\n```\n\n### Component Groups\n\n| Component | Includes |\n|-----------|----------|\n| `quick` | claude-code-npm, pixi |\n| `core` | quick + homebrew, linters |\n| `claude` | chunkhound, github-mcp, tweakcc, ccstatusline |\n| `quality` | linters, devcontainers-cli |\n| `extras` | bcvk, appimage-manager |\n| `google` | gemini-cli, firebase-cli, wrangler |\n| `environment` | All of the above |\n\n## Services vs Install\n\n| For This | Use This |\n|----------|----------|\n| JupyterLab | `ujust jupyter install` |\n| GitHub Runners | `ujust runners install` |\n| Jellyfin | `ujust jellyfin install` |\n| Ollama | `ujust ollama install` |\n| Standalone tools | `ujust install <tool>` |\n\nServices have lifecycle commands (start/stop/logs). Standalone tools are just installed.\n\n## Flatpak Details\n\n### Development\n\n```bash\nujust install flatpaks-dev\n# Includes: VS Code, PyCharm, etc.\n\n```\n\n### Media\n\n```bash\nujust install flatpaks-media\n# Includes: GIMP, Inkscape, Kdenlive, etc.\n\n```\n\n### Gaming\n\n```bash\nujust install flatpaks-gaming\n# Includes: Lutris, Heroic, ProtonUp-Qt, etc.\n\n```\n\n### All Flatpaks\n\n```bash\nujust install flatpaks-all\n# Installs all categories\n\n```\n\n## Troubleshooting\n\n### Installation Failed\n\n**Check:**\n\n```bash\n# For npm-based tools\nnpm --version\n\n# For Homebrew tools\nbrew --version\n\n# For Flatpaks\nflatpak --version\n\n```\n\n### Claude Code Not Found After Install\n\n**Cause:** Shell not reloaded\n\n**Fix:**\n\n```bash\nexec $SHELL\n# Or\nsource ~/.bashrc\n\n```\n\n### Pixi Not Found\n\n**Fix:**\n\n```bash\n# Add to PATH\nexport PATH=\"$HOME/.pixi/bin:$PATH\"\n\n# Or reload shell\nexec $SHELL\n\n```\n\n### Flatpak Install Fails\n\n**Check:**\n\n```bash\n# Verify Flathub remote\nflatpak remote-list\n\n```\n\n**Fix:**\n\n```bash\n# Add Flathub if missing\nflatpak remote-add --if-not-exists flathub [https://flathub.org/repo/flathub.flatpakrepo]([https://flathub.org/repo/flathub.flatpakrepo](https://flathub.org/repo/flathub.flatpakrepo))\n```\n\n## Cross-References\n\n- **Services:** `jupyter`, `runners`, `jellyfin`, `ollama` (have lifecycle commands)\n\n- **Configuration:** `configure` (for enabling system services)\n\n- **VM Tools:** `vm`, `bootc` (after installing bcvk)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install claude code\", \"setup claude\", \"claude cli\"\n\n- \"install pixi\", \"conda alternative\"\n\n- \"install kubernetes\", \"setup kind\", \"minikube\"\n\n- \"install flatpaks\", \"flatpak applications\"\n\n- \"install development tools\", \"dev environment\"\n\n- \"install bcvk\", \"bootc tools\""
              },
              {
                "name": "jellyfin",
                "description": "Jellyfin media server management via Podman Quadlet. Supports multi-instance\ndeployment, hardware transcoding (NVIDIA/AMD/Intel), and FUSE filesystem\nmounts. Use when users need to set up or manage Jellyfin media servers.\n",
                "path": "bazzite-ai/skills/jellyfin/SKILL.md",
                "frontmatter": {
                  "name": "jellyfin",
                  "description": "Jellyfin media server management via Podman Quadlet. Supports multi-instance\ndeployment, hardware transcoding (NVIDIA/AMD/Intel), and FUSE filesystem\nmounts. Use when users need to set up or manage Jellyfin media servers.\n"
                },
                "content": "# Jellyfin - Media Server Management\n\n## Overview\n\nThe `jellyfin` command manages Jellyfin media server instances using Podman Quadlet containers. It supports hardware transcoding and FUSE filesystem compatibility for network mounts.\n\n**Key Concept:** Multi-instance support allows running multiple media libraries. FUSE compatibility enables rclone/sshfs mounts for cloud or remote storage.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust jellyfin config --config-dir=<CONFIG> --cache-dir=<CACHE> --media-dir=<MEDIA>` | Configure instance |\n| Start | `ujust jellyfin start [--instance=N\\|all]` | Start instance(s) |\n| Stop | `ujust jellyfin stop [--instance=N\\|all]` | Stop instance(s) |\n| Restart | `ujust jellyfin restart [--instance=N\\|all]` | Restart instance(s) |\n| Logs | `ujust jellyfin logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust jellyfin list` | List all instances |\n| Status | `ujust jellyfin status [--instance=N]` | Show instance status |\n| URL | `ujust jellyfin url [--instance=N]` | Show access URL |\n| Shell | `ujust jellyfin shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Delete | `ujust jellyfin delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Required | Description |\n|-----------|-----------|-------|----------|-------------|\n| Config Dir | `--config-dir` | `-c` | Yes | Configuration directory |\n| Cache Dir | `--cache-dir` | - | Yes | Cache directory (transcoding) |\n| Media Dir | `--media-dir` | - | Yes | Media library path |\n| Instance | `--instance` | `-n` | No | Instance number (default: 1) |\n| GPU Type | `--gpu-type` | `-g` | No | GPU: nvidia, amd, intel, auto |\n| Image | `--image` | `-i` | No | Container image |\n| Tag | `--tag` | `-t` | No | Image tag (default: stable) |\n| Workspace | `--workspace-dir` | `-w` | No | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | No | Bind address |\n| Port | `--port` | `-p` | No | Service port |\n| Lines | `--lines` | `-l` | No | Log lines to show |\n\n### Configuration Examples\n\n```bash\n# Basic installation (long form)\nujust jellyfin config --config-dir=~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media\n\n# With NVIDIA GPU for transcoding\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media --gpu-type=nvidia\n\n# Second instance for different library\nujust jellyfin config -c ~/jellyfin2/config --cache-dir=~/jellyfin2/cache --media-dir=~/videos --instance=2\n\n# With short forms\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media -n 1 -g nvidia\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust jellyfin shell\n\n# Run specific command (use -- separator)\nujust jellyfin shell -- df -h\n\n# Shell in specific instance\nujust jellyfin shell --instance=2 -- ls /media\n```\n\n## Lifecycle Commands\n\n### Start/Stop\n\n```bash\n# Single instance\nujust jellyfin start --instance=1\nujust jellyfin stop --instance=1\n\n# Short form\nujust jellyfin start -n 1\nujust jellyfin stop -n 1\n\n# All instances\nujust jellyfin start --instance=all\nujust jellyfin stop --instance=all\n```\n\n### View Logs\n\n```bash\n# Follow logs\nujust jellyfin logs\n\n# Specific instance with line count\nujust jellyfin logs --instance=1 --lines=100\n\n# Short form\nujust jellyfin logs -n 1 -l 100\n```\n\n### Get URL\n\n```bash\nujust jellyfin url\n# Output: http://localhost:8096\n\n# Specific instance\nujust jellyfin url --instance=2\n```\n\n## Port Allocation\n\n| Instance | Port |\n|----------|------|\n| 1 | 8096 |\n| 2 | 8097 |\n| 3 | 8098 |\n| N | 8095+N |\n\n## Hardware Transcoding\n\n### GPU Types\n\n| GPU | Flag Value | Transcoding |\n|-----|------------|-------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | NVENC/NVDEC |\n| AMD | `--gpu-type=amd` or `-g amd` | VAAPI |\n| Intel | `--gpu-type=intel` or `-g intel` | QuickSync |\n\n### Enable GPU\n\n```bash\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media --gpu-type=nvidia\n```\n\n### Verify GPU\n\n```bash\n# Check inside container\nujust jellyfin shell -- nvidia-smi  # or vainfo for AMD/Intel\n```\n\n## FUSE Filesystem Support\n\nJellyfin containers support FUSE mounts (rclone, sshfs) for remote storage.\n\n### Mount Before Starting\n\n```bash\n# Mount cloud storage\nrclone mount gdrive:media ~/media --daemon\n\n# Then start Jellyfin\nujust jellyfin start 1\n```\n\n### Why Host Networking?\n\nJellyfin uses host networking for:\n\n- DLNA discovery\n- mDNS/Bonjour\n- Chromecast\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/jellyfin-1.container` |\n| Instance config | Settings | `~/.config/jellyfin/instance-1.env` |\n| Jellyfin data | Libraries, users | `<CONFIG>/` |\n| Transcoding cache | Temp files | `<CACHE>/` |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Create directories\nmkdir -p ~/jellyfin/{config,cache}\n\n# 2. Configure Jellyfin\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/media --gpu-type=nvidia\n\n# 3. Start it\nujust jellyfin start\n\n# 4. Access web UI\nujust jellyfin url\n# Open http://localhost:8096\n```\n\n### Multiple Libraries\n\n```bash\n# Movies library\nujust jellyfin config -c ~/jellyfin-movies/config --cache-dir=~/jellyfin-movies/cache --media-dir=~/movies -n 1\n\n# TV library\nujust jellyfin config -c ~/jellyfin-tv/config --cache-dir=~/jellyfin-tv/cache --media-dir=~/tv -n 2\n\n# Start both\nujust jellyfin start --instance=all\n```\n\n### Cloud Storage\n\n```bash\n# 1. Mount cloud storage\nrclone mount gdrive:media ~/cloud-media --daemon --vfs-cache-mode writes\n\n# 2. Configure Jellyfin pointing to mount\nujust jellyfin config -c ~/jellyfin/config --cache-dir=~/jellyfin/cache --media-dir=~/cloud-media\n\n# 3. Start\nujust jellyfin start\n```\n\n## Initial Configuration\n\nFirst-time setup via web UI:\n\n1. Open `http://localhost:8096`\n2. Create admin user\n3. Add media libraries\n4. Configure transcoding (if GPU)\n5. Set up remote access\n\n## Troubleshooting\n\n### Jellyfin Won't Start\n\n**Check:**\n\n```bash\nujust jellyfin status\nujust jellyfin logs --lines=50\n```\n\n**Common causes:**\n\n- Port conflict (8096 in use)\n- Invalid paths\n- GPU driver issues\n\n### Transcoding Fails\n\n**Check:**\n\n```bash\n# View logs for transcoding errors\nujust jellyfin logs | grep -i transcode\n```\n\n**Common causes:**\n\n- GPU not passed through\n- Missing codec support\n\n**Fix:**\n\n```bash\n# Reconfigure with GPU\nujust jellyfin delete\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=~/media --gpu-type=nvidia\n```\n\n### Media Not Found\n\n**Check:**\n\n- Media directory exists\n- Correct path in config\n- Permissions\n\n**Fix:**\n\n```bash\n# Verify path\nls ~/media\n\n# Reconfigure with correct path\nujust jellyfin delete\nujust jellyfin config -c ~/config --cache-dir=~/cache --media-dir=/correct/path\n```\n\n### DLNA Not Working\n\n**Cause:** Network isolation\n\nJellyfin uses host networking, but ensure:\n\n- Firewall allows mDNS (5353/udp)\n- Same network as clients\n\n## Cross-References\n\n- **Related Skills:** `configure gpu` (GPU setup)\n- **Jellyfin Docs:** <https://jellyfin.org/docs/>\n- **Web UI:** [http://localhost:8096](http://localhost:8096)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install jellyfin\", \"setup media server\"\n- \"jellyfin not working\", \"jellyfin transcoding\"\n- \"jellyfin GPU\", \"hardware transcoding\"\n- \"multiple jellyfin\", \"jellyfin instances\""
              },
              {
                "name": "jupyter",
                "description": "JupyterLab ML/AI development environment management via Podman Quadlet.\nSupports multi-instance deployment, GPU acceleration (NVIDIA/AMD/Intel),\ntoken authentication, and per-instance configuration. Use when users need\nto configure, start, stop, or manage JupyterLab containers for ML development.\n",
                "path": "bazzite-ai/skills/jupyter/SKILL.md",
                "frontmatter": {
                  "name": "jupyter",
                  "description": "JupyterLab ML/AI development environment management via Podman Quadlet.\nSupports multi-instance deployment, GPU acceleration (NVIDIA/AMD/Intel),\ntoken authentication, and per-instance configuration. Use when users need\nto configure, start, stop, or manage JupyterLab containers for ML development.\n"
                },
                "content": "# Jupyter - ML/AI Development Environment\n\n## Overview\n\nThe `jupyter` command manages JupyterLab instances for ML/AI development using Podman Quadlet containers. Each instance runs as a systemd user service with optional GPU acceleration.\n\n**Key Concept:** Multi-instance support allows running multiple isolated JupyterLab environments simultaneously, each on different ports with different GPU configurations.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust jupyter config [--instance=N] [--port=...] [--gpu-type=...]` | Configure instance N |\n| Start | `ujust jupyter start [--instance=N\\|all]` | Start instance(s) |\n| Stop | `ujust jupyter stop [--instance=N\\|all]` | Stop instance(s) |\n| Restart | `ujust jupyter restart [--instance=N\\|all]` | Restart instance(s) |\n| Logs | `ujust jupyter logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust jupyter list` | List all instances |\n| Status | `ujust jupyter status [--instance=N]` | Show instance status |\n| URL | `ujust jupyter url [--instance=N]` | Show access URL |\n| Shell | `ujust jupyter shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Token enable | `ujust jupyter token-enable [--instance=N]` | Enable token auth |\n| Token show | `ujust jupyter token-show [--instance=N]` | Show token |\n| Token disable | `ujust jupyter token-disable [--instance=N]` | Disable token auth |\n| Token regenerate | `ujust jupyter token-regenerate [--instance=N]` | Generate new token |\n| Delete | `ujust jupyter delete [--instance=N\\|all]` | Remove instance(s) and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Instance | `--instance` | `-n` | `1` | Instance number (1, 2, 3...) |\n| Port | `--port` | `-p` | `8888` | Web UI port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type: `nvidia`, `amd`, `intel`, `none`, `auto` |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n### Instance Numbering\n\n- Instance 1: Port 8888 (default)\n- Instance 2: Port 8889\n- Instance N: Port 8887+N\n\n## Configuration Examples\n\n```bash\n# Default: Instance 1, port 8888, auto-detect GPU\nujust jupyter config\n\n# Instance 2 with custom port and NVIDIA GPU (long form)\nujust jupyter config --instance=2 --port=8889 --gpu-type=nvidia\n\n# Instance 2 with custom port and NVIDIA GPU (short form)\nujust jupyter config -n 2 -p 8889 -g nvidia\n\n# Instance 3 with AMD GPU\nujust jupyter config -n 3 -p 8890 -g amd\n\n# No GPU acceleration\nujust jupyter config --gpu-type=none\n\n# With workspace mount\nujust jupyter config --gpu-type=nvidia --workspace-dir=/home/user/projects\n\n# Network-wide access\nujust jupyter config --bind=0.0.0.0\n\n# Combine multiple options\nujust jupyter config -n 2 -p 8889 -g nvidia -b 0.0.0.0 -w /home/user/projects\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust jupyter shell\n\n# Run specific command (use -- separator)\nujust jupyter shell -- pip list\n\n# Shell in specific instance\nujust jupyter shell --instance=2 -- nvidia-smi\n\n# Short form\nujust jupyter shell -n 2 -- nvidia-smi\n```\n\n## Lifecycle Commands\n\n### Start/Stop/Restart\n\n```bash\n# Single instance (long form)\nujust jupyter start --instance=1\nujust jupyter stop --instance=1\nujust jupyter restart --instance=1\n\n# Single instance (short form)\nujust jupyter start -n 1\nujust jupyter stop -n 1\nujust jupyter restart -n 1\n\n# All instances\nujust jupyter start --instance=all\nujust jupyter stop --instance=all\nujust jupyter restart --instance=all\n```\n\n### View Logs\n\n```bash\n# Follow logs (instance 1 default)\nujust jupyter logs\n\n# Specific instance\nujust jupyter logs --instance=1\n\n# Last N lines (long form)\nujust jupyter logs --lines=100\n\n# Last N lines (short form)\nujust jupyter logs -l 100 -n 2\n```\n\n### Get Access URL\n\n```bash\nujust jupyter url\n# Output: http://localhost:8888\n\n# Specific instance\nujust jupyter url --instance=2\n```\n\n## Token Authentication\n\nBy default, JupyterLab requires no token for local development. Enable token auth for remote access or shared environments.\n\n```bash\n# Enable token (generates random token) - instance 1 default\nujust jupyter token-enable\n\n# Enable token for specific instance\nujust jupyter token-enable --instance=2\n\n# Show current token\nujust jupyter token-show --instance=1\n\n# Disable token (password-less access)\nujust jupyter token-disable\n\n# Generate new token\nujust jupyter token-regenerate --instance=1\n```\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/jupyter-1.container` |\n| Instance config | Per-instance settings | `~/.config/jupyter/instance-1.env` |\n\n## Volume Mounts\n\n| Container Path | Host Path | Purpose |\n|----------------|-----------|---------|\n| `/workspace` | `$HOME` | User home directory |\n| `/home/jovyan/.jupyter` | `~/.jupyter` | Jupyter config |\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure JupyterLab with GPU support\nujust jupyter config --gpu-type=nvidia\n\n# 2. Start the instance\nujust jupyter start\n\n# 3. Get the URL\nujust jupyter url\n\n# 4. Open in browser\n# http://localhost:8888\n```\n\n### Multiple Environments\n\n```bash\n# PyTorch environment (instance 1)\nujust jupyter config --instance=1 --gpu-type=nvidia\n\n# TensorFlow environment (instance 2)\nujust jupyter config -n 2 -p 8889 -g nvidia\n\n# CPU-only data science (instance 3)\nujust jupyter config -n 3 -p 8890 -g none\n\n# Start all\nujust jupyter start --instance=all\n\n# List all\nujust jupyter list\n```\n\n### Remote Access\n\n```bash\n# Enable token for security\nujust jupyter token-enable\n\n# Get token\nujust jupyter token-show\n# Use: http://your-ip:8888/?token=<token>\n```\n\n## GPU Support\n\n### Automatic Detection\n\n```bash\nujust jupyter config  # Auto-detects GPU type\n```\n\n### Manual Selection\n\n| GPU Type | Flag Value | Requirements |\n|----------|------------|--------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | NVIDIA drivers + nvidia-container-toolkit |\n| AMD | `--gpu-type=amd` or `-g amd` | ROCm drivers |\n| Intel | `--gpu-type=intel` or `-g intel` | oneAPI runtime |\n| None | `--gpu-type=none` or `-g none` | CPU only |\n\n### Verify GPU Access\n\n```bash\nujust jupyter shell -- nvidia-smi  # NVIDIA\nujust jupyter shell -- rocm-smi    # AMD\n```\n\n## Troubleshooting\n\n### Instance Won't Start\n\n**Symptom:** `ujust jupyter start` fails\n\n**Check:**\n\n```bash\n# Check service status\nsystemctl --user status jupyter-1\n\n# Check logs\nujust jupyter logs --lines=50\n```\n\n**Common causes:**\n\n- Port already in use\n- GPU not available\n- Image not pulled\n\n### GPU Not Detected\n\n**Symptom:** No GPU acceleration in notebooks\n\n**Check:**\n\n```bash\n# Verify GPU config\nujust jupyter status\n\n# Test inside container\nujust jupyter shell -- nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit GPU type\nujust jupyter delete\nujust jupyter config --gpu-type=nvidia\n```\n\n### Token Issues\n\n**Symptom:** Can't access Jupyter, token required\n\n**Fix:**\n\n```bash\n# Show current token\nujust jupyter token-show\n\n# Or disable token for local use\nujust jupyter token-disable\n```\n\n### Port Conflict\n\n**Symptom:** \"Address already in use\"\n\n**Fix:**\n\n```bash\n# Find what's using the port\nlsof -i :8888\n\n# Use different port\nujust jupyter config --port=8889\n```\n\n## Cross-References\n\n- **Related Skills:** `pod` (build images), `configure gpu` (GPU setup)\n- **GPU Setup:** `ujust config gpu setup`\n- **Documentation:** [Podman Quadlet Docs](https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install jupyter\", \"setup jupyterlab\", \"ML development\"\n- \"start jupyter\", \"stop jupyter\", \"restart jupyter\"\n- \"jupyter not working\", \"jupyter won't start\"\n- \"jupyter token\", \"jupyter password\", \"jupyter authentication\"\n- \"jupyter GPU\", \"jupyter nvidia\", \"jupyter cuda\"\n- \"multiple jupyter\", \"second jupyter instance\""
              },
              {
                "name": "localai",
                "description": "LocalAI local inference API management via Podman Quadlet. Provides an\nOpenAI-compatible API for local model inference with GPU acceleration.\nUse when users need to configure, start, or manage the LocalAI service.\n",
                "path": "bazzite-ai/skills/localai/SKILL.md",
                "frontmatter": {
                  "name": "localai",
                  "description": "LocalAI local inference API management via Podman Quadlet. Provides an\nOpenAI-compatible API for local model inference with GPU acceleration.\nUse when users need to configure, start, or manage the LocalAI service.\n"
                },
                "content": "# LocalAI - Local AI Inference API\n\n## Overview\n\nThe `localai` command manages the LocalAI service using Podman Quadlet containers. It provides an OpenAI-compatible API for running AI models locally with GPU acceleration.\n\n**Key Features:**\n\n- OpenAI-compatible API endpoints\n- GPU-specific container images (auto-selected)\n- Multiple GPU support (NVIDIA, AMD, Intel)\n- Cross-pod DNS via `bazzite-ai` network\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust localai config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust localai start [--instance=...]` | Start service |\n| Stop | `ujust localai stop [--instance=...]` | Stop service |\n| Restart | `ujust localai restart [--instance=...]` | Restart service |\n| Logs | `ujust localai logs [--lines=...]` | View logs |\n| Status | `ujust localai status [--instance=...]` | Show status |\n| URL | `ujust localai url [--instance=...]` | Show API URL |\n| List | `ujust localai list` | List instances |\n| Shell | `ujust localai shell [-- CMD...]` | Container shell |\n| Delete | `ujust localai delete [--instance=...]` | Remove service |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `8080` | Host port for API |\n| Image | `--image` | `-i` | (auto by GPU) | Container image |\n| Tag | `--tag` | `-t` | `latest` | Image tag |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Config Dir | `--config-dir` | `-c` | `~/.config/localai/1` | Config/models directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Workspace mount |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type |\n| Instance | `--instance` | `-n` | `1` | Instance number or `all` |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n## GPU-Specific Images\n\nLocalAI uses different container images optimized for each GPU type:\n\n| GPU Type | Image | Auto-Selected? |\n|----------|-------|----------------|\n| CPU (none) | `localai/localai:latest` | Yes |\n| NVIDIA | `localai/localai:latest-gpu-nvidia-cuda-12` | Yes |\n| AMD | `localai/localai:latest-gpu-hipblas` | Yes |\n| Intel | `localai/localai:latest-gpu-intel` | Yes |\n\nThe appropriate image is automatically selected based on detected GPU hardware.\n\n## Configuration\n\n```bash\n# Default configuration (auto-detects GPU, port 8080)\nujust localai config\n\n# Custom port (long form)\nujust localai config --port=8081\n\n# Custom port (short form)\nujust localai config -p 8081\n\n# Network-wide access\nujust localai config --bind=0.0.0.0\n\n# Force CPU image (ignore GPU)\nujust localai config --image=localai/localai:latest\n\n# Combine parameters (long form)\nujust localai config --port=8081 --bind=0.0.0.0\n\n# Combine parameters (short form)\nujust localai config -p 8081 -b 0.0.0.0\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured updates the existing settings:\n\n```bash\n# Change only the bind address\nujust localai config --bind=0.0.0.0\n\n# Update port without affecting other settings\nujust localai config --port=8082\n```\n\n## Lifecycle Management\n\n```bash\n# Start LocalAI\nujust localai start\n\n# Stop service\nujust localai stop\n\n# Restart (apply config changes)\nujust localai restart\n\n# View logs (default 50 lines)\nujust localai logs\n\n# View more logs (long form)\nujust localai logs --lines=200\n\n# View more logs (short form)\nujust localai logs -l 200\n\n# Check status\nujust localai status\n\n# Show API URL\nujust localai url\n```\n\n## Multi-Instance Support\n\n```bash\n# Start all instances (long form)\nujust localai start --instance=all\n\n# Start all instances (short form)\nujust localai start -n all\n\n# Stop specific instance\nujust localai stop --instance=2\n\n# Delete all instances\nujust localai delete --instance=all\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust localai shell\n\n# Run specific command (use -- separator)\nujust localai shell -- ls -la /models\nujust localai shell -- nvidia-smi\n```\n\n## Network Architecture\n\nLocalAI uses the `bazzite-ai` bridge network for cross-container DNS:\n\n```\n+-------------------+     DNS      +-------------------+\n|   Open WebUI      | -----------> |     LocalAI       |\n|   (openwebui)     |              |    (localai)      |\n|   Port 3000       |              |   Port 8080       |\n+-------------------+              +-------------------+\n         |                                  |\n         +------ bazzite-ai network --------+\n                         |\n+-------------------+    |    +-------------------+\n|     Ollama        |----+----+     Jupyter       |\n|    (ollama)       |         |    (jupyter)      |\n|   Port 11434      |         |   Port 8888       |\n+-------------------+         +-------------------+\n```\n\n**Cross-Pod DNS:**\n\n- LocalAI accessible as `http://localai:8080` from other containers\n- Can replace Ollama as backend for OpenWebUI\n\n## API Endpoints (OpenAI-Compatible)\n\n| Endpoint | Description |\n|----------|-------------|\n| `/v1/models` | List available models |\n| `/v1/chat/completions` | Chat completions |\n| `/v1/completions` | Text completions |\n| `/v1/embeddings` | Generate embeddings |\n| `/v1/images/generations` | Image generation |\n| `/v1/audio/transcriptions` | Speech-to-text |\n\n### Example API Usage\n\n```bash\n# List models\ncurl http://localhost:8080/v1/models\n\n# Chat completion\ncurl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Model Storage\n\n| Path | Description |\n|------|-------------|\n| `~/.config/localai/<INSTANCE>/models` | Model files |\n\nModels persist across container restarts. Each instance has isolated storage.\n\n### Loading Models\n\nPlace model files (GGUF, GGML) in the models directory:\n\n```bash\n# Copy a model\ncp my-model.gguf ~/.config/localai/1/models/\n\n# Or download directly\ncurl -L -o ~/.config/localai/1/models/model.gguf \\\n  https://huggingface.co/.../model.gguf\n```\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure LocalAI (auto-detects GPU)\nujust localai config\n\n# 2. Start the service\nujust localai start\n\n# 3. Check the API\nujust localai url\n# Output: http://127.0.0.1:8080\n\n# 4. Test the API\ncurl http://localhost:8080/v1/models\n```\n\n### Use with OpenWebUI\n\nOpenWebUI can use LocalAI as an OpenAI-compatible backend:\n\n```bash\n# Start LocalAI\nujust localai start\n\n# In OpenWebUI settings, add connection:\n# URL: http://localai:8080/v1  (cross-pod DNS)\n# Or: http://host.containers.internal:8080/v1  (from host)\n```\n\n### Remote Access Setup\n\n```bash\n# Configure for network access\nujust localai config --bind=0.0.0.0\n\n# Start the service\nujust localai start\n\n# Or use Tailscale for secure access\nujust tailscale serve --service=localai\n```\n\n## GPU Support\n\nGPU is automatically detected and the appropriate image is selected:\n\n| GPU Type | Detection | Device Passthrough |\n|----------|-----------|-------------------|\n| NVIDIA | `nvidia-smi` | CDI (`nvidia.com/gpu=all`) |\n| AMD | lspci | `/dev/dri` + `/dev/kfd` |\n| Intel | lspci | `/dev/dri` |\n\n### Check GPU in Container\n\n```bash\n# NVIDIA\nujust localai shell -- nvidia-smi\n\n# Check GPU environment\nujust localai shell -- env | grep -i gpu\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n```bash\n# Check status\nujust localai status\n\n# View logs\nujust localai logs --lines=100\n\n# Check image was pulled\npodman images | grep localai\n```\n\n**Common causes:**\n\n- Port 8080 already in use\n- Container image not pulled\n- GPU driver issues\n\n### GPU Not Detected\n\n**NVIDIA:**\n\n```bash\n# Check CDI configuration\nnvidia-ctk cdi list\n\n# Regenerate CDI spec\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n```\n\n**AMD:**\n\n```bash\n# Check /dev/kfd exists\nls -la /dev/kfd\n\n# Check ROCm\nrocminfo\n```\n\n### API Errors\n\n```bash\n# Test API endpoint\ncurl http://localhost:8080/v1/models\n\n# Check logs for errors\nujust localai logs --lines=100\n```\n\n### Clear Data and Start Fresh\n\n```bash\n# Delete everything\nujust localai delete --instance=all\n\n# Reconfigure\nujust localai config\nujust localai start\n```\n\n## Cross-References\n\n- **Network peers:** ollama, openwebui, jupyter, comfyui (all use bazzite-ai network)\n- **Alternative:** `ollama` (simpler model management, different API)\n- **Client:** `openwebui` (can use LocalAI as backend)\n- **Docs:** [LocalAI Documentation](https://localai.io/)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install localai\", \"setup local inference\", \"openai-compatible api\"\n- \"configure localai\", \"change port\", \"gpu acceleration\"\n- \"localai not working\", \"api error\", \"model loading\"\n- \"localai logs\", \"debug localai\"\n- \"delete localai\", \"uninstall\""
              },
              {
                "name": "ollama",
                "description": "Ollama LLM inference server management via Podman Quadlet. Single-instance\ndesign with GPU acceleration for running local LLMs. Use when users need\nto configure Ollama, pull models, run inference, or manage the Ollama server.\n",
                "path": "bazzite-ai/skills/ollama/SKILL.md",
                "frontmatter": {
                  "name": "ollama",
                  "description": "Ollama LLM inference server management via Podman Quadlet. Single-instance\ndesign with GPU acceleration for running local LLMs. Use when users need\nto configure Ollama, pull models, run inference, or manage the Ollama server.\n"
                },
                "content": "# Ollama - Local LLM Inference Server\n\n## Overview\n\nThe `ollama` command manages the Ollama LLM inference server using Podman Quadlet containers. It provides a single-instance server for running local LLMs with GPU acceleration.\n\n**Key Concept:** Unlike Jupyter, Ollama uses a single-instance design because GPU memory is shared across all loaded models. The API is accessible at port 11434.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust ollama config [--port=...] [--gpu-type=...]` | Configure server |\n| Start | `ujust ollama start` | Start server |\n| Stop | `ujust ollama stop` | Stop server |\n| Restart | `ujust ollama restart` | Restart server |\n| Logs | `ujust ollama logs [--lines=...]` | View logs |\n| Status | `ujust ollama status` | Show server status |\n| Pull | `ujust ollama pull --model=<MODEL>` | Download a model |\n| List | `ujust ollama list` | List installed models |\n| Run | `ujust ollama run --model=<MODEL> [--prompt=...]` | Run model |\n| Shell | `ujust ollama shell [-- CMD...]` | Open container shell |\n| Delete | `ujust ollama delete` | Remove server and images |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `11434` | API port |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type: `nvidia`, `amd`, `intel`, `none`, `auto` |\n| Image | `--image` | `-i` | (default image) | Container image |\n| Tag | `--tag` | `-t` | `stable` | Image tag |\n| Config Dir | `--config-dir` | `-c` | `~/.config/ollama/1` | Config/data directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Optional mount to /workspace |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n| Model | `--model` | `-m` | `qwen3:4b` | Model for pull/run actions |\n| Prompt | `--prompt` | - | `say hi` | Prompt for run action |\n| Context Length | `--context-length` | - | `8192` | Context window size |\n| Instance | `--instance` | `-n` | `1` | Instance number |\n\n## Configuration\n\n```bash\n# Default: Port 11434, auto-detect GPU\nujust ollama config\n\n# Custom port with NVIDIA GPU (long form)\nujust ollama config --port=11435 --gpu-type=nvidia\n\n# Custom port with NVIDIA GPU (short form)\nujust ollama config -p 11435 -g nvidia\n\n# CPU only\nujust ollama config --gpu-type=none\n\n# With workspace mount\nujust ollama config --gpu-type=nvidia --workspace-dir=/home/user/projects\n\n# Custom context length\nujust ollama config --context-length=16384\n\n# Network-wide access\nujust ollama config --bind=0.0.0.0\n\n# Combine multiple options\nujust ollama config -p 11435 -g nvidia -b 0.0.0.0 --context-length=16384\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust ollama shell\n\n# Run specific command (use -- separator)\nujust ollama shell -- nvidia-smi\nujust ollama shell -- df -h\nujust ollama shell -- ls -la /root/.ollama\n```\n\n## Model Management\n\n### Pull Models\n\n```bash\n# Download popular models (long form)\nujust ollama pull --model=llama3.2\nujust ollama pull --model=codellama\nujust ollama pull --model=mistral\nujust ollama pull --model=phi3\n\n# Short form\nujust ollama pull -m llama3.2\nujust ollama pull -m codellama\n\n# Specific versions\nujust ollama pull -m llama3.2:7b\nujust ollama pull -m llama3.2:70b\n```\n\n### List Models\n\n```bash\nujust ollama list\n```\n\nOutput:\n\n```\nNAME              SIZE      MODIFIED\nllama3.2:latest   4.7 GB    2 hours ago\ncodellama:latest  3.8 GB    1 day ago\n```\n\n### Run Models\n\n```bash\n# Interactive chat (long form)\nujust ollama run --model=llama3.2\n\n# Interactive chat (short form)\nujust ollama run -m llama3.2\n\n# Single prompt\nujust ollama run -m llama3.2 --prompt=\"Explain quantum computing\"\n\n# Code generation\nujust ollama run -m codellama --prompt=\"Write a Python function to sort a list\"\n```\n\n## API Access\n\n### Default Endpoint\n\n```\nhttp://localhost:11434\n```\n\n### API Examples\n\n```bash\n# Generate completion\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Hello, how are you?\"\n}'\n\n# Chat\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n}'\n\n# List models\ncurl http://localhost:11434/api/tags\n```\n\n### Integration with Tools\n\n```bash\n# Claude Code with Ollama\nexport OLLAMA_HOST=http://localhost:11434\n\n# LangChain\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"llama3.2\", base_url=\"http://localhost:11434\")\n```\n\n## Volume Mounts\n\n| Container Path | Host Path | Purpose |\n|----------------|-----------|---------|\n| `/root/.ollama` | `~/.ollama` | Model storage |\n\nModels are persisted in `~/.ollama` and survive container restarts.\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Configure Ollama with GPU\nujust ollama config --gpu-type=nvidia\n\n# 2. Start the server\nujust ollama start\n\n# 3. Pull a model\nujust ollama pull -m llama3.2\n\n# 4. Test it\nujust ollama run -m llama3.2 --prompt=\"Hello!\"\n```\n\n### Development with Local LLM\n\n```bash\n# Start Ollama\nujust ollama start\n\n# In your code, use:\n# OLLAMA_HOST=http://localhost:11434\n```\n\n### Model Comparison\n\n```bash\n# Pull multiple models\nujust ollama pull -m llama3.2\nujust ollama pull -m mistral\nujust ollama pull -m phi3\n\n# Compare responses\nujust ollama run -m llama3.2 --prompt=\"Explain REST APIs\"\nujust ollama run -m mistral --prompt=\"Explain REST APIs\"\nujust ollama run -m phi3 --prompt=\"Explain REST APIs\"\n```\n\n## GPU Support\n\n### Automatic Detection\n\n```bash\nujust ollama config  # Auto-detects GPU\n```\n\n### Manual Selection\n\n| GPU Type | Flag Value | VRAM Usage |\n|----------|------------|------------|\n| NVIDIA | `--gpu-type=nvidia` or `-g nvidia` | Full GPU acceleration |\n| AMD | `--gpu-type=amd` or `-g amd` | ROCm acceleration |\n| Intel | `--gpu-type=intel` or `-g intel` | oneAPI acceleration |\n| None | `--gpu-type=none` or `-g none` | CPU only (slower) |\n\n### Check GPU Status\n\n```bash\nujust ollama shell -- nvidia-smi  # NVIDIA\nujust ollama shell -- rocm-smi    # AMD\n```\n\n## Model Size Guide\n\n| Model | Parameters | VRAM Needed | Quality |\n|-------|------------|-------------|---------|\n| phi3 | 3B | 4GB | Fast, basic |\n| llama3.2 | 8B | 8GB | Balanced |\n| mistral | 7B | 8GB | Good coding |\n| codellama | 7B | 8GB | Code-focused |\n| llama3.2:70b | 70B | 48GB+ | Best quality |\n\n## Troubleshooting\n\n### Server Won't Start\n\n**Check:**\n\n```bash\nsystemctl --user status ollama\nujust ollama logs --lines=50\n```\n\n**Common causes:**\n\n- Port 11434 already in use\n- GPU driver issues\n- Image not pulled\n\n### Model Loading Fails\n\n**Symptom:** \"out of memory\" or slow loading\n\n**Cause:** Model too large for GPU VRAM\n\n**Fix:**\n\n```bash\n# Use smaller model\nujust ollama pull -m phi3  # Only 4GB VRAM\n\n# Or use quantized version\nujust ollama pull -m llama3.2:7b-q4_0\n```\n\n### GPU Not Used\n\n**Symptom:** Inference very slow\n\n**Check:**\n\n```bash\nujust ollama status\nujust ollama shell -- nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Reconfigure with explicit GPU\nujust ollama delete\nujust ollama config --gpu-type=nvidia\n```\n\n### API Not Responding\n\n**Symptom:** `curl localhost:11434` fails\n\n**Check:**\n\n```bash\nujust ollama status\nujust ollama logs\n```\n\n**Fix:**\n\n```bash\nujust ollama restart\n```\n\n## Cross-References\n\n- **Related Skills:** `configure gpu` (GPU setup), `jupyter` (ML development)\n- **API Docs:** [https://ollama.ai/docs](https://ollama.ai/docs)\n- **Model Library:** [https://ollama.ai/library](https://ollama.ai/library)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install ollama\", \"setup local LLM\", \"run LLM locally\"\n- \"pull model\", \"download llama\", \"get mistral\"\n- \"ollama not working\", \"model won't load\"\n- \"ollama GPU\", \"ollama cuda\", \"ollama slow\"\n- \"ollama API\", \"integrate with ollama\""
              },
              {
                "name": "openwebui",
                "description": "Open WebUI AI chat interface management via Podman Quadlet. Provides a web UI\nfor interacting with Ollama models. Use when users need to configure, start,\nor manage the Open WebUI service.\n",
                "path": "bazzite-ai/skills/openwebui/SKILL.md",
                "frontmatter": {
                  "name": "openwebui",
                  "description": "Open WebUI AI chat interface management via Podman Quadlet. Provides a web UI\nfor interacting with Ollama models. Use when users need to configure, start,\nor manage the Open WebUI service.\n"
                },
                "content": "# Open WebUI - AI Chat Interface\n\n## Overview\n\nThe `openwebui` command manages the Open WebUI service using Podman Quadlet containers. It provides a web-based chat interface for interacting with Ollama LLM models.\n\n**Key Concept:** Open WebUI connects to Ollama via the `bazzite-ai` network using DNS (`http://ollama:11434`). Ensure Ollama is running before using Open WebUI.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust openwebui config [--port=...] [--bind=...]` | Configure instance |\n| Start | `ujust openwebui start [--instance=...]` | Start service |\n| Stop | `ujust openwebui stop [--instance=...]` | Stop service |\n| Restart | `ujust openwebui restart [--instance=...]` | Restart service |\n| Logs | `ujust openwebui logs [--lines=...]` | View logs |\n| Status | `ujust openwebui status [--instance=...]` | Show status |\n| URL | `ujust openwebui url [--instance=...]` | Show access URL |\n| List | `ujust openwebui list` | List instances |\n| Shell | `ujust openwebui shell [-- CMD...]` | Container shell |\n| Delete | `ujust openwebui delete [--instance=...]` | Remove service |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| Port | `--port` | `-p` | `3000` | Host port for web UI |\n| Image | `--image` | `-i` | `ghcr.io/open-webui/open-webui:main` | Container image |\n| Tag | `--tag` | `-t` | `main` | Image tag |\n| Bind | `--bind` | `-b` | `127.0.0.1` | Bind address |\n| Config Dir | `--config-dir` | `-c` | `~/.config/openwebui/1` | Config/data directory |\n| Workspace | `--workspace-dir` | `-w` | (empty) | Workspace mount |\n| GPU Type | `--gpu-type` | `-g` | `auto` | GPU type |\n| Instance | `--instance` | `-n` | `1` | Instance number or `all` |\n| Lines | `--lines` | `-l` | `50` | Log lines to show |\n\n## Configuration\n\n```bash\n# Default configuration (port 3000, localhost only)\nujust openwebui config\n\n# Custom port (long form)\nujust openwebui config --port=3001\n\n# Custom port (short form)\nujust openwebui config -p 3001\n\n# Network-wide access\nujust openwebui config --bind=0.0.0.0\n\n# Combine parameters (long form)\nujust openwebui config --port=3001 --bind=0.0.0.0\n\n# Combine parameters (short form)\nujust openwebui config -p 3001 -b 0.0.0.0\n\n# GPU-optimized image\nujust openwebui config --image=ghcr.io/open-webui/open-webui:cuda\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured updates the existing settings:\n\n```bash\n# Change only the bind address\nujust openwebui config --bind=0.0.0.0\n\n# Update port without affecting other settings\nujust openwebui config --port=3002\n```\n\n## Container Images\n\n| Image | Description |\n|-------|-------------|\n| `ghcr.io/open-webui/open-webui:main` | Standard image (default) |\n| `ghcr.io/open-webui/open-webui:cuda` | NVIDIA CUDA optimized |\n| `ghcr.io/open-webui/open-webui:ollama` | Bundled with Ollama (not recommended) |\n\n**Note:** GPU is auto-detected and attached regardless of image choice.\n\n## Lifecycle Management\n\n```bash\n# Start Open WebUI\nujust openwebui start\n\n# Stop service\nujust openwebui stop\n\n# Restart (apply config changes)\nujust openwebui restart\n\n# View logs (default 50 lines)\nujust openwebui logs\n\n# View more logs (long form)\nujust openwebui logs --lines=200\n\n# View more logs (short form)\nujust openwebui logs -l 200\n\n# Check status\nujust openwebui status\n\n# Show access URL\nujust openwebui url\n```\n\n## Multi-Instance Support\n\n```bash\n# Start all instances (long form)\nujust openwebui start --instance=all\n\n# Start all instances (short form)\nujust openwebui start -n all\n\n# Stop specific instance\nujust openwebui stop --instance=2\n\n# Delete all instances\nujust openwebui delete --instance=all\n```\n\n## Shell Access\n\n```bash\n# Interactive shell\nujust openwebui shell\n\n# Run specific command (use -- separator)\nujust openwebui shell -- ls -la /app/backend/data\nujust openwebui shell -- cat /app/backend/data/config.json\n```\n\n## Network Architecture\n\nOpen WebUI uses the `bazzite-ai` bridge network for cross-container DNS:\n\n```\n+-------------------+     DNS      +-------------------+\n|   Open WebUI      | -----------> |      Ollama       |\n|   (openwebui)     |              |    (ollama)       |\n|   Port 3000       |              |   Port 11434      |\n+-------------------+              +-------------------+\n         |                                  |\n         +------ bazzite-ai network --------+\n```\n\n**Environment Variables (injected automatically):**\n\n```\nOLLAMA_BASE_URL=http://ollama:11434\nOLLAMA_HOST=http://ollama:11434\nJUPYTER_HOST=http://jupyter:8888\nCOMFYUI_HOST=http://comfyui:8188\n```\n\n## Network Binding\n\n| Bind Address | Access | Use Case |\n|--------------|--------|----------|\n| `127.0.0.1` | Localhost only | Default, secure |\n| `0.0.0.0` | All interfaces | Network access, Tailscale |\n\n**Security Note:** Using `--bind=0.0.0.0` exposes the service to your network. Consider using Tailscale for secure remote access:\n\n```bash\n# Expose via Tailscale (secure)\nujust tailscale serve --service=openwebui\n```\n\n## Data Persistence\n\n| Path | Description |\n|------|-------------|\n| `~/.config/openwebui/<INSTANCE>/data` | Users, chats, settings |\n\nData persists across container restarts. Each instance has isolated data.\n\n## Common Workflows\n\n### Initial Setup\n\n```bash\n# 1. Ensure Ollama is running\nujust ollama start\n\n# 2. Configure Open WebUI\nujust openwebui config\n\n# 3. Start the service\nujust openwebui start\n\n# 4. Access the web UI\nujust openwebui url\n# Output: http://127.0.0.1:3000\n```\n\n### Remote Access Setup\n\n```bash\n# Configure for network access\nujust openwebui config --bind=0.0.0.0\n\n# Start the service\nujust openwebui start\n\n# Or use Tailscale for secure access\nujust tailscale serve --service=openwebui\n```\n\n### Upgrade Container Image\n\n```bash\n# Stop service\nujust openwebui stop\n\n# Update to new image\nujust openwebui config --image=ghcr.io/open-webui/open-webui:main\n\n# Restart\nujust openwebui start\n```\n\n## GPU Support\n\nGPU is automatically detected and attached:\n\n| GPU Type | Detection | Quadlet Config |\n|----------|-----------|----------------|\n| NVIDIA | `nvidia-smi` | `AddDevice=nvidia.com/gpu=all` |\n| AMD | lspci | `AddDevice=/dev/dri` |\n| Intel | lspci | `AddDevice=/dev/dri` |\n\nCheck GPU status:\n\n```bash\nujust openwebui shell -- nvidia-smi\n```\n\n## Troubleshooting\n\n### Service Won't Start\n\n```bash\n# Check status\nujust openwebui status\n\n# View logs\nujust openwebui logs --lines=100\n\n# Check if Ollama is running\nujust ollama status\n```\n\n**Common causes:**\n\n- Port 3000 already in use\n- Ollama not running\n- Container image not pulled\n\n### Can't Connect to Ollama\n\n**Symptom:** \"No models available\" in web UI\n\n**Check:**\n\n```bash\n# Verify Ollama is running\nujust ollama status\n\n# Test Ollama connection from Open WebUI container\nujust openwebui shell -- curl http://ollama:11434/api/tags\n```\n\n**Fix:**\n\n```bash\n# Start Ollama first\nujust ollama start\n\n# Restart Open WebUI\nujust openwebui restart\n```\n\n### Web UI Not Accessible\n\n**Symptom:** Browser can't connect to `http://localhost:3000`\n\n**Check:**\n\n```bash\nujust openwebui status\nujust openwebui url\n```\n\n**Fix:**\n\n```bash\n# If using wrong bind address\nujust openwebui config --bind=127.0.0.1\nujust openwebui restart\n```\n\n### Clear Data and Start Fresh\n\n```bash\n# Delete everything\nujust openwebui delete --instance=all\n\n# Reconfigure\nujust openwebui config\nujust openwebui start\n```\n\n## Cross-References\n\n- **Required:** `ollama` (Ollama must be running for models)\n- **Related:** `jupyter` (ML development), `comfyui` (image generation)\n- **Network:** Uses `bazzite-ai` network (shared with ollama, jupyter, comfyui)\n- **Docs:** [Open WebUI GitHub](https://github.com/open-webui/open-webui)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"install open webui\", \"setup chat interface\", \"web ui for ollama\"\n- \"configure openwebui\", \"change port\", \"network access\"\n- \"open webui not working\", \"can't see models\", \"connection error\"\n- \"open webui logs\", \"debug open webui\"\n- \"delete open webui\", \"uninstall\""
              },
              {
                "name": "pods",
                "description": "Aggregate management for all AI pod services. Provides status overview\nand bulk operations across all pod containers (ollama, jupyter, comfyui,\nopenwebui, localai, fiftyone, jellyfin, runners).\n",
                "path": "bazzite-ai/skills/pods/SKILL.md",
                "frontmatter": {
                  "name": "pods",
                  "description": "Aggregate management for all AI pod services. Provides status overview\nand bulk operations across all pod containers (ollama, jupyter, comfyui,\nopenwebui, localai, fiftyone, jellyfin, runners).\n"
                },
                "content": "# Pods - Aggregate Pod Management\n\n## Overview\n\nThe `pods` command provides aggregate management for all AI pod services. It shows combined status and enables bulk operations across all running pod containers.\n\n**Key Concept:** This is a meta-command for managing multiple pods at once. For individual pod management, use the specific service command (e.g., `ujust ollama`, `ujust jupyter`).\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Status | `ujust pods status` | Show status of all pods |\n| Purge | `ujust pods purge` | Remove all pod containers |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust pods ACTION=\"\"\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | `status`, `purge` | Action to perform |\n\nWithout `ACTION`, shows interactive menu (requires TTY).\n\n## Commands\n\n### Status\n\n```bash\nujust pods status\n```\n\nShows status of all pod services:\n\n- Container running state\n- Port bindings\n- GPU attachment\n- Resource usage\n\n**Output includes:**\n\n- Ollama\n- Jupyter\n- ComfyUI\n- Open WebUI\n- LocalAI\n- FiftyOne\n- Jellyfin\n- GitHub Runners\n\n### Purge\n\n```bash\nujust pods purge\n```\n\nRemoves all pod containers and their configurations:\n\n1. Stops all running pods\n2. Removes all pod containers\n3. Cleans up Quadlet configs\n4. Reloads systemd\n\n**Warning:** This removes ALL pod containers. Data in workspace directories is preserved.\n\n## Common Workflows\n\n### Check All Services\n\n```bash\n# Quick status overview\nujust pods status\n```\n\n### Clean Restart\n\n```bash\n# Remove all pods\nujust pods purge\n\n# Reconfigure and start individual services\nujust ollama config\nujust ollama start\n```\n\n### Before System Update\n\n```bash\n# Check what's running\nujust pods status\n\n# Stop all if needed\nujust pods purge\n```\n\n## Non-Interactive Usage\n\nAll commands work without TTY:\n\n```bash\n# CI/automation-friendly\nujust pods status\nujust pods purge\n```\n\n## Troubleshooting\n\n### Status Shows Stale Containers\n\n**Symptom:** Status shows containers that don't exist\n\n**Cause:** Quadlet configs out of sync\n\n**Fix:**\n\n```bash\nsystemctl --user daemon-reload\nujust pods status\n```\n\n### Purge Doesn't Remove All\n\n**Symptom:** Some containers remain after purge\n\n**Cause:** Containers created outside Quadlet\n\n**Fix:**\n\n```bash\n# Manual cleanup\npodman ps -a\npodman rm -f <container-id>\n```\n\n## Cross-References\n\n- **Individual Services:** `ollama`, `jupyter`, `comfyui`, `openwebui`, `localai`, `fiftyone`, `jellyfin`, `runners` skills\n- **Testing:** `test pods` for lifecycle testing\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"all pods status\", \"check all services\"\n- \"remove all containers\", \"clean up pods\"\n- \"what's running\", \"show all services\"\n- \"purge containers\", \"reset pods\""
              },
              {
                "name": "runners",
                "description": "Self-hosted GitHub Actions runner management via Podman Quadlet. Supports\nmulti-instance pools with ephemeral storage, automatic token generation,\nand rolling updates. Use when users need to set up CI/CD runners for\ntheir GitHub repositories.\n",
                "path": "bazzite-ai/skills/runners/SKILL.md",
                "frontmatter": {
                  "name": "runners",
                  "description": "Self-hosted GitHub Actions runner management via Podman Quadlet. Supports\nmulti-instance pools with ephemeral storage, automatic token generation,\nand rolling updates. Use when users need to set up CI/CD runners for\ntheir GitHub repositories.\n"
                },
                "content": "# Runners - GitHub Actions Self-Hosted Runners\n\n## Overview\n\nThe `runners` command manages self-hosted GitHub Actions runners using Podman Quadlet containers. It supports multi-instance pools with ephemeral storage for clean builds.\n\n**Key Concept:** Each runner instance connects to a GitHub repository and picks up workflow jobs. Ephemeral storage ensures each job starts with a clean state.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Config | `ujust runners config --repo-url=<URL> --instance=<N>` | Configure runner N for repo |\n| Start | `ujust runners start [--instance=N\\|all]` | Start runner(s) |\n| Stop | `ujust runners stop [--instance=N\\|all]` | Stop runner(s) |\n| Restart | `ujust runners restart [--instance=N\\|all]` | Restart runner(s) |\n| Update | `ujust runners update [--instance=N\\|all]` | Update to latest image |\n| Rolling update | `ujust runners rolling-update` | Update with zero downtime |\n| Sync | `ujust runners sync [--instance=N]` | Sync config from source |\n| Logs | `ujust runners logs [--instance=N] [--lines=...]` | View logs |\n| List | `ujust runners list` | List all runners |\n| Shell | `ujust runners shell [--instance=N] [-- CMD...]` | Open shell in container |\n| Delete | `ujust runners delete [--instance=N\\|all]` | Remove runner(s) and images |\n\n## Prerequisites\n\n```bash\n# 1. Authenticate GitHub CLI\ngh auth login\n\n# 2. Verify authentication\ngh auth status\n```\n\n## Configuration\n\n### Parameters\n\n| Parameter | Long Flag | Short | Required | Description |\n|-----------|-----------|-------|----------|-------------|\n| Repo URL | `--repo-url` | `-r` | Yes | GitHub repository URL |\n| Instance | `--instance` | `-n` | Yes | Instance number (1, 2, 3...) |\n| Image | `--image` | `-i` | No | Container image |\n| Tag | `--tag` | `-t` | No | Image tag (default: stable) |\n| Workspace | `--workspace-dir` | `-w` | No | Optional mount to /workspace |\n| Lines | `--lines` | `-l` | No | Log lines to show |\n\n### Configuration Examples\n\n```bash\n# Basic runner (long form)\nujust runners config --repo-url=https://github.com/owner/repo --instance=1\n\n# Basic runner (short form)\nujust runners config -r https://github.com/owner/repo -n 1\n\n# Runner with testing tag\nujust runners config -r https://github.com/owner/repo -n 1 --tag=testing\n\n# Runner with workspace mount\nujust runners config -r https://github.com/owner/repo -n 1 --workspace-dir=/home/user\n```\n\n### Install Multiple Runners\n\n```bash\n# Runner pool for a repository\nujust runners config -r https://github.com/owner/repo -n 1\nujust runners config -r https://github.com/owner/repo -n 2\nujust runners config -r https://github.com/owner/repo -n 3\n\n# Start all\nujust runners start --instance=all\n```\n\n### Update Existing Configuration\n\nRunning `config` when already configured will update the existing configuration, preserving values not explicitly changed.\n\n### Shell Access\n\n```bash\n# Interactive bash shell\nujust runners shell\n\n# Run specific command (use -- separator)\nujust runners shell -- df -h\n\n# Shell in specific instance\nujust runners shell --instance=2 -- cat /config/runner.env\n```\n\n## Lifecycle Commands\n\n### Start/Stop\n\n```bash\n# Single runner\nujust runners start --instance=1\nujust runners stop --instance=1\n\n# Short form\nujust runners start -n 1\nujust runners stop -n 1\n\n# All runners\nujust runners start --instance=all\nujust runners stop --instance=all\n```\n\n### Updates\n\n```bash\n# Fast update (stops runner briefly)\nujust runners update --instance=1\n\n# Rolling update (zero-downtime)\nujust runners rolling-update\n```\n\nRolling update:\n\n1. Stops runner 1\n2. Updates runner 1\n3. Starts runner 1\n4. Waits for healthy state\n5. Repeats for runner 2, 3, ...\n\n### View Logs\n\n```bash\n# Follow logs\nujust runners logs\n\n# Specific instance with line count\nujust runners logs --instance=1 --lines=100\n\n# Short form\nujust runners logs -n 1 -l 100\n```\n\n## Token Management\n\nTokens are **automatically generated** via GitHub API - no manual copying required!\n\n### How It Works\n\n1. Config command calls GitHub API\n2. Generates registration token\n3. Configures runner with token\n4. Token auto-refreshes on restart\n\n### Requirements\n\n- GitHub CLI authenticated (`gh auth login`)\n- Admin access to repository\n\n## Architecture\n\n### Ephemeral Storage\n\nEach runner has ephemeral storage:\n\n- Clean state on every restart\n- No stale artifacts between jobs\n- Prevents cache bloat\n\n### Host Image Cache\n\nRunners access host container cache (read-only):\n\n- Fast container image pulls\n- Shared cache across runners\n- No duplicate downloads\n\n## Configuration Files\n\n| File | Purpose | Location |\n|------|---------|----------|\n| Quadlet unit | Service definition | `~/.config/containers/systemd/github-runner-1.container` |\n| Runner config | Per-runner settings | `~/.config/github-runner/runner-1.env` |\n\n## Common Workflows\n\n### Setup CI for Repository\n\n```bash\n# 1. Authenticate GitHub\ngh auth login\n\n# 2. Configure runner\nujust runners config -r https://github.com/myorg/myrepo -n 1\n\n# 3. Start runner\nujust runners start\n\n# 4. Verify in GitHub\n# Settings  Actions  Runners\n```\n\n### Scale Up Runner Pool\n\n```bash\n# Add more runners\nujust runners config -r https://github.com/myorg/myrepo -n 2\nujust runners config -r https://github.com/myorg/myrepo -n 3\n\n# Start all\nujust runners start --instance=all\n\n# List pool\nujust runners list\n```\n\n### Update All Runners\n\n```bash\n# Option 1: Fast update (brief downtime)\nujust runners stop --instance=all\nujust runners update --instance=all\nujust runners start --instance=all\n\n# Option 2: Rolling update (zero downtime)\nujust runners rolling-update\n```\n\n### Clean Reinstall\n\n```bash\n# Delete runner\nujust runners delete --instance=1\n\n# Reconfigure\nujust runners config -r https://github.com/myorg/myrepo -n 1\nujust runners start\n```\n\n## Workflow Labels\n\nRunners automatically get these labels:\n\n- `self-hosted`\n- `linux`\n- `x64`\n- `bazzite-ai`\n\nUse in workflow:\n\n```yaml\nruns-on: [self-hosted, bazzite-ai]\n```\n\n## Troubleshooting\n\n### Runner Not Appearing in GitHub\n\n**Check:**\n\n```bash\nujust runners status\nujust runners logs --lines=50\n```\n\n**Common causes:**\n\n- GitHub CLI not authenticated\n- Token generation failed\n- Network issues\n\n**Fix:**\n\n```bash\n# Re-authenticate\ngh auth login\n\n# Reconfigure runner\nujust runners delete --instance=1\nujust runners config -r https://github.com/owner/repo -n 1\n```\n\n### Jobs Not Running\n\n**Symptom:** Runner shows \"Idle\" but jobs queue\n\n**Check:**\n\n```bash\nujust runners logs\n```\n\n**Common causes:**\n\n- Labels don't match workflow\n- Runner offline\n- Repository permissions\n\n### Runner Keeps Restarting\n\n**Check:**\n\n```bash\nsystemctl --user status github-runner-1\nujust runners logs --lines=100\n```\n\n**Common causes:**\n\n- Token expired (auto-fixes on restart)\n- Image issues\n- Resource exhaustion\n\n## Cross-References\n\n- **Related Skills:** `pod` (build images)\n- **GitHub Docs:** Actions  Self-hosted runners\n- **Authentication:** `gh auth login`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"setup github runner\", \"self-hosted runner\", \"CI runner\"\n- \"install runner\", \"add runner\", \"more runners\"\n- \"runner not working\", \"runner offline\"\n- \"update runner\", \"rolling update\"\n- \"runner logs\", \"runner status\""
              },
              {
                "name": "tailscale",
                "description": "Tailscale Serve management for exposing local services to your tailnet.\nAuto-detects running bazzite-ai services and creates persistent HTTPS\nendpoints. Use when users need to expose Jupyter, Ollama, ComfyUI or\nother services to their Tailscale network.\n",
                "path": "bazzite-ai/skills/tailscale/SKILL.md",
                "frontmatter": {
                  "name": "tailscale",
                  "description": "Tailscale Serve management for exposing local services to your tailnet.\nAuto-detects running bazzite-ai services and creates persistent HTTPS\nendpoints. Use when users need to expose Jupyter, Ollama, ComfyUI or\nother services to their Tailscale network.\n"
                },
                "content": "# Tailscale - Service Exposure via Tailnet\n\n## Overview\n\nThe `tailscale` command manages Tailscale Serve to expose local bazzite-ai services to your tailnet. It provides HTTPS endpoints with auto-provisioned certificates.\n\n**Key Concept:** Tailscale Serve exposes local services only to your tailnet (not the public internet). HTTPS certificates are automatically provisioned and managed by Tailscale.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Serve | `ujust tailscale serve [--service=...] [--port=...]` | Expose service to tailnet |\n| Unserve | `ujust tailscale unserve [--service=...\\|all]` | Stop exposing service |\n| Status | `ujust tailscale status` | Show current serve configuration |\n| List | `ujust tailscale list` | List available services |\n| Help | `ujust tailscale help` | Show help |\n\n## Prerequisites\n\n```bash\n# Tailscale must be installed and logged in\nsudo dnf install tailscale\nsudo systemctl enable --now tailscaled\ntailscale up\n```\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: serve, unserve, status, list |\n| service | `--service` | `-s` | `\"\"` | Service name or port number |\n| port | `--port` | `-p` | `\"\"` | Tailscale HTTPS port to expose on |\n\n## Known Services\n\n| Service | Default Port | Description |\n|---------|--------------|-------------|\n| `jupyter` | 8888 | JupyterLab notebooks |\n| `ollama` | 11434 | Ollama LLM API |\n| `comfyui` | 8188 | ComfyUI Stable Diffusion |\n| `openwebui` | 3000 | Open WebUI chat interface |\n| `fiftyone` | 5151 | FiftyOne dataset visualization |\n\n## Serve Commands\n\n### Serve by Service Name\n\n```bash\n# Auto-detect port for known services (long form)\nujust tailscale serve --service=jupyter\n\n# Auto-detect port for known services (short form)\nujust tailscale serve -s jupyter\n\n# Serve ollama\nujust tailscale serve -s ollama\n\n# Serve comfyui\nujust tailscale serve -s comfyui\n\n# Serve openwebui\nujust tailscale serve -s openwebui\n```\n\n### Serve by Port Number\n\n```bash\n# Serve arbitrary port (long form)\nujust tailscale serve --service=8080\n\n# Serve arbitrary port (short form)\nujust tailscale serve -s 8080\n\n# Or just the port\nujust tailscale serve 8080\n```\n\n### Serve with Custom Tailscale Port\n\n```bash\n# Expose jupyter on tailscale port 443 (long form)\nujust tailscale serve --service=jupyter --port=443\n\n# Expose jupyter on tailscale port 443 (short form)\nujust tailscale serve -s jupyter -p 443\n\n# Expose on multiple tailscale ports\nujust tailscale serve -s jupyter -p 8888\nujust tailscale serve -s ollama -p 11434\n```\n\n## Unserve Commands\n\n```bash\n# Stop serving a specific service (long form)\nujust tailscale unserve --service=jupyter\n\n# Stop serving a specific service (short form)\nujust tailscale unserve -s jupyter\n\n# Stop serving by port\nujust tailscale unserve -s 8888\n\n# Stop all serves\nujust tailscale unserve all\n```\n\n## Status Commands\n\n```bash\n# Show current serve configuration\nujust tailscale status\n\n# List available bazzite-ai services\nujust tailscale list\n```\n\n## Common Workflows\n\n### Expose JupyterLab\n\n```bash\n# 1. Ensure Jupyter is running\nujust jupyter start\n\n# 2. Expose to tailnet\nujust tailscale serve -s jupyter\n\n# 3. Access from any tailnet device\n# https://<hostname>.<tailnet-name>.ts.net:8888\n```\n\n### Expose Multiple Services\n\n```bash\n# Start services\nujust jupyter start\nujust ollama start\nujust comfyui start\n\n# Expose all\nujust tailscale serve -s jupyter\nujust tailscale serve -s ollama\nujust tailscale serve -s comfyui\n\n# Check status\nujust tailscale status\n```\n\n### Remote AI Development\n\n```bash\n# On your server\nujust jupyter config --bind=127.0.0.1  # Only localhost\nujust jupyter start\nujust tailscale serve -s jupyter\n\n# On your laptop (connected to same tailnet)\n# Access: https://<server>.<tailnet>.ts.net:8888\n```\n\n### Clean Up All Serves\n\n```bash\n# Stop all tailscale serves\nujust tailscale unserve all\n\n# Verify\nujust tailscale status\n```\n\n## Features\n\n### Auto-Detection\n\nWhen you serve a known service, the command auto-detects:\n\n- Whether the service is running\n- The correct local port\n- Appropriate HTTPS configuration\n\n### Persistent Serves\n\nServes persist across reboots. Tailscale remembers your configuration.\n\n### HTTPS Certificates\n\nTailscale automatically:\n\n- Provisions certificates\n- Handles renewals\n- Terminates TLS at edge\n\n### Tailnet-Only\n\nUnlike Tailscale Funnel, Serve only exposes to your tailnet:\n\n- No public internet exposure\n- Access limited to your devices\n- Requires Tailscale authentication\n\n## Troubleshooting\n\n### Service Not Found\n\n**Symptom:** \"Service not found\" error\n\n**Check:**\n\n```bash\n# Verify service is running\nujust jupyter status\nsystemctl --user status jupyter-1\n```\n\n**Fix:**\n\n```bash\n# Start the service first\nujust jupyter start\n# Then serve\nujust tailscale serve -s jupyter\n```\n\n### Tailscale Not Running\n\n**Symptom:** \"Tailscale not running or not logged in\"\n\n**Fix:**\n\n```bash\n# Start tailscaled\nsudo systemctl start tailscaled\n\n# Login to Tailscale\ntailscale up\n```\n\n### Cannot Access from Other Device\n\n**Check:**\n\n```bash\n# Verify serve is active\nujust tailscale status\n\n# Check Tailscale connection\ntailscale status\n```\n\n**Common causes:**\n\n- Not on same tailnet\n- Firewall blocking\n- Service not bound to localhost\n\n**Fix:**\n\n```bash\n# Ensure service binds to localhost (required for Serve)\nujust jupyter config --bind=127.0.0.1\nujust jupyter restart\nujust tailscale serve -s jupyter\n```\n\n### Port Conflict\n\n**Symptom:** \"Port already in use on tailscale\"\n\n**Fix:**\n\n```bash\n# Use different tailscale port\nujust tailscale serve -s jupyter -p 8889\n```\n\n## Security Considerations\n\n**Tailscale Serve is secure by design:**\n\n- Only accessible from your tailnet\n- Requires Tailscale authentication\n- Uses WireGuard encryption\n- HTTPS with auto-managed certificates\n\n**Best practices:**\n\n1. Keep services bound to localhost (`127.0.0.1`)\n2. Use strong Tailscale ACLs\n3. Review serves periodically with `status`\n4. Remove unused serves with `unserve`\n\n## Cross-References\n\n- **Related Skills:** `jupyter`, `ollama`, `comfyui`, `openwebui`\n- **Prerequisites:** `ujust config tailscale enable`\n- **Tailscale Docs:** <https://tailscale.com/kb/1242/tailscale-serve>\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"expose to tailnet\", \"tailscale serve\"\n- \"access jupyter remotely\", \"remote access\"\n- \"share service with tailscale\"\n- \"tailscale status\", \"stop tailscale serve\""
              },
              {
                "name": "test",
                "description": "Runtime verification tests for bazzite-ai installation. Tests GPU detection,\nCUDA, PyTorch, service health, network connectivity, and pod lifecycles.\nUse when users need to verify their bazzite-ai installation works correctly.\n",
                "path": "bazzite-ai/skills/test/SKILL.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Runtime verification tests for bazzite-ai installation. Tests GPU detection,\nCUDA, PyTorch, service health, network connectivity, and pod lifecycles.\nUse when users need to verify their bazzite-ai installation works correctly.\n"
                },
                "content": "# Test - Runtime Verification\n\n## Overview\n\nThe `test` command provides comprehensive runtime verification for bazzite-ai installations. It tests GPU detection, CUDA/PyTorch functionality, service health, network connectivity, and pod container lifecycles.\n\n**Key Concept:** Tests run on the LOCAL system to verify actual functionality, not just syntax.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Quick test | `ujust test quick` | GPU + service status (~30s) |\n| GPU test | `ujust test gpu` | GPU detection and CDI check |\n| CUDA test | `ujust test cuda` | CUDA tests in nvidia container |\n| PyTorch test | `ujust test pytorch` | PyTorch GPU access test |\n| All tests | `ujust test all` | Full test suite (~2min) |\n| Help | `ujust test help` | Show all options |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust test ACTION=\"\"\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | See actions below | Test to run |\n\nWithout `ACTION`, shows interactive menu (requires TTY).\n\n### Test Options\n\n```bash\nujust test [ACTION] [--instance=N] [--image=IMAGE] [--cpus=N] [--ram=MB] [--ssh-port=PORT]\n```\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `--instance, -n` | `90` | Pod instance number for test pods |\n| `--image, -i` | (default) | Container image for bootc testing |\n| `--cpus` | `4` | CPUs for bootc VM |\n| `--ram` | `8192` | RAM in MB for bootc VM |\n| `--ssh-port` | `2222` | SSH port for bootc VM |\n\n## Available Tests\n\n### Quick Tests\n\n```bash\nujust test quick      # GPU + service status (~30s)\nujust test status     # Test status summary\n```\n\n### GPU Tests\n\n```bash\nujust test gpu        # GPU detection and CDI check\nujust test cuda       # CUDA tests in nvidia container\nujust test pytorch    # PyTorch GPU access test\n```\n\n### Service Tests\n\n```bash\nujust test ollama     # Ollama health + quick inference\nujust test jupyter    # Jupyter service health\nujust test comfyui    # ComfyUI service health\nujust test openwebui  # Open WebUI service health\nujust test services   # All installed services status\n```\n\n### Infrastructure Tests\n\n```bash\nujust test config     # Configuration dispatcher test\nujust test network    # Registry connectivity test\nujust test apptainer  # Apptainer GPU detection\n```\n\n### VM Tests\n\n```bash\nujust test bootc                    # Ephemeral bootc VM (auto-cleanup)\nujust test bootc --image=stable     # Test specific image\n```\n\n### Pod Lifecycle Tests\n\n```bash\nujust test pods config --instance=91   # Configure test pods\nujust test pods start --instance=91    # Start test pods\nujust test pods status --instance=91   # Check test pods status\nujust test pods stop --instance=91     # Stop test pods\nujust test pods delete --instance=91   # Delete test pod configs\nujust test pods all --instance=91      # Full lifecycle test\n```\n\n## Common Workflows\n\n### Verify New Installation\n\n```bash\n# Quick verification\nujust test quick\n\n# If issues found, run full suite\nujust test all\n```\n\n### Verify GPU Support\n\n```bash\n# Check GPU detection\nujust test gpu\n\n# Test CUDA if NVIDIA\nujust test cuda\n\n# Test PyTorch GPU access\nujust test pytorch\n```\n\n### Verify Services\n\n```bash\n# Test all services\nujust test services\n\n# Or individual services\nujust test ollama\nujust test jupyter\n```\n\n### Test Pod Lifecycle\n\n```bash\n# Full lifecycle test with isolated instance\nujust test pods all --instance=91\n```\n\n## Non-Interactive Usage\n\nAll tests work without TTY:\n\n```bash\n# CI/automation-friendly\nujust test quick\nujust test gpu\nujust test all\n```\n\n## Troubleshooting\n\n### GPU Not Detected\n\n**Symptom:** `ujust test gpu` shows no GPU\n\n**Cause:** GPU drivers not loaded or CDI not configured\n\n**Fix:**\n\n```bash\n# Check NVIDIA driver\nnvidia-smi\n\n# Check CDI\nls /etc/cdi/\n\n# Setup GPU container support\nujust config gpu setup\n```\n\n### CUDA Test Fails\n\n**Symptom:** `ujust test cuda` fails\n\n**Cause:** NVIDIA container toolkit not configured\n\n**Fix:**\n\n```bash\nujust config gpu setup\n# May require reboot\n```\n\n### Service Test Fails\n\n**Symptom:** Service test shows unhealthy\n\n**Cause:** Service not running or misconfigured\n\n**Fix:**\n\n```bash\n# Check specific service\nujust <service> status\n\n# View logs\nujust <service> logs\n\n# Restart service\nujust <service> restart\n```\n\n## Cross-References\n\n- **GPU Setup:** `config` skill (ujust config gpu setup)\n- **Service Management:** Individual service skills (ollama, jupyter, etc.)\n- **Pod Management:** `pods` skill for aggregate operations\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"verify installation\", \"test bazzite-ai\", \"check if working\"\n- \"GPU test\", \"CUDA test\", \"PyTorch test\"\n- \"service health\", \"check services\"\n- \"network connectivity\", \"registry access\"\n- \"test pods\", \"lifecycle test\""
              },
              {
                "name": "vm",
                "description": "QCOW2 virtual machine management using libvirt. Creates VMs from pre-built\nimages downloaded from R2 CDN with cloud-init customization. Supports SSH,\nVNC, and virtiofs home directory sharing. Use when users need to create,\nmanage, or connect to bazzite-ai VMs.\n",
                "path": "bazzite-ai/skills/vm/SKILL.md",
                "frontmatter": {
                  "name": "vm",
                  "description": "QCOW2 virtual machine management using libvirt. Creates VMs from pre-built\nimages downloaded from R2 CDN with cloud-init customization. Supports SSH,\nVNC, and virtiofs home directory sharing. Use when users need to create,\nmanage, or connect to bazzite-ai VMs.\n"
                },
                "content": "# VM - QCOW2 Virtual Machine Management\n\n## Overview\n\nThe `vm` command manages bazzite-ai virtual machines using libvirt. VMs are created from pre-built QCOW2 images downloaded from R2 CDN, customized via cloud-init.\n\n**Key Concept:** VMs run in user session (qemu:///session), not requiring root. Home directory is shared via virtiofs at `/workspace` in the VM.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Add VM | `ujust vm add [NAME] [--cpus=...] [--ram=...]` | Download + create VM |\n| Update VM | `ujust vm update [NAME] [--what=...]` | Update VM config |\n| Delete VM | `ujust vm delete [NAME]` | Remove VM |\n| Download | `ujust vm download [--branch=...]` | Download QCOW2 image |\n| Seed | `ujust vm seed [NAME] [--username=...]` | Create cloud-init ISO |\n| Create | `ujust vm create [NAME] [--cpus=...] [--ram=...]` | Create VM from image |\n| Start | `ujust vm start [NAME]` | Start VM |\n| Stop | `ujust vm stop [NAME]` | Stop VM |\n| SSH | `ujust vm ssh [NAME] [--ssh-user=...]` | SSH to VM |\n| VNC | `ujust vm vnc [NAME]` | Open VNC viewer |\n| Status | `ujust vm status [NAME]` | Show VM status |\n| Help | `ujust vm help` | Show help |\n\n## Parameters\n\n| Parameter | Long Flag | Short | Default | Description |\n|-----------|-----------|-------|---------|-------------|\n| action | (positional) | - | required | Action: add, update, delete, download, etc. |\n| vm_name | (positional) | - | `bazzite-ai` | VM name |\n| url | `--url` | - | R2 CDN URL | QCOW2 image URL |\n| cpus | `--cpus` | - | `4` | Number of CPUs |\n| ram | `--ram` | - | `8192` | Memory in MB |\n| disk_size | `--disk-size` | - | `100G` | Disk size |\n| username | `--username` | `-u` | `$USER` | VM username |\n| password | `--password` | - | (empty) | VM password |\n| autologin | `--autologin` | - | `true` | Enable autologin |\n| ssh_port | `--ssh-port` | - | `4444` | SSH port forwarding |\n| vnc_port | `--vnc-port` | - | `5900` | VNC port |\n| ssh_user | `--ssh-user` | - | `$USER` | SSH user for connection |\n| share_dir | `--share-dir` | - | `$HOME` | Directory to share |\n| branch | `--branch` | `-b` | `stable` | Image branch (stable/testing) |\n| what | `--what` | - | - | Update target (for update action) |\n\n## Add VM (Full Workflow)\n\n```bash\n# Default: bazzite-ai VM with auto-detect settings\nujust vm add\n\n# Named VM with custom config (long form)\nujust vm add myvm --cpus=8 --ram=16384 --disk-size=200G\n\n# Testing branch image\nujust vm add testing-vm --branch=testing\n\n# Short form for branch\nujust vm add testing-vm -b testing\n\n# Different SSH port\nujust vm add dev-vm --ssh-port=4445\n\n# No home sharing\nujust vm add isolated --share-dir=''\n```\n\nThe `add` command:\n\n1. Downloads QCOW2 image (cached)\n2. Creates cloud-init seed ISO\n3. Creates libvirt VM\n4. Configures port forwarding\n\n## Individual Steps\n\n### Download QCOW2\n\n```bash\n# Stable image (default)\nujust vm download\n\n# Testing branch (long form)\nujust vm download --branch=testing\n\n# Testing branch (short form)\nujust vm download -b testing\n\n# Custom URL\nujust vm download --url=https://example.com/custom.qcow2\n```\n\n### Create Seed ISO\n\n```bash\n# Long form\nujust vm seed myvm --username=developer --password=secret\n\n# Short form for username\nujust vm seed myvm -u developer --password=secret\n```\n\n### Create VM\n\n```bash\nujust vm create myvm --cpus=4 --ram=8192\n```\n\n## VM Lifecycle\n\n### Start VM\n\n```bash\nujust vm start              # Default VM\nujust vm start myvm         # Named VM\n```\n\nAuto-adds VM if it doesn't exist.\n\n### Stop VM\n\n```bash\nujust vm stop              # Graceful shutdown\nujust vm stop myvm         # Named VM\n```\n\n### Delete VM\n\n```bash\nujust vm delete myvm        # Remove VM and disk\n```\n\n## Connecting to VM\n\n### SSH Connection\n\n```bash\n# Connect to default VM\nujust vm ssh\n\n# Named VM\nujust vm ssh myvm\n\n# Different user\nujust vm ssh myvm --ssh-user=root\n\n# Run command (use -- separator)\nujust vm ssh myvm -- ls -la\n```\n\nDefault SSH: `ssh -p 4444 localhost`\n\n### VNC Connection\n\n```bash\nujust vm vnc              # Opens VNC viewer\nujust vm vnc myvm\n```\n\nDefault VNC: Port 5900\n\n## Home Directory Sharing\n\nBy default, your home directory is shared to the VM at `/workspace` via virtiofs.\n\n```bash\n# Default: $HOME -> /workspace\nujust vm add\n\n# Disable sharing\nujust vm add isolated --share-dir=''\n\n# Share specific directory\nujust vm add project --share-dir=/path/to/project\n```\n\nInside VM:\n\n```bash\nls /workspace  # Your home directory\n```\n\n## Image Branches\n\n| Branch | Tag | Description |\n|--------|-----|-------------|\n| `stable` | `:stable` | Production, tested |\n| `testing` | `:testing` | Latest features |\n\n```bash\n# Long form\nujust vm download --branch=stable\nujust vm download --branch=testing\n\n# Short form\nujust vm download -b stable\nujust vm download -b testing\n```\n\n## Storage Locations\n\n| Item | Location |\n|------|----------|\n| Download cache | `~/.local/share/bazzite-ai/vm/cache/` |\n| VM disks | `~/.local/share/libvirt/images/` |\n| VM config | `~/.local/share/bazzite-ai/vm/<name>.conf` |\n| Seed ISO | `~/.local/share/bazzite-ai/vm/<name>-seed.iso` |\n\n## Common Workflows\n\n### Quick Test VM\n\n```bash\n# Add and start default VM\nujust vm add\nujust vm start\nujust vm ssh\n```\n\n### Development Environment\n\n```bash\n# Create dev VM with more resources\nujust vm add dev --cpus=8 --ram=16384 --disk-size=200G\n\n# Start it\nujust vm start dev\n\n# SSH in\nujust vm ssh dev\n\n# Your home is at /workspace\n```\n\n### Testing Branch\n\n```bash\n# Test latest features (long form)\nujust vm add testing-vm --branch=testing\n\n# Or short form\nujust vm add testing-vm -b testing\n\nujust vm start testing-vm\nujust vm ssh testing-vm\n```\n\n### Multiple VMs\n\n```bash\n# Create VMs on different ports\nujust vm add dev1 --ssh-port=4444\nujust vm add dev2 --ssh-port=4445\nujust vm add dev3 --ssh-port=4446\n\n# Start all (not a built-in command, use loop)\nfor vm in dev1 dev2 dev3; do ujust vm start $vm; done\n```\n\n## Troubleshooting\n\n### VM Won't Start\n\n**Check:**\n\n```bash\nujust vm status myvm\nvirsh --connect qemu:///session list --all\n```\n\n**Common causes:**\n\n- Disk image not found\n- Port conflict\n- Virtiofs path issue\n\n**Fix:**\n\n```bash\nujust vm delete myvm\nujust vm add myvm\n```\n\n### SSH Connection Refused\n\n**Check:**\n\n```bash\nssh -p 4444 localhost\n```\n\n**Common causes:**\n\n- VM not fully booted\n- Wrong SSH port\n- SSH not started in VM\n\n**Fix:**\n\n```bash\n# Wait longer after start\nsleep 30\nujust vm ssh myvm\n\n# Check VM console via VNC\nujust vm vnc myvm\n```\n\n### Virtiofs Not Working\n\n**Symptom:** `/workspace` empty or not mounted\n\n**Cause:** SHARE_DIR path issue (symlinks)\n\n**Fix:**\n\n```bash\n# Delete and recreate with canonical path\nujust vm delete myvm\nujust vm add myvm --share-dir=$(readlink -f $HOME)\n```\n\n### Out of Disk Space\n\n**Check:**\n\n```bash\nqemu-img info ~/.local/share/libvirt/images/myvm.qcow2\n```\n\n**Fix:**\n\n```bash\n# Create new VM with larger disk\nujust vm delete myvm\nujust vm add myvm --disk-size=200G\n```\n\n## Cross-References\n\n- **Related Skills:** `bootc` (alternative: bootc-based VMs)\n- **Prerequisites:** `ujust config libvirtd enable`\n- **bcvk alternative:** `ujust install bcvk` + `ujust bootc`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"create VM\", \"add VM\", \"start VM\"\n- \"ssh to VM\", \"connect to VM\"\n- \"download qcow2\", \"VM image\"\n- \"VM not starting\", \"VM connection failed\"\n- \"share directory with VM\", \"virtiofs\""
              }
            ]
          },
          {
            "name": "bazzite-ai-dev",
            "description": "Development tools and enforcement agents for Bazzite AI contributors",
            "source": "./bazzite-ai-dev",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "atrawog"
            },
            "install_commands": [
              "/plugin marketplace add atrawog/bazzite-ai-plugins",
              "/plugin install bazzite-ai-dev@bazzite-ai-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T13:36:51Z",
              "created_at": "2025-12-26T20:21:42Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "build",
                "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n",
                "path": "bazzite-ai-dev/skills/build/SKILL.md",
                "frontmatter": {
                  "name": "build",
                  "description": "Development: Unified build system for OS images, pods, VMs, and ISOs.\nRun from repository root with 'just build <subcommand>'. Includes smart\ncache strategy that matches GitHub Actions for optimal build times.\n"
                },
                "content": "# Build - Unified Build System\n\n## Overview\n\nThe `build` command provides a unified interface for all bazzite-ai build operations:\n\n- OS container images\n- Pod container variants\n- VM images (QCOW2/RAW)\n- Live ISO installers\n- Push to registry\n- Sign with cosign\n\n**Smart Caching:** Automatically detects git branch and uses matching cache tag, ensuring local builds are compatible with GitHub Actions builds.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Build OS | `just build os` | Build OS container image |\n| Build pod | `just build pod nvidia` | Build specific pod variant |\n| Build all pods | `just build pod all` | Build all pod variants |\n| Build ISO | `just build iso` | Build live ISO installer |\n| Build QCOW2 | `just build qcow2` | Build QCOW2 VM image |\n| Build RAW | `just build raw` | Build RAW VM image |\n| Generate lock | `just build pixi python` | Generate pixi.lock |\n| Push OS | `just build push os` | Push OS image to registry |\n| Push pod | `just build push pod nvidia` | Push pod to registry |\n| Sign OS | `just build sign os` | Sign OS image with cosign |\n| Sign pod | `just build sign pod nvidia` | Sign pod with cosign |\n| Show status | `just build status` | Show cache/build status |\n\n## Pod Variants\n\n| Variant | Image Name | Description |\n|---------|------------|-------------|\n| `base` | `bazzite-ai-pod` | CPU-only development |\n| `nvidia` | `bazzite-ai-pod-nvidia` | GPU compute with CUDA |\n| `nvidia-python` | `bazzite-ai-pod-nvidia-python` | NVIDIA + ML packages |\n| `jupyter` | `bazzite-ai-pod-jupyter` | JupyterLab + ML stack |\n| `ollama` | `bazzite-ai-pod-ollama` | LLM inference |\n| `comfyui` | `bazzite-ai-pod-comfyui` | Stable Diffusion UI |\n| `devops` | `bazzite-ai-pod-devops` | AWS/kubectl/Helm tools |\n| `githubrunner` | `bazzite-ai-pod-githubrunner` | CI/CD pipeline |\n\n## Smart Cache Strategy\n\nThe build system automatically detects your git branch and uses the appropriate cache tag to maximize cache reuse between local and CI builds:\n\n| Branch | Cache Tag | Build Tag |\n|--------|-----------|-----------|\n| `main` | `stable` | `stable` |\n| `testing` | `testing` | `testing` |\n| Other | `{branch}` | `{branch}` |\n\nThis ensures that when you build locally on the `testing` branch, you pull cache layers from the `:testing` images pushed by GitHub Actions.\n\n## Environment Variables\n\nFor CI integration, the following environment variables are supported:\n\n| Variable | Purpose |\n|----------|---------|\n| `COSIGN_PRIVATE_KEY` | Private key for signing with cosign |\n| `BUILD_LABELS` | Space-separated OCI labels to apply during build |\n| `BUILD_TAGS` | Space-separated tags to apply (overrides default) |\n| `BASE_IMAGE` | Override base image for pod builds (for digest pinning) |\n\n## Common Workflows\n\n### Build OS Image\n\n```bash\n# Build with branch-appropriate tag\njust build os\n\n# Build with custom tag\njust build os custom-tag\n```\n\n### Build Pods\n\n```bash\n# Interactive selection\njust build pod\n\n# Specific variant\njust build pod nvidia\n\n# All variants\njust build pod all\n```\n\n### Build VM/ISO\n\n```bash\n# Build QCOW2 VM image\njust build qcow2\n\n# Build live ISO\njust build iso\n\n# Build RAW image\njust build raw\n```\n\n### Push to Registry\n\n```bash\n# Push OS image\njust build push os\n\n# Push specific pod\njust build push pod nvidia\n\n# Push all pods\njust build push pod all\n```\n\n### Sign Images\n\n```bash\n# Sign OS image (requires COSIGN_PRIVATE_KEY env var)\nCOSIGN_PRIVATE_KEY=$KEY just build sign os\n\n# Sign pod\nCOSIGN_PRIVATE_KEY=$KEY just build sign pod nvidia\n```\n\n### Generate Pixi Locks\n\n```bash\n# Python variant\njust build pixi python\n\n# Jupyter variant\njust build pixi jupyter\n\n# All variants\njust build pixi all\n```\n\n## CI Integration\n\nThe build commands are designed for GitHub Actions integration:\n\n```yaml\n# Build, push, and sign in CI\n- name: Build and push OS\n  env:\n    BUILD_LABELS: ${{ steps.metadata.outputs.labels }}\n    COSIGN_PRIVATE_KEY: ${{ secrets.SIGNING_SECRET }}\n  run: |\n    just build os $TAG\n    just build push os $TAG\n    just build sign os $TAG\n\n# Build pod with base image digest\n- name: Build nvidia pod\n  env:\n    BASE_IMAGE: ghcr.io/owner/bazzite-ai-pod@${{ needs.base.outputs.digest }}\n  run: just build pod nvidia $TAG\n```\n\n## Output Images\n\nImages are tagged with the registry prefix:\n\n```\nghcr.io/atrawog/bazzite-ai:{tag}           # OS image\nghcr.io/atrawog/bazzite-ai-pod:{tag}       # Base pod\nghcr.io/atrawog/bazzite-ai-pod-nvidia:{tag} # NVIDIA pod\nghcr.io/atrawog/bazzite-ai-pod-comfyui:{tag} # ComfyUI pod\n```\n\n## Requirements\n\n- Podman installed and configured\n- Git repository cloned\n- Sufficient disk space (~10GB for OS, ~20GB for ISO)\n- Network access (pulls base images)\n- cosign installed (for signing)\n- Registry authentication (for push)\n\n## Troubleshooting\n\n### Build Fails with Cache Error\n\n**Symptom:** Cache layer not found\n\n**Cause:** Remote image not yet pushed for this branch\n\n**Fix:**\n\n```bash\n# Build without cache (first build on new branch)\n# Or check status to see cache state\njust build status\n```\n\n### Pod Build Fails with Base Image Missing\n\n**Symptom:** Cannot find base pod image\n\n**Cause:** Parent variant not built\n\n**Fix:**\n\n```bash\n# Build in order (base -> nvidia -> jupyter)\njust build pod base\njust build pod nvidia\njust build pod jupyter\n```\n\n### Push Fails with Authentication Error\n\n**Symptom:** unauthorized: authentication required\n\n**Cause:** Not logged into registry\n\n**Fix:**\n\n```bash\n# Login to GitHub Container Registry\npodman login ghcr.io\n```\n\n### Sign Fails\n\n**Symptom:** cosign not found or key not set\n\n**Cause:** cosign not installed or COSIGN_PRIVATE_KEY not set\n\n**Fix:**\n\n```bash\n# Check cosign is installed\nwhich cosign\n\n# Set signing key\nexport COSIGN_PRIVATE_KEY=\"$(cat cosign.key)\"\n```\n\n### CUDA Test Fails\n\n**Symptom:** nvidia-smi not found\n\n**Cause:** No GPU available or CDI not configured\n\n**Fix:**\n\n```bash\n# Verify GPU on host\nnvidia-smi\n\n# Check CDI configuration\nls /etc/cdi/\n```\n\n## Cross-References\n\n- **Related Skills:** `clean` (cleanup build artifacts)\n- **System Commands:** `ujust jupyter`, `ujust ollama` (use built pods)\n- **Documentation:** See `Containerfile` for image layers\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"build os\", \"build image\", \"build container\"\n- \"build pod\", \"build nvidia\", \"build jupyter\", \"build comfyui\"\n- \"build iso\", \"build qcow2\", \"build vm\"\n- \"push os\", \"push pod\", \"push to registry\"\n- \"sign image\", \"cosign\", \"sign pod\"\n- \"pixi lock\", \"generate lock\"\n- \"just build\" (any build command)"
              },
              {
                "name": "clean",
                "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n",
                "path": "bazzite-ai-dev/skills/clean/SKILL.md",
                "frontmatter": {
                  "name": "clean",
                  "description": "Development: Cleanup and maintenance for the development environment.\nRemoves build artifacts, caches, containers, and recovers disk space.\nRun from repository root with 'just clean'. Use when developers need\nto free disk space or reset the build environment.\n"
                },
                "content": "# Clean - Cleanup & Maintenance\n\n## Overview\n\nThe `clean` development commands remove build artifacts, caches, containers, and other temporary files to recover disk space and reset the development environment.\n\n**Key Concept:** This is a **development command** - run with `just` from the repository root, not `ujust`. It provides both interactive menu and non-interactive modes.\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Interactive menu | `just clean` | Show cleanup options |\n| Status report | `just clean status` | Show what would be cleaned |\n| Safe cleanup | `just clean all` | Safe cleanup (preserves running containers) |\n| Nuclear cleanup | `just clean nuke` | NUCLEAR: destroy everything (requires NUKE confirmation) |\n| Podman prune | `just clean podman` | Full podman system prune |\n| Images | `just clean images` | Dangling images only |\n| All images | `just clean images all` | All unused images |\n| Build cache | `just clean images build-cache` | Podman builder cache |\n| Containers | `just clean containers` | Stopped containers |\n| Runners | `just clean runners` | Stop/restart GitHub runners |\n| VMs | `just clean vm` | VM images (libvirt + cache) |\n| System | `just clean system` | Tmp files + journal |\n| Logs | `just clean logs` | Remove *.log files |\n| Docs | `just clean docs` | Remove site/ directory |\n| Output | `just clean output` | Remove output/ contents |\n| Cache menu | `just clean cache` | Cache cleanup submenu |\n| Pixi cache | `just clean cache pixi` | .pixi/ + ~/.cache/rattler |\n| Venv | `just clean cache venv` | venv/ directory |\n| Chunkhound | `just clean cache chunkhound` | .chunkhound/ directory |\n| Pip | `just clean cache pip` | ~/.cache/pip/ |\n| Pre-commit | `just clean cache precommit` | ~/.cache/pre-commit/ |\n| GitHub CLI | `just clean cache gh` | ~/.cache/gh/ |\n\n## Safe vs Nuclear Cleanup\n\n### Safe Cleanup (`just clean all`)\n\nSafe cleanup that preserves running containers and configurations:\n\n1. Stop GitHub runners\n2. Remove runner containers\n3. Remove stopped containers\n4. Remove buildah working containers\n5. Clean /var/tmp (buildah artifacts)\n6. Podman system prune\n7. Clean builder cache\n8. Prune unused images\n9. Remove build logs\n10. Remove docs output\n11. Remove build output\n12. Clean all caches\n13. Vacuum journal logs\n14. Prune volumes\n15. Restart GitHub runners\n\n**Use when:** You want to free disk space but keep your pod configurations intact.\n\n### Nuclear Cleanup (`just clean nuke`)\n\n**DESTROYS EVERYTHING** - requires typing 'NUKE' to confirm:\n\n- Removes ALL containers (running and stopped)\n- Removes ALL images\n- Removes ALL volumes\n- Removes ALL pod configurations\n- Removes ALL cached data\n- Cleans system caches\n\n**Use when:** You want a completely fresh start or are troubleshooting persistent issues.\n\n**Warning:** This will delete:\n\n- All pod configurations (you'll need to reconfigure)\n- All downloaded container images (will need to re-pull)\n- All model data if stored in containers\n- All runner configurations\n\n## Parameters\n\n```bash\njust clean [ACTION] [SUBOPTION]\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | See quick reference | Cleanup action |\n| `SUBOPTION` | Varies by action | Sub-action for nested menus |\n\n## Cleanup Actions\n\n### status\n\nShow what would be cleaned (dry-run):\n\n```bash\njust clean status\n```\n\n**Reports:**\n\n- Podman images/containers\n- System files (/var/tmp, journal)\n- Build artifacts (logs, docs, output)\n- Caches (pixi, venv, pip, etc.)\n\n### all\n\nSafe cleanup (15 steps):\n\n```bash\njust clean all\n```\n\n### nuke\n\nNuclear option (requires NUKE confirmation):\n\n```bash\njust clean nuke\n# Type 'NUKE' when prompted to confirm\n```\n\n### podman\n\nFull podman system prune:\n\n```bash\njust clean podman\n```\n\n**Removes:**\n\n- All unused images\n- Stopped containers\n- Unused volumes\n- Builder cache\n\n### images\n\nClean podman images:\n\n```bash\njust clean images              # Dangling only\njust clean images all          # All unused\njust clean images build-cache  # Builder cache\n```\n\n### containers\n\nRemove stopped containers:\n\n```bash\njust clean containers\n```\n\n### runners\n\nManage GitHub runners:\n\n```bash\njust clean runners stop   # Stop runners\njust clean runners start  # Start runners\n```\n\n### vm\n\nClean VM images:\n\n```bash\njust clean vm            # Interactive\njust clean vm libvirt    # Libvirt VMs\njust clean vm cache      # VM cache\n```\n\n### system\n\nSystem cleanup:\n\n```bash\njust clean system        # Interactive\njust clean system tmp    # Clean /var/tmp\njust clean system journal # Vacuum journal logs\n```\n\n### cache\n\nClean development caches:\n\n```bash\njust clean cache          # Interactive\njust clean cache pixi     # .pixi/ + ~/.cache/rattler\njust clean cache venv     # venv/\njust clean cache chunkhound # .chunkhound/\njust clean cache pip      # ~/.cache/pip/\njust clean cache precommit # ~/.cache/pre-commit/\njust clean cache gh       # ~/.cache/gh/\n```\n\n## Common Workflows\n\n### Check Before Cleanup\n\n```bash\n# See what would be cleaned\njust clean status\n\n# Then decide what to clean\njust clean podman\n```\n\n### Recover Disk Space\n\n```bash\n# Safe cleanup\njust clean all\n\n# Or targeted cleanup\njust clean images all\njust clean cache pixi\njust clean output\n```\n\n### Reset Build Environment\n\n```bash\n# Clean all caches and build artifacts\njust clean cache all\njust clean output\njust clean docs\n\n# Reinstall dependencies\njust docs-install\n```\n\n### Before Major Rebuild\n\n```bash\n# Clean containers and images\njust clean podman\n\n# Then rebuild\njust build os\n```\n\n### Complete Fresh Start\n\n```bash\n# Nuclear option - destroys everything\njust clean nuke\n# Type 'NUKE' to confirm\n\n# Reconfigure everything from scratch\nujust jupyter config\nujust ollama config\n```\n\n## Disk Space Targets\n\n| Target | Typical Size | Command |\n|--------|--------------|---------|\n| Podman images | 10-50GB | `clean podman` |\n| Builder cache | 1-10GB | `clean images build-cache` |\n| /var/tmp | 1-5GB | `clean system tmp` |\n| Journal logs | 100MB-1GB | `clean system journal` |\n| Pixi cache | 1-5GB | `clean cache pixi` |\n| Output/ | 1-20GB | `clean output` |\n\n## Troubleshooting\n\n### Cleanup Fails with Permission Error\n\n**Symptom:** Cannot remove files in output/ or /var/tmp\n\n**Fix:**\n\n```bash\n# Fix permissions\nsudo chown -R $USER:$USER output/\n\n# For /var/tmp\nsudo rm -rf /var/tmp/buildah*\n```\n\n### Podman Prune Doesn't Free Space\n\n**Symptom:** Images still present after prune\n\n**Cause:** Containers referencing images\n\n**Fix:**\n\n```bash\n# Stop and remove all containers first\njust clean containers\njust clean runners stop\n\n# Then prune\njust clean podman\n```\n\n### GitHub Runners Won't Restart\n\n**Symptom:** Runners fail to start after cleanup\n\n**Cause:** Configuration lost or token expired\n\n**Fix:**\n\n```bash\n# Re-authenticate\njust gh-login\n\n# Reconfigure runners\nujust runners config <REPO_URL> 1\n```\n\n## Cross-References\n\n- **Related Skills:** `pods` (build pods), `vms` (build VMs), `docs` (build docs)\n- **GitHub Runners:** `ujust runners` (runner management)\n- **Disk Analysis:** `just clean status`\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"clean up\", \"cleanup\", \"free disk space\"\n- \"remove containers\", \"prune images\"\n- \"clean cache\", \"clear cache\"\n- \"just clean\", \"clean podman\"\n- \"disk full\", \"out of space\"\n- \"reset environment\", \"fresh start\"\n- \"nuclear cleanup\", \"destroy everything\""
              },
              {
                "name": "test",
                "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n",
                "path": "bazzite-ai-dev/skills/test/SKILL.md",
                "frontmatter": {
                  "name": "test",
                  "description": "Overlay testing session management for bazzite-ai development. Enables live\nediting of justfiles via symlinks to /usr on immutable OS (OSTree) or traditional\nLinux systems. Use when users need to test ujust changes, enable overlay mode,\ntroubleshoot testing sessions, or run VM/install tests.\n"
                },
                "content": "# Test - Overlay Testing Management\n\n## Overview\n\nThe `test` command manages overlay testing sessions for bazzite-ai development. It creates symlinks from the repository to `/usr/share/bazzite-ai/just/`, allowing live editing of justfiles without rebuilding the OS image.\n\n**Key Concept:** On immutable OSTree systems (Bazzite-AI, Silverblue), `/usr` is read-only. Overlay mode temporarily unlocks it. On traditional systems (Fedora, CentOS), symlinks provide the same live-editing capability.\n\n**Command Prefix:**\n- `just test` - Development mode (from repository root, any Linux system)\n- `ujust test` - Installed mode (on bazzite-ai system with test.just installed)\n\nThe Quick Reference shows `ujust` commands (installed mode). The Common Workflows section shows `just` commands (development mode from repo root).\n\n## Quick Reference\n\n| Action | Command | Description |\n|--------|---------|-------------|\n| Enable overlay | `ujust test overlay enable` | Bootstrap overlay testing session |\n| Check status | `ujust test overlay check` | Show current overlay/symlink status |\n| Refresh | `ujust test overlay refresh` | Regenerate 60-custom.just after changes |\n| VM testing | `ujust test vm` | VM testing submenu |\n| Install testing | `ujust test install` | Test install commands |\n| Install all | `ujust test install all` | Test all install commands |\n| System info | `ujust test info` | Show detailed system info |\n| Help | `ujust test help` | Show usage help |\n\n## Parameters\n\n### ACTION Parameter\n\n```bash\nujust test ACTION=\"\" SUBACTION=\"\" ARGS...\n\n```\n\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `ACTION` | `overlay`, `vm`, `install`, `info`, `help` | Primary action |\n| `SUBACTION` | `enable`, `check`, `refresh` (for overlay) | Subaction |\n| `ARGS` | varies | Additional arguments for vm/install |\n\n### Rule of Intent\n\nWhen `ACTION` is provided, the command runs non-interactively. Without it, an interactive menu appears.\n\n## Overlay Subcommands\n\n### Enable Overlay\n\n```bash\nujust test overlay enable\n\n```\n\n1. Activates overlay mode (OSTree) or creates symlinks (traditional)\n2. Detects repository location automatically\n3. Sets up symlinks to `/usr/share/bazzite-ai/just/`\n4. Generates `60-custom.just` import file\n5. Requires sudo (handles internally)\n\n### Check Status\n\n```bash\nujust test overlay check\n\n```\n\nShows current status:\n\n- **Immutable OS**: Whether overlay mode is active\n\n- **Traditional OS**: Whether symlinks are configured\n\n- Target repository path\n\n### Refresh Overlay\n\n```bash\nujust test overlay refresh\n\n```\n\nUse after:\n\n- Adding new `.just` files\n\n- Removing `.just` files\n\n- Modifying the generator script\n\nRegenerates `60-custom.just` without full restart.\n\n## VM Testing\n\n```bash\nujust test vm              # Interactive VM test menu\nujust test vm list         # List available VM tests\nujust test vm <name>       # Run specific VM test\n\n```\n\nDelegates to the VM testing harness for testing in virtual machines.\n\n## Install Testing\n\n```bash\nujust test install         # Interactive install test menu\nujust test install all     # Test all install commands\nujust test install <name>  # Test specific install command\n\n```\n\nTests install commands for validation.\n\n## Common Workflows\n\n### Initial Development Setup\n\n```bash\n# 1. Clone repository\ngit clone <repo-url> && cd bazzite-ai\n\n# 2. Enable overlay testing (one-time)\njust test overlay enable\n\n# 3. Make changes to justfiles\nvim just/bazzite-ai/my-feature.just\n\n# 4. Test immediately with ujust\nujust my-feature\n\n# 5. If adding new files, refresh\njust test overlay refresh\n\n```\n\n### After Reboot (Immutable OS Only)\n\n```bash\n# Overlay resets on reboot - re-enable\njust test overlay enable\n\n# Your git commits persist, overlay changes don't\n\n```\n\n### Testing a New Command\n\n```bash\n# 1. Create/edit the justfile\nvim just/bazzite-ai/new-command.just\n\n# 2. Refresh to pick up new file\njust test overlay refresh\n\n# 3. Test the command\nujust new-command\n\n```\n\n## OS Type Detection\n\n| OS Type | Detection | Overlay Method |\n|---------|-----------|----------------|\n| Immutable (OSTree) | `/run/ostree-booted` exists | `rpm-ostree` overlay |\n| Traditional | No OSTree marker | Symlinks only |\n\n## Troubleshooting\n\n### Overlay Not Active After Enable\n\n**Symptom:** `ujust test overlay check` shows \"Normal immutable mode\"\n\n**Cause:** Overlay activation failed or needs reboot\n\n**Fix:**\n\n```bash\n# Check if rpm-ostree unlock succeeded\nsudo rpm-ostree status | grep -i unlock\n\n# If not, try manual unlock\nsudo rpm-ostree usroverlay\n\n```\n\n### Symlinks Not Working\n\n**Symptom:** Changes to justfiles not reflected in `ujust` output\n\n**Cause:** Symlinks not properly created or 60-custom.just not regenerated\n\n**Fix:**\n\n```bash\n# Check symlink status\nls -la /usr/share/bazzite-ai/just/\n\n# Re-enable overlay\njust test overlay enable\n\n# Refresh imports\njust test overlay refresh\n\n```\n\n### Command Not Found After Adding File\n\n**Symptom:** New recipe not available in `ujust --list`\n\n**Cause:** 60-custom.just needs regeneration\n\n**Fix:**\n\n```bash\njust test overlay refresh\n\n```\n\n### Permission Denied\n\n**Symptom:** `sudo: a terminal is required`\n\n**Cause:** Running in non-interactive mode without passwordless sudo\n\n**Fix:**\n\n```bash\n# Enable passwordless sudo first\nujust config passwordless-sudo enable\n\n# Then retry\njust test overlay enable\n\n```\n\n## Cross-References\n\n- **Related Skills:** `install` (for testing install commands), `vm` (for VM testing)\n\n- **Configuration:** `ujust config passwordless-sudo enable` for sudo access\n\n- **Documentation:** [Overlay Testing Architecture](./references/overlay-architecture.md)\n\n## When to Use This Skill\n\nUse when the user asks about:\n\n- \"enable overlay\", \"start testing session\", \"development mode\"\n\n- \"test my changes\", \"live reload justfiles\"\n\n- \"overlay not working\", \"symlinks not configured\"\n\n- \"refresh overlay\", \"pick up new files\"\n\n- \"VM testing\", \"test in VM\"\n\n- \"test install commands\""
              }
            ]
          },
          {
            "name": "bazzite-ai-jupyter",
            "description": "ML/AI development workflows for JupyterLab - LangChain, RAG, fine-tuning, and model optimization",
            "source": "./bazzite-ai-jupyter",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "atrawog"
            },
            "install_commands": [
              "/plugin marketplace add atrawog/bazzite-ai-plugins",
              "/plugin install bazzite-ai-jupyter@bazzite-ai-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T13:36:51Z",
              "created_at": "2025-12-26T20:21:42Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "dpo",
                "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n",
                "path": "bazzite-ai-jupyter/skills/dpo/SKILL.md",
                "frontmatter": {
                  "name": "dpo",
                  "description": "Direct Preference Optimization for learning from preference pairs. Covers DPOTrainer,\npreference dataset preparation, implicit reward modeling, and beta tuning for\nstable preference learning without explicit reward models. Includes thinking quality patterns.\n"
                },
                "content": "# Direct Preference Optimization (DPO)\n\n## Overview\n\nDPO learns from preference pairs (chosen vs rejected responses) without training an explicit reward model. It directly optimizes the policy using the Bradley-Terry preference model, making it simpler than RLHF while achieving comparable results. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `DPOTrainer` | Trainer for preference optimization |\n| `DPOConfig` | Training hyperparameters |\n| `beta` | Temperature for implicit reward (0.1 typical) |\n| `learning_rate` | 5e-6 (most conservative of RL methods) |\n| `ref_model` | Reference model for KL constraint |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## DPO Concepts\n\n### How DPO Works\n\n1. Given prompt + chosen response + rejected response\n2. Compute log-probabilities under policy and reference\n3. Optimize policy to increase P(chosen) / P(rejected) ratio\n4. Beta controls how strongly to enforce preference\n\n### Key Differences from RLHF\n\n| Aspect | DPO | RLHF |\n|--------|-----|------|\n| Reward Model | Implicit | Explicit |\n| Training | Single stage | Multi-stage |\n| Complexity | Simpler | More complex |\n| Compute | Lower | Higher |\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is when a function calls itself with a simpler version of the problem, including a base case to stop.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### From Comparison Data\n\n```python\ndef format_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"question\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"better_response\"],\n        \"rejected\": sample[\"worse_response\"],\n    }\n\ndataset = raw_dataset.map(format_preferences)\n```\n\n### Thinking Quality Preference Pairs\n\nFor thinking models, create preference pairs based on reasoning quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller, similar pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\nWhat's needed for it to work? A base case to stop the recursion.\n</think>\n\nRecursion is a programming technique where a function calls itself to solve a problem by breaking it into smaller, similar subproblems. For example, calculating factorial: n! = n * (n-1)!. Every recursive solution needs a base case to prevent infinite loops.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\nI can verify: 42 - 15 = 27. Correct!\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n    {\n        \"prompt\": \"Explain the difference between TCP and UDP.\",\n        \"chosen\": \"\"\"<think>\nWhat are TCP and UDP? They're network transport protocols.\nWhat's the key difference? TCP is connection-oriented, UDP is connectionless.\nWhat does that mean practically?\n- TCP: Reliable, ordered delivery with acknowledgments\n- UDP: Fast, no guarantees, better for streaming\nWhen would you use each?\n- TCP: File transfer, web browsing, email\n- UDP: Video streaming, gaming, DNS\n</think>\n\nTCP is a connection-oriented protocol that guarantees reliable, ordered delivery through acknowledgments and retransmission. UDP is connectionless, offering faster but unreliable delivery without guarantees. Use TCP for reliability (file transfers, web), UDP for speed (streaming, gaming).\"\"\",\n        \"rejected\": \"TCP is reliable, UDP is not.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n\ndef format_thinking_preferences(sample):\n    return {\n        \"prompt\": tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        \"chosen\": sample[\"chosen\"],\n        \"rejected\": sample[\"rejected\"],\n    }\n\ndataset = dataset.map(format_thinking_preferences)\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for DPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## DPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import DPOConfig\n\ndpo_config = DPOConfig(\n    output_dir=\"./dpo_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=5e-6,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    beta=0.1,\n    max_length=512,\n    max_prompt_length=256,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.1-0.5 | Implicit reward temperature |\n| `learning_rate` | 1e-6 to 5e-6 | Lower than SFT |\n| `max_length` | 512-1024 | Max combined length |\n| `max_prompt_length` | 256-512 | Max prompt length |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import DPOTrainer\n\ntrainer = DPOTrainer(\n    model=model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n### With Reference Model\n\n```python\n# For stronger KL constraint\nref_model, _ = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=dpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n```\n\n## Beta Selection Guide\n\n| Beta | Use Case |\n|------|----------|\n| 0.01 | Weak preference signal |\n| 0.1 | Standard (recommended) |\n| 0.3 | Strong preference enforcement |\n| 0.5+ | Very strong (may overfit) |\n\n## Troubleshooting\n\n### Chosen/Rejected Scores Similar\n\n**Symptom:** Model doesn't distinguish preferences\n\n**Fix:**\n- Increase `beta` for stronger signal\n- Train longer\n- Check data quality (clear preference differences)\n\n### Overfitting to Preferences\n\n**Symptom:** Model only outputs chosen-style responses\n\n**Fix:**\n- Lower `beta`\n- Use reference model\n- Add regularization\n\n### Low Accuracy\n\n**Symptom:** DPO accuracy metric stays low\n\n**Fix:**\n- Ensure chosen is genuinely better than rejected\n- Increase training steps\n- Check prompt formatting\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Set `ref_model=None` (uses implicit reference)\n- Reduce `max_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nDPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- You have preference data (chosen vs rejected)\n- Simpler pipeline than RLHF desired\n- No reward model available\n- Post-SFT alignment\n- Human preference learning\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before DPO\n- `bazzite-ai-jupyter:grpo` - Alternative with explicit rewards\n- `bazzite-ai-jupyter:rloo` - Alternative RL with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models (alternative to DPO)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "evaluation",
                "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n",
                "path": "bazzite-ai-jupyter/skills/evaluation/SKILL.md",
                "frontmatter": {
                  "name": "evaluation",
                  "description": "LLM evaluation and prompt optimization with Evidently.ai. Covers text\ndescriptors, dataset metrics, LLM-as-a-Judge patterns, and automated\nprompt optimization for classification and generation tasks.\n"
                },
                "content": "# LLM Evaluation with Evidently.ai\n\n## Overview\n\nEvidently.ai provides tools for evaluating LLM outputs using descriptors (row-level metrics) and reports. It supports automated prompt optimization and LLM-as-a-Judge patterns for quality assessment.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `Dataset` | Wrapper for evaluation data |\n| `Descriptor` | Row-level score or label |\n| `Report` | Aggregate metrics |\n| `TextEvals` | Text quality metrics |\n| `LLMJudge` | LLM-based evaluation |\n| `PromptOptimizer` | Automated prompt tuning |\n\n## Basic Setup\n\n```python\nimport pandas as pd\nfrom evidently import Dataset, DataDefinition\nfrom evidently.descriptors import TextLength, Sentiment, WordCount\n\n# Sample data\ndata = [\n    {\"question\": \"What is Python?\", \"answer\": \"Python is a programming language.\"},\n    {\"question\": \"Explain AI.\", \"answer\": \"AI is artificial intelligence.\"},\n]\n\ndf = pd.DataFrame(data)\n\n# Define data structure\ndefinition = DataDefinition(text_columns=[\"question\", \"answer\"])\n\n# Create Evidently Dataset\neval_dataset = Dataset.from_pandas(df, data_definition=definition)\n```\n\n## Text Descriptors\n\n### Basic Metrics\n\n```python\nfrom evidently.descriptors import TextLength, WordCount, Sentiment\n\n# Add descriptors\neval_dataset.add_descriptors(descriptors=[\n    TextLength(column=\"answer\"),\n    WordCount(column=\"answer\"),\n    Sentiment(column=\"answer\")\n])\n\n# View results\neval_dataset.as_dataframe()\n```\n\n### Available Descriptors\n\n| Descriptor | Description |\n|------------|-------------|\n| `TextLength` | Character count |\n| `WordCount` | Word count |\n| `Sentiment` | Sentiment score (-1 to 1) |\n| `RegexMatch` | Regex pattern matching |\n| `Contains` | Substring presence |\n| `IsValidJSON` | JSON validity check |\n| `IsValidPython` | Python syntax check |\n\n## LLM-as-a-Judge\n\n### Binary Classification\n\n```python\nimport os\nfrom evidently.descriptors import LLMJudge\nfrom evidently.llm import OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Configure Ollama as provider\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Create judge\njudge = LLMJudge(\n    provider=provider,\n    template=\"Is this answer helpful? Answer YES or NO.\\n\\nQuestion: {question}\\nAnswer: {answer}\",\n    include_reasoning=True\n)\n\neval_dataset.add_descriptors(descriptors=[judge])\n```\n\n### Multi-Class Classification\n\n```python\nfrom evidently.descriptors import LLMJudge\n\njudge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Classify this query into one category: BOOKING, CANCELLATION, GENERAL.\n\nQuery: {query}\n\nCategory:\"\"\",\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"],\n    include_reasoning=True\n)\n```\n\n### Quality Scoring\n\n```python\nfrom evidently.descriptors import LLMJudge\n\nquality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Rate this code review on a scale of 1-5.\n\nCode Review: {review}\n\nScore (1-5):\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Prompt Optimization\n\n### Setup Optimizer\n\n```python\nfrom evidently.llm import PromptOptimizer, OpenAIProvider\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprovider = OpenAIProvider(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\noptimizer = PromptOptimizer(\n    provider=provider,\n    max_iterations=10\n)\n```\n\n### Binary Classification Optimization\n\n```python\n# Initial prompt template\ninitial_prompt = \"\"\"Classify if this code review is good or bad.\n\nReview: {review}\n\nAnswer (GOOD or BAD):\"\"\"\n\n# Define judge for evaluation\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"GOOD\", \"BAD\"]\n)\n\n# Run optimization\nbest_prompt = optimizer.optimize(\n    dataset=eval_dataset,\n    initial_template=initial_prompt,\n    target_column=\"label\",  # Ground truth column\n    judge=judge\n)\n\nprint(\"Best prompt found:\")\nprint(best_prompt)\n```\n\n### Multi-Class Optimization\n\n```python\ninitial_prompt = \"\"\"Classify this query.\n\nQuery: {query}\n\nCategory (BOOKING/CANCELLATION/GENERAL):\"\"\"\n\njudge = LLMJudge(\n    provider=provider,\n    template=initial_prompt,\n    options=[\"BOOKING\", \"CANCELLATION\", \"GENERAL\"]\n)\n\nbest_prompt = optimizer.optimize(\n    dataset=dataset,\n    initial_template=initial_prompt,\n    target_column=\"category\",\n    judge=judge\n)\n```\n\n## Reports\n\n### Generate Report\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import TextDescriptorsDriftMetric\n\nreport = Report(metrics=[\n    TextDescriptorsDriftMetric(column=\"answer\")\n])\n\nreport.run(reference_data=reference_dataset, current_data=current_dataset)\nreport.show()\n```\n\n### Save Report\n\n```python\nreport.save_html(\"evaluation_report.html\")\nreport.save_json(\"evaluation_report.json\")\n```\n\n## Common Patterns\n\n### Evaluate RAG Quality\n\n```python\nfrom evidently.descriptors import LLMJudge, TextLength, Contains\n\n# Relevance judge\nrelevance_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer relevant to the question?\n\nQuestion: {question}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\n# Factuality judge\nfactuality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Is this answer factually accurate based on the context?\n\nContext: {context}\nAnswer: {answer}\n\nAnswer YES or NO:\"\"\"\n)\n\neval_dataset.add_descriptors([\n    relevance_judge,\n    factuality_judge,\n    TextLength(column=\"answer\")\n])\n```\n\n### Compare Models\n\n```python\n# Evaluate model A\nmodel_a_dataset = run_inference(model_a, test_data)\nmodel_a_dataset.add_descriptors([quality_judge])\n\n# Evaluate model B\nmodel_b_dataset = run_inference(model_b, test_data)\nmodel_b_dataset.add_descriptors([quality_judge])\n\n# Compare\nprint(\"Model A average score:\", model_a_dataset.as_dataframe()[\"quality\"].mean())\nprint(\"Model B average score:\", model_b_dataset.as_dataframe()[\"quality\"].mean())\n```\n\n## Troubleshooting\n\n### Slow Evaluation\n\n**Symptom:** Evaluation takes too long\n\n**Fix:**\n\n- Reduce dataset size for initial testing\n- Use smaller/faster judge model\n- Batch requests where possible\n\n### Inconsistent Judgments\n\n**Symptom:** LLM judge gives inconsistent scores\n\n**Fix:**\n\n- Lower temperature (0.0-0.3)\n- Make prompt more specific\n- Add examples to prompt\n- Use structured output options\n\n### Optimization Not Improving\n\n**Symptom:** Prompt optimization stuck\n\n**Fix:**\n\n- Increase `max_iterations`\n- Try different initial prompts\n- Check ground truth labels are correct\n- Use more training examples\n\n## When to Use This Skill\n\nUse when:\n\n- Measuring LLM output quality\n- Comparing different prompts\n- Automating prompt engineering\n- Building evaluation pipelines\n- Monitoring LLM performance over time\n\n## Evaluating Thinking Models\n\nFor thinking models (Qwen3-Thinking), evaluate both thinking quality and response quality:\n\n```python\nthinking_quality_judge = LLMJudge(\n    provider=provider,\n    template=\"\"\"Evaluate the quality of reasoning in this response.\n\nQuestion: {question}\nResponse: {response}\n\nScore the THINKING quality (1-5):\n1 = No reasoning shown\n2 = Minimal reasoning\n3 = Some step-by-step thinking\n4 = Good reasoning with self-questioning\n5 = Excellent thorough reasoning\n\nScore:\"\"\",\n    score_range=(1, 5)\n)\n```\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain for LLM calls\n- `bazzite-ai-jupyter:rag` - RAG evaluation patterns\n- `bazzite-ai-jupyter:sft` - Training thinking models\n- `bazzite-ai-jupyter:inference` - Thinking model parsing\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility"
              },
              {
                "name": "finetuning",
                "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n",
                "path": "bazzite-ai-jupyter/skills/finetuning/SKILL.md",
                "frontmatter": {
                  "name": "finetuning",
                  "description": "Model fine-tuning with PyTorch and HuggingFace Trainer. Covers dataset\npreparation, tokenization, training loops, TrainingArguments, SFTTrainer\nfor instruction tuning, evaluation, and checkpoint management. Includes Unsloth recommendations.\n"
                },
                "content": "# Model Fine-Tuning\n\n## Overview\n\nFine-tuning adapts a pre-trained LLM to specific tasks by training on task-specific data. This skill covers both manual PyTorch training and HuggingFace's high-level Trainer API.\n\n**Recommended**: For 2x faster training with less memory, use **Unsloth** (see `bazzite-ai-jupyter:sft`).\n\n## Quick Reference\n\n| Approach | Use Case | Speed |\n|----------|----------|-------|\n| **Unsloth + SFTTrainer** | **Recommended default** | **2x faster** |\n| PyTorch Manual | Full control, custom training | Baseline |\n| HuggingFace Trainer | Standard training, less code | Fast |\n| SFTTrainer | Instruction/chat fine-tuning | Fast |\n\n## Method Comparison\n\n| Method | Learning Rate | Use Case |\n|--------|---------------|----------|\n| SFT | 2e-4 | Instruction tuning (first step) |\n| GRPO | 1e-5 | RL with rewards |\n| DPO | 5e-6 | Preference learning |\n| RLOO | 1e-5 | RL with lower variance |\n| Reward | 1e-5 | Reward model training |\n\n## Unsloth Quickstart (Recommended)\n\n```python\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Apply LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train\ntrainer = SFTTrainer(\n    model=model, tokenizer=tokenizer, train_dataset=dataset,\n    args=SFTConfig(\n        output_dir=\"./output\",\n        max_steps=100,\n        learning_rate=2e-4,\n        bf16=is_bf16_supported(),\n        optim=\"adamw_8bit\",\n    ),\n)\ntrainer.train()\n```\n\nSee `bazzite-ai-jupyter:sft` for complete Unsloth patterns.\n\n## Dataset Preparation\n\n### Load from HuggingFace Hub\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"test\"]\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n```\n\n### Data Format\n\n```python\n# Example conversation format\nexample = train_data[0]\nprint(example[\"text\"])\n\n# Output:\n# ### Human: What is Python?\n# ### Assistant: Python is a programming language...\n```\n\n### Create Prompt Template\n\n```python\ndef build_prompt(instruction, response=None):\n    prompt = f\"### Human: {instruction}\\n### Assistant:\"\n    if response:\n        prompt += f\" {response}\"\n    return prompt\n\n# For training\ntrain_prompt = build_prompt(\"What is AI?\", \"AI is artificial intelligence.\")\n\n# For inference\ninference_prompt = build_prompt(\"What is AI?\")\n```\n\n## Tokenization\n\n### Setup Tokenizer\n\n```python\nfrom transformers import AutoTokenizer\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Ensure pad token exists\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Tokenize Dataset\n\n```python\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\ntokenized_train = train_data.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_data.column_names\n)\n\ntokenized_train.set_format(\"torch\")\n```\n\n## PyTorch Training (Manual)\n\n### Setup Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n```\n\n### Training Configuration\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass TrainConfig:\n    batch_size: int = 4\n    learning_rate: float = 2e-5\n    num_epochs: int = 3\n    max_length: int = 512\n    warmup_steps: int = 100\n    weight_decay: float = 0.01\n    output_dir: str = \"./checkpoints\"\n\ncfg = TrainConfig()\n```\n\n### DataLoader\n\n```python\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    tokenized_train,\n    batch_size=cfg.batch_size,\n    shuffle=True\n)\n```\n\n### Optimizer and Scheduler\n\n```python\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.learning_rate,\n    weight_decay=cfg.weight_decay\n)\n\ntotal_steps = len(train_loader) * cfg.num_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=cfg.warmup_steps,\n    num_training_steps=total_steps\n)\n```\n\n### Training Loop\n\n```python\nfrom tqdm.auto import tqdm\n\nmodel.train()\ndevice = next(model.parameters()).device\n\nfor epoch in range(cfg.num_epochs):\n    total_loss = 0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in progress:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = input_ids.clone()\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        progress.set_postfix({\"loss\": loss.item()})\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n\n    # Save checkpoint\n    model.save_pretrained(f\"{cfg.output_dir}/epoch_{epoch+1}\")\n```\n\n## HuggingFace Trainer\n\n### TrainingArguments\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True,\n    fp16=True,  # Mixed precision\n)\n```\n\n### Create Trainer\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n)\n```\n\n### Train and Evaluate\n\n```python\n# Train\ntrain_result = trainer.train()\n\n# Save\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\n\n# Evaluate\nmetrics = trainer.evaluate()\nprint(metrics)\n```\n\n## SFTTrainer (Instruction Tuning)\n\n### Setup\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-5,\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    packing=False,  # Don't pack multiple samples\n)\n```\n\n### Train with SFTTrainer\n\n```python\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_data,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",  # Column with training text\n)\n\ntrainer.train()\ntrainer.save_model(\"./sft_model\")\n```\n\n## Evaluation\n\n### Evaluation Function\n\n```python\ndef evaluate(model, dataloader):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = input_ids.clone()\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            total_loss += outputs.loss.item()\n\n    return total_loss / len(dataloader)\n```\n\n### Perplexity\n\n```python\nimport math\n\neval_loss = evaluate(model, val_loader)\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity:.2f}\")\n```\n\n## Inference with Fine-Tuned Model\n\n```python\ndef generate_response(model, tokenizer, prompt, max_new_tokens=128):\n    model.eval()\n    device = next(model.parameters()).device\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test\nprompt = build_prompt(\"What is machine learning?\")\nresponse = generate_response(model, tokenizer, prompt)\nprint(response)\n```\n\n## Checkpointing\n\n### Save Checkpoint\n\n```python\n# Save model and tokenizer\nmodel.save_pretrained(\"./checkpoint\")\ntokenizer.save_pretrained(\"./checkpoint\")\n```\n\n### Load Checkpoint\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"./checkpoint\")\n```\n\n### Resume Training\n\n```python\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n)\n\ntrainer.train(resume_from_checkpoint=\"./checkpoint\")\n```\n\n## Hyperparameters Guide\n\n| Parameter | Typical Values | Notes |\n|-----------|----------------|-------|\n| `learning_rate` | 1e-5 to 5e-5 | Lower for larger models |\n| `batch_size` | 4, 8, 16 | Limited by GPU memory |\n| `epochs` | 1-5 | More for smaller datasets |\n| `warmup_steps` | 5-10% of total | Stabilizes early training |\n| `weight_decay` | 0.01-0.1 | Regularization |\n| `max_length` | 512, 1024, 2048 | Context window |\n\n## When to Use This Skill\n\nUse when:\n\n- Adapting LLM to specific domain/task\n- Improving model performance on your data\n- Creating instruction-following models\n- Need full control over training process\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Unsloth-optimized SFT (recommended)\n- `bazzite-ai-jupyter:grpo` - RL with reward functions\n- `bazzite-ai-jupyter:dpo` - Preference learning\n- `bazzite-ai-jupyter:rloo` - RL with lower variance\n- `bazzite-ai-jupyter:quantization` - Memory-efficient training\n- `bazzite-ai-jupyter:peft` - Parameter-efficient fine-tuning\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Architecture understanding"
              },
              {
                "name": "grpo",
                "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n",
                "path": "bazzite-ai-jupyter/skills/grpo/SKILL.md",
                "frontmatter": {
                  "name": "grpo",
                  "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.\nCovers GRPOTrainer, reward function design, policy optimization, and KL divergence\nconstraints for stable RLHF training. Includes thinking-aware reward patterns.\n"
                },
                "content": "# Group Relative Policy Optimization (GRPO)\n\n## Overview\n\nGRPO is a reinforcement learning method for LLM alignment. It generates multiple completions per prompt, scores them with a reward function, and optimizes the policy to favor higher-reward responses using relative policy gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `GRPOTrainer` | RL trainer for policy optimization |\n| `GRPOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `beta` | KL penalty coefficient (0.1 typical) |\n| `num_generations` | Completions per prompt (2-4) |\n| `learning_rate` | 1e-5 (10x lower than SFT) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import GRPOConfig, GRPOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Setting `ACCELERATE_MIXED_PRECISION` after imports may cause training issues.\n\n## GRPO Concepts\n\n### How GRPO Works\n\n1. Generate multiple completions for each prompt\n2. Score completions with reward function(s)\n3. Compute relative advantages within each group\n4. Update policy to favor higher-reward completions\n5. Apply KL penalty to prevent divergence from reference\n\n### Key Differences from PPO\n\n| Aspect | GRPO | PPO |\n|--------|------|-----|\n| Baseline | Group relative | Value function |\n| Critic | Not needed | Required |\n| Memory | Lower | Higher |\n| Stability | Good | Can be unstable |\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for GRPO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Dataset Format\n\n```python\n# GRPO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"What is recursion?\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response length.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        if length < 5:\n            rewards.append(-1.0)\n        elif length < 50:\n            rewards.append(1.0)\n        else:\n            rewards.append(0.5)\n    return rewards\n```\n\n### LLM-as-Judge Reward\n\n```python\ndef llm_judge_reward(completions, prompts):\n    \"\"\"Use another LLM to score responses.\"\"\"\n    rewards = []\n    for prompt, completion in zip(prompts, completions):\n        score = judge_model.evaluate(prompt, completion)\n        rewards.append(score)\n    return rewards\n```\n\n### Rule-Based Reward\n\n```python\ndef format_reward(completions, prompts=None):\n    \"\"\"Reward proper formatting.\"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n        if completion.endswith(\".\"):\n            score += 0.5\n        if not completion.startswith(\" \"):\n            score += 0.5\n        rewards.append(score)\n    return rewards\n```\n\n### Composite Rewards\n\n```python\ndef combined_reward(completions, prompts):\n    \"\"\"Combine multiple reward signals.\"\"\"\n    length_scores = length_reward(completions)\n    format_scores = format_reward(completions)\n    return [0.5 * l + 0.5 * f for l, f in zip(length_scores, format_scores)]\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n### Multi-Objective Thinking Reward (Token-Based)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef comprehensive_thinking_reward(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Evaluate multiple aspects of thinking quality using token IDs.\n\n    Scoring breakdown:\n    - Has </think> token: +0.3\n    - Thinking depth (20+ tokens): +0.3\n    - Structured sentences: +0.2\n    - Self-questioning: +0.1\n    - Step-by-step reasoning: +0.1\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        score = 0.0\n\n        # Token-based boundary detection\n        if THINK_END_TOKEN_ID in comp_ids:\n            score += 0.3  # Has proper </think> token\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count\n\n            # Extract thinking content for text analysis\n            thinking = completion.split('</think>')[0]\n\n            # Depth (token count from IDs)\n            if thinking_length >= 20:\n                score += 0.3\n            elif thinking_length >= 10:\n                score += 0.2\n\n            # Structure (sentences in text)\n            sentences = thinking.count('.') + thinking.count('!')\n            if sentences >= 2:\n                score += 0.2\n\n            # Self-questioning\n            if '?' in thinking:\n                score += 0.1\n\n            # Step-by-step reasoning\n            if any(w in thinking.lower() for w in ['first', 'then', 'next', 'finally']):\n                score += 0.1\n        else:\n            score = -0.5  # Penalize missing </think> token\n\n        rewards.append(score)\n\n    return rewards\n```\n\n## GRPOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import GRPOConfig\n\ngrpo_config = GRPOConfig(\n    output_dir=\"./grpo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_completion_length=128,\n    num_generations=4,\n    beta=0.1,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `beta` | 0.01-0.1 | KL penalty strength |\n| `num_generations` | 2-8 | Completions per prompt |\n| `max_completion_length` | 64-256 | Generation length |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n\n## Training\n\n### Basic Training Loop\n\n```python\nfrom trl import GRPOTrainer\n\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=length_reward,\n)\n\ntrainer.train()\n```\n\n### Multiple Reward Functions\n\n```python\ntrainer = GRPOTrainer(\n    model=model,\n    args=grpo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_funcs=[length_reward, format_reward],\n    reward_weights=[0.5, 0.5],\n)\n```\n\n## Troubleshooting\n\n### Reward Hacking\n\n**Symptom:** Model exploits reward function (e.g., always outputs same length)\n\n**Fix:**\n- Add diversity penalties\n- Use multiple reward signals\n- Cap maximum reward\n\n### KL Divergence Too High\n\n**Symptom:** Policy diverges too far from reference\n\n**Fix:**\n- Increase `beta` (stronger KL penalty)\n- Reduce `learning_rate`\n- Fewer training steps\n\n### Training Instability\n\n**Symptom:** Loss spikes or NaN\n\n**Fix:**\n- Lower `learning_rate` to 5e-6\n- Reduce `num_generations` to 2\n- Check reward scale (should be roughly -1 to 1)\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2\n- Use gradient checkpointing\n- Reduce `max_completion_length`\n\n## Kernel Shutdown (Jupyter)\n\nGRPO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Aligning models with human preferences\n- Optimizing for specific behaviors\n- Post-SFT refinement\n- Building reward-driven systems\n- Simpler alternative to PPO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before GRPO\n- `bazzite-ai-jupyter:dpo` - Simpler preference learning (no reward model)\n- `bazzite-ai-jupyter:rloo` - Alternative RL method with lower variance\n- `bazzite-ai-jupyter:reward` - Training reward models for GRPO\n- `bazzite-ai-jupyter:peft` - LoRA for efficient RL\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-ollama:api` - Reward model inference"
              },
              {
                "name": "inference",
                "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n",
                "path": "bazzite-ai-jupyter/skills/inference/SKILL.md",
                "frontmatter": {
                  "name": "inference",
                  "description": "Fast inference with Unsloth and vLLM backend. Covers model loading, fast_generate(),\nthinking model output parsing, and memory management for efficient inference.\n"
                },
                "content": "# Fast Inference\n\n## Overview\n\nUnsloth provides optimized inference through the vLLM backend, enabling 2x faster generation compared to standard HuggingFace inference. This skill covers fast inference setup, thinking model output parsing, and memory management.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `fast_inference=True` | Enable vLLM backend for 2x speedup |\n| `model.fast_generate()` | vLLM-accelerated generation |\n| `SamplingParams` | Control generation (temperature, top_p, etc.) |\n| `FastLanguageModel.for_inference()` | Merge LoRA adapters for inference |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\nimport torch\nimport vllm\nfrom vllm import SamplingParams\n```\n\n## Environment Verification\n\nBefore inference, verify your environment is correctly configured:\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nimport vllm\n\n# Check versions\nprint(f\"unsloth: {unsloth.__version__}\")\nprint(f\"vLLM: {vllm.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n```\n\n## Standard Inference (No vLLM)\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# Prepare for inference (merges LoRA adapters if present)\nFastLanguageModel.for_inference(model)\n```\n\n### Generate Response\n\n```python\nmessages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode only new tokens\ninput_length = inputs[\"input_ids\"].shape[1]\nresponse = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\nprint(response)\n```\n\n## Fast Inference (vLLM Backend)\n\n### Load Model with Fast Inference\n\n```python\nfrom unsloth import FastLanguageModel\nfrom vllm import SamplingParams\n\nMODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    MODEL_NAME,\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,  # Enable vLLM backend\n)\n```\n\n### Fast Generate\n\n```python\nFastLanguageModel.for_inference(model)\n\nmessages = [{\"role\": \"user\", \"content\": \"What is 15 + 27? Show your thinking.\"}]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nsampling_params = SamplingParams(\n    temperature=0.6,      # Recommended for thinking models\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,      # Increased for thinking + response\n)\n\n# Use fast_generate instead of generate\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\n\n# Extract output\nraw_output = outputs[0].outputs[0].text\noutput_token_ids = outputs[0].outputs[0].token_ids\nprint(raw_output)\n```\n\n### Sampling Parameters\n\n```python\nfrom vllm import SamplingParams\n\n# Conservative (factual responses)\nconservative = SamplingParams(\n    temperature=0.3,\n    top_p=0.9,\n    max_tokens=512,\n)\n\n# Balanced (general use)\nbalanced = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=1024,\n)\n\n# Creative (diverse outputs)\ncreative = SamplingParams(\n    temperature=0.9,\n    top_p=0.95,\n    top_k=50,\n    max_tokens=2048,\n)\n\n# Thinking models (allow long reasoning)\nthinking = SamplingParams(\n    temperature=0.6,\n    top_p=0.95,\n    top_k=20,\n    max_tokens=2048,  # Extra space for <think> content\n)\n```\n\n## Thinking Model Output Parsing\n\nQwen3-Thinking models use `<think>...</think>` tags to separate reasoning from final responses. Use token-based parsing for accuracy.\n\n### Token-Based Parsing (Recommended)\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"\n    Parse thinking model output using token ID boundary.\n\n    With Thinking models + add_generation_prompt=True:\n    - Template adds <think> to prompt\n    - Model output starts with thinking content\n    - Model outputs </think> (token 151668) when done\n    - Final response follows </think>\n\n    Args:\n        token_ids: Output token IDs from generation\n        tokenizer: Model tokenizer\n\n    Returns:\n        tuple: (thinking_content, response_content)\n    \"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking_tokens = token_list[:end_idx]\n        response_tokens = token_list[end_idx + 1:]\n\n        thinking = tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n    else:\n        # No </think> found - model may still be thinking\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True).strip()\n        response = \"(Model did not complete thinking - increase max_tokens)\"\n\n    return thinking, response\n```\n\n### Usage Example\n\n```python\n# Generate with fast_inference\noutputs = model.fast_generate([prompt], sampling_params=sampling_params)\noutput_token_ids = outputs[0].outputs[0].token_ids\n\n# Parse thinking and response\nthinking, response = parse_thinking_response(output_token_ids, tokenizer)\n\nprint(\"=== THINKING ===\")\nprint(thinking)\nprint(\"\\n=== RESPONSE ===\")\nprint(response)\n```\n\n### Verification\n\n```python\n# Verify parsing worked correctly\nthink_tag_found = THINK_END_TOKEN_ID in list(output_token_ids)\nhas_thinking = bool(thinking) and \"did not complete\" not in response\nhas_response = bool(response) and \"did not complete\" not in response\n\nprint(f\"</think> token found: {'Yes' if think_tag_found else 'No'}\")\nprint(f\"Thinking extracted: {'Yes' if has_thinking else 'No'}\")\nprint(f\"Response extracted: {'Yes' if has_response else 'No'}\")\n\nif not think_tag_found:\n    print(\"Tip: Increase max_tokens in SamplingParams\")\n```\n\n## Batch Inference\n\n### Multiple Prompts\n\n```python\nprompts = [\n    \"What is recursion?\",\n    \"Explain machine learning in simple terms.\",\n    \"What is the difference between Python and JavaScript?\",\n]\n\n# Format all prompts\nformatted_prompts = [\n    tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": p}],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    for p in prompts\n]\n\n# Batch generate (vLLM handles parallelization)\nsampling_params = SamplingParams(temperature=0.6, max_tokens=512)\noutputs = model.fast_generate(formatted_prompts, sampling_params=sampling_params)\n\n# Process results\nfor i, output in enumerate(outputs):\n    print(f\"\\n=== Prompt {i+1} ===\")\n    print(f\"Q: {prompts[i]}\")\n    print(f\"A: {output.outputs[0].text}\")\n```\n\n## Memory Management\n\n### GPU Memory Monitoring\n\n```python\nimport subprocess\n\ndef measure_gpu_memory():\n    \"\"\"Measure current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\n# Usage\nprint(f\"GPU memory used: {measure_gpu_memory()} MB\")\n```\n\n### Memory Cleanup\n\n```python\nimport gc\nimport torch\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage after inference\ncleanup_memory()\nprint(f\"GPU memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Jupyter Kernel Shutdown (Critical for vLLM)\n\n**vLLM does NOT release GPU memory within a Jupyter session.** Kernel restart is required between model tests:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of notebooks that use `fast_inference=True`. Without kernel shutdown, loading a different model will fail with OOM.\n\n**Notebook pattern**: All finetuning notebooks end with a shutdown cell.\n\n## Model Loading Patterns\n\n### Pre-Quantized Models (Recommended)\n\n```python\n# Fast loading with pre-quantized models\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # Pre-quantized\n    max_seq_length=1024,\n    load_in_4bit=True,\n    fast_inference=True,\n)\n```\n\n### On-Demand Quantization\n\n```python\n# Quantize during loading (slower initial load)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize on load\n    fast_inference=True,\n)\n```\n\n### Post-Training Inference\n\n```python\n# After SFT/GRPO/DPO training\nFastLanguageModel.for_inference(model)  # Merge LoRA adapters\n\n# Then generate as normal\noutputs = model.generate(**inputs, max_new_tokens=512)\n```\n\n## Supported Models\n\n| Model | Path | Parameters | Use Case |\n|-------|------|------------|----------|\n| Qwen3-4B-Thinking | `unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit` | 4B | Reasoning, chain-of-thought |\n| Ministral-3B-Reasoning | `unsloth/Ministral-3-3B-Reasoning-2512` | 3B | Fast reasoning |\n| Qwen3-4B | `unsloth/Qwen3-4B-unsloth-bnb-4bit` | 4B | General instruction following |\n| Llama-3.2-3B | `unsloth/Llama-3.2-3B-Instruct-bnb-4bit` | 3B | General instruction following |\n\n## Troubleshooting\n\n### vLLM Not Available\n\n**Symptom:** `fast_inference=True` fails or falls back to standard inference\n\n**Fix:**\n```python\n# Check vLLM installation\nimport inspect\nsig = inspect.signature(FastLanguageModel.from_pretrained)\nif 'fast_inference' in sig.parameters:\n    print(\"fast_inference parameter available\")\nelse:\n    print(\"vLLM not available - using standard inference\")\n```\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory during inference\n\n**Fix:**\n- Use 4-bit quantization (`load_in_4bit=True`)\n- Reduce `max_seq_length`\n- Reduce `max_tokens` in SamplingParams\n- Use `cleanup_memory()` between batches\n\n### Incomplete Thinking\n\n**Symptom:** `</think>` token not found in output\n\n**Fix:**\n- Increase `max_tokens` in SamplingParams (try 2048+)\n- Check that model is a Thinking variant\n- Verify `add_generation_prompt=True` in chat template\n\n### GPU Memory Not Released\n\n**Symptom:** Memory stays high after inference\n\n**Fix:**\n- Call `cleanup_memory()`\n- Restart Jupyter kernel between model tests\n- Use `del model` then `cleanup_memory()`\n\n## When to Use This Skill\n\nUse when:\n- Running inference on fine-tuned models\n- Need fast batch inference\n- Working with thinking/reasoning models\n- Optimizing inference latency\n- Parsing chain-of-thought outputs\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Supervised fine-tuning (train before inference)\n- `bazzite-ai-jupyter:peft` - LoRA adapter loading\n- `bazzite-ai-jupyter:quantization` - Quantization options\n- `bazzite-ai-jupyter:transformers` - Transformer architecture background\n- `bazzite-ai-ollama:api` - Ollama deployment for production"
              },
              {
                "name": "langchain",
                "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n",
                "path": "bazzite-ai-jupyter/skills/langchain/SKILL.md",
                "frontmatter": {
                  "name": "langchain",
                  "description": "LangChain framework for LLM applications. Covers model wrappers (HuggingFace,\nOllama), prompt templates, few-shot learning, output parsing, and chaining\ntechniques for building sophisticated LLM workflows.\n"
                },
                "content": "# LangChain Framework\n\n## Overview\n\nLangChain is a framework for building LLM applications. It provides abstractions for prompts, models, chains, and output parsing that work with both local models (HuggingFace, Ollama) and cloud APIs (OpenAI, Anthropic).\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `ChatOpenAI` | Connect to Ollama (OpenAI-compatible) |\n| `HuggingFacePipeline` | Wrap local HuggingFace models |\n| `ChatHuggingFace` | Chat interface for HF models |\n| `PromptTemplate` | Single-string prompt formatting |\n| `ChatPromptTemplate` | Multi-message prompt formatting |\n| `PydanticOutputParser` | Structured output parsing |\n\n## Model Wrappers\n\n### Ollama via OpenAI-Compatible API\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",  # Required by library, ignored by Ollama\n    model=MODEL,\n    temperature=0.7,\n    max_tokens=150\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n### HuggingFace Local Model\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n\nHF_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n\n# 4-bit quantization for memory efficiency\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\n# Create pipeline\ntext_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=150,\n    return_full_text=False\n)\n\n# Wrap for LangChain\nllm = HuggingFacePipeline(pipeline=text_pipeline)\nchat_llm = ChatHuggingFace(llm=llm)\n```\n\n## LLM Methods\n\n### invoke() - Single Input\n\n```python\nresponse = llm.invoke(\"Tell me a fact about Mars.\")\nprint(response)\n```\n\n### batch() - Multiple Inputs\n\n```python\nprompts = [\"Tell me a joke\", \"Translate to German: Hello!\"]\nresults = llm.batch(prompts)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {result}\\n\")\n```\n\n### generate() - With Metadata\n\n```python\nresults = llm.generate([\"Where should I go for a Safari?\"])\n\nfor gen in results.generations:\n    print(gen[0].text)\n\n# Access token counts\nprint(results.llm_output)\n```\n\n### stream() - Token Streaming\n\n```python\nfor chunk in llm.stream(\"Tell me a story about a cat.\"):\n    print(chunk, end=\"\", flush=True)\n```\n\n## Prompt Templates\n\n### Basic PromptTemplate\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Explain {topic} in simple terms.\"\n)\n\nformatted = template.format(topic=\"quantum computing\")\nresponse = llm.invoke(formatted)\n```\n\n### ChatPromptTemplate\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful legal translator.\"),\n    (\"human\", \"Simplify this legal text: {legal_text}\")\n])\n\nmessages = chat_prompt.format_messages(legal_text=\"...\")\nresponse = chat_llm.invoke(messages)\n```\n\n## Few-Shot Learning\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define examples\nexamples = [\n    {\"input\": \"Legal term 1\", \"output\": \"Plain explanation 1\"},\n    {\"input\": \"Legal term 2\", \"output\": \"Plain explanation 2\"}\n]\n\n# Build few-shot prompt\nmessages = [\n    (\"system\", \"Translate legal terms to plain language.\")\n]\nfor ex in examples:\n    messages.append((\"human\", ex[\"input\"]))\n    messages.append((\"assistant\", ex[\"output\"]))\nmessages.append((\"human\", \"{new_input}\"))\n\nfew_shot_prompt = ChatPromptTemplate.from_messages(messages)\n```\n\n## Output Parsing\n\n### Pydantic Parser\n\n```python\nfrom pydantic import BaseModel, Field\nfrom langchain.output_parsers import PydanticOutputParser\n\nclass LegalClause(BaseModel):\n    parties: list[str] = Field(description=\"Parties involved\")\n    obligations: str = Field(description=\"Main obligations\")\n    conditions: str = Field(description=\"Key conditions\")\n\nparser = PydanticOutputParser(pydantic_object=LegalClause)\n\nprompt = PromptTemplate(\n    input_variables=[\"clause\"],\n    template=\"Parse this legal clause:\\n{clause}\\n\\n{format_instructions}\",\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\nformatted = prompt.format(clause=\"...\")\nresponse = llm.invoke(formatted)\nparsed = parser.parse(response)\n\nprint(parsed.parties)\nprint(parsed.obligations)\n```\n\n## Chaining\n\n### Sequential Chain (Pipe Syntax)\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\n# Define chains\ntemplate1 = \"Give a bullet point outline for a blog about {topic}\"\ntemplate2 = \"Write a blog post from this outline:\\n{outline}\"\n\nchain1 = PromptTemplate.from_template(template1) | llm\nchain2 = PromptTemplate.from_template(template2) | llm\n\n# Compose\nfull_chain = chain1 | chain2\n\nresult = full_chain.invoke({\"topic\": \"AI\"})\n```\n\n### Multi-Step Processing\n\n```python\ntemplate1 = \"Summarize this review:\\n{review}\"\ntemplate2 = \"Identify weaknesses:\\n{summary}\"\ntemplate3 = \"Create improvement plan:\\n{weaknesses}\"\n\nchain_1 = PromptTemplate.from_template(template1) | llm\nchain_2 = PromptTemplate.from_template(template2) | llm\nchain_3 = PromptTemplate.from_template(template3) | llm\n\nfull_chain = chain_1 | chain_2 | chain_3\nresult = full_chain.invoke(employee_review)\n```\n\n### Router Chain\n\n```python\nfrom langchain.chains.router import MultiPromptChain\n\nbeginner_template = \"Explain {input} simply for a child.\"\nexpert_template = \"Explain {input} technically for an expert.\"\n\nprompt_infos = [\n    {\"name\": \"beginner\", \"description\": \"For simple questions\", \"prompt_template\": beginner_template},\n    {\"name\": \"expert\", \"description\": \"For technical questions\", \"prompt_template\": expert_template}\n]\n\nchain = MultiPromptChain.from_prompts(llm, prompt_infos, verbose=True)\nresult = chain.invoke(\"How do Feynman diagrams work?\")\n```\n\n## Caching\n\n```python\nimport langchain\nfrom langchain.cache import SQLiteCache\n\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n\n# First call - hits LLM\nresponse1 = llm.invoke(\"What is Python?\")\n\n# Second call - uses cache (instant)\nresponse2 = llm.invoke(\"What is Python?\")\n```\n\n## Messages\n\n```python\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is 2+2?\"),\n    AIMessage(content=\"4\"),\n    HumanMessage(content=\"And times 3?\")\n]\n\nresponse = chat_llm.invoke(messages)\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Building LLM applications with structured workflows\n- Need prompt templating and variable substitution\n- Chaining multiple LLM calls together\n- Parsing structured output from LLMs\n- Working with both local and cloud models\n\n## Cross-References\n\n- `bazzite-ai-jupyter:rag` - RAG pipelines using LangChain\n- `bazzite-ai-jupyter:evaluation` - LLM evaluation\n- `bazzite-ai-ollama:openai` - Ollama OpenAI compatibility\n- `bazzite-ai-ollama:python` - Native Ollama Python library"
              },
              {
                "name": "peft",
                "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n",
                "path": "bazzite-ai-jupyter/skills/peft/SKILL.md",
                "frontmatter": {
                  "name": "peft",
                  "description": "Parameter-efficient fine-tuning with LoRA and Unsloth. Covers LoraConfig,\ntarget module selection, QLoRA for 4-bit training, adapter merging, and\nUnsloth optimizations for 2x faster training.\n"
                },
                "content": "# Parameter-Efficient Fine-Tuning (PEFT)\n\n## Overview\n\nPEFT methods like LoRA train only a small number of adapter parameters instead of the full model, reducing memory by 10-100x while maintaining quality.\n\n## Quick Reference\n\n| Method | Memory | Speed | Quality |\n|--------|--------|-------|---------|\n| Full Fine-tune | High | Slow | Best |\n| LoRA | Low | Fast | Very Good |\n| QLoRA | Very Low | Fast | Good |\n| Unsloth | Very Low | 2x Faster | Good |\n\n## LoRA Concepts\n\n### How LoRA Works\n\n```\nOriginal weight matrix W (frozen):     d x k\nLoRA adapters A and B:                 d x r, r x k (where r << min(d,k))\n\nForward pass:\n  output = x @ W + x @ A @ B * (alpha / r)\n\nTrainable params: 2 * r * d  (instead of d * k)\n```\n\n### Memory Savings\n\n```python\ndef lora_savings(d, k, r):\n    original = d * k\n    lora = 2 * r * max(d, k)\n    reduction = (1 - lora / original) * 100\n    return reduction\n\n# Example: 4096 x 4096 matrix with rank 8\nprint(f\"Memory reduction: {lora_savings(4096, 4096, 8):.1f}%\")\n# Output: ~99.6% reduction\n```\n\n## Basic LoRA Setup\n\n### Configure LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=8,                          # Rank (capacity)\n    lora_alpha=16,                # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers\n    lora_dropout=0.05,            # Regularization\n    bias=\"none\",                  # Don't train biases\n    task_type=TaskType.CAUSAL_LM  # Task type\n)\n```\n\n### Apply to Model\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 4,194,304 || all params: 1,100,048,384 || trainable%: 0.38%\n```\n\n## LoRA Parameters\n\n### Key Parameters\n\n| Parameter | Values | Effect |\n|-----------|--------|--------|\n| `r` | 4, 8, 16, 32 | Adapter capacity |\n| `lora_alpha` | r to 2*r | Scaling (higher = stronger) |\n| `target_modules` | List | Which layers to adapt |\n| `lora_dropout` | 0.0-0.1 | Regularization |\n\n### Target Modules\n\n```python\n# Common target modules for different models\n\n# LLaMA / Mistral / TinyLlama\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# GPT-2\ntarget_modules = [\"c_attn\", \"c_proj\"]\n\n# BLOOM\ntarget_modules = [\"query_key_value\", \"dense\"]\n\n# All linear layers (most aggressive)\ntarget_modules = \"all-linear\"\n```\n\n### Rank Selection Guide\n\n| Rank (r) | Use Case |\n|----------|----------|\n| 4 | Simple tasks, small datasets |\n| 8 | General purpose (recommended) |\n| 16 | Complex tasks, more capacity |\n| 32+ | Near full fine-tune quality |\n\n## QLoRA (Quantized LoRA)\n\n### Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training (important!)\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n```\n\n## Training with PEFT\n\n### Using SFTTrainer\n\n```python\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n\nsft_config = SFTConfig(\n    output_dir=\"./lora_checkpoints\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-4,  # Higher LR for LoRA\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=512,\n    gradient_accumulation_steps=4,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset[\"train\"],\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    peft_config=lora_config,  # Pass LoRA config\n)\n\ntrainer.train()\n```\n\n## Unsloth (2x Faster Training)\n\n### Setup\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Load model with Unsloth optimizations\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/tinyllama-chat-bnb-4bit\",  # Pre-quantized\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\n# Add LoRA with Unsloth\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=42,\n)\n```\n\n### Train with Unsloth\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./unsloth_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=42,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n## Save and Load Adapters\n\n### Save Adapters Only\n\n```python\n# Save just the LoRA weights (small!)\nmodel.save_pretrained(\"./lora_adapters\")\n```\n\n### Load Adapters\n\n```python\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n```\n\n### Merge Adapters into Base Model\n\n```python\n# Merge LoRA weights into base model (for deployment)\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged_model\")\n```\n\n## Inference with Adapters\n\n```python\nfrom peft import PeftModel\n\n# Load base + adapters\nbase_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n\n# Generate\nmodel.eval()\ninputs = tokenizer(\"What is Python?\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap them at inference time without reloading the base model.\n\n### Train Multiple Adapters\n\n```python\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, SFTConfig\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual responses\n    \"creative\": creative_data,     # Imaginative, expressive responses\n    \"code\": code_data,             # Code-focused analysis\n}\n\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(\n        model, r=16, lora_alpha=16,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n    )\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\nfrom unsloth import FastLanguageModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\ndef load_and_generate(adapter_path, prompt):\n    \"\"\"Load adapter and generate response.\"\"\"\n    # Hot-swap adapter onto base model\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(adapted_model.device)\n\n    outputs = adapted_model.generate(input_ids=inputs, max_new_tokens=128)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Use different adapters for different tasks\ntechnical_response = load_and_generate(\"./adapters/technical\", \"Explain TCP vs UDP\")\ncreative_response = load_and_generate(\"./adapters/creative\", \"Write a haiku about coding\")\ncode_response = load_and_generate(\"./adapters/code\", \"Explain Python decorators\")\n```\n\n### Adapter Storage Efficiency\n\n| Component | Size |\n|-----------|------|\n| Base model (4-bit) | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters total | ~1.3GB |\n\n**Multi-adapter approach**: 8GB + 1.3GB = 9.3GB total\n**vs 10 full models**: 80GB total\n\n## Comparison: Full vs LoRA vs QLoRA\n\n| Aspect | Full Fine-tune | LoRA | QLoRA |\n|--------|----------------|------|-------|\n| Trainable % | 100% | ~0.1-1% | ~0.1-1% |\n| Memory | 4x model | ~1.2x model | ~0.5x model |\n| Training speed | Slow | Fast | Fast |\n| Quality | Best | Very Good | Good |\n| 7B model | 28GB+ | ~16GB | ~6GB |\n\n## Troubleshooting\n\n### Out of Memory\n\n**Fix:**\n\n```python\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Use smaller batch with accumulation\nper_device_train_batch_size=1\ngradient_accumulation_steps=8\n```\n\n### Poor Quality\n\n**Fix:**\n\n- Increase `r` (rank)\n- Add more target modules\n- Train longer\n- Check data quality\n\n### NaN Loss\n\n**Fix:**\n\n- Lower learning rate\n- Use gradient clipping\n- Check for data issues\n\n## When to Use This Skill\n\nUse when:\n\n- GPU memory is limited\n- Fine-tuning large models (7B+)\n- Need fast training iterations\n- Want to swap adapters for different tasks\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning basics\n- `bazzite-ai-jupyter:quantization` - Quantization for QLoRA\n- `bazzite-ai-jupyter:sft` - SFT training with LoRA\n- `bazzite-ai-jupyter:inference` - Fast inference with adapters\n- `bazzite-ai-jupyter:transformers` - Target module selection"
              },
              {
                "name": "qlora",
                "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n",
                "path": "bazzite-ai-jupyter/skills/qlora/SKILL.md",
                "frontmatter": {
                  "name": "qlora",
                  "description": "Advanced QLoRA experiments and comparisons. Covers alpha scaling, LoRA rank selection,\ntarget module strategies, continual learning, multi-adapter hot-swapping, and\nquantization comparison (4-bit vs BF16).\n"
                },
                "content": "# Advanced QLoRA Experiments\n\n## Overview\n\nThis skill covers advanced QLoRA experimentation patterns for optimizing fine-tuning performance. Learn how to select the best LoRA rank, alpha scaling, target modules, and quantization settings for your specific use case.\n\n## Quick Reference\n\n| Topic | Key Finding |\n|-------|-------------|\n| **Rank (r)** | r=16 is optimal balance; r=8 for memory constrained |\n| **Alpha** | alpha=r (1.0x scaling) is standard; alpha=2r for aggressive |\n| **Target Modules** | all_linear for general; mlp_only for knowledge injection |\n| **Quantization** | 4-bit NF4 matches BF16 quality with 11-15% memory savings |\n| **Continual Learning** | Sequential training adds knowledge without forgetting |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Import unsloth FIRST\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n```\n\n## Alpha Scaling\n\n### Formula\n\nThe effective LoRA scaling factor is:\n\n```\nscaling_factor = alpha / r\n```\n\nThis acts as a learning rate multiplier for adapter weights.\n\n### Alpha Comparison Code\n\n```python\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import TrainerCallback\n\nALPHAS = [8, 16, 32, 64]\nFIXED_RANK = 16\nresults = []\n\nfor alpha in ALPHAS:\n    scaling_factor = alpha / FIXED_RANK\n    print(f\"\\n=== Testing alpha={alpha} (scaling={scaling_factor}x) ===\")\n\n    # Load fresh model\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA with specific alpha\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=FIXED_RANK,\n        lora_alpha=alpha,  # Variable alpha\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Train and record results\n    trainer = SFTTrainer(model=model, tokenizer=tokenizer, ...)\n    stats = trainer.train()\n\n    results.append({\n        \"alpha\": alpha,\n        \"scaling\": scaling_factor,\n        \"final_loss\": stats.metrics[\"train_loss\"]\n    })\n```\n\n### Alpha Scaling Results\n\n| Alpha | Scaling | Final Loss | Behavior |\n|-------|---------|------------|----------|\n| 8 | 0.5x | ~3.02 | Conservative, slower convergence |\n| 16 | 1.0x | ~2.94 | Standard, balanced |\n| 32 | 2.0x | ~2.80 | Aggressive, faster convergence |\n| 64 | 4.0x | ~2.60 | Very aggressive, risk of instability |\n\n### Recommendations\n\n- **Standard**: `alpha = r` (1.0x scaling)\n- **Aggressive training**: `alpha = 2r` with reduced learning rate\n- **Stability priority**: `alpha = r/2` (0.5x scaling)\n\n## LoRA Rank Comparison\n\n### Rank Selection Code\n\n```python\nRANKS = [4, 8, 16, 32, 64]\n\nfor rank in RANKS:\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=rank,\n        lora_alpha=rank,  # Keep alpha = r for fair comparison\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=42,\n    )\n\n    # Count parameters\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    pct = 100 * trainable / total\n\n    print(f\"r={rank}: {trainable:,} trainable ({pct:.2f}%)\")\n```\n\n### Rank Comparison Results (Qwen3-4B)\n\n| Rank | Trainable Params | % of Total | Memory | Best For |\n|------|------------------|------------|--------|----------|\n| 4 | ~8M | 0.3% | Lowest | Quick experiments |\n| 8 | ~16M | 0.6% | Low | Memory constrained |\n| **16** | ~33M | 1.3% | Medium | **General use (default)** |\n| 32 | ~66M | 2.6% | High | Complex tasks |\n| 64 | ~132M | 5.2% | Highest | Maximum capacity |\n\n### Rank Selection Guidelines\n\n```python\ndef recommend_rank(gpu_vram_gb, task_complexity, dataset_size):\n    \"\"\"Recommend LoRA rank based on constraints.\"\"\"\n\n    # Memory constraints\n    if gpu_vram_gb < 8:\n        max_rank = 8\n    elif gpu_vram_gb < 12:\n        max_rank = 16\n    elif gpu_vram_gb < 24:\n        max_rank = 32\n    else:\n        max_rank = 64\n\n    # Task complexity adjustment\n    if task_complexity == \"simple\":\n        suggested = 8\n    elif task_complexity == \"medium\":\n        suggested = 16\n    elif task_complexity == \"complex\":\n        suggested = 32\n    else:\n        suggested = 16\n\n    # Dataset size adjustment\n    if dataset_size < 1000:\n        suggested = min(suggested, 16)  # Avoid overfitting\n    elif dataset_size > 10000:\n        suggested = max(suggested, 16)  # Can use higher rank\n\n    return min(suggested, max_rank)\n```\n\n## Target Module Selection\n\n### Available Configurations\n\n```python\nTARGET_CONFIGS = {\n    \"qv_only\": {\n        \"modules\": [\"q_proj\", \"v_proj\"],\n        \"params\": \"~9M\",\n        \"description\": \"Query + Value only (minimal, original LoRA paper)\"\n    },\n    \"attention_only\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"params\": \"~18M\",\n        \"description\": \"All attention layers\"\n    },\n    \"mlp_only\": {\n        \"modules\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~15M\",\n        \"description\": \"MLP/FFN layers only\"\n    },\n    \"all_linear\": {\n        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"params\": \"~33M\",\n        \"description\": \"All linear layers (maximum capacity)\"\n    },\n}\n```\n\n### Module Function Analysis\n\n**Attention Layers (q, k, v, o):**\n- Control how model attends to input\n- Affect reasoning patterns and style\n- Best for: Format adaptation, thinking pattern changes\n\n**MLP Layers (gate, up, down):**\n- Store factual knowledge\n- Process and transform representations\n- Best for: Knowledge injection, domain adaptation\n\n### Use Case Recommendations\n\n| Use Case | Config | Rationale |\n|----------|--------|-----------|\n| Minimal fine-tuning | `qv_only` | Fastest, smallest adapters |\n| Style/format change | `attention_only` | Changes reasoning patterns |\n| Knowledge injection | `mlp_only` | Updates knowledge only |\n| **General fine-tuning** | `all_linear` | **Maximum flexibility (default)** |\n| Preserve reasoning | `mlp_only` | Keeps thinking style |\n\n### Target Module Selection Code\n\n```python\ndef get_target_modules(use_case):\n    \"\"\"Select target modules based on use case.\"\"\"\n\n    configs = {\n        \"minimal\": [\"q_proj\", \"v_proj\"],\n        \"style\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        \"knowledge\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n        \"full\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                 \"gate_proj\", \"up_proj\", \"down_proj\"],\n    }\n\n    return configs.get(use_case, configs[\"full\"])\n\n# Usage\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=get_target_modules(\"full\"),\n    ...\n)\n```\n\n## Continual Learning\n\nSequential training adds new knowledge without catastrophic forgetting.\n\n### Sequential Training Pattern\n\n```python\nTRAINING_STAGES = [\n    (\"medical\", medical_dataset),\n    (\"legal\", legal_dataset),\n    (\"technical\", technical_dataset),\n]\n\n# Load model ONCE\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Apply LoRA ONCE\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train sequentially\nfor stage_idx, (domain_name, domain_data) in enumerate(TRAINING_STAGES):\n    print(f\"\\n=== Stage {stage_idx + 1}: Training on {domain_name} ===\")\n\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=domain_data,\n        args=SFTConfig(\n            output_dir=f\"./continual_{domain_name}\",\n            max_steps=5,\n            learning_rate=2e-4,\n            ...\n        ),\n    )\n    trainer.train()\n\n    # Save checkpoint\n    model.save_pretrained(f\"./checkpoint_stage_{stage_idx}\")\n\n    # Test retention on ALL previous domains\n    test_retention(model, tokenizer, TRAINING_STAGES[:stage_idx+1])\n```\n\n### Retention Testing\n\n```python\ndef test_retention(model, tokenizer, trained_domains):\n    \"\"\"Verify model retains knowledge from previous domains.\"\"\"\n\n    RETENTION_TESTS = {\n        \"medical\": \"What is hypertension and how is it treated?\",\n        \"legal\": \"Explain the concept of due process.\",\n        \"technical\": \"What is a REST API?\",\n    }\n\n    FastLanguageModel.for_inference(model)\n\n    print(\"\\n--- Retention Test ---\")\n    for domain_name, _ in trained_domains:\n        prompt = RETENTION_TESTS[domain_name]\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        inputs = tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(model.device)\n\n        outputs = model.generate(input_ids=inputs, max_new_tokens=100)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Check response quality\n        has_content = len(response.split()) > 10\n        print(f\"{domain_name}: {'PASS' if has_content else 'FAIL'}\")\n```\n\n### Continual Learning Benefits\n\n- **No catastrophic forgetting**: Base weights frozen, adapters accumulate knowledge\n- **Incremental updates**: Add new domains without full retraining\n- **Curriculum learning**: Simple  complex topic progression\n- **Personalization**: Adapt over time with user feedback\n\n## Multi-Adapter Hot-Swapping\n\nTrain task-specific adapters and swap at inference time.\n\n### Training Multiple Adapters\n\n```python\nfrom peft import PeftModel\n\nTASK_DATASETS = {\n    \"technical\": technical_data,   # Precise, factual\n    \"creative\": creative_data,     # Imaginative, expressive\n    \"code\": code_data,             # Code-focused\n}\n\n# Train separate adapters\nfor task_name, task_data in TASK_DATASETS.items():\n    # Load base model fresh\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        max_seq_length=512,\n        load_in_4bit=True,\n    )\n\n    # Apply LoRA\n    model = FastLanguageModel.get_peft_model(model, r=16, lora_alpha=16, ...)\n\n    # Train on task-specific data\n    trainer = SFTTrainer(model=model, train_dataset=task_data, ...)\n    trainer.train()\n\n    # Save lightweight adapter (~130MB each)\n    model.save_pretrained(f\"./adapters/{task_name}\")\n    print(f\"Saved {task_name} adapter\")\n```\n\n### Hot-Swap at Inference\n\n```python\nfrom peft import PeftModel\n\n# Load base model ONCE\nbase_model, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Function to swap adapters\ndef load_adapter(base_model, adapter_path):\n    \"\"\"Load specific adapter onto base model.\"\"\"\n    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n    FastLanguageModel.for_inference(adapted_model)\n    return adapted_model\n\n# Usage\ntechnical_model = load_adapter(base_model, \"./adapters/technical\")\nresponse = generate(technical_model, \"Explain TCP vs UDP\")\n\ncreative_model = load_adapter(base_model, \"./adapters/creative\")\nresponse = generate(creative_model, \"Write a haiku about coding\")\n```\n\n### Adapter Storage\n\n| Component | Size |\n|-----------|------|\n| Base model | ~8GB |\n| Each adapter | ~130MB |\n| 10 adapters | ~1.3GB total |\n\nMulti-adapter approach: 8GB + 1.3GB = 9.3GB total\nvs. 10 full models = 80GB\n\n## Quantization Comparison\n\n### 4-bit vs BF16 Code\n\n```python\nQUANT_CONFIGS = {\n    \"4bit_nf4\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n        \"load_in_4bit\": True,\n    },\n    \"bf16\": {\n        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507\",\n        \"load_in_4bit\": False,\n    },\n}\n\nresults = []\n\nfor config_name, config in QUANT_CONFIGS.items():\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        config[\"model_name\"],\n        max_seq_length=512,\n        load_in_4bit=config.get(\"load_in_4bit\", False),\n    )\n\n    # Measure memory\n    memory_before = measure_gpu_memory()\n\n    # Train\n    trainer = SFTTrainer(model=model, ...)\n    stats = trainer.train()\n\n    memory_after = measure_gpu_memory()\n\n    results.append({\n        \"config\": config_name,\n        \"memory_mb\": memory_after,\n        \"final_loss\": stats.metrics[\"train_loss\"],\n    })\n```\n\n### Quantization Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves identical final loss with 11-15% memory savings.\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n## Utility Functions\n\n### Loss History Callback\n\n```python\nfrom transformers import TrainerCallback\n\nclass LossHistoryCallback(TrainerCallback):\n    \"\"\"Track loss during training for comparison.\"\"\"\n\n    def __init__(self):\n        self.losses = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and 'loss' in logs:\n            self.losses.append({\n                'step': state.global_step,\n                'loss': logs['loss']\n            })\n\n# Usage\nloss_callback = LossHistoryCallback()\ntrainer = SFTTrainer(..., callbacks=[loss_callback])\ntrainer.train()\n\n# Access loss history\nfor entry in loss_callback.losses:\n    print(f\"Step {entry['step']}: Loss {entry['loss']:.4f}\")\n```\n\n### GPU Memory Measurement\n\n```python\nimport subprocess\nimport gc\nimport torch\n\ndef measure_gpu_memory():\n    \"\"\"Get current GPU memory usage in MB.\"\"\"\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip().split('\\n')[0])\n\ndef cleanup_memory():\n    \"\"\"Force garbage collection and clear CUDA cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n# Usage\nprint(f\"Memory before: {measure_gpu_memory()} MB\")\ncleanup_memory()\nprint(f\"Memory after cleanup: {measure_gpu_memory()} MB\")\n```\n\n### Parameter Counting\n\n```python\ndef count_parameters(model):\n    \"\"\"Count trainable and total parameters.\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return {\n        \"trainable\": trainable,\n        \"total\": total,\n        \"trainable_formatted\": f\"{trainable:,}\",\n        \"total_formatted\": f\"{total:,}\",\n        \"percentage\": f\"{100 * trainable / total:.2f}%\"\n    }\n\n# Usage\nparams = count_parameters(model)\nprint(f\"Trainable: {params['trainable_formatted']} ({params['percentage']})\")\n```\n\n## Decision Tree\n\n```\nWhat's your priority?\n\n Memory constrained (<12GB VRAM)\n    Use r=8 or r=4\n    Use 4-bit quantization\n    Use qv_only or attention_only modules\n\n Maximum quality\n    Use r=32\n    Use BF16 if VRAM allows\n    Use all_linear modules\n\n Knowledge injection only\n    Use mlp_only modules\n    Preserves reasoning style\n\n Multiple tasks\n    Train separate adapters\n    Hot-swap at inference\n\n Incremental updates\n     Sequential training\n     Test retention after each stage\n```\n\n## Kernel Shutdown (Jupyter)\n\nQLoRA experiments require loading/unloading multiple models. Shutdown kernel between experiments to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Each experiment in the loop should clean up memory with `del model` and `gc.collect()`, but kernel shutdown is required between different experiment notebooks.\n\n## When to Use This Skill\n\nUse when:\n- Optimizing LoRA hyperparameters\n- Memory-constrained training\n- Building multi-task systems\n- Incrementally updating models\n- Comparing quantization approaches\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - Basic LoRA setup\n- `bazzite-ai-jupyter:quantization` - Quantization fundamentals\n- `bazzite-ai-jupyter:sft` - Training with SFTTrainer\n- `bazzite-ai-jupyter:inference` - Fast inference patterns"
              },
              {
                "name": "quantization",
                "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n",
                "path": "bazzite-ai-jupyter/skills/quantization/SKILL.md",
                "frontmatter": {
                  "name": "quantization",
                  "description": "Model quantization for efficient inference and training. Covers precision\ntypes (FP32, FP16, BF16, INT8, INT4), BitsAndBytes configuration, memory\nestimation, and performance tradeoffs.\n"
                },
                "content": "# Model Quantization\n\n## Overview\n\nQuantization reduces model precision to save memory and speed up inference. A 7B model at FP32 requires ~28GB, but at 4-bit only ~4GB.\n\n## Quick Reference\n\n| Precision | Bits | Memory | Quality | Speed |\n|-----------|------|--------|---------|-------|\n| FP32 | 32 | 4x | Best | Slowest |\n| FP16 | 16 | 2x | Excellent | Fast |\n| BF16 | 16 | 2x | Excellent | Fast |\n| INT8 | 8 | 1x | Good | Faster |\n| INT4 | 4 | 0.5x | Acceptable | Fastest |\n\n## Memory Estimation\n\n```python\ndef estimate_memory(params_billions, precision_bits):\n    \"\"\"Estimate model memory in GB.\"\"\"\n    bytes_per_param = precision_bits / 8\n    return params_billions * bytes_per_param\n\n# Example: 7B model\nmodel_size = 7  # billion parameters\n\nprint(f\"FP32: {estimate_memory(7, 32):.1f} GB\")  # 28 GB\nprint(f\"FP16: {estimate_memory(7, 16):.1f} GB\")  # 14 GB\nprint(f\"INT8: {estimate_memory(7, 8):.1f} GB\")   # 7 GB\nprint(f\"INT4: {estimate_memory(7, 4):.1f} GB\")   # 3.5 GB\n```\n\n## Measure Model Size\n\n```python\ndef get_model_size(model):\n    \"\"\"Get model size in GB including buffers.\"\"\"\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    total = (param_size + buffer_size) / 1024**3\n    return total\n\nprint(f\"Model size: {get_model_size(model):.2f} GB\")\n```\n\n## Load Model at Different Precisions\n\n### FP32 (Default)\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel_32bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    device_map=\"auto\"\n)\n\nprint(f\"FP32 size: {get_model_size(model_32bit):.2f} GB\")\n```\n\n### FP16 / BF16\n\n```python\nimport torch\n\nmodel_16bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    torch_dtype=torch.float16,  # or torch.bfloat16\n    device_map=\"auto\"\n)\n\nprint(f\"FP16 size: {get_model_size(model_16bit):.2f} GB\")\n```\n\n### 8-bit Quantization\n\n```python\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"8-bit size: {get_model_size(model_8bit):.2f} GB\")\n```\n\n### 4-bit Quantization (Recommended)\n\n```python\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True  # Nested quantization\n)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nprint(f\"4-bit size: {get_model_size(model_4bit):.2f} GB\")\n```\n\n## BitsAndBytesConfig Options\n\n### 4-bit Configuration\n\n```python\nfrom transformers import BitsAndBytesConfig\nimport torch\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n\n    # Quantization type\n    bnb_4bit_quant_type=\"nf4\",  # \"nf4\" or \"fp4\"\n\n    # Compute dtype for dequantized weights\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\n    # Double quantization (saves more memory)\n    bnb_4bit_use_double_quant=True,\n)\n```\n\n### Options Explained\n\n| Option | Values | Effect |\n|--------|--------|--------|\n| `load_in_4bit` | True/False | Enable 4-bit |\n| `bnb_4bit_quant_type` | \"nf4\", \"fp4\" | nf4 better for LLMs |\n| `bnb_4bit_compute_dtype` | float16, bfloat16 | Computation precision |\n| `bnb_4bit_use_double_quant` | True/False | Quantize quantization constants |\n\n## Compare Precision Performance\n\n```python\nfrom transformers import pipeline\nimport time\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Test message\nmessages = [{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}]\n\ndef benchmark(model, tokenizer, name):\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n    start = time.time()\n    output = pipe(messages, max_new_tokens=100, return_full_text=False)\n    elapsed = time.time() - start\n\n    print(f\"{name}:\")\n    print(f\"  Time: {elapsed:.2f}s\")\n    print(f\"  Size: {get_model_size(model):.2f} GB\")\n    print(f\"  Output: {output[0]['generated_text'][:50]}...\")\n    print()\n\n# Benchmark each precision\nbenchmark(model_32bit, tokenizer, \"FP32\")\nbenchmark(model_16bit, tokenizer, \"FP16\")\nbenchmark(model_8bit, tokenizer, \"8-bit\")\nbenchmark(model_4bit, tokenizer, \"4-bit\")\n```\n\n## Quantization for Training\n\n### QLoRA Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit base model\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## Precision Comparison\n\n| Precision | Memory | Quality | Training | Best For |\n|-----------|--------|---------|----------|----------|\n| FP32 | 4x | Perfect | Yes | Research, baselines |\n| FP16 | 2x | Excellent | Yes | Standard training |\n| BF16 | 2x | Excellent | Yes | Large models |\n| INT8 | 1x | Good | Limited | Inference |\n| INT4 | 0.5x | Acceptable | QLoRA | Memory-constrained |\n\n## FP16 vs BF16\n\n| Aspect | FP16 | BF16 |\n|--------|------|------|\n| Range | Smaller | Larger (like FP32) |\n| Precision | Higher | Lower |\n| Overflow risk | Higher | Lower |\n| Hardware | All GPUs | Ampere+ |\n| Best for | Inference | Training |\n\n## 4-bit NF4 vs BF16 Comparison (Tested)\n\nBased on experiments with Qwen3-4B-Thinking models:\n\n### Comparison Results\n\n| Method | Peak Memory | Final Loss | Quality |\n|--------|-------------|------------|---------|\n| 4-bit NF4 | ~5.7GB | 3.0742 | Excellent |\n| BF16 | ~6.5GB | 3.0742 | Reference |\n\n**Key Finding**: 4-bit NF4 achieves **identical final loss** with 11-15% memory savings.\n\n### Pre-Quantized Models (Recommended)\n\nUse pre-quantized models for faster loading:\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Pre-quantized (fast loading)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",  # -bnb-4bit suffix\n    max_seq_length=1024,\n    load_in_4bit=True,\n)\n\n# vs. On-demand quantization (slower)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Full precision\n    max_seq_length=1024,\n    load_in_4bit=True,  # Quantize during load\n)\n```\n\n### GPU Memory Recommendations\n\n| GPU VRAM | Recommended | Notes |\n|----------|-------------|-------|\n| <12GB | 4-bit NF4 | Required for training |\n| 12-16GB | 4-bit NF4 | Allows larger batches |\n| >16GB | BF16 or 4-bit | Choose based on batch needs |\n\n### Quality Preservation\n\n4-bit NF4 preserves:\n- Training convergence (identical final loss)\n- Thinking tag structure (`<think>...</think>`)\n- Response quality and coherence\n- Model reasoning capabilities\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA OOM error\n\n**Fix:**\n\n```python\n# Use 4-bit quantization\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True\n)\n```\n\n### Quality Degradation\n\n**Symptom:** Poor model outputs after quantization\n\n**Fix:**\n\n- Use nf4 instead of fp4\n- Try 8-bit instead of 4-bit\n- Increase LoRA rank if fine-tuning\n\n### Slow Loading\n\n**Symptom:** Model takes long to load\n\n**Fix:**\n\n- Quantization happens at load time\n- Use `device_map=\"auto\"` for multi-GPU\n\n## When to Use This Skill\n\nUse when:\n\n- Model doesn't fit in GPU memory\n- Need faster inference\n- Training with limited resources (QLoRA)\n- Deploying to edge devices\n\n## Cross-References\n\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments\n- `bazzite-ai-jupyter:peft` - LoRA with quantization (QLoRA)\n- `bazzite-ai-jupyter:finetuning` - Full fine-tuning\n- `bazzite-ai-jupyter:sft` - SFT training with quantization\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:transformers` - Model architecture"
              },
              {
                "name": "rag",
                "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n",
                "path": "bazzite-ai-jupyter/skills/rag/SKILL.md",
                "frontmatter": {
                  "name": "rag",
                  "description": "Retrieval-Augmented Generation (RAG) for grounding LLM responses with\nexternal knowledge. Covers document chunking, embeddings, vector stores\n(pandas, ChromaDB), similarity search, and conversational RAG pipelines.\n"
                },
                "content": "# Retrieval-Augmented Generation (RAG)\n\n## Overview\n\nRAG enhances LLM responses by retrieving relevant context from a knowledge base before generation. This grounds responses in specific documents and reduces hallucination.\n\n## Quick Reference\n\n| Step | Component |\n|------|-----------|\n| 1. Chunk | Split documents into segments |\n| 2. Embed | Convert chunks to vectors |\n| 3. Store | Save in vector database |\n| 4. Retrieve | Find relevant chunks |\n| 5. Generate | LLM answers with context |\n\n## Basic RAG Pipeline\n\n### 1. Document Chunking\n\n```python\nimport textwrap\n\ndocument = \"\"\"\nYour long document text here...\nMultiple paragraphs of content...\n\"\"\"\n\n# Chunk into segments of max 1000 characters\nchunks = textwrap.wrap(document, width=1000)\n\nprint(f\"Created {len(chunks)} chunks\")\n```\n\n### 2. Generate Embeddings\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nEMBED_MODEL = \"llama3.2:latest\"\n\nclient = OpenAI(base_url=f\"{OLLAMA_HOST}/v1\", api_key=\"ollama\")\n\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=EMBED_MODEL,\n        input=text\n    )\n    return response.data[0].embedding\n\n# Embed all chunks\nembeddings = [get_embedding(chunk) for chunk in chunks]\nprint(f\"Embedding dimensions: {len(embeddings[0])}\")\n```\n\n### 3. Create Vector Database (Pandas)\n\n```python\nimport pandas as pd\nimport numpy as np\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": [np.array(e) for e in embeddings]\n})\n```\n\n### 4. Similarity Search\n\n```python\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef search(query, n_results=5):\n    query_embedding = get_embedding(query)\n\n    similarities = vector_db[\"embeddings\"].apply(\n        lambda x: cosine_similarity(query_embedding, x)\n    )\n\n    top_indices = similarities.nlargest(n_results).index\n    return vector_db.loc[top_indices, \"text\"].tolist()\n\n# Find relevant chunks\nrelevant = search(\"What are the symptoms?\", n_results=3)\n```\n\n### 5. Generate with Context\n\n```python\nLLM_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\ndef rag_query(question, n_docs=5):\n    # Retrieve context\n    context_chunks = search(question, n_results=n_docs)\n    context = \"\\n\\n\".join(context_chunks)\n\n    # Build prompt\n    messages = [\n        {\"role\": \"system\", \"content\": f\"Answer based on this context:\\n\\n{context}\"},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\n    # Generate\n    response = client.chat.completions.create(\n        model=LLM_MODEL,\n        messages=messages,\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content\n\nanswer = rag_query(\"What are the main symptoms of Omicron?\")\n```\n\n## LangChain RAG with ChromaDB\n\n### Setup\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\nMODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# LLM for generation\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=MODEL\n)\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n```\n\n### Create Vector Store\n\n```python\nimport textwrap\n\n# Chunk document\ndocument = \"Your document text...\"\nchunks = textwrap.wrap(document, width=1000)\n\n# Create ChromaDB store\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n```\n\n### Build RAG Chain\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_classic.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\n# Prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer based on this context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{input}\")\n])\n\n# Create chains\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\n```\n\n### Conversational RAG\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nchat_history = []\n\ndef chat(question):\n    result = rag_chain.invoke({\n        \"input\": question,\n        \"chat_history\": chat_history\n    })\n\n    # Update history\n    chat_history.append(HumanMessage(content=question))\n    chat_history.append(AIMessage(content=result[\"answer\"]))\n\n    return result[\"answer\"]\n\n# Multi-turn conversation\nprint(chat(\"What is Omicron?\"))\nprint(chat(\"What are its symptoms?\"))\nprint(chat(\"How does it compare to Delta?\"))\n```\n\n## Chunking Strategies\n\n### Fixed Size\n\n```python\ndef fixed_chunks(text, size=1000):\n    return textwrap.wrap(text, width=size)\n```\n\n### Sentence-Based\n\n```python\nimport re\n\ndef sentence_chunks(text, max_sentences=5):\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    chunks = []\n    current = []\n\n    for sent in sentences:\n        current.append(sent)\n        if len(current) >= max_sentences:\n            chunks.append(\" \".join(current))\n            current = []\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n```\n\n### Overlap Chunks\n\n```python\ndef overlap_chunks(text, size=1000, overlap=200):\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + size\n        chunks.append(text[start:end])\n        start = end - overlap\n\n    return chunks\n```\n\n## Vector Store Options\n\n### Pandas DataFrame (Simple)\n\n```python\nimport pandas as pd\n\nvector_db = pd.DataFrame({\n    \"text\": chunks,\n    \"embeddings\": embeddings\n})\n```\n\n### ChromaDB (Persistent)\n\n```python\nfrom langchain_community.vectorstores import Chroma\n\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n```\n\n### FAISS (Fast)\n\n```python\nfrom langchain_community.vectorstores import FAISS\n\nvectorstore = FAISS.from_texts(chunks, embeddings)\nvectorstore.save_local(\"./faiss_index\")\n```\n\n## Troubleshooting\n\n### Poor Retrieval Quality\n\n**Symptom:** Retrieved chunks not relevant\n\n**Fix:**\n\n- Increase chunk overlap\n- Use smaller chunk sizes\n- Try different embedding models\n- Increase `k` in retriever\n\n### Slow Embedding\n\n**Symptom:** Takes long to embed documents\n\n**Fix:**\n\n- Batch embeddings\n- Use smaller embedding model\n- Cache embeddings to disk\n\n### Out of Context\n\n**Symptom:** LLM ignores retrieved context\n\n**Fix:**\n\n- Increase `max_tokens`\n- Use explicit system prompt\n- Reduce number of retrieved chunks\n\n## When to Use This Skill\n\nUse when:\n\n- LLM needs to answer from specific documents\n- Reducing hallucination is critical\n- Building Q&A systems over documents\n- Need up-to-date information not in training data\n\n## Cross-References\n\n- `bazzite-ai-jupyter:langchain` - LangChain fundamentals\n- `bazzite-ai-jupyter:evaluation` - Evaluate RAG quality\n- `bazzite-ai-ollama:python` - Ollama embeddings API"
              },
              {
                "name": "reward",
                "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n",
                "path": "bazzite-ai-jupyter/skills/reward/SKILL.md",
                "frontmatter": {
                  "name": "reward",
                  "description": "Reward model training for RLHF pipelines. Covers RewardTrainer, preference dataset\npreparation, sequence classification heads, and reward scaling for stable\nreinforcement learning. Includes thinking quality scoring patterns.\n"
                },
                "content": "# Reward Model Training\n\n## Overview\n\nReward models learn to score responses based on human preferences. They're used in RLHF pipelines (PPO, GRPO, RLOO) to provide reward signals for policy optimization. The model outputs a scalar reward for each response. This skill includes patterns for scoring thinking/reasoning quality.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RewardTrainer` | Trainer for reward model |\n| `RewardConfig` | Training hyperparameters |\n| `AutoModelForSequenceClassification` | Model with `num_labels=1` |\n| `task_type=\"SEQ_CLS\"` | LoRA task type for reward models |\n| Preference pairs | Training data format |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# Standard transformers for reward models (not Unsloth)\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import RewardTrainer, RewardConfig\nfrom datasets import Dataset\nimport torch\n```\n\n## Reward Model Concepts\n\n### How Reward Models Work\n\n1. Take prompt + response as input\n2. Output scalar reward score\n3. Trained on preference pairs (chosen > rejected)\n4. Used to guide RL policy optimization\n\n### Architecture\n\n```\nInput: [prompt + response]\n  \nBase LLM (frozen or LoRA)\n  \nClassification Head (Linear  Scalar)\n  \nOutput: Reward score (float)\n```\n\n## Dataset Format\n\n### Required Fields\n\n```python\ndataset = [\n    {\n        \"prompt\": \"What is recursion?\",\n        \"chosen\": \"Recursion is a function calling itself with a base case.\",\n        \"rejected\": \"Recursion is loops.\"\n    },\n    # ... more preference pairs\n]\n```\n\n### Preprocessing\n\n```python\ndef format_for_reward(sample):\n    prompt = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n        tokenize=False, add_generation_prompt=True\n    )\n    return {\n        \"input_ids_chosen\": tokenizer(prompt + sample[\"chosen\"])[\"input_ids\"],\n        \"input_ids_rejected\": tokenizer(prompt + sample[\"rejected\"])[\"input_ids\"],\n    }\n```\n\n### Thinking Quality Preference Dataset\n\nTrain reward model to score thinking quality:\n\n```python\n# Chosen = Good thinking, Rejected = Poor/no thinking\nthinking_preference_data = [\n    {\n        \"prompt\": \"Explain recursion in programming.\",\n        \"chosen\": \"\"\"<think>\nWhat is recursion exactly? It's when a function calls itself.\nWhy would we use this? To break down problems into smaller pieces.\nWhat's a good example? Factorial: 5! = 5 * 4!\n</think>\n\nRecursion is a technique where a function calls itself with a simpler version of the problem.\"\"\",\n        \"rejected\": \"Recursion is just loops.\"\n    },\n    {\n        \"prompt\": \"What is 15 + 27?\",\n        \"chosen\": \"\"\"<think>\nI need to add 15 and 27.\nLet me break it down: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42.\n</think>\n\n15 + 27 = 42\"\"\",\n        \"rejected\": \"42\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_preference_data)\n```\n\n## Setup\n\n### Load Reward Model\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\n# Load as sequence classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"Qwen/Qwen3-4B-Thinking-2507\",  # Non-quantized base\n    num_labels=1,  # Single scalar reward output\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n\n# Setup pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## RewardTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RewardConfig\n\nreward_config = RewardConfig(\n    output_dir=\"./reward_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_length=512,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 1e-5 to 5e-5 | Training speed |\n| `max_length` | 512-1024 | Input truncation |\n| `center_rewards_coefficient` | 0.0-0.1 | Reward centering |\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RewardTrainer\n\ntrainer = RewardTrainer(\n    model=model,\n    args=reward_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\n\ntrainer.train()\n```\n\n## Using the Reward Model\n\n### Score Responses\n\n```python\ndef get_reward(prompt, response):\n    text = prompt + response\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        reward = outputs.logits[0, 0].item()\n\n    return reward\n\n# Example\nscore = get_reward(\"What is Python?\", \"A programming language.\")\nprint(f\"Reward: {score:.3f}\")\n```\n\n### Batch Scoring\n\n```python\ndef get_rewards_batch(prompts, responses):\n    texts = [p + r for p, r in zip(prompts, responses)]\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        rewards = outputs.logits[:, 0].tolist()\n\n    return rewards\n```\n\n### In GRPO/RLOO\n\n```python\ndef reward_fn(completions, prompts):\n    return get_rewards_batch(prompts, completions)\n\ngrpo_trainer = GRPOTrainer(\n    model=policy_model,\n    args=grpo_config,\n    train_dataset=dataset,\n    reward_funcs=reward_fn,\n)\n```\n\n## Reward Scaling\n\n### Normalize Rewards\n\n```python\ndef normalized_reward(completions, prompts):\n    raw_rewards = get_rewards_batch(prompts, completions)\n    mean = sum(raw_rewards) / len(raw_rewards)\n    std = (sum((r - mean) ** 2 for r in raw_rewards) / len(raw_rewards)) ** 0.5\n    return [(r - mean) / (std + 1e-8) for r in raw_rewards]\n```\n\n### Clip Rewards\n\n```python\ndef clipped_reward(completions, prompts):\n    rewards = get_rewards_batch(prompts, completions)\n    return [max(-1.0, min(1.0, r)) for r in rewards]\n```\n\n## Troubleshooting\n\n### Poor Discrimination\n\n**Symptom:** Similar scores for chosen and rejected\n\n**Fix:**\n- More training steps\n- Higher learning rate\n- Check data quality\n\n### Reward Hacking\n\n**Symptom:** RL model exploits reward model\n\n**Fix:**\n- Add diversity in training data\n- Ensemble multiple reward models\n- Regularization during RL\n\n### Overconfident Scores\n\n**Symptom:** Extreme reward values\n\n**Fix:**\n- Use `center_rewards_coefficient`\n- Normalize outputs\n- Clip reward range\n\n### Memory Issues\n\n**Symptom:** OOM during training\n\n**Fix:**\n- Use LoRA instead of full fine-tuning\n- Reduce `max_length`\n- Smaller batch size\n\n## Kernel Shutdown (Jupyter)\n\nReward model training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Building RLHF pipelines\n- Need explicit reward signal\n- Have preference data\n- Want interpretable scoring\n- Planning to use GRPO or RLOO\n\n## Cross-References\n\n- `bazzite-ai-jupyter:grpo` - Uses reward models for RL\n- `bazzite-ai-jupyter:rloo` - Uses reward models for RL\n- `bazzite-ai-jupyter:dpo` - Alternative that doesn't need reward model\n- `bazzite-ai-jupyter:peft` - LoRA for efficient reward training\n- `bazzite-ai-jupyter:sft` - Pre-training before reward modeling\n- `bazzite-ai-jupyter:inference` - Inference for reward scoring"
              },
              {
                "name": "rloo",
                "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n",
                "path": "bazzite-ai-jupyter/skills/rloo/SKILL.md",
                "frontmatter": {
                  "name": "rloo",
                  "description": "Reinforcement Learning with Leave-One-Out estimation for policy optimization.\nCovers RLOOTrainer, reward function integration, baseline estimation, and\nvariance reduction techniques for stable RL training. Includes thinking-aware patterns.\n"
                },
                "content": "# Reinforcement Learning with Leave-One-Out (RLOO)\n\n## Overview\n\nRLOO is a reinforcement learning method that uses leave-one-out baseline estimation for variance reduction. Like GRPO, it generates multiple completions per prompt but uses a different baseline computation that can provide more stable gradients. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `RLOOTrainer` | RL trainer with RLOO baseline |\n| `RLOOConfig` | Training hyperparameters |\n| `reward_funcs` | Reward function(s) for scoring |\n| `completion_ids` | Token IDs passed to reward functions (no re-tokenization) |\n| `num_generations` | Completions per prompt (4 typical) |\n| `kl_coef` | KL penalty coefficient (0.05, lower than GRPO) |\n| `learning_rate` | 1e-5 (same as GRPO) |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# CRITICAL: Set BEFORE importing unsloth/TRL\nos.environ['ACCELERATE_MIXED_PRECISION'] = 'bf16'\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then TRL imports\nfrom trl import RLOOConfig, RLOOTrainer\nfrom datasets import Dataset\nimport torch\n```\n\n## RLOO Concepts\n\n### How RLOO Works\n\n1. Generate K completions for each prompt\n2. Score all completions with reward function\n3. For each completion, compute baseline as mean of other K-1 rewards\n4. Advantage = reward - leave-one-out baseline\n5. Update policy using advantages\n\n### Leave-One-Out Baseline\n\n```\nFor completion i:\n  baseline_i = mean(rewards except reward_i)\n  advantage_i = reward_i - baseline_i\n\nThis reduces variance compared to:\n  - Single-sample estimates (high variance)\n  - Fixed baselines (may be inaccurate)\n```\n\n### Comparison with GRPO\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| Compute | Similar | Similar |\n| Stability | Often better | Good |\n\n## Dataset Format\n\n```python\n# RLOO requires prompts only (completions generated during training)\ndataset = Dataset.from_dict({\n    \"prompt\": [\n        tokenizer.apply_chat_template(\n            [{\"role\": \"user\", \"content\": \"Explain recursion.\"}],\n            tokenize=False, add_generation_prompt=True\n        ),\n        # ... more prompts\n    ]\n})\n```\n\n## Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n\n# Setup pad token (required for RLOO)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n## RLOOTrainer Configuration\n\n### Basic Configuration\n\n```python\nfrom trl import RLOOConfig\n\nrloo_config = RLOOConfig(\n    output_dir=\"./rloo_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=1e-5,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    num_generations=4,\n    max_completion_length=128,\n    kl_coef=0.05,\n)\n```\n\n### Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `num_generations` | 4-8 | Completions per prompt |\n| `kl_coef` | 0.01-0.1 | KL penalty strength |\n| `learning_rate` | 1e-6 to 1e-5 | Lower than SFT |\n| `max_completion_length` | 64-256 | Generation length |\n\n## Reward Functions\n\n### Simple Reward Function\n\n```python\ndef length_reward(completions, prompts=None):\n    \"\"\"Reward based on response quality heuristics.\"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion.split())\n        score = 0.0\n\n        # Prefer medium length\n        if 10 <= length <= 50:\n            score += 1.0\n        elif length < 10:\n            score -= 0.5\n\n        # Prefer complete sentences\n        if completion.strip().endswith(\".\"):\n            score += 0.5\n\n        rewards.append(score)\n    return rewards\n```\n\n### Using Trained Reward Model\n\n```python\ndef trained_reward(completions, prompts):\n    \"\"\"Use trained reward model.\"\"\"\n    return reward_model.get_rewards(prompts, completions)\n```\n\n### Thinking-Aware Reward Function (Token-Based)\n\nUse `completion_ids` parameter from TRL for efficient token-based parsing (same pattern as GRPO):\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking models\n\ndef thinking_reward_fn(completions, prompts=None, completion_ids=None, **kwargs):\n    \"\"\"\n    Token-based reward function using completion_ids provided by TRL.\n\n    Benefits over string matching:\n    - No re-tokenization overhead (faster training)\n    - Exact token boundaries (no regex edge cases)\n    - Consistent with inference code pattern\n\n    Scoring:\n    - No </think> token: -1.0 (strongly penalized)\n    - Short thinking (<10 tokens): 0.3\n    - Medium thinking (10-30 tokens): 0.7\n    - Long thinking (>30 tokens): 1.0\n    - Bonus +0.1 for self-questioning (contains '?')\n    \"\"\"\n    rewards = []\n\n    for completion, comp_ids in zip(completions, completion_ids):\n        # Token-based detection using </think> token ID\n        if THINK_END_TOKEN_ID in comp_ids:\n            end_idx = comp_ids.index(THINK_END_TOKEN_ID)\n            thinking_length = end_idx  # Token count before </think>\n\n            # String-based content analysis for question detection\n            thinking_content = completion.split('</think>')[0]\n            has_self_questions = '?' in thinking_content\n\n            # Score based on thinking token count\n            if thinking_length < 10:\n                reward = 0.3  # Minimal thinking\n            elif thinking_length < 30:\n                reward = 0.7 + (0.1 if has_self_questions else 0)\n            else:\n                reward = 1.0 + (0.1 if has_self_questions else 0)\n        else:\n            reward = -1.0  # No </think> token found\n\n        rewards.append(reward)\n\n    return rewards\n```\n\n**Key insight**: TRL passes `completion_ids` directly to reward functions, eliminating re-tokenization overhead.\n\n## Training\n\n### Basic Training\n\n```python\nfrom trl import RLOOTrainer\n\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=length_reward,\n)\n\ntrainer.train()\n```\n\n### With Reward Model Instance\n\n```python\ntrainer = RLOOTrainer(\n    model=model,\n    args=rloo_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    reward_model=trained_reward_model,\n)\n```\n\n## num_generations Selection\n\n| K | Use Case |\n|---|----------|\n| 2 | Minimum (limited variance reduction) |\n| 4 | Standard (recommended) |\n| 8 | Better baseline estimation (more compute) |\n| 16+ | Diminishing returns |\n\n**Trade-off:** Higher K = better baseline but more memory/compute\n\n## Troubleshooting\n\n### High Variance\n\n**Symptom:** Unstable training, jumpy rewards\n\n**Fix:**\n- Increase `num_generations` to 6-8\n- Lower `learning_rate`\n- Increase `kl_coef`\n\n### KL Divergence Explosion\n\n**Symptom:** Model output degrades quickly\n\n**Fix:**\n- Increase `kl_coef` to 0.1\n- Reduce `learning_rate`\n- More frequent evaluation\n\n### Reward Collapse\n\n**Symptom:** All generations get similar rewards\n\n**Fix:**\n- Check reward function diversity\n- Increase `temperature` during generation\n- More diverse prompts\n\n### Memory Issues\n\n**Symptom:** OOM with multiple generations\n\n**Fix:**\n- Reduce `num_generations` to 2-4\n- Reduce `max_completion_length`\n- Use gradient checkpointing\n\n## Kernel Shutdown (Jupyter)\n\nRLOO training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Want lower variance than GRPO\n- Have compute for multiple generations\n- Building RLHF pipelines\n- Need stable RL training\n- Policy optimization from rewards\n\n## RLOO vs GRPO Comparison\n\n| Aspect | RLOO | GRPO |\n|--------|------|------|\n| Baseline | Leave-one-out mean | Group mean |\n| Variance | Lower | Higher |\n| KL penalty (beta) | 0.05 | 0.1 |\n| num_generations | 4 | 2 |\n| batch_size | 4 | 2 |\n| Stability | Often better | Good |\n| Use when | Need stable training | Faster iteration |\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Pre-training before RLOO\n- `bazzite-ai-jupyter:grpo` - Alternative RL method (higher variance)\n- `bazzite-ai-jupyter:reward` - Training reward models for RLOO\n- `bazzite-ai-jupyter:dpo` - Simpler alternative (no RL)\n- `bazzite-ai-jupyter:peft` - LoRA for efficient training\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM"
              },
              {
                "name": "sft",
                "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n",
                "path": "bazzite-ai-jupyter/skills/sft/SKILL.md",
                "frontmatter": {
                  "name": "sft",
                  "description": "Supervised Fine-Tuning with SFTTrainer and Unsloth. Covers dataset preparation,\nchat template formatting, training configuration, and Unsloth optimizations\nfor 2x faster instruction tuning. Includes thinking model patterns.\n"
                },
                "content": "# Supervised Fine-Tuning (SFT)\n\n## Overview\n\nSFT adapts a pre-trained LLM to follow instructions by training on instruction-response pairs. Unsloth provides an optimized SFTTrainer for 2x faster training with reduced memory usage. This skill includes patterns for training thinking/reasoning models.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastLanguageModel` | Load model with Unsloth optimizations |\n| `SFTTrainer` | Trainer for instruction tuning |\n| `SFTConfig` | Training hyperparameters |\n| `dataset_text_field` | Column containing formatted text |\n| Token ID 151668 | `</think>` boundary for Qwen3-Thinking models |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then other imports\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import Dataset\nimport torch\n```\n\n**Warning**: Importing TRL before Unsloth will disable optimizations and may cause errors.\n\n## Dataset Formats\n\n### Instruction-Response Format\n\n```python\ndataset = [\n    {\"instruction\": \"What is Python?\", \"response\": \"A programming language.\"},\n    {\"instruction\": \"Explain ML.\", \"response\": \"Machine learning is...\"},\n]\n```\n\n### Chat/Conversation Format\n\n```python\ndataset = [\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"What is Python?\"},\n        {\"role\": \"assistant\", \"content\": \"A programming language.\"}\n    ]},\n]\n```\n\n### Using Chat Templates\n\n```python\ndef format_conversation(sample):\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\ndataset = dataset.map(format_conversation)\n```\n\n### Thinking Model Format\n\nFor models like Qwen3-Thinking, include `<think>` tags in the assistant response. Use **self-questioning internal dialogue** style:\n\n```python\ndef format_thinking_conversation(sample):\n    \"\"\"Format with thinking/reasoning tags.\"\"\"\n    # Combine thinking and response with tags\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n\n# Sample dataset with self-questioning thinking style\nthinking_data = [\n    {\n        \"instruction\": \"What is machine learning?\",\n        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n    },\n    {\n        \"instruction\": \"Explain Python in one sentence.\",\n        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n    },\n    {\n        \"instruction\": \"What is a neural network?\",\n        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n    },\n]\n\ndataset = Dataset.from_list(thinking_data)\ndataset = dataset.map(format_thinking_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n```\n\n**Thinking Style Patterns:**\n- \"What is the user asking here?\"\n- \"Let me think about the key concepts...\"\n- \"How should I structure this explanation?\"\n- \"What's most important about X?\"\n\n## Unsloth SFT Setup\n\n### Load Model\n\n```python\nfrom unsloth import FastLanguageModel\n\n# Standard model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    max_seq_length=512,\n    load_in_4bit=True,\n)\n\n# Thinking model (for reasoning tasks)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n    max_seq_length=1024,  # Increased for thinking content\n    load_in_4bit=True,\n)\n```\n\n### Apply LoRA\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    use_gradient_checkpointing=\"unsloth\",\n)\n```\n\n### Training Configuration\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft_output\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    learning_rate=2e-4,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=512,\n)\n```\n\n## SFTTrainer Usage\n\n### Basic Training\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    args=sft_config,\n)\n\ntrainer.train()\n```\n\n### With Custom Formatting\n\n```python\ndef formatting_func(examples):\n    texts = []\n    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n        texts.append(text)\n    return texts\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    formatting_func=formatting_func,\n    args=sft_config,\n)\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `learning_rate` | 2e-4 to 2e-5 | Training speed vs stability |\n| `per_device_train_batch_size` | 1-4 | Memory usage |\n| `gradient_accumulation_steps` | 2-8 | Effective batch size |\n| `max_seq_length` | 512-2048 | Context window |\n| `optim` | \"adamw_8bit\" | Memory-efficient optimizer |\n\n## Save and Load\n\n### Save Model\n\n```python\n# Save LoRA adapters only (small)\nmodel.save_pretrained(\"./sft_lora\")\n\n# Save merged model (full size)\nmodel.save_pretrained_merged(\"./sft_merged\", tokenizer)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"./sft_lora\")\nFastLanguageModel.for_inference(model)\n```\n\n### Thinking Model Inference\n\nParse thinking content from model output using token IDs:\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> token for Qwen3-Thinking\n\ndef generate_with_thinking(model, tokenizer, prompt):\n    \"\"\"Generate and parse thinking model output.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Setup pad token if needed\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    # Extract only generated tokens\n    input_length = inputs.shape[1]\n    generated_ids = outputs[0][input_length:].tolist()\n\n    # Parse thinking and response\n    if THINK_END_TOKEN_ID in generated_ids:\n        end_idx = generated_ids.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(generated_ids[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(generated_ids[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(generated_ids, skip_special_tokens=True)\n        response = \"(incomplete - increase max_new_tokens)\"\n\n    return thinking.strip(), response.strip()\n\n# Usage\nFastLanguageModel.for_inference(model)\nthinking, response = generate_with_thinking(model, tokenizer, \"What is 15 + 27?\")\nprint(f\"Thinking: {thinking}\")\nprint(f\"Response: {response}\")\n```\n\n## Ollama Integration\n\n### Export to GGUF\n\n```python\n# Export to GGUF for Ollama\nmodel.save_pretrained_gguf(\n    \"model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n```\n\n### Deploy to Ollama\n\n```bash\nollama create mymodel -f Modelfile\nollama run mymodel\n```\n\n## Troubleshooting\n\n### Out of Memory\n\n**Symptom:** CUDA out of memory error\n\n**Fix:**\n- Use `gradient_checkpointing=\"unsloth\"`\n- Reduce `per_device_train_batch_size` to 1\n- Use 4-bit quantization (`load_in_4bit=True`)\n\n### NaN Loss\n\n**Symptom:** Loss becomes NaN during training\n\n**Fix:**\n- Lower `learning_rate` to 1e-5\n- Check data quality (no empty samples)\n- Use gradient clipping\n\n### Slow Training\n\n**Symptom:** Training slower than expected\n\n**Fix:**\n- Ensure Unsloth is imported FIRST (before TRL)\n- Use `bf16=True` if supported\n- Enable `use_gradient_checkpointing=\"unsloth\"`\n\n## Kernel Shutdown (Jupyter)\n\nSFT training uses significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## When to Use This Skill\n\nUse when:\n\n- Creating instruction-following models\n- Fine-tuning for chat/conversation\n- Adapting to domain-specific tasks\n- Building custom assistants\n- First step before preference optimization (DPO/GRPO)\n\n## Cross-References\n\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:qlora` - Advanced QLoRA experiments (alpha, rank, modules)\n- `bazzite-ai-jupyter:finetuning` - General fine-tuning concepts\n- `bazzite-ai-jupyter:dpo` - Direct Preference Optimization after SFT\n- `bazzite-ai-jupyter:grpo` - GRPO reinforcement learning after SFT\n- `bazzite-ai-jupyter:inference` - Fast inference with vLLM\n- `bazzite-ai-jupyter:vision` - Vision model fine-tuning\n- `bazzite-ai-ollama:api` - Ollama deployment"
              },
              {
                "name": "transformers",
                "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n",
                "path": "bazzite-ai-jupyter/skills/transformers/SKILL.md",
                "frontmatter": {
                  "name": "transformers",
                  "description": "Transformer architecture fundamentals. Covers self-attention mechanism,\nmulti-head attention, feed-forward networks, layer normalization, and\nresidual connections. Essential concepts for understanding LLMs.\n"
                },
                "content": "# Transformer Architecture\n\n## Overview\n\nThe Transformer architecture is the foundation of modern LLMs. Understanding its components helps with fine-tuning decisions, model selection, and debugging performance issues.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| Self-Attention | Learn relationships between tokens |\n| Multi-Head Attention | Multiple attention perspectives |\n| Feed-Forward Network | Transform representations |\n| Layer Normalization | Stabilize training |\n| Residual Connections | Enable deep networks |\n\n## Self-Attention Mechanism\n\n### Concept\n\nSelf-attention allows each token to attend to all other tokens in a sequence, learning contextual relationships.\n\n```\n\"The cat sat on the mat\"\n       \n  Each word attends to every other word\n       \n  Contextual representations\n```\n\n### Implementation\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n# Example tokens\ntokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\nseq_length = len(tokens)\nembed_dim = 8\n\n# Random embeddings (in practice, learned)\nembeddings = torch.randn(seq_length, embed_dim)\n\n# Query, Key, Value weight matrices\nW_q = torch.randn(embed_dim, embed_dim)\nW_k = torch.randn(embed_dim, embed_dim)\nW_v = torch.randn(embed_dim, embed_dim)\n\n# Compute Q, K, V\nQ = embeddings @ W_q  # Queries: what am I looking for?\nK = embeddings @ W_k  # Keys: what do I contain?\nV = embeddings @ W_v  # Values: what information do I provide?\n\n# Attention scores\nscores = Q @ K.T / (embed_dim ** 0.5)  # Scale by sqrt(d_k)\n\n# Softmax for attention weights\nattention_weights = F.softmax(scores, dim=-1)\n\n# Weighted sum of values\noutput = attention_weights @ V\n\nprint(f\"Input shape: {embeddings.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attention_weights.shape}\")\n```\n\n### Attention Formula\n\n```\nAttention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n```\n\nWhere:\n\n- Q = Query matrix\n- K = Key matrix\n- V = Value matrix\n- d_k = Key dimension (for scaling)\n\n## Multi-Head Attention\n\n### Concept\n\nMultiple attention heads learn different aspects of relationships (syntax, semantics, etc.).\n\n```python\nnum_heads = 4\nhead_dim = embed_dim // num_heads\n\n# Split into heads\ndef split_heads(x, num_heads):\n    batch_size, seq_len, embed_dim = x.shape\n    head_dim = embed_dim // num_heads\n    return x.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n\n# Compute attention for each head\nheads = []\nfor h in range(num_heads):\n    W_q_h = torch.randn(embed_dim, head_dim)\n    W_k_h = torch.randn(embed_dim, head_dim)\n    W_v_h = torch.randn(embed_dim, head_dim)\n\n    Q_h = embeddings @ W_q_h\n    K_h = embeddings @ W_k_h\n    V_h = embeddings @ W_v_h\n\n    scores_h = Q_h @ K_h.T / (head_dim ** 0.5)\n    attn_h = F.softmax(scores_h, dim=-1)\n    head_output = attn_h @ V_h\n    heads.append(head_output)\n\n# Concatenate heads\nmulti_head_output = torch.cat(heads, dim=-1)\n\n# Project back to embed_dim\nW_o = torch.randn(embed_dim, embed_dim)\nfinal_output = multi_head_output @ W_o\n\nprint(f\"Multi-head output shape: {final_output.shape}\")\n```\n\n## Feed-Forward Network\n\n### Concept\n\nTwo linear layers with activation, applied to each position independently.\n\n```python\nimport torch.nn as nn\n\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n        self.activation = nn.GELU()  # or ReLU\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n\nffn = FeedForward(embed_dim=512)\nx = torch.randn(1, 10, 512)  # (batch, seq_len, embed_dim)\noutput = ffn(x)\n\nprint(f\"FFN output shape: {output.shape}\")\n```\n\n### Formula\n\n```\nFFN(x) = GELU(xW_1 + b_1)W_2 + b_2\n```\n\n## Layer Normalization\n\n### Concept\n\nNormalizes across the embedding dimension to stabilize training.\n\n```python\nclass LayerNorm(nn.Module):\n    def __init__(self, embed_dim, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(embed_dim))\n        self.beta = nn.Parameter(torch.zeros(embed_dim))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nlayer_norm = nn.LayerNorm(embed_dim)\nnormalized = layer_norm(embeddings)\n```\n\n## Residual Connections\n\n### Concept\n\nSkip connections that add input to output, enabling gradient flow in deep networks.\n\n```python\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # Self-attention with residual\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)  # Residual connection\n\n        # FFN with residual\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)  # Residual connection\n\n        return x\n```\n\n## Complete Transformer Layer\n\n```python\nclass TransformerLayer(nn.Module):\n    def __init__(self, embed_dim=512, num_heads=8, hidden_dim=2048, dropout=0.1):\n        super().__init__()\n\n        # Multi-head attention\n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n\n        # Feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n        # Layer norms\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention block\n        attn_out, attn_weights = self.self_attn(x, x, x, attn_mask=mask)\n        x = self.norm1(x + self.dropout(attn_out))\n\n        # FFN block\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n\n        return x, attn_weights\n\n# Example usage\nlayer = TransformerLayer()\nx = torch.randn(10, 1, 512)  # (seq_len, batch, embed_dim)\noutput, weights = layer(x)\nprint(f\"Output shape: {output.shape}\")\n```\n\n## Key Parameters\n\n| Parameter | Typical Values | Effect |\n|-----------|----------------|--------|\n| `embed_dim` | 768, 1024, 4096 | Model capacity |\n| `num_heads` | 8, 12, 16 | Attention perspectives |\n| `num_layers` | 12, 24, 32 | Model depth |\n| `hidden_dim` | 4 * embed_dim | FFN capacity |\n| `dropout` | 0.1 | Regularization |\n\n## Thinking Model Special Tokens\n\nQwen3-Thinking models use special tokens for chain-of-thought reasoning.\n\n### Token IDs\n\n| Token | ID | Purpose |\n|-------|----| --------|\n| `<think>` | 151667 | Start of thinking block |\n| `</think>` | 151668 | End of thinking block |\n\n### Parsing Thinking Output\n\n```python\nTHINK_END_TOKEN_ID = 151668  # </think> for Qwen3-Thinking\n\ndef parse_thinking_response(token_ids, tokenizer):\n    \"\"\"Parse thinking model output using token ID boundary.\"\"\"\n    token_list = list(token_ids)\n\n    if THINK_END_TOKEN_ID in token_list:\n        end_idx = token_list.index(THINK_END_TOKEN_ID)\n        thinking = tokenizer.decode(token_list[:end_idx], skip_special_tokens=True)\n        response = tokenizer.decode(token_list[end_idx + 1:], skip_special_tokens=True)\n    else:\n        thinking = tokenizer.decode(token_list, skip_special_tokens=True)\n        response = \"(incomplete - increase max_tokens)\"\n\n    return thinking.strip(), response.strip()\n```\n\n### Chat Template with Thinking\n\n```python\n# Format training data with thinking tags\ndef format_thinking_sample(sample):\n    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n    messages = [\n        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": assistant_content}\n    ]\n    return {\"text\": tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )}\n```\n\n## Model Size Estimation\n\n```python\ndef estimate_params(vocab_size, embed_dim, num_layers, hidden_dim, num_heads):\n    # Embedding\n    embedding_params = vocab_size * embed_dim\n\n    # Per layer\n    attn_params = 4 * embed_dim * embed_dim  # Q, K, V, O projections\n    ffn_params = 2 * embed_dim * hidden_dim  # Two linear layers\n    norm_params = 4 * embed_dim  # Two layer norms\n\n    layer_params = attn_params + ffn_params + norm_params\n    total_layer_params = num_layers * layer_params\n\n    # Output head\n    output_params = embed_dim * vocab_size\n\n    total = embedding_params + total_layer_params + output_params\n    return total / 1e9  # Billions\n\n# Example: LLaMA-7B-like\nparams_b = estimate_params(\n    vocab_size=32000,\n    embed_dim=4096,\n    num_layers=32,\n    hidden_dim=11008,\n    num_heads=32\n)\nprint(f\"Estimated parameters: {params_b:.1f}B\")\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Understanding model architecture for fine-tuning\n- Debugging attention patterns\n- Selecting target modules for LoRA\n- Estimating model size and memory\n- Building custom transformer components\n\n## Cross-References\n\n- `bazzite-ai-jupyter:finetuning` - Fine-tuning transformers\n- `bazzite-ai-jupyter:sft` - SFT with thinking models\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:peft` - Parameter-efficient tuning\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              },
              {
                "name": "vision",
                "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n",
                "path": "bazzite-ai-jupyter/skills/vision/SKILL.md",
                "frontmatter": {
                  "name": "vision",
                  "description": "Vision model fine-tuning with FastVisionModel. Covers Pixtral, Ministral VL training,\nUnslothVisionDataCollator, image+text datasets, and vision-specific LoRA configuration.\n"
                },
                "content": "# Vision Model Fine-Tuning\n\n## Overview\n\nUnsloth provides `FastVisionModel` for fine-tuning vision-language models (VLMs) like Pixtral and Ministral with 2x faster training. This skill covers vision model loading, dataset preparation with images, and vision-specific LoRA configuration.\n\n## Quick Reference\n\n| Component | Purpose |\n|-----------|---------|\n| `FastVisionModel` | Load vision models with Unsloth optimizations |\n| `UnslothVisionDataCollator` | Handle image+text modality in batches |\n| `finetune_vision_layers` | Enable training of vision encoder |\n| `finetune_language_layers` | Enable training of language model |\n| `skip_prepare_dataset=True` | Required for vision datasets |\n| `dataset_text_field=\"\"` | Empty string for vision (not a field name) |\n| List dataset format | Use `[convert(s) for s in dataset]`, not `.map()` |\n\n## Critical Environment Setup\n\n```python\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Force text-based progress in Jupyter\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n```\n\n## Critical Import Order\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\n\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nimport torch\n```\n\n## Supported Vision Models\n\n| Model | Path | Parameters | Best For |\n|-------|------|------------|----------|\n| Pixtral-12B | `unsloth/pixtral-12b-2409-bnb-4bit` | 12.7B | High-quality vision tasks |\n| Ministral-8B-Vision | `unsloth/Ministral-8B-Vision-2507-bnb-4bit` | 8B | Balanced quality/speed |\n| Llama-3.2-11B-Vision | `unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit` | 11B | General vision tasks |\n\n## Load Vision Model\n\n```python\nfrom unsloth import FastVisionModel, is_bf16_supported\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\n\nprint(f\"Model loaded: {type(model).__name__}\")\nprint(f\"Tokenizer: {type(tokenizer).__name__}\")\n```\n\n## Vision-Specific LoRA Configuration\n\nVision models require special LoRA flags to enable training of vision encoder layers:\n\n```python\nmodel = FastVisionModel.get_peft_model(\n    model,\n    # Vision-specific flags\n    finetune_vision_layers=True,      # Train vision encoder\n    finetune_language_layers=True,    # Train language model\n    finetune_attention_modules=True,  # Train attention layers\n    finetune_mlp_modules=True,        # Train MLP/FFN layers\n\n    # Standard LoRA parameters\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Check trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n```\n\n### LoRA Flag Combinations\n\n| Use Case | vision_layers | language_layers | attention | mlp |\n|----------|--------------|-----------------|-----------|-----|\n| Full fine-tune | True | True | True | True |\n| Vision only | True | False | True | True |\n| Language only | False | True | True | True |\n| Minimal | False | True | True | False |\n\n## Dataset Format\n\nVision datasets require messages with multi-modal content containing both text and images.\n\n### Image + Text Format\n\n```python\nfrom datasets import Dataset\nfrom PIL import Image\n\n# Sample dataset structure\nsamples = [\n    {\n        \"image\": Image.open(\"equation1.png\"),\n        \"instruction\": \"Convert this equation to LaTeX.\",\n        \"response\": \"\\\\frac{d}{dx} x^2 = 2x\"\n    },\n    {\n        \"image\": Image.open(\"equation2.png\"),\n        \"instruction\": \"What does this equation represent?\",\n        \"response\": \"This is the quadratic formula: x = \\\\frac{-b \\\\pm \\\\sqrt{b^2-4ac}}{2a}\"\n    },\n]\n\ndataset = Dataset.from_list(samples)\n```\n\n### Converting to Chat Format\n\n```python\ndef convert_to_vision_conversation(sample):\n    \"\"\"Convert sample to vision chat format with image content.\"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"instruction\"]},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"response\"]}\n            ]\n        }\n    ]\n    return {\"messages\": messages}\n\n# Apply conversion\nconverted_dataset = dataset.map(convert_to_vision_conversation)\n```\n\n### Using HuggingFace Datasets\n\n**Important**: Use list comprehension, NOT `.map()` for vision datasets:\n\n```python\nfrom datasets import load_dataset\n\n# Load LaTeX OCR dataset from HuggingFace (via Unsloth mirror)\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:100]\")\n\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    \"\"\"Format sample for vision training.\"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: Use list comprehension, NOT .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\n```\n\n**Why list format?** Vision datasets with PIL images work more reliably as plain Python lists than HuggingFace Dataset objects with `.map()`.\n\n## Vision Data Collator\n\nThe `UnslothVisionDataCollator` handles image+text batching:\n\n```python\nfrom unsloth.trainer import UnslothVisionDataCollator\n\ndata_collator = UnslothVisionDataCollator(model, tokenizer)\n```\n\n## Training Configuration\n\nVision training requires specific SFTConfig settings:\n\n```python\nfrom trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./vision_output\",\n    per_device_train_batch_size=1,      # Keep low for large vision models\n    gradient_accumulation_steps=4,       # Effective batch size = 4\n    max_steps=100,                       # Or num_train_epochs=1\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n\n    # Precision settings\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n\n    # Optimizer\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n\n    # Sequence length\n    max_seq_length=1024,\n\n    # CRITICAL for vision - all 3 are required\n    remove_unused_columns=False,         # Keep image column\n    dataset_text_field=\"\",               # Empty string (NOT a field name)\n    dataset_kwargs={\"skip_prepare_dataset\": True},  # Required for vision\n\n    # Other\n    seed=3407,\n    report_to=\"none\",\n)\n```\n\n**Critical settings explained:**\n- `remove_unused_columns=False`: Preserves image column during training\n- `dataset_text_field=\"\"`: Empty string tells TRL to use the messages format\n- `skip_prepare_dataset=True`: Prevents TRL from processing vision data incorrectly\n\n## SFTTrainer for Vision\n\n```python\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\n# Train\ntrainer_stats = trainer.train()\n\nprint(f\"Training completed!\")\nprint(f\"Final loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Complete Training Example\n\nThis example matches the tested notebook pattern:\n\n```python\n# 1. Environment Setup\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nos.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n\n# 2. Imports (unsloth FIRST)\nimport unsloth\nfrom unsloth import FastVisionModel, is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\n# 3. Load model\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/pixtral-12b-2409-bnb-4bit\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\nprint(f\"Model loaded: {type(model).__name__}\")\n\n# 4. Apply LoRA\nmodel = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers=True,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=3407,\n)\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"LoRA applied ({trainable:,} trainable params)\")\n\n# 5. Prepare dataset (use LIST, not .map())\ndataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:50]\")\ninstruction = \"Write the LaTeX representation for this image.\"\n\ndef convert_to_conversation(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]}\n        ]\n    }\n\n# CRITICAL: List comprehension, not .map()\nconverted_dataset = [convert_to_conversation(s) for s in dataset]\nprint(f\"Dataset loaded ({len(converted_dataset)} samples)\")\n\n# 6. Configure training\nsft_config = SFTConfig(\n    output_dir=\"./vision_lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=50,\n    warmup_steps=5,\n    learning_rate=2e-4,\n    logging_steps=1,\n    fp16=not is_bf16_supported(),\n    bf16=is_bf16_supported(),\n    optim=\"adamw_8bit\",\n    max_seq_length=1024,\n    # CRITICAL for vision - all 3 required\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    seed=3407,\n    report_to=\"none\",\n)\n\n# 7. Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=sft_config,\n)\n\ntrainer_stats = trainer.train()\nprint(f\"Training complete! Loss: {trainer_stats.metrics.get('train_loss', 'N/A'):.4f}\")\n```\n\n## Inference with Vision Models\n\n### Prepare for Inference\n\n```python\nFastVisionModel.for_inference(model)\n```\n\n### Generate from Image\n\n```python\nfrom PIL import Image\n\n# Load test image\ntest_image = Image.open(\"test_equation.png\")\n\n# Format as conversation\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Convert this to LaTeX:\"},\n            {\"type\": \"image\", \"image\": test_image}\n        ]\n    }\n]\n\n# Apply chat template\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs,\n    max_new_tokens=256,\n    temperature=0.1,      # Low for accurate transcription\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\n# Decode\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n\n### Batch Inference\n\n```python\nfrom PIL import Image\n\nimages = [Image.open(f\"image_{i}.png\") for i in range(3)]\nprompts = [\"Describe this image.\", \"What objects are in this image?\", \"Transcribe the text.\"]\n\nfor img, prompt in zip(images, prompts):\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\"type\": \"image\", \"image\": img}\n        ]}\n    ]\n\n    inputs = tokenizer.apply_chat_template(\n        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(model.device)\n\n    outputs = model.generate(input_ids=inputs, max_new_tokens=128)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Save and Load\n\n### Save LoRA Adapter\n\n```python\n# Save only LoRA weights (~66MB for Pixtral)\nmodel.save_pretrained(\"./vision_lora\")\ntokenizer.save_pretrained(\"./vision_lora\")\n```\n\n### Save Merged Model\n\n```python\n# Save full merged model (large)\nmodel.save_pretrained_merged(\n    \"./vision_merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",\n)\n```\n\n### Load for Inference\n\n```python\nfrom unsloth import FastVisionModel\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"./vision_lora\",\n    load_in_4bit=True,\n)\nFastVisionModel.for_inference(model)\n```\n\n## Memory Requirements\n\n| Model | 4-bit VRAM | Training VRAM |\n|-------|------------|---------------|\n| Pixtral-12B | ~8GB | ~12GB |\n| Ministral-8B-Vision | ~6GB | ~10GB |\n| Llama-3.2-11B-Vision | ~7GB | ~11GB |\n\n## Troubleshooting\n\n### Image Not Processed\n\n**Symptom:** Model ignores image content\n\n**Fix:**\n- Ensure `remove_unused_columns=False` in SFTConfig\n- Use `skip_prepare_dataset=True` in dataset_kwargs\n- Verify image is PIL.Image object, not path string\n\n### Out of Memory\n\n**Symptom:** CUDA OOM during vision training\n\n**Fix:**\n- Reduce `per_device_train_batch_size` to 1\n- Increase `gradient_accumulation_steps`\n- Use smaller model (Ministral-8B instead of Pixtral-12B)\n- Enable gradient checkpointing\n\n### Poor Generation Quality\n\n**Symptom:** Model outputs nonsense for images\n\n**Fix:**\n- Increase training steps (50-100+)\n- Check dataset quality (image-text alignment)\n- Use lower learning rate (1e-4)\n- Ensure vision layers are being trained (`finetune_vision_layers=True`)\n\n### Data Collator Error\n\n**Symptom:** `KeyError` or shape mismatch in data collator\n\n**Fix:**\n- Use `UnslothVisionDataCollator(model, tokenizer)`\n- Ensure dataset has \"messages\" field with correct structure\n- Check that images are valid PIL.Image objects\n\n## Kernel Shutdown (Jupyter)\n\nVision models use significant GPU memory. Shutdown kernel to release memory:\n\n```python\nimport IPython\nprint(\"Shutting down kernel to release GPU memory...\")\napp = IPython.Application.instance()\napp.kernel.do_shutdown(restart=False)\n```\n\n**Important**: Always run this at the end of training notebooks before switching to different models.\n\n## Use Cases\n\n- **OCR/Document Processing**: LaTeX equation recognition, receipt scanning\n- **Image Captioning**: Generate descriptions for images\n- **Visual QA**: Answer questions about image content\n- **Chart/Graph Analysis**: Extract data from visualizations\n- **Medical Imaging**: X-ray, scan analysis (with appropriate data)\n\n## When to Use This Skill\n\nUse when:\n- Fine-tuning models to understand images\n- Building OCR or document processing pipelines\n- Creating image captioning systems\n- Developing visual question-answering applications\n\n## Cross-References\n\n- `bazzite-ai-jupyter:sft` - Standard SFT for text-only models\n- `bazzite-ai-jupyter:peft` - LoRA configuration details\n- `bazzite-ai-jupyter:inference` - Fast inference patterns\n- `bazzite-ai-jupyter:quantization` - Memory optimization"
              }
            ]
          },
          {
            "name": "bazzite-ai-ollama",
            "description": "Ollama API operations for LLM inference, embeddings, and model management",
            "source": "./bazzite-ai-ollama",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "atrawog"
            },
            "install_commands": [
              "/plugin marketplace add atrawog/bazzite-ai-plugins",
              "/plugin install bazzite-ai-ollama@bazzite-ai-plugins"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T13:36:51Z",
              "created_at": "2025-12-26T20:21:42Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "api",
                "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/api/SKILL.md",
                "frontmatter": {
                  "name": "api",
                  "description": "Direct REST API operations for Ollama using the requests library.\nCovers all /api/* endpoints for model management, text generation,\nchat completion, embeddings, and streaming responses.\n"
                },
                "content": "# Ollama REST API\n\n## Overview\n\nThe Ollama REST API provides direct HTTP access to all Ollama functionality. Use the `requests` library for maximum control over API interactions.\n\n**Default Endpoint:** `http://localhost:11434` (or `http://ollama:11434` in containers)\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/api/tags` | GET | List available models |\n| `/api/show` | POST | Show model details |\n| `/api/ps` | GET | List running models |\n| `/api/generate` | POST | Generate text |\n| `/api/chat` | POST | Chat completion |\n| `/api/embed` | POST | Generate embeddings |\n| `/api/copy` | POST | Copy a model |\n| `/api/delete` | DELETE | Delete a model |\n\n## Setup\n\n```python\nimport os\nimport requests\nimport json\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n```\n\n## List Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/tags\")\nmodels = response.json()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/show\",\n    json={\"model\": \"llama3.2:latest\"}\n)\nmodel_info = response.json()\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Why is the sky blue?\",\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Count from 1 to 5.\",\n        \"stream\": True\n    },\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line:\n        chunk = json.loads(line)\n        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n        if chunk.get(\"done\"):\n            break\n```\n\n## Chat Completion\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/chat\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ],\n        \"stream\": False\n    }\n)\nresult = response.json()\nprint(result[\"message\"][\"content\"])\n```\n\n## Generate Embeddings\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/embed\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"input\": \"Ollama makes running LLMs locally easy.\"\n    }\n)\nresult = response.json()\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\n```\n\n## Copy Model\n\n```python\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/copy\",\n    json={\n        \"source\": \"llama3.2:latest\",\n        \"destination\": \"llama3.2-backup:latest\"\n    }\n)\nif response.status_code == 200:\n    print(\"Copy successful!\")\n```\n\n## Delete Model\n\n```python\nresponse = requests.delete(\n    f\"{OLLAMA_HOST}/api/delete\",\n    json={\"model\": \"llama3.2-backup:latest\"}\n)\nif response.status_code == 200:\n    print(\"Delete successful!\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = requests.post(\n        f\"{OLLAMA_HOST}/api/generate\",\n        json={\"model\": \"nonexistent\", \"prompt\": \"Hello\"},\n        timeout=30\n    )\n    if response.status_code != 200:\n        print(f\"Error: {response.status_code} - {response.text}\")\n    else:\n        result = response.json()\n        if \"error\" in result:\n            print(f\"API Error: {result['error']}\")\nexcept requests.exceptions.ConnectionError:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\nThe generate endpoint returns useful metrics:\n\n```python\nresult = response.json()\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## When to Use This Skill\n\nUse when:\n- You need direct control over HTTP requests\n- Debugging API interactions\n- Building custom integrations\n- Working with streaming responses\n- Checking raw API responses\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Higher-level Python library\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              },
              {
                "name": "gpu",
                "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n",
                "path": "bazzite-ai-ollama/skills/gpu/SKILL.md",
                "frontmatter": {
                  "name": "gpu",
                  "description": "GPU monitoring and performance metrics for Ollama inference. Check GPU\nstatus, VRAM usage, loaded models, and inference performance metrics\nlike tokens per second.\n"
                },
                "content": "# GPU Monitoring for Ollama\n\n## Overview\n\nMonitor GPU usage and performance when running Ollama with GPU acceleration. This skill covers checking GPU status, VRAM usage, models loaded in GPU memory, and inference performance metrics.\n\n## Quick Reference\n\n| Check | Method |\n|-------|--------|\n| GPU status | `nvidia-smi` / `rocm-smi` |\n| Models in memory | `GET /api/ps` |\n| Inference metrics | Response metadata |\n| VRAM usage | Both nvidia-smi and /api/ps |\n\n## GPU Status Check\n\n### NVIDIA\n\n```python\nimport subprocess\n\ndef check_nvidia_gpu():\n    \"\"\"Check NVIDIA GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.used,memory.total,utilization.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            lines = result.stdout.strip().split(\"\\n\")\n            for i, line in enumerate(lines):\n                parts = line.split(\", \")\n                if len(parts) >= 4:\n                    name, mem_used, mem_total, util = parts\n                    print(f\"GPU {i}: {name}\")\n                    print(f\"  Memory: {mem_used} MB / {mem_total} MB\")\n                    print(f\"  Utilization: {util}%\")\n    except FileNotFoundError:\n        print(\"nvidia-smi not found - NVIDIA GPU may not be available\")\n    except subprocess.TimeoutExpired:\n        print(\"nvidia-smi timed out\")\n\ncheck_nvidia_gpu()\n```\n\n### AMD\n\n```python\nimport subprocess\n\ndef check_amd_gpu():\n    \"\"\"Check AMD GPU status.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"rocm-smi\", \"--showmeminfo\", \"vram\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        print(result.stdout)\n    except FileNotFoundError:\n        print(\"rocm-smi not found - AMD GPU may not be available\")\n\ncheck_amd_gpu()\n```\n\n## Models Loaded in GPU Memory\n\n```python\nimport os\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\nrunning = response.json()\n\nif running.get(\"models\"):\n    print(\"=== Models Loaded in GPU Memory ===\")\n    for model in running[\"models\"]:\n        name = model.get(\"name\", \"Unknown\")\n        size = model.get(\"size\", 0) / (1024**3)\n        vram = model.get(\"size_vram\", 0) / (1024**3)\n        expires = model.get(\"expires_at\", \"N/A\")\n        print(f\"  - {name}\")\n        print(f\"    Total Size: {size:.2f} GB\")\n        print(f\"    VRAM Usage: {vram:.2f} GB\")\n        print(f\"    Expires: {expires}\")\nelse:\n    print(\"No models currently loaded in memory\")\n```\n\n## Inference Performance Metrics\n\n```python\nimport os\nimport time\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\n# Run inference\nstart_time = time.perf_counter()\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a haiku about computers.\",\n        \"stream\": False\n    }\n)\nend_time = time.perf_counter()\n\nresult = response.json()\n\nprint(f\"Response: {result['response']}\")\nprint()\nprint(\"=== Inference Metrics ===\")\nprint(f\"Wall clock time: {end_time - start_time:.2f}s\")\nprint(f\"Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\nprint(f\"Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Eval count (tokens generated): {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\nprint(f\"Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## GPU Usage During Inference\n\n```python\nimport os\nimport subprocess\nimport requests\nimport threading\nimport time\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\ndef monitor_gpu(stop_event, readings):\n    \"\"\"Monitor GPU usage in background.\"\"\"\n    while not stop_event.is_set():\n        try:\n            result = subprocess.run(\n                [\"nvidia-smi\",\n                 \"--query-gpu=utilization.gpu,memory.used\",\n                 \"--format=csv,noheader,nounits\"],\n                capture_output=True,\n                text=True,\n                timeout=1\n            )\n            if result.returncode == 0:\n                parts = result.stdout.strip().split(\", \")\n                if len(parts) >= 2:\n                    readings.append({\n                        \"util\": int(parts[0]),\n                        \"mem\": int(parts[1])\n                    })\n        except:\n            pass\n        time.sleep(0.5)\n\n# Start monitoring\nstop_event = threading.Event()\nreadings = []\nmonitor_thread = threading.Thread(target=monitor_gpu, args=(stop_event, readings))\nmonitor_thread.start()\n\n# Run inference\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/generate\",\n    json={\n        \"model\": \"llama3.2:latest\",\n        \"prompt\": \"Write a short story about AI.\",\n        \"stream\": False\n    }\n)\n\n# Stop monitoring\nstop_event.set()\nmonitor_thread.join()\n\n# Report\nif readings:\n    avg_util = sum(r[\"util\"] for r in readings) / len(readings)\n    max_mem = max(r[\"mem\"] for r in readings)\n    print(f\"Average GPU utilization: {avg_util:.1f}%\")\n    print(f\"Peak memory usage: {max_mem} MB\")\n```\n\n## Complete Health Check\n\n```python\nimport os\nimport subprocess\nimport requests\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\nDEFAULT_MODEL = \"llama3.2:latest\"\n\ndef complete_gpu_health_check():\n    \"\"\"Complete GPU and Ollama health check.\"\"\"\n    print(\"=== GPU Health Check ===\")\n    print()\n\n    # 1. Check GPU hardware\n    print(\"1. GPU Hardware:\")\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\",\n             \"--query-gpu=name,memory.total\",\n             \"--format=csv,noheader\"],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            print(f\"   {result.stdout.strip()}\")\n        else:\n            print(\"   nvidia-smi failed\")\n    except FileNotFoundError:\n        print(\"   NVIDIA GPU not detected\")\n\n    # 2. Check Ollama server\n    print()\n    print(\"2. Ollama Server:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            print(\"   Server is running\")\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            if DEFAULT_MODEL in model_names:\n                print(f\"   Model '{DEFAULT_MODEL}' available\")\n            else:\n                print(f\"   Model '{DEFAULT_MODEL}' NOT available\")\n        else:\n            print(f\"   Server error: {response.status_code}\")\n    except requests.exceptions.ConnectionError:\n        print(\"   Cannot connect to server\")\n\n    # 3. Check models in GPU memory\n    print()\n    print(\"3. Models in GPU Memory:\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n        running = response.json()\n        if running.get(\"models\"):\n            for model in running[\"models\"]:\n                vram = model.get(\"size_vram\", 0) / (1024**3)\n                print(f\"   {model['name']}: {vram:.2f} GB VRAM\")\n        else:\n            print(\"   No models loaded\")\n    except:\n        print(\"   Cannot check running models\")\n\ncomplete_gpu_health_check()\n```\n\n## Model Size Guide\n\n| Model | Parameters | VRAM Needed | Tokens/sec (typical) |\n|-------|------------|-------------|----------------------|\n| phi3 | 3B | 4GB | 60-80 |\n| llama3.2 | 8B | 8GB | 40-60 |\n| mistral | 7B | 8GB | 40-60 |\n| codellama | 7B | 8GB | 40-60 |\n| llama3.2:70b | 70B | 48GB+ | 10-20 |\n\n## Troubleshooting\n\n### GPU Not Used\n\n**Symptom:** Low tokens/second, nvidia-smi shows 0% utilization\n\n**Check:**\n\n```bash\n# Check GPU inside container (adjust container name as needed)\ndocker exec -it ollama nvidia-smi\n# or\npodman exec -it ollama nvidia-smi\n```\n\n**Fix:**\n\n```bash\n# Restart Ollama container with GPU access\n# Refer to bazzite-ai-pod-ollama documentation for container setup\n```\n\n### Out of Memory\n\n**Symptom:** \"out of memory\" error during model loading\n\n**Fix:**\n\n```python\n# Use smaller/quantized model via API\nimport requests\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nresponse = requests.post(\n    f\"{OLLAMA_HOST}/api/pull\",\n    json={\"name\": \"llama3.2:7b-q4_0\"},\n    stream=True\n)\nfor line in response.iter_lines():\n    if line:\n        print(line.decode())\n```\n\n### Slow Inference\n\n**Symptom:** Very low tokens/second\n\n**Possible causes:**\n1. Model too large for VRAM (using CPU fallback)\n2. Wrong GPU type configured\n3. Driver issues\n\n**Check:**\n\n```python\n# Check VRAM usage vs model size\nresponse = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n# If size_vram << size, model is partially on CPU\n```\n\n## When to Use This Skill\n\nUse when:\n- Debugging slow inference\n- Checking if GPU is being utilized\n- Monitoring VRAM usage\n- Benchmarking different models\n- Troubleshooting GPU issues\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - API for running inference\n- `bazzite-ai-ollama:python` - Python library for inference"
              },
              {
                "name": "huggingface",
                "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n",
                "path": "bazzite-ai-ollama/skills/huggingface/SKILL.md",
                "frontmatter": {
                  "name": "huggingface",
                  "description": "Import GGUF models from HuggingFace into Ollama. Pull models directly\nusing the hf.co/ prefix, track download progress, and use imported\nmodels for inference.\n"
                },
                "content": "# HuggingFace Model Import\n\n## Overview\n\nOllama can directly pull GGUF models from HuggingFace using the `hf.co/` prefix. This enables access to thousands of quantized models beyond the official Ollama library.\n\n## Quick Reference\n\n| Action | Syntax |\n|--------|--------|\n| Pull model | `hf.co/{org}/{repo}:{quantization}` |\n| List models | `ollama.list()` |\n| Use model | Same as any Ollama model |\n| Delete model | `ollama.delete(\"hf.co/...\")` |\n\n## Model Naming Format\n\n```\nhf.co/{organization}/{repository}-GGUF:{quantization}\n```\n\n**Examples:**\n\n```\nhf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\nhf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M\nhf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M\n```\n\n## Common Quantizations\n\n| Quantization | Size | Quality | Use Case |\n|--------------|------|---------|----------|\n| Q2_K | Smallest | Lowest | Testing only |\n| Q4_K_M | Medium | Good | Recommended default |\n| Q5_K_M | Larger | Better | Quality-focused |\n| Q6_K | Large | High | Near-original quality |\n| Q8_0 | Largest | Highest | Maximum quality |\n\n## Pull Model from HuggingFace\n\n### With Progress Tracking\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nprint(f\"Pulling {HF_MODEL}...\")\n\nlast_status = \"\"\nfor progress in ollama.pull(HF_MODEL, stream=True):\n    status = progress.get(\"status\", \"\")\n    digest = progress.get(\"digest\", \"\")\n    total = progress.get(\"total\")\n\n    # Only print when status changes\n    if status != last_status:\n        if status == \"pulling manifest\":\n            print(f\"  {status}\")\n        elif status.startswith(\"pulling\") and digest:\n            short_digest = digest.split(\":\")[-1][:12] if \":\" in digest else digest[:12]\n            size_mb = (total / 1024 / 1024) if total else 0\n            if size_mb > 100:\n                print(f\"  pulling {short_digest}... ({size_mb:.0f} MB)\")\n        elif status in [\"verifying sha256 digest\", \"writing manifest\", \"success\"]:\n            print(f\"  {status}\")\n\n        last_status = status\n\nprint(\"Model pulled successfully!\")\n```\n\n### Simple Pull\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Non-streaming (blocks until complete)\nollama.pull(HF_MODEL)\nprint(\"Model pulled!\")\n```\n\n## Verify Installation\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodels = ollama.list()\nmodel_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n\n# Check for the HF model\nhf_model_installed = any(\n    \"Nous-Hermes\" in name or HF_MODEL in name\n    for name in model_names\n)\n\nif hf_model_installed:\n    print(\"Model is installed!\")\n    for name in model_names:\n        if \"Nous-Hermes\" in name or \"hf.co\" in name:\n            print(f\"  Name: {name}\")\nelse:\n    print(\"Model not found\")\n```\n\n## Show Model Details\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nmodel_info = ollama.show(HF_MODEL)\n\nprint(f\"Model: {HF_MODEL}\")\nif \"details\" in model_info:\n    details = model_info[\"details\"]\n    print(f\"Family: {details.get('family', 'N/A')}\")\n    print(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\n    print(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## Use Imported Model\n\n### Generate Text\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nresult = ollama.generate(\n    model=HF_MODEL,\n    prompt=\"What is the capital of France?\"\n)\nprint(result[\"response\"])\n```\n\n### Chat Completion\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\n# Nous-Hermes-2 uses ChatML format natively\nresponse = ollama.chat(\n    model=HF_MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Hermes 2, a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in two sentences.\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n## Delete Imported Model\n\n```python\nimport ollama\n\nHF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n\nollama.delete(HF_MODEL)\nprint(\"Model deleted!\")\n```\n\n## Popular HuggingFace Models\n\n### General Purpose\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Nous-Hermes-2-Mistral | `hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M` | 4.4 GB |\n| Llama-2-7B-Chat | `hf.co/TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M` | 4.1 GB |\n| Mistral-7B-Instruct | `hf.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF:Q4_K_M` | 4.4 GB |\n\n### Code Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| CodeLlama-7B | `hf.co/TheBloke/CodeLlama-7B-Instruct-GGUF:Q4_K_M` | 4.1 GB |\n| Phind-CodeLlama | `hf.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF:Q4_K_M` | 20 GB |\n| WizardCoder | `hf.co/TheBloke/WizardCoder-Python-7B-V1.0-GGUF:Q4_K_M` | 4.1 GB |\n\n### Small/Fast Models\n\n| Model | HuggingFace Path | Size |\n|-------|------------------|------|\n| Phi-3-mini | `hf.co/microsoft/Phi-3-mini-4k-instruct-gguf:Q4_K_M` | 2.4 GB |\n| TinyLlama | `hf.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q4_K_M` | 0.7 GB |\n\n## Finding Models on HuggingFace\n\n1. Go to [huggingface.co/models](https://huggingface.co/models)\n2. Filter by:\n   - **Library:** GGUF\n   - **Task:** Text Generation\n3. Look for models with `-GGUF` suffix\n4. Check the \"Files\" tab for available quantizations\n\n## Troubleshooting\n\n### Model Not Found\n\n**Symptom:** Error pulling model\n\n**Check:**\n- Repository exists on HuggingFace\n- Repository has GGUF files\n- Quantization tag is correct\n\n```python\n# Verify HuggingFace URL\n# https://huggingface.co/{org}/{repo}/tree/main\n```\n\n### Download Fails\n\n**Symptom:** Download interrupted or fails\n\n**Fix:**\n- Check internet connection\n- Try again (Ollama resumes partial downloads)\n- Check disk space\n\n### Wrong Prompt Format\n\n**Symptom:** Model gives poor responses\n\n**Fix:**\n- Check model card for correct prompt template\n- Some models require specific formats (ChatML, Alpaca, etc.)\n\n```python\n# ChatML format example (Nous-Hermes-2)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\n# The ollama library handles format conversion automatically\n```\n\n## When to Use This Skill\n\nUse when:\n- You need a model not in the official Ollama library\n- Testing specific model variants\n- Using specialized/fine-tuned models\n- Comparing different quantizations\n\n## Resources\n\n- [Ollama Import Docs](https://docs.ollama.com/import)\n- [HuggingFace Ollama Integration](https://huggingface.co/docs/hub/ollama)\n- [TheBloke's GGUF Models](https://huggingface.co/TheBloke)\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Using imported models\n- `bazzite-ai-ollama:api` - REST API for model management"
              },
              {
                "name": "openai",
                "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n",
                "path": "bazzite-ai-ollama/skills/openai/SKILL.md",
                "frontmatter": {
                  "name": "openai",
                  "description": "OpenAI compatibility layer for Ollama. Use the official OpenAI Python\nlibrary to interact with Ollama, enabling easy migration from OpenAI\nand compatibility with LangChain, LlamaIndex, and other OpenAI-based tools.\n"
                },
                "content": "# Ollama OpenAI Compatibility\n\n## Overview\n\nOllama provides an OpenAI-compatible API at `/v1/*` endpoints. This allows using the official `openai` Python library with Ollama, enabling:\n\n- **Migration** - Drop-in replacement for OpenAI API\n- **Tool ecosystem** - Works with LangChain, LlamaIndex, etc.\n- **Familiar interface** - Standard OpenAI patterns\n\n## Quick Reference\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/v1/models` | GET | List models |\n| `/v1/completions` | POST | Text generation |\n| `/v1/chat/completions` | POST | Chat completion |\n| `/v1/embeddings` | POST | Generate embeddings |\n\n### Limitations\n\nThe OpenAI compatibility layer does **not** support:\n\n- Show model details (`/api/show`)\n- List running models (`/api/ps`)\n- Copy model (`/api/copy`)\n- Delete model (`/api/delete`)\n\nUse `bazzite-ai-ollama:api` or `bazzite-ai-ollama:python` for these operations.\n\n## Setup\n\n```python\nimport os\nfrom openai import OpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n\nclient = OpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\"  # Required by library but ignored by Ollama\n)\n```\n\n## List Models\n\n```python\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"  - {model.id}\")\n```\n\n## Text Completions\n\n```python\nresponse = client.completions.create(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\",\n    max_tokens=100\n)\n\nprint(response.choices[0].text)\nprint(f\"Tokens used: {response.usage.completion_tokens}\")\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n    ],\n    temperature=0.7,\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\nprint(f\"Tokens used: {response.usage.total_tokens}\")\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}\n]\n\n# Turn 1\nmessages.append({\"role\": \"user\", \"content\": \"What is 2 + 2?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nassistant_msg = response.choices[0].message.content\nmessages.append({\"role\": \"assistant\", \"content\": assistant_msg})\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {assistant_msg}\")\n\n# Turn 2\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=messages,\n    max_tokens=50\n)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response.choices[0].message.content}\")\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n## Error Handling\n\n```python\ntry:\n    response = client.chat.completions.create(\n        model=\"invalid-model\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n```\n\n## Migration from OpenAI\n\n### Before (OpenAI)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()  # Uses OPENAI_API_KEY\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### After (Ollama)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2:latest\",  # Change model name\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain Integration\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.invoke(\"What is Python?\")\nprint(response.content)\n```\n\n## LlamaIndex Integration\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(\n    api_base=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n    model=\"llama3.2:latest\"\n)\n\nresponse = llm.complete(\"What is Python?\")\nprint(response.text)\n```\n\n## Connection Health Check\n\n```python\nimport requests\n\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n    try:\n        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models = response.json()\n            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n            return True, model in model_names\n        return False, False\n    except requests.exceptions.RequestException:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## When to Use This Skill\n\nUse when:\n\n- Migrating from OpenAI to local LLMs\n- Using LangChain, LlamaIndex, or other OpenAI-based tools\n- You prefer the OpenAI client interface\n- Building applications that may switch between OpenAI and Ollama\n\n## Cross-References\n\n- `bazzite-ai-ollama:python` - Native Ollama library (more features)\n- `bazzite-ai-ollama:api` - Direct REST API access"
              },
              {
                "name": "python",
                "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n",
                "path": "bazzite-ai-ollama/skills/python/SKILL.md",
                "frontmatter": {
                  "name": "python",
                  "description": "Official ollama Python library for LLM inference. Provides a clean,\nPythonic interface for text generation, chat completion, embeddings,\nmodel management, and streaming responses.\n"
                },
                "content": "# Ollama Python Library\n\n## Overview\n\nThe official `ollama` Python library provides a clean, Pythonic interface to all Ollama functionality. It automatically connects to the Ollama server and handles serialization.\n\n## Quick Reference\n\n| Function | Purpose |\n|----------|---------|\n| `ollama.list()` | List available models |\n| `ollama.show()` | Show model details |\n| `ollama.ps()` | List running models |\n| `ollama.generate()` | Generate text |\n| `ollama.chat()` | Chat completion |\n| `ollama.embed()` | Generate embeddings |\n| `ollama.copy()` | Copy a model |\n| `ollama.delete()` | Delete a model |\n| `ollama.pull()` | Pull a model |\n\n## Setup\n\n```python\nimport ollama\n\n# The library automatically uses OLLAMA_HOST environment variable\n# Default: http://localhost:11434\n```\n\n## List Models\n\n```python\nmodels = ollama.list()\n\nfor model in models.get(\"models\", []):\n    size_gb = model.get(\"size\", 0) / (1024**3)\n    print(f\"  - {model['model']} ({size_gb:.2f} GB)\")\n```\n\n## Show Model Details\n\n```python\nmodel_info = ollama.show(\"llama3.2:latest\")\n\ndetails = model_info.get(\"details\", {})\nprint(f\"Family: {details.get('family', 'N/A')}\")\nprint(f\"Parameter Size: {details.get('parameter_size', 'N/A')}\")\nprint(f\"Quantization: {details.get('quantization_level', 'N/A')}\")\n```\n\n## List Running Models\n\n```python\nrunning = ollama.ps()\n\nfor model in running.get(\"models\", []):\n    name = model.get(\"name\", \"Unknown\")\n    size = model.get(\"size\", 0) / (1024**3)\n    vram = model.get(\"size_vram\", 0) / (1024**3)\n    print(f\"  - {name}: {size:.2f} GB (VRAM: {vram:.2f} GB)\")\n```\n\n## Generate Text\n\n### Non-Streaming\n\n```python\nresult = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Why is the sky blue? Answer in one sentence.\"\n)\nprint(result[\"response\"])\n```\n\n### Streaming\n\n```python\nstream = ollama.generate(\n    model=\"llama3.2:latest\",\n    prompt=\"Count from 1 to 5.\",\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"response\"], end=\"\", flush=True)\n```\n\n## Chat Completion\n\n### Single Turn\n\n```python\nresponse = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is Python?\"}\n    ]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n### Multi-Turn Conversation\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n]\n\n# First turn\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: What is 2 + 2?\")\nprint(f\"Assistant: {response['message']['content']}\")\n\n# Continue conversation\nmessages.append(response[\"message\"])\nmessages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n\nresponse = ollama.chat(model=\"llama3.2:latest\", messages=messages)\nprint(f\"User: And what is that multiplied by 3?\")\nprint(f\"Assistant: {response['message']['content']}\")\n```\n\n### Streaming Chat\n\n```python\nstream = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n```\n\n## Generate Embeddings\n\n```python\nresult = ollama.embed(\n    model=\"llama3.2:latest\",\n    input=\"Ollama makes running LLMs locally easy.\"\n)\n\nembeddings = result.get(\"embeddings\", [[]])[0]\nprint(f\"Dimensions: {len(embeddings)}\")\nprint(f\"First 5 values: {embeddings[:5]}\")\n```\n\n## Model Management\n\n### Copy Model\n\n```python\nollama.copy(source=\"llama3.2:latest\", destination=\"llama3.2-backup:latest\")\nprint(\"Copy successful!\")\n```\n\n### Delete Model\n\n```python\nollama.delete(\"llama3.2-backup:latest\")\nprint(\"Delete successful!\")\n```\n\n### Pull Model\n\n```python\n# Non-streaming\nollama.pull(\"llama3.2:latest\")\n\n# With progress\nfor progress in ollama.pull(\"llama3.2:latest\", stream=True):\n    status = progress.get(\"status\", \"\")\n    print(status)\n```\n\n## Error Handling\n\n```python\ntry:\n    result = ollama.generate(\n        model=\"nonexistent-model\",\n        prompt=\"Hello\"\n    )\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}: {e}\")\n\n# Connection check\ntry:\n    models = ollama.list()\n    print(\"Ollama server is running!\")\nexcept Exception as e:\n    print(\"Cannot connect to Ollama. Ensure server is running at OLLAMA_HOST\")\n```\n\n## Connection Health Check\n\n```python\ndef check_ollama_health(model=\"llama3.2:latest\"):\n    \"\"\"Check if Ollama server is running and model is available.\"\"\"\n    try:\n        models = ollama.list()\n        model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n        return True, model in model_names\n    except Exception:\n        return False, False\n\nserver_ok, model_ok = check_ollama_health()\n```\n\n## Response Metrics\n\n```python\nresult = ollama.generate(model=\"llama3.2:latest\", prompt=\"Hello!\")\n\nprint(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\nprint(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.2f}s\")\n\nif result.get('eval_count') and result.get('eval_duration'):\n    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n    print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n```\n\n## Common Patterns\n\n### Conversation Class\n\n```python\nclass Conversation:\n    def __init__(self, model=\"llama3.2:latest\", system_prompt=None):\n        self.model = model\n        self.messages = []\n        if system_prompt:\n            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n\n    def chat(self, user_message):\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n        response = ollama.chat(model=self.model, messages=self.messages)\n        assistant_message = response[\"message\"]\n        self.messages.append(assistant_message)\n        return assistant_message[\"content\"]\n\n# Usage\nconv = Conversation(system_prompt=\"You are a helpful assistant.\")\nprint(conv.chat(\"What is Python?\"))\nprint(conv.chat(\"What are its main features?\"))\n```\n\n## When to Use This Skill\n\nUse when:\n\n- You want a clean, Pythonic interface\n- Building Python applications\n- Need IDE autocompletion support\n- Working with multi-turn conversations\n- Prefer not to handle HTTP directly\n\n## Cross-References\n\n- `bazzite-ai-ollama:api` - Direct REST API access\n- `bazzite-ai-ollama:openai` - OpenAI-compatible interface"
              }
            ]
          }
        ]
      }
    }
  ]
}