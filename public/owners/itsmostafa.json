{
  "owner": {
    "id": "itsmostafa",
    "display_name": "Mostafa",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/24395434?u=fc63e2baaea264df36ba590ccf3eabd6ed2bd54c&v=4",
    "url": "https://github.com/itsmostafa",
    "bio": "I build stuff",
    "stats": {
      "total_repos": 2,
      "total_plugins": 2,
      "total_commands": 0,
      "total_skills": 27,
      "total_stars": 950,
      "total_forks": 416
    }
  },
  "repos": [
    {
      "full_name": "itsmostafa/aws-agent-skills",
      "url": "https://github.com/itsmostafa/aws-agent-skills",
      "description": "AWS Skills for Agents",
      "homepage": "",
      "signals": {
        "stars": 940,
        "forks": 416,
        "pushed_at": "2026-01-12T09:08:18Z",
        "created_at": "2019-05-18T19:38:59Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 991
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/weekly-aws-doc-check.yml",
          "type": "blob",
          "size": 3088
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 584
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1056
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 4708
        },
        {
          "path": "REFERENCES.md",
          "type": "blob",
          "size": 5880
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/check-aws-updates.py",
          "type": "blob",
          "size": 10386
        },
        {
          "path": "scripts/generate-update-issues.py",
          "type": "blob",
          "size": 4495
        },
        {
          "path": "scripts/requirements.txt",
          "type": "blob",
          "size": 47
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/api-gateway",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/api-gateway/SKILL.md",
          "type": "blob",
          "size": 8947
        },
        {
          "path": "skills/api-gateway/integration-patterns.md",
          "type": "blob",
          "size": 9246
        },
        {
          "path": "skills/bedrock",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/bedrock/SKILL.md",
          "type": "blob",
          "size": 10823
        },
        {
          "path": "skills/bedrock/model-invocation.md",
          "type": "blob",
          "size": 13261
        },
        {
          "path": "skills/cloudformation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cloudformation/SKILL.md",
          "type": "blob",
          "size": 9819
        },
        {
          "path": "skills/cloudformation/template-patterns.md",
          "type": "blob",
          "size": 8693
        },
        {
          "path": "skills/cloudwatch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cloudwatch/SKILL.md",
          "type": "blob",
          "size": 10496
        },
        {
          "path": "skills/cloudwatch/alarms-metrics.md",
          "type": "blob",
          "size": 11278
        },
        {
          "path": "skills/cognito",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cognito/SKILL.md",
          "type": "blob",
          "size": 9223
        },
        {
          "path": "skills/cognito/auth-flows.md",
          "type": "blob",
          "size": 9131
        },
        {
          "path": "skills/dynamodb",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dynamodb/SKILL.md",
          "type": "blob",
          "size": 9890
        },
        {
          "path": "skills/dynamodb/query-patterns.md",
          "type": "blob",
          "size": 9266
        },
        {
          "path": "skills/ec2",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ec2/SKILL.md",
          "type": "blob",
          "size": 9452
        },
        {
          "path": "skills/ec2/instance-management.md",
          "type": "blob",
          "size": 9387
        },
        {
          "path": "skills/ecs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ecs/SKILL.md",
          "type": "blob",
          "size": 9410
        },
        {
          "path": "skills/ecs/task-definitions.md",
          "type": "blob",
          "size": 8770
        },
        {
          "path": "skills/eks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/eks/SKILL.md",
          "type": "blob",
          "size": 9611
        },
        {
          "path": "skills/eks/cluster-setup.md",
          "type": "blob",
          "size": 8714
        },
        {
          "path": "skills/eventbridge",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/eventbridge/SKILL.md",
          "type": "blob",
          "size": 9306
        },
        {
          "path": "skills/eventbridge/event-patterns.md",
          "type": "blob",
          "size": 7573
        },
        {
          "path": "skills/iam",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/iam/SKILL.md",
          "type": "blob",
          "size": 7075
        },
        {
          "path": "skills/iam/best-practices.md",
          "type": "blob",
          "size": 7299
        },
        {
          "path": "skills/iam/policies.md",
          "type": "blob",
          "size": 6873
        },
        {
          "path": "skills/lambda",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lambda/SKILL.md",
          "type": "blob",
          "size": 8617
        },
        {
          "path": "skills/lambda/debugging.md",
          "type": "blob",
          "size": 8731
        },
        {
          "path": "skills/lambda/deployment.md",
          "type": "blob",
          "size": 7300
        },
        {
          "path": "skills/rds",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rds/SKILL.md",
          "type": "blob",
          "size": 9728
        },
        {
          "path": "skills/rds/administration.md",
          "type": "blob",
          "size": 8066
        },
        {
          "path": "skills/s3",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/s3/SKILL.md",
          "type": "blob",
          "size": 9316
        },
        {
          "path": "skills/s3/security.md",
          "type": "blob",
          "size": 7611
        },
        {
          "path": "skills/secrets-manager",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/secrets-manager/SKILL.md",
          "type": "blob",
          "size": 9046
        },
        {
          "path": "skills/secrets-manager/rotation-strategies.md",
          "type": "blob",
          "size": 11139
        },
        {
          "path": "skills/sns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sns/SKILL.md",
          "type": "blob",
          "size": 9796
        },
        {
          "path": "skills/sns/notification-patterns.md",
          "type": "blob",
          "size": 7850
        },
        {
          "path": "skills/sqs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sqs/SKILL.md",
          "type": "blob",
          "size": 9040
        },
        {
          "path": "skills/sqs/messaging-patterns.md",
          "type": "blob",
          "size": 10738
        },
        {
          "path": "skills/step-functions",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/step-functions/SKILL.md",
          "type": "blob",
          "size": 9709
        },
        {
          "path": "skills/step-functions/workflow-patterns.md",
          "type": "blob",
          "size": 8970
        },
        {
          "path": "tracking",
          "type": "tree",
          "size": null
        },
        {
          "path": "tracking/last-check.json",
          "type": "blob",
          "size": 352
        },
        {
          "path": "tracking/pending-updates.json",
          "type": "blob",
          "size": 2
        }
      ],
      "marketplace": {
        "name": "aws-agent-skills",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "itsmostafa",
          "email": "mostafaxcodes@gmail.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "aws-agent-skills",
            "description": "Collection of AWS engineering skills for IAM, Lambda, DynamoDB, S3, and 14 other core AWS services",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add itsmostafa/aws-agent-skills",
              "/plugin install aws-agent-skills@aws-agent-skills"
            ],
            "signals": {
              "stars": 940,
              "forks": 416,
              "pushed_at": "2026-01-12T09:08:18Z",
              "created_at": "2019-05-18T19:38:59Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "api-gateway",
                "description": "AWS API Gateway for REST and HTTP API management. Use when creating APIs, configuring integrations, setting up authorization, managing stages, implementing rate limiting, or troubleshooting API issues.",
                "path": "skills/api-gateway/SKILL.md",
                "frontmatter": {
                  "name": "api-gateway",
                  "description": "AWS API Gateway for REST and HTTP API management. Use when creating APIs, configuring integrations, setting up authorization, managing stages, implementing rate limiting, or troubleshooting API issues.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/apigateway/latest/developerguide/"
                },
                "content": "# AWS API Gateway\n\nAmazon API Gateway is a fully managed service for creating, publishing, and securing APIs at any scale. Supports REST APIs, HTTP APIs, and WebSocket APIs.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### API Types\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **HTTP API** | Low-latency, cost-effective | Simple APIs, Lambda proxy |\n| **REST API** | Full-featured, more control | Complex APIs, transformation |\n| **WebSocket API** | Bidirectional communication | Real-time apps, chat |\n\n### Key Components\n\n- **Resources**: URL paths (/users, /orders/{id})\n- **Methods**: HTTP verbs (GET, POST, PUT, DELETE)\n- **Integrations**: Backend connections (Lambda, HTTP, AWS services)\n- **Stages**: Deployment environments (dev, prod)\n\n### Integration Types\n\n| Type | Description |\n|------|-------------|\n| **Lambda Proxy** | Pass-through to Lambda (recommended) |\n| **Lambda Custom** | Transform request/response |\n| **HTTP Proxy** | Pass-through to HTTP endpoint |\n| **AWS Service** | Direct integration with AWS services |\n| **Mock** | Return static response |\n\n## Common Patterns\n\n### Create HTTP API with Lambda\n\n**AWS CLI:**\n\n```bash\n# Create HTTP API\naws apigatewayv2 create-api \\\n  --name my-api \\\n  --protocol-type HTTP \\\n  --target arn:aws:lambda:us-east-1:123456789012:function:MyFunction\n\n# Get API endpoint\naws apigatewayv2 get-api --api-id abc123 --query 'ApiEndpoint'\n```\n\n**SAM Template:**\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  MyApi:\n    Type: AWS::Serverless::HttpApi\n    Properties:\n      StageName: prod\n\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: app.handler\n      Runtime: python3.12\n      Events:\n        ApiEvent:\n          Type: HttpApi\n          Properties:\n            ApiId: !Ref MyApi\n            Path: /items\n            Method: GET\n```\n\n### Create REST API with Lambda Proxy\n\n```bash\n# Create REST API\naws apigateway create-rest-api \\\n  --name my-rest-api \\\n  --endpoint-configuration types=REGIONAL\n\nAPI_ID=abc123\n\n# Get root resource ID\nROOT_ID=$(aws apigateway get-resources --rest-api-id $API_ID --query 'items[0].id' --output text)\n\n# Create resource\naws apigateway create-resource \\\n  --rest-api-id $API_ID \\\n  --parent-id $ROOT_ID \\\n  --path-part items\n\nRESOURCE_ID=xyz789\n\n# Create method\naws apigateway put-method \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method GET \\\n  --authorization-type NONE\n\n# Create Lambda integration\naws apigateway put-integration \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method GET \\\n  --type AWS_PROXY \\\n  --integration-http-method POST \\\n  --uri arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:123456789012:function:MyFunction/invocations\n\n# Deploy to stage\naws apigateway create-deployment \\\n  --rest-api-id $API_ID \\\n  --stage-name prod\n```\n\n### Lambda Handler for API Gateway\n\n```python\nimport json\n\ndef handler(event, context):\n    # HTTP API event\n    http_method = event.get('requestContext', {}).get('http', {}).get('method')\n    path = event.get('rawPath', '')\n    query_params = event.get('queryStringParameters', {})\n    body = event.get('body', '')\n\n    if body and event.get('isBase64Encoded'):\n        import base64\n        body = base64.b64decode(body).decode('utf-8')\n\n    # Process request\n    response_body = {'message': 'Success', 'path': path}\n\n    return {\n        'statusCode': 200,\n        'headers': {\n            'Content-Type': 'application/json'\n        },\n        'body': json.dumps(response_body)\n    }\n```\n\n### Configure CORS\n\n**HTTP API:**\n\n```bash\naws apigatewayv2 update-api \\\n  --api-id abc123 \\\n  --cors-configuration '{\n    \"AllowOrigins\": [\"https://example.com\"],\n    \"AllowMethods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n    \"AllowHeaders\": [\"Content-Type\", \"Authorization\"],\n    \"MaxAge\": 86400\n  }'\n```\n\n**REST API:**\n\n```bash\n# Enable CORS on resource\naws apigateway put-method \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method OPTIONS \\\n  --authorization-type NONE\n\naws apigateway put-integration \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method OPTIONS \\\n  --type MOCK \\\n  --request-templates '{\"application/json\": \"{\\\"statusCode\\\": 200}\"}'\n\naws apigateway put-method-response \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method OPTIONS \\\n  --status-code 200 \\\n  --response-parameters '{\n    \"method.response.header.Access-Control-Allow-Headers\": true,\n    \"method.response.header.Access-Control-Allow-Methods\": true,\n    \"method.response.header.Access-Control-Allow-Origin\": true\n  }'\n\naws apigateway put-integration-response \\\n  --rest-api-id $API_ID \\\n  --resource-id $RESOURCE_ID \\\n  --http-method OPTIONS \\\n  --status-code 200 \\\n  --response-parameters '{\n    \"method.response.header.Access-Control-Allow-Headers\": \"'\\''Content-Type,Authorization'\\''\",\n    \"method.response.header.Access-Control-Allow-Methods\": \"'\\''GET,POST,PUT,DELETE,OPTIONS'\\''\",\n    \"method.response.header.Access-Control-Allow-Origin\": \"'\\''*'\\''\"\n  }'\n```\n\n### JWT Authorization (HTTP API)\n\n```bash\naws apigatewayv2 create-authorizer \\\n  --api-id abc123 \\\n  --name jwt-authorizer \\\n  --authorizer-type JWT \\\n  --identity-source '$request.header.Authorization' \\\n  --jwt-configuration '{\n    \"Issuer\": \"https://cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123\",\n    \"Audience\": [\"client-id\"]\n  }'\n```\n\n## CLI Reference\n\n### HTTP API (apigatewayv2)\n\n| Command | Description |\n|---------|-------------|\n| `aws apigatewayv2 create-api` | Create API |\n| `aws apigatewayv2 get-apis` | List APIs |\n| `aws apigatewayv2 create-route` | Create route |\n| `aws apigatewayv2 create-integration` | Create integration |\n| `aws apigatewayv2 create-stage` | Create stage |\n| `aws apigatewayv2 create-authorizer` | Create authorizer |\n\n### REST API (apigateway)\n\n| Command | Description |\n|---------|-------------|\n| `aws apigateway create-rest-api` | Create API |\n| `aws apigateway get-rest-apis` | List APIs |\n| `aws apigateway create-resource` | Create resource |\n| `aws apigateway put-method` | Create method |\n| `aws apigateway put-integration` | Create integration |\n| `aws apigateway create-deployment` | Deploy API |\n\n## Best Practices\n\n### Performance\n\n- **Use HTTP APIs** for simple use cases (70% cheaper, lower latency)\n- **Enable caching** for REST APIs\n- **Use regional endpoints** unless global distribution needed\n- **Implement pagination** for list endpoints\n\n### Security\n\n- **Use authorization** on all endpoints\n- **Enable WAF** for REST APIs\n- **Use API keys** for rate limiting (not authentication)\n- **Enable access logging**\n- **Use HTTPS only**\n\n### Reliability\n\n- **Set up throttling** to protect backends\n- **Configure timeout** appropriately\n- **Use canary deployments** for updates\n- **Monitor with CloudWatch**\n\n## Troubleshooting\n\n### 403 Forbidden\n\n**Causes:**\n- Missing authorization\n- Invalid API key\n- WAF blocking\n- Resource policy denying\n\n**Debug:**\n\n```bash\n# Check API key\naws apigateway get-api-key --api-key abc123 --include-value\n\n# Check authorizer\naws apigatewayv2 get-authorizer --api-id abc123 --authorizer-id xyz789\n```\n\n### 502 Bad Gateway\n\n**Causes:**\n- Lambda error\n- Integration timeout\n- Invalid response format\n\n**Lambda response format:**\n\n```python\n# Correct format\nreturn {\n    'statusCode': 200,\n    'headers': {'Content-Type': 'application/json'},\n    'body': json.dumps({'message': 'success'})\n}\n\n# Wrong - missing statusCode\nreturn {'message': 'success'}\n```\n\n### 504 Gateway Timeout\n\n**Causes:**\n- Backend timeout (Lambda max 29 seconds for REST API)\n- Integration timeout too short\n\n**Solutions:**\n- Increase Lambda timeout\n- Use async processing for long operations\n- Increase integration timeout (max 29s for REST, 30s for HTTP)\n\n### CORS Errors\n\n**Debug:**\n- Check OPTIONS method exists\n- Verify headers in response\n- Check origin matches allowed origins\n\n## References\n\n- [API Gateway Developer Guide](https://docs.aws.amazon.com/apigateway/latest/developerguide/)\n- [API Gateway REST API Reference](https://docs.aws.amazon.com/apigateway/latest/api/)\n- [API Gateway CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/apigateway/)\n- [boto3 API Gateway](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/apigateway.html)"
              },
              {
                "name": "bedrock",
                "description": "AWS Bedrock foundation models for generative AI. Use when invoking foundation models, building AI applications, creating embeddings, configuring model access, or implementing RAG patterns.",
                "path": "skills/bedrock/SKILL.md",
                "frontmatter": {
                  "name": "bedrock",
                  "description": "AWS Bedrock foundation models for generative AI. Use when invoking foundation models, building AI applications, creating embeddings, configuring model access, or implementing RAG patterns.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/bedrock/latest/userguide/"
                },
                "content": "# AWS Bedrock\n\nAmazon Bedrock provides access to foundation models (FMs) from AI companies through a unified API. Build generative AI applications with text generation, embeddings, and image generation capabilities.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Foundation Models\n\nPre-trained models available through Bedrock:\n- **Claude** (Anthropic): Text generation, analysis, coding\n- **Titan** (Amazon): Text, embeddings, image generation\n- **Llama** (Meta): Open-weight text generation\n- **Mistral**: Efficient text generation\n- **Stable Diffusion** (Stability AI): Image generation\n\n### Model Access\n\nModels must be enabled in your account before use:\n- Request access in Bedrock console\n- Some models require acceptance of EULAs\n- Access is region-specific\n\n### Inference Types\n\n| Type | Use Case | Pricing |\n|------|----------|---------|\n| **On-Demand** | Variable workloads | Per token |\n| **Provisioned Throughput** | Consistent high-volume | Hourly commitment |\n| **Batch Inference** | Async large-scale | Discounted per token |\n\n## Common Patterns\n\n### Invoke Model (Text Generation)\n\n**AWS CLI:**\n\n```bash\n# Invoke Claude\naws bedrock-runtime invoke-model \\\n  --model-id anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --content-type application/json \\\n  --accept application/json \\\n  --body '{\n    \"anthropic_version\": \"bedrock-2023-05-31\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Explain AWS Lambda in 3 sentences.\"}\n    ]\n  }' \\\n  response.json\n\ncat response.json | jq -r '.content[0].text'\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport json\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef invoke_claude(prompt, max_tokens=1024):\n    response = bedrock.invoke_model(\n        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            'anthropic_version': 'bedrock-2023-05-31',\n            'max_tokens': max_tokens,\n            'messages': [\n                {'role': 'user', 'content': prompt}\n            ]\n        })\n    )\n\n    result = json.loads(response['body'].read())\n    return result['content'][0]['text']\n\n# Usage\nresponse = invoke_claude('What is Amazon S3?')\nprint(response)\n```\n\n### Streaming Response\n\n```python\nimport boto3\nimport json\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef stream_claude(prompt):\n    response = bedrock.invoke_model_with_response_stream(\n        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            'anthropic_version': 'bedrock-2023-05-31',\n            'max_tokens': 1024,\n            'messages': [\n                {'role': 'user', 'content': prompt}\n            ]\n        })\n    )\n\n    for event in response['body']:\n        chunk = json.loads(event['chunk']['bytes'])\n        if chunk['type'] == 'content_block_delta':\n            yield chunk['delta'].get('text', '')\n\n# Usage\nfor text in stream_claude('Write a haiku about cloud computing.'):\n    print(text, end='', flush=True)\n```\n\n### Generate Embeddings\n\n```python\nimport boto3\nimport json\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef get_embedding(text):\n    response = bedrock.invoke_model(\n        modelId='amazon.titan-embed-text-v2:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            'inputText': text,\n            'dimensions': 1024,\n            'normalize': True\n        })\n    )\n\n    result = json.loads(response['body'].read())\n    return result['embedding']\n\n# Usage\nembedding = get_embedding('AWS Lambda is a serverless compute service.')\nprint(f'Embedding dimension: {len(embedding)}')\n```\n\n### Conversation with History\n\n```python\nimport boto3\nimport json\n\nbedrock = boto3.client('bedrock-runtime')\n\nclass Conversation:\n    def __init__(self, system_prompt=None):\n        self.messages = []\n        self.system = system_prompt\n\n    def chat(self, user_message):\n        self.messages.append({\n            'role': 'user',\n            'content': user_message\n        })\n\n        body = {\n            'anthropic_version': 'bedrock-2023-05-31',\n            'max_tokens': 1024,\n            'messages': self.messages\n        }\n\n        if self.system:\n            body['system'] = self.system\n\n        response = bedrock.invoke_model(\n            modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n            contentType='application/json',\n            accept='application/json',\n            body=json.dumps(body)\n        )\n\n        result = json.loads(response['body'].read())\n        assistant_message = result['content'][0]['text']\n\n        self.messages.append({\n            'role': 'assistant',\n            'content': assistant_message\n        })\n\n        return assistant_message\n\n# Usage\nconv = Conversation(system_prompt='You are an AWS solutions architect.')\nprint(conv.chat('What database should I use for a chat application?'))\nprint(conv.chat('What about for time-series data?'))\n```\n\n### List Available Models\n\n```bash\n# List all foundation models\naws bedrock list-foundation-models \\\n  --query 'modelSummaries[*].[modelId,modelName,providerName]' \\\n  --output table\n\n# Filter by provider\naws bedrock list-foundation-models \\\n  --by-provider anthropic \\\n  --query 'modelSummaries[*].modelId'\n\n# Get model details\naws bedrock get-foundation-model \\\n  --model-identifier anthropic.claude-3-sonnet-20240229-v1:0\n```\n\n### Request Model Access\n\n```bash\n# List model access status\naws bedrock list-foundation-model-agreement-offers \\\n  --model-id anthropic.claude-3-sonnet-20240229-v1:0\n```\n\n## CLI Reference\n\n### Bedrock (Control Plane)\n\n| Command | Description |\n|---------|-------------|\n| `aws bedrock list-foundation-models` | List available models |\n| `aws bedrock get-foundation-model` | Get model details |\n| `aws bedrock list-custom-models` | List fine-tuned models |\n| `aws bedrock create-model-customization-job` | Start fine-tuning |\n| `aws bedrock list-provisioned-model-throughputs` | List provisioned capacity |\n\n### Bedrock Runtime (Data Plane)\n\n| Command | Description |\n|---------|-------------|\n| `aws bedrock-runtime invoke-model` | Invoke model synchronously |\n| `aws bedrock-runtime invoke-model-with-response-stream` | Invoke with streaming |\n| `aws bedrock-runtime converse` | Multi-turn conversation API |\n| `aws bedrock-runtime converse-stream` | Streaming conversation |\n\n### Bedrock Agent Runtime\n\n| Command | Description |\n|---------|-------------|\n| `aws bedrock-agent-runtime invoke-agent` | Invoke a Bedrock agent |\n| `aws bedrock-agent-runtime retrieve` | Query knowledge base |\n| `aws bedrock-agent-runtime retrieve-and-generate` | RAG query |\n\n## Best Practices\n\n### Cost Optimization\n\n- **Use appropriate models**: Smaller models for simple tasks\n- **Set max_tokens**: Limit output length when possible\n- **Cache responses**: For repeated identical queries\n- **Batch when possible**: Use batch inference for bulk processing\n- **Monitor usage**: Set up CloudWatch alarms for cost\n\n### Performance\n\n- **Use streaming**: For better user experience with long outputs\n- **Connection pooling**: Reuse boto3 clients\n- **Regional deployment**: Use closest region to reduce latency\n- **Provisioned throughput**: For consistent high-volume workloads\n\n### Security\n\n- **Least privilege IAM**: Only grant needed model access\n- **VPC endpoints**: Keep traffic private\n- **Guardrails**: Implement content filtering\n- **Audit with CloudTrail**: Track model invocations\n\n### IAM Permissions\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock:InvokeModel\",\n        \"bedrock:InvokeModelWithResponseStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0\"\n      ]\n    }\n  ]\n}\n```\n\n## Troubleshooting\n\n### AccessDeniedException\n\n**Causes:**\n- Model access not enabled in console\n- IAM policy missing `bedrock:InvokeModel`\n- Wrong model ID or region\n\n**Debug:**\n\n```bash\n# Check model access status\naws bedrock list-foundation-models \\\n  --query 'modelSummaries[?modelId==`anthropic.claude-3-sonnet-20240229-v1:0`]'\n\n# Test IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:role/my-role \\\n  --action-names bedrock:InvokeModel \\\n  --resource-arns \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\"\n```\n\n### ModelNotReadyException\n\n**Cause:** Model is still being provisioned or temporarily unavailable.\n\n**Solution:** Implement retry with exponential backoff:\n\n```python\nimport time\nfrom botocore.exceptions import ClientError\n\ndef invoke_with_retry(bedrock, body, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return bedrock.invoke_model(\n                modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n                body=json.dumps(body)\n            )\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'ModelNotReadyException':\n                time.sleep(2 ** attempt)\n            else:\n                raise\n    raise Exception('Max retries exceeded')\n```\n\n### ThrottlingException\n\n**Causes:**\n- Exceeded on-demand quota\n- Too many concurrent requests\n\n**Solutions:**\n- Request quota increase\n- Implement exponential backoff\n- Consider provisioned throughput\n\n### ValidationException\n\n**Common issues:**\n- Invalid model ID\n- Malformed request body\n- max_tokens exceeds model limit\n\n**Debug:**\n\n```python\n# Check model-specific requirements\naws bedrock get-foundation-model \\\n  --model-identifier anthropic.claude-3-sonnet-20240229-v1:0 \\\n  --query 'modelDetails.inferenceTypesSupported'\n```\n\n## References\n\n- [Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/)\n- [Bedrock API Reference](https://docs.aws.amazon.com/bedrock/latest/APIReference/)\n- [Bedrock Runtime API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html)\n- [Model Parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html)\n- [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)"
              },
              {
                "name": "cloudformation",
                "description": "AWS CloudFormation infrastructure as code for stack management. Use when writing templates, deploying stacks, managing drift, troubleshooting deployments, or organizing infrastructure with nested stacks.",
                "path": "skills/cloudformation/SKILL.md",
                "frontmatter": {
                  "name": "cloudformation",
                  "description": "AWS CloudFormation infrastructure as code for stack management. Use when writing templates, deploying stacks, managing drift, troubleshooting deployments, or organizing infrastructure with nested stacks.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/"
                },
                "content": "# AWS CloudFormation\n\nAWS CloudFormation provisions and manages AWS resources using templates. Define infrastructure as code, version control it, and deploy consistently across environments.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Templates\n\nJSON or YAML files defining AWS resources. Key sections:\n- **Parameters**: Input values\n- **Mappings**: Static lookup tables\n- **Conditions**: Conditional resource creation\n- **Resources**: AWS resources (required)\n- **Outputs**: Return values\n\n### Stacks\n\nCollection of resources managed as a single unit. Created from templates.\n\n### Change Sets\n\nPreview changes before executing updates.\n\n### Stack Sets\n\nDeploy stacks across multiple accounts and regions.\n\n## Common Patterns\n\n### Basic Template Structure\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: My infrastructure template\n\nParameters:\n  Environment:\n    Type: String\n    AllowedValues: [dev, staging, prod]\n    Default: dev\n\nMappings:\n  EnvironmentConfig:\n    dev:\n      InstanceType: t3.micro\n    prod:\n      InstanceType: t3.large\n\nConditions:\n  IsProd: !Equals [!Ref Environment, prod]\n\nResources:\n  MyBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Sub 'my-app-${Environment}-${AWS::AccountId}'\n      VersioningConfiguration:\n        Status: !If [IsProd, Enabled, Suspended]\n\nOutputs:\n  BucketName:\n    Description: S3 bucket name\n    Value: !Ref MyBucket\n    Export:\n      Name: !Sub '${AWS::StackName}-BucketName'\n```\n\n### Deploy a Stack\n\n**AWS CLI:**\n\n```bash\n# Create stack\naws cloudformation create-stack \\\n  --stack-name my-stack \\\n  --template-body file://template.yaml \\\n  --parameters ParameterKey=Environment,ParameterValue=prod \\\n  --capabilities CAPABILITY_IAM\n\n# Wait for completion\naws cloudformation wait stack-create-complete --stack-name my-stack\n\n# Update stack\naws cloudformation update-stack \\\n  --stack-name my-stack \\\n  --template-body file://template.yaml \\\n  --parameters ParameterKey=Environment,ParameterValue=prod\n\n# Delete stack\naws cloudformation delete-stack --stack-name my-stack\n```\n\n### Use Change Sets\n\n```bash\n# Create change set\naws cloudformation create-change-set \\\n  --stack-name my-stack \\\n  --change-set-name my-changes \\\n  --template-body file://template.yaml \\\n  --parameters ParameterKey=Environment,ParameterValue=prod\n\n# Describe changes\naws cloudformation describe-change-set \\\n  --stack-name my-stack \\\n  --change-set-name my-changes\n\n# Execute change set\naws cloudformation execute-change-set \\\n  --stack-name my-stack \\\n  --change-set-name my-changes\n```\n\n### Lambda Function\n\n```yaml\nResources:\n  LambdaFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: !Sub '${AWS::StackName}-function'\n      Runtime: python3.12\n      Handler: index.handler\n      Role: !GetAtt LambdaRole.Arn\n      Code:\n        ZipFile: |\n          def handler(event, context):\n              return {'statusCode': 200, 'body': 'Hello'}\n      Environment:\n        Variables:\n          ENVIRONMENT: !Ref Environment\n\n  LambdaRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n```\n\n### VPC with Subnets\n\n```yaml\nResources:\n  VPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: 10.0.0.0/16\n      EnableDnsHostnames: true\n      Tags:\n        - Key: Name\n          Value: !Sub '${AWS::StackName}-vpc'\n\n  PublicSubnet1:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref VPC\n      AvailabilityZone: !Select [0, !GetAZs '']\n      CidrBlock: 10.0.1.0/24\n      MapPublicIpOnLaunch: true\n\n  PrivateSubnet1:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref VPC\n      AvailabilityZone: !Select [0, !GetAZs '']\n      CidrBlock: 10.0.10.0/24\n\n  InternetGateway:\n    Type: AWS::EC2::InternetGateway\n\n  AttachGateway:\n    Type: AWS::EC2::VPCGatewayAttachment\n    Properties:\n      VpcId: !Ref VPC\n      InternetGatewayId: !Ref InternetGateway\n\n  PublicRouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref VPC\n\n  PublicRoute:\n    Type: AWS::EC2::Route\n    DependsOn: AttachGateway\n    Properties:\n      RouteTableId: !Ref PublicRouteTable\n      DestinationCidrBlock: 0.0.0.0/0\n      GatewayId: !Ref InternetGateway\n\n  PublicSubnet1RouteTableAssociation:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PublicSubnet1\n      RouteTableId: !Ref PublicRouteTable\n```\n\n### DynamoDB Table\n\n```yaml\nResources:\n  OrdersTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      TableName: !Sub '${AWS::StackName}-orders'\n      AttributeDefinitions:\n        - AttributeName: PK\n          AttributeType: S\n        - AttributeName: SK\n          AttributeType: S\n        - AttributeName: GSI1PK\n          AttributeType: S\n        - AttributeName: GSI1SK\n          AttributeType: S\n      KeySchema:\n        - AttributeName: PK\n          KeyType: HASH\n        - AttributeName: SK\n          KeyType: RANGE\n      GlobalSecondaryIndexes:\n        - IndexName: GSI1\n          KeySchema:\n            - AttributeName: GSI1PK\n              KeyType: HASH\n            - AttributeName: GSI1SK\n              KeyType: RANGE\n          Projection:\n            ProjectionType: ALL\n      BillingMode: PAY_PER_REQUEST\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n```\n\n## CLI Reference\n\n### Stack Operations\n\n| Command | Description |\n|---------|-------------|\n| `aws cloudformation create-stack` | Create stack |\n| `aws cloudformation update-stack` | Update stack |\n| `aws cloudformation delete-stack` | Delete stack |\n| `aws cloudformation describe-stacks` | Get stack info |\n| `aws cloudformation list-stacks` | List stacks |\n| `aws cloudformation describe-stack-events` | Get events |\n| `aws cloudformation describe-stack-resources` | Get resources |\n\n### Change Sets\n\n| Command | Description |\n|---------|-------------|\n| `aws cloudformation create-change-set` | Create change set |\n| `aws cloudformation describe-change-set` | View changes |\n| `aws cloudformation execute-change-set` | Apply changes |\n| `aws cloudformation delete-change-set` | Delete change set |\n\n### Template\n\n| Command | Description |\n|---------|-------------|\n| `aws cloudformation validate-template` | Validate template |\n| `aws cloudformation get-template` | Get stack template |\n| `aws cloudformation get-template-summary` | Get template info |\n\n## Best Practices\n\n### Template Design\n\n- **Use parameters** for environment-specific values\n- **Use mappings** for static lookup tables\n- **Use conditions** for optional resources\n- **Export outputs** for cross-stack references\n- **Add descriptions** to parameters and outputs\n\n### Security\n\n- **Use IAM roles** instead of access keys\n- **Enable termination protection** for production\n- **Use stack policies** to protect resources\n- **Never hardcode secrets** â€” use Secrets Manager\n\n```bash\n# Enable termination protection\naws cloudformation update-termination-protection \\\n  --stack-name my-stack \\\n  --enable-termination-protection\n```\n\n### Organization\n\n- **Use nested stacks** for complex infrastructure\n- **Create reusable modules**\n- **Version control templates**\n- **Use consistent naming conventions**\n\n### Reliability\n\n- **Use DependsOn** for explicit dependencies\n- **Configure creation policies** for instances\n- **Use update policies** for Auto Scaling groups\n- **Implement rollback triggers**\n\n## Troubleshooting\n\n### Stack Creation Failed\n\n```bash\n# Get failure reason\naws cloudformation describe-stack-events \\\n  --stack-name my-stack \\\n  --query 'StackEvents[?ResourceStatus==`CREATE_FAILED`]'\n\n# Common causes:\n# - IAM permissions\n# - Resource limits\n# - Invalid property values\n# - Dependency failures\n```\n\n### Stack Stuck in DELETE_FAILED\n\n```bash\n# Identify resources that couldn't be deleted\naws cloudformation describe-stack-resources \\\n  --stack-name my-stack \\\n  --query 'StackResources[?ResourceStatus==`DELETE_FAILED`]'\n\n# Retry with resources to skip\naws cloudformation delete-stack \\\n  --stack-name my-stack \\\n  --retain-resources ResourceLogicalId1 ResourceLogicalId2\n```\n\n### Drift Detection\n\n```bash\n# Detect drift\naws cloudformation detect-stack-drift --stack-name my-stack\n\n# Check drift status\naws cloudformation describe-stack-drift-detection-status \\\n  --stack-drift-detection-id abc123\n\n# View drifted resources\naws cloudformation describe-stack-resource-drifts \\\n  --stack-name my-stack\n```\n\n### Rollback Failed\n\n```bash\n# Continue update rollback\naws cloudformation continue-update-rollback \\\n  --stack-name my-stack \\\n  --resources-to-skip ResourceLogicalId1\n```\n\n## References\n\n- [CloudFormation User Guide](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/)\n- [CloudFormation API Reference](https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/)\n- [CloudFormation CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/cloudformation/)\n- [Resource and Property Reference](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html)"
              },
              {
                "name": "cloudwatch",
                "description": "AWS CloudWatch monitoring for logs, metrics, alarms, and dashboards. Use when setting up monitoring, creating alarms, querying logs with Insights, configuring metric filters, building dashboards, or troubleshooting application issues.",
                "path": "skills/cloudwatch/SKILL.md",
                "frontmatter": {
                  "name": "cloudwatch",
                  "description": "AWS CloudWatch monitoring for logs, metrics, alarms, and dashboards. Use when setting up monitoring, creating alarms, querying logs with Insights, configuring metric filters, building dashboards, or troubleshooting application issues.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/"
                },
                "content": "# AWS CloudWatch\n\nAmazon CloudWatch provides monitoring and observability for AWS resources and applications. It collects metrics, logs, and events, enabling you to monitor, troubleshoot, and optimize your AWS environment.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Metrics\n\nTime-ordered data points published to CloudWatch. Key components:\n- **Namespace**: Container for metrics (e.g., `AWS/Lambda`)\n- **Metric name**: Name of the measurement (e.g., `Invocations`)\n- **Dimensions**: Name-value pairs for filtering (e.g., `FunctionName=MyFunc`)\n- **Statistics**: Aggregations (Sum, Average, Min, Max, SampleCount, pN)\n\n### Logs\n\nLog data from AWS services and applications:\n- **Log groups**: Collections of log streams\n- **Log streams**: Sequences of log events from same source\n- **Log events**: Individual log entries with timestamp and message\n\n### Alarms\n\nAutomated actions based on metric thresholds:\n- **States**: OK, ALARM, INSUFFICIENT_DATA\n- **Actions**: SNS notifications, Auto Scaling, EC2 actions\n\n## Common Patterns\n\n### Create a Metric Alarm\n\n**AWS CLI:**\n\n```bash\n# CPU utilization alarm for EC2\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"HighCPU-i-1234567890abcdef0\" \\\n  --metric-name CPUUtilization \\\n  --namespace AWS/EC2 \\\n  --statistic Average \\\n  --period 300 \\\n  --threshold 80 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 2 \\\n  --dimensions Name=InstanceId,Value=i-1234567890abcdef0 \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:alerts \\\n  --ok-actions arn:aws:sns:us-east-1:123456789012:alerts\n```\n\n**boto3:**\n\n```python\nimport boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\ncloudwatch.put_metric_alarm(\n    AlarmName='HighCPU-i-1234567890abcdef0',\n    MetricName='CPUUtilization',\n    Namespace='AWS/EC2',\n    Statistic='Average',\n    Period=300,\n    Threshold=80.0,\n    ComparisonOperator='GreaterThanThreshold',\n    EvaluationPeriods=2,\n    Dimensions=[\n        {'Name': 'InstanceId', 'Value': 'i-1234567890abcdef0'}\n    ],\n    AlarmActions=['arn:aws:sns:us-east-1:123456789012:alerts'],\n    OKActions=['arn:aws:sns:us-east-1:123456789012:alerts']\n)\n```\n\n### Lambda Error Rate Alarm\n\n```bash\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"LambdaErrorRate-MyFunction\" \\\n  --metrics '[\n    {\n      \"Id\": \"errors\",\n      \"MetricStat\": {\n        \"Metric\": {\n          \"Namespace\": \"AWS/Lambda\",\n          \"MetricName\": \"Errors\",\n          \"Dimensions\": [{\"Name\": \"FunctionName\", \"Value\": \"MyFunction\"}]\n        },\n        \"Period\": 60,\n        \"Stat\": \"Sum\"\n      },\n      \"ReturnData\": false\n    },\n    {\n      \"Id\": \"invocations\",\n      \"MetricStat\": {\n        \"Metric\": {\n          \"Namespace\": \"AWS/Lambda\",\n          \"MetricName\": \"Invocations\",\n          \"Dimensions\": [{\"Name\": \"FunctionName\", \"Value\": \"MyFunction\"}]\n        },\n        \"Period\": 60,\n        \"Stat\": \"Sum\"\n      },\n      \"ReturnData\": false\n    },\n    {\n      \"Id\": \"errorRate\",\n      \"Expression\": \"errors/invocations*100\",\n      \"Label\": \"Error Rate\",\n      \"ReturnData\": true\n    }\n  ]' \\\n  --threshold 5 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 3 \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:alerts\n```\n\n### Query Logs with Insights\n\n```bash\n# Find errors in Lambda logs\naws logs start-query \\\n  --log-group-name /aws/lambda/MyFunction \\\n  --start-time $(date -d '1 hour ago' +%s) \\\n  --end-time $(date +%s) \\\n  --query-string '\n    fields @timestamp, @message\n    | filter @message like /ERROR/\n    | sort @timestamp desc\n    | limit 50\n  '\n\n# Get query results\naws logs get-query-results --query-id <query-id>\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport time\n\nlogs = boto3.client('logs')\n\n# Start query\nresponse = logs.start_query(\n    logGroupName='/aws/lambda/MyFunction',\n    startTime=int(time.time()) - 3600,\n    endTime=int(time.time()),\n    queryString='''\n        fields @timestamp, @message\n        | filter @message like /ERROR/\n        | sort @timestamp desc\n        | limit 50\n    '''\n)\n\nquery_id = response['queryId']\n\n# Wait for results\nwhile True:\n    result = logs.get_query_results(queryId=query_id)\n    if result['status'] == 'Complete':\n        break\n    time.sleep(1)\n\nfor row in result['results']:\n    print(row)\n```\n\n### Create Metric Filter\n\nExtract metrics from log patterns:\n\n```bash\n# Create metric filter for error count\naws logs put-metric-filter \\\n  --log-group-name /aws/lambda/MyFunction \\\n  --filter-name ErrorCount \\\n  --filter-pattern \"ERROR\" \\\n  --metric-transformations \\\n    metricName=ErrorCount,metricNamespace=MyApp,metricValue=1,defaultValue=0\n```\n\n### Publish Custom Metrics\n\n```python\nimport boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\ncloudwatch.put_metric_data(\n    Namespace='MyApp',\n    MetricData=[\n        {\n            'MetricName': 'OrdersProcessed',\n            'Value': 1,\n            'Unit': 'Count',\n            'Dimensions': [\n                {'Name': 'Environment', 'Value': 'Production'},\n                {'Name': 'OrderType', 'Value': 'Standard'}\n            ]\n        }\n    ]\n)\n```\n\n### Create Dashboard\n\n```bash\ncat > dashboard.json << 'EOF'\n{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 6,\n      \"properties\": {\n        \"title\": \"Lambda Invocations\",\n        \"metrics\": [\n          [\"AWS/Lambda\", \"Invocations\", \"FunctionName\", \"MyFunction\"]\n        ],\n        \"period\": 60,\n        \"stat\": \"Sum\",\n        \"region\": \"us-east-1\"\n      }\n    },\n    {\n      \"type\": \"log\",\n      \"x\": 12, \"y\": 0, \"width\": 12, \"height\": 6,\n      \"properties\": {\n        \"title\": \"Recent Errors\",\n        \"query\": \"SOURCE '/aws/lambda/MyFunction' | filter @message like /ERROR/ | limit 20\",\n        \"region\": \"us-east-1\"\n      }\n    }\n  ]\n}\nEOF\n\naws cloudwatch put-dashboard \\\n  --dashboard-name MyAppDashboard \\\n  --dashboard-body file://dashboard.json\n```\n\n## CLI Reference\n\n### Metrics Commands\n\n| Command | Description |\n|---------|-------------|\n| `aws cloudwatch put-metric-data` | Publish custom metrics |\n| `aws cloudwatch get-metric-data` | Retrieve metric values |\n| `aws cloudwatch get-metric-statistics` | Get aggregated statistics |\n| `aws cloudwatch list-metrics` | List available metrics |\n\n### Alarms Commands\n\n| Command | Description |\n|---------|-------------|\n| `aws cloudwatch put-metric-alarm` | Create or update alarm |\n| `aws cloudwatch describe-alarms` | List alarms |\n| `aws cloudwatch set-alarm-state` | Manually set alarm state |\n| `aws cloudwatch delete-alarms` | Delete alarms |\n\n### Logs Commands\n\n| Command | Description |\n|---------|-------------|\n| `aws logs create-log-group` | Create log group |\n| `aws logs put-log-events` | Write log events |\n| `aws logs filter-log-events` | Search log events |\n| `aws logs start-query` | Start Insights query |\n| `aws logs put-metric-filter` | Create metric filter |\n| `aws logs put-retention-policy` | Set log retention |\n\n## Best Practices\n\n### Metrics\n\n- **Use dimensions wisely** â€” too many creates metric explosion\n- **Aggregate before publishing** â€” batch custom metrics\n- **Use high-resolution metrics** (1-second) only when needed\n- **Set meaningful units** for custom metrics\n\n### Alarms\n\n- **Use composite alarms** for complex conditions\n- **Set appropriate evaluation periods** to avoid flapping\n- **Include OK actions** to track recovery\n- **Use anomaly detection** for dynamic thresholds\n\n### Logs\n\n- **Set retention policies** â€” don't keep logs forever\n- **Use structured logging** (JSON) for better querying\n- **Create metric filters** for key events\n- **Use Contributor Insights** for top-N analysis\n\n### Cost Optimization\n\n- **Delete unused dashboards**\n- **Reduce log retention** for non-critical logs\n- **Avoid high-resolution metrics** unless necessary\n- **Use log subscription filters** instead of polling\n\n## Troubleshooting\n\n### Missing Metrics\n\n**Causes:**\n- Service not publishing yet (wait 1-5 minutes)\n- Wrong namespace/dimensions\n- Detailed monitoring not enabled (EC2)\n\n**Debug:**\n\n```bash\n# List metrics for a namespace\naws cloudwatch list-metrics \\\n  --namespace AWS/Lambda \\\n  --dimensions Name=FunctionName,Value=MyFunction\n```\n\n### Alarm Stuck in INSUFFICIENT_DATA\n\n**Causes:**\n- Metric not being published\n- Dimensions mismatch\n- Evaluation period too short\n\n**Debug:**\n\n```bash\n# Check if metric has data\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Invocations \\\n  --dimensions Name=FunctionName,Value=MyFunction \\\n  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Sum\n```\n\n### Log Events Not Appearing\n\n**Causes:**\n- IAM permissions missing\n- CloudWatch Logs agent not running\n- Log group doesn't exist\n\n**Debug:**\n\n```bash\n# Check log streams\naws logs describe-log-streams \\\n  --log-group-name /aws/lambda/MyFunction \\\n  --order-by LastEventTime \\\n  --descending \\\n  --limit 5\n```\n\n### High CloudWatch Costs\n\n**Check usage:**\n\n```bash\n# Get PutLogEvents usage\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Logs \\\n  --metric-name IncomingBytes \\\n  --dimensions Name=LogGroupName,Value=/aws/lambda/MyFunction \\\n  --start-time $(date -d '7 days ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 86400 \\\n  --statistics Sum\n```\n\n## References\n\n- [CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/)\n- [CloudWatch Logs User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/)\n- [CloudWatch API Reference](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/)\n- [CloudWatch CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/)\n- [Logs Insights Query Syntax](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html)\n- [boto3 CloudWatch](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudwatch.html)"
              },
              {
                "name": "cognito",
                "description": "AWS Cognito user authentication and authorization service. Use when setting up user pools, configuring identity pools, implementing OAuth flows, managing user attributes, or integrating with social identity providers.",
                "path": "skills/cognito/SKILL.md",
                "frontmatter": {
                  "name": "cognito",
                  "description": "AWS Cognito user authentication and authorization service. Use when setting up user pools, configuring identity pools, implementing OAuth flows, managing user attributes, or integrating with social identity providers.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/cognito/latest/developerguide/"
                },
                "content": "# AWS Cognito\n\nAmazon Cognito provides authentication, authorization, and user management for web and mobile applications. Users can sign in directly or through federated identity providers.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### User Pools\n\nUser directory for sign-up and sign-in. Provides:\n- User registration and authentication\n- OAuth 2.0 / OpenID Connect tokens\n- MFA and password policies\n- Customizable UI and flows\n\n### Identity Pools (Federated Identities)\n\nProvide temporary AWS credentials to access AWS services. Users can be:\n- Cognito User Pool users\n- Social identity (Google, Facebook, Apple)\n- SAML/OIDC enterprise identity\n- Anonymous guests\n\n### Tokens\n\n| Token | Purpose | Lifetime |\n|-------|---------|----------|\n| **ID Token** | User identity claims | 1 hour |\n| **Access Token** | API authorization | 1 hour |\n| **Refresh Token** | Get new ID/Access tokens | 30 days (configurable) |\n\n## Common Patterns\n\n### Create User Pool\n\n**AWS CLI:**\n\n```bash\naws cognito-idp create-user-pool \\\n  --pool-name my-app-users \\\n  --policies '{\n    \"PasswordPolicy\": {\n      \"MinimumLength\": 12,\n      \"RequireUppercase\": true,\n      \"RequireLowercase\": true,\n      \"RequireNumbers\": true,\n      \"RequireSymbols\": true\n    }\n  }' \\\n  --auto-verified-attributes email \\\n  --username-attributes email \\\n  --mfa-configuration OPTIONAL \\\n  --user-attribute-update-settings '{\n    \"AttributesRequireVerificationBeforeUpdate\": [\"email\"]\n  }'\n```\n\n### Create App Client\n\n```bash\naws cognito-idp create-user-pool-client \\\n  --user-pool-id us-east-1_abc123 \\\n  --client-name my-web-app \\\n  --generate-secret \\\n  --explicit-auth-flows ALLOW_USER_SRP_AUTH ALLOW_REFRESH_TOKEN_AUTH \\\n  --supported-identity-providers COGNITO \\\n  --callback-urls https://myapp.com/callback \\\n  --logout-urls https://myapp.com/logout \\\n  --allowed-o-auth-flows code \\\n  --allowed-o-auth-scopes openid email profile \\\n  --allowed-o-auth-flows-user-pool-client \\\n  --access-token-validity 60 \\\n  --id-token-validity 60 \\\n  --refresh-token-validity 30 \\\n  --token-validity-units '{\n    \"AccessToken\": \"minutes\",\n    \"IdToken\": \"minutes\",\n    \"RefreshToken\": \"days\"\n  }'\n```\n\n### Sign Up User\n\n```python\nimport boto3\nimport hmac\nimport hashlib\nimport base64\n\ncognito = boto3.client('cognito-idp')\n\ndef get_secret_hash(username, client_id, client_secret):\n    message = username + client_id\n    dig = hmac.new(\n        client_secret.encode('utf-8'),\n        message.encode('utf-8'),\n        digestmod=hashlib.sha256\n    ).digest()\n    return base64.b64encode(dig).decode()\n\nresponse = cognito.sign_up(\n    ClientId='client-id',\n    SecretHash=get_secret_hash('user@example.com', 'client-id', 'client-secret'),\n    Username='user@example.com',\n    Password='SecurePassword123!',\n    UserAttributes=[\n        {'Name': 'email', 'Value': 'user@example.com'},\n        {'Name': 'name', 'Value': 'John Doe'}\n    ]\n)\n```\n\n### Confirm Sign Up\n\n```python\ncognito.confirm_sign_up(\n    ClientId='client-id',\n    SecretHash=get_secret_hash('user@example.com', 'client-id', 'client-secret'),\n    Username='user@example.com',\n    ConfirmationCode='123456'\n)\n```\n\n### Authenticate User\n\n```python\nresponse = cognito.initiate_auth(\n    ClientId='client-id',\n    AuthFlow='USER_SRP_AUTH',\n    AuthParameters={\n        'USERNAME': 'user@example.com',\n        'SECRET_HASH': get_secret_hash('user@example.com', 'client-id', 'client-secret'),\n        'SRP_A': srp_a  # From SRP library\n    }\n)\n\n# For simple password auth (not recommended for production)\nresponse = cognito.admin_initiate_auth(\n    UserPoolId='us-east-1_abc123',\n    ClientId='client-id',\n    AuthFlow='ADMIN_USER_PASSWORD_AUTH',\n    AuthParameters={\n        'USERNAME': 'user@example.com',\n        'PASSWORD': 'password',\n        'SECRET_HASH': get_secret_hash('user@example.com', 'client-id', 'client-secret')\n    }\n)\n\ntokens = response['AuthenticationResult']\nid_token = tokens['IdToken']\naccess_token = tokens['AccessToken']\nrefresh_token = tokens['RefreshToken']\n```\n\n### Refresh Tokens\n\n```python\nresponse = cognito.initiate_auth(\n    ClientId='client-id',\n    AuthFlow='REFRESH_TOKEN_AUTH',\n    AuthParameters={\n        'REFRESH_TOKEN': refresh_token,\n        'SECRET_HASH': get_secret_hash('user@example.com', 'client-id', 'client-secret')\n    }\n)\n```\n\n### Create Identity Pool\n\n```bash\naws cognito-identity create-identity-pool \\\n  --identity-pool-name my-app-identities \\\n  --allow-unauthenticated-identities \\\n  --cognito-identity-providers \\\n    ProviderName=cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123,\\\nClientId=client-id,\\\nServerSideTokenCheck=true\n```\n\n### Get AWS Credentials\n\n```python\nimport boto3\n\ncognito_identity = boto3.client('cognito-identity')\n\n# Get identity ID\nresponse = cognito_identity.get_id(\n    IdentityPoolId='us-east-1:12345678-1234-1234-1234-123456789012',\n    Logins={\n        'cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123': id_token\n    }\n)\nidentity_id = response['IdentityId']\n\n# Get credentials\nresponse = cognito_identity.get_credentials_for_identity(\n    IdentityId=identity_id,\n    Logins={\n        'cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123': id_token\n    }\n)\n\ncredentials = response['Credentials']\n# Use credentials['AccessKeyId'], credentials['SecretKey'], credentials['SessionToken']\n```\n\n## CLI Reference\n\n### User Pool\n\n| Command | Description |\n|---------|-------------|\n| `aws cognito-idp create-user-pool` | Create user pool |\n| `aws cognito-idp describe-user-pool` | Get pool details |\n| `aws cognito-idp update-user-pool` | Update pool settings |\n| `aws cognito-idp delete-user-pool` | Delete pool |\n| `aws cognito-idp list-user-pools` | List pools |\n\n### Users\n\n| Command | Description |\n|---------|-------------|\n| `aws cognito-idp admin-create-user` | Create user (admin) |\n| `aws cognito-idp admin-delete-user` | Delete user |\n| `aws cognito-idp admin-get-user` | Get user details |\n| `aws cognito-idp list-users` | List users |\n| `aws cognito-idp admin-set-user-password` | Set password |\n| `aws cognito-idp admin-disable-user` | Disable user |\n\n### Authentication\n\n| Command | Description |\n|---------|-------------|\n| `aws cognito-idp initiate-auth` | Start authentication |\n| `aws cognito-idp respond-to-auth-challenge` | Respond to MFA |\n| `aws cognito-idp admin-initiate-auth` | Admin authentication |\n\n## Best Practices\n\n### Security\n\n- **Enable MFA** for all users (at least optional)\n- **Use strong password policies**\n- **Enable advanced security features** (adaptive auth)\n- **Verify email/phone** before allowing sign-in\n- **Use short token lifetimes** for sensitive apps\n- **Never expose client secrets** in frontend code\n\n### User Experience\n\n- **Use hosted UI** for quick implementation\n- **Customize UI** with CSS\n- **Implement proper error handling**\n- **Provide clear password requirements**\n\n### Architecture\n\n- **Use identity pools** for AWS resource access\n- **Use access tokens** for API Gateway\n- **Store refresh tokens securely**\n- **Implement token refresh** before expiry\n\n## Troubleshooting\n\n### User Cannot Sign In\n\n**Causes:**\n- User not confirmed\n- Password incorrect\n- User disabled\n- Account locked (too many attempts)\n\n**Debug:**\n\n```bash\naws cognito-idp admin-get-user \\\n  --user-pool-id us-east-1_abc123 \\\n  --username user@example.com\n```\n\n### Token Validation Failed\n\n**Causes:**\n- Token expired\n- Wrong user pool/client ID\n- Token signature invalid\n\n**Validate JWT:**\n\n```python\nimport jwt\nimport requests\n\n# Get JWKS\njwks_url = f'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123/.well-known/jwks.json'\njwks = requests.get(jwks_url).json()\n\n# Decode and verify (use python-jose or similar)\nfrom jose import jwt\n\nclaims = jwt.decode(\n    token,\n    jwks,\n    algorithms=['RS256'],\n    audience='client-id',\n    issuer='https://cognito-idp.us-east-1.amazonaws.com/us-east-1_abc123'\n)\n```\n\n### Hosted UI Not Working\n\n**Check:**\n- Callback URLs configured correctly\n- Domain configured for user pool\n- OAuth settings enabled\n\n```bash\n# Check domain\naws cognito-idp describe-user-pool \\\n  --user-pool-id us-east-1_abc123 \\\n  --query 'UserPool.Domain'\n```\n\n### Rate Limiting\n\n**Symptom:** `TooManyRequestsException`\n\n**Solutions:**\n- Implement exponential backoff\n- Request quota increase\n- Cache tokens appropriately\n\n## References\n\n- [Cognito Developer Guide](https://docs.aws.amazon.com/cognito/latest/developerguide/)\n- [Cognito User Pools API](https://docs.aws.amazon.com/cognito-user-identity-pools/latest/APIReference/)\n- [Cognito Identity API](https://docs.aws.amazon.com/cognitoidentity/latest/APIReference/)\n- [Cognito CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/cognito-idp/)"
              },
              {
                "name": "dynamodb",
                "description": "AWS DynamoDB NoSQL database for scalable data storage. Use when designing table schemas, writing queries, configuring indexes, managing capacity, implementing single-table design, or troubleshooting performance issues.",
                "path": "skills/dynamodb/SKILL.md",
                "frontmatter": {
                  "name": "dynamodb",
                  "description": "AWS DynamoDB NoSQL database for scalable data storage. Use when designing table schemas, writing queries, configuring indexes, managing capacity, implementing single-table design, or troubleshooting performance issues.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/"
                },
                "content": "# AWS DynamoDB\n\nAmazon DynamoDB is a fully managed NoSQL database service providing fast, predictable performance at any scale. It supports key-value and document data structures.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Keys\n\n| Key Type | Description |\n|----------|-------------|\n| **Partition Key (PK)** | Required. Determines data distribution |\n| **Sort Key (SK)** | Optional. Enables range queries within partition |\n| **Composite Key** | PK + SK combination |\n\n### Secondary Indexes\n\n| Index Type | Description |\n|------------|-------------|\n| **GSI (Global Secondary Index)** | Different PK/SK, separate throughput, eventually consistent |\n| **LSI (Local Secondary Index)** | Same PK, different SK, shares table throughput, strongly consistent option |\n\n### Capacity Modes\n\n| Mode | Use Case |\n|------|----------|\n| **On-Demand** | Unpredictable traffic, pay-per-request |\n| **Provisioned** | Predictable traffic, lower cost, can use auto-scaling |\n\n## Common Patterns\n\n### Create a Table\n\n**AWS CLI:**\n\n```bash\naws dynamodb create-table \\\n  --table-name Users \\\n  --attribute-definitions \\\n    AttributeName=PK,AttributeType=S \\\n    AttributeName=SK,AttributeType=S \\\n  --key-schema \\\n    AttributeName=PK,KeyType=HASH \\\n    AttributeName=SK,KeyType=RANGE \\\n  --billing-mode PAY_PER_REQUEST\n```\n\n**boto3:**\n\n```python\nimport boto3\n\ndynamodb = boto3.resource('dynamodb')\n\ntable = dynamodb.create_table(\n    TableName='Users',\n    KeySchema=[\n        {'AttributeName': 'PK', 'KeyType': 'HASH'},\n        {'AttributeName': 'SK', 'KeyType': 'RANGE'}\n    ],\n    AttributeDefinitions=[\n        {'AttributeName': 'PK', 'AttributeType': 'S'},\n        {'AttributeName': 'SK', 'AttributeType': 'S'}\n    ],\n    BillingMode='PAY_PER_REQUEST'\n)\n\ntable.wait_until_exists()\n```\n\n### Basic CRUD Operations\n\n```python\nimport boto3\nfrom boto3.dynamodb.conditions import Key, Attr\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('Users')\n\n# Put item\ntable.put_item(\n    Item={\n        'PK': 'USER#123',\n        'SK': 'PROFILE',\n        'name': 'John Doe',\n        'email': 'john@example.com',\n        'created_at': '2024-01-15T10:30:00Z'\n    }\n)\n\n# Get item\nresponse = table.get_item(\n    Key={'PK': 'USER#123', 'SK': 'PROFILE'}\n)\nitem = response.get('Item')\n\n# Update item\ntable.update_item(\n    Key={'PK': 'USER#123', 'SK': 'PROFILE'},\n    UpdateExpression='SET #name = :name, updated_at = :updated',\n    ExpressionAttributeNames={'#name': 'name'},\n    ExpressionAttributeValues={\n        ':name': 'John Smith',\n        ':updated': '2024-01-16T10:30:00Z'\n    }\n)\n\n# Delete item\ntable.delete_item(\n    Key={'PK': 'USER#123', 'SK': 'PROFILE'}\n)\n```\n\n### Query Operations\n\n```python\n# Query by partition key\nresponse = table.query(\n    KeyConditionExpression=Key('PK').eq('USER#123')\n)\n\n# Query with sort key condition\nresponse = table.query(\n    KeyConditionExpression=Key('PK').eq('USER#123') & Key('SK').begins_with('ORDER#')\n)\n\n# Query with filter\nresponse = table.query(\n    KeyConditionExpression=Key('PK').eq('USER#123'),\n    FilterExpression=Attr('status').eq('active')\n)\n\n# Query with projection\nresponse = table.query(\n    KeyConditionExpression=Key('PK').eq('USER#123'),\n    ProjectionExpression='PK, SK, #name, email',\n    ExpressionAttributeNames={'#name': 'name'}\n)\n\n# Paginated query\npaginator = dynamodb.meta.client.get_paginator('query')\nfor page in paginator.paginate(\n    TableName='Users',\n    KeyConditionExpression='PK = :pk',\n    ExpressionAttributeValues={':pk': {'S': 'USER#123'}}\n):\n    for item in page['Items']:\n        print(item)\n```\n\n### Batch Operations\n\n```python\n# Batch write (up to 25 items)\nwith table.batch_writer() as batch:\n    for i in range(100):\n        batch.put_item(Item={\n            'PK': f'USER#{i}',\n            'SK': 'PROFILE',\n            'name': f'User {i}'\n        })\n\n# Batch get (up to 100 items)\ndynamodb = boto3.resource('dynamodb')\nresponse = dynamodb.batch_get_item(\n    RequestItems={\n        'Users': {\n            'Keys': [\n                {'PK': 'USER#1', 'SK': 'PROFILE'},\n                {'PK': 'USER#2', 'SK': 'PROFILE'}\n            ]\n        }\n    }\n)\n```\n\n### Create GSI\n\n```bash\naws dynamodb update-table \\\n  --table-name Users \\\n  --attribute-definitions AttributeName=email,AttributeType=S \\\n  --global-secondary-index-updates '[\n    {\n      \"Create\": {\n        \"IndexName\": \"email-index\",\n        \"KeySchema\": [{\"AttributeName\": \"email\", \"KeyType\": \"HASH\"}],\n        \"Projection\": {\"ProjectionType\": \"ALL\"}\n      }\n    }\n  ]'\n```\n\n### Conditional Writes\n\n```python\nfrom botocore.exceptions import ClientError\n\n# Only put if item doesn't exist\ntry:\n    table.put_item(\n        Item={'PK': 'USER#123', 'SK': 'PROFILE', 'name': 'John'},\n        ConditionExpression='attribute_not_exists(PK)'\n    )\nexcept ClientError as e:\n    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n        print(\"Item already exists\")\n\n# Optimistic locking with version\ntable.update_item(\n    Key={'PK': 'USER#123', 'SK': 'PROFILE'},\n    UpdateExpression='SET #name = :name, version = version + :inc',\n    ConditionExpression='version = :current_version',\n    ExpressionAttributeNames={'#name': 'name'},\n    ExpressionAttributeValues={\n        ':name': 'New Name',\n        ':inc': 1,\n        ':current_version': 5\n    }\n)\n```\n\n## CLI Reference\n\n### Table Operations\n\n| Command | Description |\n|---------|-------------|\n| `aws dynamodb create-table` | Create table |\n| `aws dynamodb describe-table` | Get table info |\n| `aws dynamodb update-table` | Modify table/indexes |\n| `aws dynamodb delete-table` | Delete table |\n| `aws dynamodb list-tables` | List all tables |\n\n### Item Operations\n\n| Command | Description |\n|---------|-------------|\n| `aws dynamodb put-item` | Create/replace item |\n| `aws dynamodb get-item` | Read single item |\n| `aws dynamodb update-item` | Update item attributes |\n| `aws dynamodb delete-item` | Delete item |\n| `aws dynamodb query` | Query by key |\n| `aws dynamodb scan` | Full table scan |\n\n### Batch Operations\n\n| Command | Description |\n|---------|-------------|\n| `aws dynamodb batch-write-item` | Batch write (25 max) |\n| `aws dynamodb batch-get-item` | Batch read (100 max) |\n| `aws dynamodb transact-write-items` | Transaction write |\n| `aws dynamodb transact-get-items` | Transaction read |\n\n## Best Practices\n\n### Data Modeling\n\n- **Design for access patterns** â€” know your queries before designing\n- **Use composite keys** â€” PK for grouping, SK for sorting/filtering\n- **Prefer query over scan** â€” scans are expensive\n- **Use sparse indexes** â€” only items with index attributes are indexed\n- **Consider single-table design** for related entities\n\n### Performance\n\n- **Distribute partition keys evenly** â€” avoid hot partitions\n- **Use batch operations** to reduce API calls\n- **Enable DAX** for read-heavy workloads\n- **Use projections** to reduce data transfer\n\n### Cost Optimization\n\n- **Use on-demand** for variable workloads\n- **Use provisioned + auto-scaling** for predictable workloads\n- **Set TTL** for expiring data\n- **Archive to S3** for cold data\n\n## Troubleshooting\n\n### Throttling\n\n**Symptom:** `ProvisionedThroughputExceededException`\n\n**Causes:**\n- Hot partition (uneven key distribution)\n- Burst traffic exceeding capacity\n- GSI throttling affecting base table\n\n**Solutions:**\n\n```python\n# Use exponential backoff\nimport time\nfrom botocore.config import Config\n\nconfig = Config(\n    retries={\n        'max_attempts': 10,\n        'mode': 'adaptive'\n    }\n)\ndynamodb = boto3.resource('dynamodb', config=config)\n```\n\n### Hot Partitions\n\n**Debug:**\n\n```bash\n# Check consumed capacity by partition\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ConsumedReadCapacityUnits \\\n  --dimensions Name=TableName,Value=Users \\\n  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Sum\n```\n\n**Solutions:**\n- Add randomness to partition keys\n- Use write sharding\n- Distribute access across partitions\n\n### Query Returns No Items\n\n**Debug checklist:**\n1. Verify key values exactly match (case-sensitive)\n2. Check key types (S, N, B)\n3. Confirm table/index name\n4. Review filter expressions (they apply AFTER read)\n\n### Scan Performance\n\n**Issue:** Scans are slow and expensive\n\n**Solutions:**\n- Use parallel scan for large tables\n- Create GSI for the access pattern\n- Use filter expressions to reduce returned data\n\n```python\n# Parallel scan\nimport concurrent.futures\n\ndef scan_segment(segment, total_segments):\n    return table.scan(\n        Segment=segment,\n        TotalSegments=total_segments\n    )\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = list(executor.map(\n        lambda s: scan_segment(s, 4),\n        range(4)\n    ))\n```\n\n## References\n\n- [DynamoDB Developer Guide](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/)\n- [DynamoDB API Reference](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/)\n- [DynamoDB CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/dynamodb/)\n- [boto3 DynamoDB](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html)\n- [DynamoDB Best Practices](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html)"
              },
              {
                "name": "ec2",
                "description": "AWS EC2 virtual machine management for instances, AMIs, and networking. Use when launching instances, configuring security groups, managing key pairs, troubleshooting connectivity, or automating instance lifecycle.",
                "path": "skills/ec2/SKILL.md",
                "frontmatter": {
                  "name": "ec2",
                  "description": "AWS EC2 virtual machine management for instances, AMIs, and networking. Use when launching instances, configuring security groups, managing key pairs, troubleshooting connectivity, or automating instance lifecycle.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/"
                },
                "content": "# AWS EC2\n\nAmazon Elastic Compute Cloud (EC2) provides resizable compute capacity in the cloud. Launch virtual servers, configure networking and security, and manage storage.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Instance Types\n\n| Category | Example | Use Case |\n|----------|---------|----------|\n| General Purpose | t3, m6i | Web servers, dev environments |\n| Compute Optimized | c6i | Batch processing, gaming |\n| Memory Optimized | r6i | Databases, caching |\n| Storage Optimized | i3, d3 | Data warehousing |\n| Accelerated | p4d, g5 | ML, graphics |\n\n### Purchasing Options\n\n| Option | Description |\n|--------|-------------|\n| On-Demand | Pay by the hour/second |\n| Reserved | 1-3 year commitment, up to 72% discount |\n| Spot | Unused capacity, up to 90% discount |\n| Savings Plans | Flexible commitment-based discount |\n\n### AMI (Amazon Machine Image)\n\nTemplate containing OS, software, and configuration for launching instances.\n\n### Security Groups\n\nVirtual firewalls controlling inbound and outbound traffic.\n\n## Common Patterns\n\n### Launch an Instance\n\n**AWS CLI:**\n\n```bash\n# Create key pair\naws ec2 create-key-pair \\\n  --key-name my-key \\\n  --query 'KeyMaterial' \\\n  --output text > my-key.pem\nchmod 400 my-key.pem\n\n# Create security group\naws ec2 create-security-group \\\n  --group-name web-server-sg \\\n  --description \"Web server security group\" \\\n  --vpc-id vpc-12345678\n\n# Allow SSH and HTTP\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-12345678 \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr 10.0.0.0/8\n\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-12345678 \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n\n# Launch instance\naws ec2 run-instances \\\n  --image-id ami-0123456789abcdef0 \\\n  --instance-type t3.micro \\\n  --key-name my-key \\\n  --security-group-ids sg-12345678 \\\n  --subnet-id subnet-12345678 \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=web-server}]'\n```\n\n**boto3:**\n\n```python\nimport boto3\n\nec2 = boto3.resource('ec2')\n\ninstances = ec2.create_instances(\n    ImageId='ami-0123456789abcdef0',\n    InstanceType='t3.micro',\n    KeyName='my-key',\n    SecurityGroupIds=['sg-12345678'],\n    SubnetId='subnet-12345678',\n    MinCount=1,\n    MaxCount=1,\n    TagSpecifications=[{\n        'ResourceType': 'instance',\n        'Tags': [{'Key': 'Name', 'Value': 'web-server'}]\n    }]\n)\n\ninstance = instances[0]\ninstance.wait_until_running()\ninstance.reload()\nprint(f\"Instance ID: {instance.id}\")\nprint(f\"Public IP: {instance.public_ip_address}\")\n```\n\n### User Data Script\n\n```bash\naws ec2 run-instances \\\n  --image-id ami-0123456789abcdef0 \\\n  --instance-type t3.micro \\\n  --key-name my-key \\\n  --security-group-ids sg-12345678 \\\n  --subnet-id subnet-12345678 \\\n  --user-data '#!/bin/bash\n    yum update -y\n    yum install -y httpd\n    systemctl start httpd\n    systemctl enable httpd\n    echo \"<h1>Hello from $(hostname)</h1>\" > /var/www/html/index.html\n  '\n```\n\n### Attach IAM Role\n\n```bash\n# Create instance profile\naws iam create-instance-profile \\\n  --instance-profile-name web-server-profile\n\naws iam add-role-to-instance-profile \\\n  --instance-profile-name web-server-profile \\\n  --role-name web-server-role\n\n# Launch with profile\naws ec2 run-instances \\\n  --image-id ami-0123456789abcdef0 \\\n  --instance-type t3.micro \\\n  --iam-instance-profile Name=web-server-profile \\\n  ...\n```\n\n### Create AMI from Instance\n\n```bash\naws ec2 create-image \\\n  --instance-id i-1234567890abcdef0 \\\n  --name \"my-custom-ami-$(date +%Y%m%d)\" \\\n  --description \"Custom AMI with web server\" \\\n  --no-reboot\n```\n\n### Spot Instance Request\n\n```bash\naws ec2 request-spot-instances \\\n  --instance-count 1 \\\n  --type \"one-time\" \\\n  --launch-specification '{\n    \"ImageId\": \"ami-0123456789abcdef0\",\n    \"InstanceType\": \"c5.large\",\n    \"KeyName\": \"my-key\",\n    \"SecurityGroupIds\": [\"sg-12345678\"],\n    \"SubnetId\": \"subnet-12345678\"\n  }' \\\n  --spot-price \"0.05\"\n```\n\n### EBS Volume Management\n\n```bash\n# Create volume\naws ec2 create-volume \\\n  --availability-zone us-east-1a \\\n  --size 100 \\\n  --volume-type gp3 \\\n  --iops 3000 \\\n  --throughput 125 \\\n  --encrypted\n\n# Attach to instance\naws ec2 attach-volume \\\n  --volume-id vol-12345678 \\\n  --instance-id i-1234567890abcdef0 \\\n  --device /dev/sdf\n\n# Create snapshot\naws ec2 create-snapshot \\\n  --volume-id vol-12345678 \\\n  --description \"Daily backup\"\n```\n\n## CLI Reference\n\n### Instance Management\n\n| Command | Description |\n|---------|-------------|\n| `aws ec2 run-instances` | Launch instances |\n| `aws ec2 describe-instances` | List instances |\n| `aws ec2 start-instances` | Start stopped instances |\n| `aws ec2 stop-instances` | Stop running instances |\n| `aws ec2 reboot-instances` | Reboot instances |\n| `aws ec2 terminate-instances` | Terminate instances |\n| `aws ec2 modify-instance-attribute` | Modify instance settings |\n\n### Security Groups\n\n| Command | Description |\n|---------|-------------|\n| `aws ec2 create-security-group` | Create security group |\n| `aws ec2 describe-security-groups` | List security groups |\n| `aws ec2 authorize-security-group-ingress` | Add inbound rule |\n| `aws ec2 revoke-security-group-ingress` | Remove inbound rule |\n| `aws ec2 authorize-security-group-egress` | Add outbound rule |\n\n### AMIs\n\n| Command | Description |\n|---------|-------------|\n| `aws ec2 describe-images` | List AMIs |\n| `aws ec2 create-image` | Create AMI from instance |\n| `aws ec2 copy-image` | Copy AMI to another region |\n| `aws ec2 deregister-image` | Delete AMI |\n\n### EBS Volumes\n\n| Command | Description |\n|---------|-------------|\n| `aws ec2 create-volume` | Create EBS volume |\n| `aws ec2 attach-volume` | Attach to instance |\n| `aws ec2 detach-volume` | Detach from instance |\n| `aws ec2 create-snapshot` | Create snapshot |\n| `aws ec2 modify-volume` | Resize/modify volume |\n\n## Best Practices\n\n### Security\n\n- **Use IAM roles** instead of access keys on instances\n- **Restrict security groups** â€” principle of least privilege\n- **Use private subnets** for backend instances\n- **Enable IMDSv2** to prevent SSRF attacks\n- **Encrypt EBS volumes** at rest\n\n```bash\n# Require IMDSv2\naws ec2 modify-instance-metadata-options \\\n  --instance-id i-1234567890abcdef0 \\\n  --http-tokens required \\\n  --http-endpoint enabled\n```\n\n### Performance\n\n- **Right-size instances** â€” monitor and adjust\n- **Use EBS-optimized instances**\n- **Choose appropriate EBS volume type**\n- **Use placement groups** for low-latency networking\n\n### Cost Optimization\n\n- **Use Spot Instances** for fault-tolerant workloads\n- **Stop/terminate unused instances**\n- **Use Reserved Instances** for steady-state workloads\n- **Delete unused EBS volumes and snapshots**\n\n### Reliability\n\n- **Use Auto Scaling Groups** for high availability\n- **Deploy across multiple AZs**\n- **Use Elastic Load Balancer** for traffic distribution\n- **Implement health checks**\n\n## Troubleshooting\n\n### Cannot SSH to Instance\n\n**Checklist:**\n\n1. Security group allows SSH (port 22) from your IP\n2. Instance has public IP or use bastion/SSM\n3. Key pair matches instance\n4. Instance is running\n5. Network ACL allows traffic\n\n```bash\n# Check security group\naws ec2 describe-security-groups --group-ids sg-12345678\n\n# Check instance state\naws ec2 describe-instances \\\n  --instance-ids i-1234567890abcdef0 \\\n  --query \"Reservations[].Instances[].{State:State.Name,PublicIP:PublicIpAddress}\"\n```\n\n**Use Session Manager instead:**\n\n```bash\naws ssm start-session --target i-1234567890abcdef0\n```\n\n### Instance Won't Start\n\n**Causes:**\n- Reached instance limits\n- Insufficient capacity in AZ\n- EBS volume issue\n- Invalid AMI\n\n```bash\n# Check instance state reason\naws ec2 describe-instances \\\n  --instance-ids i-1234567890abcdef0 \\\n  --query \"Reservations[].Instances[].StateReason\"\n```\n\n### Instance Unreachable\n\n**Debug:**\n\n```bash\n# Check instance status\naws ec2 describe-instance-status \\\n  --instance-ids i-1234567890abcdef0\n\n# Get console output\naws ec2 get-console-output \\\n  --instance-id i-1234567890abcdef0\n\n# Get screenshot (for Windows/GUI issues)\naws ec2 get-console-screenshot \\\n  --instance-id i-1234567890abcdef0\n```\n\n### High CPU/Memory\n\n```bash\n# Enable detailed monitoring\naws ec2 monitor-instances \\\n  --instance-ids i-1234567890abcdef0\n\n# Check CloudWatch metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/EC2 \\\n  --metric-name CPUUtilization \\\n  --dimensions Name=InstanceId,Value=i-1234567890abcdef0 \\\n  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Average\n```\n\n## References\n\n- [EC2 User Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/)\n- [EC2 API Reference](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/)\n- [EC2 CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/ec2/)\n- [boto3 EC2](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html)"
              },
              {
                "name": "ecs",
                "description": "AWS ECS container orchestration for running Docker containers. Use when deploying containerized applications, configuring task definitions, setting up services, managing clusters, or troubleshooting container issues.",
                "path": "skills/ecs/SKILL.md",
                "frontmatter": {
                  "name": "ecs",
                  "description": "AWS ECS container orchestration for running Docker containers. Use when deploying containerized applications, configuring task definitions, setting up services, managing clusters, or troubleshooting container issues.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/"
                },
                "content": "# AWS ECS\n\nAmazon Elastic Container Service (ECS) is a fully managed container orchestration service. Run containers on AWS Fargate (serverless) or EC2 instances.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Cluster\n\nLogical grouping of tasks or services. Can contain Fargate tasks, EC2 instances, or both.\n\n### Task Definition\n\nBlueprint for your application. Defines containers, resources, networking, and IAM roles.\n\n### Task\n\nRunning instance of a task definition. Can run standalone or as part of a service.\n\n### Service\n\nMaintains desired count of tasks. Handles deployments, load balancing, and auto scaling.\n\n### Launch Types\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **Fargate** | Serverless, pay per task | Most workloads |\n| **EC2** | Self-managed instances | GPU, Windows, specific requirements |\n\n## Common Patterns\n\n### Create a Fargate Cluster\n\n**AWS CLI:**\n\n```bash\n# Create cluster\naws ecs create-cluster --cluster-name my-cluster\n\n# With capacity providers\naws ecs create-cluster \\\n  --cluster-name my-cluster \\\n  --capacity-providers FARGATE FARGATE_SPOT \\\n  --default-capacity-provider-strategy \\\n    capacityProvider=FARGATE,weight=1 \\\n    capacityProvider=FARGATE_SPOT,weight=1\n```\n\n### Register Task Definition\n\n```bash\ncat > task-definition.json << 'EOF'\n{\n  \"family\": \"web-app\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"256\",\n  \"memory\": \"512\",\n  \"executionRoleArn\": \"arn:aws:iam::123456789012:role/ecsTaskExecutionRole\",\n  \"taskRoleArn\": \"arn:aws:iam::123456789012:role/ecsTaskRole\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"web\",\n      \"image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-app:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\"name\": \"NODE_ENV\", \"value\": \"production\"}\n      ],\n      \"secrets\": [\n        {\n          \"name\": \"DB_PASSWORD\",\n          \"valueFrom\": \"arn:aws:secretsmanager:us-east-1:123456789012:secret:db-password\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/web-app\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      },\n      \"healthCheck\": {\n        \"command\": [\"CMD-SHELL\", \"curl -f http://localhost:8080/health || exit 1\"],\n        \"interval\": 30,\n        \"timeout\": 5,\n        \"retries\": 3,\n        \"startPeriod\": 60\n      }\n    }\n  ]\n}\nEOF\n\naws ecs register-task-definition --cli-input-json file://task-definition.json\n```\n\n### Create Service with Load Balancer\n\n```bash\naws ecs create-service \\\n  --cluster my-cluster \\\n  --service-name web-service \\\n  --task-definition web-app:1 \\\n  --desired-count 2 \\\n  --launch-type FARGATE \\\n  --network-configuration \"awsvpcConfiguration={\n    subnets=[subnet-12345678,subnet-87654321],\n    securityGroups=[sg-12345678],\n    assignPublicIp=DISABLED\n  }\" \\\n  --load-balancers \"targetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/web-tg/1234567890123456,containerName=web,containerPort=8080\" \\\n  --health-check-grace-period-seconds 60\n```\n\n### Run Standalone Task\n\n```bash\naws ecs run-task \\\n  --cluster my-cluster \\\n  --task-definition my-batch-job:1 \\\n  --launch-type FARGATE \\\n  --network-configuration \"awsvpcConfiguration={\n    subnets=[subnet-12345678],\n    securityGroups=[sg-12345678],\n    assignPublicIp=ENABLED\n  }\"\n```\n\n### Update Service (Deploy New Image)\n\n```bash\n# Register new task definition with updated image\naws ecs register-task-definition --cli-input-json file://task-definition.json\n\n# Update service to use new version\naws ecs update-service \\\n  --cluster my-cluster \\\n  --service web-service \\\n  --task-definition web-app:2 \\\n  --force-new-deployment\n```\n\n### Auto Scaling\n\n```bash\n# Register scalable target\naws application-autoscaling register-scalable-target \\\n  --service-namespace ecs \\\n  --resource-id service/my-cluster/web-service \\\n  --scalable-dimension ecs:service:DesiredCount \\\n  --min-capacity 2 \\\n  --max-capacity 10\n\n# Target tracking policy\naws application-autoscaling put-scaling-policy \\\n  --service-namespace ecs \\\n  --resource-id service/my-cluster/web-service \\\n  --scalable-dimension ecs:service:DesiredCount \\\n  --policy-name cpu-target-tracking \\\n  --policy-type TargetTrackingScaling \\\n  --target-tracking-scaling-policy-configuration '{\n    \"TargetValue\": 70.0,\n    \"PredefinedMetricSpecification\": {\n      \"PredefinedMetricType\": \"ECSServiceAverageCPUUtilization\"\n    },\n    \"ScaleOutCooldown\": 60,\n    \"ScaleInCooldown\": 120\n  }'\n```\n\n## CLI Reference\n\n### Cluster Management\n\n| Command | Description |\n|---------|-------------|\n| `aws ecs create-cluster` | Create cluster |\n| `aws ecs describe-clusters` | Get cluster details |\n| `aws ecs list-clusters` | List clusters |\n| `aws ecs delete-cluster` | Delete cluster |\n\n### Task Definitions\n\n| Command | Description |\n|---------|-------------|\n| `aws ecs register-task-definition` | Create task definition |\n| `aws ecs describe-task-definition` | Get task definition |\n| `aws ecs list-task-definitions` | List task definitions |\n| `aws ecs deregister-task-definition` | Deregister version |\n\n### Services\n\n| Command | Description |\n|---------|-------------|\n| `aws ecs create-service` | Create service |\n| `aws ecs update-service` | Update service |\n| `aws ecs describe-services` | Get service details |\n| `aws ecs delete-service` | Delete service |\n\n### Tasks\n\n| Command | Description |\n|---------|-------------|\n| `aws ecs run-task` | Run standalone task |\n| `aws ecs stop-task` | Stop running task |\n| `aws ecs describe-tasks` | Get task details |\n| `aws ecs list-tasks` | List tasks |\n\n## Best Practices\n\n### Security\n\n- **Use task roles** for AWS API access (not access keys)\n- **Use execution roles** for ECR/Secrets access\n- **Store secrets in Secrets Manager** or Parameter Store\n- **Use private subnets** with NAT gateway\n- **Enable CloudTrail** for API auditing\n\n### Performance\n\n- **Right-size CPU/memory** â€” monitor and adjust\n- **Use Fargate Spot** for fault-tolerant workloads (70% savings)\n- **Enable container insights** for monitoring\n- **Use service discovery** for internal communication\n\n### Reliability\n\n- **Deploy across multiple AZs**\n- **Configure health checks** properly\n- **Set appropriate deregistration delay**\n- **Use circuit breaker** for deployments\n\n```bash\naws ecs update-service \\\n  --cluster my-cluster \\\n  --service web-service \\\n  --deployment-configuration '{\n    \"deploymentCircuitBreaker\": {\n      \"enable\": true,\n      \"rollback\": true\n    }\n  }'\n```\n\n### Cost Optimization\n\n- **Use Fargate Spot** for batch workloads\n- **Right-size task resources**\n- **Scale to zero** when not needed\n- **Use capacity providers** for mixed Fargate/Spot\n\n## Troubleshooting\n\n### Task Fails to Start\n\n**Check:**\n\n```bash\n# View stopped tasks\naws ecs describe-tasks \\\n  --cluster my-cluster \\\n  --tasks $(aws ecs list-tasks --cluster my-cluster --desired-status STOPPED --query 'taskArns[0]' --output text)\n```\n\n**Common causes:**\n- Image not found (ECR permissions)\n- Secrets access denied\n- Network configuration (subnets, security groups)\n- Resource limits exceeded\n\n### Container Keeps Restarting\n\n**Debug:**\n\n```bash\n# Check CloudWatch logs\naws logs get-log-events \\\n  --log-group-name /ecs/web-app \\\n  --log-stream-name \"ecs/web/abc123\"\n\n# Check task details\naws ecs describe-tasks \\\n  --cluster my-cluster \\\n  --tasks task-arn \\\n  --query 'tasks[0].containers[0].{reason:reason,exitCode:exitCode}'\n```\n\n**Causes:**\n- Health check failing\n- Application crashing\n- Out of memory\n\n### Service Stuck Deploying\n\n```bash\n# Check deployment status\naws ecs describe-services \\\n  --cluster my-cluster \\\n  --services web-service \\\n  --query 'services[0].deployments'\n\n# Check events\naws ecs describe-services \\\n  --cluster my-cluster \\\n  --services web-service \\\n  --query 'services[0].events[:5]'\n```\n\n**Causes:**\n- Health check failing on new tasks\n- Not enough capacity\n- Target group health checks failing\n\n### Cannot Pull Image from ECR\n\n**Check execution role has:**\n\n```json\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"ecr:GetAuthorizationToken\",\n    \"ecr:BatchCheckLayerAvailability\",\n    \"ecr:GetDownloadUrlForLayer\",\n    \"ecr:BatchGetImage\"\n  ],\n  \"Resource\": \"*\"\n}\n```\n\n**Also check:**\n- VPC endpoint for ECR (if private subnet)\n- NAT gateway (if private subnet)\n- Security group allows HTTPS outbound\n\n## References\n\n- [ECS Developer Guide](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/)\n- [ECS API Reference](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/)\n- [ECS CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/ecs/)\n- [boto3 ECS](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html)"
              },
              {
                "name": "eks",
                "description": "AWS EKS Kubernetes management for clusters, node groups, and workloads. Use when creating clusters, configuring IRSA, managing node groups, deploying applications, or integrating with AWS services.",
                "path": "skills/eks/SKILL.md",
                "frontmatter": {
                  "name": "eks",
                  "description": "AWS EKS Kubernetes management for clusters, node groups, and workloads. Use when creating clusters, configuring IRSA, managing node groups, deploying applications, or integrating with AWS services.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/eks/latest/userguide/"
                },
                "content": "# AWS EKS\n\nAmazon Elastic Kubernetes Service (EKS) runs Kubernetes without installing and operating your own control plane. EKS manages the control plane and integrates with AWS services.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Control Plane\n\nManaged by AWS. Runs Kubernetes API server, etcd, and controllers across multiple AZs.\n\n### Node Groups\n\n| Type | Description |\n|------|-------------|\n| **Managed** | AWS manages provisioning, updates |\n| **Self-managed** | You manage EC2 instances |\n| **Fargate** | Serverless, per-pod compute |\n\n### IRSA (IAM Roles for Service Accounts)\n\nAssociates Kubernetes service accounts with IAM roles for fine-grained AWS permissions.\n\n### Add-ons\n\nOperational software: CoreDNS, kube-proxy, VPC CNI, EBS CSI driver.\n\n## Common Patterns\n\n### Create a Cluster\n\n**AWS CLI:**\n\n```bash\n# Create cluster role\naws iam create-role \\\n  --role-name eks-cluster-role \\\n  --assume-role-policy-document '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"Service\": \"eks.amazonaws.com\"},\n      \"Action\": \"sts:AssumeRole\"\n    }]\n  }'\n\naws iam attach-role-policy \\\n  --role-name eks-cluster-role \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\n\n# Create cluster\naws eks create-cluster \\\n  --name my-cluster \\\n  --role-arn arn:aws:iam::123456789012:role/eks-cluster-role \\\n  --resources-vpc-config subnetIds=subnet-12345678,subnet-87654321,securityGroupIds=sg-12345678\n\n# Wait for cluster\naws eks wait cluster-active --name my-cluster\n\n# Update kubeconfig\naws eks update-kubeconfig --name my-cluster --region us-east-1\n```\n\n**eksctl (Recommended):**\n\n```bash\n# Create cluster with managed node group\neksctl create cluster \\\n  --name my-cluster \\\n  --region us-east-1 \\\n  --version 1.29 \\\n  --nodegroup-name standard-workers \\\n  --node-type t3.medium \\\n  --nodes 3 \\\n  --nodes-min 1 \\\n  --nodes-max 5 \\\n  --managed\n```\n\n### Add Managed Node Group\n\n```bash\n# Create node role\naws iam create-role \\\n  --role-name eks-node-role \\\n  --assume-role-policy-document '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"Service\": \"ec2.amazonaws.com\"},\n      \"Action\": \"sts:AssumeRole\"\n    }]\n  }'\n\naws iam attach-role-policy --role-name eks-node-role --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\naws iam attach-role-policy --role-name eks-node-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\naws iam attach-role-policy --role-name eks-node-role --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n\n# Create node group\naws eks create-nodegroup \\\n  --cluster-name my-cluster \\\n  --nodegroup-name standard-workers \\\n  --node-role arn:aws:iam::123456789012:role/eks-node-role \\\n  --subnets subnet-12345678 subnet-87654321 \\\n  --instance-types t3.medium \\\n  --scaling-config minSize=1,maxSize=5,desiredSize=3 \\\n  --ami-type AL2_x86_64\n```\n\n### Configure IRSA\n\n```bash\n# Enable OIDC provider\neksctl utils associate-iam-oidc-provider \\\n  --cluster my-cluster \\\n  --approve\n\n# Create IAM role for service account\neksctl create iamserviceaccount \\\n  --cluster my-cluster \\\n  --namespace default \\\n  --name my-app-sa \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess \\\n  --approve\n```\n\n**Manual IRSA setup:**\n\n```bash\n# Get OIDC issuer\nOIDC_ISSUER=$(aws eks describe-cluster --name my-cluster --query \"cluster.identity.oidc.issuer\" --output text)\nOIDC_ID=${OIDC_ISSUER##*/}\n\n# Create trust policy\ncat > trust-policy.json << EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Federated\": \"arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/${OIDC_ID}\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n      \"StringEquals\": {\n        \"oidc.eks.us-east-1.amazonaws.com/id/${OIDC_ID}:sub\": \"system:serviceaccount:default:my-app-sa\",\n        \"oidc.eks.us-east-1.amazonaws.com/id/${OIDC_ID}:aud\": \"sts.amazonaws.com\"\n      }\n    }\n  }]\n}\nEOF\n\naws iam create-role --role-name my-app-role --assume-role-policy-document file://trust-policy.json\n```\n\n### Kubernetes Service Account\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-app-sa\n  namespace: default\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/my-app-role\n```\n\n### Install Add-ons\n\n```bash\n# CoreDNS\naws eks create-addon \\\n  --cluster-name my-cluster \\\n  --addon-name coredns \\\n  --addon-version v1.11.1-eksbuild.4\n\n# VPC CNI\naws eks create-addon \\\n  --cluster-name my-cluster \\\n  --addon-name vpc-cni \\\n  --addon-version v1.16.0-eksbuild.1\n\n# kube-proxy\naws eks create-addon \\\n  --cluster-name my-cluster \\\n  --addon-name kube-proxy \\\n  --addon-version v1.29.0-eksbuild.1\n\n# EBS CSI Driver\naws eks create-addon \\\n  --cluster-name my-cluster \\\n  --addon-name aws-ebs-csi-driver \\\n  --addon-version v1.27.0-eksbuild.1 \\\n  --service-account-role-arn arn:aws:iam::123456789012:role/ebs-csi-role\n```\n\n### Deploy Application\n\n```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      serviceAccountName: my-app-sa\n      containers:\n      - name: app\n        image: 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-app:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: my-app\n```\n\n## CLI Reference\n\n### Cluster Management\n\n| Command | Description |\n|---------|-------------|\n| `aws eks create-cluster` | Create cluster |\n| `aws eks describe-cluster` | Get cluster details |\n| `aws eks update-cluster-config` | Update cluster settings |\n| `aws eks delete-cluster` | Delete cluster |\n| `aws eks update-kubeconfig` | Configure kubectl |\n\n### Node Groups\n\n| Command | Description |\n|---------|-------------|\n| `aws eks create-nodegroup` | Create node group |\n| `aws eks describe-nodegroup` | Get node group details |\n| `aws eks update-nodegroup-config` | Update node group |\n| `aws eks delete-nodegroup` | Delete node group |\n\n### Add-ons\n\n| Command | Description |\n|---------|-------------|\n| `aws eks create-addon` | Install add-on |\n| `aws eks describe-addon` | Get add-on details |\n| `aws eks update-addon` | Update add-on |\n| `aws eks delete-addon` | Remove add-on |\n\n## Best Practices\n\n### Security\n\n- **Use IRSA** for pod-level AWS permissions\n- **Enable cluster encryption** with KMS\n- **Use private endpoint** for API server\n- **Enable audit logging** to CloudWatch\n- **Use security groups for pods**\n- **Implement network policies**\n\n```bash\n# Enable secrets encryption\naws eks create-cluster \\\n  --name my-cluster \\\n  --encryption-config '[{\n    \"provider\": {\"keyArn\": \"arn:aws:kms:us-east-1:123456789012:key/...\"},\n    \"resources\": [\"secrets\"]\n  }]' \\\n  ...\n```\n\n### High Availability\n\n- **Deploy across multiple AZs**\n- **Use managed node groups**\n- **Set pod disruption budgets**\n- **Configure horizontal pod autoscaling**\n\n### Cost Optimization\n\n- **Use Spot instances** for non-critical workloads\n- **Right-size nodes and pods**\n- **Use Fargate** for variable workloads\n- **Implement cluster autoscaler**\n- **Use Karpenter** for efficient scaling\n\n## Troubleshooting\n\n### Cannot Connect to Cluster\n\n```bash\n# Verify kubeconfig\naws eks update-kubeconfig --name my-cluster --region us-east-1\n\n# Check IAM identity\naws sts get-caller-identity\n\n# Verify cluster status\naws eks describe-cluster --name my-cluster --query 'cluster.status'\n```\n\n### Nodes Not Joining\n\n**Check:**\n- Node IAM role has required policies\n- Security groups allow node-to-control-plane communication\n- Nodes have network access to API server\n\n```bash\n# Check node status\nkubectl get nodes\n\n# Check aws-auth ConfigMap\nkubectl describe configmap aws-auth -n kube-system\n\n# Check node logs (SSH to node)\njournalctl -u kubelet\n```\n\n### Pod Cannot Access AWS Services\n\n```bash\n# Verify IRSA setup\nkubectl describe sa my-app-sa\n\n# Check pod environment\nkubectl exec my-pod -- env | grep AWS\n\n# Test credentials\nkubectl exec my-pod -- aws sts get-caller-identity\n```\n\n### DNS Issues\n\n```bash\n# Check CoreDNS pods\nkubectl get pods -n kube-system -l k8s-app=kube-dns\n\n# Test DNS resolution\nkubectl run test --image=busybox:1.28 --rm -it -- nslookup kubernetes\n\n# Check CoreDNS logs\nkubectl logs -n kube-system -l k8s-app=kube-dns\n```\n\n## References\n\n- [EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/)\n- [EKS API Reference](https://docs.aws.amazon.com/eks/latest/APIReference/)\n- [EKS CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/eks/)\n- [eksctl](https://eksctl.io/)\n- [EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/)"
              },
              {
                "name": "eventbridge",
                "description": "AWS EventBridge serverless event bus for event-driven architectures. Use when creating rules, configuring event patterns, setting up scheduled events, integrating with SaaS, or building cross-account event routing.",
                "path": "skills/eventbridge/SKILL.md",
                "frontmatter": {
                  "name": "eventbridge",
                  "description": "AWS EventBridge serverless event bus for event-driven architectures. Use when creating rules, configuring event patterns, setting up scheduled events, integrating with SaaS, or building cross-account event routing.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/eventbridge/latest/userguide/"
                },
                "content": "# AWS EventBridge\n\nAmazon EventBridge is a serverless event bus that connects applications using events. Route events from AWS services, custom applications, and SaaS partners.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Event Bus\n\nChannel that receives events. Types:\n- **Default**: Receives AWS service events\n- **Custom**: Your application events\n- **Partner**: SaaS application events\n\n### Rules\n\nMatch incoming events and route to targets. Each rule can have up to 5 targets.\n\n### Event Patterns\n\nJSON patterns that define which events match a rule.\n\n### Targets\n\nAWS services that receive matched events (Lambda, SQS, SNS, Step Functions, etc.).\n\n### Scheduler\n\nSchedule one-time or recurring events to invoke targets.\n\n## Common Patterns\n\n### Create Custom Event Bus and Rule\n\n**AWS CLI:**\n\n```bash\n# Create custom event bus\naws events create-event-bus --name my-app-events\n\n# Create rule\naws events put-rule \\\n  --name order-created-rule \\\n  --event-bus-name my-app-events \\\n  --event-pattern '{\n    \"source\": [\"my-app.orders\"],\n    \"detail-type\": [\"Order Created\"]\n  }'\n\n# Add Lambda target\naws events put-targets \\\n  --rule order-created-rule \\\n  --event-bus-name my-app-events \\\n  --targets '[{\n    \"Id\": \"process-order\",\n    \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessOrder\"\n  }]'\n\n# Add Lambda permission\naws lambda add-permission \\\n  --function-name ProcessOrder \\\n  --statement-id eventbridge-order-created \\\n  --action lambda:InvokeFunction \\\n  --principal events.amazonaws.com \\\n  --source-arn arn:aws:events:us-east-1:123456789012:rule/my-app-events/order-created-rule\n```\n\n**boto3:**\n\n```python\nimport boto3\n\nevents = boto3.client('events')\n\n# Create event bus\nevents.create_event_bus(Name='my-app-events')\n\n# Create rule\nevents.put_rule(\n    Name='order-created-rule',\n    EventBusName='my-app-events',\n    EventPattern=json.dumps({\n        'source': ['my-app.orders'],\n        'detail-type': ['Order Created']\n    }),\n    State='ENABLED'\n)\n\n# Add target\nevents.put_targets(\n    Rule='order-created-rule',\n    EventBusName='my-app-events',\n    Targets=[{\n        'Id': 'process-order',\n        'Arn': 'arn:aws:lambda:us-east-1:123456789012:function:ProcessOrder'\n    }]\n)\n```\n\n### Publish Custom Events\n\n```python\nimport boto3\nimport json\n\nevents = boto3.client('events')\n\nevents.put_events(\n    Entries=[\n        {\n            'Source': 'my-app.orders',\n            'DetailType': 'Order Created',\n            'Detail': json.dumps({\n                'order_id': '12345',\n                'customer_id': 'cust-789',\n                'total': 99.99,\n                'items': [\n                    {'product_id': 'prod-1', 'quantity': 2}\n                ]\n            }),\n            'EventBusName': 'my-app-events'\n        }\n    ]\n)\n```\n\n### Scheduled Events\n\n```bash\n# Run every 5 minutes\naws events put-rule \\\n  --name every-5-minutes \\\n  --schedule-expression \"rate(5 minutes)\"\n\n# Run at specific times (cron)\naws events put-rule \\\n  --name daily-cleanup \\\n  --schedule-expression \"cron(0 2 * * ? *)\"\n\n# Add target\naws events put-targets \\\n  --rule every-5-minutes \\\n  --targets '[{\n    \"Id\": \"cleanup-function\",\n    \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:Cleanup\"\n  }]'\n```\n\n### EventBridge Scheduler (One-Time and Flexible)\n\n```bash\n# One-time schedule\naws scheduler create-schedule \\\n  --name send-reminder \\\n  --schedule-expression \"at(2024-12-25T09:00:00)\" \\\n  --target '{\n    \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:SendReminder\",\n    \"RoleArn\": \"arn:aws:iam::123456789012:role/scheduler-role\",\n    \"Input\": \"{\\\"message\\\": \\\"Merry Christmas!\\\"}\"\n  }' \\\n  --flexible-time-window '{\"Mode\": \"OFF\"}'\n\n# Recurring with flexible window\naws scheduler create-schedule \\\n  --name hourly-sync \\\n  --schedule-expression \"rate(1 hour)\" \\\n  --target '{\n    \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:SyncData\",\n    \"RoleArn\": \"arn:aws:iam::123456789012:role/scheduler-role\"\n  }' \\\n  --flexible-time-window '{\"Mode\": \"FLEXIBLE\", \"MaximumWindowInMinutes\": 15}'\n```\n\n### AWS Service Events\n\n```bash\n# EC2 state changes\naws events put-rule \\\n  --name ec2-state-change \\\n  --event-pattern '{\n    \"source\": [\"aws.ec2\"],\n    \"detail-type\": [\"EC2 Instance State-change Notification\"],\n    \"detail\": {\n      \"state\": [\"stopped\", \"terminated\"]\n    }\n  }'\n\n# S3 object created\naws events put-rule \\\n  --name s3-upload \\\n  --event-pattern '{\n    \"source\": [\"aws.s3\"],\n    \"detail-type\": [\"Object Created\"],\n    \"detail\": {\n      \"bucket\": {\"name\": [\"my-bucket\"]},\n      \"object\": {\"key\": [{\"prefix\": \"uploads/\"}]}\n    }\n  }'\n```\n\n## CLI Reference\n\n### Event Buses\n\n| Command | Description |\n|---------|-------------|\n| `aws events create-event-bus` | Create event bus |\n| `aws events delete-event-bus` | Delete event bus |\n| `aws events list-event-buses` | List event buses |\n| `aws events describe-event-bus` | Get event bus details |\n\n### Rules\n\n| Command | Description |\n|---------|-------------|\n| `aws events put-rule` | Create or update rule |\n| `aws events delete-rule` | Delete rule |\n| `aws events list-rules` | List rules |\n| `aws events describe-rule` | Get rule details |\n| `aws events enable-rule` | Enable rule |\n| `aws events disable-rule` | Disable rule |\n\n### Targets\n\n| Command | Description |\n|---------|-------------|\n| `aws events put-targets` | Add targets to rule |\n| `aws events remove-targets` | Remove targets |\n| `aws events list-targets-by-rule` | List rule targets |\n\n### Events\n\n| Command | Description |\n|---------|-------------|\n| `aws events put-events` | Publish events |\n\n## Best Practices\n\n### Event Design\n\n- **Use meaningful source names** â€” `company.service.component`\n- **Use descriptive detail-types** â€” `Order Created`, `User Signed Up`\n- **Include correlation IDs** for tracing\n- **Keep events small** (< 256 KB)\n- **Use versioning** for event schemas\n\n```python\n# Good event structure\n{\n    'Source': 'mycompany.orders.api',\n    'DetailType': 'Order Created',\n    'Detail': json.dumps({\n        'version': '1.0',\n        'correlation_id': 'req-abc-123',\n        'timestamp': '2024-01-15T10:30:00Z',\n        'order_id': '12345',\n        'data': {...}\n    })\n}\n```\n\n### Reliability\n\n- **Use DLQs** for failed deliveries\n- **Implement idempotency** in consumers\n- **Monitor failed invocations**\n- **Use archive and replay** for recovery\n\n### Security\n\n- **Use resource policies** to control access\n- **Enable encryption** with KMS\n- **Use IAM roles** for targets\n\n### Cost Optimization\n\n- **Use specific event patterns** to reduce matches\n- **Batch events** when publishing (up to 10 per call)\n- **Archive selectively** â€” not all events\n\n## Troubleshooting\n\n### Rule Not Triggering\n\n**Debug:**\n\n```bash\n# Check rule status\naws events describe-rule --name my-rule\n\n# Check targets\naws events list-targets-by-rule --rule my-rule\n\n# Test event pattern\naws events test-event-pattern \\\n  --event-pattern '{\"source\": [\"my-app\"]}' \\\n  --event '{\"source\": \"my-app\", \"detail-type\": \"Test\"}'\n```\n\n**Common causes:**\n- Rule disabled\n- Event pattern doesn't match\n- Target permissions missing\n\n### Lambda Not Invoked\n\n**Check Lambda permissions:**\n\n```bash\naws lambda get-policy --function-name MyFunction\n```\n\n**Required permission:**\n\n```json\n{\n  \"Principal\": \"events.amazonaws.com\",\n  \"Action\": \"lambda:InvokeFunction\",\n  \"Resource\": \"function-arn\",\n  \"Condition\": {\n    \"ArnLike\": {\n      \"AWS:SourceArn\": \"rule-arn\"\n    }\n  }\n}\n```\n\n### Events Not Reaching Custom Bus\n\n**Check:**\n- Publishing to correct bus name\n- Event format is valid JSON\n- Put events has proper permissions\n\n```bash\n# Test publish\naws events put-events \\\n  --entries '[{\n    \"Source\": \"test\",\n    \"DetailType\": \"Test Event\",\n    \"Detail\": \"{}\",\n    \"EventBusName\": \"my-app-events\"\n  }]'\n```\n\n### Viewing Failed Events\n\n```bash\n# Enable CloudWatch metrics\naws events put-rule \\\n  --name my-rule \\\n  --event-pattern '...' \\\n  --state ENABLED\n\n# Check FailedInvocations metric\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/Events \\\n  --metric-name FailedInvocations \\\n  --dimensions Name=RuleName,Value=my-rule \\\n  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum\n```\n\n## References\n\n- [EventBridge User Guide](https://docs.aws.amazon.com/eventbridge/latest/userguide/)\n- [EventBridge API Reference](https://docs.aws.amazon.com/eventbridge/latest/APIReference/)\n- [EventBridge CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/events/)\n- [boto3 EventBridge](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/events.html)\n- [Event Pattern Reference](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-patterns.html)"
              },
              {
                "name": "iam",
                "description": "AWS Identity and Access Management for users, roles, policies, and permissions. Use when creating IAM policies, configuring cross-account access, setting up service roles, troubleshooting permission errors, or managing access control.",
                "path": "skills/iam/SKILL.md",
                "frontmatter": {
                  "name": "iam",
                  "description": "AWS Identity and Access Management for users, roles, policies, and permissions. Use when creating IAM policies, configuring cross-account access, setting up service roles, troubleshooting permission errors, or managing access control.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/IAM/latest/UserGuide/"
                },
                "content": "# AWS IAM\n\nAWS Identity and Access Management (IAM) enables secure access control to AWS services and resources. IAM is foundational to AWS securityâ€”every AWS API call is authenticated and authorized through IAM.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Principals\n\nEntities that can make requests to AWS: IAM users, roles, federated users, and applications.\n\n### Policies\n\nJSON documents defining permissions. Types:\n- **Identity-based**: Attached to users, groups, or roles\n- **Resource-based**: Attached to resources (S3 buckets, SQS queues)\n- **Permission boundaries**: Maximum permissions an identity can have\n- **Service control policies (SCPs)**: Organization-wide limits\n\n### Roles\n\nIdentities with permissions that can be assumed by trusted entities. No permanent credentialsâ€”uses temporary security tokens.\n\n### Trust Relationships\n\nDefine which principals can assume a role. Configured via the role's trust policy.\n\n## Common Patterns\n\n### Create a Service Role for Lambda\n\n**AWS CLI:**\n\n```bash\n# Create the trust policy\ncat > trust-policy.json << 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": { \"Service\": \"lambda.amazonaws.com\" },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\n\n# Create the role\naws iam create-role \\\n  --role-name MyLambdaRole \\\n  --assume-role-policy-document file://trust-policy.json\n\n# Attach a managed policy\naws iam attach-role-policy \\\n  --role-name MyLambdaRole \\\n  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport json\n\niam = boto3.client('iam')\n\ntrust_policy = {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\n# Create role\niam.create_role(\n    RoleName='MyLambdaRole',\n    AssumeRolePolicyDocument=json.dumps(trust_policy)\n)\n\n# Attach managed policy\niam.attach_role_policy(\n    RoleName='MyLambdaRole',\n    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n)\n```\n\n### Create Custom Policy with Least Privilege\n\n```bash\ncat > policy.json << 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:Query\"\n      ],\n      \"Resource\": \"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\"\n    }\n  ]\n}\nEOF\n\naws iam create-policy \\\n  --policy-name MyDynamoDBPolicy \\\n  --policy-document file://policy.json\n```\n\n### Cross-Account Role Assumption\n\n```bash\n# In Account B (trusted account), create role with trust for Account A\ncat > cross-account-trust.json << 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": { \"AWS\": \"arn:aws:iam::111111111111:root\" },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": { \"sts:ExternalId\": \"unique-external-id\" }\n      }\n    }\n  ]\n}\nEOF\n\n# From Account A, assume the role\naws sts assume-role \\\n  --role-arn arn:aws:iam::222222222222:role/CrossAccountRole \\\n  --role-session-name MySession \\\n  --external-id unique-external-id\n```\n\n## CLI Reference\n\n### Essential Commands\n\n| Command | Description |\n|---------|-------------|\n| `aws iam create-role` | Create a new IAM role |\n| `aws iam create-policy` | Create a customer managed policy |\n| `aws iam attach-role-policy` | Attach a managed policy to a role |\n| `aws iam put-role-policy` | Add an inline policy to a role |\n| `aws iam get-role` | Get role details |\n| `aws iam list-roles` | List all roles |\n| `aws iam simulate-principal-policy` | Test policy permissions |\n| `aws sts assume-role` | Assume a role and get temporary credentials |\n| `aws sts get-caller-identity` | Get current identity |\n\n### Useful Flags\n\n- `--query`: Filter output with JMESPath\n- `--output table`: Human-readable output\n- `--no-cli-pager`: Disable pager for scripting\n\n## Best Practices\n\n### Security\n\n- **Never use root account** for daily tasks\n- **Enable MFA** for all human users\n- **Use roles** instead of long-term access keys\n- **Apply least privilege** â€” grant only required permissions\n- **Use conditions** to restrict access by IP, time, or MFA\n- **Rotate credentials** regularly\n- **Use permission boundaries** for delegated administration\n\n### Policy Design\n\n- Start with AWS managed policies, customize as needed\n- Use policy variables (`${aws:username}`) for dynamic policies\n- Prefer explicit denies for sensitive actions\n- Group related permissions logically\n\n### Monitoring\n\n- Enable **CloudTrail** for API auditing\n- Use **IAM Access Analyzer** to identify overly permissive policies\n- Review **credential reports** regularly\n- Set up alerts for root account usage\n\n## Troubleshooting\n\n### Access Denied Errors\n\n**Symptom:** `AccessDeniedException` or `UnauthorizedAccess`\n\n**Debug steps:**\n1. Verify identity: `aws sts get-caller-identity`\n2. Check attached policies: `aws iam list-attached-role-policies --role-name MyRole`\n3. Simulate the action:\n   ```bash\n   aws iam simulate-principal-policy \\\n     --policy-source-arn arn:aws:iam::123456789012:role/MyRole \\\n     --action-names dynamodb:GetItem \\\n     --resource-arns arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\n   ```\n4. Check for explicit denies in SCPs or permission boundaries\n5. Verify resource-based policies allow the principal\n\n### Role Cannot Be Assumed\n\n**Symptom:** `AccessDenied` when calling `AssumeRole`\n\n**Causes:**\n- Trust policy doesn't include the calling principal\n- Missing `sts:AssumeRole` permission on the caller\n- ExternalId mismatch (for cross-account roles)\n- Session duration exceeds maximum\n\n**Fix:** Review and update the role's trust relationship.\n\n### Policy Size Limits\n\n- Managed policy: 6,144 characters\n- Inline policy: 2,048 characters (user), 10,240 characters (role/group)\n- Trust policy: 2,048 characters\n\n**Solution:** Use multiple policies, reference resources by prefix/wildcard, or use tags-based access control.\n\n## References\n\n- [IAM User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/)\n- [IAM API Reference](https://docs.aws.amazon.com/IAM/latest/APIReference/)\n- [IAM CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/iam/)\n- [Policy Reference](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies.html)\n- [boto3 IAM](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html)"
              },
              {
                "name": "lambda",
                "description": "AWS Lambda serverless functions for event-driven compute. Use when creating functions, configuring triggers, debugging invocations, optimizing cold starts, setting up event source mappings, or managing layers.",
                "path": "skills/lambda/SKILL.md",
                "frontmatter": {
                  "name": "lambda",
                  "description": "AWS Lambda serverless functions for event-driven compute. Use when creating functions, configuring triggers, debugging invocations, optimizing cold starts, setting up event source mappings, or managing layers.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/lambda/latest/dg/"
                },
                "content": "# AWS Lambda\n\nAWS Lambda runs code without provisioning servers. You pay only for compute time consumed. Lambda automatically scales from a few requests per day to thousands per second.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Function\n\nYour code packaged with configuration. Includes runtime, handler, memory, timeout, and IAM role.\n\n### Invocation Types\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **Synchronous** | Caller waits for response | API Gateway, direct invoke |\n| **Asynchronous** | Fire and forget | S3, SNS, EventBridge |\n| **Poll-based** | Lambda polls source | SQS, Kinesis, DynamoDB Streams |\n\n### Execution Environment\n\nLambda creates execution environments to run your function. Components:\n- **Cold start**: New environment initialization\n- **Warm start**: Reusing existing environment\n- **Handler**: Entry point function\n- **Context**: Runtime information\n\n### Layers\n\nReusable packages of libraries, dependencies, or custom runtimes (up to 5 per function).\n\n## Common Patterns\n\n### Create a Python Function\n\n**AWS CLI:**\n\n```bash\n# Create deployment package\nzip function.zip lambda_function.py\n\n# Create function\naws lambda create-function \\\n  --function-name MyFunction \\\n  --runtime python3.12 \\\n  --role arn:aws:iam::123456789012:role/lambda-role \\\n  --handler lambda_function.handler \\\n  --zip-file fileb://function.zip \\\n  --timeout 30 \\\n  --memory-size 256\n\n# Update function code\naws lambda update-function-code \\\n  --function-name MyFunction \\\n  --zip-file fileb://function.zip\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport zipfile\nimport io\n\nlambda_client = boto3.client('lambda')\n\n# Create zip in memory\nzip_buffer = io.BytesIO()\nwith zipfile.ZipFile(zip_buffer, 'w') as zf:\n    zf.writestr('lambda_function.py', '''\ndef handler(event, context):\n    return {\"statusCode\": 200, \"body\": \"Hello\"}\n''')\nzip_buffer.seek(0)\n\n# Create function\nlambda_client.create_function(\n    FunctionName='MyFunction',\n    Runtime='python3.12',\n    Role='arn:aws:iam::123456789012:role/lambda-role',\n    Handler='lambda_function.handler',\n    Code={'ZipFile': zip_buffer.read()},\n    Timeout=30,\n    MemorySize=256\n)\n```\n\n### Add S3 Trigger\n\n```bash\n# Add permission for S3 to invoke Lambda\naws lambda add-permission \\\n  --function-name MyFunction \\\n  --statement-id s3-trigger \\\n  --action lambda:InvokeFunction \\\n  --principal s3.amazonaws.com \\\n  --source-arn arn:aws:s3:::my-bucket \\\n  --source-account 123456789012\n\n# Configure S3 notification (see S3 skill)\n```\n\n### Add SQS Event Source\n\n```bash\naws lambda create-event-source-mapping \\\n  --function-name MyFunction \\\n  --event-source-arn arn:aws:sqs:us-east-1:123456789012:my-queue \\\n  --batch-size 10 \\\n  --maximum-batching-window-in-seconds 5\n```\n\n### Environment Variables\n\n```bash\naws lambda update-function-configuration \\\n  --function-name MyFunction \\\n  --environment \"Variables={DB_HOST=mydb.cluster-xyz.us-east-1.rds.amazonaws.com,LOG_LEVEL=INFO}\"\n```\n\n### Create and Attach Layer\n\n```bash\n# Create layer\nzip -r layer.zip python/\n\naws lambda publish-layer-version \\\n  --layer-name my-dependencies \\\n  --compatible-runtimes python3.12 \\\n  --zip-file fileb://layer.zip\n\n# Attach to function\naws lambda update-function-configuration \\\n  --function-name MyFunction \\\n  --layers arn:aws:lambda:us-east-1:123456789012:layer:my-dependencies:1\n```\n\n### Invoke Function\n\n```bash\n# Synchronous invoke\naws lambda invoke \\\n  --function-name MyFunction \\\n  --payload '{\"key\": \"value\"}' \\\n  response.json\n\n# Asynchronous invoke\naws lambda invoke \\\n  --function-name MyFunction \\\n  --invocation-type Event \\\n  --payload '{\"key\": \"value\"}' \\\n  response.json\n```\n\n## CLI Reference\n\n### Function Management\n\n| Command | Description |\n|---------|-------------|\n| `aws lambda create-function` | Create new function |\n| `aws lambda update-function-code` | Update function code |\n| `aws lambda update-function-configuration` | Update settings |\n| `aws lambda delete-function` | Delete function |\n| `aws lambda list-functions` | List all functions |\n| `aws lambda get-function` | Get function details |\n\n### Invocation\n\n| Command | Description |\n|---------|-------------|\n| `aws lambda invoke` | Invoke function |\n| `aws lambda invoke-async` | Async invoke (deprecated) |\n\n### Event Sources\n\n| Command | Description |\n|---------|-------------|\n| `aws lambda create-event-source-mapping` | Add event source |\n| `aws lambda list-event-source-mappings` | List mappings |\n| `aws lambda update-event-source-mapping` | Update mapping |\n| `aws lambda delete-event-source-mapping` | Remove mapping |\n\n### Permissions\n\n| Command | Description |\n|---------|-------------|\n| `aws lambda add-permission` | Add resource-based policy |\n| `aws lambda remove-permission` | Remove permission |\n| `aws lambda get-policy` | View resource policy |\n\n## Best Practices\n\n### Performance\n\n- **Right-size memory**: More memory = more CPU = faster execution\n- **Minimize cold starts**: Keep functions warm, use Provisioned Concurrency\n- **Optimize package size**: Smaller packages deploy faster\n- **Use layers** for shared dependencies\n- **Initialize outside handler**: Reuse connections across invocations\n\n```python\n# GOOD: Initialize outside handler\nimport boto3\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyTable')\n\ndef handler(event, context):\n    # Reuses existing connection\n    return table.get_item(Key={'id': event['id']})\n```\n\n### Security\n\n- **Least privilege IAM roles** â€” only grant needed permissions\n- **Use Secrets Manager** for sensitive data\n- **Enable VPC** only if needed (adds latency)\n- **Encrypt environment variables** with KMS\n\n### Cost Optimization\n\n- **Set appropriate timeout** â€” don't use max 15 minutes unnecessarily\n- **Use ARM architecture** (Graviton2) for 34% better price/performance\n- **Batch process** where possible\n- **Use Reserved Concurrency** to limit costs\n\n### Reliability\n\n- **Configure DLQ** for async invocations\n- **Handle retries** â€” async events retry twice\n- **Make handlers idempotent**\n- **Use structured logging**\n\n## Troubleshooting\n\n### Timeout Errors\n\n**Symptom:** `Task timed out after X seconds`\n\n**Causes:**\n- Function takes longer than timeout\n- Network call to unreachable resource\n- VPC configuration issues\n\n**Debug:**\n\n```bash\n# Check function configuration\naws lambda get-function-configuration \\\n  --function-name MyFunction \\\n  --query \"Timeout\"\n\n# Increase timeout\naws lambda update-function-configuration \\\n  --function-name MyFunction \\\n  --timeout 60\n```\n\n### Out of Memory\n\n**Symptom:** Function crashes with memory error\n\n**Fix:**\n\n```bash\naws lambda update-function-configuration \\\n  --function-name MyFunction \\\n  --memory-size 512\n```\n\n### Cold Start Latency\n\n**Causes:**\n- Large deployment package\n- VPC configuration\n- Many dependencies to load\n\n**Solutions:**\n- Use Provisioned Concurrency\n- Reduce package size\n- Use layers for dependencies\n- Consider Graviton2 (ARM)\n\n```bash\n# Enable Provisioned Concurrency\naws lambda put-provisioned-concurrency-config \\\n  --function-name MyFunction \\\n  --qualifier LIVE \\\n  --provisioned-concurrent-executions 5\n```\n\n### Permission Denied\n\n**Symptom:** `AccessDeniedException`\n\n**Debug:**\n\n```bash\n# Check execution role\naws lambda get-function-configuration \\\n  --function-name MyFunction \\\n  --query \"Role\"\n\n# Check role policies\naws iam list-attached-role-policies \\\n  --role-name lambda-role\n```\n\n### VPC Connectivity Issues\n\n**Symptom:** Cannot reach internet or AWS services\n\n**Causes:**\n- No NAT Gateway for internet access\n- Missing VPC endpoint for AWS services\n- Security group blocking outbound\n\n**Solutions:**\n- Add NAT Gateway for internet\n- Add VPC endpoints for AWS services\n- Check security group rules\n\n## References\n\n- [Lambda Developer Guide](https://docs.aws.amazon.com/lambda/latest/dg/)\n- [Lambda API Reference](https://docs.aws.amazon.com/lambda/latest/api/)\n- [Lambda CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/lambda/)\n- [boto3 Lambda](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html)"
              },
              {
                "name": "rds",
                "description": "AWS RDS relational database service for managed databases. Use when provisioning databases, configuring backups, managing replicas, troubleshooting connectivity, or optimizing performance.",
                "path": "skills/rds/SKILL.md",
                "frontmatter": {
                  "name": "rds",
                  "description": "AWS RDS relational database service for managed databases. Use when provisioning databases, configuring backups, managing replicas, troubleshooting connectivity, or optimizing performance.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/"
                },
                "content": "# AWS RDS\n\nAmazon Relational Database Service (RDS) provides managed relational databases including MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, and Aurora. RDS handles provisioning, patching, backups, and failover.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### DB Instance Classes\n\n| Category | Example | Use Case |\n|----------|---------|----------|\n| Standard | db.m6g.large | General purpose |\n| Memory Optimized | db.r6g.large | High memory workloads |\n| Burstable | db.t3.medium | Variable workloads, dev/test |\n\n### Storage Types\n\n| Type | IOPS | Use Case |\n|------|------|----------|\n| gp3 | 3,000-16,000 | Most workloads |\n| io1/io2 | Up to 256,000 | High-performance OLTP |\n| magnetic | N/A | Legacy, avoid |\n\n### Multi-AZ Deployments\n\n- **Multi-AZ Instance**: Synchronous standby in different AZ\n- **Multi-AZ Cluster**: One writer, two reader instances (Aurora-like)\n\n### Read Replicas\n\nAsynchronous copies for read scaling. Can be cross-region.\n\n## Common Patterns\n\n### Create a PostgreSQL Instance\n\n**AWS CLI:**\n\n```bash\n# Create DB subnet group\naws rds create-db-subnet-group \\\n  --db-subnet-group-name my-db-subnet-group \\\n  --db-subnet-group-description \"Private subnets for RDS\" \\\n  --subnet-ids subnet-12345678 subnet-87654321\n\n# Create security group (allow PostgreSQL from app)\naws ec2 create-security-group \\\n  --group-name rds-postgres-sg \\\n  --description \"RDS PostgreSQL access\" \\\n  --vpc-id vpc-12345678\n\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-rds12345 \\\n  --protocol tcp \\\n  --port 5432 \\\n  --source-group sg-app12345\n\n# Create RDS instance\naws rds create-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --db-instance-class db.t3.medium \\\n  --engine postgres \\\n  --engine-version 16.1 \\\n  --master-username admin \\\n  --master-user-password 'SecurePassword123!' \\\n  --allocated-storage 100 \\\n  --storage-type gp3 \\\n  --db-subnet-group-name my-db-subnet-group \\\n  --vpc-security-group-ids sg-rds12345 \\\n  --multi-az \\\n  --backup-retention-period 7 \\\n  --storage-encrypted \\\n  --no-publicly-accessible\n```\n\n**boto3:**\n\n```python\nimport boto3\n\nrds = boto3.client('rds')\n\nresponse = rds.create_db_instance(\n    DBInstanceIdentifier='my-postgres',\n    DBInstanceClass='db.t3.medium',\n    Engine='postgres',\n    EngineVersion='16.1',\n    MasterUsername='admin',\n    MasterUserPassword='SecurePassword123!',\n    AllocatedStorage=100,\n    StorageType='gp3',\n    DBSubnetGroupName='my-db-subnet-group',\n    VpcSecurityGroupIds=['sg-rds12345'],\n    MultiAZ=True,\n    BackupRetentionPeriod=7,\n    StorageEncrypted=True,\n    PubliclyAccessible=False\n)\n```\n\n### Create Read Replica\n\n```bash\naws rds create-db-instance-read-replica \\\n  --db-instance-identifier my-postgres-replica \\\n  --source-db-instance-identifier my-postgres \\\n  --db-instance-class db.t3.medium \\\n  --availability-zone us-east-1b\n```\n\n### Take a Snapshot\n\n```bash\naws rds create-db-snapshot \\\n  --db-snapshot-identifier my-postgres-snapshot-2024-01-15 \\\n  --db-instance-identifier my-postgres\n```\n\n### Restore from Snapshot\n\n```bash\naws rds restore-db-instance-from-db-snapshot \\\n  --db-instance-identifier my-postgres-restored \\\n  --db-snapshot-identifier my-postgres-snapshot-2024-01-15 \\\n  --db-instance-class db.t3.medium \\\n  --db-subnet-group-name my-db-subnet-group \\\n  --vpc-security-group-ids sg-rds12345\n```\n\n### Point-in-Time Recovery\n\n```bash\naws rds restore-db-instance-to-point-in-time \\\n  --source-db-instance-identifier my-postgres \\\n  --target-db-instance-identifier my-postgres-pitr \\\n  --restore-time 2024-01-15T10:30:00Z \\\n  --db-instance-class db.t3.medium\n```\n\n### Modify Instance\n\n```bash\n# Change instance class (with downtime)\naws rds modify-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --db-instance-class db.m6g.large \\\n  --apply-immediately\n\n# Scale storage (no downtime)\naws rds modify-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --allocated-storage 200 \\\n  --apply-immediately\n```\n\n### Connect with IAM Authentication\n\n```python\nimport boto3\nimport psycopg2\n\nrds = boto3.client('rds')\n\n# Generate auth token\ntoken = rds.generate_db_auth_token(\n    DBHostname='my-postgres.abc123.us-east-1.rds.amazonaws.com',\n    Port=5432,\n    DBUsername='iam_user',\n    Region='us-east-1'\n)\n\n# Connect\nconn = psycopg2.connect(\n    host='my-postgres.abc123.us-east-1.rds.amazonaws.com',\n    port=5432,\n    database='mydb',\n    user='iam_user',\n    password=token,\n    sslmode='require'\n)\n```\n\n## CLI Reference\n\n### Instance Management\n\n| Command | Description |\n|---------|-------------|\n| `aws rds create-db-instance` | Create instance |\n| `aws rds describe-db-instances` | List instances |\n| `aws rds modify-db-instance` | Modify settings |\n| `aws rds delete-db-instance` | Delete instance |\n| `aws rds reboot-db-instance` | Reboot instance |\n| `aws rds start-db-instance` | Start stopped instance |\n| `aws rds stop-db-instance` | Stop instance |\n\n### Backups\n\n| Command | Description |\n|---------|-------------|\n| `aws rds create-db-snapshot` | Manual snapshot |\n| `aws rds describe-db-snapshots` | List snapshots |\n| `aws rds restore-db-instance-from-db-snapshot` | Restore from snapshot |\n| `aws rds restore-db-instance-to-point-in-time` | Point-in-time restore |\n| `aws rds copy-db-snapshot` | Copy snapshot |\n\n### Replicas\n\n| Command | Description |\n|---------|-------------|\n| `aws rds create-db-instance-read-replica` | Create read replica |\n| `aws rds promote-read-replica` | Promote to standalone |\n\n## Best Practices\n\n### Security\n\n- **Never make publicly accessible** â€” use VPC and security groups\n- **Enable encryption** at rest (KMS) and in transit (SSL)\n- **Use IAM authentication** for application access\n- **Store credentials in Secrets Manager** with rotation\n- **Use parameter groups** to enforce SSL\n\n```bash\n# Enforce SSL in PostgreSQL\naws rds modify-db-parameter-group \\\n  --db-parameter-group-name my-pg-params \\\n  --parameters \"ParameterName=rds.force_ssl,ParameterValue=1,ApplyMethod=pending-reboot\"\n```\n\n### Performance\n\n- **Right-size instances** â€” monitor CPU, memory, IOPS\n- **Use gp3** for cost-effective performance\n- **Enable Performance Insights** for query analysis\n- **Use read replicas** for read scaling\n- **Optimize queries** â€” check slow query log\n\n### High Availability\n\n- **Enable Multi-AZ** for production\n- **Use Aurora** for mission-critical workloads\n- **Configure appropriate backup retention**\n- **Test failover** periodically\n- **Monitor replication lag** for replicas\n\n### Cost Optimization\n\n- **Use Reserved Instances** for steady-state workloads\n- **Stop dev/test instances** when not in use\n- **Delete old snapshots** regularly\n- **Right-size instance classes**\n\n## Troubleshooting\n\n### Cannot Connect\n\n**Causes:**\n1. Security group not allowing access\n2. Instance not in VPC subnet\n3. SSL required but not used\n4. Wrong endpoint/port\n\n**Debug:**\n\n```bash\n# Check security group\naws ec2 describe-security-groups --group-ids sg-rds12345\n\n# Check instance status\naws rds describe-db-instances \\\n  --db-instance-identifier my-postgres \\\n  --query \"DBInstances[0].{Status:DBInstanceStatus,Endpoint:Endpoint}\"\n\n# Test connectivity from EC2\nnc -zv my-postgres.abc123.us-east-1.rds.amazonaws.com 5432\n```\n\n### High CPU/Memory\n\n**Debug:**\n\n```bash\n# Enable Enhanced Monitoring\naws rds modify-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --monitoring-interval 60 \\\n  --monitoring-role-arn arn:aws:iam::123456789012:role/rds-monitoring-role\n\n# Enable Performance Insights\naws rds modify-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --enable-performance-insights \\\n  --performance-insights-retention-period 7\n```\n\n**Solutions:**\n- Scale up instance class\n- Optimize slow queries\n- Add read replicas\n- Check for locking/blocking\n\n### Storage Full\n\n**Symptom:** Instance becomes unavailable\n\n**Prevention:**\n\n```bash\n# Enable storage autoscaling\naws rds modify-db-instance \\\n  --db-instance-identifier my-postgres \\\n  --max-allocated-storage 500\n\n# Set CloudWatch alarm\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"RDS-Storage-Low\" \\\n  --metric-name FreeStorageSpace \\\n  --namespace AWS/RDS \\\n  --dimensions Name=DBInstanceIdentifier,Value=my-postgres \\\n  --statistic Average \\\n  --period 300 \\\n  --threshold 10000000000 \\\n  --comparison-operator LessThanThreshold \\\n  --evaluation-periods 2 \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:alerts\n```\n\n### Replication Lag\n\n**Monitor:**\n\n```bash\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/RDS \\\n  --metric-name ReplicaLag \\\n  --dimensions Name=DBInstanceIdentifier,Value=my-postgres-replica \\\n  --start-time $(date -d '1 hour ago' -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Average\n```\n\n**Causes:**\n- Replica instance too small\n- Heavy write load\n- Network issues\n- Long-running queries on replica\n\n## References\n\n- [RDS User Guide](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/)\n- [RDS API Reference](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/)\n- [RDS CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/rds/)\n- [boto3 RDS](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html)"
              },
              {
                "name": "s3",
                "description": "AWS S3 object storage for bucket management, object operations, and access control. Use when creating buckets, uploading files, configuring lifecycle policies, setting up static websites, managing permissions, or implementing cross-region replication.",
                "path": "skills/s3/SKILL.md",
                "frontmatter": {
                  "name": "s3",
                  "description": "AWS S3 object storage for bucket management, object operations, and access control. Use when creating buckets, uploading files, configuring lifecycle policies, setting up static websites, managing permissions, or implementing cross-region replication.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/"
                },
                "content": "# AWS S3\n\nAmazon Simple Storage Service (S3) provides scalable object storage with industry-leading durability (99.999999999%). S3 is fundamental to AWSâ€”used for data lakes, backups, static websites, and as storage for many other AWS services.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Buckets\n\nContainers for objects. Bucket names are globally unique across all AWS accounts.\n\n### Objects\n\nFiles stored in S3, consisting of data, metadata, and a unique key (path). Maximum size: 5 TB.\n\n### Storage Classes\n\n| Class | Use Case | Durability | Availability |\n|-------|----------|------------|--------------|\n| Standard | Frequently accessed | 99.999999999% | 99.99% |\n| Intelligent-Tiering | Unknown access patterns | 99.999999999% | 99.9% |\n| Standard-IA | Infrequent access | 99.999999999% | 99.9% |\n| Glacier Instant | Archive with instant retrieval | 99.999999999% | 99.9% |\n| Glacier Flexible | Archive (minutes to hours) | 99.999999999% | 99.99% |\n| Glacier Deep Archive | Long-term archive | 99.999999999% | 99.99% |\n\n### Versioning\n\nKeeps multiple versions of an object. Essential for data protection and recovery.\n\n## Common Patterns\n\n### Create a Bucket with Best Practices\n\n**AWS CLI:**\n\n```bash\n# Create bucket (us-east-1 doesn't need LocationConstraint)\naws s3api create-bucket \\\n  --bucket my-secure-bucket-12345 \\\n  --region us-west-2 \\\n  --create-bucket-configuration LocationConstraint=us-west-2\n\n# Enable versioning\naws s3api put-bucket-versioning \\\n  --bucket my-secure-bucket-12345 \\\n  --versioning-configuration Status=Enabled\n\n# Block public access\naws s3api put-public-access-block \\\n  --bucket my-secure-bucket-12345 \\\n  --public-access-block-configuration \\\n    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\n\n# Enable encryption\naws s3api put-bucket-encryption \\\n  --bucket my-secure-bucket-12345 \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]\n  }'\n```\n\n**boto3:**\n\n```python\nimport boto3\n\ns3 = boto3.client('s3', region_name='us-west-2')\n\n# Create bucket\ns3.create_bucket(\n    Bucket='my-secure-bucket-12345',\n    CreateBucketConfiguration={'LocationConstraint': 'us-west-2'}\n)\n\n# Enable versioning\ns3.put_bucket_versioning(\n    Bucket='my-secure-bucket-12345',\n    VersioningConfiguration={'Status': 'Enabled'}\n)\n\n# Block public access\ns3.put_public_access_block(\n    Bucket='my-secure-bucket-12345',\n    PublicAccessBlockConfiguration={\n        'BlockPublicAcls': True,\n        'IgnorePublicAcls': True,\n        'BlockPublicPolicy': True,\n        'RestrictPublicBuckets': True\n    }\n)\n```\n\n### Upload and Download Objects\n\n```bash\n# Upload a single file\naws s3 cp myfile.txt s3://my-bucket/path/myfile.txt\n\n# Upload with metadata\naws s3 cp myfile.txt s3://my-bucket/path/myfile.txt \\\n  --metadata \"environment=production,version=1.0\"\n\n# Download a file\naws s3 cp s3://my-bucket/path/myfile.txt ./myfile.txt\n\n# Sync a directory\naws s3 sync ./local-folder s3://my-bucket/prefix/ --delete\n\n# Copy between buckets\naws s3 cp s3://source-bucket/file.txt s3://dest-bucket/file.txt\n```\n\n### Generate Presigned URL\n\n```python\nimport boto3\nfrom botocore.config import Config\n\ns3 = boto3.client('s3', config=Config(signature_version='s3v4'))\n\n# Generate presigned URL for download (GET)\nurl = s3.generate_presigned_url(\n    'get_object',\n    Params={'Bucket': 'my-bucket', 'Key': 'path/to/file.txt'},\n    ExpiresIn=3600  # URL valid for 1 hour\n)\n\n# Generate presigned URL for upload (PUT)\nupload_url = s3.generate_presigned_url(\n    'put_object',\n    Params={\n        'Bucket': 'my-bucket',\n        'Key': 'uploads/newfile.txt',\n        'ContentType': 'text/plain'\n    },\n    ExpiresIn=3600\n)\n```\n\n### Configure Lifecycle Policy\n\n```bash\ncat > lifecycle.json << 'EOF'\n{\n  \"Rules\": [\n    {\n      \"ID\": \"MoveToGlacierAfter90Days\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\"Prefix\": \"logs/\"},\n      \"Transitions\": [\n        {\"Days\": 90, \"StorageClass\": \"GLACIER\"}\n      ],\n      \"Expiration\": {\"Days\": 365}\n    },\n    {\n      \"ID\": \"DeleteOldVersions\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {},\n      \"NoncurrentVersionExpiration\": {\"NoncurrentDays\": 30}\n    }\n  ]\n}\nEOF\n\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket my-bucket \\\n  --lifecycle-configuration file://lifecycle.json\n```\n\n### Event Notifications to Lambda\n\n```bash\naws s3api put-bucket-notification-configuration \\\n  --bucket my-bucket \\\n  --notification-configuration '{\n    \"LambdaFunctionConfigurations\": [\n      {\n        \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessS3Upload\",\n        \"Events\": [\"s3:ObjectCreated:*\"],\n        \"Filter\": {\n          \"Key\": {\n            \"FilterRules\": [\n              {\"Name\": \"prefix\", \"Value\": \"uploads/\"},\n              {\"Name\": \"suffix\", \"Value\": \".jpg\"}\n            ]\n          }\n        }\n      }\n    ]\n  }'\n```\n\n## CLI Reference\n\n### High-Level Commands (aws s3)\n\n| Command | Description |\n|---------|-------------|\n| `aws s3 ls` | List buckets or objects |\n| `aws s3 cp` | Copy files |\n| `aws s3 mv` | Move files |\n| `aws s3 rm` | Delete files |\n| `aws s3 sync` | Sync directories |\n| `aws s3 mb` | Make bucket |\n| `aws s3 rb` | Remove bucket |\n\n### Low-Level Commands (aws s3api)\n\n| Command | Description |\n|---------|-------------|\n| `aws s3api create-bucket` | Create bucket with options |\n| `aws s3api put-object` | Upload with full control |\n| `aws s3api get-object` | Download with options |\n| `aws s3api delete-object` | Delete single object |\n| `aws s3api put-bucket-policy` | Set bucket policy |\n| `aws s3api put-bucket-versioning` | Enable versioning |\n| `aws s3api list-object-versions` | List all versions |\n\n### Useful Flags\n\n- `--recursive`: Process all objects in prefix\n- `--exclude/--include`: Filter objects\n- `--dryrun`: Preview changes\n- `--storage-class`: Set storage class\n- `--acl`: Set access control (prefer policies instead)\n\n## Best Practices\n\n### Security\n\n- **Block public access** at account and bucket level\n- **Enable versioning** for data protection\n- **Use bucket policies** over ACLs\n- **Enable encryption** (SSE-S3 or SSE-KMS)\n- **Enable access logging** for audit\n- **Use VPC endpoints** for private access\n- **Enable MFA Delete** for critical buckets\n\n### Performance\n\n- **Use Transfer Acceleration** for distant uploads\n- **Use multipart upload** for files > 100 MB\n- **Randomize key prefixes** for high-throughput (less relevant with 2024 improvements)\n- **Use byte-range fetches** for large file downloads\n\n### Cost Optimization\n\n- **Use lifecycle policies** to transition to cheaper storage\n- **Enable Intelligent-Tiering** for unpredictable access\n- **Delete incomplete multipart uploads**:\n  ```json\n  {\n    \"Rules\": [{\n      \"ID\": \"AbortIncompleteMultipartUpload\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {},\n      \"AbortIncompleteMultipartUpload\": {\"DaysAfterInitiation\": 7}\n    }]\n  }\n  ```\n- **Use S3 Storage Lens** to analyze storage patterns\n\n## Troubleshooting\n\n### Access Denied Errors\n\n**Causes:**\n1. Bucket policy denies access\n2. IAM policy missing permissions\n3. Public access block preventing access\n4. Object owned by different account\n5. VPC endpoint policy blocking\n\n**Debug steps:**\n\n```bash\n# Check your identity\naws sts get-caller-identity\n\n# Check bucket policy\naws s3api get-bucket-policy --bucket my-bucket\n\n# Check public access block\naws s3api get-public-access-block --bucket my-bucket\n\n# Check object ownership\naws s3api get-object-attributes \\\n  --bucket my-bucket \\\n  --key myfile.txt \\\n  --object-attributes ObjectOwner\n```\n\n### CORS Errors\n\n**Symptom:** Browser blocks cross-origin request\n\n**Fix:**\n\n```bash\naws s3api put-bucket-cors --bucket my-bucket --cors-configuration '{\n  \"CORSRules\": [{\n    \"AllowedOrigins\": [\"https://myapp.com\"],\n    \"AllowedMethods\": [\"GET\", \"PUT\", \"POST\"],\n    \"AllowedHeaders\": [\"*\"],\n    \"ExposeHeaders\": [\"ETag\"],\n    \"MaxAgeSeconds\": 3600\n  }]\n}'\n```\n\n### Slow Uploads\n\n**Solutions:**\n- Use multipart upload for large files\n- Enable Transfer Acceleration\n- Use `aws s3 cp` with `--expected-size` for large files\n- Check network throughput to the region\n\n### 403 on Presigned URL\n\n**Causes:**\n- URL expired\n- Signer lacks permissions\n- Bucket policy blocks access\n- Region mismatch (v4 signatures are region-specific)\n\n**Fix:** Ensure signer has permissions and use correct region.\n\n## References\n\n- [S3 User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)\n- [S3 API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/)\n- [S3 CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/s3/)\n- [boto3 S3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)"
              },
              {
                "name": "secrets-manager",
                "description": "AWS Secrets Manager for secure secret storage and rotation. Use when storing credentials, configuring automatic rotation, managing secret versions, retrieving secrets in applications, or integrating with RDS.",
                "path": "skills/secrets-manager/SKILL.md",
                "frontmatter": {
                  "name": "secrets-manager",
                  "description": "AWS Secrets Manager for secure secret storage and rotation. Use when storing credentials, configuring automatic rotation, managing secret versions, retrieving secrets in applications, or integrating with RDS.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/"
                },
                "content": "# AWS Secrets Manager\n\nAWS Secrets Manager helps protect access to applications, services, and IT resources. Store, retrieve, and automatically rotate credentials, API keys, and other secrets.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Secrets\n\nEncrypted data stored in Secrets Manager. Can contain:\n- Database credentials\n- API keys\n- OAuth tokens\n- Any key-value pairs (up to 64 KB)\n\n### Versions\n\nEach secret can have multiple versions:\n- **AWSCURRENT**: Current active version\n- **AWSPENDING**: Version being rotated to\n- **AWSPREVIOUS**: Previous version\n\n### Rotation\n\nAutomatic credential rotation using Lambda functions. Built-in support for:\n- Amazon RDS\n- Amazon Redshift\n- Amazon DocumentDB\n- Custom secrets\n\n## Common Patterns\n\n### Create a Secret\n\n**AWS CLI:**\n\n```bash\n# Create secret with JSON\naws secretsmanager create-secret \\\n  --name prod/myapp/database \\\n  --description \"Production database credentials\" \\\n  --secret-string '{\"username\":\"admin\",\"password\":\"MySecurePassword123!\",\"host\":\"mydb.cluster-xyz.us-east-1.rds.amazonaws.com\",\"port\":5432,\"database\":\"myapp\"}'\n\n# Create secret with binary data\naws secretsmanager create-secret \\\n  --name prod/myapp/certificate \\\n  --secret-binary fileb://certificate.pem\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport json\n\nsecrets = boto3.client('secretsmanager')\n\nresponse = secrets.create_secret(\n    Name='prod/myapp/database',\n    Description='Production database credentials',\n    SecretString=json.dumps({\n        'username': 'admin',\n        'password': 'MySecurePassword123!',\n        'host': 'mydb.cluster-xyz.us-east-1.rds.amazonaws.com',\n        'port': 5432,\n        'database': 'myapp'\n    }),\n    Tags=[\n        {'Key': 'Environment', 'Value': 'production'},\n        {'Key': 'Application', 'Value': 'myapp'}\n    ]\n)\n```\n\n### Retrieve a Secret\n\n```python\nimport boto3\nimport json\n\nsecrets = boto3.client('secretsmanager')\n\ndef get_secret(secret_name):\n    response = secrets.get_secret_value(SecretId=secret_name)\n\n    if 'SecretString' in response:\n        return json.loads(response['SecretString'])\n    else:\n        import base64\n        return base64.b64decode(response['SecretBinary'])\n\n# Usage\ncredentials = get_secret('prod/myapp/database')\ndb_password = credentials['password']\n```\n\n### Caching Secrets\n\n```python\nfrom aws_secretsmanager_caching import SecretCache, SecretCacheConfig\n\n# Configure cache\ncache_config = SecretCacheConfig(\n    max_cache_size=100,\n    secret_refresh_interval=3600,\n    secret_version_stage_refresh_interval=3600\n)\n\ncache = SecretCache(config=cache_config)\n\ndef get_cached_secret(secret_name):\n    secret = cache.get_secret_string(secret_name)\n    return json.loads(secret)\n```\n\n### Update a Secret\n\n```bash\n# Update secret value\naws secretsmanager update-secret \\\n  --secret-id prod/myapp/database \\\n  --secret-string '{\"username\":\"admin\",\"password\":\"NewPassword456!\"}'\n\n# Put new version with staging labels\naws secretsmanager put-secret-value \\\n  --secret-id prod/myapp/database \\\n  --secret-string '{\"username\":\"admin\",\"password\":\"NewPassword456!\"}' \\\n  --version-stages AWSCURRENT\n```\n\n### Enable Rotation for RDS\n\n```bash\naws secretsmanager rotate-secret \\\n  --secret-id prod/myapp/database \\\n  --rotation-lambda-arn arn:aws:lambda:us-east-1:123456789012:function:SecretsManagerRDSPostgreSQLRotation \\\n  --rotation-rules AutomaticallyAfterDays=30\n```\n\n### Create Secret with Rotation\n\n```bash\n# Use CloudFormation for RDS secret with rotation\naws cloudformation deploy \\\n  --template-file rds-secret.yaml \\\n  --stack-name rds-secret\n```\n\n```yaml\n# rds-secret.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  DBSecret:\n    Type: AWS::SecretsManager::Secret\n    Properties:\n      Name: prod/myapp/database\n      GenerateSecretString:\n        SecretStringTemplate: '{\"username\": \"admin\"}'\n        GenerateStringKey: password\n        PasswordLength: 32\n        ExcludeCharacters: '\"@/\\'\n\n  DBSecretRotation:\n    Type: AWS::SecretsManager::RotationSchedule\n    Properties:\n      SecretId: !Ref DBSecret\n      RotationLambdaARN: !GetAtt RotationLambda.Arn\n      RotationRules:\n        AutomaticallyAfterDays: 30\n```\n\n### Use in Lambda with Extension\n\n```python\nimport json\nimport urllib.request\n\ndef handler(event, context):\n    # Use AWS Parameters and Secrets Lambda Extension\n    secrets_port = 2773\n    secret_name = 'prod/myapp/database'\n\n    url = f'http://localhost:{secrets_port}/secretsmanager/get?secretId={secret_name}'\n    headers = {'X-Aws-Parameters-Secrets-Token': os.environ['AWS_SESSION_TOKEN']}\n\n    request = urllib.request.Request(url, headers=headers)\n    response = urllib.request.urlopen(request)\n    secret = json.loads(response.read())['SecretString']\n\n    credentials = json.loads(secret)\n    return credentials\n```\n\n## CLI Reference\n\n### Secret Management\n\n| Command | Description |\n|---------|-------------|\n| `aws secretsmanager create-secret` | Create secret |\n| `aws secretsmanager describe-secret` | Get secret metadata |\n| `aws secretsmanager get-secret-value` | Retrieve secret value |\n| `aws secretsmanager update-secret` | Update secret |\n| `aws secretsmanager delete-secret` | Delete secret |\n| `aws secretsmanager restore-secret` | Restore deleted secret |\n| `aws secretsmanager list-secrets` | List secrets |\n\n### Versions\n\n| Command | Description |\n|---------|-------------|\n| `aws secretsmanager put-secret-value` | Add new version |\n| `aws secretsmanager list-secret-version-ids` | List versions |\n| `aws secretsmanager update-secret-version-stage` | Move staging labels |\n\n### Rotation\n\n| Command | Description |\n|---------|-------------|\n| `aws secretsmanager rotate-secret` | Configure/trigger rotation |\n| `aws secretsmanager cancel-rotate-secret` | Cancel rotation |\n\n## Best Practices\n\n### Secret Organization\n\n- **Use hierarchical names**: `environment/application/secret-type`\n- **Tag secrets** for organization and cost allocation\n- **Separate by environment** (dev, staging, prod)\n\n### Security\n\n- **Use resource policies** to control access\n- **Enable encryption** with customer-managed KMS keys\n- **Rotate secrets** regularly (30-90 days)\n- **Audit access** with CloudTrail\n- **Use VPC endpoints** for private access\n\n### Access Control\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:DescribeSecret\"\n      ],\n      \"Resource\": \"arn:aws:secretsmanager:us-east-1:123456789012:secret:prod/*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"secretsmanager:ResourceTag/Environment\": \"production\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Application Integration\n\n- **Cache secrets** to reduce API calls\n- **Handle rotation** gracefully (retry with new credentials)\n- **Use Lambda extension** for faster access\n- **Never log secrets**\n\n## Troubleshooting\n\n### AccessDeniedException\n\n**Causes:**\n- IAM policy missing `secretsmanager:GetSecretValue`\n- Resource policy denying access\n- KMS key policy missing permissions\n\n**Debug:**\n\n```bash\n# Check secret resource policy\naws secretsmanager get-resource-policy --secret-id my-secret\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:role/my-role \\\n  --action-names secretsmanager:GetSecretValue \\\n  --resource-arns arn:aws:secretsmanager:us-east-1:123456789012:secret:my-secret\n```\n\n### Rotation Failed\n\n**Debug:**\n\n```bash\n# Check rotation status\naws secretsmanager describe-secret --secret-id my-secret\n\n# Check Lambda logs\naws logs filter-log-events \\\n  --log-group-name /aws/lambda/SecretsManagerRotation \\\n  --filter-pattern \"ERROR\"\n```\n\n**Common causes:**\n- Lambda timeout (increase to 30+ seconds)\n- Network connectivity (VPC configuration)\n- Database connection issues\n- Wrong secret format\n\n### Secret Not Found\n\n```bash\n# List secrets to find correct name\naws secretsmanager list-secrets \\\n  --filters Key=name,Values=myapp\n\n# Check if deleted (within recovery window)\naws secretsmanager list-secrets \\\n  --include-planned-deletion\n```\n\n## References\n\n- [Secrets Manager User Guide](https://docs.aws.amazon.com/secretsmanager/latest/userguide/)\n- [Secrets Manager API Reference](https://docs.aws.amazon.com/secretsmanager/latest/apireference/)\n- [Secrets Manager CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/secretsmanager/)\n- [boto3 Secrets Manager](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/secretsmanager.html)"
              },
              {
                "name": "sns",
                "description": "AWS SNS notification service for pub/sub messaging. Use when creating topics, managing subscriptions, configuring message filtering, sending notifications, or setting up mobile push.",
                "path": "skills/sns/SKILL.md",
                "frontmatter": {
                  "name": "sns",
                  "description": "AWS SNS notification service for pub/sub messaging. Use when creating topics, managing subscriptions, configuring message filtering, sending notifications, or setting up mobile push.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/sns/latest/dg/"
                },
                "content": "# AWS SNS\n\nAmazon Simple Notification Service (SNS) is a fully managed pub/sub messaging service for application-to-application (A2A) and application-to-person (A2P) communication.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Topics\n\nNamed channels for publishing messages. Publishers send to topics, subscribers receive from topics.\n\n### Topic Types\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **Standard** | Best-effort ordering, at-least-once | Most use cases |\n| **FIFO** | Strict ordering, exactly-once | Order-sensitive |\n\n### Subscription Protocols\n\n| Protocol | Description |\n|----------|-------------|\n| **Lambda** | Invoke Lambda function |\n| **SQS** | Send to SQS queue |\n| **HTTP/HTTPS** | POST to endpoint |\n| **Email** | Send email |\n| **SMS** | Send text message |\n| **Application** | Mobile push notification |\n\n### Message Filtering\n\nRoute messages to specific subscribers based on message attributes.\n\n## Common Patterns\n\n### Create Topic and Subscribe\n\n**AWS CLI:**\n\n```bash\n# Create standard topic\naws sns create-topic --name my-topic\n\n# Create FIFO topic\naws sns create-topic \\\n  --name my-topic.fifo \\\n  --attributes FifoTopic=true\n\n# Subscribe Lambda\naws sns subscribe \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \\\n  --protocol lambda \\\n  --notification-endpoint arn:aws:lambda:us-east-1:123456789012:function:my-function\n\n# Subscribe SQS\naws sns subscribe \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \\\n  --protocol sqs \\\n  --notification-endpoint arn:aws:sqs:us-east-1:123456789012:my-queue\n\n# Subscribe email\naws sns subscribe \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \\\n  --protocol email \\\n  --notification-endpoint user@example.com\n```\n\n**boto3:**\n\n```python\nimport boto3\n\nsns = boto3.client('sns')\n\n# Create topic\nresponse = sns.create_topic(Name='my-topic')\ntopic_arn = response['TopicArn']\n\n# Subscribe Lambda\nsns.subscribe(\n    TopicArn=topic_arn,\n    Protocol='lambda',\n    Endpoint='arn:aws:lambda:us-east-1:123456789012:function:my-function'\n)\n\n# Subscribe SQS with filter\nsns.subscribe(\n    TopicArn=topic_arn,\n    Protocol='sqs',\n    Endpoint='arn:aws:sqs:us-east-1:123456789012:order-queue',\n    Attributes={\n        'FilterPolicy': '{\"event_type\": [\"order_created\", \"order_updated\"]}'\n    }\n)\n```\n\n### Publish Messages\n\n```python\nimport boto3\nimport json\n\nsns = boto3.client('sns')\ntopic_arn = 'arn:aws:sns:us-east-1:123456789012:my-topic'\n\n# Simple publish\nsns.publish(\n    TopicArn=topic_arn,\n    Message='Hello, World!',\n    Subject='Notification'\n)\n\n# Publish with attributes (for filtering)\nsns.publish(\n    TopicArn=topic_arn,\n    Message=json.dumps({'order_id': '12345', 'status': 'created'}),\n    MessageAttributes={\n        'event_type': {\n            'DataType': 'String',\n            'StringValue': 'order_created'\n        },\n        'priority': {\n            'DataType': 'Number',\n            'StringValue': '1'\n        }\n    }\n)\n\n# Publish to FIFO topic\nsns.publish(\n    TopicArn='arn:aws:sns:us-east-1:123456789012:my-topic.fifo',\n    Message=json.dumps({'order_id': '12345'}),\n    MessageGroupId='order-12345',\n    MessageDeduplicationId='unique-id'\n)\n```\n\n### Message Filtering\n\n```bash\n# Add filter policy to subscription\naws sns set-subscription-attributes \\\n  --subscription-arn arn:aws:sns:us-east-1:123456789012:my-topic:abc123 \\\n  --attribute-name FilterPolicy \\\n  --attribute-value '{\n    \"event_type\": [\"order_created\"],\n    \"priority\": [{\"numeric\": [\">=\", 1]}]\n  }'\n```\n\nFilter policy examples:\n\n```json\n// Exact match\n{\"event_type\": [\"order_created\", \"order_updated\"]}\n\n// Prefix match\n{\"customer_id\": [{\"prefix\": \"PREMIUM-\"}]}\n\n// Numeric comparison\n{\"price\": [{\"numeric\": [\">=\", 100, \"<=\", 500]}]}\n\n// Exists check\n{\"customer_id\": [{\"exists\": true}]}\n\n// Anything but\n{\"event_type\": [{\"anything-but\": [\"deleted\"]}]}\n\n// Combined\n{\n  \"event_type\": [\"order_created\"],\n  \"region\": [\"us-east\", \"us-west\"],\n  \"priority\": [{\"numeric\": [\">=\", 1]}]\n}\n```\n\n### Fan-Out Pattern (SNS to Multiple SQS)\n\n```python\nimport boto3\nimport json\n\nsns = boto3.client('sns')\nsqs = boto3.client('sqs')\n\n# Create topic\ntopic = sns.create_topic(Name='orders-topic')\ntopic_arn = topic['TopicArn']\n\n# Create queues for different processors\nqueues = {\n    'analytics': sqs.create_queue(QueueName='order-analytics')['QueueUrl'],\n    'fulfillment': sqs.create_queue(QueueName='order-fulfillment')['QueueUrl'],\n    'notification': sqs.create_queue(QueueName='order-notification')['QueueUrl']\n}\n\n# Subscribe each queue\nfor name, queue_url in queues.items():\n    queue_arn = sqs.get_queue_attributes(\n        QueueUrl=queue_url,\n        AttributeNames=['QueueArn']\n    )['Attributes']['QueueArn']\n\n    sns.subscribe(\n        TopicArn=topic_arn,\n        Protocol='sqs',\n        Endpoint=queue_arn\n    )\n\n# One publish reaches all queues\nsns.publish(\n    TopicArn=topic_arn,\n    Message=json.dumps({'order_id': '12345', 'total': 99.99})\n)\n```\n\n### Lambda Permission for SNS\n\n```bash\naws lambda add-permission \\\n  --function-name my-function \\\n  --statement-id sns-trigger \\\n  --action lambda:InvokeFunction \\\n  --principal sns.amazonaws.com \\\n  --source-arn arn:aws:sns:us-east-1:123456789012:my-topic\n```\n\n## CLI Reference\n\n### Topic Management\n\n| Command | Description |\n|---------|-------------|\n| `aws sns create-topic` | Create topic |\n| `aws sns delete-topic` | Delete topic |\n| `aws sns list-topics` | List topics |\n| `aws sns get-topic-attributes` | Get topic settings |\n| `aws sns set-topic-attributes` | Update topic settings |\n\n### Subscriptions\n\n| Command | Description |\n|---------|-------------|\n| `aws sns subscribe` | Create subscription |\n| `aws sns unsubscribe` | Remove subscription |\n| `aws sns list-subscriptions` | List all subscriptions |\n| `aws sns list-subscriptions-by-topic` | List topic subscriptions |\n| `aws sns confirm-subscription` | Confirm pending subscription |\n\n### Publishing\n\n| Command | Description |\n|---------|-------------|\n| `aws sns publish` | Publish message |\n\n## Best Practices\n\n### Reliability\n\n- **Use SQS for durability** â€” SNS is push-based, SQS queues messages\n- **Implement retries** for HTTP/HTTPS endpoints\n- **Configure DLQ** for failed deliveries\n- **Use FIFO topics** for ordering requirements\n\n### Security\n\n- **Use topic policies** to control access\n- **Enable encryption** with SSE\n- **Use VPC endpoints** for private access\n\n```bash\n# Enable SSE\naws sns set-topic-attributes \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \\\n  --attribute-name KmsMasterKeyId \\\n  --attribute-value alias/my-key\n```\n\n### Cost Optimization\n\n- **Use message filtering** to reduce unnecessary deliveries\n- **Batch operations** where possible\n- **Monitor and clean up** unused topics/subscriptions\n\n### Message Design\n\n- **Keep messages small** (256 KB limit)\n- **Use message attributes** for routing\n- **Include correlation IDs** for tracing\n\n## Troubleshooting\n\n### Subscription Not Receiving Messages\n\n**Check:**\n1. Subscription is confirmed (not pending)\n2. Filter policy matches message attributes\n3. Target permissions (Lambda, SQS)\n\n```bash\n# Check subscription status\naws sns list-subscriptions-by-topic \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic\n\n# Check subscription attributes\naws sns get-subscription-attributes \\\n  --subscription-arn arn:aws:sns:us-east-1:123456789012:my-topic:abc123\n```\n\n### HTTP Endpoint Not Working\n\n**Debug:**\n\n```bash\n# Check delivery status logging\naws sns set-topic-attributes \\\n  --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \\\n  --attribute-name DeliveryPolicy \\\n  --attribute-value '{\n    \"http\": {\n      \"defaultHealthyRetryPolicy\": {\n        \"minDelayTarget\": 20,\n        \"maxDelayTarget\": 20,\n        \"numRetries\": 3,\n        \"numMaxDelayRetries\": 0,\n        \"numNoDelayRetries\": 0,\n        \"numMinDelayRetries\": 0,\n        \"backoffFunction\": \"linear\"\n      }\n    }\n  }'\n```\n\n### Messages Not Matching Filter\n\n**Verify:**\n- Message attributes are set (not in body)\n- Attribute types match (String vs Number)\n- Filter policy syntax is correct\n\n```python\n# Correct: attributes must be message attributes\nsns.publish(\n    TopicArn=topic_arn,\n    Message='body content',\n    MessageAttributes={\n        'event_type': {\n            'DataType': 'String',\n            'StringValue': 'order_created'  # This is filtered\n        }\n    }\n)\n\n# Wrong: this won't be filtered\nsns.publish(\n    TopicArn=topic_arn,\n    Message=json.dumps({'event_type': 'order_created'})  # Not filtered\n)\n```\n\n### SQS Not Receiving from SNS\n\n**Check SQS queue policy:**\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"Service\": \"sns.amazonaws.com\"},\n      \"Action\": \"sqs:SendMessage\",\n      \"Resource\": \"arn:aws:sqs:us-east-1:123456789012:my-queue\",\n      \"Condition\": {\n        \"ArnEquals\": {\n          \"aws:SourceArn\": \"arn:aws:sns:us-east-1:123456789012:my-topic\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## References\n\n- [SNS Developer Guide](https://docs.aws.amazon.com/sns/latest/dg/)\n- [SNS API Reference](https://docs.aws.amazon.com/sns/latest/api/)\n- [SNS CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/sns/)\n- [boto3 SNS](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sns.html)"
              },
              {
                "name": "sqs",
                "description": "AWS SQS message queue service for decoupled architectures. Use when creating queues, configuring dead-letter queues, managing visibility timeouts, implementing FIFO ordering, or integrating with Lambda.",
                "path": "skills/sqs/SKILL.md",
                "frontmatter": {
                  "name": "sqs",
                  "description": "AWS SQS message queue service for decoupled architectures. Use when creating queues, configuring dead-letter queues, managing visibility timeouts, implementing FIFO ordering, or integrating with Lambda.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/"
                },
                "content": "# AWS SQS\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service for decoupling and scaling microservices, distributed systems, and serverless applications.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Queue Types\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **Standard** | At-least-once, best-effort ordering | High throughput |\n| **FIFO** | Exactly-once, strict ordering | Order-sensitive processing |\n\n### Key Settings\n\n| Setting | Description | Default |\n|---------|-------------|---------|\n| **Visibility Timeout** | Time message is hidden after receive | 30 seconds |\n| **Message Retention** | How long messages are kept | 4 days (max 14) |\n| **Delay Seconds** | Delay before message is available | 0 |\n| **Max Message Size** | Maximum message size | 256 KB |\n\n### Dead-Letter Queue (DLQ)\n\nQueue for messages that failed processing after maxReceiveCount attempts.\n\n## Common Patterns\n\n### Create a Standard Queue\n\n**AWS CLI:**\n\n```bash\naws sqs create-queue \\\n  --queue-name my-queue \\\n  --attributes '{\n    \"VisibilityTimeout\": \"60\",\n    \"MessageRetentionPeriod\": \"604800\",\n    \"ReceiveMessageWaitTimeSeconds\": \"20\"\n  }'\n```\n\n**boto3:**\n\n```python\nimport boto3\n\nsqs = boto3.client('sqs')\n\nresponse = sqs.create_queue(\n    QueueName='my-queue',\n    Attributes={\n        'VisibilityTimeout': '60',\n        'MessageRetentionPeriod': '604800',\n        'ReceiveMessageWaitTimeSeconds': '20'  # Long polling\n    }\n)\nqueue_url = response['QueueUrl']\n```\n\n### Create FIFO Queue\n\n```bash\naws sqs create-queue \\\n  --queue-name my-queue.fifo \\\n  --attributes '{\n    \"FifoQueue\": \"true\",\n    \"ContentBasedDeduplication\": \"true\"\n  }'\n```\n\n### Configure Dead-Letter Queue\n\n```bash\n# Create DLQ\naws sqs create-queue --queue-name my-queue-dlq\n\n# Get DLQ ARN\nDLQ_ARN=$(aws sqs get-queue-attributes \\\n  --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue-dlq \\\n  --attribute-names QueueArn \\\n  --query 'Attributes.QueueArn' --output text)\n\n# Set redrive policy on main queue\naws sqs set-queue-attributes \\\n  --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue \\\n  --attributes \"{\n    \\\"RedrivePolicy\\\": \\\"{\\\\\\\"deadLetterTargetArn\\\\\\\":\\\\\\\"${DLQ_ARN}\\\\\\\",\\\\\\\"maxReceiveCount\\\\\\\":\\\\\\\"3\\\\\\\"}\\\"\n  }\"\n```\n\n### Send Messages\n\n```python\nimport boto3\nimport json\n\nsqs = boto3.client('sqs')\nqueue_url = 'https://sqs.us-east-1.amazonaws.com/123456789012/my-queue'\n\n# Send single message\nsqs.send_message(\n    QueueUrl=queue_url,\n    MessageBody=json.dumps({'order_id': '12345', 'action': 'process'}),\n    MessageAttributes={\n        'MessageType': {\n            'DataType': 'String',\n            'StringValue': 'Order'\n        }\n    }\n)\n\n# Send to FIFO queue\nsqs.send_message(\n    QueueUrl='https://sqs.us-east-1.amazonaws.com/123456789012/my-queue.fifo',\n    MessageBody=json.dumps({'order_id': '12345'}),\n    MessageGroupId='order-12345',\n    MessageDeduplicationId='unique-id-12345'\n)\n\n# Batch send (up to 10 messages)\nsqs.send_message_batch(\n    QueueUrl=queue_url,\n    Entries=[\n        {'Id': '1', 'MessageBody': json.dumps({'id': 1})},\n        {'Id': '2', 'MessageBody': json.dumps({'id': 2})},\n        {'Id': '3', 'MessageBody': json.dumps({'id': 3})}\n    ]\n)\n```\n\n### Receive and Process Messages\n\n```python\nimport boto3\nimport json\n\nsqs = boto3.client('sqs')\nqueue_url = 'https://sqs.us-east-1.amazonaws.com/123456789012/my-queue'\n\nwhile True:\n    # Long polling (wait up to 20 seconds)\n    response = sqs.receive_message(\n        QueueUrl=queue_url,\n        MaxNumberOfMessages=10,\n        WaitTimeSeconds=20,\n        MessageAttributeNames=['All'],\n        AttributeNames=['All']\n    )\n\n    messages = response.get('Messages', [])\n\n    for message in messages:\n        try:\n            body = json.loads(message['Body'])\n            print(f\"Processing: {body}\")\n\n            # Process message...\n\n            # Delete on success\n            sqs.delete_message(\n                QueueUrl=queue_url,\n                ReceiptHandle=message['ReceiptHandle']\n            )\n        except Exception as e:\n            print(f\"Error processing message: {e}\")\n            # Message will become visible again after visibility timeout\n```\n\n### Lambda Integration\n\n```bash\n# Create event source mapping\naws lambda create-event-source-mapping \\\n  --function-name my-function \\\n  --event-source-arn arn:aws:sqs:us-east-1:123456789012:my-queue \\\n  --batch-size 10 \\\n  --maximum-batching-window-in-seconds 5\n```\n\nLambda handler:\n\n```python\ndef handler(event, context):\n    for record in event['Records']:\n        body = json.loads(record['body'])\n        message_id = record['messageId']\n\n        try:\n            process_message(body)\n        except Exception as e:\n            # Raise to put message back in queue\n            raise\n\n    return {'batchItemFailures': []}\n```\n\n## CLI Reference\n\n### Queue Management\n\n| Command | Description |\n|---------|-------------|\n| `aws sqs create-queue` | Create queue |\n| `aws sqs delete-queue` | Delete queue |\n| `aws sqs list-queues` | List queues |\n| `aws sqs get-queue-url` | Get queue URL by name |\n| `aws sqs get-queue-attributes` | Get queue settings |\n| `aws sqs set-queue-attributes` | Update queue settings |\n\n### Messaging\n\n| Command | Description |\n|---------|-------------|\n| `aws sqs send-message` | Send single message |\n| `aws sqs send-message-batch` | Send up to 10 messages |\n| `aws sqs receive-message` | Receive messages |\n| `aws sqs delete-message` | Delete message |\n| `aws sqs delete-message-batch` | Delete up to 10 messages |\n| `aws sqs purge-queue` | Delete all messages |\n\n### Visibility\n\n| Command | Description |\n|---------|-------------|\n| `aws sqs change-message-visibility` | Change timeout |\n| `aws sqs change-message-visibility-batch` | Batch change |\n\n## Best Practices\n\n### Message Processing\n\n- **Use long polling** (WaitTimeSeconds=20) to reduce API calls\n- **Delete messages promptly** after successful processing\n- **Configure appropriate visibility timeout** (> processing time)\n- **Implement idempotent consumers** for at-least-once delivery\n\n### Dead-Letter Queues\n\n- **Always configure DLQ** for production queues\n- **Set appropriate maxReceiveCount** (usually 3-5)\n- **Monitor DLQ depth** with CloudWatch alarms\n- **Process DLQ messages** manually or with automation\n\n### FIFO Queues\n\n- **Use message group IDs** to partition ordering\n- **Enable content-based deduplication** or provide dedup IDs\n- **Throughput**: 300 msgs/sec without batching, 3000 with\n\n### Security\n\n- **Use queue policies** to control access\n- **Enable encryption** with SSE-SQS or SSE-KMS\n- **Use VPC endpoints** for private access\n\n## Troubleshooting\n\n### Messages Not Being Received\n\n**Causes:**\n- Short polling returning empty\n- All messages in flight (visibility timeout)\n- Messages delayed (DelaySeconds)\n\n**Debug:**\n\n```bash\n# Check queue attributes\naws sqs get-queue-attributes \\\n  --queue-url $QUEUE_URL \\\n  --attribute-names All\n\n# Check approximate message counts\naws sqs get-queue-attributes \\\n  --queue-url $QUEUE_URL \\\n  --attribute-names \\\n    ApproximateNumberOfMessages,\\\n    ApproximateNumberOfMessagesNotVisible,\\\n    ApproximateNumberOfMessagesDelayed\n```\n\n### Messages Going to DLQ\n\n**Causes:**\n- Processing errors\n- Visibility timeout too short\n- Consumer not deleting messages\n\n**Redrive from DLQ:**\n\n```bash\n# Enable redrive allow policy on source queue\naws sqs set-queue-attributes \\\n  --queue-url $MAIN_QUEUE_URL \\\n  --attributes '{\"RedriveAllowPolicy\": \"{\\\"redrivePermission\\\":\\\"allowAll\\\"}\"}'\n\n# Start redrive\naws sqs start-message-move-task \\\n  --source-arn arn:aws:sqs:us-east-1:123456789012:my-queue-dlq \\\n  --destination-arn arn:aws:sqs:us-east-1:123456789012:my-queue\n```\n\n### Duplicate Processing\n\n**Solutions:**\n- Use FIFO queues for exactly-once\n- Implement idempotency in consumer\n- Track processed message IDs in database\n\n### Lambda Not Processing\n\n```bash\n# Check event source mapping\naws lambda list-event-source-mappings \\\n  --function-name my-function\n\n# Check for errors\naws lambda get-event-source-mapping \\\n  --uuid <mapping-uuid>\n```\n\n## References\n\n- [SQS Developer Guide](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/)\n- [SQS API Reference](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/)\n- [SQS CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/sqs/)\n- [boto3 SQS](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sqs.html)"
              },
              {
                "name": "step-functions",
                "description": "AWS Step Functions workflow orchestration with state machines. Use when designing workflows, implementing error handling, configuring parallel execution, integrating with AWS services, or debugging executions.",
                "path": "skills/step-functions/SKILL.md",
                "frontmatter": {
                  "name": "step-functions",
                  "description": "AWS Step Functions workflow orchestration with state machines. Use when designing workflows, implementing error handling, configuring parallel execution, integrating with AWS services, or debugging executions.",
                  "last_updated": "2026-01-07",
                  "doc_source": "https://docs.aws.amazon.com/step-functions/latest/dg/"
                },
                "content": "# AWS Step Functions\n\nAWS Step Functions is a serverless orchestration service that lets you build and run workflows using state machines. Coordinate multiple AWS services into business-critical applications.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Common Patterns](#common-patterns)\n- [CLI Reference](#cli-reference)\n- [Best Practices](#best-practices)\n- [Troubleshooting](#troubleshooting)\n- [References](#references)\n\n## Core Concepts\n\n### Workflow Types\n\n| Type | Description | Pricing |\n|------|-------------|---------|\n| **Standard** | Long-running, durable, exactly-once | Per state transition |\n| **Express** | High-volume, short-duration | Per execution (time + memory) |\n\n### State Types\n\n| State | Description |\n|-------|-------------|\n| **Task** | Execute work (Lambda, API call) |\n| **Choice** | Conditional branching |\n| **Parallel** | Execute branches concurrently |\n| **Map** | Iterate over array |\n| **Wait** | Delay execution |\n| **Pass** | Pass input to output |\n| **Succeed** | End successfully |\n| **Fail** | End with failure |\n\n### Amazon States Language (ASL)\n\nJSON-based language for defining state machines.\n\n## Common Patterns\n\n### Simple Lambda Workflow\n\n```json\n{\n  \"Comment\": \"Process order workflow\",\n  \"StartAt\": \"ValidateOrder\",\n  \"States\": {\n    \"ValidateOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ValidateOrder\",\n      \"Next\": \"ProcessPayment\"\n    },\n    \"ProcessPayment\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessPayment\",\n      \"Next\": \"FulfillOrder\"\n    },\n    \"FulfillOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:FulfillOrder\",\n      \"End\": true\n    }\n  }\n}\n```\n\n### Create State Machine\n\n**AWS CLI:**\n\n```bash\naws stepfunctions create-state-machine \\\n  --name OrderWorkflow \\\n  --definition file://workflow.json \\\n  --role-arn arn:aws:iam::123456789012:role/StepFunctionsRole \\\n  --type STANDARD\n```\n\n**boto3:**\n\n```python\nimport boto3\nimport json\n\nsfn = boto3.client('stepfunctions')\n\ndefinition = {\n    \"Comment\": \"Order workflow\",\n    \"StartAt\": \"ProcessOrder\",\n    \"States\": {\n        \"ProcessOrder\": {\n            \"Type\": \"Task\",\n            \"Resource\": \"arn:aws:lambda:...\",\n            \"End\": True\n        }\n    }\n}\n\nresponse = sfn.create_state_machine(\n    name='OrderWorkflow',\n    definition=json.dumps(definition),\n    roleArn='arn:aws:iam::123456789012:role/StepFunctionsRole',\n    type='STANDARD'\n)\n```\n\n### Start Execution\n\n```python\nimport boto3\nimport json\n\nsfn = boto3.client('stepfunctions')\n\nresponse = sfn.start_execution(\n    stateMachineArn='arn:aws:states:us-east-1:123456789012:stateMachine:OrderWorkflow',\n    name='order-12345',\n    input=json.dumps({\n        'order_id': '12345',\n        'customer_id': 'cust-789',\n        'items': [{'product_id': 'prod-1', 'quantity': 2}]\n    })\n)\n\nexecution_arn = response['executionArn']\n```\n\n### Choice State (Conditional Logic)\n\n```json\n{\n  \"StartAt\": \"CheckOrderValue\",\n  \"States\": {\n    \"CheckOrderValue\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [\n        {\n          \"Variable\": \"$.total\",\n          \"NumericGreaterThan\": 1000,\n          \"Next\": \"HighValueOrder\"\n        },\n        {\n          \"Variable\": \"$.priority\",\n          \"StringEquals\": \"rush\",\n          \"Next\": \"RushOrder\"\n        }\n      ],\n      \"Default\": \"StandardOrder\"\n    },\n    \"HighValueOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:...:function:ProcessHighValue\",\n      \"End\": true\n    },\n    \"RushOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:...:function:ProcessRush\",\n      \"End\": true\n    },\n    \"StandardOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:...:function:ProcessStandard\",\n      \"End\": true\n    }\n  }\n}\n```\n\n### Parallel Execution\n\n```json\n{\n  \"StartAt\": \"ProcessInParallel\",\n  \"States\": {\n    \"ProcessInParallel\": {\n      \"Type\": \"Parallel\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"UpdateInventory\",\n          \"States\": {\n            \"UpdateInventory\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:lambda:...:function:UpdateInventory\",\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"SendNotification\",\n          \"States\": {\n            \"SendNotification\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:lambda:...:function:SendNotification\",\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"UpdateAnalytics\",\n          \"States\": {\n            \"UpdateAnalytics\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:lambda:...:function:UpdateAnalytics\",\n              \"End\": true\n            }\n          }\n        }\n      ],\n      \"Next\": \"Complete\"\n    },\n    \"Complete\": {\n      \"Type\": \"Succeed\"\n    }\n  }\n}\n```\n\n### Map State (Iteration)\n\n```json\n{\n  \"StartAt\": \"ProcessItems\",\n  \"States\": {\n    \"ProcessItems\": {\n      \"Type\": \"Map\",\n      \"ItemsPath\": \"$.items\",\n      \"MaxConcurrency\": 10,\n      \"Iterator\": {\n        \"StartAt\": \"ProcessItem\",\n        \"States\": {\n          \"ProcessItem\": {\n            \"Type\": \"Task\",\n            \"Resource\": \"arn:aws:lambda:...:function:ProcessItem\",\n            \"End\": true\n          }\n        }\n      },\n      \"ResultPath\": \"$.processedItems\",\n      \"End\": true\n    }\n  }\n}\n```\n\n### Error Handling\n\n```json\n{\n  \"StartAt\": \"ProcessWithRetry\",\n  \"States\": {\n    \"ProcessWithRetry\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:...:function:Process\",\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\"Lambda.ServiceException\", \"Lambda.TooManyRequestsException\"],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        },\n        {\n          \"ErrorEquals\": [\"States.Timeout\"],\n          \"IntervalSeconds\": 5,\n          \"MaxAttempts\": 3,\n          \"BackoffRate\": 1.5\n        }\n      ],\n      \"Catch\": [\n        {\n          \"ErrorEquals\": [\"CustomError\"],\n          \"ResultPath\": \"$.error\",\n          \"Next\": \"HandleCustomError\"\n        },\n        {\n          \"ErrorEquals\": [\"States.ALL\"],\n          \"ResultPath\": \"$.error\",\n          \"Next\": \"HandleAllErrors\"\n        }\n      ],\n      \"End\": true\n    },\n    \"HandleCustomError\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:...:function:HandleCustom\",\n      \"End\": true\n    },\n    \"HandleAllErrors\": {\n      \"Type\": \"Fail\",\n      \"Error\": \"ProcessingFailed\",\n      \"Cause\": \"An error occurred during processing\"\n    }\n  }\n}\n```\n\n## CLI Reference\n\n### State Machine Management\n\n| Command | Description |\n|---------|-------------|\n| `aws stepfunctions create-state-machine` | Create state machine |\n| `aws stepfunctions update-state-machine` | Update definition |\n| `aws stepfunctions delete-state-machine` | Delete state machine |\n| `aws stepfunctions list-state-machines` | List state machines |\n| `aws stepfunctions describe-state-machine` | Get details |\n\n### Executions\n\n| Command | Description |\n|---------|-------------|\n| `aws stepfunctions start-execution` | Start execution |\n| `aws stepfunctions stop-execution` | Stop execution |\n| `aws stepfunctions describe-execution` | Get execution details |\n| `aws stepfunctions list-executions` | List executions |\n| `aws stepfunctions get-execution-history` | Get execution history |\n\n## Best Practices\n\n### Design\n\n- **Keep states focused** â€” one purpose per state\n- **Use meaningful state names**\n- **Implement comprehensive error handling**\n- **Use Parallel for independent tasks**\n- **Use Map for batch processing**\n\n### Performance\n\n- **Use Express workflows** for high-volume, short tasks\n- **Set appropriate timeouts**\n- **Limit Map concurrency** to avoid throttling\n- **Use SDK integrations** when possible (avoid Lambda wrapper)\n\n### Reliability\n\n- **Retry transient errors**\n- **Catch and handle specific errors**\n- **Use idempotent operations**\n- **Enable X-Ray tracing**\n\n### Cost Optimization\n\n- **Use Express for short workflows** (< 5 minutes)\n- **Combine related operations** to reduce transitions\n- **Use Wait states** instead of Lambda delays\n\n## Troubleshooting\n\n### Execution Failed\n\n```bash\n# Get execution history\naws stepfunctions get-execution-history \\\n  --execution-arn arn:aws:states:us-east-1:123456789012:execution:MyWorkflow:exec-123 \\\n  --query 'events[?type==`TaskFailed` || type==`ExecutionFailed`]'\n```\n\n### Lambda Timeout\n\n**Causes:**\n- Lambda running too long\n- Task timeout too short\n\n**Fix:**\n\n```json\n{\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:...\",\n  \"TimeoutSeconds\": 300,\n  \"HeartbeatSeconds\": 60\n}\n```\n\n### State Stuck\n\n**Check:**\n- Task state waiting for callback\n- Wait state not yet elapsed\n- Activity worker not responding\n\n### Invalid State Machine\n\n```bash\n# Validate definition\naws stepfunctions validate-state-machine-definition \\\n  --definition file://workflow.json\n```\n\n## References\n\n- [Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/)\n- [Step Functions API Reference](https://docs.aws.amazon.com/step-functions/latest/apireference/)\n- [Step Functions CLI Reference](https://docs.aws.amazon.com/cli/latest/reference/stepfunctions/)\n- [Amazon States Language](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html)"
              }
            ]
          }
        ]
      }
    },
    {
      "full_name": "itsmostafa/llm-engineering-skills",
      "url": "https://github.com/itsmostafa/llm-engineering-skills",
      "description": "LLM Engineering Claude Skills",
      "homepage": "",
      "signals": {
        "stars": 10,
        "forks": 0,
        "pushed_at": "2026-01-05T21:35:53Z",
        "created_at": "2026-01-05T00:58:54Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 815
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 4708
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 2469
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 5420
        },
        {
          "path": "REFERENCES.md",
          "type": "blob",
          "size": 2642
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/agents/SKILL.md",
          "type": "blob",
          "size": 11183
        },
        {
          "path": "skills/context-engineering",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-engineering/SKILL.md",
          "type": "blob",
          "size": 12233
        },
        {
          "path": "skills/context-engineering/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/context-engineering/reference/evaluation-strategies.md",
          "type": "blob",
          "size": 9533
        },
        {
          "path": "skills/context-engineering/reference/summarization-patterns.md",
          "type": "blob",
          "size": 11242
        },
        {
          "path": "skills/lora",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lora/SKILL.md",
          "type": "blob",
          "size": 11615
        },
        {
          "path": "skills/lora/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/lora/reference/advanced-techniques.md",
          "type": "blob",
          "size": 6456
        },
        {
          "path": "skills/mlx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/mlx/SKILL.md",
          "type": "blob",
          "size": 8493
        },
        {
          "path": "skills/mlx/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/mlx/reference/fine-tuning.md",
          "type": "blob",
          "size": 8086
        },
        {
          "path": "skills/mlx/reference/quantization.md",
          "type": "blob",
          "size": 6001
        },
        {
          "path": "skills/prompt-engineering",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/prompt-engineering/SKILL.md",
          "type": "blob",
          "size": 9540
        },
        {
          "path": "skills/pytorch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pytorch/SKILL.md",
          "type": "blob",
          "size": 9790
        },
        {
          "path": "skills/pytorch/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pytorch/reference/debugging.md",
          "type": "blob",
          "size": 7610
        },
        {
          "path": "skills/pytorch/reference/training-patterns.md",
          "type": "blob",
          "size": 7574
        },
        {
          "path": "skills/qlora",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/qlora/SKILL.md",
          "type": "blob",
          "size": 12229
        },
        {
          "path": "skills/rlhf",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rlhf/SKILL.md",
          "type": "blob",
          "size": 12489
        },
        {
          "path": "skills/rlhf/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rlhf/reference/direct-alignment.md",
          "type": "blob",
          "size": 9221
        },
        {
          "path": "skills/rlhf/reference/policy-optimization.md",
          "type": "blob",
          "size": 9173
        },
        {
          "path": "skills/rlhf/reference/reward-modeling.md",
          "type": "blob",
          "size": 7884
        },
        {
          "path": "skills/transformers",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/transformers/SKILL.md",
          "type": "blob",
          "size": 14959
        },
        {
          "path": "skills/transformers/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/transformers/reference/fine-tuning.md",
          "type": "blob",
          "size": 12534
        }
      ],
      "marketplace": {
        "name": "llm-engineering-skills",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "itsmostafa",
          "email": "mostafaxcodes@gmail.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "llm-engineering-skills",
            "description": "Collection of LLM engineering skills for PyTorch, Transformers, LoRA, and MLX",
            "source": "./",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add itsmostafa/llm-engineering-skills",
              "/plugin install llm-engineering-skills@llm-engineering-skills"
            ],
            "signals": {
              "stars": 10,
              "forks": 0,
              "pushed_at": "2026-01-05T21:35:53Z",
              "created_at": "2026-01-05T00:58:54Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "agents",
                "description": "Patterns and architectures for building AI agents and workflows with LLMs. Use when designing systems that involve tool use, multi-step reasoning, autonomous decision-making, or orchestration of LLM-driven tasks.",
                "path": "skills/agents/SKILL.md",
                "frontmatter": {
                  "name": "agents",
                  "description": "Patterns and architectures for building AI agents and workflows with LLMs. Use when designing systems that involve tool use, multi-step reasoning, autonomous decision-making, or orchestration of LLM-driven tasks."
                },
                "content": "# Building Agents\n\nAgents are systems where LLMs dynamically direct their own processes and tool usage. This skill covers when to use agents vs workflows, common architectural patterns, and practical implementation guidance.\n\n## Table of Contents\n\n- [Agents vs Workflows](#agents-vs-workflows)\n- [Workflow Patterns](#workflow-patterns)\n- [Agent Architectures](#agent-architectures)\n- [ReAct Pattern](#react-pattern)\n- [Tool Design](#tool-design)\n- [Best Practices](#best-practices)\n- [References](#references)\n\n## Agents vs Workflows\n\n| Aspect | Workflows | Agents |\n|--------|-----------|--------|\n| **Control flow** | Predefined code paths | LLM determines next step |\n| **Predictability** | High - deterministic steps | Lower - dynamic decisions |\n| **Complexity** | Simpler to debug and test | More complex, harder to predict |\n| **Best for** | Well-defined, repeatable tasks | Open-ended, adaptive problems |\n\n**Key principle**: Start with the simplest solution. Use workflows when the task is predictable; use agents when flexibility is required.\n\n## Workflow Patterns\n\n### 1. Prompt Chaining\n\nDecompose tasks into sequential LLM calls, where each step's output feeds the next.\n\n```python\nasync def prompt_chain(input_text):\n    # Step 1: Extract key information\n    extracted = await llm.generate(\n        \"Extract the main entities and relationships from: \" + input_text\n    )\n\n    # Step 2: Analyze\n    analysis = await llm.generate(\n        \"Analyze these entities for patterns: \" + extracted\n    )\n\n    # Step 3: Generate output\n    return await llm.generate(\n        \"Based on this analysis, provide recommendations: \" + analysis\n    )\n```\n\n**Use when**: Tasks naturally decompose into fixed sequential steps.\n\n### 2. Routing\n\nClassify inputs and direct them to specialized handlers.\n\n```python\nasync def route_request(user_input):\n    # Classify the input\n    category = await llm.generate(\n        f\"Classify this request into one of: [billing, technical, general]\\n{user_input}\"\n    )\n\n    handlers = {\n        \"billing\": handle_billing,\n        \"technical\": handle_technical,\n        \"general\": handle_general,\n    }\n\n    return await handlers[category.strip()](user_input)\n```\n\n**Use when**: Different input types need fundamentally different processing.\n\n### 3. Parallelization\n\nRun multiple LLM calls concurrently for independent subtasks.\n\n```python\nimport asyncio\n\nasync def parallel_analysis(document):\n    # Run independent analyses in parallel\n    results = await asyncio.gather(\n        llm.generate(f\"Summarize: {document}\"),\n        llm.generate(f\"Extract key facts: {document}\"),\n        llm.generate(f\"Identify sentiment: {document}\"),\n    )\n\n    summary, facts, sentiment = results\n    return {\"summary\": summary, \"facts\": facts, \"sentiment\": sentiment}\n```\n\n**Variants**:\n- **Sectioning**: Break task into parallel subtasks\n- **Voting**: Run same prompt multiple times, aggregate results\n\n### 4. Orchestrator-Workers\n\nCentral LLM decomposes tasks and delegates to worker LLMs.\n\n```python\nclass Orchestrator:\n    async def run(self, task):\n        # Break down the task\n        subtasks = await self.plan(task)\n\n        # Delegate to workers\n        results = []\n        for subtask in subtasks:\n            worker_result = await self.delegate(subtask)\n            results.append(worker_result)\n\n        # Synthesize results\n        return await self.synthesize(results)\n\n    async def plan(self, task):\n        response = await llm.generate(\n            f\"Break this task into subtasks:\\n{task}\\n\\nReturn as JSON array.\"\n        )\n        return json.loads(response)\n\n    async def delegate(self, subtask):\n        return await llm.generate(f\"Complete this subtask:\\n{subtask}\")\n\n    async def synthesize(self, results):\n        return await llm.generate(\n            f\"Combine these results into a coherent response:\\n{results}\"\n        )\n```\n\n**Use when**: Tasks require dynamic decomposition that can't be predetermined.\n\n### 5. Evaluator-Optimizer\n\nOne LLM generates, another evaluates and requests improvements.\n\n```python\nasync def generate_with_feedback(task, max_iterations=3):\n    response = await llm.generate(f\"Complete this task:\\n{task}\")\n\n    for _ in range(max_iterations):\n        evaluation = await llm.generate(\n            f\"Evaluate this response for quality and correctness:\\n{response}\\n\"\n            \"If improvements needed, specify them. Otherwise respond 'APPROVED'.\"\n        )\n\n        if \"APPROVED\" in evaluation:\n            return response\n\n        response = await llm.generate(\n            f\"Improve this response based on feedback:\\n\"\n            f\"Original: {response}\\nFeedback: {evaluation}\"\n        )\n\n    return response\n```\n\n**Use when**: Output quality is critical and can be objectively evaluated.\n\n## Agent Architectures\n\n### Autonomous Agent Loop\n\nAgents operate in a loop: observe, think, act, repeat.\n\n```python\nclass Agent:\n    def __init__(self, tools: list, system_prompt: str):\n        self.tools = {t.name: t for t in tools}\n        self.system_prompt = system_prompt\n\n    async def run(self, task: str, max_steps: int = 10):\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": task},\n        ]\n\n        for step in range(max_steps):\n            response = await llm.generate(messages, tools=self.tools)\n            messages.append({\"role\": \"assistant\", \"content\": response})\n\n            if response.tool_calls:\n                for call in response.tool_calls:\n                    result = await self.execute_tool(call)\n                    messages.append({\n                        \"role\": \"tool\",\n                        \"tool_call_id\": call.id,\n                        \"content\": result\n                    })\n            else:\n                # No tool calls - agent is done\n                return response.content\n\n        return \"Max steps reached\"\n\n    async def execute_tool(self, call):\n        tool = self.tools[call.name]\n        return await tool.execute(**call.arguments)\n```\n\n### Human-in-the-Loop\n\nPause for human approval at critical checkpoints.\n\n```python\nclass HumanInLoopAgent(Agent):\n    def __init__(self, tools, system_prompt, approval_required: list):\n        super().__init__(tools, system_prompt)\n        self.approval_required = set(approval_required)\n\n    async def execute_tool(self, call):\n        if call.name in self.approval_required:\n            approved = await self.request_approval(call)\n            if not approved:\n                return \"Action cancelled by user\"\n\n        return await super().execute_tool(call)\n\n    async def request_approval(self, call):\n        print(f\"Agent wants to execute: {call.name}({call.arguments})\")\n        response = input(\"Approve? (y/n): \")\n        return response.lower() == \"y\"\n```\n\n## ReAct Pattern\n\nReAct (Reasoning and Acting) alternates between thinking and taking actions.\n\n```python\nREACT_PROMPT = \"\"\"Answer the question using the available tools.\n\nFor each step:\n1. Thought: Reason about what to do next\n2. Action: Choose a tool and inputs\n3. Observation: See the result\n4. Repeat until you have the answer\n\nAvailable tools: {tools}\n\nQuestion: {question}\n\"\"\"\n\nasync def react_agent(question, tools):\n    prompt = REACT_PROMPT.format(\n        tools=format_tools(tools),\n        question=question\n    )\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    while True:\n        response = await llm.generate(messages)\n        messages.append({\"role\": \"assistant\", \"content\": response})\n\n        if \"Final Answer:\" in response:\n            return extract_final_answer(response)\n\n        action = parse_action(response)\n        if action:\n            observation = await execute_tool(action, tools)\n            messages.append({\n                \"role\": \"user\",\n                \"content\": f\"Observation: {observation}\"\n            })\n```\n\n**Advantages**:\n- Explicit reasoning traces aid debugging\n- More interpretable decision-making\n- Better handling of complex multi-step tasks\n\n## Tool Design\n\n### Principles\n\n1. **Self-contained**: Tools return complete, usable information\n2. **Scoped**: Each tool does one thing well\n3. **Descriptive**: Clear names and descriptions guide the LLM\n4. **Error-robust**: Return informative errors, not exceptions\n\n### Tool Definition Pattern\n\n```python\nclass Tool:\n    def __init__(self, name: str, description: str, parameters: dict, fn):\n        self.name = name\n        self.description = description\n        self.parameters = parameters\n        self.fn = fn\n\n    async def execute(self, **kwargs):\n        try:\n            return await self.fn(**kwargs)\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n# Example tool\nsearch_tool = Tool(\n    name=\"search_database\",\n    description=\"Search the database for records matching a query. \"\n                \"Returns up to 10 matching records with their IDs and summaries.\",\n    parameters={\n        \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n        \"limit\": {\"type\": \"integer\", \"description\": \"Max results (default 10)\"},\n    },\n    fn=search_database\n)\n```\n\n### Tool Interface Guidelines\n\n- Prefer text inputs/outputs over complex structured data\n- Include usage examples in descriptions for ambiguous tools\n- Return truncated results when output could be large\n- Provide clear feedback on what the tool did\n\n## Best Practices\n\n1. **Start simple**: Begin with the simplest architecture that could work. Add complexity only when it demonstrably improves outcomes.\n\n2. **Maintain transparency**: Ensure the agent's planning steps are visible. This aids debugging and builds user trust.\n\n3. **Design for failure**: Agents will make mistakes. Include guardrails, retries, and graceful degradation.\n\n4. **Test extensively**: Use sandboxed environments. Test edge cases and failure modes, not just happy paths.\n\n5. **Limit tool proliferation**: More tools means more confusion. Keep the tool set focused and well-documented.\n\n6. **Implement checkpoints**: For long-running tasks, save state periodically to enable recovery.\n\n7. **Set resource limits**: Cap iterations, token usage, and tool calls to prevent runaway agents.\n\n8. **Log everything**: Record all LLM calls, tool executions, and decisions for debugging and improvement.\n\n9. **Handle ambiguity**: When uncertain, have the agent ask for clarification rather than guessing.\n\n10. **Measure outcomes**: Track task completion rates, accuracy, and efficiency to guide improvements.\n\n## References\n\n- [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) - Anthropic's guide to agent patterns and best practices\n- [LangGraph Workflows & Agents](https://docs.langchain.com/oss/javascript/langgraph/workflows-agents) - LangGraph documentation on agent architectures\n- [ReAct: Synergizing Reasoning and Acting](https://arxiv.org/abs/2210.03629) - Paper introducing the ReAct prompting pattern"
              },
              {
                "name": "context-engineering",
                "description": "Strategies for managing LLM context windows effectively in AI agents. Use when building agents that handle long conversations, multi-step tasks, tool orchestration, or need to maintain coherence across extended interactions.",
                "path": "skills/context-engineering/SKILL.md",
                "frontmatter": {
                  "name": "context-engineering",
                  "description": "Strategies for managing LLM context windows effectively in AI agents. Use when building agents that handle long conversations, multi-step tasks, tool orchestration, or need to maintain coherence across extended interactions."
                },
                "content": "# Context Engineering\n\nContext engineering is the discipline of curating and maintaining the optimal set of tokens during LLM inference. Unlike prompt engineering (crafting individual prompts), context engineering focuses on what information enters the context window and when.\n\n## Table of Contents\n\n- [Core Principles](#core-principles)\n- [Context Management Strategies](#context-management-strategies)\n- [System Prompt Design](#system-prompt-design)\n- [Tool Design for Context Efficiency](#tool-design-for-context-efficiency)\n- [Long-Horizon Task Patterns](#long-horizon-task-patterns)\n- [Implementation Patterns](#implementation-patterns)\n- [Best Practices](#best-practices)\n- [References](#references)\n\n## Core Principles\n\n### Context as a Finite Resource\n\nLLMs have limited \"attention budgets.\" As context length increases, models experience **context rot**â€”decreased ability to accurately recall information. The goal is finding the smallest possible set of high-signal tokens that maximize desired outcomes.\n\n```\nEffective Context = Relevant Information / Total Tokens\n```\n\n**Key insight**: More context isn't better. The right context is better.\n\n### The Context Pollution Problem\n\nEvery token added to context has costs:\n- Increased latency and compute\n- Diluted attention to important information\n- Higher risk of hallucination from conflicting data\n- Reduced model performance on retrieval tasks\n\n## Context Management Strategies\n\n### 1. Context Trimming\n\nDrop older conversation turns, keeping only the last N turns.\n\n| Aspect | Details |\n|--------|---------|\n| **Mechanism** | Sliding window over conversation history |\n| **Pros** | Deterministic, zero latency, preserves recent context verbatim |\n| **Cons** | Abrupt loss of long-range context, \"amnesia\" effect |\n| **Best for** | Independent tasks, short interactions, predictable workflows |\n\n```python\ndef trim_context(messages: list, keep_last_n: int = 10) -> list:\n    \"\"\"Keep system message + last N turns.\"\"\"\n    system_msgs = [m for m in messages if m[\"role\"] == \"system\"]\n    other_msgs = [m for m in messages if m[\"role\"] != \"system\"]\n    return system_msgs + other_msgs[-keep_last_n:]\n```\n\n### 2. Context Summarization\n\nCompress prior messages into structured summaries.\n\n| Aspect | Details |\n|--------|---------|\n| **Mechanism** | LLM generates summary of older context |\n| **Pros** | Retains long-range memory, smoother UX, scalable |\n| **Cons** | Summarization bias risk, added latency, potential compounding errors |\n| **Best for** | Complex multi-step tasks, long-horizon interactions |\n\n```python\nSUMMARIZATION_PROMPT = \"\"\"Summarize the conversation so far, preserving:\n1. Key decisions made\n2. Important context established\n3. Current task state and goals\n4. Any constraints or preferences expressed\n\nBe concise but complete. Output as structured markdown.\"\"\"\n\nasync def summarize_context(messages: list, model) -> str:\n    \"\"\"Generate a summary of conversation history.\"\"\"\n    conversation_text = format_messages_for_summary(messages)\n    response = await model.generate(\n        system=SUMMARIZATION_PROMPT,\n        user=conversation_text\n    )\n    return response.content\n```\n\n### 3. Hybrid Approach\n\nCombine trimming and summarization for optimal balance.\n\n```python\nclass HybridContextManager:\n    def __init__(\n        self,\n        keep_recent: int = 5,      # Recent turns to keep verbatim\n        summary_threshold: int = 20, # When to trigger summarization\n    ):\n        self.keep_recent = keep_recent\n        self.summary_threshold = summary_threshold\n        self.running_summary = \"\"\n\n    def process(self, messages: list) -> list:\n        if len(messages) < self.summary_threshold:\n            return messages\n\n        # Summarize older messages\n        old_messages = messages[:-self.keep_recent]\n        self.running_summary = summarize(old_messages, self.running_summary)\n\n        # Return summary + recent messages\n        return [\n            {\"role\": \"system\", \"content\": f\"Previous context:\\n{self.running_summary}\"},\n            *messages[-self.keep_recent:]\n        ]\n```\n\n## System Prompt Design\n\n### Principles for Context-Efficient Prompts\n\n1. **Clear and direct language**: Avoid ambiguity that requires clarification turns\n2. **Structured sections**: Organize by purpose (role, capabilities, constraints)\n3. **Minimal yet comprehensive**: Include only what affects behavior\n4. **Self-contained instructions**: Reduce need for context retrieval\n\n### Example Structure\n\n```markdown\n# Role\nYou are [specific role] that [primary function].\n\n# Capabilities\n- [Capability 1 with scope]\n- [Capability 2 with scope]\n\n# Constraints\n- [Hard constraint]\n- [Preference]\n\n# Output Format\n[Specific format requirements]\n```\n\n## Tool Design for Context Efficiency\n\n### Just-in-Time Context Loading\n\nInstead of front-loading all possible context, load information dynamically as needed.\n\n```python\n# Anti-pattern: Loading everything upfront\ncontext = load_all_user_data()  # Large, mostly unused\ncontext += load_all_documents()  # Even larger\n\n# Better: Just-in-time retrieval\ntools = [\n    Tool(\n        name=\"get_user_preference\",\n        description=\"Get specific user preference by key\",\n        # Only fetches what's needed when asked\n    ),\n    Tool(\n        name=\"search_documents\",\n        description=\"Search documents by query\",\n        # Returns relevant subset\n    ),\n]\n```\n\n### Tool Design Principles\n\n1. **Self-contained**: Each tool returns complete, usable information\n2. **Scoped**: Tools do one thing well\n3. **Descriptive**: Names and descriptions guide LLM toward correct usage\n4. **Error-robust**: Return informative errors that don't pollute context\n\n```python\n# Well-designed tool\ndef search_codebase(query: str, max_results: int = 5) -> str:\n    \"\"\"Search codebase for relevant code snippets.\n\n    Args:\n        query: Natural language description of what to find\n        max_results: Maximum snippets to return (default 5)\n\n    Returns:\n        Formatted code snippets with file paths and line numbers,\n        or 'No results found' if nothing matches.\n    \"\"\"\n    results = perform_search(query, limit=max_results)\n    if not results:\n        return \"No results found for query.\"\n    return format_results(results)  # Concise, structured output\n```\n\n## Long-Horizon Task Patterns\n\n### Pattern 1: Compaction\n\nPeriodically compress conversation history to reclaim context space.\n\n```python\nasync def compaction_loop(agent, messages, task):\n    while not task.complete:\n        # Process next step\n        response = await agent.run(messages)\n        messages.append(response)\n\n        # Compact when approaching limit\n        if estimate_tokens(messages) > TOKEN_LIMIT * 0.8:\n            summary = await summarize_context(messages[:-3])\n            messages = [\n                {\"role\": \"system\", \"content\": agent.system_prompt},\n                {\"role\": \"assistant\", \"content\": f\"Summary of progress:\\n{summary}\"},\n                *messages[-3:]  # Keep recent context\n            ]\n\n    return messages\n```\n\n### Pattern 2: Structured Note-Taking\n\nAgent maintains external notes, retrieving as needed.\n\n```python\nclass NoteTakingAgent:\n    def __init__(self):\n        self.notes = {}  # Key-value store outside context\n\n    async def run(self, messages):\n        tools = [\n            Tool(\"save_note\", self.save_note, \"Save information for later\"),\n            Tool(\"get_note\", self.get_note, \"Retrieve saved information\"),\n            Tool(\"list_notes\", self.list_notes, \"List all saved note keys\"),\n        ]\n        return await self.agent.run(messages, tools=tools)\n\n    def save_note(self, key: str, content: str) -> str:\n        self.notes[key] = content\n        return f\"Saved note: {key}\"\n\n    def get_note(self, key: str) -> str:\n        return self.notes.get(key, f\"No note found for key: {key}\")\n```\n\n### Pattern 3: Sub-Agent Architecture\n\nDelegate focused tasks to specialized agents with clean context.\n\n```python\nclass OrchestratorAgent:\n    def __init__(self):\n        self.sub_agents = {\n            \"researcher\": ResearchAgent(),\n            \"coder\": CodingAgent(),\n            \"reviewer\": ReviewAgent(),\n        }\n\n    async def delegate(self, task: str, agent_type: str) -> str:\n        \"\"\"Delegate to sub-agent, receive condensed summary.\"\"\"\n        agent = self.sub_agents[agent_type]\n\n        # Sub-agent works with fresh context\n        result = await agent.run(task)\n\n        # Return only essential findings to main context\n        return result.summary  # Not the full conversation\n```\n\n**Benefits**:\n- Each sub-agent has focused, clean context\n- Main agent receives condensed results\n- Parallelization opportunities\n- Failure isolation\n\n## Implementation Patterns\n\n### Session Memory Manager\n\n```python\nclass SessionMemory:\n    def __init__(\n        self,\n        keep_last_n_turns: int = 5,\n        context_limit: int = 100_000,  # tokens\n        summarizer = None,\n    ):\n        self.keep_last_n_turns = keep_last_n_turns\n        self.context_limit = context_limit\n        self.summarizer = summarizer\n        self.messages = []\n        self.summary = \"\"\n\n    async def add_message(self, message: dict):\n        self.messages.append(message)\n        await self._maybe_compact()\n\n    async def _maybe_compact(self):\n        current_tokens = estimate_tokens(self.messages)\n\n        if current_tokens > self.context_limit * 0.8:\n            # Summarize all but recent messages\n            old_messages = self.messages[:-self.keep_last_n_turns]\n            new_summary = await self.summarizer.summarize(\n                old_messages,\n                previous_summary=self.summary\n            )\n            self.summary = new_summary\n            self.messages = self.messages[-self.keep_last_n_turns:]\n\n    def get_context(self) -> list:\n        context = []\n        if self.summary:\n            context.append({\n                \"role\": \"system\",\n                \"content\": f\"Conversation summary:\\n{self.summary}\"\n            })\n        context.extend(self.messages)\n        return context\n```\n\n### Token Estimation\n\n```python\ndef estimate_tokens(messages: list) -> int:\n    \"\"\"Rough token estimation (4 chars â‰ˆ 1 token for English).\"\"\"\n    total_chars = sum(\n        len(m.get(\"content\", \"\"))\n        for m in messages\n    )\n    return total_chars // 4\n\ndef estimate_tokens_accurate(messages: list, model: str) -> int:\n    \"\"\"Accurate token count using tiktoken.\"\"\"\n    import tiktoken\n    encoding = tiktoken.encoding_for_model(model)\n    return sum(\n        len(encoding.encode(m.get(\"content\", \"\")))\n        for m in messages\n    )\n```\n\n## Best Practices\n\n1. **Treat context as precious**: Every token has a cost. Include only information that improves task performance.\n\n2. **Use progressive disclosure**: Start minimal, expand context only when needed via tools.\n\n3. **Design for recoverability**: Agents should be able to reconstruct critical context from external sources.\n\n4. **Monitor context health**: Track token usage, retrieval accuracy, and task completion rates.\n\n5. **Prefer structured over raw data**: JSON, markdown tables, and clear formatting improve information density.\n\n6. **Implement graceful degradation**: When context limits approach, prioritize recent and high-signal information.\n\n7. **Test with long conversations**: Validate agent behavior after many turns, not just initial interactions.\n\n8. **Separate concerns**: Use different context regions for system instructions, user history, and tool outputs.\n\n9. **Version your summaries**: When compacting, maintain enough structure to debug summarization issues.\n\n10. **Measure and iterate**: Context engineering is empiricalâ€”test what information actually improves outcomes.\n\n## References\n\n- [reference/evaluation-strategies.md](reference/evaluation-strategies.md) - Testing context management effectiveness\n- [reference/summarization-patterns.md](reference/summarization-patterns.md) - Detailed summarization implementations"
              },
              {
                "name": "lora",
                "description": "Parameter-efficient fine-tuning with Low-Rank Adaptation (LoRA). Use when fine-tuning large language models with limited GPU memory, creating task-specific adapters, or when you need to train multiple specialized models from a single base.",
                "path": "skills/lora/SKILL.md",
                "frontmatter": {
                  "name": "lora",
                  "description": "Parameter-efficient fine-tuning with Low-Rank Adaptation (LoRA). Use when fine-tuning large language models with limited GPU memory, creating task-specific adapters, or when you need to train multiple specialized models from a single base."
                },
                "content": "# Using LoRA for Fine-tuning\n\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by freezing pretrained weights and injecting small trainable matrices into transformer layers. This reduces trainable parameters to ~0.1% of the original model while maintaining performance.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Basic Setup](#basic-setup)\n- [Configuration Parameters](#configuration-parameters)\n- [QLoRA (Quantized LoRA)](#qlora-quantized-lora)\n- [Training Patterns](#training-patterns)\n- [Saving and Loading](#saving-and-loading)\n- [Merging Adapters](#merging-adapters)\n- [Best Practices](#best-practices)\n\n## Core Concepts\n\n### How LoRA Works\n\nInstead of updating all weights during fine-tuning, LoRA decomposes weight updates into low-rank matrices:\n\n```\nW' = W + BA\n```\n\nWhere:\n- `W` is the frozen pretrained weight matrix (d Ã— k)\n- `B` is a trainable matrix (d Ã— r)\n- `A` is a trainable matrix (r Ã— k)\n- `r` is the rank, much smaller than d and k\n\nThe key insight: weight updates during fine-tuning have low intrinsic rank, so we can represent them efficiently with smaller matrices.\n\n### Why Use LoRA\n\n| Aspect | Full Fine-tuning | LoRA |\n|--------|------------------|------|\n| Trainable params | 100% | ~0.1-1% |\n| Memory usage | High | Low |\n| Adapter size | Full model | ~3-100 MB |\n| Training speed | Slower | Faster |\n| Multiple tasks | Separate models | Swap adapters |\n\n## Basic Setup\n\n### Installation\n\n```bash\npip install peft transformers accelerate\n```\n\n### Minimal Example\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport torch\n\n# Load base model\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# trainable params: 3,407,872 || all params: 1,238,300,672 || trainable%: 0.28%\n```\n\n## Configuration Parameters\n\n### LoraConfig Options\n\n```python\nfrom peft import LoraConfig, TaskType\n\nconfig = LoraConfig(\n    # Core parameters\n    r=16,                          # Rank of update matrices\n    lora_alpha=32,                 # Scaling factor (alpha/r applied to updates)\n    target_modules=[\"q_proj\", \"v_proj\"],  # Layers to adapt\n\n    # Regularization\n    lora_dropout=0.05,             # Dropout on LoRA layers\n    bias=\"none\",                   # \"none\", \"all\", or \"lora_only\"\n\n    # Task configuration\n    task_type=TaskType.CAUSAL_LM,  # CAUSAL_LM, SEQ_CLS, SEQ_2_SEQ_LM, TOKEN_CLS\n\n    # Advanced\n    modules_to_save=None,          # Additional modules to train (e.g., [\"lm_head\"])\n    layers_to_transform=None,      # Specific layer indices to adapt\n    use_rslora=False,              # Rank-stabilized LoRA scaling\n    use_dora=False,                # Weight-Decomposed LoRA\n)\n```\n\n### Target Modules by Architecture\n\n```python\n# Llama, Mistral, Qwen\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n# GPT-2, GPT-J\ntarget_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n\n# BERT, RoBERTa\ntarget_modules = [\"query\", \"key\", \"value\", \"dense\"]\n\n# Falcon\ntarget_modules = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n\n# Phi\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"]\n```\n\n### Finding Target Modules\n\n```python\n# Print all linear layer names\nfrom peft.utils import get_peft_model_state_dict\n\ndef find_target_modules(model):\n    linear_modules = set()\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            # Get the last part of the name (e.g., \"q_proj\" from \"model.layers.0.self_attn.q_proj\")\n            layer_name = name.split(\".\")[-1]\n            linear_modules.add(layer_name)\n    return list(linear_modules)\n\nprint(find_target_modules(model))\n```\n\n## QLoRA (Quantized LoRA)\n\nQLoRA combines 4-bit quantization with LoRA, enabling fine-tuning of large models on consumer GPUs.\n\n### Setup\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",           # Normalized float 4-bit\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,       # Nested quantization\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n### Memory Requirements\n\n| Model Size | Full FT (16-bit) | LoRA (16-bit) | QLoRA (4-bit) |\n|------------|------------------|---------------|---------------|\n| 7B         | ~60 GB           | ~16 GB        | ~6 GB         |\n| 13B        | ~104 GB          | ~28 GB        | ~10 GB        |\n| 70B        | ~560 GB          | ~160 GB       | ~48 GB        |\n\n## Training Patterns\n\n### With Hugging Face Trainer\n\n```python\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\n\n# Prepare dataset\ndataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n\ndef format_prompt(example):\n    if example[\"input\"]:\n        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n    else:\n        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n    return {\"text\": text}\n\ndataset = dataset.map(format_prompt)\n\ndef tokenize(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=512,\n        padding=False,\n    )\n\ntokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n\n# Training arguments (note higher learning rate)\ntraining_args = TrainingArguments(\n    output_dir=\"./lora-output\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    learning_rate=2e-4,              # Higher than full fine-tuning\n    bf16=True,\n    logging_steps=10,\n    save_steps=500,\n    warmup_ratio=0.03,\n    gradient_checkpointing=True,\n    optim=\"adamw_torch_fused\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\ntrainer.train()\n```\n\n### With SFTTrainer (TRL)\n\n```python\nfrom trl import SFTTrainer, SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./sft-lora\",\n    max_seq_length=1024,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    bf16=True,\n    logging_steps=10,\n    gradient_checkpointing=True,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    peft_config=lora_config,      # Pass config directly, SFTTrainer applies it\n    dataset_text_field=\"text\",\n)\n\ntrainer.train()\n```\n\n### Classification Task\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2,\n)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS,\n    modules_to_save=[\"classifier\"],  # Train classification head fully\n)\n\nmodel = get_peft_model(model, lora_config)\n```\n\n## Saving and Loading\n\n### Save Adapter\n\n```python\n# Save only LoRA weights (small file)\nmodel.save_pretrained(\"./my-lora-adapter\")\ntokenizer.save_pretrained(\"./my-lora-adapter\")\n\n# Push to Hub\nmodel.push_to_hub(\"username/my-lora-adapter\")\n```\n\n### Load Adapter\n\n```python\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-1B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\n# Load adapter\nmodel = PeftModel.from_pretrained(base_model, \"./my-lora-adapter\")\n\n# For inference\nmodel.eval()\n```\n\n### Switch Between Adapters\n\n```python\n# Load multiple adapters\nmodel.load_adapter(\"./adapter-1\", adapter_name=\"task1\")\nmodel.load_adapter(\"./adapter-2\", adapter_name=\"task2\")\n\n# Switch active adapter\nmodel.set_adapter(\"task1\")\noutput = model.generate(**inputs)\n\nmodel.set_adapter(\"task2\")\noutput = model.generate(**inputs)\n\n# Disable adapter (use base model)\nwith model.disable_adapter():\n    output = model.generate(**inputs)\n```\n\n## Merging Adapters\n\nMerge LoRA weights into the base model for deployment without adapter overhead.\n\n```python\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-1B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"cpu\",  # Merge on CPU to avoid memory issues\n)\n\n# Load adapter\nmodel = PeftModel.from_pretrained(base_model, \"./my-lora-adapter\")\n\n# Merge and unload\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged-model\")\ntokenizer.save_pretrained(\"./merged-model\")\n\n# Push merged model to Hub\nmerged_model.push_to_hub(\"username/my-merged-model\")\n```\n\n## Best Practices\n\n1. **Start with r=16**: Scale up to 32 or 64 if the model underfits, down to 8 if overfitting or memory-constrained\n\n2. **Set lora_alpha = 2 Ã— r**: This is a common heuristic; the effective scaling is `alpha/r`\n\n3. **Target all attention and MLP layers**: For best results on LLMs, include gate/up/down projections:\n   ```python\n   target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n   ```\n\n4. **Use higher learning rate**: 2e-4 is typical for LoRA vs 2e-5 for full fine-tuning\n\n5. **Enable gradient checkpointing**: Reduces memory at cost of ~20% slower training:\n   ```python\n   model.gradient_checkpointing_enable()\n   ```\n\n6. **Use QLoRA for large models**: Essential for fine-tuning 7B+ models on consumer GPUs\n\n7. **Keep dropout low**: 0.05 is usually sufficient; higher values may hurt performance\n\n8. **Save checkpoints frequently**: LoRA adapters are small, so save often\n\n9. **Evaluate on base model too**: Ensure adapter doesn't degrade base capabilities\n\n10. **Consider modules_to_save for task heads**: For classification, train the classifier fully:\n    ```python\n    modules_to_save=[\"classifier\", \"score\"]\n    ```\n\n## References\n\nSee `reference/` for detailed documentation:\n- `advanced-techniques.md` - DoRA, rsLoRA, adapter composition, and debugging"
              },
              {
                "name": "mlx",
                "description": "Running and fine-tuning LLMs on Apple Silicon with MLX. Use when working with models locally on Mac, converting Hugging Face models to MLX format, fine-tuning with LoRA/QLoRA on Apple Silicon, or serving models via HTTP API.",
                "path": "skills/mlx/SKILL.md",
                "frontmatter": {
                  "name": "mlx",
                  "description": "Running and fine-tuning LLMs on Apple Silicon with MLX. Use when working with models locally on Mac, converting Hugging Face models to MLX format, fine-tuning with LoRA/QLoRA on Apple Silicon, or serving models via HTTP API."
                },
                "content": "# Using MLX for LLMs on Apple Silicon\n\nMLX-LM is a Python package for running large language models on Apple Silicon, leveraging the MLX framework for optimized performance with unified memory architecture.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Installation](#installation)\n- [Text Generation](#text-generation)\n- [Interactive Chat](#interactive-chat)\n- [Model Conversion](#model-conversion)\n- [Quantization](#quantization)\n- [Fine-tuning with LoRA](#fine-tuning-with-lora)\n- [Serving Models](#serving-models)\n- [Best Practices](#best-practices)\n- [References](#references)\n\n## Core Concepts\n\n### Why MLX\n\n| Aspect | PyTorch on Mac | MLX |\n|--------|----------------|-----|\n| Memory | Separate CPU/GPU copies | Unified memory, no copies |\n| Optimization | Generic Metal backend | Apple Silicon native |\n| Model loading | Slower, more memory | Lazy loading, efficient |\n| Quantization | Limited support | Built-in 4/8-bit |\n\nMLX arrays live in shared memory, accessible by both CPU and GPU without data transfer overhead.\n\n### Supported Models\n\nMLX-LM supports most popular architectures: Llama, Mistral, Qwen, Phi, Gemma, Cohere, and many more. Check the [mlx-community](https://huggingface.co/mlx-community) on Hugging Face for pre-converted models.\n\n## Installation\n\n```bash\npip install mlx-lm\n```\n\nRequires macOS 13.5+ and Apple Silicon (M1/M2/M3/M4).\n\n## Text Generation\n\n### Python API\n\n```python\nfrom mlx_lm import load, generate\n\n# Load model (from HF hub or local path)\nmodel, tokenizer = load(\"mlx-community/Llama-3.2-3B-Instruct-4bit\")\n\n# Generate text\nresponse = generate(\n    model,\n    tokenizer,\n    prompt=\"Explain quantum computing in simple terms:\",\n    max_tokens=256,\n    temp=0.7,\n)\nprint(response)\n```\n\n### Streaming Generation\n\n```python\nfrom mlx_lm import load, stream_generate\n\nmodel, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n\nprompt = \"Write a haiku about programming:\"\nfor response in stream_generate(model, tokenizer, prompt, max_tokens=100):\n    print(response.text, end=\"\", flush=True)\nprint()\n```\n\n### Batch Generation\n\n```python\nfrom mlx_lm import load, batch_generate\n\nmodel, tokenizer = load(\"mlx-community/Qwen2.5-7B-Instruct-4bit\")\n\nprompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks:\",\n    \"Define deep learning:\",\n]\n\nresponses = batch_generate(\n    model,\n    tokenizer,\n    prompts,\n    max_tokens=100,\n)\n\nfor prompt, response in zip(prompts, responses):\n    print(f\"Q: {prompt}\\nA: {response}\\n\")\n```\n\n### CLI Generation\n\n```bash\n# Basic generation\nmlx_lm.generate --model mlx-community/Llama-3.2-3B-Instruct-4bit \\\n    --prompt \"Explain recursion:\" \\\n    --max-tokens 256\n\n# With sampling parameters\nmlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \\\n    --prompt \"Write a poem about AI:\" \\\n    --temp 0.8 \\\n    --top-p 0.95\n```\n\n## Interactive Chat\n\n### CLI Chat\n\n```bash\n# Start chat REPL (context preserved between turns)\nmlx_lm.chat --model mlx-community/Llama-3.2-3B-Instruct-4bit\n```\n\n### Python Chat\n\n```python\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"mlx-community/Llama-3.2-3B-Instruct-4bit\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nresponse = generate(model, tokenizer, prompt=prompt, max_tokens=256)\nprint(response)\n```\n\n## Model Conversion\n\nConvert Hugging Face models to MLX format:\n\n### CLI Conversion\n\n```bash\n# Convert with 4-bit quantization\nmlx_lm.convert --hf-path meta-llama/Llama-3.2-3B-Instruct \\\n    -q  # Quantize to 4-bit\n\n# With specific quantization\nmlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 \\\n    -q \\\n    --q-bits 8 \\\n    --q-group-size 64\n\n# Upload to Hugging Face Hub\nmlx_lm.convert --hf-path meta-llama/Llama-3.2-1B-Instruct \\\n    -q \\\n    --upload-repo your-username/Llama-3.2-1B-Instruct-4bit-mlx\n```\n\n### Python Conversion\n\n```python\nfrom mlx_lm import convert\n\nconvert(\n    hf_path=\"meta-llama/Llama-3.2-3B-Instruct\",\n    mlx_path=\"./llama-3.2-3b-mlx\",\n    quantize=True,\n    q_bits=4,\n    q_group_size=64,\n)\n```\n\n### Conversion Options\n\n| Option | Default | Description |\n|--------|---------|-------------|\n| `--q-bits` | 4 | Quantization bits (4 or 8) |\n| `--q-group-size` | 64 | Group size for quantization |\n| `--dtype` | float16 | Data type for non-quantized weights |\n\n## Quantization\n\nMLX supports multiple quantization methods for different use cases:\n\n| Method | Best For | Command |\n|--------|----------|---------|\n| Basic | Quick conversion | `mlx_lm.convert -q` |\n| DWQ | Quality-preserving | `mlx_lm.dwq` |\n| AWQ | Activation-aware | `mlx_lm.awq` |\n| Dynamic | Per-layer precision | `mlx_lm.dynamic_quant` |\n| GPTQ | Established method | `mlx_lm.gptq` |\n\n### Quick Quantization\n\n```bash\n# 4-bit quantization during conversion\nmlx_lm.convert --hf-path mistralai/Mistral-7B-v0.3 -q\n\n# 8-bit for higher quality\nmlx_lm.convert --hf-path mistralai/Mistral-7B-v0.3 -q --q-bits 8\n```\n\nFor detailed coverage of each method, see `reference/quantization.md`.\n\n## Fine-tuning with LoRA\n\nMLX supports LoRA and QLoRA fine-tuning for efficient adaptation on Apple Silicon.\n\n### Quick Start\n\n```bash\n# Prepare training data (JSONL format)\n# {\"text\": \"Your training text here\"}\n# or\n# {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n\n# Fine-tune with LoRA\nmlx_lm.lora --model mlx-community/Llama-3.2-3B-Instruct-4bit \\\n    --train \\\n    --data ./data \\\n    --iters 1000\n\n# Generate with adapter\nmlx_lm.generate --model mlx-community/Llama-3.2-3B-Instruct-4bit \\\n    --adapter-path ./adapters \\\n    --prompt \"Your prompt here\"\n```\n\n### Fuse Adapter into Model\n\n```bash\n# Merge LoRA weights into base model\nmlx_lm.fuse --model mlx-community/Llama-3.2-3B-Instruct-4bit \\\n    --adapter-path ./adapters \\\n    --save-path ./fused-model\n\n# Or export to GGUF\nmlx_lm.fuse --model mlx-community/Llama-3.2-3B-Instruct-4bit \\\n    --adapter-path ./adapters \\\n    --export-gguf\n```\n\nFor detailed LoRA configuration and training patterns, see `reference/fine-tuning.md`.\n\n## Serving Models\n\n### OpenAI-Compatible Server\n\n```bash\n# Start server\nmlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8080\n\n# Use with OpenAI client\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"default\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"max_tokens\": 256\n    }'\n```\n\n### Python Client\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n\nresponse = client.chat.completions.create(\n    model=\"default\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain MLX in one sentence.\"}],\n    max_tokens=100,\n)\nprint(response.choices[0].message.content)\n```\n\n## Best Practices\n\n1. **Use pre-quantized models**: Download from `mlx-community` on Hugging Face for immediate use\n\n2. **Match quantization to your hardware**: M1/M2 with 8GB: use 4-bit; M2/M3 Pro/Max: 8-bit for quality\n\n3. **Leverage unified memory**: Unlike CUDA, MLX models can exceed \"GPU memory\" by using swap (slower but works)\n\n4. **Use streaming for UX**: `stream_generate` provides responsive output for interactive applications\n\n5. **Cache prompt prefixes**: Use `mlx_lm.cache_prompt` for repeated prompts with varying suffixes\n\n6. **Batch similar requests**: `batch_generate` is more efficient than sequential generation\n\n7. **Start with 4-bit quantization**: Good quality/size tradeoff; upgrade to 8-bit if quality issues\n\n8. **Fuse adapters for deployment**: After fine-tuning, fuse adapters for faster inference without loading separately\n\n9. **Monitor memory with Activity Monitor**: Watch memory pressure to avoid swap thrashing\n\n10. **Use chat templates**: Always apply `tokenizer.apply_chat_template()` for instruction-tuned models\n\n## References\n\nSee `reference/` for detailed documentation:\n- `quantization.md` - Detailed quantization methods and when to use each\n- `fine-tuning.md` - Complete LoRA/QLoRA training guide with data formats and configuration"
              },
              {
                "name": "prompt-engineering",
                "description": "Crafting effective prompts for LLMs. Use when designing prompts, improving output quality, structuring complex instructions, or debugging poor model responses.",
                "path": "skills/prompt-engineering/SKILL.md",
                "frontmatter": {
                  "name": "prompt-engineering",
                  "description": "Crafting effective prompts for LLMs. Use when designing prompts, improving output quality, structuring complex instructions, or debugging poor model responses."
                },
                "content": "# Prompt Engineering\n\nPrompt engineering is the practice of designing inputs that guide LLMs to produce desired outputs. Effective prompts reduce errors, improve consistency, and unlock model capabilities.\n\n## Table of Contents\n\n- [Core Principles](#core-principles)\n- [Be Clear and Direct](#be-clear-and-direct)\n- [Use Examples (Multishot)](#use-examples-multishot)\n- [Chain of Thought](#chain-of-thought)\n- [XML Tags](#xml-tags)\n- [Role Prompting](#role-prompting)\n- [Long Context](#long-context)\n- [Output Control](#output-control)\n- [Self-Verification](#self-verification)\n- [Best Practices](#best-practices)\n- [References](#references)\n\n## Core Principles\n\n**Golden rule**: Show your prompt to a colleague with minimal context. If they're confused, the model will be too.\n\n1. **Be explicit** - State exactly what you want; never assume the model knows your preferences\n2. **Provide context** - Include what the output is for, who the audience is, and what success looks like\n3. **Use structure** - Sequential steps, XML tags, and clear formatting reduce ambiguity\n4. **Show examples** - Demonstrations outperform descriptions for complex formats\n\n## Be Clear and Direct\n\nTreat the model as a capable but context-free collaborator. Specify:\n- What the task results will be used for\n- What audience the output is meant for\n- What a successful completion looks like\n\n### Vague vs Specific\n\n```\n# Vague\nAnalyze this data and give insights.\n\n# Specific\nAnalyze this Q2 sales data for our board presentation.\n1. Identify the top 3 revenue trends\n2. Flag any anomalies exceeding 15% variance\n3. Recommend 2-3 actionable next steps\nFormat as bullet points, max 200 words.\n```\n\n### Sequential Steps\n\nUse numbered lists for multi-step tasks:\n\n```\nYour task is to anonymize customer feedback.\n\nInstructions:\n1. Replace customer names with \"CUSTOMER_[ID]\"\n2. Replace emails with \"EMAIL_[ID]@example.com\"\n3. Redact phone numbers as \"PHONE_[ID]\"\n4. Leave product names intact\n5. Output only processed messages, separated by \"---\"\n```\n\n## Use Examples (Multishot)\n\nProvide 3-5 diverse examples to demonstrate expected behavior. Examples reduce misinterpretation and enforce consistent formatting.\n\n### Structure\n\n```\nCategorize customer feedback by issue type and sentiment.\n\n<examples>\n<example>\nInput: The dashboard loads slowly and the export button is hidden.\nCategory: UI/UX, Performance\nSentiment: Negative\nPriority: High\n</example>\n\n<example>\nInput: Love the Salesforce integration! Would be great to add Hubspot.\nCategory: Integration, Feature Request\nSentiment: Positive\nPriority: Medium\n</example>\n</examples>\n\nNow categorize: {{FEEDBACK}}\n```\n\n### Tips\n\n- Make examples **relevant** to actual use cases\n- Include **edge cases** and potential challenges\n- Vary examples to prevent unintended pattern matching\n- Wrap in `<example>` tags for clarity\n\n## Chain of Thought\n\nEncourage step-by-step reasoning for complex tasks. This improves accuracy in math, logic, analysis, and multi-factor decisions.\n\n### Basic\n\n```\nDetermine the best investment option for this client. Think step-by-step.\n```\n\n### Guided\n\nSpecify what steps to consider:\n\n```\nThink before answering:\n1. Consider the client's risk tolerance given their 5-year timeline\n2. Calculate potential returns for each option\n3. Factor in market volatility history\n4. Then provide your recommendation\n```\n\n### Structured (Recommended)\n\nSeparate reasoning from output with tags:\n\n```\nAnalyze this contract for legal risks.\n\nIn <thinking> tags, work through:\n- Indemnification implications\n- Liability exposure\n- IP ownership concerns\n\nThen provide your recommendation in <answer> tags.\n```\n\nThis makes reasoning visible for debugging and the answer extractable for post-processing.\n\n## XML Tags\n\nUse XML tags to separate prompt components. This prevents instruction/content confusion and improves parseability.\n\n### Common Tags\n\n```\n<instructions>Task steps and requirements</instructions>\n<context>Background information</context>\n<document>Source material to process</document>\n<example>Demonstration of expected behavior</example>\n<constraints>Boundaries and limitations</constraints>\n<output_format>Expected response structure</output_format>\n```\n\n### Nested Structure\n\n```\n<documents>\n  <document index=\"1\">\n    <source>annual_report_2023.pdf</source>\n    <content>{{REPORT_CONTENT}}</content>\n  </document>\n  <document index=\"2\">\n    <source>competitor_analysis.xlsx</source>\n    <content>{{ANALYSIS_CONTENT}}</content>\n  </document>\n</documents>\n\n<instructions>\nCompare revenue trends across both documents.\nIdentify strategic advantages mentioned in the annual report.\n</instructions>\n```\n\n### Reference Tags in Instructions\n\nBe explicit when referring to tagged content:\n\n```\nUsing the contract in <contract> tags, identify all clauses\nrelated to termination.\n```\n\n## Role Prompting\n\nSet expertise context via system prompts to improve domain-specific performance.\n\n### System Prompt Pattern\n\n```python\nsystem = \"You are a senior securities lawyer at a Fortune 500 company.\"\nuser = \"Review this acquisition agreement for regulatory risks.\"\n```\n\n### Effective Roles\n\n```\n# General\nYou are a [role] at [organization type].\n\n# Specific (better)\nYou are the General Counsel of a Fortune 500 tech company\nspecializing in M&A transactions.\n\n# With behavioral guidance (best)\nYou are a senior data scientist. You prioritize statistical\nrigor over speed. When uncertain, you state assumptions\nexplicitly and suggest validation approaches.\n```\n\n### When to Use\n\n- Complex analysis requiring domain expertise\n- Tasks where tone/style matters (legal, medical, executive)\n- When a specific perspective would improve output quality\n\n## Long Context\n\nFor prompts with large documents (20K+ tokens):\n\n### Document Placement\n\nPlace long documents **at the top**, before instructions:\n\n```\n<documents>\n{{LARGE_DOCUMENT_CONTENT}}\n</documents>\n\n<instructions>\nSummarize the key findings from the document above.\nFocus on financial implications.\n</instructions>\n```\n\n### Quote Grounding\n\nAsk the model to cite sources before analyzing:\n\n```\n<documents>\n{{PATIENT_RECORDS}}\n</documents>\n\nFirst, find and quote the relevant sections in <quotes> tags.\nThen provide your diagnosis in <analysis> tags, referencing\nthe quoted evidence.\n```\n\n### Multi-Document Metadata\n\nInclude source information for attribution:\n\n```\n<documents>\n  <document index=\"1\">\n    <source>quarterly_report_q2.pdf</source>\n    <date>2024-07-15</date>\n    <content>{{CONTENT}}</content>\n  </document>\n</documents>\n```\n\n## Output Control\n\n### Verbosity Specification\n\n```\n<output_format>\n- Default responses: 3-6 sentences or â‰¤5 bullets\n- Simple factual questions: â‰¤2 sentences\n- Complex analysis: 1 overview paragraph + â‰¤5 tagged bullets\n</output_format>\n```\n\n### Format Constraints\n\n```\nOutput requirements:\n- Use markdown tables for comparisons\n- Code blocks for any technical content\n- No introductory phrases (\"Here's...\", \"Sure...\")\n- End with exactly 3 action items\n```\n\n### Scope Boundaries\n\nPrevent drift from original intent:\n\n```\nImplement EXACTLY and ONLY what is requested.\n- Do not add features beyond the specification\n- Do not refactor surrounding code\n- Choose the simplest valid interpretation\n- Ask for clarification rather than assuming\n```\n\n## Self-Verification\n\nFor high-stakes outputs, include verification steps:\n\n```\n<verification>\nBefore finalizing your response:\n1. Re-read the original request\n2. Check that all requirements are addressed\n3. Verify any specific claims against provided documents\n4. Soften language where certainty is low\n5. Flag any assumptions you made\n</verification>\n```\n\n### Uncertainty Acknowledgment\n\n```\nWhen uncertain:\n- Explicitly state \"Based on the provided context...\"\n- Offer 2-3 plausible interpretations if ambiguous\n- Never fabricate specific details (dates, numbers, quotes)\n- Say \"I don't have enough information to...\" when applicable\n```\n\n## Best Practices\n\n1. **Start specific, then generalize** - Begin with detailed prompts; relax constraints only after validating output quality\n2. **Test with edge cases** - Include unusual inputs in your evaluation to catch failure modes\n3. **Iterate on examples** - When outputs miss the mark, add an example demonstrating the correct behavior\n4. **Separate instructions from content** - Use XML tags to prevent the model from confusing your instructions with input data\n5. **Put documents before queries** - For long context, place source material at the top of the prompt\n6. **Make reasoning visible** - Use `<thinking>` tags to debug why the model produces certain outputs\n7. **Constrain output format explicitly** - Specify structure, length, and style to reduce post-processing\n8. **Version your prompts** - Track changes to understand what modifications improved or degraded performance\n9. **Use system prompts for role, user prompts for task** - Keep role context stable; vary task instructions\n10. **Validate with fresh eyes** - Have someone unfamiliar with the task review your prompt for clarity\n\n## References\n\n- [Claude Prompt Engineering Guide](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n- [GPT-5 Prompting Guide](https://cookbook.openai.com/examples/gpt-5/gpt-5-2_prompting_guide)\n- [Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library)"
              },
              {
                "name": "pytorch",
                "description": "Building and training neural networks with PyTorch. Use when implementing deep learning models, training loops, data pipelines, model optimization with torch.compile, distributed training, or deploying PyTorch models.",
                "path": "skills/pytorch/SKILL.md",
                "frontmatter": {
                  "name": "pytorch",
                  "description": "Building and training neural networks with PyTorch. Use when implementing deep learning models, training loops, data pipelines, model optimization with torch.compile, distributed training, or deploying PyTorch models."
                },
                "content": "# Using PyTorch\n\nPyTorch is a deep learning framework with dynamic computation graphs, strong GPU acceleration, and Pythonic design. This skill covers practical patterns for building production-quality neural networks.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Model Architecture](#model-architecture)\n- [Training Loop](#training-loop)\n- [Data Loading](#data-loading)\n- [Performance Optimization](#performance-optimization)\n- [Distributed Training](#distributed-training)\n- [Saving and Loading](#saving-and-loading)\n\n## Core Concepts\n\n### Tensors\n\n```python\nimport torch\n\n# Create tensors\nx = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\nx = torch.zeros(3, 4)\nx = torch.randn(3, 4)  # Normal distribution\n\n# Device management\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx = x.to(device)\n\n# Operations (all return new tensors)\ny = x + 1\ny = x @ x.T  # Matrix multiplication\ny = x.view(2, 6)  # Reshape\n```\n\n### Autograd\n\n```python\n# Enable gradient tracking\nx = torch.randn(3, requires_grad=True)\ny = x ** 2\nloss = y.sum()\n\n# Compute gradients\nloss.backward()\nprint(x.grad)  # dy/dx\n\n# Disable gradients for inference\nwith torch.no_grad():\n    pred = model(x)\n\n# Or use inference mode (more efficient)\nwith torch.inference_mode():\n    pred = model(x)\n```\n\n## Model Architecture\n\n### nn.Module Pattern\n\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n```\n\n### Common Layers\n\n```python\n# Convolution\nnn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n# Normalization\nnn.BatchNorm2d(num_features)\nnn.LayerNorm(normalized_shape)\n\n# Attention\nnn.MultiheadAttention(embed_dim, num_heads)\n\n# Recurrent\nnn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\nnn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n```\n\n### Weight Initialization\n\n```python\ndef init_weights(module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.normal_(module.weight, std=0.02)\n\nmodel.apply(init_weights)\n```\n\n## Training Loop\n\n### Standard Pattern\n\n```python\nmodel = Model(input_dim, hidden_dim, output_dim).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n\n        # Optional: gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n    scheduler.step()\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            # ... validation logic\n```\n\n### Mixed Precision Training\n\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor batch in train_loader:\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    optimizer.zero_grad()\n\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n```\n\n### Gradient Accumulation\n\n```python\n# Requires setup from Mixed Precision Training above:\n# scaler = GradScaler(), model, criterion, optimizer, device\n\naccumulation_steps = 4\n\nfor i, batch in enumerate(train_loader):\n    inputs, targets = batch\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets) / accumulation_steps\n\n    scaler.scale(loss).backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n```\n\n## Data Loading\n\n### Dataset and DataLoader\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        x = self.data[idx]\n        if self.transform:\n            x = self.transform(x)\n        return x, self.labels[idx]\n\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,  # Faster GPU transfer\n    drop_last=True,   # Consistent batch sizes\n)\n```\n\n### Collate Functions\n\n```python\ndef collate_fn(batch):\n    \"\"\"Custom batching for variable-length sequences.\"\"\"\n    inputs, targets = zip(*batch)\n    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n    targets = torch.stack(targets)\n    return inputs, targets\n\nloader = DataLoader(dataset, collate_fn=collate_fn)\n```\n\n## Performance Optimization\n\n### torch.compile (PyTorch 2.0+)\n\n```python\n# Basic compilation\nmodel = torch.compile(model)\n\n# With options\nmodel = torch.compile(\n    model,\n    mode=\"reduce-overhead\",  # Options: default, reduce-overhead, max-autotune\n    fullgraph=True,          # Enforce no graph breaks\n)\n\n# Compile specific functions\n@torch.compile\ndef train_step(model, inputs, targets):\n    outputs = model(inputs)\n    return criterion(outputs, targets)\n```\n\n**Compilation modes:**\n- `default`: Good balance of compile time and speedup\n- `reduce-overhead`: Minimizes framework overhead, good for small models\n- `max-autotune`: Maximum performance, longer compile time\n\n### Memory Optimization\n\n```python\n# Activation checkpointing (trade compute for memory)\nfrom torch.utils.checkpoint import checkpoint\n\nclass Model(nn.Module):\n    def forward(self, x):\n        # Recompute activations during backward\n        x = checkpoint(self.expensive_layer, x, use_reentrant=False)\n        return self.output_layer(x)\n\n# Clear cache\ntorch.cuda.empty_cache()\n\n# Monitor memory\nprint(torch.cuda.memory_allocated() / 1e9, \"GB\")\nprint(torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n```\n\n## Distributed Training\n\n### DistributedDataParallel (DDP)\n\n```python\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef setup(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size):\n    setup(rank, world_size)\n\n    model = Model().to(rank)\n    model = DDP(model, device_ids=[rank])\n\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    loader = DataLoader(dataset, sampler=sampler)\n\n    for epoch in range(num_epochs):\n        sampler.set_epoch(epoch)  # Important for shuffling\n        # ... training loop\n\n    cleanup()\n\n# Launch with: torchrun --nproc_per_node=4 train.py\n```\n\n### FullyShardedDataParallel (FSDP)\n\n```python\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp import MixedPrecision\n\nmp_policy = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    reduce_dtype=torch.bfloat16,\n    buffer_dtype=torch.bfloat16,\n)\n\nmodel = FSDP(\n    model,\n    mixed_precision=mp_policy,\n    use_orig_params=True,  # Required for torch.compile compatibility\n)\n```\n\n## Saving and Loading\n\n### Checkpoints\n\n```python\n# Save\ntorch.save({\n    \"epoch\": epoch,\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n    \"loss\": loss,\n}, \"checkpoint.pt\")\n\n# Load\ncheckpoint = torch.load(\"checkpoint.pt\", map_location=device)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n```\n\n### Export for Deployment\n\n```python\n# TorchScript\nscripted = torch.jit.script(model)\nscripted.save(\"model.pt\")\n\n# ONNX\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model.onnx\",\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n)\n```\n\n## Best Practices\n\n1. **Always set model mode**: Use `model.train()` and `model.eval()` appropriately\n2. **Use inference_mode over no_grad**: More efficient for inference\n3. **Pin memory for GPU training**: Set `pin_memory=True` in DataLoader\n4. **Profile before optimizing**: Use `torch.profiler` to find bottlenecks\n5. **Prefer bfloat16 over float16**: Better numerical stability on modern GPUs\n6. **Use torch.compile**: Significant speedups with minimal code changes\n7. **Set deterministic mode for reproducibility**:\n   ```python\n   torch.manual_seed(42)\n   torch.backends.cudnn.deterministic = True\n   torch.backends.cudnn.benchmark = False\n   ```\n\n## References\n\nSee `reference/` for detailed documentation:\n- `training-patterns.md` - Advanced training techniques\n- `debugging.md` - Debugging and profiling tools"
              },
              {
                "name": "qlora",
                "description": "Memory-efficient fine-tuning with 4-bit quantization and LoRA adapters. Use when fine-tuning large models (7B+) on consumer GPUs, when VRAM is limited, or when standard LoRA still exceeds memory. Builds on the lora skill.",
                "path": "skills/qlora/SKILL.md",
                "frontmatter": {
                  "name": "qlora",
                  "description": "Memory-efficient fine-tuning with 4-bit quantization and LoRA adapters. Use when fine-tuning large models (7B+) on consumer GPUs, when VRAM is limited, or when standard LoRA still exceeds memory. Builds on the lora skill."
                },
                "content": "# QLoRA: Quantized Low-Rank Adaptation\n\nQLoRA enables fine-tuning of large language models on consumer GPUs by combining 4-bit quantization with LoRA adapters. A 65B model can be fine-tuned on a single 48GB GPU while matching 16-bit fine-tuning performance.\n\n> **Prerequisites**: This skill assumes familiarity with LoRA. See the `lora` skill for LoRA fundamentals (LoraConfig, target_modules, training patterns).\n\n## Table of Contents\n\n- [Core Innovations](#core-innovations)\n- [BitsAndBytesConfig Deep Dive](#bitsandbytesconfig-deep-dive)\n- [Memory Requirements](#memory-requirements)\n- [Complete Training Example](#complete-training-example)\n- [Inference and Merging](#inference-and-merging)\n- [Troubleshooting](#troubleshooting)\n- [Best Practices](#best-practices)\n\n## Core Innovations\n\nQLoRA introduces three techniques that reduce memory usage without sacrificing performance:\n\n### 4-bit NormalFloat (NF4)\n\nNF4 is an information-theoretically optimal quantization data type for normally distributed weights. Neural network weights are typically normally distributed, making NF4 more efficient than standard 4-bit floats.\n\n```\nStorage: 4-bit NF4 (quantized weights)\nCompute: 16-bit BF16 (dequantized for forward/backward pass)\n```\n\nThe key insight: weights are stored in 4-bit but dequantized to bf16 for computation. Only the frozen base model is quantized; LoRA adapters remain in full precision.\n\n**NF4 vs FP4:**\n\n| Quantization | Description | Use Case |\n|--------------|-------------|----------|\n| `nf4` | Normalized Float 4-bit, optimal for normal distributions | Default, recommended |\n| `fp4` | Standard 4-bit float | Legacy, rarely needed |\n\n### Double Quantization\n\nStandard quantization requires storing scaling constants (typically fp32) for each quantization block. Double quantization quantizes these constants too:\n\n```\nFirst quantization:  weights â†’ 4-bit + fp32 scaling constants\nDouble quantization: scaling constants â†’ 8-bit + fp32 second-level constants\n```\n\nThis saves approximately **0.37 bits per parameter**â€”significant for billion-parameter models:\n- 7B model: ~325 MB savings\n- 70B model: ~3.2 GB savings\n\n### Paged Optimizers\n\nDuring training, gradient checkpointing can cause memory spikes when processing long sequences. Paged optimizers use NVIDIA unified memory to automatically transfer optimizer states between GPU and CPU:\n\n```\nNormal training: OOM on memory spike\nPaged optimizers: GPU â†” CPU transfer handles spikes gracefully\n```\n\nThis is handled automatically by bitsandbytes when using 4-bit training.\n\n## BitsAndBytesConfig Deep Dive\n\n### All Parameters Explained\n\n```python\nfrom transformers import BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    # Core 4-bit settings\n    load_in_4bit=True,              # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",      # \"nf4\" (recommended) or \"fp4\"\n\n    # Double quantization\n    bnb_4bit_use_double_quant=True, # Quantize the quantization constants\n\n    # Compute precision\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Dequantize to this dtype for compute\n\n    # Optional: specific storage type (usually auto-detected)\n    bnb_4bit_quant_storage=torch.uint8,     # Storage dtype for quantized weights\n)\n```\n\n### Compute Dtype Selection\n\n| Dtype | Hardware | Notes |\n|-------|----------|-------|\n| `torch.bfloat16` | Ampere+ (RTX 30xx, A100) | Recommended, faster |\n| `torch.float16` | Older GPUs (V100, RTX 20xx) | Use if bf16 not supported |\n| `torch.float32` | Any | Slower, only for debugging |\n\nCheck bf16 support:\n```python\nimport torch\nprint(torch.cuda.is_bf16_supported())  # True on Ampere+\n```\n\n### Comparison: Quantization Options\n\n```python\n# Recommended: NF4 + double quant + bf16\noptimal_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Maximum memory savings (slightly slower)\nmax_savings_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,  # fp16 uses less memory than bf16\n)\n\n# 8-bit alternative (less compression, sometimes more stable)\neight_bit_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n```\n\n## Memory Requirements\n\n| Model Size | Full Fine-tuning | LoRA (16-bit) | QLoRA (4-bit) |\n|------------|------------------|---------------|---------------|\n| 7B         | ~60 GB           | ~16 GB        | ~6 GB         |\n| 13B        | ~104 GB          | ~28 GB        | ~10 GB        |\n| 34B        | ~272 GB          | ~75 GB        | ~20 GB        |\n| 70B        | ~560 GB          | ~160 GB       | ~48 GB        |\n\n**Notes:**\n- QLoRA memory includes model + optimizer states + activations\n- Actual usage varies with batch size, sequence length, and gradient checkpointing\n- Add ~20% buffer for safe operation\n\n### GPU Recommendations\n\n| GPU VRAM | Max Model Size (QLoRA) |\n|----------|------------------------|\n| 8 GB     | 7B (tight)             |\n| 16 GB    | 7-13B                  |\n| 24 GB    | 13-34B                 |\n| 48 GB    | 34-70B                 |\n| 80 GB    | 70B+ comfortably       |\n\n## Complete Training Example\n\n```python\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nimport torch\n\n# 1. Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# 2. Load quantized model\nmodel_name = \"meta-llama/Llama-3.1-8B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",  # Optional: faster attention\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# 3. Prepare for k-bit training (critical step!)\nmodel = prepare_model_for_kbit_training(model)\n\n# 4. LoRA config (see lora skill for parameter details)\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 5. Dataset\ndataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")\n\ndef format_example(example):\n    if example[\"input\"]:\n        return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"}\n    return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n\ndataset = dataset.map(format_example)\n\n# 6. Training\nsft_config = SFTConfig(\n    output_dir=\"./qlora-output\",\n    max_seq_length=512,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    bf16=True,\n    logging_steps=10,\n    save_steps=100,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    optim=\"paged_adamw_8bit\",  # Paged optimizer for memory efficiency\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    dataset_text_field=\"text\",\n)\n\ntrainer.train()\n\n# 7. Save adapter\nmodel.save_pretrained(\"./qlora-adapter\")\ntokenizer.save_pretrained(\"./qlora-adapter\")\n```\n\n## Inference and Merging\n\n### Inference with Quantized Model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-8B\"\n\n# Load quantized base model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load adapter\nmodel = PeftModel.from_pretrained(base_model, \"./qlora-adapter\")\nmodel.eval()\n\n# Generate\ninputs = tokenizer(\"### Instruction:\\nExplain quantum computing.\\n\\n### Response:\\n\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n### Merging to Full Precision\n\nTo merge QLoRA adapters into a full-precision model (for deployment without bitsandbytes):\n\n```python\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\n# Load base model in full precision (on CPU to avoid OOM)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"cpu\",\n)\n\n# Load adapter\nmodel = PeftModel.from_pretrained(base_model, \"./qlora-adapter\")\n\n# Merge and unload\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged-model\")\n```\n\n**Note**: Merging requires enough RAM to hold the full-precision model. For 70B models, this means ~140GB RAM.\n\n## Troubleshooting\n\n### CUDA Version Issues\n\n```bash\n# Check CUDA version\nnvcc --version\npython -c \"import torch; print(torch.version.cuda)\"\n\n# bitsandbytes requires CUDA 11.7+\n# If version mismatch, reinstall:\npip uninstall bitsandbytes\npip install bitsandbytes --upgrade\n```\n\n### \"cannot find libcudart\" or Missing Library Errors\n\n```bash\n# Find CUDA installation\nfind /usr -name \"libcudart*\" 2>/dev/null\n\n# Set environment variable\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Or for conda:\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n```\n\n### Slow Training\n\nCommon cause: compute dtype mismatch\n\n```python\n# Check if model is using expected dtype\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"{name}: {param.dtype}\")\n        break  # All LoRA params should match\n\n# Ensure bf16 is used in training args if BitsAndBytesConfig uses bf16\n# Mismatch causes constant dtype conversions\n```\n\n### Out of Memory\n\n```python\n# 1. Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# 2. Reduce batch size, increase accumulation\nper_device_train_batch_size = 1\ngradient_accumulation_steps = 16\n\n# 3. Use paged optimizer\noptim = \"paged_adamw_8bit\"\n\n# 4. Reduce sequence length\nmax_seq_length = 256\n\n# 5. Target fewer modules\ntarget_modules = [\"q_proj\", \"v_proj\"]  # Minimal set\n```\n\n### Model Loads But Training Fails\n\n```python\n# Ensure prepare_model_for_kbit_training is called\nfrom peft import prepare_model_for_kbit_training\nmodel = prepare_model_for_kbit_training(model)  # Don't skip this!\n\n# Enable input gradients if needed\nmodel.enable_input_require_grads()\n```\n\n## Best Practices\n\n1. **Always use `prepare_model_for_kbit_training`**: This enables gradient computation through the frozen quantized layers\n\n2. **Match compute dtype with training precision**: If `bnb_4bit_compute_dtype=torch.bfloat16`, use `bf16=True` in training args\n\n3. **Use paged optimizers for large models**: `optim=\"paged_adamw_8bit\"` or `\"paged_adamw_32bit\"` handles memory spikes\n\n4. **Start with NF4 + double quantization**: This is the recommended default; only change if debugging\n\n5. **Gradient checkpointing is essential**: Always enable for QLoRA training to fit larger batch sizes\n\n6. **Test inference before long training runs**: Load the model and generate a few tokens to catch configuration issues early\n\n7. **Monitor GPU memory**: Use `nvidia-smi` or `torch.cuda.memory_summary()` to track actual usage\n\n8. **Consider 8-bit for unstable training**: If 4-bit training shows instability, try `load_in_8bit=True` as a middle ground"
              },
              {
                "name": "rlhf",
                "description": "Understanding Reinforcement Learning from Human Feedback (RLHF) for aligning language models. Use when learning about preference data, reward modeling, policy optimization, or direct alignment algorithms like DPO.",
                "path": "skills/rlhf/SKILL.md",
                "frontmatter": {
                  "name": "rlhf",
                  "description": "Understanding Reinforcement Learning from Human Feedback (RLHF) for aligning language models. Use when learning about preference data, reward modeling, policy optimization, or direct alignment algorithms like DPO."
                },
                "content": "# Understanding RLHF\n\nReinforcement Learning from Human Feedback (RLHF) is a technique for aligning language models with human preferences. Rather than relying solely on next-token prediction, RLHF uses human judgment to guide model behavior toward helpful, harmless, and honest outputs.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [The RLHF Pipeline](#the-rlhf-pipeline)\n- [Preference Data](#preference-data)\n- [Instruction Tuning](#instruction-tuning)\n- [Reward Modeling](#reward-modeling)\n- [Policy Optimization](#policy-optimization)\n- [Direct Alignment Algorithms](#direct-alignment-algorithms)\n- [Challenges](#challenges)\n- [Best Practices](#best-practices)\n- [References](#references)\n\n## Core Concepts\n\n### Why RLHF?\n\nPretraining produces models that predict likely text, not necessarily *good* text. A model trained on internet data learns to complete text in ways that reflect its training distributionâ€”including toxic, unhelpful, or dishonest patterns. RLHF addresses this gap by optimizing for human preferences rather than likelihood.\n\nThe core insight: humans can often recognize good outputs more easily than they can specify what makes an output good. RLHF exploits this by collecting human judgments and using them to shape model behavior.\n\n### The Alignment Problem\n\nLanguage models face several alignment challenges:\n\n- **Helpfulness**: Following instructions and providing useful information\n- **Harmlessness**: Avoiding toxic, dangerous, or inappropriate outputs\n- **Honesty**: Acknowledging uncertainty and avoiding fabrication\n- **Intent alignment**: Understanding what users actually want, not just what they say\n\nRLHF provides a framework for encoding these properties through preference data.\n\n### Key Components\n\n1. **Preference data**: Human judgments comparing model outputs\n2. **Reward model**: A learned function approximating human preferences\n3. **Policy optimization**: RL algorithms that maximize expected reward\n4. **Regularization**: Constraints preventing deviation from the base model\n\n## The RLHF Pipeline\n\nThe standard RLHF pipeline consists of three main stages:\n\n### Stage 1: Supervised Fine-Tuning (SFT)\n\nStart with a pretrained language model and fine-tune it on high-quality demonstrations. This teaches the model the desired format and style of responses.\n\n**Input**: Pretrained model + demonstration dataset\n**Output**: SFT model that can follow instructions\n\n### Stage 2: Reward Model Training\n\nTrain a model to predict human preferences between pairs of outputs. The reward model learns to score outputs in a way that correlates with human judgment.\n\n**Input**: SFT model + preference dataset (chosen/rejected pairs)\n**Output**: Reward model that scores any output\n\n### Stage 3: Policy Optimization\n\nUse reinforcement learning to optimize the SFT model against the reward model, while staying close to the original SFT distribution.\n\n**Input**: SFT model + reward model\n**Output**: Final aligned model\n\n### Alternative: Direct Alignment\n\nDirect alignment algorithms (DPO, IPO, KTO) skip the reward model entirely, optimizing directly from preference data. This simplifies the pipeline but trades off some flexibility.\n\n## Preference Data\n\nPreference data encodes human judgment about model outputs. The most common format is pairwise comparisons.\n\n### Pairwise Preferences\n\nGiven a prompt, collect two or more model outputs and have humans indicate which is better:\n\n```\nPrompt: \"Explain quantum entanglement\"\n\nResponse A: [technical explanation]\nResponse B: [simpler explanation with analogy]\n\nHuman preference: B > A\n```\n\nThis creates (prompt, chosen, rejected) tuples for training.\n\n### Collection Methods\n\n**Human annotation**: Trained annotators compare outputs according to guidelines. Most reliable but expensive and slow.\n\n**AI feedback**: Use a capable model to generate preferences. Faster and cheaper but may propagate biases. This is the basis for Constitutional AI (CAI) and RLAIF.\n\n**Implicit signals**: User interactions like upvotes, regeneration requests, or conversation length. Noisy but abundant.\n\n### Data Quality Considerations\n\n- **Annotator agreement**: Low agreement suggests ambiguous criteria or subjective preferences\n- **Distribution coverage**: Preferences should cover the range of model behaviors\n- **Prompt diversity**: Diverse prompts prevent overfitting to narrow scenarios\n- **Preference strength**: Some comparisons are clear; others are nearly ties\n\n## Instruction Tuning\n\nInstruction tuning (supervised fine-tuning on instruction-response pairs) serves as the foundation for RLHF.\n\n### Purpose\n\n- Teaches the model to follow instructions rather than just complete text\n- Establishes the format and style for responses\n- Creates a starting point that already exhibits desired behaviors\n- Provides the reference policy for KL regularization\n\n### Dataset Composition\n\nTypical instruction tuning datasets include:\n\n- **Single-turn QA**: Questions with direct answers\n- **Multi-turn dialogue**: Conversational exchanges\n- **Task instructions**: Specific tasks with examples\n- **Chain-of-thought**: Reasoning traces for complex problems\n\n### Relationship to RLHF\n\nThe SFT model defines the \"prior\" that RLHF refines. A better SFT model means:\n\n- The reward model has better starting outputs to compare\n- Policy optimization has less work to do\n- The KL penalty keeps the final model closer to this baseline\n\n## Reward Modeling\n\nThe reward model transforms pairwise preferences into a scalar signal for RL optimization.\n\n### The Bradley-Terry Model\n\nPreferences are modeled using the Bradley-Terry framework:\n\n```\nP(A > B) = sigmoid(r(A) - r(B))\n```\n\nWhere r(x) is the reward for output x. This assumes preferences depend only on the difference in rewards.\n\nThe loss function is:\n\n```\nL = -log(sigmoid(r(chosen) - r(rejected)))\n```\n\nThis pushes the reward model to assign higher scores to chosen outputs.\n\n### Architecture\n\nReward models are typically:\n\n- The SFT model with a scalar head instead of the language modeling head\n- Trained on (prompt, chosen, rejected) tuples\n- Output a single scalar reward for any (prompt, response) pair\n\n### Considerations\n\n- **Scaling**: Larger reward models generally produce better signals\n- **Calibration**: Absolute reward values are less important than rankings\n- **Generalization**: The model must score outputs it hasn't seen during training\n- **Over-optimization**: Policies can exploit reward model weaknesses\n\nSee `reference/reward-modeling.md` for detailed training procedures.\n\n## Policy Optimization\n\nPolicy optimization uses RL to maximize expected reward while staying close to the reference policy.\n\n### The RLHF Objective\n\n```\nmaximize E[R(x, y)] - Î² * KL(Ï€ || Ï€_ref)\n```\n\nWhere:\n- R(x, y) is the reward for response y to prompt x\n- KL(Ï€ || Ï€_ref) measures deviation from the reference policy\n- Î² controls the strength of the regularization\n\n### PPO (Proximal Policy Optimization)\n\nPPO is the most common algorithm for RLHF:\n\n1. Sample responses from the current policy\n2. Score responses with the reward model\n3. Compute advantage estimates\n4. Update policy with clipped surrogate objective\n\nThe clipping prevents large policy updates that could destabilize training.\n\n### KL Regularization\n\nThe KL penalty serves multiple purposes:\n\n- **Prevents reward hacking**: Stops the policy from finding adversarial inputs to the reward model\n- **Maintains capabilities**: Keeps the model close to the pretrained distribution\n- **Stabilizes training**: Limits how far the policy can move per update\n\nHigher Î² means more conservative updates; lower Î² allows more aggressive optimization.\n\n### REINFORCE vs PPO\n\nREINFORCE is simpler but has higher variance:\n\n- Uses raw returns without value function baseline\n- Single-sample gradient estimates\n- Can work for simpler problems\n\nPPO adds complexity but improves stability:\n\n- Clipped surrogate objective\n- Multiple epochs per batch\n- Better sample efficiency\n\nSee `reference/policy-optimization.md` for algorithm details.\n\n## Direct Alignment Algorithms\n\nDirect alignment methods optimize the RLHF objective without training a separate reward model.\n\n### DPO (Direct Preference Optimization)\n\nDPO reparameterizes the RLHF objective to derive a closed-form loss:\n\n```\nL = -log sigmoid(Î² * (log Ï€(y_w|x)/Ï€_ref(y_w|x) - log Ï€(y_l|x)/Ï€_ref(y_l|x)))\n```\n\nWhere y_w is the preferred response and y_l is the dispreferred response.\n\n**Advantages**:\n- No separate reward model training\n- Simpler pipeline with fewer hyperparameters\n- More stable than RL-based methods\n\n**Trade-offs**:\n- Less flexible than explicit reward models\n- Cannot reuse reward model for other purposes\n- May be more sensitive to data quality\n\n### IPO (Identity Preference Optimization)\n\nIPO addresses potential overfitting in DPO by using a different loss formulation that doesn't assume the Bradley-Terry model perfectly describes preferences.\n\n### KTO (Kahneman-Tversky Optimization)\n\nKTO works with binary feedback (good/bad) rather than pairwise comparisons, making data collection easier. It's based on prospect theory from behavioral economics.\n\n### When to Use Direct Alignment\n\nDirect alignment is preferred when:\n- Simplicity is important\n- Computational resources are limited\n- The reward model won't be reused\n\nReward-based RLHF is preferred when:\n- You need the reward model for other purposes (filtering, ranking)\n- You have a large preference dataset\n- You want maximum flexibility in optimization\n\nSee `reference/direct-alignment.md` for detailed algorithm comparisons.\n\n## Challenges\n\n### Over-Optimization\n\nAs optimization proceeds, the policy may exploit weaknesses in the reward model rather than improving on the true objective. Symptoms include:\n\n- Rising reward model scores but declining human evaluation\n- Increasingly verbose or formulaic outputs\n- Sycophantic behavior (agreeing with users regardless of correctness)\n\nMitigations:\n- Stronger KL regularization\n- Reward model ensembles\n- Early stopping based on held-out evaluation\n\n### Reward Hacking\n\nThe policy finds inputs that score highly with the reward model but don't represent genuine improvement:\n\n- Length exploitation (longer responses score higher)\n- Style mimicry (copying patterns from high-reward examples)\n- Adversarial outputs that confuse the reward model\n\n### Evaluation\n\nEvaluating aligned models is difficult because:\n\n- Human preferences are subjective and context-dependent\n- Automated metrics don't capture alignment properties well\n- A/B testing is expensive and slow\n- Models may perform differently on evaluation vs deployment\n\n### Distribution Shift\n\nThe preference data comes from a specific distribution of prompts and responses. The deployed model will encounter different inputs, and the reward model may not generalize well.\n\n## Best Practices\n\n1. **Start with a strong SFT model**: RLHF refines behavior; it works best when the base model already exhibits desired patterns\n2. **Invest in preference data quality**: Garbage in, garbage outâ€”clear guidelines and trained annotators matter\n3. **Use KL regularization**: Don't optimize reward too aggressively; the reward model is an imperfect proxy\n4. **Monitor for reward hacking**: Track human evaluations alongside reward model scores\n5. **Consider direct alignment first**: DPO is simpler and often performs comparably to PPO\n6. **Iterate on reward model**: Improve the reward model as you discover its weaknesses\n7. **Diverse prompts**: Ensure preference data covers the distribution you care about\n8. **Regularize appropriately**: Higher Î² for safety-critical applications; lower Î² for capability-focused training\n\n## References\n\n### Reference Files\n\n- `reference/reward-modeling.md` - Detailed reward model training procedures\n- `reference/policy-optimization.md` - PPO and policy gradient algorithms for RLHF\n- `reference/direct-alignment.md` - DPO, IPO, KTO and other direct methods\n\n### External Resources\n\n- [RLHF Book by Nathan Lambert](https://rlhfbook.com/) - Comprehensive textbook on RLHF\n- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) - InstructGPT paper\n- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) - DPO paper"
              },
              {
                "name": "transformers",
                "description": "Loading and using pretrained models with Hugging Face Transformers. Use when working with pretrained models from the Hub, running inference with Pipeline API, fine-tuning models with Trainer, or handling text, vision, audio, and multimodal tasks.",
                "path": "skills/transformers/SKILL.md",
                "frontmatter": {
                  "name": "transformers",
                  "description": "Loading and using pretrained models with Hugging Face Transformers. Use when working with pretrained models from the Hub, running inference with Pipeline API, fine-tuning models with Trainer, or handling text, vision, audio, and multimodal tasks."
                },
                "content": "# Using Hugging Face Transformers\n\nTransformers is the model-definition framework for state-of-the-art machine learning across text, vision, audio, and multimodal domains. It provides unified APIs for loading pretrained models, running inference, and fine-tuning.\n\n## Table of Contents\n\n- [Core Concepts](#core-concepts)\n- [Pipeline API](#pipeline-api)\n- [Model Loading](#model-loading)\n- [Inference Patterns](#inference-patterns)\n- [Fine-tuning with Trainer](#fine-tuning-with-trainer)\n- [Working with Modalities](#working-with-modalities)\n- [Memory and Performance](#memory-and-performance)\n- [Best Practices](#best-practices)\n\n## Core Concepts\n\n### The Three Core Classes\n\nEvery model in Transformers has three core components:\n\n```python\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n# Configuration: hyperparameters and architecture settings\nconfig = AutoConfig.from_pretrained(\"bert-base-uncased\")\n\n# Model: the neural network weights\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n# Tokenizer/Processor: converts inputs to tensors\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```\n\n### The `from_pretrained` Pattern\n\nAll loading uses `from_pretrained()` which handles downloading, caching, and device placement:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",  # Automatic device placement\n)\n```\n\n### Auto Classes\n\nUse task-specific Auto classes for the correct model head:\n\n```python\nfrom transformers import (\n    AutoModelForCausalLM,          # Text generation (GPT, Llama)\n    AutoModelForSeq2SeqLM,         # Encoder-decoder (T5, BART)\n    AutoModelForSequenceClassification,  # Classification\n    AutoModelForTokenClassification,     # NER, POS tagging\n    AutoModelForQuestionAnswering,       # Extractive QA\n    AutoModelForMaskedLM,                # BERT-style masked LM\n    AutoModelForImageClassification,     # Vision models\n    AutoModelForSpeechSeq2Seq,           # Speech recognition\n)\n```\n\n## Pipeline API\n\nThe `pipeline()` function provides high-level inference with minimal code:\n\n### Text Tasks\n\n```python\nfrom transformers import pipeline\n\n# Text generation\ngenerator = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\noutput = generator(\"The secret to success is\", max_new_tokens=50)\n\n# Text classification\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"I love this product!\")\n# [{'label': 'POSITIVE', 'score': 0.9998}]\n\n# Named entity recognition\nner = pipeline(\"ner\", aggregation_strategy=\"simple\")\nentities = ner(\"Hugging Face is based in New York City.\")\n\n# Question answering\nqa = pipeline(\"question-answering\")\nanswer = qa(question=\"What is the capital?\", context=\"Paris is the capital of France.\")\n\n# Summarization\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\nsummary = summarizer(long_text, max_length=130, min_length=30)\n\n# Translation\ntranslator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\nresult = translator(\"Hello, how are you?\")\n```\n\n### Chat/Conversational\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"},\n]\n\nresponse = pipe(messages, max_new_tokens=256)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\n### Vision Tasks\n\n```python\n# Image classification\nclassifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\nresult = classifier(\"path/to/image.jpg\")\n\n# Object detection\ndetector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\nobjects = detector(\"path/to/image.jpg\")\n\n# Image segmentation\nsegmenter = pipeline(\"image-segmentation\", model=\"facebook/mask2former-swin-base-coco-panoptic\")\nmasks = segmenter(\"path/to/image.jpg\")\n```\n\n### Audio Tasks\n\n```python\n# Speech recognition\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\ntext = transcriber(\"path/to/audio.mp3\")\n\n# Audio classification\nclassifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-ks\")\nresult = classifier(\"path/to/audio.wav\")\n```\n\n### Multimodal Tasks\n\n```python\n# Visual question answering\nvqa = pipeline(\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\nanswer = vqa(image=\"image.jpg\", question=\"What color is the car?\")\n\n# Image-to-text (captioning)\ncaptioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\ncaption = captioner(\"image.jpg\")\n\n# Document question answering\ndoc_qa = pipeline(\"document-question-answering\", model=\"impira/layoutlm-document-qa\")\nanswer = doc_qa(image=\"document.png\", question=\"What is the total?\")\n```\n\n## Model Loading\n\n### Device Placement\n\n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Automatic placement across available devices\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\n# Specific device\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    device_map=\"cuda:0\",\n)\n\n# Custom device map for model parallelism\ndevice_map = {\n    \"model.embed_tokens\": 0,\n    \"model.layers.0\": 0,\n    \"model.layers.1\": 1,\n    \"model.norm\": 1,\n    \"lm_head\": 1,\n}\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n```\n\n### Loading from Local Path\n\n```python\n# Save model locally\nmodel.save_pretrained(\"./my_model\")\ntokenizer.save_pretrained(\"./my_model\")\n\n# Load from local path\nmodel = AutoModelForCausalLM.from_pretrained(\"./my_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./my_model\")\n```\n\n### Trust Remote Code\n\nSome models require executing custom code from the Hub:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-2\",\n    trust_remote_code=True,  # Required for custom architectures\n)\n```\n\n## Inference Patterns\n\n### Text Generation\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\n# Basic generation\ninputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=100)\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# With generation config\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=50,\n    repetition_penalty=1.1,\n)\n```\n\n### Chat Templates\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n]\n\n# Apply chat template\ninput_text = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=100)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n```\n\n### Getting Embeddings\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\ndef get_embeddings(texts: list[str]) -> torch.Tensor:\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Mean pooling\n    attention_mask = inputs[\"attention_mask\"]\n    embeddings = outputs.last_hidden_state\n    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n    sum_embeddings = (embeddings * mask_expanded).sum(1)\n    sum_mask = mask_expanded.sum(1).clamp(min=1e-9)\n    return sum_embeddings / sum_mask\n\nembeddings = get_embeddings([\"Hello world\", \"How are you?\"])\n```\n\n### Classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"I love this movie!\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.softmax(outputs.logits, dim=-1)\n\nlabels = model.config.id2label\nfor idx, prob in enumerate(predictions[0]):\n    print(f\"{labels[idx]}: {prob:.4f}\")\n```\n\n## Fine-tuning with Trainer\n\n### Basic Fine-tuning\n\n```python\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import load_dataset\n\n# Load data and model\ndataset = load_dataset(\"imdb\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n)\n\n# Tokenize dataset\ndef tokenize(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized = dataset.map(tokenize, batched=True)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\n# Train\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n)\n\ntrainer.train()\n```\n\n### Pushing to Hub\n\n```python\n# Login first: huggingface-cli login\n\n# Push model and tokenizer\nmodel.push_to_hub(\"my-username/my-fine-tuned-model\")\ntokenizer.push_to_hub(\"my-username/my-fine-tuned-model\")\n\n# Or use trainer\ntrainer.push_to_hub()\n```\n\nSee `reference/fine-tuning.md` for advanced patterns including LoRA, custom data collators, and evaluation metrics.\n\n## Working with Modalities\n\n### Vision Models\n\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\n\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n\nimage = Image.open(\"image.jpg\")\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_class = outputs.logits.argmax(-1).item()\n    print(model.config.id2label[predicted_class])\n```\n\n### Audio Models\n\n```python\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    \"openai/whisper-large-v3\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Load audio (use librosa, soundfile, or datasets)\nimport librosa\naudio, sr = librosa.load(\"audio.mp3\", sr=16000)\n\ninputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n\ngenerated_ids = model.generate(**inputs)\ntranscription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Vision-Language Models\n\n```python\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom PIL import Image\nimport torch\n\nmodel_name = \"llava-hf/llava-1.5-7b-hf\"\nprocessor = AutoProcessor.from_pretrained(model_name)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nimage = Image.open(\"image.jpg\")\nprompt = \"USER: <image>\\nDescribe this image in detail.\\nASSISTANT:\"\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=200)\nresponse = processor.decode(outputs[0], skip_special_tokens=True)\n```\n\n## Memory and Performance\n\n### Quantization\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\n# 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\n# 8-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n```\n\n### Flash Attention\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",  # Requires flash-attn package\n    device_map=\"auto\",\n)\n```\n\n### torch.compile\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nmodel = torch.compile(model, mode=\"reduce-overhead\")\n```\n\n### Batched Inference\n\n```python\ntexts = [\"First prompt\", \"Second prompt\", \"Third prompt\"]\ninputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=50)\n\ndecoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n```\n\nSee `reference/advanced-inference.md` for streaming, KV caching, and serving patterns.\n\n## Best Practices\n\n1. **Use bfloat16 over float16**: Better numerical stability on modern GPUs\n2. **Set pad token for generation**: `tokenizer.pad_token = tokenizer.eos_token`\n3. **Use device_map=\"auto\"**: Let Accelerate handle device placement\n4. **Enable Flash Attention**: Significant speedup for long sequences\n5. **Batch when possible**: Amortize fixed costs across multiple inputs\n6. **Use pipeline for quick prototyping**: Switch to manual control for production\n7. **Cache models locally**: Set `HF_HOME` environment variable for model cache location\n8. **Check model license**: Verify usage rights before deployment\n\n## References\n\nSee `reference/` for detailed documentation:\n- `fine-tuning.md` - Advanced fine-tuning patterns with LoRA, PEFT, and custom training\n- `advanced-inference.md` - Generation strategies, streaming, and serving"
              }
            ]
          }
        ]
      }
    }
  ]
}