{
  "owner": {
    "id": "ddunnock",
    "display_name": "DavidAD",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/210689323?u=d1449a2d92b09442acd7206d2ed75e577c646376&v=4",
    "url": "https://github.com/ddunnock",
    "bio": "Chief Systems Engineer & DoD contractor in RF/SATCOM/space systems. MBSE-focused, building AI-assisted engineering workflows.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 7,
      "total_commands": 20,
      "total_skills": 6,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "ddunnock/claude-plugins",
      "url": "https://github.com/ddunnock/claude-plugins",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-03T21:11:39Z",
        "created_at": "2026-01-02T13:33:56Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1393
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 390
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 3293
        },
        {
          "path": "_examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "_examples/automation-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "_examples/automation-builder/SKILL.md",
          "type": "blob",
          "size": 6098
        },
        {
          "path": "_examples/automation-builder/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "_examples/automation-builder/assets/command-template.md",
          "type": "blob",
          "size": 1312
        },
        {
          "path": "_examples/automation-builder/assets/output-template.md",
          "type": "blob",
          "size": 1576
        },
        {
          "path": "_examples/automation-builder/assets/script-template.sh",
          "type": "blob",
          "size": 5435
        },
        {
          "path": "_examples/automation-builder/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "_examples/automation-builder/references/command-patterns.md",
          "type": "blob",
          "size": 6932
        },
        {
          "path": "_examples/automation-builder/references/script-patterns.md",
          "type": "blob",
          "size": 11787
        },
        {
          "path": "_examples/automation-builder/references/template-patterns.md",
          "type": "blob",
          "size": 6702
        },
        {
          "path": "_examples/automation-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "_examples/automation-builder/scripts/init-automation.sh",
          "type": "blob",
          "size": 6931
        },
        {
          "path": "_examples/automation-builder/scripts/validate-automation.sh",
          "type": "blob",
          "size": 10678
        },
        {
          "path": "_references",
          "type": "tree",
          "size": null
        },
        {
          "path": "_references/agent-skills.md",
          "type": "blob",
          "size": 17886
        },
        {
          "path": "_references/claude-cookbooks",
          "type": "commit",
          "size": null
        },
        {
          "path": "_references/skill-authoring-best-practices.md",
          "type": "blob",
          "size": 41517
        },
        {
          "path": "marketplace.json",
          "type": "blob",
          "size": 1497
        },
        {
          "path": "mcps",
          "type": "tree",
          "size": null
        },
        {
          "path": "mcps/session-memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "mcps/session-memory/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "mcps/session-memory/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 139
        },
        {
          "path": "mcps/session-memory/MCP.md",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "mcps/session-memory/README.md",
          "type": "blob",
          "size": 4605
        },
        {
          "path": "mcps/session-memory/config.json",
          "type": "blob",
          "size": 234
        },
        {
          "path": "mcps/session-memory/plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "mcps/session-memory/plugins/__init__.py",
          "type": "blob",
          "size": 490
        },
        {
          "path": "mcps/session-memory/plugins/base.py",
          "type": "blob",
          "size": 4079
        },
        {
          "path": "mcps/session-memory/plugins/generic.py",
          "type": "blob",
          "size": 5911
        },
        {
          "path": "mcps/session-memory/plugins/spec_refiner.py",
          "type": "blob",
          "size": 13024
        },
        {
          "path": "mcps/session-memory/plugins/speckit.py",
          "type": "blob",
          "size": 8731
        },
        {
          "path": "mcps/session-memory/server.py",
          "type": "blob",
          "size": 40648
        },
        {
          "path": "plugin-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-creator/SKILL.md",
          "type": "blob",
          "size": 14868
        },
        {
          "path": "plugin-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "plugin-creator/references/validation-checklist.md",
          "type": "blob",
          "size": 2555
        },
        {
          "path": "plugin-creator/references/workflows.md",
          "type": "blob",
          "size": 3020
        },
        {
          "path": "plugin-creator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-creator/scripts/init_skill.py",
          "type": "blob",
          "size": 10897
        },
        {
          "path": "plugin-creator/scripts/package_skill.py",
          "type": "blob",
          "size": 3996
        },
        {
          "path": "plugin-creator/scripts/quick_validate.py",
          "type": "blob",
          "size": 4519
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 136
        },
        {
          "path": "skills/documentation-architect/GUARDRAILS.md",
          "type": "blob",
          "size": 19665
        },
        {
          "path": "skills/documentation-architect/SKILL.md",
          "type": "blob",
          "size": 13969
        },
        {
          "path": "skills/documentation-architect/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/commands/docs.analyze.md",
          "type": "blob",
          "size": 832
        },
        {
          "path": "skills/documentation-architect/commands/docs.generate.md",
          "type": "blob",
          "size": 1144
        },
        {
          "path": "skills/documentation-architect/commands/docs.init.md",
          "type": "blob",
          "size": 1089
        },
        {
          "path": "skills/documentation-architect/commands/docs.inventory.md",
          "type": "blob",
          "size": 982
        },
        {
          "path": "skills/documentation-architect/commands/docs.plan.md",
          "type": "blob",
          "size": 962
        },
        {
          "path": "skills/documentation-architect/commands/docs.readme.md",
          "type": "blob",
          "size": 730
        },
        {
          "path": "skills/documentation-architect/commands/docs.sync.md",
          "type": "blob",
          "size": 1353
        },
        {
          "path": "skills/documentation-architect/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/references/change-management.md",
          "type": "blob",
          "size": 15050
        },
        {
          "path": "skills/documentation-architect/references/chunking-strategy.md",
          "type": "blob",
          "size": 8459
        },
        {
          "path": "skills/documentation-architect/references/command-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/analyze-workflow.md",
          "type": "blob",
          "size": 8988
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/generate-workflow.md",
          "type": "blob",
          "size": 8615
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/init-workflow.md",
          "type": "blob",
          "size": 8116
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/inventory-workflow.md",
          "type": "blob",
          "size": 6627
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/plan-workflow.md",
          "type": "blob",
          "size": 8527
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/readme-workflow.md",
          "type": "blob",
          "size": 9083
        },
        {
          "path": "skills/documentation-architect/references/command-workflows/sync-workflow.md",
          "type": "blob",
          "size": 11445
        },
        {
          "path": "skills/documentation-architect/references/diataxis-framework.md",
          "type": "blob",
          "size": 8584
        },
        {
          "path": "skills/documentation-architect/references/quality-criteria.md",
          "type": "blob",
          "size": 9913
        },
        {
          "path": "skills/documentation-architect/references/source-tracking.md",
          "type": "blob",
          "size": 8323
        },
        {
          "path": "skills/documentation-architect/references/speckit-integration.md",
          "type": "blob",
          "size": 9316
        },
        {
          "path": "skills/documentation-architect/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/scripts/analyze_docs.py",
          "type": "blob",
          "size": 10120
        },
        {
          "path": "skills/documentation-architect/scripts/doc_research.py",
          "type": "blob",
          "size": 7376
        },
        {
          "path": "skills/documentation-architect/scripts/generate_wbs.py",
          "type": "blob",
          "size": 11189
        },
        {
          "path": "skills/documentation-architect/scripts/readme_generator.py",
          "type": "blob",
          "size": 14877
        },
        {
          "path": "skills/documentation-architect/scripts/sync_codebase.py",
          "type": "blob",
          "size": 14789
        },
        {
          "path": "skills/documentation-architect/scripts/validate_docs.py",
          "type": "blob",
          "size": 15883
        },
        {
          "path": "skills/documentation-architect/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/templates/cascade-tracker.md",
          "type": "blob",
          "size": 4718
        },
        {
          "path": "skills/documentation-architect/templates/change-log.md",
          "type": "blob",
          "size": 4256
        },
        {
          "path": "skills/documentation-architect/templates/changelog-template.md",
          "type": "blob",
          "size": 4962
        },
        {
          "path": "skills/documentation-architect/templates/docs-structure",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/documentation-architect/templates/docs-structure/developer-section.md",
          "type": "blob",
          "size": 6379
        },
        {
          "path": "skills/documentation-architect/templates/docs-structure/index.md",
          "type": "blob",
          "size": 1758
        },
        {
          "path": "skills/documentation-architect/templates/docs-structure/user-section.md",
          "type": "blob",
          "size": 4575
        },
        {
          "path": "skills/documentation-architect/templates/inventory.md",
          "type": "blob",
          "size": 2518
        },
        {
          "path": "skills/documentation-architect/templates/progress-tracker.md",
          "type": "blob",
          "size": 5168
        },
        {
          "path": "skills/documentation-architect/templates/readme-template.md",
          "type": "blob",
          "size": 4446
        },
        {
          "path": "skills/documentation-architect/templates/source-reference.md",
          "type": "blob",
          "size": 2434
        },
        {
          "path": "skills/documentation-architect/templates/terminology-registry.md",
          "type": "blob",
          "size": 3537
        },
        {
          "path": "skills/documentation-architect/templates/wbs-item.md",
          "type": "blob",
          "size": 3467
        },
        {
          "path": "skills/research-opportunity-investigator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-opportunity-investigator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-opportunity-investigator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 145
        },
        {
          "path": "skills/research-opportunity-investigator/GUARDRAILS.md",
          "type": "blob",
          "size": 11356
        },
        {
          "path": "skills/research-opportunity-investigator/SKILL.md",
          "type": "blob",
          "size": 25784
        },
        {
          "path": "skills/research-opportunity-investigator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-opportunity-investigator/references/research-checklist.md",
          "type": "blob",
          "size": 10137
        },
        {
          "path": "skills/research-opportunity-investigator/references/rfc-template.md",
          "type": "blob",
          "size": 6290
        },
        {
          "path": "skills/research-opportunity-investigator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-opportunity-investigator/templates/acp-summary-template.md",
          "type": "blob",
          "size": 9506
        },
        {
          "path": "skills/research-opportunity-investigator/templates/research-summary-template.md",
          "type": "blob",
          "size": 2303
        },
        {
          "path": "skills/specification-refiner",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-refiner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-refiner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 114
        },
        {
          "path": "skills/specification-refiner/SKILL.md",
          "type": "blob",
          "size": 15184
        },
        {
          "path": "skills/specification-refiner/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-refiner/assets/analysis-state-template.md",
          "type": "blob",
          "size": 5129
        },
        {
          "path": "skills/specification-refiner/assets/traceability-matrix-template.md",
          "type": "blob",
          "size": 4025
        },
        {
          "path": "skills/specification-refiner/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-refiner/references/critical-path-analysis.md",
          "type": "blob",
          "size": 7896
        },
        {
          "path": "skills/specification-refiner/references/gate-templates.md",
          "type": "blob",
          "size": 9668
        },
        {
          "path": "skills/specification-refiner/references/seams-framework.md",
          "type": "blob",
          "size": 7707
        },
        {
          "path": "skills/specification-refiner/references/spec-hierarchy.md",
          "type": "blob",
          "size": 7229
        },
        {
          "path": "skills/speckit-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 156
        },
        {
          "path": "skills/speckit-generator/SKILL.md",
          "type": "blob",
          "size": 14942
        },
        {
          "path": "skills/speckit-generator/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/memory/constitution.md",
          "type": "blob",
          "size": 16252
        },
        {
          "path": "skills/speckit-generator/assets/memory/documentation.md",
          "type": "blob",
          "size": 17472
        },
        {
          "path": "skills/speckit-generator/assets/memory/git-cicd.md",
          "type": "blob",
          "size": 9616
        },
        {
          "path": "skills/speckit-generator/assets/memory/python.md",
          "type": "blob",
          "size": 14067
        },
        {
          "path": "skills/speckit-generator/assets/memory/react-nextjs.md",
          "type": "blob",
          "size": 17099
        },
        {
          "path": "skills/speckit-generator/assets/memory/rust.md",
          "type": "blob",
          "size": 11918
        },
        {
          "path": "skills/speckit-generator/assets/memory/security.md",
          "type": "blob",
          "size": 11020
        },
        {
          "path": "skills/speckit-generator/assets/memory/tailwind-shadcn.md",
          "type": "blob",
          "size": 22371
        },
        {
          "path": "skills/speckit-generator/assets/memory/testing.md",
          "type": "blob",
          "size": 10886
        },
        {
          "path": "skills/speckit-generator/assets/memory/typescript.md",
          "type": "blob",
          "size": 12998
        },
        {
          "path": "skills/speckit-generator/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/commands/command-template.md",
          "type": "blob",
          "size": 2761
        },
        {
          "path": "skills/speckit-generator/assets/templates/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/memory/status-template.md",
          "type": "blob",
          "size": 962
        },
        {
          "path": "skills/speckit-generator/assets/templates/plan",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/plan/domain-plan.md",
          "type": "blob",
          "size": 2223
        },
        {
          "path": "skills/speckit-generator/assets/templates/plan/master-plan.md",
          "type": "blob",
          "size": 2434
        },
        {
          "path": "skills/speckit-generator/assets/templates/report",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/report/analysis-report.md",
          "type": "blob",
          "size": 2811
        },
        {
          "path": "skills/speckit-generator/assets/templates/report/clarify-session.md",
          "type": "blob",
          "size": 2138
        },
        {
          "path": "skills/speckit-generator/assets/templates/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/scripts/python-script-template.py",
          "type": "blob",
          "size": 3974
        },
        {
          "path": "skills/speckit-generator/assets/templates/tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/assets/templates/tasks/tasks-template.md",
          "type": "blob",
          "size": 2485
        },
        {
          "path": "skills/speckit-generator/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/commands/speckit.analyze.md",
          "type": "blob",
          "size": 1514
        },
        {
          "path": "skills/speckit-generator/commands/speckit.clarify.md",
          "type": "blob",
          "size": 1668
        },
        {
          "path": "skills/speckit-generator/commands/speckit.implement.md",
          "type": "blob",
          "size": 1688
        },
        {
          "path": "skills/speckit-generator/commands/speckit.init.md",
          "type": "blob",
          "size": 2733
        },
        {
          "path": "skills/speckit-generator/commands/speckit.plan.md",
          "type": "blob",
          "size": 2696
        },
        {
          "path": "skills/speckit-generator/commands/speckit.tasks.md",
          "type": "blob",
          "size": 2343
        },
        {
          "path": "skills/speckit-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/references/command-workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/analyze-workflow.md",
          "type": "blob",
          "size": 6364
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/clarify-workflow.md",
          "type": "blob",
          "size": 8157
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/implement-workflow.md",
          "type": "blob",
          "size": 8447
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/init-workflow.md",
          "type": "blob",
          "size": 4929
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/plan-workflow.md",
          "type": "blob",
          "size": 5246
        },
        {
          "path": "skills/speckit-generator/references/command-workflows/tasks-workflow.md",
          "type": "blob",
          "size": 6582
        },
        {
          "path": "skills/speckit-generator/references/deliverable-principles.md",
          "type": "blob",
          "size": 10881
        },
        {
          "path": "skills/speckit-generator/references/plan-structure.md",
          "type": "blob",
          "size": 8218
        },
        {
          "path": "skills/speckit-generator/references/tech-stack-detection.md",
          "type": "blob",
          "size": 7308
        },
        {
          "path": "skills/speckit-generator/references/validation-rules.md",
          "type": "blob",
          "size": 8864
        },
        {
          "path": "skills/speckit-generator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/speckit-generator/scripts/analyze_artifacts.py",
          "type": "blob",
          "size": 18901
        },
        {
          "path": "skills/speckit-generator/scripts/check_assumptions.py",
          "type": "blob",
          "size": 8240
        },
        {
          "path": "skills/speckit-generator/scripts/detect_stack.py",
          "type": "blob",
          "size": 12586
        },
        {
          "path": "skills/speckit-generator/scripts/select_memory.py",
          "type": "blob",
          "size": 14008
        },
        {
          "path": "skills/speckit-generator/scripts/validate_plan.py",
          "type": "blob",
          "size": 11831
        },
        {
          "path": "skills/speckit-generator/scripts/validate_speckit.py",
          "type": "blob",
          "size": 12931
        },
        {
          "path": "skills/streaming-output",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/streaming-output/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/streaming-output/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 166
        },
        {
          "path": "skills/streaming-output/SKILL.md",
          "type": "blob",
          "size": 17357
        },
        {
          "path": "skills/streaming-output/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/streaming-output/commands/stream.finalize.md",
          "type": "blob",
          "size": 1601
        },
        {
          "path": "skills/streaming-output/commands/stream.init.md",
          "type": "blob",
          "size": 1378
        },
        {
          "path": "skills/streaming-output/commands/stream.repair.md",
          "type": "blob",
          "size": 1365
        },
        {
          "path": "skills/streaming-output/commands/stream.resume.md",
          "type": "blob",
          "size": 1360
        },
        {
          "path": "skills/streaming-output/commands/stream.status.md",
          "type": "blob",
          "size": 1487
        },
        {
          "path": "skills/streaming-output/commands/stream.write.md",
          "type": "blob",
          "size": 1275
        },
        {
          "path": "skills/streaming-output/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/streaming-output/scripts/stream_cleanup.py",
          "type": "blob",
          "size": 6020
        },
        {
          "path": "skills/streaming-output/scripts/stream_repair.py",
          "type": "blob",
          "size": 13748
        },
        {
          "path": "skills/streaming-output/scripts/stream_status.py",
          "type": "blob",
          "size": 8598
        },
        {
          "path": "skills/streaming-output/scripts/stream_write.py",
          "type": "blob",
          "size": 8767
        },
        {
          "path": "skills/trade-study-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/trade-study-analysis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/trade-study-analysis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 127
        },
        {
          "path": "skills/trade-study-analysis/GUARDRAILS.md",
          "type": "blob",
          "size": 7855
        },
        {
          "path": "skills/trade-study-analysis/SKILL.md",
          "type": "blob",
          "size": 21974
        },
        {
          "path": "skills/trade-study-analysis/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/trade-study-analysis/scripts/assumption_tracker.py",
          "type": "blob",
          "size": 22618
        },
        {
          "path": "skills/trade-study-analysis/scripts/fishbone.py",
          "type": "blob",
          "size": 19514
        },
        {
          "path": "skills/trade-study-analysis/scripts/five_whys.py",
          "type": "blob",
          "size": 14829
        },
        {
          "path": "skills/trade-study-analysis/scripts/generate_report.py",
          "type": "blob",
          "size": 20300
        },
        {
          "path": "skills/trade-study-analysis/scripts/normalize.py",
          "type": "blob",
          "size": 10293
        },
        {
          "path": "skills/trade-study-analysis/scripts/problem_analyzer.py",
          "type": "blob",
          "size": 22400
        },
        {
          "path": "skills/trade-study-analysis/scripts/score.py",
          "type": "blob",
          "size": 12621
        },
        {
          "path": "skills/trade-study-analysis/scripts/sensitivity.py",
          "type": "blob",
          "size": 19405
        },
        {
          "path": "skills/trade-study-analysis/scripts/source_tracker.py",
          "type": "blob",
          "size": 16752
        },
        {
          "path": "skills/trade-study-analysis/scripts/visualize.py",
          "type": "blob",
          "size": 20514
        },
        {
          "path": "skills/trade-study-analysis/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/trade-study-analysis/templates/trade_study_report.md",
          "type": "blob",
          "size": 6983
        },
        {
          "path": "skills/trade-study-analysis/workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/trade-study-analysis/workflow/DATA_COLLECTION.md",
          "type": "blob",
          "size": 23052
        },
        {
          "path": "skills/trade-study-analysis/workflow/PROBLEM_DEFINITION.md",
          "type": "blob",
          "size": 27908
        },
        {
          "path": "skills/trade-study-analysis/workflow/REPORT_GENERATION.md",
          "type": "blob",
          "size": 30725
        },
        {
          "path": "skills/trade-study-analysis/workflow/SCORING_WEIGHTING.md",
          "type": "blob",
          "size": 28246
        },
        {
          "path": "skills/trade-study-analysis/workflow/SENSITIVITY.md",
          "type": "blob",
          "size": 28818
        },
        {
          "path": "tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "tools/init_plugin.py",
          "type": "blob",
          "size": 10420
        },
        {
          "path": "tools/install_mcp.py",
          "type": "blob",
          "size": 6873
        },
        {
          "path": "tools/package_plugin.py",
          "type": "blob",
          "size": 5632
        },
        {
          "path": "tools/validate_plugin.py",
          "type": "blob",
          "size": 9023
        }
      ],
      "marketplace": {
        "name": "dunnock-plugins",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "dunnock"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "documentation-architect",
            "description": "Transform documentation using the Diátaxis framework",
            "source": "./skills/documentation-architect",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install documentation-architect@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [
              {
                "name": "/docs.analyze",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.analyze.md",
                "frontmatter": null,
                "content": "Read-only audit of documentation quality.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Characteristics\n- **Read-only**: Never modifies files\n- **Deterministic**: Same input = same output\n- **Stable IDs**: Finding IDs consistent across runs\n\n## Checks Performed\n- Document quality scores\n- Diátaxis coverage matrix\n- Broken links, orphan pages\n- TODO/placeholder markers\n- User journey coverage\n\n## Quality Metrics\n| Metric | Weight | Criteria |\n|--------|--------|----------|\n| Accuracy | 25% | Claims verified, sources cited |\n| Clarity | 25% | Scannable, examples present |\n| Completeness | 25% | All sections, no TODOs |\n| Structure | 25% | Follows quadrant template |\n\n## Outputs\n- `docs/_meta/analysis-report.md`\n\n## Guardrails\n- Read-only, stable IDs\n- Source-grounded findings\n"
              },
              {
                "name": "/docs.generate",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.generate.md",
                "frontmatter": null,
                "content": "Execute plan to create documentation.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Selection Options\n- Default: All pending items\n- `WBS-001`: Specific item\n- `\"Phase 1\"`: Items in phase\n- `--user`: User docs only\n- `--dev`: Developer docs only\n\n## Workflow\n1. Load plan WBS items\n2. Resolve dependencies\n3. For each item:\n   - Load sources\n   - Apply Diátaxis guidelines\n   - Generate document\n   - **Document Review Loop** (mandatory)\n   - Log changes\n   - Cascade analysis\n   - Update memory files\n4. Phase gate check\n\n## Document Review Loop (per document)\n```\nGenerate → Present Review → Collect Feedback\n                              ↓\n                   [Approved] → Update Memory → Next\n                   [Changes] → Apply → Log → Cascade → Re-present\n```\n\n## Outputs\n- Generated docs in docs/user/ and docs/developer/\n\n## Guardrails\n- Mandatory document review loop: every document individually reviewed\n- Mandatory change logging: all changes tracked\n- Mandatory cascade analysis: cross-document impact assessed\n- Idempotent: preserves completed, regenerates pending\n"
              },
              {
                "name": "/docs.init",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.init.md",
                "frontmatter": null,
                "content": "Establish documentation foundation for this project.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Workflow\n1. Check existing state (docs/, .claude/memory/docs-*.md)\n2. Detect project type (library, CLI, app, service)\n3. Create docs/ structure with Diátaxis layout\n4. Create documentation memory files\n5. Present summary and next steps\n\n## Outputs to Create\n```\ndocs/\n├── index.md\n├── user/\n│   ├── getting-started/\n│   ├── guides/\n│   ├── concepts/\n│   └── reference/\n├── developer/\n│   ├── architecture/\n│   ├── contributing/\n│   └── reference/api/\n└── _meta/\n    ├── inventory.md\n    ├── plan.md\n    └── progress.md\n\n.claude/memory/\n├── docs-constitution.md\n├── docs-terminology.md\n└── docs-sources.md\n```\n\n## Guardrails\n- No assumptions without approval\n- No proceeding without confirmation\n- Idempotent: skips existing, updates changed, preserves customizations\n\nAfter completion, suggest `/docs.inventory` as next step.\n"
              },
              {
                "name": "/docs.inventory",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.inventory.md",
                "frontmatter": null,
                "content": "Catalog and classify documentation sources.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Workflow\n1. Scan source locations (.claude/resources/, codebase, uploads)\n2. Classify by type (SPEC, ADR, RFC, CODE, DOC)\n3. Map to Diátaxis quadrants\n4. Estimate token counts for chunking\n5. Identify coverage gaps\n\n## Source Types\n| Type | Description | Example |\n|------|-------------|---------|\n| SPEC | Requirements, features | requirements.md, PRD |\n| ADR | Architecture decisions | ADR-001-auth.md |\n| RFC | Proposals, designs | RFC-002-api.md |\n| CODE | Docstrings, comments | Python/TS files |\n| DOC | Existing documentation | README, guides |\n\n## Outputs\n- `docs/_meta/inventory.md`\n- Updated `docs-sources.md`\n\n## Guardrails\n- No assumptions without approval\n- Source-grounded content: every claim cites its source\n- Idempotent: re-scans, updates registry, adds new, never removes\n\nAfter completion, suggest `/docs.plan` as next step.\n"
              },
              {
                "name": "/docs.plan",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.plan.md",
                "frontmatter": null,
                "content": "Create documentation plan (Work Breakdown Structure).\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Modes\n- Default: Plan from inventory\n- `--from-speckit`: Use speckit artifacts as input\n\n## Workflow\n1. Load inventory\n2. Analyze sources for documentation needs\n3. Map to Diátaxis quadrants\n4. Design target structure\n5. Create prioritized WBS\n6. Define phases and gates\n\n## WBS Item Structure\n```markdown\n| ID | Document | Quadrant | Priority | Sources | Dependencies |\n|----|----------|----------|----------|---------|--------------|\n| WBS-001 | quickstart.md | Tutorial | HIGH | SRC-001 | None |\n| WBS-002 | authentication.md | How-To | HIGH | SRC-002,SRC-003 | WBS-001 |\n```\n\n## Outputs\n- `docs/_meta/plan.md`\n\n## Guardrails\n- No assumptions without approval\n- No proceeding without confirmation\n- Idempotent: detects existing, offers update/regenerate\n\nAfter completion, suggest `/docs.generate` as next step.\n"
              },
              {
                "name": "/docs.readme",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.readme.md",
                "frontmatter": null,
                "content": "Direct management of README.md and CHANGELOG.md.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n## Modes\n- Default: Update README.md\n- `--init`: Create from scratch\n- `--changelog VERSION`: Add changelog entry\n\n## README Best Practices\n```markdown\n# Project Name\n> One-line description\n\n## Installation\n## Quick Start\n## Features\n## Documentation (links to docs/)\n## Contributing\n## License\n```\n\n## CHANGELOG Format (Keep a Changelog)\n```markdown\n## [Unreleased]\n### Added / Changed / Fixed\n\n## [1.0.0] - YYYY-MM-DD\n### Added\n- Initial release\n```\n\n## Outputs\n- README.md\n- CHANGELOG.md (with user review)\n\n## Guardrails\n- Proposes changes, user approves\n- Mandatory document review loop\n"
              },
              {
                "name": "/docs.sync",
                "description": null,
                "path": "skills/documentation-architect/commands/docs.sync.md",
                "frontmatter": null,
                "content": "Walk codebase, sync documentation with implementation reality.\n\n## Before Starting\nRead and internalize the behavioral constraints in `GUARDRAILS.md`.\n\nDocumentation drifts from reality during implementation. This command bridges that gap by treating code as source of truth.\n\n## Modes\n- Default: Incremental sync (changed files)\n- `--walkthrough`: Full code exploration\n- `--component auth`: Specific component\n\n## Workflow\n1. Code walkthrough (analyze implementation)\n2. Extract reality (APIs, configs, behavior)\n3. Compare to existing documentation\n4. Generate discrepancy report\n5. Present update options per finding:\n   - **Auto-update**: Apply simple fixes\n   - **Manual review**: Complex changes\n   - **Skip**: Acknowledge, don't change\n   - **Code issue**: Doc is correct, code needs fix\n6. Update memory files with code snapshot\n\n## Discrepancy Types\n| Type | Severity | Example |\n|------|----------|---------|\n| MISSING | HIGH | Public API not documented |\n| INCORRECT | HIGH | Doc says 201, code returns 200 |\n| OUTDATED | MEDIUM | References deprecated endpoint |\n| UNDOCUMENTED | MEDIUM | Public function lacks docstring |\n\n## Outputs\n- `docs/_meta/sync-report.md`\n- `.claude/memory/docs-codebase-snapshot.md`\n- Updated documentation\n\n## Guardrails\n- Always safe, produces comparison, user chooses updates\n- No assumptions without approval\n"
              }
            ],
            "skills": [
              {
                "name": "documentation-architect",
                "description": "Transform documentation from any starting point into professional, comprehensive documentation packages using the Diátaxis framework. 7 commands: init (create structure), inventory (catalog sources), plan (create WBS), generate (create docs), sync (update from code reality), analyze (quality audit), readme (manage README/CHANGELOG). Integrates with speckit-generator for implementation-to-docs workflow. Supports specs, ADRs, RFCs as input with code walkthrough for syncing docs to reality.\n",
                "path": "skills/documentation-architect/SKILL.md",
                "frontmatter": {
                  "name": "documentation-architect",
                  "description": "Transform documentation from any starting point into professional, comprehensive documentation packages using the Diátaxis framework. 7 commands: init (create structure), inventory (catalog sources), plan (create WBS), generate (create docs), sync (update from code reality), analyze (quality audit), readme (manage README/CHANGELOG). Integrates with speckit-generator for implementation-to-docs workflow. Supports specs, ADRs, RFCs as input with code walkthrough for syncing docs to reality.\n"
                },
                "content": "# Documentation Architect\n\nCreate comprehensive, professional documentation using the Diátaxis framework. Works with any starting point: greenfield projects, existing specs/ADRs/RFCs, or scattered documentation.\n\n## Critical: Read GUARDRAILS.md First\n\nBefore any command, Claude **MUST** read and internalize the behavioral constraints in `GUARDRAILS.md`.\n\n**Key Guardrails Summary**:\n| # | Guardrail | Brief |\n|---|-----------|-------|\n| 1 | No assumptions without approval | Every inference requires confirmation |\n| 2 | No proceeding without confirmation | User controls progression |\n| 3 | Source-grounded content | Every claim cites its source |\n| 4 | Mandatory document review loop | Every document individually reviewed |\n| 5 | Mandatory change logging | All changes tracked |\n| 6 | Mandatory cascade analysis | Cross-document impact assessed |\n| 7 | Idempotent operations | Safe to run at any project stage |\n\n---\n\n## Commands\n\n| Command | Purpose | When to Use |\n|---------|---------|-------------|\n| `/docs.init` | Create docs/ structure and memory files | Starting documentation |\n| `/docs.inventory` | Catalog and classify sources | After init or adding sources |\n| `/docs.plan` | Create documentation WBS | Before generating docs |\n| `/docs.generate` | Execute plan, create docs | When plan is ready |\n| `/docs.sync` | Walk code, sync with reality | After implementation |\n| `/docs.analyze` | Quality audit (read-only) | Before release, CI/CD |\n| `/docs.readme` | Manage README.md, CHANGELOG.md | Project root files |\n\n---\n\n## Quick Start\n\n```\nNew project:\n  /docs.init → /docs.inventory → /docs.plan → /docs.generate\n\nAfter speckit implementation:\n  /docs.sync → review discrepancies → update docs\n\nQuality check before release:\n  /docs.analyze → address findings → /docs.readme --changelog VERSION\n```\n\n---\n\n## The Diátaxis Framework\n\nDocumentation organized around four user needs:\n\n```\n                PRACTICAL                      THEORETICAL\n          ┌─────────────────────────────┬─────────────────────────────┐\n          │                             │                             │\nLEARNING  │      TUTORIALS              │      EXPLANATION            │\n          │  \"Help me learn\"            │  \"Help me understand why\"   │\n          │                             │                             │\n          ├─────────────────────────────┼─────────────────────────────┤\n          │                             │                             │\nWORKING   │      HOW-TO GUIDES          │      REFERENCE              │\n          │  \"Help me do X\"             │  \"Give me the facts\"        │\n          │                             │                             │\n          └─────────────────────────────┴─────────────────────────────┘\n```\n\nSee `references/diataxis-framework.md` for detailed guidance.\n\n---\n\n## /docs.init\n\n**Purpose**: Establish documentation foundation.\n\n**Workflow**:\n1. Check existing state (docs/, .claude/memory/docs-*.md)\n2. Detect project type (library, CLI, app, service)\n3. Create docs/ structure with Diátaxis layout\n4. Create documentation memory files\n5. Present summary and next steps\n\n**Outputs**:\n```\ndocs/\n├── index.md\n├── user/\n│   ├── getting-started/\n│   ├── guides/\n│   ├── concepts/\n│   └── reference/\n├── developer/\n│   ├── architecture/\n│   ├── contributing/\n│   └── reference/api/\n└── _meta/\n    ├── inventory.md\n    ├── plan.md\n    └── progress.md\n\n.claude/memory/\n├── docs-constitution.md\n├── docs-terminology.md\n└── docs-sources.md\n```\n\n**Integration**: Auto-suggested after `/speckit.init`\n\nSee `references/command-workflows/init-workflow.md` for details.\n\n---\n\n## /docs.inventory\n\n**Purpose**: Catalog and classify documentation sources.\n\n**Workflow**:\n1. Scan source locations (.claude/resources/, codebase, uploads)\n2. Classify by type (SPEC, ADR, RFC, CODE, DOC)\n3. Map to Diátaxis quadrants\n4. Estimate token counts for chunking\n5. Identify coverage gaps\n\n**Source Types**:\n| Type | Description | Example |\n|------|-------------|---------|\n| SPEC | Requirements, features | requirements.md, PRD |\n| ADR | Architecture decisions | ADR-001-auth.md |\n| RFC | Proposals, designs | RFC-002-api.md |\n| CODE | Docstrings, comments | Python/TS files |\n| DOC | Existing documentation | README, guides |\n\n**Outputs**: `docs/_meta/inventory.md`, updated `docs-sources.md`\n\nSee `references/command-workflows/inventory-workflow.md` for details.\n\n---\n\n## /docs.plan\n\n**Purpose**: Create documentation plan (Work Breakdown Structure).\n\n**Modes**:\n- `/docs.plan` - Plan from inventory\n- `/docs.plan --from-speckit` - Use speckit artifacts as input\n\n**Workflow**:\n1. Load inventory\n2. Analyze sources for documentation needs\n3. Map to Diátaxis quadrants\n4. Design target structure\n5. Create prioritized WBS\n6. Define phases and gates\n\n**WBS Item Structure**:\n```markdown\n| ID | Document | Quadrant | Priority | Sources | Dependencies |\n|----|----------|----------|----------|---------|--------------|\n| WBS-001 | quickstart.md | Tutorial | HIGH | SRC-001 | None |\n| WBS-002 | authentication.md | How-To | HIGH | SRC-002,SRC-003 | WBS-001 |\n```\n\n**Outputs**: `docs/_meta/plan.md`\n\nSee `references/command-workflows/plan-workflow.md` for details.\n\n---\n\n## /docs.generate\n\n**Purpose**: Execute plan to create documentation.\n\n**Selection Options**:\n- `/docs.generate` - All pending items\n- `/docs.generate WBS-001` - Specific item\n- `/docs.generate \"Phase 1\"` - Items in phase\n- `/docs.generate --user` - User docs only\n- `/docs.generate --dev` - Developer docs only\n\n**Workflow**:\n1. Load plan WBS items\n2. Resolve dependencies\n3. For each item:\n   - Load sources\n   - Apply Diátaxis guidelines\n   - Generate document\n   - **Document Review Loop** (mandatory)\n   - Log changes\n   - Cascade analysis\n   - Update memory files\n4. Phase gate check\n\n**Document Review Loop** (per document):\n```\nGenerate → Present Review → Collect Feedback\n                               ↓\n                    [Approved] → Update Memory → Next\n                    [Changes] → Apply → Log → Cascade → Re-present\n```\n\n**Outputs**: Generated docs in docs/user/ and docs/developer/\n\nSee `references/command-workflows/generate-workflow.md` for details.\n\n---\n\n## /docs.sync\n\n**Purpose**: Walk codebase, sync documentation with implementation reality.\n\n**Key Insight**: Documentation drifts from reality during implementation. This command bridges that gap by treating code as source of truth.\n\n**Modes**:\n- `/docs.sync` - Incremental sync (changed files)\n- `/docs.sync --walkthrough` - Full code exploration\n- `/docs.sync --component auth` - Specific component\n\n**Workflow**:\n1. Code walkthrough (analyze implementation)\n2. Extract reality (APIs, configs, behavior)\n3. Compare to existing documentation\n4. Generate discrepancy report\n5. Present update options per finding:\n   - **Auto-update**: Apply simple fixes\n   - **Manual review**: Complex changes\n   - **Skip**: Acknowledge, don't change\n   - **Code issue**: Doc is correct, code needs fix\n6. Update memory files with code snapshot\n\n**Discrepancy Types**:\n| Type | Severity | Example |\n|------|----------|---------|\n| MISSING | HIGH | Public API not documented |\n| INCORRECT | HIGH | Doc says 201, code returns 200 |\n| OUTDATED | MEDIUM | References deprecated endpoint |\n| UNDOCUMENTED | MEDIUM | Public function lacks docstring |\n\n**Outputs**:\n- `docs/_meta/sync-report.md`\n- `.claude/memory/docs-codebase-snapshot.md`\n- Updated documentation\n\n**Integration**: Suggested after `/speckit.implement`\n\nSee `references/command-workflows/sync-workflow.md` for details.\n\n---\n\n## /docs.analyze\n\n**Purpose**: Read-only audit of documentation quality.\n\n**Characteristics**:\n- **Read-only**: Never modifies files\n- **Deterministic**: Same input = same output\n- **Stable IDs**: Finding IDs consistent across runs\n\n**Checks**:\n- Document quality scores\n- Diátaxis coverage matrix\n- Broken links, orphan pages\n- TODO/placeholder markers\n- User journey coverage\n\n**Quality Metrics**:\n| Metric | Weight | Criteria |\n|--------|--------|----------|\n| Accuracy | 25% | Claims verified, sources cited |\n| Clarity | 25% | Scannable, examples present |\n| Completeness | 25% | All sections, no TODOs |\n| Structure | 25% | Follows quadrant template |\n\n**Outputs**: `docs/_meta/analysis-report.md`\n\nSee `references/command-workflows/analyze-workflow.md` for details.\n\n---\n\n## /docs.readme\n\n**Purpose**: Direct management of README.md and CHANGELOG.md.\n\n**Modes**:\n- `/docs.readme` - Update README.md\n- `/docs.readme --init` - Create from scratch\n- `/docs.readme --changelog VERSION` - Add changelog entry\n\n**README Best Practices**:\n```markdown\n# Project Name\n> One-line description\n\n## Installation\n## Quick Start\n## Features\n## Documentation (links to docs/)\n## Contributing\n## License\n```\n\n**CHANGELOG Format** (Keep a Changelog):\n```markdown\n## [Unreleased]\n### Added / Changed / Fixed\n\n## [1.0.0] - YYYY-MM-DD\n### Added\n- Initial release\n```\n\n**Outputs**: README.md, CHANGELOG.md (with user review)\n\nSee `references/command-workflows/readme-workflow.md` for details.\n\n---\n\n## speckit-generator Integration\n\n| After speckit Command | Suggested docs Action |\n|----------------------|----------------------|\n| `/speckit.init` | `/docs.init` |\n| `/speckit.plan` | `/docs.plan --from-speckit` |\n| `/speckit.tasks` | `/docs.inventory` |\n| `/speckit.implement` | `/docs.sync` |\n\nSee `references/speckit-integration.md` for integration details.\n\n---\n\n## docs/ Output Structure\n\n```\ndocs/\n├── index.md                    # Landing page\n│\n├── user/                       # USER-FACING\n│   ├── getting-started/        # Tutorials\n│   │   ├── quickstart.md\n│   │   └── installation.md\n│   ├── guides/                 # How-To\n│   │   └── [task].md\n│   ├── concepts/               # Explanations\n│   │   └── [concept].md\n│   └── reference/              # Reference\n│       └── configuration.md\n│\n├── developer/                  # DEVELOPER-FACING\n│   ├── architecture/           # Design, ADRs\n│   │   ├── overview.md\n│   │   └── decisions/\n│   ├── contributing/           # Contribution guides\n│   │   └── getting-started.md\n│   └── reference/              # API/Technical\n│       └── api/\n│\n└── _meta/                      # Metadata (not published)\n    ├── inventory.md\n    ├── plan.md\n    ├── progress.md\n    ├── change-log.md\n    ├── sync-report.md\n    └── analysis-report.md\n```\n\n---\n\n## Memory Files\n\n| File | Purpose | Created By |\n|------|---------|------------|\n| `docs-constitution.md` | Documentation principles | `/docs.init` |\n| `docs-sources.md` | Source registry | `/docs.inventory` |\n| `docs-terminology.md` | Term definitions | `/docs.generate` |\n| `docs-codebase-snapshot.md` | Code state snapshot | `/docs.sync` |\n\n---\n\n## Idempotency Guarantees\n\n| Command | Behavior |\n|---------|----------|\n| `/docs.init` | Skips existing, updates changed, preserves customizations |\n| `/docs.inventory` | Re-scans, updates registry, adds new, never removes |\n| `/docs.plan` | Detects existing, offers update/regenerate |\n| `/docs.generate` | Preserves completed, regenerates pending |\n| `/docs.sync` | Always safe, produces comparison, user chooses |\n| `/docs.analyze` | Read-only, stable IDs |\n| `/docs.readme` | Proposes changes, user approves |\n\n---\n\n## Scripts\n\n| Script | Purpose | Used By |\n|--------|---------|---------|\n| `scripts/analyze_docs.py` | Quality analysis | `/docs.analyze` |\n| `scripts/doc_research.py` | Research competitors | `/docs.inventory` |\n| `scripts/generate_wbs.py` | Generate WBS | `/docs.plan` |\n| `scripts/validate_docs.py` | Validation checks | `/docs.analyze` |\n| `scripts/sync_codebase.py` | Code walkthrough | `/docs.sync` |\n| `scripts/readme_generator.py` | README/CHANGELOG | `/docs.readme` |\n\n---\n\n## References\n\n| Document | Purpose |\n|----------|---------|\n| `GUARDRAILS.md` | Behavioral constraints |\n| `references/diataxis-framework.md` | Quadrant guidelines |\n| `references/chunking-strategy.md` | Large source handling |\n| `references/source-tracking.md` | Citation protocols |\n| `references/quality-criteria.md` | Assessment rubrics |\n| `references/speckit-integration.md` | speckit workflow |\n| `references/command-workflows/*.md` | Command details |\n\n---\n\n## Anti-Patterns\n\n| Anti-Pattern | Correct Approach |\n|--------------|------------------|\n| Mixed quadrant content | Separate by user need |\n| No quickstart | 5-minute first experience |\n| Scattered explanations | Dedicated concepts section |\n| Reference-only docs | Full quadrant coverage |\n| Docs drift from code | Use `/docs.sync` after implementation |\n| Generating without sources | Register sources first |\n| Proceeding without confirmation | Wait for user approval |"
              }
            ]
          },
          {
            "name": "research-opportunity-investigator",
            "description": "Research and opportunity investigation for protocols",
            "source": "./skills/research-opportunity-investigator",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install research-opportunity-investigator@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "research-opportunity-investigator",
                "description": "Conduct systematic research and opportunity investigation for ACP protocol integration, collaboration, and enhancement opportunities. Use when the user wants to research external projects, protocols, or tools for potential collaboration; investigate gap-filling opportunities for IDE integrations; assess compatibility between ACP and external protocols; or identify opportunities for ACP adoption. Guides through discovery, analysis, validation, and RFC generation with mandatory gates and source grounding. Outputs include comprehensive summary documents, gap analyses, and formal RFC proposals.",
                "path": "skills/research-opportunity-investigator/SKILL.md",
                "frontmatter": {
                  "name": "research-opportunity-investigator",
                  "description": "Conduct systematic research and opportunity investigation for ACP protocol integration, collaboration, and enhancement opportunities. Use when the user wants to research external projects, protocols, or tools for potential collaboration; investigate gap-filling opportunities for IDE integrations; assess compatibility between ACP and external protocols; or identify opportunities for ACP adoption. Guides through discovery, analysis, validation, and RFC generation with mandatory gates and source grounding. Outputs include comprehensive summary documents, gap analyses, and formal RFC proposals."
                },
                "content": "# Research & Opportunity Investigator\n\nSystematic research and opportunity analysis for ACP protocol integration with external projects, protocols, and tools.\n\n## CRITICAL BEHAVIORAL REQUIREMENTS\n\n**This skill operates under strict guardrails. The assistant MUST:**\n\n### 1. NEVER Proceed Without Explicit User Confirmation\n- Ask clarifying questions at EVERY phase gate before proceeding\n- Do NOT proceed based on assumed understanding\n- Wait for explicit user responses before moving forward\n- Present findings and wait for validation\n\n### 2. NEVER Make Ungrounded Claims\n- All findings MUST reference specific sources (URLs, documentation, code)\n- Format: `[Statement] (Source: [URL/document], [section])`\n- If information cannot be verified, mark as: `[UNGROUNDED—requires verification]`\n- Maintain running source registry throughout research\n\n### 3. ALL Analysis Must Be Source-Grounded\n- Every technical claim requires evidence\n- Cite specific code, documentation, or announcements\n- Distinguish between: `[VERIFIED]`, `[INFERRED]`, `[ASSUMED]`\n\n### 4. Mandatory ACP Summary Document Before RFC\n- MUST create comprehensive ACP summary document\n- Summary MUST cover: existing RFCs, schemas, spec chapters\n- Summary enables RFC validation against current state\n- User MUST approve summary before RFC generation\n\n### 5. RFCs Must Trace to Existing ACP Specification\n- Every RFC proposal MUST reference existing spec sections\n- Show which chapters/RFCs are affected\n- Demonstrate compatibility with current design\n\n---\n\n## Workflow Overview\n\n```\nResearch & Opportunity Investigation Workflow:\n\n□ Phase 1: RESEARCH SCOPING\n  ├─ Define research target and objectives\n  ├─ Establish success criteria\n  └─ GATE: User confirms research scope\n\n□ Phase 2: DISCOVERY & COLLECTION\n  ├─ Web search for documentation, repos, announcements\n  ├─ Source registration and cataloging\n  └─ GATE: User confirms source coverage\n\n□ Phase 3: DEEP ANALYSIS\n  ├─ Technical architecture analysis\n  ├─ Feature mapping and comparison\n  ├─ Gap identification\n  └─ GATE: User confirms analysis accuracy\n\n□ Phase 4: ACP CONTEXT SUMMARY\n  ├─ Generate comprehensive ACP summary\n  ├─ Map existing RFCs, schemas, spec chapters\n  ├─ Identify integration points\n  └─ GATE: User approves ACP summary document\n\n□ Phase 5: OPPORTUNITY ASSESSMENT\n  ├─ Gap analysis (what target lacks that ACP provides)\n  ├─ Collaboration opportunities\n  ├─ Implementation feasibility\n  └─ GATE: User confirms opportunity assessment\n\n□ Phase 6: RFC GENERATION\n  ├─ Draft RFC for identified opportunities\n  ├─ Validate against ACP summary\n  ├─ Cross-reference existing specs\n  └─ GATE: User approves RFC content\n\n□ Phase 7: DELIVERABLES PACKAGING\n  ├─ Final summary document\n  ├─ Gap analysis report\n  ├─ RFC proposal(s)\n  └─ GATE: User confirms all deliverables\n```\n\n---\n\n## Phase 1: Research Scoping\n\n### 1.1 Scope Definition Template\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n🔍 RESEARCH SCOPE DEFINITION\n═══════════════════════════════════════════════════════════════════════════════\n\nRESEARCH TARGET:\n  Name: [Project/Protocol/Tool name]\n  Type: [IDE/Protocol/Framework/Tool]\n  Primary URL: [Main website/repository]\n\nRESEARCH OBJECTIVES:\n  Primary Goal: [What are we trying to learn/achieve?]\n  \n  Specific Questions:\n    1. [Question 1]\n    2. [Question 2]\n    3. [Question 3]\n\nSUCCESS CRITERIA:\n  □ [Criterion 1 - measurable outcome]\n  □ [Criterion 2 - measurable outcome]\n  □ [Criterion 3 - measurable outcome]\n\nACP INTEGRATION FOCUS:\n  □ Gap-filling opportunity (target lacks capability ACP provides)\n  □ Protocol integration (technical compatibility)\n  □ Collaboration opportunity (partnership/adoption)\n  □ Competitive analysis (understanding landscape)\n  □ Other: _______________\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  Please confirm this scope before proceeding with discovery.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### 1.2 Scoping Questions\n\nAsk these questions to establish research scope (2-3 per message max):\n\n**Target Identification:**\n- What specific project/protocol/tool are we researching?\n- What is the primary URL or repository?\n- What problem does this target solve?\n\n**Objective Clarification:**\n- What do you hope to achieve through this research?\n- Are you looking for integration, collaboration, or competitive understanding?\n- What would a successful outcome look like?\n\n**Constraints:**\n- Are there any aspects that are out of scope?\n- What timeline or resource constraints exist?\n- Are there any competing priorities?\n\n---\n\n## Phase 2: Discovery & Collection\n\n### 2.1 Source Registration Template\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📚 SOURCE REGISTRATION\n═══════════════════════════════════════════════════════════════════════════════\n\nREGISTERED SOURCES:\n\n  [SRC-001] Official Documentation\n    └─ URL: [url]\n    └─ Type: documentation\n    └─ Accessed: [date]\n    └─ Relevance: [high/medium/low]\n    └─ Key Sections: [list relevant sections]\n\n  [SRC-002] GitHub Repository\n    └─ URL: [url]\n    └─ Type: source_code\n    └─ Accessed: [date]\n    └─ Relevance: [high/medium/low]\n    └─ Key Files: [list relevant files]\n\n  [SRC-003] ...\n\n───────────────────────────────────────────────────────────────────────────────\nSOURCE GAPS (information needed but not found):\n\n  □ [Gap 1] - Required for: [analysis area]\n  □ [Gap 2] - Required for: [analysis area]\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  Please confirm these sources are sufficient or identify additional sources.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### 2.2 Discovery Search Strategy\n\nFor each research target, systematically search:\n\n**Tier 1 - Primary Sources (MUST search):**\n```\n□ Official documentation site\n□ GitHub/GitLab repository\n□ Official announcements/blog posts\n□ API/Protocol specifications\n```\n\n**Tier 2 - Secondary Sources (SHOULD search):**\n```\n□ Technical blog posts from team members\n□ Conference talks/presentations\n□ Community discussions (Discord, Slack, Forums)\n□ Integration guides from partners\n```\n\n**Tier 3 - Tertiary Sources (MAY search):**\n```\n□ Third-party reviews and analyses\n□ Comparison articles\n□ Issue tracker discussions\n□ Social media announcements\n```\n\n### 2.3 Evidence Grounding Format\n\nAll findings MUST use this grounding format:\n\n```markdown\n[Finding Statement]\n  └─ Source: [SRC-XXX], [specific section/line/page]\n  └─ Evidence Type: [VERIFIED|INFERRED|ASSUMED]\n  └─ Confidence: [HIGH|MEDIUM|LOW]\n  └─ Quote/Reference: \"[relevant excerpt]\"\n```\n\n---\n\n## Phase 3: Deep Analysis\n\n### 3.1 Technical Architecture Analysis\n\nFor each research target, analyze:\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n🏗️ TECHNICAL ARCHITECTURE ANALYSIS\n═══════════════════════════════════════════════════════════════════════════════\n\nCORE ARCHITECTURE:\n  \n  Components:\n    □ [Component 1]: [Description] (Source: [SRC-XXX])\n    □ [Component 2]: [Description] (Source: [SRC-XXX])\n  \n  Data Flow:\n    [Component A] → [Component B] → [Component C]\n  \n  Key Abstractions:\n    □ [Abstraction 1]: [Purpose]\n    □ [Abstraction 2]: [Purpose]\n\nPROTOCOL/API DESIGN:\n  \n  Communication Pattern: [request-response/streaming/event-driven]\n  Data Format: [JSON/Protocol Buffers/Other]\n  Transport: [HTTP/WebSocket/IPC/Other]\n  \n  Key Endpoints/Methods:\n    □ [Endpoint 1]: [Purpose] (Source: [SRC-XXX])\n    □ [Endpoint 2]: [Purpose] (Source: [SRC-XXX])\n\nEXTENSION POINTS:\n  \n  □ [Extension Point 1]: [How external tools integrate]\n  □ [Extension Point 2]: [How external tools integrate]\n\n───────────────────────────────────────────────────────────────────────────────\n```\n\n### 3.2 Feature Mapping Template\n\n```markdown\n## Feature Comparison Matrix\n\n| Feature Area | Target Has | ACP Provides | Gap/Overlap |\n|--------------|------------|--------------|-------------|\n| [Feature 1] | [Yes/No/Partial] | [Yes/No/Partial] | [Gap/Overlap/None] |\n| [Feature 2] | [Yes/No/Partial] | [Yes/No/Partial] | [Gap/Overlap/None] |\n\n### Gap Details\n\n#### Gap G-001: [Gap Name]\n- **Target Status**: [What target lacks]\n- **ACP Capability**: [What ACP provides]\n- **Integration Potential**: [HIGH/MEDIUM/LOW]\n- **Evidence**: (Source: [SRC-XXX])\n\n#### Gap G-002: ...\n```\n\n---\n\n## Phase 4: ACP Context Summary\n\n### 4.1 Summary Generation Requirements\n\n**BEFORE generating any RFC, MUST create comprehensive ACP summary covering:**\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📋 ACP PROTOCOL SUMMARY DOCUMENT\n═══════════════════════════════════════════════════════════════════════════════\n\nGenerated: [timestamp]\nPurpose: Reference document for RFC validation and integration planning\n\n───────────────────────────────────────────────────────────────────────────────\nSECTION 1: SPECIFICATION OVERVIEW\n───────────────────────────────────────────────────────────────────────────────\n\nCurrent ACP Version: [version]\nSpec Location: acp-protocol/acp-spec/spec/\n\nKey Chapters:\n  □ 01-introduction.md: [Summary of goals and non-goals]\n  □ 03-cache-format.md: [Summary of cache structure]\n  □ 04-config-format.md: [Summary of configuration options]\n  □ 05-annotations.md: [Summary of annotation syntax]\n  □ 06-constraints.md: [Summary of constraint system]\n  □ [Additional relevant chapters...]\n\n───────────────────────────────────────────────────────────────────────────────\nSECTION 2: EXISTING RFCs\n───────────────────────────────────────────────────────────────────────────────\n\nRFC-001: Self-Documenting Annotations\n  Status: [status]\n  Summary: [brief summary]\n  Key Changes: [what it introduced]\n  Relevance to Research: [how it relates to current investigation]\n\nRFC-002: Documentation References\n  Status: [status]\n  Summary: [brief summary]\n  Key Changes: [what it introduced]\n  Relevance to Research: [how it relates]\n\nRFC-003: Annotation Provenance\n  Status: [status]\n  Summary: [brief summary]\n  Key Changes: [what it introduced]\n  Relevance to Research: [how it relates]\n\n[Continue for all RFCs...]\n\n───────────────────────────────────────────────────────────────────────────────\nSECTION 3: SCHEMA INVENTORY\n───────────────────────────────────────────────────────────────────────────────\n\nLocation: acp-protocol/acp-spec/schemas/\n\nSchemas:\n  □ cache.schema.json: [purpose, key fields]\n  □ config.schema.json: [purpose, key fields]\n  □ vars.schema.json: [purpose, key fields]\n  □ sync.schema.json: [purpose, key fields]\n  [Continue for all schemas...]\n\n───────────────────────────────────────────────────────────────────────────────\nSECTION 4: INTEGRATION POINTS\n───────────────────────────────────────────────────────────────────────────────\n\nExisting Integration Mechanisms:\n  □ MCP Integration (acp-mcp): [current capabilities]\n  □ CLI Interface (acp-cli): [current capabilities]\n  □ LSP Planning (acp-lsp): [planned capabilities]\n\nExtension Points for External Protocols:\n  □ [Extension point 1]: [how external tools would integrate]\n  □ [Extension point 2]: [how external tools would integrate]\n\n───────────────────────────────────────────────────────────────────────────────\nSECTION 5: DESIGN PRINCIPLES\n───────────────────────────────────────────────────────────────────────────────\n\nCore Principles (from spec):\n  □ Self-documenting annotations\n  □ Token efficiency\n  □ Deterministic constraints\n  □ Language-agnostic syntax\n  □ Progressive disclosure\n\nCompatibility Requirements:\n  □ Backward compatibility policy\n  □ Versioning approach\n  □ RFC process requirements\n\n═══════════════════════════════════════════════════════════════════════════════\n⚠️  User MUST approve this summary before RFC generation proceeds.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### 4.2 Summary Validation Checklist\n\nBefore proceeding to RFC generation:\n\n```\n□ All existing RFCs catalogued with summaries\n□ All relevant spec chapters summarized\n□ All schemas inventoried with key fields\n□ Integration points identified\n□ Design principles extracted\n□ User has reviewed and approved summary\n```\n\n---\n\n## Phase 5: Opportunity Assessment\n\n### 5.1 Gap Analysis Framework\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n🎯 OPPORTUNITY ASSESSMENT\n═══════════════════════════════════════════════════════════════════════════════\n\nGAP ANALYSIS:\n\n  What [TARGET] Lacks That ACP Provides:\n  \n    GAP-001: [Gap Name]\n      └─ Target Status: [Current capability or lack]\n      └─ ACP Capability: [What ACP offers]\n      └─ Strategic Value: [HIGH/MEDIUM/LOW]\n      └─ Implementation Effort: [HIGH/MEDIUM/LOW]\n      └─ Evidence: (Source: [SRC-XXX])\n    \n    GAP-002: ...\n\nCOLLABORATION OPPORTUNITIES:\n\n  OPP-001: [Opportunity Name]\n    └─ Description: [What could be achieved]\n    └─ Mutual Benefit: [How both parties benefit]\n    └─ Required Changes:\n        - ACP: [What ACP would need to change/add]\n        - Target: [What target would need to change/add]\n    └─ Feasibility: [HIGH/MEDIUM/LOW]\n    └─ Evidence: (Source: [SRC-XXX])\n\n  OPP-002: ...\n\nRISK ASSESSMENT:\n\n  RISK-001: [Risk Name]\n    └─ Description: [What could go wrong]\n    └─ Likelihood: [HIGH/MEDIUM/LOW]\n    └─ Impact: [HIGH/MEDIUM/LOW]\n    └─ Mitigation: [How to address]\n\n───────────────────────────────────────────────────────────────────────────────\nRECOMMENDATION:\n\n  □ Proceed with RFC development for: [specific opportunities]\n  □ Defer: [what to defer and why]\n  □ Decline: [what to decline and why]\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  Please confirm this assessment before RFC generation.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n---\n\n## Phase 6: RFC Generation\n\n### 6.1 RFC Requirements\n\n**MUST follow ACP RFC structure. See `references/rfc-template.md` for complete template.**\n\nKey RFC sections required:\n- Summary (2-3 sentences)\n- Motivation with research basis and gap analysis\n- Specification with affected components\n- Backward compatibility analysis\n- Implementation guidance\n- Alternatives considered\n- References to sources\n\n### 6.2 RFC Validation Checklist\n\nBefore finalizing RFC:\n\n```\nRFC VALIDATION CHECKLIST:\n\nStructure:\n  □ All required sections present\n  □ RFC number follows convention\n  □ Status correctly set to Draft\n\nContent:\n  □ Summary is concise (2-3 sentences)\n  □ Motivation clearly explains problem\n  □ Research basis documented with sources\n  □ Gap analysis included with IDs\n\nSpecification:\n  □ All affected components identified\n  □ Changes reference existing spec sections\n  □ Proposed changes are specific and implementable\n  □ Examples provided where helpful\n\nCompatibility:\n  □ Breaking changes identified (or stated as none)\n  □ Migration path provided if needed\n  □ Deprecation schedule if applicable\n\nValidation:\n  □ Cross-referenced against ACP Summary document\n  □ No conflicts with existing RFCs\n  □ Consistent with ACP design principles\n  □ User has reviewed and approved\n```\n\n---\n\n## Phase 7: Deliverables Packaging\n\n### 7.1 Deliverable Checklist\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📦 DELIVERABLES PACKAGE\n═══════════════════════════════════════════════════════════════════════════════\n\nRESEARCH DELIVERABLES:\n\n  □ research-summary-[target]-[date].md\n    └─ Complete research findings\n    └─ All sources catalogued\n    └─ Technical analysis complete\n    \n  □ gap-analysis-[target]-[date].md\n    └─ All gaps identified with IDs\n    └─ Opportunity assessment\n    └─ Recommendations\n\nACP CONTEXT DELIVERABLES:\n\n  □ acp-summary-[date].md\n    └─ Current RFCs summarized\n    └─ Schemas inventoried\n    └─ Spec chapters mapped\n    └─ Integration points identified\n\nRFC DELIVERABLES:\n\n  □ RFC-XXXX-[title].md\n    └─ Complete RFC proposal\n    └─ Validated against ACP summary\n    └─ All sections complete\n\n───────────────────────────────────────────────────────────────────────────────\nQUALITY VERIFICATION:\n\n  □ All sources cited with [SRC-XXX] format\n  □ No ungrounded claims remain\n  □ All assumptions marked as [ASSUMED]\n  □ User approved each phase gate\n  □ RFC traces to existing specification\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  Please confirm all deliverables are complete and accurate.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n---\n\n## Guardrails Reference\n\n### Prohibited Behaviors\n\n| Prohibited | Required Instead |\n|------------|------------------|\n| Making claims without sources | Cite specific source [SRC-XXX] |\n| Proceeding without confirmation | Wait for explicit user approval at gates |\n| Generating RFC without ACP summary | Create and approve summary first |\n| Assuming target capabilities | Verify with source evidence |\n| Skipping phases | Execute all mandatory gates |\n| Ungrounded RFC proposals | Trace all changes to existing spec |\n\n### Evidence Grounding Standards\n\n| Evidence Type | When to Use | Example |\n|---------------|-------------|---------|\n| `[VERIFIED]` | Directly confirmed from primary source | Official docs, source code |\n| `[INFERRED]` | Logically derived from verified facts | Architectural implications |\n| `[ASSUMED]` | Reasonable assumption, needs validation | User must confirm |\n| `[UNGROUNDED]` | Cannot find source | Flag for investigation |\n\n### Source Confidence Levels\n\n| Level | Definition | Usage |\n|-------|------------|-------|\n| HIGH | Official docs, source code, announcements | Core claims |\n| MEDIUM | Blog posts, talks, community discussions | Supporting evidence |\n| LOW | Third-party analyses, speculation | Context only |\n\n---\n\n## Reference Documents\n\n- **RFC Template**: See `references/rfc-template.md` for complete RFC structure\n- **Research Checklist**: See `references/research-checklist.md` for comprehensive checklist\n- **ACP Spec Locations**: \n  - Specification: `acp-protocol/acp-spec/spec/`\n  - RFCs: `acp-protocol/acp-spec/rfcs/`\n  - Schemas: `acp-protocol/acp-spec/schemas/`\n\n---\n\n## Quick Reference\n\n### Common Research Targets\n\n| Target Type | Key Areas to Investigate |\n|-------------|-------------------------|\n| IDE | Extension API, LSP support, agent framework |\n| Protocol | Message format, transport, extension points |\n| AI Tool | Context handling, constraint system, codebase awareness |\n| Framework | Plugin architecture, configuration, integration hooks |\n\n### Common ACP Integration Points\n\n| Integration Point | Relevance |\n|-------------------|-----------|\n| Bootstrap prompts | Minimal context injection |\n| Cache format | Structured codebase metadata |\n| Constraint system | Lock levels and guardrails |\n| Annotation syntax | Self-documenting directives |\n| Query interface | CLI commands for AI access |\n| MCP integration | Dynamic tool connection |"
              }
            ]
          },
          {
            "name": "specification-refiner",
            "description": "Refine and improve specifications",
            "source": "./skills/specification-refiner",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install specification-refiner@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "specification-refiner",
                "description": "Systematic analysis and refinement of specifications, requirements, architecture designs, and project plans. Use when the user wants to identify gaps, weaknesses, inefficiencies, or complications in a proposed plan, specification, or design document. Triggers on gap analysis, specification review, requirements analysis, architecture critique, design validation, plan assessment, weakness identification, assumption auditing, or when users share specs/plans asking for feedback. Produces actionable findings with remediations and maintains analysis state across iterations.\n",
                "path": "skills/specification-refiner/SKILL.md",
                "frontmatter": {
                  "name": "specification-refiner",
                  "description": "Systematic analysis and refinement of specifications, requirements, architecture designs, and project plans. Use when the user wants to identify gaps, weaknesses, inefficiencies, or complications in a proposed plan, specification, or design document. Triggers on gap analysis, specification review, requirements analysis, architecture critique, design validation, plan assessment, weakness identification, assumption auditing, or when users share specs/plans asking for feedback. Produces actionable findings with remediations and maintains analysis state across iterations.\n"
                },
                "content": "# Specification Refiner\n\nSystematically analyze and refine specifications, requirements, and architectural designs through iterative gap analysis with persistent memory and explicit user confirmation at each phase.\n\n## Core Workflow\n\n```\n0. ASSESS     → Evaluate complexity, select mode, confirm with user\n1. INGEST     → Load document, confirm understanding with user\n2. ANALYZE    → Run SEAMS + Critical Path, present preliminary findings\n3. PRESENT    → Surface detailed findings, manage questions with user\n4. ITERATE    → Accept changes, re-analyze, present deltas\n5. SYNTHESIZE → Present comprehensive summary for user approval\n6. OUTPUT     → Generate refined specification(s) in Draft status\n7. VALIDATE   → Review, validate traceability, advance status\n```\n\nEach phase ends with a **full summary gate** requiring user confirmation before proceeding.\n\n---\n\n## Phase 0: ASSESS\n\nOn receiving a specification document, first assess complexity to determine the appropriate mode.\n\n### Complexity Assessment\n\nEvaluate these factors:\n- **Document size**: Page/word count\n- **Domains identified**: Single vs. multi-domain\n- **Stakeholder count**: How many perspectives involved\n- **Scope clarity**: Clear, moderate, or ambiguous boundaries\n\n### Mode Selection\n\nPresent to user:\n\n```\nBased on initial assessment:\n- Document size: [X pages / Y words]\n- Domains identified: [list domains]\n- Stakeholder count: [N stakeholders]\n- Scope clarity: [Clear/Moderate/Ambiguous]\n\nRecommended mode: [SIMPLE/COMPLEX]\n\nOptions:\n1. Proceed with recommended mode\n2. Override to SIMPLE mode\n3. Override to COMPLEX mode\n4. Explain the modes in more detail\n\nYour choice:\n```\n\n**SIMPLE Mode**: Single-domain, <10 pages, clear scope\n- SEAMS analysis only\n- Single A-Spec output with numbered requirements (`A-REQ-NNN`)\n\n**COMPLEX Mode**: Multi-domain, >10 pages, ambiguous scope\n- Full dual-framework analysis (SEAMS + Critical Path)\n- A-Spec/B-Spec hierarchy per domain (see `references/spec-hierarchy.md`)\n- Requirements Traceability Matrix generation\n\n### Phase 0 Gate\n\nPresent gate summary (see `references/gate-templates.md` for full format). Wait for user confirmation before proceeding.\n\n---\n\n## Phase 1: INGEST\n\n### Actions\n1. Parse document structure (sections, dependencies, interfaces)\n2. Create initial memory file: `analysis-state.md` using template from `assets/analysis-state-template.md`\n3. Record mode selection in memory file\n4. Identify document type and select appropriate analysis lenses\n5. Note any questions that arise during parsing\n\n### Question Management\n- Add questions to the Open Questions list with \"Raised In: Phase 1: INGEST\"\n- Attempt to answer any Phase 0 questions from document content\n- Update question statuses\n\n### Phase 1 Gate\n\nPresent full summary including: document info, sections identified, key entities, dependencies, and question status. See `references/gate-templates.md` for format. Wait for user confirmation—user may answer questions here.\n\n---\n\n## Phase 2: ANALYZE\n\nRun analysis frameworks based on mode.\n\n### SIMPLE Mode\nRun SEAMS Analysis only (see `references/seams-framework.md`).\n\n### COMPLEX Mode\nRun BOTH frameworks in parallel:\n\n#### Framework A: SEAMS Analysis\n**S**tructure → **E**xecution → **A**ssumptions → **M**ismatches → **S**takeholders\n\n| Lens | Questions to Answer |\n|------|---------------------|\n| **Structure** | Completeness of I/O paths? Cohesion? Coupling risks? Boundary clarity? |\n| **Execution** | Happy path works? Edge cases covered? Failure modes handled? |\n| **Assumptions** | Technical assumptions? Organizational? Environmental? |\n| **Mismatches** | Requirements ↔ Design aligned? Design ↔ Implementation consistent? |\n| **Stakeholders** | Operator view? Security view? Integrator view? End-user view? |\n\n#### Framework B: Critical Path Analysis\nSee `references/critical-path-analysis.md` for detailed methods.\n\n1. **Dependency Mapping**: Build N² matrix\n2. **Critical Path Identification**: Find longest/riskiest chains\n3. **Single Points of Failure**: Cascade risk components\n4. **Bottleneck Detection**: Throughput limiters\n5. **Temporal Analysis**: Sequencing issues\n\n### Question Management\n- Add new questions discovered during analysis with \"Raised In: Phase 2: ANALYZE\"\n- Note which findings are blocked by unanswered questions\n\n### Phase 2 Gate\n\nPresent preliminary findings summary with severity counts, top 3 issues, question status, and blocked findings. See `references/gate-templates.md` for format. Wait for user confirmation—user may answer blocking questions here.\n\n---\n\n## Phase 3: PRESENT\n\n### Finding Format\n\nFor EACH identified issue, include: ID, title, category, severity, confidence, blocked-by status, description, evidence, impact, remediation options (with trade-offs), and related issues. See `references/gate-templates.md` for full template.\n\n### Presentation Order\n\nPresent findings grouped by:\n1. **Critical blockers** (must fix before proceeding)\n2. **Significant gaps** (high impact, clear remediation)\n3. **Optimization opportunities** (efficiency improvements)\n4. **Considerations** (context-dependent, need user input)\n\n### Question Management\n- Present all unanswered questions explicitly\n- Ask user to answer questions or confirm assumptions\n- Update question tracking with answers received\n\n### Phase 3 Gate\n\nPresent findings summary with severity counts, question status, and assumptions made. Offer options: (1) Iterate, (2) Skip to Synthesize, (3) Answer questions, (4) Review details. See `references/gate-templates.md` for format.\n\n---\n\n## Phase 4: ITERATE\n\nWhen user provides new information, constraints, or requests changes:\n\n### Actions\n1. **Validate new input** against existing analysis\n2. **Identify affected areas** in the specification\n3. **Re-run analysis** on affected sections only (delta analysis)\n4. **Assess cascading effects** on previously-identified issues\n5. **Update memory file** with new state\n\n### Question Management\n- Add new questions with \"Raised In: Phase 4: ITERATE\"\n- Check if new input answers existing questions\n- Update all question statuses\n\n### Phase 4 Gate\n\nPresent delta summary: changes incorporated, new/modified/resolved findings, key changes, question status. Offer options: (1) Continue iterating, (2) Synthesize, (3) Review findings. See `references/gate-templates.md` for format.\n\n---\n\n## Phase 5: SYNTHESIZE\n\nBefore generating any output, present a comprehensive summary for user approval.\n\n### Summary Contents\n\nPresent comprehensive summary covering:\n- **Document Overview**: Title, mode, iteration count\n- **Findings Resolution**: By severity with resolved/unresolved breakdown, key resolutions, unresolved critical/high items\n- **Questions Status**: Answered, unanswered (with impact), deferred (with reason)\n- **Assumptions**: Confirmed vs. unverified\n- **Proposed Output Structure**: Based on mode (single doc for SIMPLE, separate files for COMPLEX)\n\nSee `references/gate-templates.md` for full format.\n\n### Phase 5 Gate\n\nOffer options: (1) Approve and output, (2) Return to iterate, (3) Modify structure, (4) Answer questions. Wait for explicit approval before generating output.\n\n---\n\n## Phase 6: OUTPUT\n\nGenerate refined specification(s) based on mode, all in **Draft** status.\n\n### SIMPLE Mode Output\nGenerate single A-Spec document (`refined-specification.md`) with:\n- Numbered requirements (`A-REQ-001`, `A-REQ-002`, etc.)\n- All resolved findings incorporated\n- Selected remediations applied\n- Documented assumptions\n- Remaining open questions in dedicated section\n\n### COMPLEX Mode Output\nGenerate specification hierarchy per domain:\n\n**A-Spec files** (one per domain):\n- Naming: `[domain]-a-spec.md`\n- Requirements: `A-REQ-[DOMAIN]-NNN`\n- High-level requirements defining WHAT\n\n**B-Spec files** (one or more per domain):\n- Naming: `[domain]-[subsystem]-b-spec.md`\n- Requirements: `B-REQ-[DOMAIN]-NNN`\n- Each requirement MUST include `Traces to: A-REQ-XXX-NNN`\n- Detailed requirements defining HOW\n\n**Supporting files**:\n- `traceability-matrix.md` - Full RTM (see `assets/traceability-matrix-template.md`)\n- `cross-cutting-concerns.md` (if applicable)\n- `open-items.md`\n\nSee `references/spec-hierarchy.md` for detailed format specifications.\n\n### RTM Generation (COMPLEX mode)\n1. Extract all A-REQ-* from A-Spec files\n2. Extract all B-REQ-* from B-Spec files with their traces\n3. Build coverage matrix\n4. Calculate coverage percentage\n5. Identify gaps (A-Reqs with no B-Req coverage)\n6. Generate `traceability-matrix.md`\n7. Update `analysis-state.md` with RTM summary\n\n### Final Actions\n1. Set all specification statuses to **Draft**\n2. Update `analysis-state.md` with completion status and RTM summary\n3. Present output files to user\n4. Summarize what was generated\n5. Prompt transition to Phase 7\n\n### Phase 6 Completion\n\nPresent: mode, files created with requirement counts, RTM summary (COMPLEX mode), findings addressed, assumptions documented. Prompt user to proceed to Phase 7. See `references/gate-templates.md` for format.\n\n---\n\n## Phase 7: VALIDATE\n\nFinal review and validation phase ensuring specifications are comprehensive, traceable, and approved. **Mandatory for both SIMPLE and COMPLEX modes.**\n\n### Status Workflow\nSpecifications progress through statuses:\n```\nDraft → Reviewed → Approved → Baselined\n```\n\n- **Draft**: Initial output from Phase 6 (automatic)\n- **Reviewed**: Technical review complete, no critical gaps\n- **Approved**: Stakeholder sign-off received\n- **Baselined**: Locked for change control\n\n### Validation Actions\n1. **Completeness check**: All required sections present, cross-references valid\n2. **RTM validation** (COMPLEX): Coverage percentage, gap identification\n3. **Traceability check** (COMPLEX): All B-Reqs trace to A-Reqs\n4. **Consistency check**: Terminology, formatting, priority scales aligned\n5. **Present validation findings** with severity\n\n### Status Advancement\n- Require explicit user approval to advance status\n- Document status change with timestamp and approver\n- Update `analysis-state.md` with new status and history\n\n### Advancement Criteria\n| Transition | Requirements |\n|------------|--------------|\n| Draft → Reviewed | No critical RTM gaps, completeness checks pass |\n| Reviewed → Approved | All high-priority issues resolved, stakeholder review |\n| Approved → Baselined | Formal approval documented, change control established |\n\n### Phase 7 Gate\nPresent validation summary with:\n- Current status and proposed advancement\n- RTM coverage metrics (COMPLEX mode)\n- Completeness and consistency checklist\n- Validation findings\n- Options: advance status, return to fix issues, review details\n\nSee `references/gate-templates.md` for full template.\n\n---\n\n## Question Tracking System\n\n### Question Categories\n- **Technical**: Architecture, implementation, performance\n- **Process**: Workflow, governance, approval\n- **Scope**: Boundaries, requirements, features\n- **Stakeholder**: Roles, responsibilities, ownership\n- **Timeline**: Sequencing, dependencies, deadlines\n\n### Question Lifecycle\n1. **Raised**: Question identified, logged with phase\n2. **Answered**: Response received from user or inferred from analysis\n3. **Deferred**: Explicitly set aside with reason and revisit trigger\n\n### Question Table Format\n\nTrack questions in `analysis-state.md` using tables for Unanswered (ID, Question, Category, Raised In, Blocks), Answered (ID, Question, Answer, Answered By, Answered In), and Deferred (ID, Question, Reason, Deferred In, Revisit When).\n\n---\n\n## Memory File Management\n\n### Required Memory File: `analysis-state.md`\n\nUse the template from `assets/analysis-state-template.md`. Key sections:\n\n- Document metadata (title, version, hash)\n- Mode selection (SIMPLE/COMPLEX)\n- Analysis iterations table\n- Active and resolved findings\n- Question tracking tables (per-phase)\n- Assumption register\n- User-provided constraints\n\n### Update Protocol\n\nAfter EACH phase:\n1. Read current `analysis-state.md`\n2. Update phase completion status\n3. Update finding statuses\n4. Update question tables\n5. Record new assumptions\n6. Log user decisions\n7. Write updated file\n\n---\n\n## Analysis Depth Calibration\n\nMatch depth to document maturity:\n\n| Document Stage | Analysis Focus |\n|----------------|----------------|\n| **Concept/Idea** | Feasibility, scope clarity, key assumptions |\n| **Draft Spec** | Completeness, internal consistency, missing sections |\n| **Detailed Design** | Interface contracts, error handling, edge cases |\n| **Implementation Plan** | Dependencies, sequencing, resource conflicts |\n| **Review/Audit** | Full SEAMS sweep, stakeholder perspectives |\n\n---\n\n## Quick Assessment Mode\n\nFor rapid feedback when full analysis is not needed:\n\n1. **Boundaries**: What's in/out of scope?\n2. **One Thread**: Trace critical path from input to output\n3. **Three Assumptions**: The riskiest unstated beliefs\n4. **Silent Failure**: What breaks without notice?\n5. **Naive Question**: What would a newcomer ask that has no answer?\n\nNote: Quick Assessment skips the full phase gate workflow but still creates `analysis-state.md`.\n\n---\n\n## Output Formatting\n\n### Severity Indicators\n- 🔴 **Critical**: Blocks progress, must address\n- 🟠 **High**: Significant risk, should address soon\n- 🟡 **Medium**: Notable issue, plan to address\n- 🟢 **Low**: Minor concern, address opportunistically\n\n### Confidence Qualifiers\n- **High confidence**: Clear evidence, well-understood domain\n- **Medium confidence**: Reasonable inference, some ambiguity\n- **Low confidence**: Pattern recognition, needs validation\n\n---\n\n## Handling Incomplete Information\n\nWhen specifications are incomplete:\n1. Note the gap explicitly\n2. State what would be needed to complete analysis\n3. Offer reasonable assumptions (clearly marked)\n4. Add question to tracking system\n5. Ask user to confirm or provide missing details at next gate\n6. Document in assumption register\n\n---\n\n## Anti-Patterns to Avoid\n\n- Vague criticism without specific evidence\n- Recommendations without trade-off analysis\n- Analysis paralysis on minor issues\n- Ignoring stated constraints to suggest \"ideal\" solutions\n- Failing to update memory after iterations\n- Treating all findings as equal severity\n- **Proceeding past a gate without user confirmation**\n- **Losing track of open questions**\n- **Generating output without synthesis approval**\n- **Generating B-Specs without traces to A-Specs**\n- **Skipping RTM generation for COMPLEX mode**\n- **Advancing status without validation**\n- **Baselining specs with unresolved critical gaps**"
              }
            ]
          },
          {
            "name": "speckit-generator",
            "description": "Generate automation packages from requirements",
            "source": "./skills/speckit-generator",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install speckit-generator@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [
              {
                "name": "/command-template",
                "description": "[One-line description of what this command produces]",
                "path": "skills/speckit-generator/assets/templates/commands/command-template.md",
                "frontmatter": {
                  "description": "[One-line description of what this command produces]",
                  "agent": {
                    "model": "sonnet"
                  }
                },
                "content": "# [Command Name]\n\n## Purpose\n\n[Clear statement of the deliverable this command produces]\n\n**Deliverable Type**: [Generator | Analyzer | Orchestrator | Decision]\n\n## Prerequisites\n\n- [ ] [Required input 1] - Available at: [location/source]\n- [ ] [Required input 2] - Available at: [location/source]\n- [ ] [Prior command] completed (if dependent)\n\n## Workflow\n\n### Phase 1: Preparation\n\n1. Verify all prerequisites are met\n2. Load required resources:\n   - Template: `.claude/templates/[template-name].md`\n   - State: `.claude/memory/[state-file].md`\n3. Initialize output structure\n\n### Phase 2: [Primary Action Name]\n\n1. [Action step 1]\n   - [Sub-step if needed]\n   - [Sub-step if needed]\n\n2. [Action step 2]\n\n3. [Action step 3]\n\n**Script Support** (if applicable):\n```bash\npython scripts/python/[script-name].py <input> --json\n```\n\n**Decision Point** (if applicable):\n- If [condition]: [action]\n- If [condition]: [action]\n\n### Phase 3: Quality Validation\n\n1. Run validation checks:\n\n```bash\npython scripts/python/validate-output.py <output-path> --json\n```\n\n2. Verify against completion criteria (below)\n\n3. Document any issues or TBDs:\n   - Update `.claude/memory/[relevant-status].md`\n   - Log assumptions to `.claude/memory/assumptions-log.md`\n\n### Phase 4: Output Finalization\n\n1. Save output to designated location\n2. Update status in `.claude/memory/[status-file].md`\n3. Prepare handoff context for next command\n\n## Outputs\n\n| Output | Location | Format |\n|--------|----------|--------|\n| Primary Deliverable | `[PATH]` | [FORMAT] |\n| Status Update | `.claude/memory/[file].md` | Markdown |\n| [Other Output] | `[PATH]` | [FORMAT] |\n\n## Completion Criteria\n\n- [ ] [Verifiable criterion 1]\n- [ ] [Verifiable criterion 2]\n- [ ] Validation script passes without errors\n- [ ] All TBD items documented\n- [ ] Assumptions logged\n\n## Error Handling\n\n| Error Condition | Resolution |\n|-----------------|------------|\n| [Error 1] | [How to resolve] |\n| [Error 2] | [How to resolve] |\n\n## Handoffs\n\n### Proceed to [Next Command Name]\n\n**Context**: \n[Summary of what was accomplished in this command]\n\n**Inputs Ready**:\n- [Input 1]: `[location]`\n- [Input 2]: `[location]`\n\n**Objective**:\n[What the next command should accomplish]\n\n**Constraints**:\n- [Any constraints to carry forward]\n\nUse: `/[next-command]`\n\n---\n\n### Alternative Handoff: [Alternative Path]\n\n**Context**:\n[When to use this alternative path]\n\nUse: `/[alternative-command]`\n\n---\n\n## Notes\n\n<!-- \nGROUNDING: Document evidence sources and assumptions\n\n[VERIFIED: source-url] - Any verified facts used\n[ASSUMPTION: rationale] - Any assumptions made\n\nUpdate assumptions-log.md with any new assumptions.\n-->"
              },
              {
                "name": "/speckit.analyze",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.analyze.md",
                "frontmatter": null,
                "content": "Deterministic, read-only audit of project artifacts for consistency and completeness.\n\n## Usage\n- `/speckit.analyze` - Full analysis\n- `/speckit.analyze --verbose` - Detailed output\n- `/speckit.analyze --category gaps` - Filter by category\n\n## Characteristics\n\n- **Read-only**: Never modifies files\n- **Deterministic**: Same inputs = same outputs\n- **Stable IDs**: Finding IDs remain stable across runs\n- **Quantified**: Metrics for coverage, completeness\n\n## Analysis Categories\n\n| Category | Description |\n|----------|-------------|\n| GAPS | Missing required elements |\n| INCONSISTENCIES | Contradictions between artifacts |\n| AMBIGUITIES | Unclear or undefined items |\n| ORPHANS | Unreferenced elements |\n| ASSUMPTIONS | Untracked/unvalidated assumptions |\n\n## Severity Levels\n\n| Level | Meaning |\n|-------|---------|\n| CRITICAL | Blocks progress, must fix |\n| HIGH | Significant risk, should fix |\n| MEDIUM | Notable issue, plan to fix |\n| LOW | Minor concern |\n\n## Output Format\n\n```markdown\n# Analysis Report\n\nGenerated: [timestamp]\nArtifacts analyzed: [count]\n\n## Summary\n| Category | Critical | High | Medium | Low |\n|----------|----------|------|--------|-----|\n| GAPS     | 2        | 3    | 5      | 1   |\n| ...      |          |      |        |     |\n\n## Findings\n\n### GAP-001 [CRITICAL]\n**Location**: spec.md:45\n**Description**: Missing error handling specification\n**Recommendation**: Define error states for API failures\n```\n\n## Idempotency\n- Read-only, always safe\n- Stable finding IDs across runs\n"
              },
              {
                "name": "/speckit.clarify",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.clarify.md",
                "frontmatter": null,
                "content": "Structured ambiguity resolution with immediate spec updates.\n\n## Usage\n- `/speckit.clarify` - Start clarification session\n- `/speckit.clarify spec.md` - Clarify specific spec\n\n## Characteristics\n\n- **One question at a time**: Focused, manageable\n- **Multiple choice or short phrase**: Quick answers\n- **5-question maximum per session**: Avoid fatigue\n- **Immediate updates**: Specs updated after each answer\n- **9-category taxonomy**: Structured classification\n\n## Ambiguity Categories\n\n| Category | Example Question |\n|----------|-----------------|\n| SCOPE | \"Should X include Y functionality?\" |\n| BEHAVIOR | \"What happens when user does X?\" |\n| DATA | \"What format should X be stored in?\" |\n| ERROR | \"How should X error be handled?\" |\n| SEQUENCE | \"Does X happen before or after Y?\" |\n| CONSTRAINT | \"What are the limits for X?\" |\n| INTERFACE | \"How does X communicate with Y?\" |\n| AUTHORITY | \"Who approves X?\" |\n| TEMPORAL | \"How long should X take?\" |\n\n## Workflow\n\n1. **Scan for ambiguity** - Find [TBD], [NEEDS CLARIFICATION], vague language\n2. **Prioritize** - Rank by impact on implementation\n3. **Present question** - One at a time with options\n4. **Update spec** - Apply answer immediately\n5. **Log session** - Record Q&A for traceability\n\n## Question Format\n\n```\nCLARIFY-001 [BEHAVIOR]\n\nThe spec mentions \"user authentication\" but doesn't specify the method.\n\nWhich authentication method should be used?\n1. OAuth 2.0 with Google/GitHub (Recommended)\n2. Email/password with JWT\n3. Magic link (passwordless)\n4. Other (please specify)\n\nYour choice:\n```\n\n## Idempotency\n- Tracks answered questions\n- Skips already-clarified items\n- Session history preserved\n"
              },
              {
                "name": "/speckit.implement",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.implement.md",
                "frontmatter": null,
                "content": "Execute tasks from *-tasks.md with batch+gates execution model.\n\n## Usage\n- `/speckit.implement TASK-001` - Single task\n- `/speckit.implement TASK-001..TASK-005` - Range of tasks\n- `/speckit.implement \"Phase 1\"` - All tasks in phase\n- `/speckit.implement @foundation` - All tasks with @foundation group\n\n## Task Selection\n\n| Selector | Meaning |\n|----------|---------|\n| `TASK-001` | Single task |\n| `TASK-001..TASK-005` | Range of tasks |\n| `\"Phase 1\"` | All tasks in phase |\n| `@foundation` | All tasks with @foundation group |\n\n## Execution Model: Batch + Gates\n\n```\nExecute Phase 1 tasks\n    ↓\nGATE: \"Phase 1 complete. Review outputs?\"\n    ↓\n[User confirms]\n    ↓\nExecute Phase 2 tasks\n    ↓\nGATE: \"Phase 2 complete. Review outputs?\"\n    ...\n```\n\n## Workflow\n\nFor each task:\n1. **Load context** - Read referenced constitution sections + memory files\n2. **Present context** - Show agent the relevant guidelines\n3. **Execute** - Perform the task\n4. **Update status** - PENDING → IN_PROGRESS → COMPLETED\n5. **Verify** - Check acceptance criteria\n\n## Gate Behavior\n\nAt phase/group boundaries:\n```\nPhase [N] complete.\n\nTasks completed: [count]\nTasks failed: [count]\n\nOptions:\n1. Continue to Phase [N+1]\n2. Review completed work\n3. Re-run failed tasks\n4. Stop execution\n```\n\n## Context Loading\n\n```markdown\n## Execution Context for TASK-001\n\n### Constitution Requirements (§4.1, §4.2)\n[Extracted sections from constitution.md]\n\n### Memory File Guidelines\nFrom typescript.md:\n[Relevant sections]\n\nFrom git-cicd.md:\n[Relevant sections]\n\n### Task Details\n[Full task content]\n```\n\n## Idempotency\n- Skips COMPLETED tasks\n- Resumes from last position\n- Re-runnable for failed tasks\n"
              },
              {
                "name": "/speckit.init",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.init.md",
                "frontmatter": null,
                "content": "Establish the `.claude/` foundation with appropriate memory files for the project.\n\n## Workflow\n\n1. **Check existing state** - Detect if .claude/ exists\n2. **Detect tech stack** - Analyze project for languages/frameworks\n3. **Present detection** - Show detected stack and recommended memory files\n4. **Create structure** - Build directory structure\n5. **Copy memory files** - Select and copy based on tech stack\n6. **Generate project context** - Create project-context.md\n\n## Directory Structure Created\n\n```\n.claude/\n├── commands/      # Custom project commands\n├── memory/        # constitution.md + tech-specific files\n│   └── MANIFEST.md\n├── resources/     # Specifications, designs\n├── templates/     # Output templates\n└── scripts/       # Project scripts\n```\n\n## Memory File Selection\n\n| Category | Files | Selection |\n|----------|-------|-----------|\n| Universal | constitution.md, documentation.md, git-cicd.md, security.md, testing.md | Always |\n| TypeScript/JS | typescript.md | If TS/JS detected |\n| React/Next.js | react-nextjs.md | If React/Next detected |\n| Tailwind | tailwind-shadcn.md | If Tailwind detected |\n| Python | python.md | If Python detected |\n| Rust | rust.md | If Rust detected |\n\n## Options\n\n```\n1. Accept recommended selection\n2. Add additional memory files\n3. Remove memory files from selection\n4. Override detected stack manually\n```\n\n## Idempotency\n- Skips existing directories\n- Updates changed memory files only\n- Preserves project customizations\n\n---\n\n## GATE: Required Before Proceeding\n\n**STOP after initialization. Present results and confirm before proceeding.**\n\nAfter initialization, you MUST:\n\n1. **Present the created structure** showing:\n   - Directories created\n   - Memory files installed\n   - Detected tech stack\n\n2. **Confirm memory file selection** is appropriate:\n   - List universal files (always included)\n   - List tech-specific files (based on detection)\n   - Ask if any adjustments needed\n\n3. **Wait for user confirmation** before proceeding to other commands\n\n### Gate Response Template\n\n```\n## Initialization Complete\n\n### Directory Structure Created\n.claude/\n├── commands/\n├── memory/\n├── resources/\n├── templates/\n└── scripts/\n\n### Detected Tech Stack\n- [Languages detected]\n- [Frameworks detected]\n\n### Memory Files Installed\n**Universal:**\n- constitution.md\n- documentation.md\n- git-cicd.md\n- security.md\n- testing.md\n\n**Tech-Specific:**\n- [Based on detection]\n\n### Next Steps\n1. Review the memory files for your project needs\n2. Add specifications to .claude/resources/\n3. Run `/speckit.plan` when specs are ready\n\n**Is this configuration correct, or would you like to adjust the memory files?**\n```\n"
              },
              {
                "name": "/speckit.plan",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.plan.md",
                "frontmatter": null,
                "content": "Create implementation plans from specification files. Hierarchical for complex/multi-domain specs.\n\n## Usage\n- `/speckit.plan` - Plan from all specs\n- `/speckit.plan spec.md` - Plan from specific spec\n- `/speckit.plan --all` - Force regenerate all plans\n\n## Workflow\n\n1. **Locate specs** - Find spec files in .claude/resources/\n2. **Assess complexity** - Single domain vs multi-domain\n3. **Generate plans** - Create plan.md (and domain plans if complex)\n4. **Validate** - Check plan completeness and consistency\n\n## Output Structure\n\n**Simple (single domain)**:\n```\n.claude/resources/\n├── spec.md\n└── plan.md\n```\n\n**Complex (multi-domain)**:\n```\n.claude/resources/\n├── spec.md\n├── plan.md              # Master plan with domain references\n└── plans/\n    ├── domain-a-plan.md\n    ├── domain-b-plan.md\n    └── domain-c-plan.md\n```\n\n## Plan Content\n\nPlans contain:\n- Requirements mapping (which spec sections covered)\n- Architecture decisions\n- Implementation approach (phases, NOT tasks)\n- Verification strategy\n- Notes for task generation\n\nPlans do NOT contain:\n- Individual tasks (that's /speckit.tasks)\n- Implementation code\n- Detailed how-to instructions\n\n## Complexity Detection\n\n| Indicator | Simple | Complex |\n|-----------|--------|---------|\n| Domains | Single | Multiple distinct |\n| Page count | <10 pages | >10 pages |\n| Stakeholder count | 1-2 | 3+ |\n\nUser can override detection.\n\n## Idempotency\n- Detects existing plans\n- Offers update or regenerate\n- Preserves manual edits with warning\n\n---\n\n## GATE: Required Before Proceeding\n\n**STOP after plan generation. DO NOT proceed to `/speckit.tasks` automatically.**\n\nAfter generating plans, you MUST:\n\n1. **Present a plan summary** to the user showing:\n   - Number of plans created (master + domain plans if complex)\n   - Key architectural decisions made\n   - Any assumptions or open questions identified\n\n2. **Recommend next steps**:\n   - Run `/speckit.analyze` to check plan compliance with constitution.md\n   - Run `/speckit.clarify` if any `[TBD]` or `[NEEDS CLARIFICATION]` items exist\n   - Review plans manually before approval\n\n3. **Wait for explicit user approval** before proceeding to tasks\n\n### Gate Response Template\n\n```\n## Plan Generation Complete\n\nCreated [N] plan(s):\n- plan.md (master plan)\n- plans/domain-a-plan.md\n- [etc.]\n\n### Key Decisions\n- [List major architectural/approach decisions]\n\n### Open Questions\n- [Any [TBD] items or ambiguities found]\n\n### Recommended Next Steps\n1. Review the generated plan(s)\n2. Run `/speckit.analyze` to validate compliance\n3. Run `/speckit.clarify` to resolve open questions\n\n**Awaiting your approval before generating tasks.**\n```\n"
              },
              {
                "name": "/speckit.tasks",
                "description": null,
                "path": "skills/speckit-generator/commands/speckit.tasks.md",
                "frontmatter": null,
                "content": "Generate implementation tasks from plans + constitution + memory files.\n\n## Usage\n- `/speckit.tasks` - Generate tasks from all plans\n- `/speckit.tasks plan.md` - Generate from specific plan\n- `/speckit.tasks --all` - Force regenerate all tasks\n\n## Workflow\n\n1. **Load plan(s)** - Read plan files\n2. **Load constitution** - Extract relevant sections\n3. **Load memory files** - Get tech-specific guidelines\n4. **Generate tasks** - Create *-tasks.md with phases\n5. **Validate** - Check task completeness\n\n## Output Format\n\n```markdown\n# [Domain] Tasks\n\n## Phase 1: Foundation\n\n### TASK-001: [Title]\n**Status**: PENDING\n**Priority**: P1\n**Constitution Sections**: §4.1, §4.2\n**Memory Files**: typescript.md, git-cicd.md\n**Plan Reference**: PLAN-001\n**Description**: ...\n**Acceptance Criteria**:\n- [ ] Criterion 1\n- [ ] Criterion 2\n```\n\n## Task Statuses\n\n| Status | Meaning |\n|--------|---------|\n| PENDING | Not started |\n| IN_PROGRESS | Currently being worked |\n| BLOCKED | Waiting on dependency |\n| COMPLETED | Done and verified |\n| SKIPPED | Intentionally not done |\n\n## Idempotency\n- Preserves task statuses\n- Adds new tasks for new plan items\n- Never removes manually added tasks\n\n---\n\n## GATE: Required Before Proceeding\n\n**STOP after task generation. DO NOT proceed to `/speckit.implement` automatically.**\n\nAfter generating tasks, you MUST:\n\n1. **Present a task summary** to the user showing:\n   - Total number of tasks generated\n   - Breakdown by phase\n   - Task priority distribution (P1/P2/P3)\n   - Constitution sections referenced\n\n2. **Highlight any concerns**:\n   - Tasks with unclear acceptance criteria\n   - Dependencies that may cause blocking\n   - Tasks that may need clarification\n\n3. **Wait for explicit user approval** before implementation\n\n### Gate Response Template\n\n```\n## Task Generation Complete\n\nGenerated [N] tasks across [M] phases:\n\n| Phase | Tasks | P1 | P2 | P3 |\n|-------|-------|----|----|----|\n| Phase 1: Foundation | 5 | 3 | 2 | 0 |\n| [etc.] |\n\n### Constitution Coverage\n- §4.1 (Error Handling): 8 tasks\n- §4.2 (Logging): 5 tasks\n- [etc.]\n\n### Potential Concerns\n- [Any blocking dependencies]\n- [Tasks needing clarification]\n\n### Recommended Next Steps\n1. Review the generated tasks\n2. Adjust priorities if needed\n3. Resolve any blocking dependencies\n\n**Awaiting your approval before implementation.**\n```\n"
              }
            ],
            "skills": [
              {
                "name": "speckit-generator",
                "description": "Project-focused specification and task management system with 6 individual commands. Each command MUST be invoked separately and requires user approval before proceeding. Commands: /speckit.init, /speckit.plan, /speckit.tasks, /speckit.analyze, /speckit.clarify, /speckit.implement. NEVER chain commands automatically - each produces output that requires user review. Use /speckit.plan when user wants to create plans from specs. Use /speckit.tasks only AFTER user has approved plans.\n",
                "path": "skills/speckit-generator/SKILL.md",
                "frontmatter": {
                  "name": "speckit-generator",
                  "description": "Project-focused specification and task management system with 6 individual commands. Each command MUST be invoked separately and requires user approval before proceeding. Commands: /speckit.init, /speckit.plan, /speckit.tasks, /speckit.analyze, /speckit.clarify, /speckit.implement. NEVER chain commands automatically - each produces output that requires user review. Use /speckit.plan when user wants to create plans from specs. Use /speckit.tasks only AFTER user has approved plans.\n"
                },
                "content": "# SpecKit Generator\n\nProject-focused specification management with 6 commands that work together to transform specifications into executed implementations.\n\n## Table of Contents\n- [Critical Workflow Rules](#critical-workflow-rules)\n- [Overview](#overview)\n- [Commands](#commands)\n- [Command: init](#command-init)\n- [Command: plan](#command-plan)\n- [Command: tasks](#command-tasks)\n- [Command: analyze](#command-analyze)\n- [Command: clarify](#command-clarify)\n- [Command: implement](#command-implement)\n- [Memory File System](#memory-file-system)\n- [Idempotency](#idempotency)\n\n---\n\n## Critical Workflow Rules\n\n**MANDATORY: Commands must NOT be chained automatically.**\n\nEach command produces artifacts that require user review and approval before proceeding to the next phase. This is not optional.\n\n### Required Gates\n\n| After Command | MUST DO | Before Proceeding To |\n|---------------|---------|---------------------|\n| `/speckit.init` | Present created structure, confirm memory files | Any other command |\n| `/speckit.plan` | Present plan summary, wait for explicit approval | `/speckit.tasks` |\n| `/speckit.tasks` | Present task summary, wait for explicit approval | `/speckit.implement` |\n\n### Recommended Workflow\n\n```\n/speckit.init\n    ↓ [User reviews structure]\n/speckit.plan\n    ↓ [User reviews plan]\n/speckit.analyze ← Run BEFORE approving plan\n    ↓ [Address any CRITICAL/HIGH findings]\n/speckit.clarify ← Run if [TBD] items exist\n    ↓ [User approves final plan]\n/speckit.tasks\n    ↓ [User reviews tasks]\n/speckit.implement\n```\n\n### What NOT To Do\n\n- ❌ Run `/speckit.plan` then immediately `/speckit.tasks` without user approval\n- ❌ Generate all artifacts in one session without checkpoints\n- ❌ Skip `/speckit.analyze` before plan approval\n- ❌ Proceed past a GATE without explicit user confirmation\n\n### Gate Response Format\n\nAfter completing a command, present results in this format:\n\n```\n## [Command] Complete\n\n[Summary of what was created/modified]\n\n### Artifacts Created\n- [list of files]\n\n### Recommended Next Steps\n1. Review the [artifacts] above\n2. Run `/speckit.analyze` to check compliance (if applicable)\n3. Run `/speckit.clarify` to resolve any [TBD] items (if applicable)\n\n**Awaiting your approval before proceeding.**\n```\n\n---\n\n## Overview\n\nSpecKit provides a complete workflow for specification-driven development:\n\n```\ninit → plan → tasks → implement\n  ↑      ↑      ↑         ↑\n  └──────┴──────┴─────────┘\n         analyze/clarify (anytime)\n```\n\n### Core Principles\n\n1. **Separation of Concerns**: Plans define WHAT, tasks define HOW\n2. **Memory-Driven Compliance**: All execution references constitution.md and relevant memory files\n3. **Idempotent Operations**: All commands safe to run repeatedly\n4. **Deterministic Analysis**: analyze produces identical output for identical input\n\n---\n\n## Commands\n\n| Command | Purpose | When to Use |\n|---------|---------|-------------|\n| `/speckit.init` | Establish .claude/ foundation | New projects or incomplete setup |\n| `/speckit.plan` | Create plans from specifications | After specs exist in resources/ |\n| `/speckit.tasks` | Generate tasks from plans | After plans are approved |\n| `/speckit.analyze` | Audit project consistency | Anytime for health check |\n| `/speckit.clarify` | Resolve ambiguities | When specs have open questions |\n| `/speckit.implement` | Execute tasks | When ready to implement |\n\n---\n\n## Command: init\n\nEstablish the `.claude/` foundation with appropriate memory files for the project.\n\n### Trigger\n- Explicit: `/speckit.init`\n- Automatic: Other commands detect missing setup\n\n### Workflow\n\n1. **Check existing state** - Detect if .claude/ exists\n2. **Detect tech stack** - Analyze project for languages/frameworks\n3. **Present detection** - Show detected stack and recommended memory files\n4. **Create structure** - Build directory structure\n5. **Copy memory files** - Select and copy based on tech stack\n6. **Generate project context** - Create project-context.md\n\n### Directory Structure Created\n\n```\n.claude/\n├── commands/      # Custom project commands\n├── memory/        # constitution.md + tech-specific files\n│   └── MANIFEST.md\n├── resources/     # Specifications, designs\n├── templates/     # Output templates\n└── scripts/       # Project scripts\n```\n\n### Memory File Selection\n\n| Category | Files | Selection |\n|----------|-------|-----------|\n| Universal | constitution.md, documentation.md, git-cicd.md, security.md, testing.md | Always |\n| TypeScript/JS | typescript.md | If TS/JS detected |\n| React/Next.js | react-nextjs.md | If React/Next detected |\n| Tailwind | tailwind-shadcn.md | If Tailwind detected |\n| Python | python.md | If Python detected |\n| Rust | rust.md | If Rust detected |\n\n### Options\n\n```\nOptions:\n1. Accept recommended selection\n2. Add additional memory files\n3. Remove memory files from selection\n4. Override detected stack manually\n```\n\nSee `references/command-workflows/init-workflow.md` for detailed workflow.\n\n---\n\n## Command: plan\n\nCreate implementation plans from specification files. Hierarchical for complex/multi-domain specs.\n\n### Trigger\n- `/speckit.plan`\n- `/speckit.plan spec.md`\n- `/speckit.plan --all`\n\n### Workflow\n\n1. **Locate specs** - Find spec files in .claude/resources/\n2. **Assess complexity** - Single domain vs multi-domain\n3. **Generate plans** - Create plan.md (and domain plans if complex)\n4. **Validate** - Check plan completeness and consistency\n\n### Output Structure\n\n**Simple (single domain)**:\n```\n.claude/resources/\n├── spec.md\n└── plan.md\n```\n\n**Complex (multi-domain)**:\n```\n.claude/resources/\n├── spec.md\n├── plan.md              # Master plan with domain references\n└── plans/\n    ├── domain-a-plan.md\n    ├── domain-b-plan.md\n    └── domain-c-plan.md\n```\n\n### Plan Content\n\nPlans contain:\n- Requirements mapping (which spec sections covered)\n- Architecture decisions\n- Implementation approach (phases, NOT tasks)\n- Verification strategy\n- Notes for task generation\n\nPlans do NOT contain:\n- Individual tasks (that's /speckit.tasks)\n- Implementation code\n- Detailed how-to instructions\n\n### Complexity Detection\n\n| Indicator | Simple | Complex |\n|-----------|--------|---------|\n| Domains | Single | Multiple distinct |\n| Page count | <10 pages | >10 pages |\n| Stakeholder count | 1-2 | 3+ |\n\nUser can override detection.\n\nSee `references/command-workflows/plan-workflow.md` for detailed workflow.\n\n---\n\n## Command: tasks\n\nGenerate implementation tasks from plans + constitution + memory files.\n\n### Trigger\n- `/speckit.tasks`\n- `/speckit.tasks plan.md`\n- `/speckit.tasks --all`\n\n### Workflow\n\n1. **Load plan(s)** - Read plan files\n2. **Load constitution** - Extract relevant sections\n3. **Load memory files** - Get tech-specific guidelines\n4. **Generate tasks** - Create *-tasks.md with phases\n5. **Validate** - Check task completeness\n\n### Output\n\n```markdown\n# [Domain] Tasks\n\n## Phase 1: Foundation\n\n### TASK-001: [Title]\n**Status**: PENDING\n**Priority**: P1\n**Constitution Sections**: §4.1, §4.2\n**Memory Files**: typescript.md, git-cicd.md\n**Plan Reference**: PLAN-001\n**Description**: ...\n**Acceptance Criteria**:\n- [ ] Criterion 1\n- [ ] Criterion 2\n```\n\n### Task Statuses\n\n| Status | Meaning |\n|--------|---------|\n| PENDING | Not started |\n| IN_PROGRESS | Currently being worked |\n| BLOCKED | Waiting on dependency |\n| COMPLETED | Done and verified |\n| SKIPPED | Intentionally not done |\n\nSee `references/command-workflows/tasks-workflow.md` for detailed workflow.\n\n---\n\n## Command: analyze\n\nDeterministic, read-only audit of project artifacts for consistency and completeness.\n\n### Trigger\n- `/speckit.analyze`\n- `/speckit.analyze --verbose`\n- `/speckit.analyze --category gaps`\n\n### Characteristics\n\n- **Read-only**: Never modifies files\n- **Deterministic**: Same inputs = same outputs\n- **Stable IDs**: Finding IDs remain stable across runs\n- **Quantified**: Metrics for coverage, completeness\n\n### Analysis Categories\n\n| Category | Description |\n|----------|-------------|\n| GAPS | Missing required elements |\n| INCONSISTENCIES | Contradictions between artifacts |\n| AMBIGUITIES | Unclear or undefined items |\n| ORPHANS | Unreferenced elements |\n| ASSUMPTIONS | Untracked/unvalidated assumptions |\n\n### Severity Levels\n\n| Level | Meaning |\n|-------|---------|\n| CRITICAL | Blocks progress, must fix |\n| HIGH | Significant risk, should fix |\n| MEDIUM | Notable issue, plan to fix |\n| LOW | Minor concern |\n\n### Output Format\n\n```markdown\n# Analysis Report\n\nGenerated: [timestamp]\nArtifacts analyzed: [count]\n\n## Summary\n| Category | Critical | High | Medium | Low |\n|----------|----------|------|--------|-----|\n| GAPS     | 2        | 3    | 5      | 1   |\n| ...      |          |      |        |     |\n\n## Findings\n\n### GAP-001 [CRITICAL]\n**Location**: spec.md:45\n**Description**: Missing error handling specification\n**Recommendation**: Define error states for API failures\n```\n\nSee `references/command-workflows/analyze-workflow.md` for detailed workflow.\n\n---\n\n## Command: clarify\n\nStructured ambiguity resolution with immediate spec updates.\n\n### Trigger\n- `/speckit.clarify`\n- `/speckit.clarify spec.md`\n\n### Characteristics\n\n- **One question at a time**: Focused, manageable\n- **Multiple choice or short phrase**: Quick answers\n- **5-question maximum per session**: Avoid fatigue\n- **Immediate updates**: Specs updated after each answer\n- **9-category taxonomy**: Structured classification\n\n### Ambiguity Categories\n\n| Category | Example Question |\n|----------|-----------------|\n| SCOPE | \"Should X include Y functionality?\" |\n| BEHAVIOR | \"What happens when user does X?\" |\n| DATA | \"What format should X be stored in?\" |\n| ERROR | \"How should X error be handled?\" |\n| SEQUENCE | \"Does X happen before or after Y?\" |\n| CONSTRAINT | \"What are the limits for X?\" |\n| INTERFACE | \"How does X communicate with Y?\" |\n| AUTHORITY | \"Who approves X?\" |\n| TEMPORAL | \"How long should X take?\" |\n\n### Workflow\n\n1. **Scan for ambiguity** - Find [TBD], [NEEDS CLARIFICATION], vague language\n2. **Prioritize** - Rank by impact on implementation\n3. **Present question** - One at a time with options\n4. **Update spec** - Apply answer immediately\n5. **Log session** - Record Q&A for traceability\n\n### Question Format\n\n```\nCLARIFY-001 [BEHAVIOR]\n\nThe spec mentions \"user authentication\" but doesn't specify the method.\n\nWhich authentication method should be used?\n1. OAuth 2.0 with Google/GitHub (Recommended)\n2. Email/password with JWT\n3. Magic link (passwordless)\n4. Other (please specify)\n\nYour choice:\n```\n\nSee `references/command-workflows/clarify-workflow.md` for detailed workflow.\n\n---\n\n## Command: implement\n\nExecute tasks from *-tasks.md with batch+gates execution model.\n\n### Trigger\n- `/speckit.implement TASK-001`\n- `/speckit.implement TASK-001..TASK-005`\n- `/speckit.implement \"Phase 1\"`\n- `/speckit.implement @foundation`\n\n### Task Selection\n\n| Selector | Meaning |\n|----------|---------|\n| `TASK-001` | Single task |\n| `TASK-001..TASK-005` | Range of tasks |\n| `\"Phase 1\"` | All tasks in phase |\n| `@foundation` | All tasks with @foundation group |\n\n### Execution Model: Batch + Gates\n\n```\nExecute Phase 1 tasks\n    ↓\nGATE: \"Phase 1 complete. Review outputs?\"\n    ↓\n[User confirms]\n    ↓\nExecute Phase 2 tasks\n    ↓\nGATE: \"Phase 2 complete. Review outputs?\"\n    ...\n```\n\n### Workflow\n\nFor each task:\n1. **Load context** - Read referenced constitution sections + memory files\n2. **Present context** - Show agent the relevant guidelines\n3. **Execute** - Perform the task\n4. **Update status** - PENDING → IN_PROGRESS → COMPLETED\n5. **Verify** - Check acceptance criteria\n\n### Gate Behavior\n\nAt phase/group boundaries:\n```\nPhase [N] complete.\n\nTasks completed: [count]\nTasks failed: [count]\n\nOptions:\n1. Continue to Phase [N+1]\n2. Review completed work\n3. Re-run failed tasks\n4. Stop execution\n```\n\n### Context Loading\n\n```markdown\n## Execution Context for TASK-001\n\n### Constitution Requirements (§4.1, §4.2)\n[Extracted sections from constitution.md]\n\n### Memory File Guidelines\nFrom typescript.md:\n[Relevant sections]\n\nFrom git-cicd.md:\n[Relevant sections]\n\n### Task Details\n[Full task content]\n```\n\nSee `references/command-workflows/implement-workflow.md` for detailed workflow.\n\n---\n\n## Memory File System\n\nMemory files provide persistent guidelines that inform all commands.\n\n### Universal Files (Always Loaded)\n\n| File | Purpose |\n|------|---------|\n| constitution.md | Core principles, mandatory constraints |\n| documentation.md | Documentation standards |\n| git-cicd.md | Git workflow, CI/CD practices |\n| security.md | Security requirements |\n| testing.md | Testing strategies |\n\n### Tech-Specific Files (Loaded by Detection)\n\n| File | Triggers |\n|------|----------|\n| typescript.md | TypeScript, JavaScript, Node.js |\n| react-nextjs.md | React, Next.js |\n| tailwind-shadcn.md | Tailwind CSS, shadcn/ui |\n| python.md | Python, Django, Flask, FastAPI |\n| rust.md | Rust |\n\n### Constitution Section References\n\nTasks reference constitution sections by ID:\n- `§1.0` - Chapter reference\n- `§1.1` - Section reference\n- `§1.1.a` - Subsection reference\n\nExample task:\n```markdown\n**Constitution Sections**: §4.1 (Error Handling), §4.2 (Logging)\n```\n\n---\n\n## Idempotency\n\nAll commands are designed to be safe when run repeatedly.\n\n### Init Idempotency\n\n- Skips existing directories\n- Updates changed memory files only\n- Preserves project customizations\n\n### Plan Idempotency\n\n- Detects existing plans\n- Offers update or regenerate\n- Preserves manual edits with warning\n\n### Tasks Idempotency\n\n- Preserves task statuses\n- Adds new tasks for new plan items\n- Never removes manually added tasks\n\n### Analyze Idempotency\n\n- Read-only, always safe\n- Stable finding IDs across runs\n\n### Clarify Idempotency\n\n- Tracks answered questions\n- Skips already-clarified items\n- Session history preserved\n\n### Implement Idempotency\n\n- Skips COMPLETED tasks\n- Resumes from last position\n- Re-runnable for failed tasks\n\n---\n\n## Catching Up\n\nWhen running on an existing project:\n\n1. **Init detects state** - Finds what exists, what's missing\n2. **Commands offer catch-up** - \"Missing setup. Run init first?\"\n3. **Incremental updates** - Only process what's new\n4. **Never destructive** - No deletions without explicit request"
              }
            ]
          },
          {
            "name": "streaming-output",
            "description": "Handle streaming output patterns",
            "source": "./skills/streaming-output",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install streaming-output@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [
              {
                "name": "/stream.finalize",
                "description": null,
                "path": "skills/streaming-output/commands/stream.finalize.md",
                "frontmatter": null,
                "content": "Strip markers and validate completeness.\n\n## Usage\n`/stream.finalize <filepath> [--output <output-filepath>]`\n\n## Workflow\n1. Run full integrity check\n2. Verify all sections completed\n3. Verify all hashes valid\n4. Remove `SECTION_START` and `SECTION_END` markers\n5. Remove YAML frontmatter stream metadata\n6. Validate no incomplete markers remain\n7. Write to output file (or overwrite in place)\n\n## Script\n```bash\npython scripts/stream_cleanup.py report.md --output final_report.md\n```\n\n## Pre-finalize Validation\n```\nFinalize Check: report.md\n\nSections:\n  [x] introduction ✓\n  [x] background ✓\n  [x] analysis ✓\n  [x] recommendations ✓\n  [x] conclusion ✓\n\nAll sections complete: YES\nAll hashes valid: YES\nReady to finalize: YES\n\nFinalizing...\nOutput written to: final_report.md\nMarkers removed: 10\nLines in final document: 1,247\n```\n\n## If Validation Fails\n```\nFinalize Check: report.md\n\n⚠️  CANNOT FINALIZE - Issues detected:\n\n  [ ] recommendations - PENDING (not written)\n  [!] conclusion - CORRUPTED (hash mismatch)\n\nFix required before finalization:\n1. /stream.write recommendations\n2. /stream.repair conclusion\n```\n\n## Before/After\n**Before**:\n```markdown\n---\nstream_plan:\n  sections: [...]\n---\n<!-- SECTION_START: intro | hash:a1b2c3d4 -->\n## Introduction\nContent...\n<!-- SECTION_END: intro | hash:a1b2c3d4 -->\n```\n\n**After**:\n```markdown\n## Introduction\nContent...\n```\n\n## Pre-finalize Checklist\n- Run `/stream.status --verify` to confirm all sections complete\n- Check for any sections with hash mismatches\n- Address any corrupted sections with `/stream.repair` before finalizing\n"
              },
              {
                "name": "/stream.init",
                "description": null,
                "path": "skills/streaming-output/commands/stream.init.md",
                "frontmatter": null,
                "content": "Initialize an output file with a section plan for streaming long-form content.\n\n## Usage\n`/stream.init <filepath> --sections \"<comma-separated-list>\" [--template <template-name>]`\n\n## Workflow\n1. Create output file (or detect existing)\n2. Write header with section plan as YAML frontmatter\n3. Generate content hash placeholder for integrity checking\n4. Present checklist for tracking\n\n## Templates (optional)\n- `bspec` - 15-section B-SPEC structure\n- `report` - Standard report structure\n- `spec` - Generic specification structure\n\n## Example\n```bash\n# Standard initialization\n/stream.init report.md --sections \"introduction,background,analysis,recommendations,conclusion\"\n\n# B-SPEC template (pre-defined 15 sections)\n/stream.init b-spec-010.md --template bspec\n```\n\n## Output file structure\n```markdown\n---\nstream_plan:\n  version: \"2.0\"\n  sections:\n    - id: introduction\n      status: pending\n      hash: null\n    - id: background\n      status: pending\n      hash: null\n    - id: analysis\n      status: pending\n      hash: null\n  created: 2024-01-15T10:30:00\n  last_modified: null\n  integrity_check: true\n---\n\n# Report\n\n<!-- Content will be streamed below -->\n```\n\n## Script\n```bash\npython scripts/stream_write.py init report.md \\\n  --sections \"introduction,background,analysis,recommendations,conclusion\"\n```\n\nAfter initialization, use `/stream.write` to write each section.\n"
              },
              {
                "name": "/stream.repair",
                "description": null,
                "path": "skills/streaming-output/commands/stream.repair.md",
                "frontmatter": null,
                "content": "Fix corrupted or partial sections.\n\n## Usage\n`/stream.repair <filepath> <section-id> [--strategy <strategy>]`\n\n## Strategies\n- `remove` (default): Remove partial content, reset section to pending\n- `complete`: Attempt to add missing END marker (use with caution)\n- `backup`: Create backup before repair\n\n## Workflow\n1. Create backup of current file (if --strategy backup)\n2. Locate corrupted section\n3. Remove content from `SECTION_START` to end of partial content\n4. Update section status to `pending`\n5. Report repair results\n\n## Script\n```bash\npython scripts/stream_repair.py report.md analysis --strategy remove\n```\n\n## Output\n```\nRepair Report: report.md\n\nSection: analysis\nIssue: Orphaned SECTION_START (no SECTION_END found)\nStrategy: remove\nAction: Removed 847 characters of partial content\n\nBefore:\n  <!-- SECTION_START: analysis | hash:null -->\n  ## Analysis\n\n  Partial content here...\n  [truncated]\n\nAfter:\n  Section 'analysis' reset to pending status\n\nBackup created: report.md.backup.20240115-103045\n\nReady to regenerate: /stream.write analysis\n```\n\n## When to Use\n- `/stream.status --verify` detects orphaned SECTION_START markers\n- Hash mismatch between START and END markers\n- Empty section content detected\n- After context compaction truncated a write operation\n\n## After Repair\nRun `/stream.write <section-id>` to regenerate the repaired section.\n"
              },
              {
                "name": "/stream.resume",
                "description": null,
                "path": "skills/streaming-output/commands/stream.resume.md",
                "frontmatter": null,
                "content": "Continue writing from the last incomplete section.\n\n## Usage\n`/stream.resume <filepath>`\n\n## Workflow\n1. Run status with verification to find resume point\n2. Check for corrupted sections (repair if needed)\n3. Read existing content for context\n4. Continue with `/stream.write` for next pending section\n\n## Script\n```bash\npython scripts/stream_status.py report.md --resume\n```\n\n## Output\n```\nResume Point: report.md\n\nLast completed: background\nNext pending: analysis\n\nContext from previous sections loaded (2,450 tokens)\nReady to write: analysis\n\nCommand: /stream.write analysis\n```\n\n## If Corruption Detected\n```\nResume Point: report.md\n\n⚠️  CORRUPTION DETECTED\nSection 'analysis' has incomplete markers.\n\nRecommended action:\n1. Run `/stream.repair analysis` to remove partial content\n2. Then run `/stream.write analysis` to regenerate\n\nCommand: /stream.repair analysis\n```\n\n## Recovery Scenarios\n\n### Scenario 1: Context limit hit mid-section\nThe section has `SECTION_START` but no `SECTION_END`:\n\n**Recovery**:\n1. `/stream.status --verify` detects incomplete section\n2. `/stream.repair analysis` removes partial content\n3. `/stream.write analysis` regenerates the section\n\n### Scenario 2: Session ended between sections\nAll written sections have both markers.\n\n**Recovery**:\n1. `/stream.resume` finds next pending section\n2. Continue with `/stream.write`\n"
              },
              {
                "name": "/stream.status",
                "description": null,
                "path": "skills/streaming-output/commands/stream.status.md",
                "frontmatter": null,
                "content": "Show current progress, identify resume point, and check integrity.\n\n## Usage\n`/stream.status <filepath> [--verify]`\n\n## Script\n```bash\npython scripts/stream_status.py report.md\npython scripts/stream_status.py report.md --verify  # Full integrity check\n```\n\n## Standard Output\n```\nStream Status: report.md\n\nSections:\n  [x] introduction (completed) ✓\n  [x] background (completed) ✓\n  [ ] analysis (pending) <- RESUME HERE\n  [ ] recommendations (pending)\n  [ ] conclusion (pending)\n\nProgress: 2/5 sections (40%)\nNext section: analysis\n```\n\n## With --verify flag (integrity check)\n```\nStream Status: report.md\n\nIntegrity Check:\n  [x] introduction - hash:a1b2c3d4 ✓ valid\n  [x] background - hash:e5f6g7h8 ✓ valid\n  [!] analysis - CORRUPTED (START without END)\n  [ ] recommendations - pending\n  [ ] conclusion - pending\n\n⚠️  CORRUPTION DETECTED in section: analysis\n    Run `/stream.repair analysis` to fix\n\nProgress: 2/5 sections (40%)\nNext section: analysis (requires repair)\n```\n\n## Corruption Detection\nThe status command detects:\n- **Orphaned START**: `SECTION_START` exists without matching `SECTION_END`\n- **Hash mismatch**: START and END marker hashes don't match\n- **Empty section**: Markers exist but no content between them\n- **Duplicate sections**: Same section ID appears multiple times\n\n## Use Cases\n- Check progress after interruption\n- Verify all sections completed before finalizing\n- Identify which section to resume from\n- Detect corruption with `--verify` flag\n"
              },
              {
                "name": "/stream.write",
                "description": null,
                "path": "skills/streaming-output/commands/stream.write.md",
                "frontmatter": null,
                "content": "Write a single section to the file with markers and integrity verification.\n\n## Usage\n`/stream.write <section-id>`\n\n## Workflow\n1. Check section exists in plan and is pending\n2. Generate content for section\n3. Compute content hash\n4. Write to temporary location first\n5. Validate write completed (check for SECTION_END marker)\n6. Append to main file with `SECTION_START` and `SECTION_END` markers\n7. Update section status to `completed` with hash\n\n## Script\n```bash\npython scripts/stream_write.py write report.md introduction \"Your content here...\"\n```\n\n## Markers in file\n```markdown\n<!-- SECTION_START: introduction | hash:a1b2c3d4 -->\n## Introduction\n\nYour introduction content here...\n\n<!-- SECTION_END: introduction | hash:a1b2c3d4 -->\n```\n\n## Write Verification\nAfter each write, the skill automatically verifies:\n1. `SECTION_START` marker exists\n2. `SECTION_END` marker exists\n3. Hashes in both markers match\n4. Content between markers is non-empty\n\nIf verification fails, the write is flagged and `/stream.repair` is recommended.\n\n## Important\n- Write ONE section at a time\n- Verify success before proceeding to next section\n- Hash in START and END markers must match (integrity check)\n- If interrupted mid-section, run `/stream.status --verify` to detect corruption\n"
              }
            ],
            "skills": [
              {
                "name": "streaming-output",
                "description": "Stream long-form content to markdown files with resume capability. Writes content incrementally with section markers, enabling recovery if context limits are hit. Use when generating long documents (over 1000 lines), B-SPEC or specification writing, multi-section reports, any task where context compaction may occur mid-generation, or when user explicitly requests streaming output. Commands: init, write, status, resume, finalize, repair.\n",
                "path": "skills/streaming-output/SKILL.md",
                "frontmatter": {
                  "name": "streaming-output",
                  "description": "Stream long-form content to markdown files with resume capability. Writes content incrementally with section markers, enabling recovery if context limits are hit. Use when generating long documents (over 1000 lines), B-SPEC or specification writing, multi-section reports, any task where context compaction may occur mid-generation, or when user explicitly requests streaming output. Commands: init, write, status, resume, finalize, repair.\n"
                },
                "content": "# Streaming Output\n\nWrite long-form content incrementally to markdown files with automatic resume capability. If context limits are hit mid-generation, work persists and can be continued.\n\n## ⚠️ MANDATORY USE CASES\n\n**ALWAYS use this skill when:**\n- Generating B-SPEC documents (typically 1,500-4,000 lines)\n- Writing any document expected to exceed 1,000 lines\n- Creating multi-section specifications or reports\n- Context compaction has already occurred in the conversation\n- The continuation prompt mentions streaming-output\n\n**DO NOT use manual heredoc appends (`cat >> file << 'EOF'`) for long documents.** This pattern fails silently when context compaction occurs mid-write, causing content corruption that is difficult to detect and repair.\n\n---\n\n## Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/stream.init` | Initialize output file with section plan |\n| `/stream.write` | Write next section to file |\n| `/stream.status` | Show progress, detect resume point, check integrity |\n| `/stream.resume` | Continue from last completed section |\n| `/stream.repair` | Fix corrupted/partial sections |\n| `/stream.finalize` | Strip markers, validate completeness |\n\n---\n\n## Quick Start\n\n```bash\n# 1. Initialize with a plan\n/stream.init report.md --sections \"intro,methodology,findings,conclusion\"\n\n# 2. Write sections (repeat for each)\n/stream.write intro\n/stream.write methodology\n# ... if interrupted, resume later with:\n/stream.resume\n\n# 3. Finalize when complete\n/stream.finalize\n```\n\n---\n\n## /stream.init\n\nInitialize an output file with a section plan.\n\n**Usage**: `/stream.init <filepath> --sections \"<comma-separated-list>\" [--template <template-name>]`\n\n**Workflow**:\n1. Create output file (or detect existing)\n2. Write header with section plan as YAML frontmatter\n3. Generate content hash placeholder for integrity checking\n4. Present checklist for tracking\n\n**Templates** (optional):\n- `bspec` - 15-section B-SPEC structure\n- `report` - Standard report structure\n- `spec` - Generic specification structure\n\n**Example**:\n```bash\n# Standard initialization\npython scripts/stream_write.py init report.md \\\n  --sections \"introduction,background,analysis,recommendations,conclusion\"\n\n# B-SPEC template (pre-defined 15 sections)\npython scripts/stream_write.py init b-spec-010.md --template bspec\n```\n\n**Output file structure**:\n```markdown\n---\nstream_plan:\n  version: \"2.0\"\n  sections:\n    - id: introduction\n      status: pending\n      hash: null\n    - id: background\n      status: pending\n      hash: null\n    - id: analysis\n      status: pending\n      hash: null\n  created: 2024-01-15T10:30:00\n  last_modified: null\n  integrity_check: true\n---\n\n# Report\n\n<!-- Content will be streamed below -->\n```\n\n---\n\n## /stream.write\n\nWrite a single section to the file with markers and integrity verification.\n\n**Usage**: `/stream.write <section-id>`\n\n**Workflow**:\n1. Check section exists in plan and is pending\n2. Generate content for section\n3. Compute content hash\n4. Write to temporary location first\n5. Validate write completed (check for SECTION_END marker)\n6. Append to main file with `SECTION_START` and `SECTION_END` markers\n7. Update section status to `completed` with hash\n\n**Script**:\n```bash\npython scripts/stream_write.py write report.md introduction \"Your content here...\"\n```\n\n**Markers in file**:\n```markdown\n<!-- SECTION_START: introduction | hash:a1b2c3d4 -->\n## Introduction\n\nYour introduction content here...\n\n<!-- SECTION_END: introduction | hash:a1b2c3d4 -->\n```\n\n**Important**: \n- Write ONE section at a time\n- Verify success before proceeding\n- Hash in START and END markers must match (integrity check)\n\n### Write Verification\n\nAfter each write, the skill automatically verifies:\n1. `SECTION_START` marker exists\n2. `SECTION_END` marker exists\n3. Hashes in both markers match\n4. Content between markers is non-empty\n\nIf verification fails, the write is flagged and `/stream.repair` is recommended.\n\n---\n\n## /stream.status\n\nShow current progress, identify resume point, and check integrity.\n\n**Usage**: `/stream.status <filepath> [--verify]`\n\n**Script**:\n```bash\npython scripts/stream_status.py report.md\npython scripts/stream_status.py report.md --verify  # Full integrity check\n```\n\n**Standard Output**:\n```\nStream Status: report.md\n\nSections:\n  [x] introduction (completed) ✓\n  [x] background (completed) ✓\n  [ ] analysis (pending) <- RESUME HERE\n  [ ] recommendations (pending)\n  [ ] conclusion (pending)\n\nProgress: 2/5 sections (40%)\nNext section: analysis\n```\n\n**With --verify flag (integrity check)**:\n```\nStream Status: report.md\n\nIntegrity Check:\n  [x] introduction - hash:a1b2c3d4 ✓ valid\n  [x] background - hash:e5f6g7h8 ✓ valid\n  [!] analysis - CORRUPTED (START without END)\n  [ ] recommendations - pending\n  [ ] conclusion - pending\n\n⚠️  CORRUPTION DETECTED in section: analysis\n    Run `/stream.repair analysis` to fix\n\nProgress: 2/5 sections (40%)\nNext section: analysis (requires repair)\n```\n\n### Corruption Detection\n\nThe status command detects:\n- **Orphaned START**: `SECTION_START` exists without matching `SECTION_END`\n- **Hash mismatch**: START and END marker hashes don't match\n- **Empty section**: Markers exist but no content between them\n- **Duplicate sections**: Same section ID appears multiple times\n\n---\n\n## /stream.resume\n\nContinue writing from the last incomplete section.\n\n**Usage**: `/stream.resume <filepath>`\n\n**Workflow**:\n1. Run status with verification to find resume point\n2. Check for corrupted sections (repair if needed)\n3. Read existing content for context\n4. Continue with `/stream.write` for next pending section\n\n**Script**:\n```bash\npython scripts/stream_status.py report.md --resume\n```\n\n**Output**:\n```\nResume Point: report.md\n\nLast completed: background\nNext pending: analysis\n\nContext from previous sections loaded (2,450 tokens)\nReady to write: analysis\n\nCommand: /stream.write analysis\n```\n\nIf corruption is detected:\n```\nResume Point: report.md\n\n⚠️  CORRUPTION DETECTED\nSection 'analysis' has incomplete markers.\n\nRecommended action:\n1. Run `/stream.repair analysis` to remove partial content\n2. Then run `/stream.write analysis` to regenerate\n\nCommand: /stream.repair analysis\n```\n\n---\n\n## /stream.repair\n\nFix corrupted or partial sections.\n\n**Usage**: `/stream.repair <filepath> <section-id> [--strategy <strategy>]`\n\n**Strategies**:\n- `remove` (default): Remove partial content, reset section to pending\n- `complete`: Attempt to add missing END marker (use with caution)\n- `backup`: Create backup before repair\n\n**Workflow**:\n1. Create backup of current file (if --strategy backup)\n2. Locate corrupted section\n3. Remove content from `SECTION_START` to end of partial content\n4. Update section status to `pending`\n5. Report repair results\n\n**Script**:\n```bash\npython scripts/stream_repair.py report.md analysis --strategy remove\n```\n\n**Output**:\n```\nRepair Report: report.md\n\nSection: analysis\nIssue: Orphaned SECTION_START (no SECTION_END found)\nStrategy: remove\nAction: Removed 847 characters of partial content\n\nBefore: \n  <!-- SECTION_START: analysis | hash:null -->\n  ## Analysis\n  \n  Partial content here...\n  [truncated]\n\nAfter:\n  Section 'analysis' reset to pending status\n\nBackup created: report.md.backup.20240115-103045\n\nReady to regenerate: /stream.write analysis\n```\n\n---\n\n## /stream.finalize\n\nStrip markers and validate completeness.\n\n**Usage**: `/stream.finalize <filepath> [--output <output-filepath>]`\n\n**Workflow**:\n1. Run full integrity check\n2. Verify all sections completed\n3. Verify all hashes valid\n4. Remove `SECTION_START` and `SECTION_END` markers\n5. Remove YAML frontmatter stream metadata\n6. Validate no incomplete markers remain\n7. Write to output file (or overwrite in place)\n\n**Script**:\n```bash\npython scripts/stream_cleanup.py report.md --output final_report.md\n```\n\n**Pre-finalize validation**:\n```\nFinalize Check: report.md\n\nSections:\n  [x] introduction ✓\n  [x] background ✓\n  [x] analysis ✓\n  [x] recommendations ✓\n  [x] conclusion ✓\n\nAll sections complete: YES\nAll hashes valid: YES\nReady to finalize: YES\n\nFinalizing...\nOutput written to: final_report.md\nMarkers removed: 10\nLines in final document: 1,247\n```\n\n**If validation fails**:\n```\nFinalize Check: report.md\n\n⚠️  CANNOT FINALIZE - Issues detected:\n\n  [ ] recommendations - PENDING (not written)\n  [!] conclusion - CORRUPTED (hash mismatch)\n\nFix required before finalization:\n1. /stream.write recommendations\n2. /stream.repair conclusion\n```\n\n---\n\n## Recovery Scenarios\n\n### Scenario 1: Context limit hit mid-section\n\nThe section has `SECTION_START` but no `SECTION_END`:\n```markdown\n<!-- SECTION_START: analysis | hash:null -->\n## Analysis\n\nPartial content here...\n[truncated - no SECTION_END]\n```\n\n**Recovery**:\n1. `/stream.status --verify` detects incomplete section\n2. `/stream.repair analysis` removes partial content **and preserves it as context**\n3. `/stream.status` shows the preserved context file with preview\n4. `/stream.write analysis` **continues** from where interrupted (don't restart!)\n\n**Context Preservation**:\nWhen repair runs, incomplete content is saved to a `.context` file:\n```\nreport.analysis.context  # Contains the incomplete content\n```\n\nThis allows Claude to:\n- Review what was being written\n- Continue coherently from where interrupted\n- Not waste tokens regenerating already-written content\n\n### Scenario 2: Session ended between sections\n\nAll written sections have both markers:\n```markdown\n<!-- SECTION_END: background | hash:e5f6g7h8 -->\n```\n\n**Recovery**:\n1. `/stream.resume` finds next pending section\n2. Continue with `/stream.write`\n\n### Scenario 3: Hash mismatch detected\n\nSTART and END hashes don't match (content corrupted during write):\n```markdown\n<!-- SECTION_START: analysis | hash:a1b2c3d4 -->\n...\n<!-- SECTION_END: analysis | hash:x9y8z7w6 -->\n```\n\n**Recovery**:\n1. `/stream.status --verify` flags hash mismatch\n2. `/stream.repair analysis --strategy remove`\n3. `/stream.write analysis` regenerates\n\n### Scenario 4: Context compaction during heredoc append (what to avoid)\n\n**Problem**: Using `cat >> file << 'EOF'` without streaming-output markers:\n```bash\n# DON'T DO THIS for long documents\ncat >> spec.md << 'EOF'\n## Section 12\nContent...\nEOF\n```\n\nIf context compaction occurs, the heredoc may be split, causing:\n- Partial content written\n- No markers to detect corruption\n- No way to identify resume point\n\n**Prevention**: Always use streaming-output for documents >1000 lines.\n\n---\n\n## B-SPEC Template\n\nFor B-SPEC documents, use the built-in template:\n\n```bash\n/stream.init b-spec-010-llm-integration.md --template bspec\n```\n\nThis creates the standard 15-section structure:\n\n```yaml\nstream_plan:\n  version: \"2.0\"\n  template: bspec\n  sections:\n    - id: overview\n      status: pending\n      hash: null\n    - id: architecture\n      status: pending\n      hash: null\n    - id: section-03  # Domain-specific\n      status: pending\n      hash: null\n    - id: section-04\n      status: pending\n      hash: null\n    - id: section-05\n      status: pending\n      hash: null\n    - id: section-06\n      status: pending\n      hash: null\n    - id: section-07\n      status: pending\n      hash: null\n    - id: section-08\n      status: pending\n      hash: null\n    - id: section-09\n      status: pending\n      hash: null\n    - id: section-10\n      status: pending\n      hash: null\n    - id: section-11\n      status: pending\n      hash: null\n    - id: database-schema\n      status: pending\n      hash: null\n    - id: api-specification\n      status: pending\n      hash: null\n    - id: implementation-guide\n      status: pending\n      hash: null\n    - id: appendices\n      status: pending\n      hash: null\n```\n\n---\n\n## Workflow Checklist\n\nCopy and track progress:\n\n```\nStream Progress: [document-name]\n- [ ] Initialize file with section plan\n- [ ] Write each section (one at a time):\n  - [ ] Section 1: ___________\n  - [ ] Section 2: ___________\n  - [ ] Section 3: ___________\n  - [ ] ...\n- [ ] Run status --verify to check integrity\n- [ ] Repair any corrupted sections\n- [ ] Finalize to strip markers\n```\n\n---\n\n## Context-Aware Resume\n\nWhen context compaction interrupts a section write, the system preserves your work:\n\n### Automatic Context Preservation\n\n1. **During repair**: Incomplete content is saved to `<file>.<section>.context`\n2. **On status check**: Context files are detected and previewed\n3. **When resuming**: Review the context file to continue coherently\n\n### Resume Workflow\n\n```bash\n# 1. Check status and see context\npython scripts/stream_status.py report.md\n\n# Output includes:\n# 📝 Context files (from previous incomplete writes):\n#    analysis: report.analysis.context (2847 bytes)\n#    Preview (last 500 chars):\n#    | The normalization formula uses...\n#    | This ensures that all criteria...\n\n# 2. Read the full context if needed\ncat report.analysis.context\n\n# 3. Continue writing (DON'T restart from scratch)\n# Your content should pick up where the context file ended\n```\n\n### Important: Continue, Don't Restart\n\nWhen a context file exists:\n- **DO**: Read it, understand where you left off, continue from that point\n- **DON'T**: Ignore it and regenerate the entire section from scratch\n\nThis saves significant tokens and maintains coherence.\n\n---\n\n## Best Practices\n\n1. **Plan sections upfront**: Define all sections before starting\n2. **One section at a time**: Write, verify, then proceed\n3. **Check status often**: Especially after interruptions or compaction\n4. **Keep sections reasonable**: 500-2000 words per section works well\n5. **Save context**: Don't hold generated content in memory; write immediately\n6. **Use --verify flag**: Run `/stream.status --verify` after resuming\n7. **Never skip repair**: If corruption is detected, repair before continuing\n8. **Use templates**: For known document types like B-SPECs, use built-in templates\n9. **Review context files**: When resuming, always check for and use `.context` files\n\n---\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| Manual heredoc appends | No corruption detection | Use streaming-output |\n| Skipping status checks | Miss corruption | Run status after each write |\n| Ignoring hash mismatches | Propagate bad content | Repair immediately |\n| Writing multiple sections at once | Can't identify failure point | One section at a time |\n| Not using templates | Inconsistent structure | Use bspec template |\n| Finalizing without verify | May include corrupted content | Always --verify first |\n\n---\n\n## Integration with Continuation Prompts\n\nWhen creating continuation prompts for long document generation:\n\n1. **Explicitly state**: \"Use streaming-output skill for all document generation\"\n2. **Include status command**: \"Run `/stream.status <file> --verify` to check progress\"\n3. **Document section plan**: List all sections and their status\n4. **Specify resume point**: \"Resume from section: X\"\n\nExample continuation prompt snippet:\n```markdown\n## Document Generation Status\n\nUsing streaming-output skill for B-SPEC-010.\n\nCurrent status:\n- Sections 1-5: Complete ✓\n- Section 6: Pending (resume here)\n- Sections 7-15: Pending\n\nResume command: `/stream.resume b-spec-010.md`\n```\n\n---\n\n## Scripts Reference\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `stream_write.py` | Init and write operations | `python scripts/stream_write.py <command> <args>` |\n| `stream_status.py` | Progress, verification, and resume detection | `python scripts/stream_status.py <filepath> [--verify] [--resume]` |\n| `stream_repair.py` | Fix corrupted sections | `python scripts/stream_repair.py <filepath> <section> [--strategy]` |\n| `stream_cleanup.py` | Finalize and strip markers | `python scripts/stream_cleanup.py <filepath> [--output]` |\n\nRun any script with `--help` for detailed options.\n\n---\n\n## Changelog\n\n### v2.1 (2026-01-03)\n- **Context preservation on repair**: Incomplete content now saved to `.context` files\n- **Context-aware status**: Status command shows context files with previews\n- **Resume guidance**: Clear instructions to continue from context, not restart\n- Added \"Context-Aware Resume\" section with workflow guidance\n- Updated recovery scenarios to emphasize context preservation\n\n### v2.0 (2026-01-03)\n- Added `/stream.repair` command for fixing corrupted sections\n- Added hash-based integrity checking for section markers\n- Added `--verify` flag to status command\n- Added B-SPEC template (`--template bspec`)\n- Added corruption detection (orphaned START, hash mismatch, empty sections)\n- Added \"Mandatory Use Cases\" section\n- Added \"Anti-Patterns to Avoid\" section\n- Added \"Integration with Continuation Prompts\" guidance\n- Improved recovery scenarios with specific commands\n- Enhanced status output with integrity indicators\n\n### v1.0 (Original)\n- Initial streaming output implementation\n- Basic section markers\n- Status and finalize commands"
              }
            ]
          },
          {
            "name": "trade-study-analysis",
            "description": "Systematic trade study using DAU 9-Step Process",
            "source": "./skills/trade-study-analysis",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install trade-study-analysis@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "trade-study-analysis",
                "description": "Conduct systematic trade study analyses using the DAU 9-Step Trade Study Process. Guides engineers through problem definition, root cause analysis (5 Whys, Fishbone), data collection from alternatives and datasheets, normalization calculations, weighted scoring, sensitivity analysis, and professional report generation with visualizations and decision matrices. Use when evaluating alternatives, comparing solutions, conducting trade-offs, or making engineering decisions.",
                "path": "skills/trade-study-analysis/SKILL.md",
                "frontmatter": {
                  "name": "trade-study-analysis",
                  "description": "Conduct systematic trade study analyses using the DAU 9-Step Trade Study Process. Guides engineers through problem definition, root cause analysis (5 Whys, Fishbone), data collection from alternatives and datasheets, normalization calculations, weighted scoring, sensitivity analysis, and professional report generation with visualizations and decision matrices. Use when evaluating alternatives, comparing solutions, conducting trade-offs, or making engineering decisions."
                },
                "content": "# Trade Study Analysis Skill\n\nSystematic trade study analysis following the DAU 9-Step Trade Study Process with mandatory verification gates.\n\n## CRITICAL BEHAVIORAL REQUIREMENTS\n\n**This skill operates under strict guardrails. The assistant MUST:**\n\n### 1. NEVER Proceed Without Explicit User Confirmation\n- Ask clarifying questions at EVERY step before proceeding\n- Do NOT proceed based on assumed confidence levels\n- Wait for explicit user responses before moving forward\n- Present options and wait for selection\n\n### 2. NEVER Make Assumptions\n- All data must come from user-provided sources\n- If information is missing, ASK for it—do not infer\n- Flag any gaps explicitly: \"I need the following information: [list]\"\n- Never fill in placeholder values without explicit user approval\n\n### 3. ALL Outputs Must Be Source-Grounded\n- Every claim must reference a specific source document\n- Format: \"[Statement] (Source: [document name], [page/section])\"\n- If no source exists for a claim, state: \"UNGROUNDED—requires source\"\n- Maintain running source registry throughout analysis\n\n### 4. Mandatory Assumption Review Before Finalization\n- Present complete assumption summary before any final output\n- User must explicitly approve or correct each assumption\n- No report generation until assumptions are cleared\n\n### 5. Mandatory Diagram Selection\n- Before generating visualizations, present full diagram menu\n- User selects which diagrams to include\n- Do not auto-generate all diagrams\n\n---\n\n## Workflow Checklist\n\n```\nTrade Study Workflow (with mandatory gates):\n\n□ Step 1: Define Problem Statement\n  └─ GATE: User confirms problem statement text\n\n□ Step 2: Conduct Root Cause Analysis  \n  └─ GATE: User confirms root cause identification\n\n□ Step 3: SOURCE REGISTRATION\n  └─ GATE: User provides/confirms available sources\n\n□ Step 4: Collect Alternative Data\n  └─ GATE: User confirms alternatives list and data\n\n□ Step 5: Define Evaluation Criteria\n  └─ GATE: User confirms criteria definitions\n\n□ Step 6: Assign Criteria Weights\n  └─ GATE: User confirms weight assignments\n\n□ Step 7: Normalize Data\n  └─ GATE: User confirms normalization approach\n\n□ Step 8: Score Alternatives\n  └─ GATE: User confirms scoring method\n\n□ Step 9: Perform Sensitivity Analysis\n  └─ GATE: User confirms sensitivity parameters\n\n□ Step 10: ASSUMPTION REVIEW\n  └─ GATE: User approves all documented assumptions\n\n□ Step 11: SELECT OUTPUT DIAGRAMS\n  └─ GATE: User selects which visualizations to include\n\n□ Step 12: Generate Report\n  └─ GATE: User approves final report\n```\n\n---\n\n## Mandatory Interaction Templates\n\n### Source Registration (MUST execute before data collection)\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📚 SOURCE REGISTRATION\n═══════════════════════════════════════════════════════════════════════════════\n\nBefore proceeding, I need to establish the source documents for this analysis.\n\nAVAILABLE SOURCE TYPES:\n  [A] Product Datasheets (specifications, performance data)\n  [B] Test Reports (measured performance, validation results)  \n  [C] Prior Trade Studies (historical analyses, lessons learned)\n  [D] Requirements Documents (system requirements, constraints)\n  [E] Cost Estimates (pricing, TCO analyses)\n  [F] Standards/Specifications (compliance requirements)\n  [G] Expert Inputs (documented SME assessments)\n  [H] Other (describe)\n\nCURRENT REGISTERED SOURCES: [None]\n\n───────────────────────────────────────────────────────────────────────────────\nREQUIRED ACTIONS:\n\n1. Which source types do you have available? [List letters]\n\n2. Please provide/upload the source documents, OR describe each source:\n   - Document name:\n   - Document type:\n   - Date/version:\n   - Relevant sections:\n\n3. Are there sources you need but don't have? [Y/N]\n   If Y, I will flag data gaps requiring these sources.\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  I cannot proceed with analysis until sources are registered.\n    All analysis outputs will reference these registered sources.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### Clarifying Questions Template (MUST use at each step)\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n❓ CLARIFICATION REQUIRED — [STEP NAME]\n═══════════════════════════════════════════════════════════════════════════════\n\nBefore proceeding with [step description], I need clarification on:\n\nQUESTION 1: [Specific question]\n  Options (if applicable):\n    [A] [Option A]\n    [B] [Option B]\n    [C] Other (please specify)\n\nQUESTION 2: [Specific question]\n  Your input: _______________________________________________\n\nQUESTION 3: [Specific question]\n  Your input: _______________________________________________\n\n───────────────────────────────────────────────────────────────────────────────\n⚠️  I will NOT proceed until you respond to all questions above.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### Assumption Tracking Template\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📋 ASSUMPTION REGISTER — Entry #[N]\n═══════════════════════════════════════════════════════════════════════════════\n\nASSUMPTION ID: A-[XXX]\nCATEGORY: [Data/Methodology/Scope/Constraint/Source Interpretation]\nDESCRIPTION: [What is being assumed]\nREASON: [Why this assumption is necessary]\nSOURCE BASIS: [Source that partially supports this, or \"NONE—requires user input\"]\nIMPACT IF WRONG: [Low/Medium/High] — [Explanation]\nALTERNATIVES CONSIDERED: [Other options if assumption is invalid]\n\nSTATUS: □ Pending User Approval  □ Approved  □ Rejected  □ Modified\n\n───────────────────────────────────────────────────────────────────────────────\nYour response:\n  [A] Approve this assumption\n  [B] Reject—provide alternative: _________________________________\n  [C] Modify—specify change: _____________________________________\n  [D] Need more information before deciding\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### Diagram Selection Template (MUST execute before visualization)\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n📊 OUTPUT DIAGRAM SELECTION\n═══════════════════════════════════════════════════════════════════════════════\n\nSelect which diagrams to include in the final report.\n\nAVAILABLE DIAGRAMS:\n\n  DECISION ANALYSIS:\n    □ [1] Decision Matrix Heatmap — Shows all scores in colored grid\n    □ [2] Score Comparison Bar Chart — Horizontal bars by alternative\n    □ [3] Radar/Spider Chart — Multi-dimensional comparison overlay\n\n  WEIGHT ANALYSIS:\n    □ [4] Criteria Weight Pie Chart — Weight distribution\n    □ [5] Weight Comparison Bar Chart — Side-by-side weights\n\n  SENSITIVITY ANALYSIS:\n    □ [6] Tornado Diagram — Sensitivity ranking by parameter\n    □ [7] Monte Carlo Distribution — Win probability histogram\n    □ [8] Breakeven Analysis Chart — Threshold visualization\n\n  ROOT CAUSE ANALYSIS:\n    □ [9] Fishbone (Ishikawa) Diagram — Cause categories\n    □ [10] 5 Whys Chain Diagram — Causal chain visualization\n\n  DATA QUALITY:\n    □ [11] Source Coverage Matrix — Shows data grounding by criterion\n    □ [12] Confidence Level Heatmap — Data reliability indicators\n\n───────────────────────────────────────────────────────────────────────────────\nEnter diagram numbers to include (comma-separated): _______________\n\nExample: 1, 2, 4, 6, 11\n\n⚠️  I will ONLY generate the diagrams you select.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n### Pre-Finalization Assumption Summary (MUST execute before report)\n\n```\n═══════════════════════════════════════════════════════════════════════════════\n⚠️  MANDATORY ASSUMPTION REVIEW — FINALIZATION GATE\n═══════════════════════════════════════════════════════════════════════════════\n\nBefore generating the final report, review ALL assumptions made during analysis:\n\n───────────────────────────────────────────────────────────────────────────────\nASSUMPTION SUMMARY\n───────────────────────────────────────────────────────────────────────────────\n\nTOTAL ASSUMPTIONS: [N]\n  • Data Assumptions: [n]\n  • Methodology Assumptions: [n]  \n  • Scope Assumptions: [n]\n  • Source Interpretation Assumptions: [n]\n\n───────────────────────────────────────────────────────────────────────────────\nDETAILED ASSUMPTION LIST:\n───────────────────────────────────────────────────────────────────────────────\n\nA-001: [Description]\n       Source: [Source or \"User-provided\" or \"UNGROUNDED\"]\n       Status: [Approved/Pending]\n\nA-002: [Description]\n       Source: [Source or \"User-provided\" or \"UNGROUNDED\"]\n       Status: [Approved/Pending]\n\n[Continue for all assumptions...]\n\n───────────────────────────────────────────────────────────────────────────────\nUNGROUNDED CLAIMS (require source or removal):\n───────────────────────────────────────────────────────────────────────────────\n\n• [Claim 1] — Missing source for: [what's needed]\n• [Claim 2] — Missing source for: [what's needed]\n\n───────────────────────────────────────────────────────────────────────────────\nREQUIRED ACTIONS:\n───────────────────────────────────────────────────────────────────────────────\n\n1. Review each assumption above\n2. For PENDING assumptions, respond:\n   [A] Approve all pending assumptions\n   [B] Reject/modify specific assumptions (list IDs): _______________\n   [C] Provide additional sources to ground ungrounded claims\n\n3. Confirm you accept responsibility for approved assumptions: [Y/N]\n\n⛔ I CANNOT generate the final report until all assumptions are resolved.\n═══════════════════════════════════════════════════════════════════════════════\n```\n\n---\n\n## Workflow Navigation\n\n| Step | Description | Reference |\n|------|-------------|-----------|\n| 1-2 | Problem Definition & Root Cause | [PROBLEM_DEFINITION.md](workflow/PROBLEM_DEFINITION.md) |\n| 3 | Source Registration & Data Collection | [DATA_COLLECTION.md](workflow/DATA_COLLECTION.md) |\n| 4-6 | Criteria, Weighting & Normalization | [SCORING_WEIGHTING.md](workflow/SCORING_WEIGHTING.md) |\n| 7-8 | Scoring & Sensitivity | [SENSITIVITY.md](workflow/SENSITIVITY.md) |\n| 9-12 | Review & Report Generation | [REPORT_GENERATION.md](workflow/REPORT_GENERATION.md) |\n\n---\n\n## Guardrails Reference\n\n### Prohibited Behaviors\n\n| Prohibited | Required Instead |\n|------------|------------------|\n| Assuming missing data values | Ask user to provide or flag as gap |\n| Proceeding without confirmation | Wait for explicit user response |\n| Making subjective assessments | Request documented basis or SME input |\n| Auto-generating all diagrams | Present selection menu, await choice |\n| Finalizing with pending assumptions | Complete assumption review gate |\n| Ungrounded claims in report | Cite source or mark as \"UNGROUNDED\" |\n| Inferring user preferences | Ask explicit questions |\n| Skipping steps for efficiency | Execute all mandatory gates |\n\n### Source Citation Format\n\nAll analysis outputs must use this citation format:\n\n```\n[Statement or data point] \n  └─ Source: [Document Name], [Section/Page], [Date]\n  └─ Confidence: [High/Medium/Low based on source quality]\n```\n\nExample:\n```\nAlternative A provides 150 Mbps throughput\n  └─ Source: \"Vendor A Datasheet v2.3\", Section 4.2, 2024-03-15\n  └─ Confidence: High (manufacturer specification)\n```\n\n### Confidence Levels (for source quality)\n\n| Level | Definition | Source Types |\n|-------|------------|--------------|\n| High | Verified, authoritative | Test reports, certified specs, validated data |\n| Medium | Credible but unverified | Vendor datasheets, estimates, projections |\n| Low | Uncertain or incomplete | Expert opinion, extrapolations, partial data |\n| UNGROUNDED | No source available | Requires user input or exclusion |\n\n---\n\n## Core Scripts\n\nInstall dependencies:\n```bash\npip install pandas numpy matplotlib seaborn scipy openpyxl python-docx --break-system-packages\n```\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `problem_analyzer.py` | Validate problem statements | `python scripts/problem_analyzer.py \"statement\"` |\n| `five_whys.py` | Interactive 5 Whys analysis | `python scripts/five_whys.py --interactive` |\n| `fishbone.py` | Generate Fishbone diagrams | `python scripts/fishbone.py data.json -o diagram.png` |\n| `normalize.py` | Normalize raw data | `python scripts/normalize.py data.csv --method min-max` |\n| `score.py` | Calculate weighted scores | `python scripts/score.py normalized.csv weights.json` |\n| `sensitivity.py` | Sensitivity analysis | `python scripts/sensitivity.py scores.csv weights.json` |\n| `visualize.py` | Generate selected visualizations | `python scripts/visualize.py results.json --diagrams 1,2,4` |\n| `source_tracker.py` | Manage source registry | `python scripts/source_tracker.py --add \"source.pdf\"` |\n| `assumption_tracker.py` | Track and review assumptions | `python scripts/assumption_tracker.py --summary` |\n| `generate_report.py` | Create trade study report | `python scripts/generate_report.py study.json -o report.docx` |\n\n---\n\n## Data Models\n\n### Source Registry (sources.json)\n\n```json\n{\n  \"sources\": [\n    {\n      \"id\": \"SRC-001\",\n      \"name\": \"Vendor A Datasheet\",\n      \"type\": \"datasheet\",\n      \"version\": \"2.3\",\n      \"date\": \"2024-03-15\",\n      \"file_path\": \"sources/vendor_a_datasheet.pdf\",\n      \"relevant_sections\": [\"4.2 Performance\", \"5.1 Specifications\"],\n      \"confidence\": \"medium\",\n      \"notes\": \"Vendor-provided, not independently verified\"\n    }\n  ],\n  \"gaps\": [\n    {\n      \"description\": \"Cost data for Alternative C\",\n      \"required_for\": [\"criterion C2: Total Cost\"],\n      \"requested_source_type\": \"cost_estimate\"\n    }\n  ]\n}\n```\n\n### Assumption Registry (assumptions.json)\n\n```json\n{\n  \"assumptions\": [\n    {\n      \"id\": \"A-001\",\n      \"category\": \"data\",\n      \"description\": \"Vendor B throughput measured under similar conditions to Vendor A\",\n      \"reason\": \"Different test methodologies in datasheets\",\n      \"source_basis\": \"SRC-002, Section 3.1 (partial)\",\n      \"impact_if_wrong\": \"medium\",\n      \"impact_explanation\": \"Could affect throughput comparison by ±15%\",\n      \"status\": \"pending\",\n      \"user_response\": null,\n      \"timestamp\": \"2024-12-22T10:30:00Z\"\n    }\n  ],\n  \"summary\": {\n    \"total\": 5,\n    \"approved\": 3,\n    \"pending\": 2,\n    \"rejected\": 0\n  }\n}\n```\n\n### Diagram Selection Record (diagram_selection.json)\n\n```json\n{\n  \"selected_diagrams\": [1, 2, 4, 6, 11],\n  \"selection_timestamp\": \"2024-12-22T14:00:00Z\",\n  \"user_confirmed\": true,\n  \"diagrams\": {\n    \"1\": {\"name\": \"Decision Matrix Heatmap\", \"selected\": true},\n    \"2\": {\"name\": \"Score Comparison Bar Chart\", \"selected\": true},\n    \"3\": {\"name\": \"Radar Chart\", \"selected\": false},\n    \"4\": {\"name\": \"Weight Pie Chart\", \"selected\": true},\n    \"5\": {\"name\": \"Weight Bar Chart\", \"selected\": false},\n    \"6\": {\"name\": \"Tornado Diagram\", \"selected\": true},\n    \"7\": {\"name\": \"Monte Carlo Distribution\", \"selected\": false},\n    \"8\": {\"name\": \"Breakeven Chart\", \"selected\": false},\n    \"9\": {\"name\": \"Fishbone Diagram\", \"selected\": false},\n    \"10\": {\"name\": \"5 Whys Chain\", \"selected\": false},\n    \"11\": {\"name\": \"Source Coverage Matrix\", \"selected\": true},\n    \"12\": {\"name\": \"Confidence Heatmap\", \"selected\": false}\n  }\n}\n```\n\n---\n\n## Output Structure\n\n```\ntrade_study_output/\n├── sources/                 # Registered source documents\n│   └── source_registry.json\n├── assumptions/             # Assumption tracking\n│   └── assumption_registry.json\n├── analysis/                # Root cause analysis artifacts\n├── data/                    # Collected and processed data\n├── normalized/              # Normalized datasets  \n├── scores/                  # Scoring results\n├── sensitivity/             # Sensitivity analysis results\n├── visualizations/          # ONLY user-selected diagrams\n│   └── diagram_selection.json\n└── reports/                 # Final reports with source citations\n```\n\n---\n\n## Quality Gates Summary\n\n| Gate | Location | Blocking Condition |\n|------|----------|-------------------|\n| Source Registration | Before Step 3 | No sources registered |\n| Problem Statement | Step 1 | User hasn't confirmed text |\n| Root Cause | Step 2 | User hasn't confirmed cause |\n| Alternatives | Step 4 | User hasn't confirmed list |\n| Criteria | Step 5 | User hasn't confirmed definitions |\n| Weights | Step 6 | User hasn't confirmed assignments |\n| Normalization | Step 7 | User hasn't confirmed method |\n| Scoring | Step 8 | User hasn't confirmed approach |\n| Sensitivity | Step 9 | User hasn't confirmed parameters |\n| Assumption Review | Step 10 | Pending assumptions exist |\n| Diagram Selection | Step 11 | User hasn't selected diagrams |\n| Report Generation | Step 12 | Any prior gate incomplete |"
              }
            ]
          },
          {
            "name": "session-memory",
            "description": "Persistent session memory with searchable storage and checkpoints",
            "source": "./mcps/session-memory",
            "category": null,
            "version": null,
            "author": null,
            "install_commands": [
              "/plugin marketplace add ddunnock/claude-plugins",
              "/plugin install session-memory@dunnock-plugins"
            ],
            "signals": {
              "stars": 1,
              "forks": 0,
              "pushed_at": "2026-01-03T21:11:39Z",
              "created_at": "2026-01-02T13:33:56Z",
              "license": null
            },
            "commands": [],
            "skills": []
          }
        ]
      }
    }
  ]
}