{
  "owner": {
    "id": "JosiahSiegel",
    "display_name": "Josiah Siegel",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/5522990?v=4",
    "url": "https://github.com/JosiahSiegel",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 25,
      "total_commands": 69,
      "total_skills": 76,
      "total_stars": 5,
      "total_forks": 1
    }
  },
  "repos": [
    {
      "full_name": "JosiahSiegel/claude-plugin-marketplace",
      "url": "https://github.com/JosiahSiegel/claude-plugin-marketplace",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 5,
        "forks": 1,
        "pushed_at": "2026-01-10T20:58:53Z",
        "created_at": "2025-10-22T23:30:38Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 45427
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/commands/auto-improve-all.md",
          "type": "blob",
          "size": 15014
        },
        {
          "path": ".claude/settings.json",
          "type": "blob",
          "size": 27
        },
        {
          "path": ".claude/settings.local.json",
          "type": "blob",
          "size": 1764
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 694
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 8900
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 5678
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2825
        },
        {
          "path": "plugins/adf-master/AUTONOMOUS_IMPROVEMENTS_SUMMARY.md",
          "type": "blob",
          "size": 9574
        },
        {
          "path": "plugins/adf-master/README-addon.md",
          "type": "blob",
          "size": 2210
        },
        {
          "path": "plugins/adf-master/README.md",
          "type": "blob",
          "size": 27702
        },
        {
          "path": "plugins/adf-master/WINDOWS_GITBASH_IMPROVEMENTS.md",
          "type": "blob",
          "size": 9844
        },
        {
          "path": "plugins/adf-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/agents/adf-expert.md",
          "type": "blob",
          "size": 34256
        },
        {
          "path": "plugins/adf-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/commands/adf-create-pipeline.md",
          "type": "blob",
          "size": 3755
        },
        {
          "path": "plugins/adf-master/commands/adf-debug.md",
          "type": "blob",
          "size": 4790
        },
        {
          "path": "plugins/adf-master/commands/adf-expression.md",
          "type": "blob",
          "size": 5035
        },
        {
          "path": "plugins/adf-master/commands/adf-linked-service.md",
          "type": "blob",
          "size": 4643
        },
        {
          "path": "plugins/adf-master/commands/adf-validate.md",
          "type": "blob",
          "size": 2714
        },
        {
          "path": "plugins/adf-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/scripts/compare-adf-environments.ps1",
          "type": "blob",
          "size": 9873
        },
        {
          "path": "plugins/adf-master/scripts/export-pipeline-dependencies.ps1",
          "type": "blob",
          "size": 10703
        },
        {
          "path": "plugins/adf-master/scripts/generate-arm-parameters.ps1",
          "type": "blob",
          "size": 5862
        },
        {
          "path": "plugins/adf-master/scripts/validate-adf-pipelines.ps1",
          "type": "blob",
          "size": 24768
        },
        {
          "path": "plugins/adf-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/adf-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/adf-master/SKILL.md",
          "type": "blob",
          "size": 23918
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references/activity-types.md",
          "type": "blob",
          "size": 12097
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references/datasets.md",
          "type": "blob",
          "size": 10046
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references/expression-functions.md",
          "type": "blob",
          "size": 11733
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references/linked-services.md",
          "type": "blob",
          "size": 13065
        },
        {
          "path": "plugins/adf-master/skills/adf-master/references/triggers.md",
          "type": "blob",
          "size": 9362
        },
        {
          "path": "plugins/adf-master/skills/adf-validation-rules",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/adf-validation-rules/SKILL.md",
          "type": "blob",
          "size": 21689
        },
        {
          "path": "plugins/adf-master/skills/adf-validation-rules/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/adf-validation-rules/references/nesting-rules.md",
          "type": "blob",
          "size": 7404
        },
        {
          "path": "plugins/adf-master/skills/adf-validation-rules/references/resource-limits.md",
          "type": "blob",
          "size": 5440
        },
        {
          "path": "plugins/adf-master/skills/databricks-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/databricks-2025/SKILL.md",
          "type": "blob",
          "size": 22912
        },
        {
          "path": "plugins/adf-master/skills/fabric-onelake-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/fabric-onelake-2025/SKILL.md",
          "type": "blob",
          "size": 19500
        },
        {
          "path": "plugins/adf-master/skills/windows-git-bash-compatibility",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/adf-master/skills/windows-git-bash-compatibility/SKILL.md",
          "type": "blob",
          "size": 11935
        },
        {
          "path": "plugins/ado-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2732
        },
        {
          "path": "plugins/ado-master/CHANGELOG.md",
          "type": "blob",
          "size": 11528
        },
        {
          "path": "plugins/ado-master/README.md",
          "type": "blob",
          "size": 10783
        },
        {
          "path": "plugins/ado-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/agents/ado-expert.md",
          "type": "blob",
          "size": 12890
        },
        {
          "path": "plugins/ado-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/skills/ado-pipeline-best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/skills/ado-pipeline-best-practices/SKILL.md",
          "type": "blob",
          "size": 3506
        },
        {
          "path": "plugins/ado-master/skills/ado-windows-git-bash-compatibility",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/skills/ado-windows-git-bash-compatibility/SKILL.md",
          "type": "blob",
          "size": 14176
        },
        {
          "path": "plugins/ado-master/skills/defender-for-devops",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/skills/defender-for-devops/SKILL.md",
          "type": "blob",
          "size": 11372
        },
        {
          "path": "plugins/ado-master/skills/sprint-254-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ado-master/skills/sprint-254-features/SKILL.md",
          "type": "blob",
          "size": 14100
        },
        {
          "path": "plugins/azure-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1758
        },
        {
          "path": "plugins/azure-master/README.md",
          "type": "blob",
          "size": 14249
        },
        {
          "path": "plugins/azure-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/agents/azure-expert.md",
          "type": "blob",
          "size": 17860
        },
        {
          "path": "plugins/azure-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/aks-automatic-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/aks-automatic-2025/SKILL.md",
          "type": "blob",
          "size": 14628
        },
        {
          "path": "plugins/azure-master/skills/azure-openai-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/azure-openai-2025/SKILL.md",
          "type": "blob",
          "size": 17572
        },
        {
          "path": "plugins/azure-master/skills/azure-well-architected-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/azure-well-architected-framework/SKILL.md",
          "type": "blob",
          "size": 11151
        },
        {
          "path": "plugins/azure-master/skills/container-apps-gpu-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/container-apps-gpu-2025/SKILL.md",
          "type": "blob",
          "size": 15163
        },
        {
          "path": "plugins/azure-master/skills/deployment-stacks-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-master/skills/deployment-stacks-2025/SKILL.md",
          "type": "blob",
          "size": 18588
        },
        {
          "path": "plugins/azure-to-docker-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2188
        },
        {
          "path": "plugins/azure-to-docker-master/IMPROVEMENTS-v1.2.0.md",
          "type": "blob",
          "size": 5985
        },
        {
          "path": "plugins/azure-to-docker-master/README.md",
          "type": "blob",
          "size": 14325
        },
        {
          "path": "plugins/azure-to-docker-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/agents/azure-to-docker-expert.md",
          "type": "blob",
          "size": 8224
        },
        {
          "path": "plugins/azure-to-docker-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/commands/export-database.md",
          "type": "blob",
          "size": 16369
        },
        {
          "path": "plugins/azure-to-docker-master/commands/extract-infrastructure.md",
          "type": "blob",
          "size": 16147
        },
        {
          "path": "plugins/azure-to-docker-master/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/docs/AZURE-CLI-COMMANDS-REFERENCE.md",
          "type": "blob",
          "size": 27148
        },
        {
          "path": "plugins/azure-to-docker-master/docs/AZURE-EXTRACTION-SUMMARY.md",
          "type": "blob",
          "size": 15827
        },
        {
          "path": "plugins/azure-to-docker-master/docs/AZURE-TO-DOCKER-COMPLETE-GUIDE.md",
          "type": "blob",
          "size": 23205
        },
        {
          "path": "plugins/azure-to-docker-master/docs/AZURE-TO-DOCKER-INDEX.md",
          "type": "blob",
          "size": 16273
        },
        {
          "path": "plugins/azure-to-docker-master/docs/AZURE-TO-DOCKER-QUICKSTART.md",
          "type": "blob",
          "size": 8902
        },
        {
          "path": "plugins/azure-to-docker-master/docs/EXAMPLE-COMPLETE-WORKFLOW.md",
          "type": "blob",
          "size": 21827
        },
        {
          "path": "plugins/azure-to-docker-master/docs/README-AZURE-DOCKER-TOOLKIT.md",
          "type": "blob",
          "size": 13821
        },
        {
          "path": "plugins/azure-to-docker-master/docs/WINDOWS-GIT-BASH-GUIDE.md",
          "type": "blob",
          "size": 8827
        },
        {
          "path": "plugins/azure-to-docker-master/docs/azure-to-docker-compose-guide.md",
          "type": "blob",
          "size": 44759
        },
        {
          "path": "plugins/azure-to-docker-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/scripts/azure-infrastructure-extractor.ps1",
          "type": "blob",
          "size": 21061
        },
        {
          "path": "plugins/azure-to-docker-master/scripts/azure-infrastructure-extractor.sh",
          "type": "blob",
          "size": 54163
        },
        {
          "path": "plugins/azure-to-docker-master/scripts/dockerfile-generator.sh",
          "type": "blob",
          "size": 16611
        },
        {
          "path": "plugins/azure-to-docker-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/skills/azure-emulators-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/skills/azure-emulators-2025/SKILL.md",
          "type": "blob",
          "size": 16444
        },
        {
          "path": "plugins/azure-to-docker-master/skills/compose-patterns-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/skills/compose-patterns-2025/SKILL.md",
          "type": "blob",
          "size": 15466
        },
        {
          "path": "plugins/azure-to-docker-master/skills/docker-watch-mode-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/azure-to-docker-master/skills/docker-watch-mode-2025/SKILL.md",
          "type": "blob",
          "size": 1271
        },
        {
          "path": "plugins/bash-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2158
        },
        {
          "path": "plugins/bash-master/README.md",
          "type": "blob",
          "size": 6936
        },
        {
          "path": "plugins/bash-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/agents/bash-expert.md",
          "type": "blob",
          "size": 5572
        },
        {
          "path": "plugins/bash-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/commands/bash-analyze.md",
          "type": "blob",
          "size": 4067
        },
        {
          "path": "plugins/bash-master/commands/bash-optimize.md",
          "type": "blob",
          "size": 5601
        },
        {
          "path": "plugins/bash-master/commands/bash-template.md",
          "type": "blob",
          "size": 5390
        },
        {
          "path": "plugins/bash-master/commands/pwsh-script.md",
          "type": "blob",
          "size": 4774
        },
        {
          "path": "plugins/bash-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/scripts/parallel-runner.sh",
          "type": "blob",
          "size": 8180
        },
        {
          "path": "plugins/bash-master/scripts/script-profiler.sh",
          "type": "blob",
          "size": 10816
        },
        {
          "path": "plugins/bash-master/scripts/shellcheck-batch.sh",
          "type": "blob",
          "size": 9652
        },
        {
          "path": "plugins/bash-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/advanced-array-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/advanced-array-patterns/SKILL.md",
          "type": "blob",
          "size": 13212
        },
        {
          "path": "plugins/bash-master/skills/bash-53-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/bash-53-features/SKILL.md",
          "type": "blob",
          "size": 10310
        },
        {
          "path": "plugins/bash-master/skills/bash-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/bash-master/SKILL.md",
          "type": "blob",
          "size": 30073
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/best_practices.md",
          "type": "blob",
          "size": 29804
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/patterns_antipatterns.md",
          "type": "blob",
          "size": 20553
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/platform_specifics.md",
          "type": "blob",
          "size": 25183
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/platform_specifics.md.backup",
          "type": "blob",
          "size": 23544
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/resources.md",
          "type": "blob",
          "size": 18739
        },
        {
          "path": "plugins/bash-master/skills/bash-master/references/windows-git-bash-paths.md",
          "type": "blob",
          "size": 18604
        },
        {
          "path": "plugins/bash-master/skills/debugging-troubleshooting-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/debugging-troubleshooting-2025/SKILL.md",
          "type": "blob",
          "size": 13139
        },
        {
          "path": "plugins/bash-master/skills/modern-automation-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/modern-automation-patterns/SKILL.md",
          "type": "blob",
          "size": 14478
        },
        {
          "path": "plugins/bash-master/skills/parallel-processing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/parallel-processing-patterns/SKILL.md",
          "type": "blob",
          "size": 18441
        },
        {
          "path": "plugins/bash-master/skills/process-substitution-fifos",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/process-substitution-fifos/SKILL.md",
          "type": "blob",
          "size": 17233
        },
        {
          "path": "plugins/bash-master/skills/security-first-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/security-first-2025/SKILL.md",
          "type": "blob",
          "size": 13159
        },
        {
          "path": "plugins/bash-master/skills/shellcheck-cicd-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/shellcheck-cicd-2025/SKILL.md",
          "type": "blob",
          "size": 9451
        },
        {
          "path": "plugins/bash-master/skills/string-manipulation-mastery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/bash-master/skills/string-manipulation-mastery/SKILL.md",
          "type": "blob",
          "size": 16995
        },
        {
          "path": "plugins/context-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1179
        },
        {
          "path": "plugins/context-master/IMPROVEMENTS_2025.md",
          "type": "blob",
          "size": 7493
        },
        {
          "path": "plugins/context-master/IMPROVEMENTS_WINDOWS_GITBASH_2025.md",
          "type": "blob",
          "size": 7497
        },
        {
          "path": "plugins/context-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/context-master/README.md",
          "type": "blob",
          "size": 5988
        },
        {
          "path": "plugins/context-master/WINDOWS_GIT_BASH_GUIDE.md",
          "type": "blob",
          "size": 6755
        },
        {
          "path": "plugins/context-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/commands/plan-project.md",
          "type": "blob",
          "size": 5088
        },
        {
          "path": "plugins/context-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/skills/context-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/skills/context-master/SKILL.md",
          "type": "blob",
          "size": 49829
        },
        {
          "path": "plugins/context-master/skills/context-master/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/skills/context-master/references/agent-skills-integration-2025.md",
          "type": "blob",
          "size": 7357
        },
        {
          "path": "plugins/context-master/skills/context-master/references/context_strategies.md",
          "type": "blob",
          "size": 9618
        },
        {
          "path": "plugins/context-master/skills/context-master/references/subagent_patterns.md",
          "type": "blob",
          "size": 16709
        },
        {
          "path": "plugins/context-master/skills/context-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/context-master/skills/context-master/scripts/create_subagent.py",
          "type": "blob",
          "size": 7469
        },
        {
          "path": "plugins/context-master/skills/context-master/scripts/generate_claude_md.py",
          "type": "blob",
          "size": 9942
        },
        {
          "path": "plugins/docker-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2101
        },
        {
          "path": "plugins/docker-master/README.md",
          "type": "blob",
          "size": 12701
        },
        {
          "path": "plugins/docker-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/agents/docker-expert.md",
          "type": "blob",
          "size": 12242
        },
        {
          "path": "plugins/docker-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-2025-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-2025-features/SKILL.md",
          "type": "blob",
          "size": 17984
        },
        {
          "path": "plugins/docker-master/skills/docker-best-practices",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-best-practices/SKILL.md",
          "type": "blob",
          "size": 13433
        },
        {
          "path": "plugins/docker-master/skills/docker-git-bash-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-git-bash-guide/SKILL.md",
          "type": "blob",
          "size": 13378
        },
        {
          "path": "plugins/docker-master/skills/docker-platform-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-platform-guide/SKILL.md",
          "type": "blob",
          "size": 17044
        },
        {
          "path": "plugins/docker-master/skills/docker-security-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docker-master/skills/docker-security-guide/SKILL.md",
          "type": "blob",
          "size": 22554
        },
        {
          "path": "plugins/dotnet-microservices-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotnet-microservices-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotnet-microservices-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 705
        },
        {
          "path": "plugins/dotnet-microservices-master/README.md",
          "type": "blob",
          "size": 8718
        },
        {
          "path": "plugins/dotnet-microservices-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotnet-microservices-master/agents/dotnet-microservices-expert.md",
          "type": "blob",
          "size": 19485
        },
        {
          "path": "plugins/dotnet-microservices-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotnet-microservices-master/skills/microsoft-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dotnet-microservices-master/skills/microsoft-guide/NET-Microservices-Architecture.md",
          "type": "blob",
          "size": 774314
        },
        {
          "path": "plugins/dotnet-microservices-master/skills/microsoft-guide/SKILL.md",
          "type": "blob",
          "size": 2836
        },
        {
          "path": "plugins/fal-ai-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2177
        },
        {
          "path": "plugins/fal-ai-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/fal-ai-master/README.md",
          "type": "blob",
          "size": 12479
        },
        {
          "path": "plugins/fal-ai-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/agents/fal-ai-expert.md",
          "type": "blob",
          "size": 23475
        },
        {
          "path": "plugins/fal-ai-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/commands/fal-debug.md",
          "type": "blob",
          "size": 11604
        },
        {
          "path": "plugins/fal-ai-master/commands/fal-deploy.md",
          "type": "blob",
          "size": 11029
        },
        {
          "path": "plugins/fal-ai-master/commands/fal-generate-image.md",
          "type": "blob",
          "size": 5170
        },
        {
          "path": "plugins/fal-ai-master/commands/fal-generate-video.md",
          "type": "blob",
          "size": 8279
        },
        {
          "path": "plugins/fal-ai-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/scripts/check-fal-auth.sh",
          "type": "blob",
          "size": 4354
        },
        {
          "path": "plugins/fal-ai-master/scripts/fal-cost-estimator.sh",
          "type": "blob",
          "size": 7539
        },
        {
          "path": "plugins/fal-ai-master/scripts/list-fal-models.sh",
          "type": "blob",
          "size": 6950
        },
        {
          "path": "plugins/fal-ai-master/scripts/test-fal-generation.sh",
          "type": "blob",
          "size": 5088
        },
        {
          "path": "plugins/fal-ai-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/SKILL.md",
          "type": "blob",
          "size": 11860
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/references/flux-kontext-editing.md",
          "type": "blob",
          "size": 5689
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/references/realtime-websocket.md",
          "type": "blob",
          "size": 4137
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/references/serverless-scaling.md",
          "type": "blob",
          "size": 5132
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-api-reference/references/video-generation.md",
          "type": "blob",
          "size": 5852
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-audio",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-audio/SKILL.md",
          "type": "blob",
          "size": 15091
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-image-to-image",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-image-to-image/SKILL.md",
          "type": "blob",
          "size": 15303
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-image-to-video",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-image-to-video/SKILL.md",
          "type": "blob",
          "size": 15162
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-model-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-model-guide/SKILL.md",
          "type": "blob",
          "size": 11703
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-optimization/SKILL.md",
          "type": "blob",
          "size": 13059
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-serverless-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-serverless-guide/SKILL.md",
          "type": "blob",
          "size": 14902
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-text-to-image",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-text-to-image/SKILL.md",
          "type": "blob",
          "size": 13123
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-text-to-video",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-text-to-video/SKILL.md",
          "type": "blob",
          "size": 15162
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-video-to-video",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fal-ai-master/skills/fal-video-to-video/SKILL.md",
          "type": "blob",
          "size": 13469
        },
        {
          "path": "plugins/ffmpeg-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 5674
        },
        {
          "path": "plugins/ffmpeg-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/ffmpeg-master/PYTHON_INTEGRATION_ENHANCEMENT.md",
          "type": "blob",
          "size": 11830
        },
        {
          "path": "plugins/ffmpeg-master/README.md",
          "type": "blob",
          "size": 28581
        },
        {
          "path": "plugins/ffmpeg-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/agents/ffmpeg-expert.md",
          "type": "blob",
          "size": 30328
        },
        {
          "path": "plugins/ffmpeg-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-audio.md",
          "type": "blob",
          "size": 2778
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-batch-social.md",
          "type": "blob",
          "size": 16856
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-color.md",
          "type": "blob",
          "size": 4710
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-debug.md",
          "type": "blob",
          "size": 3195
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-effects.md",
          "type": "blob",
          "size": 3340
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-kinetic.md",
          "type": "blob",
          "size": 6188
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-stream.md",
          "type": "blob",
          "size": 4488
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-transcode.md",
          "type": "blob",
          "size": 3635
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-viral-shorts.md",
          "type": "blob",
          "size": 12958
        },
        {
          "path": "plugins/ffmpeg-master/commands/ffmpeg-viral-tiktok.md",
          "type": "blob",
          "size": 9555
        },
        {
          "path": "plugins/ffmpeg-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/scripts/analyze-video.sh",
          "type": "blob",
          "size": 6250
        },
        {
          "path": "plugins/ffmpeg-master/scripts/apply-color-grade.sh",
          "type": "blob",
          "size": 6002
        },
        {
          "path": "plugins/ffmpeg-master/scripts/apply-glitch-effects.sh",
          "type": "blob",
          "size": 5193
        },
        {
          "path": "plugins/ffmpeg-master/scripts/benchmark-encoding.sh",
          "type": "blob",
          "size": 5077
        },
        {
          "path": "plugins/ffmpeg-master/scripts/check-codecs.sh",
          "type": "blob",
          "size": 5011
        },
        {
          "path": "plugins/ffmpeg-master/scripts/generate-karaoke.sh",
          "type": "blob",
          "size": 20329
        },
        {
          "path": "plugins/ffmpeg-master/scripts/generate-whisper-subtitles.sh",
          "type": "blob",
          "size": 4269
        },
        {
          "path": "plugins/ffmpeg-master/scripts/validate-stream.sh",
          "type": "blob",
          "size": 5340
        },
        {
          "path": "plugins/ffmpeg-master/scripts/validate_python_integration.py",
          "type": "blob",
          "size": 8382
        },
        {
          "path": "plugins/ffmpeg-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-animation-timing-reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-animation-timing-reference/SKILL.md",
          "type": "blob",
          "size": 35085
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-audio-processing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-audio-processing/SKILL.md",
          "type": "blob",
          "size": 14350
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-captions-subtitles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-captions-subtitles/SKILL.md",
          "type": "blob",
          "size": 19729
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-cicd-runners",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-cicd-runners/SKILL.md",
          "type": "blob",
          "size": 14777
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-cloudflare-containers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-cloudflare-containers/SKILL.md",
          "type": "blob",
          "size": 14354
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-color-grading-chromakey",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-color-grading-chromakey/SKILL.md",
          "type": "blob",
          "size": 15892
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-docker-containers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-docker-containers/SKILL.md",
          "type": "blob",
          "size": 14910
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025/SKILL.md",
          "type": "blob",
          "size": 19373
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025/references/vulkan-compute-codecs.md",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025/references/vvc-h266-encoding.md",
          "type": "blob",
          "size": 4032
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-fundamentals-2025/references/whisper-transcription.md",
          "type": "blob",
          "size": 6437
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-glitch-distortion-effects",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-glitch-distortion-effects/SKILL.md",
          "type": "blob",
          "size": 14727
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-hardware-acceleration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-hardware-acceleration/SKILL.md",
          "type": "blob",
          "size": 17053
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-karaoke-animated-text",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-karaoke-animated-text/SKILL.md",
          "type": "blob",
          "size": 28035
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-kinetic-captions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-kinetic-captions/SKILL.md",
          "type": "blob",
          "size": 30087
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-modal-containers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-modal-containers/SKILL.md",
          "type": "blob",
          "size": 27854
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-opencv-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-opencv-integration/SKILL.md",
          "type": "blob",
          "size": 33223
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-python-integration-reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-python-integration-reference/skill.md",
          "type": "blob",
          "size": 45761
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-shapes-graphics",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-shapes-graphics/SKILL.md",
          "type": "blob",
          "size": 17625
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-streaming",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-streaming/SKILL.md",
          "type": "blob",
          "size": 14977
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-transitions-effects",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-transitions-effects/SKILL.md",
          "type": "blob",
          "size": 17255
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-waveforms-visualization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-waveforms-visualization/SKILL.md",
          "type": "blob",
          "size": 17813
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-webassembly-workers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/ffmpeg-webassembly-workers/SKILL.md",
          "type": "blob",
          "size": 14689
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-animated-captions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-animated-captions/SKILL.md",
          "type": "blob",
          "size": 29742
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-hook-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-hook-templates/SKILL.md",
          "type": "blob",
          "size": 23930
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-platform-specs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ffmpeg-master/skills/viral-video-platform-specs/SKILL.md",
          "type": "blob",
          "size": 28261
        },
        {
          "path": "plugins/git-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2875
        },
        {
          "path": "plugins/git-master/README.md",
          "type": "blob",
          "size": 13476
        },
        {
          "path": "plugins/git-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/commands/git-safe-rebase.md",
          "type": "blob",
          "size": 2926
        },
        {
          "path": "plugins/git-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/git-2-49-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/git-2-49-features/SKILL.md",
          "type": "blob",
          "size": 8014
        },
        {
          "path": "plugins/git-master/skills/git-2025-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/git-2025-features/SKILL.md",
          "type": "blob",
          "size": 10531
        },
        {
          "path": "plugins/git-master/skills/git-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/git-master/SKILL.md",
          "type": "blob",
          "size": 38777
        },
        {
          "path": "plugins/git-master/skills/git-security-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/git-security-2025/SKILL.md",
          "type": "blob",
          "size": 16039
        },
        {
          "path": "plugins/git-master/skills/github-actions-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/github-actions-2025/SKILL.md",
          "type": "blob",
          "size": 10260
        },
        {
          "path": "plugins/git-master/skills/github-ai-features-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/git-master/skills/github-ai-features-2025/SKILL.md",
          "type": "blob",
          "size": 8558
        },
        {
          "path": "plugins/modal-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2489
        },
        {
          "path": "plugins/modal-master/README.md",
          "type": "blob",
          "size": 4747
        },
        {
          "path": "plugins/modal-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/agents/modal-expert.md",
          "type": "blob",
          "size": 26280
        },
        {
          "path": "plugins/modal-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/commands/modal-debug.md",
          "type": "blob",
          "size": 6901
        },
        {
          "path": "plugins/modal-master/commands/modal-deploy.md",
          "type": "blob",
          "size": 3621
        },
        {
          "path": "plugins/modal-master/commands/modal-gpu.md",
          "type": "blob",
          "size": 5123
        },
        {
          "path": "plugins/modal-master/commands/modal-schedule.md",
          "type": "blob",
          "size": 6886
        },
        {
          "path": "plugins/modal-master/commands/modal-setup.md",
          "type": "blob",
          "size": 2635
        },
        {
          "path": "plugins/modal-master/commands/modal-web.md",
          "type": "blob",
          "size": 6321
        },
        {
          "path": "plugins/modal-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/scripts/check-gpu-availability.sh",
          "type": "blob",
          "size": 3075
        },
        {
          "path": "plugins/modal-master/scripts/modal-cost-estimator.sh",
          "type": "blob",
          "size": 6364
        },
        {
          "path": "plugins/modal-master/scripts/validate-modal-app.sh",
          "type": "blob",
          "size": 5725
        },
        {
          "path": "plugins/modal-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/SKILL.md",
          "type": "blob",
          "size": 10130
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/references/batched-dynamic-batching.md",
          "type": "blob",
          "size": 6966
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/references/sandboxes-code-execution.md",
          "type": "blob",
          "size": 6161
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/references/scaling-autoscaler.md",
          "type": "blob",
          "size": 5138
        },
        {
          "path": "plugins/modal-master/skills/modal-knowledge/references/storage-volumes-cloud.md",
          "type": "blob",
          "size": 6863
        },
        {
          "path": "plugins/nextjs-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2374
        },
        {
          "path": "plugins/nextjs-master/LICENSE",
          "type": "blob",
          "size": 1088
        },
        {
          "path": "plugins/nextjs-master/README.md",
          "type": "blob",
          "size": 3479
        },
        {
          "path": "plugins/nextjs-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/agents/nextjs-expert.md",
          "type": "blob",
          "size": 21902
        },
        {
          "path": "plugins/nextjs-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/commands/nextjs-api-route.md",
          "type": "blob",
          "size": 6748
        },
        {
          "path": "plugins/nextjs-master/commands/nextjs-middleware.md",
          "type": "blob",
          "size": 8592
        },
        {
          "path": "plugins/nextjs-master/commands/nextjs-page.md",
          "type": "blob",
          "size": 3996
        },
        {
          "path": "plugins/nextjs-master/commands/nextjs-server-action.md",
          "type": "blob",
          "size": 7374
        },
        {
          "path": "plugins/nextjs-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/scripts/generate-nextjs-component.py",
          "type": "blob",
          "size": 18398
        },
        {
          "path": "plugins/nextjs-master/scripts/migrate-to-nextjs16.py",
          "type": "blob",
          "size": 11900
        },
        {
          "path": "plugins/nextjs-master/scripts/nextjs-analyzer.py",
          "type": "blob",
          "size": 16999
        },
        {
          "path": "plugins/nextjs-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-app-router",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-app-router/SKILL.md",
          "type": "blob",
          "size": 13492
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-authentication",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-authentication/SKILL.md",
          "type": "blob",
          "size": 15495
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-caching",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-caching/SKILL.md",
          "type": "blob",
          "size": 11781
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-data-fetching",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-data-fetching/SKILL.md",
          "type": "blob",
          "size": 13169
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-deployment/SKILL.md",
          "type": "blob",
          "size": 11118
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-middleware",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-middleware/SKILL.md",
          "type": "blob",
          "size": 14261
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-modal-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-modal-integration/SKILL.md",
          "type": "blob",
          "size": 27889
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-routing-advanced",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-routing-advanced/SKILL.md",
          "type": "blob",
          "size": 13779
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-server-actions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-master/skills/nextjs-server-actions/SKILL.md",
          "type": "blob",
          "size": 15278
        },
        {
          "path": "plugins/plugin-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1034
        },
        {
          "path": "plugins/plugin-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/plugin-master/README.md",
          "type": "blob",
          "size": 3854
        },
        {
          "path": "plugins/plugin-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/agents/plugin-expert.md",
          "type": "blob",
          "size": 7900
        },
        {
          "path": "plugins/plugin-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/commands/create-plugin.md",
          "type": "blob",
          "size": 3299
        },
        {
          "path": "plugins/plugin-master/commands/validate-plugin.md",
          "type": "blob",
          "size": 3531
        },
        {
          "path": "plugins/plugin-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/scripts/README.md",
          "type": "blob",
          "size": 2219
        },
        {
          "path": "plugins/plugin-master/scripts/validate-agent.sh",
          "type": "blob",
          "size": 4100
        },
        {
          "path": "plugins/plugin-master/scripts/validate-plugin.sh",
          "type": "blob",
          "size": 5153
        },
        {
          "path": "plugins/plugin-master/scripts/validate-skill.sh",
          "type": "blob",
          "size": 3868
        },
        {
          "path": "plugins/plugin-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/SKILL.md",
          "type": "blob",
          "size": 5278
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/examples/hook-scripts.md",
          "type": "blob",
          "size": 7701
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/references/hooks-advanced.md",
          "type": "blob",
          "size": 6371
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/references/mcp-patterns.md",
          "type": "blob",
          "size": 6254
        },
        {
          "path": "plugins/plugin-master/skills/advanced-features-2025/references/team-distribution.md",
          "type": "blob",
          "size": 5933
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/SKILL.md",
          "type": "blob",
          "size": 6667
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/examples/full-plugin.md",
          "type": "blob",
          "size": 7496
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/examples/minimal-plugin.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/references/component-patterns.md",
          "type": "blob",
          "size": 6556
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/references/manifest-reference.md",
          "type": "blob",
          "size": 4735
        },
        {
          "path": "plugins/plugin-master/skills/plugin-master/references/publishing-guide.md",
          "type": "blob",
          "size": 5541
        },
        {
          "path": "plugins/powershell-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2257
        },
        {
          "path": "plugins/powershell-master/README.md",
          "type": "blob",
          "size": 9747
        },
        {
          "path": "plugins/powershell-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/agents/powershell-expert.md",
          "type": "blob",
          "size": 19884
        },
        {
          "path": "plugins/powershell-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/commands/ps-analyze.md",
          "type": "blob",
          "size": 2134
        },
        {
          "path": "plugins/powershell-master/commands/ps-ci.md",
          "type": "blob",
          "size": 7972
        },
        {
          "path": "plugins/powershell-master/commands/ps-migrate.md",
          "type": "blob",
          "size": 3262
        },
        {
          "path": "plugins/powershell-master/commands/ps-module.md",
          "type": "blob",
          "size": 4663
        },
        {
          "path": "plugins/powershell-master/commands/ps-secure.md",
          "type": "blob",
          "size": 4330
        },
        {
          "path": "plugins/powershell-master/commands/ps-test.md",
          "type": "blob",
          "size": 4899
        },
        {
          "path": "plugins/powershell-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/scripts/Convert-WmicToGetCimInstance.ps1",
          "type": "blob",
          "size": 8928
        },
        {
          "path": "plugins/powershell-master/scripts/Initialize-SecretVault.ps1",
          "type": "blob",
          "size": 5683
        },
        {
          "path": "plugins/powershell-master/scripts/Test-PowerShellEnvironment.ps1",
          "type": "blob",
          "size": 8088
        },
        {
          "path": "plugins/powershell-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-2025-changes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-2025-changes/SKILL.md",
          "type": "blob",
          "size": 10080
        },
        {
          "path": "plugins/powershell-master/skills/powershell-7.5-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-7.5-features/SKILL.md",
          "type": "blob",
          "size": 20584
        },
        {
          "path": "plugins/powershell-master/skills/powershell-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-master/SKILL.md",
          "type": "blob",
          "size": 30185
        },
        {
          "path": "plugins/powershell-master/skills/powershell-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-security/SKILL.md",
          "type": "blob",
          "size": 12025
        },
        {
          "path": "plugins/powershell-master/skills/powershell-shell-detection",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/powershell-master/skills/powershell-shell-detection/SKILL.md",
          "type": "blob",
          "size": 12839
        },
        {
          "path": "plugins/python-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 4593
        },
        {
          "path": "plugins/python-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/python-master/README.md",
          "type": "blob",
          "size": 8577
        },
        {
          "path": "plugins/python-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/agents/python-expert.md",
          "type": "blob",
          "size": 12177
        },
        {
          "path": "plugins/python-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/commands/python-init.md",
          "type": "blob",
          "size": 2180
        },
        {
          "path": "plugins/python-master/commands/python-test.md",
          "type": "blob",
          "size": 1616
        },
        {
          "path": "plugins/python-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/scripts/analyze-coverage.sh",
          "type": "blob",
          "size": 3262
        },
        {
          "path": "plugins/python-master/scripts/check-async.sh",
          "type": "blob",
          "size": 4121
        },
        {
          "path": "plugins/python-master/scripts/check-deps.sh",
          "type": "blob",
          "size": 3778
        },
        {
          "path": "plugins/python-master/scripts/check-types.sh",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "plugins/python-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-asyncio",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-asyncio/SKILL.md",
          "type": "blob",
          "size": 14900
        },
        {
          "path": "plugins/python-master/skills/python-asyncio/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-asyncio/references/async-patterns-library.md",
          "type": "blob",
          "size": 12355
        },
        {
          "path": "plugins/python-master/skills/python-cloudflare",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-cloudflare/SKILL.md",
          "type": "blob",
          "size": 12623
        },
        {
          "path": "plugins/python-master/skills/python-fastapi",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-fastapi/SKILL.md",
          "type": "blob",
          "size": 17028
        },
        {
          "path": "plugins/python-master/skills/python-fastapi/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-fastapi/references/dependency-injection-patterns.md",
          "type": "blob",
          "size": 12594
        },
        {
          "path": "plugins/python-master/skills/python-ffmpeg",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-ffmpeg/SKILL.md",
          "type": "blob",
          "size": 30742
        },
        {
          "path": "plugins/python-master/skills/python-ffmpeg/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-ffmpeg/references/ffmpeg-advanced-patterns.md",
          "type": "blob",
          "size": 23988
        },
        {
          "path": "plugins/python-master/skills/python-fundamentals-313",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-fundamentals-313/SKILL.md",
          "type": "blob",
          "size": 12388
        },
        {
          "path": "plugins/python-master/skills/python-github-actions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-github-actions/SKILL.md",
          "type": "blob",
          "size": 13682
        },
        {
          "path": "plugins/python-master/skills/python-gotchas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-gotchas/SKILL.md",
          "type": "blob",
          "size": 12091
        },
        {
          "path": "plugins/python-master/skills/python-modal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-modal/SKILL.md",
          "type": "blob",
          "size": 30356
        },
        {
          "path": "plugins/python-master/skills/python-opencv",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-opencv/SKILL.md",
          "type": "blob",
          "size": 27786
        },
        {
          "path": "plugins/python-master/skills/python-opencv/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-opencv/references/opencv-advanced-patterns.md",
          "type": "blob",
          "size": 20471
        },
        {
          "path": "plugins/python-master/skills/python-package-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-package-management/SKILL.md",
          "type": "blob",
          "size": 13033
        },
        {
          "path": "plugins/python-master/skills/python-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-testing/SKILL.md",
          "type": "blob",
          "size": 14554
        },
        {
          "path": "plugins/python-master/skills/python-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-testing/references/pytest-fixtures-cookbook.md",
          "type": "blob",
          "size": 11358
        },
        {
          "path": "plugins/python-master/skills/python-type-hints",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-type-hints/SKILL.md",
          "type": "blob",
          "size": 14971
        },
        {
          "path": "plugins/python-master/skills/python-type-hints/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-type-hints/references/advanced-typing-patterns.md",
          "type": "blob",
          "size": 12381
        },
        {
          "path": "plugins/python-master/skills/python-video-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-video-pipeline/SKILL.md",
          "type": "blob",
          "size": 29262
        },
        {
          "path": "plugins/python-master/skills/python-video-pipeline/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-master/skills/python-video-pipeline/references/modal-video-patterns.md",
          "type": "blob",
          "size": 22269
        },
        {
          "path": "plugins/react-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1791
        },
        {
          "path": "plugins/react-master/LICENSE",
          "type": "blob",
          "size": 1080
        },
        {
          "path": "plugins/react-master/README.md",
          "type": "blob",
          "size": 4808
        },
        {
          "path": "plugins/react-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/agents/react-expert.md",
          "type": "blob",
          "size": 21468
        },
        {
          "path": "plugins/react-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/commands/react-component.md",
          "type": "blob",
          "size": 3282
        },
        {
          "path": "plugins/react-master/commands/react-hook.md",
          "type": "blob",
          "size": 5004
        },
        {
          "path": "plugins/react-master/commands/react-optimize.md",
          "type": "blob",
          "size": 3541
        },
        {
          "path": "plugins/react-master/commands/react-test.md",
          "type": "blob",
          "size": 5216
        },
        {
          "path": "plugins/react-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/scripts/analyze-bundle.sh",
          "type": "blob",
          "size": 4165
        },
        {
          "path": "plugins/react-master/scripts/check-components.sh",
          "type": "blob",
          "size": 4755
        },
        {
          "path": "plugins/react-master/scripts/check-hooks.sh",
          "type": "blob",
          "size": 4088
        },
        {
          "path": "plugins/react-master/scripts/check-testing.sh",
          "type": "blob",
          "size": 5939
        },
        {
          "path": "plugins/react-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-forms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-forms/SKILL.md",
          "type": "blob",
          "size": 32399
        },
        {
          "path": "plugins/react-master/skills/react-fundamentals-19",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-fundamentals-19/SKILL.md",
          "type": "blob",
          "size": 14194
        },
        {
          "path": "plugins/react-master/skills/react-hooks-complete",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-hooks-complete/SKILL.md",
          "type": "blob",
          "size": 20808
        },
        {
          "path": "plugins/react-master/skills/react-hooks-complete/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-hooks-complete/references/custom-hooks-library.md",
          "type": "blob",
          "size": 18258
        },
        {
          "path": "plugins/react-master/skills/react-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-patterns/SKILL.md",
          "type": "blob",
          "size": 25458
        },
        {
          "path": "plugins/react-master/skills/react-performance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-performance/SKILL.md",
          "type": "blob",
          "size": 16768
        },
        {
          "path": "plugins/react-master/skills/react-performance/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-performance/references/virtualization-guide.md",
          "type": "blob",
          "size": 14931
        },
        {
          "path": "plugins/react-master/skills/react-state-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-state-management/SKILL.md",
          "type": "blob",
          "size": 19946
        },
        {
          "path": "plugins/react-master/skills/react-state-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-state-management/references/zustand-patterns.md",
          "type": "blob",
          "size": 14457
        },
        {
          "path": "plugins/react-master/skills/react-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-testing/SKILL.md",
          "type": "blob",
          "size": 21177
        },
        {
          "path": "plugins/react-master/skills/react-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-testing/references/testing-recipes.md",
          "type": "blob",
          "size": 21517
        },
        {
          "path": "plugins/react-master/skills/react-typescript",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/react-master/skills/react-typescript/SKILL.md",
          "type": "blob",
          "size": 23541
        },
        {
          "path": "plugins/salesforce-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2886
        },
        {
          "path": "plugins/salesforce-master/README.md",
          "type": "blob",
          "size": 16423
        },
        {
          "path": "plugins/salesforce-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/agents/salesforce-expert.md",
          "type": "blob",
          "size": 21424
        },
        {
          "path": "plugins/salesforce-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/agentforce-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/agentforce-2025/SKILL.md",
          "type": "blob",
          "size": 21532
        },
        {
          "path": "plugins/salesforce-master/skills/data-cloud-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/data-cloud-2025/SKILL.md",
          "type": "blob",
          "size": 36671
        },
        {
          "path": "plugins/salesforce-master/skills/flow-orchestrator-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/flow-orchestrator-2025/SKILL.md",
          "type": "blob",
          "size": 18800
        },
        {
          "path": "plugins/salesforce-master/skills/hyperforce-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/hyperforce-2025/SKILL.md",
          "type": "blob",
          "size": 17781
        },
        {
          "path": "plugins/salesforce-master/skills/lightning-2025-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/salesforce-master/skills/lightning-2025-features/SKILL.md",
          "type": "blob",
          "size": 8459
        },
        {
          "path": "plugins/ssdt-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1942
        },
        {
          "path": "plugins/ssdt-master/README.md",
          "type": "blob",
          "size": 25554
        },
        {
          "path": "plugins/ssdt-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/agents/ssdt-expert.md",
          "type": "blob",
          "size": 17294
        },
        {
          "path": "plugins/ssdt-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/skills/sql-server-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/skills/sql-server-2025/SKILL.md",
          "type": "blob",
          "size": 23212
        },
        {
          "path": "plugins/ssdt-master/skills/ssdt-cicd-best-practices-2025",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/skills/ssdt-cicd-best-practices-2025/SKILL.md",
          "type": "blob",
          "size": 19737
        },
        {
          "path": "plugins/ssdt-master/skills/windows-git-bash-paths",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ssdt-master/skills/windows-git-bash-paths/SKILL.md",
          "type": "blob",
          "size": 14892
        },
        {
          "path": "plugins/tailwindcss-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1891
        },
        {
          "path": "plugins/tailwindcss-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/tailwindcss-master/README.md",
          "type": "blob",
          "size": 8643
        },
        {
          "path": "plugins/tailwindcss-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/agents/tailwindcss-expert.md",
          "type": "blob",
          "size": 21296
        },
        {
          "path": "plugins/tailwindcss-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/commands/tailwind-component.md",
          "type": "blob",
          "size": 5242
        },
        {
          "path": "plugins/tailwindcss-master/commands/tailwind-debug.md",
          "type": "blob",
          "size": 4078
        },
        {
          "path": "plugins/tailwindcss-master/commands/tailwind-responsive.md",
          "type": "blob",
          "size": 5028
        },
        {
          "path": "plugins/tailwindcss-master/commands/tailwind-setup.md",
          "type": "blob",
          "size": 3173
        },
        {
          "path": "plugins/tailwindcss-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/scripts/analyze-bundle.sh",
          "type": "blob",
          "size": 4643
        },
        {
          "path": "plugins/tailwindcss-master/scripts/check-accessibility.sh",
          "type": "blob",
          "size": 7489
        },
        {
          "path": "plugins/tailwindcss-master/scripts/check-classes.sh",
          "type": "blob",
          "size": 4742
        },
        {
          "path": "plugins/tailwindcss-master/scripts/check-dark-mode.sh",
          "type": "blob",
          "size": 6107
        },
        {
          "path": "plugins/tailwindcss-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-accessibility",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-accessibility/SKILL.md",
          "type": "blob",
          "size": 15225
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-components/SKILL.md",
          "type": "blob",
          "size": 18327
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-design-systems",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-design-systems/SKILL.md",
          "type": "blob",
          "size": 15421
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-layouts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-layouts/SKILL.md",
          "type": "blob",
          "size": 13637
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-animations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-animations/SKILL.md",
          "type": "blob",
          "size": 9458
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-animations/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-animations/references/animation-patterns.md",
          "type": "blob",
          "size": 8535
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-debugging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-debugging/SKILL.md",
          "type": "blob",
          "size": 6520
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-debugging/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-debugging/references/troubleshooting-guide.md",
          "type": "blob",
          "size": 5735
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-framework-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-framework-integration/SKILL.md",
          "type": "blob",
          "size": 9674
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-framework-integration/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-framework-integration/references/react-patterns.md",
          "type": "blob",
          "size": 8867
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4/SKILL.md",
          "type": "blob",
          "size": 8773
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4/references/advanced-theme-patterns.md",
          "type": "blob",
          "size": 5094
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4/references/custom-utilities-guide.md",
          "type": "blob",
          "size": 4851
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-performance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-performance/SKILL.md",
          "type": "blob",
          "size": 7282
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-performance/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-performance/references/bundle-optimization.md",
          "type": "blob",
          "size": 4923
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-plugins/SKILL.md",
          "type": "blob",
          "size": 10073
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-responsive-darkmode",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-responsive-darkmode/SKILL.md",
          "type": "blob",
          "size": 9370
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-responsive-darkmode/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tailwindcss-master/skills/tailwindcss-responsive-darkmode/references/advanced-responsive-patterns.md",
          "type": "blob",
          "size": 7155
        },
        {
          "path": "plugins/terraform-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2516
        },
        {
          "path": "plugins/terraform-master/README.md",
          "type": "blob",
          "size": 23504
        },
        {
          "path": "plugins/terraform-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/agents/terraform-expert.md",
          "type": "blob",
          "size": 81850
        },
        {
          "path": "plugins/terraform-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/commands/tf-apply.md",
          "type": "blob",
          "size": 2678
        },
        {
          "path": "plugins/terraform-master/commands/tf-fmt.md",
          "type": "blob",
          "size": 3120
        },
        {
          "path": "plugins/terraform-master/commands/tf-import.md",
          "type": "blob",
          "size": 3305
        },
        {
          "path": "plugins/terraform-master/commands/tf-init.md",
          "type": "blob",
          "size": 2010
        },
        {
          "path": "plugins/terraform-master/commands/tf-plan.md",
          "type": "blob",
          "size": 2397
        },
        {
          "path": "plugins/terraform-master/commands/tf-state.md",
          "type": "blob",
          "size": 3612
        },
        {
          "path": "plugins/terraform-master/commands/tf-validate.md",
          "type": "blob",
          "size": 2838
        },
        {
          "path": "plugins/terraform-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/scripts/check-terraform-version.sh",
          "type": "blob",
          "size": 4088
        },
        {
          "path": "plugins/terraform-master/scripts/list-terraform-resources.sh",
          "type": "blob",
          "size": 4845
        },
        {
          "path": "plugins/terraform-master/scripts/terraform-cost-estimate.sh",
          "type": "blob",
          "size": 3249
        },
        {
          "path": "plugins/terraform-master/scripts/validate-terraform-config.sh",
          "type": "blob",
          "size": 5308
        },
        {
          "path": "plugins/terraform-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide/SKILL.md",
          "type": "blob",
          "size": 15374
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide/references/opentofu-1.10-features.md",
          "type": "blob",
          "size": 3391
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide/references/opentofu-1.11-features.md",
          "type": "blob",
          "size": 4210
        },
        {
          "path": "plugins/terraform-master/skills/opentofu-guide/references/state-encryption.md",
          "type": "blob",
          "size": 5480
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/SKILL.md",
          "type": "blob",
          "size": 9964
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/references/aws-provider-6.md",
          "type": "blob",
          "size": 2632
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/references/azurerm-4.md",
          "type": "blob",
          "size": 3477
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/references/ephemeral-values.md",
          "type": "blob",
          "size": 4044
        },
        {
          "path": "plugins/terraform-master/skills/terraform-tasks/references/terraform-stacks.md",
          "type": "blob",
          "size": 4127
        },
        {
          "path": "plugins/test-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1965
        },
        {
          "path": "plugins/test-master/README.md",
          "type": "blob",
          "size": 18412
        },
        {
          "path": "plugins/test-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/agents/test-expert.md",
          "type": "blob",
          "size": 17483
        },
        {
          "path": "plugins/test-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/skills/vitest-3-features",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/skills/vitest-3-features/SKILL.md",
          "type": "blob",
          "size": 12543
        },
        {
          "path": "plugins/test-master/skills/windows-git-bash-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/test-master/skills/windows-git-bash-testing/SKILL.md",
          "type": "blob",
          "size": 14042
        },
        {
          "path": "plugins/tsql-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1381
        },
        {
          "path": "plugins/tsql-master/README.md",
          "type": "blob",
          "size": 5103
        },
        {
          "path": "plugins/tsql-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/agents/tsql-expert.md",
          "type": "blob",
          "size": 7194
        },
        {
          "path": "plugins/tsql-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/commands/analyze-query.md",
          "type": "blob",
          "size": 2718
        },
        {
          "path": "plugins/tsql-master/commands/explain-plan.md",
          "type": "blob",
          "size": 4895
        },
        {
          "path": "plugins/tsql-master/commands/suggest-indexes.md",
          "type": "blob",
          "size": 4635
        },
        {
          "path": "plugins/tsql-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/scripts/README.md",
          "type": "blob",
          "size": 2016
        },
        {
          "path": "plugins/tsql-master/scripts/check-sargability.sql",
          "type": "blob",
          "size": 4474
        },
        {
          "path": "plugins/tsql-master/scripts/index-analysis.sql",
          "type": "blob",
          "size": 8174
        },
        {
          "path": "plugins/tsql-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/advanced-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/advanced-patterns/SKILL.md",
          "type": "blob",
          "size": 8790
        },
        {
          "path": "plugins/tsql-master/skills/azure-sql-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/azure-sql-optimization/SKILL.md",
          "type": "blob",
          "size": 7225
        },
        {
          "path": "plugins/tsql-master/skills/index-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/index-strategies/SKILL.md",
          "type": "blob",
          "size": 6802
        },
        {
          "path": "plugins/tsql-master/skills/query-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/query-optimization/SKILL.md",
          "type": "blob",
          "size": 6519
        },
        {
          "path": "plugins/tsql-master/skills/query-optimization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/query-optimization/references/dmv-diagnostic-queries.md",
          "type": "blob",
          "size": 10071
        },
        {
          "path": "plugins/tsql-master/skills/tsql-functions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/tsql-functions/SKILL.md",
          "type": "blob",
          "size": 4728
        },
        {
          "path": "plugins/tsql-master/skills/tsql-functions/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tsql-master/skills/tsql-functions/references/string-functions.md",
          "type": "blob",
          "size": 7916
        },
        {
          "path": "plugins/tsql-master/skills/tsql-functions/references/window-functions.md",
          "type": "blob",
          "size": 8629
        },
        {
          "path": "plugins/viral-video-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 2109
        },
        {
          "path": "plugins/viral-video-master/LICENSE",
          "type": "blob",
          "size": 1070
        },
        {
          "path": "plugins/viral-video-master/README.md",
          "type": "blob",
          "size": 5507
        },
        {
          "path": "plugins/viral-video-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/agents/viral-video-expert.md",
          "type": "blob",
          "size": 23310
        },
        {
          "path": "plugins/viral-video-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/commands/viral-audit.md",
          "type": "blob",
          "size": 4605
        },
        {
          "path": "plugins/viral-video-master/commands/viral-hook.md",
          "type": "blob",
          "size": 5071
        },
        {
          "path": "plugins/viral-video-master/commands/viral-optimize.md",
          "type": "blob",
          "size": 7394
        },
        {
          "path": "plugins/viral-video-master/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/scripts/analyze-video-metrics.py",
          "type": "blob",
          "size": 15723
        },
        {
          "path": "plugins/viral-video-master/scripts/generate-content-calendar.py",
          "type": "blob",
          "size": 11658
        },
        {
          "path": "plugins/viral-video-master/scripts/generate-hook-variations.py",
          "type": "blob",
          "size": 15603
        },
        {
          "path": "plugins/viral-video-master/scripts/hashtag-analyzer.py",
          "type": "blob",
          "size": 13778
        },
        {
          "path": "plugins/viral-video-master/scripts/video-spec-generator.py",
          "type": "blob",
          "size": 15153
        },
        {
          "path": "plugins/viral-video-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-hooks-retention",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-hooks-retention/SKILL.md",
          "type": "blob",
          "size": 14422
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-hooks-retention/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-hooks-retention/references/hook-psychology.md",
          "type": "blob",
          "size": 5564
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-hooks-retention/references/retention-optimization.md",
          "type": "blob",
          "size": 5519
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-long-form",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-long-form/SKILL.md",
          "type": "blob",
          "size": 15144
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-long-form/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-long-form/references/connected-tv-optimization.md",
          "type": "blob",
          "size": 4217
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-long-form/references/youtube-seo-2026.md",
          "type": "blob",
          "size": 4994
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-platform-algorithms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-platform-algorithms/SKILL.md",
          "type": "blob",
          "size": 14267
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-platform-algorithms/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-platform-algorithms/references/faceless-content-2026.md",
          "type": "blob",
          "size": 5836
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-platform-algorithms/references/social-commerce-2026.md",
          "type": "blob",
          "size": 5879
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form/SKILL.md",
          "type": "blob",
          "size": 11890
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form/references/instagram-reels-2026.md",
          "type": "blob",
          "size": 3900
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form/references/tiktok-algorithm-2026.md",
          "type": "blob",
          "size": 3568
        },
        {
          "path": "plugins/viral-video-master/skills/viral-video-short-form/references/youtube-shorts-2026.md",
          "type": "blob",
          "size": 3174
        },
        {
          "path": "plugins/windows-path-master",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1116
        },
        {
          "path": "plugins/windows-path-master/README.md",
          "type": "blob",
          "size": 11712
        },
        {
          "path": "plugins/windows-path-master/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/agents/windows-path-expert.md",
          "type": "blob",
          "size": 14434
        },
        {
          "path": "plugins/windows-path-master/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/commands/path-fix.md",
          "type": "blob",
          "size": 13688
        },
        {
          "path": "plugins/windows-path-master/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/skills/windows-path-troubleshooting",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/windows-path-master/skills/windows-path-troubleshooting/SKILL.md",
          "type": "blob",
          "size": 17040
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/CLAUDE.md",
          "type": "blob",
          "size": 8697
        },
        {
          "path": "scripts/version-tracker.sh",
          "type": "blob",
          "size": 1419
        },
        {
          "path": "scripts/version_ops.py",
          "type": "blob",
          "size": 11274
        }
      ],
      "marketplace": {
        "name": "claude-plugin-marketplace",
        "version": null,
        "description": "A curated collection of Claude Code plugins for plugin development, context optimization, and productivity tools",
        "owner_info": {
          "name": "Josiah Siegel",
          "email": "JosiahSiegel@users.noreply.github.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "context-master",
            "description": "Universal context management and planning system. PROACTIVELY activate for: (1) ANY complex task requiring planning, (2) Multi-file projects/websites/apps, (3) Architecture decisions, (4) Research tasks, (5) Refactoring, (6) Long coding sessions, (7) Tasks with 3+ sequential steps. Provides: optimal file creation order, context-efficient workflows, extended thinking delegation (23x context efficiency), Claude Sonnet 4.5 token budgeting, recursive delegation patterns, clear-and-verify production workflows, passive deep analysis architecture, progressive task decomposition, and prevents redundant work. Saves 62% context on average.",
            "source": "./plugins/context-master",
            "category": null,
            "version": "2.2.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install context-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/plan-project",
                "description": "Plan optimal file creation order for multi-file projects before implementation",
                "path": "plugins/context-master/commands/plan-project.md",
                "frontmatter": {
                  "description": "Plan optimal file creation order for multi-file projects before implementation"
                },
                "content": "## CRITICAL GUIDELINES\r\n\r\n### Windows File Path Requirements\r\n\r\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\r\n\r\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\r\n\r\n**Examples:**\r\n- WRONG: `D:/repos/project/file.tsx`\r\n- CORRECT: `D:\repos\\project\file.tsx`\r\n\r\nThis applies to:\r\n- Edit tool file_path parameter\r\n- Write tool file_path parameter\r\n- All file operations on Windows systems\r\n\r\n### Windows/Git Bash Path Conversion\r\n\r\nWhen using Git Bash on Windows, automatic path conversion may occur:\r\n- Unix paths (`/foo`) convert to Windows paths automatically\r\n- This usually works transparently\r\n- See WINDOWS_GIT_BASH_GUIDE.md for advanced scenarios and troubleshooting\r\n\r\n### Documentation Guidelines\r\n\r\n**NEVER create new documentation files unless explicitly requested by the user.**\r\n\r\n- **Priority**: Update existing README.md files rather than creating new documentation\r\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\r\n- **Style**: Documentation should be concise, direct, and professional\r\n- **User preference**: Only create additional .md files when user specifically asks\r\n\r\n---\r\n\r\n# Plan Project\r\n\r\n## Purpose\r\nBefore creating any files in a multi-file project (3+ related files), this command helps you plan the optimal creation order, identify dependencies, and prevent redundant work.\r\n\r\n## When to Use\r\n- Creating websites with multiple pages\r\n- Building applications with multiple components\r\n- Projects with shared dependencies (CSS, config files)\r\n- API implementations with multiple endpoints\r\n- Documentation sets with multiple files\r\n- Any task involving 3+ related files\r\n\r\n## Instructions\r\n\r\n### Step 1: Extended Thinking for Architecture\r\nIMMEDIATELY use extended thinking to analyze the project:\r\n\r\n```\r\n\"Think hard about the architecture for this [project type]:\r\n- What files are needed and what is their purpose?\r\n- What are the shared dependencies (CSS, config, base classes)?\r\n- What is the optimal creation order and why?\r\n- What are the cross-file references?\r\n- What could go wrong if we create files in the wrong order?\"\r\n```\r\n\r\n### Step 2: Create Architecture Plan\r\nBased on the thinking, create a plan following this template:\r\n\r\n```\r\nARCHITECTURE PLAN:\r\n\r\nFILES NEEDED:\r\n  - [filename]: [purpose]\r\n  - [filename]: [purpose]\r\n\r\nSHARED DEPENDENCIES (must be created first):\r\n  - [dependency]: [what files need this]\r\n\r\nCREATION ORDER (numbered with reasoning):\r\n  1. [file] - Reason: [why this first]\r\n  2. [file] - Reason: [why this second]\r\n  3. [file] - Reason: [why this third]\r\n\r\nCROSS-FILE REFERENCES:\r\n  - [file A] references [file B] via [method]\r\n\r\nPOTENTIAL ISSUES TO AVOID:\r\n  - [what could go wrong]\r\n  - [common mistake]\r\n```\r\n\r\n### Step 3: Announce the Plan to User\r\nTell the user your file creation order before starting:\r\n\r\n```\r\n\"I'll create these files in this order:\r\n1. [file] - [reason]\r\n2. [file] - [reason]\r\n3. [file] - [reason]\r\n...\r\n\r\nThis order ensures all dependencies are in place before files that need them.\"\r\n```\r\n\r\n### Step 4: Create Files in Optimal Order\r\nFollow the plan:\r\n- Create foundation files first (CSS, config, base classes)\r\n- Create dependent files after their dependencies exist\r\n- Keep consistent naming and structure\r\n- Add comments about dependencies\r\n\r\n### Step 5: Verify\r\nAfter creating all files, verify:\r\n- All file paths are correct\r\n- CSS/JS references load properly\r\n- Navigation between pages works\r\n- Cross-file dependencies resolve\r\n- No broken links or missing file references\r\n\r\n## Key Principles\r\n\r\n**Foundations First:**\r\n- CSS files before HTML files that use them\r\n- Configuration files before code that needs them\r\n- Base classes before derived classes\r\n\r\n**Core Before Features:**\r\n- index.html before other pages\r\n- main.js before feature modules\r\n- Core API before additional endpoints\r\n\r\n**Structure Before Content:**\r\n- HTML structure before detailed content\r\n- API structure before implementation details\r\n- Component scaffolds before full logic\r\n\r\n## Token Savings\r\n- Without planning: ~8,000 tokens (redundant work + fixes)\r\n- With planning: ~3,000 tokens (efficient creation)\r\n- **Savings: ~5,000 tokens (62% reduction) per project**\r\n\r\n## Example: Portfolio Website\r\n\r\n**User Request:** \"Create a portfolio with home, about, projects, and contact pages\"\r\n\r\n**Your Response:**\r\n1. Use extended thinking to plan\r\n2. Announce: \"I'll create: 1. styles.css, 2. index.html, 3. about.html, 4. projects.html, 5. contact.html\"\r\n3. Create files in that order\r\n4. Verify all HTML files reference styles.css correctly\r\n\r\n**Result:** Efficient, no refactoring needed!\r\n\r\n## Windows/Git Bash Notes\r\n\r\nOn Windows with Git Bash:\r\n- Path planning uses forward slashes (Unix format)\r\n- Actual file creation uses backslashes (Windows format)\r\n- Verification handles both formats automatically\r\n- See WINDOWS_GIT_BASH_GUIDE.md for detailed guidance"
              }
            ],
            "skills": [
              {
                "name": "context-master",
                "description": "Universal context management and planning system. PROACTIVELY activate for: (1) ANY complex task requiring planning, (2) Multi-file projects/websites/apps, (3) Architecture decisions, (4) Research tasks, (5) Refactoring, (6) Long coding sessions, (7) Tasks with 3+ sequential steps. Provides: optimal file creation order, context-efficient workflows, extended thinking delegation (23x context efficiency), passive deep analysis architecture, progressive task decomposition, and prevents redundant work. Saves 62% context on average. Essential for maintaining session performance and analytical depth.",
                "path": "plugins/context-master/skills/context-master/SKILL.md",
                "frontmatter": {
                  "name": "context-master",
                  "description": "Universal context management and planning system. PROACTIVELY activate for: (1) ANY complex task requiring planning, (2) Multi-file projects/websites/apps, (3) Architecture decisions, (4) Research tasks, (5) Refactoring, (6) Long coding sessions, (7) Tasks with 3+ sequential steps. Provides: optimal file creation order, context-efficient workflows, extended thinking delegation (23x context efficiency), passive deep analysis architecture, progressive task decomposition, and prevents redundant work. Saves 62% context on average. Essential for maintaining session performance and analytical depth."
                },
                "content": "# Context Master\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nUniversal context management and planning system for complex tasks, long coding sessions, and efficient workflow optimization.\n\n---\n\n##  TL;DR QUICK START (Read This First)\n\n**For ANY multi-file project, follow these 5 steps:**\n\n```\n1 STOP - Don't create files yet\n2 PLAN - Use \"think hard\" OR create planning document\n3 ANNOUNCE - Tell user your file creation order\n4 CREATE - Make files in optimal order (dependencies first)\n5 VERIFY - Check all references work\n```\n\n**Example:**\n```\nUser: \"Create a portfolio with home, about, projects pages\"\n\n Step 1: STOP [Don't immediately create index.html]\n Step 2: PLAN [Think: Need styles.css + 3 HTML files, CSS first]\n Step 3: ANNOUNCE [\"I'll create: 1. styles.css, 2. index.html, 3. about.html, 4. projects.html\"]\n Step 4: CREATE [Make them in that order]\n Step 5: VERIFY [Check all HTML files link to styles.css correctly]\n\nResult: Done efficiently, no refactoring needed!\n```\n\n**Token savings: ~5,000 tokens (62%) vs doing it wrong**\n\n**Continue reading below for detailed guidance...**\n\n---\n\n## Overview\n\nThis skill provides comprehensive context management, planning strategies, and workflow optimization for ANY complex coding task, not just multi-file projects.\n\n**MUST use this skill for:**\n-  ANY complex task requiring planning or strategy\n-  Multi-file projects (HTML, CSS, JS, APIs, apps, docs)\n-  Architecture or design decisions\n-  Research tasks requiring analysis\n-  Refactoring work\n-  Long coding sessions (context optimization)\n-  Tasks with 3+ sequential steps\n\n**What this skill provides:**\n- **Optimal file creation order** - Which files to create first, dependency management\n- **Context-efficient workflows** - 62% average context savings\n- **Extended thinking delegation** - 23x context efficiency for deep analysis\n- **Passive deep thinking architecture** - Get analytical depth without context cost\n- **Progressive task decomposition** - Break complex tasks into manageable phases\n- **Planning frameworks** - Think before coding, prevent redundant work\n- **Session optimization** - Maintain performance in long interactions\n\n**This skill activates automatically for:**\n- Complex tasks requiring planning (\"build...\", \"create...\", \"implement...\")\n- Architecture decisions (\"should we use...\", \"which approach...\")\n- Research requests (\"research...\", \"analyze...\", \"compare...\")\n- Refactoring work (\"refactor...\", \"improve...\", \"optimize...\")\n- Multi-step workflows (any task with 3+ steps)\n- Long coding sessions (automatic context monitoring)\n\n---\n\n#  MANDATORY FIRST STEP - READ THIS BEFORE DOING ANYTHING \n\n##  STOP - DO THIS FIRST \n\n**IMMEDIATELY use extended thinking to plan. Do NOT create any files yet.**\n\n**Your exact next output MUST be:**\n\n```\n\"Think hard about the architecture for this [project type]:\n- What files are needed and what is their purpose?\n- What are the shared dependencies (CSS, config, base classes)?\n- What is the optimal creation order and why?\n- What are the cross-file references?\n- What could go wrong if we create files in the wrong order?\"\n```\n\n**After the extended thinking completes, THEN announce your plan to the user.**\n\n**DO NOT create files until you:**\n1.  Complete extended thinking \n2.  Announce the plan to the user\n3.  Get their acknowledgment (or proceed if plan is sound)\n\n---\n\n##  PLANNING METHOD OPTIONS\n\n**You have TWO equally effective planning approaches:**\n\n### Option A: Extended Thinking (Pure Mental Planning)\n```\n\"Think hard about the architecture for this [project]:\n- What files are needed?\n- What is the optimal creation order?\n- What dependencies exist?\"\n```\n\n**Best for:** Quick projects, straightforward structures, when planning fits in thinking block\n\n### Option B: Planning Document (Structured Written Plan)\n```\nUse bash_tool or create an artifact for the planning document:\n\nARCHITECTURE_PLAN.md:\n- Files needed: [list]\n- Creation order: [numbered list]\n- Dependencies: [diagram/list]\n- Potential issues: [list]\n```\n\n**Best for:** Complex projects, when you want a reference document, when planning is extensive\n\n**Both work equally well!** Choose based on project complexity and your preference.\n\n**Example using bash_tool for planning:**\n```bash\ncat > ARCHITECTURE_PLAN.md << 'EOF'\n# Portfolio Website Architecture\n\n## Files Needed\n1. styles.css - Shared styling\n2. index.html - Home page\n3. about.html - About page\n4. projects.html - Projects page\n5. contact.html - Contact page\n\n## Creation Order\n1. styles.css (shared dependency, created first)\n2. index.html (references styles.css)\n3. about.html (references styles.css)\n4. projects.html (references styles.css)\n5. contact.html (references styles.css)\n\n## Cross-References\n- All HTML files link to styles.css via <link rel=\"stylesheet\">\n- All pages navigate to each other via <a href=\"...\">\nEOF\n```\n\n**Benefit of planning document:** You can reference it throughout the project, and it serves as documentation.\n\n---##  WHY THIS MATTERS: Token Savings\n\n**Without planning:**\n- Create files  Realize structure is wrong  Refactor  More explanations\n- **Cost: ~8,000 tokens** (redundant work + explanations + fixes)\n\n**With planning (this skill):**\n- Think first  Create files in optimal order  Done correctly first time\n- **Cost: ~3,000 tokens** (efficient creation only)\n\n** Savings: ~5,000 tokens (62% reduction) per multi-file project**\n\nOver a long session with multiple projects, this compounds significantly.\n\n### Real-World Token Savings by Project Size\n\n**Small Project (3-4 files) - Portfolio Website**\n```\nWithout planning: ~6,000 tokens\n  - Create HTML  Add inline styles  Extract CSS  Update refs\nWith planning: ~2,500 tokens\n  - Plan  Create CSS  Create HTML with refs\n Savings: ~3,500 tokens (58%)\n```\n\n**Medium Project (7-8 files) - Multi-page App**\n```\nWithout planning: ~12,000 tokens\n  - Create pages  Realize shared components  Refactor  Fix imports\nWith planning: ~4,500 tokens\n  - Plan  Create shared  Create pages  No refactoring\n Savings: ~7,500 tokens (63%)\n```\n\n**Large Project (20+ files) - Full Application**\n```\nWithout planning: ~35,000 tokens\n  - Create files randomly  Multiple refactoring cycles  Fix dependencies\nWith planning: ~12,000 tokens\n  - Plan architecture  Create in optimal order  Minimal fixes\n Savings: ~23,000 tokens (66%)\n```\n\n**Context window capacity:**\n- Standard: 200K tokens\n- With planning: Can complete 16-17 medium projects\n- Without planning: Can complete only 7-8 medium projects\n- **Effective capacity increase: 2.1x**\n\n---##  ACTIVATION TRIGGERS (You are seeing one of these RIGHT NOW)\n\nIf the user's request includes ANY of these phrases, this skill activated for a reason:\n\n-  \"create a website with...\"  **YOU ARE HERE**\n-  \"build 3+ pages/files\"\n-  \"make a [type] application\"\n-  \"create [home/about/contact] pages\"\n-  \"build an API with...\"\n-  \"generate documentation for...\"\n\n** Your NEXT output should be extended thinking about architecture, NOT file creation**\n\n---\n\n##  POST-PROJECT REFLECTION (Optional But Valuable)\n\n**After completing a multi-file project, take a moment to assess the context savings:**\n\n### Quick Self-Assessment Questions\n\n```\n1. Did you plan before creating files? [Yes/No]\n   \n2. How many files did you create? [Number]\n\n3. Did you have to refactor or fix file references? [Yes/No]\n   \n4. If you planned first:\n   - Estimated context used: ~[2,500-4,500] tokens for [3-8] files\n   \n5. If you created without planning:\n   - You likely used: ~[6,000-12,000] tokens\n   - Potential savings missed: ~[3,500-7,500] tokens\n```\n\n### Success Indicators\n\n** You used the skill effectively if:**\n- Created foundation files (CSS, config) before dependent files\n- No major refactoring needed after file creation\n- All file references worked on first try\n- Could describe file creation order before starting\n- Spent more time planning than fixing\n\n** You could improve if:**\n- Had to go back and add shared dependencies\n- Needed to refactor file structure after creation\n- Found broken references between files\n- Created files in no particular order\n- Spent more time fixing than planning\n\n### Context Savings Calculator\n\n**Estimate your actual savings:**\n```\nFiles created: [N]\nDid planning: [Yes/No]\n\nIf Yes:\n  Tokens used: ~(N  350) + 500 for planning\n  Tokens saved: ~(N  800)\n  Efficiency: ~70%\n\nIf No:\n  Tokens used: ~(N  1,150) \n  Missed savings: ~(N  800)\n  Next time: Plan first!\n```\n\n**Example for 5-file project:**\n- With planning: ~2,250 tokens\n- Without planning: ~5,750 tokens  \n- Actual savings: ~3,500 tokens (60%)\n\nThis reflection helps you recognize when the skill is working and when to apply it more strictly next time!\n\n---\n\n##  REQUIRED WORKFLOW CHECKLIST\n\n**For EVERY multi-file project, follow this exact sequence:**\n\n```\n Step 1: THINK FIRST - Use \"think hard\" to plan architecture\n         (List all files, determine optimal order, identify dependencies)\n         \n Step 2: ANNOUNCE THE PLAN - Tell user the file creation order\n         (\"I'll create files in this order: 1. CSS, 2. index.html, 3...\")\n         \n Step 3: CREATE FOUNDATION FILES - Shared dependencies first\n         (CSS files, config files, base classes)\n         \n Step 4: CREATE DEPENDENT FILES - Files that use the foundations\n         (HTML pages that reference CSS, components that use base classes)\n         \n Step 5: VERIFY - Check all references/imports work\n```\n\n**DO NOT skip Step 1. ALWAYS think before creating files.**\n\n---\n\n##  COMMON MISTAKE TO AVOID\n\n**WRONG APPROACH (what you might do without this skill):**\n```\nUser: \"Create a portfolio with home, about, and projects pages\"\nYou: [Creates index.html]\nYou: [Creates about.html]\nYou: [Creates projects.html]\nYou: [Realizes CSS should be shared, has to refactor]\nResult: Wasted effort, redundant work\n```\n\n**CORRECT APPROACH (what you MUST do with this skill):**\n```\nUser: \"Create a portfolio with home, about, and projects pages\"\nYou: \"Think hard about the architecture first...\"\n     [Plans: Need 1 CSS file + 3 HTML files, CSS should come first]\nYou: \"I'll create files in this order: 1. styles.css, 2. index.html, 3. about.html, 4. projects.html\"\nYou: [Creates files in that order]\nResult: Efficient, no redundant work\n```\n\n---\n\n##  MORE ANTI-PATTERNS (What NOT to Do)\n\n### Anti-Pattern 1: Creating JS Modules Before Main App File\n**Wrong:**\n```\n1. Create utils.js\n2. Create helpers.js\n3. Create api.js\n4. Create app.js (main file that imports all the above)\nProblem: Had to keep going back to app.js to add imports\n```\n\n**Right:**\n```\n1. Think about module structure\n2. Create app.js (with import statements planned)\n3. Create utils.js (knowing what app.js needs)\n4. Create helpers.js (knowing what app.js needs)\n5. Create api.js (knowing what app.js needs)\nBenefit: App.js structured correctly from the start\n```\n\n### Anti-Pattern 2: Writing Inline Styles Then Extracting Later\n**Wrong:**\n```\n1. Create index.html with inline styles\n2. Create about.html with inline styles\n3. Realize styles are duplicated\n4. Extract to styles.css\n5. Update all HTML files to reference it\nProblem: Redundant work, had to edit multiple files\n```\n\n**Right:**\n```\n1. Think: These pages will share styling\n2. Create styles.css first\n3. Create HTML files that reference styles.css\nBenefit: No duplication, no refactoring needed\n```\n\n### Anti-Pattern 3: Building Components Before Data Structure\n**Wrong:**\n```\n1. Create UserProfile.jsx component\n2. Create UserList.jsx component\n3. Realize data structure is unclear\n4. Go back and modify components to match data\nProblem: Components built on assumptions\n```\n\n**Right:**\n```\n1. Think about data structure first\n2. Create types.js or schema.js\n3. Create components that use defined data structure\nBenefit: Components built correctly from the start\n```\n\n### Anti-Pattern 4: Creating Pages Before Shared Layout\n**Wrong:**\n```\n1. Create home.html with full layout\n2. Create about.html with full layout\n3. Realize layout should be shared\n4. Extract to layout component/template\n5. Refactor all pages\nProblem: Major refactoring required\n```\n\n**Right:**\n```\n1. Think: Pages will share layout\n2. Create layout.html or Layout component\n3. Create pages that use the layout\nBenefit: DRY from the start\n```\n\n### Anti-Pattern 5: Creating Config Files Last\n**Wrong:**\n```\n1. Create multiple files with hardcoded values\n2. Realize config should be centralized\n3. Create config.js\n4. Update all files to use config\nProblem: Config scattered, hard to change\n```\n\n**Right:**\n```\n1. Think: What values will be used across files?\n2. Create config.js first\n3. Create other files that import config\nBenefit: Centralized configuration from start\n```\n\n---\n\n#  PART 1: UNIVERSAL GUIDANCE (All Users - Web, API, CLI)\n\n**The sections below apply to ALL users. Read these first regardless of your environment.**\n\n---\n\n## Core Principles (All Environments)\n\n### 1. Extended Thinking for Complex Tasks\n\nUse extended thinking to keep reasoning separate from main context:\n\n**Trigger phrases:**\n- `\"think about...\"` - Standard extended thinking\n- `\"think hard about...\"` - More thorough analysis  \n- `\"think harder about...\"` - Deep analysis\n- `\"ultrathink...\"` - Maximum thinking budget\n\n**When to use:**\n- Planning complex implementations\n- Analyzing multiple approaches\n- Design decisions with tradeoffs\n- Any task requiring deep reasoning\n\n**Benefit:** Reasoning happens in separate blocks that don't clutter your main context.\n\n### 2. Artifacts for Content Offloading\n\nCreate artifacts for substantial content instead of inline responses:\n\n**Use artifacts for:**\n- Code files (>20 lines)\n- Documents, reports, articles\n- Data analysis results\n- Complex visualizations\n- Any reusable content\n\n**Why it works:** Content lives in artifacts, not the conversation context.\n\n### 3. Progressive Task Decomposition\n\nBreak complex requests into phases:\n\n**Instead of:**\n\"Build me a complete app with authentication, database, and frontend\"\n\n**Do this:**\n```\nPhase 1: \"think about the architecture for this app\"\n[Review architecture plan]\n\nPhase 2: \"Create the database schema\"\n[Review schema]\n\nPhase 3: \"Build the authentication system\"\n[Continue phase by phase]\n```\n\n**Benefit:** Each phase has fresh context, no accumulation of old decisions.\n\n### 4. Explicit Context Boundaries\n\nSignal when to start fresh:\n\n- \"Let's start fresh with a new approach\"\n- \"Setting aside the previous discussion...\"\n- \"Here's a new angle on this problem...\"\n\n**In Claude Code:** Use `/clear` command\n**In web/API:** Explicitly state context reset\n\n## Multi-File Project Planning (Critical Section)\n\n** QUICK REMINDER: Did you think first? If not, go back to \"STOP - DO THIS FIRST\" above.**\n\n**When creating any project with 3+ related files, ALWAYS start with this planning workflow:**\n\n### Step 1: Architecture Planning\n\n**Choose your planning method (both equally effective):**\n\n**Method A: Extended Thinking**\n```\n\"Think hard about the architecture for this [project]:\n- What files are needed and their purpose?\n- What are shared dependencies?\n- What is optimal creation order?\n- What are cross-file references?\n- What could go wrong?\"\n```\n\n**Method B: Planning Document**\n```\nCreate ARCHITECTURE_PLAN.md (via bash_tool or artifact):\n- Files needed with purposes\n- Shared dependencies\n- Numbered creation order with reasoning\n- Cross-file reference map\n- Potential issues to avoid\n```\n\n**Before creating any files, use extended thinking OR create planning document with this template:**\n\n```\nARCHITECTURE PLAN TEMPLATE:\n\n FILES NEEDED:\n  - [filename]: [purpose]\n  - [filename]: [purpose]\n  - [filename]: [purpose]\n\n SHARED DEPENDENCIES (must be created first):\n  - [dependency]: [what files need this]\n\n CREATION ORDER (numbered with reasoning):\n  1. [file] - Reason: [why this first]\n  2. [file] - Reason: [why this second]\n  3. [file] - Reason: [why this third]\n\n CROSS-FILE REFERENCES:\n  - [file A] references [file B] via [method]\n  - [file C] imports [file D] via [method]\n\n POTENTIAL ISSUES TO AVOID:\n  - [what could go wrong]\n  - [common mistake]\n```\n\n**Example filled template for portfolio website:**\n\n```\nARCHITECTURE PLAN:\n\n FILES NEEDED:\n  - styles.css: Shared styling for all pages\n  - index.html: Home page with navigation\n  - about.html: About page\n  - projects.html: Portfolio showcase\n  - contact.html: Contact form\n\n SHARED DEPENDENCIES:\n  - styles.css: All HTML files need this for consistent styling\n\n CREATION ORDER:\n  1. styles.css - Reason: Shared dependency, all HTML files will reference it\n  2. index.html - Reason: Main entry point, establishes structure\n  3. about.html - Reason: References styles.css which now exists\n  4. projects.html - Reason: References styles.css which now exists\n  5. contact.html - Reason: References styles.css which now exists\n\n CROSS-FILE REFERENCES:\n  - All HTML files link to styles.css via <link rel=\"stylesheet\">\n  - All HTML pages link to each other via <a href=\"...\">\n\n POTENTIAL ISSUES TO AVOID:\n  - Creating HTML before CSS  Would require going back to add links\n  - Inline styles in HTML  Would require extraction later\n  - Inconsistent navigation  Hard to maintain across files\n```\n\n**Use this template in your extended thinking output.**\n\n### Step 2: Optimal File Creation Order\n\n**General principles:**\n\n1. **Foundations first** - Shared dependencies before dependents\n   - CSS files before HTML files that use them\n   - Configuration files before code that needs them\n   - Base classes before derived classes\n\n2. **Core before features** - Essential files before optional ones\n   - index.html before other pages\n   - main.js before feature modules\n   - Core API before additional endpoints\n\n3. **Structure before content** - Layout before details\n   - HTML structure before detailed content\n   - API structure before implementation details\n   - Component scaffolds before full logic\n\n**Common file creation orders:**\n\n**Website project:**\n```\n1. styles.css (shared styling)\n2. index.html (home page - references styles.css)\n3. about.html (references styles.css)\n4. projects.html (references styles.css)\n5. contact.html (references styles.css)\n6. script.js (if needed)\n```\n\n**React application:**\n```\n1. package.json (dependencies)\n2. App.js (main component)\n3. components/Header.js (layout components)\n4. components/Footer.js\n5. pages/Home.js (page components)\n6. pages/About.js\n7. styles/main.css\n```\n\n**Backend API:**\n```\n1. config.js (configuration)\n2. database.js (DB connection)\n3. models/User.js (data models)\n4. routes/auth.js (route handlers)\n5. routes/api.js\n6. server.js (entry point)\n```\n\n### Step 3: Create Files with Awareness\n\n**As you create each file:**\n- Reference what's already been created\n- Note what future files will depend on this one\n- Keep consistent naming and structure\n- Add comments about dependencies\n\n### Step 4: Verify and Test\n\n**After creating all files, perform these verification checks:**\n\n####  File Path Verification\n```\n Check all file paths are correct\n  - CSS links: <link href=\"styles.css\"> (not \"style.css\" or \"css/styles.css\")\n  - JS scripts: <script src=\"script.js\"> \n  - Images: <img src=\"image.png\">\n  - Relative paths match actual file structure\n```\n\n####  Reference Loading Verification\n```\n Ensure CSS/JS references load properly\n  - HTML files can find the CSS file\n  - JavaScript imports resolve correctly\n  - No 404 errors for missing files\n  - Correct syntax in link/script tags\n```\n\n####  Navigation Verification (for websites)\n```\n Test navigation between pages\n  - All navigation links point to correct files\n  - Links use correct relative paths\n  - No broken navigation (links to non-existent pages)\n  - Back/forward navigation works logically\n```\n\n####  Cross-File Reference Verification\n```\n Validate cross-file dependencies work\n  - Components import correctly\n  - Modules can access exported functions\n  - Shared utilities are accessible\n  - API calls reference correct endpoints\n```\n\n####  Consistency Verification\n```\n Check consistency across files\n  - Naming conventions are consistent\n  - Styling is uniform (if using shared CSS)\n  - Code structure follows same patterns\n  - Documentation style matches across files\n```\n\n**Example verification for portfolio website:**\n```\nAfter creating styles.css, index.html, about.html, projects.html, contact.html:\n\n Verification checklist:\n  [] All HTML files have <link rel=\"stylesheet\" href=\"styles.css\">\n  [] styles.css exists and has content\n  [] Navigation links: \n      - index.html links to about.html, projects.html, contact.html \n      - All other pages link back to index.html \n  [] All pages use consistent styling from styles.css \n  [] No broken links or missing file references \n\nResult: Project structure verified, ready to use!\n```\n\n**If verification fails, fix issues before considering the project complete.**\n\n### When to Use This Planning Approach\n\n**ALWAYS plan first for:**\n- Websites with multiple pages (3+ HTML files)\n- Applications with multiple components\n- Projects with shared dependencies (CSS, config files)\n- API implementations with multiple endpoints\n- Documentation sets with multiple files\n- Any project where files reference each other\n\n**Don't need planning for:**\n- Single file creations\n- Truly independent files with no relationships\n- Simple, obvious structures\n\n## Passive Deep Thinking Architecture\n\n**The key insight:** Extended thinking can happen in isolated contexts (subagents/artifacts), keeping the main session lean while still getting deep analysis.\n\n### The Architectural Pattern\n\n```\nMain Session (Orchestrator)\n Stays high-level and focused\n Makes decisions based on summaries\n Delegates complex analysis\n\n         delegates with thinking triggers \n\nAnalysis Layer (Agents/Artifacts)\n Extended thinking happens HERE (5K+ tokens)\n Deep reasoning happens HERE\n Context-heavy work happens HERE\n Returns concise summaries UP (~200 tokens)\n\n         returns actionable conclusions \n\nMain Session\n Receives well-reasoned recommendations\n Context stays clean for sustained work\n```\n\n### How This Achieves Passive Deep Thinking\n\n**Without thinking delegation:**\n- Extended thinking happens in main context\n- Reasoning accumulates (~5K tokens per analysis)\n- Context fills quickly over long sessions\n- Eventually hits limits\n\n**With thinking delegation:**\n- Subagents/artifacts do extended thinking in isolation\n- Main context only receives summaries (~200 tokens)\n- Can sustain 25+ analyses before context issues\n- Deep thinking happens passively through architecture\n\n**Key benefit:** You get the depth of extended thinking without the context cost.\n\n### Implementation by Environment\n\n#### **Claude Code: Thinking Subagents**\n\nCreate subagents that automatically use extended thinking:\n\n```bash\n# Create a deep analyzer for complex decisions\npython scripts/create_subagent.py architecture-advisor --type deep_analyzer\n\n# Create thinking-enabled researcher\npython scripts/create_subagent.py pattern-researcher --type researcher\n```\n\n**Usage:**\n```\nMain session: \"I need to decide between microservices and monolith\"\n\n/agent architecture-advisor \"Analyze microservices vs monolith \nfor an e-commerce platform with 10M users, considering team size \nof 8 developers\"\n\nSubagent's isolated context:\n  - Uses \"ultrathink\" automatically\n  - 5K+ tokens of deep reasoning\n  - Evaluates tradeoffs thoroughly\n\nReturns to main: \"After deep analysis, I recommend starting with \na modular monolith because [3 key reasons]. Microservices would \nadd complexity your team size can't support yet.\"\n\nMain session: Receives actionable advice, context clean\n```\n\n**Context saved:** ~4,800 tokens per analysis\n\n#### **Web/API: Thinking Artifacts**\n\nUse artifacts as \"thinking containers\":\n\n**Pattern:**\n```\nUser: \"Analyze the best database for this use case\"\n\nClaude: \"I'll create a deep analysis artifact where I can think \nthrough this thoroughly.\"\n\n[Creates artifact: \"database-analysis.md\"]\n[Inside artifact:\n  - Extended thinking block (collapsed in UI)\n  - Detailed analysis\n  - Comparison tables\n  - Final recommendation\n]\n\nMain conversation: \"Based on the analysis artifact, I recommend \nPostgreSQL because [2-sentence summary]. See artifact for complete \nreasoning including performance comparisons and scaling considerations.\"\n```\n\n**Why this works:**\n- Thinking is visually separated (in artifact)\n- Main conversation stays summary-focused\n- User can reference artifact when needed\n- Conversational context stays lean\n\n### When to Use Thinking Delegation\n\n**Delegate to thinking agents/artifacts for:**\n- Architecture decisions\n- Technology evaluations\n- Complex tradeoff analysis\n- Multi-factor comparisons\n- Design pattern selection\n- Performance optimization strategies\n- Security assessment\n- Refactoring approach planning\n\n**Keep in main context for:**\n- Simple implementation\n- Quick queries\n- Tactical decisions with obvious answers\n- Tasks requiring full project context\n\n### Example: Complex Decision with Thinking Delegation\n\n**Scenario:** Choose state management for React app\n\n**Traditional approach (main context):**\n```\nUser: \"What state management should I use?\"\nClaude: [5K tokens of thinking in main context]\nClaude: [Another 2K tokens explaining recommendation]\nTotal: ~7K tokens consumed\n```\n\n**Thinking delegation approach:**\n```\nUser: \"What state management should I use for a large e-commerce app?\"\n\nClaude Code:\n\"This warrants deep analysis. Let me delegate to a deep analyzer.\"\n/agent architecture-advisor \"Think deeply about state management \noptions (Redux, Zustand, Jotai, Context) for large-scale e-commerce \nwith real-time inventory\"\n\n[Subagent uses ultrathink in isolated context - 5K tokens]\n[Returns summary - 200 tokens]\n\nMain context: \"The advisor recommends Zustand for these reasons...\"\nTotal in main: ~300 tokens\n```\n\n**Context efficiency:** 23x improvement while maintaining analytical depth\n\n### Compound Effect Over Long Sessions\n\n**Session without thinking delegation:**\n- Analysis 1: 7K tokens\n- Analysis 2: 7K tokens  \n- Analysis 3: 7K tokens\n- Analysis 4: 7K tokens\n- Analysis 5: 7K tokens\n- **Total: 35K tokens** (17% of 200K context)\n\n**Session with thinking delegation:**\n- Analysis 1: 300 tokens\n- Analysis 2: 300 tokens\n- Analysis 3: 300 tokens\n- Analysis 4: 300 tokens\n- Analysis 5: 300 tokens\n- **Total: 1.5K tokens** (0.75% of 200K context)\n\n**Result:** Can sustain 23x more analyses in same context window while maintaining analytical rigor.\n\n## Universal Strategies\n\n### Strategy 1: Research  Think  Implement\n\n**Works in all environments:**\n\n```\nStep 1: Research phase\n\"Search for [topic] and gather key information\"\n\nStep 2: Analysis phase  \n\"think hard about the best approach based on those findings\"\n\nStep 3: Implementation phase\n\"Now implement [specific task] using the approach we decided\"\n```\n\n**Why it works:** Each phase has clear purpose, prevents context sprawl.\n\n### Strategy 2: Artifact-Driven Development\n\n**For coding tasks:**\n\n```\n1. \"Create a [file type] artifact with [functionality]\"\n2. Test/review the artifact\n3. \"Update the artifact to add [feature]\"\n4. Iterate within the artifact\n```\n\n**Why it works:** Code lives in artifact, conversation stays focused on decisions.\n\n### Strategy 3: Document Plans Before Executing\n\n**For complex projects:**\n\n```\n1. \"think about the plan for this project\"\n2. \"Create a markdown artifact with the plan\"\n3. Reference the plan artifact as you work\n4. Update the plan artifact as decisions change\n```\n\n**Why it works:** Plan persists in artifact, you can reference it without re-explaining.\n\n### Strategy 4: Chunked Research\n\n**For large research tasks:**\n\n```\n1. \"Research aspect A of [topic]\"\n2. \"Create a summary artifact\"\n3. [New conversation or context reset]\n4. \"Research aspect B of [topic]\" \n5. \"Create a summary artifact\"\n6. \"Synthesize findings from both research phases\"\n```\n\n**Why it works:** Each research phase stays focused, final synthesis combines cleanly.\n\n## Environment-Specific Techniques\n\n### Web Interface & API\n\n**Strategies:**\n- Use extended thinking liberally for complex reasoning\n- Create artifacts for code, documents, and substantial content\n- Break long tasks into explicit phases\n- Signal context resets when changing direction\n\n**Example workflow:**\n```\n\"I need to build a REST API. Let me break this into phases:\n\nPhase 1: ultrathink about the API design and architecture\n[Review thinking output]\n\nPhase 2: Create an artifact with the OpenAPI specification\n[Review spec]\n\nPhase 3: Implement the authentication endpoints\n[Continue implementation]\n```\n\n### Claude Code (CLI)\n\n**Additional commands:**\n- `/clear` - Reset context between tasks\n- `/compact` - Compress context while keeping key decisions\n- `/continue` - Resume previous session\n- Subagents - Delegate research/testing to isolated contexts\n\n**Generate project-specific CLAUDE.md:**\n```bash\npython scripts/generate_claude_md.py --type [TYPE] --output ./CLAUDE.md\n```\n\n**Create subagents for recurring tasks:**\n```bash\npython scripts/create_subagent.py [NAME] --type [TYPE]\n```\n\nSee **Claude Code Tooling** section below for details.\n\n## Common Workflows (All Environments)\n\n### Workflow 0: Multi-File Website/Project Creation  MOST COMMON\n\n** If user said: \"create a website/app with multiple pages\"  YOU ARE IN THIS WORKFLOW RIGHT NOW**\n\n**MANDATORY WORKFLOW - FOLLOW THIS EXACT SEQUENCE:**\n\n```\n STEP 1: STOP AND THINK (DO THIS FIRST, ALWAYS)\n   Output: \"Think hard about the architecture for this [project]...\"\n   [Extended thinking plans: files needed, creation order, dependencies]\n\n STEP 2: ANNOUNCE THE PLAN\n   Output: \"I'll create these files in this order:\n            1. styles.css (shared styling)\n            2. index.html (home page)\n            3. about.html\n            4. projects.html\n            5. contact.html\"\n\n STEP 3: CREATE FOUNDATION FILES FIRST\n   Create: styles.css\n   \n STEP 4: CREATE DEPENDENT FILES\n   Create: index.html (references styles.css)\n   Create: about.html (references styles.css)\n   Create: projects.html (references styles.css)\n   Create: contact.html (references styles.css)\n\n STEP 5: VERIFY\n   Check: All HTML files reference styles.css correctly\n```\n\n**Example: Portfolio Website Request**\n\n```\nUser: \"Create a portfolio website with home, about, projects, and contact pages\"\n\n BEFORE YOU CREATE ANY FILES, YOU MUST OUTPUT:\n\n\"Let me think hard about the architecture first...\"\n\n[Extended thinking output should plan:\n - Files needed: index.html, about.html, projects.html, contact.html, styles.css\n - Optimal order: styles.css FIRST (it's a shared dependency), then HTML pages\n - Dependencies: All HTML files will reference styles.css\n - Structure: Simple multi-page site with shared stylesheet]\n\nTHEN announce the plan:\n\n\"I'll create the files in this order:\n\n1. styles.css - Shared styling for all pages\n2. index.html - Home page (will reference styles.css)\n3. about.html - About page (will reference styles.css)\n4. projects.html - Projects page (will reference styles.css)\n5. contact.html - Contact page (will reference styles.css)\n\nThis order ensures all dependencies are in place before files that need them.\"\n\nTHEN create files in that exact order.\n```\n\n** WRONG - What NOT to do:**\n```\nUser: \"Create a portfolio website with home, about, projects, and contact pages\"\n\n[Immediately creates index.html without thinking]\n[Creates about.html]\n[Creates projects.html]\n[Realizes CSS should be shared, has to go back and add it]\n\nThis wastes effort and context!\n```\n\n** RIGHT - What to do:**\n```\nUser: \"Create a portfolio website with home, about, projects, and contact pages\"\n\n\"Think hard about architecture...\" [Plans first]\n\"I'll create in this order: CSS first, then HTML pages\" [Announces plan]\n[Creates styles.css]\n[Creates HTML pages that reference styles.css]\n\nEfficient! No redundant work!\n```\n\n---\n\n### Workflow 1: Complex Decision Making\n\n**With Thinking Delegation:**\n\n**Claude Code:**\n```\nUser: \"Should we use microservices or monolith?\"\n\n1. /agent deep-analyzer \"Ultrathink about architecture choice \n   for 10M user e-commerce platform, 8 dev team\"\n2. [Receives well-reasoned recommendation in main context]\n3. Make decision based on analysis\n4. Proceed with implementation\n```\n\n**Web/API:**\n```\nUser: \"Should we use microservices or monolith?\"\n\n1. \"Create a deep-analysis.md artifact and ultrathink about this\"\n2. [Artifact contains extended thinking + conclusion]\n3. Main conversation: \"Based on analysis, recommend monolith because...\"\n4. Proceed with implementation\n```\n\n**Context efficiency:** Deep thinking happens, main context stays clean.\n\n---\n\n### Workflow 2: Complex Feature Development\n\n```\nPhase 1: Architecture Analysis\nClaude Code: /agent deep-analyzer \"Think deeply about architecture for [feature]\"\nWeb/API: \"Create architecture-analysis artifact with deep thinking\"\n[Isolated thinking  summary to main]\n\nPhase 2: Design Planning\n\"Based on that analysis, create implementation plan artifact\"\n[Main context references analysis conclusions]\n\nPhase 3: Implementation\n\"Implement component A based on the plan\"\n[Create code artifact]\n\nPhase 4: Testing\nClaude Code: /agent test-runner \"Run tests and analyze failures\"\nWeb/API: \"Run tests\" [test output in separate space]\n\nPhase 5: Integration\n\"Integrate based on architecture plan\"\n```\n\n**Why it works:** Each phase has clear purpose, thinking isolated where needed.\n\n---\n\n### Workflow 3: Research & Technology Evaluation\n\n```\nPhase 1: Deep Research\nClaude Code: /agent pattern-researcher \"Research and think hard about \n  authentication approaches, analyze tradeoffs\"\nWeb/API: \"Create research-findings artifact and think through options\"\n\nPhase 2: Synthesis\n[Receives summary of findings]\n\"Create comparison table artifact\"\n\nPhase 3: Recommendation\nClaude Code: /agent deep-analyzer \"Based on research, recommend approach\"\nWeb/API: \"Based on research artifact, ultrathink and recommend\"\n\nPhase 4: Implementation\n\"Implement recommended approach\"\n```\n\n**Why it works:** Research and deep analysis isolated, implementation focused.\n\n---\n\n### Workflow 4: Code Generation & Iteration\n\n```\n1. \"Create a [language] script that [functionality]\"\n    Artifact created\n\n2. \"Add [feature] to the script\"\n    Artifact updated\n\n3. \"Optimize the [specific part]\"\n    Targeted update\n\n4. \"Add error handling\"\n    Incremental improvement\n```\n\n**Why it works:** All code lives in the artifact, conversation stays focused on what to change.\n\n---\n\n### Workflow 5: Refactoring with Analysis\n\n**Claude Code:**\n```\n1. /agent analyzer \"Think hard about refactoring approach \n   for legacy auth system\"\n2. [Receives analysis in main: strategy, risks, order]\n3. \"Create REFACTOR.md plan based on analysis\"\n4. /clear\n5. For each module:\n   - Refactor according to plan\n   - /agent test-runner \"verify changes\"\n   - Commit\n   - /clear before next\n```\n\n**Web/API:**\n```\n1. \"Create refactoring-analysis artifact, think deeply about approach\"\n2. [Artifact has thinking + strategy]\n3. \"Create refactoring-plan artifact based on analysis\"\n4. Implement module by module\n5. Reference plan artifact as you work\n```\n\n**Why it works:** Deep analysis happens once (isolated), execution follows clean plan.\n\n\n\n## Best Practices (Universal)\n\n### 1. Delegate Complex Analysis to Isolated Contexts\n\n**The most powerful pattern for context efficiency:**\n\n**Claude Code:**\n```\n /agent deep-analyzer \"Ultrathink about [complex decision]\"\n \"Think about [complex decision]\" [happens in main context]\n```\n\n**Web/API:**\n```\n \"Create analysis artifact and ultrathink about [decision]\"\n \"Ultrathink about [decision]\" [thinking stays in conversation]\n```\n\n**Benefit:** 5K+ tokens of reasoning happens in isolation, main context receives ~200 token summary. 23x context efficiency while maintaining analytical depth.\n\n### 2. Use Extended Thinking for Planning\nBefore diving into implementation:\n```\n\"think hard about the approach for [task]\"\n```\n\n**Even better with delegation:**\n- Claude Code: Delegate to deep_analyzer subagent\n- Web/API: Use thinking artifact\n\n**Benefit:** Reasoning stays out of main context, you get thoughtful plans.\n\n### 3. Create Artifacts for Substantial Content\nDon't inline long code or documents in conversation:\n```\n \"Create a Python script artifact that [functionality]\"\n \"Show me the Python code for [functionality]\"\n```\n\n**Benefit:** Content lives in artifacts, not conversation history.\n\n### 4. Break Complex Tasks Into Explicit Phases\nState phase transitions clearly:\n```\n\"Phase 1 complete. Moving to Phase 2: [description]\"\n```\n\n**With thinking delegation:**\n```\nPhase 1: /agent deep-analyzer \"analyze approaches\"\nPhase 2: Implement based on analysis\n```\n\n**Benefit:** Each phase has clear purpose and boundaries.\n\n### 5. Document Decisions in Artifacts\nCreate persistent references:\n```\n\"Create a decisions.md artifact tracking our key choices\"\n```\n\n**Benefit:** Can reference decisions without re-explaining full context.\n\n### 6. Progressive Disclosure\nDon't request everything at once:\n```\n \"First, analyze the requirements\"\n    \"Now, design the data model\"\n    \"Now, implement the core logic\"\n\n \"Analyze requirements, design data model, and implement everything\"\n```\n\n**Benefit:** Each step builds on the last without overwhelming context.\n\n### 7. Use Thinking for Exploration\nWhen uncertain about approach:\n```\n\"ultrathink about multiple approaches and recommend the best one\"\n```\n\n**Even better:** Delegate to deep_analyzer (Claude Code) or thinking artifact (Web/API)\n\n**Benefit:** Deep analysis without context clutter.\n\n### 8. Signal Context Resets\nWhen changing direction:\n```\n\"Setting aside the previous approach, let's try a different angle...\"\n```\n\n**Benefit:** Clear boundaries prevent old context from interfering.\n\n## Advanced Patterns (Universal)\n\n### Pattern: Iterative Refinement\n```\n1. \"Create initial version of [artifact]\"\n2. Review\n3. \"Improve [specific aspect]\"\n4. Review\n5. \"Add [feature]\"\n6. Continue iterating\n```\n\n**Benefit:** Focused improvements, not recreating everything each time.\n\n### Pattern: Multi-Artifact Projects\n```\n1. \"Create architecture.md artifact\"\n2. \"Create database-schema.sql artifact\"\n3. \"Create api-spec.yaml artifact\"\n4. \"Now implement based on these artifacts\"\n```\n\n**Benefit:** Each artifact is a stable reference, no context accumulation.\n\n### Pattern: Thinking  Document  Execute\n```\n1. \"ultrathink about [complex problem]\"\n2. \"Document the decision in a plan artifact\"\n3. \"Execute phase 1 of the plan\"\n4. Reference plan artifact as you continue\n```\n\n**Benefit:** Separation of reasoning, planning, and execution.\n\n### Pattern: Chunked Content Generation\nFor long documents:\n```\n1. \"Create outline artifact\"\n2. \"Write introduction (add to artifact)\"\n3. \"Write section 1 (add to artifact)\"\n4. Continue section by section\n```\n\n**Benefit:** Build progressively without loading entire document context each time.\n\n---\n\n#  PART 2: CLI-SPECIFIC FEATURES (Claude Code Users Only)\n\n**The sections below are ONLY for users of Claude Code CLI. Web and API users can skip this part.**\n\n---\n\n## Claude Code Tooling (Bonus Features)\n\nWhen using Claude Code CLI, additional optimization tools are available:\n\n### Quick Start: Generate CLAUDE.md\n\nCreate a context-optimized CLAUDE.md file for your project:\n\n```bash\npython scripts/generate_claude_md.py --type [TYPE] --output ./CLAUDE.md\n```\n\n**Available types:**\n- `general` - General-purpose projects\n- `backend` - API/service projects  \n- `frontend` - Web applications\n- `fullstack` - Full-stack applications\n- `data` - Data science/ML projects\n- `library` - Library/package development\n\n**What it does:** Generates a CLAUDE.md file that Claude Code reads automatically, providing persistent project guidance across sessions.\n\n### Create Subagents\n\nFor recurring tasks, create dedicated subagents:\n\n```bash\npython scripts/create_subagent.py [NAME] --type [TYPE] --output [DIR]\n```\n\n**Available types:**\n- `researcher` - Documentation searches with deep analysis\n- `tester` - Test execution with failure analysis\n- `analyzer` - Code analysis with architectural insights\n- `builder` - Build and deployment tasks\n- `deep_analyzer` - Complex decisions requiring extensive thinking (recommended for architecture, tech choices, design patterns)\n\n**What it does:** Creates `.claude/agents/[NAME].md` configuration that can be invoked with:\n```\n/agent [NAME] [task description]\n```\n\n### Claude Code Commands\n\n- `/clear` - Reset context between tasks\n- `/compact` - Compress context while preserving key decisions\n- `/continue` - Resume previous session\n- `/agent [NAME]` - Delegate task to a subagent with isolated context\n\n### Claude Code Patterns\n\n**Pattern: Deep Analysis Delegation**\n```\n1. /clear (start fresh)\n2. /agent deep-analyzer \"Ultrathink about [complex decision]\"\n3. [Receives well-reasoned analysis in ~200 tokens]\n4. Make decision and implement\n5. Main context stayed clean throughout\n```\n\n**Pattern: Research with Thinking**\n```\n1. /agent pattern-researcher \"Research [topic] and think hard about implications\"\n2. [Subagent searches + thinks in isolation]\n3. Review findings in main context\n4. Proceed with informed decision\n```\n\n**Pattern: Test-Driven Development with Analysis**\n```\n1. Write test in main context\n2. /agent test-runner \"Run test and think hard if it fails\"\n3. [Subagent analyzes root cause in isolation]\n4. Implement fix based on analysis\n5. /agent test-runner \"verify\"\n6. If passing: commit and /clear\n```\n\n**Pattern: Architecture Evolution**\n```\n1. /agent analyzer \"Think deeply about current architecture issues\"\n2. [Receives analysis: bottlenecks, technical debt, opportunities]\n3. /agent deep-analyzer \"Recommend evolution strategy\"\n4. Create EVOLUTION.md plan\n5. /clear\n6. Execute plan phase by phase\n```\n\n**Pattern: Large Refactoring with Thinking**\n```\n1. /agent analyzer \"Think hard about refactoring scope and risks\"\n2. [Receives risk assessment + strategy]\n3. Create REFACTOR.md plan\n4. /clear\n5. For each file:\n   - Load and refactor\n   - /agent test-runner \"analyze test results\"\n   - /clear before next\n```\n\nFor detailed Claude Code patterns, see [references/subagent_patterns.md](references/subagent_patterns.md) and [references/context_strategies.md](references/context_strategies.md).\n\n## Troubleshooting (All Environments)\n\n### \"Responses are getting less focused\"\n**Symptoms:** Claude references old, irrelevant information or responses drift off topic.\n\n**Solutions:**\n- **Web/API:** Explicitly state \"Setting aside previous discussion, let's focus on...\"\n- **Claude Code:** Use `/clear` or `/compact`\n- **Universal:** Break task into new phases with clear boundaries\n\n### \"Complex task feels overwhelming\"\n**Symptoms:** Unsure where to start, too many moving parts.\n\n**Solutions:**\n1. \"think harder about breaking this into phases\"\n2. Create a planning artifact\n3. Execute one phase at a time\n4. Reference plan artifact as you go\n\n### \"Conversation getting too long\"\n**Symptoms:** Long history, hard to track what's been decided.\n\n**Solutions:**\n- **Web/API:** Create a \"decisions.md\" artifact to summarize key points\n- **Claude Code:** Use `/compact` to compress history\n- **Universal:** Start a new conversation with \"Previously we decided X, Y, Z. Now let's...\"\n\n### \"Need to maintain context across sessions\"\n**Symptoms:** Have to re-explain everything each time.\n\n**Solutions:**\n- Create artifacts documenting key decisions and context\n- **Claude Code:** Use CLAUDE.md for persistent project memory\n- Start new sessions with: \"Continuing from previous work where we [brief summary]\"\n\n### \"Code keeps being regenerated instead of edited\"\n**Symptoms:** Small changes result in entire code rewrites.\n\n**Solutions:**\n1. Use artifacts for code\n2. Request specific edits: \"Update the handle_request function to add validation\"\n3. Don't say \"show me the code again\" - reference the existing artifact\n\n### \"Responses include too much explanation\"\n**Symptoms:** Getting lengthy explanations when you just want output.\n\n**Solutions:**\n- Be explicit: \"Just create the artifact, minimal explanation\"\n- \"Output only, no commentary\"\n- \"Concise response please\"\n\n### \"Extended thinking not being used\"\n**Symptoms:** Jumping straight to solutions without analysis.\n\n**Solutions:**\n- Explicitly request: \"think hard about...\"\n- Use stronger triggers: \"ultrathink about...\"\n- Ask for planning: \"think about multiple approaches\"\n\n## Reference Documentation\n\nFor deeper understanding, consult the reference files:\n\n- **[references/context_strategies.md](references/context_strategies.md)** - Comprehensive workflows and scenario-based strategies (includes Claude Code specific strategies)\n- **[references/subagent_patterns.md](references/subagent_patterns.md)** - Detailed subagent usage patterns for Claude Code users\n\nLoad these when you need:\n- Scenario-based workflow guidance\n- Advanced context management techniques\n- Claude Code-specific patterns\n- Troubleshooting complex context issues\n\n## Scripts Reference (Claude Code Users)\n\n### generate_claude_md.py\n\nGenerate project-specific CLAUDE.md files:\n\n```bash\npython scripts/generate_claude_md.py --type TYPE --output PATH\n```\n\n**Options:**\n- `--type`: Project type (general, backend, frontend, fullstack, data, library)\n- `--output`: Output path (default: ./CLAUDE.md)\n\n### create_subagent.py\n\nCreate subagent configurations:\n\n```bash\npython scripts/create_subagent.py NAME --type TYPE --output DIR\n```\n\n**Options:**\n- `NAME`: Agent name (e.g., test-runner, doc-searcher)\n- `--type`: Agent type (researcher, tester, analyzer, builder)\n- `--output`: Output directory (default: current directory)\n\n## Getting the Most from This Skill\n\n### For All Claude Users:\n\n1. **Delegate complex analysis** - Use thinking delegation architecture\n   - Web/API: Create analysis artifacts for deep thinking\n   - Claude Code: Use deep_analyzer subagent for decisions\n2. **Use extended thinking liberally** - \"think hard\" for planning, but delegate when possible\n3. **Create artifacts for substantial content** - Keep conversation focused\n4. **Break tasks into explicit phases** - Clear boundaries prevent context sprawl\n5. **Document decisions in artifacts** - Persistent references you can return to\n\n### For Claude Code Users (Additional):\n\n6. **Create thinking-enabled subagents immediately:**\n   ```bash\n   python scripts/create_subagent.py architecture-advisor --type deep_analyzer\n   python scripts/create_subagent.py pattern-researcher --type researcher\n   python scripts/create_subagent.py code-analyzer --type analyzer\n   python scripts/create_subagent.py test-analyzer --type tester\n   ```\n7. **Generate CLAUDE.md for each project** - Persistent project memory\n8. **Practice thinking delegation** - `/agent deep-analyzer` for complex decisions\n9. **Use `/clear` between major tasks** - Start each task fresh\n10. **Monitor context usage** - Claude reports remaining tokens\n\n### Activation Tips:\n\nThis skill activates automatically for complex queries, but you can explicitly invoke it:\n- \"Help me manage context for this task\"\n- \"What's the best approach to keep context efficient?\"\n- \"Analyze this decision deeply but keep context clean\"\n- \"Think deeply about this\" (will suggest delegation)\n\n### Measuring Success:\n\n**Signs thinking delegation is working:**\n- Complex decisions made with minimal main context usage\n- Multiple analyses in single session without context bloat\n- Clear, well-reasoned recommendations without verbose explanations\n- Can sustain 10+ complex analyses in one session\n\n**Context efficiency metrics:**\n- Traditional: ~7K tokens per complex analysis\n- With delegation: ~300 tokens per complex analysis\n- **Target:** 20+ analyses per 200K context window\n\nThe goal is sustainable, high-quality Claude interactions that maintain performance and analytical depth regardless of task complexity or conversation length."
              }
            ]
          },
          {
            "name": "plugin-master",
            "description": "Complete Claude Code plugin development system. PROACTIVELY activate when users want to: (1) Create/build plugins with 2025 features, (2) Add skills/commands/agents/hooks, (3) Validate plugin structure, (4) Publish to marketplace, (5) Get plugin development guidance. Provides: agent-first design patterns, progressive disclosure skills, hook automation, MCP integration, marketplace publishing. Includes plugin-expert agent and validation utilities.",
            "source": "./plugins/plugin-master",
            "category": null,
            "version": "3.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install plugin-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/create-plugin",
                "description": "Create a comprehensive Claude Code plugin with all necessary components and marketplace structure",
                "path": "plugins/plugin-master/commands/create-plugin.md",
                "frontmatter": {
                  "description": "Create a comprehensive Claude Code plugin with all necessary components and marketplace structure",
                  "argument-hint": "[plugin-name or description]"
                },
                "content": "# Create Plugin\n\nCreate production-ready Claude Code plugins with complete structure, documentation, and marketplace packaging.\n\n## Process\n\n### Step 1: Detect Context\n\nBefore creating files:\n\n1. **Check for marketplace repo:**\n   ```bash\n   if [[ -f .claude-plugin/marketplace.json ]]; then\n       # Create in plugins/ subdirectory\n       PLUGIN_DIR=\"plugins/PLUGIN_NAME\"\n   else\n       # Create in current directory\n       PLUGIN_DIR=\"PLUGIN_NAME\"\n   fi\n   ```\n\n2. **Get author from git config:**\n   ```bash\n   AUTHOR_NAME=$(git config user.name)\n   AUTHOR_EMAIL=$(git config user.email)\n   ```\n\n### Step 2: Create Structure\n\nCreate required directories:\n\n```bash\nmkdir -p $PLUGIN_DIR/.claude-plugin\nmkdir -p $PLUGIN_DIR/agents\nmkdir -p $PLUGIN_DIR/skills/domain-knowledge\n```\n\n### Step 3: Create Files\n\n**Required files:**\n\n1. **`.claude-plugin/plugin.json`** - Plugin manifest\n2. **`agents/domain-expert.md`** - Primary expert agent\n3. **`README.md`** - Documentation\n\n**Optional files:**\n\n- `skills/domain-knowledge/SKILL.md` - Core knowledge skill\n- `commands/*.md` - Slash commands (0-2 max)\n- `hooks/hooks.json` - Event automation\n- `.mcp.json` - MCP server configuration\n\n### Step 4: Register in Marketplace\n\n**CRITICAL**: If `.claude-plugin/marketplace.json` exists at repo root, add the plugin entry:\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"source\": \"./plugins/plugin-name\",\n  \"description\": \"Same as plugin.json description\",\n  \"version\": \"1.0.0\",\n  \"author\": { \"name\": \"Author Name\" },\n  \"keywords\": [\"keyword1\", \"keyword2\"]\n}\n```\n\n## Plugin Design Rules\n\n### Agent-First Design\n\n- **Primary interface**: ONE expert agent named `{domain}-expert`\n- **Minimal commands**: Only 0-2 for specific automation workflows\n- **Naming**: `docker-master`  `docker-expert`\n\n### plugin.json Rules\n\n```json\n{\n  \"name\": \"plugin-name\",           // Required, kebab-case\n  \"version\": \"1.0.0\",              // String, not number\n  \"description\": \"...\",\n  \"author\": {                      // Object, NOT string\n    \"name\": \"Name\",\n    \"email\": \"email@example.com\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"word1\", \"word2\"]   // Array, NOT string\n}\n```\n\n**DO NOT include**: `agents`, `skills`, `slashCommands` - these are auto-discovered\n\n### Agent Format\n\n```markdown\n---\nname: domain-expert\ndescription: |\n  Use this agent when... Examples:\n  <example>\n  Context: ...\n  user: \"...\"\n  assistant: \"...\"\n  <commentary>Why trigger</commentary>\n  </example>\nmodel: inherit\ncolor: blue\n---\n\nSystem prompt content...\n```\n\n## Validation\n\nAfter creating plugin, validate with:\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.sh plugins/plugin-name\n```\n\nOr use: `/validate-plugin`\n\n## Examples\n\n```\n/create-plugin for Docker workflow automation\n/create-plugin API testing helper\n/create-plugin deployment automation with rollback\n```\n\n## Tips\n\n- Infer requirements from context - don't ask unnecessary questions\n- Use git config values for author fields\n- Create progressive disclosure skills (SKILL.md + references/)\n- Include comprehensive README with installation instructions\n- Always register in marketplace.json if in marketplace repo"
              },
              {
                "name": "/validate-plugin",
                "description": "Validate plugin structure, manifests, and configuration against 2025 standards",
                "path": "plugins/plugin-master/commands/validate-plugin.md",
                "frontmatter": {
                  "description": "Validate plugin structure, manifests, and configuration against 2025 standards",
                  "argument-hint": "[plugin-path]"
                },
                "content": "# Validate Plugin\n\nSystematically validate plugin files against Claude Code requirements and best practices.\n\n## Process\n\n### Step 1: Locate Plugin\n\nUse current directory or specified path:\n\n```bash\nPLUGIN_PATH=\"${1:-.}\"\n```\n\n### Step 2: Validate plugin.json\n\n**Check location:**\n- Must be at `.claude-plugin/plugin.json`\n\n**Check required fields:**\n- `name` - Present and kebab-case\n\n**Check field formats:**\n- `author` - Must be object `{ \"name\": \"...\" }`, NOT string\n- `version` - Must be string `\"1.0.0\"`, NOT number\n- `keywords` - Must be array `[\"word1\"]`, NOT string\n\n**Check for deprecated fields:**\n- `agents`, `skills`, `slashCommands` should NOT be in plugin.json\n\n### Step 3: Check Directory Structure\n\n```\nplugin-name/\n .claude-plugin/\n    plugin.json          # Required\n commands/                 # Optional, auto-discovered\n agents/                   # Optional, auto-discovered\n skills/                   # Optional, auto-discovered\n    skill-name/\n        SKILL.md\n hooks/\n     hooks.json           # Optional\n```\n\n### Step 4: Validate Components\n\n**For each command in commands/:**\n- Check frontmatter exists (starts with `---`)\n- Check `description` field present\n\n**For each agent in agents/:**\n- Check frontmatter exists\n- Check required fields: `name`, `description`, `model`, `color`\n- Check `<example>` blocks in description\n\n**For each skill in skills/*/:**\n- Check SKILL.md exists\n- Check frontmatter exists\n- Check `description` field present\n\n**For hooks/hooks.json:**\n- Check valid JSON syntax\n- Check event names are valid\n\n### Step 5: Check marketplace.json (if present)\n\nIf `.claude-plugin/marketplace.json` exists at repo root:\n- Check plugin is registered\n- Check source path is correct\n- Check descriptions match\n\n## Validation Report\n\nGenerate report with:\n\n```\n================================\nPlugin Validation Report\n================================\n\nPlugin: plugin-name\nPath: /path/to/plugin\n\n plugin.json found\n Valid JSON syntax\n Name: plugin-name\n Author is object format\n Version: 1.0.0\n\nComponents:\n 1 agent(s) found\n 2 skill(s) found\n 1 command(s) found\n\nWarnings:\n- Consider adding keywords for discovery\n\nErrors:\n(none)\n\n================================\nPASSED\n================================\n```\n\n## Severity Levels\n\n**Critical (Must Fix):**\n- plugin.json missing or invalid\n- Missing required fields\n- Wrong field types (author as string, etc.)\n\n**Warnings (Should Fix):**\n- Missing recommended fields\n- Inconsistent naming\n- Missing frontmatter\n\n**Suggestions (Nice to Have):**\n- Add more keywords\n- Add examples\n- Improve documentation\n\n## Usage\n\n```bash\n# Validate current directory\n/validate-plugin\n\n# Validate specific path\n/validate-plugin plugins/my-plugin\n\n# Validate before publishing\n/validate-plugin . && echo \"Ready to publish!\"\n```\n\n## Quick Validation Script\n\nRun validation script directly:\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/validate-plugin.sh [path]\n```\n\n## Checklist\n\nBefore publishing:\n\n- [ ] plugin.json has valid syntax\n- [ ] `name` is kebab-case\n- [ ] `author` is an object\n- [ ] `version` is a string\n- [ ] `keywords` is an array\n- [ ] No `agents`/`skills`/`slashCommands` fields\n- [ ] All components have frontmatter\n- [ ] Agent has `<example>` blocks\n- [ ] Registered in marketplace.json (if applicable)"
              }
            ],
            "skills": [
              {
                "name": "advanced-features-2025",
                "description": "Advanced 2025 Claude Code plugin features. PROACTIVELY activate for:\n(1) Agent Skills with progressive disclosure\n(2) Hook automation (PreToolUse, PostToolUse, etc.)\n(3) MCP server integration\n(4) Repository-level configuration\n(5) Team plugin distribution\n(6) Context efficiency optimization\nProvides cutting-edge plugin capabilities and patterns.\n",
                "path": "plugins/plugin-master/skills/advanced-features-2025/SKILL.md",
                "frontmatter": {
                  "name": "advanced-features-2025",
                  "description": "Advanced 2025 Claude Code plugin features. PROACTIVELY activate for:\n(1) Agent Skills with progressive disclosure\n(2) Hook automation (PreToolUse, PostToolUse, etc.)\n(3) MCP server integration\n(4) Repository-level configuration\n(5) Team plugin distribution\n(6) Context efficiency optimization\nProvides cutting-edge plugin capabilities and patterns.\n",
                  "license": "MIT"
                },
                "content": "# Advanced Plugin Features (2025)\n\n## Quick Reference\n\n| Feature | Location | Purpose |\n|---------|----------|---------|\n| Agent Skills | `skills/*/SKILL.md` | Dynamic knowledge loading |\n| Hooks | `hooks/hooks.json` | Event automation |\n| MCP Servers | `.mcp.json` | External integrations |\n| Team Config | `.claude/settings.json` | Repository plugins |\n\n| Hook Event | When Fired | Use Case |\n|------------|------------|----------|\n| PreToolUse | Before tool | Validation |\n| PostToolUse | After tool | Testing, linting |\n| SessionStart | Session begins | Logging, setup |\n| SessionEnd | Session ends | Cleanup |\n| UserPromptSubmit | Prompt submitted | Preprocessing |\n| PreCompact | Before compact | State save |\n| Notification | Notification shown | Custom alerts |\n| Stop | User stops | Cleanup |\n| SubagentStop | Subagent ends | Logging |\n\n| Variable | Purpose |\n|----------|---------|\n| `${CLAUDE_PLUGIN_ROOT}` | Plugin installation path |\n| `${TOOL_INPUT_*}` | Tool input parameters |\n\n## Agent Skills\n\n### Concept\n\nSkills are dynamically loaded based on task context, enabling:\n- **Unbounded capacity**: Knowledge split across files\n- **Context efficiency**: Only load what's needed\n- **Progressive disclosure**: Three-tier loading\n\n### Three-Tier Loading\n\n1. **Frontmatter**: Loaded at startup (triggers)\n2. **SKILL.md body**: Loaded on activation\n3. **references/**: Loaded when detail needed\n\n### Structure\n\n```\nskills/\n skill-name/\n     SKILL.md           # Core content\n     references/        # Detailed docs\n        deep-dive.md\n     examples/          # Working code\n        example.md\n     scripts/           # Utilities\n         tool.sh\n```\n\n### SKILL.md Format\n\n```markdown\n---\nname: skill-name\ndescription: |\n  When to activate this skill. Include:\n  (1) Use case 1\n  (2) Use case 2\n  Provides: what it offers\n---\n\n# Skill Title\n\n## Quick Reference\n[Tables, key points]\n\n## Core Content\n[Essential information - keep lean]\n\n## Additional Resources\nSee `references/` for detailed guidance.\n```\n\n## Hooks\n\n### Configuration\n\n**Inline in plugin.json:**\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [{\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [{\n        \"type\": \"command\",\n        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/lint.sh\"\n      }]\n    }]\n  }\n}\n```\n\n**Separate hooks.json:**\n```json\n{\n  \"PostToolUse\": [{\n    \"matcher\": \"Write\",\n    \"hooks\": [{\n      \"type\": \"command\",\n      \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/format.sh\",\n      \"timeout\": 5000\n    }]\n  }]\n}\n```\n\n### Matchers\n\n- `Write` - File writes\n- `Edit` - File edits\n- `Bash` - Shell commands\n- `Write|Edit` - Multiple tools\n- `.*` - Any tool (use sparingly)\n\n### Common Patterns\n\n**Auto-test after changes:**\n```json\n{\n  \"PostToolUse\": [{\n    \"matcher\": \"Write|Edit\",\n    \"hooks\": [{\n      \"type\": \"command\",\n      \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/run-tests.sh\"\n    }]\n  }]\n}\n```\n\n**Validate before Bash:**\n```json\n{\n  \"PreToolUse\": [{\n    \"matcher\": \"Bash\",\n    \"hooks\": [{\n      \"type\": \"command\",\n      \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/validate-cmd.sh\"\n    }]\n  }]\n}\n```\n\n## MCP Server Integration\n\n### Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"server-name\": {\n      \"command\": \"node\",\n      \"args\": [\"${CLAUDE_PLUGIN_ROOT}/mcp/server.js\"],\n      \"env\": {\n        \"API_KEY\": \"${API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n### External Services\n\n```json\n{\n  \"mcpServers\": {\n    \"stripe\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@stripe/mcp-server\"],\n      \"env\": {\n        \"STRIPE_API_KEY\": \"${STRIPE_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n## Repository Configuration\n\n### Team Distribution\n\nCreate `.claude/settings.json` at repo root:\n\n```json\n{\n  \"extraKnownMarketplaces\": [\n    \"company/internal-plugins\"\n  ],\n  \"plugins\": {\n    \"enabled\": [\n      \"deployment-helper@company\",\n      \"code-standards@company\"\n    ]\n  }\n}\n```\n\n### Workflow\n\n1. Maintainer creates `.claude/settings.json`\n2. Team members clone repo\n3. Trust folder when prompted\n4. Plugins install automatically\n\n## Best Practices\n\n### Progressive Disclosure\n\n- Keep SKILL.md under 500 lines\n- Move details to `references/`\n- Use `examples/` for working code\n- Reference files with relative paths\n\n### Hooks\n\n- Use specific matchers (avoid `.*`)\n- Set reasonable timeouts\n- Use `${CLAUDE_PLUGIN_ROOT}` for paths\n- Test scripts independently\n\n### MCP\n\n- Document required env vars\n- Provide setup instructions\n- Use environment variables for secrets\n- Test connection before distribution\n\n## Additional Resources\n\nFor detailed patterns, see:\n- **`references/hooks-advanced.md`** - Complete hook patterns\n- **`references/mcp-patterns.md`** - MCP integration examples\n- **`references/team-distribution.md`** - Repository configuration\n- **`examples/hook-scripts.md`** - Working hook scripts"
              },
              {
                "name": "plugin-master",
                "description": "Complete Claude Code plugin development system. PROACTIVELY activate when users want to:\n(1) Create/build/make plugins with 2025 features\n(2) Add skills/commands/agents to plugins\n(3) Package existing code as plugins\n(4) Publish plugins to marketplace\n(5) Validate plugin structure\n(6) Get plugin development guidance\nAutonomously creates production-ready plugins with proper structure and best practices.\n",
                "path": "plugins/plugin-master/skills/plugin-master/SKILL.md",
                "frontmatter": {
                  "name": "plugin-master",
                  "description": "Complete Claude Code plugin development system. PROACTIVELY activate when users want to:\n(1) Create/build/make plugins with 2025 features\n(2) Add skills/commands/agents to plugins\n(3) Package existing code as plugins\n(4) Publish plugins to marketplace\n(5) Validate plugin structure\n(6) Get plugin development guidance\nAutonomously creates production-ready plugins with proper structure and best practices.\n",
                  "license": "MIT"
                },
                "content": "# Plugin Development Guide\n\n## Quick Reference\n\n| Component | Location | Required |\n|-----------|----------|----------|\n| Plugin manifest | `.claude-plugin/plugin.json` | Yes |\n| Commands | `commands/*.md` | No (auto-discovered) |\n| Agents | `agents/*.md` | No (auto-discovered) |\n| Skills | `skills/*/SKILL.md` | No (auto-discovered) |\n| Hooks | `hooks/hooks.json` | No |\n| MCP Servers | `.mcp.json` | No |\n\n| Task | Action |\n|------|--------|\n| Create plugin | Ask: \"Create a plugin for X\" |\n| Validate plugin | Run: `/validate-plugin` |\n| Install from marketplace | `/plugin marketplace add user/repo` then `/plugin install name@user` |\n\n## Critical Rules\n\n### Directory Structure\n\n```\nplugin-name/\n .claude-plugin/\n    plugin.json          # MUST be inside .claude-plugin/\n agents/\n    domain-expert.md\n commands/\n skills/\n    skill-name/\n        SKILL.md\n        references/\n        examples/\n README.md\n```\n\n### Plugin.json Schema\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Complete [domain] expertise. PROACTIVELY activate for: (1) ...\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"email@example.com\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"keyword1\", \"keyword2\"]\n}\n```\n\n**Validation Rules:**\n- `author` MUST be an object `{ \"name\": \"...\" }` - NOT a string\n- `version` MUST be a string `\"1.0.0\"` - NOT a number\n- `keywords` MUST be an array `[\"word1\", \"word2\"]` - NOT a string\n- Do NOT include `agents`, `skills`, `slashCommands` - these are auto-discovered\n\n### YAML Frontmatter (REQUIRED)\n\nALL markdown files in agents/, commands/, skills/ MUST begin with frontmatter:\n\n```markdown\n---\ndescription: Brief description of what this component does\n---\n\n# Content...\n```\n\n**Without frontmatter, components will NOT load.**\n\n## Plugin Design Philosophy (2025)\n\n### Agent-First Design\n\n- **Primary interface**: ONE expert agent named `{domain}-expert`\n- **Minimal commands**: Only 0-2 for automation workflows\n- **Why**: Users want conversational interaction, not command menus\n\n**Naming Standard:**\n- `docker-master`  agent named `docker-expert`\n- `terraform-master`  agent named `terraform-expert`\n\n### Progressive Disclosure for Skills\n\nSkills use three-tier loading:\n1. **Frontmatter** - Loaded at startup for triggering\n2. **SKILL.md body** - Loaded when skill activates\n3. **references/** - Loaded only when specific detail needed\n\nThis enables unbounded capacity without context bloat.\n\n## Creating a Plugin\n\n### Step 1: Detect Repository Context\n\nBefore creating files, check:\n```bash\n# Check if in marketplace repo\nif [[ -f .claude-plugin/marketplace.json ]]; then\n    PLUGIN_DIR=\"plugins/PLUGIN_NAME\"\nelse\n    PLUGIN_DIR=\"PLUGIN_NAME\"\nfi\n\n# Get author from git config\nAUTHOR_NAME=$(git config user.name)\nAUTHOR_EMAIL=$(git config user.email)\n```\n\n### Step 2: Create Structure\n\n```bash\nmkdir -p $PLUGIN_DIR/.claude-plugin\nmkdir -p $PLUGIN_DIR/agents\nmkdir -p $PLUGIN_DIR/skills/domain-knowledge\n```\n\n### Step 3: Create Files\n\n1. **plugin.json** - Manifest with metadata\n2. **agents/domain-expert.md** - Primary expert agent\n3. **skills/domain-knowledge/SKILL.md** - Core knowledge\n4. **README.md** - Documentation\n\n### Step 4: Register in Marketplace\n\n**CRITICAL**: If `.claude-plugin/marketplace.json` exists at repo root, you MUST add the plugin:\n\n```json\n{\n  \"plugins\": [\n    {\n      \"name\": \"plugin-name\",\n      \"source\": \"./plugins/plugin-name\",\n      \"description\": \"Same as plugin.json description\",\n      \"version\": \"1.0.0\",\n      \"author\": { \"name\": \"Author\" },\n      \"keywords\": [\"same\", \"as\", \"plugin.json\"]\n    }\n  ]\n}\n```\n\n## Component Types\n\n### Commands\n\nUser-initiated slash commands in `commands/*.md`:\n\n```markdown\n---\ndescription: What this command does\n---\n\n# Command Name\n\nInstructions for Claude to execute...\n```\n\n### Agents\n\nAutonomous subagents in `agents/*.md`:\n\n```markdown\n---\nname: agent-name\ndescription: |\n  Use this agent when... Examples:\n  <example>\n  Context: ...\n  user: \"...\"\n  assistant: \"...\"\n  <commentary>Why trigger</commentary>\n  </example>\nmodel: inherit\ncolor: blue\n---\n\nSystem prompt for agent...\n```\n\n### Skills\n\nDynamic knowledge in `skills/skill-name/SKILL.md`:\n\n```markdown\n---\nname: skill-name\ndescription: When to use this skill...\n---\n\n# Skill content with progressive disclosure...\n```\n\n### Hooks\n\nEvent automation in `hooks/hooks.json`:\n\n```json\n{\n  \"PostToolUse\": [{\n    \"matcher\": \"Write|Edit\",\n    \"hooks\": [{\n      \"type\": \"command\",\n      \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/lint.sh\"\n    }]\n  }]\n}\n```\n\n**Events**: PreToolUse, PostToolUse, SessionStart, SessionEnd, UserPromptSubmit, PreCompact, Notification, Stop, SubagentStop\n\n## Best Practices\n\n### Naming Conventions\n\n- **Plugins**: `kebab-case` (e.g., `code-review-helper`)\n- **Commands**: verb-based (e.g., `review-pr`, `run-tests`)\n- **Agents**: role-based (e.g., `code-reviewer`, `test-generator`)\n- **Skills**: topic-based (e.g., `api-design`, `error-handling`)\n\n### Portability\n\nUse `${CLAUDE_PLUGIN_ROOT}` for all internal paths:\n\n```json\n\"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/run.sh\"\n```\n\nNever use hardcoded absolute paths.\n\n### Platform Notes\n\n- **Windows**: Use GitHub marketplace installation (local paths may fail)\n- **Git Bash/MinGW**: Detect with `$MSYSTEM`, use GitHub method\n- **Mac/Linux**: All installation methods work\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Plugin not loading | Check plugin.json is in `.claude-plugin/` |\n| Commands missing | Verify frontmatter has `description` field |\n| Agent not triggering | Add `<example>` blocks to description |\n| Marketplace not found | Ensure repo is public, check path in marketplace.json |\n\n## Additional Resources\n\nFor detailed information, see:\n\n- **`references/manifest-reference.md`** - Complete plugin.json fields\n- **`references/component-patterns.md`** - Advanced component patterns\n- **`references/publishing-guide.md`** - Marketplace publishing details\n- **`examples/minimal-plugin.md`** - Simplest working plugin\n- **`examples/full-plugin.md`** - Complete plugin with all features"
              }
            ]
          },
          {
            "name": "bash-master",
            "description": "Expert bash/shell scripting system across ALL platforms with Bash 5.3 and 2025 security-first practices. PROACTIVELY activate for: (1) ANY bash/shell script task, (2) Bash 5.3 features (${ } in-shell substitution, ${| } REPLY syntax, BASH_TRAPSIG), (3) Security-first patterns (60%+ exploits from poor validation, HISTFILE protection, absolute paths), (4) ShellCheck v0.11.0 validation (non-negotiable, POSIX.1-2024), (5) Modern automation (containers/CI/CD/cloud), (6) Scripts under 50 lines (Google Style), (7) Comprehensive debugging (tracing, profiling, breakpoints), (8) Git hooks and pipeline integration, (9) Container-aware scripting, (10) DevOps/deployment automation. Provides: Bash 5.3 complete features (BASH_TRAPSIG, C23 conformance), ShellCheck v0.11.0 (SC2327/SC2328/SC2294/SC2295), security-first mandatory validation patterns, debugging/troubleshooting techniques, container/Kubernetes detection, cloud provider helpers (AWS/Azure), CI/CD platform integration (GitHub Actions/Azure DevOps), parallel processing patterns, blue-green/canary deployments, Google Shell Style Guide compliance, cross-platform compatibility (Linux/macOS/Windows/containers), POSIX.1-2024 compliance, and production-ready 2025 patterns. Ensures secure, optimized, cloud-native scripts following latest 2025 standards.",
            "source": "./plugins/bash-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install bash-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/bash-analyze",
                "description": "Deep analysis of bash scripts for security, performance, portability, and best practices compliance",
                "path": "plugins/bash-master/commands/bash-analyze.md",
                "frontmatter": {
                  "name": "bash-analyze",
                  "description": "Deep analysis of bash scripts for security, performance, portability, and best practices compliance",
                  "argument-hint": "<script.sh or directory>"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Bash Script Analyzer\n\n## Purpose\n\nPerform comprehensive analysis of bash scripts covering security vulnerabilities, performance issues, portability concerns, and adherence to modern best practices.\n\n## Analysis Categories\n\n### 1. Security Analysis\n- Command injection vulnerabilities\n- Path traversal risks\n- Unsafe temporary file handling\n- Unquoted variable expansions\n- Privilege escalation risks\n- Sensitive data exposure\n- Race condition vulnerabilities\n\n### 2. Performance Analysis\n- Unnecessary subshell spawning\n- Inefficient loops (UUOC, UUOG)\n- External command overuse vs bash builtins\n- Pipeline optimization opportunities\n- Array usage efficiency\n- Process substitution opportunities\n\n### 3. Portability Analysis\n- Bash version compatibility (4.x, 5.x features)\n- POSIX compliance level\n- Platform-specific issues (Linux/macOS/BSD)\n- GNU vs BSD tool differences\n- Shebang portability\n\n### 4. Best Practices Compliance\n- ShellCheck warnings (all severity levels)\n- Google Shell Style Guide adherence\n- Error handling patterns\n- Cleanup and trap usage\n- Function structure and modularity\n- Documentation and comments\n\n## Analysis Output Format\n\nFor each script analyzed, provide:\n\n```\n## Script: <filename>\n\n### Security Issues\n- [CRITICAL] <issue description>\n- [HIGH] <issue description>\n- [MEDIUM] <issue description>\n\n### Performance Issues\n- [IMPACT: HIGH] <issue description>\n- [IMPACT: MEDIUM] <issue description>\n\n### Portability Issues\n- [Bash 5.x required] <feature used>\n- [GNU-specific] <command/flag used>\n\n### Best Practice Violations\n- [ShellCheck SC####] <description>\n- [Style] <description>\n\n### Recommendations\n1. <actionable recommendation>\n2. <actionable recommendation>\n\n### Score: X/100\n- Security: X/25\n- Performance: X/25\n- Portability: X/25\n- Best Practices: X/25\n```\n\n## Analysis Checklist\n\nWhen analyzing scripts, systematically check:\n\n### Security Checklist\n- [ ] All variables quoted in command arguments\n- [ ] No eval with user input\n- [ ] No unvalidated external input in commands\n- [ ] Temp files created securely (mktemp)\n- [ ] No world-readable sensitive data\n- [ ] Proper file permission handling\n- [ ] No symlink following vulnerabilities\n\n### Performance Checklist\n- [ ] Using bash builtins over external commands\n- [ ] Avoiding unnecessary subshells\n- [ ] Using arrays instead of string parsing\n- [ ] Efficient file reading patterns\n- [ ] Process substitution where appropriate\n- [ ] Avoiding cat | grep (UUOC)\n- [ ] Using mapfile for file reading\n\n### Portability Checklist\n- [ ] Shebang uses /usr/bin/env bash or /bin/bash\n- [ ] Bash version requirements documented\n- [ ] No undocumented bashisms\n- [ ] GNU/BSD command compatibility\n- [ ] Works on major platforms\n\n### Best Practices Checklist\n- [ ] set -euo pipefail at start\n- [ ] trap for cleanup on EXIT\n- [ ] Functions under 50 lines\n- [ ] Proper error messages to stderr\n- [ ] Meaningful exit codes\n- [ ] ShellCheck passes with zero warnings\n- [ ] Consistent coding style\n\n## Usage Examples\n\n**Analyze single script:**\n```\n/bash-analyze backup.sh\n```\n\n**Analyze directory:**\n```\n/bash-analyze scripts/\n```\n\n**Focus on security:**\n```\n/bash-analyze deploy.sh focus on security vulnerabilities\n```\n\n**Check portability:**\n```\n/bash-analyze build.sh check POSIX compliance\n```\n\n## After Analysis\n\nBased on findings, I will:\n1. Provide detailed analysis report\n2. List all issues with severity ratings\n3. Give specific fix recommendations\n4. Offer to fix issues automatically if requested\n5. Suggest architectural improvements if needed\n\n---\n\n**Comprehensive bash script analysis using 2025 best practices and security standards.**"
              },
              {
                "name": "/bash-optimize",
                "description": null,
                "path": "plugins/bash-master/commands/bash-optimize.md",
                "frontmatter": null,
                "content": "---\nname: bash-optimize\ndescription: Optimize bash scripts for performance, reducing execution time and resource usage\nargument-hint: <script.sh> [target: speed|memory|both]\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Bash Script Optimizer\n\n## Purpose\n\nAnalyze and optimize bash scripts for maximum performance, reducing execution time and resource usage while maintaining correctness and readability.\n\n## Optimization Targets\n\n### Speed Optimization\n- Reduce subshell spawning\n- Replace external commands with bash builtins\n- Optimize loop structures\n- Implement parallel processing\n- Use efficient I/O patterns\n\n### Memory Optimization\n- Stream processing instead of loading files\n- Efficient array usage\n- Proper variable scoping\n- Cleanup temporary data\n- Avoid unnecessary string copies\n\n## Optimization Techniques\n\n### 1. Subshell Elimination\n\n**Before:**\n```bash\n# Spawns subshell for each\nresult=$(echo \"$var\" | tr 'a-z' 'A-Z')\ncount=$(cat file.txt | wc -l)\nbasename=$(basename \"$path\")\n```\n\n**After:**\n```bash\n# Pure bash - no subshells\nresult=\"${var^^}\"\ncount=0; while IFS= read -r _; do ((count++)); done < file.txt\nbasename=\"${path##*/}\"\n```\n\n### 2. External Command Replacement\n\n| External | Bash Equivalent |\n|----------|-----------------|\n| `basename \"$p\"` | `\"${p##*/}\"` |\n| `dirname \"$p\"` | `\"${p%/*}\"` |\n| `echo \"$s\" \\| tr a-z A-Z` | `\"${s^^}\"` |\n| `expr $a + $b` | `$((a + b))` |\n| `cat file` | `< file` or `mapfile` |\n| `cut -d: -f1` | `\"${var%%:*}\"` |\n| `sed 's/a/b/g'` | `\"${var//a/b}\"` |\n\n### 3. Loop Optimization\n\n**Before:**\n```bash\n# Creates subshell, variables lost\ncat file.txt | while read -r line; do\n    ((count++))\ndone\necho \"$count\"  # Always 0!\n```\n\n**After:**\n```bash\n# No subshell, variables preserved\nwhile IFS= read -r line; do\n    ((count++))\ndone < file.txt\necho \"$count\"  # Correct value\n```\n\n### 4. Array vs String Processing\n\n**Before:**\n```bash\n# Slow: string manipulation\nfiles=\"file1.txt file2.txt file3.txt\"\nfor f in $files; do\n    process \"$f\"\ndone\n```\n\n**After:**\n```bash\n# Fast: array iteration\nfiles=(\"file1.txt\" \"file2.txt\" \"file3.txt\")\nfor f in \"${files[@]}\"; do\n    process \"$f\"\ndone\n```\n\n### 5. File Reading Optimization\n\n**Before:**\n```bash\n# Multiple reads of same file\ngrep \"pattern1\" file.txt > result1.txt\ngrep \"pattern2\" file.txt > result2.txt\ngrep \"pattern3\" file.txt > result3.txt\n```\n\n**After:**\n```bash\n# Single pass with multiple outputs\nwhile IFS= read -r line; do\n    [[ \"$line\" == *pattern1* ]] && echo \"$line\" >> result1.txt\n    [[ \"$line\" == *pattern2* ]] && echo \"$line\" >> result2.txt\n    [[ \"$line\" == *pattern3* ]] && echo \"$line\" >> result3.txt\ndone < file.txt\n```\n\n### 6. Parallel Processing\n\n**Before:**\n```bash\n# Sequential processing\nfor file in *.txt; do\n    process_file \"$file\"\ndone\n```\n\n**After:**\n```bash\n# Parallel with GNU Parallel\nparallel -j \"$(nproc)\" process_file ::: *.txt\n\n# Or with xargs\nprintf '%s\\0' *.txt | xargs -0 -P \"$(nproc)\" -I {} process_file {}\n\n# Or with job control\nmax_jobs=4\nfor file in *.txt; do\n    process_file \"$file\" &\n    ((++running >= max_jobs)) && wait -n && ((running--))\ndone\nwait\n```\n\n### 7. Process Substitution\n\n**Before:**\n```bash\n# Temporary files\nsort file1.txt > /tmp/sorted1\nsort file2.txt > /tmp/sorted2\ndiff /tmp/sorted1 /tmp/sorted2\nrm /tmp/sorted1 /tmp/sorted2\n```\n\n**After:**\n```bash\n# Process substitution - no temp files\ndiff <(sort file1.txt) <(sort file2.txt)\n```\n\n### 8. Here-String vs Echo Pipe\n\n**Before:**\n```bash\necho \"$var\" | command\n```\n\n**After:**\n```bash\ncommand <<< \"$var\"\n```\n\n## Optimization Process\n\nWhen optimizing a script:\n\n1. **Profile first** - Identify actual bottlenecks\n2. **Measure baseline** - Record current execution time\n3. **Apply optimizations** - Implement changes incrementally\n4. **Verify correctness** - Ensure output matches original\n5. **Measure improvement** - Quantify the gains\n\n## Profiling Commands\n\n```bash\n# Time execution\ntime ./script.sh\n\n# Detailed timing with bash\nTIMEFORMAT='real: %R, user: %U, sys: %S'\ntime ./script.sh\n\n# Line-by-line profiling\nPS4='+ $(date +%s.%N) ${BASH_SOURCE}:${LINENO}: '\nset -x\n./script.sh\nset +x\n\n# Trace with timestamps\nbash -x script.sh 2>&1 | ts -s '%.s'\n```\n\n## Usage Examples\n\n**General optimization:**\n```\n/bash-optimize deploy.sh\n```\n\n**Focus on speed:**\n```\n/bash-optimize process.sh target: speed\n```\n\n**Focus on memory:**\n```\n/bash-optimize large-file-handler.sh target: memory\n```\n\n**Both speed and memory:**\n```\n/bash-optimize etl-script.sh target: both\n```\n\n## Output Format\n\n```\n## Optimization Report: <script>\n\n### Performance Profile\n- Original execution time: X.XXs\n- Bottlenecks identified: N\n\n### Optimizations Applied\n1. [IMPACT: HIGH] Replace `cat | grep` with direct grep\n   - Before: 150ms\n   - After: 20ms\n   - Improvement: 87%\n\n2. [IMPACT: MEDIUM] Use bash string manipulation\n   - Replaced: external `basename`\n   - With: parameter expansion\n\n### Summary\n- Total optimizations: N\n- Estimated speedup: X.Xx faster\n- Subshells eliminated: N\n- External commands replaced: N\n\n### Optimized Script\n[Full optimized script or diff]\n```\n\n## After Optimization\n\nI will:\n1. Show a detailed report of changes\n2. Provide the optimized script\n3. Explain each optimization\n4. Offer to apply changes automatically\n5. Suggest further improvements if applicable\n\n---\n\n**Transform slow bash scripts into high-performance automation using proven optimization techniques.**\n"
              },
              {
                "name": "/bash-template",
                "description": null,
                "path": "plugins/bash-master/commands/bash-template.md",
                "frontmatter": null,
                "content": "---\nname: bash-template\ndescription: Generate production-ready bash script templates with modern patterns, error handling, and configurable features\nargument-hint: <type> [options] (types: cli, daemon, library, installer, ci)\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Bash Template Generator\n\n## Purpose\n\nGenerate production-ready bash script templates with modern 2025 patterns, comprehensive error handling, and configurable features based on script type and requirements.\n\n## Available Template Types\n\n### 1. CLI Tool (`cli`)\nFull-featured command-line tool with:\n- Argument parsing (getopts/long options)\n- Help and version commands\n- Configuration file support\n- Logging with verbosity levels\n- Progress indicators\n- Color output support\n\n### 2. Daemon/Service (`daemon`)\nBackground service script with:\n- Daemonization support\n- PID file management\n- Signal handling (SIGHUP, SIGTERM)\n- Log rotation integration\n- Systemd unit file generation\n- Health check endpoint\n\n### 3. Library (`library`)\nReusable function library with:\n- Namespaced functions\n- Dependency checking\n- Self-documentation\n- Version tracking\n- Safe sourcing patterns\n- Unit test integration\n\n### 4. Installer (`installer`)\nSoftware installation script with:\n- Dependency verification\n- Platform detection\n- Root privilege handling\n- Rollback support\n- Progress reporting\n- Post-install verification\n\n### 5. CI/CD Pipeline (`ci`)\nCI/CD pipeline script with:\n- Environment detection\n- Secret handling\n- Artifact management\n- Test execution\n- Deployment patterns\n- Notification integration\n\n## Template Options\n\nSpecify additional options for customization:\n\n| Option | Description |\n|--------|-------------|\n| `--minimal` | Basic template without advanced features |\n| `--full` | All features enabled |\n| `--config` | Include configuration file handling |\n| `--logging` | Include structured logging |\n| `--parallel` | Include parallel processing support |\n| `--interactive` | Include user prompts and menus |\n| `--docker` | Include Docker integration |\n| `--aws` | Include AWS CLI patterns |\n| `--k8s` | Include Kubernetes patterns |\n\n## Usage Examples\n\n**Basic CLI tool:**\n```\n/bash-template cli\n```\n\n**Full-featured daemon:**\n```\n/bash-template daemon --full --logging\n```\n\n**Minimal installer:**\n```\n/bash-template installer --minimal\n```\n\n**CI script with Docker:**\n```\n/bash-template ci --docker --parallel\n```\n\n**Library with tests:**\n```\n/bash-template library my-utils\n```\n\n## Template Structure\n\nAll templates include:\n\n```bash\n#!/usr/bin/env bash\n#\n# script-name - Brief description\n# Version: 1.0.0\n# Author: <author>\n# License: MIT\n#\n# Usage: script-name [OPTIONS] <arguments>\n#\n# Description:\n#   Longer description of what the script does.\n#\n\n# Strict mode\nset -euo pipefail\n\n# Script metadata\nreadonly VERSION=\"1.0.0\"\nreadonly SCRIPT_NAME=\"${BASH_SOURCE[0]##*/}\"\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\n# Configuration defaults\ndeclare -A CONFIG=(\n    [verbose]=false\n    [dry_run]=false\n    [config_file]=\"\"\n)\n\n# Cleanup handler\ncleanup() {\n    local exit_code=$?\n    # Cleanup logic here\n    exit \"$exit_code\"\n}\ntrap cleanup EXIT INT TERM\n\n# Main function\nmain() {\n    parse_args \"$@\"\n    validate_environment\n    execute\n}\n\nmain \"$@\"\n```\n\n## Feature Modules\n\nTemplates can include these feature modules:\n\n### Argument Parsing\n```bash\nparse_args() {\n    while [[ $# -gt 0 ]]; do\n        case \"$1\" in\n            -h|--help) show_help; exit 0 ;;\n            -v|--version) echo \"$VERSION\"; exit 0 ;;\n            -V|--verbose) CONFIG[verbose]=true ;;\n            --) shift; break ;;\n            -*) die \"Unknown option: $1\" ;;\n            *) ARGS+=(\"$1\") ;;\n        esac\n        shift\n    done\n}\n```\n\n### Logging System\n```bash\ndeclare -A LOG_LEVELS=([DEBUG]=0 [INFO]=1 [WARN]=2 [ERROR]=3)\nLOG_LEVEL=\"${LOG_LEVEL:-INFO}\"\n\nlog() {\n    local level=\"$1\"; shift\n    local msg=\"$*\"\n    local ts; ts=$(date '+%Y-%m-%d %H:%M:%S')\n\n    if [[ ${LOG_LEVELS[$level]} -ge ${LOG_LEVELS[$LOG_LEVEL]} ]]; then\n        printf '[%s] [%s] %s\\n' \"$ts\" \"$level\" \"$msg\" >&2\n    fi\n}\n\ndebug() { log DEBUG \"$@\"; }\ninfo()  { log INFO \"$@\"; }\nwarn()  { log WARN \"$@\"; }\nerror() { log ERROR \"$@\"; }\n```\n\n### Color Output\n```bash\nsetup_colors() {\n    if [[ -t 2 ]] && [[ -z \"${NO_COLOR:-}\" ]]; then\n        RED=$'\\033[0;31m'\n        GREEN=$'\\033[0;32m'\n        YELLOW=$'\\033[0;33m'\n        BLUE=$'\\033[0;34m'\n        RESET=$'\\033[0m'\n    else\n        RED='' GREEN='' YELLOW='' BLUE='' RESET=''\n    fi\n}\n```\n\n### Progress Indicator\n```bash\nspinner() {\n    local pid=$1\n    local delay=0.1\n    local spinstr=''\n\n    while kill -0 \"$pid\" 2>/dev/null; do\n        for ((i=0; i<${#spinstr}; i++)); do\n            printf '\\r%s %s' \"${spinstr:$i:1}\" \"$2\"\n            sleep \"$delay\"\n        done\n    done\n    printf '\\r %s\\n' \"$2\"\n}\n```\n\n## Output\n\nWhen you request a template, I will:\n\n1. Generate the complete script with requested features\n2. Include inline documentation\n3. Provide usage examples\n4. Add ShellCheck-compliant code\n5. Include relevant test patterns\n\n---\n\n**Generate modern, production-ready bash scripts instantly with best practices built-in.**\n"
              },
              {
                "name": "/pwsh-script",
                "description": "Create, review, or optimize bash/shell scripts following 2025 best practices and cross-platform standards",
                "path": "plugins/bash-master/commands/pwsh-script.md",
                "frontmatter": {
                  "name": "pwsh-script",
                  "description": "Create, review, or optimize bash/shell scripts following 2025 best practices and cross-platform standards"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Create/Review Bash Scripts\n\n## Purpose\n\nAutonomously create professional, production-ready bash scripts or review/optimize existing scripts following 2025 industry standards.\n\n## What This Command Does\n\n**Automatic Actions:**\n1.  Creates scripts with mandatory error handling (set -euo pipefail)\n2.  Implements ShellCheck-compliant code\n3.  Follows Google Shell Style Guide (50-line recommendation)\n4.  Adds comprehensive error handling and cleanup\n5.  Ensures cross-platform compatibility\n6.  Includes proper documentation\n7.  Validates security patterns\n\n**For Reviews:**\n- Identifies ShellCheck issues\n- Checks for security vulnerabilities\n- Validates error handling\n- Suggests performance optimizations\n- Verifies cross-platform compatibility\n\n## Usage\n\n**Create a new script:**\n```\n/pwsh-script Create a backup script that archives /data to S3 with error handling\n```\n\n**Review existing script:**\n```\n/pwsh-script Review backup.sh for security issues and best practices\n```\n\n**Optimize performance:**\n```\n/pwsh-script Optimize deploy.sh for better performance\n```\n\n## What You'll Get\n\n### New Scripts Include:\n- `#!/usr/bin/env bash` shebang\n- Safety settings (set -euo pipefail, IFS=$'\\n\\t')\n- Proper function structure\n- Input validation\n- Error handling with trap\n- Usage/help text\n- Logging capabilities\n- Cross-platform considerations\n- ShellCheck compliance\n\n### Reviews Provide:\n- ShellCheck validation results\n- Security vulnerability assessment\n- Anti-pattern identification\n- Performance improvement suggestions\n- Cross-platform compatibility notes\n- Best practice recommendations\n\n## Example Output\n\n```bash\n#!/usr/bin/env bash\n#\n# backup.sh - Archive data to S3 with error handling\n# Version: 1.0.0\n\nset -euo pipefail\nIFS=$'\\n\\t'\n\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n\n# Cleanup on exit\ncleanup() {\n    local exit_code=$?\n    [[ -n \"${TEMP_DIR:-}\" ]] && rm -rf \"$TEMP_DIR\"\n    exit \"$exit_code\"\n}\ntrap cleanup EXIT INT TERM\n\n# Main function\nmain() {\n    local source_dir=\"${1:?Source directory required}\"\n    local s3_bucket=\"${2:?S3 bucket required}\"\n\n    # Validate source directory\n    if [[ ! -d \"$source_dir\" ]]; then\n        echo \"Error: Source directory not found: $source_dir\" >&2\n        return 1\n    fi\n\n    # Create archive\n    local archive_name=\"backup-$(date +%Y%m%d-%H%M%S).tar.gz\"\n    tar -czf \"$archive_name\" -C \"$source_dir\" . || {\n        echo \"Error: Failed to create archive\" >&2\n        return 1\n    }\n\n    # Upload to S3\n    aws s3 cp \"$archive_name\" \"s3://$s3_bucket/\" || {\n        echo \"Error: Failed to upload to S3\" >&2\n        rm -f \"$archive_name\"\n        return 1\n    }\n\n    # Cleanup\n    rm -f \"$archive_name\"\n    echo \"Backup completed successfully\"\n}\n\nmain \"$@\"\n```\n\n## 2025 Standards Applied\n\n- **ShellCheck validation** - Zero warnings\n- **Google Style Guide** - Modular functions under 50 lines\n- **Modern error handling** - errexit, nounset, pipefail trio\n- **Security hardening** - Input validation, path sanitization\n- **Cross-platform** - Works on Linux/macOS/Windows (Git Bash/WSL)\n- **Production-ready** - Proper cleanup, logging, exit codes\n\n## When To Use\n\n- Creating new bash scripts for any purpose\n- Automating system tasks\n- DevOps/CI/CD pipeline scripts\n- Build and deployment automation\n- Reviewing security of existing scripts\n- Optimizing script performance\n- Debugging script issues\n- Converting manual commands to automated scripts\n\n---\n\n**After running this command, you'll have production-ready, secure, optimized bash scripts following all 2025 best practices.**"
              }
            ],
            "skills": [
              {
                "name": "advanced-array-patterns",
                "description": "Advanced bash array patterns including mapfile, readarray, associative arrays, and array manipulation (2025)",
                "path": "plugins/bash-master/skills/advanced-array-patterns/SKILL.md",
                "frontmatter": {
                  "name": "advanced-array-patterns",
                  "description": "Advanced bash array patterns including mapfile, readarray, associative arrays, and array manipulation (2025)"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Advanced Bash Array Patterns (2025)\n\n## Overview\n\nComprehensive guide to bash arrays including indexed arrays, associative arrays, mapfile/readarray, and advanced manipulation patterns following 2025 best practices.\n\n## Indexed Arrays\n\n### Declaration and Initialization\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Method 1: Direct assignment\nfiles=(\"file1.txt\" \"file2.txt\" \"file with spaces.txt\")\n\n# Method 2: Compound assignment\ndeclare -a numbers=(1 2 3 4 5)\n\n# Method 3: Individual assignment\nfruits[0]=\"apple\"\nfruits[1]=\"banana\"\nfruits[2]=\"cherry\"\n\n# Method 4: From command output (CAREFUL with word splitting)\n#  DANGEROUS - splits on spaces\nfiles_bad=$(ls)\n\n#  SAFE - preserves filenames with spaces\nmapfile -t files_good < <(find . -name \"*.txt\")\n\n# Method 5: Brace expansion\nnumbers=({1..100})\nletters=({a..z})\n```\n\n### Array Operations\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\narr=(\"first\" \"second\" \"third\" \"fourth\" \"fifth\")\n\n# Length\necho \"Length: ${#arr[@]}\"  # 5\n\n# Access elements\necho \"First: ${arr[0]}\"\necho \"Last: ${arr[-1]}\"  # Bash 4.3+\necho \"Second to last: ${arr[-2]}\"\n\n# All elements (properly quoted for spaces)\nfor item in \"${arr[@]}\"; do\n    echo \"Item: $item\"\ndone\n\n# All indices\nfor idx in \"${!arr[@]}\"; do\n    echo \"Index $idx: ${arr[$idx]}\"\ndone\n\n# Slice (offset:length)\necho \"${arr[@]:1:3}\"  # second third fourth\n\n# Slice from offset to end\necho \"${arr[@]:2}\"  # third fourth fifth\n\n# Append element\narr+=(\"sixth\")\n\n# Insert at position (complex)\narr=(\"${arr[@]:0:2}\" \"inserted\" \"${arr[@]:2}\")\n\n# Remove element by index\nunset 'arr[2]'\n\n# Remove by value (all occurrences)\narr_new=()\nfor item in \"${arr[@]}\"; do\n    [[ \"$item\" != \"second\" ]] && arr_new+=(\"$item\")\ndone\narr=(\"${arr_new[@]}\")\n\n# Check if empty\nif [[ ${#arr[@]} -eq 0 ]]; then\n    echo \"Array is empty\"\nfi\n\n# Check if element exists\ncontains() {\n    local needle=\"$1\"\n    shift\n    local item\n    for item in \"$@\"; do\n        [[ \"$item\" == \"$needle\" ]] && return 0\n    done\n    return 1\n}\n\nif contains \"third\" \"${arr[@]}\"; then\n    echo \"Found 'third'\"\nfi\n```\n\n### Array Transformation\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\narr=(\"apple\" \"banana\" \"cherry\" \"date\")\n\n# Map (transform each element)\nupper_arr=()\nfor item in \"${arr[@]}\"; do\n    upper_arr+=(\"${item^^}\")  # Uppercase\ndone\n\n# Filter\nfiltered=()\nfor item in \"${arr[@]}\"; do\n    [[ ${#item} -gt 5 ]] && filtered+=(\"$item\")\ndone\n\n# Join array to string\nIFS=','\njoined=\"${arr[*]}\"\nunset IFS\necho \"$joined\"  # apple,banana,cherry,date\n\n# Split string to array\nIFS=',' read -ra split_arr <<< \"one,two,three\"\n\n# Unique values\ndeclare -A seen\nunique=()\nfor item in \"${arr[@]}\"; do\n    if [[ -z \"${seen[$item]:-}\" ]]; then\n        seen[$item]=1\n        unique+=(\"$item\")\n    fi\ndone\n\n# Sort array\nreadarray -t sorted < <(printf '%s\\n' \"${arr[@]}\" | sort)\n\n# Reverse array\nreversed=()\nfor ((i=${#arr[@]}-1; i>=0; i--)); do\n    reversed+=(\"${arr[$i]}\")\ndone\n\n# Or using tac\nreadarray -t reversed < <(printf '%s\\n' \"${arr[@]}\" | tac)\n```\n\n## Associative Arrays (Bash 4+)\n\n### Declaration and Usage\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# MUST declare with -A\ndeclare -A config\n\n# Assignment\nconfig[\"host\"]=\"localhost\"\nconfig[\"port\"]=\"8080\"\nconfig[\"debug\"]=\"true\"\n\n# Or compound assignment\ndeclare -A user=(\n    [name]=\"John Doe\"\n    [email]=\"john@example.com\"\n    [role]=\"admin\"\n)\n\n# Access\necho \"Host: ${config[host]}\"\necho \"User: ${user[name]}\"\n\n# Default value if key missing\necho \"${config[missing]:-default}\"\n\n# Check if key exists\nif [[ -v config[host] ]]; then\n    echo \"Host is set\"\nfi\n\n# Alternative check\nif [[ -n \"${config[host]+x}\" ]]; then\n    echo \"Host key exists (even if empty)\"\nfi\n\n# All keys\necho \"Keys: ${!config[@]}\"\n\n# All values\necho \"Values: ${config[@]}\"\n\n# Length (number of keys)\necho \"Size: ${#config[@]}\"\n\n# Iterate\nfor key in \"${!config[@]}\"; do\n    echo \"$key = ${config[$key]}\"\ndone\n\n# Delete key\nunset 'config[debug]'\n\n# Clear entire array\nconfig=()\n```\n\n### Real-World Associative Array Patterns\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Configuration parser\nparse_config() {\n    local config_file=\"$1\"\n    declare -gA CONFIG  # Global associative array\n\n    while IFS='=' read -r key value; do\n        # Skip comments and empty lines\n        [[ \"$key\" =~ ^[[:space:]]*# ]] && continue\n        [[ -z \"$key\" ]] && continue\n\n        # Trim whitespace\n        key=\"${key//[[:space:]]/}\"\n        value=\"${value#\"${value%%[![:space:]]*}\"}\"  # Left trim\n        value=\"${value%\"${value##*[![:space:]]}\"}\"  # Right trim\n\n        CONFIG[\"$key\"]=\"$value\"\n    done < \"$config_file\"\n}\n\n# Usage\nparse_config \"/etc/myapp.conf\"\necho \"Database: ${CONFIG[database]:-not set}\"\n\n# Counter/frequency map\ncount_words() {\n    local file=\"$1\"\n    declare -A word_count\n\n    while read -ra words; do\n        for word in \"${words[@]}\"; do\n            # Normalize: lowercase, remove punctuation\n            word=\"${word,,}\"\n            word=\"${word//[^a-z]/}\"\n            [[ -n \"$word\" ]] && ((word_count[$word]++))\n        done\n    done < \"$file\"\n\n    # Print sorted by count\n    for word in \"${!word_count[@]}\"; do\n        echo \"${word_count[$word]} $word\"\n    done | sort -rn | head -10\n}\n\n# Caching pattern\ndeclare -A CACHE\n\ncached_expensive_operation() {\n    local key=\"$1\"\n\n    # Check cache\n    if [[ -n \"${CACHE[$key]+x}\" ]]; then\n        echo \"${CACHE[$key]}\"\n        return 0\n    fi\n\n    # Compute and cache\n    local result\n    result=$(expensive_computation \"$key\")\n    CACHE[\"$key\"]=\"$result\"\n    echo \"$result\"\n}\n\n# JSON-like nested data (using delimited keys)\ndeclare -A data\ndata[\"user.name\"]=\"John\"\ndata[\"user.email\"]=\"john@example.com\"\ndata[\"user.address.city\"]=\"NYC\"\ndata[\"user.address.zip\"]=\"10001\"\n\n# Access nested\necho \"City: ${data[user.address.city]}\"\n```\n\n## mapfile / readarray (Bash 4+)\n\n### Basic Usage\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Read file into array (each line = element)\nmapfile -t lines < file.txt\n# Or equivalently:\nreadarray -t lines < file.txt\n\n# -t removes trailing newlines\n# Without -t, each element includes \\n\n\n# Process each line\nfor line in \"${lines[@]}\"; do\n    echo \"Line: $line\"\ndone\n\n# Read from command output\nmapfile -t files < <(find . -name \"*.sh\")\n\n# Read from here-doc\nmapfile -t data <<'EOF'\nline1\nline2\nline3\nEOF\n```\n\n### Advanced mapfile Options\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# -n COUNT: Read at most COUNT lines\nmapfile -t -n 10 first_10 < large_file.txt\n\n# -s COUNT: Skip first COUNT lines\nmapfile -t -s 1 skip_header < data.csv  # Skip header row\n\n# -O INDEX: Start at INDEX instead of 0\nexisting_array=(\"a\" \"b\")\nmapfile -t -O \"${#existing_array[@]}\" existing_array < more_data.txt\n\n# -d DELIM: Use DELIM instead of newline (Bash 4.4+)\n# Read NUL-delimited data (safe for filenames with newlines)\nmapfile -t -d '' files < <(find . -name \"*.txt\" -print0)\n\n# -C CALLBACK: Execute callback every QUANTUM lines\n# -c QUANTUM: Number of lines between callbacks (default 5000)\nprocess_chunk() {\n    local index=$1\n    echo \"Processing lines around index $index\" >&2\n}\nexport -f process_chunk\nmapfile -t -c 1000 -C process_chunk lines < huge_file.txt\n```\n\n### CSV Processing with mapfile\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Parse CSV file\nparse_csv() {\n    local csv_file=\"$1\"\n    local -n result_array=\"$2\"  # nameref (Bash 4.3+)\n\n    while IFS=',' read -ra row; do\n        result_array+=(\"${row[*]}\")  # Store as delimited string\n    done < \"$csv_file\"\n}\n\n# Better: Store as 2D array simulation\ndeclare -A csv_data\nrow_num=0\n\nwhile IFS=',' read -ra fields; do\n    for col_num in \"${!fields[@]}\"; do\n        csv_data[\"$row_num,$col_num\"]=\"${fields[$col_num]}\"\n    done\n    ((row_num++))\ndone < data.csv\n\n# Access cell\necho \"Row 2, Col 3: ${csv_data[2,3]}\"\n```\n\n## Performance Patterns\n\n### Efficient Array Building\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  SLOW - Command substitution in loop\nslow_build() {\n    local arr=()\n    for i in {1..1000}; do\n        arr+=(\"$(echo \"$i\")\")  # Subshell for each!\n    done\n}\n\n#  FAST - Direct assignment\nfast_build() {\n    local arr=()\n    for i in {1..1000}; do\n        arr+=(\"$i\")  # No subshell\n    done\n}\n\n#  FASTEST - mapfile for file data\nfastest_file_read() {\n    mapfile -t arr < file.txt\n}\n```\n\n### Avoid Subshells in Loops\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  SLOW - Subshell each iteration\nslow_process() {\n    local sum=0\n    for num in \"${numbers[@]}\"; do\n        result=$(echo \"$num * 2\" | bc)  # Subshell!\n        ((sum += result))\n    done\n}\n\n#  FAST - Bash arithmetic\nfast_process() {\n    local sum=0\n    for num in \"${numbers[@]}\"; do\n        ((sum += num * 2))\n    done\n}\n\n#  FAST - Process substitution for parallel reads\nwhile read -r line1 <&3 && read -r line2 <&4; do\n    echo \"$line1 | $line2\"\ndone 3< <(command1) 4< <(command2)\n```\n\n### Large Array Operations\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# For very large arrays, consider:\n# 1. Process in chunks\n# 2. Use external tools (awk, sort)\n# 3. Stream processing instead of loading all\n\n# Chunk processing\nprocess_in_chunks() {\n    local -n arr=\"$1\"\n    local chunk_size=\"${2:-1000}\"\n    local len=\"${#arr[@]}\"\n\n    for ((i=0; i<len; i+=chunk_size)); do\n        local chunk=(\"${arr[@]:i:chunk_size}\")\n        process_chunk \"${chunk[@]}\"\n    done\n}\n\n# Stream processing (memory efficient)\n# Instead of:\n#   mapfile -t all_lines < huge_file.txt\n#   process \"${all_lines[@]}\"\n# Use:\nwhile IFS= read -r line; do\n    process_line \"$line\"\ndone < huge_file.txt\n```\n\n## Bash 5.3+ Array Features\n\n### Enhanced Array Subscripts\n\n```bash\n#!/usr/bin/env bash\n# Requires Bash 5.2+\n\nset -euo pipefail\n\ndeclare -A config\n\n# Subscript expressions evaluated once (5.2+)\nkey=\"host\"\nconfig[$key]=\"localhost\"  # Evaluated correctly\n\n# '@' and '*' subscripts for associative arrays\n# Can now unset just the key '@' instead of entire array\ndeclare -A special\nspecial[@]=\"at sign value\"\nspecial[*]=\"asterisk value\"\nspecial[normal]=\"normal value\"\n\n# Unset specific key (Bash 5.2+)\nunset 'special[@]'  # Only removes '@' key, not whole array\n```\n\n### GLOBSORT with Arrays\n\n```bash\n#!/usr/bin/env bash\n# Requires Bash 5.3\n\nset -euo pipefail\n\n# Sort glob results by modification time (newest first)\nGLOBSORT=\"-mtime\"\nrecent_files=(*.txt)\n\n# Sort by size\nGLOBSORT=\"size\"\nfiles_by_size=(*.log)\n\n# Reset to default (alphabetical)\nGLOBSORT=\"name\"\n```\n\n## Common Array Patterns\n\n### Stack Implementation\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\ndeclare -a STACK=()\n\npush() {\n    STACK+=(\"$1\")\n}\n\npop() {\n    if [[ ${#STACK[@]} -eq 0 ]]; then\n        echo \"Stack empty\" >&2\n        return 1\n    fi\n    echo \"${STACK[-1]}\"\n    unset 'STACK[-1]'\n}\n\npeek() {\n    if [[ ${#STACK[@]} -gt 0 ]]; then\n        echo \"${STACK[-1]}\"\n    fi\n}\n\n# Usage\npush \"first\"\npush \"second\"\npush \"third\"\necho \"Top: $(peek)\"     # third\necho \"Pop: $(pop)\"      # third\necho \"Pop: $(pop)\"      # second\n```\n\n### Queue Implementation\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\ndeclare -a QUEUE=()\n\nenqueue() {\n    QUEUE+=(\"$1\")\n}\n\ndequeue() {\n    if [[ ${#QUEUE[@]} -eq 0 ]]; then\n        echo \"Queue empty\" >&2\n        return 1\n    fi\n    echo \"${QUEUE[0]}\"\n    QUEUE=(\"${QUEUE[@]:1}\")\n}\n\n# Usage\nenqueue \"task1\"\nenqueue \"task2\"\nenqueue \"task3\"\necho \"Next: $(dequeue)\"  # task1\necho \"Next: $(dequeue)\"  # task2\n```\n\n### Set Operations\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Union\narray_union() {\n    local -n arr1=\"$1\"\n    local -n arr2=\"$2\"\n    local -A seen\n    local result=()\n\n    for item in \"${arr1[@]}\" \"${arr2[@]}\"; do\n        if [[ -z \"${seen[$item]:-}\" ]]; then\n            seen[$item]=1\n            result+=(\"$item\")\n        fi\n    done\n\n    printf '%s\\n' \"${result[@]}\"\n}\n\n# Intersection\narray_intersection() {\n    local -n arr1=\"$1\"\n    local -n arr2=\"$2\"\n    local -A set1\n    local result=()\n\n    for item in \"${arr1[@]}\"; do\n        set1[$item]=1\n    done\n\n    for item in \"${arr2[@]}\"; do\n        if [[ -n \"${set1[$item]:-}\" ]]; then\n            result+=(\"$item\")\n        fi\n    done\n\n    printf '%s\\n' \"${result[@]}\"\n}\n\n# Difference (arr1 - arr2)\narray_difference() {\n    local -n arr1=\"$1\"\n    local -n arr2=\"$2\"\n    local -A set2\n    local result=()\n\n    for item in \"${arr2[@]}\"; do\n        set2[$item]=1\n    done\n\n    for item in \"${arr1[@]}\"; do\n        if [[ -z \"${set2[$item]:-}\" ]]; then\n            result+=(\"$item\")\n        fi\n    done\n\n    printf '%s\\n' \"${result[@]}\"\n}\n```\n\n## Resources\n\n- [Bash Arrays](https://www.gnu.org/software/bash/manual/html_node/Arrays.html)\n- [BashFAQ/005 - Arrays](https://mywiki.wooledge.org/BashFAQ/005)\n- [Bash Hackers - Arrays](https://wiki.bash-hackers.org/syntax/arrays)\n\n---\n\n**Master bash arrays for efficient data manipulation and avoid common pitfalls like word splitting and subshell overhead.**"
              },
              {
                "name": "bash-53-features",
                "description": "Bash 5.3 new features and modern patterns (2025)",
                "path": "plugins/bash-master/skills/bash-53-features/SKILL.md",
                "frontmatter": {
                  "name": "bash-53-features",
                  "description": "Bash 5.3 new features and modern patterns (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Bash 5.3 Features (2025)\n\n## Overview\n\nBash 5.3 (released July 2025) introduces significant new features that improve performance, readability, and functionality.\n\n## Key New Features\n\n### 1. In-Shell Command Substitution\n\n**New: ${ command; } syntax** - Executes without forking a subshell (runs in current shell context):\n\n```bash\n# OLD way (Bash < 5.3) - Creates subshell\noutput=$(expensive_command)\n\n# NEW way (Bash 5.3+) - Runs in current shell, faster\noutput=${ expensive_command; }\n```\n\n**Benefits:**\n- No subshell overhead (faster)\n- Preserves variable scope\n- Better performance in loops\n\n**Example:**\n```bash\n#!/usr/bin/env bash\n\n# Traditional approach\ncount=0\nfor file in *.txt; do\n    lines=$(wc -l < \"$file\")  # Subshell created\n    ((count += lines))\ndone\n\n# Bash 5.3 approach (faster)\ncount=0\nfor file in *.txt; do\n    lines=${ wc -l < \"$file\"; }  # No subshell\n    ((count += lines))\ndone\n```\n\n### 2. REPLY Variable Command Substitution\n\n**New: ${| command; } syntax** - Stores result in REPLY variable:\n\n```bash\n# Runs command, result goes to $REPLY automatically\n${| complex_calculation; }\necho \"Result: $REPLY\"\n\n# Multiple operations\n${|\n    local_var=\"processing\"\n    echo \"$local_var: $((42 * 2))\"\n}\necho \"Got: $REPLY\"\n```\n\n**Use Cases:**\n- Avoid variable naming conflicts\n- Clean syntax for temporary values\n- Standardized result variable\n\n### 3. Enhanced `read` Builtin\n\n**New: -E option** - Uses readline with programmable completion:\n\n```bash\n# Interactive input with tab completion\nread -E -p \"Enter filename: \" filename\n# User can now tab-complete file paths!\n\n# With custom completion\nread -E -p \"Select environment: \" env\n# Enables full readline features (history, editing)\n```\n\n**Benefits:**\n- Better UX for interactive scripts\n- Built-in path completion\n- Command history support\n\n### 4. Enhanced `source` Builtin\n\n**New: -p PATH option** - Custom search path for sourcing:\n\n```bash\n# OLD way\nsource /opt/myapp/lib/helpers.sh\n\n# NEW way - Search custom path\nsource -p /opt/myapp/lib:/usr/local/lib helpers.sh\n\n# Respects CUSTOM_PATH instead of current directory\nCUSTOM_PATH=/app/modules:/shared/lib\nsource -p \"$CUSTOM_PATH\" database.sh\n```\n\n**Benefits:**\n- Modular library organization\n- Avoid hard-coded paths\n- Environment-specific sourcing\n\n### 5. Enhanced `compgen` Builtin\n\n**New: Variable output option** - Store completions in variable:\n\n```bash\n# OLD way - Output to stdout\ncompletions=$(compgen -f)\n\n# NEW way - Directly to variable\ncompgen -v completions_var -f\n# Results now in $completions_var\n```\n\n**Benefits:**\n- Cleaner completion handling\n- No extra subshells\n- Better performance\n\n### 6. GLOBSORT Variable\n\n**New: Control glob sorting behavior**:\n\n```bash\n# Default: alphabetical sort\necho *.txt\n\n# Sort by modification time (newest first)\nGLOBSORT=\"-mtime\"\necho *.txt\n\n# Sort by size\nGLOBSORT=\"size\"\necho *.txt\n\n# Reverse alphabetical\nGLOBSORT=\"reverse\"\necho *.txt\n```\n\n**Options:**\n- `name` - Alphabetical (default)\n- `reverse` - Reverse alphabetical\n- `size` - By file size\n- `mtime` - By modification time\n- `-mtime` - Reverse modification time\n\n### 7. BASH_TRAPSIG Variable\n\n**New: Signal number variable in traps**:\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# BASH_TRAPSIG contains the signal number being handled\nhandle_signal() {\n    echo \"Caught signal: $BASH_TRAPSIG\" >&2\n    case \"$BASH_TRAPSIG\" in\n        15) echo \"SIGTERM (15) received, shutting down gracefully\" ;;\n        2)  echo \"SIGINT (2) received, cleaning up\" ;;\n        *)  echo \"Signal $BASH_TRAPSIG received\" ;;\n    esac\n}\n\ntrap handle_signal SIGTERM SIGINT SIGHUP\n```\n\n**Benefits:**\n- Reusable signal handlers\n- Dynamic signal-specific behavior\n- Better logging and debugging\n\n### 8. Floating-Point Arithmetic\n\n**New: `fltexpr` loadable builtin**:\n\n```bash\n# Enable floating-point support\nenable -f /usr/lib/bash/fltexpr fltexpr\n\n# Perform calculations\nfltexpr result = 42.5 * 1.5\necho \"$result\"  # 63.75\n\n# Complex expressions\nfltexpr pi_area = 3.14159 * 5 * 5\necho \"Area: $pi_area\"\n```\n\n**Use Cases:**\n- Scientific calculations\n- Financial computations\n- Avoid external tools (bc, awk)\n\n## Performance Improvements\n\n### Avoid Subshells\n\n```bash\n#  OLD (Bash < 5.3) - Multiple subshells\nfor i in {1..1000}; do\n    result=$(echo \"$i * 2\" | bc)\n    process \"$result\"\ndone\n\n#  NEW (Bash 5.3+) - No subshells\nfor i in {1..1000}; do\n    result=${ echo $((i * 2)); }\n    process \"$result\"\ndone\n```\n\n**Performance Gain:** ~40% faster in benchmarks\n\n### Efficient File Processing\n\n```bash\n#!/usr/bin/env bash\n\n# Process large file efficiently\nprocess_log() {\n    local line_count=0\n    local error_count=0\n\n    while IFS= read -r line; do\n        ((line_count++))\n\n        # Bash 5.3: No subshell for grep\n        if ${ grep -q \"ERROR\" <<< \"$line\"; }; then\n            ((error_count++))\n        fi\n    done < \"$1\"\n\n    echo \"Processed $line_count lines, found $error_count errors\"\n}\n\nprocess_log /var/log/app.log\n```\n\n## Migration Guide\n\n### Check Bash Version\n\n```bash\n#!/usr/bin/env bash\n\n# Require Bash 5.3+\nif ((BASH_VERSINFO[0] < 5 || (BASH_VERSINFO[0] == 5 && BASH_VERSINFO[1] < 3))); then\n    echo \"Error: Bash 5.3+ required (found $BASH_VERSION)\" >&2\n    exit 1\nfi\n```\n\n### Feature Detection\n\n```bash\n# Test for 5.3 features\nhas_bash_53_features() {\n    # Try using ${ } syntax\n    if eval 'test=${ echo \"yes\"; }' 2>/dev/null; then\n        return 0\n    else\n        return 1\n    fi\n}\n\nif has_bash_53_features; then\n    echo \"Bash 5.3 features available\"\nelse\n    echo \"Using legacy mode\"\nfi\n```\n\n### Gradual Adoption\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Support both old and new bash\nif ((BASH_VERSINFO[0] > 5 || (BASH_VERSINFO[0] == 5 && BASH_VERSINFO[1] >= 3))); then\n    # Bash 5.3+ path\n    result=${ compute_value; }\nelse\n    # Legacy path\n    result=$(compute_value)\nfi\n```\n\n## Best Practices (2025)\n\n1. **Use ${ } for performance-critical loops**\n   ```bash\n   for item in \"${large_array[@]}\"; do\n       processed=${ transform \"$item\"; }\n   done\n   ```\n\n2. **Use ${| } for clean temporary values**\n   ```bash\n   ${| calculate_hash \"$file\"; }\n   if [[ \"$REPLY\" == \"$expected_hash\" ]]; then\n       echo \"Valid\"\n   fi\n   ```\n\n3. **Enable readline for interactive scripts**\n   ```bash\n   read -E -p \"Config file: \" config\n   ```\n\n4. **Use source -p for modular libraries**\n   ```bash\n   source -p \"$LIB_PATH\" database.sh logging.sh\n   ```\n\n5. **Document version requirements**\n   ```bash\n   # Requires: Bash 5.3+ for performance features\n   ```\n\n## Compatibility Notes\n\n### Bash 5.3 Availability (2025)\n\n**Note:** Bash 5.3 (released July 2025) is the latest stable version. There is no Bash 5.4 as of October 2025.\n\n- **Linux**: Ubuntu 24.04+, Fedora 40+, Arch (current)\n- **macOS**: Homebrew (`brew install bash`)\n- **Windows**: WSL2 with Ubuntu 24.04+\n- **Containers**: `bash:5.3` official image\n\n### C23 Conformance\n\nBash 5.3 updated to C23 language standard. **Note:** K&R style C compilers are no longer supported.\n\n### Fallback Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Detect bash version\nreadonly BASH_53_PLUS=$((BASH_VERSINFO[0] > 5 || (BASH_VERSINFO[0] == 5 && BASH_VERSINFO[1] >= 3)))\n\nprocess_items() {\n    local item\n    for item in \"$@\"; do\n        if ((BASH_53_PLUS)); then\n            result=${ transform \"$item\"; }  # Fast path\n        else\n            result=$(transform \"$item\")      # Compatible path\n        fi\n        echo \"$result\"\n    done\n}\n```\n\n## Real-World Examples\n\n### Fast Log Parser\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Parse log file (Bash 5.3 optimized)\nparse_log() {\n    local file=\"$1\"\n    local stats_errors=0\n    local stats_warnings=0\n    local stats_lines=0\n\n    while IFS= read -r line; do\n        ((stats_lines++))\n\n        # Fast pattern matching (no subshell)\n        ${| grep -q \"ERROR\" <<< \"$line\"; } && ((stats_errors++))\n        ${| grep -q \"WARN\" <<< \"$line\"; }  && ((stats_warnings++))\n    done < \"$file\"\n\n    echo \"Lines: $stats_lines, Errors: $stats_errors, Warnings: $stats_warnings\"\n}\n\nparse_log /var/log/application.log\n```\n\n### Interactive Configuration\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Interactive setup with readline\nsetup_config() {\n    echo \"Configuration Setup\"\n    echo \"===================\"\n\n    # Tab completion for paths\n    read -E -p \"Data directory: \" data_dir\n    read -E -p \"Config file: \" config_file\n\n    # Validate and store\n    ${|\n        [[ -d \"$data_dir\" ]] && echo \"valid\" || echo \"invalid\"\n    }\n\n    if [[ \"$REPLY\" == \"valid\" ]]; then\n        echo \"DATA_DIR=$data_dir\" > config.env\n        echo \"CONFIG_FILE=$config_file\" >> config.env\n        echo \" Configuration saved\"\n    else\n        echo \" Invalid directory\" >&2\n        return 1\n    fi\n}\n\nsetup_config\n```\n\n## Resources\n\n- [Bash 5.3 Release Notes](https://lists.gnu.org/archive/html/bash-announce/2025-07/msg00000.html)\n- [Bash Manual - Command Substitution](https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html)\n- [ShellCheck Bash 5.3 Support](https://github.com/koalaman/shellcheck/releases)\n\n---\n\n**Bash 5.3 provides significant performance and usability improvements. Adopt these features gradually while maintaining backwards compatibility for older systems.**"
              },
              {
                "name": "bash-master",
                "description": "Expert bash/shell scripting system across ALL platforms. PROACTIVELY activate for: (1) ANY bash/shell script task, (2) System automation, (3) DevOps/CI/CD scripts, (4) Build/deployment automation, (5) Script review/debugging, (6) Converting commands to scripts. Provides: Google Shell Style Guide compliance, ShellCheck validation, cross-platform compatibility (Linux/macOS/Windows/containers), POSIX compliance, security hardening, error handling, performance optimization, testing with BATS, and production-ready patterns. Ensures professional-grade, secure, portable scripts every time.",
                "path": "plugins/bash-master/skills/bash-master/SKILL.md",
                "frontmatter": {
                  "name": "bash-master",
                  "description": "Expert bash/shell scripting system across ALL platforms. PROACTIVELY activate for: (1) ANY bash/shell script task, (2) System automation, (3) DevOps/CI/CD scripts, (4) Build/deployment automation, (5) Script review/debugging, (6) Converting commands to scripts. Provides: Google Shell Style Guide compliance, ShellCheck validation, cross-platform compatibility (Linux/macOS/Windows/containers), POSIX compliance, security hardening, error handling, performance optimization, testing with BATS, and production-ready patterns. Ensures professional-grade, secure, portable scripts every time."
                },
                "content": "# Bash Scripting Mastery\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nComprehensive guide for writing professional, portable, and maintainable bash scripts across all platforms.\n\n---\n\n## TL;DR QUICK REFERENCE\n\n**Essential Checklist for Every Bash Script:**\n```bash\n#!/usr/bin/env bash\nset -euo pipefail  # Exit on error, undefined vars, pipe failures\nIFS=$'\\n\\t'        # Safe word splitting\n\n# Use: shellcheck your_script.sh before deployment\n# Test on target platform(s) before production\n```\n\n**Platform Compatibility Quick Check:**\n```bash\n# Linux/macOS:  Full bash features\n# Git Bash (Windows):  Most features,  Some system calls\n# Containers:  Depends on base image\n# POSIX mode: Use /bin/sh and avoid bashisms\n```\n\n---\n\n## Overview\n\nThis skill provides expert bash/shell scripting knowledge for ANY scripting task, ensuring professional-grade quality across all platforms.\n\n**MUST use this skill for:**\n-  ANY bash/shell script creation or modification\n-  System automation and tooling\n-  DevOps/CI/CD pipeline scripts\n-  Build and deployment automation\n-  Script review, debugging, or optimization\n-  Converting manual commands to automated scripts\n-  Cross-platform script compatibility\n\n**What this skill provides:**\n- **Google Shell Style Guide compliance** - Industry-standard formatting and patterns\n- **ShellCheck validation** - Automatic detection of common issues\n- **Cross-platform compatibility** - Linux, macOS, Windows (Git Bash/WSL), containers\n- **POSIX compliance** - Portable scripts that work everywhere\n- **Security hardening** - Input validation, injection prevention, privilege management\n- **Error handling** - Robust `set -euo pipefail`, trap handlers, exit codes\n- **Performance optimization** - Efficient patterns, avoiding anti-patterns\n- **Testing with BATS** - Unit testing, integration testing, CI/CD integration\n- **Debugging techniques** - Logging, troubleshooting, profiling\n- **Production-ready patterns** - Templates and best practices for real-world use\n\n**This skill activates automatically for:**\n- Any mention of \"bash\", \"shell\", \"script\" in task\n- System automation requests\n- DevOps/CI/CD tasks\n- Build/deployment automation\n- Command line tool creation\n\n---\n\n## Core Principles\n\n### 1. Safety First\n\n**ALWAYS start scripts with safety settings:**\n\n```bash\n#!/usr/bin/env bash\n\n# Fail fast and loud\nset -e          # Exit on any error\nset -u          # Exit on undefined variable\nset -o pipefail # Exit on pipe failure\nset -E          # ERR trap inherited by functions\n\n# Optionally:\n# set -x        # Debug mode (print commands before execution)\n# set -C        # Prevent file overwrites with redirection\n\n# Safe word splitting\nIFS=$'\\n\\t'\n\n# Script metadata\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n```\n\n**Why this matters:**\n- `set -e`: Prevents cascading failures\n- `set -u`: Catches typos in variable names\n- `set -o pipefail`: Catches failures in the middle of pipes\n- `IFS=$'\\n\\t'`: Prevents word splitting on spaces (security issue)\n\n### 2. POSIX Compatibility vs Bash Features\n\n**Know when to use which:**\n\n```bash\n# POSIX-compliant (portable across shells)\n#!/bin/sh\n# Use: [ ] tests, no arrays, no [[ ]], no <(process substitution)\n\n# Bash-specific (modern features, clearer syntax)\n#!/usr/bin/env bash\n# Use: [[ ]], arrays, associative arrays, <(), process substitution\n```\n\n**Decision matrix:**\n- Need to run on any UNIX system  Use `#!/bin/sh` and POSIX only\n- Control the environment (modern Linux/macOS)  Use `#!/usr/bin/env bash`\n- Need advanced features (arrays, regex)  Use `#!/usr/bin/env bash`\n\n### 3. Quoting Rules (Critical)\n\n```bash\n# ALWAYS quote variables to prevent word splitting and globbing\nbad_cmd=$file_path          #  WRONG - word splitting\ngood_cmd=\"$file_path\"       #  CORRECT\n\n# Arrays: Quote expansion\nfiles=(\"file 1.txt\" \"file 2.txt\")\nprocess \"${files[@]}\"       #  CORRECT - each element quoted\nprocess \"${files[*]}\"       #  WRONG - all elements as one string\n\n# Command substitution: Quote the result\nresult=\"$(command)\"         #  CORRECT\nresult=$(command)           #  WRONG (unless you want word splitting)\n\n# Exception: When you WANT word splitting\n# shellcheck disable=SC2086\nflags=\"-v -x -z\"\ncommand $flags              # Intentional word splitting\n```\n\n### 4. Use ShellCheck\n\n**ALWAYS run ShellCheck before deployment:**\n\n```bash\n# Install\n# Ubuntu/Debian: apt-get install shellcheck\n# macOS: brew install shellcheck\n# Windows: scoop install shellcheck\n\n# Usage\nshellcheck your_script.sh\nshellcheck -x your_script.sh  # Follow source statements\n\n# In CI/CD\nfind . -name \"*.sh\" -exec shellcheck {} +\n```\n\n**ShellCheck catches:**\n- Quoting issues\n- Bashisms in POSIX scripts\n- Common logic errors\n- Security vulnerabilities\n- Performance anti-patterns\n\n---\n\n## Platform-Specific Considerations\n\n\n### Windows (Git Bash) Path Conversion - CRITICAL\n\n**ESSENTIAL KNOWLEDGE:** Git Bash/MINGW automatically converts Unix-style paths to Windows paths. This is the most common source of cross-platform scripting errors on Windows.\n\n**Complete Guide:** See `references/windows-git-bash-paths.md` for comprehensive documentation.\n\n**Quick Reference:**\n\n```bash\n# Automatic conversion happens for:\n/foo  C:/Program Files/Git/usr/foo\n--dir=/tmp  --dir=C:/msys64/tmp\n\n# Disable conversion when needed\nMSYS_NO_PATHCONV=1 command /path/that/should/not/convert\n\n# Manual conversion with cygpath\nunix_path=$(cygpath -u \"C:\\Windows\\System32\")  # Windows to Unix\nwin_path=$(cygpath -w \"/c/Users/username\")        # Unix to Windows\n\n# Shell detection (fastest method)\nif [[ \"$OSTYPE\" == \"msys\" ]] || [[ \"$OSTYPE\" == \"mingw\"* ]]; then\n    echo \"Git Bash detected\"\n    # Use path conversion\nfi\n\n# Or check $MSYSTEM variable (Git Bash/MSYS2 specific)\ncase \"${MSYSTEM:-}\" in\n    MINGW64|MINGW32|MSYS)\n        echo \"MSYS2/Git Bash environment: $MSYSTEM\"\n        ;;\nesac\n```\n\n**Common Issues:**\n\n```bash\n# Problem: Flags converted to paths\ncommand /e /s  # /e becomes C:/Program Files/Git/e\n\n# Solution: Use double slashes or dashes\ncommand //e //s  # OR: command -e -s\n\n# Problem: Spaces in paths\ncd C:\\Program Files\\Git  # Fails\n\n# Solution: Quote paths\ncd \"C:\\Program Files\\Git\"  # OR: cd /c/Program\\ Files/Git\n```\n\n\n### Linux\n\n**Primary target for most bash scripts:**\n\n```bash\n# Linux-specific features available\n/proc filesystem\nsystemd integration\nLinux-specific commands (apt, yum, systemctl)\n\n# Check for Linux\nif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n    # Linux-specific code\nfi\n```\n\n### macOS\n\n**BSD-based utilities (different from GNU):**\n\n```bash\n# macOS differences\nsed -i ''                    # macOS requires empty string\nsed -i                       # Linux doesn't need it\n\n# Use ggrep, gsed, etc. for GNU versions\nif command -v gsed &> /dev/null; then\n    SED=gsed\nelse\n    SED=sed\nfi\n\n# Check for macOS\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n    # macOS-specific code\nfi\n```\n\n### Windows (Git Bash / WSL)\n\n**Git Bash limitations:**\n\n```bash\n# Available in Git Bash:\n- Most core utils\n- File operations\n- Process management (limited)\n\n# NOT available:\n- systemd\n- Some signals (SIGHUP behavior differs)\n- /proc filesystem\n- Native Windows path handling issues\n\n# Path handling\n# Git Bash uses Unix paths: /c/Users/...\n# Convert if needed:\nwinpath=$(cygpath -w \"$unixpath\")  # Unix  Windows\nunixpath=$(cygpath -u \"$winpath\")  # Windows  Unix\n\n# Check for Git Bash\nif [[ \"$OSTYPE\" == \"msys\" ]] || [[ \"$OSTYPE\" == \"cygwin\" ]]; then\n    # Git Bash / Cygwin code\nfi\n```\n\n**WSL (Windows Subsystem for Linux):**\n```bash\n# WSL is essentially Linux, but:\n# - Can access Windows filesystem at /mnt/c/\n# - Some syscalls behave differently\n# - Network configuration differs\n\n# Check for WSL\nif grep -qi microsoft /proc/version 2>/dev/null; then\n    # WSL-specific code\nfi\n```\n\n### Containers (Docker/Kubernetes)\n\n**Container-aware scripting:**\n\n```bash\n# Minimal base images may not have bash\n# Use #!/bin/sh or install bash explicitly\n\n# Container detection\nif [ -f /.dockerenv ] || grep -q docker /proc/1/cgroup 2>/dev/null; then\n    # Running in Docker\nfi\n\n# Kubernetes detection\nif [ -n \"$KUBERNETES_SERVICE_HOST\" ]; then\n    # Running in Kubernetes\nfi\n\n# Best practices:\n# - Minimize dependencies\n# - Use absolute paths or PATH\n# - Don't assume user/group existence\n# - Handle signals properly (PID 1 issues)\n```\n\n### Cross-Platform Template\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Detect platform\ndetect_platform() {\n    case \"$OSTYPE\" in\n        linux-gnu*)   echo \"linux\" ;;\n        darwin*)      echo \"macos\" ;;\n        msys*|cygwin*) echo \"windows\" ;;\n        *)            echo \"unknown\" ;;\n    esac\n}\n\nPLATFORM=$(detect_platform)\n\n# Platform-specific paths\ncase \"$PLATFORM\" in\n    linux)\n        SED=sed\n        ;;\n    macos)\n        SED=$(command -v gsed || echo sed)\n        ;;\n    windows)\n        # Git Bash specifics\n        ;;\nesac\n```\n\n---\n\n## Best Practices\n\n### Function Design\n\n```bash\n# Good function structure\nfunction_name() {\n    # 1. Local variables first\n    local arg1=\"$1\"\n    local arg2=\"${2:-default_value}\"\n    local result=\"\"\n\n    # 2. Input validation\n    if [[ -z \"$arg1\" ]]; then\n        echo \"Error: arg1 is required\" >&2\n        return 1\n    fi\n\n    # 3. Main logic\n    result=$(some_operation \"$arg1\" \"$arg2\")\n\n    # 4. Output/return\n    echo \"$result\"\n    return 0\n}\n\n# Use functions, not scripts-in-scripts\n# Benefits: testability, reusability, namespacing\n```\n\n### Variable Naming\n\n```bash\n# Constants: UPPER_CASE\nreadonly MAX_RETRIES=3\nreadonly CONFIG_FILE=\"/etc/app/config.conf\"\n\n# Global variables: UPPER_CASE or lower_case (be consistent)\nGLOBAL_STATE=\"initialized\"\n\n# Local variables: lower_case\nlocal user_name=\"john\"\nlocal file_count=0\n\n# Environment variables: UPPER_CASE (by convention)\nexport DATABASE_URL=\"postgres://...\"\n\n# Readonly when possible\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n```\n\n### Error Handling\n\n```bash\n# Method 1: Check exit codes explicitly\nif ! command_that_might_fail; then\n    echo \"Error: Command failed\" >&2\n    return 1\nfi\n\n# Method 2: Use || for alternative actions\ncommand_that_might_fail || {\n    echo \"Error: Command failed\" >&2\n    return 1\n}\n\n# Method 3: Trap for cleanup\ncleanup() {\n    local exit_code=$?\n    # Cleanup operations\n    rm -f \"$TEMP_FILE\"\n    exit \"$exit_code\"\n}\ntrap cleanup EXIT\n\n# Method 4: Custom error handler\nerror_exit() {\n    local message=\"$1\"\n    local code=\"${2:-1}\"\n    echo \"Error: $message\" >&2\n    exit \"$code\"\n}\n\n# Usage\n[[ -f \"$config_file\" ]] || error_exit \"Config file not found: $config_file\"\n```\n\n### Input Validation\n\n```bash\nvalidate_input() {\n    local input=\"$1\"\n\n    # Check if empty\n    if [[ -z \"$input\" ]]; then\n        echo \"Error: Input cannot be empty\" >&2\n        return 1\n    fi\n\n    # Check format (example: alphanumeric only)\n    if [[ ! \"$input\" =~ ^[a-zA-Z0-9_-]+$ ]]; then\n        echo \"Error: Input contains invalid characters\" >&2\n        return 1\n    fi\n\n    # Check length\n    if [[ ${#input} -gt 255 ]]; then\n        echo \"Error: Input too long (max 255 characters)\" >&2\n        return 1\n    fi\n\n    return 0\n}\n\n# Validate before use\nread -r user_input\nif validate_input \"$user_input\"; then\n    process \"$user_input\"\nfi\n```\n\n### Argument Parsing\n\n```bash\n# Simple argument parsing\nusage() {\n    cat <<EOF\nUsage: $SCRIPT_NAME [OPTIONS] <command>\n\nOptions:\n    -h, --help          Show this help\n    -v, --verbose       Verbose output\n    -f, --file FILE     Input file\n    -o, --output DIR    Output directory\n\nCommands:\n    build               Build the project\n    test                Run tests\nEOF\n}\n\nmain() {\n    local verbose=false\n    local input_file=\"\"\n    local output_dir=\".\"\n    local command=\"\"\n\n    # Parse arguments\n    while [[ $# -gt 0 ]]; do\n        case \"$1\" in\n            -h|--help)\n                usage\n                exit 0\n                ;;\n            -v|--verbose)\n                verbose=true\n                shift\n                ;;\n            -f|--file)\n                input_file=\"$2\"\n                shift 2\n                ;;\n            -o|--output)\n                output_dir=\"$2\"\n                shift 2\n                ;;\n            -*)\n                echo \"Error: Unknown option: $1\" >&2\n                usage >&2\n                exit 1\n                ;;\n            *)\n                command=\"$1\"\n                shift\n                break\n                ;;\n        esac\n    done\n\n    # Validate required arguments\n    if [[ -z \"$command\" ]]; then\n        echo \"Error: Command is required\" >&2\n        usage >&2\n        exit 1\n    fi\n\n    # Execute command\n    case \"$command\" in\n        build) do_build ;;\n        test)  do_test ;;\n        *)\n            echo \"Error: Unknown command: $command\" >&2\n            usage >&2\n            exit 1\n            ;;\n    esac\n}\n\nmain \"$@\"\n```\n\n### Logging\n\n```bash\n# Logging levels\nreadonly LOG_LEVEL_DEBUG=0\nreadonly LOG_LEVEL_INFO=1\nreadonly LOG_LEVEL_WARN=2\nreadonly LOG_LEVEL_ERROR=3\n\n# Current log level\nLOG_LEVEL=${LOG_LEVEL:-$LOG_LEVEL_INFO}\n\nlog_debug() { [[ $LOG_LEVEL -le $LOG_LEVEL_DEBUG ]] && echo \"[DEBUG] $*\" >&2; }\nlog_info()  { [[ $LOG_LEVEL -le $LOG_LEVEL_INFO  ]] && echo \"[INFO]  $*\" >&2; }\nlog_warn()  { [[ $LOG_LEVEL -le $LOG_LEVEL_WARN  ]] && echo \"[WARN]  $*\" >&2; }\nlog_error() { [[ $LOG_LEVEL -le $LOG_LEVEL_ERROR ]] && echo \"[ERROR] $*\" >&2; }\n\n# With timestamps\nlog_with_timestamp() {\n    local level=\"$1\"\n    shift\n    echo \"[$(date +'%Y-%m-%d %H:%M:%S')] [$level] $*\" >&2\n}\n\n# Usage\nlog_info \"Starting process\"\nlog_error \"Failed to connect to database\"\n```\n\n---\n\n## Security Best Practices\n\n### Command Injection Prevention\n\n```bash\n# NEVER use eval with user input\n#  WRONG - DANGEROUS\neval \"$user_input\"\n\n# NEVER use dynamic variable names from user input\n#  WRONG - DANGEROUS\neval \"var_$user_input=value\"\n\n# NEVER concatenate user input into commands\n#  WRONG - DANGEROUS\ngrep \"$user_pattern\" file.txt  # If pattern contains -e flag, injection possible\n\n#  CORRECT - Use arrays\ngrep_args=(\"$user_pattern\" \"file.txt\")\ngrep \"${grep_args[@]}\"\n\n#  CORRECT - Use -- to separate options from arguments\ngrep -- \"$user_pattern\" file.txt\n```\n\n### Path Traversal Prevention\n\n```bash\n# Sanitize file paths\nsanitize_path() {\n    local path=\"$1\"\n\n    # Remove .. components\n    path=\"${path//..\\/}\"\n    path=\"${path//\\/..\\//}\"\n\n    # Remove leading /\n    path=\"${path#/}\"\n\n    echo \"$path\"\n}\n\n# Validate path is within allowed directory\nis_safe_path() {\n    local file_path=\"$1\"\n    local base_dir=\"$2\"\n\n    # Resolve to absolute path\n    local real_path\n    real_path=$(readlink -f \"$file_path\" 2>/dev/null) || return 1\n    local real_base\n    real_base=$(readlink -f \"$base_dir\" 2>/dev/null) || return 1\n\n    # Check if path starts with base directory\n    [[ \"$real_path\" == \"$real_base\"/* ]]\n}\n\n# Usage\nif is_safe_path \"$user_file\" \"/var/app/data\"; then\n    process_file \"$user_file\"\nelse\n    echo \"Error: Invalid file path\" >&2\n    exit 1\nfi\n```\n\n### Privilege Management\n\n```bash\n# Check if running as root\nif [[ $EUID -eq 0 ]]; then\n    echo \"Error: Do not run this script as root\" >&2\n    exit 1\nfi\n\n# Drop privileges if needed\ndrop_privileges() {\n    local user=\"$1\"\n\n    if [[ $EUID -eq 0 ]]; then\n        exec sudo -u \"$user\" \"$0\" \"$@\"\n    fi\n}\n\n# Run specific command with elevated privileges\nrun_as_root() {\n    if [[ $EUID -ne 0 ]]; then\n        sudo \"$@\"\n    else\n        \"$@\"\n    fi\n}\n```\n\n### Temporary File Handling\n\n```bash\n# Create secure temporary files\nreadonly TEMP_DIR=$(mktemp -d)\nreadonly TEMP_FILE=$(mktemp)\n\n# Cleanup on exit\ncleanup() {\n    rm -rf \"$TEMP_DIR\"\n    rm -f \"$TEMP_FILE\"\n}\ntrap cleanup EXIT\n\n# Secure temporary file (only readable by owner)\nsecure_temp=$(mktemp)\nchmod 600 \"$secure_temp\"\n```\n\n---\n\n## Performance Optimization\n\n### Avoid Unnecessary Subshells\n\n```bash\n#  SLOW - Creates subshell for each iteration\nwhile IFS= read -r line; do\n    count=$(echo \"$count + 1\" | bc)\ndone < file.txt\n\n#  FAST - Arithmetic in bash\ncount=0\nwhile IFS= read -r line; do\n    ((count++))\ndone < file.txt\n```\n\n### Use Bash Built-ins\n\n```bash\n#  SLOW - External commands\ndirname=$(dirname \"$path\")\nbasename=$(basename \"$path\")\n\n#  FAST - Parameter expansion\ndirname=\"${path%/*}\"\nbasename=\"${path##*/}\"\n\n#  SLOW - grep for simple checks\nif echo \"$string\" | grep -q \"pattern\"; then\n\n#  FAST - Bash regex\nif [[ \"$string\" =~ pattern ]]; then\n\n#  SLOW - awk for simple extraction\nfield=$(echo \"$line\" | awk '{print $3}')\n\n#  FAST - Read into array\nread -ra fields <<< \"$line\"\nfield=\"${fields[2]}\"\n```\n\n### Process Substitution vs Pipes\n\n```bash\n# When you need to read multiple commands' output\n#  GOOD - Process substitution\nwhile IFS= read -r line1 <&3 && IFS= read -r line2 <&4; do\n    echo \"$line1 - $line2\"\ndone 3< <(command1) 4< <(command2)\n\n# Parallel processing\ncommand1 &\ncommand2 &\nwait  # Wait for all background jobs\n```\n\n### Array Operations\n\n```bash\n#  FAST - Native array operations\nfiles=(*.txt)\necho \"Found ${#files[@]} files\"\n\n#  SLOW - Parsing ls output\ncount=$(ls -1 *.txt | wc -l)\n\n#  FAST - Array filtering\nfiltered=()\nfor item in \"${array[@]}\"; do\n    [[ \"$item\" =~ ^[0-9]+$ ]] && filtered+=(\"$item\")\ndone\n\n#  FAST - Array joining\nIFS=,\njoined=\"${array[*]}\"\nIFS=$'\\n\\t'\n```\n\n---\n\n## Testing\n\n### Unit Testing with BATS\n\n```bash\n# Install BATS\n# git clone https://github.com/bats-core/bats-core.git\n# cd bats-core && ./install.sh /usr/local\n\n# test/script.bats\n#!/usr/bin/env bats\n\n# Load script to test\nload '../script.sh'\n\n@test \"function returns correct value\" {\n    result=$(my_function \"input\")\n    [ \"$result\" = \"expected\" ]\n}\n\n@test \"function handles empty input\" {\n    run my_function \"\"\n    [ \"$status\" -eq 1 ]\n    [ \"${lines[0]}\" = \"Error: Input cannot be empty\" ]\n}\n\n@test \"function validates input format\" {\n    run my_function \"invalid@input\"\n    [ \"$status\" -eq 1 ]\n}\n\n# Run tests\n# bats test/script.bats\n```\n\n### Integration Testing\n\n```bash\n# integration_test.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Setup\nsetup() {\n    export TEST_DIR=$(mktemp -d)\n    export TEST_FILE=\"$TEST_DIR/test.txt\"\n}\n\n# Teardown\nteardown() {\n    rm -rf \"$TEST_DIR\"\n}\n\n# Test case\ntest_file_creation() {\n    ./script.sh create \"$TEST_FILE\"\n\n    if [[ ! -f \"$TEST_FILE\" ]]; then\n        echo \"FAIL: File was not created\"\n        return 1\n    fi\n\n    echo \"PASS: File creation works\"\n    return 0\n}\n\n# Run tests\nmain() {\n    setup\n    trap teardown EXIT\n\n    test_file_creation || exit 1\n\n    echo \"All tests passed\"\n}\n\nmain\n```\n\n### CI/CD Integration\n\n```yaml\n# .github/workflows/test.yml\nname: Test\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install shellcheck\n        run: sudo apt-get install -y shellcheck\n\n      - name: Run shellcheck\n        run: find . -name \"*.sh\" -exec shellcheck {} +\n\n      - name: Install bats\n        run: |\n          git clone https://github.com/bats-core/bats-core.git\n          cd bats-core\n          sudo ./install.sh /usr/local\n\n      - name: Run tests\n        run: bats test/\n```\n\n---\n\n## Debugging Techniques\n\n### Debug Mode\n\n```bash\n# Method 1: set -x (print commands)\nset -x\ncommand1\ncommand2\nset +x  # Turn off\n\n# Method 2: PS4 for better output\nexport PS4='+(${BASH_SOURCE}:${LINENO}): ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'\nset -x\n\n# Method 3: Conditional debugging\nDEBUG=${DEBUG:-false}\ndebug() {\n    if [[ \"$DEBUG\" == \"true\" ]]; then\n        echo \"[DEBUG] $*\" >&2\n    fi\n}\n\n# Usage: DEBUG=true ./script.sh\n```\n\n### Tracing and Profiling\n\n```bash\n# Trace function calls\ntrace() {\n    echo \"[TRACE] Function: ${FUNCNAME[1]}, Args: $*\" >&2\n}\n\nmy_function() {\n    trace \"$@\"\n    # Function logic\n}\n\n# Execution time profiling\nprofile() {\n    local start=$(date +%s%N)\n    \"$@\"\n    local end=$(date +%s%N)\n    local duration=$(( (end - start) / 1000000 ))\n    echo \"[PROFILE] Command '$*' took ${duration}ms\" >&2\n}\n\n# Usage\nprofile slow_command arg1 arg2\n```\n\n### Common Issues and Solutions\n\n```bash\n# Issue: Script works in bash but not in sh\n# Solution: Check for bashisms\ncheckbashisms script.sh\n\n# Issue: Works locally but not on server\n# Solution: Check PATH and environment\nenv\necho \"$PATH\"\n\n# Issue: Whitespace in filenames breaking script\n# Solution: Always quote variables\nfor file in *.txt; do\n    process \"$file\"  # Not: process $file\ndone\n\n# Issue: Script behaves differently in cron\n# Solution: Set PATH explicitly\nPATH=/usr/local/bin:/usr/bin:/bin\nexport PATH\n```\n\n---\n\n## Advanced Patterns\n\n### Configuration File Parsing\n\n```bash\n# Simple key=value config\nload_config() {\n    local config_file=\"$1\"\n\n    if [[ ! -f \"$config_file\" ]]; then\n        echo \"Error: Config file not found: $config_file\" >&2\n        return 1\n    fi\n\n    # Source config (dangerous if not trusted)\n    # shellcheck source=/dev/null\n    source \"$config_file\"\n}\n\n# Safe config parsing (no code execution)\nread_config() {\n    local config_file=\"$1\"\n\n    while IFS='=' read -r key value; do\n        # Skip comments and empty lines\n        [[ \"$key\" =~ ^[[:space:]]*# ]] && continue\n        [[ -z \"$key\" ]] && continue\n\n        # Trim whitespace\n        key=$(echo \"$key\" | tr -d ' ')\n        value=$(echo \"$value\" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')\n\n        # Export variable\n        declare -g \"$key=$value\"\n    done < \"$config_file\"\n}\n```\n\n### Parallel Processing\n\n```bash\n# Simple background jobs\nprocess_files_parallel() {\n    local max_jobs=4\n    local job_count=0\n\n    for file in *.txt; do\n        # Start background job\n        process_file \"$file\" &\n\n        # Limit concurrent jobs\n        ((job_count++))\n        if [[ $job_count -ge $max_jobs ]]; then\n            wait -n  # Wait for any job to finish\n            ((job_count--))\n        fi\n    done\n\n    # Wait for remaining jobs\n    wait\n}\n\n# GNU Parallel (if available)\nparallel_with_gnu() {\n    parallel -j 4 process_file ::: *.txt\n}\n```\n\n### Signal Handling\n\n```bash\n# Graceful shutdown\nshutdown_requested=false\n\nhandle_sigterm() {\n    echo \"Received SIGTERM, shutting down gracefully...\" >&2\n    shutdown_requested=true\n}\n\ntrap handle_sigterm SIGTERM SIGINT\n\nmain_loop() {\n    while [[ \"$shutdown_requested\" == \"false\" ]]; do\n        # Do work\n        sleep 1\n    done\n\n    echo \"Shutdown complete\" >&2\n}\n\nmain_loop\n```\n\n### Retries with Exponential Backoff\n\n```bash\nretry_with_backoff() {\n    local max_attempts=5\n    local timeout=1\n    local attempt=1\n    local exitCode=0\n\n    while [[ $attempt -le $max_attempts ]]; do\n        if \"$@\"; then\n            return 0\n        else\n            exitCode=$?\n        fi\n\n        echo \"Attempt $attempt failed! Retrying in $timeout seconds...\" >&2\n        sleep \"$timeout\"\n        attempt=$((attempt + 1))\n        timeout=$((timeout * 2))\n    done\n\n    echo \"Command failed after $max_attempts attempts!\" >&2\n    return \"$exitCode\"\n}\n\n# Usage\nretry_with_backoff curl -f https://api.example.com/health\n```\n\n---\n\n## Resources for Additional Information\n\n### Official Documentation\n\n1. **Bash Reference Manual**\n   - URL: https://www.gnu.org/software/bash/manual/\n   - The authoritative source for bash features and behavior\n\n2. **POSIX Shell Command Language**\n   - URL: https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html\n   - For writing portable scripts\n\n### Style Guides\n\n1. **Google Shell Style Guide**\n   - URL: https://google.github.io/styleguide/shellguide.html\n   - Industry-standard practices from Google\n\n2. **Defensive Bash Programming**\n   - URL: https://kfirlavi.herokuapp.com/blog/2012/11/14/defensive-bash-programming\n   - Best practices for robust scripts\n\n### Tools\n\n1. **ShellCheck**\n   - URL: https://www.shellcheck.net/\n   - GitHub: https://github.com/koalaman/shellcheck\n   - Static analysis tool for shell scripts\n\n2. **BATS (Bash Automated Testing System)**\n   - GitHub: https://github.com/bats-core/bats-core\n   - Unit testing framework for bash\n\n3. **shfmt**\n   - GitHub: https://github.com/mvdan/sh\n   - Shell script formatter\n\n### Learning Resources\n\n1. **Bash Academy**\n   - URL: https://www.bash.academy/\n   - Comprehensive bash learning resource\n\n2. **Bash Guide for Beginners**\n   - URL: https://tldp.org/LDP/Bash-Beginners-Guide/html/\n   - From The Linux Documentation Project\n\n3. **Advanced Bash-Scripting Guide**\n   - URL: https://tldp.org/LDP/abs/html/\n   - In-depth coverage of advanced topics\n\n4. **Bash Pitfalls**\n   - URL: https://mywiki.wooledge.org/BashPitfalls\n   - Common mistakes and how to avoid them\n\n5. **explainshell.com**\n   - URL: https://explainshell.com/\n   - Interactive tool to explain shell commands\n\n### Platform-Specific Resources\n\n1. **GNU Coreutils Manual**\n   - URL: https://www.gnu.org/software/coreutils/manual/\n   - For Linux-specific commands\n\n2. **FreeBSD Manual Pages**\n   - URL: https://www.freebsd.org/cgi/man.cgi\n   - For macOS (BSD-based) differences\n\n3. **Git for Windows**\n   - URL: https://gitforwindows.org/\n   - Git Bash documentation and issues\n\n4. **WSL Documentation**\n   - URL: https://docs.microsoft.com/en-us/windows/wsl/\n   - Windows Subsystem for Linux specifics\n\n### Community Resources\n\n1. **Stack Overflow - Bash Tag**\n   - URL: https://stackoverflow.com/questions/tagged/bash\n   - Community Q&A\n\n2. **Unix & Linux Stack Exchange**\n   - URL: https://unix.stackexchange.com/\n   - Shell scripting expertise\n\n3. **Reddit - r/bash**\n   - URL: https://www.reddit.com/r/bash/\n   - Community discussions\n\n### Quick Reference\n\n1. **Bash Cheat Sheet**\n   - URL: https://devhints.io/bash\n   - Quick syntax reference\n\n2. **ShellCheck Wiki**\n   - URL: https://www.shellcheck.net/wiki/\n   - Explanations of ShellCheck warnings\n\n---\n\n## Reference Files\n\nFor deeper coverage of specific topics, see the reference files:\n\n- **[references/platform_specifics.md](references/platform_specifics.md)** - Detailed platform differences and workarounds\n- **[references/best_practices.md](references/best_practices.md)** - Comprehensive industry standards and guidelines\n- **[references/patterns_antipatterns.md](references/patterns_antipatterns.md)** - Common patterns and pitfalls with solutions\n\n---\n\n## When to Use This Skill\n\n**Always activate for:**\n- Writing new bash scripts\n- Reviewing/refactoring existing scripts\n- Debugging shell script issues\n- Cross-platform shell scripting\n- DevOps automation tasks\n- CI/CD pipeline scripts\n- System administration automation\n\n**Key indicators:**\n- User mentions bash, shell, or script\n- Task involves automation\n- Platform compatibility is a concern\n- Security or robustness is important\n- Performance optimization needed\n\n---\n\n## Success Criteria\n\nA bash script using this skill should:\n\n1.  Pass ShellCheck with no warnings\n2.  Include proper error handling (set -euo pipefail)\n3.  Quote all variable expansions\n4.  Include usage/help text\n5.  Use functions for reusable logic\n6.  Include appropriate comments\n7.  Handle edge cases (empty input, missing files, etc.)\n8.  Work across target platforms\n9.  Follow consistent style (Google Shell Style Guide)\n10.  Include cleanup (trap EXIT)\n\n**Quality checklist:**\n```bash\n# Run before deployment\nshellcheck script.sh              # No errors or warnings\nbash -n script.sh                 # Syntax check\nbats test/script.bats             # Unit tests pass\n./script.sh --help                # Usage text displays\nDEBUG=true ./script.sh            # Debug mode works\n```\n\n---\n\n## Troubleshooting\n\n### Script fails on different platform\n1. Check for bashisms: `checkbashisms script.sh`\n2. Verify commands exist: `command -v tool_name`\n3. Test command flags: `sed --version` (GNU) vs `sed` (BSD)\n\n### ShellCheck warnings\n1. Read the explanation: `shellcheck -W SC2086`\n2. Fix the issue (don't just disable)\n3. Only disable with justification: `# shellcheck disable=SC2086 reason: intentional word splitting`\n\n### Script works interactively but fails in cron\n1. Set PATH explicitly\n2. Use absolute paths\n3. Redirect output for debugging: `./script.sh >> /tmp/cron.log 2>&1`\n\n### Performance issues\n1. Profile with `time command`\n2. Enable tracing: `set -x`\n3. Avoid unnecessary subshells and external commands\n4. Use bash built-ins where possible\n\n---\n\nThis skill provides comprehensive bash scripting knowledge. Combined with the reference files, you have access to industry-standard practices and platform-specific guidance for any bash scripting task."
              },
              {
                "name": "debugging-troubleshooting-2025",
                "description": "Comprehensive bash script debugging and troubleshooting techniques for 2025",
                "path": "plugins/bash-master/skills/debugging-troubleshooting-2025/SKILL.md",
                "frontmatter": {
                  "name": "debugging-troubleshooting-2025",
                  "description": "Comprehensive bash script debugging and troubleshooting techniques for 2025"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Bash Debugging & Troubleshooting (2025)\n\n## Overview\n\nComprehensive debugging techniques and troubleshooting patterns for bash scripts following 2025 best practices.\n\n## Debug Mode Techniques\n\n### 1. Basic Debug Mode (set -x)\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Enable debug mode\nset -x\n\n# Your commands here\ncommand1\ncommand2\n\n# Disable debug mode\nset +x\n\n# Continue without debug\ncommand3\n```\n\n### 2. Enhanced Debug Output (PS4)\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Custom debug prompt with file:line:function\nexport PS4='+(${BASH_SOURCE}:${LINENO}): ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'\n\nset -x\nmy_function() {\n    local var=\"value\"\n    echo \"$var\"\n}\nmy_function\nset +x\n```\n\n**Output:**\n```\n+(script.sh:10): my_function(): local var=value\n+(script.sh:11): my_function(): echo value\nvalue\n```\n\n### 3. Conditional Debugging\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Enable via environment variable\nDEBUG=\"${DEBUG:-false}\"\n\ndebug() {\n    if [[ \"$DEBUG\" == \"true\" ]]; then\n        echo \"[DEBUG] $*\" >&2\n    fi\n}\n\n# Usage\ndebug \"Starting process\"\nprocess_data\ndebug \"Process complete\"\n\n# Run: DEBUG=true ./script.sh\n```\n\n### 4. Debugging Specific Functions\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Debug wrapper\ndebug_function() {\n    local func_name=\"$1\"\n    shift\n\n    echo \"[TRACE] Calling: $func_name $*\" >&2\n    set -x\n    \"$func_name\" \"$@\"\n    local exit_code=$?\n    set +x\n    echo \"[TRACE] Exit code: $exit_code\" >&2\n    return $exit_code\n}\n\n# Usage\nmy_complex_function() {\n    local arg1=\"$1\"\n    # Complex logic\n    echo \"Result: $arg1\"\n}\n\ndebug_function my_complex_function \"test\"\n```\n\n## Tracing and Profiling\n\n### 1. Execution Time Profiling\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Profile function execution time\nprofile() {\n    local start_ns end_ns duration_ms\n    start_ns=$(date +%s%N)\n\n    \"$@\"\n    local exit_code=$?\n\n    end_ns=$(date +%s%N)\n    duration_ms=$(( (end_ns - start_ns) / 1000000 ))\n\n    echo \"[PROFILE] '$*' took ${duration_ms}ms (exit: $exit_code)\" >&2\n    return $exit_code\n}\n\n# Usage\nprofile slow_command arg1 arg2\n```\n\n### 2. Function Call Tracing\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Trace all function calls\ntrace_on() {\n    set -o functrace\n    trap 'echo \"[TRACE] ${FUNCNAME[0]}() called from ${BASH_SOURCE[1]}:${BASH_LINENO[0]}\" >&2' DEBUG\n}\n\ntrace_off() {\n    set +o functrace\n    trap - DEBUG\n}\n\n# Usage\ntrace_on\nfunction1\nfunction2\ntrace_off\n```\n\n### 3. Variable Inspection\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Inspect all variables at any point\ninspect_vars() {\n    echo \"=== Variable Dump ===\" >&2\n    declare -p | grep -v \"^declare -[^ ]*r \" | sort >&2\n    echo \"===================\" >&2\n}\n\n# Inspect specific variable\ninspect_var() {\n    local var_name=\"$1\"\n    echo \"[INSPECT] $var_name = ${!var_name:-<unset>}\" >&2\n}\n\n# Usage\nmy_var=\"test\"\ninspect_var my_var\ninspect_vars\n```\n\n## Error Handling and Recovery\n\n### 1. Trap-Based Error Handler\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Comprehensive error handler\nerror_handler() {\n    local exit_code=$?\n    local line_number=$1\n\n    echo \"ERROR: Command failed with exit code $exit_code\" >&2\n    echo \"  File: ${BASH_SOURCE[1]}\" >&2\n    echo \"  Line: $line_number\" >&2\n    echo \"  Function: ${FUNCNAME[1]:-main}\" >&2\n\n    # Print stack trace\n    local frame=0\n    while caller $frame; do\n        ((frame++))\n    done >&2\n\n    exit \"$exit_code\"\n}\n\ntrap 'error_handler $LINENO' ERR\n\n# Your script logic\nrisky_command\n```\n\n### 2. Dry-Run Mode\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nDRY_RUN=\"${DRY_RUN:-false}\"\n\n# Safe execution wrapper\nexecute() {\n    if [[ \"$DRY_RUN\" == \"true\" ]]; then\n        echo \"[DRY-RUN] Would execute: $*\" >&2\n        return 0\n    else\n        \"$@\"\n    fi\n}\n\n# Usage\nexecute rm -rf /tmp/data\nexecute cp file.txt backup/\n\n# Run: DRY_RUN=true ./script.sh\n```\n\n### 3. Rollback on Failure\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nOPERATIONS=()\n\n# Track operations for rollback\ntrack_operation() {\n    local rollback_cmd=\"$1\"\n    OPERATIONS+=(\"$rollback_cmd\")\n}\n\n# Execute rollback\nrollback() {\n    echo \"Rolling back operations...\" >&2\n    for ((i=${#OPERATIONS[@]}-1; i>=0; i--)); do\n        echo \"  Executing: ${OPERATIONS[$i]}\" >&2\n        eval \"${OPERATIONS[$i]}\" || true\n    done\n}\n\ntrap rollback ERR EXIT\n\n# Example usage\nmkdir /tmp/mydir\ntrack_operation \"rmdir /tmp/mydir\"\n\ntouch /tmp/mydir/file.txt\ntrack_operation \"rm /tmp/mydir/file.txt\"\n\n# If script fails, rollback executes automatically\n```\n\n## Common Issues and Solutions\n\n### 1. Script Works Interactively but Fails in Cron\n\n**Problem:** Script runs fine manually but fails when scheduled.\n\n**Solution:**\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Fix PATH for cron\nexport PATH=\"/usr/local/bin:/usr/bin:/bin\"\n\n# Set working directory\ncd \"$(dirname \"$0\")\" || exit 1\n\n# Log everything for debugging\nexec 1>> /var/log/myscript.log 2>&1\n\necho \"[$(date)] Script starting\"\n# Your commands here\necho \"[$(date)] Script complete\"\n```\n\n### 2. Whitespace in Filenames Breaking Script\n\n**Problem:** Script fails when processing files with spaces.\n\n**Debugging:**\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Show exactly what the script sees\ndebug_filename() {\n    local filename=\"$1\"\n    echo \"Filename: '$filename'\" >&2\n    echo \"Length: ${#filename}\" >&2\n    hexdump -C <<< \"$filename\" >&2\n}\n\n# Proper handling\nwhile IFS= read -r -d '' file; do\n    debug_filename \"$file\"\n    # Process \"$file\"\ndone < <(find . -name \"*.txt\" -print0)\n```\n\n### 3. Script Behaves Differently on Different Systems\n\n**Problem:** Works on Linux but fails on macOS.\n\n**Debugging:**\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Platform detection and debugging\ndetect_platform() {\n    echo \"=== Platform Info ===\" >&2\n    echo \"OS: $OSTYPE\" >&2\n    echo \"Bash: $BASH_VERSION\" >&2\n    echo \"PATH: $PATH\" >&2\n\n    # Check tool versions\n    for tool in sed awk grep; do\n        if command -v \"$tool\" &> /dev/null; then\n            echo \"$tool: $($tool --version 2>&1 | head -1)\" >&2\n        fi\n    done\n    echo \"====================\" >&2\n}\n\ndetect_platform\n\n# Use portable patterns\ncase \"$OSTYPE\" in\n    linux*)   SED_CMD=\"sed\" ;;\n    darwin*)  SED_CMD=$(command -v gsed || echo sed) ;;\n    *)        echo \"Unknown platform\" >&2; exit 1 ;;\nesac\n```\n\n### 4. Variable Scope Issues\n\n**Problem:** Variables not available where expected.\n\n**Debugging:**\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Show variable scope\ntest_scope() {\n    local local_var=\"local\"\n    global_var=\"global\"\n\n    echo \"Inside function:\" >&2\n    echo \"  local_var=$local_var\" >&2\n    echo \"  global_var=$global_var\" >&2\n}\n\ntest_scope\n\necho \"Outside function:\" >&2\necho \"  local_var=${local_var:-<not set>}\" >&2\necho \"  global_var=${global_var:-<not set>}\" >&2\n\n# Subshell scope issue\necho \"test\" | (\n    read -r value\n    echo \"In subshell: $value\"\n)\necho \"After subshell: ${value:-<not set>}\"  # Empty!\n```\n\n## Interactive Debugging\n\n### 1. Breakpoint Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Interactive breakpoint\nbreakpoint() {\n    local message=\"${1:-Breakpoint}\"\n    echo \"$message\" >&2\n    echo \"Variables:\" >&2\n    declare -p | grep -v \"^declare -[^ ]*r \" >&2\n\n    read -rp \"Press Enter to continue, 'i' for inspect: \" choice\n    if [[ \"$choice\" == \"i\" ]]; then\n        bash  # Drop into interactive shell\n    fi\n}\n\n# Usage\nvalue=42\nbreakpoint \"Before critical operation\"\ncritical_operation \"$value\"\n```\n\n### 2. Watch Mode (Continuous Debugging)\n\n```bash\n#!/usr/bin/env bash\n\n# Watch script execution in real-time\nwatch_script() {\n    local script=\"$1\"\n    shift\n\n    while true; do\n        clear\n        echo \"=== Running: $script $* ===\"\n        echo \"=== $(date) ===\"\n        bash -x \"$script\" \"$@\" 2>&1 | tail -50\n        sleep 2\n    done\n}\n\n# Usage: watch_script myscript.sh arg1 arg2\n```\n\n## Logging Best Practices\n\n### 1. Structured Logging\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nreadonly LOG_FILE=\"${LOG_FILE:-/var/log/myscript.log}\"\n\nlog() {\n    local level=\"$1\"\n    shift\n    local timestamp\n    timestamp=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\n    echo \"${timestamp} [${level}] $*\" | tee -a \"$LOG_FILE\" >&2\n}\n\nlog_info()  { log \"INFO\"  \"$@\"; }\nlog_warn()  { log \"WARN\"  \"$@\"; }\nlog_error() { log \"ERROR\" \"$@\"; }\nlog_debug() { [[ \"${DEBUG:-false}\" == \"true\" ]] && log \"DEBUG\" \"$@\"; }\n\n# Usage\nlog_info \"Starting process\"\nlog_debug \"Debug info\"\nlog_error \"Something failed\"\n```\n\n### 2. Log Rotation Awareness\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Ensure log file exists and is writable\nsetup_logging() {\n    local log_file=\"${1:-/var/log/myscript.log}\"\n    local log_dir\n    log_dir=$(dirname \"$log_file\")\n\n    if [[ ! -d \"$log_dir\" ]]; then\n        mkdir -p \"$log_dir\" || {\n            echo \"Cannot create log directory: $log_dir\" >&2\n            return 1\n        }\n    fi\n\n    if [[ ! -w \"$log_dir\" ]]; then\n        echo \"Log directory not writable: $log_dir\" >&2\n        return 1\n    fi\n\n    # Redirect all output to log\n    exec 1>> \"$log_file\"\n    exec 2>&1\n}\n\nsetup_logging\n```\n\n## Performance Debugging\n\n### 1. Identify Slow Commands\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Profile each command in script\nprofile_script() {\n    export PS4='+ $(date +%s.%N) ${BASH_SOURCE}:${LINENO}: '\n    set -x\n\n    # Your commands here\n    command1\n    command2\n    command3\n\n    set +x\n}\n\n# Analyze output:\n# + 1698765432.123456 script.sh:10: command1  (fast)\n# + 1698765437.654321 script.sh:11: command2  (5 seconds - slow!)\n```\n\n### 2. Memory Usage Tracking\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Track memory usage\ncheck_memory() {\n    local pid=${1:-$$}\n    ps -o pid,vsz,rss,comm -p \"$pid\" | tail -1\n}\n\n# Monitor during execution\nmonitor_memory() {\n    while true; do\n        check_memory\n        sleep 1\n    done &\n    local monitor_pid=$!\n\n    # Your commands here\n    \"$@\"\n\n    kill \"$monitor_pid\" 2>/dev/null || true\n    wait \"$monitor_pid\" 2>/dev/null || true\n}\n\nmonitor_memory ./memory_intensive_task.sh\n```\n\n## Testing Patterns\n\n### 1. Unit Test Template\n\n```bash\n#!/usr/bin/env bash\n# test_functions.sh\n\n# Source the script to test\nsource ./functions.sh\n\n# Test counter\nTESTS_RUN=0\nTESTS_PASSED=0\nTESTS_FAILED=0\n\n# Assert function\nassert_equals() {\n    local expected=\"$1\"\n    local actual=\"$2\"\n    local test_name=\"${3:-Test}\"\n\n    ((TESTS_RUN++))\n\n    if [[ \"$expected\" == \"$actual\" ]]; then\n        echo \" $test_name\" >&2\n        ((TESTS_PASSED++))\n    else\n        echo \" $test_name\" >&2\n        echo \"  Expected: $expected\" >&2\n        echo \"  Actual:   $actual\" >&2\n        ((TESTS_FAILED++))\n    fi\n}\n\n# Run tests\ntest_add_numbers() {\n    local result\n    result=$(add_numbers 2 3)\n    assert_equals \"5\" \"$result\" \"add_numbers 2 3\"\n}\n\ntest_add_numbers\n\n# Summary\necho \"========================================\" >&2\necho \"Tests run: $TESTS_RUN\" >&2\necho \"Passed: $TESTS_PASSED\" >&2\necho \"Failed: $TESTS_FAILED\" >&2\n\n[[ $TESTS_FAILED -eq 0 ]]\n```\n\n## ShellCheck Integration\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Validate script with ShellCheck\nvalidate_script() {\n    local script=\"$1\"\n\n    if ! command -v shellcheck &> /dev/null; then\n        echo \"ShellCheck not installed\" >&2\n        return 1\n    fi\n\n    echo \"Running ShellCheck on $script...\" >&2\n    if shellcheck --severity=warning \"$script\"; then\n        echo \" ShellCheck passed\" >&2\n        return 0\n    else\n        echo \" ShellCheck failed\" >&2\n        return 1\n    fi\n}\n\n# Usage\nvalidate_script myscript.sh\n```\n\n## Resources\n\n- [Bash Hackers Wiki - Debugging](https://wiki.bash-hackers.org/scripting/debuggingtips)\n- [ShellCheck](https://www.shellcheck.net/)\n- [BATS Testing Framework](https://github.com/bats-core/bats-core)\n- [Bash Reference Manual - Debugging](https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html)\n\n---\n\n**Effective debugging requires systematic approaches, comprehensive logging, and proper tooling. Master these techniques for production-ready bash scripts in 2025.**"
              },
              {
                "name": "modern-automation-patterns",
                "description": "Modern DevOps and CI/CD automation patterns with containers and cloud (2025)",
                "path": "plugins/bash-master/skills/modern-automation-patterns/SKILL.md",
                "frontmatter": {
                  "name": "modern-automation-patterns",
                  "description": "Modern DevOps and CI/CD automation patterns with containers and cloud (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Modern Automation Patterns (2025)\n\n## Overview\n\nProduction-ready patterns for DevOps automation, CI/CD pipelines, and cloud-native operations following 2025 industry standards.\n\n## Container-Aware Scripting\n\n### Docker/Kubernetes Detection\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Detect container environment\ndetect_container() {\n    if [[ -f /.dockerenv ]]; then\n        echo \"docker\"\n    elif grep -q docker /proc/1/cgroup 2>/dev/null; then\n        echo \"docker\"\n    elif [[ -n \"${KUBERNETES_SERVICE_HOST:-}\" ]]; then\n        echo \"kubernetes\"\n    else\n        echo \"host\"\n    fi\n}\n\n# Container-aware configuration\nreadonly CONTAINER_ENV=$(detect_container)\n\ncase \"$CONTAINER_ENV\" in\n    docker|kubernetes)\n        # Container-specific paths\n        DATA_DIR=\"/data\"\n        CONFIG_DIR=\"/config\"\n        ;;\n    host)\n        # Host-specific paths\n        DATA_DIR=\"/var/lib/app\"\n        CONFIG_DIR=\"/etc/app\"\n        ;;\nesac\n```\n\n### Minimal Container Scripts\n\n```bash\n#!/bin/sh\n# Use /bin/sh for Alpine-based containers (no bash)\n\nset -eu  # Note: No pipefail in POSIX sh\n\n# Check if running as PID 1\nif [ $$ -eq 1 ]; then\n    # PID 1 must handle signals properly\n    trap 'kill -TERM $child 2>/dev/null' TERM INT\n\n    # Start main process in background\n    /app/main &\n    child=$!\n\n    # Wait for child\n    wait \"$child\"\nelse\n    # Not PID 1, run directly\n    exec /app/main\nfi\n```\n\n### Health Check Scripts\n\n```bash\n#!/usr/bin/env bash\n# healthcheck.sh - Container health probe\n\nset -euo pipefail\n\n# Quick health check (< 1 second)\ncheck_health() {\n    local timeout=1\n\n    # Check process is running\n    if ! pgrep -f \"myapp\" > /dev/null; then\n        echo \"Process not running\" >&2\n        return 1\n    fi\n\n    # Check HTTP endpoint\n    if ! timeout \"$timeout\" curl -sf http://localhost:8080/health > /dev/null; then\n        echo \"Health endpoint failed\" >&2\n        return 1\n    fi\n\n    # Check critical files\n    if [[ ! -f /app/ready ]]; then\n        echo \"Not ready\" >&2\n        return 1\n    fi\n\n    return 0\n}\n\ncheck_health\n```\n\n## CI/CD Pipeline Patterns\n\n### GitHub Actions Helper\n\n```bash\n#!/usr/bin/env bash\n# ci-helper.sh - GitHub Actions utilities\n\nset -euo pipefail\n\n# Detect GitHub Actions\nis_github_actions() {\n    [[ \"${GITHUB_ACTIONS:-false}\" == \"true\" ]]\n}\n\n# GitHub Actions output\ngh_output() {\n    local name=\"$1\"\n    local value=\"$2\"\n\n    if is_github_actions; then\n        echo \"${name}=${value}\" >> \"$GITHUB_OUTPUT\"\n    fi\n}\n\n# GitHub Actions annotations\ngh_error() {\n    if is_github_actions; then\n        echo \"::error::$*\"\n    else\n        echo \"ERROR: $*\" >&2\n    fi\n}\n\ngh_warning() {\n    if is_github_actions; then\n        echo \"::warning::$*\"\n    else\n        echo \"WARN: $*\" >&2\n    fi\n}\n\ngh_notice() {\n    if is_github_actions; then\n        echo \"::notice::$*\"\n    else\n        echo \"INFO: $*\"\n    fi\n}\n\n# Set job summary\ngh_summary() {\n    if is_github_actions; then\n        echo \"$*\" >> \"$GITHUB_STEP_SUMMARY\"\n    fi\n}\n\n# Usage example\ngh_notice \"Starting build\"\nbuild_result=$(make build 2>&1)\ngh_output \"build_result\" \"$build_result\"\ngh_summary \"## Build Complete\\n\\nStatus: Success\"\n```\n\n### Azure DevOps Helper\n\n```bash\n#!/usr/bin/env bash\n# azdo-helper.sh - Azure DevOps utilities\n\nset -euo pipefail\n\n# Detect Azure DevOps\nis_azure_devops() {\n    [[ -n \"${TF_BUILD:-}\" ]]\n}\n\n# Azure DevOps output variable\nazdo_output() {\n    local name=\"$1\"\n    local value=\"$2\"\n\n    if is_azure_devops; then\n        echo \"##vso[task.setvariable variable=$name]$value\"\n    fi\n}\n\n# Azure DevOps logging\nazdo_error() {\n    if is_azure_devops; then\n        echo \"##vso[task.logissue type=error]$*\"\n    else\n        echo \"ERROR: $*\" >&2\n    fi\n}\n\nazdo_warning() {\n    if is_azure_devops; then\n        echo \"##vso[task.logissue type=warning]$*\"\n    else\n        echo \"WARN: $*\" >&2\n    fi\n}\n\n# Section grouping\nazdo_section_start() {\n    is_azure_devops && echo \"##[section]$*\"\n}\n\nazdo_section_end() {\n    is_azure_devops && echo \"##[endsection]\"\n}\n\n# Usage\nazdo_section_start \"Running Tests\"\ntest_result=$(npm test)\nazdo_output \"test_result\" \"$test_result\"\nazdo_section_end\n```\n\n### Multi-Platform CI Detection\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Detect CI environment\ndetect_ci() {\n    if [[ \"${GITHUB_ACTIONS:-false}\" == \"true\" ]]; then\n        echo \"github\"\n    elif [[ -n \"${TF_BUILD:-}\" ]]; then\n        echo \"azuredevops\"\n    elif [[ -n \"${GITLAB_CI:-}\" ]]; then\n        echo \"gitlab\"\n    elif [[ -n \"${CIRCLECI:-}\" ]]; then\n        echo \"circleci\"\n    elif [[ -n \"${JENKINS_URL:-}\" ]]; then\n        echo \"jenkins\"\n    else\n        echo \"local\"\n    fi\n}\n\n# Universal output\nci_output() {\n    local name=\"$1\"\n    local value=\"$2\"\n    local ci_env\n    ci_env=$(detect_ci)\n\n    case \"$ci_env\" in\n        github)\n            echo \"${name}=${value}\" >> \"$GITHUB_OUTPUT\"\n            ;;\n        azuredevops)\n            echo \"##vso[task.setvariable variable=$name]$value\"\n            ;;\n        gitlab)\n            echo \"${name}=${value}\" >> ci_output.env\n            ;;\n        *)\n            echo \"export ${name}=\\\"${value}\\\"\"\n            ;;\n    esac\n}\n\n# Universal error\nci_error() {\n    local ci_env\n    ci_env=$(detect_ci)\n\n    case \"$ci_env\" in\n        github)\n            echo \"::error::$*\"\n            ;;\n        azuredevops)\n            echo \"##vso[task.logissue type=error]$*\"\n            ;;\n        *)\n            echo \"ERROR: $*\" >&2\n            ;;\n    esac\n}\n```\n\n## Cloud Provider Patterns\n\n### AWS Helper Functions\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Check AWS CLI availability\nrequire_aws() {\n    if ! command -v aws &> /dev/null; then\n        echo \"Error: AWS CLI not installed\" >&2\n        exit 1\n    fi\n}\n\n# Get AWS account ID\nget_aws_account_id() {\n    aws sts get-caller-identity --query Account --output text\n}\n\n# Get secret from AWS Secrets Manager\nget_aws_secret() {\n    local secret_name=\"$1\"\n\n    aws secretsmanager get-secret-value \\\n        --secret-id \"$secret_name\" \\\n        --query SecretString \\\n        --output text\n}\n\n# Upload to S3 with retry\ns3_upload_retry() {\n    local file=\"$1\"\n    local s3_path=\"$2\"\n    local max_attempts=3\n    local attempt=1\n\n    while ((attempt <= max_attempts)); do\n        if aws s3 cp \"$file\" \"$s3_path\"; then\n            return 0\n        fi\n\n        echo \"Upload failed (attempt $attempt/$max_attempts)\" >&2\n        ((attempt++))\n        sleep $((attempt * 2))\n    done\n\n    return 1\n}\n\n# Assume IAM role\nassume_role() {\n    local role_arn=\"$1\"\n    local session_name=\"${2:-bash-script}\"\n\n    local credentials\n    credentials=$(aws sts assume-role \\\n        --role-arn \"$role_arn\" \\\n        --role-session-name \"$session_name\" \\\n        --query Credentials \\\n        --output json)\n\n    export AWS_ACCESS_KEY_ID=$(echo \"$credentials\" | jq -r .AccessKeyId)\n    export AWS_SECRET_ACCESS_KEY=$(echo \"$credentials\" | jq -r .SecretAccessKey)\n    export AWS_SESSION_TOKEN=$(echo \"$credentials\" | jq -r .SessionToken)\n}\n```\n\n### Azure Helper Functions\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Check Azure CLI\nrequire_az() {\n    if ! command -v az &> /dev/null; then\n        echo \"Error: Azure CLI not installed\" >&2\n        exit 1\n    fi\n}\n\n# Get Azure subscription ID\nget_az_subscription_id() {\n    az account show --query id --output tsv\n}\n\n# Get secret from Azure Key Vault\nget_keyvault_secret() {\n    local vault_name=\"$1\"\n    local secret_name=\"$2\"\n\n    az keyvault secret show \\\n        --vault-name \"$vault_name\" \\\n        --name \"$secret_name\" \\\n        --query value \\\n        --output tsv\n}\n\n# Upload to Azure Blob Storage\naz_blob_upload() {\n    local file=\"$1\"\n    local container=\"$2\"\n    local blob_name=\"${3:-$(basename \"$file\")}\"\n\n    az storage blob upload \\\n        --file \"$file\" \\\n        --container-name \"$container\" \\\n        --name \"$blob_name\" \\\n        --overwrite\n}\n\n# Get managed identity token\nget_managed_identity_token() {\n    local resource=\"${1:-https://management.azure.com/}\"\n\n    curl -sf -H Metadata:true \\\n        \"http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=$resource\" \\\n        | jq -r .access_token\n}\n```\n\n## Parallel Processing Patterns\n\n### GNU Parallel\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Check for GNU parallel\nif command -v parallel &> /dev/null; then\n    # Process files in parallel\n    export -f process_file\n    find data/ -name \"*.json\" | parallel -j 4 process_file {}\nelse\n    # Fallback to bash background jobs\n    max_jobs=4\n    job_count=0\n\n    for file in data/*.json; do\n        process_file \"$file\" &\n\n        ((job_count++))\n        if ((job_count >= max_jobs)); then\n            wait -n\n            ((job_count--))\n        fi\n    done\n\n    wait  # Wait for remaining jobs\nfi\n```\n\n### Job Pool Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Job pool implementation\njob_pool_init() {\n    readonly MAX_JOBS=\"${1:-4}\"\n    JOB_POOL_PIDS=()\n}\n\njob_pool_run() {\n    # Wait if pool is full\n    while ((${#JOB_POOL_PIDS[@]} >= MAX_JOBS)); do\n        job_pool_wait_any\n    done\n\n    # Start new job\n    \"$@\" &\n    JOB_POOL_PIDS+=($!)\n}\n\njob_pool_wait_any() {\n    # Wait for any job to finish\n    if ((${#JOB_POOL_PIDS[@]} > 0)); then\n        local pid\n        wait -n\n        # Remove finished PIDs\n        for i in \"${!JOB_POOL_PIDS[@]}\"; do\n            if ! kill -0 \"${JOB_POOL_PIDS[$i]}\" 2>/dev/null; then\n                unset 'JOB_POOL_PIDS[$i]'\n            fi\n        done\n        JOB_POOL_PIDS=(\"${JOB_POOL_PIDS[@]}\")  # Reindex\n    fi\n}\n\njob_pool_wait_all() {\n    wait\n    JOB_POOL_PIDS=()\n}\n\n# Usage\njob_pool_init 4\nfor file in data/*.txt; do\n    job_pool_run process_file \"$file\"\ndone\njob_pool_wait_all\n```\n\n## Deployment Patterns\n\n### Blue-Green Deployment\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Blue-green deployment helper\ndeploy_blue_green() {\n    local new_version=\"$1\"\n    local health_check_url=\"$2\"\n\n    echo \"Starting blue-green deployment: $new_version\"\n\n    # Deploy to green (inactive) environment\n    echo \"Deploying to green environment...\"\n    deploy_to_environment \"green\" \"$new_version\"\n\n    # Health check\n    echo \"Running health checks...\"\n    if ! check_health \"$health_check_url\"; then\n        echo \"Health check failed, rolling back\" >&2\n        return 1\n    fi\n\n    # Switch traffic to green\n    echo \"Switching traffic to green...\"\n    switch_traffic \"green\"\n\n    # Keep blue as backup for rollback\n    echo \"Deployment complete. Blue environment kept for rollback.\"\n}\n\ncheck_health() {\n    local url=\"$1\"\n    local max_attempts=30\n    local attempt=1\n\n    while ((attempt <= max_attempts)); do\n        if curl -sf \"$url\" > /dev/null; then\n            return 0\n        fi\n\n        echo \"Health check attempt $attempt/$max_attempts failed\"\n        sleep 2\n        ((attempt++))\n    done\n\n    return 1\n}\n```\n\n### Canary Deployment\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Canary deployment with gradual rollout\ndeploy_canary() {\n    local new_version=\"$1\"\n    local stages=(5 10 25 50 100)  # Percentage stages\n\n    echo \"Starting canary deployment: $new_version\"\n\n    for percentage in \"${stages[@]}\"; do\n        echo \"Rolling out to $percentage% of traffic...\"\n\n        # Update traffic split\n        update_traffic_split \"$percentage\" \"$new_version\"\n\n        # Monitor for issues\n        echo \"Monitoring for 5 minutes...\"\n        if ! monitor_metrics 300; then\n            echo \"Issues detected, rolling back\" >&2\n            update_traffic_split 0 \"$new_version\"\n            return 1\n        fi\n\n        echo \"Stage $percentage% successful\"\n    done\n\n    echo \"Canary deployment complete\"\n}\n\nmonitor_metrics() {\n    local duration=\"$1\"\n    local end_time=$((SECONDS + duration))\n\n    while ((SECONDS < end_time)); do\n        # Check error rate\n        local error_rate\n        error_rate=$(get_error_rate)\n\n        if ((error_rate > 5)); then\n            echo \"Error rate too high: $error_rate%\" >&2\n            return 1\n        fi\n\n        sleep 10\n    done\n\n    return 0\n}\n```\n\n## Logging and Monitoring\n\n### Structured Logging\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# JSON structured logging\nlog_json() {\n    local level=\"$1\"\n    local message=\"$2\"\n    shift 2\n\n    local timestamp\n    timestamp=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\n    # Build JSON\n    local json\n    json=$(jq -n \\\n        --arg timestamp \"$timestamp\" \\\n        --arg level \"$level\" \\\n        --arg message \"$message\" \\\n        --arg script \"$SCRIPT_NAME\" \\\n        --argjson context \"$(echo \"$@\" | jq -Rs .)\" \\\n        '{\n            timestamp: $timestamp,\n            level: $level,\n            message: $message,\n            script: $script,\n            context: $context\n        }')\n\n    echo \"$json\" >&2\n}\n\nlog_info() { log_json \"INFO\" \"$@\"; }\nlog_error() { log_json \"ERROR\" \"$@\"; }\nlog_warn() { log_json \"WARN\" \"$@\"; }\n\n# Usage\nlog_info \"Processing file\" \"file=/data/input.txt\" \"size=1024\"\n```\n\n## Resources\n\n- [12-Factor App Methodology](https://12factor.net/)\n- [Container Best Practices](https://cloud.google.com/architecture/best-practices-for-building-containers)\n- [CI/CD Best Practices](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment)\n\n---\n\n**Modern automation requires cloud-native patterns, container awareness, and robust CI/CD integration. These patterns ensure production-ready deployments in 2025.**"
              },
              {
                "name": "parallel-processing-patterns",
                "description": "Parallel and concurrent processing patterns in bash including GNU Parallel, xargs, job pools, and async patterns (2025)",
                "path": "plugins/bash-master/skills/parallel-processing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "parallel-processing-patterns",
                  "description": "Parallel and concurrent processing patterns in bash including GNU Parallel, xargs, job pools, and async patterns (2025)"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Parallel Processing Patterns in Bash (2025)\n\n## Overview\n\nComprehensive guide to parallel and concurrent execution in bash, covering GNU Parallel, xargs parallelization, job control, worker pools, and modern async patterns for maximum performance.\n\n## GNU Parallel (Recommended)\n\n### Installation\n\n```bash\n# Debian/Ubuntu\nsudo apt-get install parallel\n\n# macOS\nbrew install parallel\n\n# From source\nwget https://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2\ntar -xjf parallel-latest.tar.bz2\ncd parallel-*\n./configure && make && sudo make install\n```\n\n### Basic Usage\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Process multiple files in parallel\nparallel gzip ::: *.txt\n\n# Equivalent to:\n# for f in *.txt; do gzip \"$f\"; done\n# But runs in parallel!\n\n# Using find with parallel\nfind . -name \"*.jpg\" | parallel convert {} -resize 50% resized/{}\n\n# Specify number of jobs\nparallel -j 8 process_file ::: *.dat\n\n# From stdin\ncat urls.txt | parallel -j 10 wget -q\n\n# Multiple inputs\nparallel echo ::: A B C ::: 1 2 3\n# Output: A 1, A 2, A 3, B 1, B 2, B 3, C 1, C 2, C 3\n\n# Paired inputs with :::+\nparallel echo ::: A B C :::+ 1 2 3\n# Output: A 1, B 2, C 3\n```\n\n### Input Handling\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Input from file\nparallel -a input.txt process_line\n\n# Multiple input files\nparallel -a file1.txt -a file2.txt 'echo {1} {2}'\n\n# Column-based input\ncat data.tsv | parallel --colsep '\\t' 'echo Name: {1}, Value: {2}'\n\n# Named columns\ncat data.csv | parallel --header : --colsep ',' 'echo {name}: {value}'\n\n# Null-delimited for safety with special characters\nfind . -name \"*.txt\" -print0 | parallel -0 wc -l\n\n# Line-based chunking\ncat huge_file.txt | parallel --pipe -N1000 'wc -l'\n```\n\n### Replacement Strings\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# {} - Full input\nparallel echo 'Processing: {}' ::: file1.txt file2.txt\n\n# {.} - Remove extension\nparallel echo '{.}' ::: file.txt file.csv\n# Output: file, file\n\n# {/} - Basename\nparallel echo '{/}' ::: /path/to/file.txt\n# Output: file.txt\n\n# {//} - Directory path\nparallel echo '{//}' ::: /path/to/file.txt\n# Output: /path/to\n\n# {/.} - Basename without extension\nparallel echo '{/.}' ::: /path/to/file.txt\n# Output: file\n\n# {#} - Job number (1-based)\nparallel echo 'Job {#}: {}' ::: A B C\n\n# {%} - Slot number (recycled job slot)\nparallel -j 2 'echo \"Slot {%}: {}\"' ::: A B C D E\n\n# Combined\nparallel 'convert {} -resize 50% {//}/thumb_{/.}.jpg' ::: *.png\n```\n\n### Progress and Logging\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Show progress bar\nparallel --bar process_item ::: {1..100}\n\n# Progress with ETA\nparallel --progress process_item ::: {1..100}\n\n# Verbose output\nparallel --verbose gzip ::: *.txt\n\n# Log to file\nparallel --joblog jobs.log gzip ::: *.txt\n\n# Resume from where it left off (skip completed jobs)\nparallel --joblog jobs.log --resume gzip ::: *.txt\n\n# Results logging\nparallel --results results_dir 'echo {1} + {2}' ::: 1 2 3 ::: 4 5 6\n# Creates: results_dir/1/4/stdout, results_dir/1/4/stderr, etc.\n```\n\n### Resource Management\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# CPU-based parallelism (number of cores)\nparallel -j \"$(nproc)\" process_item ::: {1..1000}\n\n# Leave some cores free\nparallel -j '-2' process_item ::: {1..1000}  # nproc - 2\n\n# Percentage of cores\nparallel -j '50%' process_item ::: {1..1000}\n\n# Load-based throttling\nparallel --load 80% process_item ::: {1..1000}\n\n# Memory-based throttling\nparallel --memfree 2G process_item ::: {1..1000}\n\n# Rate limiting (max jobs per second)\nparallel -j 4 --delay 0.5 wget ::: url1 url2 url3 url4\n\n# Timeout per job\nparallel --timeout 60 long_process ::: {1..100}\n\n# Retry failed jobs\nparallel --retries 3 flaky_process ::: {1..100}\n```\n\n### Distributed Execution\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Run on multiple servers\nparallel --sshloginfile servers.txt process_item ::: {1..1000}\n\n# servers.txt format:\n# 4/server1.example.com  (4 jobs on server1)\n# 8/server2.example.com  (8 jobs on server2)\n# :                       (local machine)\n\n# Transfer files before execution\nparallel --sshloginfile servers.txt --transferfile {} process {} ::: *.dat\n\n# Return results\nparallel --sshloginfile servers.txt --return {.}.result process {} ::: *.dat\n\n# Cleanup after transfer\nparallel --sshloginfile servers.txt --transfer --return {.}.out --cleanup \\\n    'process {} > {.}.out' ::: *.dat\n\n# Environment variables\nexport MY_VAR=\"value\"\nparallel --env MY_VAR --sshloginfile servers.txt 'echo $MY_VAR' ::: A B C\n```\n\n### Complex Pipelines\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Pipe mode - distribute stdin across workers\ncat huge_file.txt | parallel --pipe -N1000 'sort | uniq -c'\n\n# Block size for pipe mode\ncat data.bin | parallel --pipe --block 10M 'process_chunk'\n\n# Keep order of output\nparallel --keep-order 'sleep $((RANDOM % 3)); echo {}' ::: A B C D E\n\n# Group output (don't mix output from different jobs)\nparallel --group 'for i in 1 2 3; do echo \"Job {}: line $i\"; done' ::: A B C\n\n# Tag output with job identifier\nparallel --tag 'echo \"output from {}\"' ::: A B C\n\n# Sequence output (output as they complete, but grouped)\nparallel --ungroup 'echo \"Starting {}\"; sleep 1; echo \"Done {}\"' ::: A B C\n```\n\n## xargs Parallelization\n\n### Basic Parallel xargs\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# -P for parallel jobs\nfind . -name \"*.txt\" | xargs -P 4 -I {} gzip {}\n\n# -n for items per command\necho {1..100} | xargs -n 10 -P 4 echo \"Batch:\"\n\n# Null-delimited for safety\nfind . -name \"*.txt\" -print0 | xargs -0 -P 4 -I {} process {}\n\n# Multiple arguments per process\ncat urls.txt | xargs -P 10 -n 5 wget -q\n\n# Limit max total arguments\necho {1..1000} | xargs -P 4 --max-args=50 echo\n```\n\n### xargs with Complex Commands\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Use sh -c for complex commands\nfind . -name \"*.jpg\" -print0 | \\\n    xargs -0 -P 4 -I {} sh -c 'convert \"$1\" -resize 50% \"thumb_$(basename \"$1\")\"' _ {}\n\n# Multiple placeholders\npaste file1.txt file2.txt | \\\n    xargs -P 4 -n 2 sh -c 'diff \"$1\" \"$2\" > \"diff_$(basename \"$1\" .txt).patch\"' _\n\n# Process in batches\nfind . -name \"*.log\" -print0 | \\\n    xargs -0 -P 4 -n 100 tar -czvf logs_batch.tar.gz\n\n# With failure handling\nfind . -name \"*.dat\" -print0 | \\\n    xargs -0 -P 4 -I {} sh -c 'process \"$1\" || echo \"Failed: $1\" >> failures.log' _ {}\n```\n\n## Job Control Patterns\n\n### Background Job Management\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Track background jobs\ndeclare -a PIDS=()\n\n# Start jobs\nfor item in {1..10}; do\n    process_item \"$item\" &\n    PIDS+=($!)\ndone\n\n# Wait for all\nfor pid in \"${PIDS[@]}\"; do\n    wait \"$pid\"\ndone\n\necho \"All jobs complete\"\n\n# Or wait for any to complete\nwait -n  # Bash 4.3+\necho \"At least one job complete\"\n```\n\n### Job Pool with Semaphore\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Maximum concurrent jobs\nMAX_JOBS=4\n\n# Simple semaphore using a counter\njob_count=0\n\nrun_with_limit() {\n    local cmd=(\"$@\")\n\n    # Wait if at limit\n    while ((job_count >= MAX_JOBS)); do\n        wait -n 2>/dev/null || true\n        ((job_count--))\n    done\n\n    # Start new job\n    \"${cmd[@]}\" &\n    ((job_count++))\n}\n\n# Usage\nfor item in {1..20}; do\n    run_with_limit process_item \"$item\"\ndone\n\n# Wait for remaining\nwait\n```\n\n### FIFO-Based Job Pool\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nMAX_JOBS=4\nJOB_FIFO=\"/tmp/job_pool_$$\"\n\n# Create job slots\nmkfifo \"$JOB_FIFO\"\ntrap 'rm -f \"$JOB_FIFO\"' EXIT\n\n# Initialize slots\nexec 3<>\"$JOB_FIFO\"\nfor ((i=0; i<MAX_JOBS; i++)); do\n    echo >&3\ndone\n\n# Run with slot\nrun_with_slot() {\n    local cmd=(\"$@\")\n\n    read -u 3  # Acquire slot (blocks if none available)\n\n    {\n        \"${cmd[@]}\"\n        echo >&3  # Release slot\n    } &\n}\n\n# Usage\nfor item in {1..20}; do\n    run_with_slot process_item \"$item\"\ndone\n\nwait\nexec 3>&-\n```\n\n### Worker Pool Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nWORK_QUEUE=\"/tmp/work_queue_$$\"\nRESULT_QUEUE=\"/tmp/result_queue_$$\"\nNUM_WORKERS=4\n\nmkfifo \"$WORK_QUEUE\" \"$RESULT_QUEUE\"\ntrap 'rm -f \"$WORK_QUEUE\" \"$RESULT_QUEUE\"' EXIT\n\n# Worker function\nworker() {\n    local id=\"$1\"\n    while read -r task; do\n        [[ \"$task\" == \"STOP\" ]] && break\n\n        # Process task\n        local result\n        result=$(process_task \"$task\" 2>&1)\n        echo \"RESULT:$id:$task:$result\"\n    done\n}\n\n# Start workers\nfor ((i=0; i<NUM_WORKERS; i++)); do\n    worker \"$i\" < \"$WORK_QUEUE\" > \"$RESULT_QUEUE\" &\ndone\n\n# Result collector (background)\ncollect_results() {\n    while read -r line; do\n        [[ \"$line\" == \"DONE\" ]] && break\n        echo \"$line\" >> results.txt\n    done < \"$RESULT_QUEUE\"\n} &\nCOLLECTOR_PID=$!\n\n# Producer - send work\n{\n    for task in \"${TASKS[@]}\"; do\n        echo \"$task\"\n    done\n\n    # Stop signals for workers\n    for ((i=0; i<NUM_WORKERS; i++)); do\n        echo \"STOP\"\n    done\n} > \"$WORK_QUEUE\"\n\n# Signal end of results\nwait  # Wait for workers\necho \"DONE\" > \"$RESULT_QUEUE\"\nwait \"$COLLECTOR_PID\"\n```\n\n## Modern Async Patterns\n\n### Promise-Like Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Async function wrapper\nasync() {\n    local result_var=\"$1\"\n    shift\n    local cmd=(\"$@\")\n\n    # Create temp file for result\n    local result_file\n    result_file=$(mktemp)\n\n    # Run in background, save result\n    {\n        if \"${cmd[@]}\" > \"$result_file\" 2>&1; then\n            echo \"0\" >> \"$result_file.status\"\n        else\n            echo \"$?\" >> \"$result_file.status\"\n        fi\n    } &\n\n    # Store PID and result file location\n    eval \"${result_var}_pid=$!\"\n    eval \"${result_var}_file='$result_file'\"\n}\n\n# Await result\nawait() {\n    local result_var=\"$1\"\n    local pid_var=\"${result_var}_pid\"\n    local file_var=\"${result_var}_file\"\n\n    # Wait for completion\n    wait \"${!pid_var}\"\n\n    # Get result\n    cat \"${!file_var}\"\n    local status\n    status=$(cat \"${!file_var}.status\")\n\n    # Cleanup\n    rm -f \"${!file_var}\" \"${!file_var}.status\"\n\n    return \"$status\"\n}\n\n# Usage\nasync result1 curl -s \"https://api1.example.com/data\"\nasync result2 curl -s \"https://api2.example.com/data\"\nasync result3 process_local_data\n\n# Do other work here...\n\n# Get results (blocks until complete)\ndata1=$(await result1)\ndata2=$(await result2)\ndata3=$(await result3)\n```\n\n### Event Loop Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\ndeclare -A TASKS\ndeclare -A TASK_RESULTS\nTASK_COUNTER=0\n\n# Register async task\nschedule() {\n    local cmd=(\"$@\")\n    local task_id=$((++TASK_COUNTER))\n    local output_file=\"/tmp/task_${task_id}_$$\"\n\n    \"${cmd[@]}\" > \"$output_file\" 2>&1 &\n\n    TASKS[$task_id]=$!\n    TASK_RESULTS[$task_id]=\"$output_file\"\n\n    echo \"$task_id\"\n}\n\n# Check if task complete\nis_complete() {\n    local task_id=\"$1\"\n    ! kill -0 \"${TASKS[$task_id]}\" 2>/dev/null\n}\n\n# Get task result\nget_result() {\n    local task_id=\"$1\"\n    wait \"${TASKS[$task_id]}\" 2>/dev/null || true\n    cat \"${TASK_RESULTS[$task_id]}\"\n    rm -f \"${TASK_RESULTS[$task_id]}\"\n}\n\n# Event loop\nrun_event_loop() {\n    local pending=(\"${!TASKS[@]}\")\n\n    while ((${#pending[@]} > 0)); do\n        local still_pending=()\n\n        for task_id in \"${pending[@]}\"; do\n            if is_complete \"$task_id\"; then\n                local result\n                result=$(get_result \"$task_id\")\n                on_task_complete \"$task_id\" \"$result\"\n            else\n                still_pending+=(\"$task_id\")\n            fi\n        done\n\n        pending=(\"${still_pending[@]}\")\n\n        # Small sleep to prevent busy-waiting\n        ((${#pending[@]} > 0)) && sleep 0.1\n    done\n}\n\n# Callback for completed tasks\non_task_complete() {\n    local task_id=\"$1\"\n    local result=\"$2\"\n    echo \"Task $task_id complete: ${result:0:50}...\"\n}\n```\n\n### Fan-Out/Fan-In Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Fan-out: distribute work\nfan_out() {\n    local -n items=\"$1\"\n    local workers=\"$2\"\n    local worker_func=\"$3\"\n\n    local chunk_size=$(( (${#items[@]} + workers - 1) / workers ))\n    local pids=()\n\n    for ((i=0; i<workers; i++)); do\n        local start=$((i * chunk_size))\n        local chunk=(\"${items[@]:start:chunk_size}\")\n\n        if ((${#chunk[@]} > 0)); then\n            $worker_func \"${chunk[@]}\" &\n            pids+=($!)\n        fi\n    done\n\n    # Return PIDs for fan_in\n    echo \"${pids[*]}\"\n}\n\n# Fan-in: collect results\nfan_in() {\n    local -a pids=($1)\n    local results=()\n\n    for pid in \"${pids[@]}\"; do\n        wait \"$pid\"\n    done\n}\n\n# Example worker\nprocess_chunk() {\n    local items=(\"$@\")\n    for item in \"${items[@]}\"; do\n        echo \"Processed: $item\"\n    done\n}\n\n# Usage\ndata=({1..100})\npids=$(fan_out data 4 process_chunk)\nfan_in \"$pids\"\n```\n\n### Map-Reduce Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Map function\nparallel_map() {\n    local -n input=\"$1\"\n    local map_func=\"$2\"\n    local workers=\"${3:-$(nproc)}\"\n\n    printf '%s\\n' \"${input[@]}\" | \\\n        parallel -j \"$workers\" \"$map_func\"\n}\n\n# Reduce function\nreduce() {\n    local reduce_func=\"$1\"\n    local accumulator=\"$2\"\n\n    while IFS= read -r value; do\n        accumulator=$($reduce_func \"$accumulator\" \"$value\")\n    done\n\n    echo \"$accumulator\"\n}\n\n# Example: Sum of squares\nsquare() { echo $(($1 * $1)); }\nadd() { echo $(($1 + $2)); }\n\nnumbers=({1..100})\nsum_of_squares=$(\n    parallel_map numbers square 4 | reduce add 0\n)\necho \"Sum of squares: $sum_of_squares\"\n\n# Word count example\nword_count_map() {\n    tr ' ' '\\n' | sort | uniq -c\n}\n\nword_count_reduce() {\n    sort -k2 | awk '{\n        if ($2 == prev) { count += $1 }\n        else { if (prev) print count, prev; count = $1; prev = $2 }\n    } END { if (prev) print count, prev }'\n}\n\ncat large_text.txt | \\\n    parallel --pipe -N1000 word_count_map | \\\n    word_count_reduce\n```\n\n## Performance Optimization\n\n### Batch Processing\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Process in optimal batch sizes\noptimal_batch_process() {\n    local items=(\"$@\")\n    local batch_size=100\n    local workers=$(nproc)\n\n    printf '%s\\n' \"${items[@]}\" | \\\n        parallel --pipe -N\"$batch_size\" -j\"$workers\" '\n            while IFS= read -r item; do\n                process_item \"$item\"\n            done\n        '\n}\n\n# Dynamic batch sizing based on memory\ndynamic_batch() {\n    local mem_available\n    mem_available=$(free -m | awk '/^Mem:/ {print $7}')\n\n    # Adjust batch size based on available memory\n    local batch_size=$((mem_available / 100))  # 100MB per batch\n    ((batch_size < 10)) && batch_size=10\n    ((batch_size > 1000)) && batch_size=1000\n\n    parallel --pipe -N\"$batch_size\" process_batch\n}\n```\n\n### I/O Optimization\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Use tmpfs for intermediate files\nsetup_fast_temp() {\n    local tmpdir=\"/dev/shm/parallel_$$\"\n    mkdir -p \"$tmpdir\"\n    trap 'rm -rf \"$tmpdir\"' EXIT\n    echo \"$tmpdir\"\n}\n\n# Buffer I/O operations\nbuffered_parallel() {\n    local input=\"$1\"\n    local tmpdir\n    tmpdir=$(setup_fast_temp)\n\n    # Split input into chunks\n    split -l 1000 \"$input\" \"$tmpdir/chunk_\"\n\n    # Process chunks in parallel\n    parallel process_chunk {} ::: \"$tmpdir\"/chunk_*\n\n    # Combine results\n    cat \"$tmpdir\"/result_* > output.txt\n}\n\n# Avoid disk I/O with process substitution\nno_disk_parallel() {\n    # Instead of:\n    #   command > temp.txt\n    #   parallel process ::: temp.txt\n    #   rm temp.txt\n\n    # Do this:\n    command | parallel --pipe process\n}\n```\n\n### CPU Affinity\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Pin workers to specific CPUs\ncpu_pinned_parallel() {\n    local num_cpus\n    num_cpus=$(nproc)\n\n    for ((cpu=0; cpu<num_cpus; cpu++)); do\n        taskset -c \"$cpu\" process_worker \"$cpu\" &\n    done\n\n    wait\n}\n\n# NUMA-aware processing\nnuma_parallel() {\n    local num_nodes\n    num_nodes=$(numactl --hardware | grep \"available:\" | awk '{print $2}')\n\n    for ((node=0; node<num_nodes; node++)); do\n        numactl --cpunodebind=\"$node\" --membind=\"$node\" \\\n            process_chunk \"$node\" &\n    done\n\n    wait\n}\n```\n\n## Error Handling\n\n### Graceful Failure Handling\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Track failures\ndeclare -A FAILURES\n\nparallel_with_retry() {\n    local max_retries=3\n    local items=(\"$@\")\n\n    for item in \"${items[@]}\"; do\n        local retries=0\n        local success=false\n\n        while ((retries < max_retries)) && ! $success; do\n            if process_item \"$item\"; then\n                success=true\n            else\n                ((retries++))\n                echo \"Retry $retries for $item\" >&2\n                sleep $((retries * 2))  # Exponential backoff\n            fi\n        done\n\n        if ! $success; then\n            FAILURES[\"$item\"]=\"Failed after $max_retries retries\"\n        fi\n    done &\n\n    wait\n}\n\n# Report failures\nreport_failures() {\n    if ((${#FAILURES[@]} > 0)); then\n        echo \"Failures:\" >&2\n        for item in \"${!FAILURES[@]}\"; do\n            echo \"  $item: ${FAILURES[$item]}\" >&2\n        done\n        return 1\n    fi\n}\n```\n\n### Cancellation Support\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Global cancellation flag\nCANCELLED=false\ndeclare -a WORKER_PIDS=()\n\ncancel_all() {\n    CANCELLED=true\n    for pid in \"${WORKER_PIDS[@]}\"; do\n        kill \"$pid\" 2>/dev/null || true\n    done\n}\n\ntrap cancel_all SIGINT SIGTERM\n\ncancellable_worker() {\n    local id=\"$1\"\n    while ! $CANCELLED; do\n        # Check for work\n        if work=$(get_next_work); then\n            process_work \"$work\"\n        else\n            sleep 0.1\n        fi\n    done\n}\n\n# Start workers\nfor ((i=0; i<NUM_WORKERS; i++)); do\n    cancellable_worker \"$i\" &\n    WORKER_PIDS+=($!)\ndone\n\n# Wait with interrupt support\nwait || true\n```\n\n## Resources\n\n- [GNU Parallel Tutorial](https://www.gnu.org/software/parallel/parallel_tutorial.html)\n- [GNU Parallel Manual](https://www.gnu.org/software/parallel/man.html)\n- [Bash Job Control](https://www.gnu.org/software/bash/manual/html_node/Job-Control.html)\n- [Advanced Bash-Scripting Guide - Process Substitution](https://tldp.org/LDP/abs/html/process-sub.html)\n\n---\n\n**Master parallel processing for efficient multi-core utilization and faster script execution.**"
              },
              {
                "name": "process-substitution-fifos",
                "description": "Process substitution, named pipes (FIFOs), and advanced IPC patterns for efficient bash data streaming (2025)",
                "path": "plugins/bash-master/skills/process-substitution-fifos/SKILL.md",
                "frontmatter": {
                  "name": "process-substitution-fifos",
                  "description": "Process substitution, named pipes (FIFOs), and advanced IPC patterns for efficient bash data streaming (2025)"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Process Substitution & FIFOs (2025)\n\n## Overview\n\nMaster advanced inter-process communication patterns in bash using process substitution, named pipes (FIFOs), and efficient data streaming techniques. These patterns enable powerful data pipelines without temporary files.\n\n## Process Substitution Basics\n\n### Input Process Substitution `<(command)`\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Compare two command outputs\ndiff <(sort file1.txt) <(sort file2.txt)\n\n# Compare remote and local files\ndiff <(ssh server 'cat /etc/config') /etc/config\n\n# Merge sorted files\nsort -m <(sort file1.txt) <(sort file2.txt) <(sort file3.txt)\n\n# Read from multiple sources simultaneously\npaste <(cut -f1 data.tsv) <(cut -f3 data.tsv)\n\n# Feed command output to programs expecting files\n# Many programs require filename arguments, not stdin\nwc -l <(grep \"error\" *.log)\n\n# Process API response with tool expecting file\njq '.items[]' <(curl -s \"https://api.example.com/data\")\n\n# Source environment from command output\nsource <(aws configure export-credentials --format env)\n\n# Feed to while loop without subshell issues\nwhile IFS= read -r line; do\n    ((count++))\n    process \"$line\"\ndone < <(find . -name \"*.txt\")\necho \"Processed $count files\"  # Variable survives!\n```\n\n### Output Process Substitution `>(command)`\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Write to multiple destinations simultaneously (tee alternative)\necho \"Log message\" | tee >(logger -t myapp) >(mail -s \"Alert\" admin@example.com)\n\n# Compress and checksum in one pass\ntar cf - /data | tee >(gzip > backup.tar.gz) >(sha256sum > backup.sha256)\n\n# Send output to multiple processors\ngenerate_data | tee >(processor1 > result1.txt) >(processor2 > result2.txt) > /dev/null\n\n# Log and process simultaneously\n./build.sh 2>&1 | tee >(grep -i error > errors.log) >(grep -i warning > warnings.log)\n\n# Real-time filtering with multiple outputs\ntail -f /var/log/syslog | tee \\\n    >(grep --line-buffered \"ERROR\" >> errors.log) \\\n    >(grep --line-buffered \"WARNING\" >> warnings.log) \\\n    >(grep --line-buffered \"CRITICAL\" | mail -s \"Critical Alert\" admin@example.com)\n```\n\n### Combining Input and Output Substitution\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Transform and compare\ndiff <(sort input.txt | uniq) <(sort reference.txt | uniq)\n\n# Pipeline with multiple branches\ncat data.csv | tee \\\n    >(awk -F, '{print $1}' > column1.txt) \\\n    >(awk -F, '{print $2}' > column2.txt) \\\n    | wc -l\n\n# Complex data flow\nprocess_data() {\n    local input=\"$1\"\n\n    # Read from process substitution, write to multiple outputs\n    while IFS= read -r line; do\n        echo \"$line\" | tee \\\n            >(echo \"LOG: $line\" >> \"$log_file\") \\\n            >(process_line \"$line\" >> results.txt)\n    done < <(cat \"$input\" | filter_input)\n}\n```\n\n## Named Pipes (FIFOs)\n\n### Creating and Using FIFOs\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Create FIFO\nmkfifo my_pipe\n\n# Clean up on exit\ntrap 'rm -f my_pipe' EXIT\n\n# Writer (in background or separate terminal)\necho \"Hello from writer\" > my_pipe &\n\n# Reader (blocks until data available)\ncat < my_pipe\n\n# With timeout (using read)\nif read -t 5 line < my_pipe; then\n    echo \"Received: $line\"\nelse\n    echo \"Timeout waiting for data\"\nfi\n```\n\n### Bidirectional Communication\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Create two FIFOs for bidirectional communication\nREQUEST_PIPE=\"/tmp/request_$$\"\nRESPONSE_PIPE=\"/tmp/response_$$\"\n\nmkfifo \"$REQUEST_PIPE\" \"$RESPONSE_PIPE\"\ntrap 'rm -f \"$REQUEST_PIPE\" \"$RESPONSE_PIPE\"' EXIT\n\n# Server process\nserver() {\n    while true; do\n        if read -r request < \"$REQUEST_PIPE\"; then\n            case \"$request\" in\n                \"QUIT\")\n                    echo \"BYE\" > \"$RESPONSE_PIPE\"\n                    break\n                    ;;\n                \"TIME\")\n                    date > \"$RESPONSE_PIPE\"\n                    ;;\n                \"UPTIME\")\n                    uptime > \"$RESPONSE_PIPE\"\n                    ;;\n                *)\n                    echo \"UNKNOWN: $request\" > \"$RESPONSE_PIPE\"\n                    ;;\n            esac\n        fi\n    done\n}\n\n# Client function\nsend_request() {\n    local request=\"$1\"\n    echo \"$request\" > \"$REQUEST_PIPE\"\n    cat < \"$RESPONSE_PIPE\"\n}\n\n# Start server in background\nserver &\nSERVER_PID=$!\n\n# Send requests\nsend_request \"TIME\"\nsend_request \"UPTIME\"\nsend_request \"QUIT\"\n\nwait \"$SERVER_PID\"\n```\n\n### Producer-Consumer Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nWORK_QUEUE=\"/tmp/work_queue_$$\"\nmkfifo \"$WORK_QUEUE\"\ntrap 'rm -f \"$WORK_QUEUE\"' EXIT\n\n# Producer\nproducer() {\n    local item\n    for item in {1..100}; do\n        echo \"TASK:$item\"\n    done\n    echo \"DONE\"\n}\n\n# Consumer (can have multiple)\nconsumer() {\n    local id=\"$1\"\n    while read -r item; do\n        [[ \"$item\" == \"DONE\" ]] && break\n        echo \"Consumer $id processing: $item\"\n        sleep 0.1  # Simulate work\n    done\n}\n\n# Start consumers (they'll block waiting for data)\nconsumer 1 < \"$WORK_QUEUE\" &\nconsumer 2 < \"$WORK_QUEUE\" &\nconsumer 3 < \"$WORK_QUEUE\" &\n\n# Start producer\nproducer > \"$WORK_QUEUE\"\n\nwait\necho \"All work complete\"\n```\n\n### FIFO with File Descriptors\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nFIFO=\"/tmp/fd_fifo_$$\"\nmkfifo \"$FIFO\"\ntrap 'rm -f \"$FIFO\"' EXIT\n\n# Open FIFO for read/write on FD 3\n# Opening for both prevents blocking on open\nexec 3<>\"$FIFO\"\n\n# Write to FIFO via FD\necho \"Message 1\" >&3\necho \"Message 2\" >&3\n\n# Read from FIFO via FD\nread -r msg1 <&3\nread -r msg2 <&3\necho \"Got: $msg1, $msg2\"\n\n# Close FD\nexec 3>&-\n```\n\n## Coprocess (Bash 4+)\n\n### Basic Coprocess Usage\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Start coprocess (bidirectional pipe)\ncoproc BC { bc -l; }\n\n# Send data to coprocess\necho \"scale=10; 355/113\" >&\"${BC[1]}\"\n\n# Read result\nread -r result <&\"${BC[0]}\"\necho \"Pi approximation: $result\"\n\n# More calculations\necho \"sqrt(2)\" >&\"${BC[1]}\"\nread -r sqrt2 <&\"${BC[0]}\"\necho \"Square root of 2: $sqrt2\"\n\n# Close write end to signal EOF\nexec {BC[1]}>&-\n\n# Wait for coprocess to finish\nwait \"$BC_PID\"\n```\n\n### Named Coprocess\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Named coprocess for Python interpreter\ncoproc PYTHON { python3 -u -c \"\nimport sys\nfor line in sys.stdin:\n    exec(line.strip())\n\"; }\n\n# Send Python commands\necho \"print('Hello from Python')\" >&\"${PYTHON[1]}\"\nread -r output <&\"${PYTHON[0]}\"\necho \"Python said: $output\"\n\necho \"print(2**100)\" >&\"${PYTHON[1]}\"\nread -r big_num <&\"${PYTHON[0]}\"\necho \"2^100 = $big_num\"\n\n# Cleanup\nexec {PYTHON[1]}>&-\nwait \"$PYTHON_PID\" 2>/dev/null || true\n```\n\n### Coprocess Pool Pattern\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Create pool of worker coprocesses\ndeclare -A WORKERS\ndeclare -A WORKER_PIDS\n\nstart_workers() {\n    local count=\"$1\"\n    local i\n\n    for ((i=0; i<count; i++)); do\n        # Each worker runs a processing loop\n        coproc \"WORKER_$i\" {\n            while IFS= read -r task; do\n                [[ \"$task\" == \"QUIT\" ]] && exit 0\n                # Simulate work\n                sleep 0.1\n                echo \"DONE:$task\"\n            done\n        }\n\n        # Store FDs dynamically\n        local -n write_fd=\"WORKER_${i}[1]\"\n        local -n read_fd=\"WORKER_${i}[0]\"\n        local -n pid=\"WORKER_${i}_PID\"\n\n        WORKERS[\"$i,in\"]=\"$write_fd\"\n        WORKERS[\"$i,out\"]=\"$read_fd\"\n        WORKER_PIDS[\"$i\"]=\"$pid\"\n    done\n}\n\n# Note: Coprocess pool management is complex\n# Consider GNU Parallel for production workloads\n```\n\n## Advanced Patterns\n\n### Progress Monitoring with FIFO\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nPROGRESS_PIPE=\"/tmp/progress_$$\"\nmkfifo \"$PROGRESS_PIPE\"\ntrap 'rm -f \"$PROGRESS_PIPE\"' EXIT\n\n# Progress monitor\nmonitor_progress() {\n    local total=\"$1\"\n    local current=0\n\n    while read -r update; do\n        ((current++))\n        local pct=$((current * 100 / total))\n        printf \"\\rProgress: [%-50s] %d%%\" \\\n            \"$(printf '#%.0s' $(seq 1 $((pct/2))))\" \"$pct\"\n    done < \"$PROGRESS_PIPE\"\n    echo\n}\n\n# Worker that reports progress\ndo_work() {\n    local items=(\"$@\")\n    local item\n\n    for item in \"${items[@]}\"; do\n        process_item \"$item\"\n        echo \"done\" > \"$PROGRESS_PIPE\"\n    done\n}\n\n# Usage\nitems=(item1 item2 item3 ... item100)\nmonitor_progress \"${#items[@]}\" &\nMONITOR_PID=$!\n\ndo_work \"${items[@]}\"\n\nexec 3>\"$PROGRESS_PIPE\"  # Keep pipe open\nexec 3>&-                 # Close to signal completion\nwait \"$MONITOR_PID\"\n```\n\n### Log Aggregator with Multiple FIFOs\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nLOG_DIR=\"/tmp/logs_$$\"\nmkdir -p \"$LOG_DIR\"\n\n# Create FIFOs for each log level\nfor level in DEBUG INFO WARN ERROR; do\n    mkfifo \"$LOG_DIR/$level\"\ndone\n\ntrap 'rm -rf \"$LOG_DIR\"' EXIT\n\n# Aggregator process\naggregate_logs() {\n    local output_file=\"$1\"\n\n    # Open all FIFOs for reading\n    exec 3<\"$LOG_DIR/DEBUG\"\n    exec 4<\"$LOG_DIR/INFO\"\n    exec 5<\"$LOG_DIR/WARN\"\n    exec 6<\"$LOG_DIR/ERROR\"\n\n    while true; do\n        # Use select-like behavior with read timeout\n        read -t 0.1 -r msg <&3 && echo \"[DEBUG] $(date '+%H:%M:%S') $msg\" >> \"$output_file\"\n        read -t 0.1 -r msg <&4 && echo \"[INFO]  $(date '+%H:%M:%S') $msg\" >> \"$output_file\"\n        read -t 0.1 -r msg <&5 && echo \"[WARN]  $(date '+%H:%M:%S') $msg\" >> \"$output_file\"\n        read -t 0.1 -r msg <&6 && echo \"[ERROR] $(date '+%H:%M:%S') $msg\" >> \"$output_file\"\n    done\n}\n\n# Logging functions\nlog_debug() { echo \"$*\" > \"$LOG_DIR/DEBUG\"; }\nlog_info()  { echo \"$*\" > \"$LOG_DIR/INFO\"; }\nlog_warn()  { echo \"$*\" > \"$LOG_DIR/WARN\"; }\nlog_error() { echo \"$*\" > \"$LOG_DIR/ERROR\"; }\n\n# Start aggregator\naggregate_logs \"/var/log/app.log\" &\nAGGREGATOR_PID=$!\n\n# Application code uses logging functions\nlog_info \"Application started\"\nlog_debug \"Processing item\"\nlog_warn \"Resource running low\"\nlog_error \"Critical failure\"\n\n# Cleanup\nkill \"$AGGREGATOR_PID\" 2>/dev/null\n```\n\n### Data Pipeline with Buffering\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Buffered pipeline stage\nbuffered_stage() {\n    local name=\"$1\"\n    local buffer_size=\"${2:-100}\"\n    local buffer=()\n\n    while IFS= read -r line || [[ ${#buffer[@]} -gt 0 ]]; do\n        if [[ -n \"$line\" ]]; then\n            buffer+=(\"$line\")\n        fi\n\n        # Flush when buffer full or EOF\n        if [[ ${#buffer[@]} -ge $buffer_size ]] || [[ -z \"$line\" && ${#buffer[@]} -gt 0 ]]; then\n            printf '%s\\n' \"${buffer[@]}\" | process_batch\n            buffer=()\n        fi\n    done\n}\n\n# Parallel pipeline with process substitution\nrun_parallel_pipeline() {\n    local input=\"$1\"\n\n    cat \"$input\" | \\\n        tee >(filter_a | transform_a > output_a.txt) \\\n            >(filter_b | transform_b > output_b.txt) \\\n            >(filter_c | transform_c > output_c.txt) \\\n        > /dev/null\n\n    # Wait for all background processes\n    wait\n}\n```\n\n### Streaming JSON Processing\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Stream JSON array elements\nstream_json_array() {\n    local url=\"$1\"\n\n    # Use jq to stream array elements one per line\n    curl -s \"$url\" | jq -c '.items[]' | while IFS= read -r item; do\n        process_json_item \"$item\"\n    done\n}\n\n# Parallel JSON processing with process substitution\nparallel_json_process() {\n    local input=\"$1\"\n    local workers=4\n\n    # Split input across workers\n    jq -c '.[]' \"$input\" | \\\n        parallel --pipe -N100 --jobs \"$workers\" '\n            while IFS= read -r item; do\n                echo \"$item\" | jq \".processed = true\"\n            done\n        ' | jq -s '.'\n}\n\n# Transform JSON stream\ntransform_json_stream() {\n    jq -c '.' | while IFS= read -r obj; do\n        # Process with bash\n        local id\n        id=$(echo \"$obj\" | jq -r '.id')\n\n        # Enrich and output\n        echo \"$obj\" | jq --arg ts \"$(date -Iseconds)\" '. + {timestamp: $ts}'\n    done\n}\n```\n\n## Bash 5.3 In-Shell Substitution\n\n### No-Fork Command Substitution\n\n```bash\n#!/usr/bin/env bash\n# Requires Bash 5.3+\nset -euo pipefail\n\n# Traditional: forks subshell\nresult=$(echo \"hello\")\n\n# Bash 5.3: No fork, runs in current shell\nresult=${ echo \"hello\"; }\n\n# Significant for variable modifications\ncounter=0\n# Traditional - counter stays 0 (subshell)\nresult=$(counter=$((counter + 1)); echo \"$counter\")\necho \"Counter: $counter\"  # Still 0\n\n# Bash 5.3 - counter is modified (same shell)\nresult=${ counter=$((counter + 1)); echo \"$counter\"; }\necho \"Counter: $counter\"  # Now 1\n\n# REPLY variable syntax (even more concise)\n${ REPLY=\"computed value\"; }\necho \"$REPLY\"\n\n# Or using ${| } syntax\n${| REPLY=$(expensive_computation); }\necho \"Result: $REPLY\"\n```\n\n### Performance-Critical Pipelines\n\n```bash\n#!/usr/bin/env bash\n# Requires Bash 5.3+\nset -euo pipefail\n\n# Build result without forks\nbuild_path() {\n    local parts=(\"$@\")\n    local result=\"\"\n\n    for part in \"${parts[@]}\"; do\n        # No fork for each concatenation\n        result=${ printf '%s/%s' \"$result\" \"$part\"; }\n    done\n\n    echo \"${result#/}\"\n}\n\n# Accumulate values efficiently\naccumulate() {\n    local -n arr=\"$1\"\n    local sum=0\n\n    for val in \"${arr[@]}\"; do\n        # In-shell arithmetic capture\n        sum=${ echo $((sum + val)); }\n    done\n\n    echo \"$sum\"\n}\n```\n\n## Error Handling in Pipelines\n\n### Pipeline Error Detection\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Check all pipeline stages\nrun_pipeline() {\n    local result\n\n    # pipefail ensures we catch errors in any stage\n    if ! result=$(stage1 | stage2 | stage3); then\n        echo \"Pipeline failed\" >&2\n        return 1\n    fi\n\n    echo \"$result\"\n}\n\n# PIPESTATUS for detailed error info\nrun_with_status() {\n    cmd1 | cmd2 | cmd3\n\n    local -a status=(\"${PIPESTATUS[@]}\")\n\n    for i in \"${!status[@]}\"; do\n        if [[ \"${status[$i]}\" -ne 0 ]]; then\n            echo \"Stage $i failed with status ${status[$i]}\" >&2\n        fi\n    done\n\n    # Return highest exit status\n    local max=0\n    for s in \"${status[@]}\"; do\n        ((s > max)) && max=\"$s\"\n    done\n    return \"$max\"\n}\n```\n\n### Cleanup on Pipeline Failure\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Track resources for cleanup\ndeclare -a CLEANUP_PIDS=()\ndeclare -a CLEANUP_FILES=()\n\ncleanup() {\n    local pid file\n\n    for pid in \"${CLEANUP_PIDS[@]}\"; do\n        kill \"$pid\" 2>/dev/null || true\n    done\n\n    for file in \"${CLEANUP_FILES[@]}\"; do\n        rm -f \"$file\" 2>/dev/null || true\n    done\n}\n\ntrap cleanup EXIT\n\n# Register cleanup\nregister_pid() { CLEANUP_PIDS+=(\"$1\"); }\nregister_file() { CLEANUP_FILES+=(\"$1\"); }\n\n# Example usage\nrun_safe_pipeline() {\n    local fifo=\"/tmp/pipeline_$$\"\n    mkfifo \"$fifo\"\n    register_file \"$fifo\"\n\n    producer > \"$fifo\" &\n    register_pid \"$!\"\n\n    consumer < \"$fifo\" &\n    register_pid \"$!\"\n\n    wait\n}\n```\n\n## Best Practices\n\n### FIFO Naming Convention\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Include PID and descriptive name\ncreate_fifo() {\n    local name=\"$1\"\n    local fifo=\"/tmp/${name}_$$_$(date +%s)\"\n    mkfifo -m 600 \"$fifo\"  # Restrictive permissions\n    echo \"$fifo\"\n}\n\n# Use tmpdir for security\ncreate_secure_fifo() {\n    local name=\"$1\"\n    local tmpdir\n    tmpdir=$(mktemp -d)\n    local fifo=\"$tmpdir/$name\"\n    mkfifo -m 600 \"$fifo\"\n    echo \"$fifo\"\n}\n```\n\n### Preventing Deadlocks\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  DEADLOCK - writer blocks, reader never starts\n# mkfifo pipe\n# echo \"data\" > pipe  # Blocks forever\n\n#  SAFE - open both ends or use background\nmkfifo pipe\ntrap 'rm -f pipe' EXIT\n\n# Option 1: Background writer\necho \"data\" > pipe &\ncat < pipe\n\n# Option 2: Open for read/write\nexec 3<>pipe\necho \"data\" >&3\nread -r data <&3\nexec 3>&-\n\n# Option 3: Non-blocking open (requires careful handling)\nexec 3<pipe &\nexec 4>pipe\necho \"data\" >&4\nread -r data <&3\n```\n\n### Timeout Patterns\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Read with timeout\nread_with_timeout() {\n    local fifo=\"$1\"\n    local timeout=\"$2\"\n    local result\n\n    if read -t \"$timeout\" -r result < \"$fifo\"; then\n        echo \"$result\"\n        return 0\n    else\n        echo \"Timeout after ${timeout}s\" >&2\n        return 1\n    fi\n}\n\n# Write with timeout (using timeout command)\nwrite_with_timeout() {\n    local fifo=\"$1\"\n    local timeout=\"$2\"\n    local data=\"$3\"\n\n    if timeout \"$timeout\" bash -c \"echo '$data' > '$fifo'\"; then\n        return 0\n    else\n        echo \"Write timeout after ${timeout}s\" >&2\n        return 1\n    fi\n}\n```\n\n## Resources\n\n- [Bash Reference - Process Substitution](https://www.gnu.org/software/bash/manual/html_node/Process-Substitution.html)\n- [Bash Reference - Coprocesses](https://www.gnu.org/software/bash/manual/html_node/Coprocesses.html)\n- [BashFAQ - Process Substitution](https://mywiki.wooledge.org/ProcessSubstitution)\n- [Named Pipes (FIFOs)](https://www.gnu.org/software/libc/manual/html_node/FIFO-Special-Files.html)\n\n---\n\n**Master process substitution and FIFOs for efficient inter-process communication without temporary files.**"
              },
              {
                "name": "security-first-2025",
                "description": "Security-first bash scripting patterns for 2025 (mandatory validation, zero-trust)",
                "path": "plugins/bash-master/skills/security-first-2025/SKILL.md",
                "frontmatter": {
                  "name": "security-first-2025",
                  "description": "Security-first bash scripting patterns for 2025 (mandatory validation, zero-trust)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Security-First Bash Scripting (2025)\n\n## Overview\n\n2025 security assessments reveal **60%+ of exploited automation tools lacked adequate input sanitization**. This skill provides mandatory security patterns.\n\n## Critical Security Patterns\n\n### 1. Input Validation (Non-Negotiable)\n\n**Every input MUST be validated before use:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  REQUIRED: Validate all inputs\nvalidate_input() {\n    local input=\"$1\"\n    local pattern=\"$2\"\n    local max_length=\"${3:-255}\"\n\n    # Check empty\n    if [[ -z \"$input\" ]]; then\n        echo \"Error: Input required\" >&2\n        return 1\n    fi\n\n    # Check pattern\n    if [[ ! \"$input\" =~ $pattern ]]; then\n        echo \"Error: Invalid format\" >&2\n        return 1\n    fi\n\n    # Check length\n    if [[ ${#input} -gt $max_length ]]; then\n        echo \"Error: Input too long (max $max_length)\" >&2\n        return 1\n    fi\n\n    return 0\n}\n\n# Usage\nread -r user_input\nif validate_input \"$user_input\" '^[a-zA-Z0-9_-]+$' 50; then\n    process \"$user_input\"\nelse\n    exit 1\nfi\n```\n\n### 2. Command Injection Prevention\n\n**NEVER use eval or dynamic execution with user input:**\n\n```bash\n#  DANGEROUS - Command injection vulnerability\nuser_input=\"$(cat user_file.txt)\"\neval \"$user_input\"  # NEVER DO THIS\n\n#  DANGEROUS - Indirect command injection\ngrep \"$user_pattern\" file.txt  # If pattern is \"-e /etc/passwd\"\n\n#  SAFE - Use -- separator\ngrep -- \"$user_pattern\" file.txt\n\n#  SAFE - Use arrays\ngrep_args=(\"$user_pattern\" \"file.txt\")\ngrep \"${grep_args[@]}\"\n\n#  SAFE - Validate before use\nif [[ \"$user_pattern\" =~ ^[a-zA-Z0-9]+$ ]]; then\n    grep \"$user_pattern\" file.txt\nfi\n```\n\n### 3. Path Traversal Prevention\n\n**Sanitize and validate ALL file paths:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Sanitize path components\nsanitize_path() {\n    local path=\"$1\"\n\n    # Remove dangerous patterns\n    path=\"${path//..\\/}\"     # Remove ../\n    path=\"${path//\\/..\\//}\"  # Remove /../\n    path=\"${path#/}\"         # Remove leading /\n\n    echo \"$path\"\n}\n\n# Validate path is within allowed directory\nis_safe_path() {\n    local file_path=\"$1\"\n    local base_dir=\"$2\"\n\n    # Resolve to absolute paths\n    local real_path real_base\n    real_path=$(readlink -f \"$file_path\" 2>/dev/null) || return 1\n    real_base=$(readlink -f \"$base_dir\" 2>/dev/null) || return 1\n\n    # Check path starts with base\n    [[ \"$real_path\" == \"$real_base\"/* ]]\n}\n\n# Usage\nuser_file=$(sanitize_path \"$user_input\")\nif is_safe_path \"/var/app/uploads/$user_file\" \"/var/app/uploads\"; then\n    cat \"/var/app/uploads/$user_file\"\nelse\n    echo \"Error: Access denied\" >&2\n    exit 1\nfi\n```\n\n### 4. Secure Temporary Files\n\n**Never use predictable temp file names:**\n\n```bash\n#  DANGEROUS - Race condition vulnerability\ntemp_file=\"/tmp/myapp.tmp\"\necho \"data\" > \"$temp_file\"  # Can be symlinked by attacker\n\n#  DANGEROUS - Predictable name\ntemp_file=\"/tmp/myapp-$$.tmp\"  # PID can be guessed\n\n#  SAFE - Use mktemp\ntemp_file=$(mktemp)\nchmod 600 \"$temp_file\"  # Owner-only permissions\necho \"data\" > \"$temp_file\"\n\n#  SAFE - Automatic cleanup\nreadonly TEMP_FILE=$(mktemp)\ntrap 'rm -f \"$TEMP_FILE\"' EXIT INT TERM\n\n#  SAFE - Temp directory\nreadonly TEMP_DIR=$(mktemp -d)\ntrap 'rm -rf \"$TEMP_DIR\"' EXIT INT TERM\nchmod 700 \"$TEMP_DIR\"\n```\n\n### 5. Secrets Management\n\n**NEVER hardcode secrets or expose them:**\n\n```bash\n#  DANGEROUS - Hardcoded secrets\nDB_PASSWORD=\"supersecret123\"\n\n#  DANGEROUS - Secrets in environment (visible in ps)\nexport DB_PASSWORD=\"supersecret123\"\n\n#  SAFE - Read from secure file\nif [[ -f /run/secrets/db_password ]]; then\n    DB_PASSWORD=$(< /run/secrets/db_password)\n    chmod 600 /run/secrets/db_password\nelse\n    echo \"Error: Secret not found\" >&2\n    exit 1\nfi\n\n#  SAFE - Use cloud secret managers\nget_secret() {\n    local secret_name=\"$1\"\n\n    # AWS Secrets Manager\n    aws secretsmanager get-secret-value \\\n        --secret-id \"$secret_name\" \\\n        --query SecretString \\\n        --output text\n}\n\nDB_PASSWORD=$(get_secret \"production/database/password\")\n\n#  SAFE - Prompt for sensitive data (no echo)\nread -rsp \"Enter password: \" password\necho  # Newline after password\n```\n\n### 6. Privilege Management\n\n**Follow least privilege principle:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Check not running as root\nif [[ $EUID -eq 0 ]]; then\n    echo \"Error: Do not run as root\" >&2\n    exit 1\nfi\n\n# Drop privileges if started as root\ndrop_privileges() {\n    local target_user=\"$1\"\n\n    if [[ $EUID -eq 0 ]]; then\n        echo \"Dropping privileges to $target_user\" >&2\n        exec sudo -u \"$target_user\" \"$0\" \"$@\"\n    fi\n}\n\n# Run specific command with minimal privileges\nrun_privileged() {\n    local command=\"$1\"\n    shift\n\n    # Use sudo with minimal scope\n    sudo --non-interactive \\\n         --reset-timestamp \\\n         \"$command\" \"$@\"\n}\n\n# Usage\ndrop_privileges \"appuser\"\n```\n\n### 7. Environment Variable Sanitization\n\n**Clean environment before executing:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Clean environment\nclean_environment() {\n    # Unset dangerous variables\n    unset IFS\n    unset CDPATH\n    unset GLOBIGNORE\n\n    # Set safe PATH (absolute paths only)\n    export PATH=\"/usr/local/bin:/usr/bin:/bin\"\n\n    # Set safe IFS\n    IFS=$'\\n\\t'\n}\n\n# Execute command in clean environment\nexec_clean() {\n    env -i \\\n        HOME=\"$HOME\" \\\n        USER=\"$USER\" \\\n        PATH=\"/usr/local/bin:/usr/bin:/bin\" \\\n        \"$@\"\n}\n\n# Usage\nclean_environment\nexec_clean /usr/local/bin/myapp\n```\n\n### 8. Absolute Path Usage (2025 Best Practice)\n\n**Always use absolute paths to prevent PATH hijacking:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  DANGEROUS - Vulnerable to PATH manipulation\ncurl https://example.com/data\njq '.items[]' data.json\n\n#  SAFE - Absolute paths\n/usr/bin/curl https://example.com/data\n/usr/bin/jq '.items[]' data.json\n\n#  SAFE - Verify command location\nCURL=$(command -v curl) || { echo \"curl not found\" >&2; exit 1; }\n\"$CURL\" https://example.com/data\n```\n\n**Why This Matters:**\n- Prevents malicious binaries in user PATH\n- Standard practice in enterprise environments\n- Required for security-sensitive scripts\n\n### 9. History File Protection (2025 Security)\n\n**Disable history for credential operations:**\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Disable history for this session\nHISTFILE=/dev/null\nexport HISTFILE\n\n# Or disable specific commands\nHISTIGNORE=\"*password*:*secret*:*token*\"\nexport HISTIGNORE\n\n# Handle sensitive operations\nread -rsp \"Enter database password: \" db_password\necho\n\n# Use password (not logged to history)\n/usr/bin/mysql -p\"$db_password\" -e \"SELECT 1\"\n\n# Clear variable\nunset db_password\n```\n\n## Security Checklist (2025)\n\nEvery script MUST pass these checks:\n\n### Input Validation\n- [ ] All user inputs validated with regex patterns\n- [ ] Maximum length enforced on all inputs\n- [ ] Empty/null inputs rejected\n- [ ] Special characters escaped or rejected\n\n### Command Safety\n- [ ] No eval with user input\n- [ ] No dynamic variable names from user input\n- [ ] All command arguments use -- separator\n- [ ] Arrays used instead of string concatenation\n\n### File Operations\n- [ ] All paths validated against directory traversal\n- [ ] Temp files created with mktemp\n- [ ] File permissions set restrictively (600/700)\n- [ ] Cleanup handlers registered (trap EXIT)\n\n### Secrets\n- [ ] No hardcoded passwords/keys/tokens\n- [ ] Secrets read from secure storage\n- [ ] Secrets never logged or printed\n- [ ] Secrets cleared from memory when done\n\n### Privileges\n- [ ] Runs with minimum required privileges\n- [ ] Root execution rejected unless necessary\n- [ ] Privilege drops implemented where needed\n- [ ] Sudo scope minimized\n\n### Error Handling\n- [ ] set -euo pipefail enabled\n- [ ] All errors logged to stderr\n- [ ] Sensitive data not exposed in errors\n- [ ] Exit codes meaningful\n\n## Automated Security Scanning\n\n### ShellCheck Integration\n\n```bash\n# .github/workflows/security.yml\nname: Security Scan\n\non: [push, pull_request]\n\njobs:\n  shellcheck:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: ShellCheck\n        run: |\n          # Fail on security issues\n          find . -name \"*.sh\" -exec shellcheck \\\n            --severity=error \\\n            --enable=all \\\n            {} +\n```\n\n### Custom Security Linting\n\n```bash\n#!/usr/bin/env bash\n# security-lint.sh - Check scripts for security issues\n\nset -euo pipefail\n\nlint_script() {\n    local script=\"$1\"\n    local issues=0\n\n    echo \"Checking: $script\"\n\n    # Check for eval\n    if grep -n \"eval\" \"$script\"; then\n        echo \"   Found eval (command injection risk)\"\n        ((issues++))\n    fi\n\n    # Check for hardcoded secrets\n    if grep -nE \"(password|secret|token|key)\\s*=\\s*['\\\"][^'\\\"]+['\\\"]\" \"$script\"; then\n        echo \"   Found hardcoded secrets\"\n        ((issues++))\n    fi\n\n    # Check for predictable temp files\n    if grep -n \"/tmp/[a-zA-Z0-9_-]*\\\\.tmp\" \"$script\"; then\n        echo \"   Found predictable temp file\"\n        ((issues++))\n    fi\n\n    # Check for unquoted variables\n    if grep -nE '\\$[A-Z_]+[^\"]' \"$script\"; then\n        echo \"    Found unquoted variables\"\n        ((issues++))\n    fi\n\n    if ((issues == 0)); then\n        echo \"   No security issues found\"\n    fi\n\n    return \"$issues\"\n}\n\n# Scan all scripts\ntotal_issues=0\nwhile IFS= read -r -d '' script; do\n    lint_script \"$script\" || ((total_issues++))\ndone < <(find . -name \"*.sh\" -type f -print0)\n\nif ((total_issues > 0)); then\n    echo \" Found security issues in $total_issues scripts\"\n    exit 1\nelse\n    echo \" All scripts passed security checks\"\nfi\n```\n\n## Real-World Secure Script Template\n\n```bash\n#!/usr/bin/env bash\n#\n# Secure Script Template (2025)\n#\n\nset -euo pipefail\nIFS=$'\\n\\t'\n\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"${BASH_SOURCE[0]}\")\"\n\n# Security: Reject root execution\nif [[ $EUID -eq 0 ]]; then\n    echo \"Error: Do not run as root\" >&2\n    exit 1\nfi\n\n# Security: Clean environment\nexport PATH=\"/usr/local/bin:/usr/bin:/bin\"\nunset CDPATH GLOBIGNORE\n\n# Security: Secure temp file\nreadonly TEMP_FILE=$(mktemp)\ntrap 'rm -f \"$TEMP_FILE\"; exit' EXIT INT TERM\nchmod 600 \"$TEMP_FILE\"\n\n# Validate input\nvalidate_input() {\n    local input=\"$1\"\n\n    if [[ -z \"$input\" ]]; then\n        echo \"Error: Input required\" >&2\n        return 1\n    fi\n\n    if [[ ! \"$input\" =~ ^[a-zA-Z0-9_/-]+$ ]]; then\n        echo \"Error: Invalid characters in input\" >&2\n        return 1\n    fi\n\n    if [[ ${#input} -gt 255 ]]; then\n        echo \"Error: Input too long\" >&2\n        return 1\n    fi\n\n    return 0\n}\n\n# Sanitize file path\nsanitize_path() {\n    local path=\"$1\"\n    path=\"${path//..\\/}\"\n    path=\"${path#/}\"\n    echo \"$path\"\n}\n\n# Main function\nmain() {\n    local user_input=\"${1:-}\"\n\n    # Validate\n    if ! validate_input \"$user_input\"; then\n        exit 1\n    fi\n\n    # Sanitize\n    local safe_path\n    safe_path=$(sanitize_path \"$user_input\")\n\n    # Process safely\n    echo \"Processing: $safe_path\"\n    # ... your logic here ...\n}\n\nmain \"$@\"\n```\n\n## Compliance Standards (2025)\n\n### CIS Benchmarks\n- Use ShellCheck for automated compliance\n- Implement input validation on all user data\n- Secure temporary file handling\n- Least privilege execution\n\n### NIST Guidelines\n- Strong input validation (NIST SP 800-53)\n- Secure coding practices\n- Logging and monitoring\n- Access control enforcement\n\n### OWASP Top 10\n- A03: Injection - Prevent command injection\n- A01: Broken Access Control - Path validation\n- A02: Cryptographic Failures - Secure secrets\n\n## Resources\n\n- [CIS Docker Benchmark](https://www.cisecurity.org/benchmark/docker)\n- [OWASP Command Injection](https://owasp.org/www-community/attacks/Command_Injection)\n- [ShellCheck Security Rules](https://www.shellcheck.net/wiki/)\n- [NIST SP 800-53](https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final)\n\n---\n\n**Security-first development is non-negotiable in 2025. Every script must pass all security checks before deployment.**"
              },
              {
                "name": "shellcheck-cicd-2025",
                "description": "ShellCheck validation as non-negotiable 2025 workflow practice",
                "path": "plugins/bash-master/skills/shellcheck-cicd-2025/SKILL.md",
                "frontmatter": {
                  "name": "shellcheck-cicd-2025",
                  "description": "ShellCheck validation as non-negotiable 2025 workflow practice"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# ShellCheck CI/CD Integration (2025)\n\n## ShellCheck: Non-Negotiable in 2025\n\nShellCheck is now considered **mandatory** in modern bash workflows (2025 best practices):\n\n### Latest Version: v0.11.0 (August 2025)\n\n**What's New:**\n- Full Bash 5.3 support (`${| cmd; }` and `source -p`)\n- **New warnings**: SC2327/SC2328 (capture group issues)\n- **POSIX.1-2024 compliance**: SC3013 removed (-ot/-nt/-ef now POSIX standard)\n- Enhanced static analysis capabilities\n- Improved performance and accuracy\n\n### Why Mandatory?\n\n- Catches subtle bugs before production\n- Prevents common security vulnerabilities\n- Enforces consistent code quality\n- Required by most DevOps teams\n- Standard in enterprise environments\n- Supports latest POSIX.1-2024 standard\n\n## Installation\n\n```bash\n# Ubuntu/Debian\napt-get install shellcheck\n\n# macOS\nbrew install shellcheck\n\n# Alpine (Docker)\napk add shellcheck\n\n# Windows (WSL/Git Bash)\nchoco install shellcheck\n\n# Or download binary\nwget https://github.com/koalaman/shellcheck/releases/latest/download/shellcheck-stable.linux.x86_64.tar.xz\ntar -xf shellcheck-stable.linux.x86_64.tar.xz\nsudo cp shellcheck-stable/shellcheck /usr/local/bin/\n```\n\n## GitHub Actions Integration\n\n### Mandatory Pre-Merge Check\n\n```yaml\n# .github/workflows/shellcheck.yml\nname: ShellCheck\n\non:\n  pull_request:\n    paths:\n      - '**.sh'\n      - '**Dockerfile'\n  push:\n    branches: [main]\n\njobs:\n  shellcheck:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Run ShellCheck\n      uses: ludeeus/action-shellcheck@master\n      with:\n        severity: warning\n        format: gcc  # or: tty, json, checkstyle\n        scandir: './scripts'\n        # Fail on any issues\n        ignore_paths: 'node_modules'\n\n    # Block merge on failures\n    - name: Annotate PR\n      if: failure()\n      uses: actions/github-script@v6\n      with:\n        script: |\n          github.rest.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body: ' ShellCheck validation failed. Fix issues before merging.'\n          })\n```\n\n## Azure DevOps Integration\n\n```yaml\n# azure-pipelines.yml\ntrigger:\n- main\n\npr:\n- main\n\nstages:\n- stage: Validate\n  jobs:\n  - job: ShellCheck\n    pool:\n      vmImage: 'ubuntu-24.04'\n\n    steps:\n    - script: |\n        sudo apt-get install -y shellcheck\n      displayName: 'Install ShellCheck'\n\n    - script: |\n        find . -name \"*.sh\" -type f | xargs shellcheck --format=gcc --severity=warning\n      displayName: 'Run ShellCheck'\n      failOnStderr: true\n\n    - task: PublishTestResults@2\n      condition: always()\n      inputs:\n        testResultsFormat: 'JUnit'\n        testResultsFiles: '**/shellcheck-results.xml'\n        failTaskOnFailedTests: true\n```\n\n## Git Hooks (Pre-Commit)\n\n```bash\n# .git/hooks/pre-commit\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\n\n# Find all staged .sh files\nmapfile -t STAGED_SH < <(git diff --cached --name-only --diff-filter=ACMR | grep '\\.sh$' || true)\n\nif [ ${#STAGED_SH[@]} -eq 0 ]; then\n  exit 0\nfi\n\necho \"Running ShellCheck on staged files...\"\n\n# Run ShellCheck\nshellcheck --format=gcc --severity=warning \"${STAGED_SH[@]}\"\n\nif [ $? -ne 0 ]; then\n  echo \" ShellCheck failed. Fix issues before committing.\"\n  exit 1\nfi\n\necho \" ShellCheck passed\"\nexit 0\n```\n\n**Install Pre-Commit Hook:**\n```bash\nchmod +x .git/hooks/pre-commit\n\n# Or use pre-commit framework\n# .pre-commit-config.yaml\nrepos:\n- repo: https://github.com/shellcheck-py/shellcheck-py\n  rev: v0.11.0.0\n  hooks:\n  - id: shellcheck\n    args: ['--severity=warning']\n\n# Install\npip install pre-commit\npre-commit install\n```\n\n## VS Code Integration\n\n```json\n// .vscode/settings.json\n{\n  \"shellcheck.enable\": true,\n  \"shellcheck.run\": \"onType\",\n  \"shellcheck.executablePath\": \"/usr/local/bin/shellcheck\",\n  \"shellcheck.exclude\": [\"SC1090\", \"SC1091\"],  // Optional excludes\n  \"shellcheck.customArgs\": [\n    \"-x\",  // Follow source files\n    \"--severity=warning\"\n  ]\n}\n```\n\n## Docker Build Integration\n\n```dockerfile\n# Dockerfile with ShellCheck validation\nFROM alpine:3.19 AS builder\n\n# Install ShellCheck\nRUN apk add --no-cache shellcheck bash\n\n# Copy scripts\nCOPY scripts/ /scripts/\n\n# Validate all scripts before continuing\nRUN find /scripts -name \"*.sh\" -type f -exec shellcheck --severity=warning {} +\n\n# Final stage\nFROM alpine:3.19\nCOPY --from=builder /scripts/ /scripts/\nRUN chmod +x /scripts/*.sh\n\nENTRYPOINT [\"/scripts/entrypoint.sh\"]\n```\n\n## Common ShellCheck Rules (2025)\n\n### New in v0.11.0: SC2327/SC2328 - Capture Groups\n\n```bash\n#  Bad - Capture groups may not work as expected\nif [[ \"$string\" =~ ([0-9]+)\\.([0-9]+) ]]; then\n  echo \"$1\"  # Wrong: $1 is script arg, not capture group\nfi\n\n#  Good - Use BASH_REMATCH array\nif [[ \"$string\" =~ ([0-9]+)\\.([0-9]+) ]]; then\n  echo \"${BASH_REMATCH[1]}.${BASH_REMATCH[2]}\"\nfi\n```\n\n### SC2294: eval Negates Array Benefits (New)\n\n```bash\n#  Bad - eval defeats array safety\neval \"command ${array[@]}\"\n\n#  Good - Direct array usage\ncommand \"${array[@]}\"\n```\n\n### SC2295: Quote Expansions Inside ${}\n\n```bash\n#  Bad\necho \"${var-$default}\"  # $default not quoted\n\n#  Good\necho \"${var-\"$default\"}\"\n```\n\n### SC2086: Quote Variables\n\n```bash\n#  Bad\nfile=$1\ncat $file  # Fails if filename has spaces\n\n#  Good\nfile=$1\ncat \"$file\"\n```\n\n### SC2046: Quote Command Substitution\n\n```bash\n#  Bad\nfor file in $(find . -name \"*.txt\"); do\n  echo $file\ndone\n\n#  Good\nfind . -name \"*.txt\" -print0 | while IFS= read -r -d '' file; do\n  echo \"$file\"\ndone\n```\n\n### SC2155: Separate Declaration and Assignment\n\n```bash\n#  Bad\nlocal result=$(command)  # Hides command exit code\n\n#  Good\nlocal result\nresult=$(command)\n```\n\n### SC2164: Use cd || exit\n\n```bash\n#  Bad\ncd /some/directory\n./script.sh  # Runs in wrong dir if cd fails\n\n#  Good\ncd /some/directory || exit 1\n./script.sh\n```\n\n## Google Shell Style Guide (50-Line Limit)\n\n2025 recommendation: Keep scripts under 50 lines:\n\n```bash\n#  Bad: 500-line monolithic script\n#!/usr/bin/env bash\n# ... 500 lines of code ...\n\n#  Good: Modular scripts < 50 lines each\n\n# lib/logging.sh (20 lines)\nlog_info() { echo \"[INFO] $*\"; }\nlog_error() { echo \"[ERROR] $*\" >&2; }\n\n# lib/validation.sh (30 lines)\nvalidate_input() { ... }\ncheck_dependencies() { ... }\n\n# main.sh (40 lines)\nsource \"$(dirname \"$0\")/lib/logging.sh\"\nsource \"$(dirname \"$0\")/lib/validation.sh\"\n\nmain() {\n  validate_input \"$@\"\n  check_dependencies\n  # ... core logic ...\n}\n\nmain \"$@\"\n```\n\n## Enforce in CI/CD\n\n### Fail Build on Issues\n\n```yaml\n# Strict enforcement\n- name: ShellCheck (Strict)\n  run: |\n    shellcheck --severity=warning scripts/*.sh\n  # Exit code 1 fails the build\n\n# Advisory only (warnings but don't fail)\n- name: ShellCheck (Advisory)\n  run: |\n    shellcheck --severity=warning scripts/*.sh || true\n  # Logs warnings but doesn't fail\n```\n\n### Generate Reports\n\n```bash\n# JSON format for parsing\nshellcheck --format=json scripts/*.sh > shellcheck-report.json\n\n# GitHub annotations format\nshellcheck --format=gcc scripts/*.sh\n\n# Human-readable\nshellcheck --format=tty scripts/*.sh\n```\n\n## Modern Error Handling Trio (2025)\n\nAlways use with ShellCheck validation:\n\n```bash\n#!/usr/bin/env bash\n\n# Modern error handling (non-negotiable in 2025)\nset -o errexit   # Exit on command failure\nset -o nounset   # Exit on undefined variable\nset -o pipefail  # Exit on pipe failure\n\n# ShellCheck approved\nmain() {\n  local config_file=\"${1:?Config file required}\"\n\n  if [[ ! -f \"$config_file\" ]]; then\n    echo \"Error: Config file not found: $config_file\" >&2\n    return 1\n  fi\n\n  # Safe command execution\n  local result\n  result=$(process_config \"$config_file\")\n\n  echo \"$result\"\n}\n\nmain \"$@\"\n```\n\n## Best Practices (2025)\n\n1. **Run ShellCheck in CI/CD (mandatory)**\n2. **Use pre-commit hooks** to catch issues early\n3. **Keep scripts under 50 lines** (Google Style Guide)\n4. **Use modern error handling trio** (errexit, nounset, pipefail)\n5. **Fix all warnings** before merging\n6. **Document any disabled rules** with reasoning\n7. **Integrate with IDE** for real-time feedback\n\n## Resources\n\n- [ShellCheck](https://www.shellcheck.net)\n- [Google Shell Style Guide](https://google.github.io/styleguide/shellguide.html)\n- [ShellCheck GitHub Action](https://github.com/ludeeus/action-shellcheck)"
              },
              {
                "name": "string-manipulation-mastery",
                "description": "Advanced bash string manipulation including parameter expansion, pattern matching, regex, and text processing (2025)",
                "path": "plugins/bash-master/skills/string-manipulation-mastery/SKILL.md",
                "frontmatter": {
                  "name": "string-manipulation-mastery",
                  "description": "Advanced bash string manipulation including parameter expansion, pattern matching, regex, and text processing (2025)"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Bash String Manipulation Mastery (2025)\n\n## Overview\n\nComprehensive guide to string manipulation in bash using parameter expansion, pattern matching, regular expressions, and built-in transformations. Master these techniques to avoid spawning external processes like `sed`, `awk`, or `cut` for simple operations.\n\n## Parameter Expansion Basics\n\n### String Length\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"Hello, World!\"\n\n# Get length\necho \"${#str}\"  # 13\n\n# Length of array element\narr=(\"short\" \"much longer string\")\necho \"${#arr[0]}\"  # 5\necho \"${#arr[1]}\"  # 18\n\n# Number of array elements (not string length)\necho \"${#arr[@]}\"  # 2\n```\n\n### Substring Extraction\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"Hello, World!\"\n\n# ${var:offset} - from offset to end\necho \"${str:7}\"     # World!\n\n# ${var:offset:length} - from offset, length chars\necho \"${str:0:5}\"   # Hello\necho \"${str:7:5}\"   # World\n\n# Negative offset (from end) - note the space or parentheses\necho \"${str: -6}\"      # World!\necho \"${str:(-6)}\"     # World!\necho \"${str: -6:5}\"    # World\n\n# Last N characters\nlast_n() {\n    local str=\"$1\" n=\"$2\"\n    echo \"${str: -$n}\"\n}\nlast_n \"Hello\" 3  # llo\n\n# Extract between positions\nbetween() {\n    local str=\"$1\" start=\"$2\" end=\"$3\"\n    echo \"${str:start:$((end - start))}\"\n}\nbetween \"0123456789\" 3 7  # 3456\n```\n\n### Default Values\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# ${var:-default} - use default if unset or empty\nname=\"${1:-Anonymous}\"\necho \"Hello, $name\"\n\n# ${var:=default} - assign default if unset or empty\n: \"${CONFIG_FILE:=/etc/app.conf}\"\n\n# ${var:+alternate} - use alternate if var IS set\ndebug_flag=\"${DEBUG:+--verbose}\"\n\n# ${var:?error} - exit with error if unset or empty\n: \"${REQUIRED_VAR:?REQUIRED_VAR must be set}\"\n\n# Without colon - only checks if unset (empty is OK)\n# ${var-default}, ${var=default}, ${var+alt}, ${var?err}\n\n# Practical example: config with defaults\nsetup_config() {\n    : \"${DB_HOST:=localhost}\"\n    : \"${DB_PORT:=5432}\"\n    : \"${DB_NAME:=myapp}\"\n    : \"${DB_USER:=postgres}\"\n}\n```\n\n### Indirect Expansion\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# ${!var} - indirect reference\nconfig_host=\"server.example.com\"\nconfig_port=\"8080\"\n\nkey=\"config_host\"\necho \"${!key}\"  # server.example.com\n\n# Iterate over related variables\nfor suffix in host port; do\n    var=\"config_$suffix\"\n    echo \"$suffix = ${!var}\"\ndone\n\n# Get all variable names matching pattern\n# ${!prefix@} or ${!prefix*}\nfor var in \"${!config_@}\"; do\n    echo \"$var = ${!var}\"\ndone\n\n# Array indirection\narr=(a b c d e)\nidx=2\necho \"${arr[$idx]}\"  # c\n\n# Indirect array reference (Bash 4.3+ nameref)\nget_array_element() {\n    local -n arr_ref=\"$1\"\n    local idx=\"$2\"\n    echo \"${arr_ref[$idx]}\"\n}\nget_array_element arr 3  # d\n```\n\n## Pattern Matching\n\n### Prefix Removal\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\npath=\"/home/user/documents/file.tar.gz\"\n\n# ${var#pattern} - remove shortest prefix match\necho \"${path#*/}\"     # home/user/documents/file.tar.gz\n\n# ${var##pattern} - remove longest prefix match\necho \"${path##*/}\"    # file.tar.gz (basename)\n\n# Remove extension\nfilename=\"archive.tar.gz\"\necho \"${filename#*.}\"   # tar.gz (first . onwards)\necho \"${filename##*.}\"  # gz (last extension only)\n\n# Remove prefix string\nurl=\"https://example.com/path\"\necho \"${url#https://}\"  # example.com/path\n\n# Practical: Get file extension\nget_extension() {\n    local file=\"$1\"\n    echo \"${file##*.}\"\n}\n\n# Practical: Strip leading zeros\nstrip_leading_zeros() {\n    local num=\"$1\"\n    echo \"${num#\"${num%%[!0]*}\"}\"\n}\nstrip_leading_zeros \"000123\"  # 123\n```\n\n### Suffix Removal\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\npath=\"/home/user/documents/file.tar.gz\"\n\n# ${var%pattern} - remove shortest suffix match\necho \"${path%/*}\"     # /home/user/documents (dirname)\n\n# ${var%%pattern} - remove longest suffix match\necho \"${path%%/*}\"    # (empty - removes everything)\n\n# Remove extension\nfilename=\"archive.tar.gz\"\necho \"${filename%.*}\"   # archive.tar\necho \"${filename%%.*}\"  # archive\n\n# Practical: Get directory\ndirname=\"${path%/*}\"\n\n# Practical: Remove file extension\nbasename=\"${path##*/}\"\nname_without_ext=\"${basename%.*}\"\n\n# Combined: Change extension\nchange_extension() {\n    local file=\"$1\" new_ext=\"$2\"\n    echo \"${file%.*}.$new_ext\"\n}\nchange_extension \"doc.txt\" \"md\"  # doc.md\n```\n\n### Pattern Substitution\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"hello hello hello\"\n\n# ${var/pattern/replacement} - replace first match\necho \"${str/hello/hi}\"     # hi hello hello\n\n# ${var//pattern/replacement} - replace all matches\necho \"${str//hello/hi}\"    # hi hi hi\n\n# ${var/#pattern/replacement} - replace if at start\necho \"${str/#hello/hi}\"    # hi hello hello\n\n# ${var/%pattern/replacement} - replace if at end\necho \"${str/%hello/goodbye}\"  # hello hello goodbye\n\n# Delete pattern (empty replacement)\necho \"${str//hello/}\"      # \"   \" (just spaces)\n\n# Practical: Sanitize filename\nsanitize_filename() {\n    local name=\"$1\"\n    # Replace spaces with underscores\n    name=\"${name// /_}\"\n    # Remove special characters\n    name=\"${name//[^a-zA-Z0-9._-]/}\"\n    echo \"$name\"\n}\nsanitize_filename \"My File (2024).txt\"  # My_File_2024.txt\n\n# Practical: Path manipulation\nnormalize_path() {\n    local path=\"$1\"\n    # Remove double slashes\n    while [[ \"$path\" == *//* ]]; do\n        path=\"${path//\\/\\//\\/}\"\n    done\n    # Remove trailing slash\n    echo \"${path%/}\"\n}\n```\n\n## Case Transformation\n\n### Basic Case Changes\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"Hello World\"\n\n# Lowercase first character\necho \"${str,}\"      # hello World\n\n# Lowercase all\necho \"${str,,}\"     # hello world\n\n# Uppercase first character\necho \"${str^}\"      # Hello World (already uppercase)\n\nstr2=\"hello world\"\necho \"${str2^}\"     # Hello world\n\n# Uppercase all\necho \"${str,,}\"     # hello world\necho \"${str2^^}\"    # HELLO WORLD\n\n# Toggle case (Bash 4.4+)\necho \"${str~~}\"     # hELLO wORLD\n```\n\n### Pattern-Based Case Changes\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"hello world\"\n\n# Uppercase only matching pattern\necho \"${str^^[aeiou]}\"  # hEllO wOrld\n\n# Lowercase only matching pattern\nstr2=\"HELLO WORLD\"\necho \"${str2,,[AEIOU]}\"  # HeLLo WoRLD\n\n# Practical: Title case\ntitle_case() {\n    local str=\"$1\"\n    local result=\"\"\n    local capitalize=true\n\n    for ((i=0; i<${#str}; i++)); do\n        local char=\"${str:$i:1}\"\n        if [[ \"$char\" == \" \" ]]; then\n            result+=\"$char\"\n            capitalize=true\n        elif $capitalize; then\n            result+=\"${char^}\"\n            capitalize=false\n        else\n            result+=\"${char,}\"\n        fi\n    done\n\n    echo \"$result\"\n}\ntitle_case \"hello WORLD from BASH\"  # Hello World From Bash\n```\n\n## Regular Expressions\n\n### Bash Regex Matching\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# =~ operator for regex matching\nstr=\"Hello World 123\"\n\nif [[ \"$str\" =~ ^Hello ]]; then\n    echo \"Starts with Hello\"\nfi\n\nif [[ \"$str\" =~ [0-9]+ ]]; then\n    echo \"Contains numbers\"\nfi\n\n# Capture groups with BASH_REMATCH\nemail=\"user@example.com\"\nif [[ \"$email\" =~ ^([^@]+)@(.+)$ ]]; then\n    echo \"User: ${BASH_REMATCH[1]}\"    # user\n    echo \"Domain: ${BASH_REMATCH[2]}\"  # example.com\n    echo \"Full match: ${BASH_REMATCH[0]}\"  # user@example.com\nfi\n\n# Store regex in variable (avoids quoting issues)\npattern='^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\ndate=\"2024-03-15\"\nif [[ \"$date\" =~ $pattern ]]; then\n    echo \"Valid date format\"\nfi\n\n# Multiple capture groups\nlog_line='2024-03-15 10:30:45 ERROR Connection failed'\npattern='^([0-9-]+) ([0-9:]+) ([A-Z]+) (.+)$'\nif [[ \"$log_line\" =~ $pattern ]]; then\n    date=\"${BASH_REMATCH[1]}\"\n    time=\"${BASH_REMATCH[2]}\"\n    level=\"${BASH_REMATCH[3]}\"\n    message=\"${BASH_REMATCH[4]}\"\nfi\n```\n\n### Practical Regex Patterns\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Email validation\nis_valid_email() {\n    local email=\"$1\"\n    local pattern='^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    [[ \"$email\" =~ $pattern ]]\n}\n\n# IP address validation\nis_valid_ip() {\n    local ip=\"$1\"\n    local octet='(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'\n    local pattern=\"^${octet}\\.${octet}\\.${octet}\\.${octet}$\"\n    [[ \"$ip\" =~ $pattern ]]\n}\n\n# URL parsing\nparse_url() {\n    local url=\"$1\"\n    local pattern='^(https?|ftp)://([^/:]+)(:([0-9]+))?(/.*)?$'\n\n    if [[ \"$url\" =~ $pattern ]]; then\n        echo \"Protocol: ${BASH_REMATCH[1]}\"\n        echo \"Host: ${BASH_REMATCH[2]}\"\n        echo \"Port: ${BASH_REMATCH[4]:-default}\"\n        echo \"Path: ${BASH_REMATCH[5]:-/}\"\n    else\n        echo \"Invalid URL\" >&2\n        return 1\n    fi\n}\n\n# Semantic version parsing\nparse_semver() {\n    local version=\"$1\"\n    local pattern='^v?([0-9]+)\\.([0-9]+)\\.([0-9]+)(-([a-zA-Z0-9.-]+))?(\\+([a-zA-Z0-9.-]+))?$'\n\n    if [[ \"$version\" =~ $pattern ]]; then\n        echo \"Major: ${BASH_REMATCH[1]}\"\n        echo \"Minor: ${BASH_REMATCH[2]}\"\n        echo \"Patch: ${BASH_REMATCH[3]}\"\n        echo \"Prerelease: ${BASH_REMATCH[5]:-none}\"\n        echo \"Build: ${BASH_REMATCH[7]:-none}\"\n    fi\n}\n```\n\n## String Splitting and Joining\n\n### Split String to Array\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Using IFS and read\nstr=\"one,two,three,four\"\n\nIFS=',' read -ra arr <<< \"$str\"\necho \"${arr[0]}\"  # one\necho \"${arr[2]}\"  # three\n\n# Multi-character delimiter (using parameter expansion)\nstr=\"one||two||three\"\narr=()\nwhile [[ \"$str\" == *\"||\"* ]]; do\n    arr+=(\"${str%%||*}\")\n    str=\"${str#*||}\"\ndone\narr+=(\"$str\")\n\n# Using mapfile (newline-delimited)\nmapfile -t lines <<< \"$(echo -e \"line1\\nline2\\nline3\")\"\n\n# Split on any whitespace\nstr=\"one   two     three\"\nread -ra arr <<< \"$str\"  # IFS defaults to whitespace\n```\n\n### Join Array to String\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\narr=(\"one\" \"two\" \"three\" \"four\")\n\n# Join with delimiter using IFS\njoin_by() {\n    local IFS=\"$1\"\n    shift\n    echo \"$*\"\n}\njoin_by ',' \"${arr[@]}\"  # one,two,three,four\njoin_by ' | ' \"${arr[@]}\"  # one | two | three | four\n\n# Alternative using printf\njoin_array() {\n    local delim=\"$1\"\n    shift\n    local first=\"$1\"\n    shift\n    printf '%s' \"$first\" \"${@/#/$delim}\"\n}\njoin_array ',' \"${arr[@]}\"\n\n# Join with custom format\nprintf '\"%s\" ' \"${arr[@]}\"  # \"one\" \"two\" \"three\" \"four\"\nprintf '%s\\n' \"${arr[@]}\"   # One per line\n```\n\n## Text Processing Without External Commands\n\n### Trim Whitespace\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Trim leading whitespace\ntrim_leading() {\n    local str=\"$1\"\n    echo \"${str#\"${str%%[![:space:]]*}\"}\"\n}\n\n# Trim trailing whitespace\ntrim_trailing() {\n    local str=\"$1\"\n    echo \"${str%\"${str##*[![:space:]]}\"}\"\n}\n\n# Trim both\ntrim() {\n    local str=\"$1\"\n    str=\"${str#\"${str%%[![:space:]]*}\"}\"\n    str=\"${str%\"${str##*[![:space:]]}\"}\"\n    echo \"$str\"\n}\n\n# Extended pattern matching version (requires shopt -s extglob)\ntrim_extglob() {\n    shopt -s extglob\n    local str=\"$1\"\n    str=\"${str##+([[:space:]])}\"\n    str=\"${str%%+([[:space:]])}\"\n    echo \"$str\"\n}\n\nstr=\"   hello world   \"\ntrim \"$str\"  # \"hello world\"\n```\n\n### String Repetition\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Repeat string N times\nrepeat() {\n    local str=\"$1\"\n    local n=\"$2\"\n    local result=\"\"\n\n    for ((i=0; i<n; i++)); do\n        result+=\"$str\"\n    done\n\n    echo \"$result\"\n}\n\nrepeat \"ab\" 5  # ababababab\n\n# Using printf\nrepeat_printf() {\n    local str=\"$1\"\n    local n=\"$2\"\n    printf '%s' $(printf '%.0s'\"$str\" $(seq 1 \"$n\"))\n}\n\n# Create separator line\nseparator() {\n    local char=\"${1:--}\"\n    local width=\"${2:-80}\"\n    printf '%*s\\n' \"$width\" '' | tr ' ' \"$char\"\n}\n\nseparator \"=\" 40  # ========================================\n```\n\n### Character Replacement\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"hello world\"\n\n# Using tr-like replacement via expansion\n# Replace all 'l' with 'L'\necho \"${str//l/L}\"  # heLLo worLd\n\n# Delete characters\necho \"${str//o/}\"   # hell wrld\n\n# Translate character by character\ntranslate() {\n    local str=\"$1\"\n    local from=\"$2\"\n    local to=\"$3\"\n\n    for ((i=0; i<${#from}; i++)); do\n        str=\"${str//${from:$i:1}/${to:$i:1}}\"\n    done\n\n    echo \"$str\"\n}\n\ntranslate \"hello\" \"el\" \"ip\"  # hippo\n```\n\n### Padding and Alignment\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n# Right pad to width\npad_right() {\n    local str=\"$1\"\n    local width=\"$2\"\n    local char=\"${3:- }\"\n    printf \"%-${width}s\" \"$str\" | tr ' ' \"$char\"\n}\n\n# Left pad to width\npad_left() {\n    local str=\"$1\"\n    local width=\"$2\"\n    local char=\"${3:- }\"\n    printf \"%${width}s\" \"$str\" | tr ' ' \"$char\"\n}\n\n# Center align\ncenter() {\n    local str=\"$1\"\n    local width=\"$2\"\n    local len=${#str}\n    local padding=$(( (width - len) / 2 ))\n\n    printf \"%*s%s%*s\" $padding \"\" \"$str\" $((width - len - padding)) \"\"\n}\n\n# Zero-pad numbers\nzero_pad() {\n    local num=\"$1\"\n    local width=\"$2\"\n    printf \"%0${width}d\" \"$num\"\n}\n\nzero_pad 42 5  # 00042\n\n# Format table\nprint_table_row() {\n    printf \"| %-20s | %10s | %-15s |\\n\" \"$1\" \"$2\" \"$3\"\n}\n\nprint_table_row \"Name\" \"Age\" \"City\"\nprint_table_row \"Alice\" \"30\" \"New York\"\n```\n\n## Extended Globbing\n\n### Enable and Use\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nshopt -s extglob\n\n# Extended patterns:\n# ?(pattern) - 0 or 1 occurrence\n# *(pattern) - 0 or more occurrences\n# +(pattern) - 1 or more occurrences\n# @(pattern) - exactly 1 occurrence\n# !(pattern) - anything except pattern\n\n# Match files\nls *.@(jpg|png|gif)      # Images only\nls !(*.bak|*.tmp)        # Exclude backup/temp files\nls +([0-9]).txt          # Files starting with digits\n\n# String manipulation\nstr=\"   hello   world   \"\n\n# Remove leading whitespace\necho \"${str##+([[:space:]])}\"  # \"hello   world   \"\n\n# Remove all whitespace\necho \"${str//+([[:space:]])/ }\"  # \" hello world \"\n\n# Remove multiple extensions\nfile=\"archive.tar.gz.bak\"\necho \"${file%.@(tar|gz|bak)*}\"  # archive\n\n# Match alternatives\ncase \"$response\" in\n    @(yes|y|Y|YES))\n        echo \"Affirmative\"\n        ;;\n    @(no|n|N|NO))\n        echo \"Negative\"\n        ;;\nesac\n```\n\n### Practical Extended Glob Patterns\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\nshopt -s extglob\n\n# Clean backup files\nrm -f *.@(bak|backup|orig|~)\n\n# Find source files only\nls *.@(c|cpp|h|hpp|cc)\n\n# Exclude certain patterns\nfor file in !(test_*|_*).py; do\n    process \"$file\"\ndone\n\n# Match complex version strings\nversion_pattern='+([0-9]).+([0-9]).+([0-9])?(-+([a-z0-9]))'\nif [[ \"$version\" == $version_pattern ]]; then\n    echo \"Valid version\"\nfi\n\n# Remove redundant characters\nclean_string() {\n    local str=\"$1\"\n    # Remove repeated spaces\n    echo \"${str//+([[:space:]])/ }\"\n}\n\n# Match optional parts\nfile_pattern='*.@(test|spec)?.@(js|ts)'\n# Matches: file.js, file.ts, file.test.js, file.spec.ts, etc.\n```\n\n## Bash 5.3+ String Features\n\n### In-Shell Substitution for Strings\n\n```bash\n#!/usr/bin/env bash\n# Requires Bash 5.3+\nset -euo pipefail\n\n# No-fork string operations\nresult=${ echo \"${str^^}\"; }  # Uppercase without subshell\n\n# REPLY syntax for string building\nbuild_path() {\n    local parts=(\"$@\")\n    REPLY=\"\"\n\n    for part in \"${parts[@]}\"; do\n        ${| REPLY+=\"${REPLY:+/}$part\"; }\n    done\n}\n\n# Efficient string accumulation\naccumulate() {\n    local -n result=\"$1\"\n    shift\n\n    for item in \"$@\"; do\n        ${| result+=\"$item\"; }\n    done\n}\n```\n\n## Performance Tips\n\n### Avoid Subshells for Simple Operations\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\nstr=\"hello world\"\n\n#  SLOW - spawns external process\nbasename=$(basename \"$path\")\ndirname=$(dirname \"$path\")\nupper=$(echo \"$str\" | tr 'a-z' 'A-Z')\nlen=$(echo -n \"$str\" | wc -c)\n\n#  FAST - pure bash\nbasename=\"${path##*/}\"\ndirname=\"${path%/*}\"\nupper=\"${str^^}\"\nlen=\"${#str}\"\n```\n\n### Batch String Operations\n\n```bash\n#!/usr/bin/env bash\nset -euo pipefail\n\n#  SLOW - multiple expansions\nstr=\"$input\"\nstr=\"${str//  / }\"\nstr=\"${str#\"${str%%[![:space:]]*}\"}\"\nstr=\"${str%\"${str##*[![:space:]]}\"}\"\nstr=\"${str,,}\"\n\n#  BETTER - single function call\nnormalize_string() {\n    local str=\"$1\"\n    str=\"${str//  / }\"\n    str=\"${str#\"${str%%[![:space:]]*}\"}\"\n    str=\"${str%\"${str##*[![:space:]]}\"}\"\n    echo \"${str,,}\"\n}\nresult=$(normalize_string \"$input\")\n```\n\n## Resources\n\n- [Bash Reference - Shell Parameter Expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html)\n- [Bash Reference - Pattern Matching](https://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html)\n- [BashFAQ/100 - String manipulation](https://mywiki.wooledge.org/BashFAQ/100)\n- [Bash Hackers - Parameter Expansion](https://wiki.bash-hackers.org/syntax/pe)\n\n---\n\n**Master bash string manipulation to write efficient scripts without external dependencies.**"
              }
            ]
          },
          {
            "name": "git-master",
            "description": "Complete Git expertise for ALL operations with 2025 features (Git 2.49+, GitHub CLI 2.x). PROACTIVELY activate for: (1) ANY Git task, (2) Git 2.49+ features (git-backfill, path-walk API, reftables, sparse-checkout, worktrees), (3) Security (signed commits, zero-trust, secret scanning, CodeQL), (4) Trunk-Based Development, (5) GitHub CLI 2.x (Copilot CLI, model evaluations), (6) GitHub Actions 2025 (1 vCPU runners, immutable releases), (7) Modern workflows (monorepo, parallel development), (8) History rewriting/recovery. Provides: Git 2.49 git-backfill for partial clones, path-walk API, reftables migration, sparse-checkout (90% space reduction), worktrees, GitHub Copilot CLI, gh models eval, zero-trust security, signed commits (GPG/SSH), GitHub Actions 2025 features, automatic backups, safety guardrails, reflog recovery.",
            "source": "./plugins/git-master",
            "category": null,
            "version": "1.5.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install git-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/git-safe-rebase",
                "description": "Perform interactive rebase with safety guardrails",
                "path": "plugins/git-master/commands/git-safe-rebase.md",
                "frontmatter": {
                  "description": "Perform interactive rebase with safety guardrails"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\nYou are an expert Git operator helping the user safely perform an interactive rebase.\n\n# Task\n\nGuide the user through a safe interactive rebase with proper backups and recovery instructions.\n\n# Safety Protocol\n\n1. **Create backup branch**:\n   ```bash\n   git branch backup-before-rebase-$(date +%Y%m%d-%H%M%S)\n   ```\n\n2. **Show what will be rebased**:\n   ```bash\n   git log --oneline --graph <base>..<current-branch>\n   ```\n\n3. **Warn about risks**:\n   - \" Interactive rebase will rewrite commit history\"\n   - \" If this branch has been pushed and others are working on it, DO NOT proceed\"\n   - \" All commits will get new hashes\"\n\n4. **Ask for confirmation**:\n   - \"Has this branch been pushed to a shared remote? (y/n)\"\n   - If yes: \" WARNING: Other team members working on this branch will have problems!\"\n   - \"Do you want to proceed? (yes/NO)\"\n\n5. **Perform rebase**:\n   ```bash\n   git rebase -i <base>\n   ```\n\n6. **Provide recovery instructions**:\n   ```\n   If something goes wrong:\n   - Abort: git rebase --abort\n   - Recover: git reset --hard backup-before-rebase-XXXXXXXX\n   ```\n\n7. **After successful rebase**:\n   - \"Rebase completed successfully\"\n   - \"If you need to push: git push --force-with-lease (only if you're sure!)\"\n   - \"To delete backup: git branch -d backup-before-rebase-XXXXXXXX\"\n\n# Rebase Commands Reference\n\nInteractive rebase commands you can use:\n- `p, pick` = use commit\n- `r, reword` = use commit, but edit message\n- `e, edit` = use commit, but stop for amending\n- `s, squash` = combine with previous commit\n- `f, fixup` = like squash, but discard message\n- `d, drop` = remove commit\n\n# Safety Rules\n\n- ALWAYS create backup branch first\n- ALWAYS warn if branch has been pushed\n- ALWAYS ask for explicit confirmation\n- ALWAYS provide recovery instructions\n- NEVER rebase shared/public branches without team coordination"
              }
            ],
            "skills": [
              {
                "name": "git-2-49-features",
                "description": "Git 2.49+ features including git-backfill, path-walk API, and performance improvements",
                "path": "plugins/git-master/skills/git-2-49-features/SKILL.md",
                "frontmatter": {
                  "name": "git-2-49-features",
                  "description": "Git 2.49+ features including git-backfill, path-walk API, and performance improvements"
                },
                "content": "# Git 2.49+ Features (2025)\n\n## git-backfill Command (New in 2.49)\n\n**What:** Efficiently download missing objects in partial clones using the path-walk API.\n\n**Why:** Dramatically improves delta compression when fetching objects from partial clones, resulting in smaller downloads and better performance.\n\n### Basic Usage\n\n```bash\n# Check if you have a partial clone\ngit config extensions.partialClone\n\n# Download missing objects in background\ngit backfill\n\n# Download with custom batch size\ngit backfill --batch-size=1000\n\n# Respect sparse-checkout patterns (only fetch needed files)\ngit backfill --sparse\n\n# Check progress\ngit backfill --verbose\n```\n\n### When to Use\n\n**Scenario 1: After cloning with --filter=blob:none**\n```bash\n# Clone without blobs\ngit clone --filter=blob:none https://github.com/large/repo.git\ncd repo\n\n# Later, prefetch all missing objects efficiently\ngit backfill\n```\n\n**Scenario 2: Sparse-checkout + Partial clone**\n```bash\n# Clone with both optimizations\ngit clone --filter=blob:none --sparse https://github.com/monorepo.git\ncd monorepo\ngit sparse-checkout set src/api\n\n# Fetch only needed objects\ngit backfill --sparse\n```\n\n**Scenario 3: CI/CD Optimization**\n```bash\n# In CI pipeline - fetch only what's needed\ngit clone --filter=blob:none --depth=1 repo\ngit backfill --sparse\n# Much faster than full clone\n```\n\n### Performance Comparison\n\n**Traditional partial clone fetch:**\n```bash\ngit fetch --unshallow\n# Downloads 500MB in random order\n# Poor delta compression\n```\n\n**With git-backfill:**\n```bash\ngit backfill\n# Downloads 150MB with optimized delta compression (70% reduction)\n# Groups objects by path for better compression\n```\n\n## Path-Walk API (New in 2.49)\n\n**What:** Internal API that groups together objects appearing at the same path, enabling much better delta compression.\n\n**How it works:** Instead of processing objects in commit order, path-walk processes them by filesystem path, allowing Git to find better delta bases.\n\n**Benefits:**\n- 50-70% better compression in partial clone scenarios\n- Faster object transfers\n- Reduced network usage\n- Optimized packfile generation\n\n**You benefit automatically when using:**\n- `git backfill`\n- `git repack` (improved in 2.49)\n- Server-side object transfers\n\n### Enable Path-Walk Optimizations\n\n```bash\n# For repack operations\ngit config pack.useBitmaps true\ngit config pack.writeBitmaps true\n\n# Repack with path-walk optimizations\ngit repack -a -d -f\n\n# Check improvement\ngit count-objects -v\n```\n\n## Performance Improvements with zlib-ng\n\n**What:** Git 2.49 includes improved performance through zlib-ng integration for compression/decompression.\n\n**Benefits:**\n- 20-30% faster compression\n- 10-15% faster decompression\n- Lower CPU usage during pack operations\n- Transparent - no configuration needed\n\n**Automatically improves:**\n- `git clone`\n- `git fetch`\n- `git push`\n- `git gc`\n- `git repack`\n\n## New Name-Hashing Algorithm\n\n**What:** Improved algorithm for selecting object pairs during delta compression.\n\n**Results:**\n- More efficient packfiles\n- Better compression ratios (5-10% improvement)\n- Faster repack operations\n\n**Automatic - no action needed.**\n\n## Rust Bindings for libgit\n\n**What:** Git 2.49 added Rust bindings (libgit-sys and libgit-rs) for Git's internal libraries.\n\n**Relevance:** Future Git tooling and performance improvements will leverage Rust for memory safety and performance.\n\n**For developers:** You can now build Git tools in Rust using official bindings.\n\n## Promisor Remote Enhancements\n\n**What:** Servers can now advertise promisor remote information to clients.\n\n**Benefits:**\n- Better handling of large files in partial clones\n- Improved lazy fetching\n- More efficient missing object retrieval\n\n**Configuration:**\n```bash\n# View promisor remote info\ngit config remote.origin.promisor\ngit config extensions.partialClone\n\n# Verify promisor packfiles\nls -lah .git/objects/pack/*.promisor\n```\n\n## Git 2.49 Workflow Examples\n\n### Example 1: Ultra-Efficient Monorepo Clone\n\n```bash\n# Clone large monorepo with maximum efficiency\ngit clone --filter=blob:none --sparse https://github.com/company/monorepo.git\ncd monorepo\n\n# Only checkout your team's service\ngit sparse-checkout set --cone services/api\n\n# Fetch needed objects with path-walk optimization\ngit backfill --sparse\n\n# Result: 95% smaller than full clone, 70% faster download\n```\n\n### Example 2: CI/CD Pipeline Optimization\n\n```yaml\n# .github/workflows/ci.yml\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout with optimizations\n        run: |\n          git clone --filter=blob:none --depth=1 --sparse ${{ github.repositoryUrl }}\n          cd repo\n          git sparse-checkout set src tests\n          git backfill --sparse\n\n      - name: Run tests\n        run: npm test\n# 80% faster than full clone in CI\n```\n\n### Example 3: Working with Huge History\n\n```bash\n# Clone repository with massive history\ngit clone --filter=blob:none https://github.com/project/with-long-history.git\ncd with-long-history\n\n# Work on recent code only (objects fetched on demand)\ngit checkout -b feature/new-feature\n\n# When you need full history\ngit backfill\n\n# Repack for optimal storage\ngit repack -a -d -f  # Uses path-walk API\n```\n\n## Deprecated Features (Removal in Git 3.0)\n\n** Now Officially Deprecated:**\n- `.git/branches/` directory (use remotes instead)\n- `.git/remotes/` directory (use git remote commands)\n\n**Migration:**\n```bash\n# If you have old-style remotes, convert them\n# Check for deprecated directories\nls -la .git/branches .git/remotes 2>/dev/null\n\n# Use modern remote configuration\ngit remote add origin https://github.com/user/repo.git\ngit config remote.origin.fetch '+refs/heads/*:refs/remotes/origin/*'\n```\n\n## Meson Build System\n\n**What:** Continued development on Meson as alternative build system for Git.\n\n**Why:** Faster builds, better cross-platform support.\n\n**Status:** Experimental - use `make` for production.\n\n## netrc Support Re-enabled\n\n**What:** HTTP transport now supports .netrc for authentication.\n\n**Usage:**\n```bash\n# ~/.netrc\nmachine github.com\n  login your-username\n  password your-token\n\n# Git will now use these credentials automatically\ngit clone https://github.com/private/repo.git\n```\n\n## Best Practices with Git 2.49\n\n1. **Use git-backfill for partial clones:**\n   ```bash\n   git backfill --sparse  # Better than git fetch --unshallow\n   ```\n\n2. **Combine optimizations:**\n   ```bash\n   git clone --filter=blob:none --sparse <url>\n   git sparse-checkout set --cone <paths>\n   git backfill --sparse\n   ```\n\n3. **Regular maintenance:**\n   ```bash\n   git backfill  # Fill in missing objects\n   git repack -a -d -f  # Optimize with path-walk\n   git prune  # Clean up\n   ```\n\n4. **Monitor partial clone status:**\n   ```bash\n   # Check promisor remotes\n   git config extensions.partialClone\n\n   # List missing objects\n   git rev-list --objects --all --missing=print | grep \"^?\"\n   ```\n\n5. **Migrate deprecated features:**\n   ```bash\n   # Move away from .git/branches and .git/remotes\n   # Use git remote commands instead\n   ```\n\n## Troubleshooting\n\n**git-backfill not found:**\n```bash\n# Verify Git version\ngit --version  # Must be 2.49+\n\n# Update Git\nbrew upgrade git  # macOS\napt update && apt install git  # Ubuntu\n```\n\n**Promisor remote issues:**\n```bash\n# Reset promisor configuration\ngit config --unset extensions.partialClone\ngit config --unset remote.origin.promisor\n\n# Re-enable\ngit config extensions.partialClone origin\ngit config remote.origin.promisor true\n```\n\n**Poor delta compression:**\n```bash\n# Force repack with path-walk optimization\ngit repack -a -d -f --depth=250 --window=250\n```\n\n## Resources\n\n- [Git 2.49 Release Notes](https://github.blog/open-source/git/highlights-from-git-2-49/)\n- [Path-Walk API Documentation](https://git-scm.com/docs/api-path-walk)\n- [Partial Clone Documentation](https://git-scm.com/docs/partial-clone)"
              },
              {
                "name": "git-2025-features",
                "description": "Git 2.49+ features including reftables, sparse-checkout, partial clone, git-backfill, and worktrees",
                "path": "plugins/git-master/skills/git-2025-features/SKILL.md",
                "frontmatter": {
                  "name": "git-2025-features",
                  "description": "Git 2.49+ features including reftables, sparse-checkout, partial clone, git-backfill, and worktrees"
                },
                "content": "** NOTE:** For detailed Git 2.49+ features (git-backfill, path-walk API, zlib-ng), see git-2-49-features.md skill.\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Git 2025 Features - Advanced Capabilities\n\n## Git 2.49 (March 2025) - Latest\n\n**Major additions:** git-backfill, path-walk API, zlib-ng performance, improved delta compression.\n\n**See git-2-49-features.md for complete coverage.**\n\n## Git 2.48-2.49 Features\n\n### Reftables Migration (Completed in 2.48)\n\n**What:** New reference storage format replacing loose ref files and packed-refs.\n\n**Benefits:**\n- Faster ref operations (50-80% improvement)\n- Atomic ref updates\n- Better scalability for repositories with many refs\n- Reflogs fully migratable (completed in 2.48)\n\n**Migration:**\n\n```bash\n# Check current ref storage format\ngit config core.refStorage\n\n# Migrate to reftables\ngit refs migrate --ref-storage=reftables\n\n# Verify migration\ngit fsck --full\ngit log --oneline -5\n\n# Roll back if needed (before critical operations)\ngit refs migrate --ref-storage=files\n```\n\n**When to use:**\n- Repositories with 10,000+ refs\n- High-frequency branch operations\n- CI/CD systems creating many temporary refs\n- Monorepos with extensive branching\n\n### Performance Milestones (2.48-2.49)\n\n**Git 2.48:**\n- Memory leak free status achieved\n- Stable memory usage in long-running operations\n\n**Git 2.49:**\n- zlib-ng integration: 20-30% faster compression\n- Path-walk API: 50-70% better delta compression\n- New name-hashing algorithm for optimal packfiles\n\nBenefits automatically in:\n- Large repository clones\n- Extended rebase sessions\n- Bulk operations (filter-repo, GC, repack)\n\n## Sparse-Checkout (Enhanced in 2.48)\n\n**What:** Check out only a subset of files from repository.\n\n**Use cases:**\n- Monorepos (work on one service)\n- Large repositories (reduce disk usage)\n- Build systems (fetch only needed files)\n\n**Cone Mode (Default - Recommended):**\n\n```bash\n# Clone with sparse-checkout\ngit clone --filter=blob:none --sparse <repo-url>\ncd <repo>\n\n# Initialize sparse-checkout in cone mode\ngit sparse-checkout init --cone\n\n# Add directories to checkout\ngit sparse-checkout set src/api src/shared docs\n\n# Add more directories\ngit sparse-checkout add tests/integration\n\n# View current patterns\ngit sparse-checkout list\n\n# Check what would be matched\ngit sparse-checkout check-rules src/api/users.ts\n\n# Disable sparse-checkout\ngit sparse-checkout disable\n```\n\n**Advanced Patterns (Non-Cone Mode):**\n\n```bash\n# Enable pattern mode\ngit sparse-checkout init --no-cone\n\n# Add patterns (one per line)\ngit sparse-checkout set \\\n  \"*.md\" \\\n  \"src/api/*\" \\\n  \"!src/api/legacy/*\"\n\n# Read patterns from file\ngit sparse-checkout set --stdin < patterns.txt\n```\n\n**Reapply Rules:**\n\n```bash\n# After merge/rebase that materialized unwanted files\ngit sparse-checkout reapply\n```\n\n## Partial Clone\n\n**What:** Clone repository without downloading all objects initially.\n\n**Filters:**\n\n1. **blob:none** - Defer all blobs (fastest, smallest)\n2. **tree:0** - Defer all trees and blobs\n3. **blob:limit=1m** - Defer blobs larger than 1MB\n\n**Usage:**\n\n```bash\n# Clone without blobs (fetch on demand)\ngit clone --filter=blob:none <repo-url>\n\n# Clone without large files\ngit clone --filter=blob:limit=10m <repo-url>\n\n# Combine with sparse-checkout\ngit clone --filter=blob:none --sparse <repo-url>\ncd <repo>\ngit sparse-checkout set src/api\n\n# Convert existing repository to partial clone\ngit config extensions.partialClone origin\ngit config remote.origin.promisor true\ngit fetch --filter=blob:none\n\n# Prefetch all missing objects\ngit fetch --unshallow\n```\n\n**Combine Partial Clone + Sparse-Checkout:**\n\n```bash\n# Ultimate efficiency: Only objects for specific directories\ngit clone --filter=blob:none --sparse <repo-url>\ncd <repo>\ngit sparse-checkout set --cone src/api\ngit checkout main\n\n# Result: Only have objects for src/api\n```\n\n**Check promisor objects:**\n\n```bash\n# Verify partial clone status\ngit config extensions.partialClone\n\n# See promisor packfiles\nls -lah .git/objects/pack/*.promisor\n\n# Force fetch specific object\ngit rev-list --objects --missing=print HEAD | grep \"^?\"\n```\n\n## Git Worktrees\n\n**What:** Multiple working directories from one repository.\n\n**Benefits:**\n- Work on multiple branches simultaneously\n- No need to stash/commit before switching\n- Parallel work (review PR while coding)\n- Shared .git (one fetch updates all)\n\n**Basic Operations:**\n\n```bash\n# List worktrees\ngit worktree list\n\n# Create worktree for existing branch\ngit worktree add ../project-feature feature-branch\n\n# Create worktree with new branch\ngit worktree add -b new-feature ../project-new-feature\n\n# Create worktree from remote branch\ngit worktree add ../project-fix origin/fix-bug\n\n# Remove worktree\ngit worktree remove ../project-feature\n\n# Clean up stale worktree references\ngit worktree prune\n```\n\n**Advanced Patterns:**\n\n```bash\n# Worktree for PR review while coding\ngit worktree add ../myproject-pr-123 origin/pull/123/head\ncd ../myproject-pr-123\n# Review PR in separate directory\ncd -\n# Continue coding in main worktree\n\n# Worktree for hotfix\ngit worktree add --detach ../myproject-hotfix v1.2.3\ncd ../myproject-hotfix\n# Make hotfix\ngit switch -c hotfix/security-patch\ngit commit -am \"fix: patch vulnerability\"\ngit push -u origin hotfix/security-patch\n\n# Worktree organization\nmkdir -p ~/worktrees/myproject\ngit worktree add ~/worktrees/myproject/feature-a -b feature-a\ngit worktree add ~/worktrees/myproject/feature-b -b feature-b\ngit worktree add ~/worktrees/myproject/pr-review origin/pull/42/head\n```\n\n**Best Practices:**\n\n1. **Organize directory structure:**\n```bash\n~/projects/\n  myproject/           # Main worktree\n  myproject-feature/   # Feature worktree\n  myproject-review/    # Review worktree\n```\n\n2. **Clean up regularly:**\n```bash\n# Remove merged worktrees\ngit worktree list | grep feature | while read wt branch commit; do\n  if git branch --merged | grep -q \"$branch\"; then\n    git worktree remove \"$wt\"\n  fi\ndone\n```\n\n3. **Shared configuration:**\n- .git/config applies to all worktrees\n- .git/info/exclude applies to all worktrees\n- Each worktree has own index and HEAD\n\n## Scalar (Large Repository Tool)\n\n**What:** Tool for optimizing very large repositories (Microsoft-developed).\n\n```bash\n# Install scalar (comes with Git 2.47+)\nscalar register <path>\n\n# Clone with scalar optimizations\nscalar clone --branch main <repo-url>\n\n# Enables automatically:\n# - Sparse-checkout (cone mode)\n# - Partial clone (blob:none)\n# - Multi-pack-index\n# - Commit-graph\n# - Background maintenance\n\n# Unregister\nscalar unregister <path>\n\n# Delete repository\nscalar delete <path>\n```\n\n## Git Backfill (Experimental)\n\n**What:** Background process to fetch missing objects in partial clone.\n\n```bash\n# Fetch missing blobs in background\ngit backfill\n\n# Configure batch size\ngit backfill --min-batch-size=1000\n\n# Respect sparse-checkout patterns\ngit backfill --sparse\n```\n\n## Performance Comparison\n\n**Traditional Clone:**\n```bash\ngit clone large-repo\n# Size: 5GB, Time: 10 minutes\n```\n\n**Sparse-Checkout:**\n```bash\ngit clone --sparse large-repo\ngit sparse-checkout set src/api\n# Size: 500MB, Time: 3 minutes\n```\n\n**Partial Clone:**\n```bash\ngit clone --filter=blob:none large-repo\n# Size: 100MB, Time: 1 minute\n```\n\n**Partial Clone + Sparse-Checkout:**\n```bash\ngit clone --filter=blob:none --sparse large-repo\ngit sparse-checkout set src/api\n# Size: 50MB, Time: 30 seconds\n```\n\n## When to Use Each Feature\n\n**Sparse-Checkout:**\n-  Monorepos\n-  Working on specific services/modules\n-  Limited disk space\n-  Need entire codebase often\n\n**Partial Clone:**\n-  CI/CD pipelines\n-  Large repositories\n-  Good network connectivity\n-  Offline work frequently\n\n**Worktrees:**\n-  Parallel development\n-  PR reviews during work\n-  Multiple branch testing\n-  Low disk space\n\n**Combine All:**\n-  Massive monorepos (Google scale)\n-  Multiple simultaneous tasks\n-  Minimal local storage\n-  Fast network connection\n\n## Troubleshooting\n\n**Sparse-checkout not working:**\n```bash\n# Verify configuration\ngit config core.sparseCheckout\ngit config core.sparseCheckoutCone\n\n# Re-apply patterns\ngit sparse-checkout reapply\n\n# Check patterns\ngit sparse-checkout list\n```\n\n**Missing objects in partial clone:**\n```bash\n# Fetch specific object\ngit fetch origin <commit>\n\n# Fetch all missing\ngit fetch --unshallow\n\n# Verify promisor config\ngit config extensions.partialClone\n```\n\n**Worktree issues:**\n```bash\n# Locked worktree\ngit worktree unlock <path>\n\n# Corrupted worktree\ngit worktree remove --force <path>\ngit worktree prune\n\n# Branch already checked out\ngit checkout --ignore-other-worktrees <branch>\n```\n\n## Migration Guide\n\n**From traditional to optimized workflow:**\n\n```bash\n# 1. Current large clone\ncd large-project\ndu -sh .git  # 5GB\n\n# 2. Create optimized new clone\ncd ..\ngit clone --filter=blob:none --sparse large-project-new\ncd large-project-new\ngit sparse-checkout set src/api src/shared\n\n# 3. Verify size\ndu -sh .git  # 50MB\n\n# 4. Switch workflow\ncd ../large-project-new\n\n# 5. Delete old clone when comfortable\nrm -rf ../large-project\n```\n\n## Resources\n\n- [Git Partial Clone Documentation](https://git-scm.com/docs/partial-clone)\n- [Git Sparse-Checkout Guide](https://github.blog/open-source/git/bring-your-monorepo-down-to-size-with-sparse-checkout/)\n- [Git Worktree Best Practices](https://git-scm.com/docs/git-worktree)\n- [Scalar Documentation](https://github.com/microsoft/scalar)"
              },
              {
                "name": "git-master",
                "description": "Complete Git expertise system for ALL git operations. PROACTIVELY activate for: (1) ANY Git task (basic/advanced/dangerous), (2) Repository management, (3) Branch strategies and workflows, (4) Conflict resolution, (5) History rewriting/recovery, (6) Platform-specific operations (GitHub/Azure DevOps/Bitbucket), (7) Advanced commands (rebase/cherry-pick/filter-repo). Provides: complete Git command reference, safety guardrails for destructive operations, platform best practices, workflow strategies, reflog recovery techniques, and expert guidance for even the most risky operations. Always asks user preference for automatic commits vs manual control.",
                "path": "plugins/git-master/skills/git-master/SKILL.md",
                "frontmatter": {
                  "name": "git-master",
                  "description": "Complete Git expertise system for ALL git operations. PROACTIVELY activate for: (1) ANY Git task (basic/advanced/dangerous), (2) Repository management, (3) Branch strategies and workflows, (4) Conflict resolution, (5) History rewriting/recovery, (6) Platform-specific operations (GitHub/Azure DevOps/Bitbucket), (7) Advanced commands (rebase/cherry-pick/filter-repo). Provides: complete Git command reference, safety guardrails for destructive operations, platform best practices, workflow strategies, reflog recovery techniques, and expert guidance for even the most risky operations. Always asks user preference for automatic commits vs manual control."
                },
                "content": "# Git Mastery - Complete Git Expertise\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nComprehensive guide for ALL Git operations from basic to advanced, including dangerous operations with safety guardrails.\n\n---\n\n## TL;DR QUICK REFERENCE\n\n**Safety First - Before ANY Destructive Operation:**\n```bash\n# ALWAYS check status first\ngit status\ngit log --oneline -10\n\n# For risky operations, create a safety branch\ngit branch backup-$(date +%Y%m%d-%H%M%S)\n\n# Remember: git reflog is your safety net (90 days default)\ngit reflog\n```\n\n**User Preference Check:**\n- **ALWAYS ASK:** \"Would you like me to create commits automatically, or would you prefer to handle commits manually?\"\n- Respect user's choice throughout the session\n\n---\n\n## Overview\n\nThis skill provides COMPLETE Git expertise for ANY Git operation, no matter how advanced, niche, or risky. It covers:\n\n**MUST use this skill for:**\n-  ANY Git command or operation\n-  Repository initialization, cloning, configuration\n-  Branch management and strategies\n-  Commit workflows and best practices\n-  Merge strategies and conflict resolution\n-  Rebase operations (interactive and non-interactive)\n-  History rewriting (filter-repo, reset, revert)\n-  Recovery operations (reflog, fsck)\n-  Dangerous operations (force push, hard reset)\n-  Platform-specific workflows (GitHub, Azure DevOps, Bitbucket)\n-  Advanced features (submodules, worktrees, hooks)\n-  Performance optimization\n-  Cross-platform compatibility (Windows/Linux/macOS)\n\n---\n\n## Core Principles\n\n### 1. Safety Guardrails for Destructive Operations\n\n**CRITICAL: Before ANY destructive operation (reset --hard, force push, filter-repo, etc.):**\n\n1. **Always warn the user explicitly**\n2. **Explain the risks clearly**\n3. **Ask for confirmation**\n4. **Suggest creating a backup branch first**\n5. **Provide recovery instructions**\n\n```bash\n# Example safety pattern for dangerous operations\necho \"  WARNING: This operation is DESTRUCTIVE and will:\"\necho \"   - Permanently delete uncommitted changes\"\necho \"   - Rewrite Git history\"\necho \"   - [specific risks for the operation]\"\necho \"\"\necho \"Safety recommendation: Creating backup branch first...\"\ngit branch backup-before-reset-$(date +%Y%m%d-%H%M%S)\necho \"\"\necho \"To recover if needed: git reset --hard backup-before-reset-XXXXXXXX\"\necho \"\"\nread -p \"Are you SURE you want to proceed? (yes/NO): \" confirm\nif [[ \"$confirm\" != \"yes\" ]]; then\n    echo \"Operation cancelled.\"\n    exit 1\nfi\n```\n\n### 2. Commit Creation Policy\n\n**ALWAYS ASK at the start of ANY Git task:**\n\"Would you like me to:\n1. Create commits automatically with appropriate messages\n2. Stage changes only (you handle commits manually)\n3. Just provide guidance (no automatic operations)\"\n\nRespect this choice throughout the session.\n\n### 3. Platform Awareness\n\nGit behavior and workflows differ across platforms and hosting providers:\n\n**Windows (Git Bash/PowerShell):**\n- Line ending handling (core.autocrlf)\n- Path separators and case sensitivity\n- Credential management (Windows Credential Manager)\n\n**Linux/macOS:**\n- Case-sensitive filesystems\n- SSH key management\n- Permission handling\n\n**Hosting Platforms:**\n- GitHub: Pull requests, GitHub Actions, GitHub CLI\n- Azure DevOps: Pull requests, Azure Pipelines, policies\n- Bitbucket: Pull requests, Bitbucket Pipelines, Jira integration\n- GitLab: Merge requests, GitLab CI/CD\n\n---\n\n## Basic Git Operations\n\n### Repository Initialization and Cloning\n\n```bash\n# Initialize new repository\ngit init\ngit init --initial-branch=main  # Specify default branch name\n\n# Clone repository\ngit clone <url>\ngit clone <url> <directory>\ngit clone --depth 1 <url>  # Shallow clone (faster, less history)\ngit clone --branch <branch> <url>  # Clone specific branch\ngit clone --recurse-submodules <url>  # Include submodules\n```\n\n### Configuration\n\n```bash\n# User identity (required for commits)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Default branch name\ngit config --global init.defaultBranch main\n\n# Line ending handling (Windows)\ngit config --global core.autocrlf true  # Windows\ngit config --global core.autocrlf input  # macOS/Linux\n\n# Editor\ngit config --global core.editor \"code --wait\"  # VS Code\ngit config --global core.editor \"vim\"\n\n# Diff tool\ngit config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\n\n# Merge tool\ngit config --global merge.tool vscode\ngit config --global mergetool.vscode.cmd 'code --wait $MERGED'\n\n# Aliases\ngit config --global alias.st status\ngit config --global alias.co checkout\ngit config --global alias.br branch\ngit config --global alias.ci commit\ngit config --global alias.unstage 'reset HEAD --'\ngit config --global alias.last 'log -1 HEAD'\ngit config --global alias.visual '!gitk'\n\n# View configuration\ngit config --list\ngit config --global --list\ngit config --local --list\ngit config user.name  # Get specific value\n```\n\n### Basic Workflow\n\n```bash\n# Check status\ngit status\ngit status -s  # Short format\ngit status -sb  # Short with branch info\n\n# Add files\ngit add <file>\ngit add .  # Add all changes in current directory\ngit add -A  # Add all changes in repository\ngit add -p  # Interactive staging (patch mode)\n\n# Remove files\ngit rm <file>\ngit rm --cached <file>  # Remove from index, keep in working directory\ngit rm -r <directory>\n\n# Move/rename files\ngit mv <old> <new>\n\n# Commit\ngit commit -m \"message\"\ngit commit -am \"message\"  # Add and commit tracked files\ngit commit --amend  # Amend last commit\ngit commit --amend --no-edit  # Amend without changing message\ngit commit --allow-empty -m \"message\"  # Empty commit (useful for triggers)\n\n# View history\ngit log\ngit log --oneline\ngit log --graph --oneline --all --decorate\ngit log --stat  # Show file statistics\ngit log --patch  # Show diffs\ngit log -p -2  # Show last 2 commits with diffs\ngit log --since=\"2 weeks ago\"\ngit log --until=\"2025-01-01\"\ngit log --author=\"Name\"\ngit log --grep=\"pattern\"\ngit log -- <file>  # History of specific file\ngit log --follow <file>  # Follow renames\n\n# Show changes\ngit diff  # Unstaged changes\ngit diff --staged  # Staged changes\ngit diff HEAD  # All changes since last commit\ngit diff <branch>  # Compare with another branch\ngit diff <commit1> <commit2>\ngit diff <commit>  # Changes since specific commit\ngit diff <branch1>...<branch2>  # Changes between branches\n\n# Show commit details\ngit show <commit>\ngit show <commit>:<file>  # Show file at specific commit\n```\n\n---\n\n## Branch Management\n\n### Creating and Switching Branches\n\n```bash\n# List branches\ngit branch  # Local branches\ngit branch -r  # Remote branches\ngit branch -a  # All branches\ngit branch -v  # With last commit info\ngit branch -vv  # With tracking info\n\n# Create branch\ngit branch <branch-name>\ngit branch <branch-name> <start-point>  # From specific commit/tag\n\n# Switch branch\ngit switch <branch-name>\ngit checkout <branch-name>  # Old syntax, still works\n\n# Create and switch\ngit switch -c <branch-name>\ngit checkout -b <branch-name>\ngit switch -c <branch-name> <start-point>\n\n# Delete branch\ngit branch -d <branch-name>  # Safe delete (only if merged)\ngit branch -D <branch-name>  # Force delete (even if not merged)\n\n# Rename branch\ngit branch -m <old-name> <new-name>\ngit branch -m <new-name>  # Rename current branch\n\n# Set upstream tracking\ngit branch --set-upstream-to=origin/<branch>\ngit branch -u origin/<branch>\n```\n\n### Branch Strategies\n\n**Git Flow:**\n- `main/master`: Production-ready code\n- `develop`: Integration branch for features\n- `feature/*`: New features\n- `release/*`: Release preparation\n- `hotfix/*`: Production fixes\n\n**GitHub Flow:**\n- `main`: Always deployable\n- `feature/*`: Short-lived feature branches\n- Create PR, review, merge\n\n**Trunk-Based Development:**\n- `main`: Single branch\n- Short-lived feature branches (< 1 day)\n- Feature flags for incomplete features\n\n**GitLab Flow:**\n- Environment branches: `production`, `staging`, `main`\n- Feature branches merge to `main`\n- Deploy from environment branches\n\n---\n\n## Merging and Rebasing\n\n### Merge Strategies\n\n```bash\n# Fast-forward merge (default if possible)\ngit merge <branch>\n\n# Force merge commit (even if fast-forward possible)\ngit merge --no-ff <branch>\n\n# Squash merge (combine all commits into one)\ngit merge --squash <branch>\n# Then commit manually: git commit -m \"Merged feature X\"\n\n# Merge with specific strategy\ngit merge -s recursive <branch>  # Default strategy\ngit merge -s ours <branch>  # Always use \"our\" version\ngit merge -s theirs <branch>  # Always use \"their\" version (requires merge-theirs)\ngit merge -s octopus <branch1> <branch2> <branch3>  # Merge multiple branches\n\n# Merge with strategy options\ngit merge -X ours <branch>  # Prefer \"our\" changes in conflicts\ngit merge -X theirs <branch>  # Prefer \"their\" changes in conflicts\ngit merge -X ignore-all-space <branch>\ngit merge -X ignore-space-change <branch>\n\n# Abort merge\ngit merge --abort\n\n# Continue after resolving conflicts\ngit merge --continue\n```\n\n### Conflict Resolution\n\n```bash\n# When merge conflicts occur\ngit status  # See conflicted files\n\n# Conflict markers in files:\n# <<<<<<< HEAD\n# Your changes\n# =======\n# Their changes\n# >>>>>>> branch-name\n\n# Resolve conflicts manually, then:\ngit add <resolved-file>\ngit commit  # Complete the merge\n\n# Use mergetool\ngit mergetool\n\n# Accept one side completely\ngit checkout --ours <file>  # Keep our version\ngit checkout --theirs <file>  # Keep their version\ngit add <file>\n\n# View conflict diff\ngit diff  # Show conflicts\ngit diff --ours  # Compare with our version\ngit diff --theirs  # Compare with their version\ngit diff --base  # Compare with base version\n\n# List conflicts\ngit diff --name-only --diff-filter=U\n```\n\n### Rebase Operations\n\n**  WARNING: Rebase rewrites history. Never rebase commits that have been pushed to shared branches!**\n\n```bash\n# Basic rebase\ngit rebase <base-branch>\ngit rebase origin/main\n\n# Interactive rebase (POWERFUL)\ngit rebase -i <base-commit>\ngit rebase -i HEAD~5  # Last 5 commits\n\n# Interactive rebase commands:\n# p, pick = use commit\n# r, reword = use commit, but edit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like squash, but discard commit message\n# x, exec = run command (rest of line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop = remove commit\n# l, label = label current HEAD with a name\n# t, reset = reset HEAD to a label\n\n# Rebase onto different base\ngit rebase --onto <new-base> <old-base> <branch>\n\n# Continue after resolving conflicts\ngit rebase --continue\n\n# Skip current commit\ngit rebase --skip\n\n# Abort rebase\ngit rebase --abort\n\n# Preserve merge commits\ngit rebase --preserve-merges <base>  # Deprecated\ngit rebase --rebase-merges <base>  # Modern approach\n\n# Autosquash (with fixup commits)\ngit commit --fixup <commit>\ngit rebase -i --autosquash <base>\n```\n\n### Cherry-Pick\n\n```bash\n# Apply specific commit to current branch\ngit cherry-pick <commit>\n\n# Cherry-pick multiple commits\ngit cherry-pick <commit1> <commit2>\ngit cherry-pick <commit1>..<commit5>\n\n# Cherry-pick without committing\ngit cherry-pick -n <commit>\ngit cherry-pick --no-commit <commit>\n\n# Continue after resolving conflicts\ngit cherry-pick --continue\n\n# Abort cherry-pick\ngit cherry-pick --abort\n```\n\n---\n\n## Remote Operations\n\n### Remote Management\n\n```bash\n# List remotes\ngit remote\ngit remote -v  # With URLs\n\n# Add remote\ngit remote add <name> <url>\ngit remote add origin https://github.com/user/repo.git\n\n# Change remote URL\ngit remote set-url <name> <new-url>\n\n# Remove remote\ngit remote remove <name>\ngit remote rm <name>\n\n# Rename remote\ngit remote rename <old> <new>\n\n# Show remote info\ngit remote show <name>\ngit remote show origin\n\n# Prune stale remote branches\ngit remote prune origin\ngit fetch --prune\n```\n\n### Fetch and Pull\n\n```bash\n# Fetch from remote (doesn't merge)\ngit fetch\ngit fetch origin\ngit fetch --all  # All remotes\ngit fetch --prune  # Remove stale remote-tracking branches\n\n# Pull (fetch + merge)\ngit pull\ngit pull origin <branch>\ngit pull --rebase  # Fetch + rebase instead of merge\ngit pull --no-ff  # Always create merge commit\ngit pull --ff-only  # Only if fast-forward possible\n\n# Set default pull behavior\ngit config --global pull.rebase true  # Always rebase\ngit config --global pull.ff only  # Only fast-forward\n```\n\n### Push\n\n```bash\n# Push to remote\ngit push\ngit push origin <branch>\ngit push origin <local-branch>:<remote-branch>\n\n# Push new branch and set upstream\ngit push -u origin <branch>\ngit push --set-upstream origin <branch>\n\n# Push all branches\ngit push --all\n\n# Push tags\ngit push --tags\ngit push origin <tag-name>\n\n# Delete remote branch\ngit push origin --delete <branch>\ngit push origin :<branch>  # Old syntax\n\n# Delete remote tag\ngit push origin --delete <tag>\ngit push origin :refs/tags/<tag>\n\n#  DANGEROUS: Force push (overwrites remote history)\n# ALWAYS ASK USER FOR CONFIRMATION FIRST\ngit push --force\ngit push -f\n\n#  SAFER: Force push with lease (fails if remote updated)\ngit push --force-with-lease\ngit push --force-with-lease=<ref>:<expected-value>\n```\n\n**Force Push Safety Protocol:**\n\nBefore ANY force push, execute this safety check:\n\n```bash\necho \"  DANGER: Force push will overwrite remote history!\"\necho \"\"\necho \"Remote branch status:\"\ngit fetch origin\ngit log --oneline origin/<branch> ^<branch> --decorate\n\nif [ -z \"$(git log --oneline origin/<branch> ^<branch>)\" ]; then\n    echo \" No commits will be lost (remote is behind local)\"\nelse\n    echo \" WARNING: Remote has commits that will be LOST:\"\n    git log --oneline --decorate origin/<branch> ^<branch>\n    echo \"\"\n    echo \"These commits from other developers will be destroyed!\"\nfi\n\necho \"\"\necho \"Consider using --force-with-lease instead of --force\"\necho \"\"\nread -p \"Type 'force push' to confirm: \" confirm\nif [[ \"$confirm\" != \"force push\" ]]; then\n    echo \"Cancelled.\"\n    exit 1\nfi\n```\n\n---\n\n## Advanced Commands\n\n### Stash\n\n```bash\n# Stash changes\ngit stash\ngit stash save \"message\"\ngit stash push -m \"message\"\n\n# Stash including untracked files\ngit stash -u\ngit stash --include-untracked\n\n# Stash including ignored files\ngit stash -a\ngit stash --all\n\n# List stashes\ngit stash list\n\n# Show stash contents\ngit stash show\ngit stash show -p  # With diff\ngit stash show stash@{2}\n\n# Apply stash (keep in stash list)\ngit stash apply\ngit stash apply stash@{2}\n\n# Pop stash (apply and remove)\ngit stash pop\ngit stash pop stash@{2}\n\n# Drop stash\ngit stash drop\ngit stash drop stash@{2}\n\n# Clear all stashes\ngit stash clear\n\n# Create branch from stash\ngit stash branch <branch-name>\ngit stash branch <branch-name> stash@{1}\n\n# Git 2.51+ : Import/Export stashes (share stashes between machines)\n# Export stash to a file\ngit stash store --file=stash.patch stash@{0}\n\n# Import stash from a file\ngit stash import --file=stash.patch\n\n# Share stashes like branches/tags\ngit stash export > my-stash.patch\ngit stash import < my-stash.patch\n```\n\n### Reset\n\n**  WARNING: reset can permanently delete changes!**\n\n```bash\n# Soft reset (keep changes staged)\ngit reset --soft <commit>\ngit reset --soft HEAD~1  # Undo last commit, keep changes staged\n\n# Mixed reset (default - keep changes unstaged)\ngit reset <commit>\ngit reset HEAD~1  # Undo last commit, keep changes unstaged\n\n#  HARD reset (DELETE all changes - DANGEROUS!)\n# ALWAYS create backup branch first!\ngit branch backup-$(date +%Y%m%d-%H%M%S)\ngit reset --hard <commit>\ngit reset --hard HEAD~1  # Undo last commit and DELETE all changes\ngit reset --hard origin/<branch>  # Reset to remote state\n\n# Unstage files\ngit reset HEAD <file>\ngit reset -- <file>\n\n# Reset specific file to commit\ngit checkout <commit> -- <file>\n```\n\n### Revert\n\n```bash\n# Revert commit (creates new commit that undoes changes)\n# Safer than reset for shared branches\ngit revert <commit>\n\n# Revert without creating commit\ngit revert -n <commit>\ngit revert --no-commit <commit>\n\n# Revert merge commit\ngit revert -m 1 <merge-commit>  # Keep first parent\ngit revert -m 2 <merge-commit>  # Keep second parent\n\n# Revert multiple commits\ngit revert <commit1> <commit2>\ngit revert <commit1>..<commit5>\n\n# Continue after resolving conflicts\ngit revert --continue\n\n# Abort revert\ngit revert --abort\n```\n\n### Reflog (Recovery)\n\n**reflog is your safety net - it tracks all HEAD movements for 90 days (default)**\n\n```bash\n# View reflog\ngit reflog\ngit reflog show\ngit reflog show <branch>\n\n# More detailed reflog\ngit log -g  # Reflog as log\ngit log -g --all\n\n# Find lost commits\ngit reflog --all\ngit fsck --lost-found\n\n# Recover deleted branch\ngit reflog  # Find commit where branch existed\ngit branch <branch-name> <commit-hash>\n\n# Recover from hard reset\ngit reflog  # Find commit before reset\ngit reset --hard <commit-hash>\n\n# Recover deleted commits\ngit cherry-pick <commit-hash>\n\n# Reflog expiration (change retention)\ngit config gc.reflogExpire \"90 days\"\ngit config gc.reflogExpireUnreachable \"30 days\"\n```\n\n### Bisect (Find Bad Commits)\n\n```bash\n# Start bisect\ngit bisect start\n\n# Mark current commit as bad\ngit bisect bad\n\n# Mark known good commit\ngit bisect good <commit>\n\n# Test each commit, then mark as good or bad\ngit bisect good  # Current commit is good\ngit bisect bad   # Current commit is bad\n\n# Automate with test script\ngit bisect run <test-script>\n\n# Bisect shows the first bad commit\n\n# Finish bisect\ngit bisect reset\n\n# Skip commit if unable to test\ngit bisect skip\n```\n\n### Clean\n\n**  WARNING: clean permanently deletes untracked files!**\n\n```bash\n# Show what would be deleted (dry run - ALWAYS do this first!)\ngit clean -n\ngit clean --dry-run\n\n# Delete untracked files\ngit clean -f\n\n# Delete untracked files and directories\ngit clean -fd\n\n# Delete untracked and ignored files\ngit clean -fdx\n\n# Interactive clean\ngit clean -i\n```\n\n### Worktrees\n\n```bash\n# List worktrees\ngit worktree list\n\n# Add new worktree\ngit worktree add <path> <branch>\ngit worktree add ../project-feature feature-branch\n\n# Add worktree for new branch\ngit worktree add -b <new-branch> <path>\n\n# Remove worktree\ngit worktree remove <path>\n\n# Prune stale worktrees\ngit worktree prune\n```\n\n### Submodules\n\n```bash\n# Add submodule\ngit submodule add <url> <path>\n\n# Initialize submodules (after clone)\ngit submodule init\ngit submodule update\n\n# Clone with submodules\ngit clone --recurse-submodules <url>\n\n# Update submodules\ngit submodule update --remote\ngit submodule update --init --recursive\n\n# Execute command in all submodules\ngit submodule foreach <command>\ngit submodule foreach git pull origin main\n\n# Remove submodule\ngit submodule deinit <path>\ngit rm <path>\nrm -rf .git/modules/<path>\n```\n\n---\n\n## Dangerous Operations (High Risk)\n\n### Filter-Repo (History Rewriting)\n\n**  EXTREMELY DANGEROUS: Rewrites entire repository history!**\n\n```bash\n# Install git-filter-repo (not built-in)\n# pip install git-filter-repo\n\n# Remove file from all history\ngit filter-repo --path <file> --invert-paths\n\n# Remove directory from all history\ngit filter-repo --path <directory> --invert-paths\n\n# Change author info\ngit filter-repo --name-callback 'return name.replace(b\"Old Name\", b\"New Name\")'\ngit filter-repo --email-callback 'return email.replace(b\"old@email.com\", b\"new@email.com\")'\n\n# Remove large files\ngit filter-repo --strip-blobs-bigger-than 10M\n\n#  After filter-repo, force push required\ngit push --force --all\ngit push --force --tags\n```\n\n**Safety protocol for filter-repo:**\n\n```bash\necho \"  EXTREME DANGER  \"\necho \"This operation will:\"\necho \"  - Rewrite ENTIRE repository history\"\necho \"  - Change ALL commit hashes\"\necho \"  - Break all existing clones\"\necho \"  - Require all team members to re-clone\"\necho \"  - Cannot be undone after force push\"\necho \"\"\necho \"MANDATORY: Create full backup:\"\ngit clone --mirror <repo-url> backup-$(date +%Y%m%d-%H%M%S)\necho \"\"\necho \"Notify ALL team members before proceeding!\"\necho \"\"\nread -p \"Type 'I UNDERSTAND THE RISKS' to continue: \" confirm\nif [[ \"$confirm\" != \"I UNDERSTAND THE RISKS\" ]]; then\n    echo \"Cancelled.\"\n    exit 1\nfi\n```\n\n### Amend Pushed Commits\n\n**  DANGER: Changing pushed commits requires force push!**\n\n```bash\n# Amend last commit\ngit commit --amend\n\n# Amend without changing message\ngit commit --amend --no-edit\n\n# Change author of last commit\ngit commit --amend --author=\"Name <email>\"\n\n#  Force push required if already pushed\ngit push --force-with-lease\n```\n\n### Rewrite Multiple Commits\n\n**  DANGER: Interactive rebase on pushed commits!**\n\n```bash\n# Interactive rebase\ngit rebase -i HEAD~5\n\n# Change author of older commits\ngit rebase -i <commit>^\n# Mark commit as \"edit\"\n# When stopped:\ngit commit --amend --author=\"Name <email>\" --no-edit\ngit rebase --continue\n\n#  Force push required\ngit push --force-with-lease\n```\n\n---\n\n## Platform-Specific Workflows\n\n### GitHub\n\n**Pull Requests:**\n```bash\n# Install GitHub CLI\n# https://cli.github.com/\n\n# Create PR\ngh pr create\ngh pr create --title \"Title\" --body \"Description\"\ngh pr create --base main --head feature-branch\n\n# List PRs\ngh pr list\n\n# View PR\ngh pr view\ngh pr view <number>\n\n# Check out PR locally\ngh pr checkout <number>\n\n# Review PR\ngh pr review\ngh pr review --approve\ngh pr review --request-changes\ngh pr review --comment\n\n# Merge PR\ngh pr merge\ngh pr merge --squash\ngh pr merge --rebase\ngh pr merge --merge\n\n# Close PR\ngh pr close <number>\n```\n\n**GitHub Actions:**\n```yaml\n# .github/workflows/ci.yml\nname: CI\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run tests\n        run: npm test\n```\n\n### Azure DevOps\n\n**Pull Requests:**\n```bash\n# Install Azure DevOps CLI\n# https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\n\n# Create PR\naz repos pr create --title \"Title\" --description \"Description\"\naz repos pr create --source-branch feature --target-branch main\n\n# List PRs\naz repos pr list\n\n# View PR\naz repos pr show --id <id>\n\n# Complete PR\naz repos pr update --id <id> --status completed\n\n# Branch policies\naz repos policy list\naz repos policy create --config policy.json\n```\n\n**Azure Pipelines:**\n```yaml\n# azure-pipelines.yml\ntrigger:\n  - main\npool:\n  vmImage: 'ubuntu-latest'\nsteps:\n  - script: npm test\n    displayName: 'Run tests'\n```\n\n### Bitbucket\n\n**Pull Requests:**\n```bash\n# Create PR (via web or Bitbucket CLI)\nbb pr create\n\n# Review PR\nbb pr list\nbb pr view <id>\n\n# Merge PR\nbb pr merge <id>\n```\n\n**Bitbucket Pipelines:**\n```yaml\n# bitbucket-pipelines.yml\npipelines:\n  default:\n    - step:\n        script:\n          - npm test\n```\n\n### GitLab\n\n**Merge Requests:**\n```bash\n# Install GitLab CLI (glab)\n# https://gitlab.com/gitlab-org/cli\n\n# Create MR\nglab mr create\nglab mr create --title \"Title\" --description \"Description\"\n\n# List MRs\nglab mr list\n\n# View MR\nglab mr view <id>\n\n# Merge MR\nglab mr merge <id>\n\n# Close MR\nglab mr close <id>\n```\n\n**GitLab CI:**\n```yaml\n# .gitlab-ci.yml\nstages:\n  - test\ntest:\n  stage: test\n  script:\n    - npm test\n```\n\n---\n\n## Performance Optimization\n\n### Repository Maintenance\n\n```bash\n# Garbage collection\ngit gc\ngit gc --aggressive  # More thorough, slower\n\n# Prune unreachable objects\ngit prune\n\n# Verify repository\ngit fsck\ngit fsck --full\n\n# Optimize repository\ngit repack -a -d --depth=250 --window=250\n\n# Git 2.51+: Path-walk repacking (generates smaller packs)\n# More efficient delta compression by walking paths\ngit repack --path-walk -a -d\n\n# Count objects\ngit count-objects -v\n\n# Repository size\ndu -sh .git\n```\n\n### Large Files\n\n```bash\n# Find large files in history\ngit rev-list --objects --all |\n  git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' |\n  sed -n 's/^blob //p' |\n  sort --numeric-sort --key=2 |\n  tail -n 10\n\n# Git LFS (Large File Storage)\ngit lfs install\ngit lfs track \"*.psd\"\ngit lfs track \"*.zip\"\ngit add .gitattributes\ngit add file.psd\ngit commit -m \"Add large file\"\n\n# List LFS files\ngit lfs ls-files\n\n# Fetch LFS files\ngit lfs fetch\ngit lfs pull\n```\n\n### Shallow Clones\n\n```bash\n# Shallow clone (faster, less disk space)\ngit clone --depth 1 <url>\n\n# Unshallow (convert to full clone)\ngit fetch --unshallow\n\n# Fetch more history\ngit fetch --depth=100\n```\n\n---\n\n## Tags and Releases\n\n### Creating Tags\n\n```bash\n# Lightweight tag\ngit tag <tag-name>\ngit tag v1.0.0\n\n# Annotated tag (recommended - includes metadata)\ngit tag -a <tag-name> -m \"message\"\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\n\n# Tag specific commit\ngit tag -a <tag-name> <commit>\n\n# Signed tag (GPG signature)\ngit tag -s <tag-name> -m \"message\"\n```\n\n### Managing Tags\n\n```bash\n# List tags\ngit tag\ngit tag -l \"v1.*\"  # Pattern matching\n\n# Show tag info\ngit show <tag-name>\n\n# Delete local tag\ngit tag -d <tag-name>\n\n# Delete remote tag\ngit push origin --delete <tag-name>\ngit push origin :refs/tags/<tag-name>\n\n# Push tags\ngit push origin <tag-name>\ngit push --tags  # All tags\ngit push --follow-tags  # Only annotated tags\n```\n\n---\n\n## Git Hooks\n\n### Client-Side Hooks\n\n```bash\n# Hooks location: .git/hooks/\n\n# pre-commit: Run before commit\n# Example: .git/hooks/pre-commit\n#!/bin/bash\nnpm run lint || exit 1\n\n# prepare-commit-msg: Edit commit message before editor opens\n# commit-msg: Validate commit message\n#!/bin/bash\nmsg=$(cat \"$1\")\nif ! echo \"$msg\" | grep -qE \"^(feat|fix|docs|style|refactor|test|chore):\"; then\n    echo \"Error: Commit message must start with type (feat|fix|docs|...):\"\n    exit 1\nfi\n\n# post-commit: Run after commit\n# pre-push: Run before push\n# post-checkout: Run after checkout\n# post-merge: Run after merge\n\n# Make hook executable\nchmod +x .git/hooks/pre-commit\n```\n\n### Server-Side Hooks\n\n```bash\n# pre-receive: Run before refs are updated\n# update: Run for each branch being updated\n# post-receive: Run after refs are updated\n\n# Example: Reject force pushes\n#!/bin/bash\nwhile read oldrev newrev refname; do\n    if [ \"$oldrev\" != \"0000000000000000000000000000000000000000\" ]; then\n        if ! git merge-base --is-ancestor \"$oldrev\" \"$newrev\"; then\n            echo \"Error: Force push rejected\"\n            exit 1\n        fi\n    fi\ndone\n```\n\n---\n\n## Troubleshooting and Recovery\n\n### Common Problems\n\n**Detached HEAD:**\n```bash\n# You're in detached HEAD state\ngit branch temp  # Create branch at current commit\ngit switch main\ngit merge temp\ngit branch -d temp\n```\n\n**Merge conflicts:**\n```bash\n# During merge/rebase\ngit status  # See conflicted files\n# Edit files to resolve conflicts\ngit add <resolved-files>\ngit merge --continue  # or git rebase --continue\n\n# Abort and start over\ngit merge --abort\ngit rebase --abort\n```\n\n**Accidentally deleted branch:**\n```bash\n# Find branch in reflog\ngit reflog\n# Create branch at commit\ngit branch <branch-name> <commit-hash>\n```\n\n**Committed to wrong branch:**\n```bash\n# Move commit to correct branch\ngit switch correct-branch\ngit cherry-pick <commit>\ngit switch wrong-branch\ngit reset --hard HEAD~1  # Remove from wrong branch\n```\n\n**Pushed sensitive data:**\n```bash\n#  URGENT: Remove from history immediately\ngit filter-repo --path <sensitive-file> --invert-paths\ngit push --force --all\n# Then: Rotate compromised credentials immediately!\n```\n\n**Large commit by mistake:**\n```bash\n# Before pushing\ngit reset --soft HEAD~1\ngit reset HEAD <large-file>\ngit commit -m \"message\"\n\n# After pushing - use filter-repo or BFG\n```\n\n### Recovery Scenarios\n\n**Recover after hard reset:**\n```bash\ngit reflog\ngit reset --hard <commit-before-reset>\n```\n\n**Recover deleted file:**\n```bash\ngit log --all --full-history -- <file>\ngit checkout <commit>^ -- <file>\n```\n\n**Recover deleted commits:**\n```bash\ngit reflog  # Find commit hash\ngit cherry-pick <commit>\n# or\ngit merge <commit>\n# or\ngit reset --hard <commit>\n```\n\n**Recover from corrupted repository:**\n```bash\n# Verify corruption\ngit fsck --full\n\n# Attempt repair\ngit gc --aggressive\n\n# Last resort: clone from remote\n```\n\n---\n\n## Best Practices\n\n### Commit Messages\n\n**Conventional Commits format:**\n```\n<type>(<scope>): <subject>\n\n<body>\n\n<footer>\n```\n\n**Types:**\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation\n- `style`: Formatting (no code change)\n- `refactor`: Code restructuring\n- `test`: Adding tests\n- `chore`: Maintenance\n\n**Example:**\n```\nfeat(auth): add OAuth2 authentication\n\nImplement OAuth2 flow for Google and GitHub providers.\nIncludes token refresh and revocation.\n\nCloses #123\n```\n\n### Branching Best Practices\n\n1. **Keep branches short-lived** (< 2 days ideal)\n2. **Use descriptive names**: `feature/user-auth`, `fix/header-crash`\n3. **One purpose per branch**\n4. **Rebase before merge** to keep history clean\n5. **Delete merged branches**\n\n### Workflow Best Practices\n\n1. **Commit often** (small, logical chunks)\n2. **Pull before push** (stay up to date)\n3. **Review before commit** (`git diff --staged`)\n4. **Write meaningful messages**\n5. **Test before commit**\n6. **Never commit secrets** (use `.gitignore`, environment variables)\n\n### .gitignore Best Practices\n\n```gitignore\n# Environment files\n.env\n.env.local\n*.env\n\n# Dependencies\nnode_modules/\nvendor/\nvenv/\n\n# Build outputs\ndist/\nbuild/\n*.exe\n*.dll\n*.so\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# OS files\n.DS_Store\nThumbs.db\n\n# Logs\n*.log\nlogs/\n\n# Temporary files\ntmp/\ntemp/\n*.tmp\n```\n\n---\n\n## Security Best Practices\n\n### Credential Management\n\n```bash\n# Store credentials (cache for 1 hour)\ngit config --global credential.helper cache\ngit config --global credential.helper 'cache --timeout=3600'\n\n# Store credentials (permanent - use with caution)\ngit config --global credential.helper store\n\n# Windows: Use Credential Manager\ngit config --global credential.helper wincred\n\n# macOS: Use Keychain\ngit config --global credential.helper osxkeychain\n\n# Linux: Use libsecret\ngit config --global credential.helper /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n```\n\n### SSH Keys\n\n```bash\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"  # If ed25519 not supported\n\n# Start ssh-agent\neval \"$(ssh-agent -s)\"\n\n# Add key to ssh-agent\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\nssh -T git@ssh.dev.azure.com\n```\n\n### GPG Signing\n\n```bash\n# Generate GPG key\ngpg --full-generate-key\n\n# List keys\ngpg --list-secret-keys --keyid-format LONG\n\n# Configure Git to sign commits\ngit config --global user.signingkey <key-id>\ngit config --global commit.gpgsign true\n\n# Sign commits\ngit commit -S -m \"message\"\n\n# Verify signatures\ngit log --show-signature\n```\n\n### Preventing Secrets\n\n```bash\n# Git-secrets (AWS tool)\ngit secrets --install\ngit secrets --register-aws\n\n# Gitleaks\ngitleaks detect\n\n# Pre-commit hook\n#!/bin/bash\nif git diff --cached | grep -E \"(password|secret|api_key)\" ; then\n    echo \"Potential secret detected!\"\n    exit 1\nfi\n```\n\n---\n\n## Cross-Platform Considerations\n\n### Line Endings\n\n```bash\n# Windows (CRLF in working directory, LF in repository)\ngit config --global core.autocrlf true\n\n# macOS/Linux (LF everywhere)\ngit config --global core.autocrlf input\n\n# No conversion (not recommended)\ngit config --global core.autocrlf false\n\n# Use .gitattributes for consistency\n# .gitattributes:\n* text=auto\n*.sh text eol=lf\n*.bat text eol=crlf\n```\n\n### Case Sensitivity\n\n```bash\n# macOS/Windows: Case-insensitive filesystems\n# Linux: Case-sensitive filesystem\n\n# Enable case sensitivity in Git\ngit config --global core.ignorecase false\n\n# Rename file (case-only change)\ngit mv --force myfile.txt MyFile.txt\n```\n\n### Path Handling\n\n```bash\n# Git always uses forward slashes internally\n# Works on all platforms:\ngit add src/components/Header.jsx\n\n# Windows-specific tools may need backslashes in some contexts\n```\n\n### Git Bash / MINGW Path Conversion (Windows)\n\n**CRITICAL: Git Bash is the primary Git environment on Windows!**\n\nGit Bash (MINGW/MSYS2) automatically converts Unix-style paths to Windows paths for native executables, which can cause issues with Git operations.\n\n**Path Conversion Behavior:**\n```bash\n# Automatic conversions that occur:\n/foo           C:/Program Files/Git/usr/foo\n/foo:/bar      C:\\msys64\\foo;C:\\msys64\\bar\n--dir=/foo     --dir=C:/msys64/foo\n\n# What triggers conversion:\n#  Leading forward slash (/) in arguments\n#  Colon-separated path lists\n#  Arguments after - or , with path components\n\n# What's exempt from conversion:\n#  Arguments containing = (variable assignments)\n#  Drive specifiers (C:)\n#  Arguments with ; (already Windows format)\n#  Arguments starting with // (Windows switches)\n```\n\n**Controlling Path Conversion:**\n\n```bash\n# Method 1: MSYS_NO_PATHCONV (Git for Windows only)\n# Disable ALL path conversion for a command\nMSYS_NO_PATHCONV=1 git command --option=/path\n\n# Permanently disable (use with caution - can break scripts)\nexport MSYS_NO_PATHCONV=1\n\n# Method 2: MSYS2_ARG_CONV_EXCL (MSYS2)\n# Exclude specific argument patterns\nexport MSYS2_ARG_CONV_EXCL=\"*\"              # Exclude everything\nexport MSYS2_ARG_CONV_EXCL=\"--dir=;/test\"  # Specific prefixes\n\n# Method 3: Manual conversion with cygpath\ncygpath -u \"C:\\path\"     #  Unix format: /c/path\ncygpath -w \"/c/path\"     #  Windows format: C:\\path\ncygpath -m \"/c/path\"     #  Mixed format: C:/path\n\n# Method 4: Workarounds\n# Use double slashes: //e //s instead of /e /s\n# Use dash notation: -e -s instead of /e /s\n# Quote paths with spaces: \"/c/Program Files/file.txt\"\n```\n\n**Shell Detection in Git Workflows:**\n\n```bash\n# Method 1: $MSYSTEM (Most Reliable for Git Bash)\ncase \"$MSYSTEM\" in\n  MINGW64)  echo \"Git Bash 64-bit\" ;;\n  MINGW32)  echo \"Git Bash 32-bit\" ;;\n  MSYS)     echo \"MSYS environment\" ;;\nesac\n\n# Method 2: uname -s (Portable)\ncase \"$(uname -s)\" in\n  MINGW64_NT*)  echo \"Git Bash 64-bit\" ;;\n  MINGW32_NT*)  echo \"Git Bash 32-bit\" ;;\n  MSYS_NT*)     echo \"MSYS\" ;;\n  CYGWIN*)      echo \"Cygwin\" ;;\n  Darwin*)      echo \"macOS\" ;;\n  Linux*)       echo \"Linux\" ;;\nesac\n\n# Method 3: $OSTYPE (Bash-only, fast)\ncase \"$OSTYPE\" in\n  msys*)       echo \"Git Bash/MSYS\" ;;\n  cygwin*)     echo \"Cygwin\" ;;\n  darwin*)     echo \"macOS\" ;;\n  linux-gnu*)  echo \"Linux\" ;;\nesac\n```\n\n**Git Bash Path Issues & Solutions:**\n\n```bash\n# Issue: Git commands with paths fail in Git Bash\n# Example: git log --follow /path/to/file fails\n\n# Solution 1: Use relative paths\ngit log --follow ./path/to/file\n\n# Solution 2: Disable path conversion\nMSYS_NO_PATHCONV=1 git log --follow /path/to/file\n\n# Solution 3: Use Windows-style paths\ngit log --follow C:/path/to/file\n\n# Issue: Spaces in paths (Program Files)\n# Solution: Always quote paths\ngit add \"/c/Program Files/project/file.txt\"\n\n# Issue: Drive letter duplication (D:\\dev  D:\\d\\dev)\n# Solution: Use cygpath for conversion\nfile=$(cygpath -u \"D:\\dev\\file.txt\")\ngit add \"$file\"\n```\n\n**Git Bash Best Practices:**\n\n1. **Always use forward slashes in Git commands** - Git handles them on all platforms\n2. **Quote paths with spaces** - Essential in Git Bash\n3. **Use relative paths when possible** - Avoids conversion issues\n4. **Detect shell environment** - Use $MSYSTEM for Git Bash detection\n5. **Test scripts on Git Bash** - Primary Windows Git environment\n6. **Use MSYS_NO_PATHCONV selectively** - Only when needed, not globally\n\n---\n\n## Success Criteria\n\nA Git workflow using this skill should:\n\n1.  ALWAYS ask user preference for automatic commits vs manual\n2.  ALWAYS warn before destructive operations\n3.  ALWAYS create backup branches before risky operations\n4.  ALWAYS explain recovery procedures\n5.  Use appropriate branch strategy for the project\n6.  Write meaningful commit messages\n7.  Keep commit history clean and linear\n8.  Never commit secrets or large binary files\n9.  Test code before committing\n10.  Know how to recover from any mistake\n\n---\n\n## Emergency Recovery Reference\n\n**Quick recovery commands:**\n\n```bash\n# Undo last commit (keep changes)\ngit reset --soft HEAD~1\n\n# Undo changes to file\ngit checkout -- <file>\n\n# Recover deleted branch\ngit reflog\ngit branch <name> <commit>\n\n# Undo force push (if recent)\ngit reflog\ngit reset --hard <commit-before-push>\ngit push --force-with-lease\n\n# Recover from hard reset\ngit reflog\ngit reset --hard <commit-before-reset>\n\n# Find lost commits\ngit fsck --lost-found\ngit reflog --all\n\n# Recover deleted file\ngit log --all --full-history -- <file>\ngit checkout <commit>^ -- <file>\n```\n\n---\n\n## When to Use This Skill\n\n**Always activate for:**\n- Any Git command or operation\n- Repository management questions\n- Branch strategy decisions\n- Merge conflict resolution\n- History rewriting needs\n- Recovery from Git mistakes\n- Platform-specific Git questions\n- Dangerous operations (with appropriate warnings)\n\n**Key indicators:**\n- User mentions Git, GitHub, GitLab, Bitbucket, Azure DevOps\n- Version control questions\n- Commit, push, pull, merge, rebase operations\n- Branch management\n- History modification\n- Recovery scenarios\n\n---\n\nThis skill provides COMPLETE Git expertise. Combined with the reference files and safety guardrails, you have the knowledge to handle ANY Git operation safely and effectively."
              },
              {
                "name": "git-security-2025",
                "description": "Git security best practices for 2025 including signed commits, zero-trust workflows, secret scanning, and verification",
                "path": "plugins/git-master/skills/git-security-2025/SKILL.md",
                "frontmatter": {
                  "name": "git-security-2025",
                  "description": "Git security best practices for 2025 including signed commits, zero-trust workflows, secret scanning, and verification"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Git Security Best Practices 2025\n\n## Zero-Trust Security Model (2025 Standard)\n\n**What:** Every developer identity must be authenticated and authorized explicitly. All Git operations are logged, signed, and continuously monitored.\n\n**Core Principles:**\n1. **Never trust, always verify** - Every commit verified\n2. **Least privilege access** - Minimal permissions required\n3. **Continuous monitoring** - All operations logged and audited\n4. **Assume breach** - Defense in depth strategies\n\n### Implementing Zero-Trust for Git\n\n**1. Mandatory Signed Commits:**\n```bash\n# Global requirement\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n\n# Enforce via branch protection (GitHub/GitLab/Azure DevOps)\n# Repository Settings  Branches  Require signed commits\n```\n\n**2. Identity Verification:**\n```bash\n# Every commit must verify identity\ngit log --show-signature -10\n\n# Reject unsigned commits in CI/CD\n# .github/workflows/verify.yml\n- name: Verify all commits are signed\n  run: |\n    git log --pretty=\"%H\" origin/main..HEAD | while read commit; do\n      if ! git verify-commit \"$commit\" 2>/dev/null; then\n        echo \"ERROR: Unsigned commit $commit\"\n        exit 1\n      fi\n    done\n```\n\n**3. Continuous Audit Logging:**\n```bash\n# Enable Git audit trail\ngit config --global alias.audit 'log --all --pretty=\"%H|%an|%ae|%ad|%s|%GK\" --date=iso'\n\n# Export audit log\ngit audit > git-audit.log\n\n# Monitor for suspicious activity\ngit log --author=\"*\" --since=\"24 hours ago\" --pretty=format:\"%an %ae %s\"\n```\n\n**4. Least Privilege Access:**\n```yaml\n# GitHub branch protection (zero-trust model)\nbranches:\n  main:\n    protection_rules:\n      required_pull_request_reviews: true\n      dismiss_stale_reviews: true\n      require_code_owner_reviews: true\n      required_approving_review_count: 2\n      require_signed_commits: true\n      enforce_admins: true\n      restrictions:\n        users: []  # No direct push\n        teams: [\"security-team\"]\n```\n\n**5. Continuous Monitoring:**\n```bash\n# Monitor all repository changes\n# .github/workflows/security-monitor.yml\nname: Security Monitoring\non: [push, pull_request]\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Check for unsigned commits\n        run: git verify-commit HEAD || echo \"::warning::Unsigned commit detected\"\n\n      - name: Scan for secrets\n        run: gitleaks detect --exit-code 1\n\n      - name: Check commit author\n        run: |\n          AUTHOR=$(git log -1 --format='%an <%ae>')\n          echo \"Commit by: $AUTHOR\"\n          # Log to SIEM/security monitoring\n```\n\n## Signed Commits (Mandatory in 2025)\n\n**Why:** Cryptographically verify commit authorship, prevent impersonation, ensure audit trail.\n\n**Industry Trend:** Signed commits increasingly required in 2025 workflows.\n\n### GPG Signing (Traditional)\n\n**Setup:**\n\n```bash\n# Generate GPG key\ngpg --full-generate-key\n# Choose: RSA and RSA, 4096 bits, expires in 2y\n\n# List keys\ngpg --list-secret-keys --keyid-format=long\n\n# Example output:\n# sec   rsa4096/ABC123DEF456 2025-01-15 [SC] [expires: 2027-01-15]\n# uid                 [ultimate] Your Name <your.email@example.com>\n# ssb   rsa4096/GHI789JKL012 2025-01-15 [E] [expires: 2027-01-15]\n\n# Configure Git\ngit config --global user.signingkey ABC123DEF456\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n\n# Export public key for GitHub/GitLab\ngpg --armor --export ABC123DEF456\n# Copy output and add to GitHub/GitLab/Bitbucket\n\n# Sign commits\ngit commit -S -m \"feat: add authentication\"\n\n# Verify signatures\ngit log --show-signature\ngit verify-commit HEAD\ngit verify-tag v1.0.0\n```\n\n**Troubleshooting:**\n\n```bash\n# GPG agent not running\nexport GPG_TTY=$(tty)\necho 'export GPG_TTY=$(tty)' >> ~/.bashrc\n\n# Cache passphrase longer\necho 'default-cache-ttl 34560000' >> ~/.gnupg/gpg-agent.conf\necho 'max-cache-ttl 34560000' >> ~/.gnupg/gpg-agent.conf\ngpg-connect-agent reloadagent /bye\n\n# Test signing\necho \"test\" | gpg --clearsign\n```\n\n### SSH Signing (Modern Alternative - 2023+)\n\n**Why SSH:** Simpler, reuse existing SSH keys, no GPG required.\n\n**Setup:**\n\n```bash\n# Check if SSH key exists\nls -la ~/.ssh/id_ed25519.pub\n\n# Generate if needed\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Configure Git to use SSH signing\ngit config --global gpg.format ssh\ngit config --global user.signingkey ~/.ssh/id_ed25519.pub\ngit config --global commit.gpgsign true\n\n# Add public key to GitHub\ncat ~/.ssh/id_ed25519.pub\n# GitHub Settings  SSH and GPG keys  New SSH key  Key type: Signing Key\n\n# Sign commits (automatic with commit.gpgsign=true)\ngit commit -m \"feat: add feature\"\n\n# Verify\ngit log --show-signature\n```\n\n**Configure allowed signers file (for verification):**\n\n```bash\n# Create allowed signers file\necho \"your.email@example.com $(cat ~/.ssh/id_ed25519.pub)\" > ~/.ssh/allowed_signers\n\n# Configure Git\ngit config --global gpg.ssh.allowedSignersFile ~/.ssh/allowed_signers\n\n# Verify commits\ngit verify-commit HEAD\n```\n\n## Secret Scanning & Prevention\n\n### GitHub Secret Scanning (Push Protection)\n\n**Enable in repository:**\n- Settings  Code security  Secret scanning  Enable\n- Enable push protection (blocks secrets at push time)\n\n**AI-powered detection (2025):**\n- AWS credentials\n- Azure service principals\n- Google Cloud keys\n- GitHub tokens\n- Database connection strings\n- API keys (OpenAI, Stripe, Anthropic, etc.)\n- Private keys\n- OAuth tokens\n- Custom patterns\n\n**Example blocked push:**\n\n```bash\n$ git push\nremote: error: GH013: Repository rule violations found for refs/heads/main.\nremote:\nremote: - Push cannot contain secrets\nremote:\nremote:   Resolve the following violations before pushing again\nremote:\nremote:    AWS Access Key\nremote:     locations:\nremote:       - config.py:12\nremote:\nremote:   (Disable push protection: https://github.com/settings/security_analysis)\nremote:\nTo github.com:user/repo.git\n ! [remote rejected] main -> main (push declined due to repository rule violations)\n```\n\n**Fix:**\n\n```bash\n# Remove secret from file\n# Use environment variable instead\necho \"AWS_ACCESS_KEY=your_key\" >> .env\necho \".env\" >> .gitignore\n\n# Remove from history if already committed\ngit rm --cached config.py\ngit commit -m \"Remove secrets\"\n\n# If in history, use filter-repo\ngit filter-repo --path config.py --invert-paths\ngit push --force\n```\n\n### Gitleaks (Local Scanning)\n\n**Install:**\n\n```bash\n# macOS\nbrew install gitleaks\n\n# Linux\nwget https://github.com/gitleaks/gitleaks/releases/download/v8.18.0/gitleaks_8.18.0_linux_x64.tar.gz\ntar -xzf gitleaks_8.18.0_linux_x64.tar.gz\nsudo mv gitleaks /usr/local/bin/\n\n# Windows\nchoco install gitleaks\n```\n\n**Usage:**\n\n```bash\n# Scan entire repository\ngitleaks detect\n\n# Scan uncommitted changes\ngitleaks protect\n\n# Scan specific directory\ngitleaks detect --source ./src\n\n# Generate report\ngitleaks detect --report-format json --report-path gitleaks-report.json\n\n# Use in CI/CD\ngitleaks detect --exit-code 1\n```\n\n**Pre-commit hook:**\n\n```bash\n# .git/hooks/pre-commit\n#!/bin/bash\ngitleaks protect --staged --verbose\nif [ $? -ne 0 ]; then\n    echo \"  Gitleaks detected secrets. Commit blocked.\"\n    exit 1\nfi\n```\n\n### Git-secrets (AWS-focused)\n\n```bash\n# Install\nbrew install git-secrets  # macOS\n# or\ngit clone https://github.com/awslabs/git-secrets.git\ncd git-secrets\nsudo make install\n\n# Initialize in repository\ngit secrets --install\ngit secrets --register-aws\n\n# Add custom patterns\ngit secrets --add 'password\\s*=\\s*[^\\s]+'\ngit secrets --add 'api[_-]?key\\s*=\\s*[^\\s]+'\n\n# Scan\ngit secrets --scan\ngit secrets --scan-history\n```\n\n## Enforce Signed Commits\n\n### Branch Protection Rules\n\n**GitHub:**\n\n```\nRepository  Settings  Branches  Branch protection rules\n Require signed commits\n Require linear history\n Require status checks to pass\n```\n\n**GitLab:**\n\n```\nRepository  Settings  Repository  Protected branches\n Allowed to push: No one\n Allowed to merge: Maintainers\n Require all commits be signed\n```\n\n**Azure DevOps:**\n\n```\nBranch Policies  Add policy  Require signed commits\n```\n\n### Pre-receive Hook (Server-side enforcement)\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-receive (on server)\n\nzero_commit=\"0000000000000000000000000000000000000000\"\n\nwhile read oldrev newrev refname; do\n  # Skip branch deletion\n  if [ \"$newrev\" = \"$zero_commit\" ]; then\n    continue\n  fi\n\n  # Check all commits in push\n  for commit in $(git rev-list \"$oldrev\"..\"$newrev\"); do\n    # Verify commit signature\n    if ! git verify-commit \"$commit\" 2>/dev/null; then\n      echo \"Error: Commit $commit is not signed\"\n      echo \"All commits must be signed. Configure with:\"\n      echo \"  git config commit.gpgsign true\"\n      exit 1\n    fi\n  done\ndone\n\nexit 0\n```\n\n## Security Configuration\n\n### Recommended Git Config\n\n```bash\n# Enforce signed commits\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n\n# Use SSH signing (modern)\ngit config --global gpg.format ssh\ngit config --global user.signingkey ~/.ssh/id_ed25519.pub\n\n# Security settings\ngit config --global protocol.version 2\ngit config --global transfer.fsckobjects true\ngit config --global fetch.fsckobjects true\ngit config --global receive.fsckobjects true\n\n# Prevent credential leaks\ngit config --global credential.helper cache --timeout=3600\n# Or use system credential manager\ngit config --global credential.helper wincred  # Windows\ngit config --global credential.helper osxkeychain  # macOS\n\n# Line ending safety\ngit config --global core.autocrlf true  # Windows\ngit config --global core.autocrlf input  # macOS/Linux\n\n# Editor safety (avoid nano/vim leaks)\ngit config --global core.editor \"code --wait\"\n```\n\n### .gitignore Security\n\n```gitignore\n# Secrets\n.env\n.env.*\n*.pem\n*.key\n*.p12\n*.pfx\n*_rsa\n*_dsa\n*_ecdsa\n*_ed25519\ncredentials.json\nsecrets.yaml\nconfig/secrets.yml\n\n# Cloud provider\n.aws/\n.azure/\n.gcloud/\ngcloud-service-key.json\n\n# Databases\n*.sqlite\n*.db\n\n# Logs (may contain sensitive data)\n*.log\nlogs/\n\n# IDE secrets\n.vscode/settings.json\n.idea/workspace.xml\n\n# Build artifacts (may contain embedded secrets)\ndist/\nbuild/\nnode_modules/\nvendor/\n```\n\n## Credential Management\n\n### SSH Keys\n\n```bash\n# Generate secure SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\" -f ~/.ssh/id_ed25519_work\n\n# Use ed25519 (modern, secure, fast)\n# Avoid RSA < 4096 bits\n# Avoid DSA (deprecated)\n\n# Configure SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519_work\n\n# Test connection\nssh -T git@github.com\n\n# Use different keys for different services\n# ~/.ssh/config\nHost github.com\n  IdentityFile ~/.ssh/id_ed25519_github\n\nHost gitlab.com\n  IdentityFile ~/.ssh/id_ed25519_gitlab\n```\n\n### HTTPS Credentials\n\n```bash\n# Use credential manager (not plaintext!)\n\n# Windows\ngit config --global credential.helper wincred\n\n# macOS\ngit config --global credential.helper osxkeychain\n\n# Linux (libsecret)\ngit config --global credential.helper /usr/share/git/credential/libsecret/git-credential-libsecret\n\n# Cache for limited time (temporary projects)\ngit config --global credential.helper 'cache --timeout=3600'\n```\n\n### Personal Access Tokens (PAT)\n\n**GitHub:**\n- Settings  Developer settings  Personal access tokens  Fine-grained tokens\n- Set expiration (max 1 year)\n- Minimum scopes needed\n- Use for HTTPS authentication\n\n**Never commit tokens:**\n\n```bash\n# Use environment variable\nexport GITHUB_TOKEN=\"ghp_xxxxxxxxxxxx\"\ngit clone https://$GITHUB_TOKEN@github.com/user/repo.git\n\n# Or use Git credential helper\ngh auth login  # GitHub CLI method\n```\n\n## CodeQL & Security Scanning\n\n### GitHub CodeQL\n\n**.github/workflows/codeql.yml:**\n\n```yaml\nname: \"CodeQL Security Scan\"\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 0 * * 1'  # Weekly scan\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      security-events: write\n      contents: read\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'javascript', 'python', 'java' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n        queries: security-and-quality\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v3\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{ matrix.language }}\"\n```\n\n**Detects:**\n- SQL injection\n- XSS vulnerabilities\n- Path traversal\n- Command injection\n- Insecure deserialization\n- Authentication bypass\n- Hardcoded secrets\n\n## Audit Trail\n\n### Enable detailed logging\n\n```bash\n# Log all Git operations\ngit config --global alias.ll 'log --all --graph --decorate --oneline --show-signature'\n\n# Check commit verification\ngit log --show-signature -10\n\n# Export audit log\ngit log --pretty=format:\"%H,%an,%ae,%ad,%s\" --date=iso > git-audit.csv\n\n# Verify all commits in branch\ngit log --show-signature main..HEAD\n```\n\n## Security Checklist\n\n**Repository Setup:**\n-  Enable branch protection\n-  Require signed commits\n-  Enable secret scanning with push protection\n-  Enable CodeQL or similar scanning\n-  Configure Dependabot/Renovate\n-  Require 2FA for all contributors\n\n**Developer Workstation:**\n-  Use GPG or SSH commit signing\n-  Configure credential manager (never plaintext)\n-  Install and configure gitleaks\n-  Create comprehensive .gitignore\n-  Enable fsckobjects for transfers\n-  Use SSH keys with passphrase\n\n**Workflow:**\n-  Never commit secrets\n-  Review changes before commit\n-  Verify signatures on pull/merge\n-  Regular security audits\n-  Rotate credentials periodically\n-  Use environment variables for secrets\n\n## Incident Response\n\n**Secret leaked in commit:**\n\n```bash\n# 1. Rotate compromised credentials IMMEDIATELY\n# 2. Remove from latest commit (if not pushed)\ngit reset HEAD~1\n# Edit files to remove secret\ngit add .\ngit commit -m \"Remove secrets\"\n\n# 3. If pushed, remove from history\ngit filter-repo --path config/secrets.yml --invert-paths\ngit push --force\n\n# 4. Notify team to re-clone\n# 5. Enable push protection to prevent future leaks\n```\n\n**Unsigned commits detected:**\n\n```bash\n# Identify unsigned commits\ngit log --show-signature | grep \"No signature\"\n\n# Re-sign commits (if you authored them)\ngit rebase --exec 'git commit --amend --no-edit -n -S' -i HEAD~10\n\n# Force push (with team coordination)\ngit push --force-with-lease\n```\n\n## Resources\n\n- [Git Signing Documentation](https://git-scm.com/book/en/v2/Git-Tools-Signing-Your-Work)\n- [GitHub Secret Scanning](https://docs.github.com/en/code-security/secret-scanning)\n- [Gitleaks Documentation](https://github.com/gitleaks/gitleaks)\n- [CodeQL Documentation](https://codeql.github.com/docs/)"
              },
              {
                "name": "github-actions-2025",
                "description": "GitHub Actions 2025 features including 1 vCPU runners, immutable releases, and Node24 migration",
                "path": "plugins/git-master/skills/github-actions-2025/SKILL.md",
                "frontmatter": {
                  "name": "github-actions-2025",
                  "description": "GitHub Actions 2025 features including 1 vCPU runners, immutable releases, and Node24 migration"
                },
                "content": "# GitHub Actions 2025 Features\n\n## 1 vCPU Linux Runners (October 2025 - Public Preview)\n\n**What:** New lightweight runners optimized for automation tasks with lower cost.\n\n**Specs:**\n- 1 vCPU\n- 5 GB RAM\n- 15-minute job limit\n- Optimized for short-running tasks\n\n### When to Use 1 vCPU Runners\n\n**Ideal for:**\n- Issue triage automation\n- Label management\n- PR comment automation\n- Status checks\n- Lightweight scripts\n- Git operations (checkout, tag, commit)\n- Notification tasks\n\n**NOT suitable for:**\n- Build operations\n- Test suites\n- Complex CI/CD pipelines\n- Resource-intensive operations\n\n### Usage\n\n```yaml\n# .github/workflows/automation.yml\nname: Lightweight Automation\n\non:\n  issues:\n    types: [opened, labeled]\n\njobs:\n  triage:\n    runs-on: ubuntu-latest-1-core  # New 1 vCPU runner\n    timeout-minutes: 10  # Max 15 minutes\n    steps:\n      - name: Triage Issue\n        run: |\n          echo \"Triaging issue...\"\n          gh issue edit ${{ github.event.issue.number }} --add-label \"needs-review\"\n```\n\n### Cost Savings Example\n\n```yaml\n# Before: Using 2 vCPU runner for simple task\njobs:\n  label:\n    runs-on: ubuntu-latest  # 2 vCPU, higher cost\n    steps:\n      - name: Add label\n        run: gh pr edit ${{ github.event.number }} --add-label \"reviewed\"\n\n# After: Using 1 vCPU runner (lower cost)\njobs:\n  label:\n    runs-on: ubuntu-latest-1-core  # 1 vCPU, 50% cost reduction\n    timeout-minutes: 5\n    steps:\n      - name: Add label\n        run: gh pr edit ${{ github.event.number }} --add-label \"reviewed\"\n```\n\n## Immutable Releases (August 2025)\n\n**What:** Releases can now be marked immutable - assets and Git tags cannot be changed or deleted once released.\n\n**Benefits:**\n- Supply chain security\n- Audit compliance\n- Prevent tampering\n- Trust in release artifacts\n\n### Create Immutable Release\n\n```bash\n# Using GitHub CLI\ngh release create v1.0.0 \\\n  dist/*.zip \\\n  --title \"Version 1.0.0\" \\\n  --notes-file CHANGELOG.md \\\n  --immutable\n\n# Verify immutability\ngh release view v1.0.0 --json isImmutable\n```\n\n### GitHub Actions Workflow\n\n```yaml\n# .github/workflows/release.yml\nname: Create Immutable Release\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Build artifacts\n        run: npm run build\n\n      - name: Create Immutable Release\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const tag = context.ref.replace('refs/tags/', '');\n\n            await github.rest.repos.createRelease({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              tag_name: tag,\n              name: `Release ${tag}`,\n              body: fs.readFileSync('CHANGELOG.md', 'utf8'),\n              draft: false,\n              prerelease: false,\n              make_immutable: true  # Mark as immutable\n            });\n\n      - name: Upload Release Assets\n        run: gh release upload ${{ github.ref_name }} dist/*.zip --clobber\n```\n\n### Immutable Release Policy\n\n```yaml\n# Organizational policy for immutable releases\nname: Enforce Immutable Releases\n\non:\n  release:\n    types: [created]\n\njobs:\n  enforce-immutability:\n    runs-on: ubuntu-latest\n    if: \"!github.event.release.immutable && startsWith(github.event.release.tag_name, 'v')\"\n\n    steps:\n      - name: Fail if not immutable\n        run: |\n          echo \"ERROR: Production releases must be immutable\"\n          exit 1\n```\n\n## Node24 Migration (September 2025)\n\n**What:** GitHub Actions migrating from Node20 to Node24 in fall 2025.\n\n**Timeline:**\n- September 2025: Node24 support added\n- October 2025: Deprecation notices for Node20\n- November 2025: Node20 phase-out begins\n- December 2025: Full migration to Node24\n\n### Update Your Actions\n\n**Check Node version in actions:**\n\n```yaml\n# Old - Node20\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '20'  # Update to 24\n\n# New - Node24\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '24'  # Current LTS\n```\n\n### Runner Version Compatibility\n\n```yaml\n# Ensure runner supports Node24\njobs:\n  test:\n    runs-on: ubuntu-latest  # Runner v2.328.0+ supports Node24\n\n    steps:\n      - name: Verify Node version\n        run: node --version  # Should show v24.x.x\n```\n\n### Custom Actions Migration\n\nIf you maintain custom actions:\n\n```javascript\n// action.yml\nruns:\n  using: 'node24'  // Updated from 'node20'\n  main: 'index.js'\n```\n\n```bash\n# Update dependencies\nnpm install @actions/core@latest\nnpm install @actions/github@latest\n\n# Test with Node24\nnode --version  # Ensure 24.x\nnpm test\n```\n\n## Actions Environment Variables (May 2025)\n\n**What:** Actions environments now available for all plans (public and private repos).\n\n### Environment Protection Rules\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy to Production\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://app.example.com\n\n    steps:\n      - name: Deploy\n        run: |\n          echo \"Deploying to ${{ vars.DEPLOY_URL }}\"\n          # Deployment steps...\n```\n\n**Environment configuration:**\n- Settings  Environments  production\n- Add protection rules:\n  - Required reviewers\n  - Wait timer\n  - Deployment branches (only main)\n\n## Allowed Actions Policy Updates (August 2025)\n\n**What:** Enhanced governance with explicit blocking and SHA pinning.\n\n### Block Specific Actions\n\n```yaml\n# .github/workflows/policy.yml\n# Repository or organization settings\nallowed-actions:\n  verified-only: true\n\n  # Explicitly block actions\n  blocked-actions:\n    - 'untrusted/action@*'\n    - 'deprecated-org/*'\n\n  # Require SHA pinning for security\n  require-sha-pinning: true\n```\n\n### SHA Pinning for Security\n\n```yaml\n# Before: Version pinning (can be changed by action maintainer)\n- uses: actions/checkout@v4\n\n# After: SHA pinning (immutable)\n- uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1\n```\n\n### Generate SHA-Pinned Actions\n\n```bash\n# Get commit SHA for specific version\ngh api repos/actions/checkout/commits/v4.1.1 --jq '.sha'\n\n# Or use action-security tool\nnpx pin-github-action actions/checkout@v4\n# Output: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11\n```\n\n## Copilot-Triggered Workflows (April 2025)\n\n**What:** Workflows triggered by Copilot-authored events now require explicit approval.\n\n### Configure Copilot Workflow Approval\n\n```yaml\n# .github/workflows/copilot-automation.yml\nname: Copilot PR Automation\n\non:\n  pull_request:\n    types: [opened]\n\njobs:\n  copilot-review:\n    runs-on: ubuntu-latest\n\n    # Copilot-generated PRs require approval\n    if: github.event.pull_request.user.login != 'github-copilot[bot]'\n\n    steps:\n      - name: Auto-review\n        run: gh pr review --approve\n```\n\n**Manual approval required for Copilot PRs** (same mechanism as fork PRs).\n\n## Artifact Storage Architecture (February 2025)\n\n**What:** Artifacts moved to new architecture on February 1, 2025.\n\n**Breaking changes:**\n- `actions/upload-artifact@v1-v2` retired March 1, 2025\n- Must use `actions/upload-artifact@v4+`\n\n### Migration\n\n```yaml\n# Old (Retired)\n- uses: actions/upload-artifact@v2\n  with:\n    name: build-artifacts\n    path: dist/\n\n# New (Required)\n- uses: actions/upload-artifact@v4\n  with:\n    name: build-artifacts\n    path: dist/\n    retention-days: 30\n```\n\n## Windows Server 2019 Retirement (June 2025)\n\n**What:** `windows-2019` runner image fully retired June 30, 2025.\n\n### Migration\n\n```yaml\n# Old\njobs:\n  build:\n    runs-on: windows-2019  # Retired\n\n# New\njobs:\n  build:\n    runs-on: windows-2022  # Current\n    # Or windows-latest (recommended)\n```\n\n## Meta API for Self-Hosted Runners (May 2025)\n\n**What:** New `actions_inbound` section in meta API for network configuration.\n\n```bash\n# Get network requirements for self-hosted runners\ncurl https://api.github.com/meta | jq '.actions_inbound'\n\n# Configure firewall rules based on response\n{\n  \"domains\": [\n    \"*.actions.githubusercontent.com\",\n    \"*.pkg.github.com\"\n  ],\n  \"ip_ranges\": [\n    \"140.82.112.0/20\",\n    \"143.55.64.0/20\"\n  ]\n}\n```\n\n## Best Practices for 2025\n\n### 1. Use Appropriate Runners\n\n```yaml\n# Use 1 vCPU for lightweight tasks\njobs:\n  label-management:\n    runs-on: ubuntu-latest-1-core\n    timeout-minutes: 5\n\n  # Use standard runners for builds/tests\n  build:\n    runs-on: ubuntu-latest\n```\n\n### 2. Immutable Releases for Production\n\n```yaml\n# Always mark production releases as immutable\n- name: Create Release\n  run: gh release create $TAG --immutable\n```\n\n### 3. SHA Pinning for Security\n\n```yaml\n# Pin actions to SHA, not tags\n- uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11\n- uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8\n```\n\n### 4. Update to Node24\n\n```yaml\n# Use latest Node version\n- uses: actions/setup-node@v4\n  with:\n    node-version: '24'\n```\n\n### 5. Environment Protection\n\n```yaml\n# Use environments for deployments\njobs:\n  deploy:\n    environment: production\n    # Requires approval, wait timer, branch restrictions\n```\n\n## Troubleshooting\n\n**1 vCPU runner timeout:**\n```yaml\n# Ensure task completes within 15 minutes\njobs:\n  task:\n    runs-on: ubuntu-latest-1-core\n    timeout-minutes: 10  # Safety margin\n```\n\n**Node24 compatibility issues:**\n```bash\n# Test locally with Node24\nnvm install 24\nnvm use 24\nnpm test\n```\n\n**Artifact upload failures:**\n```yaml\n# Use v4 of artifact actions\n- uses: actions/upload-artifact@v4  # Not v1/v2\n```\n\n## Resources\n\n- [GitHub Actions 1 vCPU Runners](https://github.blog/changelog/2025-10-28-1-vcpu-linux-runner-now-available-in-github-actions-in-public-preview/)\n- [Immutable Releases](https://github.blog/changelog/2025-08-15-github-actions-policy-now-supports-blocking-and-sha-pinning-actions/)\n- [Node24 Migration](https://github.blog/changelog/2025-09-19-deprecation-of-node-20-on-github-actions-runners/)"
              },
              {
                "name": "github-ai-features-2025",
                "description": "GitHub AI-powered security and automation features for 2025",
                "path": "plugins/git-master/skills/github-ai-features-2025/SKILL.md",
                "frontmatter": {
                  "name": "github-ai-features-2025",
                  "description": "GitHub AI-powered security and automation features for 2025"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# GitHub AI Features 2025\n\n## Trunk-Based Development (TBD)\n\nModern workflow used by largest tech companies (Google: 35,000+ developers):\n\n### Principles\n\n1. **Short-lived branches:** Hours to 1 day maximum\n2. **Small, frequent commits:** Reduce merge conflicts\n3. **Continuous integration:** Always deployable main branch\n4. **Feature flags:** Hide incomplete features\n\n### Implementation\n\n```bash\n# Create task branch from main\ngit checkout main\ngit pull origin main\ngit checkout -b task/add-login-button\n\n# Make small changes\ngit add src/components/LoginButton.tsx\ngit commit -m \"feat: add login button component\"\n\n# Push and create PR (same day)\ngit push origin task/add-login-button\ngh pr create --title \"Add login button\" --body \"Implements login UI\"\n\n# Merge within hours, delete branch\ngh pr merge --squash --delete-branch\n```\n\n### Benefits\n\n- Reduced merge conflicts (75% decrease)\n- Faster feedback cycles\n- Easier code reviews (smaller changes)\n- Always releasable main branch\n- Simplified CI/CD pipelines\n\n## GitHub Secret Protection (AI-Powered)\n\nAI detects secrets before they reach repository:\n\n### Push Protection\n\n```bash\n# Attempt to commit secret\ngit add config.py\ngit commit -m \"Add config\"\ngit push\n\n# GitHub AI detects secret:\n\"\"\"\n Push blocked by secret scanning\n\nFound: AWS Access Key\nPattern: AKIA[0-9A-Z]{16}\nFile: config.py:12\n\nOptions:\n1. Remove secret and try again\n2. Mark as false positive (requires justification)\n3. Request review from admin\n\"\"\"\n\n# Fix: Use environment variables\n# config.py\nimport os\naws_key = os.environ.get('AWS_ACCESS_KEY')\n\ngit add config.py\ngit commit -m \"Use env vars for secrets\"\ngit push  #  Success\n```\n\n### Supported Secret Types (AI-Enhanced)\n\n- AWS credentials\n- Azure service principals\n- Google Cloud keys\n- GitHub tokens\n- Database connection strings\n- API keys (OpenAI, Stripe, etc.)\n- Private keys (SSH, TLS)\n- OAuth tokens\n- Custom patterns (regex-based)\n\n## GitHub Code Security\n\n### CodeQL Code Scanning\n\nAI-powered static analysis:\n\n```yaml\n# .github/workflows/codeql.yml\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    permissions:\n      security-events: write\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: javascript, python, java\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v2\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n```\n\n**Detects:**\n- SQL injection\n- XSS vulnerabilities\n- Path traversal\n- Command injection\n- Insecure deserialization\n- Authentication bypass\n- Logic errors\n\n### Copilot Autofix\n\nAI automatically fixes security vulnerabilities:\n\n```python\n# Vulnerable code detected by CodeQL\ndef get_user(user_id):\n    query = f\"SELECT * FROM users WHERE id = {user_id}\"  #  SQL injection\n    return db.execute(query)\n\n# Copilot Autofix suggests:\ndef get_user(user_id):\n    query = \"SELECT * FROM users WHERE id = ?\"\n    return db.execute(query, (user_id,))  #  Parameterized query\n\n# One-click to apply fix\n```\n\n## GitHub Agents (Automated Workflows)\n\nAI agents for automated bug fixes and PR generation:\n\n### Bug Fix Agent\n\n```yaml\n# .github/workflows/ai-bugfix.yml\nname: AI Bug Fixer\n\non:\n  issues:\n    types: [labeled]\n\njobs:\n  autofix:\n    if: contains(github.event.issue.labels.*.name, 'bug')\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Analyze Bug\n      uses: github/ai-agent@v1\n      with:\n        task: 'analyze-bug'\n        issue-number: ${{ github.event.issue.number }}\n\n    - name: Generate Fix\n      uses: github/ai-agent@v1\n      with:\n        task: 'generate-fix'\n        create-pr: true\n        pr-title: \"Fix: ${{ github.event.issue.title }}\"\n```\n\n### Automated PR Generation\n\n```bash\n# GitHub Agent creates PR automatically\n# When issue is labeled \"enhancement\":\n# 1. Analyzes issue description\n# 2. Generates implementation code\n# 3. Creates tests\n# 4. Opens PR with explanation\n\n# Example: Issue #42 \"Add dark mode toggle\"\n# Agent creates PR with:\n# - DarkModeToggle.tsx component\n# - ThemeContext.tsx provider\n# - Tests for theme switching\n# - Documentation update\n```\n\n## Dependency Review (AI-Enhanced)\n\nAI analyzes dependency changes in PRs:\n\n```yaml\n# .github/workflows/dependency-review.yml\nname: Dependency Review\n\non: [pull_request]\n\npermissions:\n  contents: read\n\njobs:\n  dependency-review:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - name: Dependency Review\n      uses: actions/dependency-review-action@v3\n      with:\n        fail-on-severity: high\n        fail-on-scopes: runtime\n```\n\n**AI Insights:**\n- Known vulnerabilities in new dependencies\n- License compliance issues\n- Breaking changes in updates\n- Alternative safer packages\n- Dependency freshness score\n\n## Trunk-Based Development Workflow\n\n### Daily Workflow\n\n```bash\n# Morning: Sync with main\ngit checkout main\ngit pull origin main\n\n# Create task branch\ngit checkout -b task/user-profile-api\n\n# Work in small iterations (2-4 hours)\n# First iteration: API endpoint\ngit add src/api/profile.ts\ngit commit -m \"feat: add profile API endpoint\"\ngit push origin task/user-profile-api\ngh pr create --title \"Add user profile API\" --draft\n\n# Continue work: Add tests\ngit add tests/profile.test.ts\ngit commit -m \"test: add profile API tests\"\ngit push\n\n# Mark ready for review\ngh pr ready\n# Get review (should happen within hours)\n\n# Merge same day\ngh pr merge --squash --delete-branch\n\n# Next task: Start fresh from main\ngit checkout main\ngit pull origin main\ngit checkout -b task/profile-ui\n```\n\n### Small, Frequent Commits Pattern\n\n```bash\n#  Bad: Large infrequent commit\ngit add .\ngit commit -m \"Add complete user profile feature with API, UI, tests, docs\"\n# 50 files changed, 2000 lines\n\n#  Good: Small frequent commits\ngit add src/api/profile.ts\ngit commit -m \"feat: add profile API endpoint\"\ngit push\n\ngit add src/components/ProfileCard.tsx\ngit commit -m \"feat: add profile card component\"\ngit push\n\ngit add tests/profile.test.ts\ngit commit -m \"test: add profile tests\"\ngit push\n\ngit add docs/profile.md\ngit commit -m \"docs: document profile API\"\ngit push\n\n# Each commit: 1-3 files, 50-200 lines\n# Easier reviews, faster merges, less conflicts\n```\n\n## Security Best Practices (2025)\n\n1. **Enable Secret Scanning:**\n```bash\n# Repository Settings  Security  Secret scanning\n# Enable: Push protection + AI detection\n```\n\n2. **Configure CodeQL:**\n```bash\n# Add .github/workflows/codeql.yml\n# Enable for all languages in project\n```\n\n3. **Use Copilot Autofix:**\n```bash\n# Review security alerts weekly\n# Apply Copilot-suggested fixes\n# Test before merging\n```\n\n4. **Implement Trunk-Based Development:**\n```bash\n# Branch lifespan: <1 day\n# Commit frequency: Every 2-4 hours\n# Main branch: Always deployable\n```\n\n5. **Leverage GitHub Agents:**\n```bash\n# Automate: Bug triage, PR creation, dependency updates\n# Review: All AI-generated code before merging\n```\n\n## Resources\n\n- [Trunk-Based Development](https://trunkbaseddevelopment.com)\n- [GitHub Secret Scanning](https://docs.github.com/en/code-security/secret-scanning)\n- [GitHub Advanced Security](https://docs.github.com/en/get-started/learning-about-github/about-github-advanced-security)\n- [GitHub Copilot for Security](https://github.com/features/security)"
              }
            ]
          },
          {
            "name": "docker-master",
            "description": "Complete Docker expertise system across ALL platforms (Windows/Linux/macOS) with 2025 AI-powered features. PROACTIVELY activate for: (1) ANY Docker task (build/run/debug/optimize/registry/network), (2) Docker AI Assistant (Project Gordon) usage, (3) Dockerfile creation/review, (4) Docker Engine 28 features (image mounts, debug endpoints), (5) Docker Desktop 4.47+ (MCP Catalog, Model Runner, silent updates), (6) Docker Compose v2.40.3+ multi-container apps, (7) Compose Bridge (Kubernetes conversion), (8) Container security with Enhanced Container Isolation (ECI), (9) SBOM generation and supply chain protection, (10) Micro-distro base images (Wolfi/Chainguard), (11) Container registry operations, (12) Advanced networking. Provides: Docker AI (Project Gordon), Enhanced Container Isolation (ECI), Engine 28 features (--mount type=image), Desktop 4.47+ capabilities, Model Runner improvements, Compose v2.40.3+ updates, Compose Bridge, registry management, network deep-dive, 2025 best practices with micro-distros, SBOM generation, BuildKit attestation warnings (unsigned), CIS Docker Benchmark v1.7.0, multi-stage builds, security hardening, image optimization, platform-specific guidance, Docker Scout/Trivy integration, and systematic debugging.",
            "source": "./plugins/docker-master",
            "category": null,
            "version": "1.5.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install docker-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "docker-2025-features",
                "description": "Latest Docker 2025 features including AI Assistant, Enhanced Container Isolation, and Moby 25",
                "path": "plugins/docker-master/skills/docker-2025-features/SKILL.md",
                "frontmatter": {
                  "name": "docker-2025-features",
                  "description": "Latest Docker 2025 features including AI Assistant, Enhanced Container Isolation, and Moby 25"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Docker 2025 Features\n\nThis skill covers the latest Docker features introduced in 2025, ensuring you leverage cutting-edge capabilities for security, performance, and developer experience.\n\n## Docker Engine 28 Features (2025)\n\n### 1. Image Type Mounts\n\n**What it is:**\nMount an image directory structure directly inside a container without extracting to a volume.\n\n**Key capabilities:**\n- Mount image layers as read-only filesystems\n- Share common data between containers without duplication\n- Faster startup for data-heavy containers\n- Reduced disk space usage\n\n**How to use:**\n```bash\n# Mount entire image\ndocker run --rm \\\n  --mount type=image,source=mydata:latest,target=/data \\\n  alpine ls -la /data\n\n# Mount specific path from image\ndocker run --rm \\\n  --mount type=image,source=mydata:latest,image-subpath=/config,target=/app/config \\\n  alpine cat /app/config/settings.json\n```\n\n**Use cases:**\n- Read-only configuration distribution\n- Shared ML model weights across containers\n- Static asset serving\n- Immutable data sets for testing\n\n### 2. Versioned Debug Endpoints\n\n**What it is:**\nDebug endpoints now accessible through standard versioned API paths.\n\n**Previously:** Only available at root paths like `/debug/vars`\n**Now:** Also accessible at `/v1.48/debug/vars`, `/v1.48/debug/pprof/*`\n\n**Available endpoints:**\n- `/v1.48/debug/vars` - Runtime variables\n- `/v1.48/debug/pprof/` - Profiling index\n- `/v1.48/debug/pprof/cmdline` - Command line\n- `/v1.48/debug/pprof/profile` - CPU profile\n- `/v1.48/debug/pprof/trace` - Execution trace\n- `/v1.48/debug/pprof/goroutine` - Goroutine stacks\n\n**How to use:**\n```bash\n# Access debug vars through versioned API\ncurl --unix-socket /var/run/docker.sock http://localhost/v1.48/debug/vars\n\n# Get CPU profile\ncurl --unix-socket /var/run/docker.sock http://localhost/v1.48/debug/pprof/profile?seconds=30 > profile.out\n```\n\n### 3. Component Updates\n\n**Latest versions in Engine 28.3.3:**\n- Buildx v0.26.1 - Enhanced build performance\n- Compose v2.40.3 - Latest compose features\n- BuildKit v0.25.1 - Security improvements\n- Go runtime 1.24.8 - Performance optimizations\n\n### 4. Security Fixes\n\n**CVE-2025-54388:** Fixed firewalld reload issue where published container ports could be accessed from local network even when bound to loopback.\n\n**Impact:** Critical for containers binding to 127.0.0.1 expecting localhost-only access.\n\n### 5. Deprecations\n\n**Raspberry Pi OS 32-bit (armhf):**\n- Docker Engine 28 is the last major version supporting armhf\n- Starting with Engine 29, no new armhf packages\n- Migrate to 64-bit OS or use Engine 28.x LTS\n\n## Docker Desktop 4.47 Features (October 2025)\n\n### 1. MCP Catalog Integration\n\n**What it is:**\nModel Context Protocol (MCP) server catalog with 100+ verified, containerized tools.\n\n**Key capabilities:**\n- Discover and search MCP servers\n- One-click deployment of MCP tools\n- Integration with Docker AI and Model Runner\n- Centralized management of AI agent tools\n\n**How to access:**\n- Docker Hub MCP Catalog\n- Docker Desktop MCP Toolkit\n- Web: https://www.docker.com/mcp-catalog\n\n**Use cases:**\n- AI agent tool discovery\n- Workflow automation\n- Development environment setup\n- CI/CD tool integration\n\n### 2. Model Runner Enhancements\n\n**What's new:**\n- Improved UI for model management\n- Enhanced inference APIs\n- Better inference engine performance\n- Model card inspection in Docker Desktop\n- `docker model requests` command for monitoring\n\n**How to use:**\n```bash\n# List running models\ndocker model ls\n\n# View model details (new: model cards)\ndocker model inspect llama2-7b\n\n# Monitor requests and responses (NEW)\ndocker model requests llama2-7b\n\n# Performance metrics\ndocker stats $(docker model ls -q)\n```\n\n### 3. Silent Component Updates\n\n**What it is:**\nDocker Desktop automatically updates internal components without requiring full application restart.\n\n**Benefits:**\n- Faster security patches\n- Less disruption to workflow\n- Automatic Compose, BuildKit, Containerd updates\n- Background update delivery\n\n**Configuration:**\n- Enabled by default\n- Can be disabled in Settings > General\n- Notifications for major updates only\n\n### 4. CVE Fixes\n\n**CVE-2025-10657 (v4.47):** Fixed Enhanced Container Isolation Docker Socket command restrictions not working in 4.46.0.\n\n**CVE-2025-9074 (v4.46):** Fixed malicious container escape allowing Docker Engine access without mounted socket.\n\n## Docker Desktop 4.38-4.45 Features\n\n### 1. Docker AI Assistant (Project Gordon)\n\n**What it is:**\nAI-powered assistant integrated into Docker Desktop and CLI for intelligent container development.\n\n**Key capabilities:**\n- Natural language command interface\n- Context-aware troubleshooting\n- Automated Dockerfile optimization\n- Real-time best practice recommendations\n- Intelligent error diagnosis\n\n**How to use:**\n```bash\n# Enable in Docker Desktop Settings > Features > Docker AI (Beta)\n\n# Ask questions in natural language\n\"Optimize my Python Dockerfile\"\n\"Why is my container restarting?\"\n\"Suggest secure nginx configuration\"\n```\n\n**Local Model Runner:**\n- Runs AI models directly on your machine (llama.cpp)\n- No cloud API dependencies\n- Privacy-preserving (data stays local)\n- GPU acceleration for performance\n- Works offline\n\n### 2. Enhanced Container Isolation (ECI)\n\n**What it is:**\nAdditional security layer that restricts Docker socket access and container escape vectors.\n\n**Security benefits:**\n- Prevents unauthorized Docker socket access\n- Restricts container capabilities by default\n- Blocks common escape techniques\n- Enforces stricter resource boundaries\n- Audits container operations\n\n**How to enable:**\n```bash\n# Docker Desktop Settings > Security > Enhanced Container Isolation\n# Or via CLI:\ndocker desktop settings set enhancedContainerIsolation=true\n```\n\n**Use cases:**\n- Multi-tenant environments\n- Security-critical applications\n- Compliance requirements (PCI-DSS, HIPAA)\n- Zero-trust architectures\n- Development environments with untrusted code\n\n**Compatibility:**\n- May break containers requiring Docker socket access\n- Requires Docker Desktop 4.38+\n- Supported on Windows (WSL2), macOS, Linux Desktop\n\n### 3. Model Runner\n\n**What it is:**\nBuilt-in AI model execution engine allowing developers to run large language models locally.\n\n**Features:**\n- Run AI models without cloud services\n- Optimal GPU acceleration\n- Privacy-preserving inference\n- Multiple model format support\n- Integration with Docker AI\n\n**How to use:**\n```bash\n# Install via Docker Desktop Extensions\n# Or use CLI:\ndocker model run llama2-7b\n\n# View running models:\ndocker model ls\n\n# Stop model:\ndocker model stop MODEL_ID\n```\n\n**Benefits:**\n- No API costs\n- Complete data privacy\n- Offline availability\n- Faster inference (local GPU)\n- Integration with development workflow\n\n### 4. Multi-Node Kubernetes Testing\n\n**What it is:**\nTest Kubernetes deployments with multi-node clusters directly in Docker Desktop.\n\n**Previously:** Single-node only\n**Now:** 2-5 node clusters for realistic testing\n\n**How to enable:**\n```bash\n# Docker Desktop Settings > Kubernetes > Enable multi-node\n# Specify node count (2-5)\n```\n\n**Use cases:**\n- Test pod scheduling across nodes\n- Validate affinity/anti-affinity rules\n- Test network policies\n- Simulate node failures\n- Validate StatefulSets and DaemonSets\n\n### 5. Bake (General Availability)\n\n**What it is:**\nHigh-level build orchestration tool for complex multi-target builds.\n\n**Previously:** Experimental\n**Now:** Generally available and production-ready\n\n**Features:**\n```hcl\n# docker-bake.hcl\ntarget \"app\" {\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  tags = [\"myapp:latest\"]\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n  cache-from = [\"type=registry,ref=myapp:cache\"]\n  cache-to = [\"type=registry,ref=myapp:cache,mode=max\"]\n}\n\ntarget \"test\" {\n  inherits = [\"app\"]\n  target = \"test\"\n  output = [\"type=local,dest=./coverage\"]\n}\n```\n\n```bash\n# Build all targets\ndocker buildx bake\n\n# Build specific target\ndocker buildx bake test\n```\n\n## Moby 25 Engine Updates\n\n### Performance Improvements\n\n**1. Faster Container Startup:**\n- 20-30% faster cold starts\n- Improved layer extraction\n- Optimized network initialization\n\n**2. Better Resource Management:**\n- More accurate memory accounting\n- Improved CPU throttling\n- Better cgroup v2 support\n\n**3. Storage Driver Enhancements:**\n- overlay2 performance improvements\n- Better disk space management\n- Faster image pulls\n\n### Security Updates\n\n**1. Enhanced Seccomp Profiles:**\n```json\n{\n  \"defaultAction\": \"SCMP_ACT_ERRNO\",\n  \"architectures\": [\"SCMP_ARCH_X86_64\", \"SCMP_ARCH_AARCH64\"],\n  \"syscalls\": [\n    {\n      \"names\": [\"read\", \"write\", \"exit\"],\n      \"action\": \"SCMP_ACT_ALLOW\"\n    }\n  ]\n}\n```\n\n**2. Improved AppArmor Integration:**\n- Better Docker profile generation\n- Reduced false positives\n- Enhanced logging\n\n**3. User Namespace Improvements:**\n- Easier configuration\n- Better compatibility\n- Performance optimizations\n\n## Docker Compose v2.40.3+ Features (2025)\n\n### Compose Bridge (Convert to Kubernetes)\n\n**What it is:**\nConvert local compose.yaml files to Kubernetes manifests in a single command.\n\n**Key capabilities:**\n- Automatic conversion of Compose services to Kubernetes Deployments\n- Service-to-Service mapping\n- Volume conversion to PersistentVolumeClaims\n- ConfigMap and Secret generation\n- Ingress configuration\n\n**How to use:**\n```bash\n# Convert compose file to Kubernetes manifests\ndocker compose convert --format kubernetes > k8s-manifests.yaml\n\n# Or use compose-bridge directly\ndocker compose-bridge convert docker-compose.yml\n\n# Apply to Kubernetes cluster\nkubectl apply -f k8s-manifests.yaml\n```\n\n**Example conversion:**\n```yaml\n# docker-compose.yml\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n    volumes:\n      - data:/usr/share/nginx/html\n\nvolumes:\n  data:\n\n# Converts to Kubernetes:\n# - Deployment for 'web' service\n# - Service exposing port 80\n# - PersistentVolumeClaim for 'data'\n```\n\n**Use cases:**\n- Local development to Kubernetes migration\n- Testing Kubernetes deployments locally\n- CI/CD pipeline conversion\n- Multi-environment deployment strategies\n\n### Breaking Changes\n\n**1. Version Field Obsolete:**\n```yaml\n# OLD (deprecated):\nversion: '3.8'\nservices:\n  app:\n    image: nginx\n\n# NEW (2025):\nservices:\n  app:\n    image: nginx\n```\n\nThe `version` field is now ignored and can be omitted.\n\n### New Features\n\n**1. Develop Watch with initial_sync:**\n```yaml\nservices:\n  app:\n    build: .\n    develop:\n      watch:\n        - action: sync\n          path: ./src\n          target: /app/src\n          initial_sync: full  # NEW: Sync all files on start\n```\n\n**2. Volume Type: Image:**\n```yaml\nservices:\n  app:\n    volumes:\n      - type: image\n        source: mydata:latest\n        target: /data\n        read_only: true\n```\n\n**3. Build Print:**\n```bash\n# Debug complex build configurations\ndocker compose build --print > build-config.json\n```\n\n**4. Config No-Env-Resolution:**\n```bash\n# View raw config without environment variable substitution\ndocker compose config --no-env-resolution\n```\n\n**5. Watch with Prune:**\n```bash\n# Automatically prune unused resources during watch\ndocker compose watch --prune\n```\n\n**6. Run with Quiet:**\n```bash\n# Reduce output noise\ndocker compose run --quiet app npm test\n```\n\n## BuildKit Updates (2025)\n\n### New Features\n\n**1. Git SHA-256 Support:**\n```dockerfile\n# Use SHA-256 based repositories\nADD https://github.com/user/repo#sha256:abc123... /src\n```\n\n**2. Enhanced COPY/ADD --exclude:**\n```dockerfile\n# Now generally available (was labs-only)\nCOPY --exclude=*.test.js --exclude=*.md . /app\n```\n\n**3. ADD --unpack with --chown:**\n```dockerfile\n# Extract and set ownership in one step\nADD --unpack=true --chown=appuser:appgroup archive.tar.gz /app\n```\n\n**4. Git Query Parameters:**\n```dockerfile\n# Fine-grained Git clone control\nADD https://github.com/user/repo.git?depth=1&branch=main /src\n```\n\n**5. Image Checksum Verification:**\n```dockerfile\n# Verify image integrity\nFROM alpine:3.19@sha256:abc123...\n# BuildKit verifies checksum automatically\n```\n\n### Security Enhancements\n\n**1. Improved Frontend Verification:**\n```dockerfile\n# Always use official Docker frontends\n# syntax=docker/dockerfile:1\n\n# Pin with digest for maximum security\n# syntax=docker/dockerfile:1@sha256:ac85f380a63b13dfcefa89046420e1781752bab202122f8f50032edf31be0021\n```\n\n**2. Remote Cache Improvements:**\n- Fixed concurrency issues\n- Better loop handling\n- Enhanced security\n\n## Best Practices for 2025 Features\n\n### Using Docker AI Effectively\n\n**DO:**\n- Provide specific context in queries\n- Verify AI-generated configurations\n- Combine with traditional security tools\n- Use for learning and exploration\n\n**DON'T:**\n- Trust AI blindly for security-critical apps\n- Skip manual code review\n- Ignore security scan results\n- Use in air-gapped environments without Model Runner\n\n### Enhanced Container Isolation\n\n**DO:**\n- Enable for security-sensitive workloads\n- Test containers for compatibility first\n- Document socket access requirements\n- Use with least privilege principles\n\n**DON'T:**\n- Enable without testing existing containers\n- Disable without understanding risks\n- Grant socket access unnecessarily\n- Ignore audit logs\n\n### Modern Compose Files\n\n**DO:**\n- Remove version field from new compose files\n- Use new features (volume type: image, watch improvements)\n- Leverage --print for debugging\n- Adopt --quiet for cleaner CI/CD output\n\n**DON'T:**\n- Keep version field (it's ignored anyway)\n- Rely on deprecated syntax\n- Skip testing with Compose v2.40+\n- Use outdated documentation\n\n## Migration Guide\n\n### Updating to Docker Desktop 4.38+\n\n**1. Backup existing configurations:**\n```bash\n# Export current settings\ndocker context export desktop-linux > backup.tar\n```\n\n**2. Update Docker Desktop:**\n- Download latest from docker.com\n- Run installer\n- Restart machine if required\n\n**3. Enable new features:**\n```bash\n# Enable AI Assistant (beta)\ndocker desktop settings set enableAI=true\n\n# Enable Enhanced Container Isolation\ndocker desktop settings set enhancedContainerIsolation=true\n```\n\n**4. Test existing containers:**\n```bash\n# Verify containers work with ECI\ndocker compose up -d\ndocker compose ps\ndocker compose logs\n```\n\n### Updating Compose Files\n\n**Before:**\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    image: nginx:latest\n    volumes:\n      - data:/data\n\nvolumes:\n  data:\n```\n\n**After:**\n```yaml\nservices:\n  app:\n    image: nginx:1.26.0  # Specific version\n    volumes:\n      - data:/data\n    develop:\n      watch:\n        - action: sync\n          path: ./config\n          target: /etc/nginx/conf.d\n          initial_sync: full\n\nvolumes:\n  data:\n    driver: local\n```\n\n## Troubleshooting 2025 Features\n\n### Docker AI Issues\n\n**Problem:** AI Assistant not responding\n**Solution:**\n```bash\n# Check Docker Desktop version\ndocker version\n\n# Ensure beta features enabled\ndocker desktop settings get enableAI\n\n# Restart Docker Desktop\n```\n\n**Problem:** Model Runner slow\n**Solution:**\n- Update GPU drivers\n- Increase Docker Desktop memory (Settings > Resources)\n- Close other GPU-intensive applications\n- Use smaller models for faster inference\n\n### Enhanced Container Isolation Issues\n\n**Problem:** Container fails with socket permission error\n**Solution:**\n```bash\n# Identify socket dependencies\ndocker inspect CONTAINER | grep -i socket\n\n# If truly needed, add socket access explicitly\n# (Document why in docker-compose.yml comments)\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...\n```\n\n**Problem:** ECI breaks CI/CD pipeline\n**Solution:**\n- Disable ECI temporarily: `docker desktop settings set enhancedContainerIsolation=false`\n- Review which containers need socket access\n- Refactor to eliminate socket dependencies\n- Re-enable ECI with exceptions documented\n\n### Compose v2.40 Issues\n\n**Problem:** \"version field is obsolete\" warning\n**Solution:**\n```yaml\n# Simply remove the version field\n# OLD:\nversion: '3.8'\nservices: ...\n\n# NEW:\nservices: ...\n```\n\n**Problem:** watch with initial_sync fails\n**Solution:**\n```bash\n# Check file permissions\nls -la ./src\n\n# Ensure paths are correct\ndocker compose config | grep -A 5 watch\n\n# Verify sync target exists in container\ndocker compose exec app ls -la /app/src\n```\n\n## Recommended Feature Adoption Timeline\n\n**Immediate (Production-Ready):**\n- Bake for complex builds\n- Compose v2.40 features (remove version field)\n- Moby 25 engine (via regular Docker updates)\n- BuildKit improvements (automatic)\n\n**Testing (Beta but Stable):**\n- Docker AI for development workflows\n- Model Runner for local AI testing\n- Multi-node Kubernetes for pre-production\n\n**Evaluation (Security-Critical):**\n- Enhanced Container Isolation (test thoroughly)\n- ECI with existing production containers\n- Socket access elimination strategies\n\nThis skill ensures you stay current with Docker's 2025 evolution while maintaining stability, security, and production-readiness."
              },
              {
                "name": "docker-best-practices",
                "description": "Comprehensive Docker best practices for images, containers, and production deployments",
                "path": "plugins/docker-master/skills/docker-best-practices/SKILL.md",
                "frontmatter": {
                  "name": "docker-best-practices",
                  "description": "Comprehensive Docker best practices for images, containers, and production deployments"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Docker Best Practices\n\nThis skill provides current Docker best practices across all aspects of container development, deployment, and operation.\n\n## Image Best Practices\n\n### Base Image Selection\n\n**2025 Recommended Hierarchy:**\n1. **Wolfi/Chainguard** (`cgr.dev/chainguard/*`) - Zero-CVE goal, SBOM included\n2. **Alpine** (`alpine:3.19`) - ~7MB, minimal attack surface\n3. **Distroless** (`gcr.io/distroless/*`) - ~2MB, no shell\n4. **Slim variants** (`node:20-slim`) - ~70MB, balanced\n\n**Key rules:**\n- Always specify exact version tags: `node:20.11.0-alpine3.19`\n- Never use `latest` (unpredictable, breaks reproducibility)\n- Use official images from trusted registries\n- Match base image to actual needs\n\n### Dockerfile Structure\n\n**Optimal layer ordering** (least to most frequently changing):\n```dockerfile\n1. Base image and system dependencies\n2. Application dependencies (package.json, requirements.txt, etc.)\n3. Application code\n4. Configuration and metadata\n```\n\n**Rationale:** Docker caches layers. If code changes but dependencies don't, cached dependency layers are reused, speeding up builds.\n\n**Example:**\n```dockerfile\nFROM python:3.12-slim\n\n# 1. System packages (rarely change)\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\n\n# 2. Dependencies (change occasionally)\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# 3. Application code (changes frequently)\nCOPY . /app\nWORKDIR /app\n\nCMD [\"python\", \"app.py\"]\n```\n\n### Multi-Stage Builds\n\nUse multi-stage builds to separate build dependencies from runtime:\n\n```dockerfile\n# Build stage\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM node:20-alpine AS runtime\nWORKDIR /app\n# Only copy what's needed for runtime\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nUSER node\nCMD [\"node\", \"dist/server.js\"]\n```\n\n**Benefits:**\n- Smaller final images (no build tools)\n- Better security (fewer attack vectors)\n- Faster deployment (smaller upload/download)\n\n### Layer Optimization\n\n**Combine commands** to reduce layers and image size:\n\n```dockerfile\n# Bad - 3 layers, cleanup doesn't reduce size\nRUN apt-get update\nRUN apt-get install -y curl\nRUN rm -rf /var/lib/apt/lists/*\n\n# Good - 1 layer, cleanup effective\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends curl && \\\n    rm -rf /var/lib/apt/lists/*\n```\n\n### .dockerignore\n\nAlways create `.dockerignore` to exclude unnecessary files:\n\n```\n# Version control\n.git\n.gitignore\n\n# Dependencies\nnode_modules\n__pycache__\n*.pyc\n\n# IDE\n.vscode\n.idea\n\n# OS\n.DS_Store\nThumbs.db\n\n# Logs\n*.log\nlogs/\n\n# Testing\ncoverage/\n.nyc_output\n*.test.js\n\n# Documentation\nREADME.md\ndocs/\n\n# Environment\n.env\n.env.local\n*.local\n```\n\n## Container Runtime Best Practices\n\n### Security\n\n```bash\ndocker run \\\n  # Run as non-root\n  --user 1000:1000 \\\n  # Drop all capabilities, add only needed ones\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  # Read-only filesystem\n  --read-only \\\n  # Temporary writable filesystems\n  --tmpfs /tmp:noexec,nosuid \\\n  # No new privileges\n  --security-opt=\"no-new-privileges:true\" \\\n  # Resource limits\n  --memory=\"512m\" \\\n  --cpus=\"1.0\" \\\n  my-image\n```\n\n### Resource Management\n\nAlways set resource limits in production:\n\n```yaml\n# docker-compose.yml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 1G\n        reservations:\n          cpus: '1.0'\n          memory: 512M\n```\n\n### Health Checks\n\nImplement health checks for all long-running containers:\n\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 --start-period=40s \\\n  CMD curl -f http://localhost:3000/health || exit 1\n```\n\nOr in compose:\n```yaml\nservices:\n  app:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/health\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n      start_period: 40s\n```\n\n### Logging\n\nConfigure proper logging to prevent disk fill-up:\n\n```yaml\nservices:\n  app:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n```\n\nOr system-wide in `/etc/docker/daemon.json`:\n```json\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n```\n\n### Restart Policies\n\n```yaml\nservices:\n  app:\n    # For development\n    restart: \"no\"\n\n    # For production\n    restart: unless-stopped\n\n    # Or with fine-grained control (Swarm mode)\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n```\n\n## Docker Compose Best Practices\n\n### File Structure\n\n```yaml\n# No version field needed (Compose v2.40.3+)\n\nservices:\n  # Service definitions\n  web:\n    # ...\n  api:\n    # ...\n  database:\n    # ...\n\nnetworks:\n  # Custom networks (preferred)\n  frontend:\n  backend:\n    internal: true\n\nvolumes:\n  # Named volumes (preferred for persistence)\n  db-data:\n  app-data:\n\nconfigs:\n  # Configuration files (Swarm mode)\n  app-config:\n    file: ./config/app.conf\n\nsecrets:\n  # Secrets (Swarm mode)\n  db-password:\n    file: ./secrets/db_pass.txt\n```\n\n### Network Isolation\n\n```yaml\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true  # No external access\n\nservices:\n  web:\n    networks:\n      - frontend\n\n  api:\n    networks:\n      - frontend\n      - backend\n\n  database:\n    networks:\n      - backend  # Not accessible from frontend\n```\n\n### Environment Variables\n\n```yaml\nservices:\n  app:\n    # Load from file (preferred for non-secrets)\n    env_file:\n      - .env\n\n    # Inline for service-specific vars\n    environment:\n      - NODE_ENV=production\n      - LOG_LEVEL=info\n\n    # For Swarm mode secrets\n    secrets:\n      - db_password\n```\n\n**Important:**\n- Add `.env` to `.gitignore`\n- Provide `.env.example` as template\n- Never commit secrets to version control\n\n### Dependency Management\n\n```yaml\nservices:\n  api:\n    depends_on:\n      database:\n        condition: service_healthy  # Wait for health check\n      redis:\n        condition: service_started   # Just wait for start\n```\n\n## Production Best Practices\n\n### Image Tagging Strategy\n\n```bash\n# Use semantic versioning\nmy-app:1.2.3\nmy-app:1.2\nmy-app:1\nmy-app:latest\n\n# Include git commit for traceability\nmy-app:1.2.3-abc123f\n\n# Environment tags\nmy-app:1.2.3-production\nmy-app:1.2.3-staging\n```\n\n### Secrets Management\n\n**Never do this:**\n```dockerfile\n# BAD - secret in layer history\nENV API_KEY=secret123\nRUN echo \"password\" > /app/config\n```\n\n**Do this:**\n```bash\n# Use Docker secrets (Swarm) or external secret management\ndocker secret create db_password ./password.txt\n\n# Or mount secrets at runtime\ndocker run -v /secure/secrets:/run/secrets:ro my-app\n\n# Or use environment files (not in image)\ndocker run --env-file /secure/.env my-app\n```\n\n### Monitoring & Observability\n\n```yaml\nservices:\n  app:\n    # Health checks\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/health\"]\n      interval: 30s\n\n    # Labels for monitoring tools\n    labels:\n      - \"prometheus.io/scrape=true\"\n      - \"prometheus.io/port=9090\"\n      - \"com.company.team=backend\"\n      - \"com.company.version=1.2.3\"\n\n    # Logging\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n```\n\n### Backup Strategy\n\n```bash\n# Backup named volume\ndocker run --rm \\\n  -v VOLUME_NAME:/data \\\n  -v $(pwd):/backup \\\n  alpine tar czf /backup/backup-$(date +%Y%m%d).tar.gz -C /data .\n\n# Restore volume\ndocker run --rm \\\n  -v VOLUME_NAME:/data \\\n  -v $(pwd):/backup \\\n  alpine tar xzf /backup/backup.tar.gz -C /data\n```\n\n### Update Strategy\n\n```yaml\nservices:\n  app:\n    # For Swarm mode - rolling updates\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1        # Update 1 at a time\n        delay: 10s            # Wait 10s between updates\n        failure_action: rollback\n        monitor: 60s\n      rollback_config:\n        parallelism: 1\n        delay: 5s\n```\n\n## Platform-Specific Best Practices\n\n### Linux\n\n- Use user namespace remapping for added security\n- Leverage native performance advantages\n- Use Alpine for smallest images\n- Configure SELinux/AppArmor profiles\n- Use systemd for Docker daemon management\n\n```json\n// /etc/docker/daemon.json\n{\n  \"userns-remap\": \"default\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"live-restore\": true\n}\n```\n\n### macOS\n\n- Allocate sufficient resources in Docker Desktop\n- Use `:delegated` or `:cached` for bind mounts\n- Consider multi-platform builds for ARM (M1/M2)\n- Limit file sharing to necessary directories\n\n```yaml\n# Better volume performance on macOS\nvolumes:\n  - ./src:/app/src:delegated  # Host writes are delayed\n  - ./build:/app/build:cached  # Container writes are cached\n```\n\n### Windows\n\n- Choose container type: Windows or Linux\n- Use forward slashes in paths\n- Ensure drives are shared in Docker Desktop\n- Be aware of line ending differences (CRLF vs LF)\n- Consider WSL2 backend for better performance\n\n```yaml\n# Windows-compatible paths\nvolumes:\n  - C:/Users/name/app:/app  # Forward slashes work\n  # or\n  - C:\\Users\\name\\app:/app  # Backslashes need escaping in YAML\n```\n\n## Performance Best Practices\n\n### Build Performance\n\n```bash\n# Use BuildKit (faster, better caching)\nexport DOCKER_BUILDKIT=1\n\n# Use cache mounts\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Use bind mounts for dependencies\nRUN --mount=type=bind,source=package.json,target=package.json \\\n    --mount=type=bind,source=package-lock.json,target=package-lock.json \\\n    --mount=type=cache,target=/root/.npm \\\n    npm ci\n```\n\n### Image Size\n\n- Use multi-stage builds\n- Choose minimal base images\n- Clean up in the same layer\n- Use .dockerignore\n- Remove build dependencies\n\n```dockerfile\n# Install and cleanup in one layer\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    package1 \\\n    package2 && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n```\n\n### Runtime Performance\n\n```dockerfile\n# Use exec form (no shell overhead)\nCMD [\"node\", \"server.js\"]  # Good\n# vs\nCMD node server.js         # Bad - spawns shell\n\n# Optimize signals\nSTOPSIGNAL SIGTERM\n\n# Run as non-root (slightly faster, much more secure)\nUSER appuser\n```\n\n## Security Best Practices Summary\n\n**Image Security:**\n- Use official, minimal base images\n- Scan for vulnerabilities (Docker Scout, Trivy)\n- Don't include secrets in layers\n- Run as non-root user\n- Keep images updated\n\n**Runtime Security:**\n- Drop capabilities\n- Use read-only filesystem\n- Set resource limits\n- Enable security options\n- Isolate networks\n- Use secrets management\n\n**Compliance:**\n- Follow CIS Docker Benchmark\n- Implement container scanning in CI/CD\n- Use signed images (Docker Content Trust)\n- Maintain audit logs\n- Regular security reviews\n\n## Common Anti-Patterns to Avoid\n\n **Don't:**\n- Run as root\n- Use `--privileged`\n- Mount Docker socket\n- Use `latest` tag\n- Hardcode secrets\n- Skip health checks\n- Ignore resource limits\n- Use huge base images\n- Skip vulnerability scanning\n- Expose unnecessary ports\n- Use inefficient layer caching\n- Commit secrets to Git\n\n **Do:**\n- Run as non-root\n- Use minimal capabilities\n- Isolate containers\n- Tag with versions\n- Use secrets management\n- Implement health checks\n- Set resource limits\n- Use minimal images\n- Scan regularly\n- Apply least privilege\n- Optimize build cache\n- Use .env.example templates\n\n## Checklist for Production-Ready Images\n\n- [ ] Based on official, versioned, minimal image\n- [ ] Multi-stage build (if applicable)\n- [ ] Runs as non-root user\n- [ ] No secrets in layers\n- [ ] .dockerignore configured\n- [ ] Vulnerability scan passed\n- [ ] Health check implemented\n- [ ] Proper labeling (version, description, etc.)\n- [ ] Efficient layer caching\n- [ ] Resource limits defined\n- [ ] Logging configured\n- [ ] Signals handled correctly\n- [ ] Security options set\n- [ ] Documentation complete\n- [ ] Tested on target platform(s)\n\nThis skill represents current Docker best practices. Always verify against official documentation for the latest recommendations, as Docker evolves continuously."
              },
              {
                "name": "docker-git-bash-guide",
                "description": "Comprehensive Windows Git Bash and MINGW path conversion guide for Docker volume mounts and commands",
                "path": "plugins/docker-master/skills/docker-git-bash-guide/SKILL.md",
                "frontmatter": {
                  "name": "docker-git-bash-guide",
                  "description": "Comprehensive Windows Git Bash and MINGW path conversion guide for Docker volume mounts and commands"
                },
                "content": "# Docker on Windows Git Bash / MINGW - Path Conversion Guide\n\nThis skill provides comprehensive guidance on handling Docker commands in Git Bash (MINGW) on Windows, with specific focus on volume mount path conversion issues and solutions.\n\n## The Path Conversion Problem\n\nWhen running Docker commands in Git Bash (MINGW) or MSYS2 on Windows, automatic path conversion can cause serious issues with volume mounts and other Docker commands.\n\n### What Triggers Automatic Conversion\n\nMSYS/MINGW shells automatically convert arguments that look like Unix paths when calling Windows executables (like `docker.exe`):\n\n**Examples of problematic conversions:**\n```bash\n# What you type:\ndocker run -v /c/Users/project:/app myimage\n\n# What Docker receives (BROKEN):\ndocker run -v C:\\Program Files\\Git\\c\\Users\\project:/app myimage\n```\n\n**Triggers for path conversion:**\n- Leading forward slash (`/`) in arguments\n- Colon-separated path lists (`/foo:/bar`)\n- Arguments with path components after `-` or `,`\n\n**What's exempt from conversion:**\n- Arguments containing `=` (variable assignments)\n- Drive letters (`C:`)\n- Arguments with `;` (already Windows format)\n- Arguments starting with `//` (network paths/Windows switches)\n\n## Shell Detection for Docker Commands\n\n### Detecting Git Bash / MINGW Environment\n\nUse these environment variables to detect when path conversion issues may occur:\n\n```bash\n# Most reliable: Check for MSYSTEM\nif [ -n \"$MSYSTEM\" ]; then\n  echo \"Running in Git Bash/MINGW - path conversion needed\"\nfi\n\n# Alternative: Check uname\nif [[ \"$(uname -s)\" == MINGW* ]]; then\n  echo \"Running in MINGW environment\"\nfi\n\n# Environment variable to check:\n# MSYSTEM values: MINGW64, MINGW32, MSYS\n```\n\n## Solution 1: MSYS_NO_PATHCONV (Recommended for Git Bash)\n\nThe most reliable solution for Git Bash on Windows.\n\n### Per-Command Usage\n\nPrefix each Docker command with `MSYS_NO_PATHCONV=1`:\n\n```bash\n# Volume mount with $(pwd)\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# Volume mount with absolute path\nMSYS_NO_PATHCONV=1 docker run -v /c/Users/project:/app myimage\n\n# Multiple volume mounts\nMSYS_NO_PATHCONV=1 docker run \\\n  -v $(pwd)/data:/data \\\n  -v $(pwd)/config:/etc/config \\\n  myimage\n```\n\n### Shell-Level Configuration\n\nDisable path conversion for all Docker commands in your session:\n\n```bash\n# Add to ~/.bashrc or run in current shell\nexport MSYS_NO_PATHCONV=1\n\n# Now all Docker commands work normally\ndocker run -v $(pwd):/app myimage\n```\n\n### Function Wrapper (Automatic per Docker Command)\n\nCreate a function wrapper that automatically disables path conversion:\n\n```bash\n# Add to ~/.bashrc\ndocker() {\n  (export MSYS_NO_PATHCONV=1; command docker.exe \"$@\")\n}\n\n# Or using MSYS2_ARG_CONV_EXCL for MSYS2\ndocker() {\n  (export MSYS2_ARG_CONV_EXCL='*'; command docker.exe \"$@\")\n}\n\n# Make function available in subshells\nexport -f docker\n```\n\n## Solution 2: MSYS2_ARG_CONV_EXCL (MSYS2 Specific)\n\nFor MSYS2 environments (not standard Git Bash):\n\n```bash\n# Disable all argument conversion\nexport MSYS2_ARG_CONV_EXCL='*'\n\n# Selective exclusion (specific patterns)\nexport MSYS2_ARG_CONV_EXCL='--dir=;/test'\n\n# Environment variable conversion exclusion\nexport MSYS2_ENV_CONV_EXCL='*'\n```\n\n## Solution 3: Double Leading Slash\n\nPrefix paths with an extra `/` to prevent conversion:\n\n```bash\n# Single slash (converted - BROKEN)\ndocker run -v /c/Users/project:/app myimage\n\n# Double slash (not converted - WORKS)\ndocker run -v //c/Users/project:/app myimage\n\n# Works in Git Bash on Windows\n# Treated as single slash on Linux (portable)\n```\n\n**Advantages:**\n- No environment variables needed\n- Works without wrapper functions\n- Portable (extra slash ignored on Linux)\n\n**Disadvantages:**\n- Less readable\n- Easy to forget\n- Doesn't look like standard Docker syntax\n\n## Solution 4: Use $(pwd) with Proper Escaping\n\nAlways use lowercase `$(pwd)` (not `$PWD`) with proper syntax:\n\n```bash\n# CORRECT: Round brackets, lowercase pwd, no quotes\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# CORRECT: With subdirectories\nMSYS_NO_PATHCONV=1 docker run -v $(pwd)/src:/app/src myimage\n\n# WRONG: Uppercase PWD variable (may not convert correctly)\ndocker run -v $PWD:/app myimage\n\n# WRONG: Without MSYS_NO_PATHCONV (path gets mangled)\ndocker run -v $(pwd):/app myimage\n```\n\n## Docker Volume Mount Best Practices (Git Bash on Windows)\n\n### For docker run Commands\n\n```bash\n# Named volumes (no path conversion issue)\ndocker run -v my-named-volume:/data myimage\n\n# Bind mount with MSYS_NO_PATHCONV (RECOMMENDED)\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# Bind mount with double slash (ALTERNATIVE)\ndocker run -v //c/Users/project:/app myimage\n\n# Read-only bind mount\nMSYS_NO_PATHCONV=1 docker run -v $(pwd)/config:/etc/config:ro myimage\n\n# Multiple volumes\nMSYS_NO_PATHCONV=1 docker run \\\n  -v $(pwd)/src:/app/src \\\n  -v $(pwd)/data:/app/data \\\n  -v my-named-volume:/var/lib/data \\\n  myimage\n```\n\n### For docker-compose.yml Files\n\nDocker Compose files use forward slashes and relative paths - **these work correctly** in Git Bash:\n\n```yaml\n# WORKS WITHOUT MODIFICATION in Git Bash\nservices:\n  app:\n    image: myimage\n    volumes:\n      # Relative paths (RECOMMENDED)\n      - ./src:/app/src\n      - ./data:/app/data\n\n      # Named volumes (RECOMMENDED)\n      - my-data:/var/lib/data\n\n      # Windows absolute paths with forward slashes (WORKS)\n      - C:/Users/project:/app\n\n      # Unix-style paths (WORKS if referring to WSL/MINGW paths)\n      - /c/Users/project:/app\n\nvolumes:\n  my-data:\n```\n\n**Important:** When running `docker compose` commands:\n\n```bash\n# No special handling needed for compose files\ndocker compose up -d\n\n# But if you use command-line volume overrides:\nMSYS_NO_PATHCONV=1 docker compose run -v $(pwd)/extra:/extra app\n```\n\n## Complete Examples\n\n### Example 1: Simple Application Development\n\n```bash\n# Set up environment once per session\nexport MSYS_NO_PATHCONV=1\n\n# Run with live code reload\ndocker run -d \\\n  --name dev-app \\\n  -v $(pwd)/src:/app/src \\\n  -v $(pwd)/public:/app/public \\\n  -p 3000:3000 \\\n  node:20-alpine \\\n  npm run dev\n\n# View logs\ndocker logs -f dev-app\n```\n\n### Example 2: Database with Data Persistence\n\n```bash\n# Use named volume for database data (no path issues)\ndocker run -d \\\n  --name postgres-db \\\n  -e POSTGRES_PASSWORD=mypassword \\\n  -v pgdata:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:16-alpine\n\n# Mount init scripts with MSYS_NO_PATHCONV\nMSYS_NO_PATHCONV=1 docker run -d \\\n  --name postgres-db \\\n  -e POSTGRES_PASSWORD=mypassword \\\n  -v pgdata:/var/lib/postgresql/data \\\n  -v $(pwd)/init.sql:/docker-entrypoint-initdb.d/init.sql:ro \\\n  -p 5432:5432 \\\n  postgres:16-alpine\n```\n\n### Example 3: Full Stack with Docker Compose\n\n```bash\n# Project structure:\n# /c/Users/project/\n#    docker-compose.yml\n#    backend/\n#    frontend/\n#    data/\n\n# docker-compose.yml (no special path handling needed)\ncat > docker-compose.yml <<'EOF'\nservices:\n  backend:\n    build: ./backend\n    volumes:\n      - ./backend/src:/app/src\n      - ./data:/app/data\n    ports:\n      - \"4000:4000\"\n\n  frontend:\n    build: ./frontend\n    volumes:\n      - ./frontend/src:/app/src\n      - ./frontend/public:/app/public\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - backend\n\n  database:\n    image: postgres:16-alpine\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: changeme\n\nvolumes:\n  db-data:\nEOF\n\n# Start everything (works normally)\ndocker compose up -d\n\n# Override with additional volume (needs MSYS_NO_PATHCONV)\nMSYS_NO_PATHCONV=1 docker compose run -v $(pwd)/scripts:/scripts backend bash\n```\n\n## Troubleshooting Path Issues\n\n### Problem: \"No such file or directory\" errors\n\n**Symptoms:**\n```\nError response from daemon: invalid mount config for type \"bind\":\nbind source path does not exist: C:\\Program Files\\Git\\c\\Users\\project\n```\n\n**Diagnosis:**\n- Path has been incorrectly converted by MINGW\n- Notice `C:\\Program Files\\Git\\` prefix added\n\n**Solution:**\n```bash\n# Add MSYS_NO_PATHCONV before command\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n```\n\n### Problem: Volume appears empty in container\n\n**Symptoms:**\n- Container starts successfully\n- But mounted directory is empty\n- Files exist on host\n\n**Diagnosis:**\n- Path conversion mangled the source path\n- Docker created empty directory instead\n\n**Solution:**\n```bash\n# Use MSYS_NO_PATHCONV with $(pwd)\nMSYS_NO_PATHCONV=1 docker run -v $(pwd)/data:/data myimage\n\n# Or use double slash\ndocker run -v //c/Users/project/data:/data myimage\n\n# Or use named volumes for persistent data\ndocker run -v my-named-volume:/data myimage\n```\n\n### Problem: Path with spaces fails\n\n**Symptoms:**\n```\nError: invalid reference format\n```\n\n**Diagnosis:**\n- Spaces in Windows paths not properly quoted\n- Path conversion + spaces = disaster\n\n**Solution:**\n```bash\n# Use MSYS_NO_PATHCONV and quote $(pwd)\nMSYS_NO_PATHCONV=1 docker run -v \"$(pwd)\":/app myimage\n\n# Or avoid spaces in project paths entirely (RECOMMENDED)\n# Move project from:\n#   C:/Users/My Name/My Projects/app\n# To:\n#   C:/Users/MyName/projects/app\n```\n\n### Problem: $PWD not working correctly\n\n**Symptoms:**\n- Using `$PWD` variable instead of `$(pwd)`\n- Inconsistent behavior\n\n**Solution:**\n```bash\n# WRONG: Using $PWD\ndocker run -v $PWD:/app myimage\n\n# CORRECT: Using $(pwd)\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# Explanation:\n# $(pwd) is a command that returns current directory\n# $PWD is an environment variable that may not be properly formatted\n```\n\n## Testing Your Configuration\n\nCreate a test script to verify Docker volume mounts work correctly:\n\n```bash\n#!/bin/bash\n# test-docker-volume.sh\n\necho \"Testing Docker volume mounts in Git Bash...\"\n\n# Create test file\nmkdir -p test-mount\necho \"Hello from host\" > test-mount/test.txt\n\n# Test 1: With MSYS_NO_PATHCONV\necho \"Test 1: With MSYS_NO_PATHCONV\"\nMSYS_NO_PATHCONV=1 docker run --rm -v $(pwd)/test-mount:/data alpine cat /data/test.txt\n\n# Test 2: With double slash\necho \"Test 2: With double slash\"\ndocker run --rm -v //$(pwd)/test-mount:/data alpine cat /data/test.txt\n\n# Test 3: Without workaround (should fail)\necho \"Test 3: Without workaround (may fail)\"\ndocker run --rm -v $(pwd)/test-mount:/data alpine cat /data/test.txt\n\n# Cleanup\nrm -rf test-mount\n\necho \"Testing complete!\"\n```\n\nRun it:\n```bash\nchmod +x test-docker-volume.sh\n./test-docker-volume.sh\n```\n\n## Recommended Configuration\n\nAdd to your `~/.bashrc`:\n\n```bash\n# Docker path conversion fix for Git Bash on Windows\nexport MSYS_NO_PATHCONV=1\n\n# Or use a wrapper function if you prefer per-command control\ndocker() {\n  (export MSYS_NO_PATHCONV=1; command docker.exe \"$@\")\n}\nexport -f docker\n\n# Alias for docker compose (if needed)\nalias docker-compose='MSYS_NO_PATHCONV=1 docker compose'\n```\n\n## Platform Detection Script\n\nUse this to automatically detect and configure Docker for Git Bash:\n\n```bash\n# Add to ~/.bashrc\n# Detect if running in Git Bash/MINGW on Windows\nif [ -n \"$MSYSTEM\" ] || [[ \"$(uname -s)\" == MINGW* ]]; then\n  # Running in Git Bash/MINGW\n  echo \"Git Bash detected - enabling Docker path conversion fix\"\n  export MSYS_NO_PATHCONV=1\n\n  # Optional: Create wrapper function for explicit control\n  docker() {\n    (export MSYS_NO_PATHCONV=1; command docker.exe \"$@\")\n  }\n  export -f docker\nfi\n```\n\n## Quick Reference\n\n### Environment Variables\n\n| Variable | Purpose | Values |\n|----------|---------|--------|\n| `MSYS_NO_PATHCONV` | Disable all path conversion (Git Bash) | `1` to disable |\n| `MSYS2_ARG_CONV_EXCL` | Exclude specific arguments (MSYS2) | `'*'` or patterns |\n| `MSYS2_ENV_CONV_EXCL` | Exclude environment variables (MSYS2) | `'*'` or patterns |\n| `MSYSTEM` | MSYS subsystem indicator | `MINGW64`, `MINGW32`, `MSYS` |\n\n### Command Patterns\n\n```bash\n# RECOMMENDED: Export once per session\nexport MSYS_NO_PATHCONV=1\ndocker run -v $(pwd):/app myimage\n\n# Per-command prefix\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# Double slash workaround\ndocker run -v //c/Users/project:/app myimage\n\n# Named volumes (no path issues)\ndocker run -v my-data:/data myimage\n\n# Docker Compose (relative paths work)\ndocker compose up -d\n```\n\n### What Works Without Modification\n\nThese Docker commands work normally in Git Bash without special handling:\n\n- Named volumes: `-v my-volume:/data`\n- Network commands: `docker network create`\n- Image commands: `docker build`, `docker pull`\n- Container commands without volumes: `docker run myimage`\n- Docker Compose with relative paths in YAML\n\n### What Needs MSYS_NO_PATHCONV\n\nThese commands require path conversion fixes:\n\n- Bind mounts with absolute paths: `-v /c/Users/project:/app`\n- Bind mounts with $(pwd): `-v $(pwd):/app`\n- Volume mounts on docker run command line\n- Any command with Unix-style absolute paths\n\n## Summary\n\n**Best Practice for Git Bash on Windows:**\n\n1. **Add to ~/.bashrc:** `export MSYS_NO_PATHCONV=1`\n2. **Use relative paths in docker-compose.yml:** `./src:/app/src`\n3. **Use named volumes for data:** `my-data:/var/lib/data`\n4. **Use $(pwd) with MSYS_NO_PATHCONV** for bind mounts: `MSYS_NO_PATHCONV=1 docker run -v $(pwd):/app`\n\nThis configuration ensures Docker commands work correctly in Git Bash on Windows without path conversion issues."
              },
              {
                "name": "docker-platform-guide",
                "description": "Platform-specific Docker considerations for Windows, Linux, and macOS",
                "path": "plugins/docker-master/skills/docker-platform-guide/SKILL.md",
                "frontmatter": {
                  "name": "docker-platform-guide",
                  "description": "Platform-specific Docker considerations for Windows, Linux, and macOS"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Docker Platform-Specific Guide\n\nThis skill provides detailed guidance on Docker differences, considerations, and optimizations for Windows, Linux, and macOS platforms.\n\n## Linux\n\n### Advantages\n\n- **Native containers:** No virtualization layer overhead\n- **Best performance:** Direct kernel features (cgroups, namespaces)\n- **Full feature set:** All Docker features available\n- **Production standard:** Most production deployments run on Linux\n- **Flexibility:** Multiple distributions supported\n\n### Platform Features\n\n**Container Technologies:**\n- Namespaces: PID, network, IPC, mount, UTS, user\n- cgroups v1 and v2 for resource control\n- Overlay2 storage driver (recommended)\n- SELinux and AppArmor for mandatory access control\n\n**Storage Drivers:**\n```bash\n# Check current driver\ndocker info | grep \"Storage Driver\"\n\n# Recommended: overlay2\n# /etc/docker/daemon.json\n{\n  \"storage-driver\": \"overlay2\"\n}\n```\n\n### Linux-Specific Configuration\n\n**Daemon Configuration** (`/etc/docker/daemon.json`):\n```json\n{\n  \"storage-driver\": \"overlay2\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"live-restore\": true,\n  \"userland-proxy\": false,\n  \"userns-remap\": \"default\",\n  \"icc\": false,\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 64000,\n      \"Soft\": 64000\n    }\n  }\n}\n```\n\n**User Namespace Remapping:**\n```bash\n# Enable in daemon.json\n{\n  \"userns-remap\": \"default\"\n}\n\n# Restart Docker\nsudo systemctl restart docker\n\n# Result: root in container = unprivileged user on host\n```\n\n### SELinux Integration\n\n```bash\n# Check SELinux status\nsestatus\n\n# Run container with SELinux enabled\ndocker run --security-opt label=type:svirt_sandbox_file_t myimage\n\n# Volume labels\ndocker run -v /host/path:/container/path:z myimage  # Private label\ndocker run -v /host/path:/container/path:Z myimage  # Shared label\n```\n\n### AppArmor Integration\n\n```bash\n# Check AppArmor status\nsudo aa-status\n\n# Run with default Docker profile\ndocker run --security-opt apparmor=docker-default myimage\n\n# Create custom profile\nsudo aa-genprof docker run myimage\n```\n\n### Systemd Integration\n\n```bash\n# Check Docker service status\nsudo systemctl status docker\n\n# Enable on boot\nsudo systemctl enable docker\n\n# Restart Docker\nsudo systemctl restart docker\n\n# View logs\nsudo journalctl -u docker -f\n\n# Configure service\nsudo systemctl edit docker\n```\n\n### cgroup v1 vs v2\n\n```bash\n# Check cgroup version\nstat -fc %T /sys/fs/cgroup/\n\n# If using cgroup v2, ensure Docker version >= 20.10\n\n# /etc/docker/daemon.json\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n```\n\n### Linux Distribution Specifics\n\n**Ubuntu/Debian:**\n```bash\n# Install Docker\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n# Non-root user\nsudo usermod -aG docker $USER\n```\n\n**RHEL/CentOS/Fedora:**\n```bash\n# Install Docker\nsudo dnf -y install dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\nsudo dnf install docker-ce docker-ce-cli containerd.io\n\n# Start Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Non-root user\nsudo usermod -aG docker $USER\n```\n\n**Alpine:**\n```bash\n# Install Docker\napk add docker docker-compose\n\n# Start Docker\nrc-update add docker boot\nservice docker start\n```\n\n## macOS\n\n### Architecture\n\n- **Docker Desktop:** Required (no native Docker on macOS)\n- **Virtualization:** Uses HyperKit (Intel) or Virtualization.framework (Apple Silicon)\n- **Linux VM:** Containers run in lightweight Linux VM\n- **File sharing:** osxfs or VirtioFS for bind mounts\n\n### macOS-Specific Considerations\n\n**Resource Allocation:**\n```\nDocker Desktop  Preferences  Resources  Advanced\n- CPUs: Allocate based on workload (default: half available)\n- Memory: Allocate generously (default: 2GB, recommend 4-8GB)\n- Swap: 1GB minimum\n- Disk image size: 60GB+ for development\n```\n\n**File Sharing Performance:**\n\nTraditional osxfs is slow. Improvements:\n1. **VirtioFS:** Enable in Docker Desktop settings (faster)\n2. **Delegated/Cached mounts:**\n\n```yaml\nvolumes:\n  # Host writes delayed (best for source code)\n  - ./src:/app/src:delegated\n\n  # Container writes cached (best for build outputs)\n  - ./build:/app/build:cached\n\n  # Default consistency (slowest but safest)\n  - ./data:/app/data:consistent\n```\n\n**Network Access:**\n\n```bash\n# Access host from container\nhost.docker.internal\n\n# Example: Connect to host PostgreSQL\ndocker run -e DATABASE_URL=postgresql://host.docker.internal:5432/db myapp\n```\n\n### Apple Silicon (M1/M2/M3) Specifics\n\n**Architecture Considerations:**\n```bash\n# Check image architecture\ndocker image inspect node:20-alpine | grep Architecture\n\n# M-series Macs are ARM64\n# Some images only available for AMD64\n\n# Build multi-platform\ndocker buildx build --platform linux/amd64,linux/arm64 -t myapp .\n\n# Run AMD64 image on ARM (via emulation)\ndocker run --platform linux/amd64 myimage  # Slower\n```\n\n**Rosetta 2 Integration:**\n```\nDocker Desktop  Features in development  Use Rosetta for x86/amd64 emulation\n```\nFaster AMD64 emulation on Apple Silicon.\n\n### macOS Docker Desktop Settings\n\n**General:**\n-  Start Docker Desktop when you log in\n-  Use VirtioFS (better performance)\n-  Use Virtualization framework (Apple Silicon)\n\n**Resources:**\n```\nCPUs: 4-6 (for development)\nMemory: 6-8 GB (for development)\nSwap: 1-2 GB\nDisk image size: 100+ GB (grows dynamically)\n```\n\n**Docker Engine:**\n```json\n{\n  \"builder\": {\n    \"gc\": {\n      \"enabled\": true,\n      \"defaultKeepStorage\": \"20GB\"\n    }\n  },\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": true\n  }\n}\n```\n\n### macOS File Permissions\n\n```bash\n# macOS user ID and group ID\nid -u  # Usually 501\nid -g  # Usually 20\n\n# Match in container\ndocker run --user 501:20 myimage\n\n# Or in Dockerfile\nRUN adduser -u 501 -g 20 appuser\nUSER appuser\n```\n\n### macOS Development Workflow\n\n```yaml\n# docker-compose.yml for development\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      # Source code with delegated (better performance)\n      - ./src:/app/src:delegated\n      # node_modules in volume (much faster than bind mount)\n      - node_modules:/app/node_modules\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=development\n\nvolumes:\n  node_modules:\n```\n\n### Common macOS Issues\n\n**Problem:** Slow file sync\n**Solution:**\n- Use VirtioFS\n- Use delegated/cached mounts\n- Store dependencies in volumes (not bind mounts)\n\n**Problem:** High CPU usage\n**Solution:**\n- Reduce file watching\n- Exclude large directories from file sharing\n- Allocate more resources\n\n**Problem:** Port already in use\n**Solution:**\n```bash\n# Find process using port\nlsof -i :PORT\nkill -9 PID\n```\n\n## Windows\n\n### Windows Container Types\n\n**1. Linux Containers on Windows (LCOW):**\n- Most common for development\n- Uses WSL2 or Hyper-V backend\n- Runs Linux containers\n- Good compatibility\n\n**2. Windows Containers:**\n- Native Windows containers\n- For Windows-specific workloads\n- Requires Windows Server base images\n- Less common in development\n\n### Windows Backend Options\n\n**WSL2 Backend (Recommended):**\n- Faster\n- Better resource usage\n- Native Linux kernel\n- Requires Windows 10/11 (recent versions)\n\n**Hyper-V Backend:**\n- Older option\n- More resource intensive\n- Works on older Windows versions\n\n### WSL2 Configuration\n\n**Enable WSL2:**\n```powershell\n# Run as Administrator\nwsl --install\n\n# Or manually\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n\n# Set WSL2 as default\nwsl --set-default-version 2\n\n# Install Ubuntu (or other distro)\nwsl --install -d Ubuntu\n```\n\n**Docker Desktop Integration:**\n```\nSettings  Resources  WSL Integration\n- Enable integration with default distro\n- Select additional distros\n```\n\n### Windows Path Considerations\n\n**Path Formats:**\n```bash\n# Forward slashes (recommended, works everywhere)\ndocker run -v C:/Users/name/project:/app myimage\n\n# Backslashes (need escaping in some contexts)\ndocker run -v C:\\Users\\name\\project:/app myimage\n\n# In docker-compose.yml (forward slashes)\nvolumes:\n  - C:/Users/name/project:/app\n\n# Or relative paths\nvolumes:\n  - ./src:/app/src\n```\n\n### Git Bash / MINGW Path Conversion Issues\n\n**CRITICAL ISSUE:** When using Docker in Git Bash (MINGW) on Windows, automatic path conversion breaks volume mounts.\n\n**The Problem:**\n```bash\n# What you type in Git Bash:\ndocker run -v $(pwd):/app myimage\n\n# What Git Bash converts it to (BROKEN):\ndocker run -v C:\\Program Files\\Git\\d\\repos\\project:/app myimage\n```\n\n**Solutions:**\n\n**1. MSYS_NO_PATHCONV (Recommended):**\n```bash\n# Per-command fix\nMSYS_NO_PATHCONV=1 docker run -v $(pwd):/app myimage\n\n# Session-wide fix (add to ~/.bashrc)\nexport MSYS_NO_PATHCONV=1\n\n# Function wrapper (automatic for all Docker commands)\ndocker() {\n  (export MSYS_NO_PATHCONV=1; command docker.exe \"$@\")\n}\nexport -f docker\n```\n\n**2. Double Slash Workaround:**\n```bash\n# Use double leading slash to prevent conversion\ndocker run -v //c/Users/project:/app myimage\n\n# Works with $(pwd) too\ndocker run -v //$(pwd):/app myimage\n```\n\n**3. Named Volumes (No Path Issues):**\n```bash\n# Named volumes work without any fixes\ndocker run -v my-data:/data myimage\n```\n\n**What Works Without Modification:**\n- Docker Compose YAML files with relative paths\n- Named volumes\n- Network and image commands\n- Container commands without volumes\n\n**What Needs MSYS_NO_PATHCONV:**\n- Bind mounts with `$(pwd)`\n- Bind mounts with absolute Unix-style paths\n- Volume mounts specified on command line\n\n**Shell Detection:**\n```bash\n# Detect Git Bash/MINGW and auto-configure\nif [ -n \"$MSYSTEM\" ] || [[ \"$(uname -s)\" == MINGW* ]]; then\n  export MSYS_NO_PATHCONV=1\n  echo \"Git Bash detected - Docker path conversion fix enabled\"\nfi\n```\n\n**Recommended ~/.bashrc Configuration:**\n```bash\n# Docker on Git Bash fix\nif [ -n \"$MSYSTEM\" ]; then\n  export MSYS_NO_PATHCONV=1\nfi\n```\n\nSee the `docker-git-bash-guide` skill for comprehensive path conversion documentation, troubleshooting, and examples.\n\n### Windows File Sharing\n\n**Configure Shared Drives:**\n```\nDocker Desktop  Settings  Resources  File Sharing\nAdd: C:\\, D:\\, etc.\n```\n\n**Performance Considerations:**\n- File sharing is slower than Linux/Mac\n- Use WSL2 backend for better performance\n- Store frequently accessed files in WSL2 filesystem\n\n### Windows Line Endings\n\n**Problem:** CRLF vs LF line endings\n\n**Solution:**\n```bash\n# Git configuration\ngit config --global core.autocrlf input\n\n# Or per-repo (.gitattributes)\n* text=auto\n*.sh text eol=lf\n*.bat text eol=crlf\n```\n\n```dockerfile\n# In Dockerfile for scripts\nFROM alpine\nCOPY --chmod=755 script.sh /\n# Ensure LF endings\nRUN dos2unix /script.sh || sed -i 's/\\r$//' /script.sh\n```\n\n### Windows Firewall\n\n```powershell\n# Allow Docker Desktop\nNew-NetFirewallRule -DisplayName \"Docker Desktop\" -Direction Inbound -Program \"C:\\Program Files\\Docker\\Docker\\Docker Desktop.exe\" -Action Allow\n\n# Check blocked ports\nnetstat -ano | findstr :PORT\n```\n\n### Windows-Specific Docker Commands\n\n```powershell\n# Run PowerShell in container\ndocker run -it mcr.microsoft.com/powershell:lts-7.4-windowsservercore-ltsc2022\n\n# Windows container example\ndocker run -it mcr.microsoft.com/windows/servercore:ltsc2022 cmd\n\n# Check container type\ndocker info | Select-String \"OSType\"\n```\n\n### WSL2 Disk Management\n\n**Problem:** WSL2 VHDX grows but doesn't shrink\n\n**Solution:**\n```powershell\n# Stop Docker Desktop and WSL\nwsl --shutdown\n\n# Compact disk image (run as Administrator)\n# Method 1: Optimize-VHD (requires Hyper-V tools)\nOptimize-VHD -Path \"$env:LOCALAPPDATA\\Docker\\wsl\\data\\ext4.vhdx\" -Mode Full\n\n# Method 2: diskpart\ndiskpart\n# In diskpart:\nselect vdisk file=\"C:\\Users\\YourName\\AppData\\Local\\Docker\\wsl\\data\\ext4.vhdx\"\nattach vdisk readonly\ncompact vdisk\ndetach vdisk\nexit\n```\n\n### Windows Development Workflow\n\n```yaml\n# docker-compose.yml for Windows\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      # Use forward slashes\n      - ./src:/app/src\n      # Named volumes for better performance\n      - node_modules:/app/node_modules\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=development\n    # Windows-specific: ensure proper line endings\n    command: sh -c \"dos2unix /app/scripts/*.sh && npm start\"\n\nvolumes:\n  node_modules:\n```\n\n### Common Windows Issues\n\n**Problem:** Permission denied errors\n**Solution:**\n```powershell\n# Run Docker Desktop as Administrator\n# Or grant permissions to Docker Desktop\nicacls \"C:\\ProgramData\\DockerDesktop\" /grant Users:F /T\n```\n\n**Problem:** Slow performance\n**Solution:**\n- Use WSL2 backend\n- Store project in WSL2 filesystem (`\\\\wsl$\\Ubuntu\\home\\user\\project`)\n- Use named volumes for node_modules, etc.\n\n**Problem:** Path not found\n**Solution:**\n- Use forward slashes\n- Ensure drive is shared in Docker Desktop\n- Use absolute paths or `${PWD}`\n\n## Platform Comparison\n\n| Feature | Linux | macOS | Windows |\n|---------|-------|-------|---------|\n| **Performance** | Excellent (native) | Good (VM overhead) | Good (WSL2) to Fair (Hyper-V) |\n| **File sharing** | Native | Slow (improving with VirtioFS) | Slow (better in WSL2) |\n| **Resource efficiency** | Best | Good | Good (WSL2) |\n| **Feature set** | Complete | Complete | Complete (LCOW) |\n| **Production** | Standard | Dev only | Dev only (LCOW) |\n| **Ease of use** | Moderate | Easy (Docker Desktop) | Easy (Docker Desktop) |\n| **Cost** | Free | Free (Docker Desktop Personal) | Free (Docker Desktop Personal) |\n\n## Cross-Platform Best Practices\n\n### Multi-Platform Images\n\n```bash\n# Create buildx builder\ndocker buildx create --name multiplatform --driver docker-container --use\n\n# Build for multiple platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64,linux/arm/v7 \\\n  -t myimage:latest \\\n  --push \\\n  .\n```\n\n### Platform-Agnostic Dockerfiles\n\n```dockerfile\n# Works on all platforms\nFROM node:20-alpine\n\n# Use COPY with --chmod (not RUN chmod, which is slower)\nCOPY --chmod=755 script.sh /usr/local/bin/\n\n# Use environment variables for paths\nENV APP_HOME=/app\nWORKDIR ${APP_HOME}\n\n# Use exec form for CMD/ENTRYPOINT (works on Windows containers too)\nCMD [\"node\", \"server.js\"]\n```\n\n### Cross-Platform Compose Files\n\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      # Relative paths work everywhere\n      - ./src:/app/src\n      # Named volumes (platform-agnostic)\n      - data:/app/data\n    environment:\n      # Use environment variables\n      - NODE_ENV=${NODE_ENV:-development}\n\nvolumes:\n  data:\n```\n\n### Testing Across Platforms\n\n```bash\n# Test on different platforms with buildx\ndocker buildx build --platform linux/amd64 -t myapp:amd64 --load .\ndocker run --rm myapp:amd64\n\ndocker buildx build --platform linux/arm64 -t myapp:arm64 --load .\ndocker run --rm myapp:arm64\n```\n\n## Platform Selection Guide\n\n**Choose Linux for:**\n- Production deployments\n- Maximum performance\n- Full Docker feature set\n- Minimal overhead\n- CI/CD pipelines\n\n**Choose macOS for:**\n- Development on Mac hardware\n- When you need macOS tools\n- Docker Desktop ease of use\n- M1/M2/M3 development\n\n**Choose Windows for:**\n- Development on Windows hardware\n- Windows-specific applications\n- When team uses Windows\n- WSL2 for better Linux container support\n\nThis platform guide covers the major differences. Always test on your target deployment platform before going to production."
              },
              {
                "name": "docker-security-guide",
                "description": "Comprehensive Docker security guidelines and threat mitigation strategies",
                "path": "plugins/docker-master/skills/docker-security-guide/SKILL.md",
                "frontmatter": {
                  "name": "docker-security-guide",
                  "description": "Comprehensive Docker security guidelines and threat mitigation strategies"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Docker Security Guide\n\nThis skill provides comprehensive security guidelines for Docker across all platforms, covering threats, mitigations, and compliance requirements.\n\n## Security Principles\n\n### Defense in Depth\n\nApply security at multiple layers:\n1. **Image security:** Minimal, scanned, signed images\n2. **Build security:** Secure build process, no secrets in layers\n3. **Runtime security:** Restricted capabilities, resource limits\n4. **Network security:** Isolation, least privilege\n5. **Host security:** Hardened host OS, updated Docker daemon\n6. **Orchestration security:** Secure configuration, RBAC\n7. **Monitoring:** Detection, logging, alerting\n\n### Least Privilege\n\nGrant only the minimum permissions necessary:\n- Non-root users\n- Dropped capabilities\n- Read-only filesystems\n- Minimal network exposure\n- Restricted syscalls (seccomp)\n- Limited resources\n\n## Image Security\n\n### Base Image Selection\n\n**Threat:** Vulnerable or malicious base images\n\n**Mitigation:**\n```dockerfile\n# Use official images only\nFROM node:20.11.0-alpine3.19  # Official, specific version\n\n# NOT\nFROM randomuser/node  # Unverified source\nFROM node:latest      # Unpredictable, can break\n```\n\n**Verification:**\n```bash\n# Verify image source\ndocker image inspect node:20-alpine | grep -A 5 \"Author\"\n\n# Enable Docker Content Trust (image signing)\nexport DOCKER_CONTENT_TRUST=1\ndocker pull node:20-alpine\n```\n\n### Minimal Images\n\n**Threat:** Larger attack surface, more vulnerabilities\n\n**Mitigation:**\n```dockerfile\n# Prefer minimal distributions\nFROM alpine:3.19           # ~7MB\nFROM gcr.io/distroless/static  # ~2MB\nFROM scratch               # 0MB (for static binaries)\n\n# vs\nFROM ubuntu:22.04          # ~77MB with more packages\n```\n\n**Benefits:**\n- Fewer packages = fewer vulnerabilities\n- Smaller attack surface\n- Faster downloads and starts\n- Less disk space\n\n### Micro-Distros for Security-Critical Applications (2025)\n\n**Wolfi/Chainguard Images:**\n- Zero-CVE goal, SBOM included by default\n- Nightly security patches, signed with provenance\n- Available for: Node, Python, Go, Java, .NET, etc.\n\n**Usage:**\n```dockerfile\n# Development stage (includes build tools)\nFROM cgr.dev/chainguard/node:latest-dev AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Production stage (minimal, zero-CVE goal)\nFROM cgr.dev/chainguard/node:latest\nWORKDIR /app\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY . .\nUSER node\nENTRYPOINT [\"node\", \"server.js\"]\n```\n\n**When to use:** Security-critical apps, compliance requirements (SOC2, HIPAA, PCI-DSS), zero-trust environments, supply chain security emphasis.\n\nSee `docker-best-practices` skill for full image comparison table.\n\n### Vulnerability Scanning\n\n**Tools:**\n- Docker Scout (built-in)\n- Trivy\n- Grype\n- Snyk\n- Clair\n\n**Process:**\n```bash\n# Scan with Docker Scout\ndocker scout cves IMAGE_NAME\ndocker scout recommendations IMAGE_NAME\n\n# Scan with Trivy\ntrivy image IMAGE_NAME\ntrivy image --severity HIGH,CRITICAL IMAGE_NAME\n\n# Scan Dockerfile\ntrivy config Dockerfile\n\n# Scan for secrets\ntrivy fs --scanners secret .\n```\n\n**CI/CD Integration:**\n```yaml\n# GitHub Actions example\n- name: Scan image\n  run: |\n    docker scout cves my-image:${{ github.sha }}\n    trivy image --exit-code 1 --severity CRITICAL my-image:${{ github.sha }}\n```\n\n### Multi-Stage Builds for Security\n\n**Threat:** Build tools and secrets in final image\n\n**Mitigation:**\n```dockerfile\n# Build stage with build tools\nFROM golang:1.21 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o app\n\n# Final stage - minimal, no build tools\nFROM gcr.io/distroless/base-debian11\nCOPY --from=builder /app/app /\nUSER nonroot:nonroot\nENTRYPOINT [\"/app\"]\n```\n\n**Benefits:**\n- No compiler/build tools in production image\n- Secrets used in build don't persist\n- Smaller, more secure final image\n\n## Build-Time Security\n\n### Secrets Management\n\n**NEVER:**\n```dockerfile\n# BAD - Secret in layer history\nENV API_KEY=abc123\nRUN git clone https://user:password@github.com/repo.git\nCOPY .env /app/.env\n```\n\n**DO:**\n```dockerfile\n# Use BuildKit secrets\n# syntax=docker/dockerfile:1\n\nFROM alpine\nRUN --mount=type=secret,id=github_token \\\n    git clone https://$(cat /run/secrets/github_token)@github.com/repo.git\n```\n\n```bash\n# Build with secret (not in image)\ndocker build --secret id=github_token,src=./token.txt .\n```\n\n### BuildKit Frontend Security (2025)\n\n**Threat:** Malicious or compromised BuildKit frontends can execute arbitrary code during build\n\n** 2025 CRITICAL WARNING:** BuildKit supports custom frontends (parsers) via `# syntax=` directive. Untrusted frontends have FULL BUILD-TIME code execution and can:\n- Steal secrets from build context\n- Modify build outputs\n- Exfiltrate data\n- Compromise the build environment\n\n**Risk Example:**\n```dockerfile\n#  DANGER - Untrusted frontend (code execution risk!)\n# syntax=docker/dockerfile:1@sha256:abc123...untrusted\n\nFROM alpine\nRUN echo \"This frontend could do anything during build\"\n```\n\n**Mitigation:**\n\n1. **Only use official Docker frontends:**\n```dockerfile\n#  Safe - Official Docker frontend\n# syntax=docker/dockerfile:1\n\n#  Safe - Specific version\n# syntax=docker/dockerfile:1.5\n\n#  Safe - Pinned with digest (verify from docker.com)\n# syntax=docker/dockerfile:1@sha256:ac85f380a63b13dfcefa89046420e1781752bab202122f8f50032edf31be0021\n```\n\n2. **Verify frontend sources:**\n- Use ONLY `docker/dockerfile:*` frontends\n- Pin to specific versions with SHA256 digest\n- Verify digests from official Docker documentation\n- Never use third-party frontends without thorough vetting\n\n3. **Audit all Dockerfiles for unsafe syntax directives:**\n```bash\n# Check all Dockerfiles for potentially malicious syntax directives\ngrep -r \"^# syntax=\" . --include=\"Dockerfile*\"\n\n# Verify all frontends are official Docker images\ngrep -r \"^# syntax=\" . --include=\"Dockerfile*\" | grep -v \"docker/dockerfile\"\n```\n\n4. **BuildKit security configuration (defense in depth):**\n```bash\n# Restrict frontend sources in BuildKit config\n# /etc/buildkit/buildkitd.toml\n[frontend.\"dockerfile.v0\"]\n  # Only allow official Docker frontends\n  allowedImages = [\"docker.io/docker/dockerfile:*\"]\n```\n\n**Supply Chain Protection:**\n- Treat custom frontends as HIGH RISK code execution vectors\n- Review ALL `# syntax=` directives in Dockerfiles before builds\n- Use content trust for frontend images\n- Monitor for frontend vulnerabilities\n- Include frontend verification in CI/CD security gates\n\n### SBOM (Software Bill of Materials) Generation (2025)\n\n**Critical 2025 Requirement:** Document origin and history of all components for supply chain transparency and compliance.\n\n**Why SBOM is Mandatory:**\n- Supply chain security visibility\n- Vulnerability tracking and response\n- Compliance requirements (Executive Order 14028, etc.)\n- License compliance\n- Incident response readiness\n\n**Generate SBOM with Docker Scout:**\n```bash\n# Generate SBOM for image\ndocker scout sbom IMAGE_NAME\n\n# Export SBOM in different formats\ndocker scout sbom --format spdx IMAGE_NAME > sbom.spdx.json\ndocker scout sbom --format cyclonedx IMAGE_NAME > sbom.cyclonedx.json\n\n# Include SBOM attestation during build\n#  WARNING: BuildKit attestations are NOT cryptographically signed!\ndocker buildx build \\\n  --sbom=true \\\n  --provenance=true \\\n  --tag my-image:latest \\\n  .\n\n# View SBOM attestations (unsigned metadata only)\ndocker buildx imagetools inspect my-image:latest --format \"{{ json .SBOM }}\"\n```\n\n** CRITICAL SECURITY LIMITATION:**\nBuildKit attestations (`--sbom=true`, `--provenance=true`) are **NOT cryptographically signed**. This means:\n- Anyone with push access can create tampered attestations\n- SBOMs can be incomplete or falsified\n- Provenance data cannot be trusted without external verification\n- **For production:** Use external signing tools (cosign, Notary) and Syft for SBOM generation\n\n**Generate SBOM with Syft:**\n```bash\n# Install Syft\ncurl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh\n\n# Generate SBOM from image\nsyft my-image:latest\n\n# Generate in specific format\nsyft my-image:latest -o spdx-json > sbom.spdx.json\nsyft my-image:latest -o cyclonedx-json > sbom.cyclonedx.json\n\n# Generate from Dockerfile\nsyft dir:. -o spdx-json > sbom.spdx.json\n```\n\n**SBOM in CI/CD Pipeline:**\n```yaml\n# GitHub Actions example\nname: Build with SBOM\n\njobs:\n  build:\n    steps:\n      - name: Build image with SBOM\n        run: |\n          docker buildx build \\\n            --sbom=true \\\n            --provenance=true \\\n            --tag my-image:${{ github.sha }} \\\n            --push \\\n            .\n\n      - name: Generate SBOM with Syft\n        run: |\n          syft my-image:${{ github.sha }} -o spdx-json > sbom.json\n\n      - name: Upload SBOM artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: sbom\n          path: sbom.json\n\n      - name: Scan SBOM for vulnerabilities\n        run: |\n          grype sbom:sbom.json --fail-on high\n```\n\n**SBOM Best Practices:**\n\n1. **Generate for every image:**\n   - Production images: mandatory\n   - Development images: recommended\n   - Base images: critical\n\n2. **Store SBOMs with provenance:**\n   - Version control alongside Dockerfile\n   - Artifact registry with image\n   - Dedicated SBOM repository\n\n3. **Automate SBOM generation:**\n   - Integrate into CI/CD pipeline\n   - Generate on every build\n   - Fail builds if SBOM generation fails\n\n4. **Use SBOM for vulnerability management:**\n```bash\n# Scan SBOM instead of image (faster)\ngrype sbom:sbom.json\ntrivy sbom sbom.json\n\n# Compare SBOMs between versions\ndiff <(syft old-image:1.0 -o json) <(syft new-image:2.0 -o json)\n```\n\n5. **SBOM formats:**\n   - **SPDX:** Industry standard, ISO/IEC 5962:2021\n   - **CycloneDX:** OWASP standard, security-focused\n   - Choose based on compliance requirements\n\n**Chainguard Images with Built-in SBOM:**\n```bash\n# Chainguard images include SBOM attestation by default\ndocker buildx imagetools inspect cgr.dev/chainguard/node:latest\n\n# Extract SBOM\ncosign download sbom cgr.dev/chainguard/node:latest > chainguard-node-sbom.json\n```\n\n**Or use multi-stage and don't include secrets:**\n```dockerfile\nFROM node AS builder\nARG NPM_TOKEN\nRUN echo \"//registry.npmjs.org/:_authToken=${NPM_TOKEN}\" > .npmrc && \\\n    npm install && \\\n    rm .npmrc  # Still in layer history!\n\n# Better - secret only in build stage\nFROM node AS dependencies\nRUN --mount=type=secret,id=npmrc,target=/root/.npmrc \\\n    npm install\n\nFROM node AS runtime\nCOPY --from=dependencies /app/node_modules ./node_modules\n# No .npmrc in final image\n```\n\n### Secure Build Context\n\n**Threat:** Sensitive files included in build context\n\n**Mitigation:**\nCreate comprehensive `.dockerignore`:\n```\n# Secrets\n.env\n.env.local\n*.key\n*.pem\ncredentials.json\nsecrets/\n\n# Version control\n.git\n.gitignore\n\n# Cloud credentials\n.aws/\n.gcloud/\n\n# Private data\ndatabase.sql\nbackups/\n\n# SSH keys\n.ssh/\nid_rsa\nid_rsa.pub\n\n# Sensitive logs\n*.log\nlogs/\n```\n\n### Image Signing\n\n**Enable Docker Content Trust:**\n```bash\n# Enable image signing\nexport DOCKER_CONTENT_TRUST=1\n\n# Set up keys\ndocker trust key generate my-key\ndocker trust signer add --key my-key.pub my-name my-image\n\n# Push signed image\ndocker push my-image:tag\n\n# Pull only signed images\ndocker pull my-image:tag  # Fails if not signed\n```\n\n## Runtime Security\n\n### User Privileges\n\n**Threat:** Container escape via root\n\n**Mitigation:**\n```dockerfile\n# Create and use non-root user\nFROM node:20-alpine\nRUN addgroup -g 1001 appuser && \\\n    adduser -S appuser -u 1001 -G appuser\nUSER appuser\nWORKDIR /home/appuser/app\nCOPY --chown=appuser:appuser . .\nCMD [\"node\", \"server.js\"]\n```\n\n**Verification:**\n```bash\n# Check user in running container\ndocker exec container-name whoami  # Should not be root\ndocker exec container-name id       # Check UID/GID\n```\n\n### Capabilities\n\n**Threat:** Excessive kernel capabilities\n\n**Default Docker capabilities:**\n- CHOWN, DAC_OVERRIDE, FOWNER, FSETID\n- KILL, SETGID, SETUID, SETPCAP\n- NET_BIND_SERVICE, NET_RAW\n- SYS_CHROOT, MKNOD, AUDIT_WRITE, SETFCAP\n\n**Mitigation:**\n```bash\n# Drop all, add only needed\ndocker run \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  my-image\n```\n\n**In docker-compose.yml:**\n```yaml\nservices:\n  app:\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n```\n\n**Common needed capabilities:**\n- `NET_BIND_SERVICE`: Bind to ports < 1024\n- `NET_ADMIN`: Network configuration\n- `SYS_TIME`: Set system time\n\n### Read-Only Filesystem\n\n**Threat:** Container modification, malware persistence\n\n**Mitigation:**\n```bash\ndocker run \\\n  --read-only \\\n  --tmpfs /tmp:noexec,nosuid,size=64M \\\n  --tmpfs /var/run:noexec,nosuid,size=64M \\\n  my-image\n```\n\n**In Compose:**\n```yaml\nservices:\n  app:\n    read_only: true\n    tmpfs:\n      - /tmp:noexec,nosuid,size=64M\n      - /var/run:noexec,nosuid,size=64M\n```\n\n### Security Options\n\n**no-new-privileges:**\n```bash\ndocker run --security-opt=\"no-new-privileges:true\" my-image\n```\n\nPrevents privilege escalation via setuid/setgid binaries.\n\n**AppArmor (Linux):**\n```bash\ndocker run --security-opt=\"apparmor=docker-default\" my-image\n```\n\n**SELinux (Linux):**\n```bash\ndocker run --security-opt=\"label=type:container_runtime_t\" my-image\n```\n\n**Seccomp (syscall filtering):**\n```bash\n# Use default profile\ndocker run --security-opt=\"seccomp=default\" my-image\n\n# Or custom profile\ndocker run --security-opt=\"seccomp=./seccomp-profile.json\" my-image\n```\n\n### Resource Limits\n\n**Threat:** DoS via resource exhaustion\n\n**Mitigation:**\n```bash\ndocker run \\\n  --memory=\"512m\" \\\n  --memory-swap=\"512m\" \\  # Disable swap\n  --cpus=\"1.0\" \\\n  --pids-limit=100 \\\n  --ulimit nofile=1024:1024 \\\n  my-image\n```\n\n**In Compose:**\n```yaml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 512M\n          pids: 100\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n    ulimits:\n      nofile:\n        soft: 1024\n        hard: 1024\n```\n\n### Comprehensive Secure Run Command\n\n```bash\ndocker run \\\n  --name secure-app \\\n  --detach \\\n  --restart unless-stopped \\\n  --user 1000:1000 \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  --read-only \\\n  --tmpfs /tmp:noexec,nosuid,size=64M \\\n  --security-opt=\"no-new-privileges:true\" \\\n  --security-opt=\"seccomp=default\" \\\n  --memory=\"512m\" \\\n  --cpus=\"1.0\" \\\n  --pids-limit=100 \\\n  --network=isolated-network \\\n  --publish 127.0.0.1:8080:8080 \\\n  --volume secure-data:/data:ro \\\n  --health-cmd=\"curl -f http://localhost/health || exit 1\" \\\n  --health-interval=30s \\\n  my-secure-image:1.2.3\n```\n\n## Network Security\n\n### Network Isolation\n\n**Threat:** Lateral movement between containers\n\n**Mitigation:**\n```yaml\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true  # No external access\n\nservices:\n  web:\n    networks:\n      - frontend\n\n  api:\n    networks:\n      - frontend\n      - backend\n\n  database:\n    networks:\n      - backend  # Isolated from frontend\n```\n\n### Port Exposure\n\n**Threat:** Unnecessary network exposure\n\n**Mitigation:**\n```bash\n# Bind to localhost only\ndocker run -p 127.0.0.1:8080:8080 my-image\n\n# NOT (binds to all interfaces)\ndocker run -p 8080:8080 my-image\n```\n\n**In Compose:**\n```yaml\nservices:\n  app:\n    ports:\n      - \"127.0.0.1:8080:8080\"  # Localhost only\n```\n\n### Inter-Container Communication\n\n```yaml\n# Disable default inter-container communication\n# /etc/docker/daemon.json\n{\n  \"icc\": false\n}\n```\n\nThen explicitly allow via networks:\n```yaml\nservices:\n  app1:\n    networks:\n      - app-network\n  app2:\n    networks:\n      - app-network  # Can communicate with app1\n\nnetworks:\n  app-network:\n    driver: bridge\n```\n\n## Secrets Management\n\n### Docker Secrets (Swarm Mode)\n\n```bash\n# Create secret\necho \"mypassword\" | docker secret create db_password -\n\n# Use in service\ndocker service create \\\n  --name my-service \\\n  --secret db_password \\\n  my-image\n\n# Access in container at /run/secrets/db_password\n```\n\n**In stack file:**\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    image: my-image\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    external: true\n```\n\n### Secrets Best Practices\n\n1. **Never in environment variables** (visible in `docker inspect`)\n2. **Never in images** (in layer history)\n3. **Never in version control** (Git history)\n4. **Mount as files** with restricted permissions\n5. **Use secret management systems** (Vault, AWS Secrets Manager, etc.)\n6. **Rotate regularly**\n\n**Alternative: Mounted secrets:**\n```bash\ndocker run -v /secure/secrets:/run/secrets:ro my-image\n```\n\n## Compliance & Benchmarking\n\n### CIS Docker Benchmark\n\nAutomated checking:\n```bash\n# Clone docker-bench-security\ngit clone https://github.com/docker/docker-bench-security.git\ncd docker-bench-security\nsudo sh docker-bench-security.sh\n\n# Or run as container\ndocker run --rm --net host --pid host --userns host \\\n  --cap-add audit_control \\\n  -v /var/lib:/var/lib:ro \\\n  -v /var/run/docker.sock:/var/run/docker.sock:ro \\\n  -v /usr/lib/systemd:/usr/lib/systemd:ro \\\n  -v /etc:/etc:ro \\\n  docker/docker-bench-security\n```\n\n### Key CIS Recommendations\n\n1. **Host Configuration**\n   - Keep Docker up to date\n   - Restrict network traffic between containers\n   - Set logging level to 'info'\n   - Enable Docker Content Trust\n\n2. **Docker Daemon**\n   - Use TLS for Docker daemon socket\n   - Don't expose daemon on TCP without TLS\n   - Enable user namespace support\n\n3. **Docker Files**\n   - Verify Docker files ownership and permissions\n   - Audit Docker files and directories\n\n4. **Container Images**\n   - Create user for container\n   - Use trusted base images\n   - Don't install unnecessary packages\n\n5. **Container Runtime**\n   - Run containers with limited privileges\n   - Set resource limits\n   - Don't share host network namespace\n\n## Monitoring & Detection\n\n### Logging\n\n```yaml\nservices:\n  app:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n        labels: \"service,env\"\n        env: \"ENV,VERSION\"\n```\n\n**Centralized logging:**\n```yaml\nservices:\n  app:\n    logging:\n      driver: \"syslog\"\n      options:\n        syslog-address: \"tcp://log-server:514\"\n        tag: \"{{.Name}}/{{.ID}}\"\n```\n\n### Runtime Monitoring\n\n**Tools:**\n- Falco: Runtime security monitoring\n- Sysdig: Container visibility\n- Prometheus + cAdvisor: Metrics\n- Docker events: Real-time events\n\n**Monitor for:**\n- Unexpected processes\n- File modifications\n- Network connections\n- Resource spikes\n- Failed authentication\n- Privilege escalation attempts\n\n```bash\n# Monitor Docker events\ndocker events --filter 'type=container' --filter 'event=start'\n\n# Watch specific container\ndocker events --filter \"container=my-container\"\n\n# Runtime security with Falco\ndocker run --rm -it \\\n  --privileged \\\n  -v /var/run/docker.sock:/host/var/run/docker.sock \\\n  -v /dev:/host/dev \\\n  -v /proc:/host/proc:ro \\\n  falcosecurity/falco\n```\n\n## Platform-Specific Security\n\n### Linux\n\n**User namespace remapping:**\n```json\n// /etc/docker/daemon.json\n{\n  \"userns-remap\": \"default\"\n}\n```\n\nBenefits: Root in container  unprivileged on host\n\n**SELinux:**\n```bash\n# Enable SELinux for Docker\nsetenforce 1\n\n# Run with SELinux labels\ndocker run --security-opt label=type:svirt_sandbox_file_t my-image\n\n# Volumes with SELinux\ndocker run -v /host/path:/container/path:z my-image\n```\n\n**AppArmor:**\n```bash\n# Check AppArmor status\naa-status\n\n# Run with AppArmor profile\ndocker run --security-opt apparmor=docker-default my-image\n```\n\n### Windows\n\n**Hyper-V isolation:**\n```powershell\n# More isolated than process isolation\ndocker run --isolation=hyperv my-image\n```\n\n**Windows Defender:**\n- Ensure real-time protection enabled\n- Configure exclusions carefully\n- Scan images regularly\n\n### macOS\n\n**Docker Desktop security:**\n- Keep Docker Desktop updated\n- Enable \"Use gRPC FUSE for file sharing\"\n- Limit file sharing to necessary paths\n- Review resource allocation\n\n## Security Checklist\n\n**Image:**\n- [ ] Based on official, minimal image\n- [ ] Specific version tag (not `latest`)\n- [ ] Scanned for vulnerabilities\n- [ ] No secrets in layers\n- [ ] Runs as non-root user\n- [ ] Signed (Content Trust)\n\n**Build:**\n- [ ] .dockerignore configured\n- [ ] Multi-stage build (if applicable)\n- [ ] Build secrets handled properly\n- [ ] Build from trusted sources only\n\n**Runtime:**\n- [ ] Non-root user\n- [ ] Capabilities dropped\n- [ ] Read-only filesystem (where possible)\n- [ ] Security options set\n- [ ] Resource limits configured\n- [ ] Isolated network\n- [ ] Minimal port exposure\n- [ ] Secrets mounted securely\n\n**Operations:**\n- [ ] CIS benchmark compliance\n- [ ] Logging configured\n- [ ] Monitoring in place\n- [ ] Regular vulnerability scans\n- [ ] Incident response plan\n- [ ] Regular updates\n- [ ] Audit logs enabled\n\n## Common Security Mistakes\n\n **NEVER:**\n- Run as root\n- Use `--privileged`\n- Mount Docker socket (`/var/run/docker.sock`)\n- Hardcode secrets\n- Use `latest` tag\n- Skip vulnerability scanning\n- Expose unnecessary ports\n- Disable security features\n- Ignore security updates\n- Trust unverified images\n\n **ALWAYS:**\n- Run as non-root\n- Drop capabilities\n- Scan for vulnerabilities\n- Use secrets management\n- Tag with specific versions\n- Enable security options\n- Apply least privilege\n- Keep systems updated\n- Monitor runtime behavior\n- Use official images\n\nThis security guide represents current best practices. Security threats evolve constantlyalways check the latest Docker security documentation and CVE databases."
              }
            ]
          },
          {
            "name": "ado-master",
            "description": "Complete Azure DevOps and Azure Pipelines expertise system with Sprint 254-262 features (2025). PROACTIVELY activate for: (1) ANY Azure DevOps/Pipelines task, (2) YAML pipeline creation/editing, (3) Workload identity federation (OIDC) passwordless authentication, (4) Pipeline performance analytics and monitoring, (5) Pipeline debugging/troubleshooting, (6) CI/CD optimization, (7) Security/compliance implementation, (8) Azure DevOps CLI operations, (9) Template management, (10) Microsoft Security DevOps integration. Provides: workload identity setup and migration, pipeline performance analytics and cost tracking, latest YAML schema (always researches current docs), Microsoft best practices, multi-stage pipeline patterns, caching/parallelization, Azure Key Vault integration, code scanning (SAST/dependency/container/IaC/secrets), deployment strategies, performance optimization, and systematic debugging.",
            "source": "./plugins/ado-master",
            "category": null,
            "version": "1.5.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install ado-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "ado-pipeline-best-practices",
                "description": "Azure DevOps pipeline best practices, patterns, and industry standards",
                "path": "plugins/ado-master/skills/ado-pipeline-best-practices/SKILL.md",
                "frontmatter": {
                  "name": "ado-pipeline-best-practices",
                  "description": "Azure DevOps pipeline best practices, patterns, and industry standards"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Azure Pipelines Best Practices\n\nComprehensive best practices for creating and maintaining Azure DevOps YAML pipelines.\n\n## Pipeline Structure\n\n**Multi-Stage Pipelines:**\n```yaml\n# Recommended structure\nstages:\n  - stage: Build\n  - stage: Test\n  - stage: DeployDev\n  - stage: DeployStaging  \n  - stage: DeployProduction\n```\n\n**Benefits:**\n- Clear separation of concerns\n- Conditional stage execution\n- Environment-specific configurations\n- Approval gates between stages\n\n## Triggers and Scheduling\n\n**Best practices:**\n- Use path filters to avoid unnecessary builds\n- Enable batch builds for high-frequency repos\n- Use PR triggers for validation\n- Schedule nightly/weekly builds for comprehensive testing\n\n```yaml\ntrigger:\n  batch: true\n  branches:\n    include: [main, develop]\n  paths:\n    exclude: ['docs/*', '**.md']\n\npr:\n  autoCancel: true\n  branches:\n    include: [main]\n\nschedules:\n  - cron: '0 0 * * *'\n    displayName: 'Nightly build'\n    branches:\n      include: [main]\n    always: false  # Only if code changed\n```\n\n## Variable Management\n\n**Hierarchy:**\n1. Pipeline-level variables (az devops YAML)\n2. Variable groups (shared across pipelines)\n3. Azure Key Vault (secrets)\n4. Runtime parameters (user input)\n\n**Security:**\n- Never hardcode secrets\n- Use Key Vault for sensitive data\n- Mark secrets in variable groups\n- Secrets are automatically masked in logs\n\n## Caching\n\nImplement caching for:\n- Package dependencies (npm, pip, NuGet, Maven)\n- Docker layers\n- Build outputs\n\n**Impact:**\n- Faster builds (up to 90% reduction)\n- Reduced network usage\n- Lower costs\n\n## Templates\n\n**Use templates for:**\n- Reusable build patterns\n- Standardized deployment steps\n- Consistent security scanning\n- Company-wide best practices\n\n**Benefits:**\n- DRY (Don't Repeat Yourself)\n- Centralized updates\n- Consistent processes\n\n## Security Practices\n\n**Essential:**\n- Code scanning (SAST, dependency)\n- Container image scanning\n- Secret scanning\n- Compliance checks\n- Branch protection policies\n- Required approvals\n\n## Performance\n\n**Optimize:**\n- Parallelize independent jobs\n- Use caching extensively\n- Shallow git clones (fetchDepth: 1)\n- Appropriate agent pools\n- Clean up artifacts\n\n## Monitoring\n\n**Track:**\n- Build success rates\n- Build durations\n- Test pass rates\n- Deployment frequency\n- Mean time to recovery (MTTR)\n\nAlways verify best practices against latest Azure DevOps documentation."
              },
              {
                "name": "ado-windows-git-bash-compatibility",
                "description": "Windows and Git Bash compatibility guidance for Azure Pipelines. Covers path conversion issues, shell detection in pipeline scripts, MINGW/MSYS path handling, Windows agent configuration, cross-platform script patterns, and troubleshooting common Windows-specific pipeline failures.",
                "path": "plugins/ado-master/skills/ado-windows-git-bash-compatibility/SKILL.md",
                "frontmatter": {
                  "name": "ado-windows-git-bash-compatibility",
                  "skill": true,
                  "description": "Windows and Git Bash compatibility guidance for Azure Pipelines. Covers path conversion issues, shell detection in pipeline scripts, MINGW/MSYS path handling, Windows agent configuration, cross-platform script patterns, and troubleshooting common Windows-specific pipeline failures."
                },
                "content": "# Azure Pipelines: Windows & Git Bash Compatibility\n\n## Overview\n\nAzure Pipelines frequently run on Windows agents, and teams often use Git Bash for scripting. This creates path conversion and shell compatibility challenges that can cause pipeline failures. This guide provides comprehensive solutions for Windows/Git Bash integration in Azure DevOps pipelines.\n\n## Critical Windows Agent Facts\n\n### Git Bash Integration\n\n**Microsoft's Official Position:**\n- Microsoft advises **avoiding mintty-based shells** (like git-bash) for agent configuration\n- mintty is not fully compatible with native Windows Input/Output API\n- However, Git Bash tasks in pipelines are widely used and supported\n\n**Git Version Management:**\n- Windows agents use Git bundled with agent software by default\n- Microsoft recommends using bundled Git version\n- Override available via `System.PreferGitFromPath=true`\n\n**Git Bash Location on Windows Agents:**\n```\nC:\\Program Files (x86)\\Git\\usr\\bin\\bash.exe\nC:\\Program Files\\Git\\usr\\bin\\bash.exe\n```\n\n## Path Conversion Issues in Pipelines\n\n### The Core Problem\n\nWhen using Bash tasks on Windows agents, Azure DevOps variables return Windows-style paths, but Git Bash (MINGW) performs automatic path conversion that can cause issues.\n\n### Common Failure Patterns\n\n#### Issue 1: Backslash Escape in Bash\n```yaml\n#  FAILS - Backslashes treated as escape characters\n- bash: |\n    cd $(System.DefaultWorkingDirectory)  # d:\\a\\s\\1 becomes d:as1\n```\n\n**Solution:**\n```yaml\n#  CORRECT - Use forward slashes or variable properly\n- bash: |\n    cd \"$BUILD_SOURCESDIRECTORY\"\n    # Or use PWD variable which is already set correctly\n    echo \"Working in: $PWD\"\n```\n\n#### Issue 2: Path Variables in Arguments\n```yaml\n#  FAILS - MINGW converts /d /s style arguments\n- bash: |\n    my-tool /d $(Build.SourcesDirectory)\n```\n\n**Solution:**\n```yaml\n#  CORRECT - Use double slashes or environment variable\n- bash: |\n    export MSYS_NO_PATHCONV=1\n    my-tool /d $(Build.SourcesDirectory)\n    unset MSYS_NO_PATHCONV\n```\n\n#### Issue 3: Colon-Separated Path Lists\n```yaml\n#  FAILS - MINGW converts colon-separated Windows paths\n- bash: |\n    export PATH=\"/usr/bin:$(Agent.ToolsDirectory)\"\n```\n\n**Solution:**\n```yaml\n#  CORRECT - Use semicolon for Windows or convert properly\n- bash: |\n    # For Windows-style paths\n    export PATH=\"/usr/bin;$(Agent.ToolsDirectory)\"\n```\n\n## Shell Detection in Pipeline Scripts\n\n### Method 1: Using $OSTYPE (Bash-Specific)\n\n```yaml\n- bash: |\n    case \"$OSTYPE\" in\n      linux-gnu*)\n        echo \"Running on Linux agent\"\n        BUILD_PATH=\"$(Build.SourcesDirectory)\"\n        ;;\n      darwin*)\n        echo \"Running on macOS agent\"\n        BUILD_PATH=\"$(Build.SourcesDirectory)\"\n        ;;\n      msys*|mingw*|cygwin*)\n        echo \"Running on Windows agent with Git Bash\"\n        # Windows paths already work in MINGW, but may need conversion\n        BUILD_PATH=\"$(Build.SourcesDirectory)\"\n        export MSYS_NO_PATHCONV=1\n        ;;\n      *)\n        echo \"Unknown OS: $OSTYPE\"\n        BUILD_PATH=\"$(Build.SourcesDirectory)\"\n        ;;\n    esac\n\n    echo \"Build path: $BUILD_PATH\"\n    cd \"$BUILD_PATH\"\n  displayName: 'Cross-platform path handling'\n```\n\n### Method 2: Using uname (Most Portable)\n\n```yaml\n- bash: |\n    OS_TYPE=$(uname -s)\n\n    case \"$OS_TYPE\" in\n      Darwin*)\n        echo \"macOS agent detected\"\n        ;;\n      Linux*)\n        echo \"Linux agent detected\"\n        # Check if WSL\n        if grep -qi microsoft /proc/version 2>/dev/null; then\n          echo \"Running in WSL\"\n        fi\n        ;;\n      MINGW64*|MINGW32*)\n        echo \"Git Bash on Windows detected\"\n        export MSYS_NO_PATHCONV=1\n        ;;\n      CYGWIN*)\n        echo \"Cygwin on Windows detected\"\n        ;;\n      MSYS_NT*)\n        echo \"MSYS on Windows detected\"\n        export MSYS_NO_PATHCONV=1\n        ;;\n      *)\n        echo \"Unknown OS: $OS_TYPE\"\n        ;;\n    esac\n  displayName: 'Detect shell environment'\n```\n\n### Method 3: Using Agent.OS (Azure Pipelines Variable)\n\n```yaml\n- bash: |\n    if [ \"$(Agent.OS)\" = \"Windows_NT\" ]; then\n      echo \"Windows agent - applying MINGW path handling\"\n      export MSYS_NO_PATHCONV=1\n    elif [ \"$(Agent.OS)\" = \"Linux\" ]; then\n      echo \"Linux agent\"\n    elif [ \"$(Agent.OS)\" = \"Darwin\" ]; then\n      echo \"macOS agent\"\n    fi\n  displayName: 'Agent-specific configuration'\n```\n\n## Path Conversion Control\n\n### MSYS_NO_PATHCONV (Primary Method)\n\nDisables ALL automatic path conversion in MINGW/Git Bash:\n\n```yaml\n- bash: |\n    # Disable path conversion for this script\n    export MSYS_NO_PATHCONV=1\n\n    # Now Windows paths work as-is\n    dotnet build /p:Configuration=Release\n    docker run -v \"$(Build.SourcesDirectory):/workspace\" myimage\n\n    # Optionally re-enable\n    unset MSYS_NO_PATHCONV\n  displayName: 'Build with path conversion disabled'\n```\n\n### MSYS2_ARG_CONV_EXCL (Selective Exclusion)\n\nExclude specific argument patterns from conversion:\n\n```yaml\n- bash: |\n    # Exclude specific prefixes from conversion\n    export MSYS2_ARG_CONV_EXCL=\"--config=;/p:\"\n\n    dotnet build /p:Configuration=Release --config=$(Build.SourcesDirectory)/app.config\n  displayName: 'Selective path conversion'\n```\n\n### Manual Conversion with cygpath\n\nConvert between Windows and Unix paths explicitly:\n\n```yaml\n- bash: |\n    # Convert Windows path to Unix\n    UNIX_PATH=$(cygpath -u \"$(Build.SourcesDirectory)\")\n    echo \"Unix path: $UNIX_PATH\"\n\n    # Convert Unix path to Windows\n    WINDOWS_PATH=$(cygpath -w \"$PWD\")\n    echo \"Windows path: $WINDOWS_PATH\"\n\n    # Mixed format (forward slashes with drive letter)\n    MIXED_PATH=$(cygpath -m \"$(Build.SourcesDirectory)\")\n    echo \"Mixed path: $MIXED_PATH\"\n  displayName: 'Path conversion examples'\n```\n\n## Cross-Platform Pipeline Patterns\n\n### Pattern 1: Platform-Specific Steps with Conditions\n\n```yaml\njobs:\n  - job: CrossPlatformBuild\n    strategy:\n      matrix:\n        Linux:\n          imageName: 'ubuntu-24.04'\n          osType: 'Linux'\n        Windows:\n          imageName: 'windows-2025'\n          osType: 'Windows_NT'\n        macOS:\n          imageName: 'macOS-15'\n          osType: 'Darwin'\n    pool:\n      vmImage: $(imageName)\n\n    steps:\n      # Windows-specific setup\n      - bash: |\n          export MSYS_NO_PATHCONV=1\n          echo \"Windows Git Bash configuration applied\"\n        condition: eq(variables['Agent.OS'], 'Windows_NT')\n        displayName: 'Windows Git Bash setup'\n\n      # Cross-platform build\n      - bash: |\n          echo \"Building on: $(Agent.OS)\"\n          cd \"$(Build.SourcesDirectory)\"\n          npm install\n          npm run build\n        displayName: 'Cross-platform build'\n```\n\n### Pattern 2: Reusable Template with Platform Detection\n\n```yaml\n# File: templates/cross-platform-script.yml\nparameters:\n  - name: script\n    type: string\n\nsteps:\n  - bash: |\n      # Auto-detect Windows and apply MSYS configuration\n      if [ \"$(Agent.OS)\" = \"Windows_NT\" ]; then\n        export MSYS_NO_PATHCONV=1\n      fi\n\n      # Run provided script\n      ${{ parameters.script }}\n    displayName: 'Cross-platform script execution'\n\n# Usage in main pipeline:\nsteps:\n  - template: templates/cross-platform-script.yml\n    parameters:\n      script: |\n        dotnet build /p:Configuration=Release\n        dotnet test --no-build\n```\n\n### Pattern 3: PowerShell for Windows, Bash for Unix\n\n```yaml\n- pwsh: |\n    Write-Host \"Building on Windows with PowerShell\"\n    dotnet build /p:Configuration=Release\n  condition: eq(variables['Agent.OS'], 'Windows_NT')\n  displayName: 'Windows build (PowerShell)'\n\n- bash: |\n    echo \"Building on Unix with Bash\"\n    dotnet build -p:Configuration=Release\n  condition: ne(variables['Agent.OS'], 'Windows_NT')\n  displayName: 'Unix build (Bash)'\n```\n\n## Azure DevOps CLI on Windows Agents\n\n### Common CLI Path Issues\n\n```yaml\n#  FAILS - Windows paths in bash arguments\n- bash: |\n    az pipelines run --id 123 --variables sourceDir=$(Build.SourcesDirectory)\n```\n\n**Solution:**\n```yaml\n#  CORRECT - Use MSYS_NO_PATHCONV or proper quoting\n- bash: |\n    export MSYS_NO_PATHCONV=1\n    az pipelines run --id 123 --variables sourceDir=\"$(Build.SourcesDirectory)\"\n```\n\n### Repository Operations with Paths\n\n```yaml\n- bash: |\n    # Configure Git to handle Windows paths correctly\n    git config --global core.autocrlf true\n    git config --global core.safecrlf false\n\n    # Clone with proper path handling\n    export MSYS_NO_PATHCONV=1\n    az repos pr create \\\n      --repository myrepo \\\n      --source-branch feature/new \\\n      --target-branch main\n  displayName: 'Git operations on Windows agent'\n  condition: eq(variables['Agent.OS'], 'Windows_NT')\n```\n\n## Agent Configuration Best Practices\n\n### Configure Git for Windows Agents\n\n```yaml\n- bash: |\n    # Recommended Git configuration for Windows agents\n    git config --global core.autocrlf true\n    git config --global core.longpaths true\n    git config --global core.symlinks false\n\n    # Show configuration\n    git config --list | grep core\n  displayName: 'Configure Git for Windows'\n  condition: eq(variables['Agent.OS'], 'Windows_NT')\n```\n\n### Use System.PreferGitFromPath\n\n```yaml\n# Use system Git instead of agent-bundled Git\nvariables:\n  System.PreferGitFromPath: true\n\nsteps:\n  - bash: |\n      git --version\n      which git\n    displayName: 'Check Git version'\n```\n\n### Agent .env Configuration\n\nFor self-hosted Windows agents, create `.env` file in agent root:\n\n```bash\n# File: agent/.env\nSystem.PreferGitFromPath=true\nMSYS_NO_PATHCONV=1\n```\n\n## Troubleshooting Windows Pipeline Failures\n\n### Diagnostic Script\n\n```yaml\n- bash: |\n    echo \"=== Environment Diagnostics ===\"\n    echo \"Agent.OS: $(Agent.OS)\"\n    echo \"Agent.OSArchitecture: $(Agent.OSArchitecture)\"\n    echo \"System.DefaultWorkingDirectory: $(System.DefaultWorkingDirectory)\"\n    echo \"Build.SourcesDirectory: $(Build.SourcesDirectory)\"\n    echo \"\"\n\n    echo \"=== Shell Detection ===\"\n    echo \"OSTYPE: $OSTYPE\"\n    echo \"MSYSTEM: $MSYSTEM\"\n    uname -a\n    echo \"\"\n\n    echo \"=== Path Information ===\"\n    echo \"PWD: $PWD\"\n    echo \"HOME: $HOME\"\n    echo \"PATH: $PATH\"\n    echo \"\"\n\n    echo \"=== Git Configuration ===\"\n    git --version\n    which git\n    git config --list | grep core\n    echo \"\"\n\n    echo \"=== Path Conversion Test ===\"\n    echo \"Windows-style: $(Build.SourcesDirectory)\"\n    if command -v cygpath &> /dev/null; then\n      echo \"Unix-style: $(cygpath -u \"$(Build.SourcesDirectory)\")\"\n      echo \"Mixed-style: $(cygpath -m \"$(Build.SourcesDirectory)\")\"\n    fi\n  displayName: 'Windows agent diagnostics'\n  condition: eq(variables['Agent.OS'], 'Windows_NT')\n```\n\n### Common Error Patterns and Fixes\n\n#### Error: \"No such file or directory\"\n```yaml\n# Error: bash: line 1: d:as1: No such file or directory\n\n#  Problem: Backslashes removed\n- bash: cd $(System.DefaultWorkingDirectory)\n\n#  Solution: Quote the variable\n- bash: cd \"$(System.DefaultWorkingDirectory)\"\n```\n\n#### Error: \"Invalid switch\"\n```yaml\n# Error: Invalid switch - \"/d\"\n\n#  Problem: MINGW converts /d to Windows path\n- bash: dotnet test /d:SonarQubeAnalysisPath=.\n\n#  Solution: Disable path conversion\n- bash: |\n    export MSYS_NO_PATHCONV=1\n    dotnet test /d:SonarQubeAnalysisPath=.\n```\n\n#### Error: \"Access denied\" with spaces in path\n```yaml\n# Error: Access to path 'C:\\Program' is denied\n\n#  Problem: Unquoted path with spaces\n- bash: my-tool $(Agent.ToolsDirectory)/mytool\n\n#  Solution: Always quote paths\n- bash: my-tool \"$(Agent.ToolsDirectory)/mytool\"\n```\n\n## Best Practices Summary\n\n### Always Do\n1. **Quote all path variables**: `\"$(Build.SourcesDirectory)\"`\n2. **Use MSYS_NO_PATHCONV** for Windows-specific commands\n3. **Detect platform** using `$(Agent.OS)` or `uname`\n4. **Test on Windows agents** if targeting Windows deployments\n5. **Use forward slashes** in paths when possible (Git Bash compatible)\n\n### Never Do\n1.  Use unquoted paths: `cd $(Build.SourcesDirectory)`\n2.  Assume Bash = Linux (Windows has Git Bash)\n3.  Hardcode platform-specific paths\n4.  Mix PowerShell and Bash syntax in same script\n5.  Ignore MINGW path conversion in arguments\n\n### Platform Detection Template\n\nUse this at the start of complex cross-platform scripts:\n\n```yaml\n- bash: |\n    #!/bin/bash\n    set -euo pipefail\n\n    # Detect platform and configure\n    if [ \"$(Agent.OS)\" = \"Windows_NT\" ]; then\n      echo \"Windows agent detected\"\n      export MSYS_NO_PATHCONV=1\n      PATH_SEP=\";\"\n    else\n      echo \"Unix-like agent detected\"\n      PATH_SEP=\":\"\n    fi\n\n    # Your script logic here\n    echo \"Build directory: $(Build.SourcesDirectory)\"\n    cd \"$(Build.SourcesDirectory)\"\n\n    # Platform-agnostic operations\n    npm install\n    npm run build\n  displayName: 'Cross-platform build script'\n```\n\n## Additional Resources\n\n- [Azure Pipelines Windows Agents](https://learn.microsoft.com/azure/devops/pipelines/agents/windows-agent)\n- [Git for Windows Documentation](https://git-scm.com/docs)\n- [MINGW Path Conversion](https://www.msys2.org/docs/filesystem-paths/)\n- [Azure Pipelines Variables](https://learn.microsoft.com/azure/devops/pipelines/build/variables)\n\n## Quick Reference Card\n\n| Scenario | Solution |\n|----------|----------|\n| Bash script on Windows | Use `export MSYS_NO_PATHCONV=1` |\n| Detect Windows agent | Check `$(Agent.OS)` = `Windows_NT` |\n| Detect Git Bash | Check `uname -s` starts with `MINGW` |\n| Convert Windows  Unix | `cygpath -u \"C:\\path\"` |\n| Convert Unix  Windows | `cygpath -w \"/c/path\"` |\n| Quote paths with spaces | Always use `\"$(variable)\"` |\n| Disable conversion for arg | `export MSYS2_ARG_CONV_EXCL=\"pattern\"` |\n| Check Git version | `git --version && which git` |\n| Use system Git | Set `System.PreferGitFromPath: true` |\n| Test path handling | Run diagnostic script above |\n\n---\n\n**When in doubt, use `MSYS_NO_PATHCONV=1` for Windows agents running Bash tasks.**"
              },
              {
                "name": "defender-for-devops",
                "description": "Microsoft Defender for DevOps integration with Azure Pipelines (2025)",
                "path": "plugins/ado-master/skills/defender-for-devops/SKILL.md",
                "frontmatter": {
                  "name": "defender-for-devops",
                  "description": "Microsoft Defender for DevOps integration with Azure Pipelines (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Microsoft Defender for DevOps Integration\n\nComplete guide to integrating Microsoft Defender for Cloud security scanning into Azure Pipelines.\n\n## Overview\n\nMicrosoft Security DevOps (MSDO) provides comprehensive security scanning capabilities:\n- **SAST:** Static Application Security Testing\n- **Secret Detection:** Identify hardcoded secrets and credentials\n- **Dependency Scanning:** Vulnerable package detection\n- **IaC Scanning:** Infrastructure as Code security analysis\n- **Container Scanning:** Image vulnerability assessment with Trivy\n\n## Microsoft Security DevOps Extension\n\n**Installation:**\n1. Install from Azure DevOps Marketplace\n2. Configure in pipeline YAML\n3. View results in Scans tab\n4. Integrate with Defender for Cloud\n\n**Extension Capabilities:**\n- Converts results to SARIF format\n- Displays findings in Scans tab\n- Integrates multiple security tools\n- Provides centralized security insights\n\n## YAML Integration\n\n### Basic MSDO Task\n\n```yaml\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n\npool:\n  vmImage: 'ubuntu-24.04'\n\nstages:\n  - stage: Build\n    jobs:\n      - job: BuildAndScan\n        steps:\n          - task: UseDotNet@2\n            displayName: 'Install .NET SDK'\n            inputs:\n              version: '8.x'\n\n          - task: DotNetCoreCLI@2\n            displayName: 'Build Project'\n            inputs:\n              command: 'build'\n              projects: '**/*.csproj'\n\n          # Microsoft Security DevOps Scan\n          - task: MicrosoftSecurityDevOps@1\n            displayName: 'Run Microsoft Security DevOps'\n            inputs:\n              categories: 'secrets,code,dependencies,IaC,containers'\n              break: false  # Don't fail pipeline on findings\n\n          # Publish SARIF results\n          - task: PublishSecurityAnalysisLogs@3\n            displayName: 'Publish Security Analysis Logs'\n            inputs:\n              ArtifactName: 'CodeAnalysisLogs'\n\n          # Display results in Scans tab\n          - task: PostAnalysis@2\n            displayName: 'Post Analysis'\n            inputs:\n              break: false\n```\n\n### Advanced Configuration with Breaking Builds\n\n```yaml\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Security Scanning (Break on Critical)'\n  inputs:\n    # Scan categories\n    categories: 'secrets,code,dependencies,IaC,containers'\n\n    # Break build on severity\n    break: true\n    breakSeverity: 'critical'  # Options: critical, high, medium, low\n\n    # Tool configuration\n    tools: 'all'  # Or specific: 'credscan,eslint,trivy'\n\n    # Output configuration\n    publishResults: true\n\n  continueOnError: false\n```\n\n### Conditional Scanning\n\n```yaml\n# Full scan on main, quick scan on branches\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Security Scan'\n  inputs:\n    categories: ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/main') }}:\n      value: 'secrets,code,dependencies,IaC,containers'\n    ${{ else }}:\n      value: 'secrets,code'\n    break: ${{ eq(variables['Build.SourceBranch'], 'refs/heads/main') }}\n```\n\n## Integrated Security Tools\n\n### 1. Secret Scanning\n\n**Replaced:** CredScan deprecated September 2023\n**Current:** GitHub Advanced Security for Azure DevOps or MSDO secrets scanning\n\n```yaml\n# MSDO secrets scanning\n- task: MicrosoftSecurityDevOps@1\n  inputs:\n    categories: 'secrets'\n    break: true  # Always break on secrets\n```\n\n**Common secrets detected:**\n- API keys and tokens\n- Database connection strings\n- Cloud provider credentials\n- SSH private keys\n- OAuth tokens\n\n### 2. Static Code Analysis (SAST)\n\n```yaml\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'SAST Scan'\n  inputs:\n    categories: 'code'\n    tools: 'eslint,bandit,semgrep'\n```\n\n**Supported languages:**\n- JavaScript/TypeScript (ESLint)\n- Python (Bandit)\n- Go (gosec)\n- Java (SpotBugs)\n- C# (.NET Security Guard)\n\n### 3. Dependency Scanning\n\n```yaml\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Dependency Scan'\n  inputs:\n    categories: 'dependencies'\n    tools: 'trivy,govulncheck'\n```\n\n**Detects:**\n- Known CVEs in dependencies\n- Outdated packages\n- License compliance issues\n- Transitive vulnerabilities\n\n### 4. Infrastructure as Code (IaC) Scanning\n\n```yaml\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'IaC Security Scan'\n  inputs:\n    categories: 'IaC'\n    tools: 'terrascan,checkov,templateanalyzer'\n```\n\n**Scans:**\n- Terraform configurations\n- ARM templates\n- Bicep files\n- Kubernetes manifests\n- CloudFormation templates\n\n### 5. Container Image Scanning\n\n```yaml\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Container Security Scan'\n  inputs:\n    categories: 'containers'\n    tools: 'trivy'\n```\n\n**Trivy scans for:**\n- OS vulnerabilities\n- Application dependencies\n- Misconfigurations\n- Secrets in images\n- License issues\n\n## Integration with Defender for Cloud\n\n### Enable Defender for DevOps\n\n```yaml\n# Pipeline automatically sends results to Defender for Cloud\n# when MSDO extension is connected\n\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Scan and send to Defender'\n  inputs:\n    categories: 'all'\n    publishResults: true\n\n# Results appear in:\n# Defender for Cloud  DevOps Security  Findings\n```\n\n**Benefits:**\n- Centralized security dashboard\n- Cross-pipeline insights\n- Compliance reporting\n- Security trend analysis\n- Integration with Azure Security Center\n\n## Complete Security Pipeline Example\n\n```yaml\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n\npool:\n  vmImage: 'ubuntu-24.04'\n\nvariables:\n  - name: breakOnCritical\n    value: ${{ eq(variables['Build.SourceBranch'], 'refs/heads/main') }}\n\nstages:\n  - stage: SecurityScan\n    displayName: 'Security Analysis'\n    jobs:\n      - job: StaticAnalysis\n        displayName: 'Static Security Analysis'\n        steps:\n          - checkout: self\n            fetchDepth: 1\n\n          # Install dependencies\n          - task: NodeTool@0\n            inputs:\n              versionSpec: '20.x'\n\n          - script: npm ci\n            displayName: 'Install dependencies'\n\n          # Build application\n          - script: npm run build\n            displayName: 'Build application'\n\n          # Docker build for container scanning\n          - task: Docker@2\n            displayName: 'Build Docker image'\n            inputs:\n              command: 'build'\n              Dockerfile: 'Dockerfile'\n              tags: '$(Build.BuildId)'\n\n          # Comprehensive security scan\n          - task: MicrosoftSecurityDevOps@1\n            displayName: 'Microsoft Security DevOps Scan'\n            inputs:\n              categories: 'secrets,code,dependencies,IaC,containers'\n              break: $(breakOnCritical)\n              breakSeverity: 'high'\n              tools: 'all'\n\n          # Publish SARIF results\n          - task: PublishSecurityAnalysisLogs@3\n            displayName: 'Publish SARIF Logs'\n            inputs:\n              ArtifactName: 'CodeAnalysisLogs'\n              ArtifactType: 'Container'\n\n          # Post-analysis with results\n          - task: PostAnalysis@2\n            displayName: 'Security Post Analysis'\n            inputs:\n              break: $(breakOnCritical)\n\n          # Generate security report\n          - script: |\n              echo \"Security scan completed\"\n              echo \"Results available in Scans tab\"\n            displayName: 'Security Summary'\n            condition: always()\n\n  - stage: Deploy\n    dependsOn: SecurityScan\n    condition: succeeded()\n    jobs:\n      - deployment: DeployApp\n        environment: 'production'\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                - script: echo \"Deploying secure application\"\n```\n\n## Advanced Security Features (Coming 2025)\n\n**Roadmap features:**\n- Pull request build validation\n- Break pipeline on alert severity\n- Advanced Security dashboard\n- Custom CodeQL queries\n- Integration with GitHub Advanced Security\n\n## GitHub Advanced Security for Azure DevOps\n\n**Alternative to MSDO for secret scanning:**\n\n```yaml\n# Requires GitHub Advanced Security license\n# Provides:\n# - Secret scanning\n# - Code scanning with CodeQL\n# - Dependency vulnerability alerts\n# - Security overview dashboard\n\n# Configuration in Azure DevOps organization settings\n# Scans run automatically on commits and PRs\n```\n\n## Best Practices\n\n**Pipeline Security:**\n- Run security scans on every commit\n- Break builds on critical/high severity findings\n- Scan both code and dependencies\n- Include IaC security validation\n- Scan container images before push\n- Review findings regularly\n\n**Configuration:**\n```yaml\n# Recommended configuration\n- task: MicrosoftSecurityDevOps@1\n  inputs:\n    categories: 'secrets,code,dependencies,IaC,containers'\n    break: true\n    breakSeverity: 'high'  # Adjust based on risk tolerance\n    publishResults: true\n```\n\n**Integration:**\n- Enable Defender for DevOps in Azure portal\n- Configure organization-level policies\n- Set up automated notifications\n- Create security dashboards\n- Establish remediation workflows\n\n## Viewing Results\n\n**In Pipeline:**\n1. Navigate to pipeline run\n2. Click \"Scans\" tab\n3. Review findings by severity\n4. Click findings for details and remediation\n\n**In Defender for Cloud:**\n1. Azure Portal  Defender for Cloud\n2. DevOps Security\n3. View findings across all pipelines\n4. Filter by severity, project, repository\n5. Track remediation progress\n\n## Troubleshooting\n\n**Common Issues:**\n\n**MSDO task fails:**\n```yaml\n# Enable verbose logging\n- task: MicrosoftSecurityDevOps@1\n  env:\n    MSDO_VERBOSE: true\n  inputs:\n    categories: 'all'\n```\n\n**False positives:**\n```yaml\n# Suppress findings with .gdnconfig file\n# In repository root:\n{\n  \"tools\": {\n    \"trivy\": {\n      \"enabled\": true,\n      \"severities\": [\"CRITICAL\", \"HIGH\"]\n    }\n  }\n}\n```\n\n**Performance:**\n- Cache tool downloads\n- Limit scan categories on branches\n- Use parallel stages for large repos\n\n## Resources\n\n- [Microsoft Security DevOps Extension](https://learn.microsoft.com/azure/defender-for-cloud/azure-devops-extension)\n- [Defender for DevOps Documentation](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-devops-introduction)\n- [SARIF Format Specification](https://sarifweb.azurewebsites.net/)\n- [Security Tools Integration](https://learn.microsoft.com/azure/defender-for-cloud/azure-devops-extension)"
              },
              {
                "name": "sprint-254-features",
                "description": "Azure DevOps Sprint 254-262 new features and enhancements (2025)",
                "path": "plugins/ado-master/skills/sprint-254-features/SKILL.md",
                "frontmatter": {
                  "name": "sprint-254-features",
                  "description": "Azure DevOps Sprint 254-262 new features and enhancements (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Azure DevOps 2025 Latest Features (Sprints 254-262)\n\n## New Expression Functions (Sprint 248)\n\n### iif() - Ternary Conditional Operator\n\n```yaml\n# Syntax: iif(condition, valueIfTrue, valueIfFalse)\n\nvariables:\n  environment: 'production'\n  # Use iif for conditional values\n  instanceCount: ${{ iif(eq(variables.environment, 'production'), 10, 2) }}\n  deploymentSlot: ${{ iif(eq(variables.environment, 'production'), 'production', 'staging') }}\n\nsteps:\n- script: echo \"Deploying ${{ variables.instanceCount }} instances to ${{ variables.deploymentSlot }}\"\n```\n\n### trim() - Remove Whitespace\n\n```yaml\nparameters:\n- name: branchName\n  type: string\n  default: ' feature/my-branch '\n\nvariables:\n  # Remove leading/trailing whitespace\n  cleanBranch: ${{ trim(parameters.branchName) }}\n  # Result: 'feature/my-branch' (no spaces)\n```\n\n## New Predefined Variables (Sprint 253)\n\n### Build.StageRequestedBy\n\nWho requested the stage execution:\n\n```yaml\nstages:\n- stage: Deploy\n  jobs:\n  - job: DeployJob\n    steps:\n    - script: |\n        echo \"Stage requested by: $(Build.StageRequestedBy)\"\n        echo \"Stage requester ID: $(Build.StageRequestedById)\"\n      displayName: 'Log stage requester'\n\n    # Use for approval notifications\n    - task: SendEmail@1\n      inputs:\n        to: 'approvers@example.com'\n        subject: 'Deployment requested by $(Build.StageRequestedBy)'\n```\n\n## Stage Dependencies Visualization (Sprint 254)\n\nView stage dependencies when stage is expanded in pipeline UI:\n\n```yaml\nstages:\n- stage: Build\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: echo \"Building...\"\n\n- stage: Test\n  dependsOn: Build  # Shown visually when expanded\n  jobs:\n  - job: TestJob\n    steps:\n    - script: echo \"Testing...\"\n\n- stage: Deploy_USEast\n  dependsOn: Test\n  jobs:\n  - job: DeployJob\n    steps:\n    - script: echo \"Deploying to US East...\"\n\n- stage: Deploy_EUWest\n  dependsOn: Test  # Parallel with Deploy_USEast - visualized clearly\n  jobs:\n  - job: DeployJob\n    steps:\n    - script: echo \"Deploying to EU West...\"\n```\n\n**Benefits:**\n- Visual dependency graph in UI\n- Easier debugging of complex pipelines\n- Clear multi-region deployment patterns\n- Identify parallel vs sequential stages\n\n## New OS Images\n\n### Ubuntu-24.04 (General Availability)\n\n```yaml\npool:\n  vmImage: 'ubuntu-24.04'  # Latest LTS - Recommended\n  # OR use ubuntu-latest (will map to 24.04 soon)\n  # vmImage: 'ubuntu-latest'\n\nsteps:\n- script: |\n    lsb_release -a\n    # Ubuntu 24.04 LTS (Noble Numbat)\n```\n\n**Key Information:**\n- Ubuntu 24.04 is now generally available\n- `ubuntu-latest` will soon map to `ubuntu-24.04` (currently `ubuntu-22.04`)\n- Ubuntu 20.04 fully removed April 30, 2025\n\n### Windows Server 2025 (Coming June 2025)\n\n```yaml\npool:\n  vmImage: 'windows-2025'  # GA: June 16, 2025\n\nsteps:\n- pwsh: |\n    Get-ComputerInfo | Select-Object WindowsProductName, WindowsVersion\n```\n\n**Key Information:**\n- General availability: June 16, 2025\n- `windows-latest` will map to `windows-2025` starting September 2, 2025\n- Windows Server 2019 extended support until December 31, 2025\n\n### macOS-15 Sequoia (Available)\n\n```yaml\npool:\n  vmImage: 'macOS-15'  # Sequoia\n\nsteps:\n- script: |\n    sw_vers\n    # macOS 15.x (Sequoia)\n```\n\n**Key Information:**\n- macOS 13 Ventura deprecation starts September 1, 2025\n- macOS 13 retirement planned for December 4, 2025\n- Apple Silicon (ARM64) support in preview\n\n###  Deprecated and Retired Images\n\n**Fully Removed (2025):**\n- **Ubuntu 20.04** - Removed April 30, 2025\n- **.NET 6** - Removed from Windows and Ubuntu images August 1, 2025\n\n**Extended Support:**\n- **Windows Server 2019** - Extended until December 31, 2025\n  - Deprecation starts: June 1, 2025\n  - Brownout periods: June 3-24, 2025\n  - Final removal: December 31, 2025\n\n**Upcoming Deprecations:**\n- **macOS 13 Ventura** - Deprecation: September 1, 2025, Retirement: December 4, 2025\n\n**Migration Recommendations:**\n```yaml\n# Ubuntu Migration\n# OLD (Removed)\npool:\n  vmImage: 'ubuntu-20.04'\n\n# NEW (Recommended)\npool:\n  vmImage: 'ubuntu-24.04'  # Best: explicit version\n  # OR\n  vmImage: 'ubuntu-latest'  # Will map to 24.04 soon\n\n# Windows Migration\n# OLD (Being deprecated)\npool:\n  vmImage: 'windows-2019'\n\n# NEW (Recommended)\npool:\n  vmImage: 'windows-2022'  # Current stable\n  # OR wait for\n  vmImage: 'windows-2025'  # GA June 2025\n```\n\n## GitHub Integration Improvements\n\n### Auto-linked Pull Requests\n\nGitHub branches linked to work items automatically link PRs:\n\n```yaml\n# When PR is created for branch linked to work item,\n# PR automatically appears in work item's Development section\n\ntrigger:\n  branches:\n    include:\n    - feature/*\n    - users/*\n\n# Work item auto-linking based on branch name pattern\n# AB#12345 in commits auto-links to work item 12345\n```\n\n### \"Integrated in build\" Links\n\nGitHub repos show which build integrated the PR:\n\n```yaml\npr:\n  branches:\n    include:\n    - main\n    - develop\n\n# After PR merged, work item shows:\n# \"Integrated in build: Pipeline Name #123\"\n# Direct link to build that deployed the change\n```\n\n## Stage-Level Variables\n\n```yaml\nstages:\n- stage: Build\n  variables:\n    buildConfiguration: 'Release'\n    platform: 'x64'\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: echo \"Building $(buildConfiguration) $(platform)\"\n\n- stage: Deploy\n  variables:\n    environment: 'production'\n    region: 'eastus'\n  jobs:\n  - job: DeployJob\n    steps:\n    - script: |\n        echo \"Stage: $(System.StageName)\"\n        echo \"Requested by: $(Build.StageRequestedBy)\"\n        echo \"Deploying to $(environment) in $(region)\"\n```\n\n## Practical Examples\n\n### Multi-Region Deployment with New Features\n\n```yaml\nparameters:\n- name: deployToProd\n  type: boolean\n  default: false\n\nvariables:\n  # Use iif for conditional values\n  targetEnvironment: ${{ iif(parameters.deployToProd, 'production', 'staging') }}\n\nstages:\n- stage: Build\n  jobs:\n  - job: BuildApp\n    pool:\n      vmImage: 'ubuntu-24.04'  # New image\n    steps:\n    - script: npm run build\n\n- stage: Test\n  dependsOn: Build\n  jobs:\n  - job: RunTests\n    pool:\n      vmImage: 'ubuntu-24.04'\n    steps:\n    - script: npm test\n\n- stage: Deploy_USEast\n  dependsOn: Test\n  condition: succeeded()\n  variables:\n    region: 'eastus'\n  jobs:\n  - deployment: DeployToUSEast\n    environment: ${{ variables.targetEnvironment }}\n    pool:\n      vmImage: 'ubuntu-24.04'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: |\n              echo \"Deploying to $(region)\"\n              echo \"Requested by: $(Build.StageRequestedBy)\"\n\n- stage: Deploy_EUWest\n  dependsOn: Test  # Parallel with Deploy_USEast\n  condition: succeeded()\n  variables:\n    region: 'westeurope'\n  jobs:\n  - deployment: DeployToEUWest\n    environment: ${{ variables.targetEnvironment }}\n    pool:\n      vmImage: 'ubuntu-24.04'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - script: |\n              echo \"Deploying to $(region)\"\n              echo \"Requested by: $(Build.StageRequestedBy)\"\n\n# Stage dependencies visualized clearly in UI (Sprint 254)\n```\n\n## Continuous Access Evaluation (Sprint 260 - August 2025)\n\n### Enhanced Security with CAE\n\nAzure DevOps now supports **Continuous Access Evaluation (CAE)**, enabling near real-time enforcement of Conditional Access policies through Microsoft Entra ID.\n\n**Key Benefits:**\n- Instant access revocation on critical events\n- No waiting for token expiration\n- Enhanced security posture\n\n**Triggers for Access Revocation:**\n- User account disabled\n- Password reset\n- Location or IP address changes\n- Risk detection events\n- Policy violations\n\n**Example Scenario:**\n```yaml\n# Your pipeline with CAE enabled automatically\nstages:\n  - stage: Production\n    jobs:\n      - deployment: Deploy\n        environment: 'production'\n        pool:\n          vmImage: 'ubuntu-24.04'\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                - script: echo \"Deploying...\"\n                  # If user credentials are revoked mid-deployment,\n                  # CAE will instantly terminate access\n```\n\n**Implementation:**\n- General availability: August 2025\n- Phased rollout to all customers\n- No configuration required (automatic for all Azure DevOps orgs)\n- Works with Microsoft Entra ID Conditional Access policies\n\n**Security Improvements:**\n- Immediate response to security events\n- Reduces attack window from hours/days to seconds\n- Complements existing security features (Key Vault, branch policies, etc.)\n\n## OAuth Apps Deprecation (April 2025)\n\n**Important Change:**\n- Azure DevOps no longer supports **new registrations** of Azure DevOps OAuth apps (effective April 2025)\n- First step towards retiring the Azure DevOps OAuth platform\n- Existing OAuth apps continue to work\n- Plan migration to Microsoft Entra ID authentication\n\n**Migration Recommendations:**\n```yaml\n# Use service connections with Microsoft Entra ID instead\n- task: AzureCLI@2\n  inputs:\n    azureSubscription: 'service-connection'  # Uses Managed Identity or Service Principal\n    scriptType: 'bash'\n    scriptLocation: 'inlineScript'\n    addSpnToEnvironment: true\n    inlineScript: |\n      az account show\n```\n\n## SNI Requirement (April 2025)\n\n**Network Requirement:**\n- **Server Name Indication (SNI)** required on all incoming HTTPS connections\n- Effective: April 23, 2025\n- Affects all Azure DevOps Services connections\n\n**What to Check:**\n- Ensure clients support SNI (most modern clients do)\n- Update legacy tools/scripts if needed\n- Test connectivity before April 23, 2025\n\n## OAuth Apps Deprecation (Sprint 261 - September 2025)\n\n**Critical Security Change:**\n\nAzure DevOps is enforcing one-time visibility for OAuth client secrets:\n- Newly generated client secrets displayed only once at creation\n- Get Registration Secret API will be retired\n- Change effective: September 2, 2025\n- No new OAuth app registrations allowed\n\n**Migration Path:**\n```yaml\n# Replace OAuth apps with Microsoft Entra ID authentication\n# Use service connections with Managed Identity or Service Principal\n- task: AzureCLI@2\n  inputs:\n    azureSubscription: 'entra-id-service-connection'\n    scriptType: 'bash'\n    addSpnToEnvironment: true\n    inlineScript: |\n      az account show\n      # Authenticated via Entra ID\n```\n\n**Action Required:**\n- Audit existing OAuth apps\n- Plan migration to Entra ID authentication\n- Update CI/CD pipelines to use service connections\n- Document secret rotation procedures\n\n## Agent Software Version 4 (October 2024 - Current)\n\n**Major Upgrade:**\n\nThe Azure Pipelines agent has been upgraded from v3.x to v4.x, powered by .NET 8:\n\n**Key Improvements:**\n- Built on .NET 8 for better performance and security\n- Extended platform support including ARM64\n- Improved reliability and diagnostics\n- Better resource management\n\n**Platform Support:**\n- **Linux:** Debian 11 & 12, Ubuntu 24.04, 22.04, 20.04 (ARM64 supported)\n- **macOS:** Intel and Apple Silicon (ARM64 supported)\n- **Windows:** Windows Server 2019, 2022, 2025\n\n**ARM64 Support:**\n```yaml\n# Self-hosted ARM64 agent\npool:\n  name: 'arm64-pool'\n  demands:\n    - agent.os -equals Linux\n    - Agent.OSArchitecture -equals ARM64\n\nsteps:\n  - script: uname -m\n    displayName: 'Verify ARM64 architecture'\n```\n\n**Note:** ARM64 support is available for self-hosted agents. Microsoft-hosted ARM64 macOS agents are in preview.\n\n## Sprint 262 - GitHub Copilot Integration (2025)\n\n**AI-Powered Work Item Assistance (Private Preview):**\n\nConnect Azure Boards work items directly with GitHub Copilot:\n\n**Capabilities:**\n- Send work items to Copilot coding agent\n- AI-assisted bug fixes\n- Automated feature implementation\n- Test coverage improvements\n- Documentation updates\n- Technical debt reduction\n\n**Usage Pattern:**\n1. Create work item in Azure Boards\n2. Add detailed requirements in description\n3. Send to GitHub Copilot\n4. Copilot generates code changes\n5. Review and merge via standard PR process\n\n**Integration with Pipelines:**\n```yaml\n# Work items auto-link with PRs\ntrigger:\n  branches:\n    include:\n    - feature/*\n\n# Mention work item in commit\n# Example: \"Fix login bug AB#12345\"\n# Automatically links PR to work item and tracks in build\n```\n\n## Resources\n\n- [Azure DevOps Sprint 262 Update](https://learn.microsoft.com/azure/devops/release-notes/2025/sprint-262-update)\n- [Azure DevOps Sprint 261 Update](https://learn.microsoft.com/azure/devops/release-notes/2025/general/sprint-261-update)\n- [Azure DevOps Sprint 260 Update](https://learn.microsoft.com/azure/devops/release-notes/2025/general/sprint-260-update)\n- [Azure DevOps Sprint 254 Update](https://devblogs.microsoft.com/devops/)\n- [Agent Software Version 4](https://learn.microsoft.com/azure/devops/pipelines/agents/v4-agent)\n- [Expression Functions Documentation](https://learn.microsoft.com/azure/devops/pipelines/process/expressions)\n- [Hosted Agent Images](https://learn.microsoft.com/azure/devops/pipelines/agents/hosted)\n- [Continuous Access Evaluation Documentation](https://learn.microsoft.com/azure/devops/release-notes/)"
              }
            ]
          },
          {
            "name": "test-master",
            "description": "Complete Vitest 4.0 Playwright 1.55 MSW 2.x testing system. PROACTIVELY activate for: (1) ANY testing task (unit/integration/E2E/visual), (2) Test creation and scaffolding, (3) AI-powered test generation (Playwright agents), (4) Test debugging and failure analysis, (5) Coverage analysis and optimization, (6) Visual regression testing, (7) MSW mock management, (8) Test configuration (Vitest/Playwright), (9) CI/CD test setup, (10) Multi-project test architecture. Provides: Vitest 4.0 stable browser mode, visual regression testing, Playwright 1.55 AI test agents, intelligent test running, auto-scaffolding with boilerplate, MSW 2.11+ handler generation, coverage gap analysis, Playwright trace debugging with titlePath, advanced snapshot management, parallel execution optimization, and comprehensive test infrastructure setup. Ensures production-ready testing with latest 2025 patterns across all modern JavaScript projects.",
            "source": "./plugins/test-master",
            "category": null,
            "version": "1.6.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install test-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "vitest-3-features",
                "description": "Vitest 3 Features skill",
                "path": "plugins/test-master/skills/vitest-3-features/SKILL.md",
                "frontmatter": {
                  "name": "vitest-3-features",
                  "description": "Vitest 3 Features skill"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n---\n\n\n# Vitest 3.x Features and Best Practices (2025)\n\n## Overview\n\nVitest 3.0 was released in January 2025 with major improvements to reporting, browser testing, watch mode, and developer experience. This skill provides comprehensive knowledge of Vitest 3.x features and modern testing patterns.\n\n## Major Features in Vitest 3.0+\n\n### 1. Annotation API (Vitest 3.2+)\n\nThe annotation API allows you to add custom metadata, messages, and attachments to any test, visible in UI, HTML, JUnit, TAP, and GitHub Actions reporters.\n\n**Usage:**\n```javascript\nimport { test, expect } from 'vitest';\n\ntest('user authentication', async ({ task }) => {\n  // Add custom annotation\n  task.meta.annotation = {\n    message: 'Testing OAuth2 flow with external provider',\n    attachments: [\n      { name: 'config', content: JSON.stringify(oauthConfig) }\n    ]\n  };\n\n  const result = await authenticateUser(credentials);\n  expect(result.token).toBeDefined();\n});\n```\n\n**Use Cases:**\n- Document complex test scenarios\n- Attach debug information\n- Link to external resources (tickets, docs)\n- Add performance metrics\n- Track test metadata for reporting\n\n### 2. Line Number Filtering\n\nRun specific tests by their line number in the file, enabling precise test execution from IDEs.\n\n**CLI Usage:**\n```bash\n# Run test at specific line\nvitest run tests/user.test.js:42\n\n# Run multiple tests by line\nvitest run tests/user.test.js:42 tests/user.test.js:67\n\n# Works with ranges\nvitest run tests/user.test.js:42-67\n```\n\n**IDE Integration:**\n- VS Code: Click line number gutter\n- JetBrains IDEs: Run from context menu\n- Enables \"run test at cursor\" functionality\n\n### 3. Enhanced Watch Mode\n\nSmarter and more responsive watch mode with improved change detection.\n\n**Features:**\n- Detects changes more accurately\n- Only re-runs affected tests\n- Faster rebuild times\n- Better file watching on Windows\n- Reduced false positives\n\n**Configuration:**\n```javascript\nexport default defineConfig({\n  test: {\n    watch: true,\n    watchExclude: ['**/node_modules/**', '**/dist/**'],\n    // New in 3.0: More intelligent caching\n    cache: {\n      dir: '.vitest/cache'\n    }\n  }\n});\n```\n\n### 4. Improved Test Reporting\n\nComplete overhaul of test run reporting with reduced flicker and clearer output.\n\n**Reporter API Changes:**\n```javascript\n// Custom reporter with new lifecycle\nexport default class CustomReporter {\n  onInit(ctx) {\n    // Called once at start\n  }\n\n  onTestStart(test) {\n    // More reliable test start hook\n  }\n\n  onTestComplete(test) {\n    // Includes full test metadata\n  }\n\n  onFinished(files, errors) {\n    // Final results with all context\n  }\n}\n```\n\n**Benefits:**\n- Less terminal flicker during test runs\n- Clearer test status indicators\n- Better error formatting\n- Improved progress tracking\n\n### 5. Workspace Configuration Simplification\n\nNo need for separate workspace files - define projects directly in config.\n\n**Old Way (Vitest 2.x):**\n```javascript\n// vitest.workspace.js\nexport default ['packages/*'];\n```\n\n**New Way (Vitest 3.0):**\n```javascript\n// vitest.config.js\nexport default defineConfig({\n  test: {\n    workspace: [\n      {\n        test: {\n          name: 'unit',\n          include: ['tests/unit/**/*.test.js']\n        }\n      },\n      {\n        test: {\n          name: 'integration',\n          include: ['tests/integration/**/*.test.js']\n        }\n      }\n    ]\n  }\n});\n```\n\n### 6. Enhanced Browser Testing\n\nImproved browser mode with better performance and caching.\n\n**Multiple Browser Instances:**\n```javascript\nexport default defineConfig({\n  test: {\n    browser: {\n      enabled: true,\n      instances: [\n        { browser: 'chromium', name: 'chrome' },\n        { browser: 'firefox', name: 'ff' },\n        { browser: 'webkit', name: 'safari' }\n      ]\n    }\n  }\n});\n```\n\n**Benefits:**\n- Single Vite server for all browsers\n- Cached file processing\n- Parallel browser execution\n- Reduced startup time\n\n### 7. Improved Coverage\n\nAutomatic exclusion of test files from coverage reports.\n\n**Automatic Exclusions (Vitest 3.0):**\n```javascript\nexport default defineConfig({\n  test: {\n    coverage: {\n      provider: 'v8',\n      // Test files automatically excluded\n      // No need to manually exclude *.test.js\n      exclude: [\n        // Only need to specify custom exclusions\n        '**/node_modules/**',\n        '**/dist/**'\n      ]\n    }\n  }\n});\n```\n\n### 8. Enhanced Mocking\n\nNew spy reuse and powerful matchers.\n\n**Spy Reuse:**\n```javascript\nconst spy = vi.fn();\n\n// Vitest 3.0: Reuse spy on already mocked method\nvi.spyOn(obj, 'method').mockImplementation(spy);\nvi.spyOn(obj, 'method'); // Reuses existing spy\n```\n\n**New Matchers:**\n```javascript\n// toHaveBeenCalledExactlyOnceWith\nexpect(mockFn).toHaveBeenCalledExactlyOnceWith(arg1, arg2);\n\n// Ensures called exactly once with exact arguments\n// Fails if called 0 times, 2+ times, or with different args\n```\n\n### 9. Project Exclusion Patterns\n\nExclude specific projects using negative patterns.\n\n**CLI Usage:**\n```bash\n# Run all except integration tests\nvitest run --project=!integration\n\n# Run all except multiple projects\nvitest run --project=!integration --project=!e2e\n\n# Combine inclusion and exclusion\nvitest run --project=unit --project=!slow\n```\n\n### 10. UI Improvements\n\nSignificantly enhanced test UI with better debugging and navigation.\n\n**New UI Features:**\n- Run individual tests from UI\n- Automatic scroll to failed tests\n- Toggleable node_modules visibility\n- Improved filtering and search\n- Better test management controls\n- Module graph visualization\n\n**Launch UI:**\n```bash\nvitest --ui\n```\n\n## Best Practices for Vitest 3.x\n\n### 1. Use Annotation API for Complex Tests\n\n```javascript\ntest('complex payment flow', async ({ task }) => {\n  task.meta.annotation = {\n    message: 'Tests Stripe webhook integration',\n    attachments: [\n      { name: 'webhook-payload', content: JSON.stringify(payload) },\n      { name: 'ticket', content: 'https://jira.company.com/TICKET-123' }\n    ]\n  };\n\n  // Test implementation\n});\n```\n\n### 2. Leverage Line Filtering in Development\n\n```bash\n# Quick iteration on single test\nvitest run src/auth.test.js:145 --watch\n```\n\n### 3. Organize with Workspace Projects\n\n```javascript\nexport default defineConfig({\n  test: {\n    workspace: [\n      {\n        test: {\n          name: 'unit-fast',\n          include: ['tests/unit/**/*.test.js'],\n          environment: 'node'\n        }\n      },\n      {\n        test: {\n          name: 'unit-dom',\n          include: ['tests/unit/**/*.dom.test.js'],\n          environment: 'happy-dom'\n        }\n      },\n      {\n        test: {\n          name: 'integration',\n          include: ['tests/integration/**/*.test.js'],\n          setupFiles: ['tests/setup.js']\n        }\n      }\n    ]\n  }\n});\n```\n\n### 4. Use toHaveBeenCalledExactlyOnceWith for Strict Testing\n\n```javascript\ntest('should call API exactly once with correct params', () => {\n  const apiSpy = vi.spyOn(api, 'fetchUser');\n\n  renderComponent({ userId: 123 });\n\n  // Strict assertion - fails if called 0, 2+ times or wrong args\n  expect(apiSpy).toHaveBeenCalledExactlyOnceWith(123);\n});\n```\n\n### 5. Optimize Watch Mode for Large Projects\n\n```javascript\nexport default defineConfig({\n  test: {\n    watch: true,\n    watchExclude: [\n      '**/node_modules/**',\n      '**/dist/**',\n      '**/.git/**',\n      '**/coverage/**'\n    ],\n    // Run only related tests\n    changed: true,\n    // Faster reruns\n    isolate: false // Use with caution\n  }\n});\n```\n\n### 6. Custom Reporters for CI/CD\n\n```javascript\n// reporters/junit-plus.js\nexport default class JUnitPlusReporter {\n  onFinished(files, errors) {\n    const results = files.map(file => ({\n      name: file.name,\n      tests: file.tasks.length,\n      failures: file.tasks.filter(t => t.result?.state === 'fail').length,\n      // Add annotations from tests\n      annotations: file.tasks\n        .map(t => t.meta?.annotation)\n        .filter(Boolean)\n    }));\n\n    // Export to CI system\n    exportToCI(results);\n  }\n}\n```\n\n```javascript\n// vitest.config.js\nexport default defineConfig({\n  test: {\n    reporters: ['default', './reporters/junit-plus.js']\n  }\n});\n```\n\n## Migration from Vitest 2.x to 3.x\n\n### Breaking Changes\n\n1. **Workspace Configuration:**\n   - Move workspace projects into main config\n   - Update project references\n\n2. **Reporter API:**\n   - Update custom reporters to new lifecycle\n   - Check reporter method signatures\n\n3. **Browser Mode:**\n   - Update provider syntax if using browser mode\n   - Review browser configuration\n\n### Migration Steps\n\n```bash\n# Update Vitest\nnpm install -D vitest@^3.0.0\n\n# Update related packages\nnpm install -D @vitest/ui@^3.0.0\nnpm install -D @vitest/coverage-v8@^3.0.0\n\n# Run tests to check for issues\nnpm test\n\n# Update configs based on deprecation warnings\n```\n\n## Performance Optimization Tips\n\n### 1. Use Workspace Projects for Isolation\n\n```javascript\n// Separate fast unit tests from slower integration tests\nworkspace: [\n  {\n    test: {\n      name: 'unit',\n      include: ['tests/unit/**'],\n      environment: 'node' // Fastest\n    }\n  },\n  {\n    test: {\n      name: 'integration',\n      include: ['tests/integration/**'],\n      environment: 'happy-dom',\n      setupFiles: ['tests/setup.js']\n    }\n  }\n]\n```\n\n### 2. Parallelize Test Execution\n\n```bash\n# Run projects in parallel\nvitest run --project=unit --project=integration\n\n# Exclude slow tests during development\nvitest run --project=!e2e --project=!slow\n```\n\n### 3. Use Line Filtering for TDD\n\n```bash\n# Rapid iteration on single test\nvitest run src/feature.test.js:42 --watch\n```\n\n### 4. Optimize Browser Testing\n\n```javascript\nexport default defineConfig({\n  test: {\n    browser: {\n      enabled: true,\n      // Reuse context for speed (less isolation)\n      isolate: false,\n      // Single instance for development\n      instances: [{ browser: 'chromium' }]\n    }\n  }\n});\n```\n\n## Common Patterns\n\n### Test Annotation for Debugging\n\n```javascript\ntest('payment processing', async ({ task }) => {\n  const startTime = Date.now();\n\n  try {\n    const result = await processPayment(data);\n\n    task.meta.annotation = {\n      message: `Completed in ${Date.now() - startTime}ms`,\n      attachments: [\n        { name: 'result', content: JSON.stringify(result) }\n      ]\n    };\n\n    expect(result.success).toBe(true);\n  } catch (error) {\n    task.meta.annotation = {\n      message: 'Payment processing failed',\n      attachments: [\n        { name: 'error', content: error.message },\n        { name: 'payload', content: JSON.stringify(data) }\n      ]\n    };\n    throw error;\n  }\n});\n```\n\n### Multi-Project Testing Strategy\n\n```javascript\nexport default defineConfig({\n  test: {\n    workspace: [\n      // Fast feedback loop\n      {\n        test: {\n          name: 'unit-critical',\n          include: ['tests/unit/critical/**'],\n          environment: 'node'\n        }\n      },\n      // Standard tests\n      {\n        test: {\n          name: 'unit-standard',\n          include: ['tests/unit/**'],\n          exclude: ['tests/unit/critical/**'],\n          environment: 'happy-dom'\n        }\n      },\n      // Slow integration tests\n      {\n        test: {\n          name: 'integration',\n          include: ['tests/integration/**'],\n          testTimeout: 30000\n        }\n      }\n    ]\n  }\n});\n```\n\n## Resources\n\n- [Vitest 3.0 Release Notes](https://vitest.dev/blog/vitest-3)\n- [Vitest 3.2 Release Notes](https://vitest.dev/blog/vitest-3-2)\n- [Vitest Documentation](https://vitest.dev/)\n- [Migration Guide](https://vitest.dev/guide/migration.html)"
              },
              {
                "name": "windows-git-bash-testing",
                "description": "Windows and Git Bash testing compatibility guide for Vitest, Playwright, and MSW with path conversion patterns",
                "path": "plugins/test-master/skills/windows-git-bash-testing/SKILL.md",
                "frontmatter": {
                  "name": "windows-git-bash-testing",
                  "description": "Windows and Git Bash testing compatibility guide for Vitest, Playwright, and MSW with path conversion patterns"
                },
                "content": "# Windows and Git Bash Testing Compatibility Guide\n\n## Overview\n\nThis guide provides essential knowledge for running Vitest, Playwright, and MSW tests on Windows, particularly in Git Bash/MINGW environments. It addresses common path conversion issues, shell detection, and cross-platform test execution patterns.\n\n## Shell Detection in Test Environments\n\n### Detecting Git Bash/MINGW\n\nWhen running tests in Git Bash or MINGW environments, use these detection methods:\n\n**Method 1: Environment Variable (Most Reliable)**\n```javascript\n// Detect Git Bash/MINGW in Node.js test setup\nfunction isGitBash() {\n  return !!(process.env.MSYSTEM); // MINGW64, MINGW32, MSYS\n}\n\nfunction isWindows() {\n  return process.platform === 'win32';\n}\n\nfunction needsPathConversion() {\n  return isWindows() && isGitBash();\n}\n```\n\n**Method 2: Using uname in Setup Scripts**\n```bash\n# In bash test setup scripts\ncase \"$(uname -s)\" in\n  MINGW64*|MINGW32*|MSYS_NT*)\n    # Git Bash/MINGW environment\n    export TEST_ENV=\"mingw\"\n    ;;\n  CYGWIN*)\n    # Cygwin environment\n    export TEST_ENV=\"cygwin\"\n    ;;\n  Linux*)\n    export TEST_ENV=\"linux\"\n    ;;\n  Darwin*)\n    export TEST_ENV=\"macos\"\n    ;;\nesac\n```\n\n**Method 3: Combined Detection for Test Configuration**\n```javascript\n// vitest.config.js or test setup\nimport { execSync } from 'child_process';\n\nfunction detectShell() {\n  // Check MSYSTEM first (most reliable for Git Bash)\n  if (process.env.MSYSTEM) {\n    return { type: 'mingw', subsystem: process.env.MSYSTEM };\n  }\n\n  // Try uname if available\n  try {\n    const uname = execSync('uname -s', { encoding: 'utf8' }).trim();\n    if (uname.startsWith('MINGW')) return { type: 'mingw' };\n    if (uname.startsWith('CYGWIN')) return { type: 'cygwin' };\n    if (uname === 'Darwin') return { type: 'macos' };\n    if (uname === 'Linux') return { type: 'linux' };\n  } catch {\n    // uname not available (likely Windows cmd/PowerShell)\n  }\n\n  return { type: 'unknown', platform: process.platform };\n}\n\nconst shell = detectShell();\nconsole.log('Running tests in:', shell.type);\n```\n\n## Path Conversion Issues and Solutions\n\n### Common Path Conversion Problems\n\nGit Bash automatically converts Unix-style paths to Windows paths, which can cause issues with test file paths, module imports, and test configuration.\n\n**Problem Examples:**\n```bash\n# Git Bash converts these automatically:\n/foo  C:/Program Files/Git/usr/foo\n/foo:/bar  C:\\msys64\\foo;C:\\msys64\\bar\n--dir=/foo  --dir=C:/msys64/foo\n```\n\n### Solution 1: Disable Path Conversion\n\nFor test commands where path conversion causes issues:\n\n```bash\n# Disable all path conversion for a single command\nMSYS_NO_PATHCONV=1 vitest run\n\n# Disable for specific patterns\nexport MSYS2_ARG_CONV_EXCL=\"--coverage.reporter\"\nvitest run --coverage.reporter=html\n\n# Disable for entire test session\nexport MSYS_NO_PATHCONV=1\nnpm test\n```\n\n### Solution 2: Use Native Windows Paths in Configuration\n\nWhen specifying test file paths in configuration, use Windows-style paths:\n\n```javascript\n// vitest.config.js - Windows-compatible\nexport default defineConfig({\n  test: {\n    include: [\n      'tests/unit/**/*.test.js',     // Relative paths work best\n      'tests/integration/**/*.test.js'\n    ],\n    // Avoid absolute paths starting with /c/ or C:\n    setupFiles: ['./tests/setup.js'], // Use relative paths\n    coverage: {\n      reportsDirectory: './coverage'  // Relative, not absolute\n    }\n  }\n});\n```\n\n### Solution 3: Path Conversion Helper for Test Utilities\n\nCreate a helper for converting paths in test utilities:\n\n```javascript\n// tests/helpers/paths.js\nimport { execSync } from 'child_process';\n\n/**\n * Convert Windows path to Unix path for Git Bash compatibility\n */\nexport function toUnixPath(windowsPath) {\n  if (!needsPathConversion()) return windowsPath;\n\n  try {\n    // Use cygpath if available\n    return execSync(`cygpath -u \"${windowsPath}\"`, {\n      encoding: 'utf8'\n    }).trim();\n  } catch {\n    // Fallback: manual conversion\n    // C:\\Users\\foo  /c/Users/foo\n    return windowsPath\n      .replace(/\\\\/g, '/')\n      .replace(/^([A-Z]):/, (_, drive) => `/${drive.toLowerCase()}`);\n  }\n}\n\n/**\n * Convert Unix path to Windows path\n */\nexport function toWindowsPath(unixPath) {\n  if (!needsPathConversion()) return unixPath;\n\n  try {\n    return execSync(`cygpath -w \"${unixPath}\"`, {\n      encoding: 'utf8'\n    }).trim();\n  } catch {\n    // Fallback: manual conversion\n    // /c/Users/foo  C:\\Users\\foo\n    return unixPath\n      .replace(/^\\/([a-z])\\//, (_, drive) => `${drive.toUpperCase()}:\\\\`)\n      .replace(/\\//g, '\\\\');\n  }\n}\n\nfunction needsPathConversion() {\n  return !!(process.env.MSYSTEM ||\n           (process.platform === 'win32' && process.env.TERM === 'cygwin'));\n}\n```\n\n**Usage in Tests:**\n```javascript\nimport { toUnixPath, toWindowsPath } from '../helpers/paths.js';\n\ntest('loads config file', () => {\n  const configPath = toWindowsPath('/c/project/config.json');\n  const config = loadConfig(configPath);\n  expect(config).toBeDefined();\n});\n```\n\n## Test Execution Best Practices for Windows/Git Bash\n\n### 1. NPM Scripts for Cross-Platform Compatibility\n\nDefine test scripts in package.json that work across all environments:\n\n```json\n{\n  \"scripts\": {\n    \"test\": \"vitest run\",\n    \"test:unit\": \"vitest run tests/unit\",\n    \"test:integration\": \"vitest run tests/integration\",\n    \"test:watch\": \"vitest watch\",\n    \"test:coverage\": \"vitest run --coverage\",\n    \"test:e2e\": \"playwright test\",\n    \"test:e2e:headed\": \"playwright test --headed\",\n    \"test:debug\": \"vitest run --reporter=verbose\"\n  }\n}\n```\n\n**Always use npm scripts rather than direct vitest/playwright commands** - this ensures consistent behavior across shells.\n\n### 2. Relative Path Imports\n\nUse relative paths in test files to avoid path conversion issues:\n\n```javascript\n//  Good - Relative paths work everywhere\nimport { myFunction } from '../../src/utils.js';\nimport { server } from '../mocks/server.js';\n\n//  Bad - Absolute paths can cause issues in Git Bash\nimport { myFunction } from '/c/project/src/utils.js';\n```\n\n### 3. Test File Discovery\n\nVitest and Playwright handle file patterns differently in Git Bash:\n\n```javascript\n// vitest.config.js - Use glob patterns, not absolute paths\nexport default defineConfig({\n  test: {\n    //  Good - Glob patterns work cross-platform\n    include: ['tests/**/*.test.js', 'src/**/*.test.js'],\n\n    //  Bad - Absolute paths problematic in Git Bash\n    include: ['/c/project/tests/**/*.test.js']\n  }\n});\n```\n\n```javascript\n// playwright.config.js - Relative directory paths\nexport default defineConfig({\n  //  Good\n  testDir: './tests/e2e',\n\n  //  Bad\n  testDir: '/c/project/tests/e2e'\n});\n```\n\n### 4. Temporary File Handling\n\nGit Bash uses Unix-style temp directories, which can cause issues:\n\n```javascript\n// tests/setup.js - Cross-platform temp file handling\nimport os from 'os';\nimport path from 'path';\n\nfunction getTempDir() {\n  const tmpdir = os.tmpdir();\n\n  // In Git Bash, os.tmpdir() may return Windows path\n  // Ensure it's usable by your test framework\n  if (process.env.MSYSTEM && !tmpdir.startsWith('/')) {\n    // Convert Windows temp path if needed\n    return tmpdir.replace(/\\\\/g, '/');\n  }\n\n  return tmpdir;\n}\n\n// Use in tests\nconst testTempDir = path.join(getTempDir(), 'my-tests');\n```\n\n## Playwright-Specific Windows/Git Bash Considerations\n\n### 1. Browser Installation in Git Bash\n\n```bash\n# Use npx to ensure correct path handling\nnpx playwright install\n\n# If issues occur, use Windows-native command prompt instead\n# Then run tests from Git Bash\n```\n\n### 2. Headed Mode in MINGW\n\nWhen running headed tests in Git Bash, ensure DISPLAY variables are not set:\n\n```bash\n# Unset DISPLAY if set (can interfere with Windows GUI)\nunset DISPLAY\n\n# Run headed tests\nnpx playwright test --headed\n```\n\n### 3. Screenshot and Video Paths\n\nUse relative paths for artifacts:\n\n```javascript\n// playwright.config.js\nexport default defineConfig({\n  use: {\n    //  Good - Relative paths\n    screenshot: 'only-on-failure',\n    video: 'retain-on-failure',\n  },\n  // Output directory with relative path\n  outputDir: './test-results',\n});\n```\n\n## MSW (Mock Service Worker) in Git Bash\n\nMSW generally works without issues in Git Bash, but be aware of:\n\n### 1. Handler File Imports\n\nUse relative imports in MSW setup:\n\n```javascript\n// tests/mocks/server.js\n//  Good\nimport { handlers } from './handlers.js';\n\n//  Bad - Avoid absolute paths\nimport { handlers } from '/c/project/tests/mocks/handlers.js';\n```\n\n### 2. Fetch Polyfill (Node.js)\n\nEnsure Node.js version 18+ for native Fetch API support:\n\n```bash\n# Check Node version\nnode --version  # Should be 18+\n\n# If using older Node, MSW 2.x won't work properly\n```\n\n## Common Windows/Git Bash Test Errors and Fixes\n\n### Error 1: \"No such file or directory\" in Git Bash\n\n**Symptom:**\n```\nError: /usr/bin/bash: line 1: C:UsersUsername...No such file\n```\n\n**Cause:** Path conversion issue, often with temp directories.\n\n**Fix:**\n```bash\n# Option 1: Disable path conversion\nMSYS_NO_PATHCONV=1 npm test\n\n# Option 2: Use Claude Code's Git Bash path variable\nexport CLAUDE_CODE_GIT_BASH_PATH=\"C:\\\\Program Files\\\\git\\\\bin\\\\bash.exe\"\n\n# Option 3: Run via npm scripts (recommended)\nnpm test  # npm handles path issues automatically\n```\n\n### Error 2: Module Import Failures\n\n**Symptom:**\n```\nError: Cannot find module '../src/utils'\n```\n\n**Cause:** Path separator confusion (backslash vs forward slash).\n\n**Fix:**\n```javascript\n// Use path.join() or path.resolve() for cross-platform paths\nimport path from 'path';\n\nconst utilsPath = path.resolve(__dirname, '../src/utils.js');\nconst utils = await import(utilsPath);\n```\n\n### Error 3: Playwright Browser Launch Failure\n\n**Symptom:**\n```\nError: Failed to launch browser\n```\n\n**Cause:** Git Bash environment variables interfering with browser launch.\n\n**Fix:**\n```bash\n# Clear problematic environment variables\nunset DISPLAY\nunset BROWSER\n\n# Run Playwright tests\nnpx playwright test\n```\n\n### Error 4: Coverage Report Path Issues\n\n**Symptom:**\n```\nError: Failed to write coverage to /c/project/coverage\n```\n\n**Fix:**\n```javascript\n// vitest.config.js - Use relative paths\nexport default defineConfig({\n  test: {\n    coverage: {\n      //  Good\n      reportsDirectory: './coverage',\n\n      //  Bad\n      reportsDirectory: '/c/project/coverage'\n    }\n  }\n});\n```\n\n## Testing Strategy for Multi-Platform Support\n\n### 1. CI/CD Configuration\n\nTest on multiple platforms in CI/CD:\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\non: [push, pull_request]\n\njobs:\n  test:\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        node: [18, 20, 22]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n\n      - run: npm ci\n      - run: npm test\n      - run: npm run test:e2e\n        if: matrix.os == 'ubuntu-latest'  # Run E2E on Linux only\n```\n\n### 2. Shell-Specific Test Setup\n\nCreate shell-specific setup if needed:\n\n```javascript\n// tests/setup.js\nimport { detectShell } from './helpers/shell-detect.js';\n\nconst shell = detectShell();\n\n// Apply shell-specific configuration\nif (shell.type === 'mingw') {\n  console.log('Running in Git Bash/MINGW environment');\n  // Apply MINGW-specific test setup\n  process.env.FORCE_COLOR = '1'; // Enable colors in Git Bash\n}\n\n// Standard setup continues...\n```\n\n### 3. Path-Safe Test Utilities\n\nCreate test utilities that work across all platforms:\n\n```javascript\n// tests/helpers/test-utils.js\nimport path from 'path';\nimport { fileURLToPath } from 'url';\nimport { dirname } from 'path';\n\n// Get __dirname equivalent in ESM\nexport function getCurrentDir(importMetaUrl) {\n  return dirname(fileURLToPath(importMetaUrl));\n}\n\n// Cross-platform path joining\nexport function testPath(...segments) {\n  return path.join(...segments).replace(/\\\\/g, '/');\n}\n\n// Usage in test file:\n// const __dirname = getCurrentDir(import.meta.url);\n// const fixturePath = testPath(__dirname, 'fixtures', 'data.json');\n```\n\n## Quick Reference\n\n### Shell Detection Commands\n\n```bash\n# Check if running in Git Bash\necho $MSYSTEM           # MINGW64, MINGW32, or empty\n\n# Check OS type\nuname -s               # MINGW64_NT-* for Git Bash\n\n# Check platform in Node\nnode -p process.platform  # win32 on Windows\n```\n\n### Path Conversion Quick Fixes\n\n```bash\n# Disable for single command\nMSYS_NO_PATHCONV=1 vitest run\n\n# Disable for session\nexport MSYS_NO_PATHCONV=1\n\n# Convert path manually\ncygpath -u \"C:\\path\"    # Windows  Unix\ncygpath -w \"/c/path\"    # Unix  Windows\n```\n\n### Recommended Test Execution (Git Bash on Windows)\n\n```bash\n#  Best - Use npm scripts\nnpm test\nnpm run test:e2e\n\n#  Good - With path conversion disabled\nMSYS_NO_PATHCONV=1 vitest run\n\n#  Caution - Direct vitest command may have issues\nvitest run  # May encounter path conversion issues\n```\n\n## Resources\n\n- [Git Bash Path Conversion](https://www.msys2.org/docs/filesystem-paths/)\n- [Vitest Configuration](https://vitest.dev/config/)\n- [Playwright on Windows](https://playwright.dev/docs/browsers#install-system-requirements)\n- [MSW Node.js Integration](https://mswjs.io/docs/integrations/node)\n\n## Summary\n\nWhen running tests on Windows with Git Bash:\n\n1. **Use npm scripts** for all test execution (most reliable)\n2. **Use relative paths** in configuration and imports\n3. **Detect shell environment** when needed for conditional setup\n4. **Disable path conversion** (MSYS_NO_PATHCONV=1) if issues occur\n5. **Test on multiple platforms** in CI/CD to catch platform-specific issues\n6. **Avoid absolute paths** starting with /c/ or C:\\ in test files\n7. **Use path.join()** for programmatic path construction\n\nFollowing these practices ensures tests run reliably across Windows Command Prompt, PowerShell, Git Bash, and Unix-like environments."
              }
            ]
          },
          {
            "name": "powershell-master",
            "description": "Complete PowerShell expertise system across ALL platforms (Windows/Linux/macOS). PROACTIVELY activate for: (1) ANY PowerShell task (scripts/modules/cmdlets), (2) CI/CD automation (GitHub Actions/Azure DevOps/Bitbucket), (3) Cross-platform scripting, (4) Module discovery and management (PSGallery), (5) Azure/AWS/Microsoft 365 automation, (6) Script debugging and optimization, (7) Best practices and security. Provides: PowerShell 7+ features, popular module expertise (Az, Microsoft.Graph, PnP, AWS Tools), PSGallery integration, platform-specific guidance, CI/CD pipeline patterns, cmdlet syntax mastery, and production-ready scripting patterns.",
            "source": "./plugins/powershell-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install powershell-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/ps-analyze",
                "description": "Analyze PowerShell scripts for best practices, security issues, and compatibility",
                "path": "plugins/powershell-master/commands/ps-analyze.md",
                "frontmatter": {
                  "name": "ps-analyze",
                  "description": "Analyze PowerShell scripts for best practices, security issues, and compatibility",
                  "argument-hint": "<script.ps1 or directory path>",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep",
                    "Task"
                  ]
                },
                "content": "# PowerShell Script Analyzer\n\nAnalyze PowerShell scripts for:\n1. **Security issues** - Credential handling, injection vulnerabilities, plaintext secrets\n2. **Best practices** - PSScriptAnalyzer rules, naming conventions, error handling\n3. **Compatibility** - PowerShell 5.1 vs 7.x, cross-platform issues, deprecated cmdlets\n4. **Performance** - Inefficient patterns, += in loops, unnecessary pipeline operations\n5. **2025 changes** - MSOnline/AzureAD usage, WMIC calls, PowerShell 2.0 patterns\n\n## Analysis Checklist\n\n### Security Analysis\n- [ ] Check for plaintext credentials in scripts\n- [ ] Identify `ConvertTo-SecureString -AsPlainText` without proper handling\n- [ ] Find hardcoded connection strings or API keys\n- [ ] Check for command injection vulnerabilities (Invoke-Expression with user input)\n- [ ] Verify SecretManagement usage for secrets\n\n### Best Practices\n- [ ] Function naming (Verb-Noun format)\n- [ ] Parameter validation attributes\n- [ ] Error handling (try/catch, $ErrorActionPreference)\n- [ ] Comment-based help documentation\n- [ ] Output type declarations\n\n### Compatibility\n- [ ] Check for Windows-only cmdlets used on cross-platform\n- [ ] Identify deprecated MSOnline/AzureAD module usage\n- [ ] Find WMIC calls that need replacement\n- [ ] Check for PowerShell 2.0 patterns\n\n### Performance\n- [ ] Find `+=` in loops (recommend ArrayList/List[T] or 7.6 upgrade)\n- [ ] Identify unnecessary `Where-Object` after `Get-*` cmdlets with -Filter\n- [ ] Check for repeated expensive operations inside loops\n\n## Output Format\n\nProvide a structured report with:\n1. **Summary** - Overall assessment and risk level\n2. **Critical Issues** - Security vulnerabilities requiring immediate attention\n3. **Warnings** - Best practice violations and compatibility issues\n4. **Suggestions** - Performance improvements and modernization opportunities\n5. **Code Examples** - Specific fixes for each identified issue"
              },
              {
                "name": "/ps-ci",
                "description": "Generate CI/CD pipeline for PowerShell module (GitHub Actions, Azure DevOps, GitLab)",
                "path": "plugins/powershell-master/commands/ps-ci.md",
                "frontmatter": {
                  "name": "ps-ci",
                  "description": "Generate CI/CD pipeline for PowerShell module (GitHub Actions, Azure DevOps, GitLab)",
                  "argument-hint": "<platform: github|azure|gitlab> [module path]",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob"
                  ]
                },
                "content": "# PowerShell CI/CD Pipeline Generator\n\nGenerate production-ready CI/CD pipelines for PowerShell modules.\n\n## GitHub Actions Workflow\n\n```yaml\n# .github/workflows/ci.yml\nname: PowerShell CI\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup PowerShell\n        uses: actions/setup-powershell@v1\n        with:\n          pwsh-version: '7.5.x'\n\n      - name: Install PSScriptAnalyzer\n        shell: pwsh\n        run: Install-PSResource -Name PSScriptAnalyzer -Scope CurrentUser -TrustRepository\n\n      - name: Run PSScriptAnalyzer\n        shell: pwsh\n        run: |\n          $results = Invoke-ScriptAnalyzer -Path . -Recurse -Settings PSScriptAnalyzerSettings.psd1\n          $results | Format-Table -AutoSize\n          if ($results | Where-Object Severity -eq 'Error') {\n            throw \"PSScriptAnalyzer found errors\"\n          }\n\n  test:\n    needs: lint\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        pwsh: ['7.4.x', '7.5.x']\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup PowerShell ${{ matrix.pwsh }}\n        uses: actions/setup-powershell@v1\n        with:\n          pwsh-version: ${{ matrix.pwsh }}\n\n      - name: Install Pester\n        shell: pwsh\n        run: Install-PSResource -Name Pester -Scope CurrentUser -TrustRepository\n\n      - name: Run Tests\n        shell: pwsh\n        run: |\n          $config = New-PesterConfiguration\n          $config.Run.Path = \"./Tests\"\n          $config.Run.Exit = $true\n          $config.Output.Verbosity = \"Detailed\"\n          $config.CodeCoverage.Enabled = $true\n          $config.CodeCoverage.Path = @(\"./Public/*.ps1\", \"./Private/*.ps1\")\n          $config.CodeCoverage.OutputPath = \"coverage.xml\"\n          $config.CodeCoverage.OutputFormat = \"JaCoCo\"\n          $config.TestResult.Enabled = $true\n          $config.TestResult.OutputPath = \"testResults.xml\"\n          Invoke-Pester -Configuration $config\n\n      - name: Upload Coverage\n        uses: codecov/codecov-action@v4\n        with:\n          files: coverage.xml\n          fail_ci_if_error: true\n\n  publish:\n    needs: test\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup PowerShell\n        uses: actions/setup-powershell@v1\n        with:\n          pwsh-version: '7.5.x'\n\n      - name: Publish to PSGallery\n        shell: pwsh\n        env:\n          PSGALLERY_API_KEY: ${{ secrets.PSGALLERY_API_KEY }}\n        run: |\n          Install-PSResource -Name Microsoft.PowerShell.PSResourceGet -Scope CurrentUser -TrustRepository\n          Publish-PSResource -Path ./ModuleName -Repository PSGallery -ApiKey $env:PSGALLERY_API_KEY\n```\n\n## Azure DevOps Pipeline\n\n```yaml\n# azure-pipelines.yml\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n\npool:\n  vmImage: 'ubuntu-latest'\n\nstages:\n  - stage: Build\n    jobs:\n      - job: Lint\n        steps:\n          - task: PowerShell@2\n            displayName: 'Install PSScriptAnalyzer'\n            inputs:\n              targetType: 'inline'\n              script: Install-Module PSScriptAnalyzer -Force -SkipPublisherCheck\n              pwsh: true\n\n          - task: PowerShell@2\n            displayName: 'Run PSScriptAnalyzer'\n            inputs:\n              targetType: 'inline'\n              script: |\n                $results = Invoke-ScriptAnalyzer -Path . -Recurse\n                $results | Format-Table -AutoSize\n                if ($results | Where-Object Severity -eq 'Error') {\n                  throw \"PSScriptAnalyzer found errors\"\n                }\n              pwsh: true\n\n  - stage: Test\n    dependsOn: Build\n    jobs:\n      - job: Test\n        strategy:\n          matrix:\n            Windows:\n              vmImage: 'windows-latest'\n            Linux:\n              vmImage: 'ubuntu-latest'\n            macOS:\n              vmImage: 'macos-latest'\n        pool:\n          vmImage: $(vmImage)\n        steps:\n          - task: PowerShell@2\n            displayName: 'Install Pester'\n            inputs:\n              targetType: 'inline'\n              script: Install-Module Pester -Force -SkipPublisherCheck\n              pwsh: true\n\n          - task: PowerShell@2\n            displayName: 'Run Pester Tests'\n            inputs:\n              targetType: 'inline'\n              script: |\n                $config = New-PesterConfiguration\n                $config.Run.Path = \"./Tests\"\n                $config.Run.Exit = $true\n                $config.CodeCoverage.Enabled = $true\n                $config.CodeCoverage.OutputPath = \"$(Build.ArtifactStagingDirectory)/coverage.xml\"\n                $config.TestResult.Enabled = $true\n                $config.TestResult.OutputPath = \"$(Build.ArtifactStagingDirectory)/testResults.xml\"\n                $config.TestResult.OutputFormat = \"NUnitXml\"\n                Invoke-Pester -Configuration $config\n              pwsh: true\n\n          - task: PublishTestResults@2\n            inputs:\n              testResultsFormat: 'NUnit'\n              testResultsFiles: '$(Build.ArtifactStagingDirectory)/testResults.xml'\n\n          - task: PublishCodeCoverageResults@2\n            inputs:\n              codeCoverageTool: 'JaCoCo'\n              summaryFileLocation: '$(Build.ArtifactStagingDirectory)/coverage.xml'\n\n  - stage: Publish\n    dependsOn: Test\n    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n    jobs:\n      - deployment: PublishPSGallery\n        environment: 'production'\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                - task: PowerShell@2\n                  displayName: 'Publish to PSGallery'\n                  inputs:\n                    targetType: 'inline'\n                    script: |\n                      Publish-Module -Path ./ModuleName -NuGetApiKey $(PSGalleryApiKey)\n                    pwsh: true\n```\n\n## GitLab CI Pipeline\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - lint\n  - test\n  - publish\n\nvariables:\n  PWSH_VERSION: \"7.5\"\n\n.pwsh_setup: &pwsh_setup\n  before_script:\n    - pwsh -Command \"Install-PSResource -Name Pester, PSScriptAnalyzer -Scope CurrentUser -TrustRepository\"\n\nlint:\n  stage: lint\n  image: mcr.microsoft.com/powershell:${PWSH_VERSION}\n  <<: *pwsh_setup\n  script:\n    - pwsh -Command |\n        $results = Invoke-ScriptAnalyzer -Path . -Recurse\n        $results | Format-Table -AutoSize\n        if ($results | Where-Object Severity -eq 'Error') { exit 1 }\n\ntest:\n  stage: test\n  image: mcr.microsoft.com/powershell:${PWSH_VERSION}\n  <<: *pwsh_setup\n  script:\n    - pwsh -Command |\n        $config = New-PesterConfiguration\n        $config.Run.Path = \"./Tests\"\n        $config.CodeCoverage.Enabled = $true\n        $config.CodeCoverage.OutputPath = \"coverage.xml\"\n        $config.TestResult.Enabled = $true\n        $config.TestResult.OutputPath = \"testResults.xml\"\n        Invoke-Pester -Configuration $config\n  artifacts:\n    reports:\n      junit: testResults.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage.xml\n\npublish:\n  stage: publish\n  image: mcr.microsoft.com/powershell:${PWSH_VERSION}\n  only:\n    - main\n  script:\n    - pwsh -Command \"Publish-Module -Path ./ModuleName -NuGetApiKey $env:PSGALLERY_API_KEY\"\n```\n\n## Pipeline Components\n\n| Component | Purpose |\n|-----------|---------|\n| PSScriptAnalyzer | Static code analysis |\n| Pester | Unit/integration tests |\n| Code Coverage | Coverage reporting |\n| Multi-platform | Windows, Linux, macOS testing |\n| Multi-version | PowerShell 7.4, 7.5 testing |\n| Publish | PSGallery deployment |"
              },
              {
                "name": "/ps-migrate",
                "description": "Migrate PowerShell scripts for 2025 changes - MSOnline to Graph, AzureAD to Graph, WMIC replacement",
                "path": "plugins/powershell-master/commands/ps-migrate.md",
                "frontmatter": {
                  "name": "ps-migrate",
                  "description": "Migrate PowerShell scripts for 2025 changes - MSOnline to Graph, AzureAD to Graph, WMIC replacement",
                  "argument-hint": "<script.ps1 or 'MSOnline'|'AzureAD'|'WMIC'>",
                  "allowed-tools": [
                    "Read",
                    "Edit",
                    "Glob",
                    "Grep",
                    "WebSearch",
                    "Task"
                  ]
                },
                "content": "# PowerShell 2025 Migration Assistant\n\nMigrate scripts affected by 2025 breaking changes:\n\n## Migration Targets\n\n### 1. MSOnline to Microsoft Graph PowerShell (Retired March 2025)\n```powershell\n# OLD: MSOnline\nConnect-MsolService\nGet-MsolUser -All\nSet-MsolUser -UserPrincipalName $upn -DisplayName $name\n\n# NEW: Microsoft Graph PowerShell\nConnect-MgGraph -Scopes \"User.Read.All\", \"User.ReadWrite.All\"\nGet-MgUser -All\nUpdate-MgUser -UserId $userId -DisplayName $name\n```\n\n### 2. AzureAD to Microsoft Graph PowerShell (Retired May 2025)\n```powershell\n# OLD: AzureAD\nConnect-AzureAD\nGet-AzureADUser -All $true\nGet-AzureADGroup -ObjectId $groupId\nNew-AzureADUser -DisplayName $name -UserPrincipalName $upn\n\n# NEW: Microsoft Graph PowerShell\nConnect-MgGraph -Scopes \"User.Read.All\", \"Group.Read.All\"\nGet-MgUser -All\nGet-MgGroup -GroupId $groupId\nNew-MgUser -DisplayName $name -UserPrincipalName $upn -PasswordProfile $pwdProfile\n```\n\n### 3. WMIC to PowerShell Native (Removed in Windows 11 24H2)\n```powershell\n# OLD: WMIC\nwmic os get caption\nwmic cpu get name\nwmic process list brief\nwmic product get name,version\n\n# NEW: PowerShell\nGet-CimInstance Win32_OperatingSystem | Select-Object Caption\nGet-CimInstance Win32_Processor | Select-Object Name\nGet-CimInstance Win32_Process | Select-Object Handle, Name, ProcessId\nGet-CimInstance Win32_Product | Select-Object Name, Version\n# Or for software: Get-Package (faster, doesn't use WMI)\n```\n\n## Migration Process\n\n1. **Scan** - Identify all occurrences of deprecated modules/commands\n2. **Map** - Create mapping of old commands to new equivalents\n3. **Update** - Replace commands with modern alternatives\n4. **Test** - Verify functionality after migration\n5. **Document** - Update any related documentation\n\n## Common Cmdlet Mappings\n\n| MSOnline | Graph PowerShell |\n|----------|-----------------|\n| `Connect-MsolService` | `Connect-MgGraph` |\n| `Get-MsolUser` | `Get-MgUser` |\n| `Set-MsolUser` | `Update-MgUser` |\n| `Get-MsolGroup` | `Get-MgGroup` |\n| `Add-MsolGroupMember` | `New-MgGroupMember` |\n\n| AzureAD | Graph PowerShell |\n|---------|-----------------|\n| `Connect-AzureAD` | `Connect-MgGraph` |\n| `Get-AzureADUser` | `Get-MgUser` |\n| `New-AzureADUser` | `New-MgUser` |\n| `Get-AzureADGroup` | `Get-MgGroup` |\n| `Get-AzureADApplication` | `Get-MgApplication` |\n\n| WMIC | PowerShell |\n|------|------------|\n| `wmic os get` | `Get-CimInstance Win32_OperatingSystem` |\n| `wmic cpu get` | `Get-CimInstance Win32_Processor` |\n| `wmic memorychip get` | `Get-CimInstance Win32_PhysicalMemory` |\n| `wmic diskdrive get` | `Get-CimInstance Win32_DiskDrive` |\n| `wmic process` | `Get-CimInstance Win32_Process` |\n\n## Prerequisites for Migration\n\n```powershell\n# Install Microsoft Graph PowerShell SDK\nInstall-PSResource -Name Microsoft.Graph -Scope CurrentUser\n\n# Or install specific submodules\nInstall-PSResource -Name Microsoft.Graph.Users\nInstall-PSResource -Name Microsoft.Graph.Groups\nInstall-PSResource -Name Microsoft.Graph.Authentication\n```"
              },
              {
                "name": "/ps-module",
                "description": "Create a new PowerShell module with best practices structure",
                "path": "plugins/powershell-master/commands/ps-module.md",
                "frontmatter": {
                  "name": "ps-module",
                  "description": "Create a new PowerShell module with best practices structure",
                  "argument-hint": "<ModuleName> [type: script|binary|manifest]",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Bash",
                    "Glob"
                  ]
                },
                "content": "# PowerShell Module Generator\n\nCreate a production-ready PowerShell module with:\n- Proper directory structure\n- Module manifest (.psd1) with all metadata\n- Root module (.psm1) with function organization\n- Pester tests setup\n- PSScriptAnalyzer configuration\n- CI/CD pipeline templates\n\n## Module Structure\n\n```\nModuleName/\n ModuleName.psd1          # Module manifest\n ModuleName.psm1          # Root module\n Public/                  # Exported functions\n    Get-Something.ps1\n    Set-Something.ps1\n Private/                 # Internal functions\n    Helper.ps1\n Classes/                 # PowerShell classes (optional)\n    MyClass.ps1\n Tests/\n    ModuleName.Tests.ps1\n    Public/\n        Get-Something.Tests.ps1\n .vscode/\n    settings.json\n PSScriptAnalyzerSettings.psd1\n build.ps1\n README.md\n```\n\n## Module Manifest Template (.psd1)\n\n```powershell\n@{\n    RootModule = 'ModuleName.psm1'\n    ModuleVersion = '1.0.0'\n    GUID = '<generated-guid>'\n    Author = '<author>'\n    CompanyName = '<company>'\n    Copyright = '(c) 2025 <author>. All rights reserved.'\n    Description = '<description>'\n    PowerShellVersion = '7.0'\n    CompatiblePSEditions = @('Core', 'Desktop')\n    FunctionsToExport = @('Get-Something', 'Set-Something')\n    CmdletsToExport = @()\n    VariablesToExport = @()\n    AliasesToExport = @()\n    PrivateData = @{\n        PSData = @{\n            Tags = @('tag1', 'tag2')\n            LicenseUri = 'https://github.com/user/repo/blob/main/LICENSE'\n            ProjectUri = 'https://github.com/user/repo'\n            ReleaseNotes = 'Initial release'\n            Prerelease = ''\n        }\n    }\n}\n```\n\n## Root Module Template (.psm1)\n\n```powershell\n#Requires -Version 7.0\n\n# Get public and private function files\n$Public = @(Get-ChildItem -Path \"$PSScriptRoot\\Public\\*.ps1\" -ErrorAction SilentlyContinue)\n$Private = @(Get-ChildItem -Path \"$PSScriptRoot\\Private\\*.ps1\" -ErrorAction SilentlyContinue)\n\n# Dot source the files\nforeach ($import in @($Public + $Private)) {\n    try {\n        . $import.FullName\n    } catch {\n        Write-Error \"Failed to import function $($import.FullName): $_\"\n    }\n}\n\n# Export public functions\nExport-ModuleMember -Function $Public.BaseName\n```\n\n## Function Template\n\n```powershell\nfunction Get-Something {\n    <#\n    .SYNOPSIS\n        Brief description of the function.\n\n    .DESCRIPTION\n        Detailed description of the function.\n\n    .PARAMETER Name\n        Description of the Name parameter.\n\n    .EXAMPLE\n        Get-Something -Name \"Example\"\n        Description of what this example does.\n\n    .OUTPUTS\n        System.String\n\n    .NOTES\n        Author: Your Name\n        Version: 1.0.0\n    #>\n    [CmdletBinding()]\n    [OutputType([string])]\n    param(\n        [Parameter(Mandatory, ValueFromPipeline, ValueFromPipelineByPropertyName)]\n        [ValidateNotNullOrEmpty()]\n        [string]$Name\n    )\n\n    begin {\n        # Initialization code\n    }\n\n    process {\n        # Main logic\n        Write-Output \"Processing: $Name\"\n    }\n\n    end {\n        # Cleanup code\n    }\n}\n```\n\n## PSScriptAnalyzer Settings\n\n```powershell\n# PSScriptAnalyzerSettings.psd1\n@{\n    Severity = @('Error', 'Warning', 'Information')\n    ExcludeRules = @()\n    IncludeDefaultRules = $true\n    Rules = @{\n        PSUseCompatibleSyntax = @{\n            Enable = $true\n            TargetVersions = @('7.0', '5.1')\n        }\n        PSUseCompatibleCommands = @{\n            Enable = $true\n            TargetProfiles = @(\n                'win-48_x64_10.0.17763.0_7.0.0_x64_4.0.30319.42000_core'\n                'win-8_x64_10.0.17763.0_5.1.17763.316_x64_4.0.30319.42000_framework'\n            )\n        }\n    }\n}\n```\n\n## Pester Test Template\n\n```powershell\nBeforeAll {\n    $ModulePath = Split-Path -Parent (Split-Path -Parent $PSScriptRoot)\n    Import-Module \"$ModulePath\\ModuleName.psd1\" -Force\n}\n\nDescribe 'Get-Something' {\n    Context 'When called with valid input' {\n        It 'Should return expected output' {\n            $result = Get-Something -Name 'Test'\n            $result | Should -Be 'Processing: Test'\n        }\n    }\n\n    Context 'When called with pipeline input' {\n        It 'Should process multiple items' {\n            $result = @('A', 'B', 'C') | Get-Something\n            $result | Should -HaveCount 3\n        }\n    }\n}\n```"
              },
              {
                "name": "/ps-secure",
                "description": "Set up SecretManagement vault and migrate hardcoded credentials",
                "path": "plugins/powershell-master/commands/ps-secure.md",
                "frontmatter": {
                  "name": "ps-secure",
                  "description": "Set up SecretManagement vault and migrate hardcoded credentials",
                  "argument-hint": "<script.ps1 to migrate> or 'setup' for new vault",
                  "allowed-tools": [
                    "Read",
                    "Edit",
                    "Write",
                    "Bash",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# PowerShell SecretManagement Setup\n\nSecure credential management using Microsoft.PowerShell.SecretManagement and SecretStore.\n\n## Quick Setup\n\n```powershell\n# Install SecretManagement modules\nInstall-PSResource -Name Microsoft.PowerShell.SecretManagement -Scope CurrentUser\nInstall-PSResource -Name Microsoft.PowerShell.SecretStore -Scope CurrentUser\n\n# Register the secret vault\nRegister-SecretVault -Name \"LocalVault\" -ModuleName Microsoft.PowerShell.SecretStore -DefaultVault\n\n# Configure vault (set password on first use)\nSet-SecretStoreConfiguration -Scope CurrentUser -Authentication Password -PasswordTimeout 3600\n\n# Store a secret\nSet-Secret -Name \"ApiKey\" -Secret \"your-api-key-here\" -Vault \"LocalVault\"\nSet-Secret -Name \"DbConnection\" -Secret \"Server=...;Database=...\" -Vault \"LocalVault\"\n\n# Store a credential object\n$cred = Get-Credential -Message \"Enter service account credentials\"\nSet-Secret -Name \"ServiceAccount\" -Secret $cred -Vault \"LocalVault\"\n\n# Retrieve secrets\n$apiKey = Get-Secret -Name \"ApiKey\" -AsPlainText\n$dbConn = Get-Secret -Name \"DbConnection\" -AsPlainText\n$cred = Get-Secret -Name \"ServiceAccount\"  # Returns PSCredential\n```\n\n## Migration Patterns\n\n### From Hardcoded Credentials\n\n```powershell\n# BEFORE (insecure)\n$password = \"MyP@ssw0rd!\"\n$username = \"admin\"\n$cred = New-Object PSCredential($username, (ConvertTo-SecureString $password -AsPlainText -Force))\n\n# AFTER (secure)\n$cred = Get-Secret -Name \"AdminCredential\"\n# Or if stored separately:\n$username = Get-Secret -Name \"AdminUsername\" -AsPlainText\n$password = Get-Secret -Name \"AdminPassword\"  # Returns SecureString\n$cred = New-Object PSCredential($username, $password)\n```\n\n### From Environment Variables\n\n```powershell\n# BEFORE\n$apiKey = $env:API_KEY\n\n# AFTER (for scripts)\n$apiKey = Get-Secret -Name \"ApiKey\" -AsPlainText\n\n# For CI/CD, keep using env vars but load into vault at runtime\nif ($env:CI) {\n    Set-Secret -Name \"ApiKey\" -Secret $env:API_KEY -Vault \"TempVault\"\n}\n```\n\n### From Config Files\n\n```powershell\n# BEFORE (config.json with secrets)\n$config = Get-Content \"config.json\" | ConvertFrom-Json\n$connectionString = $config.database.connectionString\n\n# AFTER (config.json references vault)\n$config = Get-Content \"config.json\" | ConvertFrom-Json\n$connectionString = Get-Secret -Name $config.database.secretName -AsPlainText\n```\n\n## Azure Key Vault Integration\n\n```powershell\n# Install Azure Key Vault extension\nInstall-PSResource -Name Az.KeyVault\nInstall-PSResource -Name Microsoft.PowerShell.SecretManagement.Azure.KeyVault\n\n# Register Azure Key Vault as a secret vault\nRegister-SecretVault -Name \"AzureVault\" `\n    -ModuleName Microsoft.PowerShell.SecretManagement.Azure.KeyVault `\n    -VaultParameters @{\n        AZKVaultName = \"my-keyvault\"\n        SubscriptionId = \"subscription-guid\"\n    }\n\n# Use secrets from Azure Key Vault\n$secret = Get-Secret -Name \"MySecret\" -Vault \"AzureVault\" -AsPlainText\n```\n\n## Vault Types\n\n| Vault Type | Use Case | Setup |\n|------------|----------|-------|\n| SecretStore | Local development | Built-in, password protected |\n| Azure Key Vault | Production/Cloud | Requires Az.KeyVault module |\n| HashiCorp Vault | Enterprise | Community extension |\n| KeePass | Personal/Team | Community extension |\n| LastPass | Personal | Community extension |\n\n## Security Best Practices\n\n1. **Never store secrets in source control**\n2. **Use environment variables in CI/CD**, load into vault at runtime\n3. **Set appropriate password timeout** for SecretStore\n4. **Use different vaults** for dev/staging/production\n5. **Audit secret access** using vault logging\n6. **Rotate secrets regularly** and update vault\n\n## Script Migration Checklist\n\n- [ ] Identify all hardcoded credentials\n- [ ] Identify all `ConvertTo-SecureString -AsPlainText` usage\n- [ ] Check for connection strings with passwords\n- [ ] Review config files for secrets\n- [ ] Set up appropriate vault type\n- [ ] Store all secrets in vault\n- [ ] Update scripts to use `Get-Secret`\n- [ ] Test scripts with vault-based secrets\n- [ ] Remove hardcoded values from source control\n- [ ] Update documentation"
              },
              {
                "name": "/ps-test",
                "description": "Run Pester tests for PowerShell scripts and modules with coverage",
                "path": "plugins/powershell-master/commands/ps-test.md",
                "frontmatter": {
                  "name": "ps-test",
                  "description": "Run Pester tests for PowerShell scripts and modules with coverage",
                  "argument-hint": "<path to tests or module> [coverage threshold: 80]",
                  "allowed-tools": [
                    "Read",
                    "Bash",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# PowerShell Pester Test Runner\n\nRun Pester tests with code coverage analysis.\n\n## Quick Commands\n\n```powershell\n# Install latest Pester\nInstall-PSResource -Name Pester -Scope CurrentUser\n\n# Run all tests in current directory\nInvoke-Pester\n\n# Run tests with verbose output\nInvoke-Pester -Output Detailed\n\n# Run specific test file\nInvoke-Pester -Path \"./Tests/MyFunction.Tests.ps1\"\n\n# Run tests matching pattern\nInvoke-Pester -Path \"./Tests\" -TagFilter \"Unit\"\n\n# Run with code coverage\nInvoke-Pester -CodeCoverage \"./Public/*.ps1\" -CodeCoverageOutputFile \"coverage.xml\"\n```\n\n## Pester Configuration (Pester 5.x)\n\n```powershell\n$config = New-PesterConfiguration\n$config.Run.Path = \"./Tests\"\n$config.Run.Exit = $true\n$config.Output.Verbosity = \"Detailed\"\n$config.CodeCoverage.Enabled = $true\n$config.CodeCoverage.Path = @(\"./Public/*.ps1\", \"./Private/*.ps1\")\n$config.CodeCoverage.OutputPath = \"coverage.xml\"\n$config.CodeCoverage.OutputFormat = \"JaCoCo\"\n$config.TestResult.Enabled = $true\n$config.TestResult.OutputPath = \"testResults.xml\"\n$config.TestResult.OutputFormat = \"NUnitXml\"\n\nInvoke-Pester -Configuration $config\n```\n\n## Test Structure Best Practices\n\n```powershell\n# MyFunction.Tests.ps1\nBeforeAll {\n    # Import module or dot-source function\n    . $PSScriptRoot/../Public/MyFunction.ps1\n}\n\nDescribe 'MyFunction' {\n    BeforeEach {\n        # Setup before each test\n    }\n\n    AfterEach {\n        # Cleanup after each test\n    }\n\n    Context 'When given valid input' {\n        It 'Should return expected result' {\n            $result = MyFunction -Param \"value\"\n            $result | Should -Be \"expected\"\n        }\n\n        It 'Should not throw' {\n            { MyFunction -Param \"value\" } | Should -Not -Throw\n        }\n    }\n\n    Context 'When given invalid input' {\n        It 'Should throw for null input' {\n            { MyFunction -Param $null } | Should -Throw\n        }\n\n        It 'Should throw specific error' {\n            { MyFunction -Param \"invalid\" } | Should -Throw \"*invalid*\"\n        }\n    }\n\n    Context 'With mocked dependencies' {\n        BeforeAll {\n            Mock Get-Something { return \"mocked\" }\n        }\n\n        It 'Should call Get-Something' {\n            MyFunction -Param \"test\"\n            Should -Invoke Get-Something -Times 1 -Exactly\n        }\n    }\n}\n```\n\n## Common Assertions\n\n```powershell\n# Equality\n$result | Should -Be \"expected\"\n$result | Should -BeExactly \"Expected\"  # Case-sensitive\n$result | Should -Not -Be \"unexpected\"\n\n# Collections\n$array | Should -Contain \"item\"\n$array | Should -HaveCount 3\n$array | Should -BeNullOrEmpty\n\n# Types\n$result | Should -BeOfType [string]\n$result | Should -BeOfType [PSCustomObject]\n\n# Strings\n$result | Should -Match \"pattern\"\n$result | Should -BeLike \"*wildcard*\"\n$result | Should -BeNullOrEmpty\n\n# Numbers\n$result | Should -BeGreaterThan 5\n$result | Should -BeLessThan 10\n$result | Should -BeIn @(1, 2, 3)\n\n# Booleans\n$result | Should -BeTrue\n$result | Should -BeFalse\n\n# Exceptions\n{ ThrowingFunction } | Should -Throw\n{ ThrowingFunction } | Should -Throw \"specific message\"\n{ ThrowingFunction } | Should -Throw -ExceptionType [System.ArgumentException]\n```\n\n## Coverage Thresholds\n\n```powershell\n$config = New-PesterConfiguration\n$config.CodeCoverage.Enabled = $true\n$config.CodeCoverage.CoveragePercentTarget = 80\n\n$result = Invoke-Pester -Configuration $config -PassThru\n\nif ($result.CodeCoverage.CoveragePercent -lt 80) {\n    throw \"Code coverage $($result.CodeCoverage.CoveragePercent)% is below 80% threshold\"\n}\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\n- name: Run Pester Tests\n  shell: pwsh\n  run: |\n    $config = New-PesterConfiguration\n    $config.Run.Path = \"./Tests\"\n    $config.Run.Exit = $true\n    $config.CodeCoverage.Enabled = $true\n    $config.CodeCoverage.OutputPath = \"coverage.xml\"\n    $config.TestResult.Enabled = $true\n    $config.TestResult.OutputPath = \"testResults.xml\"\n    Invoke-Pester -Configuration $config\n```\n\n### Azure DevOps\n\n```yaml\n- task: PowerShell@2\n  inputs:\n    targetType: 'inline'\n    script: |\n      Install-Module Pester -Force -SkipPublisherCheck\n      Invoke-Pester -OutputFile testResults.xml -OutputFormat NUnitXml -CodeCoverage ./src/*.ps1\n    pwsh: true\n\n- task: PublishTestResults@2\n  inputs:\n    testResultsFormat: 'NUnit'\n    testResultsFiles: 'testResults.xml'\n```\n\n## Output Formats\n\n| Format | Use Case | Flag |\n|--------|----------|------|\n| NUnitXml | Azure DevOps, Jenkins | `-OutputFormat NUnitXml` |\n| JUnit | GitHub Actions, GitLab | `-OutputFormat JUnitXml` |\n| JaCoCo | SonarQube coverage | `CodeCoverageOutputFormat = \"JaCoCo\"` |\n| Cobertura | Codecov, Coveralls | `CodeCoverageOutputFormat = \"Cobertura\"` |"
              }
            ],
            "skills": [
              {
                "name": "powershell-2025-changes",
                "description": "Critical PowerShell changes, deprecations, and migrations for 2025",
                "path": "plugins/powershell-master/skills/powershell-2025-changes/SKILL.md",
                "frontmatter": {
                  "name": "powershell-2025-changes",
                  "description": "Critical PowerShell changes, deprecations, and migrations for 2025"
                },
                "content": "# PowerShell 2025 Breaking Changes & Migrations\n\nCritical changes, deprecations, and migration paths for PowerShell in 2025.\n\n## PowerShell 2.0 Removal (August-September 2025)\n\n### What's Removed\n\nPowerShell 2.0 has been **completely removed** from:\n- **Windows 11 version 24H2** (August 2025)\n- **Windows Server 2025** (September 2025)\n\n**Why:** Security improvements, reduced attack surface, legacy code cleanup\n\n### Migration Path\n\n```powershell\n# Check if PowerShell 2.0 is installed\nGet-WindowsOptionalFeature -Online -FeatureName MicrosoftWindowsPowerShellV2Root\n\n# If you still need PowerShell 2.0 (NOT RECOMMENDED)\n# - Use older Windows versions\n# - Use Windows containers with older base images\n# - Upgrade scripts to PowerShell 5.1 or 7+\n\n# Recommended: Migrate to PowerShell 7.5+\nwinget install Microsoft.PowerShell\n```\n\n**Action Required:** Audit all scripts and remove `-Version 2.0` parameters from any PowerShell invocations.\n\n---\n\n## MSOnline & AzureAD Module Retirement\n\n### Retirement Timeline\n\n| Module | Stop Working | Retirement Complete |\n|--------|--------------|---------------------|\n| **MSOnline** | Late May 2025 | May 31, 2025 |\n| **AzureAD** | March 30, 2025 | After July 1, 2025 |\n\n**Critical:** These modules will stop functioning - not just deprecated, but **completely non-functional**.\n\n### Migration Path\n\n**From MSOnline/AzureAD to Microsoft.Graph:**\n\n```powershell\n# OLD (MSOnline) - STOPS WORKING MAY 2025\nConnect-MsolService\nGet-MsolUser\nSet-MsolUser -UserPrincipalName \"user@domain.com\" -UsageLocation \"US\"\n\n# NEW (Microsoft.Graph 2.32.0)\nConnect-MgGraph -Scopes \"User.ReadWrite.All\"\nGet-MgUser\nUpdate-MgUser -UserId \"user@domain.com\" -UsageLocation \"US\"\n\n# OLD (AzureAD) - STOPS WORKING MARCH 2025\nConnect-AzureAD\nGet-AzureADUser\nNew-AzureADUser -DisplayName \"John Doe\" -UserPrincipalName \"john@domain.com\"\n\n# NEW (Microsoft.Graph 2.32.0)\nConnect-MgGraph -Scopes \"User.ReadWrite.All\"\nGet-MgUser\nNew-MgUser -DisplayName \"John Doe\" -UserPrincipalName \"john@domain.com\"\n```\n\n**Alternative:** Use Microsoft Entra PowerShell module (successor to AzureAD)\n\n```powershell\nInstall-Module -Name Microsoft.Graph.Entra -Scope CurrentUser\nConnect-Entra\nGet-EntraUser\n```\n\n### Common Command Mappings\n\n| MSOnline/AzureAD | Microsoft.Graph | Notes |\n|------------------|----------------|-------|\n| `Get-MsolUser` / `Get-AzureADUser` | `Get-MgUser` | Requires User.Read.All scope |\n| `Get-MsolGroup` / `Get-AzureADGroup` | `Get-MgGroup` | Requires Group.Read.All scope |\n| `Get-MsolDevice` / `Get-AzureADDevice` | `Get-MgDevice` | Requires Device.Read.All scope |\n| `Connect-MsolService` / `Connect-AzureAD` | `Connect-MgGraph` | Scope-based permissions |\n\n---\n\n## WMIC Removal (Windows 11 25H2)\n\n### What's Removed\n\n**Windows Management Instrumentation Command-line (WMIC)** tool removed after upgrading to Windows 11 25H2+.\n\n### Migration Path\n\n**From WMIC to PowerShell WMI/CIM:**\n\n```powershell\n# OLD (WMIC) - REMOVED\nwmic process list brief\nwmic os get caption\n\n# NEW (PowerShell CIM)\nGet-CimInstance -ClassName Win32_Process | Select-Object Name, ProcessId, CommandLine\nGet-CimInstance -ClassName Win32_OperatingSystem | Select-Object Caption, Version\n\n# For detailed process info\nGet-Process | Format-Table Name, Id, CPU, WorkingSet -AutoSize\n\n# For system info\nGet-ComputerInfo | Select-Object WindowsProductName, WindowsVersion\n```\n\n---\n\n## PowerShellGet  PSResourceGet Migration\n\n### Modern Package Management (2025)\n\n**PSResourceGet** is the official successor to PowerShellGet (2x faster, actively developed).\n\n```powershell\n# Install PSResourceGet (ships with PowerShell 7.4+)\nInstall-Module -Name Microsoft.PowerShell.PSResourceGet -Force\n\n# New commands (PSResourceGet)\nInstall-PSResource -Name Az -Scope CurrentUser  # Replaces Install-Module\nFind-PSResource -Name \"*Azure*\"                 # Replaces Find-Module\nUpdate-PSResource -Name Az                      # Replaces Update-Module\nGet-InstalledPSResource                         # Replaces Get-InstalledModule\n\n# Compatibility layer available for legacy scripts\n# Your old Install-Module commands still work but call PSResourceGet internally\n```\n\n**Performance Comparison:**\n- **PowerShellGet**: 10-15 seconds to install module\n- **PSResourceGet**: 5-7 seconds to install module (2x faster)\n\n---\n\n## Test-Json Schema Changes\n\n### Breaking Change (PowerShell 7.4+)\n\n**Test-Json** now uses **JsonSchema.NET** instead of **Newtonsoft.Json.Schema**.\n\n**Impact:** No longer supports Draft 4 JSON schemas.\n\n```powershell\n# OLD (Draft 4 schema) - NO LONGER SUPPORTED\n$schema = @\"\n{\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"type\": \"object\"\n}\n\"@\n\nTest-Json -Json $json -Schema $schema  # FAILS in PowerShell 7.4+\n\n# NEW (Draft 6+ schema) - SUPPORTED\n$schema = @\"\n{\n  \"$schema\": \"http://json-schema.org/draft-06/schema#\",\n  \"type\": \"object\"\n}\n\"@\n\nTest-Json -Json $json -Schema $schema  # WORKS\n```\n\n---\n\n## #Requires -PSSnapin Removed\n\n### Breaking Change (PowerShell 7.4+)\n\nAll code related to `#Requires -PSSnapin` has been removed.\n\n```powershell\n# OLD (PowerShell 5.1 and earlier)\n#Requires -PSSnapin Microsoft.Exchange.Management.PowerShell.SnapIn\n\n# NEW (Use modules instead)\n#Requires -Modules ExchangeOnlineManagement\n\nImport-Module ExchangeOnlineManagement\nConnect-ExchangeOnline\n```\n\n---\n\n## Security Hardening (2025 Standards)\n\n### Just Enough Administration (JEA)\n\n**JEA** is now a security requirement for production environments:\n\n```powershell\n# Create JEA session configuration\nNew-PSSessionConfigurationFile -SessionType RestrictedRemoteServer `\n    -Path \"C:\\JEA\\RestrictedAdmin.pssc\" `\n    -VisibleCmdlets @{\n        Name = 'Restart-Service'\n        Parameters = @{ Name = 'Name'; ValidateSet = 'Spooler' }\n    } `\n    -LanguageMode NoLanguage\n\n# Register JEA endpoint\nRegister-PSSessionConfiguration -Name RestrictedAdmin `\n    -Path \"C:\\JEA\\RestrictedAdmin.pssc\" `\n    -Force\n\n# Connect with limited privileges\nEnter-PSSession -ComputerName Server01 -ConfigurationName RestrictedAdmin\n```\n\n### Windows Defender Application Control (WDAC)\n\n**WDAC** replaces AppLocker for PowerShell script control:\n\n```powershell\n# Create WDAC policy for PowerShell scripts\nNew-CIPolicy -FilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -ScanPath \"C:\\Scripts\" `\n    -Level FilePublisher `\n    -Fallback Hash\n\n# Convert to binary and deploy\nConvertFrom-CIPolicy -XmlFilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -BinaryFilePath \"C:\\Windows\\System32\\CodeIntegrity\\SIPolicy.p7b\"\n```\n\n### Constrained Language Mode\n\n**Constrained Language Mode** is now recommended for all users without admin privileges:\n\n```powershell\n# Check current language mode\n$ExecutionContext.SessionState.LanguageMode\n# Output: FullLanguage (admin) or ConstrainedLanguage (standard user)\n\n# Set system-wide constrained language mode via Group Policy or Environment Variable\n# Set HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\\__PSLockdownPolicy = 4\n```\n\n---\n\n## PowerShell 7.6 Preview Features\n\n### Current Status (October 2025)\n\nPowerShell 7.6.0 Preview 5 available (built on .NET 9.0.101)\n\n**New Features:**\n- **PSRedirectToVariable**: Allow redirecting to a variable\n- **Module Rename**: ThreadJob  Microsoft.PowerShell.ThreadJob\n- **PSResourceGet 1.1.0**: Improved performance and Azure Artifacts support\n\n```powershell\n# Check PowerShell version\n$PSVersionTable.PSVersion\n# 7.5.4 (stable) or 7.6.0-preview.5\n\n# .NET version\n[System.Runtime.InteropServices.RuntimeInformation]::FrameworkDescription\n# .NET 9.0.101\n```\n\n---\n\n## Migration Checklist\n\n### Immediate Actions Required (2025)\n\n- [ ] **Audit MSOnline/AzureAD usage** - Migrate to Microsoft.Graph 2.32.0 before May 2025\n- [ ] **Remove PowerShell 2.0 references** - Upgrade to PowerShell 7.5+\n- [ ] **Replace WMIC commands** - Use Get-CimInstance/Get-Process\n- [ ] **Update JSON schemas** - Migrate Draft 4 to Draft 6+\n- [ ] **Remove PSSnapin requirements** - Convert to modules\n- [ ] **Adopt PSResourceGet** - Faster, modern package management\n- [ ] **Implement JEA** - Role-based access control for production\n- [ ] **Enable WDAC** - Application control for PowerShell scripts\n- [ ] **Test Constrained Language Mode** - For non-admin users\n\n### Recommended Actions\n\n- [ ] **Upgrade to PowerShell 7.5.4** - Latest stable with .NET 9\n- [ ] **Adopt Az 14.5.0** - Latest Azure module with zone redundancy\n- [ ] **Use Microsoft.Graph 2.32.0** - Actively maintained Graph SDK\n- [ ] **Enable Script Block Logging** - Security auditing\n- [ ] **Implement Code Signing** - For production scripts\n- [ ] **Use Azure Key Vault** - For credential management\n\n---\n\n## Testing Migration\n\n```powershell\n# Test for deprecated module usage\nGet-Module MSOnline, AzureAD -ListAvailable\n# If found, plan migration immediately\n\n# Test for PowerShell 2.0 dependencies\nGet-Content \"script.ps1\" | Select-String -Pattern \"powershell.exe -Version 2\"\n# If found, remove version parameter\n\n# Test for WMIC usage\nGet-ChildItem -Path \"C:\\Scripts\" -Recurse -Filter \"*.ps1\" |\n    Select-String -Pattern \"wmic\" |\n    Select-Object Path, Line\n\n# Verify PowerShell version compatibility\n#Requires -Version 7.0\nTest-Path $PSCommandPath  # Ensures script is PowerShell 7+\n```\n\n---\n\n## Resources\n\n- [PowerShell 7.5 Release Notes](https://learn.microsoft.com/en-us/powershell/scripting/whats-new/what-s-new-in-powershell-75)\n- [MSOnline/AzureAD Retirement Info](https://techcommunity.microsoft.com/blog/microsoft-entra-blog/action-required-msonline-and-azuread-powershell-retirement---2025-info-and-resou/4364991)\n- [PSResourceGet Documentation](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.psresourceget)\n- [JEA Documentation](https://learn.microsoft.com/en-us/powershell/scripting/security/remoting/jea/overview)\n- [WDAC Documentation](https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control)\n\n---\n\n**Last Updated:** October 2025"
              },
              {
                "name": "powershell-master",
                "description": "Complete PowerShell expertise system across ALL platforms (Windows/Linux/macOS). PROACTIVELY activate for: (1) ANY PowerShell task (scripts/modules/cmdlets), (2) CI/CD automation (GitHub Actions/Azure DevOps/Bitbucket), (3) Cross-platform scripting, (4) Module discovery and management (PSGallery), (5) Azure/AWS/Microsoft 365 automation, (6) Script debugging and optimization, (7) Best practices and security. Provides: PowerShell 7+ features, popular module expertise (Az, Microsoft.Graph, PnP, AWS Tools), PSGallery integration, platform-specific guidance, CI/CD pipeline patterns, cmdlet syntax mastery, and production-ready scripting patterns. Ensures professional-grade, cross-platform PowerShell automation following industry standards.",
                "path": "plugins/powershell-master/skills/powershell-master/SKILL.md",
                "frontmatter": {
                  "name": "powershell-master",
                  "description": "Complete PowerShell expertise system across ALL platforms (Windows/Linux/macOS). PROACTIVELY activate for: (1) ANY PowerShell task (scripts/modules/cmdlets), (2) CI/CD automation (GitHub Actions/Azure DevOps/Bitbucket), (3) Cross-platform scripting, (4) Module discovery and management (PSGallery), (5) Azure/AWS/Microsoft 365 automation, (6) Script debugging and optimization, (7) Best practices and security. Provides: PowerShell 7+ features, popular module expertise (Az, Microsoft.Graph, PnP, AWS Tools), PSGallery integration, platform-specific guidance, CI/CD pipeline patterns, cmdlet syntax mastery, and production-ready scripting patterns. Ensures professional-grade, cross-platform PowerShell automation following industry standards."
                },
                "content": "# PowerShell Master\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nComplete PowerShell expertise across all platforms for scripting, automation, CI/CD, and cloud management.\n\n---\n\n##  When to Activate\n\nPROACTIVELY activate for ANY PowerShell-related task:\n\n-  **PowerShell Scripts** - Creating, reviewing, optimizing any .ps1 file\n-  **Cmdlets & Modules** - Finding, installing, using any PowerShell modules\n-  **Cross-Platform** - Windows, Linux, macOS PowerShell tasks\n-  **CI/CD Integration** - GitHub Actions, Azure DevOps, Bitbucket Pipelines\n-  **Cloud Automation** - Azure (Az), AWS, Microsoft 365 (Microsoft.Graph)\n-  **Module Management** - PSGallery search, installation, updates\n-  **Script Debugging** - Troubleshooting, performance, security\n-  **Best Practices** - Code quality, standards, production-ready scripts\n\n---\n\n##  PowerShell Overview\n\n### PowerShell Versions & Platforms\n\n**PowerShell 7+ (Recommended)**\n- Cross-platform: Windows, Linux, macOS\n- Open source, actively developed\n- Better performance than PowerShell 5.1\n- UTF-8 by default\n- Parallel execution support\n- Ternary operators, null-coalescing\n\n**Windows PowerShell 5.1 (Legacy)**\n- Windows-only\n- Ships with Windows\n- UTF-16LE default encoding\n- Required for some Windows-specific modules\n\n**Installation Locations:**\n- **Windows:** `C:\\Program Files\\PowerShell\\7\\` (PS7) or `C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\` (5.1)\n- **Linux:** `/opt/microsoft/powershell/7/` or `/usr/bin/pwsh`\n- **macOS:** `/usr/local/microsoft/powershell/7/` or `/usr/local/bin/pwsh`\n\n---\n\n##  Cross-Platform Best Practices\n\n### 1. Path Handling\n\n**DO:**\n```powershell\n# Use Join-Path for cross-platform paths\n$configPath = Join-Path -Path $PSScriptRoot -ChildPath \"config.json\"\n\n# Use [System.IO.Path] for path manipulation\n$fullPath = [System.IO.Path]::Combine($home, \"documents\", \"file.txt\")\n\n# Forward slashes work on all platforms in PowerShell 7+\n$path = \"$PSScriptRoot/subfolder/file.txt\"\n```\n\n**DON'T:**\n```powershell\n# Hardcoded backslashes (Windows-only)\n$path = \"C:\\Users\\Name\\file.txt\"\n\n# Assume case-insensitive file systems\nGet-ChildItem \"MyFile.txt\"  # Works on Windows, fails on Linux/macOS if casing is wrong\n```\n\n### 2. Platform Detection\n\n```powershell\n# Use automatic variables\nif ($IsWindows) {\n    # Windows-specific code\n    $env:Path -split ';'\n}\nelseif ($IsLinux) {\n    # Linux-specific code\n    $env:PATH -split ':'\n}\nelseif ($IsMacOS) {\n    # macOS-specific code\n    $env:PATH -split ':'\n}\n\n# Check PowerShell version\nif ($PSVersionTable.PSVersion.Major -ge 7) {\n    # PowerShell 7+ features\n}\n```\n\n### 3. Avoid Aliases in Scripts\n\n```powershell\n# DON'T use aliases (they may differ across platforms)\nls | ? {$_.Length -gt 1MB} | % {$_.Name}\n\n# DO use full cmdlet names\nGet-ChildItem | Where-Object {$_.Length -gt 1MB} | ForEach-Object {$_.Name}\n```\n\n**Why:** On Linux/macOS, aliases might invoke native commands instead of PowerShell cmdlets, causing unexpected results.\n\n### 4. Text Encoding\n\n```powershell\n# PowerShell 7+ uses UTF-8 by default\n\"Hello\" | Out-File -FilePath output.txt\n\n# For PowerShell 5.1 compatibility, specify encoding\n\"Hello\" | Out-File -FilePath output.txt -Encoding UTF8\n\n# Best practice: Always specify encoding for cross-platform scripts\n$content | Set-Content -Path $file -Encoding UTF8NoBOM\n```\n\n### 5. Environment Variables (Cross-Platform)\n\n```powershell\n# BEST PRACTICE: Use .NET Environment class for cross-platform compatibility\n[Environment]::UserName      # Works on all platforms\n[Environment]::MachineName   # Works on all platforms\n[IO.Path]::GetTempPath()     # Works on all platforms\n\n# AVOID: These are platform-specific\n$env:USERNAME                # Windows only\n$env:USER                    # Linux/macOS only\n\n# Environment variable names are CASE-SENSITIVE on Linux/macOS\n$env:PATH    # Correct on Linux/macOS\n$env:Path    # May not work on Linux/macOS\n```\n\n### 6. Shell Detection (Windows: PowerShell vs Git Bash)\n\n**CRITICAL:** On Windows, distinguish between PowerShell and Git Bash/MSYS2 environments:\n\n```powershell\n# PowerShell detection (most reliable)\nif ($env:PSModulePath -and ($env:PSModulePath -split ';').Count -ge 3) {\n    Write-Host \"Running in PowerShell\"\n}\n\n# Platform-specific automatic variables (PowerShell 7+)\nif ($IsWindows) {\n    # Windows-specific code\n}\nelseif ($IsLinux) {\n    # Linux-specific code\n}\nelseif ($IsMacOS) {\n    # macOS-specific code\n}\n```\n\n**Git Bash/MSYS2 Detection:**\n```bash\n# Bash detection - check MSYSTEM environment variable\nif [ -n \"$MSYSTEM\" ]; then\n    echo \"Running in Git Bash/MSYS2: $MSYSTEM\"\n    # MSYSTEM values: MINGW64, MINGW32, MSYS\nfi\n```\n\n**When to Use Each Shell:**\n- **PowerShell:** Windows automation, Azure/M365, PSGallery modules, object pipelines\n- **Git Bash:** Git operations, Unix tools (sed/awk/grep), POSIX scripts, text processing\n\n**Path Handling Differences:**\n- **PowerShell:** `C:\\Users\\John` or `C:/Users/John` (both work in PS 7+)\n- **Git Bash:** `/c/Users/John` (Unix-style, auto-converts to Windows when calling Windows tools)\n\nSee `powershell-shell-detection` skill for comprehensive cross-shell guidance.\n\n### 7. Line Endings\n\n```powershell\n# PowerShell handles line endings automatically\n# But be explicit for git or cross-platform tools\ngit config core.autocrlf input  # Linux/macOS\ngit config core.autocrlf true   # Windows\n```\n\n---\n\n##  Module Management (PSResourceGet & PSGallery)\n\n### PSResourceGet - Modern Package Manager (2025)\n\n**PSResourceGet** is 2x faster than PowerShellGet and actively maintained:\n\n```powershell\n# PSResourceGet ships with PowerShell 7.4+ (or install manually)\nInstall-Module -Name Microsoft.PowerShell.PSResourceGet -Force\n\n# Modern commands (PSResourceGet)\nInstall-PSResource -Name Az -Scope CurrentUser        # 2x faster\nFind-PSResource -Name \"*Azure*\"                       # Faster search\nUpdate-PSResource -Name Az                            # Batch updates\nGet-InstalledPSResource                               # List installed\nUninstall-PSResource -Name OldModule                  # Clean uninstall\n\n# Compatibility: Your old Install-Module commands still work\n# They automatically call PSResourceGet internally\nInstall-Module -Name Az -Scope CurrentUser            # Works, uses PSResourceGet\n```\n\n### Finding Modules\n\n```powershell\n# PSResourceGet (Modern)\nFind-PSResource -Name \"*Azure*\"\nFind-PSResource -Tag \"Security\"\nFind-PSResource -Name Az | Select-Object Name, Version, PublishedDate\n\n# Legacy PowerShellGet (still works)\nFind-Module -Name \"*Azure*\"\nFind-Command -Name Get-AzVM\n```\n\n### Installing Modules\n\n```powershell\n# RECOMMENDED: PSResourceGet (2x faster)\nInstall-PSResource -Name Az -Scope CurrentUser -TrustRepository\nInstall-PSResource -Name Microsoft.Graph -Version 2.32.0\n\n# Legacy: PowerShellGet (slower, but still works)\nInstall-Module -Name Az -Scope CurrentUser -Force\nInstall-Module -Name Pester -Scope AllUsers  # Requires elevation\n```\n\n### Managing Installed Modules\n\n```powershell\n# List installed (PSResourceGet)\nGet-InstalledPSResource\nGet-InstalledPSResource -Name Az\n\n# Update modules (PSResourceGet)\nUpdate-PSResource -Name Az\nUpdate-PSResource                              # Updates all\n\n# Uninstall (PSResourceGet)\nUninstall-PSResource -Name OldModule -AllVersions\n\n# Import module\nImport-Module -Name Az.Accounts\n```\n\n### Offline Installation\n\n```powershell\n# Save module (works with both)\nSave-PSResource -Name Az -Path C:\\OfflineModules\n# Or: Save-Module -Name Az -Path C:\\OfflineModules\n\n# Install from saved location\nInstall-PSResource -Name Az -Path C:\\OfflineModules\n```\n\n---\n\n##  Popular PowerShell Modules\n\n### Azure (Az Module 14.5.0)\n\n**Latest:** Az 14.5.0 (October 2025) with zone redundancy and symbolic links\n\n```powershell\n# Install Azure module 14.5.0\nInstall-PSResource -Name Az -Scope CurrentUser\n# Or: Install-Module -Name Az -Scope CurrentUser -Force\n\n# Connect to Azure\nConnect-AzAccount\n\n# Common operations\nGet-AzVM\nGet-AzResourceGroup\nNew-AzResourceGroup -Name \"MyRG\" -Location \"EastUS\"\n\n# NEW in Az 14.5: Zone redundancy for storage\nNew-AzStorageAccount -ResourceGroupName \"MyRG\" -Name \"storage123\" `\n    -Location \"EastUS\" -SkuName \"Standard_LRS\" -EnableZoneRedundancy\n\n# NEW in Az 14.5: Symbolic links in NFS File Share\nNew-AzStorageFileSymbolicLink -Context $ctx -ShareName \"nfsshare\" `\n    -Path \"symlink\" -Target \"/target/path\"\n```\n\n**Key Submodules:**\n- `Az.Accounts` - Authentication (MFA required Sep 2025+)\n- `Az.Compute` - VMs, scale sets\n- `Az.Storage` - Storage accounts (zone redundancy support)\n- `Az.Network` - Virtual networks, NSGs\n- `Az.KeyVault` - Key Vault operations\n- `Az.Resources` - Resource groups, deployments\n\n### Microsoft Graph (Microsoft.Graph 2.32.0)\n\n**CRITICAL:** MSOnline and AzureAD modules retired (March-May 2025). Use Microsoft.Graph instead.\n\n```powershell\n# Install Microsoft Graph 2.32.0 (October 2025)\nInstall-PSResource -Name Microsoft.Graph -Scope CurrentUser\n# Or: Install-Module -Name Microsoft.Graph -Scope CurrentUser\n\n# Connect with required scopes\nConnect-MgGraph -Scopes \"User.Read.All\", \"Group.ReadWrite.All\"\n\n# Common operations\nGet-MgUser\nGet-MgGroup\nNew-MgUser -DisplayName \"John Doe\" -UserPrincipalName \"john@domain.com\" -MailNickname \"john\"\nGet-MgTeam\n\n# Migration from AzureAD/MSOnline\n# OLD: Connect-AzureAD / Connect-MsolService\n# NEW: Connect-MgGraph\n# OLD: Get-AzureADUser / Get-MsolUser\n# NEW: Get-MgUser\n```\n\n### PnP PowerShell (SharePoint/Teams)\n\n```powershell\n# Install PnP PowerShell\nInstall-Module -Name PnP.PowerShell -Scope CurrentUser\n\n# Connect to SharePoint Online\nConnect-PnPOnline -Url \"https://tenant.sharepoint.com/sites/site\" -Interactive\n\n# Common operations\nGet-PnPList\nGet-PnPFile -Url \"/sites/site/Shared Documents/file.docx\"\nAdd-PnPListItem -List \"Tasks\" -Values @{\"Title\"=\"New Task\"}\n```\n\n### AWS Tools for PowerShell\n\n```powershell\n# Install AWS Tools\nInstall-Module -Name AWS.Tools.Installer -Force\nInstall-AWSToolsModule AWS.Tools.EC2,AWS.Tools.S3\n\n# Configure credentials\nSet-AWSCredential -AccessKey $accessKey -SecretKey $secretKey -StoreAs default\n\n# Common operations\nGet-EC2Instance\nGet-S3Bucket\nNew-S3Bucket -BucketName \"my-bucket\"\n```\n\n### Other Popular Modules\n\n```powershell\n# Pester (Testing framework)\nInstall-Module -Name Pester -Force\n\n# PSScriptAnalyzer (Code analysis)\nInstall-Module -Name PSScriptAnalyzer\n\n# ImportExcel (Excel manipulation without Excel)\nInstall-Module -Name ImportExcel\n\n# PowerShellGet 3.x (Modern package management)\nInstall-Module -Name Microsoft.PowerShell.PSResourceGet\n```\n\n---\n\n##  CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: PowerShell CI\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install PowerShell modules\n        shell: pwsh\n        run: |\n          Install-Module -Name Pester -Force -Scope CurrentUser\n          Install-Module -Name PSScriptAnalyzer -Force -Scope CurrentUser\n\n      - name: Run Pester tests\n        shell: pwsh\n        run: |\n          Invoke-Pester -Path ./tests -OutputFormat NUnitXml -OutputFile TestResults.xml\n\n      - name: Run PSScriptAnalyzer\n        shell: pwsh\n        run: |\n          Invoke-ScriptAnalyzer -Path . -Recurse -ReportSummary\n```\n\n**Multi-Platform Matrix:**\n```yaml\njobs:\n  test:\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v4\n      - name: Test on ${{ matrix.os }}\n        shell: pwsh\n        run: |\n          ./test-script.ps1\n```\n\n### Azure DevOps Pipelines\n\n```yaml\ntrigger:\n  - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n- task: PowerShell@2\n  inputs:\n    targetType: 'inline'\n    script: |\n      Install-Module -Name Pester -Force -Scope CurrentUser\n      Invoke-Pester -Path ./tests -OutputFormat NUnitXml\n  displayName: 'Run Pester Tests'\n\n- task: PowerShell@2\n  inputs:\n    filePath: '$(System.DefaultWorkingDirectory)/build.ps1'\n    arguments: '-Configuration Release'\n  displayName: 'Run Build Script'\n\n- task: PublishTestResults@2\n  inputs:\n    testResultsFormat: 'NUnit'\n    testResultsFiles: '**/TestResults.xml'\n```\n\n**Cross-Platform Pipeline:**\n```yaml\nstrategy:\n  matrix:\n    linux:\n      imageName: 'ubuntu-latest'\n    windows:\n      imageName: 'windows-latest'\n    mac:\n      imageName: 'macos-latest'\n\npool:\n  vmImage: $(imageName)\n\nsteps:\n- pwsh: |\n    Write-Host \"Running on $($PSVersionTable.OS)\"\n    ./test-script.ps1\n  displayName: 'Cross-platform test'\n```\n\n### Bitbucket Pipelines\n\n```yaml\nimage: mcr.microsoft.com/powershell:latest\n\npipelines:\n  default:\n    - step:\n        name: Test with PowerShell\n        script:\n          - pwsh -Command \"Install-Module -Name Pester -Force\"\n          - pwsh -Command \"Invoke-Pester -Path ./tests\"\n\n    - step:\n        name: Deploy\n        deployment: production\n        script:\n          - pwsh -File ./deploy.ps1\n```\n\n---\n\n##  PowerShell Syntax & Cmdlets\n\n### Cmdlet Structure\n\n```powershell\n# Verb-Noun pattern\nGet-ChildItem\nSet-Location\nNew-Item\nRemove-Item\n\n# Common parameters (available on all cmdlets)\nGet-Process -Verbose\nSet-Content -Path file.txt -WhatIf\nRemove-Item -Path folder -Confirm\nInvoke-RestMethod -Uri $url -ErrorAction Stop\n```\n\n### Variables & Data Types\n\n```powershell\n# Variables (loosely typed)\n$string = \"Hello World\"\n$number = 42\n$array = @(1, 2, 3, 4, 5)\n$hashtable = @{Name=\"John\"; Age=30}\n\n# Strongly typed\n[string]$name = \"John\"\n[int]$age = 30\n[datetime]$date = Get-Date\n\n# Special variables\n$PSScriptRoot  # Directory containing the script\n$PSCommandPath  # Full path to the script\n$args  # Script arguments\n$_  # Current pipeline object\n```\n\n### Operators\n\n```powershell\n# Comparison operators\n-eq  # Equal\n-ne  # Not equal\n-gt  # Greater than\n-lt  # Less than\n-match  # Regex match\n-like  # Wildcard match\n-contains  # Array contains\n\n# Logical operators\n-and\n-or\n-not\n\n# PowerShell 7+ ternary operator\n$result = $condition ? \"true\" : \"false\"\n\n# Null-coalescing (PS 7+)\n$value = $null ?? \"default\"\n```\n\n### Control Flow\n\n```powershell\n# If-ElseIf-Else\nif ($condition) {\n    # Code\n} elseif ($otherCondition) {\n    # Code\n} else {\n    # Code\n}\n\n# Switch\nswitch ($value) {\n    1 { \"One\" }\n    2 { \"Two\" }\n    {$_ -gt 10} { \"Greater than 10\" }\n    default { \"Other\" }\n}\n\n# Loops\nforeach ($item in $collection) {\n    # Process item\n}\n\nfor ($i = 0; $i -lt 10; $i++) {\n    # Loop code\n}\n\nwhile ($condition) {\n    # Loop code\n}\n\ndo {\n    # Loop code\n} while ($condition)\n```\n\n### Functions\n\n```powershell\nfunction Get-Something {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory=$true)]\n        [string]$Name,\n\n        [Parameter()]\n        [int]$Count = 1,\n\n        [Parameter(ValueFromPipeline=$true)]\n        [string[]]$InputObject\n    )\n\n    begin {\n        # Initialization\n    }\n\n    process {\n        # Process each pipeline object\n        foreach ($item in $InputObject) {\n            # Work with $item\n        }\n    }\n\n    end {\n        # Cleanup\n        return $result\n    }\n}\n```\n\n### Pipeline & Filtering\n\n```powershell\n# Pipeline basics\nGet-Process | Where-Object {$_.CPU -gt 100} | Select-Object Name, CPU\n\n# Simplified syntax (PS 3.0+)\nGet-Process | Where CPU -gt 100 | Select Name, CPU\n\n# ForEach-Object\nGet-ChildItem | ForEach-Object {\n    Write-Host $_.Name\n}\n\n# Simplified (PS 4.0+)\nGet-ChildItem | % Name\n\n# Group, Sort, Measure\nGet-Process | Group-Object ProcessName\nGet-Service | Sort-Object Status\nGet-ChildItem | Measure-Object -Property Length -Sum\n```\n\n### Error Handling\n\n```powershell\n# Try-Catch-Finally\ntry {\n    Get-Content -Path \"nonexistent.txt\" -ErrorAction Stop\n}\ncatch [System.IO.FileNotFoundException] {\n    Write-Error \"File not found\"\n}\ncatch {\n    Write-Error \"An error occurred: $_\"\n}\nfinally {\n    # Cleanup code\n}\n\n# Error action preference\n$ErrorActionPreference = \"Stop\"  # Treat all errors as terminating\n$ErrorActionPreference = \"Continue\"  # Default\n$ErrorActionPreference = \"SilentlyContinue\"  # Suppress errors\n```\n\n---\n\n##  Security Best Practices (2025 Standards)\n\n### Modern Security Framework (JEA + WDAC + Logging)\n\n**2025 Security Requirements:**\n1. **JEA** - Just Enough Administration for role-based access\n2. **WDAC** - Windows Defender Application Control for script approval\n3. **Constrained Language Mode** - For non-admin users\n4. **Script Block Logging** - For audit trails\n\n### Just Enough Administration (JEA)\n\n**Required for production environments in 2025:**\n\n```powershell\n# Create JEA session configuration file\nNew-PSSessionConfigurationFile -SessionType RestrictedRemoteServer `\n    -Path \"C:\\JEA\\HelpDesk.pssc\" `\n    -VisibleCmdlets @{\n        Name = 'Restart-Service'\n        Parameters = @{ Name = 'Name'; ValidateSet = 'Spooler', 'Wuauserv' }\n    }, @{\n        Name = 'Get-Service'\n    } `\n    -LanguageMode NoLanguage `\n    -ExecutionPolicy RemoteSigned\n\n# Register JEA endpoint\nRegister-PSSessionConfiguration -Name HelpDesk `\n    -Path \"C:\\JEA\\HelpDesk.pssc\" `\n    -Force\n\n# Connect with limited privileges\nEnter-PSSession -ComputerName Server01 -ConfigurationName HelpDesk\n```\n\n### Windows Defender Application Control (WDAC)\n\n**Replaces AppLocker for PowerShell script control:**\n\n```powershell\n# Create WDAC policy for approved scripts\nNew-CIPolicy -FilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -ScanPath \"C:\\ApprovedScripts\" `\n    -Level FilePublisher `\n    -Fallback Hash\n\n# Convert to binary\nConvertFrom-CIPolicy -XmlFilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -BinaryFilePath \"C:\\Windows\\System32\\CodeIntegrity\\SIPolicy.p7b\"\n\n# Deploy via Group Policy or MDM\n```\n\n### Constrained Language Mode\n\n**Recommended for all non-admin users:**\n\n```powershell\n# Check current language mode\n$ExecutionContext.SessionState.LanguageMode\n# Output: FullLanguage (admin) or ConstrainedLanguage (standard user)\n\n# Enable system-wide via environment variable\n[Environment]::SetEnvironmentVariable(\n    \"__PSLockdownPolicy\",\n    \"4\",\n    [System.EnvironmentVariableTarget]::Machine\n)\n```\n\n### Script Block Logging\n\n**Enable for security auditing:**\n\n```powershell\n# Enable via Group Policy or Registry\n# HKLM:\\SOFTWARE\\Policies\\Microsoft\\Windows\\PowerShell\\ScriptBlockLogging\n# EnableScriptBlockLogging = 1\n# EnableScriptBlockInvocationLogging = 1\n\n# Check logs\nGet-WinEvent -LogName \"Microsoft-Windows-PowerShell/Operational\" |\n    Where-Object Id -eq 4104 |  # Script Block Logging\n    Select-Object TimeCreated, Message -First 10\n```\n\n### Execution Policy\n\n```powershell\n# Check current execution policy\nGet-ExecutionPolicy\n\n# Set for current user (no admin needed)\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\n# Bypass for single session (use sparingly)\npwsh -ExecutionPolicy Bypass -File script.ps1\n```\n\n### Credential Management\n\n```powershell\n# NEVER hardcode credentials\n# BAD: $password = \"MyP@ssw0rd\"\n\n# Use SecretManagement module (modern approach)\nInstall-PSResource -Name Microsoft.PowerShell.SecretManagement\nInstall-PSResource -Name SecretManagement.KeyVault\n\nRegister-SecretVault -Name AzureKeyVault -ModuleName SecretManagement.KeyVault\n$secret = Get-Secret -Name \"DatabasePassword\" -Vault AzureKeyVault\n\n# Legacy: Get-Credential for interactive\n$cred = Get-Credential\n\n# Azure Key Vault for production\n$vaultName = \"MyKeyVault\"\n$secret = Get-AzKeyVaultSecret -VaultName $vaultName -Name \"DatabasePassword\"\n$secret.SecretValue\n```\n\n### Input Validation\n\n```powershell\nfunction Do-Something {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory=$true)]\n        [ValidateNotNullOrEmpty()]\n        [string]$Name,\n\n        [Parameter()]\n        [ValidateRange(1, 100)]\n        [int]$Count,\n\n        [Parameter()]\n        [ValidateSet(\"Option1\", \"Option2\", \"Option3\")]\n        [string]$Option,\n\n        [Parameter()]\n        [ValidatePattern('^\\d{3}-\\d{3}-\\d{4}$')]\n        [string]$PhoneNumber\n    )\n}\n```\n\n### Code Signing (Production)\n\n```powershell\n# Get code signing certificate\n$cert = Get-ChildItem Cert:\\CurrentUser\\My -CodeSigningCert\n\n# Sign script\nSet-AuthenticodeSignature -FilePath script.ps1 -Certificate $cert\n```\n\n---\n\n##  Performance Optimization\n\n### PowerShell 7+ Features\n\n```powershell\n# Parallel ForEach (PS 7+)\n1..10 | ForEach-Object -Parallel {\n    Start-Sleep -Seconds 1\n    \"Processed $_\"\n} -ThrottleLimit 5\n\n# Ternary operator\n$result = $value ? \"true\" : \"false\"\n\n# Null-coalescing\n$name = $userName ?? \"default\"\n\n# Null-conditional member access\n$length = $string?.Length\n```\n\n### Efficient Filtering\n\n```powershell\n# Use .NET methods for performance\n# Instead of: Get-Content large.txt | Where-Object {$_ -match \"pattern\"}\n[System.IO.File]::ReadLines(\"large.txt\") | Where-Object {$_ -match \"pattern\"}\n\n# Use -Filter parameter when available\nGet-ChildItem -Path C:\\ -Filter *.log -Recurse\n# Instead of: Get-ChildItem -Path C:\\ -Recurse | Where-Object {$_.Extension -eq \".log\"}\n```\n\n### ArrayList vs Array\n\n```powershell\n# Arrays are immutable - slow for additions\n$array = @()\n1..1000 | ForEach-Object { $array += $_ }  # SLOW\n\n# Use ArrayList for dynamic collections\n$list = [System.Collections.ArrayList]::new()\n1..1000 | ForEach-Object { [void]$list.Add($_) }  # FAST\n\n# Or use generic List\n$list = [System.Collections.Generic.List[int]]::new()\n1..1000 | ForEach-Object { $list.Add($_) }\n```\n\n---\n\n##  Testing with Pester\n\n```powershell\n# Install Pester\nInstall-Module -Name Pester -Force\n\n# Basic test structure\nDescribe \"Get-Something Tests\" {\n    Context \"When input is valid\" {\n        It \"Should return expected value\" {\n            $result = Get-Something -Name \"Test\"\n            $result | Should -Be \"Expected\"\n        }\n    }\n\n    Context \"When input is invalid\" {\n        It \"Should throw an error\" {\n            { Get-Something -Name $null } | Should -Throw\n        }\n    }\n}\n\n# Run tests\nInvoke-Pester -Path ./tests\nInvoke-Pester -Path ./tests -OutputFormat NUnitXml -OutputFile TestResults.xml\n\n# Code coverage\nInvoke-Pester -Path ./tests -CodeCoverage ./src/*.ps1\n```\n\n---\n\n##  Script Requirements & Versioning\n\n```powershell\n# Require specific PowerShell version\n#Requires -Version 7.0\n\n# Require modules\n#Requires -Modules Az.Accounts, Az.Compute\n\n# Require admin/elevated privileges (Windows)\n#Requires -RunAsAdministrator\n\n# Combine multiple requirements\n#Requires -Version 7.0\n#Requires -Modules @{ModuleName='Pester'; ModuleVersion='5.0.0'}\n\n# Use strict mode\nSet-StrictMode -Version Latest\n```\n\n---\n\n##  Common Cmdlets Reference\n\n### File System\n```powershell\nGet-ChildItem (gci, ls, dir)\nSet-Location (cd, sl)\nNew-Item (ni)\nRemove-Item (rm, del)\nCopy-Item (cp, copy)\nMove-Item (mv, move)\nRename-Item (rn, ren)\nGet-Content (gc, cat, type)\nSet-Content (sc)\nAdd-Content (ac)\n```\n\n### Process Management\n```powershell\nGet-Process (ps, gps)\nStop-Process (kill, spps)\nStart-Process (start, saps)\nWait-Process\n```\n\n### Service Management\n```powershell\nGet-Service (gsv)\nStart-Service (sasv)\nStop-Service (spsv)\nRestart-Service (srsv)\nSet-Service\n```\n\n### Network\n```powershell\nTest-Connection (ping)\nTest-NetConnection\nInvoke-WebRequest (curl, wget, iwr)\nInvoke-RestMethod (irm)\n```\n\n### Object Manipulation\n```powershell\nSelect-Object (select)\nWhere-Object (where, ?)\nForEach-Object (foreach, %)\nSort-Object (sort)\nGroup-Object (group)\nMeasure-Object (measure)\nCompare-Object (compare, diff)\n```\n\n---\n\n##  REST API & Web Requests\n\n```powershell\n# GET request\n$response = Invoke-RestMethod -Uri \"https://api.example.com/data\" -Method Get\n\n# POST with JSON body\n$body = @{\n    name = \"John\"\n    age = 30\n} | ConvertTo-Json\n\n$response = Invoke-RestMethod -Uri \"https://api.example.com/users\" `\n    -Method Post -Body $body -ContentType \"application/json\"\n\n# With headers and authentication\n$headers = @{\n    \"Authorization\" = \"Bearer $token\"\n    \"Accept\" = \"application/json\"\n}\n\n$response = Invoke-RestMethod -Uri $url -Headers $headers\n\n# Download file\nInvoke-WebRequest -Uri $url -OutFile \"file.zip\"\n```\n\n---\n\n##  Script Structure Best Practices\n\n```powershell\n<#\n.SYNOPSIS\n    Brief description\n\n.DESCRIPTION\n    Detailed description\n\n.PARAMETER Name\n    Parameter description\n\n.EXAMPLE\n    PS> .\\script.ps1 -Name \"John\"\n    Example usage\n\n.NOTES\n    Author: Your Name\n    Version: 1.0.0\n    Date: 2025-01-01\n#>\n\n[CmdletBinding()]\nparam(\n    [Parameter(Mandatory=$true)]\n    [string]$Name\n)\n\n# Script-level error handling\n$ErrorActionPreference = \"Stop\"\n\n# Use strict mode\nSet-StrictMode -Version Latest\n\ntry {\n    # Main script logic\n    Write-Verbose \"Starting script\"\n\n    # ... script code ...\n\n    Write-Verbose \"Script completed successfully\"\n}\ncatch {\n    Write-Error \"Script failed: $_\"\n    exit 1\n}\nfinally {\n    # Cleanup\n}\n```\n\n---\n\n##  Additional Resources\n\n### Official Documentation\n- PowerShell Docs: https://learn.microsoft.com/powershell\n- PowerShell Gallery: https://www.powershellgallery.com\n- Az Module Docs: https://learn.microsoft.com/powershell/azure\n- Microsoft Graph Docs: https://learn.microsoft.com/graph/powershell\n\n### Module Discovery\n```powershell\n# Find modules by keyword\nFind-Module -Tag \"Azure\"\nFind-Module -Tag \"Security\"\n\n# Explore commands in a module\nGet-Command -Module Az.Compute\nGet-Command -Verb Get -Noun *VM*\n\n# Get command help\nGet-Help Get-AzVM -Full\nGet-Help Get-AzVM -Examples\nGet-Help Get-AzVM -Online\n```\n\n### Update Help System\n```powershell\n# Update help files (requires internet)\nUpdate-Help -Force -ErrorAction SilentlyContinue\n\n# Update help for specific modules\nUpdate-Help -Module Az -Force\n```\n\n---\n\n##  Quick Decision Guide\n\n**Use PowerShell 7+ when:**\n- Cross-platform compatibility needed\n- New projects or scripts\n- Performance is important\n- Modern language features desired\n\n**Use Windows PowerShell 5.1 when:**\n- Windows-specific modules required (WSUS, GroupPolicy legacy)\n- Corporate environments with strict version requirements\n- Legacy script compatibility needed\n\n**Choose Azure CLI when:**\n- Simple one-liners needed\n- JSON output preferred\n- Bash scripting integration\n\n**Choose PowerShell Az module when:**\n- Complex automation required\n- Object manipulation needed\n- PowerShell scripting expertise available\n- Reusable scripts and modules needed\n\n---\n\n##  Pre-Flight Checklist for Scripts\n\nBefore running any PowerShell script, ensure:\n\n1.  **Platform Detection** - Use `$IsWindows`, `$IsLinux`, `$IsMacOS`\n2.  **Version Check** - `#Requires -Version 7.0` if needed\n3.  **Module Requirements** - `#Requires -Modules` specified\n4.  **Error Handling** - `try/catch` blocks in place\n5.  **Input Validation** - Parameter validation attributes used\n6.  **No Aliases** - Full cmdlet names in scripts\n7.  **Path Handling** - Use `Join-Path` or `[IO.Path]::Combine()`\n8.  **Encoding Specified** - UTF-8 for cross-platform\n9.  **Credentials Secure** - Never hardcoded\n10.  **Verbose Logging** - `Write-Verbose` for debugging\n\n---\n\n##  Common Pitfalls & Solutions\n\n### Pitfall: Out-GridView Search Broken in 7.5\n```powershell\n# Known Issue: Out-GridView search doesn't work in PowerShell 7.5 due to .NET 9 changes\n# Workaround: Use Where-Object or Select-Object for filtering\nGet-Process | Where-Object CPU -gt 100 | Format-Table\n\n# Or export to CSV and use external tools\nGet-Process | Export-Csv processes.csv -NoTypeInformation\n```\n\n### Pitfall: Case Sensitivity\n```powershell\n# Linux/macOS are case-sensitive\n# This fails on Linux if file is \"File.txt\"\nGet-Content \"file.txt\"\n\n# Solution: Use exact casing or Test-Path first\nif (Test-Path \"file.txt\") {\n    Get-Content \"file.txt\"\n}\n```\n\n### Pitfall: Execution Policy\n```powershell\n# Solution: Set for current user\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\n# Or bypass for session\npowershell.exe -ExecutionPolicy Bypass -File script.ps1\n```\n\n### Pitfall: Module Import Failures\n```powershell\n# Solution: Check module availability and install\nif (-not (Get-Module -ListAvailable -Name Az)) {\n    Install-Module -Name Az -Force -Scope CurrentUser\n}\nImport-Module -Name Az\n```\n\n### Pitfall: Array Concatenation Performance\n```powershell\n# Bad: $array += $item (recreates array each time)\n\n# Good: Use ArrayList or List\n$list = [System.Collections.Generic.List[object]]::new()\n$list.Add($item)\n```\n\n---\n\nRemember: ALWAYS research latest PowerShell documentation and module versions before implementing solutions. The PowerShell ecosystem evolves rapidly, and best practices are updated frequently."
              },
              {
                "name": "powershell-security",
                "description": "Modern PowerShell security practices including SecretManagement, JEA, WDAC, and credential protection",
                "path": "plugins/powershell-master/skills/powershell-security/SKILL.md",
                "frontmatter": {
                  "name": "powershell-security",
                  "description": "Modern PowerShell security practices including SecretManagement, JEA, WDAC, and credential protection"
                },
                "content": "# PowerShell Security Best Practices (2025)\n\nModern security practices for PowerShell scripts and automation, including credential management, SecretManagement module, and hardening techniques.\n\n## SecretManagement Module (Recommended 2025 Standard)\n\n### Overview\n\n**Microsoft.PowerShell.SecretManagement** is the official solution for secure credential storage in PowerShell.\n\n**Why use SecretManagement:**\n- Never store plaintext credentials in scripts\n- Cross-platform secret storage\n- Multiple vault provider support\n- Integration with Azure Key Vault, 1Password, KeePass, etc.\n\n### Installation\n\n```powershell\n# Install SecretManagement module\nInstall-Module -Name Microsoft.PowerShell.SecretManagement -Scope CurrentUser\n\n# Install vault provider (choose one or more)\nInstall-Module -Name Microsoft.PowerShell.SecretStore  # Local encrypted vault\nInstall-Module -Name Az.KeyVault                        # Azure Key Vault\nInstall-Module -Name SecretManagement.KeePass          # KeePass integration\n```\n\n### Basic Usage\n\n```powershell\n# Register a vault\nRegister-SecretVault -Name LocalVault -ModuleName Microsoft.PowerShell.SecretStore\n\n# Store a secret\n$password = Read-Host -AsSecureString -Prompt \"Enter password\"\nSet-Secret -Name \"DatabasePassword\" -Secret $password -Vault LocalVault\n\n# Retrieve a secret\n$dbPassword = Get-Secret -Name \"DatabasePassword\" -Vault LocalVault -AsPlainText\n# Or as SecureString\n$dbPasswordSecure = Get-Secret -Name \"DatabasePassword\" -Vault LocalVault\n\n# List secrets\nGet-SecretInfo\n\n# Remove a secret\nRemove-Secret -Name \"DatabasePassword\" -Vault LocalVault\n```\n\n### Azure Key Vault Integration\n\n```powershell\n# Install and import Az.KeyVault\nInstall-Module -Name Az.KeyVault -Scope CurrentUser\nImport-Module Az.KeyVault\n\n# Authenticate to Azure\nConnect-AzAccount\n\n# Register Azure Key Vault as secret vault\nRegister-SecretVault -Name AzureKV `\n    -ModuleName Az.KeyVault `\n    -VaultParameters @{\n        AZKVaultName = 'MyKeyVault'\n        SubscriptionId = 'your-subscription-id'\n    }\n\n# Store secret in Azure Key Vault\nSet-Secret -Name \"ApiKey\" -Secret \"your-api-key\" -Vault AzureKV\n\n# Retrieve from Azure Key Vault\n$apiKey = Get-Secret -Name \"ApiKey\" -Vault AzureKV -AsPlainText\n```\n\n### Automation Scripts with SecretManagement\n\n```powershell\n<#\n.SYNOPSIS\n    Secure automation script using SecretManagement\n\n.DESCRIPTION\n    Demonstrates secure credential handling without hardcoded secrets\n#>\n\n#Requires -Modules Microsoft.PowerShell.SecretManagement\n\n[CmdletBinding()]\nparam()\n\n# Retrieve credentials from vault\n$dbConnectionString = Get-Secret -Name \"SQLConnectionString\" -AsPlainText\n$apiToken = Get-Secret -Name \"APIToken\" -AsPlainText\n\n# Use credentials securely\ntry {\n    # Database operation\n    $connection = New-Object System.Data.SqlClient.SqlConnection($dbConnectionString)\n    $connection.Open()\n\n    # API call with token\n    $headers = @{ Authorization = \"Bearer $apiToken\" }\n    $response = Invoke-RestMethod -Uri \"https://api.example.com/data\" -Headers $headers\n\n    # Process results\n    Write-Host \"Operation completed successfully\"\n}\ncatch {\n    Write-Error \"Operation failed: $_\"\n}\nfinally {\n    if ($connection) { $connection.Close() }\n}\n```\n\n## Credential Management Best Practices\n\n### Never Hardcode Credentials\n\n```powershell\n#  WRONG - Hardcoded credentials\n$password = \"MyPassword123\"\n$username = \"admin\"\n\n#  WRONG - Plaintext in script\n$cred = New-Object System.Management.Automation.PSCredential(\"admin\", \"password\")\n\n#  CORRECT - SecretManagement\n$password = Get-Secret -Name \"AdminPassword\" -AsPlainText\n$securePassword = ConvertTo-SecureString $password -AsPlainText -Force\n$cred = New-Object System.Management.Automation.PSCredential(\"admin\", $securePassword)\n\n#  CORRECT - Interactive prompt (for manual runs)\n$cred = Get-Credential -Message \"Enter admin credentials\"\n\n#  CORRECT - Managed Identity (Azure automation)\nConnect-AzAccount -Identity\n```\n\n### Service Principal Authentication (Azure)\n\n```powershell\n# Store service principal credentials in vault\nSet-Secret -Name \"AzureAppId\" -Secret \"app-id-guid\"\nSet-Secret -Name \"AzureAppSecret\" -Secret \"app-secret-value\"\nSet-Secret -Name \"AzureTenantId\" -Secret \"tenant-id-guid\"\n\n# Retrieve and authenticate\n$appId = Get-Secret -Name \"AzureAppId\" -AsPlainText\n$appSecret = Get-Secret -Name \"AzureAppSecret\" -AsPlainText\n$tenantId = Get-Secret -Name \"AzureTenantId\" -AsPlainText\n\n$secureSecret = ConvertTo-SecureString $appSecret -AsPlainText -Force\n$credential = New-Object System.Management.Automation.PSCredential($appId, $secureSecret)\n\nConnect-AzAccount -ServicePrincipal -Credential $credential -Tenant $tenantId\n```\n\n## Just Enough Administration (JEA)\n\n### What is JEA?\n\n**Just Enough Administration** restricts PowerShell remoting sessions to specific cmdlets and parameters.\n\n### Use Cases\n\n- Delegate admin tasks without full admin rights\n- Compliance requirements (SOC 2, HIPAA, PCI-DSS)\n- Production environment hardening\n- Audit trail for privileged operations\n\n### Creating a JEA Endpoint\n\n```powershell\n# 1. Create role capability file\nNew-PSRoleCapabilityFile -Path \"C:\\JEA\\RestartServices.psrc\" `\n    -VisibleCmdlets @{\n        Name = 'Restart-Service'\n        Parameters = @{\n            Name = 'Name'\n            ValidateSet = 'Spooler', 'W32Time', 'WinRM'\n        }\n    }, 'Get-Service'\n\n# 2. Create session configuration file\nNew-PSSessionConfigurationFile -Path \"C:\\JEA\\RestartServices.pssc\" `\n    -SessionType RestrictedRemoteServer `\n    -RoleDefinitions @{\n        'DOMAIN\\ServiceAdmins' = @{ RoleCapabilities = 'RestartServices' }\n    } `\n    -LanguageMode NoLanguage\n\n# 3. Register JEA endpoint\nRegister-PSSessionConfiguration -Name RestartServices `\n    -Path \"C:\\JEA\\RestartServices.pssc\" `\n    -Force\n\n# 4. Connect to JEA endpoint (as delegated user)\nEnter-PSSession -ComputerName Server01 -ConfigurationName RestartServices\n\n# User can ONLY run allowed commands\nRestart-Service -Name Spooler  #  Allowed\nRestart-Service -Name DNS      #  Denied (not in ValidateSet)\nGet-Process                    #  Denied (not visible)\n```\n\n### JEA Audit Logging\n\n```powershell\n# Enable transcription and logging\nNew-PSSessionConfigurationFile -Path \"C:\\JEA\\AuditedSession.pssc\" `\n    -SessionType RestrictedRemoteServer `\n    -TranscriptDirectory \"C:\\JEA\\Transcripts\" `\n    -RunAsVirtualAccount\n\n# All JEA sessions are transcribed to C:\\JEA\\Transcripts\n# Review audit logs\nGet-ChildItem \"C:\\JEA\\Transcripts\" | Get-Content\n```\n\n## Windows Defender Application Control (WDAC)\n\n### PowerShell Script Control\n\n**WDAC** replaces AppLocker for controlling which PowerShell scripts can execute.\n\n```powershell\n# Create WDAC policy for signed scripts only\nNew-CIPolicy -FilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -ScanPath \"C:\\Scripts\" `\n    -Level FilePublisher `\n    -Fallback Hash `\n    -UserPEs\n\n# Allow only signed scripts\nSet-RuleOption -FilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -Option 3 # Required WHQL\n\n# Convert to binary policy\nConvertFrom-CIPolicy -XmlFilePath \"C:\\WDAC\\PowerShellPolicy.xml\" `\n    -BinaryFilePath \"C:\\Windows\\System32\\CodeIntegrity\\SIPolicy.p7b\"\n\n# Reboot to apply policy\nRestart-Computer\n```\n\n## Code Signing\n\n### Why Sign Scripts?\n\n- Verify script integrity\n- Meet organizational security policies\n- Enable WDAC enforcement\n- Prevent tampering\n\n### Signing a Script\n\n```powershell\n# Get code signing certificate\n$cert = Get-ChildItem Cert:\\CurrentUser\\My -CodeSigningCert\n\n# Sign script\nSet-AuthenticodeSignature -FilePath \"C:\\Scripts\\MyScript.ps1\" -Certificate $cert\n\n# Verify signature\n$signature = Get-AuthenticodeSignature -FilePath \"C:\\Scripts\\MyScript.ps1\"\n$signature.Status  # Should be \"Valid\"\n```\n\n### Execution Policy\n\n```powershell\n# Check current execution policy\nGet-ExecutionPolicy\n\n# Set execution policy (requires admin)\nSet-ExecutionPolicy RemoteSigned -Scope LocalMachine\n\n# Bypass for single script (testing only)\nPowerShell.exe -ExecutionPolicy Bypass -File \"script.ps1\"\n```\n\n## Constrained Language Mode\n\n### What is Constrained Language Mode?\n\nRestricts PowerShell language features to prevent malicious code execution.\n\n```powershell\n# Check current language mode\n$ExecutionContext.SessionState.LanguageMode\n# Output: FullLanguage (admin) or ConstrainedLanguage (standard user)\n\n# Set system-wide constrained language mode\n# Via Environment Variable or Group Policy\n# Set: __PSLockdownPolicy = 4\n\n# Test constrained mode behavior\n# FullLanguage allows:\n[System.Net.WebClient]::new()  #  Allowed\n\n# ConstrainedLanguage blocks:\n[System.Net.WebClient]::new()  #  Blocked\nAdd-Type -TypeDefinition \"...\"  #  Blocked\n```\n\n## Script Block Logging\n\n### Enable Logging\n\n```powershell\n# Enable via Group Policy or Registry\n# HKLM:\\Software\\Policies\\Microsoft\\Windows\\PowerShell\\ScriptBlockLogging\nNew-ItemProperty -Path \"HKLM:\\Software\\Policies\\Microsoft\\Windows\\PowerShell\\ScriptBlockLogging\" `\n    -Name \"EnableScriptBlockLogging\" -Value 1 -PropertyType DWord\n\n# Log location: Windows Event Log\n# Event Viewer > Applications and Services Logs > Microsoft > Windows > PowerShell > Operational\n```\n\n### Review Logs\n\n```powershell\n# Query script block logs\nGet-WinEvent -LogName \"Microsoft-Windows-PowerShell/Operational\" |\n    Where-Object { $_.Id -eq 4104 } |  # Script Block Logging event\n    Select-Object TimeCreated, Message |\n    Out-GridView\n```\n\n## Input Validation\n\n### Prevent Injection Attacks\n\n```powershell\n#  WRONG - No validation\nfunction Get-UserData {\n    param($Username)\n    Invoke-Sqlcmd -Query \"SELECT * FROM Users WHERE Username = '$Username'\"\n}\n# Vulnerable to SQL injection\n\n#  CORRECT - Parameterized queries\nfunction Get-UserData {\n    param(\n        [ValidatePattern('^[a-zA-Z0-9_-]+$')]\n        [string]$Username\n    )\n    Invoke-Sqlcmd -Query \"SELECT * FROM Users WHERE Username = @Username\" `\n        -Variable @{Username=$Username}\n}\n\n#  CORRECT - ValidateSet for known values\nfunction Restart-AppService {\n    param(\n        [ValidateSet('Web', 'API', 'Worker')]\n        [string]$ServiceName\n    )\n    Restart-Service -Name \"App${ServiceName}Service\"\n}\n```\n\n## Security Checklist\n\n### Script Development\n\n- [ ] Never hardcode credentials (use SecretManagement)\n- [ ] Use parameterized queries for SQL operations\n- [ ] Validate all user input with `[ValidatePattern]`, `[ValidateSet]`, etc.\n- [ ] Enable `Set-StrictMode -Version Latest`\n- [ ] Use `try/catch` for error handling\n- [ ] Avoid `Invoke-Expression` with user input\n- [ ] Sign production scripts\n- [ ] Enable Script Block Logging\n\n### Automation\n\n- [ ] Use Managed Identity or Service Principal (never passwords)\n- [ ] Store secrets in SecretManagement or Azure Key Vault\n- [ ] Implement JEA for delegated admin tasks\n- [ ] Enable audit logging for all privileged operations\n- [ ] Use least privilege principle\n- [ ] Rotate credentials regularly\n- [ ] Monitor failed authentication attempts\n\n### Production Environments\n\n- [ ] Implement WDAC policies for script control\n- [ ] Use Constrained Language Mode for non-admin users\n- [ ] Enable PowerShell logging (Script Block + Transcription)\n- [ ] Require signed scripts (via execution policy)\n- [ ] Regular security audits\n- [ ] Keep PowerShell updated (7.5+)\n- [ ] Use JEA for remote administration\n\n## Resources\n\n- [SecretManagement Documentation](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.secretmanagement)\n- [JEA Documentation](https://learn.microsoft.com/en-us/powershell/scripting/security/remoting/jea/overview)\n- [WDAC Documentation](https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control)\n- [PowerShell Security Best Practices](https://learn.microsoft.com/en-us/powershell/scripting/security/securing-powershell)\n- [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/)"
              },
              {
                "name": "powershell-shell-detection",
                "description": "Shell detection and cross-shell compatibility guidance for PowerShell vs Git Bash/MSYS2 on Windows",
                "path": "plugins/powershell-master/skills/powershell-shell-detection/SKILL.md",
                "frontmatter": {
                  "name": "powershell-shell-detection",
                  "description": "Shell detection and cross-shell compatibility guidance for PowerShell vs Git Bash/MSYS2 on Windows"
                },
                "content": "# PowerShell Shell Detection & Cross-Shell Compatibility\n\nCritical guidance for distinguishing between PowerShell and Git Bash/MSYS2 shells on Windows, with shell-specific path handling and compatibility notes.\n\n## Shell Detection Priority (Windows)\n\nWhen working on Windows, correctly identifying the shell environment is crucial for proper path handling and command execution.\n\n### Detection Order (Most Reliable First)\n\n1. **process.env.PSModulePath** (PowerShell specific)\n2. **process.env.MSYSTEM** (Git Bash/MinGW specific)\n3. **process.env.WSL_DISTRO_NAME** (WSL specific)\n4. **uname -s output** (Cross-shell, requires execution)\n\n## PowerShell Detection\n\n### Primary Indicators\n\n**PSModulePath (Most Reliable):**\n```powershell\n# PowerShell detection\nif ($env:PSModulePath) {\n    Write-Host \"Running in PowerShell\"\n    # PSModulePath contains 3+ paths separated by semicolons\n    $env:PSModulePath -split ';'\n}\n\n# Check PowerShell version\n$PSVersionTable.PSVersion\n# Output: 7.5.4 (PowerShell 7) or 5.1.x (Windows PowerShell)\n```\n\n**PowerShell-Specific Variables:**\n```powershell\n# These only exist in PowerShell\n$PSVersionTable    # Version info\n$PSScriptRoot      # Script directory\n$PSCommandPath     # Script full path\n$IsWindows         # Platform detection (PS 7+)\n$IsLinux           # Platform detection (PS 7+)\n$IsMacOS           # Platform detection (PS 7+)\n```\n\n### Shell Type Detection in Scripts\n\n```powershell\nfunction Get-ShellType {\n    if ($PSVersionTable) {\n        return \"PowerShell $($PSVersionTable.PSVersion)\"\n    }\n    elseif ($env:PSModulePath -and ($env:PSModulePath -split ';').Count -ge 3) {\n        return \"PowerShell (detected via PSModulePath)\"\n    }\n    else {\n        return \"Not PowerShell\"\n    }\n}\n\nGet-ShellType\n```\n\n## Git Bash / MSYS2 Detection\n\n### Primary Indicators\n\n**MSYSTEM Environment Variable (Most Reliable):**\n```bash\n# Bash detection in Git Bash/MSYS2\nif [ -n \"$MSYSTEM\" ]; then\n    echo \"Running in Git Bash/MSYS2: $MSYSTEM\"\nfi\n\n# MSYSTEM values:\n# MINGW64 - Native Windows 64-bit environment\n# MINGW32 - Native Windows 32-bit environment\n# MSYS    - POSIX-compliant build environment\n```\n\n**Secondary Detection Methods:**\n```bash\n# Using OSTYPE (Bash-specific)\ncase \"$OSTYPE\" in\n    msys*)   echo \"MSYS/Git Bash\" ;;\n    cygwin*) echo \"Cygwin\" ;;\n    linux*)  echo \"Linux\" ;;\n    darwin*) echo \"macOS\" ;;\nesac\n\n# Using uname (Most portable)\ncase \"$(uname -s)\" in\n    MINGW64*) echo \"Git Bash 64-bit\" ;;\n    MINGW32*) echo \"Git Bash 32-bit\" ;;\n    MSYS*)    echo \"MSYS\" ;;\n    CYGWIN*)  echo \"Cygwin\" ;;\n    Linux*)   echo \"Linux\" ;;\n    Darwin*)  echo \"macOS\" ;;\nesac\n```\n\n## Cross-Shell Compatibility on Windows\n\n### Critical Differences\n\n| Aspect | PowerShell | Git Bash/MSYS2 |\n|--------|-----------|----------------|\n| **Environment Variable** | `$env:VARIABLE` | `$VARIABLE` |\n| **Path Separator** | `;` (semicolon) | `:` (colon) |\n| **Path Style** | `C:\\Windows\\System32` | `/c/Windows/System32` |\n| **Home Directory** | `$env:USERPROFILE` | `$HOME` |\n| **Temp Directory** | `$env:TEMP` | `/tmp` |\n| **Command Format** | `Get-ChildItem` | `ls` (native command) |\n| **Aliases** | PowerShell cmdlet aliases | Unix command aliases |\n\n### Path Handling: PowerShell vs Git Bash\n\n**PowerShell Path Handling:**\n```powershell\n# Native Windows paths work directly\n$path = \"C:\\Users\\John\\Documents\"\nTest-Path $path  # True\n\n# Forward slashes also work in PowerShell 7+\n$path = \"C:/Users/John/Documents\"\nTest-Path $path  # True\n\n# Use Join-Path for cross-platform compatibility\n$configPath = Join-Path -Path $PSScriptRoot -ChildPath \"config.json\"\n\n# Use [System.IO.Path] for advanced scenarios\n$fullPath = [System.IO.Path]::Combine($home, \"documents\", \"file.txt\")\n```\n\n**Git Bash Path Handling:**\n```bash\n# Git Bash uses Unix-style paths\npath=\"/c/Users/John/Documents\"\ntest -d \"$path\" && echo \"Directory exists\"\n\n# Automatic path conversion (CAUTION)\n# Git Bash converts Unix-style paths to Windows-style\n# /c/Users  C:\\Users (automatic)\n# Arguments starting with / may be converted unexpectedly\n\n# Use cygpath for manual conversion\ncygpath -u \"C:\\path\"      #  /c/path (Unix format)\ncygpath -w \"/c/path\"      #  C:\\path (Windows format)\ncygpath -m \"/c/path\"      #  C:/path (Mixed format)\n```\n\n## Automatic Path Conversion in Git Bash (CRITICAL)\n\nGit Bash/MSYS2 automatically converts paths in certain scenarios, which can cause issues:\n\n### What Triggers Conversion\n\n```bash\n# Leading forward slash triggers conversion\ncommand /foo         # Converts to C:\\msys64\\foo\n\n# Path lists with colons\nexport PATH=/foo:/bar  # Converts to C:\\msys64\\foo;C:\\msys64\\bar\n\n# Arguments after dashes\ncommand --path=/foo    # Converts to --path=C:\\msys64\\foo\n```\n\n### What's Exempt from Conversion\n\n```bash\n# Arguments with equals sign (variable assignments)\nVAR=/foo command      # NOT converted\n\n# Drive specifiers\ncommand C:/path       # NOT converted\n\n# Arguments with semicolons (already Windows format)\ncommand \"C:\\foo;D:\\bar\"  # NOT converted\n\n# Double slashes (Windows switches)\ncommand //e //s       # NOT converted\n```\n\n### Disabling Path Conversion\n\n```bash\n# Disable ALL conversion (Git Bash)\nexport MSYS_NO_PATHCONV=1\ncommand /foo  # Stays as /foo\n\n# Exclude specific patterns (MSYS2)\nexport MSYS2_ARG_CONV_EXCL=\"*\"           # Exclude everything\nexport MSYS2_ARG_CONV_EXCL=\"--dir=;/test\"  # Specific prefixes\n```\n\n## When to Use PowerShell vs Git Bash on Windows\n\n### Use PowerShell When:\n\n-  **Windows-specific tasks** - Registry, WMI, Windows services\n-  **Azure/Microsoft 365 automation** - Az, Microsoft.Graph modules\n-  **Module ecosystem** - Leverage PSGallery modules\n-  **Object-oriented pipelines** - Rich object manipulation\n-  **Native Windows integration** - Built into Windows\n-  **CI/CD with pwsh** - GitHub Actions, Azure DevOps\n-  **Cross-platform scripting** - PowerShell 7 works on Linux/macOS\n\n**Example PowerShell Scenario:**\n```powershell\n# Azure VM management with Az module\nConnect-AzAccount\nGet-AzVM -ResourceGroupName \"Production\" |\n    Where-Object {$_.PowerState -eq \"VM running\"} |\n    Stop-AzVM -Force\n```\n\n### Use Git Bash When:\n\n-  **Unix tool compatibility** - sed, awk, grep, find\n-  **Git operations** - Native Git command-line experience\n-  **POSIX script execution** - Running Linux shell scripts\n-  **Cross-platform shell scripts** - Bash scripts from Linux/macOS\n-  **Text processing** - Unix text utilities (sed, awk, cut)\n-  **Development workflows** - Node.js, Python, Ruby with Unix tools\n\n**Example Git Bash Scenario:**\n```bash\n# Git workflow with Unix tools\ngit log --oneline | grep -i \"feature\" | awk '{print $1}' |\n    xargs git show --stat\n```\n\n## Shell-Aware Script Design\n\n### Detect and Adapt (PowerShell)\n\n```powershell\n# Detect if running in PowerShell or Git Bash context\nfunction Test-PowerShellContext {\n    return ($null -ne $PSVersionTable)\n}\n\n# Adapt path handling based on context\nfunction Get-CrossPlatformPath {\n    param([string]$Path)\n\n    if (Test-PowerShellContext) {\n        # PowerShell: Use Join-Path\n        return (Resolve-Path $Path -ErrorAction SilentlyContinue).Path\n    }\n    else {\n        # Non-PowerShell context\n        Write-Warning \"Not running in PowerShell. Path operations may differ.\"\n        return $Path\n    }\n}\n```\n\n### Detect and Adapt (Bash)\n\n```bash\n# Detect shell environment\ndetect_shell() {\n    if [ -n \"$MSYSTEM\" ]; then\n        echo \"git-bash\"\n    elif [ -n \"$PSModulePath\" ]; then\n        echo \"powershell\"\n    elif [ -n \"$WSL_DISTRO_NAME\" ]; then\n        echo \"wsl\"\n    else\n        echo \"unix\"\n    fi\n}\n\n# Adapt path handling\nconvert_path() {\n    local path=\"$1\"\n    local shell_type=$(detect_shell)\n\n    case \"$shell_type\" in\n        git-bash)\n            # Convert Windows path to Unix style\n            echo \"$path\" | sed 's|\\\\|/|g' | sed 's|^\\([A-Z]\\):|/\\L\\1|'\n            ;;\n        *)\n            echo \"$path\"\n            ;;\n    esac\n}\n\n# Usage\nshell_type=$(detect_shell)\necho \"Running in: $shell_type\"\n```\n\n## Environment Variable Comparison\n\n### Common Environment Variables\n\n| Variable | PowerShell | Git Bash | Purpose |\n|----------|-----------|----------|---------|\n| **Username** | `$env:USERNAME` | `$USER` | Current user |\n| **Home Directory** | `$env:USERPROFILE` | `$HOME` | User home |\n| **Temp Directory** | `$env:TEMP` | `/tmp` | Temporary files |\n| **Path List** | `$env:Path` (`;` sep) | `$PATH` (`:` sep) | Executable paths |\n| **Shell Detection** | `$env:PSModulePath` | `$MSYSTEM` | Shell identifier |\n\n### Cross-Shell Variable Access\n\n**PowerShell accessing environment variables:**\n```powershell\n$env:PATH              # Current PATH\n$env:PSModulePath      # PowerShell module paths\n$env:MSYSTEM           # Would be empty in PowerShell\n[Environment]::GetEnvironmentVariable(\"PATH\", \"Machine\")  # System PATH\n```\n\n**Git Bash accessing environment variables:**\n```bash\necho $PATH             # Current PATH\necho $MSYSTEM          # Git Bash: MINGW64, MINGW32, or MSYS\necho $PSModulePath     # Would be empty in pure Bash\n```\n\n## Practical Examples\n\n### Example 1: Cross-Shell File Finding\n\n**PowerShell:**\n```powershell\n# Find files modified in last 7 days\nGet-ChildItem -Path \"C:\\Projects\" -Recurse -File |\n    Where-Object { $_.LastWriteTime -gt (Get-Date).AddDays(-7) } |\n    Select-Object FullName, LastWriteTime\n```\n\n**Git Bash:**\n```bash\n# Same operation in Git Bash\nfind /c/Projects -type f -mtime -7 -exec ls -lh {} \\;\n```\n\n### Example 2: Process Management\n\n**PowerShell:**\n```powershell\n# Stop all Chrome processes\nGet-Process chrome -ErrorAction SilentlyContinue | Stop-Process -Force\n```\n\n**Git Bash:**\n```bash\n# Same operation in Git Bash\nps aux | grep chrome | awk '{print $2}' | xargs kill -9 2>/dev/null\n```\n\n### Example 3: Text File Processing\n\n**PowerShell:**\n```powershell\n# Extract unique email addresses from logs\nGet-Content \"logs.txt\" |\n    Select-String -Pattern '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' |\n    ForEach-Object { $_.Matches.Value } |\n    Sort-Object -Unique\n```\n\n**Git Bash:**\n```bash\n# Same operation in Git Bash\ngrep -oE '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' logs.txt |\n    sort -u\n```\n\n## Troubleshooting Cross-Shell Issues\n\n### Issue 1: Command Not Found\n\n**Problem:** Command works in one shell but not another\n```powershell\n# PowerShell\nGet-Process  # Works\n```\n```bash\n# Git Bash\nGet-Process  # Command not found\n```\n\n**Solution:** Understand that PowerShell cmdlets don't exist in Bash. Use native commands or install PowerShell Core (pwsh) in Git Bash:\n```bash\n# Run PowerShell from Git Bash\npwsh -Command \"Get-Process\"\n```\n\n### Issue 2: Path Format Mismatches\n\n**Problem:** Paths don't work across shells\n```bash\n# Git Bash path\n/c/Users/John/file.txt  # Works in Bash\n\n# PowerShell\nTest-Path \"/c/Users/John/file.txt\"  # May fail\n```\n\n**Solution:** Use cygpath for conversion or normalize paths:\n```bash\n# Convert to Windows format for PowerShell\nwin_path=$(cygpath -w \"/c/Users/John/file.txt\")\npwsh -Command \"Test-Path '$win_path'\"\n```\n\n### Issue 3: Alias Conflicts\n\n**Problem:** `ls`, `cd`, `cat` behave differently\n```powershell\n# PowerShell\nls  # Actually runs Get-ChildItem\n```\n```bash\n# Git Bash\nls  # Runs native Unix ls command\n```\n\n**Solution:** Use full cmdlet names in PowerShell scripts:\n```powershell\n# Instead of: ls\nGet-ChildItem  # Explicit cmdlet name\n```\n\n## Best Practices Summary\n\n### PowerShell Scripts\n1.  Use `$PSScriptRoot` for script-relative paths\n2.  Use `Join-Path` or `[IO.Path]::Combine()` for paths\n3.  Avoid hardcoded backslashes\n4.  Use full cmdlet names (no aliases)\n5.  Test on all target platforms\n6.  Use `$IsWindows`, `$IsLinux`, `$IsMacOS` for platform detection\n\n### Git Bash Scripts\n1.  Check `$MSYSTEM` for Git Bash detection\n2.  Use `cygpath` for path conversion when needed\n3.  Set `MSYS_NO_PATHCONV=1` to disable auto-conversion if needed\n4.  Quote paths with spaces\n5.  Use Unix-style paths (`/c/...`) within Bash\n6.  Convert to Windows paths when calling Windows tools\n\n### Cross-Shell Development\n1.  Document which shell your script requires\n2.  Add shell detection at script start\n3.  Provide clear error messages for wrong shell\n4.  Consider creating wrapper scripts for cross-shell compatibility\n5.  Test in both PowerShell and Git Bash if supporting both\n\n## Resources\n\n- [PowerShell Documentation](https://learn.microsoft.com/powershell)\n- [Git for Windows Documentation](https://git-scm.com/doc)\n- [MSYS2 Documentation](https://www.msys2.org/docs/what-is-msys2/)\n- [Cygpath Documentation](https://www.cygwin.com/cygwin-ug-net/cygpath.html)\n\n---\n\n**Last Updated:** October 2025"
              }
            ]
          },
          {
            "name": "terraform-master",
            "description": "Complete Terraform expertise system for all cloud providers (Azure, AWS, Google Cloud) and platforms. PROACTIVELY activate for: (1) ANY Terraform task (init/plan/apply/validate), (2) Infrastructure-as-code design, (3) Resource import and state management, (4) Multi-environment architectures, (5) CI/CD pipeline integration, (6) Security scanning and best practices, (7) Version-aware code generation, (8) CLI mastery with -chdir and all flags. Provides: comprehensive provider knowledge (AzureRM/AWS/GCP/community), version-aware implementations, state operations (mv/rm/import), bulk import strategies (Terraformer/aztfexport), security scanning (tfsec/Checkov), enterprise architecture patterns, complete CLI reference, and platform-specific guidance (Windows/Linux/macOS).",
            "source": "./plugins/terraform-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install terraform-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/tf-apply",
                "description": "Apply Terraform changes to infrastructure",
                "path": "plugins/terraform-master/commands/tf-apply.md",
                "frontmatter": {
                  "name": "tf-apply",
                  "description": "Apply Terraform changes to infrastructure",
                  "argument-hint": "[planfile] [-auto-approve] [-target=resource]"
                },
                "content": "# Terraform Apply Command\n\nApply changes to reach the desired infrastructure state.\n\n## Usage\n\n```\n/tf-apply                         # Interactive apply\n/tf-apply tfplan                  # Apply saved plan\n/tf-apply -auto-approve          # Non-interactive apply\n/tf-apply -target=aws_instance.web  # Target specific resource\n```\n\n## What This Command Does\n\n1. Reads plan (generates if not provided)\n2. Confirms changes with user (unless -auto-approve)\n3. Applies changes to infrastructure\n4. Updates state file\n5. Reports results\n\n## Execution Steps\n\n### Step 1: Pre-Flight Checks\n```bash\n# Verify initialization\nterraform validate\n\n# Review plan first\nterraform plan\n```\n\n### Step 2: Run Apply\n```bash\n# Interactive (prompts for confirmation)\nterraform apply\n\n# Apply saved plan\nterraform apply tfplan\n\n# Non-interactive (CI/CD)\nterraform apply -auto-approve\n\n# With var file\nterraform apply -var-file=\"prod.tfvars\"\n```\n\n### Step 3: Verify\n```bash\n# Check outputs\nterraform output\n\n# Verify state\nterraform state list\n```\n\n## Common Options\n\n| Option | Description |\n|--------|-------------|\n| `planfile` | Apply saved plan file |\n| `-auto-approve` | Skip confirmation |\n| `-var-file=FILE` | Load variables |\n| `-var='KEY=VALUE'` | Set variable |\n| `-target=RESOURCE` | Apply to specific resource |\n| `-parallelism=N` | Limit parallel operations |\n| `-lock-timeout=DURATION` | State lock timeout |\n| `-refresh=false` | Skip state refresh |\n\n## Safety Guidelines\n\n### Production Apply\n```bash\n# 1. Always plan first\nterraform plan -out=tfplan\n\n# 2. Review the plan carefully\n# 3. Apply with timeout protection\nterraform apply -lock-timeout=30m tfplan\n```\n\n### Never Do This\n```bash\n# Don't auto-approve without review\nterraform apply -auto-approve  # Dangerous without prior plan review!\n\n# Don't skip the plan step\nterraform apply  # Always plan first\n```\n\n## Examples\n\n### Standard Workflow\n```bash\n# 1. Plan and save\nterraform plan -out=tfplan\n\n# 2. Apply saved plan\nterraform apply tfplan\n```\n\n### CI/CD Apply\n```bash\n# Apply with timeout and no color\nterraform apply \\\n  -auto-approve \\\n  -lock-timeout=30m \\\n  -no-color \\\n  tfplan\n```\n\n### Target Specific Resource\n```bash\nterraform apply -target=aws_instance.web -auto-approve\n```\n\n## Troubleshooting\n\n### State Lock Error\n```bash\n# Wait for lock or force unlock\nterraform apply -lock-timeout=10m\n# Or (dangerous): terraform force-unlock LOCK_ID\n```\n\n### Partial Apply Failure\n```bash\n# Don't panic - state reflects actual changes\n# Fix the issue and re-run\nterraform apply\n```"
              },
              {
                "name": "/tf-fmt",
                "description": "Format Terraform configuration files",
                "path": "plugins/terraform-master/commands/tf-fmt.md",
                "frontmatter": {
                  "name": "tf-fmt",
                  "description": "Format Terraform configuration files",
                  "argument-hint": "[-check] [-recursive] [-diff]"
                },
                "content": "# Terraform Format Command\n\nFormat Terraform configuration files to canonical style.\n\n## Usage\n\n```\n/tf-fmt                   # Format current directory\n/tf-fmt -check           # Check without changing\n/tf-fmt -recursive       # Format all subdirectories\n/tf-fmt -diff            # Show what would change\n```\n\n## What This Command Does\n\n1. Reads .tf files\n2. Applies canonical formatting\n3. Rewrites files (unless -check)\n4. Handles indentation, alignment, spacing\n5. Processes subdirectories (with -recursive)\n\n## Execution Steps\n\n### Step 1: Check Format\n```bash\n# Check without changes\nterraform fmt -check\n\n# Check with diff output\nterraform fmt -check -diff\n```\n\n### Step 2: Format Files\n```bash\n# Format current directory\nterraform fmt\n\n# Format recursively\nterraform fmt -recursive\n\n# Format specific file\nterraform fmt main.tf\n```\n\n## Common Options\n\n| Option | Description |\n|--------|-------------|\n| `-check` | Check only, don't modify |\n| `-diff` | Show formatting changes |\n| `-recursive` | Process subdirectories |\n| `-write=false` | Don't write changes |\n| `-list=false` | Don't list formatted files |\n\n## CI/CD Integration\n\n### GitHub Actions\n```yaml\n- name: Terraform Format Check\n  id: fmt\n  run: terraform fmt -check -recursive\n  continue-on-error: true\n\n- name: Format Status\n  if: steps.fmt.outcome == 'failure'\n  run: |\n    echo \"Terraform files are not formatted!\"\n    terraform fmt -recursive -diff\n    exit 1\n```\n\n### Pre-Commit Hook\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\n    rev: v1.83.5\n    hooks:\n      - id: terraform_fmt\n```\n\n### Git Pre-Commit (Manual)\n```bash\n#!/bin/sh\n# .git/hooks/pre-commit\n\n# Check Terraform formatting\nif ! terraform fmt -check -recursive; then\n    echo \"Terraform files are not formatted. Run 'terraform fmt -recursive'\"\n    exit 1\nfi\n```\n\n## Exit Codes\n\n| Code | Meaning |\n|------|---------|\n| 0 | All files formatted (or no changes needed) |\n| 1 | Formatting errors found (with -check) |\n| 2 | Command error |\n\n## Examples\n\n### Check All Files\n```bash\nterraform fmt -check -recursive\n```\n\n### Show What Would Change\n```bash\nterraform fmt -diff -recursive\n```\n\n### Format and List Changed Files\n```bash\nterraform fmt -recursive\n# Output: main.tf\n#         modules/vpc/main.tf\n```\n\n### Format Without Listing\n```bash\nterraform fmt -recursive -list=false\n```\n\n## Formatting Rules Applied\n\n- 2-space indentation\n- Aligned `=` signs in blocks\n- Consistent spacing\n- Sorted block attributes\n- Standardized quotes\n\n### Before\n```hcl\nresource \"aws_instance\" \"web\" {\nami = \"ami-12345678\"\n  instance_type=\"t3.micro\"\ntags={Name=\"web\"}\n}\n```\n\n### After\n```hcl\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t3.micro\"\n  tags = {\n    Name = \"web\"\n  }\n}\n```\n\n## Best Practices\n\n1. Always format before commit\n2. Use pre-commit hooks\n3. Check in CI/CD (fail on unformatted)\n4. Format recursively for full projects\n5. Team should use same Terraform version"
              },
              {
                "name": "/tf-import",
                "description": "Import existing infrastructure into Terraform state",
                "path": "plugins/terraform-master/commands/tf-import.md",
                "frontmatter": {
                  "name": "tf-import",
                  "description": "Import existing infrastructure into Terraform state",
                  "argument-hint": "<resource_address> <resource_id>"
                },
                "content": "# Terraform Import Command\n\nImport existing infrastructure resources into Terraform management.\n\n## Usage\n\n```\n/tf-import aws_vpc.main vpc-12345678\n/tf-import azurerm_resource_group.main /subscriptions/.../resourceGroups/my-rg\n/tf-import --generate-config        # Generate config for imports (1.5+)\n```\n\n## What This Command Does\n\n1. Identifies existing resource in cloud\n2. Creates state entry for resource\n3. Associates resource with Terraform configuration\n4. Does NOT generate configuration (traditional import)\n5. CAN generate configuration (1.5+ import blocks)\n\n## Import Methods\n\n### Method 1: Traditional Import (All Versions)\n```bash\n# Requires existing resource block in config\nterraform import aws_instance.web i-1234567890abcdef0\n```\n\n### Method 2: Import Blocks (Terraform 1.5+)\n```hcl\n# imports.tf\nimport {\n  to = aws_instance.web\n  id = \"i-1234567890abcdef0\"\n}\n\n# Generate configuration\nterraform plan -generate-config-out=generated.tf\nterraform apply\n```\n\n## Common Resource IDs\n\n### AWS\n```bash\n# EC2 Instance\nterraform import aws_instance.web i-1234567890abcdef0\n\n# VPC\nterraform import aws_vpc.main vpc-12345678\n\n# S3 Bucket\nterraform import aws_s3_bucket.main my-bucket-name\n\n# Security Group\nterraform import aws_security_group.main sg-12345678\n\n# IAM Role\nterraform import aws_iam_role.main role-name\n```\n\n### Azure\n```bash\n# Resource Group\nterraform import azurerm_resource_group.main /subscriptions/SUB_ID/resourceGroups/my-rg\n\n# Storage Account\nterraform import azurerm_storage_account.main /subscriptions/SUB_ID/resourceGroups/my-rg/providers/Microsoft.Storage/storageAccounts/mystorageaccount\n\n# Virtual Network\nterraform import azurerm_virtual_network.main /subscriptions/SUB_ID/resourceGroups/my-rg/providers/Microsoft.Network/virtualNetworks/my-vnet\n```\n\n### GCP\n```bash\n# Compute Instance\nterraform import google_compute_instance.main projects/my-project/zones/us-central1-a/instances/my-instance\n\n# VPC Network\nterraform import google_compute_network.main projects/my-project/global/networks/my-network\n\n# GCS Bucket\nterraform import google_storage_bucket.main my-bucket-name\n```\n\n## Import Block Examples (1.5+)\n\n```hcl\n# Single import\nimport {\n  to = aws_vpc.main\n  id = \"vpc-12345678\"\n}\n\n# Multiple imports\nimport {\n  to = aws_subnet.public[0]\n  id = \"subnet-aaaa1111\"\n}\n\nimport {\n  to = aws_subnet.public[1]\n  id = \"subnet-bbbb2222\"\n}\n\n# OpenTofu 1.7+: Looped imports\nimport {\n  for_each = local.subnets\n  to       = aws_subnet.imported[each.key]\n  id       = each.value\n}\n```\n\n## Workflow\n\n### Step 1: Create Resource Block\n```hcl\n# main.tf\nresource \"aws_vpc\" \"main\" {\n  # Configuration will be filled after import\n}\n```\n\n### Step 2: Import\n```bash\nterraform import aws_vpc.main vpc-12345678\n```\n\n### Step 3: Update Configuration\n```bash\n# Show imported state\nterraform state show aws_vpc.main\n\n# Update config to match\n```\n\n### Step 4: Verify\n```bash\n# Should show no changes\nterraform plan\n```\n\n## Best Practices\n\n1. Always backup state before importing\n2. Import dependencies first (VPC before subnet)\n3. Use import blocks for Terraform 1.5+\n4. Verify with `terraform plan` showing no changes\n5. Document imported resources"
              },
              {
                "name": "/tf-init",
                "description": "Initialize Terraform workspace with backend configuration",
                "path": "plugins/terraform-master/commands/tf-init.md",
                "frontmatter": {
                  "name": "tf-init",
                  "description": "Initialize Terraform workspace with backend configuration",
                  "argument-hint": "[backend-config options]"
                },
                "content": "# Terraform Init Command\n\nInitialize a Terraform working directory with proper backend configuration.\n\n## Usage\n\n```\n/tf-init                          # Basic init with upgrade\n/tf-init -reconfigure            # Reconfigure backend\n/tf-init key=prod.tfstate        # Backend config override\n```\n\n## What This Command Does\n\n1. Detects current working directory\n2. Checks for existing .terraform directory\n3. Runs `terraform init` with appropriate flags\n4. Handles backend configuration\n5. Reports initialization status\n\n## Execution Steps\n\n### Step 1: Check Environment\n```bash\n# Verify terraform is available\nterraform version\n\n# Check for existing config\nls -la *.tf 2>/dev/null || echo \"No .tf files found\"\n```\n\n### Step 2: Run Init\n```bash\n# Basic init with upgrade\nterraform init -upgrade\n\n# With backend config (if provided)\nterraform init -upgrade -backend-config=\"key=${ARGS}\"\n\n# Reconfigure (if -reconfigure flag)\nterraform init -reconfigure\n```\n\n### Step 3: Validate\n```bash\n# Verify initialization\nterraform validate\n```\n\n## Common Options\n\n| Option | Description |\n|--------|-------------|\n| `-upgrade` | Upgrade providers and modules |\n| `-reconfigure` | Reconfigure backend, ignoring existing |\n| `-migrate-state` | Migrate state to new backend |\n| `-backend-config=KEY=VALUE` | Override backend config |\n| `-backend=false` | Skip backend initialization |\n\n## Examples\n\n### Initialize New Project\n```bash\nterraform init -upgrade\n```\n\n### Change Backend Configuration\n```bash\nterraform init -reconfigure -backend-config=\"key=prod.tfstate\"\n```\n\n### Migrate to New Backend\n```bash\nterraform init -migrate-state\n```\n\n## Troubleshooting\n\n### Backend Configuration Error\n```bash\n# Remove existing backend config\nrm -rf .terraform\nterraform init\n```\n\n### Provider Download Failure\n```bash\n# Clear provider cache\nrm -rf .terraform/providers\nterraform init -upgrade\n```"
              },
              {
                "name": "/tf-plan",
                "description": "Generate and review Terraform execution plan",
                "path": "plugins/terraform-master/commands/tf-plan.md",
                "frontmatter": {
                  "name": "tf-plan",
                  "description": "Generate and review Terraform execution plan",
                  "argument-hint": "[var-file] [-target=resource] [-out=planfile]"
                },
                "content": "# Terraform Plan Command\n\nGenerate an execution plan showing what Terraform will do.\n\n## Usage\n\n```\n/tf-plan                          # Basic plan\n/tf-plan prod.tfvars             # With var file\n/tf-plan -target=module.vpc      # Target specific resource\n/tf-plan -out=tfplan             # Save plan to file\n```\n\n## What This Command Does\n\n1. Refreshes state (unless -refresh=false)\n2. Compares desired state with actual state\n3. Generates execution plan\n4. Shows what will be created, changed, or destroyed\n5. Optionally saves plan to file\n\n## Execution Steps\n\n### Step 1: Pre-Flight Checks\n```bash\n# Verify initialization\nterraform validate\n\n# Check formatting\nterraform fmt -check\n```\n\n### Step 2: Run Plan\n```bash\n# Basic plan\nterraform plan\n\n# With var file\nterraform plan -var-file=\"${ARGS}\"\n\n# With target\nterraform plan -target=\"${TARGET}\"\n\n# Save plan\nterraform plan -out=tfplan\n\n# CI/CD optimized\nterraform plan -no-color -out=tfplan -detailed-exitcode\n```\n\n### Step 3: Review Output\nPlan output shows:\n- `+` Resources to create\n- `-` Resources to destroy\n- `~` Resources to modify\n- `+/-` Resources to replace\n\n## Common Options\n\n| Option | Description |\n|--------|-------------|\n| `-var-file=FILE` | Load variables from file |\n| `-var='KEY=VALUE'` | Set individual variable |\n| `-target=RESOURCE` | Plan specific resource |\n| `-out=FILE` | Save plan to file |\n| `-refresh=false` | Skip state refresh |\n| `-detailed-exitcode` | Return 2 if changes |\n| `-no-color` | Disable colored output |\n| `-parallelism=N` | Limit parallel operations |\n\n## Exit Codes (with -detailed-exitcode)\n\n| Code | Meaning |\n|------|---------|\n| 0 | No changes |\n| 1 | Error |\n| 2 | Changes detected |\n\n## Examples\n\n### Basic Plan\n```bash\nterraform plan\n```\n\n### Plan with Variables\n```bash\nterraform plan -var-file=\"environments/prod.tfvars\"\n```\n\n### Plan Specific Resource\n```bash\nterraform plan -target=aws_instance.web\nterraform plan -target=module.networking\n```\n\n### CI/CD Plan\n```bash\nterraform plan \\\n  -no-color \\\n  -out=tfplan \\\n  -detailed-exitcode \\\n  -var-file=\"prod.tfvars\"\n```\n\n## Best Practices\n\n1. Always review plan before apply\n2. Save plan to file in CI/CD\n3. Use `-detailed-exitcode` for automation\n4. Target cautiously (may miss dependencies)"
              },
              {
                "name": "/tf-state",
                "description": "Manage Terraform state (list, show, mv, rm, pull, push)",
                "path": "plugins/terraform-master/commands/tf-state.md",
                "frontmatter": {
                  "name": "tf-state",
                  "description": "Manage Terraform state (list, show, mv, rm, pull, push)",
                  "argument-hint": "<subcommand> [args]"
                },
                "content": "# Terraform State Command\n\nManage and manipulate Terraform state.\n\n## Usage\n\n```\n/tf-state list                    # List all resources\n/tf-state show aws_vpc.main      # Show resource details\n/tf-state mv old.name new.name   # Move/rename resource\n/tf-state rm aws_instance.temp   # Remove from state\n/tf-state pull                   # Download state\n```\n\n## Subcommands\n\n### list - List Resources\n```bash\n# List all resources\nterraform state list\n\n# Filter by type\nterraform state list | grep aws_instance\n\n# Filter by resource ID\nterraform state list -id=i-1234567890abcdef0\n```\n\n### show - Show Resource Details\n```bash\n# Show specific resource\nterraform state show aws_vpc.main\n\n# Show module resource\nterraform state show module.networking.aws_subnet.private[0]\n```\n\n### mv - Move/Rename Resources\n```bash\n# Rename resource\nterraform state mv aws_instance.old aws_instance.new\n\n# Move to module\nterraform state mv aws_vpc.main module.networking.aws_vpc.main\n\n# Move between modules\nterraform state mv module.old.aws_vpc.main module.new.aws_vpc.main\n\n# Count to for_each migration\nterraform state mv 'aws_subnet.public[0]' 'aws_subnet.public[\"us-east-1a\"]'\n```\n\n### rm - Remove from State\n```bash\n# Remove single resource (resource still exists!)\nterraform state rm aws_instance.temp\n\n# Remove multiple resources\nterraform state rm aws_instance.temp aws_subnet.temp\n\n# Remove module\nterraform state rm module.legacy\n\n# Remove all of type\nterraform state list | grep aws_subnet | xargs -I{} terraform state rm {}\n```\n\n### pull - Download State\n```bash\n# Download state to stdout\nterraform state pull\n\n# Save to file\nterraform state pull > backup.tfstate\n\n# Query with jq\nterraform state pull | jq '.resources[] | select(.type == \"aws_instance\")'\n```\n\n### push - Upload State\n```bash\n# Push state (DANGEROUS!)\nterraform state push backup.tfstate\n\n# Force push (VERY DANGEROUS!)\nterraform state push -force backup.tfstate\n```\n\n## Common Use Cases\n\n### Rename Resource\n```bash\n# 1. Move in state\nterraform state mv aws_rg.old_name aws_rg.new_name\n\n# 2. Update config to match\n# 3. Verify\nterraform plan  # Should show no changes\n```\n\n### Move to Module\n```bash\n# 1. Create module in config\n# 2. Move state\nterraform state mv aws_vpc.main module.networking.aws_vpc.main\nterraform state mv aws_subnet.public module.networking.aws_subnet.public\n\n# 3. Update references\n# 4. Verify\nterraform plan\n```\n\n### Split State\n```bash\n# Source state\nterraform state rm aws_vpc.shared\n\n# Target state (different directory)\ncd ../networking\nterraform import aws_vpc.shared vpc-12345678\n```\n\n### Recover from Disaster\n```bash\n# Restore from backup\nterraform state pull > current_state.tfstate.backup\nterraform state push previous_backup.tfstate\n```\n\n## Safety Tips\n\n1. **ALWAYS backup** before state operations\n   ```bash\n   terraform state pull > backup-$(date +%Y%m%d).tfstate\n   ```\n\n2. **Test in non-prod first**\n\n3. **Verify with plan** after operations\n   ```bash\n   terraform plan  # Should show no changes\n   ```\n\n4. **Never manually edit** state JSON\n\n5. **Use -dry-run** when available\n   ```bash\n   terraform state mv -dry-run old new\n   ```\n\n## Troubleshooting\n\n### State Lock Error\n```bash\n# Check who holds lock\nterraform state list  # Shows lock error with info\n\n# Force unlock (last resort)\nterraform force-unlock LOCK_ID\n```\n\n### Resource Not Found\n```bash\n# Check exact address\nterraform state list | grep -i resource_name\n\n# Check modules\nterraform state list | grep module\n```"
              },
              {
                "name": "/tf-validate",
                "description": "Validate Terraform configuration syntax and consistency",
                "path": "plugins/terraform-master/commands/tf-validate.md",
                "frontmatter": {
                  "name": "tf-validate",
                  "description": "Validate Terraform configuration syntax and consistency",
                  "argument-hint": "[-json]"
                },
                "content": "# Terraform Validate Command\n\nValidate configuration files for syntax and internal consistency.\n\n## Usage\n\n```\n/tf-validate              # Basic validation\n/tf-validate -json       # JSON output for CI/CD\n```\n\n## What This Command Does\n\n1. Checks HCL syntax\n2. Validates resource configurations\n3. Checks provider requirements\n4. Validates variable types\n5. Checks module configurations\n6. Does NOT access remote state or APIs\n\n## Execution Steps\n\n### Step 1: Initialize (Required)\n```bash\n# Must init first\nterraform init\n```\n\n### Step 2: Validate\n```bash\n# Basic validation\nterraform validate\n\n# JSON output (CI/CD)\nterraform validate -json\n\n# No color (CI/CD)\nterraform validate -no-color\n```\n\n### Step 3: Fix Issues\nCommon validation errors:\n- Missing required arguments\n- Invalid argument names\n- Type mismatches\n- Circular references\n- Missing providers\n\n## Output\n\n### Success\n```\nSuccess! The configuration is valid.\n```\n\n### JSON Success\n```json\n{\n  \"valid\": true,\n  \"error_count\": 0,\n  \"warning_count\": 0,\n  \"diagnostics\": []\n}\n```\n\n### Failure Example\n```\nError: Missing required argument\n\n  on main.tf line 5, in resource \"aws_instance\" \"web\":\n   5: resource \"aws_instance\" \"web\" {\n\nThe argument \"ami\" is required, but no definition was found.\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n```yaml\n- name: Terraform Validate\n  id: validate\n  run: terraform validate -no-color\n```\n\n### With JSON Parsing\n```yaml\n- name: Terraform Validate\n  id: validate\n  run: |\n    terraform validate -json > validation.json\n    if [ $(jq '.valid' validation.json) != \"true\" ]; then\n      jq '.diagnostics' validation.json\n      exit 1\n    fi\n```\n\n## Pre-Commit Hook\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\n    rev: v1.83.5\n    hooks:\n      - id: terraform_validate\n```\n\n## Common Errors\n\n### Missing Provider\n```\nError: Missing required provider\n\nThis configuration requires provider \"aws\", but no such provider\nis configured. Please add a provider block.\n```\n**Fix:** Add provider configuration or required_providers block.\n\n### Invalid Argument\n```\nError: Unsupported argument\n\n  on main.tf line 10, in resource \"aws_instance\" \"web\":\n  10:   invalid_arg = \"value\"\n\nAn argument named \"invalid_arg\" is not expected here.\n```\n**Fix:** Check provider documentation for valid arguments.\n\n### Type Mismatch\n```\nError: Invalid value for input variable\n\nThe given value is not valid for variable \"count\": a number is required.\n```\n**Fix:** Ensure variable values match declared types.\n\n## Best Practices\n\n1. Run validate after every change\n2. Include in CI/CD pipeline\n3. Use as pre-commit hook\n4. Check JSON output in automation\n5. Always init before validate"
              }
            ],
            "skills": [
              {
                "name": "opentofu-guide",
                "description": "Comprehensive OpenTofu expertise including migration from Terraform, state encryption, OpenTofu 1.10/1.11 features (OCI registry, native S3 locking, ephemeral resources, enabled meta-argument), and CI/CD integration. Covers when to use OpenTofu vs Terraform with decision matrix.",
                "path": "plugins/terraform-master/skills/opentofu-guide/SKILL.md",
                "frontmatter": {
                  "name": "opentofu-guide",
                  "description": "Comprehensive OpenTofu expertise including migration from Terraform, state encryption, OpenTofu 1.10/1.11 features (OCI registry, native S3 locking, ephemeral resources, enabled meta-argument), and CI/CD integration. Covers when to use OpenTofu vs Terraform with decision matrix."
                },
                "content": "<!--\nProgressive Disclosure References:\n- references/opentofu-1.10-features.md - OCI registry, native S3 locking, deprecation warnings\n- references/opentofu-1.11-features.md - Ephemeral resources, enabled meta-argument\n- references/state-encryption.md - Complete state encryption guide with KMS integration\n-->\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n---\n\n\n# OpenTofu Expertise and Migration Guide\n\n## Overview\n\nOpenTofu is the open-source fork of Terraform, created in 2023 after HashiCorp changed Terraform's license from MPL 2.0 to BSL (Business Source License). OpenTofu is stewarded by the Linux Foundation and maintains full compatibility with Terraform 1.5.x while adding community-driven features.\n\n## Key Differences (2025)\n\n### Licensing\n\n**Terraform (HashiCorp):**\n- BSL (Business Source License) since August 2023\n- Restrictions on commercial use for competing products\n- IBM acquired HashiCorp in 2024\n\n**OpenTofu:**\n- MPL 2.0 (Mozilla Public License)\n- True open-source\n- Linux Foundation governance\n- Community-driven development\n\n### Feature Innovations (2025)\n\n**OpenTofu 1.7 Features:**\n- **State Encryption**: Client-side encryption (community requested for 5+ years)\n- **Loop-able Import Blocks**: for_each in import blocks\n- **Dynamic Provider Functions**: Provider-defined functions support\n- **Early Variable Evaluation**: Variables in terraform block\n\n**OpenTofu 1.8 Features (Latest):**\n- **OpenTofu-Specific Overrides**: Balance compatibility with innovation\n- **Early Variable Evaluation Expanded**: Use variables/locals in module sources\n- **Enhanced Provider Support**: Improved provider SDK\n\n**Terraform Advantages:**\n- **HCP Terraform**: Cloud platform with Stacks, HYOK, Private VCS Access\n- **Enterprise Support**: Direct HashiCorp/IBM support\n- **Larger Ecosystem**: More established marketplace\n- **Sentinel Policies**: Policy-as-code framework (350+ NIST policies)\n\n### Compatibility\n\n**100% Compatible:**\n- HCL syntax (same language)\n- Provider ecosystem (same registry access)\n- State file format (Terraform 1.5.x)\n- Module structure\n- CLI commands\n\n**Migration Path:**\n- Drop-in replacement for Terraform 1.5.x\n- No code changes required\n- State files portable (with encryption consideration)\n\n## When to Use OpenTofu vs Terraform\n\n### Choose OpenTofu When:\n\n1. **Open-Source Requirements:**\n   - Organization policy requires open-source tools\n   - Want vendor neutrality\n   - Concerned about future license changes\n\n2. **State Encryption Needed:**\n   - Compliance requires client-side encryption\n   - Want encryption without HCP Terraform\n   - Multi-cloud encryption requirements\n\n3. **Cost Optimization:**\n   - Want free state encryption\n   - No need for HCP Terraform features\n   - Budget constraints on tooling\n\n4. **Community-Driven:**\n   - Want to influence roadmap\n   - Prefer Linux Foundation governance\n   - Value community contributions\n\n### Choose Terraform When:\n\n1. **Enterprise Features Required:**\n   - Need HCP Terraform Stacks\n   - Require HYOK (Hold Your Own Key)\n   - Want Private VCS Access\n   - Need Sentinel policy enforcement\n\n2. **Enterprise Support:**\n   - Want direct HashiCorp/IBM support\n   - Need SLA guarantees\n   - Require compliance certifications\n\n3. **Advanced Features:**\n   - Ephemeral values (1.10+)\n   - Terraform Query (1.14+)\n   - Actions blocks (1.14+)\n   - Latest provider features first\n\n4. **Established Ecosystem:**\n   - Existing HCP Terraform investment\n   - Tight integration needs\n   - Mature tooling requirements\n\n## Migration from Terraform to OpenTofu\n\n### Step 1: Assess Compatibility\n\n```bash\n# Check Terraform version\nterraform version\n# Must be 1.5.x or compatible\n\n# Check provider versions\nterraform providers\n# All providers compatible (same registry)\n```\n\n### Step 2: Install OpenTofu\n\n**Windows:**\n```powershell\n# Chocolatey\nchoco install opentofu\n\n# Scoop\nscoop install opentofu\n\n# Manual\n# Download from https://github.com/opentofu/opentofu/releases\n```\n\n**macOS:**\n```bash\n# Homebrew\nbrew install opentofu\n\n# Manual\ncurl -L https://github.com/opentofu/opentofu/releases/download/v1.8.0/tofu_1.8.0_darwin_amd64.tar.gz | tar xz\nsudo mv tofu /usr/local/bin/\n```\n\n**Linux:**\n```bash\n# Snap\nsnap install opentofu --classic\n\n# Debian/Ubuntu\ncurl -fsSL https://get.opentofu.org/install-opentofu.sh | sh\n\n# Manual\nwget https://github.com/opentofu/opentofu/releases/download/v1.8.0/tofu_1.8.0_linux_amd64.tar.gz\ntar -xzf tofu_1.8.0_linux_amd64.tar.gz\nsudo mv tofu /usr/local/bin/\n```\n\n### Step 3: Test Compatibility\n\n```bash\n# Navigate to Terraform directory\ncd /path/to/terraform/project\n\n# Initialize with OpenTofu (non-destructive)\ntofu init\n\n# Validate configuration\ntofu validate\n\n# Generate plan (compare with Terraform plan)\ntofu plan\n```\n\n### Step 4: Migrate State (Optional)\n\n**If NOT using state encryption:**\n```bash\n# State is compatible - no migration needed\n# Just switch from 'terraform' to 'tofu' commands\n\n# Verify state\ntofu show\n```\n\n**If ENABLING state encryption:**\n```bash\n# Configure encryption in .tofu file\ncat > .tofu <<EOF\nencryption {\n  state {\n    method = \"aes_gcm\"\n    keys {\n      name = \"my_key\"\n      passphrase = env.TOFU_ENCRYPTION_KEY\n    }\n  }\n\n  plan {\n    method = \"aes_gcm\"\n    keys {\n      name = \"my_key\"\n      passphrase = env.TOFU_ENCRYPTION_KEY\n    }\n  }\n}\nEOF\n\n# Set encryption key\nexport TOFU_ENCRYPTION_KEY=\"your-secure-passphrase\"\n\n# Migrate state (automatically encrypts)\ntofu init -migrate-state\n```\n\n### Step 5: Update CI/CD\n\n**GitHub Actions:**\n```yaml\n# Before (Terraform)\n- uses: hashicorp/setup-terraform@v3\n  with:\n    terraform_version: 1.5.0\n\n# After (OpenTofu)\n- uses: opentofu/setup-opentofu@v1\n  with:\n    tofu_version: 1.8.0\n\n# Or manual install\n- name: Install OpenTofu\n  run: |\n    curl -fsSL https://get.opentofu.org/install-opentofu.sh | sh\n    tofu version\n```\n\n**Azure DevOps:**\n```yaml\n# Before\n- task: TerraformInstaller@0\n  inputs:\n    terraformVersion: '1.5.0'\n\n# After\n- task: Bash@3\n  displayName: 'Install OpenTofu'\n  inputs:\n    targetType: 'inline'\n    script: |\n      curl -fsSL https://get.opentofu.org/install-opentofu.sh | sh\n      tofu version\n```\n\n**GitLab CI:**\n```yaml\n# Before\nimage: hashicorp/terraform:1.5.0\n\n# After\nimage: ghcr.io/opentofu/opentofu:1.8.0\n```\n\n## State Encryption (OpenTofu Exclusive)\n\n### Configuration\n\n**Basic Encryption:**\n```hcl\n# .tofu or terraform.tf\nencryption {\n  state {\n    method = \"aes_gcm\"\n    keys {\n      name = \"primary_key\"\n      passphrase = env.TOFU_STATE_ENCRYPTION_KEY\n    }\n  }\n}\n```\n\n**Key Rotation:**\n```hcl\nencryption {\n  state {\n    method = \"aes_gcm\"\n    keys {\n      # New key\n      name = \"key_v2\"\n      passphrase = env.TOFU_KEY_V2\n\n      # Old key (for decryption)\n      fallback {\n        name = \"key_v1\"\n        passphrase = env.TOFU_KEY_V1\n      }\n    }\n  }\n}\n```\n\n**Cloud KMS Integration:**\n```hcl\n# AWS KMS\nencryption {\n  state {\n    method = \"aws_kms\"\n    keys {\n      name = \"aws_key\"\n      kms_key_id = \"arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\"\n    }\n  }\n}\n\n# Azure Key Vault\nencryption {\n  state {\n    method = \"azurerm_key_vault\"\n    keys {\n      name = \"azure_key\"\n      key_vault_key_id = \"https://myvault.vault.azure.net/keys/mykey/version\"\n    }\n  }\n}\n\n# GCP KMS\nencryption {\n  state {\n    method = \"gcp_kms\"\n    keys {\n      name = \"gcp_key\"\n      kms_crypto_key = \"projects/PROJECT_ID/locations/LOCATION/keyRings/RING/cryptoKeys/KEY\"\n    }\n  }\n}\n```\n\n### Best Practices\n\n1. **Store Keys Securely:**\n   ```bash\n   # Never commit keys\n   echo \"TOFU_ENCRYPTION_KEY=xxx\" >> .env\n   echo \".env\" >> .gitignore\n\n   # Use CI/CD secrets\n   # GitHub: Repository Settings  Secrets\n   # Azure DevOps: Pipeline  Variables  Secret\n   ```\n\n2. **Rotate Keys Regularly:**\n   ```bash\n   # Generate new key\n   NEW_KEY=$(openssl rand -base64 32)\n\n   # Add to fallback, update configs\n   # Migrate state\n   tofu init -migrate-state\n   ```\n\n3. **Backup Unencrypted State:**\n   ```bash\n   # Before enabling encryption\n   terraform state pull > backup-unencrypted.tfstate\n\n   # Enable encryption\n   tofu init -migrate-state\n\n   # Verify\n   tofu state pull  # Should be encrypted in backend\n   ```\n\n## Loop-able Import Blocks (OpenTofu 1.7+)\n\n**Terraform 1.5+ (Single Imports):**\n```hcl\nimport {\n  to = azurerm_resource_group.example\n  id = \"/subscriptions/.../resourceGroups/my-rg\"\n}\n```\n\n**OpenTofu 1.7+ (Loop Imports):**\n```hcl\n# Import multiple resource groups\nlocals {\n  resource_groups = {\n    \"rg1\" = \"/subscriptions/.../resourceGroups/rg1\"\n    \"rg2\" = \"/subscriptions/.../resourceGroups/rg2\"\n    \"rg3\" = \"/subscriptions/.../resourceGroups/rg3\"\n  }\n}\n\nimport {\n  for_each = local.resource_groups\n  to       = azurerm_resource_group.imported[each.key]\n  id       = each.value\n}\n\nresource \"azurerm_resource_group\" \"imported\" {\n  for_each = local.resource_groups\n  name     = each.key\n  location = \"eastus\"\n}\n```\n\n## Early Variable Evaluation (OpenTofu 1.7+)\n\n**Terraform 1.5.x:**\n```hcl\n# Variables NOT allowed in terraform block\nterraform {\n  required_version = \">= 1.5.0\"  # Static only\n\n  backend \"azurerm\" {\n    resource_group_name  = \"terraform-state\"  # Static only\n    storage_account_name = \"tfstate\"\n  }\n}\n```\n\n**OpenTofu 1.7+:**\n```hcl\n# Variables allowed in terraform block\nvariable \"environment\" {\n  type = string\n}\n\nterraform {\n  required_version = \">= 1.7.0\"\n\n  backend \"azurerm\" {\n    resource_group_name  = \"terraform-state-${var.environment}\"\n    storage_account_name = \"tfstate${var.environment}\"\n    key                  = \"${var.environment}.tfstate\"\n  }\n}\n```\n\n**OpenTofu 1.8+ (Module Sources):**\n```hcl\nvariable \"module_version\" {\n  type    = string\n  default = \"v1.0.0\"\n}\n\nmodule \"networking\" {\n  source  = \"git::https://github.com/org/module.git?ref=${var.module_version}\"\n  # Dynamic module version!\n}\n```\n\n## Practical Migration Examples\n\n### Example 1: Small Project Migration\n\n```bash\n# 1. Backup existing state\nterraform state pull > backup.tfstate\n\n# 2. Install OpenTofu\nbrew install opentofu\n\n# 3. Test compatibility\ntofu init\ntofu plan\n\n# 4. Switch to OpenTofu\nalias terraform=tofu  # Optional: maintain muscle memory\n\n# 5. Verify everything works\ntofu apply\n```\n\n### Example 2: Enterprise Migration with Encryption\n\n```bash\n# 1. Generate encryption key\nENCRYPTION_KEY=$(openssl rand -base64 32)\necho \"TOFU_ENCRYPTION_KEY=$ENCRYPTION_KEY\" >> .env.production\n\n# 2. Create encryption config\ncat > .tofu <<EOF\nencryption {\n  state {\n    method = \"aes_gcm\"\n    keys {\n      name = \"prod_key\"\n      passphrase = env.TOFU_ENCRYPTION_KEY\n    }\n  }\n\n  plan {\n    method = \"aes_gcm\"\n    keys {\n      name = \"prod_key\"\n      passphrase = env.TOFU_ENCRYPTION_KEY\n    }\n  }\n}\nEOF\n\n# 3. Migrate with encryption\nsource .env.production\ntofu init -migrate-state\n\n# 4. Verify encryption\ntofu state pull  # State is now encrypted in backend\n```\n\n### Example 3: CI/CD Migration\n\n```yaml\n# .github/workflows/terraform.yml\nname: Infrastructure\n\non: [push, pull_request]\n\njobs:\n  opentofu:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup OpenTofu\n        uses: opentofu/setup-opentofu@v1\n        with:\n          tofu_version: 1.8.0\n\n      - name: Init\n        run: tofu init\n        env:\n          TOFU_ENCRYPTION_KEY: ${{ secrets.TOFU_ENCRYPTION_KEY }}\n\n      - name: Plan\n        run: tofu plan\n        env:\n          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}\n          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}\n          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}\n          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}\n          TOFU_ENCRYPTION_KEY: ${{ secrets.TOFU_ENCRYPTION_KEY }}\n\n      - name: Apply\n        if: github.ref == 'refs/heads/main'\n        run: tofu apply -auto-approve\n        env:\n          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}\n          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}\n          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}\n          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}\n          TOFU_ENCRYPTION_KEY: ${{ secrets.TOFU_ENCRYPTION_KEY }}\n```\n\n## Command Compatibility\n\nAll Terraform commands work identically in OpenTofu (just replace `terraform` with `tofu`):\n\n```bash\n# Terraform          # OpenTofu\nterraform init       tofu init\nterraform plan       tofu plan\nterraform apply      tofu apply\nterraform destroy    tofu destroy\nterraform state      tofu state\nterraform import     tofu import\nterraform validate   tofu validate\nterraform fmt        tofu fmt\nterraform output     tofu output\n```\n\n## Community and Support\n\n**OpenTofu Community:**\n- GitHub: https://github.com/opentofu/opentofu\n- Slack: OpenTofu Workspace\n- Forum: OpenTofu Discussions\n- Registry: registry.opentofu.org\n\n**Terraform Community:**\n- Forum: HashiCorp Discuss\n- GitHub: hashicorp/terraform\n- Registry: registry.terraform.io\n- Support: HashiCorp Support Portal\n\n## Decision Matrix\n\n| Factor | Terraform | OpenTofu |\n|--------|-----------|----------|\n| **License** | BSL (Proprietary) | MPL 2.0 (Open Source) |\n| **State Encryption** | Via HCP Terraform (paid) | Built-in (free) |\n| **Enterprise Features** | HCP Terraform (Stacks, HYOK) | Community alternatives |\n| **Governance** | HashiCorp/IBM | Linux Foundation |\n| **Support** | Commercial support available | Community-driven |\n| **Innovation** | HCP-focused | Community-focused |\n| **Cost** | Free CLI, paid cloud | Completely free |\n| **Compatibility** | Forward-compatible | Terraform 1.5.x compatible |\n\n## Recommendations\n\n**Start with OpenTofu if:**\n- Building new infrastructure\n- No need for HCP Terraform features\n- Want state encryption without cloud costs\n- Prefer open-source tools\n- Budget-conscious\n\n**Stay with Terraform if:**\n- Using HCP Terraform Stacks\n- Need Sentinel policies\n- Require enterprise support\n- Want latest features first (1.10+)\n- Established HCP investment\n\n**Easy to Switch:**\n- Both are viable long-term\n- Migration takes < 1 hour for most projects\n- State files portable\n- Can evaluate both without commitment\n\nThis skill provides comprehensive OpenTofu knowledge for the terraform-expert agent."
              },
              {
                "name": "terraform-tasks",
                "description": "Specialized Terraform task execution skill for autonomous infrastructure operations. Handles code generation, debugging, version management (1.10-1.14+), security scanning, and architecture design across all providers (AWS 6.0, AzureRM 4.x, GCP) and platforms. Covers ephemeral values, Terraform Stacks, policy-as-code, and 2025 best practices.",
                "path": "plugins/terraform-master/skills/terraform-tasks/SKILL.md",
                "frontmatter": {
                  "name": "terraform-tasks",
                  "description": "Specialized Terraform task execution skill for autonomous infrastructure operations. Handles code generation, debugging, version management (1.10-1.14+), security scanning, and architecture design across all providers (AWS 6.0, AzureRM 4.x, GCP) and platforms. Covers ephemeral values, Terraform Stacks, policy-as-code, and 2025 best practices."
                },
                "content": "<!--\nProgressive Disclosure References:\n- references/aws-provider-6.md - AWS Provider 6.0 breaking changes and migration\n- references/azurerm-4.md - AzureRM 4.x features and migration\n- references/ephemeral-values.md - Terraform 1.10+ ephemeral values for secrets\n- references/terraform-stacks.md - Terraform Stacks (GA 2025) reference\n-->\n\n# Terraform Tasks Skill\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nThis skill enables autonomous execution of complex Terraform tasks with comprehensive provider knowledge and platform awareness.\n\n## Capabilities\n\n### 1. Infrastructure Code Generation\n\nGenerate complete, production-ready Terraform code for any cloud provider:\n\n**Process**:\n1. Determine provider and version from user context\n2. Research latest provider documentation if needed\n3. Generate complete configurations with:\n   - Provider version constraints\n   - Resource configurations\n   - Variables with validation\n   - Outputs\n   - Security best practices\n   - Platform-specific considerations\n\n**Example Tasks**:\n- \"Create Azure Storage Account with private endpoints and customer-managed keys\"\n- \"Generate AWS VPC with 3-tier architecture and NAT gateways\"\n- \"Build GCP GKE cluster with Workload Identity and node pools\"\n\n### 2. Version Management\n\nHandle Terraform and provider version upgrades:\n\n**Process**:\n1. Check current versions\n2. Research changelogs and breaking changes\n3. Propose upgrade path\n4. Generate migration code\n5. Provide testing strategy\n\n**Example Tasks**:\n- \"Upgrade from AzureRM provider 2.x to 3.x\"\n- \"Migrate Terraform 0.12 code to 1.x\"\n- \"Update all providers to latest compatible versions\"\n\n### 3. Debugging and Troubleshooting\n\nDiagnose and fix Terraform issues:\n\n**Process**:\n1. Gather diagnostic information\n2. Analyze error messages and logs\n3. Identify root cause\n4. Provide platform-specific solution\n5. Suggest preventive measures\n\n**Example Tasks**:\n- \"Debug state lock timeout on Windows\"\n- \"Fix provider authentication failure in Azure DevOps pipeline\"\n- \"Resolve circular dependency in module structure\"\n\n### 4. Security Scanning and Remediation\n\nScan and fix security issues:\n\n**Process**:\n1. Run security scanners (tfsec, Checkov)\n2. Analyze findings\n3. Prioritize issues\n4. Generate fixes\n5. Explain security implications\n\n**Example Tasks**:\n- \"Run tfsec and fix all HIGH severity issues\"\n- \"Ensure all S3 buckets have encryption enabled\"\n- \"Implement Azure storage account with all security best practices\"\n\n### 5. Architecture Review\n\nReview and improve Terraform architecture:\n\n**Process**:\n1. Analyze current structure\n2. Identify anti-patterns\n3. Propose improvements\n4. Generate refactoring plan\n5. Document decisions (ADRs)\n\n**Example Tasks**:\n- \"Review state management strategy for 500+ resources\"\n- \"Design multi-region architecture for high availability\"\n- \"Refactor monolithic state into layered approach\"\n\n### 6. CI/CD Pipeline Generation\n\nCreate complete CI/CD pipelines:\n\n**Process**:\n1. Determine CI/CD platform\n2. Understand environment strategy\n3. Generate pipeline configuration\n4. Include security scanning\n5. Add approval gates\n6. Implement drift detection\n\n**Example Tasks**:\n- \"Create Azure DevOps pipeline with multi-stage deployment\"\n- \"Generate GitHub Actions workflow with OIDC authentication\"\n- \"Build GitLab CI pipeline with Terraform Cloud backend\"\n\n### 7. Module Development\n\nCreate reusable Terraform modules:\n\n**Process**:\n1. Design module interface\n2. Implement with best practices\n3. Add variable validation\n4. Generate documentation\n5. Create examples\n6. Set up testing\n\n**Example Tasks**:\n- \"Create Azure networking module with hub-spoke pattern\"\n- \"Build AWS ECS module with auto-scaling and ALB\"\n- \"Develop GCP Cloud Run module with custom domains\"\n\n### 8. Migration Tasks\n\nMigrate infrastructure to Terraform:\n\n**Process**:\n1. Inventory existing resources\n2. Generate import commands\n3. Create matching Terraform code\n4. Validate configurations\n5. Test import process\n6. Plan cutover strategy\n\n**Example Tasks**:\n- \"Import existing Azure resources into Terraform\"\n- \"Migrate from CloudFormation to Terraform\"\n- \"Convert ARM templates to Terraform HCL\"\n\n## Autonomous Behavior\n\nThis skill operates autonomously with minimal user intervention:\n\n### Information Gathering\n- Automatically detect Terraform and provider versions\n- Identify platform (Windows/Linux/macOS)\n- Detect CI/CD environment\n- Check for existing configurations\n\n### Research\n- Use WebSearch to find current documentation\n- Check provider changelogs for breaking changes\n- Research best practices\n- Find platform-specific solutions\n\n### Code Generation\n- Generate complete, working code\n- Include all necessary files (main.tf, variables.tf, outputs.tf, etc.)\n- Add comprehensive comments\n- Follow naming conventions\n- Apply security best practices\n\n### Validation\n- Run terraform fmt on generated code\n- Validate syntax\n- Check for security issues\n- Test configurations when possible\n\n### Documentation\n- Explain architectural decisions\n- Document usage examples\n- Note version compatibility\n- Include troubleshooting tips\n\n## Error Handling\n\nWhen encountering issues:\n\n1. **Gather Context**: Collect all relevant information\n2. **Research**: Look up error messages and solutions\n3. **Platform Awareness**: Consider OS-specific issues\n4. **Multiple Solutions**: Provide alternatives when available\n5. **Prevention**: Suggest how to avoid similar issues\n\n## Platform-Specific Considerations\n\n### Windows\n- PowerShell syntax for commands\n- Path handling (backslashes)\n- Line ending considerations\n- Execution policy issues\n- Credential management\n\n### Linux/macOS\n- Bash syntax for commands\n- File permissions\n- Package managers\n- Environment variables\n\n### CI/CD Environments\n- Pipeline-specific syntax\n- Agent capabilities\n- Authentication methods\n- Artifact handling\n\n## Quality Standards\n\nAll generated Terraform code must:\n- Be properly formatted (terraform fmt)\n- Pass validation (terraform validate)\n- Include security best practices\n- Have comprehensive variable validation\n- Include meaningful descriptions\n- Follow naming conventions\n- Be version-compatible\n- Include usage examples\n- Have proper output definitions\n\n## Provider Expertise\n\n### Azure (AzureRM)\n- All resource types\n- AzAPI for preview features\n- Service endpoints and private endpoints\n- Managed identities\n- RBAC and policies\n\n### AWS\n- All services\n- IAM roles and policies\n- VPC networking\n- S3 backend configuration\n- Cross-account deployments\n\n### Google Cloud\n- All GCP services\n- IAM and service accounts\n- VPC and networking\n- GCS backend configuration\n- Organization and folder policies\n\n### Community Providers\n- Kubernetes and Helm\n- Datadog, PagerDuty\n- GitHub, GitLab\n- HashiCorp Vault\n- And more...\n\n## Examples\n\n### Example 1: Generate Azure Storage Account\n\n**User Request**: \"Create an Azure Storage Account with all security best practices\"\n\n**Skill Actions**:\n1. Detect/ask for AzureRM provider version\n2. Research latest security requirements\n3. Generate complete configuration:\n   - Storage account with secure settings\n   - Private endpoint\n   - Diagnostic settings\n   - Customer-managed encryption keys\n   - Network rules\n   - Variables with validation\n   - Outputs\n4. Include usage examples\n5. Add security scanner ignore comments where appropriate with explanations\n\n### Example 2: Debug Plan Failure\n\n**User Request**: \"My terraform plan is failing with authentication error on Windows\"\n\n**Skill Actions**:\n1. Ask for error details\n2. Identify it's Azure CLI authentication\n3. Provide Windows-specific solution:\n   - PowerShell commands to check authentication\n   - How to refresh credentials\n   - Environment variable configuration\n   - Alternative authentication methods\n4. Explain root cause\n5. Suggest prevention\n\n### Example 3: Architecture Review\n\n**User Request**: \"Review my Terraform structure, I have 1000+ resources in one state file\"\n\n**Skill Actions**:\n1. Analyze current structure\n2. Identify issues:\n   - Large state file\n   - Slow operations\n   - Large blast radius\n3. Propose layered architecture:\n   - Split into foundation/platform/apps\n   - Separate state files\n   - Remote state data sources\n4. Generate migration plan\n5. Create ADR documenting decision\n6. Provide implementation steps\n\n## Integration with terraform-expert Agent\n\nThis skill works in tandem with the terraform-expert agent:\n- Agent provides strategic guidance\n- Skill executes tactical tasks\n- Agent validates skill outputs\n- Skill reports back to agent\n\nUse this skill when you need to autonomously execute Terraform tasks with comprehensive provider knowledge and platform awareness."
              }
            ]
          },
          {
            "name": "ssdt-master",
            "description": "Complete SQL Server Data Tools (SSDT) expertise system across ALL platforms. PROACTIVELY activate for: (1) ANY SSDT task (database projects/SqlPackage/schema compare), (2) SDK-style and legacy database project creation/migration, (3) DACPAC/BACPAC build and deployment operations, (4) SqlPackage command-line operations (publish/extract/export/import), (5) Schema and data comparison workflows, (6) Database refactoring operations (rename/schema changes/data transformations), (7) MSBuild integration and CI/CD pipeline setup, (8) Visual Studio database project troubleshooting, (9) Cross-platform CLI tools (dotnet/sqlpackage), (10) Deployment script generation and validation. Provides: comprehensive SqlPackage reference with all actions and parameters, SDK-style project structure expertise, legacy project migration patterns, schema compare functionality, deployment option mastery, refactoring best practices, CI/CD integration patterns (GitHub Actions/Azure DevOps), MSBuild property configuration, and version-specific guidance. Ensures production-ready, secure, efficient database development following Microsoft SQL Server best practices.",
            "source": "./plugins/ssdt-master",
            "category": null,
            "version": "1.6.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install ssdt-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "sql-server-2025",
                "description": "SQL Server 2025 and SqlPackage 170.2.70 (October 2025) - Vector databases, AI integration, and latest features",
                "path": "plugins/ssdt-master/skills/sql-server-2025/SKILL.md",
                "frontmatter": {
                  "name": "sql-server-2025",
                  "description": "SQL Server 2025 and SqlPackage 170.2.70 (October 2025) - Vector databases, AI integration, and latest features"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# SQL Server 2025 & SqlPackage 170.2.70 Support\n\n## Overview\n\n**SQL Server 2025** is the enterprise AI-ready database with native vector database capabilities, built-in AI model integration, and semantic search from ground to cloud.\n\n**SqlPackage 170.2.70** (October 14, 2025) - Latest production release with full SQL Server 2025 support, data virtualization, and parquet file enhancements.\n\n## SqlPackage 170.x Series (2025 Releases)\n\n### Latest Version: 170.2.70 (October 14, 2025)\n\nThree major 2025 releases:\n- **170.2.70** - October 14, 2025 (Current)\n- **170.1.61** - July 30, 2025 (Data virtualization)\n- **170.0.94** - April 15, 2025 (SQL Server 2025 initial support)\n\n### Key 2025 Features\n\n**Data Virtualization (170.1.61+)**:\n- Support for Azure SQL Database data virtualization objects\n- Import/export/extract/publish operations for external data sources\n- Parquet file support for Azure SQL Database with Azure Blob Storage\n- Automatic fallback to BCP for CLR types and LOBs > 1MB\n\n**New Data Types**:\n- **VECTOR** - Up to 3,996 dimensions with half-precision (2-byte) floating-point\n- **JSON** - Native JSON data type for Azure SQL Database\n\n**New Permissions (170.0+)**:\n- `ALTER ANY INFORMATION PROTECTION` - SQL Server 2025 & Azure SQL\n- `ALTER ANY EXTERNAL MIRROR` - Azure SQL & SQL database in Fabric\n- `CREATE/ALTER ANY EXTERNAL MODEL` - AI/ML model management\n\n**Deployment Options**:\n- `/p:IgnorePreDeployScript=True/False` - Skip pre-deployment scripts\n- `/p:IgnorePostDeployScript=True/False` - Skip post-deployment scripts\n\n### SqlPackage Commands\n\n```bash\n# Publish to SQL Server 2025\nsqlpackage /Action:Publish \\\n  /SourceFile:Database.dacpac \\\n  /TargetServerName:server2025.database.windows.net \\\n  /TargetDatabaseName:MyDatabase \\\n  /TargetDatabaseEdition:Premium \\\n  /p:TargetPlatform=SqlServer2025  # New target platform\n\n# Extract from SQL Server 2025\nsqlpackage /Action:Extract \\\n  /SourceServerName:server2025.database.windows.net \\\n  /SourceDatabaseName:MyDatabase \\\n  /TargetFile:Database.dacpac \\\n  /p:ExtractAllTableData=False \\\n  /p:VerifyExtraction=True\n\n# Export with SQL Server 2025 features\nsqlpackage /Action:Export \\\n  /SourceServerName:server2025.database.windows.net \\\n  /SourceDatabaseName:MyDatabase \\\n  /TargetFile:Database.bacpac\n```\n\n## ScriptDom Version 170.0.64\n\nNew ScriptDom version for SQL Server 2025 syntax parsing:\n\n```csharp\n// Package: Microsoft.SqlServer.TransactSql.ScriptDom 170.0.64\n\nusing Microsoft.SqlServer.TransactSql.ScriptDom;\n\n// Parse SQL Server 2025 syntax\nvar parser = new TSql170Parser(true);\nIList<ParseError> errors;\nvar fragment = parser.Parse(new StringReader(sql), out errors);\n\n// Supports SQL Server 2025 new T-SQL features\n```\n\n## Microsoft.Build.Sql 2.0.0 GA (2025)\n\n**MAJOR MILESTONE:** Microsoft.Build.Sql SDK entered General Availability in 2025!\n\n### Latest Version: 2.0.0 (Production Ready)\n\n**Breaking Change from Preview:**\n- SDK is now production-ready and recommended for all new database projects\n- No longer in preview status\n- Full cross-platform support (Windows/Linux/macOS)\n- Requires .NET 8+ (was .NET 6+ in preview)\n\n### SQL Server 2025 Support\n\n**Current Status:** SQL Server 2025 target platform support coming in future Microsoft.Build.Sql release (post-2.0.0).\n\n**Workaround for SDK-Style Projects:**\n```xml\n<!-- Database.sqlproj (SDK-style with SQL Server 2025 compatibility) -->\n<Project Sdk=\"Microsoft.Build.Sql/2.0.0\">\n  <PropertyGroup>\n    <Name>MyDatabase</Name>\n    <!-- Use SQL Server 2022 (160) provider until 2025 provider available -->\n    <DSP>Microsoft.Data.Tools.Schema.Sql.Sql160DatabaseSchemaProvider</DSP>\n    <TargetFramework>net8.0</TargetFramework>\n    <SqlServerVersion>Sql160</SqlServerVersion>\n\n    <!-- SQL Server 2025 features will still work in runtime database -->\n    <!-- Only build-time validation uses Sql160 provider -->\n  </PropertyGroup>\n\n  <ItemGroup>\n    <Folder Include=\"Tables\\\" />\n    <Folder Include=\"Views\\\" />\n    <Folder Include=\"StoredProcedures\\\" />\n  </ItemGroup>\n</Project>\n```\n\n### Visual Studio 2022 Support\n\n**Requirement:** Visual Studio 2022 version 17.12 or later for SDK-style SQL projects.\n\n**Note:** Side-by-side installation with original SQL projects (legacy SSDT) is NOT supported.\n\n## SQL Server 2025 Release Status\n\n**Current Status**: SQL Server 2025 (17.x) is in **Release Candidate (RC1)** stage as of October 2025. Public preview began May 2025.\n\n**Predicted GA Date**: November 12, 2025 (based on historical release patterns - SQL Server 2019: Nov 4, SQL Server 2022: Nov 16). Expected announcement at Microsoft Ignite conference (November 18-21, 2025).\n\n**Not Yet Production**: SQL Server 2025 is not yet generally available. All features described are available in RC builds for testing purposes only.\n\n## SQL Server 2025 New Features\n\n### Vector Database for AI\n\n**Native Enterprise Vector Store** with built-in security, compliance, and DiskANN indexing technology.\n\n**Key Capabilities:**\n- **Up to 3,996 dimensions** per vector (half-precision 2-byte floating-point)\n- **DiskANN indexing** - Disk-based approximate nearest neighbor for efficient large-scale vector search\n- **Hybrid AI vector search** - Combine vectors with SQL data for semantic + keyword search\n- **Built-in security & compliance** - Enterprise-grade data protection\n\n**Vector Embedding & Text Chunking:**\n```sql\n-- Create table with vector column\nCREATE TABLE Documents (\n    Id INT PRIMARY KEY IDENTITY,\n    Title NVARCHAR(200),\n    Content NVARCHAR(MAX),\n    -- Half-precision vectors support up to 3,996 dimensions\n    ContentVector VECTOR(1536)  -- OpenAI ada-002: 1,536 dims\n    -- ContentVector VECTOR(3072)  -- OpenAI text-embedding-3-large: 3,072 dims\n    -- ContentVector VECTOR(3996)  -- Maximum: 3,996 dims\n);\n\n-- Insert vectors (T-SQL built-in embedding generation)\nINSERT INTO Documents (Title, Content, ContentVector)\nVALUES (\n    'AI Documentation',\n    'Azure AI services...',\n    CAST('[0.1, 0.2, 0.3, ...]' AS VECTOR(1536))\n);\n\n-- Semantic similarity search with DiskANN\nDECLARE @QueryVector VECTOR(1536) = CAST('[0.15, 0.25, ...]' AS VECTOR(1536));\n\nSELECT TOP 10\n    Id,\n    Title,\n    Content,\n    VECTOR_DISTANCE('cosine', ContentVector, @QueryVector) AS Similarity\nFROM Documents\nORDER BY Similarity;\n\n-- Create DiskANN vector index for performance\nCREATE INDEX IX_Documents_Vector\nON Documents(ContentVector)\nUSING VECTOR_INDEX\nWITH (\n    DISTANCE_METRIC = 'cosine',  -- or 'euclidean', 'dot_product'\n    VECTOR_SIZE = 1536\n);\n\n-- Hybrid search: Combine vector similarity with traditional filtering\nSELECT TOP 10\n    Id,\n    Title,\n    VECTOR_DISTANCE('cosine', ContentVector, @QueryVector) AS Similarity\nFROM Documents\nWHERE Title LIKE '%Azure%'  -- Traditional keyword filter\nORDER BY Similarity;\n```\n\n### AI Model Integration\n\n**Built into T-SQL** - Seamlessly integrate AI services with model definitions directly in the database.\n\n**Supported AI Services:**\n- Azure AI Foundry\n- Azure OpenAI Service\n- OpenAI\n- Ollama (local/self-hosted models)\n- Custom REST APIs\n\n**Developer Frameworks:**\n- LangChain integration\n- Semantic Kernel integration\n- Entity Framework Core support\n- **GraphQL via Data API Builder (DAB)** - Expose SQL Server data through GraphQL endpoints\n\n**External Models (ONNX):**\n```sql\n-- Create external model from ONNX file\nCREATE EXTERNAL MODEL AIModel\nFROM 'https://storage.account.blob.core.windows.net/models/model.onnx'\nWITH (\n    TYPE = 'ONNX',\n    INPUT_DATA_FORMAT = 'JSON',\n    OUTPUT_DATA_FORMAT = 'JSON'\n);\n\n-- Use model for predictions\nDECLARE @Input NVARCHAR(MAX) = '{\"text\": \"Hello world\"}';\nSELECT PREDICT(MODEL = AIModel, DATA = @Input) AS Prediction;\n\n-- Grant model permissions (new SQL Server 2025 permission)\nGRANT CREATE ANY EXTERNAL MODEL TO [ModelAdmin];\nGRANT ALTER ANY EXTERNAL MODEL TO [ModelAdmin];\nGRANT EXECUTE ON EXTERNAL MODEL::AIModel TO [AppUser];\n```\n\n**AI Service Integration:**\n```sql\n-- Example: Azure OpenAI integration\n-- Model definitions built directly into T-SQL\n-- Access through REST APIs with built-in authentication\n```\n\n### Optimized Locking (Performance Enhancement)\n\n**Key Innovation**: Dramatically reduces lock memory consumption and minimizes blocking for concurrent transactions.\n\n**Two Primary Components**:\n\n1. **Transaction ID (TID) Locking**:\n   - Each row labeled with last TID (Transaction ID) that modified it\n   - Single lock on TID instead of many row locks\n   - Locks released as soon as row is updated\n   - Only one TID lock held until transaction ends\n   - **Example**: Updating 1,000 rows requires 1,000 X row locks, but each is released immediately, and only one TID lock is held until commit\n\n2. **Lock After Qualification (LAQ)**:\n   - Evaluates query predicates using latest committed version WITHOUT acquiring lock\n   - Requires READ COMMITTED SNAPSHOT ISOLATION (RCSI)\n   - Predicates checked optimistically on committed data\n   - X row lock taken only if predicate satisfied\n   - Lock released immediately after row update\n\n**Benefits**:\n- Reduced lock memory usage\n- Increased concurrency and scale\n- Minimized lock escalation\n- Enhanced application uptime\n- Better performance for high-concurrency workloads\n\n**Enabling Optimized Locking**:\n```sql\n-- Enable RCSI (required for LAQ)\nALTER DATABASE MyDatabase\nSET READ_COMMITTED_SNAPSHOT ON;\n\n-- Optimized locking is automatically enabled at database level\n-- No additional configuration needed for SQL Server 2025\n\n-- Verify optimized locking status\nSELECT name, is_read_committed_snapshot_on\nFROM sys.databases\nWHERE name = 'MyDatabase';\n\n-- Monitor optimized locking performance\nSELECT *\nFROM sys.dm_tran_locks\nWHERE request_session_id = @@SPID;\n```\n\n### Microsoft Fabric Mirroring (Zero-ETL Analytics)\n\n**Integration**: Near real-time replication of SQL Server databases to Microsoft Fabric OneLake for analytics.\n\n**Key Capabilities**:\n- **Zero-ETL Experience**: No complex ETL pipelines required\n- **SQL Server 2025-Specific**: Uses new change feed technology (vs CDC in SQL Server 2016-2022)\n- **Azure Arc Required**: SQL Server 2025 requires Azure Arc-enabled server for Fabric communication\n- **Real-Time Analytics**: Offload analytic workloads to Fabric without impacting production\n\n**Supported Scenarios**:\n- SQL Server 2025 on-premises (Windows)\n- NOT supported: Azure VM or Linux instances (yet)\n\n**How It Works**:\n```sql\n-- SQL Server 2025 uses change feed (automatic)\n-- Azure Arc agent handles replication to Fabric OneLake\n\n-- Traditional SQL Server 2016-2022 approach (CDC):\n-- EXEC sys.sp_cdc_enable_db;\n-- EXEC sys.sp_cdc_enable_table ...\n\n-- SQL Server 2025: Change feed is built-in, no CDC setup needed\n```\n\n**Benefits**:\n- Free Fabric compute for replication\n- Free OneLake storage (based on capacity size)\n- Near real-time data availability\n- BI and analytics without production load\n- Integration with Power BI, Synapse, Azure ML\n\n**Configuration**:\n1. Enable Azure Arc on SQL Server 2025 instance\n2. Configure Fabric workspace and OneLake\n3. Enable mirroring in Fabric portal\n4. Select database and tables to mirror\n5. Data automatically replicated with change feed\n\n**Monitoring**:\n```sql\n-- Monitor replication lag\nSELECT\n    database_name,\n    table_name,\n    last_sync_time,\n    rows_replicated,\n    replication_lag_seconds\nFROM sys.dm_fabric_replication_status;\n```\n\n### Native JSON Support Enhancements\n\n**New JSON Data Type**: Native JSON data type for Azure SQL Database (coming to SQL Server 2025).\n\n```sql\n-- New JSON data type\nCREATE TABLE Products (\n    Id INT PRIMARY KEY,\n    Name NVARCHAR(100),\n    Metadata JSON  -- Native JSON type\n);\n\n-- JSON functions enhanced\nINSERT INTO Products (Id, Name, Metadata)\nVALUES (1, 'Laptop', JSON('{\"brand\": \"Dell\", \"ram\": 16, \"ssd\": 512}'));\n\n-- Query JSON with improved performance\nSELECT\n    Id,\n    Name,\n    JSON_VALUE(Metadata, '$.brand') AS Brand,\n    JSON_VALUE(Metadata, '$.ram') AS RAM\nFROM Products;\n```\n\n### Regular Expression (RegEx) Support\n\n**T-SQL RegEx Functions**: Validate, search, and manipulate strings with regular expressions.\n\n```sql\n-- RegEx matching\nSELECT REGEXP_LIKE('test@example.com', '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$') AS IsValidEmail;\n\n-- RegEx replace\nSELECT REGEXP_REPLACE('Phone: 555-1234', '\\d+', 'XXX') AS MaskedPhone;\n\n-- RegEx extract\nSELECT REGEXP_SUBSTR('Order #12345', '\\d+') AS OrderNumber;\n```\n\n### REST API Integration\n\n**Built-in REST Capabilities**: Call external REST APIs directly from T-SQL.\n\n```sql\n-- Call REST API from T-SQL\nDECLARE @Response NVARCHAR(MAX);\n\nEXEC sp_invoke_external_rest_endpoint\n    @url = 'https://api.example.com/data',\n    @method = 'GET',\n    @headers = '{\"Authorization\": \"Bearer token123\"}',\n    @response = @Response OUTPUT;\n\nSELECT @Response AS APIResponse;\n\n-- Enrich database data with external APIs\nUPDATE Customers\nSET EnrichedData = (\n    SELECT JSON_VALUE(response, '$.data')\n    FROM OPENROWSET(REST, 'https://api.example.com/customer/' + CustomerId)\n)\nWHERE CustomerId = 12345;\n```\n\n### Optional Parameter Plan Optimization (OPPO)\n\n**Performance Enhancement**: SQL Server 2025 introduces OPPO to enable optimal execution plan selection based on customer-provided runtime parameter values.\n\n**Key Benefits:**\n- Solves parameter sniffing issues\n- Optimizes plans for specific runtime parameters\n- Improves query performance with parameter-sensitive workloads\n- Reduces need for query hints or plan guides\n\n**Enabling OPPO:**\n```sql\n-- Enable at database level\nALTER DATABASE MyDatabase\nSET PARAMETER_SENSITIVE_PLAN_OPTIMIZATION = ON;\n\n-- Check status\nSELECT name, is_parameter_sensitive_plan_optimization_on\nFROM sys.databases\nWHERE name = 'MyDatabase';\n\n-- Monitor OPPO usage\nSELECT\n    query_plan_hash,\n    parameter_values,\n    execution_count,\n    avg_duration_ms\nFROM sys.dm_exec_query_stats\nWHERE is_parameter_sensitive = 1;\n```\n\n### Microsoft Entra Managed Identities\n\n**Security Enhancement**: SQL Server 2025 adds support for Microsoft Entra managed identities for improved credential management.\n\n**Key Benefits:**\n- Eliminates hardcoded credentials\n- Reduces security vulnerabilities\n- Provides compliance and auditing capabilities\n- Simplifies credential rotation\n\n**Configuration:**\n```sql\n-- Create login with managed identity\nCREATE LOGIN [managed-identity-name] FROM EXTERNAL PROVIDER;\n\n-- Grant permissions\nCREATE USER [managed-identity-name] FOR LOGIN [managed-identity-name];\nGRANT CONTROL ON DATABASE::MyDatabase TO [managed-identity-name];\n\n-- Use in connection strings\n-- Connection string: Server=myserver;Database=mydb;Authentication=Active Directory Managed Identity;\n```\n\n### Enhanced Information Protection\n\nSensitivity classification and encryption:\n\n```sql\n-- Classify sensitive columns\nADD SENSITIVITY CLASSIFICATION TO\n    Customers.Email,\n    Customers.CreditCard\nWITH (\n    LABEL = 'Confidential',\n    INFORMATION_TYPE = 'Financial'\n);\n\n-- Query classification\nSELECT\n    schema_name(o.schema_id) AS SchemaName,\n    o.name AS TableName,\n    c.name AS ColumnName,\n    s.label AS SensitivityLabel,\n    s.information_type AS InformationType\nFROM sys.sensitivity_classifications s\nINNER JOIN sys.objects o ON s.major_id = o.object_id\nINNER JOIN sys.columns c ON s.major_id = c.object_id AND s.minor_id = c.column_id;\n```\n\n## Deployment to SQL Server 2025\n\n### Using SqlPackage\n\n```bash\n# Publish with 2025 features\nsqlpackage /Action:Publish \\\n  /SourceFile:Database.dacpac \\\n  /TargetConnectionString:\"Server=tcp:server2025.database.windows.net;Database=MyDb;Authentication=ActiveDirectoryManagedIdentity;\" \\\n  /p:BlockOnPossibleDataLoss=True \\\n  /p:IncludeCompositeObjects=True \\\n  /p:DropObjectsNotInSource=False \\\n  /p:DoNotDropObjectTypes=Users;RoleMembership \\\n  /p:GenerateSmartDefaults=True \\\n  /DiagnosticsFile:deploy.log\n```\n\n### Using MSBuild\n\n```xml\n<!-- Database.publish.xml -->\n<Project>\n  <PropertyGroup>\n    <TargetConnectionString>Server=tcp:server2025.database.windows.net;Database=MyDb;Authentication=ActiveDirectoryManagedIdentity;</TargetConnectionString>\n    <BlockOnPossibleDataLoss>True</BlockOnPossibleDataLoss>\n    <TargetDatabaseName>MyDatabase</TargetDatabaseName>\n    <ProfileVersionNumber>1</ProfileVersionNumber>\n  </PropertyGroup>\n\n  <ItemGroup>\n    <SqlCmdVariable Include=\"Environment\">\n      <Value>Production</Value>\n    </SqlCmdVariable>\n  </ItemGroup>\n</Project>\n```\n\n```bash\n# Deploy using MSBuild\nmsbuild Database.sqlproj \\\n  /t:Publish \\\n  /p:PublishProfile=Database.publish.xml \\\n  /p:TargetPlatform=SqlServer2025\n```\n\n## CI/CD Best Practices 2025\n\n### Key Principles\n\n**State-Based Deployment (Recommended):**\n- Source code represents current database state\n- All objects (procedures, tables, triggers, views) in separate .sql files\n- SqlPackage generates incremental scripts automatically\n- Preferred over migration-based approaches\n\n**Testing & Quality:**\n- **tSQLt** - Unit testing for SQL Server stored procedures and functions\n- Tests produce machine-readable results\n- Abort pipeline on test failure with immediate notifications\n- Never continue deployment if tests fail\n\n**Security:**\n- **Windows Authentication preferred** for CI/CD (avoid plain text passwords)\n- Never commit credentials to source control\n- Use Azure Key Vault or GitHub Secrets for connection strings\n\n**Version Control:**\n- All database objects in source control\n- Test scripts versioned and executed in Build step\n- Require comments on check-ins\n- Configure custom check-in policies\n\n### GitHub Actions (2025 Pattern)\n\n```yaml\nname: Deploy to SQL Server 2025\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build-and-test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Setup .NET 8\n      uses: actions/setup-dotnet@v4\n      with:\n        dotnet-version: '8.0.x'\n\n    - name: Install SqlPackage 170.2.70\n      run: dotnet tool install -g Microsoft.SqlPackage --version 170.2.70\n\n    - name: Build DACPAC\n      run: dotnet build Database.sqlproj -c Release\n\n    - name: Run tSQLt Unit Tests\n      run: |\n        # Run unit tests and capture results\n        # Abort if tests fail\n        echo \"Running tSQLt unit tests...\"\n        # Add your tSQLt test execution here\n\n    - name: Generate Deployment Report\n      run: |\n        sqlpackage /Action:DeployReport \\\n          /SourceFile:bin/Release/Database.dacpac \\\n          /TargetConnectionString:\"${{ secrets.SQL_CONNECTION_STRING }}\" \\\n          /OutputPath:deploy-report.xml \\\n          /p:BlockOnPossibleDataLoss=True\n\n    - name: Publish to SQL Server 2025\n      run: |\n        sqlpackage /Action:Publish \\\n          /SourceFile:bin/Release/Database.dacpac \\\n          /TargetConnectionString:\"${{ secrets.SQL_CONNECTION_STRING }}\" \\\n          /p:TargetPlatform=SqlServer2025 \\\n          /p:BlockOnPossibleDataLoss=True \\\n          /DiagnosticsFile:publish.log \\\n          /DiagnosticsLevel:Verbose\n\n    - name: Upload Artifacts\n      if: always()\n      uses: actions/upload-artifact@v4\n      with:\n        name: deployment-logs\n        path: |\n          publish.log\n          deploy-report.xml\n```\n\n### Azure DevOps\n\n```yaml\ntrigger:\n- main\n\npool:\n  vmImage: 'windows-2022'\n\nsteps:\n- task: MSBuild@1\n  displayName: 'Build Database Project'\n  inputs:\n    solution: 'Database.sqlproj'\n    configuration: 'Release'\n\n- task: SqlAzureDacpacDeployment@1\n  displayName: 'Deploy to SQL Server 2025'\n  inputs:\n    azureSubscription: 'Azure Subscription'\n    authenticationType: 'servicePrincipal'\n    serverName: 'server2025.database.windows.net'\n    databaseName: 'MyDatabase'\n    deployType: 'DacpacTask'\n    deploymentAction: 'Publish'\n    dacpacFile: '$(Build.SourcesDirectory)/bin/Release/Database.dacpac'\n    additionalArguments: '/p:TargetPlatform=SqlServer2025'\n```\n\n## New SqlPackage Diagnostic Features\n\n```bash\n# Enable detailed diagnostics\nsqlpackage /Action:Publish \\\n  /SourceFile:Database.dacpac \\\n  /TargetServerName:server2025.database.windows.net \\\n  /TargetDatabaseName:MyDatabase \\\n  /DiagnosticsLevel:Verbose \\\n  /DiagnosticPackageFile:diagnostics.zip\n\n# Creates diagnostics.zip containing:\n# - Deployment logs\n# - Performance metrics\n# - Error details\n# - Schema comparison results\n```\n\n## Microsoft Fabric Data Warehouse Support\n\n**New in SqlPackage 162.5+:** Full support for SQL database in Microsoft Fabric.\n\n**Fabric Deployment:**\n```bash\n# Deploy to Fabric Warehouse\nsqlpackage /Action:Publish \\\n  /SourceFile:Warehouse.dacpac \\\n  /TargetConnectionString:\"Server=tcp:myworkspace.datawarehouse.fabric.microsoft.com;Database=mywarehouse;Authentication=ActiveDirectoryInteractive;\" \\\n  /p:DatabaseEdition=Fabric \\\n  /p:DatabaseServiceObjective=SqlDbFabricDatabaseSchemaProvider\n\n# Extract from Fabric\nsqlpackage /Action:Extract \\\n  /SourceConnectionString:\"Server=tcp:myworkspace.datawarehouse.fabric.microsoft.com;Database=mywarehouse;Authentication=ActiveDirectoryInteractive;\" \\\n  /TargetFile:Fabric.dacpac\n\n# New permission: ALTER ANY EXTERNAL MIRROR (Fabric-specific)\nGRANT ALTER ANY EXTERNAL MIRROR TO [FabricAdmin];\n```\n\n## Best Practices for SQL Server 2025\n\n1. **Use Target Platform Specification:**\n```xml\n<PropertyGroup>\n  <TargetPlatform>SqlServer2025</TargetPlatform>\n</PropertyGroup>\n```\n\n2. **Test Vector Operations:**\n```sql\n-- Verify vector support\nSELECT SERVERPROPERTY('IsVectorSupported') AS VectorSupport;\n```\n\n3. **Monitor AI Model Performance:**\n```sql\n-- Track model execution\nSELECT\n    model_name,\n    AVG(execution_time_ms) AS AvgExecutionTime,\n    COUNT(*) AS ExecutionCount\nFROM sys.dm_exec_external_model_stats\nGROUP BY model_name;\n```\n\n4. **Implement Sensitivity Classification:**\n```sql\n-- Classify all PII columns\nADD SENSITIVITY CLASSIFICATION TO dbo.Customers.Email\nWITH (LABEL = 'Confidential - GDPR', INFORMATION_TYPE = 'Email');\n```\n\n## Resources\n\n- [SQL Server 2025 Preview](https://aka.ms/sqlserver2025)\n- [SqlPackage Documentation](https://learn.microsoft.com/sql/tools/sqlpackage/)\n- [SDK-Style Projects](https://learn.microsoft.com/sql/tools/sql-database-projects/concepts/sdk-style-projects)\n- [Vector Database](https://learn.microsoft.com/sql/relational-databases/vectors/)"
              },
              {
                "name": "ssdt-cicd-best-practices-2025",
                "description": "Modern CI/CD best practices for SQL Server database development with tSQLt, state-based deployment, and 2025 patterns",
                "path": "plugins/ssdt-master/skills/ssdt-cicd-best-practices-2025/SKILL.md",
                "frontmatter": {
                  "name": "ssdt-cicd-best-practices-2025",
                  "description": "Modern CI/CD best practices for SQL Server database development with tSQLt, state-based deployment, and 2025 patterns"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# SSDT CI/CD Best Practices 2025\n\n## Overview\n\nThis skill provides comprehensive guidance on implementing modern CI/CD pipelines for SQL Server database projects using SSDT, SqlPackage, and contemporary DevOps practices.\n\n## Key Principles (2025 Recommended Approach)\n\n### 1. State-Based Deployment (Recommended)\n\n**Definition**: Source code represents the current database state, not migration scripts.\n\n**How it Works**:\n- All database objects (tables, procedures, views, functions) stored in separate .sql files\n- SqlPackage automatically generates incremental deployment scripts\n- Declarative approach: \"This is what the database should look like\"\n- SSDT compares source to target and calculates differences\n\n**Advantages**:\n- Easier to maintain and understand\n- No risk of missing migration scripts\n- Git history shows complete object definitions\n- Branching and merging simplified\n- Rollback by redeploying previous version\n\n**Implementation**:\n```yaml\n# GitHub Actions example\n- name: Build DACPAC (State-Based)\n  run: dotnet build Database.sqlproj -c Release\n\n- name: Deploy State to Target\n  run: |\n    sqlpackage /Action:Publish \\\n      /SourceFile:Database.dacpac \\\n      /TargetConnectionString:\"${{ secrets.SQL_CONN }}\" \\\n      /p:BlockOnPossibleDataLoss=True\n```\n\n**Contrast with Migration-Based**:\n- Migration-based: Sequential scripts (001_CreateTable.sql, 002_AddColumn.sql)\n- State-based: Object definitions (Tables/Customer.sql contains complete CREATE TABLE)\n\n### 2. tSQLt Unit Testing (Critical for CI/CD)\n\n**Why tSQLt**:\n- Open-source SQL Server unit testing framework\n- Write tests in T-SQL language\n- Produces machine-readable XML/JSON results\n- Integrates seamlessly with CI/CD pipelines\n\n**Key Features**:\n- **Automatic Transactions**: Each test runs in a transaction and rolls back\n- **Schema Grouping**: Group related tests in schemas\n- **Mocking**: Fake tables and procedures for isolated testing\n- **Assertions**: Built-in assertion methods (assertEquals, assertEmpty, etc.)\n\n**Pipeline Abort on Failure**:\n```yaml\n# GitHub Actions with tSQLt\n- name: Run tSQLt Unit Tests\n  run: |\n    # Deploy test framework\n    sqlpackage /Action:Publish \\\n      /SourceFile:DatabaseTests.dacpac \\\n      /TargetConnectionString:\"${{ secrets.TEST_SQL_CONN }}\"\n\n    # Execute tests and capture results\n    sqlcmd -S test-server -d TestDB -Q \"EXEC tSQLt.RunAll\" -o test-results.txt\n\n    # Parse results and fail pipeline if tests fail\n    if grep -q \"Failure\" test-results.txt; then\n      echo \"Unit tests failed!\"\n      exit 1\n    fi\n\n    echo \"All tests passed!\"\n\n- name: Deploy to Production (only runs if tests pass)\n  run: |\n    sqlpackage /Action:Publish \\\n      /SourceFile:Database.dacpac \\\n      /TargetConnectionString:\"${{ secrets.PROD_SQL_CONN }}\"\n```\n\n**Test Structure**:\n```sql\n-- tSQLt test example\nCREATE SCHEMA CustomerTests;\nGO\n\nCREATE PROCEDURE CustomerTests.[test Customer Insert Sets Correct Defaults]\nAS\nBEGIN\n    -- Arrange\n    EXEC tSQLt.FakeTable 'dbo.Customers';\n\n    -- Act\n    INSERT INTO dbo.Customers (FirstName, LastName, Email)\n    VALUES ('John', 'Doe', 'john@example.com');\n\n    -- Assert\n    EXEC tSQLt.AssertEquals @Expected = 1,\n                             @Actual = (SELECT COUNT(*) FROM dbo.Customers);\n    EXEC tSQLt.AssertNotEquals @Expected = NULL,\n                                @Actual = (SELECT CreatedDate FROM dbo.Customers);\nEND;\nGO\n\n-- Run all tests\nEXEC tSQLt.RunAll;\n```\n\n**Azure DevOps Integration**:\n```yaml\n- task: PowerShell@2\n  displayName: 'Run tSQLt Tests'\n  inputs:\n    targetType: 'inline'\n    script: |\n      # Execute tSQLt tests\n      $results = Invoke-Sqlcmd -ServerInstance $(testServer) `\n                                -Database $(testDatabase) `\n                                -Query \"EXEC tSQLt.RunAll\" `\n                                -Verbose\n\n      # Check for failures\n      $failures = $results | Where-Object { $_.Class -eq 'Failure' }\n      if ($failures) {\n        Write-Error \"Tests failed: $($failures.Count) failures\"\n        exit 1\n      }\n```\n\n### 3. Windows Authentication Over SQL Authentication\n\n**Security Best Practice**: Prefer Windows Authentication (Integrated Security) for CI/CD agents.\n\n**Why Windows Auth**:\n- No passwords stored in connection strings\n- Leverages existing Active Directory infrastructure\n- Service accounts with minimal permissions\n- Audit trail via Windows Security logs\n- No credential rotation needed\n\n**Implementation**:\n\n**Self-Hosted Agents (Recommended)**:\n```yaml\n# GitHub Actions with self-hosted Windows agent\nruns-on: [self-hosted, windows, sql-deploy]\n\nsteps:\n  - name: Deploy with Windows Auth\n    run: |\n      sqlpackage /Action:Publish \\\n        /SourceFile:Database.dacpac \\\n        /TargetConnectionString:\"Server=prod-sql;Database=MyDB;Integrated Security=True;\" \\\n        /p:BlockOnPossibleDataLoss=True\n```\n\n**Azure DevOps with Service Connection**:\n```yaml\n- task: SqlAzureDacpacDeployment@1\n  inputs:\n    authenticationType: 'integratedAuth'  # Uses Windows Auth\n    serverName: 'prod-sql.domain.com'\n    databaseName: 'MyDatabase'\n    dacpacFile: '$(Build.ArtifactStagingDirectory)/Database.dacpac'\n```\n\n**Alternative for Cloud Agents (Azure SQL)**:\n```yaml\n# Use Managed Identity instead of SQL auth\n- name: Deploy with Managed Identity\n  run: |\n    sqlpackage /Action:Publish \\\n      /SourceFile:Database.dacpac \\\n      /TargetConnectionString:\"Server=tcp:server.database.windows.net;Database=MyDB;Authentication=ActiveDirectoryManagedIdentity;\" \\\n      /p:BlockOnPossibleDataLoss=True\n```\n\n**Never Do This**:\n```yaml\n# BAD: Plain text SQL auth password\nTargetConnectionString: \"Server=prod;Database=MyDB;User=sa;Password=P@ssw0rd123\"\n```\n\n**If SQL Auth Required**:\n```yaml\n# Use secrets/variables (least preferred method)\n- name: Deploy with SQL Auth (Not Recommended)\n  run: |\n    sqlpackage /Action:Publish \\\n      /SourceFile:Database.dacpac \\\n      /TargetServerName:\"${{ secrets.SQL_SERVER }}\" \\\n      /TargetDatabaseName:\"${{ secrets.SQL_DATABASE }}\" \\\n      /TargetUser:\"${{ secrets.SQL_USER }}\" \\\n      /TargetPassword:\"${{ secrets.SQL_PASSWORD }}\" \\\n      /p:BlockOnPossibleDataLoss=True\n  # Still not as secure as Windows Auth!\n```\n\n### 4. Version Control Everything\n\n**What to Version**:\n```\nDatabaseProject/\n Tables/\n    Customer.sql\n    Order.sql\n StoredProcedures/\n    GetCustomerOrders.sql\n Tests/\n    CustomerTests/\n       test_CustomerInsert.sql\n    OrderTests/\n        test_OrderValidation.sql\n Scripts/\n    Script.PreDeployment.sql\n    Script.PostDeployment.sql\n Database.sqlproj\n Database.Dev.publish.xml\n Database.Prod.publish.xml\n .gitignore\n```\n\n**.gitignore**:\n```\n# Build outputs\nbin/\nobj/\n*.dacpac\n\n# User-specific files\n*.user\n*.suo\n\n# Visual Studio folders\n.vs/\n\n# Never commit credentials\n*.publish.xml.user\n```\n\n**Check-in Requirements**:\n- Require code review for database changes\n- Mandate comments on all commits\n- Run automated tests before merge\n- Enforce naming conventions via branch policies\n\n### 5. Deployment Reports Always Required\n\n**Before Production Deployment**:\n```yaml\n- name: Generate Deployment Report\n  run: |\n    sqlpackage /Action:DeployReport \\\n      /SourceFile:Database.dacpac \\\n      /TargetConnectionString:\"${{ secrets.PROD_SQL_CONN }}\" \\\n      /OutputPath:deploy-report.xml \\\n      /p:BlockOnPossibleDataLoss=True\n\n- name: Parse and Review Report\n  run: |\n    # Extract key metrics from XML\n    echo \"=== DEPLOYMENT REPORT ===\"\n    # Parse XML for operations count\n    # Check for data loss warnings\n    # Display to user or post to PR\n\n- name: Require Manual Approval\n  uses: trstringer/manual-approval@v1\n  with:\n    approvers: database-admins\n    minimum-approvals: 1\n    instructions: \"Review deploy-report.xml before approving\"\n\n- name: Deploy After Approval\n  run: |\n    sqlpackage /Action:Publish \\\n      /SourceFile:Database.dacpac \\\n      /TargetConnectionString:\"${{ secrets.PROD_SQL_CONN }}\"\n```\n\n### 6. Environment Promotion Strategy\n\n**Standard Flow**: Dev  QA  Staging  Production\n\n**Consistent Deployment Options**:\n```yaml\n# Define environment-specific properties\nenvironments:\n  dev:\n    blockOnDataLoss: false\n    dropObjectsNotInSource: true\n    backupBeforeChanges: false\n\n  qa:\n    blockOnDataLoss: true\n    dropObjectsNotInSource: false\n    backupBeforeChanges: true\n\n  staging:\n    blockOnDataLoss: true\n    dropObjectsNotInSource: false\n    backupBeforeChanges: true\n\n  production:\n    blockOnDataLoss: true\n    dropObjectsNotInSource: false\n    backupBeforeChanges: true\n    requireApproval: true\n```\n\n## Complete GitHub Actions Pipeline (2025 Best Practice)\n\n```yaml\nname: SQL Server CI/CD Pipeline\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  DOTNET_VERSION: '8.0.x'\n  SQLPACKAGE_VERSION: '170.2.70'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup .NET 8\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: Install SqlPackage\n        run: dotnet tool install -g Microsoft.SqlPackage --version ${{ env.SQLPACKAGE_VERSION }}\n\n      - name: Build Database Project\n        run: dotnet build src/Database.sqlproj -c Release\n\n      - name: Build Test Project\n        run: dotnet build tests/DatabaseTests.sqlproj -c Release\n\n      - name: Upload DACPAC Artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: dacpacs\n          path: |\n            src/bin/Release/*.dacpac\n            tests/bin/Release/*.dacpac\n\n  test:\n    runs-on: windows-latest  # tSQLt requires SQL Server\n    needs: build\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: dacpacs\n\n      - name: Setup Test Database\n        run: |\n          sqlcmd -S localhost -Q \"CREATE DATABASE TestDB\"\n\n      - name: Deploy Database to Test\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=localhost;Database=TestDB;Integrated Security=True;\"\n\n      - name: Deploy tSQLt Framework\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:DatabaseTests.dacpac `\n            /TargetConnectionString:\"Server=localhost;Database=TestDB;Integrated Security=True;\"\n\n      - name: Run tSQLt Unit Tests\n        run: |\n          $results = Invoke-Sqlcmd -ServerInstance localhost `\n                                    -Database TestDB `\n                                    -Query \"EXEC tSQLt.RunAll\" `\n                                    -Verbose\n\n          $failures = $results | Where-Object { $_.Class -eq 'Failure' }\n          if ($failures) {\n            Write-Error \"Tests failed: $($failures.Count) failures\"\n            exit 1\n          }\n          Write-Host \"All tests passed!\"\n\n  deploy-dev:\n    runs-on: [self-hosted, windows, sql-deploy]\n    needs: test\n    if: github.ref == 'refs/heads/develop'\n    environment: dev\n    steps:\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: dacpacs\n\n      - name: Deploy to Dev (Windows Auth)\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=dev-sql;Database=MyDB;Integrated Security=True;\" `\n            /p:BlockOnPossibleDataLoss=False `\n            /p:DropObjectsNotInSource=True\n\n  deploy-staging:\n    runs-on: [self-hosted, windows, sql-deploy]\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    environment: staging\n    steps:\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: dacpacs\n\n      - name: Generate Deployment Report\n        run: |\n          sqlpackage /Action:DeployReport `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=staging-sql;Database=MyDB;Integrated Security=True;\" `\n            /OutputPath:deploy-report.xml\n\n      - name: Deploy to Staging (Windows Auth)\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=staging-sql;Database=MyDB;Integrated Security=True;\" `\n            /p:BlockOnPossibleDataLoss=True `\n            /p:BackupDatabaseBeforeChanges=True `\n            /p:DropObjectsNotInSource=False\n\n  deploy-production:\n    runs-on: [self-hosted, windows, sql-deploy]\n    needs: deploy-staging\n    environment: production\n    steps:\n      - name: Download Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: dacpacs\n\n      - name: Generate Deployment Report\n        run: |\n          sqlpackage /Action:DeployReport `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=prod-sql;Database=MyDB;Integrated Security=True;\" `\n            /OutputPath:prod-deploy-report.xml\n\n      - name: Manual Approval Required\n        uses: trstringer/manual-approval@v1\n        with:\n          approvers: database-admins,devops-leads\n          minimum-approvals: 2\n          instructions: \"Review prod-deploy-report.xml and approve deployment\"\n\n      - name: Deploy to Production (Windows Auth)\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:Database.dacpac `\n            /TargetConnectionString:\"Server=prod-sql;Database=MyDB;Integrated Security=True;\" `\n            /p:BlockOnPossibleDataLoss=True `\n            /p:BackupDatabaseBeforeChanges=True `\n            /p:DropObjectsNotInSource=False `\n            /p:DoNotDropObjectTypes=Users;Logins;RoleMembership `\n            /DiagnosticsFile:prod-deploy.log\n\n      - name: Upload Deployment Logs\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: production-deployment-logs\n          path: prod-deploy.log\n```\n\n## Azure DevOps Pipeline Example (2025)\n\n```yaml\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n\npool:\n  vmImage: 'windows-2022'\n\nvariables:\n  buildConfiguration: 'Release'\n  dotnetVersion: '8.0.x'\n  sqlPackageVersion: '170.2.70'\n\nstages:\n- stage: Build\n  jobs:\n  - job: BuildDatabase\n    steps:\n    - task: UseDotNet@2\n      displayName: 'Install .NET 8'\n      inputs:\n        version: $(dotnetVersion)\n\n    - task: DotNetCoreCLI@2\n      displayName: 'Build Database Project'\n      inputs:\n        command: 'build'\n        projects: '**/*.sqlproj'\n        arguments: '-c $(buildConfiguration)'\n\n    - task: PublishBuildArtifacts@1\n      displayName: 'Publish DACPAC'\n      inputs:\n        PathtoPublish: '$(Build.SourcesDirectory)/bin/$(buildConfiguration)'\n        ArtifactName: 'dacpacs'\n\n- stage: Test\n  dependsOn: Build\n  jobs:\n  - job: RunUnitTests\n    steps:\n    - task: DownloadBuildArtifacts@1\n      inputs:\n        artifactName: 'dacpacs'\n\n    - task: SqlAzureDacpacDeployment@1\n      displayName: 'Deploy to Test Database'\n      inputs:\n        authenticationType: 'integratedAuth'\n        serverName: 'test-sql-server'\n        databaseName: 'TestDB'\n        dacpacFile: '$(System.ArtifactsDirectory)/dacpacs/Database.dacpac'\n\n    - task: PowerShell@2\n      displayName: 'Run tSQLt Tests'\n      inputs:\n        targetType: 'inline'\n        script: |\n          $results = Invoke-Sqlcmd -ServerInstance 'test-sql-server' `\n                                    -Database 'TestDB' `\n                                    -Query \"EXEC tSQLt.RunAll\"\n\n          $failures = $results | Where-Object { $_.Class -eq 'Failure' }\n          if ($failures) {\n            throw \"Tests failed: $($failures.Count) failures\"\n          }\n\n- stage: DeployProduction\n  dependsOn: Test\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n  jobs:\n  - deployment: DeployToProduction\n    environment: 'Production'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - task: SqlAzureDacpacDeployment@1\n            displayName: 'Generate Deployment Report'\n            inputs:\n              deployType: 'DeployReport'\n              authenticationType: 'integratedAuth'\n              serverName: 'prod-sql-server'\n              databaseName: 'ProductionDB'\n              dacpacFile: '$(Pipeline.Workspace)/dacpacs/Database.dacpac'\n              outputFile: 'deploy-report.xml'\n\n          - task: SqlAzureDacpacDeployment@1\n            displayName: 'Deploy to Production'\n            inputs:\n              authenticationType: 'integratedAuth'\n              serverName: 'prod-sql-server'\n              databaseName: 'ProductionDB'\n              dacpacFile: '$(Pipeline.Workspace)/dacpacs/Database.dacpac'\n              additionalArguments: '/p:BlockOnPossibleDataLoss=True /p:BackupDatabaseBeforeChanges=True'\n```\n\n## Best Practices Checklist\n\n### Source Control\n- [ ] All database objects in source control\n- [ ] .gitignore configured for build outputs\n- [ ] No credentials committed\n- [ ] Test scripts versioned separately\n- [ ] Branching strategy defined (gitflow, trunk-based, etc.)\n\n### Testing\n- [ ] tSQLt framework deployed\n- [ ] Unit tests cover critical stored procedures\n- [ ] Tests grouped logically in schemas\n- [ ] Pipeline aborts on test failure\n- [ ] Test results published to dashboard\n\n### Security\n- [ ] Windows Authentication used for CI/CD\n- [ ] Service accounts follow principle of least privilege\n- [ ] Secrets stored in Azure Key Vault / GitHub Secrets\n- [ ] No plain text passwords\n- [ ] Audit logging enabled\n\n### Deployment\n- [ ] State-based deployment strategy\n- [ ] Deployment reports generated before production\n- [ ] Manual approval gates for production\n- [ ] Backup before changes (production)\n- [ ] BlockOnPossibleDataLoss enabled (production)\n- [ ] DoNotDropObjectTypes configured\n- [ ] Rollback plan documented\n\n### Monitoring\n- [ ] Deployment logs captured\n- [ ] Failed deployments trigger alerts\n- [ ] Performance metrics tracked\n- [ ] Schema drift detection automated\n\n## Resources\n\n- [tSQLt Official Site](https://tsqlt.org/)\n- [Microsoft.Build.Sql Documentation](https://learn.microsoft.com/sql/tools/sql-database-projects/)\n- [SqlPackage Reference](https://learn.microsoft.com/sql/tools/sqlpackage/)\n- [Azure DevOps SQL Tasks](https://learn.microsoft.com/azure/devops/pipelines/tasks/deploy/sql-azure-dacpac-deployment)\n- [GitHub Actions for SQL](https://github.com/marketplace?type=actions&query=sql+)"
              },
              {
                "name": "windows-git-bash-paths",
                "description": "Windows and Git Bash path handling for SSDT, SqlPackage, and DACPAC files with shell detection",
                "path": "plugins/ssdt-master/skills/windows-git-bash-paths/SKILL.md",
                "frontmatter": {
                  "name": "windows-git-bash-paths",
                  "description": "Windows and Git Bash path handling for SSDT, SqlPackage, and DACPAC files with shell detection"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Windows and Git Bash Path Handling for SSDT\n\n## Overview\n\nSQL Server development is Windows-heavy, and many developers use Git Bash (MINGW/MSYS2) as their preferred shell on Windows. This creates unique path conversion challenges when working with Windows-native tools like SqlPackage, MSBuild, and Visual Studio that expect Windows-style paths.\n\nThis skill provides comprehensive guidance on handling path conversion issues, shell detection, and best practices for SSDT workflows on Windows with Git Bash.\n\n## The Path Conversion Problem\n\n### What Happens in Git Bash/MINGW\n\nGit Bash automatically converts POSIX-style paths to Windows paths, but this can cause issues with command-line arguments:\n\n**Automatic Conversions:**\n- `/foo`  `C:/Program Files/Git/usr/foo`\n- `/foo:/bar`  `C:\\msys64\\foo;C:\\msys64\\bar`\n- `--dir=/foo`  `--dir=C:/msys64/foo`\n\n**Problematic for SqlPackage:**\n```bash\n# Git Bash converts /Action to a path!\nsqlpackage /Action:Publish /SourceFile:MyDB.dacpac\n# Becomes: sqlpackage C:/Program Files/Git/usr/Action:Publish ...\n```\n\n### What Triggers Conversion\n\n Leading forward slash (/) in arguments\n Colon-separated path lists\n Arguments after `-` or `,` with path components\n\n### What's Exempt\n\n Arguments containing `=` (variable assignments)\n Drive specifiers (`C:`)\n Arguments with `;` (already Windows format)\n Arguments starting with `//` (Windows switches)\n\n## Solutions for SqlPackage in Git Bash\n\n### Method 1: MSYS_NO_PATHCONV (Recommended)\n\nDisable path conversion for specific commands:\n\n```bash\n# Temporarily disable path conversion\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"MyDatabase.dacpac\" \\\n  /TargetServerName:\"localhost\" \\\n  /TargetDatabaseName:\"MyDB\"\n\n# Works for all SqlPackage actions\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Extract \\\n  /SourceConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;\" \\\n  /TargetFile:\"MyDB_backup.dacpac\"\n```\n\n**Important Notes:**\n- The VALUE doesn't matter - setting to `0`, `false`, or empty still disables conversion\n- Only matters that variable is DEFINED\n- To re-enable: `env -u MSYS_NO_PATHCONV`\n\n### Method 2: Double Slash // (Alternative)\n\nUse double slashes for SqlPackage parameters:\n\n```bash\n# Works in Git Bash and CMD\nsqlpackage //Action:Publish \\\n  //SourceFile:MyDatabase.dacpac \\\n  //TargetServerName:localhost \\\n  //TargetDatabaseName:MyDB\n\n# Extract with double slashes\nsqlpackage //Action:Extract \\\n  //SourceConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;\" \\\n  //TargetFile:output.dacpac\n```\n\n**Advantages:**\n- No environment variable needed\n- Works across shells\n- Shell-agnostic scripts\n\n### Method 3: Use Windows-Style Paths with Quotes\n\nAlways quote paths with backslashes:\n\n```bash\n# Quoted Windows paths work in Git Bash\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"D:\\Projects\\MyDB\\bin\\Release\\MyDB.dacpac\" \\\n  /TargetConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;\"\n\n# Or with forward slashes (Windows accepts both)\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"D:/Projects/MyDB/bin/Release/MyDB.dacpac\" \\\n  /TargetConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;\"\n```\n\n### Method 4: Switch to PowerShell or CMD\n\nFor Windows-native tools, consider using native shells:\n\n```powershell\n# PowerShell (recommended for Windows SSDT workflows)\nsqlpackage /Action:Publish `\n  /SourceFile:\"MyDatabase.dacpac\" `\n  /TargetServerName:\"localhost\" `\n  /TargetDatabaseName:\"MyDB\"\n```\n\n```cmd\n:: CMD\nsqlpackage /Action:Publish ^\n  /SourceFile:\"MyDatabase.dacpac\" ^\n  /TargetServerName:\"localhost\" ^\n  /TargetDatabaseName:\"MyDB\"\n```\n\n## Shell Detection for Scripts\n\n### Bash Script Detection\n\n```bash\n#!/bin/bash\n\n# Method 1: Check $OSTYPE\ncase \"$OSTYPE\" in\n  msys*)       # MSYS/Git Bash/MinGW\n    export MSYS_NO_PATHCONV=1\n    SQLPACKAGE_ARGS=\"/Action:Publish /SourceFile:MyDB.dacpac\"\n    ;;\n  cygwin*)     # Cygwin\n    export MSYS_NO_PATHCONV=1\n    SQLPACKAGE_ARGS=\"/Action:Publish /SourceFile:MyDB.dacpac\"\n    ;;\n  linux-gnu*)  # Linux\n    SQLPACKAGE_ARGS=\"/Action:Publish /SourceFile:MyDB.dacpac\"\n    ;;\n  darwin*)     # macOS\n    SQLPACKAGE_ARGS=\"/Action:Publish /SourceFile:MyDB.dacpac\"\n    ;;\nesac\n\nsqlpackage $SQLPACKAGE_ARGS\n```\n\n```bash\n# Method 2: Check uname -s (most portable)\ncase \"$(uname -s)\" in\n  MINGW64*|MINGW32*)\n    # Git Bash\n    export MSYS_NO_PATHCONV=1\n    echo \"Git Bash detected - path conversion disabled\"\n    ;;\n  MSYS_NT*)\n    # MSYS\n    export MSYS_NO_PATHCONV=1\n    echo \"MSYS detected - path conversion disabled\"\n    ;;\n  CYGWIN*)\n    # Cygwin\n    export MSYS_NO_PATHCONV=1\n    echo \"Cygwin detected - path conversion disabled\"\n    ;;\n  Linux*)\n    # Linux or WSL\n    echo \"Linux detected\"\n    ;;\n  Darwin*)\n    # macOS\n    echo \"macOS detected\"\n    ;;\nesac\n```\n\n```bash\n# Method 3: Check $MSYSTEM (Git Bash specific)\nif [ -n \"$MSYSTEM\" ]; then\n  # Running in Git Bash/MSYS2\n  export MSYS_NO_PATHCONV=1\n  echo \"MSYS environment detected: $MSYSTEM\"\n  case \"$MSYSTEM\" in\n    MINGW64) echo \"64-bit native Windows environment\" ;;\n    MINGW32) echo \"32-bit native Windows environment\" ;;\n    MSYS)    echo \"POSIX-compliant environment\" ;;\n  esac\nfi\n```\n\n### Complete Build Script Example\n\n```bash\n#!/bin/bash\n# build-and-deploy.sh - Cross-platform SSDT build script\n\nset -e  # Exit on error\n\n# Detect shell and set path conversion\nif [ -n \"$MSYSTEM\" ]; then\n  echo \"Git Bash/MSYS2 detected - disabling path conversion\"\n  export MSYS_NO_PATHCONV=1\nfi\n\n# Variables\nPROJECT_NAME=\"MyDatabase\"\nBUILD_CONFIG=\"Release\"\nDACPAC_PATH=\"bin/${BUILD_CONFIG}/${PROJECT_NAME}.dacpac\"\nTARGET_SERVER=\"${SQL_SERVER:-localhost}\"\nTARGET_DB=\"${SQL_DATABASE:-MyDB}\"\n\n# Build\necho \"Building ${PROJECT_NAME}...\"\ndotnet build \"${PROJECT_NAME}.sqlproj\" -c \"$BUILD_CONFIG\"\n\n# Verify DACPAC exists\nif [ ! -f \"$DACPAC_PATH\" ]; then\n  echo \"ERROR: DACPAC not found at $DACPAC_PATH\"\n  exit 1\nfi\n\necho \"DACPAC built successfully: $DACPAC_PATH\"\n\n# Deploy\necho \"Deploying to ${TARGET_SERVER}/${TARGET_DB}...\"\n\n# Use double-slash method for maximum compatibility\nsqlpackage //Action:Publish \\\n  //SourceFile:\"$DACPAC_PATH\" \\\n  //TargetServerName:\"$TARGET_SERVER\" \\\n  //TargetDatabaseName:\"$TARGET_DB\" \\\n  //p:BlockOnPossibleDataLoss=False\n\necho \"Deployment complete!\"\n```\n\n## Common SSDT Path Issues in Git Bash\n\n### Issue 1: DACPAC File Paths\n\n**Problem:**\n```bash\n# Git Bash mangles the path\nsqlpackage /Action:Publish /SourceFile:./bin/Release/MyDB.dacpac\n# Error: Cannot find file\n```\n\n**Solution:**\n```bash\n# Use MSYS_NO_PATHCONV\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"./bin/Release/MyDB.dacpac\"\n\n# OR use absolute Windows path\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"D:/Projects/MyDB/bin/Release/MyDB.dacpac\"\n\n# OR use double slashes\nsqlpackage //Action:Publish //SourceFile:./bin/Release/MyDB.dacpac\n```\n\n### Issue 2: SQL Project File Paths\n\n**Problem:**\n```bash\n# Path with spaces causes issues\ndotnet build \"D:/Program Files/MyProject/Database.sqlproj\"\n# Works in Git Bash (dotnet handles paths correctly)\n\n# But MSBuild paths may fail\nmsbuild \"D:/Program Files/MyProject/Database.sqlproj\"\n# May fail if not quoted properly\n```\n\n**Solution:**\n```bash\n# Always quote paths with spaces\ndotnet build \"D:/Program Files/MyProject/Database.sqlproj\"\n\n# Use backslashes for MSBuild on Windows\nmsbuild \"D:\\Program Files\\MyProject\\Database.sqlproj\"\n\n# Or use 8.3 short names (no spaces)\nmsbuild \"D:/PROGRA~1/MyProject/Database.sqlproj\"\n```\n\n### Issue 3: Publish Profile Paths\n\n**Problem:**\n```bash\n# Publish profile not found\nsqlpackage /Action:Publish \\\n  /SourceFile:MyDB.dacpac \\\n  /Profile:./Profiles/Production.publish.xml\n```\n\n**Solution:**\n```bash\n# Use MSYS_NO_PATHCONV with quoted paths\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"MyDB.dacpac\" \\\n  /Profile:\"./Profiles/Production.publish.xml\"\n\n# Or absolute Windows path\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish \\\n  /SourceFile:\"D:/Projects/MyDB.dacpac\" \\\n  /Profile:\"D:/Projects/Profiles/Production.publish.xml\"\n```\n\n### Issue 4: Connection Strings\n\n**Problem:**\n```bash\n# File paths in connection strings\n/SourceConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;AttachDbFilename=D:/Data/MyDB.mdf\"\n# Path gets mangled\n```\n\n**Solution:**\n```bash\n# Quote entire connection string\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Extract \\\n  /SourceConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;AttachDbFilename=D:\\Data\\MyDB.mdf\" \\\n  /TargetFile:\"output.dacpac\"\n\n# Or use double backslashes in connection string\n/SourceConnectionString:\"Server=localhost;Database=MyDB;Integrated Security=True;AttachDbFilename=D:\\\\Data\\\\MyDB.mdf\"\n```\n\n## CI/CD Considerations\n\n### GitHub Actions with Git Bash\n\n```yaml\nname: SSDT Build and Deploy\n\non: [push]\n\njobs:\n  build:\n    runs-on: windows-latest\n    defaults:\n      run:\n        shell: bash  # Use Git Bash\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup .NET\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: '8.0.x'\n\n      - name: Install SqlPackage\n        run: dotnet tool install -g Microsoft.SqlPackage\n\n      - name: Build Database Project\n        run: dotnet build Database.sqlproj -c Release\n\n      - name: Deploy with Path Conversion Disabled\n        env:\n          MSYS_NO_PATHCONV: 1\n        run: |\n          sqlpackage /Action:Publish \\\n            /SourceFile:\"bin/Release/MyDatabase.dacpac\" \\\n            /TargetServerName:\"localhost\" \\\n            /TargetDatabaseName:\"MyDB\"\n```\n\n### PowerShell Alternative (Recommended for Windows)\n\n```yaml\njobs:\n  build:\n    runs-on: windows-latest\n    defaults:\n      run:\n        shell: pwsh  # Use PowerShell - no path issues\n\n    steps:\n      - name: Deploy Database\n        run: |\n          sqlpackage /Action:Publish `\n            /SourceFile:\"bin/Release/MyDatabase.dacpac\" `\n            /TargetServerName:\"localhost\" `\n            /TargetDatabaseName:\"MyDB\"\n```\n\n## Best Practices Summary\n\n### For Interactive Development\n\n1. **Use PowerShell or CMD for SSDT on Windows** - avoids path conversion issues entirely\n2. **If using Git Bash**, set `MSYS_NO_PATHCONV=1` in your shell profile for SSDT work\n3. **Always quote paths** containing spaces or special characters\n4. **Use absolute paths** when possible to avoid ambiguity\n\n### For Scripts\n\n1. **Detect shell environment** and set `MSYS_NO_PATHCONV=1` conditionally\n2. **Use double-slash // syntax** for SqlPackage arguments (most portable)\n3. **Prefer PowerShell for Windows-specific workflows** (build scripts, CI/CD)\n4. **Test scripts on all target platforms** (Windows PowerShell, Git Bash, Linux)\n\n### For CI/CD\n\n1. **Use PowerShell shell in GitHub Actions** for Windows runners (`shell: pwsh`)\n2. **Self-hosted Windows agents** - use native Windows paths and shells\n3. **Set MSYS_NO_PATHCONV=1 as environment variable** if Git Bash required\n4. **Prefer dotnet CLI over MSBuild** for cross-platform compatibility\n\n### For Teams\n\n1. **Document shell requirements** in repository README\n2. **Provide scripts for all shells** (bash, PowerShell, CMD)\n3. **Standardize on PowerShell** for Windows SSDT workflows when possible\n4. **Use containerized builds** to avoid shell-specific issues\n\n## Quick Reference\n\n### Environment Variables\n\n```bash\n# Disable path conversion (Git Bash/MSYS2/Cygwin)\nexport MSYS_NO_PATHCONV=1\n\n# Re-enable path conversion\nenv -u MSYS_NO_PATHCONV\n```\n\n### SqlPackage Command Templates\n\n```bash\n# Git Bash - Method 1 (MSYS_NO_PATHCONV)\nMSYS_NO_PATHCONV=1 sqlpackage /Action:Publish /SourceFile:\"MyDB.dacpac\" /TargetServerName:\"localhost\" /TargetDatabaseName:\"MyDB\"\n\n# Git Bash - Method 2 (Double Slash)\nsqlpackage //Action:Publish //SourceFile:MyDB.dacpac //TargetServerName:localhost //TargetDatabaseName:MyDB\n\n# PowerShell (Recommended for Windows)\nsqlpackage /Action:Publish /SourceFile:\"MyDB.dacpac\" /TargetServerName:\"localhost\" /TargetDatabaseName:\"MyDB\"\n\n# CMD\nsqlpackage /Action:Publish /SourceFile:\"MyDB.dacpac\" /TargetServerName:\"localhost\" /TargetDatabaseName:\"MyDB\"\n```\n\n### Shell Detection One-Liners\n\n```bash\n# Check if Git Bash/MSYS\n[ -n \"$MSYSTEM\" ] && echo \"Git Bash/MSYS2 detected\"\n\n# Check uname\n[[ \"$(uname -s)\" =~ ^MINGW ]] && echo \"Git Bash detected\"\n\n# Set path conversion conditionally\n[ -n \"$MSYSTEM\" ] && export MSYS_NO_PATHCONV=1\n```\n\n## Resources\n\n- [Git Bash Path Conversion Guide](https://www.pascallandau.com/blog/setting-up-git-bash-mingw-msys2-on-windows/)\n- [MSYS2 Path Conversion Documentation](https://www.msys2.org/docs/filesystem-paths/)\n- [SqlPackage Documentation](https://learn.microsoft.com/sql/tools/sqlpackage/)\n- [Microsoft.Build.Sql SDK](https://www.nuget.org/packages/Microsoft.Build.Sql)\n- [Git for Windows](https://gitforwindows.org/)\n\n## Troubleshooting\n\n### \"Invalid parameter\" errors\n\n**Symptom:** SqlPackage reports \"Invalid parameter\" or \"Unknown action\"\n**Cause:** Git Bash converting `/Action` to a file path\n**Fix:** Use `MSYS_NO_PATHCONV=1` or double-slash `//Action`\n\n### \"File not found\" with valid paths\n\n**Symptom:** DACPAC or project file not found despite correct path\n**Cause:** Path conversion mangling the file path\n**Fix:** Quote paths and use `MSYS_NO_PATHCONV=1`\n\n### Build succeeds but publish fails\n\n**Symptom:** `dotnet build` works but `sqlpackage` fails\n**Cause:** dotnet CLI handles paths correctly, but SqlPackage uses `/` parameters\n**Fix:** Use `MSYS_NO_PATHCONV=1` for SqlPackage commands only\n\n### Spaces in paths cause errors\n\n**Symptom:** Paths with \"Program Files\" or other spaces fail\n**Cause:** Unquoted paths split into multiple arguments\n**Fix:** Always quote paths: `/SourceFile:\"D:/Program Files/MyDB.dacpac\"`"
              }
            ]
          },
          {
            "name": "adf-master",
            "description": "Complete Azure Data Factory expertise system with comprehensive JSON references, validation enforcement, and Microsoft Fabric integration (2025). PROACTIVELY activate for: (1) ANY ADF pipeline JSON creation or editing, (2) Activity configuration (Copy, ForEach, IfCondition, Switch, Until, Lookup, ExecutePipeline, WebActivity, DatabricksJob), (3) Activity nesting validation (ForEach/If/Switch/Until limitations with Execute Pipeline workarounds), (4) Linked service JSON (Blob Storage accountKind, Azure SQL, ADLS Gen2, Synapse, Databricks, Fabric Lakehouse/Warehouse, REST, SFTP, Snowflake), (5) Expression functions (@utcnow, @formatDateTime, @concat, @activity(), @pipeline().parameters), (6) Trigger configuration (Schedule, Tumbling Window with dependencies, Event triggers), (7) Dataset JSON schemas with parameterization, (8) CI/CD setup (GitHub Actions/Azure DevOps with Node.js 20.x, @microsoft/azure-data-factory-utilities), (9) ARM template generation and multi-environment deployment, (10) Microsoft Fabric integration (ADF mounting, Invoke Pipeline cross-workspace, Variable Libraries), (11) Pipeline debugging and error troubleshooting. Provides: Complete JSON reference library for all activity types, linked services, triggers, datasets, and expressions. Validation rules with resource limits (80/120 activities, 50 parameters, 50 batchCount). Progressive disclosure references for deep technical details. Commands for validation, debugging, and generation. Utility scripts for ARM parameters, dependency graphs, and environment comparison.",
            "source": "./plugins/adf-master",
            "category": null,
            "version": "4.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install adf-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/adf-create-pipeline",
                "description": "Generate ADF pipeline JSON with proper structure, activities, and best practices",
                "path": "plugins/adf-master/commands/adf-create-pipeline.md",
                "frontmatter": {
                  "name": "adf-create-pipeline",
                  "description": "Generate ADF pipeline JSON with proper structure, activities, and best practices",
                  "argument-hint": "<pipeline-name> <description-of-requirements>",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob",
                    "WebSearch"
                  ]
                },
                "content": "# ADF Pipeline Generator\n\nGenerate Azure Data Factory pipeline JSON files following best practices and avoiding common pitfalls.\n\n## Task\n\nCreate a complete, valid ADF pipeline JSON based on the provided requirements.\n\n## Arguments\n\n- `$ARGUMENTS`: Pipeline name followed by requirements description\n  - Example: `PL_SalesETL Copy sales data from Azure SQL to Parquet in ADLS, partitioned by date`\n  - Example: `PL_DailyLoad Load multiple tables using ForEach with config lookup`\n  - Example: `PL_APIIngestion Fetch data from REST API with pagination and error handling`\n\n## Pipeline Generation Rules\n\n### Required Structure\n```json\n{\n  \"name\": \"<PipelineName>\",\n  \"properties\": {\n    \"activities\": [],\n    \"parameters\": {},\n    \"variables\": {},\n    \"annotations\": [],\n    \"folder\": { \"name\": \"<FolderName>\" }\n  }\n}\n```\n\n### Naming Conventions\n- Pipelines: `PL_<Domain>_<Action>` (e.g., `PL_Sales_DailyLoad`)\n- Activities: `<Type>_<Purpose>` (e.g., `Copy_SalesToParquet`, `ForEach_Tables`)\n- Variables: `var<Name>` (e.g., `varCounter`, `varResults`)\n- Parameters: `<Name>` (e.g., `ProcessDate`, `TableList`)\n\n### Activity Nesting Rules\nNEVER create prohibited combinations:\n- ForEach cannot contain: ForEach, Until, Validation\n- IfCondition cannot contain: ForEach, If, Switch, Until, Validation\n- Switch cannot contain: ForEach, If, Switch, Until, Validation\n- Until cannot contain: Until, ForEach, Validation\n\nIf nested control flow is required, use Execute Pipeline pattern.\n\n### Best Practices to Apply\n1. **Always include retry policy** for Copy, Web, and external activities\n2. **Use parameters** for environment-specific values (servers, databases, paths)\n3. **Add dependsOn** with explicit dependency conditions\n4. **Include timeout** values appropriate for the operation\n5. **Use Key Vault references** for secrets (never hardcode)\n6. **Add annotations** for documentation and tagging\n7. **Set secureInput/secureOutput** when handling sensitive data\n\n### Common Patterns\n\n**Copy with Lookup Config:**\n```json\n{\n  \"activities\": [\n    {\n      \"name\": \"Lookup_GetTables\",\n      \"type\": \"Lookup\",\n      \"typeProperties\": {\n        \"source\": { \"type\": \"AzureSqlSource\", \"sqlReaderQuery\": \"SELECT * FROM Config.Tables\" },\n        \"dataset\": { \"referenceName\": \"DS_Config\", \"type\": \"DatasetReference\" },\n        \"firstRowOnly\": false\n      }\n    },\n    {\n      \"name\": \"ForEach_Tables\",\n      \"type\": \"ForEach\",\n      \"dependsOn\": [{ \"activity\": \"Lookup_GetTables\", \"dependencyConditions\": [\"Succeeded\"] }],\n      \"typeProperties\": {\n        \"items\": { \"value\": \"@activity('Lookup_GetTables').output.value\", \"type\": \"Expression\" },\n        \"isSequential\": false,\n        \"batchCount\": 20,\n        \"activities\": [...]\n      }\n    }\n  ]\n}\n```\n\n**Error Handling with Fail:**\n```json\n{\n  \"name\": \"Fail_OnError\",\n  \"type\": \"Fail\",\n  \"dependsOn\": [{ \"activity\": \"Copy_Data\", \"dependencyConditions\": [\"Failed\"] }],\n  \"typeProperties\": {\n    \"message\": \"Copy activity failed: @{activity('Copy_Data').error.message}\",\n    \"errorCode\": \"COPY_FAILED\"\n  }\n}\n```\n\n## Output\n\n1. Generate complete, valid pipeline JSON\n2. Include inline comments explaining key decisions\n3. List any required linked services and datasets\n4. Provide parameterization recommendations\n5. Note any limitations or considerations\n\n## Validation\n\nAfter generating, mentally validate against:\n- Activity nesting rules\n- Resource limits (80 activities, 50 parameters, 50 variables)\n- No SetVariable in parallel ForEach\n- Proper dependsOn chains"
              },
              {
                "name": "/adf-debug",
                "description": "Debug ADF pipeline failures, analyze error messages, and suggest fixes",
                "path": "plugins/adf-master/commands/adf-debug.md",
                "frontmatter": {
                  "name": "adf-debug",
                  "description": "Debug ADF pipeline failures, analyze error messages, and suggest fixes",
                  "argument-hint": "<error-message-or-pipeline-name>",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep",
                    "WebSearch",
                    "WebFetch"
                  ]
                },
                "content": "# ADF Pipeline Debugger\n\nAnalyze Azure Data Factory pipeline errors and provide targeted solutions.\n\n## Task\n\nDebug the provided ADF error or pipeline issue and provide specific remediation steps.\n\n## Arguments\n\n- `$ARGUMENTS`: Either:\n  - An error message or error code from ADF\n  - A pipeline name to analyze for potential issues\n  - A description of the failure behavior\n\n## Common Error Patterns and Solutions\n\n### Activity Errors\n\n**ErrorCode: UserErrorActivityRunFailed**\n- Check activity-specific error in `error.message`\n- Common causes: connection timeout, invalid query, permission denied\n- Solution: Validate linked service connectivity, check firewall rules\n\n**ErrorCode: UserErrorFailedToReadSourceData**\n- Source data unavailable or inaccessible\n- Check: file exists, permissions granted, network accessible\n- For Blob: verify container/path, SAS token not expired\n\n**ErrorCode: UserErrorFailedToWriteSinkData**\n- Cannot write to destination\n- Check: sink permissions, disk space, schema compatibility\n- For SQL: verify table exists, columns match, constraints satisfied\n\n### Copy Activity Errors\n\n**ErrorCode: SqlServerCannotConnect**\n```\nSolutions:\n1. Verify server name and database name\n2. Check firewall allows Azure services or specific IPs\n3. Verify credentials (SQL auth) or role assignments (MI/SP)\n4. For serverless: implement retry for auto-pause wake-up\n```\n\n**ErrorCode: CosmosDbPartitionKeyRangeTooLarge**\n```\nSolutions:\n1. Increase Data Integration Units (DIUs)\n2. Add WHERE clause to limit data per copy\n3. Partition the copy by date or other key\n```\n\n### Data Flow Errors\n\n**ErrorCode: DF-EXECUTOR-InvalidDataType**\n```\nSolutions:\n1. Check source column types vs expected types\n2. Add explicit cast/convert in transformation\n3. Verify date format matches expected pattern\n```\n\n**ErrorCode: DF-EXECUTOR-OutOfMemory**\n```\nSolutions:\n1. Increase cluster size (Core Count)\n2. Add partitioning before memory-intensive operations\n3. Use broadcast join for small dimension tables\n4. Optimize transformations to reduce intermediate data\n```\n\n### Linked Service Errors\n\n**ErrorCode: InvalidParameter**\n```\nFor Blob Storage with Managed Identity:\n- CRITICAL: Add \"accountKind\": \"StorageV2\" to typeProperties\n- Verify MI has Storage Blob Data Reader/Contributor role\n\nFor SQL with Managed Identity:\n- Create contained user: CREATE USER [<adf-name>] FROM EXTERNAL PROVIDER\n- Grant roles: ALTER ROLE db_datareader ADD MEMBER [<adf-name>]\n```\n\n### Trigger Errors\n\n**ErrorCode: TriggerFailedToStart**\n```\nSolutions:\n1. Verify pipeline exists and is published\n2. Check trigger parameters match pipeline parameters\n3. For event triggers: verify Event Grid subscription active\n4. For tumbling window: check dependencies are running\n```\n\n## Debugging Process\n\n1. **Identify Error Location**\n   - Which activity failed?\n   - What was the error code and message?\n   - What were the input values?\n\n2. **Check Common Causes**\n   - Connectivity (firewall, network, endpoints)\n   - Authentication (expired credentials, missing permissions)\n   - Data (schema mismatch, null values, size limits)\n   - Configuration (missing properties, invalid expressions)\n\n3. **Review Activity Configuration**\n   - Read pipeline JSON for the failing activity\n   - Validate all required properties are set\n   - Check expression syntax\n\n4. **Test Connectivity**\n   - Use Test Connection in linked service\n   - Verify private endpoints if using VNet\n   - Check DNS resolution\n\n5. **Analyze Monitoring Data**\n   - Check Activity Runs in Monitor hub\n   - Review detailed error message\n   - Check input/output for activities\n\n## Kusto Queries for Analysis\n\n```kusto\n// Find all failures in last 24 hours\nADFActivityRun\n| where Status == \"Failed\"\n| where TimeGenerated > ago(24h)\n| project TimeGenerated, PipelineName, ActivityName, ErrorCode, ErrorMessage\n| order by TimeGenerated desc\n\n// Find long-running activities\nADFActivityRun\n| where TimeGenerated > ago(7d)\n| extend DurationMinutes = datetime_diff('minute', End, Start)\n| where DurationMinutes > 30\n| summarize AvgDuration=avg(DurationMinutes), Count=count() by PipelineName, ActivityName\n| order by AvgDuration desc\n```\n\n## Output Format\n\n```\n=== ADF Debug Analysis ===\n\nError: [ErrorCode] - [Short Description]\nPipeline: [PipelineName]\nActivity: [ActivityName]\n\nRoot Cause:\n[Detailed explanation of what caused the error]\n\nSolution Steps:\n1. [First step to fix]\n2. [Second step to fix]\n3. [Verification step]\n\nPrevention:\n[How to prevent this in future]\n\nRelated Documentation:\n- [Link to relevant Microsoft docs]\n```"
              },
              {
                "name": "/adf-expression",
                "description": "Generate ADF expressions for date manipulation, string operations, and dynamic content",
                "path": "plugins/adf-master/commands/adf-expression.md",
                "frontmatter": {
                  "name": "adf-expression",
                  "description": "Generate ADF expressions for date manipulation, string operations, and dynamic content",
                  "argument-hint": "<what-you-need> [examples: yesterday-date, file-partition-path, conditional-query]",
                  "allowed-tools": [
                    "Read"
                  ]
                },
                "content": "# ADF Expression Generator\n\nGenerate Azure Data Factory expressions for common scenarios.\n\n## Task\n\nCreate valid ADF expressions based on the described requirement.\n\n## Arguments\n\n- `$ARGUMENTS`: Description of what the expression should do\n  - Example: `yesterday's date in yyyy-MM-dd format`\n  - Example: `partition path like year=2025/month=01/day=15`\n  - Example: `first day of current month`\n  - Example: `extract filename from full path`\n  - Example: `check if weekday`\n  - Example: `conditional SQL query based on parameter`\n\n## Expression Syntax Rules\n\n### Basic Syntax\n- Expressions start with `@` prefix\n- Use `@{expression}` for string interpolation\n- Nested functions: `@function1(function2(value))`\n\n### Expression vs String Interpolation\n```json\n// Full expression (returns typed value)\n\"value\": \"@utcnow()\"\n\n// String interpolation (returns string)\n\"value\": \"Date is @{utcnow()}\"\n\n// Explicit expression type\n{\n  \"value\": \"@concat('prefix_', pipeline().parameters.Name)\",\n  \"type\": \"Expression\"\n}\n```\n\n## Common Expression Patterns\n\n### Date/Time\n\n**Yesterday's Date:**\n```\n@formatDateTime(adddays(utcnow(), -1), 'yyyy-MM-dd')\n```\n\n**First Day of Month:**\n```\n@formatDateTime(startOfMonth(utcnow()), 'yyyy-MM-dd')\n```\n\n**Last Day of Previous Month:**\n```\n@formatDateTime(adddays(startOfMonth(utcnow()), -1), 'yyyy-MM-dd')\n```\n\n**Date Partition Path (year/month/day):**\n```\n@concat(\n  formatDateTime(utcnow(), 'yyyy'), '/',\n  formatDateTime(utcnow(), 'MM'), '/',\n  formatDateTime(utcnow(), 'dd')\n)\n```\n\n**Hive Partition Path:**\n```\n@concat(\n  'year=', formatDateTime(pipeline().parameters.ProcessDate, 'yyyy'),\n  '/month=', formatDateTime(pipeline().parameters.ProcessDate, 'MM'),\n  '/day=', formatDateTime(pipeline().parameters.ProcessDate, 'dd')\n)\n```\n\n**Is Weekday Check:**\n```\n@and(greater(dayOfWeek(utcnow()), 0), less(dayOfWeek(utcnow()), 6))\n```\n\n**N Days Ago:**\n```\n@formatDateTime(adddays(utcnow(), -7), 'yyyy-MM-dd')\n```\n\n### String Operations\n\n**Extract Filename from Path:**\n```\n@substring(\n  variables('FilePath'),\n  add(lastIndexOf(variables('FilePath'), '/'), 1),\n  sub(length(variables('FilePath')), add(lastIndexOf(variables('FilePath'), '/'), 1))\n)\n```\n\n**Remove File Extension:**\n```\n@substring(\n  item().name,\n  0,\n  lastIndexOf(item().name, '.')\n)\n```\n\n**Build Dynamic Table Name:**\n```\n@concat(pipeline().parameters.Schema, '.', pipeline().parameters.Table)\n```\n\n**Safe Property Access:**\n```\n@coalesce(activity('Lookup').output.firstRow.Value, 'default')\n```\n\n### Conditional Logic\n\n**Conditional SQL Query:**\n```\n@if(\n  equals(pipeline().parameters.FullLoad, true),\n  'SELECT * FROM dbo.Table',\n  concat('SELECT * FROM dbo.Table WHERE Date >= ''', pipeline().parameters.LastDate, '''')\n)\n```\n\n**Conditional File Path:**\n```\n@if(\n  equals(pipeline().parameters.Environment, 'prod'),\n  'production/data/',\n  'development/data/'\n)\n```\n\n### Collections\n\n**Array to Comma-Separated String:**\n```\n@join(activity('Lookup').output.value, ',')\n```\n\n**Check Array Not Empty:**\n```\n@greater(length(activity('Lookup').output.value), 0)\n```\n\n**Get First N Items:**\n```\n@take(activity('Lookup').output.value, 10)\n```\n\n**Skip First N Items:**\n```\n@skip(activity('Lookup').output.value, 10)\n```\n\n### Pipeline/Activity References\n\n**Current Pipeline Info:**\n```\n@pipeline().Pipeline          // Pipeline name\n@pipeline().DataFactory       // Data factory name\n@pipeline().RunId             // Current run ID\n@pipeline().TriggerName       // Trigger name\n@pipeline().TriggerType       // 'Manual', 'Schedule', 'Tumbling'\n```\n\n**Activity Output:**\n```\n@activity('LookupConfig').output.firstRow.ColumnName\n@activity('CopyData').output.rowsCopied\n@activity('CopyData').output.rowsRead\n@activity('WebCall').output.Response\n```\n\n**Tumbling Window:**\n```\n@trigger().outputs.windowStartTime\n@trigger().outputs.windowEndTime\n```\n\n**Blob Event Trigger:**\n```\n@trigger().outputs.body.fileName\n@trigger().outputs.body.folderPath\n```\n\n### ForEach Item Access\n\n**Inside ForEach:**\n```\n@item()                       // Current item\n@item().tableName             // Property access\n@item()['property-with-dash'] // Bracket notation for special chars\n```\n\n## Format Specifiers\n\n| Specifier | Output | Example |\n|-----------|--------|---------|\n| yyyy | 4-digit year | 2025 |\n| yy | 2-digit year | 25 |\n| MM | 2-digit month | 01 |\n| M | Month (no zero) | 1 |\n| MMMM | Full month | January |\n| dd | 2-digit day | 15 |\n| d | Day (no zero) | 15 |\n| dddd | Full day | Wednesday |\n| HH | 24-hour | 14 |\n| hh | 12-hour | 02 |\n| mm | Minutes | 30 |\n| ss | Seconds | 45 |\n| tt | AM/PM | PM |\n| fff | Milliseconds | 123 |\n\n## Output Format\n\nProvide:\n1. The complete expression\n2. Example output value\n3. Where to use it (parameter, activity property, etc.)\n4. Any caveats or limitations"
              },
              {
                "name": "/adf-linked-service",
                "description": "Generate ADF linked service JSON for any connector with proper authentication",
                "path": "plugins/adf-master/commands/adf-linked-service.md",
                "frontmatter": {
                  "name": "adf-linked-service",
                  "description": "Generate ADF linked service JSON for any connector with proper authentication",
                  "argument-hint": "<connector-type> <authentication-method>",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Glob"
                  ]
                },
                "content": "# ADF Linked Service Generator\n\nGenerate Azure Data Factory linked service JSON configurations with proper authentication and best practices.\n\n## Task\n\nCreate a complete, valid linked service JSON based on the specified connector type and authentication method.\n\n## Arguments\n\n- `$ARGUMENTS`: Connector type and optional authentication method\n  - Example: `blob managed-identity`\n  - Example: `azure-sql service-principal`\n  - Example: `rest oauth2`\n  - Example: `sftp ssh-key`\n  - Example: `fabric-lakehouse`\n\n## Supported Connectors\n\n### Storage\n- `blob` / `azure-blob` - Azure Blob Storage\n- `adls` / `adls-gen2` - Azure Data Lake Storage Gen2\n- `azure-files` - Azure File Storage\n\n### Databases\n- `azure-sql` / `sql-database` - Azure SQL Database\n- `synapse` / `azure-synapse` - Azure Synapse Analytics\n- `sql-server` - SQL Server (on-prem or VM)\n- `postgresql` - PostgreSQL\n- `snowflake` - Snowflake\n\n### Microsoft Fabric\n- `fabric-lakehouse` - Microsoft Fabric Lakehouse\n- `fabric-warehouse` - Microsoft Fabric Warehouse\n\n### APIs and Services\n- `rest` / `rest-api` - REST API\n- `http` - HTTP Server\n- `sftp` - SFTP Server\n- `databricks` - Azure Databricks\n- `keyvault` / `key-vault` - Azure Key Vault\n\n### SaaS Connectors\n- `servicenow` - ServiceNow (V2)\n- `salesforce` - Salesforce\n\n## Authentication Methods\n\n| Method | Description | Applicable To |\n|--------|-------------|---------------|\n| `managed-identity` / `mi` | System-assigned managed identity (Recommended) | Blob, ADLS, SQL, Synapse, REST, Databricks |\n| `service-principal` / `sp` | Azure AD application | Blob, ADLS, SQL, Synapse, REST, Databricks, Fabric |\n| `connection-string` / `cs` | Connection string | Blob, SQL |\n| `account-key` | Storage account key | Blob, ADLS |\n| `sas` | Shared Access Signature | Blob, ADLS |\n| `sql-auth` | SQL Server authentication | SQL, Synapse |\n| `basic` | Username/password | REST, SFTP |\n| `oauth2` | OAuth 2.0 client credentials | REST, ServiceNow |\n| `access-token` | Personal access token | Databricks |\n| `ssh-key` | SSH public key | SFTP |\n| `key-pair` | Key pair authentication | Snowflake |\n\n## Generation Rules\n\n### Naming Convention\n`LS_<Connector>_<Purpose>_<AuthMethod>`\n- Example: `LS_AzureBlobStorage_DataLake_MI`\n- Example: `LS_AzureSql_Staging_SP`\n\n### Required Properties by Auth Method\n\n**Managed Identity (Blob Storage):**\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://<account>.blob.core.windows.net\",\n    \"accountKind\": \"StorageV2\"  // CRITICAL - REQUIRED for MI!\n  }\n}\n```\n\n**Service Principal:**\n```json\n{\n  \"typeProperties\": {\n    \"servicePrincipalId\": \"<app-id>\",\n    \"servicePrincipalKey\": {\n      \"type\": \"AzureKeyVaultSecret\",\n      \"store\": { \"referenceName\": \"LS_KeyVault\", \"type\": \"LinkedServiceReference\" },\n      \"secretName\": \"<secret-name>\"\n    },\n    \"tenant\": \"<tenant-id>\"\n  }\n}\n```\n\n### Best Practices Applied\n\n1. **Never hardcode secrets** - Always use Key Vault references\n2. **Use Managed Identity** when possible (most secure)\n3. **Include connectVia** for private endpoints or SHIR\n4. **Parameterize** for multi-environment deployment\n5. **Set accountKind** for Blob Storage with MI/SP\n\n## Output Format\n\nGenerate complete JSON with:\n1. Linked service definition\n2. Comments explaining key properties\n3. Required RBAC roles or permissions\n4. Key Vault secret requirements (if applicable)\n5. Parameter file snippet for CI/CD\n\n```json\n// Linked Service: LS_AzureBlobStorage_DataLake_MI\n// Required Role: Storage Blob Data Reader (source) or Contributor (sink)\n// Pre-requisite: ADF managed identity must be assigned the role\n{\n  \"name\": \"LS_AzureBlobStorage_DataLake_MI\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"AzureBlobStorage\",\n    \"typeProperties\": {\n      \"serviceEndpoint\": \"https://mystorageaccount.blob.core.windows.net\",\n      \"accountKind\": \"StorageV2\"\n    },\n    \"connectVia\": {\n      \"referenceName\": \"AutoResolveIntegrationRuntime\",\n      \"type\": \"IntegrationRuntimeReference\"\n    }\n  }\n}\n```\n\n## Common Gotchas\n\n1. **Blob + Managed Identity**: MUST include `accountKind`\n2. **SQL + Managed Identity**: Must create contained database user\n3. **REST + OAuth**: Need both `tokenEndpoint` and `resource`\n4. **SFTP**: Consider `skipHostKeyValidation` for dev environments only\n5. **Databricks**: Use Job activity (2025) not Notebook for new implementations"
              },
              {
                "name": "/adf-validate",
                "description": "Validate ADF pipeline JSON files for nesting violations, resource limits, and configuration issues",
                "path": "plugins/adf-master/commands/adf-validate.md",
                "frontmatter": {
                  "name": "adf-validate",
                  "description": "Validate ADF pipeline JSON files for nesting violations, resource limits, and configuration issues",
                  "argument-hint": "[path-to-pipeline-folder]",
                  "allowed-tools": [
                    "Read",
                    "Glob",
                    "Grep",
                    "Bash"
                  ]
                },
                "content": "# ADF Pipeline Validation\n\nValidate Azure Data Factory pipeline JSON files against the complete set of ADF rules and limitations.\n\n## Task\n\nRun comprehensive validation on the specified ADF pipeline folder (or detect automatically) checking for:\n\n1. **Activity Nesting Violations:**\n   - ForEach  ForEach, Until, Validation (PROHIBITED)\n   - Until  Until, ForEach, Validation (PROHIBITED)\n   - IfCondition  ForEach, If, Switch, Until, Validation (PROHIBITED)\n   - Switch  ForEach, If, Switch, Until, Validation (PROHIBITED)\n\n2. **Resource Limits:**\n   - Activities per pipeline (max 80/120)\n   - Parameters per pipeline (max 50)\n   - Variables per pipeline (max 50)\n   - ForEach batchCount (max 50)\n\n3. **Variable Scope Violations:**\n   - SetVariable in parallel ForEach (causes race conditions)\n\n4. **Linked Service Issues:**\n   - Missing accountKind for Managed Identity Blob Storage\n   - Missing required properties for authentication types\n\n5. **Dataset Issues:**\n   - Missing fileName/wildcardFileName for file datasets\n   - Missing required location properties\n\n## Arguments\n\n- `$ARGUMENTS`: Optional path to pipeline folder. If not provided, will auto-detect common ADF folder structures (pipeline/, pipelines/, adf/)\n\n## Execution\n\n1. Detect or use specified pipeline folder path\n2. Find all pipeline JSON files\n3. Parse each pipeline and check for violations\n4. Report all errors and warnings with specific line references\n5. Suggest fixes for each violation\n6. Return exit code (0 = pass, 1 = fail)\n\n## Output Format\n\n```\n=== ADF Pipeline Validation ===\n\nScanning: [path]\nFound: X pipeline files\n\n[PIPELINE: PL_Example]\n   Activity count: 15/80\n   Parameter count: 5/50\n   ERROR: ForEach 'OuterLoop' contains prohibited ForEach 'InnerLoop'\n     Fix: Use Execute Pipeline pattern to call child pipeline with inner ForEach\n   WARNING: ForEach 'ProcessItems' has batchCount=40 (recommend 30)\n\n[PIPELINE: PL_DataLoad]\n   All validations passed\n\n=== Summary ===\nPipelines scanned: 2\nErrors: 1\nWarnings: 1\nStatus: FAILED\n```\n\n## Integration\n\nFor CI/CD integration, recommend running the PowerShell validation script at:\n`${CLAUDE_PLUGIN_ROOT}/scripts/validate-adf-pipelines.ps1`\n\n```yaml\n# GitHub Actions\n- name: Validate ADF\n  run: pwsh -File validate-adf-pipelines.ps1 -PipelinePath pipeline\n\n# Azure DevOps\n- task: PowerShell@2\n  inputs:\n    filePath: 'validate-adf-pipelines.ps1'\n    arguments: '-PipelinePath pipeline'\n    pwsh: true\n```"
              }
            ],
            "skills": [
              {
                "name": "adf-master",
                "description": "Comprehensive Azure Data Factory knowledge base with official documentation sources, CI/CD methods, deployment patterns, and troubleshooting resources",
                "path": "plugins/adf-master/skills/adf-master/SKILL.md",
                "frontmatter": {
                  "name": "adf-master",
                  "description": "Comprehensive Azure Data Factory knowledge base with official documentation sources, CI/CD methods, deployment patterns, and troubleshooting resources"
                },
                "content": "# Azure Data Factory Master Knowledge Base\n\n##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n\n---\n\nThis skill provides comprehensive reference information about Azure Data Factory, including official documentation sources, CI/CD deployment methods, and troubleshooting resources. Use this to access detailed ADF knowledge on-demand.\n\n##  CRITICAL 2025 UPDATE: Deprecated Features### Apache Airflow Workflow Orchestration Manager - DEPRECATED**Status:** Available only for existing customers as of early 2025**Retirement Date:** Not yet announced, but feature is officially deprecated**Impact:** New customers cannot provision Apache Airflow in Azure Data Factory**Official Deprecation Notice:**- Apache Airflow Workflow Orchestration Manager is deprecated with no retirement date set- Only existing deployments can continue using this feature- No new Airflow integrations can be created in ADF**Migration Path:**- **Recommended:** Migrate to Fabric Data Factory with native Airflow support- **Alternative:** Use standalone Apache Airflow deployments (Azure Container Instances, AKS, or VM-based)- **Alternative:** Migrate orchestration logic to native ADF pipelines with control flow activities**Why Deprecated:**- Microsoft focus shifted to Fabric Data Factory as the unified data integration platform- Fabric provides modern orchestration capabilities superseding Airflow integration- Limited adoption and maintenance burden for standalone Airflow feature in ADF**Action Required:**- If using Airflow in ADF: Plan migration within 12-18 months- For new projects: Do NOT use Airflow in ADF - use Fabric or native ADF patterns- Monitor Microsoft announcements for official retirement timeline**Reference:**- Microsoft Roadmap: https://www.directionsonmicrosoft.com/roadmaps/ref/azure-data-factory-roadmap/##  2025 Feature Updates### Microsoft Fabric Integration (GA June 2025)**ADF Mounting in Fabric:**- Bring existing ADF pipelines into Fabric workspaces without rebuilding- General Availability as of June 2025- Seamless integration enables hybrid ADF + Fabric workflows**Cross-Workspace Pipeline Orchestration:**- New **Invoke Pipeline** activity supports cross-platform calls- Invoke pipelines across Fabric, Azure Data Factory, and Synapse- Managed VNet support for secure cross-workspace communication**Variable Libraries:**- Environment-specific variables for CI/CD automation- Automatic value substitution during workspace promotion- Eliminates separate parameter files per environment**Connector Enhancements:**- ServiceNow V2 (V1 End of Support)- Enhanced PostgreSQL and Snowflake connectors- Native OneLake connectivity for zero-copy integration### Node.js 20.x Requirement for CI/CD**CRITICAL:** As of 2025, npm package `@microsoft/azure-data-factory-utilities` requires Node.js 20.x**Breaking Change:**- Older Node.js versions (14.x, 16.x, 18.x) may cause package incompatibility errors- Update CI/CD pipelines to use Node.js 20.x or compatible versions**GitHub Actions:**```yaml- name: Setup Node.js  uses: actions/setup-node@v4  with:    node-version: '20.x'```**Azure DevOps:**```yaml- task: UseNode@1  inputs:    version: '20.x'```\n\n##  CRITICAL 2025 UPDATE: Deprecated Features\n\n### Apache Airflow Workflow Orchestration Manager - DEPRECATED\n\n**Status:** Available only for existing customers as of early 2025\n**Retirement Date:** Not yet announced, but feature is officially deprecated\n**Impact:** New customers cannot provision Apache Airflow in Azure Data Factory\n\n**Official Deprecation Notice:**\n- Apache Airflow Workflow Orchestration Manager is deprecated with no retirement date set\n- Only existing deployments can continue using this feature\n- No new Airflow integrations can be created in ADF\n\n**Migration Path:**\n- **Recommended:** Migrate to Fabric Data Factory with native Airflow support\n- **Alternative:** Use standalone Apache Airflow deployments (Azure Container Instances, AKS, or VM-based)\n- **Alternative:** Migrate orchestration logic to native ADF pipelines with control flow activities\n\n**Why Deprecated:**\n- Microsoft focus shifted to Fabric Data Factory as the unified data integration platform\n- Fabric provides modern orchestration capabilities superseding Airflow integration\n- Limited adoption and maintenance burden for standalone Airflow feature in ADF\n\n**Action Required:**\n- If using Airflow in ADF: Plan migration within 12-18 months\n- For new projects: Do NOT use Airflow in ADF - use Fabric or native ADF patterns\n- Monitor Microsoft announcements for official retirement timeline\n\n**Reference:**\n- Microsoft Roadmap: https://www.directionsonmicrosoft.com/roadmaps/ref/azure-data-factory-roadmap/\n\n##  2025 Feature Updates\n\n### Microsoft Fabric Integration (GA June 2025)\n\n**ADF Mounting in Fabric:**\n- Bring existing ADF pipelines into Fabric workspaces without rebuilding\n- General Availability as of June 2025\n- Seamless integration enables hybrid ADF + Fabric workflows\n\n**Cross-Workspace Pipeline Orchestration:**\n- New **Invoke Pipeline** activity supports cross-platform calls\n- Invoke pipelines across Fabric, Azure Data Factory, and Synapse\n- Managed VNet support for secure cross-workspace communication\n\n**Variable Libraries:**\n- Environment-specific variables for CI/CD automation\n- Automatic value substitution during workspace promotion\n- Eliminates separate parameter files per environment\n\n**Connector Enhancements:**\n- ServiceNow V2 (V1 End of Support)\n- Enhanced PostgreSQL and Snowflake connectors\n- Native OneLake connectivity for zero-copy integration\n\n### Node.js 20.x Requirement for CI/CD\n\n**CRITICAL:** As of 2025, npm package `@microsoft/azure-data-factory-utilities` requires Node.js 20.x\n\n**Breaking Change:**\n- Older Node.js versions (14.x, 16.x, 18.x) may cause package incompatibility errors\n- Update CI/CD pipelines to use Node.js 20.x or compatible versions\n\n**GitHub Actions:**\n```yaml\n- name: Setup Node.js\n  uses: actions/setup-node@v4\n  with:\n    node-version: '20.x'\n```\n\n**Azure DevOps:**\n```yaml\n- task: UseNode@1\n  inputs:\n    version: '20.x'\n```\n\n## Official Documentation Sources\n\n### Primary Microsoft Learn Resources\n\n**Main Documentation Hub:**\n- URL: https://learn.microsoft.com/en-us/azure/data-factory/\n- Last Updated: February 2025\n- Coverage: Complete ADF documentation including tutorials, concepts, how-to guides, and reference materials\n- Key Topics: Pipelines, datasets, triggers, linked services, data flows, integration runtimes, monitoring\n\n**Introduction to Azure Data Factory:**\n- URL: https://learn.microsoft.com/en-us/azure/data-factory/introduction\n- Summary: Managed cloud service for complex hybrid ETL, ELT, and data integration projects\n- Key Features: 90+ built-in connectors, serverless architecture, code-free UI, single-pane monitoring\n\n### Context7 Library Documentation\n\n**Library ID:** `/websites/learn_microsoft_en-us_azure_data-factory`\n- Trust Score: 7.5\n- Code Snippets: 10,839\n- Topics: CI/CD, ARM templates, pipeline patterns, data flows, monitoring, troubleshooting\n\n**How to Access:**\n```\nUse Context7 MCP tool to fetch latest documentation:\nmcp__context7__get-library-docs:\n  - context7CompatibleLibraryID: /websites/learn_microsoft_en-us_azure_data-factory\n  - topic: \"CI/CD continuous integration deployment pipelines ARM templates\"\n  - tokens: 8000\n```\n\n## CI/CD Deployment Methods\n\n### Modern Automated Approach (Recommended)\n\n**npm Package:** `@microsoft/azure-data-factory-utilities`\n- **Latest Version:** 1.0.3+ (check npm for current version)\n- **npm URL:** https://www.npmjs.com/package/@microsoft/azure-data-factory-utilities\n- **Node.js Requirement:** Version 20.x or compatible\n\n**Key Features:**\n- Validates ADF resources independently of service\n- Generates ARM templates programmatically\n- Enables true CI/CD without manual publish button\n- Supports preview mode for selective trigger management\n\n**package.json Configuration:**\n```json\n{\n  \"scripts\": {\n    \"build\": \"node node_modules/@microsoft/azure-data-factory-utilities/lib/index\",\n    \"build-preview\": \"node node_modules/@microsoft/azure-data-factory-utilities/lib/index --preview\"\n  },\n  \"dependencies\": {\n    \"@microsoft/azure-data-factory-utilities\": \"^1.0.3\"\n  }\n}\n```\n\n**Commands:**\n```bash\n# Validate resources\nnpm run build validate <rootFolder> <factoryId>\n\n# Generate ARM templates\nnpm run build export <rootFolder> <factoryId> [outputFolder]\n\n# Preview mode (only stop/start modified triggers)\nnpm run build-preview export <rootFolder> <factoryId> [outputFolder]\n```\n\n**Official Documentation:**\n- URL: https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-improvements\n- Last Updated: January 2025\n- Topics: Setup, configuration, build commands, CI/CD integration\n\n### Traditional Manual Approach (Legacy)\n\n**Method:** Git integration + Publish button\n\n**Process:**\n1. Configure Git integration in ADF UI (Dev environment only)\n2. Make changes in ADF Studio\n3. Click \"Publish\" button to generate ARM templates\n4. Templates saved to `adf_publish` branch\n5. Release pipelines deploy from `adf_publish` branch\n\n**When to Use:**\n- Migrating from existing setup\n- No build pipeline infrastructure\n- Simple deployments without validation\n\n**Limitations:**\n- Requires manual publish action\n- No validation until publish\n- Not true CI/CD (manual step required)\n- Can't validate on pull requests\n\n**Migration Path:** Modern approach recommended for new implementations\n\n## ARM Template Deployment\n\n### PowerShell Deployment\n\n**Primary Command:** `New-AzResourceGroupDeployment`\n\n**Syntax:**\n```powershell\nNew-AzResourceGroupDeployment `\n  -ResourceGroupName \"<resource-group-name>\" `\n  -TemplateFile \"ARMTemplateForFactory.json\" `\n  -TemplateParameterFile \"ARMTemplateParametersForFactory.<environment>.json\" `\n  -factoryName \"<factory-name>\" `\n  -Mode Incremental `\n  -Verbose\n```\n\n**Validation:**\n```powershell\nTest-AzResourceGroupDeployment `\n  -ResourceGroupName \"<resource-group-name>\" `\n  -TemplateFile \"ARMTemplateForFactory.json\" `\n  -TemplateParameterFile \"ARMTemplateParametersForFactory.<environment>.json\" `\n  -factoryName \"<factory-name>\"\n```\n\n**What-If Analysis:**\n```powershell\nNew-AzResourceGroupDeployment `\n  -ResourceGroupName \"<resource-group-name>\" `\n  -TemplateFile \"ARMTemplateForFactory.json\" `\n  -TemplateParameterFile \"ARMTemplateParametersForFactory.<environment>.json\" `\n  -factoryName \"<factory-name>\" `\n  -WhatIf\n```\n\n### Azure CLI Deployment\n\n**Primary Command:** `az deployment group create`\n\n**Syntax:**\n```bash\naz deployment group create \\\n  --resource-group <resource-group-name> \\\n  --template-file ARMTemplateForFactory.json \\\n  --parameters ARMTemplateParametersForFactory.<environment>.json \\\n  --parameters factoryName=<factory-name> \\\n  --mode Incremental\n```\n\n**Validation:**\n```bash\naz deployment group validate \\\n  --resource-group <resource-group-name> \\\n  --template-file ARMTemplateForFactory.json \\\n  --parameters ARMTemplateParametersForFactory.<environment>.json \\\n  --parameters factoryName=<factory-name>\n```\n\n**What-If Analysis:**\n```bash\naz deployment group what-if \\\n  --resource-group <resource-group-name> \\\n  --template-file ARMTemplateForFactory.json \\\n  --parameters ARMTemplateParametersForFactory.<environment>.json \\\n  --parameters factoryName=<factory-name>\n```\n\n## PrePostDeploymentScript\n\n### Current Version: Ver2\n\n**Location:** https://github.com/Azure/Azure-DataFactory/blob/main/SamplesV2/ContinuousIntegrationAndDelivery/PrePostDeploymentScript.Ver2.ps1\n\n**Key Improvement in Ver2:**\n- Turns off/on ONLY triggers that have been modified\n- Ver1 stopped/started ALL triggers (slower, more disruptive)\n- Compares trigger payloads to determine changes\n\n**Download Command:**\n```bash\n# Linux/macOS/Git Bash\ncurl -o PrePostDeploymentScript.Ver2.ps1 https://raw.githubusercontent.com/Azure/Azure-DataFactory/main/SamplesV2/ContinuousIntegrationAndDelivery/PrePostDeploymentScript.Ver2.ps1\n\n# PowerShell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/Azure/Azure-DataFactory/main/SamplesV2/ContinuousIntegrationAndDelivery/PrePostDeploymentScript.Ver2.ps1\" -OutFile \"PrePostDeploymentScript.Ver2.ps1\"\n```\n\n### Parameters\n\n**Pre-Deployment (Stop Triggers):**\n```powershell\n./PrePostDeploymentScript.Ver2.ps1 `\n  -armTemplate \"<path-to-ARMTemplateForFactory.json>\" `\n  -ResourceGroupName \"<resource-group-name>\" `\n  -DataFactoryName \"<factory-name>\" `\n  -predeployment $true `\n  -deleteDeployment $false\n```\n\n**Post-Deployment (Start Triggers & Cleanup):**\n```powershell\n./PrePostDeploymentScript.Ver2.ps1 `\n  -armTemplate \"<path-to-ARMTemplateForFactory.json>\" `\n  -ResourceGroupName \"<resource-group-name>\" `\n  -DataFactoryName \"<factory-name>\" `\n  -predeployment $false `\n  -deleteDeployment $true\n```\n\n### PowerShell Requirements\n\n**Version:** PowerShell Core (7.0+) recommended\n- Azure DevOps: Use `pwsh: true` in AzurePowerShell@5 task\n- Locally: Use `pwsh` command, not `powershell`\n\n**Modules Required:**\n- Az.DataFactory\n- Az.Resources\n\n**Official Documentation:**\n- URL: https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-sample-script\n- Last Updated: January 2025\n\n## GitHub Actions CI/CD\n\n### Official Resources\n\n**Medium Article (Recent 2025):**\n- URL: https://medium.com/microsoftazure/azure-data-factory-build-and-deploy-with-new-ci-cd-flow-using-github-actions-cd46c95054e0\n- Author: Jared Zagelbaum (Microsoft Azure)\n- Topics: Modern CI/CD flow, npm package usage, GitHub Actions setup\n\n**Microsoft Community Hub:**\n- URL: https://techcommunity.microsoft.com/blog/fasttrackforazureblog/azure-data-factory-cicd-with-github-actions/3768493\n- Topics: End-to-end GitHub Actions setup, workload identity federation\n\n**Community Blog (February 2025):**\n- URL: https://linusdata.blog/2025/03/14/automating-azure-data-factory-deployments-with-github-actions/\n- Topics: Practical implementation guide, troubleshooting tips\n\n### Key GitHub Actions\n\n**Essential Actions:**\n- `actions/checkout@v4` - Checkout repository\n- `actions/setup-node@v4` - Setup Node.js\n- `actions/upload-artifact@v4` - Publish ARM templates\n- `actions/download-artifact@v4` - Download ARM templates in deploy workflow\n- `azure/login@v2` - Authenticate to Azure\n- `azure/arm-deploy@v2` - Deploy ARM templates\n- `azure/powershell@v2` - Run PrePostDeploymentScript\n\n### Authentication Methods\n\n**Service Principal (JSON credentials):**\n```json\n{\n  \"clientId\": \"<GUID>\",\n  \"clientSecret\": \"<STRING>\",\n  \"subscriptionId\": \"<GUID>\",\n  \"tenantId\": \"<GUID>\"\n}\n```\nStore in GitHub secret: `AZURE_CREDENTIALS`\n\n**Workload Identity Federation (More secure):**\n- No secrets stored\n- Uses OIDC (OpenID Connect)\n- Recommended for production\n- Setup: https://learn.microsoft.com/en-us/azure/developer/github/connect-from-azure\n\n## Azure DevOps CI/CD\n\n### Official Resources\n\n**Microsoft Learn:**\n- URL: https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-automate-azure-pipelines\n- Topics: Build pipeline, release pipeline, service connections, variable groups\n\n**Community Guides:**\n- Adam Marczak Blog: https://marczak.io/posts/2023/02/quick-cicd-for-data-factory/\n- Topics: Quick setup, best practices, folder structure\n\n**Towards Data Science:**\n- URL: https://towardsdatascience.com/azure-data-factory-ci-cd-made-simple-building-and-deploying-your-arm-templates-with-azure-devops-30c30595afa5\n- Topics: ARM template build and deployment workflow\n\n### Key Azure DevOps Tasks\n\n**Build Pipeline Tasks:**\n- `UseNode@1` - Install Node.js\n- `Npm@1` - Install packages, run build commands\n- `PublishPipelineArtifact@1` - Publish ARM templates\n\n**Release Pipeline Tasks:**\n- `DownloadPipelineArtifact@2` - Download ARM templates\n- `AzurePowerShell@5` - Run PrePostDeploymentScript\n- `AzureResourceManagerTemplateDeployment@3` - Deploy ARM template\n\n### Service Connection Requirements\n\n**Permissions Needed:**\n- Data Factory Contributor (on all Data Factories)\n- Contributor (on Resource Groups)\n- Key Vault access policies (if using secrets)\n\n**Configuration:**\n- Project Settings  Service connections  New service connection\n- Type: Azure Resource Manager\n- Authentication: Service Principal (recommended) or Managed Identity\n\n## Troubleshooting Resources\n\n### Official Troubleshooting Guide\n\n**URL:** https://learn.microsoft.com/en-us/azure/data-factory/ci-cd-github-troubleshoot-guide\n**Last Updated:** January 2025\n\n**Common Issues Covered:**\n1. Template parameter validation errors\n2. Integration Runtime type cannot be changed\n3. ARM template size exceeds 4MB limit\n4. Git connection problems\n5. Authentication failures\n6. Deployment errors\n\n### Diagnostic Logs\n\n**Enable Diagnostic Settings:**\n```\nAzure Portal  Data Factory  Diagnostic settings  Add diagnostic setting\nSend to: Log Analytics workspace\n\nLogs to Enable:\n- PipelineRuns\n- TriggerRuns\n- ActivityRuns\n- SandboxPipelineRuns\n- SandboxActivityRuns\n```\n\n**Kusto Queries for Troubleshooting:**\n\n```kusto\n// Failed pipeline runs in last 24 hours\nADFPipelineRun\n| where Status == \"Failed\"\n| where TimeGenerated > ago(24h)\n| project TimeGenerated, PipelineName, RunId, Status, ErrorMessage, Parameters\n| order by TimeGenerated desc\n\n// Failed CI/CD deployments\nADFActivityRun\n| where ActivityType == \"ExecutePipeline\"\n| where Status == \"Failed\"\n| where TimeGenerated > ago(7d)\n| project TimeGenerated, PipelineName, ActivityName, ErrorCode, ErrorMessage\n| order by TimeGenerated desc\n\n// Performance analysis\nADFActivityRun\n| where TimeGenerated > ago(7d)\n| extend DurationMinutes = datetime_diff('minute', End, Start)\n| summarize AvgDuration = avg(DurationMinutes) by ActivityType, ActivityName\n| where AvgDuration > 10\n| order by AvgDuration desc\n```\n\n### Common Error Patterns\n\n**Error: \"Template parameters are not valid\"**\n- Cause: Deleted triggers still referenced in parameters\n- Solution: Regenerate ARM template or use PrePostDeploymentScript cleanup\n\n**Error: \"Updating property type is not supported\"**\n- Cause: Trying to change Integration Runtime type\n- Solution: Delete and recreate IR (not in-place update)\n\n**Error: \"Operation timed out\"**\n- Cause: Network connectivity, large data volume, insufficient compute\n- Solution: Increase timeout, optimize query, increase DIUs\n\n**Error: \"Authentication failed\"**\n- Cause: Service principal expired, missing permissions, wrong credentials\n- Solution: Verify credentials, check role assignments, renew if expired\n\n## Best Practices\n\n### Repository Structure\n\n**Recommended Folder Layout:**\n```\nrepository-root/\n adf-resources/          # ADF JSON files (if using npm approach)\n    dataset/\n    pipeline/\n    trigger/\n    linkedService/\n    integrationRuntime/\n .github/\n    workflows/          # GitHub Actions workflows\n        adf-build.yml\n        adf-deploy.yml\n azure-pipelines/        # Azure DevOps pipelines\n    build.yml\n    release.yml\n parameters/             # Environment-specific parameters\n    ARMTemplateParametersForFactory.dev.json\n    ARMTemplateParametersForFactory.test.json\n    ARMTemplateParametersForFactory.prod.json\n package.json            # npm configuration\n README.md\n```\n\n### Git Configuration\n\n**Only Configure Git on Development ADF:**\n- Development: Git-integrated for source control\n- Test: CI/CD deployment only (no Git)\n- Production: CI/CD deployment only (no Git)\n\n**Rationale:** Prevents accidental manual changes in higher environments\n\n### Multi-Environment Strategy\n\n```\nEnvironment Flow:\nDev (Git)  Build  Test  Approval  Production\n            \n        ARM Templates\n```\n\n**Parameter Management:**\n- Separate parameter file per environment\n- Store secrets in Azure Key Vault\n- Reference Key Vault in parameter files\n- Never commit secrets to source control\n\n### Monitoring and Alerting\n\n**Set up alerts for:**\n- Build pipeline failures\n- Deployment failures\n- Pipeline run failures\n- Performance degradation\n- Cost anomalies\n\n**Recommended Tools:**\n- Azure Monitor (Metrics and Alerts)\n- Log Analytics (Kusto queries)\n- Application Insights (for custom logging)\n- Azure Advisor (optimization recommendations)\n\n## Additional Resources\n\n### GitHub Repositories\n\n**Official Azure Data Factory Samples:**\n- URL: https://github.com/Azure/Azure-DataFactory\n- Path: SamplesV2/ContinuousIntegrationAndDelivery/\n- Contents: PrePostDeploymentScript.Ver2.ps1, example pipelines, documentation\n\n**Community Examples:**\n- Search GitHub for \"azure-data-factory-cicd\" for real-world examples\n- Many organizations publish their CI/CD patterns as reference\n\n### Community Support\n\n**Microsoft Q&A:**\n- URL: https://learn.microsoft.com/en-us/answers/tags/130/azure-data-factory\n- Active community, Microsoft employees respond\n\n**Stack Overflow:**\n- Tag: `azure-data-factory`\n- Large knowledge base of resolved issues\n\n**Azure Status:**\n- URL: https://status.azure.com\n- Check for service outages and incidents\n\n## When to Fetch Latest Information\n\n**Situations requiring current documentation:**\n1. npm package version updates\n2. New ADF features or activities\n3. Changes to ARM template schema\n4. Updates to PrePostDeploymentScript\n5. New GitHub Actions or Azure DevOps tasks\n6. Breaking changes or deprecations\n\n**How to Fetch:**\n- Use WebFetch for Microsoft Learn articles\n- Check npm for latest package version\n- Use Context7 for comprehensive topic coverage\n- Review Azure Data Factory GitHub for script updates\n\nThis knowledge base should be your starting point for all Azure Data Factory questions. Always verify critical information with the latest official documentation when making production decisions.\n\n## Progressive Disclosure References\n\nFor detailed JSON schemas and complete reference materials, see:\n\n- **Activity Types**: `references/activity-types.md` - Complete JSON schemas for all activity types (Copy, ForEach, IfCondition, Switch, Until, Lookup, ExecutePipeline, WebActivity, DatabricksJob, SetVariable, AppendVariable, Wait, Fail, GetMetadata)\n- **Expression Functions**: `references/expression-functions.md` - Complete reference for all ADF expression functions (string, collection, logical, conversion, math, date/time, pipeline/activity references)\n- **Linked Services**: `references/linked-services.md` - Complete JSON configurations for all connector types (Blob Storage, ADLS Gen2, Azure SQL, Synapse, Fabric Lakehouse/Warehouse, Databricks, Key Vault, REST, SFTP, Snowflake, PostgreSQL)\n- **Triggers**: `references/triggers.md` - Complete JSON schemas for schedule, tumbling window, and event triggers\n- **Datasets**: `references/datasets.md` - Complete JSON schemas for all dataset types with parameterization patterns"
              },
              {
                "name": "adf-validation-rules",
                "description": "Comprehensive Azure Data Factory validation rules, activity nesting limitations, linked service requirements, and edge-case handling guidance",
                "path": "plugins/adf-master/skills/adf-validation-rules/SKILL.md",
                "frontmatter": {
                  "name": "adf-validation-rules",
                  "description": "Comprehensive Azure Data Factory validation rules, activity nesting limitations, linked service requirements, and edge-case handling guidance"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Azure Data Factory Validation Rules and Limitations\n\n##  CRITICAL: Activity Nesting Limitations\n\nAzure Data Factory has **STRICT** nesting rules for control flow activities. Violating these rules will cause pipeline failures or prevent pipeline creation.\n\n### Supported Control Flow Activities for Nesting\n\nFour control flow activities support nested activities:\n- **ForEach**: Iterates over collections and executes activities in a loop\n- **If Condition**: Branches based on true/false evaluation\n- **Until**: Implements do-until loops with timeout options\n- **Switch**: Evaluates activities matching case conditions\n\n###  PERMITTED Nesting Combinations\n\n| Parent Activity | Can Contain | Notes |\n|----------------|-------------|-------|\n| **ForEach** | If Condition |  Allowed |\n| **ForEach** | Switch |  Allowed |\n| **Until** | If Condition |  Allowed |\n| **Until** | Switch |  Allowed |\n\n###  PROHIBITED Nesting Combinations\n\n| Parent Activity | CANNOT Contain | Reason |\n|----------------|----------------|---------|\n| **If Condition** | ForEach |  Not supported - use Execute Pipeline workaround |\n| **If Condition** | Switch |  Not supported - use Execute Pipeline workaround |\n| **If Condition** | Until |  Not supported - use Execute Pipeline workaround |\n| **If Condition** | Another If |  Cannot nest If within If |\n| **Switch** | ForEach |  Not supported - use Execute Pipeline workaround |\n| **Switch** | If Condition |  Not supported - use Execute Pipeline workaround |\n| **Switch** | Until |  Not supported - use Execute Pipeline workaround |\n| **Switch** | Another Switch |  Cannot nest Switch within Switch |\n| **ForEach** | Another ForEach |  Single level only - use Execute Pipeline workaround |\n| **Until** | Another Until |  Single level only - use Execute Pipeline workaround |\n| **ForEach** | Until |  Single level only - use Execute Pipeline workaround |\n| **Until** | ForEach |  Single level only - use Execute Pipeline workaround |\n\n###  Special Activity Restrictions\n\n**Validation Activity**:\n-  **CANNOT** be placed inside ANY nested activity\n-  **CANNOT** be used within ForEach, If, Switch, or Until activities\n-  Must be at pipeline root level only\n\n###  Workaround: Execute Pipeline Pattern\n\n**The ONLY supported workaround for prohibited nesting combinations:**\n\nInstead of direct nesting, use the **Execute Pipeline Activity** to call a child pipeline:\n\n```json\n{\n  \"name\": \"ParentPipeline_WithIfCondition\",\n  \"activities\": [\n    {\n      \"name\": \"IfCondition_Parent\",\n      \"type\": \"IfCondition\",\n      \"typeProperties\": {\n        \"expression\": \"@equals(pipeline().parameters.ProcessData, 'true')\",\n        \"ifTrueActivities\": [\n          {\n            \"name\": \"ExecuteChildPipeline_WithForEach\",\n            \"type\": \"ExecutePipeline\",\n            \"typeProperties\": {\n              \"pipeline\": {\n                \"referenceName\": \"ChildPipeline_ForEachLoop\",\n                \"type\": \"PipelineReference\"\n              },\n              \"parameters\": {\n                \"ItemList\": \"@pipeline().parameters.Items\"\n              }\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n**Child Pipeline Structure:**\n```json\n{\n  \"name\": \"ChildPipeline_ForEachLoop\",\n  \"parameters\": {\n    \"ItemList\": {\"type\": \"array\"}\n  },\n  \"activities\": [\n    {\n      \"name\": \"ForEach_InChildPipeline\",\n      \"type\": \"ForEach\",\n      \"typeProperties\": {\n        \"items\": \"@pipeline().parameters.ItemList\",\n        \"activities\": [\n          // Your ForEach logic here\n        ]\n      }\n    }\n  ]\n}\n```\n\n**Why This Works:**\n- Each pipeline can have ONE level of nesting\n- Execute Pipeline creates a new pipeline context\n- Child pipeline gets its own nesting level allowance\n- Enables unlimited depth through pipeline chaining\n\n##  Activity and Resource Limits\n\n### Pipeline Limits\n| Resource | Limit | Notes |\n|----------|-------|-------|\n| **Activities per pipeline** | 80 | Includes inner activities for containers |\n| **Parameters per pipeline** | 50 | - |\n| **ForEach concurrent iterations** | 50 (maximum) | Set via `batchCount` property |\n| **ForEach items** | 100,000 | - |\n| **Lookup activity rows** | 5,000 | Maximum rows returned |\n| **Lookup activity size** | 4 MB | Maximum size of returned data |\n| **Web activity timeout** | 1 hour | Default timeout for Web activities |\n| **Copy activity timeout** | 7 days | Maximum execution time |\n\n### ForEach Activity Configuration\n```json\n{\n  \"name\": \"ForEachActivity\",\n  \"type\": \"ForEach\",\n  \"typeProperties\": {\n    \"items\": \"@pipeline().parameters.ItemList\",\n    \"isSequential\": false,  // false = parallel execution\n    \"batchCount\": 50,       // Max 50 concurrent iterations\n    \"activities\": [\n      // Nested activities\n    ]\n  }\n}\n```\n\n**Critical Considerations:**\n- `isSequential: true`  Executes one item at a time (slow but predictable)\n- `isSequential: false`  Executes up to `batchCount` items in parallel\n- Maximum `batchCount` is **50** regardless of setting\n- **Cannot use Set Variable activity** inside parallel ForEach (variable scope is pipeline-level)\n\n### Set Variable Activity Limitations\n **CANNOT** use `Set Variable` inside ForEach with `isSequential: false`\n- Reason: Variables are pipeline-scoped, not ForEach-scoped\n- Multiple parallel iterations would cause race conditions\n-  **Alternative**: Use `Append Variable` with array type, or use sequential execution\n\n##  Linked Services: Azure Blob Storage\n\n### Authentication Methods\n\n#### 1. Account Key (Basic)\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": {\n      \"type\": \"SecureString\",\n      \"value\": \"DefaultEndpointsProtocol=https;AccountName=<account>;AccountKey=<key>\"\n    }\n  }\n}\n```\n** Limitations:**\n- Secondary Blob service endpoints are **NOT supported**\n- **Security Risk**: Account keys should be stored in Azure Key Vault\n\n#### 2. Shared Access Signature (SAS)\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"sasUri\": {\n      \"type\": \"SecureString\",\n      \"value\": \"https://<account>.blob.core.windows.net/<container>?<SAS-token>\"\n    }\n  }\n}\n```\n**Critical Requirements:**\n- Dataset `folderPath` must be **absolute path from container level**\n- SAS token expiry **must extend beyond pipeline execution**\n- SAS URI path must align with dataset configuration\n\n#### 3. Service Principal\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://<account>.blob.core.windows.net\",\n    \"accountKind\": \"StorageV2\",  // REQUIRED for service principal\n    \"servicePrincipalId\": \"<client-id>\",\n    \"servicePrincipalCredential\": {\n      \"type\": \"SecureString\",\n      \"value\": \"<client-secret>\"\n    },\n    \"tenant\": \"<tenant-id>\"\n  }\n}\n```\n**Critical Requirements:**\n- `accountKind` **MUST** be set (StorageV2, BlobStorage, or BlockBlobStorage)\n- Service Principal requires **Storage Blob Data Reader** (source) or **Storage Blob Data Contributor** (sink) role\n-  **NOT compatible** with soft-deleted blob accounts in Data Flow\n\n#### 4. Managed Identity (Recommended)\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://<account>.blob.core.windows.net\",\n    \"accountKind\": \"StorageV2\"  // REQUIRED for managed identity\n  },\n  \"connectVia\": {\n    \"referenceName\": \"AutoResolveIntegrationRuntime\",\n    \"type\": \"IntegrationRuntimeReference\"\n  }\n}\n```\n**Critical Requirements:**\n- `accountKind` **MUST** be specified (cannot be empty or \"Storage\")\n-  Empty or \"Storage\" account kind will cause Data Flow failures\n- Managed identity must have **Storage Blob Data Reader/Contributor** role assigned\n- For Storage firewall: **Must enable \"Allow trusted Microsoft services\"**\n\n### Common Blob Storage Pitfalls\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Data Flow fails with managed identity | `accountKind` empty or \"Storage\" | Set `accountKind` to StorageV2 |\n| Secondary endpoint doesn't work | Using account key auth | Not supported - use different auth method |\n| SAS token expired during run | Token expiry too short | Extend SAS token validity period |\n| Cannot access $logs container | System container not visible in UI | Use direct path reference |\n| Soft-deleted blobs inaccessible | Service principal/managed identity | Use account key or SAS instead |\n| Private endpoint connection fails | Wrong endpoint for Data Flow | Ensure ADLS Gen2 private endpoint exists |\n\n##  Linked Services: Azure SQL Database\n\n### Authentication Methods\n\n#### 1. SQL Authentication\n```json\n{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"server\": \"<server-name>.database.windows.net\",\n    \"database\": \"<database-name>\",\n    \"authenticationType\": \"SQL\",\n    \"userName\": \"<username>\",\n    \"password\": {\n      \"type\": \"SecureString\",\n      \"value\": \"<password>\"\n    }\n  }\n}\n```\n**Best Practice:**\n- Store password in Azure Key Vault\n- Use connection string with Key Vault reference\n\n#### 2. Service Principal\n```json\n{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"server\": \"<server-name>.database.windows.net\",\n    \"database\": \"<database-name>\",\n    \"authenticationType\": \"ServicePrincipal\",\n    \"servicePrincipalId\": \"<client-id>\",\n    \"servicePrincipalCredential\": {\n      \"type\": \"SecureString\",\n      \"value\": \"<client-secret>\"\n    },\n    \"tenant\": \"<tenant-id>\"\n  }\n}\n```\n**Requirements:**\n- Microsoft Entra admin must be configured on SQL server\n- Service principal must have contained database user created\n- Grant appropriate role: `db_datareader`, `db_datawriter`, etc.\n\n#### 3. Managed Identity\n```json\n{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"server\": \"<server-name>.database.windows.net\",\n    \"database\": \"<database-name>\",\n    \"authenticationType\": \"SystemAssignedManagedIdentity\"\n  }\n}\n```\n**Requirements:**\n- Create contained database user for managed identity\n- Grant appropriate database roles\n- Configure firewall to allow Azure services (or specific IP ranges)\n\n### SQL Database Configuration Best Practices\n\n#### Connection String Parameters\n```\nServer=tcp:<server>.database.windows.net,1433;\nDatabase=<database>;\nEncrypt=mandatory;          // Options: mandatory, optional, strict\nTrustServerCertificate=false;\nConnectTimeout=30;\nCommandTimeout=120;\nPooling=true;\nConnectRetryCount=3;\nConnectRetryInterval=10;\n```\n\n**Critical Parameters:**\n- `Encrypt`: Default is `mandatory` (recommended)\n- `Pooling`: Set to `false` if experiencing idle connection issues\n- `ConnectRetryCount`: Recommended for transient fault handling\n- `ConnectRetryInterval`: Seconds between retries\n\n### Common SQL Database Pitfalls\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Serverless tier auto-paused | Pipeline doesn't wait for resume | Implement retry logic or keep-alive |\n| Connection pool timeout | Idle connections closed | Add `Pooling=false` or configure retry |\n| Firewall blocks connection | IP not whitelisted | Add Azure IR IPs or enable Azure services |\n| Always Encrypted fails in Data Flow | Not supported for sink | Use service principal/managed identity in copy activity |\n| Decimal precision loss | Copy supports up to 28 precision | Use string type for higher precision |\n| Parallel copy not working | No partition configuration | Enable physical or dynamic range partitioning |\n\n### Performance Optimization\n\n#### Parallel Copy Configuration\n```json\n{\n  \"source\": {\n    \"type\": \"AzureSqlSource\",\n    \"partitionOption\": \"PhysicalPartitionsOfTable\"  // or \"DynamicRange\"\n  },\n  \"parallelCopies\": 8,  // Recommended: (DIU or IR nodes)  (2 to 4)\n  \"enableStaging\": true,\n  \"stagingSettings\": {\n    \"linkedServiceName\": {\n      \"referenceName\": \"AzureBlobStorage\",\n      \"type\": \"LinkedServiceReference\"\n    }\n  }\n}\n```\n\n**Partition Options:**\n- `PhysicalPartitionsOfTable`: Uses SQL Server physical partitions\n- `DynamicRange`: Creates logical partitions based on column values\n- `None`: No partitioning (default)\n\n**Staging Best Practices:**\n- Always use staging for large data movements (> 1GB)\n- Use PolyBase or COPY statement for best performance\n- Parquet format recommended for staging files\n\n##  Data Flow Limitations\n\n### General Limits\n- **Column name length**: 128 characters maximum\n- **Row size**: 1 MB maximum (some sinks like SQL have lower limits)\n- **String column size**: Varies by sink (SQL: 8000 for varchar, 4000 for nvarchar)\n\n### Transformation-Specific Limits\n| Transformation | Limitation |\n|----------------|------------|\n| **Lookup** | Cache size limited by cluster memory |\n| **Join** | Large joins may cause memory errors |\n| **Pivot** | Maximum 10,000 unique values |\n| **Window** | Requires partitioning for large datasets |\n\n### Performance Considerations\n- **Partitioning**: Always partition large datasets before transformations\n- **Broadcast**: Use broadcast hint for small dimension tables\n- **Sink optimization**: Enable table option \"Recreate\" instead of \"Truncate\" for better performance\n\n##  Validation Checklist for Pipeline Creation\n\n### Before Creating Pipeline\n- [ ] Verify activity nesting follows permitted combinations\n- [ ] Check ForEach activities don't contain other ForEach/Until\n- [ ] Verify If/Switch activities don't contain ForEach/Until/If/Switch\n- [ ] Ensure Validation activities are at pipeline root level only\n- [ ] Confirm total activities < 80 per pipeline\n- [ ] Verify no Set Variable activities in parallel ForEach\n\n### Linked Service Validation\n- [ ] **Blob Storage**: If using managed identity/service principal, `accountKind` is set\n- [ ] **SQL Database**: Authentication method matches security requirements\n- [ ] **All services**: Secrets stored in Key Vault, not hardcoded\n- [ ] **All services**: Firewall rules configured for integration runtime IPs\n- [ ] **Network**: Private endpoints configured if using VNet integration\n\n### Activity Configuration Validation\n- [ ] **ForEach**: `batchCount`  50 if parallel execution\n- [ ] **Lookup**: Query returns < 5000 rows and < 4 MB data\n- [ ] **Copy**: DIU configured appropriately (2-256 for Azure IR)\n- [ ] **Copy**: Staging enabled for large data movements\n- [ ] **All activities**: Timeout values appropriate for expected execution time\n- [ ] **All activities**: Retry logic configured for transient failures\n\n### Data Flow Validation\n- [ ] Column names  128 characters\n- [ ] Source query doesn't return > 1 MB per row\n- [ ] Partitioning configured for large datasets\n- [ ] Sink has appropriate schema and data type mappings\n- [ ] Staging linked service configured for optimal performance\n\n##  Automated Validation Script\n\n**CRITICAL: Always run automated validation before committing or deploying ADF pipelines!**\n\nThe adf-master plugin includes a comprehensive PowerShell validation script that checks for ALL the rules and limitations documented above.\n\n### Using the Validation Script\n\n**Location:** `${CLAUDE_PLUGIN_ROOT}/scripts/validate-adf-pipelines.ps1`\n\n**Basic usage:**\n```powershell\n# From the root of your ADF repository\npwsh -File validate-adf-pipelines.ps1\n```\n\n**With custom paths:**\n```powershell\npwsh -File validate-adf-pipelines.ps1 `\n    -PipelinePath \"path/to/pipeline\" `\n    -DatasetPath \"path/to/dataset\"\n```\n\n**With strict mode (additional warnings):**\n```powershell\npwsh -File validate-adf-pipelines.ps1 -Strict\n```\n\n### What the Script Validates\n\nThe automated validation script checks for issues that Microsoft's official `@microsoft/azure-data-factory-utilities` package does **NOT** validate:\n\n1. **Activity Nesting Violations:**\n   - ForEach  ForEach, Until, Validation\n   - Until  Until, ForEach, Validation\n   - IfCondition  ForEach, If, IfCondition, Switch, Until, Validation\n   - Switch  ForEach, If, IfCondition, Switch, Until, Validation\n\n2. **Resource Limits:**\n   - Pipeline activity count (max 120, warn at 100)\n   - Pipeline parameter count (max 50)\n   - Pipeline variable count (max 50)\n   - ForEach batchCount limit (max 50, warn at 30 in strict mode)\n\n3. **Variable Scope Violations:**\n   - SetVariable in parallel ForEach (causes race conditions)\n   - Proper AppendVariable vs SetVariable usage\n\n4. **Dataset Configuration Issues:**\n   - Missing fileName or wildcardFileName for file-based datasets\n   - AzureBlobFSLocation missing required fileSystem property\n   - Missing required properties for DelimitedText, Json, Parquet types\n\n5. **Copy Activity Validations:**\n   - Source/sink type compatibility with dataset types\n   - Lookup activity firstRowOnly=false warnings (5000 row/4MB limits)\n   - Blob file dependencies (additionalColumns logging pattern)\n\n### Integration with CI/CD\n\n**GitHub Actions example:**\n```yaml\n- name: Validate ADF Pipelines\n  run: |\n    pwsh -File validate-adf-pipelines.ps1 -PipelinePath pipeline -DatasetPath dataset\n  shell: pwsh\n```\n\n**Azure DevOps example:**\n```yaml\n- task: PowerShell@2\n  displayName: 'Validate ADF Pipelines'\n  inputs:\n    filePath: 'validate-adf-pipelines.ps1'\n    arguments: '-PipelinePath pipeline -DatasetPath dataset'\n    pwsh: true\n```\n\n### Command Reference\n\nUse the `/adf-validate` command to run the validation script with proper guidance:\n\n```bash\n/adf-validate\n```\n\nThis command will:\n1. Detect your ADF repository structure\n2. Run the validation script with appropriate paths\n3. Parse and explain any errors or warnings found\n4. Provide specific solutions for each violation\n5. Recommend next actions based on results\n6. Suggest CI/CD integration patterns\n\n### Exit Codes\n\n- **0**: Validation passed (no errors)\n- **1**: Validation failed (errors found - DO NOT DEPLOY)\n\n### Best Practices\n\n1. **Run validation before every commit** to catch issues early\n2. **Add validation to CI/CD pipeline** to prevent invalid deployments\n3. **Use strict mode during development** for additional warnings\n4. **Re-validate after bulk changes** or generated pipelines\n5. **Document validation exceptions** if you must bypass a warning\n6. **Share validation results with team** to prevent repeated mistakes\n\n##  CRITICAL: Enforcement Protocol\n\n**When creating or modifying ADF pipelines:**\n\n1. **ALWAYS validate activity nesting** against the permitted/prohibited table\n2. **REJECT** any attempt to create prohibited nesting combinations\n3. **SUGGEST** Execute Pipeline workaround for complex nesting needs\n4. **VALIDATE** linked service authentication matches the connector type\n5. **CHECK** all limits (activities, parameters, ForEach iterations, etc.)\n6. **VERIFY** required properties are set (e.g., `accountKind` for managed identity)\n7. **WARN** about common pitfalls specific to the connector being used\n\n**Example Validation Response:**\n```\n INVALID PIPELINE STRUCTURE DETECTED:\n\nIssue: ForEach activity contains another ForEach activity\nLocation: Pipeline \"PL_DataProcessing\"  ForEach \"OuterLoop\"  ForEach \"InnerLoop\"\n\nThis violates Azure Data Factory nesting rules:\n- ForEach activities support only a SINGLE level of nesting\n- You CANNOT nest ForEach within ForEach\n\n RECOMMENDED SOLUTION:\nUse the Execute Pipeline pattern:\n1. Create a child pipeline with the inner ForEach logic\n2. Replace the inner ForEach with an Execute Pipeline activity\n3. Pass required parameters to the child pipeline\n\nWould you like me to generate the refactored pipeline structure?\n```\n\n##  Reference Documentation\n\n**Official Microsoft Learn Resources:**\n- Activity nesting: https://learn.microsoft.com/en-us/azure/data-factory/concepts-nested-activities\n- Blob Storage connector: https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage\n- SQL Database connector: https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database\n- Pipeline limits: https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits#data-factory-limits\n\n**Last Updated:** 2025-01-24 (Based on official Microsoft documentation)\n\nThis validation rules skill MUST be consulted before creating or modifying ANY Azure Data Factory pipeline to ensure compliance with platform limitations and best practices.\n\n## Progressive Disclosure References\n\nFor detailed validation matrices and resource limits, see:\n\n- **Nesting Rules**: `references/nesting-rules.md` - Complete matrix of permitted and prohibited activity nesting combinations with workaround patterns\n- **Resource Limits**: `references/resource-limits.md` - Complete reference for all ADF limits (pipeline, activity, trigger, data flow, integration runtime, expression, API)"
              },
              {
                "name": "databricks-2025",
                "description": "Databricks Job activity and 2025 Azure Data Factory connectors",
                "path": "plugins/adf-master/skills/databricks-2025/SKILL.md",
                "frontmatter": {
                  "name": "databricks-2025",
                  "description": "Databricks Job activity and 2025 Azure Data Factory connectors"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Azure Data Factory Databricks Integration 2025\n\n## Databricks Job Activity (Recommended 2025)\n\n** CRITICAL UPDATE (2025):** The Databricks Job activity is now the **ONLY recommended method** for orchestrating Databricks in ADF. Microsoft strongly recommends migrating from legacy Notebook, Python, and JAR activities.\n\n### Why Databricks Job Activity?\n\n**Old Pattern (Notebook Activity -  LEGACY):**\n```json\n{\n  \"name\": \"RunNotebook\",\n  \"type\": \"DatabricksNotebook\",  //  DEPRECATED - Migrate to DatabricksJob\n  \"linkedServiceName\": { \"referenceName\": \"DatabricksLinkedService\" },\n  \"typeProperties\": {\n    \"notebookPath\": \"/Users/user@example.com/MyNotebook\",\n    \"baseParameters\": { \"param1\": \"value1\" }\n  }\n}\n```\n\n**New Pattern (Databricks Job Activity -  CURRENT 2025):**\n```json\n{\n  \"name\": \"RunDatabricksWorkflow\",\n  \"type\": \"DatabricksJob\",  //  CORRECT activity type (NOT DatabricksSparkJob)\n  \"linkedServiceName\": { \"referenceName\": \"DatabricksLinkedService\" },\n  \"typeProperties\": {\n    \"jobId\": \"123456\",  // Reference existing Databricks Workflow Job\n    \"jobParameters\": {  // Pass parameters to the Job\n      \"param1\": \"value1\",\n      \"runDate\": \"@pipeline().parameters.ProcessingDate\"\n    }\n  },\n  \"policy\": {\n    \"timeout\": \"0.12:00:00\",\n    \"retry\": 2,\n    \"retryIntervalInSeconds\": 30\n  }\n}\n```\n\n### Benefits of Databricks Job Activity (2025)\n\n1. **Serverless Execution by Default:**\n   -  No cluster specification needed in linked service\n   -  Automatically runs on Databricks serverless compute\n   -  Faster startup times and lower costs\n   -  Managed infrastructure by Databricks\n\n2. **Advanced Workflow Features:**\n   -  **Run As** - Execute jobs as specific users/service principals\n   -  **Task Values** - Pass data between tasks within workflow\n   -  **Conditional Execution** - If/Else and For Each task types\n   -  **AI/BI Tasks** - Model serving endpoints, Power BI semantic models\n   -  **Repair Runs** - Rerun failed tasks without reprocessing successful ones\n   -  **Notifications/Alerts** - Built-in alerting on job failures\n   -  **Git Integration** - Version control for notebooks and code\n   -  **DABs Support** - Databricks Asset Bundles for deployment\n   -  **Built-in Lineage** - Data lineage tracking across tasks\n   -  **Queuing and Concurrent Runs** - Better resource management\n\n3. **Centralized Job Management:**\n   - Jobs defined once in Databricks workspace\n   - Single source of truth for all environments\n   - Versioning through Databricks (Git-backed)\n   - Consistent across orchestration tools\n\n4. **Better Orchestration:**\n   - Complex task dependencies within Job\n   - Multiple heterogeneous tasks (notebook, Python, SQL, Delta Live Tables)\n   - Job-level monitoring and logging\n   - Parameter passing between tasks\n\n5. **Improved Reliability:**\n   - Retry logic at Job and task level\n   - Better error handling and recovery\n   - Automatic cluster management\n\n6. **Cost Optimization:**\n   - Serverless compute (pay only for execution)\n   - Job clusters (auto-terminating)\n   - Optimized cluster sizing per task\n   - Spot instance support\n\n### Implementation\n\n#### 1. Create Databricks Job\n\n```python\n# In Databricks workspace\n# Create Job with tasks\n{\n  \"name\": \"Data Processing Job\",\n  \"tasks\": [\n    {\n      \"task_key\": \"ingest\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/Notebooks/Ingest\",\n        \"base_parameters\": {}\n      },\n      \"job_cluster_key\": \"small_cluster\"\n    },\n    {\n      \"task_key\": \"transform\",\n      \"depends_on\": [{ \"task_key\": \"ingest\" }],\n      \"notebook_task\": {\n        \"notebook_path\": \"/Notebooks/Transform\"\n      },\n      \"job_cluster_key\": \"medium_cluster\"\n    },\n    {\n      \"task_key\": \"load\",\n      \"depends_on\": [{ \"task_key\": \"transform\" }],\n      \"notebook_task\": {\n        \"notebook_path\": \"/Notebooks/Load\"\n      },\n      \"job_cluster_key\": \"small_cluster\"\n    }\n  ],\n  \"job_clusters\": [\n    {\n      \"job_cluster_key\": \"small_cluster\",\n      \"new_cluster\": {\n        \"spark_version\": \"13.3.x-scala2.12\",\n        \"node_type_id\": \"Standard_DS3_v2\",\n        \"num_workers\": 2\n      }\n    },\n    {\n      \"job_cluster_key\": \"medium_cluster\",\n      \"new_cluster\": {\n        \"spark_version\": \"13.3.x-scala2.12\",\n        \"node_type_id\": \"Standard_DS4_v2\",\n        \"num_workers\": 8\n      }\n    }\n  ]\n}\n\n# Get Job ID after creation\n```\n\n#### 2. Create ADF Pipeline with Databricks Job Activity (2025)\n\n```json\n{\n  \"name\": \"PL_Databricks_Serverless_Workflow\",\n  \"properties\": {\n    \"activities\": [\n      {\n        \"name\": \"ExecuteDatabricksWorkflow\",\n        \"type\": \"DatabricksJob\",  //  Correct activity type\n        \"dependsOn\": [],\n        \"policy\": {\n          \"timeout\": \"0.12:00:00\",\n          \"retry\": 2,\n          \"retryIntervalInSeconds\": 30\n        },\n        \"typeProperties\": {\n          \"jobId\": \"123456\",  // Databricks Job ID from workspace\n          \"jobParameters\": {  //  Use jobParameters (not parameters)\n            \"input_path\": \"/mnt/data/input\",\n            \"output_path\": \"/mnt/data/output\",\n            \"run_date\": \"@pipeline().parameters.runDate\",\n            \"environment\": \"@pipeline().parameters.environment\"\n          }\n        },\n        \"linkedServiceName\": {\n          \"referenceName\": \"DatabricksLinkedService_Serverless\",\n          \"type\": \"LinkedServiceReference\"\n        }\n      },\n      {\n        \"name\": \"LogJobExecution\",\n        \"type\": \"WebActivity\",\n        \"dependsOn\": [\n          {\n            \"activity\": \"ExecuteDatabricksWorkflow\",\n            \"dependencyConditions\": [\"Succeeded\"]\n          }\n        ],\n        \"typeProperties\": {\n          \"url\": \"@pipeline().parameters.LoggingEndpoint\",\n          \"method\": \"POST\",\n          \"body\": {\n            \"jobId\": \"123456\",\n            \"runId\": \"@activity('ExecuteDatabricksWorkflow').output.runId\",\n            \"status\": \"Succeeded\",\n            \"duration\": \"@activity('ExecuteDatabricksWorkflow').output.executionDuration\"\n          }\n        }\n      }\n    ],\n    \"parameters\": {\n      \"runDate\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"@utcnow()\"\n      },\n      \"environment\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"production\"\n      },\n      \"LoggingEndpoint\": {\n        \"type\": \"string\"\n      }\n    }\n  }\n}\n```\n\n#### 3. Configure Linked Service (2025 - Serverless)\n\n** RECOMMENDED: Serverless Linked Service (No Cluster Configuration)**\n```json\n{\n  \"name\": \"DatabricksLinkedService_Serverless\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"AzureDatabricks\",\n    \"typeProperties\": {\n      \"domain\": \"https://adb-123456789.azuredatabricks.net\",\n      \"authentication\": \"MSI\"  //  Managed Identity (recommended 2025)\n      //  NO existingClusterId or newClusterNodeType needed for serverless!\n      // The Databricks Job activity automatically uses serverless compute\n    }\n  }\n}\n```\n\n**Alternative: Access Token Authentication**\n```json\n{\n  \"name\": \"DatabricksLinkedService_Token\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"AzureDatabricks\",\n    \"typeProperties\": {\n      \"domain\": \"https://adb-123456789.azuredatabricks.net\",\n      \"accessToken\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"databricks-access-token\"\n      }\n    }\n  }\n}\n```\n\n** CRITICAL: For Databricks Job activity, DO NOT specify cluster properties in the linked service. The job configuration in Databricks workspace controls compute resources.**\n\n##  2025 New Connectors and Enhancements\n\n### ServiceNow V2 Connector (RECOMMENDED - V1 End of Support)\n\n** CRITICAL: ServiceNow V1 connector is at End of Support stage. Migrate to V2 immediately!**\n\n**Key Features of V2:**\n-  **Native Query Builder** - Aligns with ServiceNow's condition builder experience\n-  **Enhanced Performance** - Optimized data extraction\n-  **Better Error Handling** - Improved diagnostics and retry logic\n-  **OData Support** - Modern API integration patterns\n\n**Copy Activity Example:**\n```json\n{\n  \"name\": \"CopyFromServiceNowV2\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"referenceName\": \"ServiceNowV2Source\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"referenceName\": \"AzureSqlSink\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"ServiceNowV2Source\",\n      \"query\": \"sysparm_query=active=true^priority=1^sys_created_on>=javascript:gs.dateGenerate('2025-01-01')\",\n      \"httpRequestTimeout\": \"00:01:40\"  // 100 seconds\n    },\n    \"sink\": {\n      \"type\": \"AzureSqlSink\",\n      \"writeBehavior\": \"upsert\",\n      \"upsertSettings\": {\n        \"useTempDB\": true,\n        \"keys\": [\"sys_id\"]\n      }\n    },\n    \"enableStaging\": true,\n    \"stagingSettings\": {\n      \"linkedServiceName\": {\n        \"referenceName\": \"AzureBlobStorage\",\n        \"type\": \"LinkedServiceReference\"\n      }\n    }\n  }\n}\n```\n\n**Linked Service (OAuth2 - Recommended):**\n```json\n{\n  \"name\": \"ServiceNowV2LinkedService\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"ServiceNowV2\",\n    \"typeProperties\": {\n      \"endpoint\": \"https://dev12345.service-now.com\",\n      \"authenticationType\": \"OAuth2\",\n      \"clientId\": \"your-oauth-client-id\",\n      \"clientSecret\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"servicenow-client-secret\"\n      },\n      \"username\": \"service-account@company.com\",\n      \"password\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"servicenow-password\"\n      },\n      \"grantType\": \"password\"\n    }\n  }\n}\n```\n\n**Linked Service (Basic Authentication - Legacy):**\n```json\n{\n  \"name\": \"ServiceNowV2LinkedService_Basic\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"ServiceNowV2\",\n    \"typeProperties\": {\n      \"endpoint\": \"https://dev12345.service-now.com\",\n      \"authenticationType\": \"Basic\",\n      \"username\": \"admin\",\n      \"password\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"servicenow-password\"\n      }\n    }\n  }\n}\n```\n\n**Migration from V1 to V2:**\n1. Update linked service type from `ServiceNow` to `ServiceNowV2`\n2. Update source type from `ServiceNowSource` to `ServiceNowV2Source`\n3. Test queries in ServiceNow UI's condition builder first\n4. Adjust timeout settings if needed (V2 may have different performance)\n\n### Enhanced PostgreSQL Connector\n\nImproved performance and features:\n\n```json\n{\n  \"name\": \"PostgreSQLLinkedService\",\n  \"type\": \"PostgreSql\",\n  \"typeProperties\": {\n    \"connectionString\": \"host=myserver.postgres.database.azure.com;port=5432;database=mydb;uid=myuser\",\n    \"password\": {\n      \"type\": \"AzureKeyVaultSecret\",\n      \"store\": { \"referenceName\": \"KeyVault\" },\n      \"secretName\": \"postgres-password\"\n    },\n    // 2025 enhancement\n    \"enableSsl\": true,\n    \"sslMode\": \"Require\"\n  }\n}\n```\n\n### Microsoft Fabric Warehouse Connector (NEW 2025)\n\n** Native support for Microsoft Fabric Warehouse (Q3 2024+)**\n\n**Supported Activities:**\n-  Copy Activity (source and sink)\n-  Lookup Activity\n-  Get Metadata Activity\n-  Script Activity\n-  Stored Procedure Activity\n\n**Linked Service Configuration:**\n```json\n{\n  \"name\": \"FabricWarehouseLinkedService\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Warehouse\",  //  NEW dedicated Fabric Warehouse type\n    \"typeProperties\": {\n      \"endpoint\": \"myworkspace.datawarehouse.fabric.microsoft.com\",\n      \"warehouse\": \"MyWarehouse\",\n      \"authenticationType\": \"ServicePrincipal\",  // Recommended\n      \"servicePrincipalId\": \"<app-registration-id>\",\n      \"servicePrincipalKey\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"fabric-warehouse-sp-key\"\n      },\n      \"tenant\": \"<tenant-id>\"\n    }\n  }\n}\n```\n\n**Alternative: Managed Identity Authentication (Preferred)**\n```json\n{\n  \"name\": \"FabricWarehouseLinkedService_ManagedIdentity\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Warehouse\",\n    \"typeProperties\": {\n      \"endpoint\": \"myworkspace.datawarehouse.fabric.microsoft.com\",\n      \"warehouse\": \"MyWarehouse\",\n      \"authenticationType\": \"SystemAssignedManagedIdentity\"\n    }\n  }\n}\n```\n\n**Copy Activity Example:**\n```json\n{\n  \"name\": \"CopyToFabricWarehouse\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"referenceName\": \"AzureSqlSource\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"referenceName\": \"FabricWarehouseSink\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"AzureSqlSource\"\n    },\n    \"sink\": {\n      \"type\": \"WarehouseSink\",\n      \"writeBehavior\": \"insert\",  // or \"upsert\"\n      \"writeBatchSize\": 10000,\n      \"tableOption\": \"autoCreate\"  // Auto-create table if not exists\n    },\n    \"enableStaging\": true,  // Recommended for large data\n    \"stagingSettings\": {\n      \"linkedServiceName\": {\n        \"referenceName\": \"AzureBlobStorage\",\n        \"type\": \"LinkedServiceReference\"\n      },\n      \"path\": \"staging/fabric-warehouse\"\n    },\n    \"translator\": {\n      \"type\": \"TabularTranslator\",\n      \"mappings\": [\n        {\n          \"source\": { \"name\": \"CustomerID\" },\n          \"sink\": { \"name\": \"customer_id\" }\n        }\n      ]\n    }\n  }\n}\n```\n\n**Best Practices for Fabric Warehouse:**\n-  Use managed identity for authentication (no secret rotation)\n-  Enable staging for large data loads (> 1GB)\n-  Use `tableOption: autoCreate` for dynamic schema creation\n-  Leverage Fabric's lakehouse integration for unified analytics\n-  Monitor Fabric capacity units (CU) consumption\n\n### Enhanced Snowflake Connector\n\nImproved performance:\n\n```json\n{\n  \"name\": \"SnowflakeLinkedService\",\n  \"type\": \"Snowflake\",\n  \"typeProperties\": {\n    \"connectionString\": \"jdbc:snowflake://myaccount.snowflakecomputing.com\",\n    \"database\": \"mydb\",\n    \"warehouse\": \"mywarehouse\",\n    \"authenticationType\": \"KeyPair\",\n    \"username\": \"myuser\",\n    \"privateKey\": {\n      \"type\": \"AzureKeyVaultSecret\",\n      \"store\": { \"referenceName\": \"KeyVault\" },\n      \"secretName\": \"snowflake-private-key\"\n    },\n    \"privateKeyPassphrase\": {\n      \"type\": \"AzureKeyVaultSecret\",\n      \"store\": { \"referenceName\": \"KeyVault\" },\n      \"secretName\": \"snowflake-passphrase\"\n    }\n  }\n}\n```\n\n## Managed Identity for Azure Storage (2025)\n\n### Azure Table Storage\n\nNow supports system-assigned and user-assigned managed identity:\n\n```json\n{\n  \"name\": \"AzureTableStorageLinkedService\",\n  \"type\": \"AzureTableStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://mystorageaccount.table.core.windows.net\",\n    \"authenticationType\": \"ManagedIdentity\"  // New in 2025\n    // Or user-assigned:\n    // \"credential\": {\n    //   \"referenceName\": \"UserAssignedManagedIdentity\"\n    // }\n  }\n}\n```\n\n### Azure Files\n\nNow supports managed identity authentication:\n\n```json\n{\n  \"name\": \"AzureFilesLinkedService\",\n  \"type\": \"AzureFileStorage\",\n  \"typeProperties\": {\n    \"fileShare\": \"myshare\",\n    \"accountName\": \"mystorageaccount\",\n    \"authenticationType\": \"ManagedIdentity\"  // New in 2025\n  }\n}\n```\n\n## Mapping Data Flows - Spark 3.3\n\nSpark 3.3 now powers Mapping Data Flows:\n\n**Performance Improvements:**\n- 30% faster data processing\n- Improved memory management\n- Better partition handling\n- Enhanced join performance\n\n**New Features:**\n- Adaptive Query Execution (AQE)\n- Dynamic partition pruning\n- Improved caching\n- Better column statistics\n\n```json\n{\n  \"name\": \"DataFlow1\",\n  \"type\": \"MappingDataFlow\",\n  \"typeProperties\": {\n    \"sources\": [\n      {\n        \"dataset\": { \"referenceName\": \"SourceDataset\" }\n      }\n    ],\n    \"transformations\": [\n      {\n        \"name\": \"Transform1\"\n      }\n    ],\n    \"sinks\": [\n      {\n        \"dataset\": { \"referenceName\": \"SinkDataset\" }\n      }\n    ]\n  }\n}\n```\n\n## Azure DevOps Server 2022 Support\n\nGit integration now supports on-premises Azure DevOps Server 2022:\n\n```json\n{\n  \"name\": \"DataFactory\",\n  \"properties\": {\n    \"repoConfiguration\": {\n      \"type\": \"AzureDevOpsGit\",\n      \"accountName\": \"on-prem-ado-server\",\n      \"projectName\": \"MyProject\",\n      \"repositoryName\": \"adf-repo\",\n      \"collaborationBranch\": \"main\",\n      \"rootFolder\": \"/\",\n      \"hostName\": \"https://ado-server.company.com\"  // On-premises server\n    }\n  }\n}\n```\n\n##  Managed Identity 2025 Best Practices\n\n### User-Assigned vs System-Assigned Managed Identity\n\n**System-Assigned Managed Identity:**\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://mystorageaccount.blob.core.windows.net\",\n    \"accountKind\": \"StorageV2\"\n    //  Uses Data Factory's system-assigned identity automatically\n  }\n}\n```\n\n**User-Assigned Managed Identity (NEW 2025):**\n```json\n{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"serviceEndpoint\": \"https://mystorageaccount.blob.core.windows.net\",\n    \"accountKind\": \"StorageV2\",\n    \"credential\": {\n      \"referenceName\": \"UserAssignedManagedIdentityCredential\",\n      \"type\": \"CredentialReference\"\n    }\n  }\n}\n```\n\n**When to Use User-Assigned:**\n-  Sharing identity across multiple data factories\n-  Complex multi-environment setups\n-  Granular permission management\n-  Identity lifecycle independent of data factory\n\n**Credential Consolidation (NEW 2025):**\n\nADF now supports a centralized **Credentials** feature:\n```json\n{\n  \"name\": \"ManagedIdentityCredential\",\n  \"type\": \"Microsoft.DataFactory/factories/credentials\",\n  \"properties\": {\n    \"type\": \"ManagedIdentity\",\n    \"typeProperties\": {\n      \"resourceId\": \"/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identity-name}\"\n    }\n  }\n}\n```\n\n**Benefits:**\n-  Consolidate all Microsoft Entra ID-based credentials in one place\n-  Reuse credentials across multiple linked services\n-  Centralized permission management\n-  Easier audit and compliance tracking\n\n### MFA Enforcement Compatibility (October 2025)\n\n** IMPORTANT: Azure requires MFA for all users by October 2025**\n\n**Impact on ADF:**\n-  **Managed identities are UNAFFECTED** - No MFA required for service accounts\n-  Continue using system-assigned and user-assigned identities without changes\n-  **Interactive user logins affected** - Personal Azure AD accounts need MFA\n-  **Service principals with certificate auth** - Recommended alternative to secrets\n\n**Best Practice:**\n```json\n{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"server\": \"myserver.database.windows.net\",\n    \"database\": \"mydb\",\n    \"authenticationType\": \"SystemAssignedManagedIdentity\"\n    //  No MFA needed, no secret rotation, passwordless\n  }\n}\n```\n\n### Principle of Least Privilege (2025)\n\n**Storage Blob Data Roles:**\n- `Storage Blob Data Reader` - Read-only access (source)\n- `Storage Blob Data Contributor` - Read/write access (sink)\n-  Avoid `Storage Blob Data Owner` unless needed\n\n**SQL Database Roles:**\n```sql\n-- Create contained database user for managed identity\nCREATE USER [datafactory-name] FROM EXTERNAL PROVIDER;\n\n-- Grant minimal required permissions\nALTER ROLE db_datareader ADD MEMBER [datafactory-name];\nALTER ROLE db_datawriter ADD MEMBER [datafactory-name];\n\n--  Avoid db_owner unless truly needed\n```\n\n**Key Vault Access Policies:**\n```json\n{\n  \"permissions\": {\n    \"secrets\": [\"Get\"]  //  Only Get permission needed\n    //  Don't grant List, Set, Delete unless required\n  }\n}\n```\n\n## Best Practices (2025)\n\n1. **Use Databricks Job Activity (MANDATORY):**\n   -  STOP using Notebook, Python, JAR activities\n   -  Migrate to DatabricksJob activity immediately\n   -  Define workflows in Databricks workspace\n   -  Leverage serverless compute (no cluster config needed)\n   -  Utilize advanced features (Run As, Task Values, If/Else, Repair Runs)\n\n2. **Managed Identity Authentication (MANDATORY 2025):**\n   -  Use managed identities for ALL Azure resources\n   -  Prefer system-assigned for simple scenarios\n   -  Use user-assigned for shared identity needs\n   -  Leverage Credentials feature for consolidation\n   -  MFA-compliant for October 2025 enforcement\n   -  Avoid access keys and connection strings\n   -  Store any remaining secrets in Key Vault\n\n3. **Monitor Job Execution:**\n   - Track Databricks Job run IDs from ADF output\n   - Log Job parameters for auditability\n   - Set up alerts for job failures\n   - Use Databricks job-level monitoring\n   - Leverage built-in lineage tracking\n\n4. **Optimize Spark 3.3 Usage (Data Flows):**\n   - Enable Adaptive Query Execution (AQE)\n   - Use appropriate partition counts (4-8 per core)\n   - Monitor execution plans in Databricks\n   - Use broadcast joins for small dimensions\n   - Implement dynamic partition pruning\n\n## Resources\n\n- [Databricks Job Activity](https://learn.microsoft.com/azure/data-factory/transform-data-using-databricks-spark-job)\n- [ADF Connectors](https://learn.microsoft.com/azure/data-factory/connector-overview)\n- [Managed Identity Authentication](https://learn.microsoft.com/azure/data-factory/data-factory-service-identity)\n- [Mapping Data Flows](https://learn.microsoft.com/azure/data-factory/concepts-data-flow-overview)"
              },
              {
                "name": "fabric-onelake-2025",
                "description": "Microsoft Fabric Lakehouse, OneLake, and Fabric Warehouse connectors for Azure Data Factory (2025)",
                "path": "plugins/adf-master/skills/fabric-onelake-2025/SKILL.md",
                "frontmatter": {
                  "name": "fabric-onelake-2025",
                  "description": "Microsoft Fabric Lakehouse, OneLake, and Fabric Warehouse connectors for Azure Data Factory (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Microsoft Fabric Integration with Azure Data Factory (2025)\n\n## Overview\n\nMicrosoft Fabric represents a unified SaaS analytics platform that combines Power BI, Azure Synapse Analytics, and Azure Data Factory capabilities. Azure Data Factory now provides native connectors for Fabric Lakehouse and Fabric Warehouse, enabling seamless data movement between ADF and Fabric workspaces.\n\n## Microsoft Fabric Lakehouse Connector\n\nThe Fabric Lakehouse connector enables both read and write operations to Microsoft Fabric Lakehouse for tables and files.\n\n### Supported Activities\n-  Copy Activity (source and sink)\n-  Lookup Activity\n-  Get Metadata Activity\n-  Delete Activity\n\n### Linked Service Configuration\n\n**Using Service Principal Authentication (Recommended):**\n```json\n{\n  \"name\": \"FabricLakehouseLinkedService\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Lakehouse\",\n    \"typeProperties\": {\n      \"workspaceId\": \"12345678-1234-1234-1234-123456789abc\",\n      \"artifactId\": \"87654321-4321-4321-4321-cba987654321\",\n      \"servicePrincipalId\": \"<app-registration-client-id>\",\n      \"servicePrincipalKey\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"fabric-service-principal-key\"\n      },\n      \"tenant\": \"<tenant-id>\"\n    }\n  }\n}\n```\n\n**Using Managed Identity Authentication (Preferred 2025):**\n```json\n{\n  \"name\": \"FabricLakehouseLinkedService_ManagedIdentity\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Lakehouse\",\n    \"typeProperties\": {\n      \"workspaceId\": \"12345678-1234-1234-1234-123456789abc\",\n      \"artifactId\": \"87654321-4321-4321-4321-cba987654321\"\n      // Managed identity used automatically - no credentials needed!\n    }\n  }\n}\n```\n\n**Finding Workspace and Artifact IDs:**\n1. Navigate to Fabric workspace in browser\n2. Copy workspace ID from URL: `https://app.powerbi.com/groups/<workspaceId>/...`\n3. Open Lakehouse settings to find artifact ID\n4. Or use Fabric REST API to enumerate workspace items\n\n### Dataset Configuration\n\n**For Lakehouse Files:**\n```json\n{\n  \"name\": \"FabricLakehouseFiles\",\n  \"properties\": {\n    \"type\": \"LakehouseTable\",\n    \"linkedServiceName\": {\n      \"referenceName\": \"FabricLakehouseLinkedService\",\n      \"type\": \"LinkedServiceReference\"\n    },\n    \"typeProperties\": {\n      \"table\": \"Files/raw/sales/2025\"\n    }\n  }\n}\n```\n\n**For Lakehouse Tables:**\n```json\n{\n  \"name\": \"FabricLakehouseTables\",\n  \"properties\": {\n    \"type\": \"LakehouseTable\",\n    \"linkedServiceName\": {\n      \"referenceName\": \"FabricLakehouseLinkedService\",\n      \"type\": \"LinkedServiceReference\"\n    },\n    \"typeProperties\": {\n      \"table\": \"SalesData\"  // Table name in Lakehouse\n    }\n  }\n}\n```\n\n### Copy Activity Examples\n\n**Copy from Azure SQL to Fabric Lakehouse:**\n```json\n{\n  \"name\": \"CopyToFabricLakehouse\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"referenceName\": \"AzureSqlSource\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"referenceName\": \"FabricLakehouseTables\",\n      \"type\": \"DatasetReference\",\n      \"parameters\": {\n        \"tableName\": \"DimCustomer\"\n      }\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"AzureSqlSource\",\n      \"sqlReaderQuery\": \"SELECT * FROM dbo.Customers WHERE ModifiedDate > '@{pipeline().parameters.LastRunTime}'\"\n    },\n    \"sink\": {\n      \"type\": \"LakehouseTableSink\",\n      \"tableActionOption\": \"append\"  // or \"overwrite\"\n    },\n    \"enableStaging\": false,\n    \"translator\": {\n      \"type\": \"TabularTranslator\",\n      \"mappings\": [\n        {\n          \"source\": { \"name\": \"CustomerID\" },\n          \"sink\": { \"name\": \"customer_id\", \"type\": \"Int32\" }\n        },\n        {\n          \"source\": { \"name\": \"CustomerName\" },\n          \"sink\": { \"name\": \"customer_name\", \"type\": \"String\" }\n        }\n      ]\n    }\n  }\n}\n```\n\n**Copy Parquet Files to Fabric Lakehouse:**\n```json\n{\n  \"name\": \"CopyParquetToLakehouse\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"referenceName\": \"AzureBlobParquetFiles\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"referenceName\": \"FabricLakehouseFiles\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"ParquetSource\",\n      \"storeSettings\": {\n        \"type\": \"AzureBlobStorageReadSettings\",\n        \"recursive\": true,\n        \"wildcardFolderPath\": \"raw/sales/2025\",\n        \"wildcardFileName\": \"*.parquet\"\n      }\n    },\n    \"sink\": {\n      \"type\": \"LakehouseFileSink\",\n      \"storeSettings\": {\n        \"type\": \"LakehouseWriteSettings\",\n        \"copyBehavior\": \"PreserveHierarchy\"\n      }\n    }\n  }\n}\n```\n\n### Lookup Activity Example\n\n```json\n{\n  \"name\": \"LookupFabricLakehouseTable\",\n  \"type\": \"Lookup\",\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"LakehouseTableSource\",\n      \"query\": \"SELECT MAX(LastUpdated) as MaxDate FROM SalesData\"\n    },\n    \"dataset\": {\n      \"referenceName\": \"FabricLakehouseTables\",\n      \"type\": \"DatasetReference\"\n    }\n  }\n}\n```\n\n## Microsoft Fabric Warehouse Connector\n\nThe Fabric Warehouse connector provides T-SQL based data warehousing capabilities within the Fabric ecosystem.\n\n### Supported Activities\n-  Copy Activity (source and sink)\n-  Lookup Activity\n-  Get Metadata Activity\n-  Script Activity\n-  Stored Procedure Activity\n\n### Linked Service Configuration\n\n**Using Service Principal:**\n```json\n{\n  \"name\": \"FabricWarehouseLinkedService\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Warehouse\",\n    \"typeProperties\": {\n      \"endpoint\": \"myworkspace.datawarehouse.fabric.microsoft.com\",\n      \"warehouse\": \"MyWarehouse\",\n      \"authenticationType\": \"ServicePrincipal\",\n      \"servicePrincipalId\": \"<app-registration-id>\",\n      \"servicePrincipalKey\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVault\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"fabric-warehouse-sp-key\"\n      },\n      \"tenant\": \"<tenant-id>\"\n    }\n  }\n}\n```\n\n**Using System-Assigned Managed Identity (Recommended):**\n```json\n{\n  \"name\": \"FabricWarehouseLinkedService_SystemMI\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Warehouse\",\n    \"typeProperties\": {\n      \"endpoint\": \"myworkspace.datawarehouse.fabric.microsoft.com\",\n      \"warehouse\": \"MyWarehouse\",\n      \"authenticationType\": \"SystemAssignedManagedIdentity\"\n    }\n  }\n}\n```\n\n**Using User-Assigned Managed Identity:**\n```json\n{\n  \"name\": \"FabricWarehouseLinkedService_UserMI\",\n  \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n  \"properties\": {\n    \"type\": \"Warehouse\",\n    \"typeProperties\": {\n      \"endpoint\": \"myworkspace.datawarehouse.fabric.microsoft.com\",\n      \"warehouse\": \"MyWarehouse\",\n      \"authenticationType\": \"UserAssignedManagedIdentity\",\n      \"credential\": {\n        \"referenceName\": \"UserAssignedManagedIdentityCredential\",\n        \"type\": \"CredentialReference\"\n      }\n    }\n  }\n}\n```\n\n### Copy Activity to Fabric Warehouse\n\n**Bulk Insert Pattern:**\n```json\n{\n  \"name\": \"CopyToFabricWarehouse\",\n  \"type\": \"Copy\",\n  \"inputs\": [\n    {\n      \"referenceName\": \"AzureSqlSource\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"referenceName\": \"FabricWarehouseSink\",\n      \"type\": \"DatasetReference\"\n    }\n  ],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"AzureSqlSource\",\n      \"sqlReaderQuery\": \"SELECT * FROM dbo.FactSales WHERE OrderDate >= '@{pipeline().parameters.StartDate}'\"\n    },\n    \"sink\": {\n      \"type\": \"WarehouseSink\",\n      \"preCopyScript\": \"TRUNCATE TABLE staging.FactSales\",\n      \"writeBehavior\": \"insert\",\n      \"writeBatchSize\": 10000,\n      \"tableOption\": \"autoCreate\",  // Auto-create table if doesn't exist\n      \"disableMetricsCollection\": false\n    },\n    \"enableStaging\": true,\n    \"stagingSettings\": {\n      \"linkedServiceName\": {\n        \"referenceName\": \"AzureBlobStorage\",\n        \"type\": \"LinkedServiceReference\"\n      },\n      \"path\": \"staging/fabric-warehouse\",\n      \"enableCompression\": true\n    },\n    \"parallelCopies\": 4,\n    \"dataIntegrationUnits\": 8\n  }\n}\n```\n\n**Upsert Pattern:**\n```json\n{\n  \"sink\": {\n    \"type\": \"WarehouseSink\",\n    \"writeBehavior\": \"upsert\",\n    \"upsertSettings\": {\n      \"useTempDB\": true,\n      \"keys\": [\"customer_id\"],\n      \"interimSchemaName\": \"staging\"\n    },\n    \"writeBatchSize\": 10000\n  }\n}\n```\n\n### Stored Procedure Activity\n\n```json\n{\n  \"name\": \"ExecuteFabricWarehouseStoredProcedure\",\n  \"type\": \"SqlServerStoredProcedure\",\n  \"linkedServiceName\": {\n    \"referenceName\": \"FabricWarehouseLinkedService\",\n    \"type\": \"LinkedServiceReference\"\n  },\n  \"typeProperties\": {\n    \"storedProcedureName\": \"dbo.usp_ProcessSalesData\",\n    \"storedProcedureParameters\": {\n      \"StartDate\": {\n        \"value\": \"@pipeline().parameters.StartDate\",\n        \"type\": \"DateTime\"\n      },\n      \"EndDate\": {\n        \"value\": \"@pipeline().parameters.EndDate\",\n        \"type\": \"DateTime\"\n      }\n    }\n  }\n}\n```\n\n### Script Activity\n\n```json\n{\n  \"name\": \"ExecuteFabricWarehouseScript\",\n  \"type\": \"Script\",\n  \"linkedServiceName\": {\n    \"referenceName\": \"FabricWarehouseLinkedService\",\n    \"type\": \"LinkedServiceReference\"\n  },\n  \"typeProperties\": {\n    \"scripts\": [\n      {\n        \"type\": \"Query\",\n        \"text\": \"DELETE FROM staging.FactSales WHERE LoadDate < DATEADD(day, -30, GETDATE())\"\n      },\n      {\n        \"type\": \"Query\",\n        \"text\": \"UPDATE dbo.FactSales SET ProcessedFlag = 1 WHERE OrderDate = '@{pipeline().parameters.ProcessDate}'\"\n      }\n    ],\n    \"scriptBlockExecutionTimeout\": \"02:00:00\"\n  }\n}\n```\n\n## OneLake Integration Patterns\n\n### Pattern 1: Azure Data Lake Gen2 to OneLake via Shortcuts\n\n**Concept:** Use OneLake shortcuts instead of copying data\n\nOneLake shortcuts allow you to reference data in Azure Data Lake Gen2 without physically copying it:\n\n1. In Fabric Lakehouse, create shortcut to ADLS Gen2 container\n2. Data appears in OneLake immediately (zero-copy)\n3. Use ADF to orchestrate transformations on shortcut data\n4. Write results back to OneLake\n\n**Benefits:**\n- Zero data duplication\n- Real-time data access\n- Reduced storage costs\n- Single source of truth\n\n**ADF Pipeline Pattern:**\n```json\n{\n  \"name\": \"PL_Process_Shortcut_Data\",\n  \"activities\": [\n    {\n      \"name\": \"TransformShortcutData\",\n      \"type\": \"ExecuteDataFlow\",\n      \"typeProperties\": {\n        \"dataFlow\": {\n          \"referenceName\": \"DF_Transform\",\n          \"type\": \"DataFlowReference\"\n        },\n        \"compute\": {\n          \"coreCount\": 8,\n          \"computeType\": \"General\"\n        }\n      }\n    },\n    {\n      \"name\": \"WriteToCuratedZone\",\n      \"type\": \"Copy\",\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"ParquetSource\"\n        },\n        \"sink\": {\n          \"type\": \"LakehouseTableSink\",\n          \"tableActionOption\": \"overwrite\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Pattern 2: Incremental Load to Fabric Lakehouse\n\n```json\n{\n  \"name\": \"PL_Incremental_Load_To_Fabric\",\n  \"activities\": [\n    {\n      \"name\": \"GetLastWatermark\",\n      \"type\": \"Lookup\",\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"LakehouseTableSource\",\n          \"query\": \"SELECT MAX(LoadTimestamp) as LastLoad FROM ControlTable\"\n        }\n      }\n    },\n    {\n      \"name\": \"CopyIncrementalData\",\n      \"type\": \"Copy\",\n      \"dependsOn\": [\n        {\n          \"activity\": \"GetLastWatermark\",\n          \"dependencyConditions\": [\"Succeeded\"]\n        }\n      ],\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"AzureSqlSource\",\n          \"sqlReaderQuery\": \"SELECT * FROM dbo.Orders WHERE ModifiedDate > '@{activity('GetLastWatermark').output.firstRow.LastLoad}'\"\n        },\n        \"sink\": {\n          \"type\": \"LakehouseTableSink\",\n          \"tableActionOption\": \"append\"\n        }\n      }\n    },\n    {\n      \"name\": \"UpdateWatermark\",\n      \"type\": \"Script\",\n      \"dependsOn\": [\n        {\n          \"activity\": \"CopyIncrementalData\",\n          \"dependencyConditions\": [\"Succeeded\"]\n        }\n      ],\n      \"linkedServiceName\": {\n        \"referenceName\": \"FabricLakehouseLinkedService\",\n        \"type\": \"LinkedServiceReference\"\n      },\n      \"typeProperties\": {\n        \"scripts\": [\n          {\n            \"type\": \"Query\",\n            \"text\": \"INSERT INTO ControlTable VALUES ('@{utcnow()}')\"\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n### Pattern 3: Cross-Platform Pipeline with Invoke Pipeline\n\n**NEW 2025: Invoke Pipeline Activity for Cross-Platform Calls**\n\n```json\n{\n  \"name\": \"PL_ADF_Orchestrates_Fabric_Pipeline\",\n  \"activities\": [\n    {\n      \"name\": \"PrepareDataInADF\",\n      \"type\": \"Copy\",\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"AzureSqlSource\"\n        },\n        \"sink\": {\n          \"type\": \"LakehouseTableSink\"\n        }\n      }\n    },\n    {\n      \"name\": \"InvokeFabricPipeline\",\n      \"type\": \"InvokePipeline\",\n      \"dependsOn\": [\n        {\n          \"activity\": \"PrepareDataInADF\",\n          \"dependencyConditions\": [\"Succeeded\"]\n        }\n      ],\n      \"typeProperties\": {\n        \"workspaceId\": \"12345678-1234-1234-1234-123456789abc\",\n        \"pipelineId\": \"87654321-4321-4321-4321-cba987654321\",\n        \"waitOnCompletion\": true,\n        \"parameters\": {\n          \"processDate\": \"@pipeline().parameters.RunDate\",\n          \"environment\": \"production\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Permission Configuration\n\n### Azure Data Factory Managed Identity Permissions in Fabric\n\n**For Fabric Lakehouse:**\n1. Open Fabric workspace\n2. Go to Workspace settings  Manage access\n3. Add ADF managed identity with **Contributor** role\n4. Or assign **Workspace Admin** for full access\n\n**For Fabric Warehouse:**\n1. Navigate to Warehouse SQL endpoint\n2. Execute SQL to create user:\n```sql\nCREATE USER [your-adf-name] FROM EXTERNAL PROVIDER;\nALTER ROLE db_datareader ADD MEMBER [your-adf-name];\nALTER ROLE db_datawriter ADD MEMBER [your-adf-name];\n```\n\n### Service Principal Permissions\n\n**App Registration Setup:**\n1. Register app in Microsoft Entra ID\n2. Create client secret (store in Key Vault)\n3. Add app to Fabric workspace with Contributor role\n4. For Warehouse, create SQL user as shown above\n\n## Best Practices (2025)\n\n### 1. Use Managed Identity\n-  System-assigned for single ADF\n-  User-assigned for multiple ADFs\n-  Avoid service principal keys when possible\n-  Store any secrets in Key Vault\n\n### 2. Enable Staging for Large Loads\n```json\n{\n  \"enableStaging\": true,\n  \"stagingSettings\": {\n    \"linkedServiceName\": {\n      \"referenceName\": \"AzureBlobStorage\",\n      \"type\": \"LinkedServiceReference\"\n    },\n    \"path\": \"staging/fabric-loads\",\n    \"enableCompression\": true\n  }\n}\n```\n\n**When to Stage:**\n- Data volume > 1 GB\n- Complex transformations\n- Loading to Fabric Warehouse\n- Need better performance\n\n### 3. Leverage OneLake Shortcuts\n\n**Instead of:**\n```\nADLS Gen2  [Copy Activity]  Fabric Lakehouse\n```\n\n**Use:**\n```\nADLS Gen2  [OneLake Shortcut]  Direct Access in Fabric\n```\n\n**Benefits:**\n- No data movement\n- Instant availability\n- Reduced ADF costs\n- Lower storage costs\n\n### 4. Monitor Fabric Capacity Units (CU)\n\nFabric uses capacity-based pricing. Monitor:\n- CU consumption per pipeline run\n- Peak usage times\n- Throttling events\n- Optimize by:\n  - Using incremental loads\n  - Scheduling during off-peak\n  - Right-sizing copy parallelism\n\n### 5. Use Table Option AutoCreate\n\n```json\n{\n  \"sink\": {\n    \"type\": \"WarehouseSink\",\n    \"tableOption\": \"autoCreate\"  // Creates table if missing\n  }\n}\n```\n\n**Benefits:**\n- No manual schema management\n- Automatic type mapping\n- Faster development\n- Works for dynamic schemas\n\n### 6. Implement Error Handling\n\n```json\n{\n  \"activities\": [\n    {\n      \"name\": \"CopyToFabric\",\n      \"type\": \"Copy\",\n      \"policy\": {\n        \"retry\": 2,\n        \"retryIntervalInSeconds\": 30,\n        \"timeout\": \"0.12:00:00\"\n      }\n    },\n    {\n      \"name\": \"LogFailure\",\n      \"type\": \"WebActivity\",\n      \"dependsOn\": [\n        {\n          \"activity\": \"CopyToFabric\",\n          \"dependencyConditions\": [\"Failed\"]\n        }\n      ],\n      \"typeProperties\": {\n        \"url\": \"@pipeline().parameters.LoggingEndpoint\",\n        \"method\": \"POST\",\n        \"body\": {\n          \"error\": \"@activity('CopyToFabric').error.message\",\n          \"pipeline\": \"@pipeline().Pipeline\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Common Issues and Solutions\n\n### Issue 1: Permission Denied\n\n**Error:** \"User does not have permission to access Fabric workspace\"\n\n**Solution:**\n1. Verify ADF managed identity added to Fabric workspace\n2. Check role is **Contributor** or higher\n3. For Warehouse, verify SQL user created\n4. Allow up to 5 minutes for permission propagation\n\n### Issue 2: Endpoint Not Found\n\n**Error:** \"Unable to connect to endpoint\"\n\n**Solution:**\n1. Verify `workspaceId` and `artifactId` are correct\n2. Check Fabric workspace URL in browser\n3. Ensure Lakehouse/Warehouse is not paused\n4. Verify firewall rules allow ADF IP ranges\n\n### Issue 3: Schema Mismatch\n\n**Error:** \"Column types do not match\"\n\n**Solution:**\n1. Use `tableOption: \"autoCreate\"` for initial load\n2. Explicitly define column mappings in translator\n3. Enable staging for complex transformations\n4. Use Data Flow for schema evolution\n\n### Issue 4: Performance Degradation\n\n**Symptoms:** Slow copy performance to Fabric\n\n**Solutions:**\n1. Enable staging for large datasets\n2. Increase `parallelCopies` (try 4-8)\n3. Use appropriate `dataIntegrationUnits` (8-32)\n4. Check Fabric capacity unit throttling\n5. Schedule during off-peak hours\n\n## Resources\n\n- [Fabric Lakehouse Connector](https://learn.microsoft.com/azure/data-factory/connector-microsoft-fabric-lakehouse)\n- [Fabric Warehouse Connector](https://learn.microsoft.com/azure/data-factory/connector-microsoft-fabric-warehouse)\n- [OneLake Documentation](https://learn.microsoft.com/fabric/onelake/)\n- [Fabric Capacity Management](https://learn.microsoft.com/fabric/enterprise/licenses)\n- [ADF to Fabric Integration Guide](https://learn.microsoft.com/fabric/data-factory/how-to-ingest-data-into-fabric-from-azure-data-factory)\n\nThis comprehensive guide enables seamless integration between Azure Data Factory and Microsoft Fabric's modern data platform capabilities."
              },
              {
                "name": "windows-git-bash-compatibility",
                "description": "Windows and Git Bash compatibility guidance for Azure Data Factory development and CI/CD",
                "path": "plugins/adf-master/skills/windows-git-bash-compatibility/SKILL.md",
                "frontmatter": {
                  "name": "windows-git-bash-compatibility",
                  "description": "Windows and Git Bash compatibility guidance for Azure Data Factory development and CI/CD"
                },
                "content": "# Windows & Git Bash Compatibility for Azure Data Factory\n\n## Overview\n\nAzure Data Factory development frequently occurs on Windows machines using Git Bash (MINGW64) as the primary shell. This introduces path conversion challenges that can break CI/CD pipelines, npm commands, and deployment scripts.\n\n## Git Bash Path Conversion Behavior\n\n### Automatic Path Conversion\n\nGit Bash (MINGW) automatically converts Unix-style paths to Windows paths:\n\n**Conversions:**\n- `/foo`  `C:/Program Files/Git/usr/foo`\n- `/foo:/bar`  `C:\\msys64\\foo;C:\\msys64\\bar` (path lists)\n- `--dir=/foo`  `--dir=C:/msys64/foo` (arguments)\n\n**What Triggers Conversion:**\n- Leading forward slash (`/`) in arguments\n- Colon-separated path lists\n- Arguments after `-` or `,` with path components\n\n**What's Exempt:**\n- Arguments containing `=` (variable assignments)\n- Drive specifiers (`C:`)\n- Arguments with `;` (already Windows format)\n- Arguments starting with `//` (Windows switches)\n\n## ADF-Specific Path Issues\n\n### npm Build Commands\n\n**Problem:**\n```bash\n# This fails in Git Bash due to path conversion\nnpm run build validate ./adf-resources /subscriptions/abc/resourceGroups/rg/providers/Microsoft.DataFactory/factories/myFactory\n\n# Path gets converted incorrectly\n```\n\n**Solution:**\n```bash\n# Disable path conversion before running\nexport MSYS_NO_PATHCONV=1\nnpm run build validate ./adf-resources /subscriptions/abc/resourceGroups/rg/providers/Microsoft.DataFactory/factories/myFactory\n\n# Or wrap the command\nMSYS_NO_PATHCONV=1 npm run build export ./adf-resources /subscriptions/.../myFactory \"ARMTemplate\"\n```\n\n### PowerShell Scripts\n\n**Problem:**\n```bash\n# Calling PowerShell scripts from Git Bash\npwsh ./PrePostDeploymentScript.Ver2.ps1 -armTemplate \"./ARMTemplate/ARMTemplateForFactory.json\"\n# Path conversion may interfere\n```\n\n**Solution:**\n```bash\n# Disable conversion for PowerShell calls\nMSYS_NO_PATHCONV=1 pwsh ./PrePostDeploymentScript.Ver2.ps1 -armTemplate \"./ARMTemplate/ARMTemplateForFactory.json\"\n```\n\n### ARM Template Paths\n\n**Problem:**\n```bash\n# Azure CLI deployment from Git Bash\naz deployment group create \\\n  --resource-group myRG \\\n  --template-file ARMTemplate/ARMTemplateForFactory.json  # Path may get converted\n```\n\n**Solution:**\n```bash\n# Use relative paths with ./ prefix or absolute Windows paths\nexport MSYS_NO_PATHCONV=1\naz deployment group create \\\n  --resource-group myRG \\\n  --template-file ./ARMTemplate/ARMTemplateForFactory.json\n```\n\n## Shell Detection Patterns\n\n### Bash Shell Detection\n\n```bash\n#!/usr/bin/env bash\n\n# Method 1: Check $MSYSTEM (Git Bash/MSYS2 specific)\nif [ -n \"$MSYSTEM\" ]; then\n  echo \"Running in Git Bash/MinGW ($MSYSTEM)\"\n  export MSYS_NO_PATHCONV=1\nfi\n\n# Method 2: Check uname -s (more portable)\ncase \"$(uname -s)\" in\n  MINGW64*|MINGW32*|MSYS*)\n    echo \"Git Bash detected\"\n    export MSYS_NO_PATHCONV=1\n    ;;\n  Linux*)\n    if grep -q Microsoft /proc/version 2>/dev/null; then\n      echo \"WSL detected\"\n    else\n      echo \"Native Linux\"\n    fi\n    ;;\n  Darwin*)\n    echo \"macOS\"\n    ;;\nesac\n\n# Method 3: Check $OSTYPE (bash-specific)\ncase \"$OSTYPE\" in\n  msys*)\n    echo \"Git Bash/MSYS\"\n    export MSYS_NO_PATHCONV=1\n    ;;\n  linux-gnu*)\n    echo \"Linux\"\n    ;;\n  darwin*)\n    echo \"macOS\"\n    ;;\nesac\n```\n\n### Node.js Shell Detection\n\n```javascript\n// detect-shell.js - For use in npm scripts or Node tools\nfunction detectShell() {\n  const env = process.env;\n\n  // Git Bash/MinGW (MOST RELIABLE)\n  if (env.MSYSTEM) {\n    return {\n      type: 'mingw',\n      subsystem: env.MSYSTEM,  // MINGW64, MINGW32, or MSYS\n      needsPathFix: true\n    };\n  }\n\n  // WSL\n  if (env.WSL_DISTRO_NAME) {\n    return {\n      type: 'wsl',\n      distro: env.WSL_DISTRO_NAME,\n      needsPathFix: false\n    };\n  }\n\n  // PowerShell (3+ paths in PSModulePath)\n  if (env.PSModulePath?.split(';').length >= 3) {\n    return {\n      type: 'powershell',\n      needsPathFix: false\n    };\n  }\n\n  // CMD\n  if (process.platform === 'win32' && env.PROMPT === '$P$G') {\n    return {\n      type: 'cmd',\n      needsPathFix: false\n    };\n  }\n\n  // Cygwin\n  if (env.TERM === 'cygwin') {\n    return {\n      type: 'cygwin',\n      needsPathFix: true\n    };\n  }\n\n  // Unix shells\n  if (env.SHELL?.includes('bash')) {\n    return { type: 'bash', needsPathFix: false };\n  }\n  if (env.SHELL?.includes('zsh')) {\n    return { type: 'zsh', needsPathFix: false };\n  }\n\n  return {\n    type: 'unknown',\n    platform: process.platform,\n    needsPathFix: false\n  };\n}\n\n// Usage\nconst shell = detectShell();\nconsole.log(`Detected shell: ${shell.type}`);\n\nif (shell.needsPathFix) {\n  process.env.MSYS_NO_PATHCONV = '1';\n  console.log('Path conversion disabled for Git Bash compatibility');\n}\n\nmodule.exports = { detectShell };\n```\n\n### PowerShell Detection\n\n```powershell\n# Detect PowerShell edition and version\nfunction Get-ShellInfo {\n  $info = @{\n    Edition = $PSVersionTable.PSEdition\n    Version = $PSVersionTable.PSVersion\n    OS = $PSVersionTable.OS\n    Platform = $PSVersionTable.Platform\n  }\n\n  if ($info.Edition -eq 'Core') {\n    Write-Host \"PowerShell Core (pwsh) - Cross-platform compatible\" -ForegroundColor Green\n    $info.CrossPlatform = $true\n  } else {\n    Write-Host \"Windows PowerShell - Windows only\" -ForegroundColor Yellow\n    $info.CrossPlatform = $false\n  }\n\n  return $info\n}\n\n$shellInfo = Get-ShellInfo\n```\n\n## CI/CD Pipeline Patterns\n\n### Local Development Scripts\n\n**validate-adf.sh** (Git Bash compatible):\n```bash\n#!/usr/bin/env bash\nset -e\n\n# Detect and handle Git Bash\nif [ -n \"$MSYSTEM\" ]; then\n  export MSYS_NO_PATHCONV=1\n  echo \" Git Bash detected - path conversion disabled\"\nfi\n\n# Configuration\nADF_ROOT=\"./adf-resources\"\nFACTORY_ID=\"/subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.DataFactory/factories/${FACTORY_NAME}\"\n\n# Validate ADF resources\necho \" Validating ADF resources...\"\nnpm run build validate \"$ADF_ROOT\" \"$FACTORY_ID\"\n\n# Generate ARM templates\necho \" Generating ARM templates...\"\nnpm run build export \"$ADF_ROOT\" \"$FACTORY_ID\" \"ARMTemplate\"\n\necho \" Validation complete\"\n```\n\n**deploy-adf.sh** (Cross-platform):\n```bash\n#!/usr/bin/env bash\nset -e\n\n# Detect shell\ndetect_shell() {\n  if [ -n \"$MSYSTEM\" ]; then echo \"git-bash\"\n  elif [ -n \"$WSL_DISTRO_NAME\" ]; then echo \"wsl\"\n  elif [[ \"$OSTYPE\" == \"darwin\"* ]]; then echo \"macos\"\n  else echo \"linux\"\n  fi\n}\n\nSHELL_TYPE=$(detect_shell)\necho \"  Detected shell: $SHELL_TYPE\"\n\n# Handle Git Bash\nif [ \"$SHELL_TYPE\" = \"git-bash\" ]; then\n  export MSYS_NO_PATHCONV=1\nfi\n\n# Download PrePostDeploymentScript\ncurl -sLo PrePostDeploymentScript.Ver2.ps1 \\\n  https://raw.githubusercontent.com/Azure/Azure-DataFactory/main/SamplesV2/ContinuousIntegrationAndDelivery/PrePostDeploymentScript.Ver2.ps1\n\n# Stop triggers\necho \"  Stopping triggers...\"\nMSYS_NO_PATHCONV=1 pwsh ./PrePostDeploymentScript.Ver2.ps1 \\\n  -armTemplate \"./ARMTemplate/ARMTemplateForFactory.json\" \\\n  -ResourceGroupName \"$RESOURCE_GROUP\" \\\n  -DataFactoryName \"$FACTORY_NAME\" \\\n  -predeployment $true \\\n  -deleteDeployment $false\n\n# Deploy ARM template\necho \" Deploying ARM template...\"\naz deployment group create \\\n  --resource-group \"$RESOURCE_GROUP\" \\\n  --template-file ./ARMTemplate/ARMTemplateForFactory.json \\\n  --parameters ./ARMTemplate/ARMTemplateParametersForFactory.json \\\n  --parameters factoryName=\"$FACTORY_NAME\"\n\n# Start triggers\necho \"  Starting triggers...\"\nMSYS_NO_PATHCONV=1 pwsh ./PrePostDeploymentScript.Ver2.ps1 \\\n  -armTemplate \"./ARMTemplate/ARMTemplateForFactory.json\" \\\n  -ResourceGroupName \"$RESOURCE_GROUP\" \\\n  -DataFactoryName \"$FACTORY_NAME\" \\\n  -predeployment $false \\\n  -deleteDeployment $true\n\necho \" Deployment complete\"\n```\n\n### package.json with Shell Detection\n\n```json\n{\n  \"scripts\": {\n    \"prevalidate\": \"node scripts/detect-shell.js\",\n    \"validate\": \"node node_modules/@microsoft/azure-data-factory-utilities/lib/index validate\",\n    \"prebuild\": \"node scripts/detect-shell.js\",\n    \"build\": \"node node_modules/@microsoft/azure-data-factory-utilities/lib/index export\"\n  },\n  \"dependencies\": {\n    \"@microsoft/azure-data-factory-utilities\": \"^1.0.3\"\n  }\n}\n```\n\n**scripts/detect-shell.js**:\n```javascript\nconst detectShell = () => {\n  if (process.env.MSYSTEM) {\n    console.log(' Git Bash detected - disabling path conversion');\n    process.env.MSYS_NO_PATHCONV = '1';\n    return 'git-bash';\n  }\n  console.log(`  Shell: ${process.platform}`);\n  return process.platform;\n};\n\ndetectShell();\n```\n\n## Common Issues and Solutions\n\n### Issue 1: npm build validate fails with \"Resource not found\"\n\n**Symptom:**\n```bash\nnpm run build validate ./adf-resources /subscriptions/abc/...\n# Error: Resource '/subscriptions/C:/Program Files/Git/subscriptions/abc/...' not found\n```\n\n**Cause:** Git Bash converted the factory ID path\n\n**Solution:**\n```bash\nexport MSYS_NO_PATHCONV=1\nnpm run build validate ./adf-resources /subscriptions/abc/...\n```\n\n### Issue 2: PowerShell script paths incorrect\n\n**Symptom:**\n```bash\npwsh PrePostDeploymentScript.Ver2.ps1 -armTemplate \"./ARM/template.json\"\n# Error: Cannot find path 'C:/Program Files/Git/ARM/template.json'\n```\n\n**Cause:** Git Bash converted the ARM template path\n\n**Solution:**\n```bash\nMSYS_NO_PATHCONV=1 pwsh PrePostDeploymentScript.Ver2.ps1 -armTemplate \"./ARM/template.json\"\n```\n\n### Issue 3: Azure CLI template-file parameter fails\n\n**Symptom:**\n```bash\naz deployment group create --template-file ./ARMTemplate/file.json\n# Error: Template file not found\n```\n\n**Cause:** Path conversion interfering with Azure CLI\n\n**Solution:**\n```bash\nexport MSYS_NO_PATHCONV=1\naz deployment group create --template-file ./ARMTemplate/file.json\n```\n\n## Best Practices\n\n### 1. Set MSYS_NO_PATHCONV in .bashrc\n\n```bash\n# Add to ~/.bashrc for Git Bash\nif [ -n \"$MSYSTEM\" ]; then\n  export MSYS_NO_PATHCONV=1\nfi\n```\n\n### 2. Create Wrapper Scripts\n\n```bash\n# adf-cli.sh - Wrapper for ADF npm commands\n#!/usr/bin/env bash\nexport MSYS_NO_PATHCONV=1\nnpm run build \"$@\"\n```\n\n### 3. Use Relative Paths with ./\n\n```bash\n# Prefer this (less likely to trigger conversion)\n./ARMTemplate/ARMTemplateForFactory.json\n\n# Over this\nARMTemplate/ARMTemplateForFactory.json\n```\n\n### 4. Document Shell Requirements\n\n```markdown\n# README.md\n\n## Development Environment\n\n### Windows Users\n- Use Git Bash or PowerShell Core (pwsh)\n- Git Bash users: Add `export MSYS_NO_PATHCONV=1` to .bashrc\n- Alternative: Use WSL2 for native Linux environment\n```\n\n### 5. Test on Multiple Shells\n\n```bash\n# Test matrix for Windows developers\n- Git Bash (MINGW64)\n- PowerShell Core 7+\n- WSL2 (Ubuntu/Debian)\n- cmd.exe (if applicable)\n```\n\n## Quick Reference\n\n| Environment Variable | Purpose | Value |\n|---------------------|---------|-------|\n| `MSYS_NO_PATHCONV` | Disable all path conversion (Git for Windows) | `1` |\n| `MSYS2_ARG_CONV_EXCL` | Exclude specific arguments from conversion (MSYS2) | `*` or patterns |\n| `MSYSTEM` | Current MSYS subsystem | `MINGW64`, `MINGW32`, `MSYS` |\n| `WSL_DISTRO_NAME` | WSL distribution name | `Ubuntu`, `Debian`, etc. |\n\n## Resources\n\n- [Git for Windows Path Conversion](https://github.com/git-for-windows/git/wiki/FAQ#path-conversion)\n- [MSYS2 Path Conversion](https://www.msys2.org/docs/filesystem-paths/)\n- [Azure CLI on Windows](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows)\n- [PowerShell Core](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell)\n\n## Summary\n\n**Key Takeaways:**\n1. Git Bash automatically converts Unix-style paths to Windows paths\n2. Use `export MSYS_NO_PATHCONV=1` to disable conversion\n3. Detect shell environment using `$MSYSTEM` variable\n4. Test CI/CD scripts on all shells used by your team\n5. Use PowerShell Core (pwsh) for cross-platform scripts\n6. Add shell detection to local development scripts"
              }
            ]
          },
          {
            "name": "salesforce-master",
            "description": "Complete Salesforce expertise system across ALL platforms and integration scenarios. PROACTIVELY activate for: (1) ANY Salesforce task (API/data/Apex/Lightning/Flow), (2) SOQL/SOSL query design and optimization, (3) REST/SOAP API integration and authentication, (4) Apex development (classes/triggers/batch/scheduled) with Spring '25 features, (5) Lightning Web Components with lightning/graphql module and Aura development, (6) Data model design and schema management (objects/fields/relationships), (7) Integration patterns (source-to-Salesforce, Salesforce-to-target), (8) Deployment and metadata management (change sets/SFDX/CLI), (9) Security model implementation (profiles/permissions/sharing), (10) Flow/Process Builder/Flow Orchestrator automation, (11) Agentforce 2.0 AI agents and autonomous automation, (12) Data Cloud Vector Database with semantic search, (13) Hyperforce public cloud architecture. Provides: comprehensive SFDC object schema knowledge (standard/custom objects, all fields, relationships), complete API reference (REST/SOAP/Bulk/Streaming) for API 63.0/64.0, Apex language mastery with Compression and FormulaEval namespaces (Spring '25 GA), Lightning platform expertise with Winter '26 GraphQL updates and SLDS 2.0 dark mode, SOQL/SOSL optimization, authentication methods (OAuth/JWT/Session), limits and best practices, governor limits handling, integration patterns (ETL/real-time/batch/Data Cloud), metadata API operations, SFDX CLI commands, version-specific guidance, platform event architecture, Agentforce 2.0 development with LLM Open Connector, Data Cloud Vector Database for unstructured data and hybrid search, Flow Orchestrator multi-user workflows, and Hyperforce cloud-native architecture. Ensures production-ready, scalable, secure Salesforce solutions following Spring '25/Summer '25 best practices and Well-Architected Framework.",
            "source": "./plugins/salesforce-master",
            "category": null,
            "version": "2.2.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install salesforce-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "agentforce-2025",
                "description": "Salesforce Agentforce AI agents and autonomous automation (2025)",
                "path": "plugins/salesforce-master/skills/agentforce-2025/SKILL.md",
                "frontmatter": {
                  "name": "agentforce-2025",
                  "description": "Salesforce Agentforce AI agents and autonomous automation (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Agentforce: AI Agents for Salesforce (2025)\n\n## What is Agentforce?\n\nAgentforce is Salesforce's enterprise AI agent platform that enables autonomous, proactive applications to execute specialized tasks for employees and customers. Agentforce agents use large language models (LLMs) with the Atlas Reasoning Engine to analyze context, reason through decisions, and take action autonomously.\n\n**Key Distinction**: Agentforce represents the evolution from Einstein Copilot (conversational assistant) to autonomous agents that can complete tasks without human prompting.\n\n## Core Architecture\n\n### Atlas Reasoning Engine\n\nThe Atlas Reasoning Engine is the brain of Agentforce, enabling agents to:\n- **Understand**: Analyze full context of customer interactions or automated triggers\n- **Decide**: Reason through decisions using LLMs and business logic\n- **Act**: Execute actions autonomously across any system\n- **Learn**: Improve over time based on outcomes and feedback\n\n### Agent Components\n\n```\n\n              Agentforce Agent                   \n\n  1. Topics (what agent handles)                 \n  2. Actions (what agent can do)                 \n  3. Instructions (how agent behaves)            \n  4. Channel Integrations (where agent works)    \n  5. Data Sources (what agent knows)             \n\n```\n\n## Agentforce 2.0 (GA December 2024)\n\nAgentforce 2.0 is the digital labor platform for enterprises, enabling a limitless workforce through AI agents. Key enhancements:\n\n- **Pre-built Skills Library**: Rapid agent customization with out-of-the-box capabilities\n- **Workflow Integrations**: MuleSoft for Flow, MuleSoft API Catalog, Topic Center (Q1 2025)\n- **Slack Deployment**: Native Slack integration for collaboration agents\n- **Enhanced RAG**: Improved retrieval augmented generation for accurate responses\n- **Advanced Reasoning**: More sophisticated Atlas Reasoning Engine capabilities\n- **Pricing**: $2 per conversation (GA October 25, 2024)\n\n## Building Agents with Agentforce Builder (GA December 2024)\n\n### Step 1: Define Agent Purpose\n\nIdentify what the agent should accomplish:\n- **Service Agent**: Handle support cases, answer FAQs, resolve issues (4 new actions in Spring '25)\n- **Sales Development Agent**: Qualify leads, answer product questions, book meetings\n- **Personal Shopper Agent**: Recommend products, handle orders, track shipments\n- **Operations Agent**: Automate approvals, process requests, manage workflows\n- **Slack Agent**: Proactive notifications and collaboration assistance\n\n### Step 2: Configure Agent Topics\n\nTopics define what the agent can help with:\n\n```apex\n// Example: Service Agent Topics\nTopic: Password Reset\n- Intent: User wants to reset password\n- Required Data: Email, Username\n- Connected Action: PasswordResetFlow\n\nTopic: Order Status\n- Intent: User wants order status\n- Required Data: Order Number or Email\n- Connected Action: GetOrderStatus\n\nTopic: Escalate to Human\n- Intent: User needs human assistance\n- Required Data: Case context\n- Connected Action: CreateCase + NotifyAgent\n```\n\n### Step 3: Define Agent Actions\n\nActions are what the agent can execute. These can be:\n- **Standard Actions**: Pre-built Salesforce actions (create/update records)\n- **Flow Actions**: Custom Flow automations\n- **Apex Actions**: Custom Apex invocable methods\n- **MuleSoft Actions**: API integrations via MuleSoft connectors\n- **External API Actions**: REST/SOAP callouts\n\n**Example Apex Action**:\n```apex\npublic class AgentActions {\n    @InvocableMethod(label='Get Order Status' description='Retrieves order status for customer')\n    public static List<OrderStatus> getOrderStatus(List<OrderRequest> requests) {\n        List<OrderStatus> results = new List<OrderStatus>();\n\n        for (OrderRequest req : requests) {\n            Order order = [SELECT Id, Status, EstimatedDelivery__c\n                          FROM Order\n                          WHERE OrderNumber = :req.orderNumber\n                          LIMIT 1];\n\n            OrderStatus status = new OrderStatus();\n            status.orderNumber = req.orderNumber;\n            status.status = order.Status;\n            status.estimatedDelivery = order.EstimatedDelivery__c;\n            results.add(status);\n        }\n\n        return results;\n    }\n\n    public class OrderRequest {\n        @InvocableVariable(required=true)\n        public String orderNumber;\n    }\n\n    public class OrderStatus {\n        @InvocableVariable\n        public String orderNumber;\n        @InvocableVariable\n        public String status;\n        @InvocableVariable\n        public Date estimatedDelivery;\n    }\n}\n```\n\n### Step 4: Write Agent Instructions\n\nInstructions guide the agent's behavior and tone:\n\n```\nYou are a helpful customer service agent for Acme Corp.\n\nPersonality:\n- Friendly, professional, and empathetic\n- Patient with customers who are frustrated\n- Proactive in offering solutions\n\nGuidelines:\n- Always greet customers by name if available\n- Verify customer identity before sharing account information\n- Offer alternatives if the requested action cannot be completed\n- Escalate to human agent for: refunds >$500, legal issues, abusive customers\n- Use simple language, avoid jargon\n- Provide order numbers and case numbers in responses\n\nSecurity:\n- Never share: passwords, credit card numbers, SSN\n- Always verify identity using: email, phone, or account number\n- Log all interactions for compliance\n\nResponse Format:\n- Keep responses under 3 sentences when possible\n- Use bullet points for multiple items\n- Include next steps or call-to-action\n```\n\n### Step 5: Connect Data Sources\n\nAgentforce agents can access:\n- **Salesforce Objects**: Standard and custom objects\n- **Data Cloud**: Unified customer data from all sources\n- **Knowledge Base**: Salesforce Knowledge articles\n- **External Systems**: Via APIs and MuleSoft connectors\n\n**Data Cloud Integration**:\n```apex\n// Query Data Cloud from Agentforce\npublic class AgentDataCloudActions {\n    @InvocableMethod(label='Get Customer 360 View')\n    public static List<Customer360> getCustomer360(List<String> customerIds) {\n        // Query Data Cloud for unified customer data\n        List<Customer360> results = new List<Customer360>();\n\n        for (String customerId : customerIds) {\n            // Data Cloud connector provides unified view\n            DataCloudConnector.QueryRequest req = new DataCloudConnector.QueryRequest();\n            req.sql = 'SELECT * FROM Unified_Customer WHERE customer_id = \\'' + customerId + '\\'';\n\n            DataCloudConnector.QueryResponse res = DataCloudConnector.query(req);\n\n            Customer360 customer = new Customer360();\n            customer.customerId = customerId;\n            customer.totalPurchases = (Decimal)res.data.get('total_purchases');\n            customer.preferredChannel = (String)res.data.get('preferred_channel');\n            customer.lifetimeValue = (Decimal)res.data.get('lifetime_value');\n            results.add(customer);\n        }\n\n        return results;\n    }\n}\n```\n\n### Step 6: Configure Channels\n\nDeploy agents across multiple channels:\n- **Web Chat**: Embedded on website\n- **Mobile App**: In Salesforce Mobile or custom apps\n- **SMS/WhatsApp**: Messaging platforms\n- **Slack/Teams**: Collaboration tools\n- **Voice**: Phone support with voice-to-text\n- **Email**: Email case management\n\n## Agent Types and Use Cases\n\n### 1. Service Agent (Customer Support)\n\n**Capabilities**:\n- Answer FAQs from Knowledge Base\n- Retrieve order/account status\n- Process returns and exchanges\n- Reset passwords and unlock accounts\n- Create and route cases to specialists\n- Provide troubleshooting steps\n\n**Example Flow**:\n```\nCustomer: \"Where is my order #12345?\"\n\nAgent: Validates order number\n\nAgent: Queries Order object\n\nAgent: Retrieves tracking information\n\nAgent: \"Your order #12345 shipped yesterday and will arrive Thursday.\n       Tracking: UPS 1Z999AA10123456784. Need anything else?\"\n```\n\n### 2. Sales Development Agent (SDR)\n\n**Capabilities**:\n- Qualify inbound leads\n- Answer product questions\n- Handle objections with sales playbooks\n- Book meetings with sales reps\n- Send follow-up emails\n- Update lead scores based on engagement\n\n**Example Flow**:\n```\nLead: \"Tell me about your enterprise plan\"\n\nAgent: Retrieves product information\n\nAgent: Explains features, pricing\n\nAgent: Detects buying intent\n\nAgent: \"Would you like to schedule a demo with our sales team?\"\n\nAgent: Creates meeting, updates lead status to \"Meeting Scheduled\"\n```\n\n### 3. Personal Shopper Agent (E-commerce)\n\n**Capabilities**:\n- Recommend products based on preferences\n- Answer product questions\n- Check inventory and availability\n- Process orders and payments\n- Apply discounts and promotions\n- Handle cart abandonment\n\n### 4. Operations Agent (Internal Automation)\n\n**Capabilities**:\n- Process employee requests (PTO, equipment)\n- Automate approvals based on rules\n- Onboard new employees\n- Generate reports and insights\n- Monitor system health\n- Trigger workflows based on events\n\n## Integrating Agentforce with Platform Events\n\nPublish events to trigger Agentforce actions:\n\n```apex\n// Publish event when order status changes\npublic class OrderEventPublisher {\n    public static void publishOrderUpdate(Id orderId, String newStatus) {\n        OrderStatusChangeEvent__e event = new OrderStatusChangeEvent__e(\n            OrderId__c = orderId,\n            NewStatus__c = newStatus,\n            Timestamp__c = System.now()\n        );\n\n        EventBus.publish(event);\n\n        // Agentforce subscribes to this event\n        // Triggers proactive customer notification\n    }\n}\n\n// Trigger\ntrigger OrderTrigger on Order (after update) {\n    for (Order ord : Trigger.new) {\n        if (ord.Status != Trigger.oldMap.get(ord.Id).Status) {\n            OrderEventPublisher.publishOrderUpdate(ord.Id, ord.Status);\n        }\n    }\n}\n```\n\n**Agentforce Flow** (subscribed to OrderStatusChangeEvent__e):\n```\n1. Receive event\n2. Query order and customer details\n3. Determine notification channel (email, SMS, push)\n4. Generate personalized message using LLM\n5. Send notification via preferred channel\n6. Log interaction in Customer timeline\n```\n\n## Agentforce with External AI Systems\n\nIntegrate Agentforce with external AI providers:\n\n### OpenAI GPT Integration\n\n```apex\npublic class AgentOpenAIIntegration {\n    @InvocableMethod(label='Generate Response with GPT-4')\n    public static List<String> generateResponse(List<AIRequest> requests) {\n        List<String> responses = new List<String>();\n\n        for (AIRequest req : requests) {\n            HttpRequest httpReq = new HttpRequest();\n            httpReq.setEndpoint('callout:OpenAI/v1/chat/completions');\n            httpReq.setMethod('POST');\n            httpReq.setHeader('Content-Type', 'application/json');\n\n            Map<String, Object> payload = new Map<String, Object>{\n                'model' => 'gpt-4',\n                'messages' => new List<Object>{\n                    new Map<String, String>{\n                        'role' => 'system',\n                        'content' => req.systemPrompt\n                    },\n                    new Map<String, String>{\n                        'role' => 'user',\n                        'content' => req.userMessage\n                    }\n                },\n                'temperature' => 0.7,\n                'max_tokens' => 500\n            };\n\n            httpReq.setBody(JSON.serialize(payload));\n\n            Http http = new Http();\n            HttpResponse httpRes = http.send(httpReq);\n\n            if (httpRes.getStatusCode() == 200) {\n                Map<String, Object> result = (Map<String, Object>)JSON.deserializeUntyped(httpRes.getBody());\n                List<Object> choices = (List<Object>)result.get('choices');\n                Map<String, Object> choice = (Map<String, Object>)choices[0];\n                Map<String, Object> message = (Map<String, Object>)choice.get('message');\n                responses.add((String)message.get('content'));\n            }\n        }\n\n        return responses;\n    }\n\n    public class AIRequest {\n        @InvocableVariable(required=true)\n        public String systemPrompt;\n        @InvocableVariable(required=true)\n        public String userMessage;\n    }\n}\n```\n\n### Anthropic Claude Integration\n\n```apex\npublic class AgentClaudeIntegration {\n    @InvocableMethod(label='Generate Response with Claude')\n    public static List<String> generateResponse(List<AIRequest> requests) {\n        List<String> responses = new List<String>();\n\n        for (AIRequest req : requests) {\n            HttpRequest httpReq = new HttpRequest();\n            httpReq.setEndpoint('callout:Anthropic/v1/messages');\n            httpReq.setMethod('POST');\n            httpReq.setHeader('Content-Type', 'application/json');\n            httpReq.setHeader('anthropic-version', '2023-06-01');\n\n            Map<String, Object> payload = new Map<String, Object>{\n                'model' => 'claude-3-5-sonnet-20241022',\n                'max_tokens' => 1024,\n                'system' => req.systemPrompt,\n                'messages' => new List<Object>{\n                    new Map<String, String>{\n                        'role' => 'user',\n                        'content' => req.userMessage\n                    }\n                }\n            };\n\n            httpReq.setBody(JSON.serialize(payload));\n\n            Http http = new Http();\n            HttpResponse httpRes = http.send(httpReq);\n\n            if (httpRes.getStatusCode() == 200) {\n                Map<String, Object> result = (Map<String, Object>)JSON.deserializeUntyped(httpRes.getBody());\n                List<Object> content = (List<Object>)result.get('content');\n                Map<String, Object> contentBlock = (Map<String, Object>)content[0];\n                responses.add((String)contentBlock.get('text'));\n            }\n        }\n\n        return responses;\n    }\n}\n```\n\n## Monitoring and Analytics\n\n### Agent Performance Metrics\n\nTrack agent effectiveness:\n- **Resolution Rate**: % of interactions resolved without escalation\n- **Average Handle Time**: Time to resolve customer request\n- **Customer Satisfaction**: Post-interaction survey scores\n- **Containment Rate**: % of interactions handled by agent vs human\n- **Action Success Rate**: % of successful action executions\n\n**Custom Reporting Object**:\n```apex\npublic class AgentInteractionLogger {\n    public static void logInteraction(String agentName, String topic,\n                                     Boolean resolved, Decimal duration) {\n        AgentInteraction__c interaction = new AgentInteraction__c(\n            AgentName__c = agentName,\n            Topic__c = topic,\n            Resolved__c = resolved,\n            Duration__c = duration,\n            Timestamp__c = System.now()\n        );\n        insert interaction;\n    }\n}\n```\n\n### Analytics Dashboard Queries\n\n```sql\n-- Resolution rate by agent\nSELECT AgentName__c,\n       COUNT(Id) as TotalInteractions,\n       SUM(CASE WHEN Resolved__c = true THEN 1 ELSE 0 END) as Resolved,\n       AVG(Duration__c) as AvgDuration\nFROM AgentInteraction__c\nWHERE CreatedDate = LAST_N_DAYS:30\nGROUP BY AgentName__c\n\n-- Top topics requiring human escalation\nSELECT Topic__c,\n       COUNT(Id) as Escalations\nFROM AgentInteraction__c\nWHERE Resolved__c = false\n  AND CreatedDate = LAST_N_DAYS:30\nGROUP BY Topic__c\nORDER BY COUNT(Id) DESC\nLIMIT 10\n```\n\n## Best Practices\n\n### Security and Compliance\n- **Field-Level Security**: Always use WITH SECURITY_ENFORCED in SOQL\n- **Data Access**: Respect sharing rules with `with sharing` keywords\n- **PII Protection**: Never log sensitive data (SSN, credit cards, passwords)\n- **Audit Trail**: Log all agent actions for compliance\n- **Human Oversight**: Require approval for high-impact actions\n\n### Performance Optimization\n- **Batch Processing**: Group similar actions to reduce API calls\n- **Caching**: Cache frequently accessed data (product catalogs, FAQs)\n- **Async Execution**: Use @future or Queueable for non-critical actions\n- **Rate Limiting**: Implement throttling for external API calls\n- **Timeout Handling**: Set appropriate timeouts and retry logic\n\n### User Experience\n- **Response Time**: Aim for <3 second responses\n- **Personalization**: Use customer data for personalized responses\n- **Transparency**: Clearly identify agent vs human interactions\n- **Escalation**: Provide easy path to human agent when needed\n- **Feedback Loop**: Collect user feedback to improve agent\n\n### Testing\n- **Unit Tests**: Test individual actions in isolation\n- **Integration Tests**: Test end-to-end agent flows\n- **Load Tests**: Simulate high volume to test scalability\n- **User Acceptance Tests**: Validate with real users in sandbox\n- **A/B Testing**: Compare different agent configurations\n\n## Agentforce Pricing and Licensing (2025)\n\n- **Agentforce Service Agent**: $2 per conversation (GA October 2024)\n- **Agentforce Sales Development Agent**: $2 per conversation\n- **Custom Agents**: Available with Einstein 1 Edition or add-on\n- **Agent Runs**: 600 free orchestration runs per year (Enterprise+)\n- **Consumption Model**: Pay-per-use based on conversations\n- **Vision**: One billion agents with Agentforce by end of 2025\n\n## Spring '25 and Summer '25 Updates\n\n### Spring '25 (API 63.0) - Available Q2 2025\n- **Enhanced Service Agent**: 4 new agent actions + 1 new topic\n- **Salesforce LLM Open Connector**: Connect any LLM (OpenAI, Claude, custom models)\n- **Conversation Context Testing**: Specify language, app, page type for precise testing\n- **Einstein Decision Element**: Automate flow paths based on email engagement metrics\n- **Einstein-Powered Flow Creation**: New Einstein Panel in Flow Builder\n\n### Summer '25 (API 64.0) - Available Q3 2025\n- **Hybrid Search**: Combines semantic search with keyword search for accuracy\n- **Multi-language Semantic Search**: Cross-language case similarity (e.g., French  English)\n- **Report Formula Generation**: Plain language descriptions create complex formulas\n- **AI-driven Account Summarization**: Automated insights for service agents\n\n## Resources\n\n- **Agentforce Platform**: https://www.salesforce.com/agentforce/\n- **Agentforce Builder Documentation**: https://help.salesforce.com/s/articleView?id=sf.einstein_studio.htm\n- **Einstein 1 Studio**: https://help.salesforce.com/s/articleView?id=sf.einstein_studio.htm\n- **Atlas Reasoning Engine**: Technical documentation in Winter '26 release notes\n- **Agentforce Trailhead**: Search \"Agentforce\" on Trailhead for modules\n- **LLM Open Connector**: Spring '25 release notes\n\n## Migration from Einstein Copilot\n\n**IMPORTANT**: Einstein Copilot was retired in January 2025 and renamed to \"Agentforce (Default)\". It is now one of the Agentforce agents.\n\nIf you have Einstein Copilot (now Agentforce Assistant), migration path:\n\n```\nEinstein Copilot  Agentforce (Default)\n- Automatically migrated in January 2025\n- Conversational UI remains the same\n- Add autonomous triggers and workflows\n- Convert Copilot Actions to Agent Actions\n- Add proactive agent behaviors\n- Enable multi-channel deployment (including Slack in 2.0)\n```\n\n**Action Migration Example**:\n```apex\n// Einstein Copilot Action (reactive)\n@InvocableMethod(label='Copilot: Get Account Info')\npublic static List<Account> getAccountInfo(List<Id> accountIds) {\n    return [SELECT Id, Name, Industry FROM Account WHERE Id IN :accountIds];\n}\n\n// Agentforce Action (proactive + reactive)\n@InvocableMethod(label='Agent: Monitor and Alert on Account Changes')\npublic static void monitorAccounts(List<Id> accountIds) {\n    // Not only retrieve data, but also:\n    // 1. Monitor for changes\n    // 2. Detect anomalies (sudden revenue drop)\n    // 3. Proactively alert account manager\n    // 4. Suggest next best actions\n}\n```\n\nAgentforce represents a paradigm shift from conversational assistants to autonomous agents that can complete complex, multi-step tasks without human intervention while maintaining trust and security."
              },
              {
                "name": "data-cloud-2025",
                "description": "Salesforce Data Cloud integration patterns and architecture (2025)",
                "path": "plugins/salesforce-master/skills/data-cloud-2025/SKILL.md",
                "frontmatter": {
                  "name": "data-cloud-2025",
                  "description": "Salesforce Data Cloud integration patterns and architecture (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Salesforce Data Cloud Integration Patterns (2025)\n\n## What is Salesforce Data Cloud?\n\nSalesforce Data Cloud is a real-time customer data platform (CDP) that unifies data from any source to create a complete, actionable view of every customer. It powers AI, automation, and analytics across the entire Customer 360 platform.\n\n**Key Capabilities**:\n- **Data Ingestion**: Connect 200+ sources (Salesforce, external systems, data lakes)\n- **Data Harmonization**: Map disparate data to unified data model\n- **Identity Resolution**: Match and merge customer records across sources\n- **Real-Time Activation**: Trigger actions based on streaming data\n- **Zero Copy Architecture**: Query data in place without moving it\n- **AI/ML Ready**: Powers Einstein, Agentforce, and predictive models\n- **Vector Database** (GA March 2025): Store and query unstructured data with semantic search\n- **Hybrid Search** (Pilot 2025): Combine semantic and keyword search for accuracy\n\n## Data Cloud Architecture\n\n```\n\n                    Data Sources                          \n\n  Salesforce CRM  External Apps  Data Warehouses  APIs \n\n                                                   \n    \n             Data Cloud Connectors & Ingestion            \n       Real-time Streaming (Change Data Capture)        \n       Batch Import (scheduled/on-demand)               \n       Zero Copy (Snowflake, Databricks, BigQuery)      \n    \n                             \n    \n                Data Model & Harmonization                \n       Map to Common Data Model (DMO objects)           \n       Identity Resolution (match & merge)              \n       Data Transformation (calculated insights)        \n    \n                             \n    \n             Unified Customer Profile (360 View)         \n       Demographics, Transactions, Behavior, Events     \n       Real-time Profile API for instant access         \n    \n                             \n    \n                  Activation & Actions                    \n       Salesforce Flow (real-time automation)           \n       Marketing Cloud (segmentation/journeys)          \n       Agentforce (AI agents)                           \n       Einstein AI (predictions/recommendations)        \n       External Systems (reverse ETL)                   \n    \n```\n\n## Data Ingestion Patterns\n\n### Pattern 1: Real-Time Streaming with Change Data Capture\n\n**Use Case**: Keep Data Cloud synchronized with Salesforce objects in real-time\n\n```apex\n// Enable Change Data Capture for objects\n// Setup  Change Data Capture  Select: Account, Contact, Opportunity\n\n// Data Cloud automatically subscribes to CDC channels\n// No code needed - configure in Data Cloud UI\n\n// Optional: Custom streaming logic\npublic class DataCloudStreamHandler {\n    public static void publishCustomEvent(Id recordId, String changeType) {\n        // Publish custom platform event for Data Cloud\n        DataCloudChangeEvent__e event = new DataCloudChangeEvent__e(\n            RecordId__c = recordId,\n            ObjectType__c = 'Custom_Object__c',\n            ChangeType__c = changeType,\n            Timestamp__c = System.now(),\n            PayloadJson__c = JSON.serialize(getRecordData(recordId))\n        );\n\n        EventBus.publish(event);\n    }\n\n    private static Map<String, Object> getRecordData(Id recordId) {\n        // Retrieve and return record data\n        String objectType = recordId.getSObjectType().getDescribe().getName();\n        String query = 'SELECT FIELDS(ALL) FROM ' + objectType +\n                      ' WHERE Id = :recordId LIMIT 1';\n        SObject record = Database.query(query);\n        return (Map<String, Object>)JSON.deserializeUntyped(JSON.serialize(record));\n    }\n}\n```\n\n### Pattern 2: Batch Import from External Systems\n\n**Use Case**: Import data from ERP, e-commerce, or other business systems\n\n**Data Cloud Configuration**:\n```\n1. Create Data Source (Setup  Data Cloud  Data Sources)\n   - Type: Amazon S3, SFTP, Azure Blob, Google Cloud Storage\n   - Authentication: API key, OAuth, IAM role\n   - Schedule: Hourly, Daily, Weekly\n\n2. Map to Data Model Objects (DMO)\n   - Source Field  DMO Field mapping\n   - Data type conversions\n   - Formula fields and transformations\n\n3. Configure Identity Resolution\n   - Match rules (email, customer ID, phone)\n   - Reconciliation rules (which source wins)\n```\n\n**API-Based Batch Import**:\n```python\n# Python example: Push data to Data Cloud via API\nimport requests\nimport pandas as pd\n\ndef upload_to_data_cloud(csv_file, object_name, access_token, instance_url):\n    \"\"\"Upload CSV to Data Cloud via Bulk API\"\"\"\n\n    # Step 1: Create ingestion job\n    job_url = f\"{instance_url}/services/data/v62.0/jobs/ingest\"\n    job_payload = {\n        \"object\": object_name,\n        \"operation\": \"upsert\",\n        \"externalIdFieldName\": \"ExternalId__c\"\n    }\n\n    response = requests.post(\n        job_url,\n        headers={\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        },\n        json=job_payload\n    )\n\n    job_id = response.json()[\"id\"]\n\n    # Step 2: Upload CSV data\n    with open(csv_file, 'rb') as f:\n        csv_data = f.read()\n\n    upload_url = f\"{job_url}/{job_id}/batches\"\n    requests.put(\n        upload_url,\n        headers={\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"text/csv\"\n        },\n        data=csv_data\n    )\n\n    # Step 3: Close job\n    close_url = f\"{job_url}/{job_id}\"\n    requests.patch(\n        close_url,\n        headers={\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        },\n        json={\"state\": \"UploadComplete\"}\n    )\n\n    return job_id\n```\n\n### Pattern 3: Zero Copy Integration (Snowflake, Databricks)\n\n**Use Case**: Access data warehouse data without copying to Salesforce\n\n**Benefits**:\n- No data duplication (single source of truth)\n- No data transfer costs\n- Real-time access to warehouse data\n- Maintain data governance in warehouse\n\n**Snowflake Zero Copy Setup**:\n```sql\n-- In Snowflake: Grant access to Salesforce\nGRANT USAGE ON DATABASE customer_data TO ROLE salesforce_role;\nGRANT USAGE ON SCHEMA customer_data.public TO ROLE salesforce_role;\nGRANT SELECT ON TABLE customer_data.public.orders TO ROLE salesforce_role;\n\n-- Create secure share\nCREATE SHARE salesforce_data_share;\nGRANT USAGE ON DATABASE customer_data TO SHARE salesforce_data_share;\nALTER SHARE salesforce_data_share ADD ACCOUNTS = 'SALESFORCE_ORG_ID';\n```\n\n**Data Cloud Configuration**:\n```\n1. Add Zero Copy Connector (Data Cloud  Data Sources)\n   - Type: Snowflake Zero Copy\n   - Connection: Account URL, username, private key\n   - Database/Schema selection\n\n2. Create Data Stream (virtual tables)\n   - Select Snowflake tables to expose\n   - Map to DMO or keep as is\n   - Configure refresh (real-time or scheduled)\n\n3. Query in Salesforce\n   - Use SOQL-like syntax to query Snowflake data\n   - Join with Salesforce data\n   - No data movement required\n```\n\n**Query Zero Copy Data**:\n```apex\n// Query Snowflake data from Apex (via Data Cloud)\npublic class DataCloudZeroCopyQuery {\n    public static List<Map<String, Object>> querySnowflakeOrders(String customerId) {\n        // Data Cloud Query API\n        String query = 'SELECT order_id, total_amount, order_date ' +\n                      'FROM snowflake_orders ' +\n                      'WHERE customer_id = \\'' + customerId + '\\' ' +\n                      'ORDER BY order_date DESC LIMIT 10';\n\n        HttpRequest req = new HttpRequest();\n        req.setEndpoint('callout:DataCloud/v1/query');\n        req.setMethod('POST');\n        req.setHeader('Content-Type', 'application/json');\n        req.setBody(JSON.serialize(new Map<String, String>{'query' => query}));\n\n        Http http = new Http();\n        HttpResponse res = http.send(req);\n\n        if (res.getStatusCode() == 200) {\n            Map<String, Object> result = (Map<String, Object>)JSON.deserializeUntyped(res.getBody());\n            return (List<Map<String, Object>>)result.get('data');\n        }\n\n        return new List<Map<String, Object>>();\n    }\n}\n```\n\n## Identity Resolution\n\n### Matching Rules\n\n**Configure identity resolution to create unified profiles**:\n\n```\nMatch Rules Configuration:\n Primary Match (exact match on email)\n   IF email matches THEN merge profiles\n Secondary Match (fuzzy match on name + phone)\n   IF firstName + lastName similar AND phone matches THEN merge\n Tertiary Match (external ID)\n    IF ExternalCustomerId matches THEN merge\n\nReconciliation Rules (conflict resolution):\n Most Recent: Use most recently updated value\n Source Priority: Salesforce > ERP > Website\n Field-Level Rules: Email from Salesforce, Revenue from ERP\n```\n\n**Custom Matching Logic**:\n```apex\n// Custom matching for complex scenarios\npublic class DataCloudMatchingService {\n    public static Boolean shouldMatch(Map<String, Object> profile1,\n                                     Map<String, Object> profile2) {\n        // Custom matching logic beyond standard rules\n\n        String email1 = (String)profile1.get('email');\n        String email2 = (String)profile2.get('email');\n\n        // Exact email match\n        if (email1 != null && email1.equalsIgnoreCase(email2)) {\n            return true;\n        }\n\n        // Fuzzy name + address match\n        String name1 = (String)profile1.get('fullName');\n        String name2 = (String)profile2.get('fullName');\n        String address1 = (String)profile1.get('address');\n        String address2 = (String)profile2.get('address');\n\n        if (isNameSimilar(name1, name2) && isSameAddress(address1, address2)) {\n            return true;\n        }\n\n        return false;\n    }\n\n    private static Boolean isNameSimilar(String name1, String name2) {\n        // Implement Levenshtein distance or phonetic matching\n        return calculateSimilarity(name1, name2) > 0.85;\n    }\n}\n```\n\n## Real-Time Activation Patterns\n\n### Pattern 1: Flow Automation Based on Data Cloud Events\n\n**Use Case**: Trigger Flow when customer behavior detected in Data Cloud\n\n```\nData Cloud Calculated Insight: \"High-Value Customer at Risk\"\n- Logic: Purchase frequency decreased by 50% in last 30 days\n- Trigger: When insight calculated\n\nPlatform Event: HighValueCustomerRisk__e\n\nSalesforce Flow: \"Retain High-Value Customer\"\n- Create Task for Account Manager\n- Send personalized offer via Marketing Cloud\n- Add to \"At-Risk\" campaign\n- Log activity timeline\n```\n\n**Apex Implementation**:\n```apex\n// Subscribe to Data Cloud insights\ntrigger DataCloudInsightTrigger on HighValueCustomerRisk__e (after insert) {\n    List<Task> tasks = new List<Task>();\n\n    for (HighValueCustomerRisk__e event : Trigger.new) {\n        // Create retention task\n        Task task = new Task(\n            Subject = 'Urgent: High-value customer at risk',\n            Description = 'Customer ' + event.CustomerName__c +\n                         ' shows declining engagement. Take action.',\n            WhatId = event.AccountId__c,\n            Priority = 'High',\n            Status = 'Open',\n            ActivityDate = Date.today().addDays(1)\n        );\n        tasks.add(task);\n\n        // Trigger retention campaign\n        RetentionCampaignService.addToRetentionCampaign(\n            event.CustomerId__c,\n            event.RiskScore__c\n        );\n    }\n\n    if (!tasks.isEmpty()) {\n        insert tasks;\n    }\n}\n```\n\n### Pattern 2: Agentforce with Data Cloud\n\n**Use Case**: AI agent uses Data Cloud for complete customer context\n\n```apex\n// Agentforce action: Get unified customer view\npublic class AgentforceDataCloudActions {\n    @InvocableMethod(label='Get Customer 360 Profile')\n    public static List<CustomerProfile> getCustomer360(List<String> customerIds) {\n        List<CustomerProfile> profiles = new List<CustomerProfile>();\n\n        for (String customerId : customerIds) {\n            // Query Data Cloud unified profile\n            HttpRequest req = new HttpRequest();\n            req.setEndpoint('callout:DataCloud/v1/profile/' + customerId);\n            req.setMethod('GET');\n\n            Http http = new Http();\n            HttpResponse res = http.send(req);\n\n            if (res.getStatusCode() == 200) {\n                Map<String, Object> data = (Map<String, Object>)\n                    JSON.deserializeUntyped(res.getBody());\n\n                CustomerProfile profile = new CustomerProfile();\n                profile.customerId = customerId;\n\n                // Demographics\n                profile.name = (String)data.get('name');\n                profile.email = (String)data.get('email');\n                profile.segment = (String)data.get('segment');\n\n                // Behavioral\n                profile.totalPurchases = (Decimal)data.get('total_purchases');\n                profile.avgOrderValue = (Decimal)data.get('avg_order_value');\n                profile.lastPurchaseDate = Date.valueOf((String)data.get('last_purchase_date'));\n                profile.preferredChannel = (String)data.get('preferred_channel');\n\n                // Engagement\n                profile.emailEngagement = (Decimal)data.get('email_engagement_score');\n                profile.websiteVisits = (Integer)data.get('website_visits_30d');\n                profile.supportCases = (Integer)data.get('support_cases_90d');\n\n                // Predictive\n                profile.churnRisk = (Decimal)data.get('churn_risk_score');\n                profile.lifetimeValue = (Decimal)data.get('predicted_lifetime_value');\n                profile.nextBestAction = (String)data.get('next_best_action');\n\n                profiles.add(profile);\n            }\n        }\n\n        return profiles;\n    }\n\n    public class CustomerProfile {\n        @InvocableVariable public String customerId;\n        @InvocableVariable public String name;\n        @InvocableVariable public String email;\n        @InvocableVariable public String segment;\n        @InvocableVariable public Decimal totalPurchases;\n        @InvocableVariable public Decimal avgOrderValue;\n        @InvocableVariable public Date lastPurchaseDate;\n        @InvocableVariable public String preferredChannel;\n        @InvocableVariable public Decimal emailEngagement;\n        @InvocableVariable public Integer websiteVisits;\n        @InvocableVariable public Integer supportCases;\n        @InvocableVariable public Decimal churnRisk;\n        @InvocableVariable public Decimal lifetimeValue;\n        @InvocableVariable public String nextBestAction;\n    }\n}\n```\n\n### Pattern 3: Reverse ETL (Data Cloud  External Systems)\n\n**Use Case**: Push enriched Data Cloud data back to external systems\n\n**Configuration**:\n```\nData Cloud  Data Actions  Create Data Action\n- Target: External API endpoint\n- Trigger: Segment membership change, insight calculated\n- Payload: Customer profile fields\n- Authentication: Named Credential\n- Schedule: Real-time or batch\n```\n\n**Apex Outbound Sync**:\n```apex\npublic class DataCloudReverseETL {\n    @InvocableMethod(label='Sync Enriched Profile to External System')\n    public static void syncToExternalSystem(List<String> customerIds) {\n        for (String customerId : customerIds) {\n            // Get enriched profile from Data Cloud\n            Map<String, Object> profile = DataCloudService.getProfile(customerId);\n\n            // Transform for external system\n            Map<String, Object> payload = new Map<String, Object>{\n                'customer_id' => customerId,\n                'segment' => profile.get('segment'),\n                'lifetime_value' => profile.get('ltv'),\n                'churn_risk' => profile.get('churn_risk'),\n                'next_best_product' => profile.get('next_best_product')\n            };\n\n            // Send to external system\n            HttpRequest req = new HttpRequest();\n            req.setEndpoint('callout:ExternalCRM/api/customers/' + customerId);\n            req.setMethod('PUT');\n            req.setHeader('Content-Type', 'application/json');\n            req.setBody(JSON.serialize(payload));\n\n            Http http = new Http();\n            HttpResponse res = http.send(req);\n\n            // Log result\n            DataCloudSyncLog__c log = new DataCloudSyncLog__c(\n                CustomerId__c = customerId,\n                Direction__c = 'Outbound',\n                Success__c = res.getStatusCode() == 200,\n                Timestamp__c = System.now()\n            );\n            insert log;\n        }\n    }\n}\n```\n\n## Calculated Insights and Segmentation\n\n### Create Calculated Insights\n\n**Use Case**: Define metrics and KPIs on unified data\n\n```sql\n-- Example: Customer Lifetime Value\nCREATE CALCULATED INSIGHT customer_lifetime_value AS\nSELECT\n    customer_id,\n    SUM(order_total) as total_revenue,\n    COUNT(order_id) as total_orders,\n    AVG(order_total) as avg_order_value,\n    DATEDIFF(day, first_order_date, CURRENT_DATE) as customer_age_days,\n    SUM(order_total) / NULLIF(DATEDIFF(day, first_order_date, CURRENT_DATE), 0) * 365 as annual_revenue,\n    (SUM(order_total) / NULLIF(DATEDIFF(day, first_order_date, CURRENT_DATE), 0) * 365) * 5 as predicted_ltv_5yr\nFROM unified_orders\nGROUP BY customer_id, first_order_date\n```\n\n### Dynamic Segmentation\n\n**Use Case**: Create segments that update in real-time\n\n```sql\n-- Segment: High-Value Active Customers\nCREATE SEGMENT high_value_active_customers AS\nSELECT customer_id\nFROM customer_360_profile\nWHERE\n    predicted_ltv_5yr > 10000\n    AND last_purchase_date >= CURRENT_DATE - INTERVAL '30' DAY\n    AND email_engagement_score > 0.7\n    AND churn_risk_score < 0.3\n```\n\n**Use in Salesforce**:\n```apex\n// Query segment membership\nList<Contact> highValueContacts = [\n    SELECT Id, Name, Email\n    FROM Contact\n    WHERE Id IN (\n        SELECT ContactId__c\n        FROM DataCloudSegmentMember__c\n        WHERE SegmentName__c = 'high_value_active_customers'\n    )\n];\n```\n\n## Data Cloud Vector Database (GA March 2025)\n\n### What is Vector Database?\n\nData Cloud Vector Database ingests, stores, unifies, indexes, and allows semantic queries of unstructured data using generative AI techniques. It creates embeddings that enable semantic querying and seamless integration with structured data in the Einstein platform.\n\n**Supported Unstructured Data**:\n- Emails and email threads\n- Text documents (PDFs, Word, etc.)\n- Social media content\n- Web content and chat transcripts\n- Call transcripts and recordings\n- Knowledge base articles\n- Customer reviews and feedback\n\n### How Vector Database Works\n\n```\n\n            Unstructured Data Sources                     \n  Emails  Documents  Transcripts  Social  Knowledge  \n\n                      \n    \n              Text Embedding Generation                   \n      Uses LLM to convert text  vector embeddings        \n      (768-dimensional numeric representations)           \n    \n                      \n    \n            Vector Database Storage & Indexing            \n      Stores embeddings with metadata                     \n      Creates high-performance vector index               \n    \n                      \n    \n               Semantic Search Queries                    \n      Natural language query  embedding  similarity     \n      Returns most semantically similar content           \n    \n```\n\n### Semantic Search with Einstein Copilot Search\n\nSemantic search understands the meaning and intent of queries, going beyond keyword matching:\n\n**Example**:\n- **Query**: \"How do I return a defective product?\"\n- **Traditional Keyword Search**: Matches documents containing exact words \"return\", \"defective\", \"product\"\n- **Semantic Search**: Finds documents about:\n  - Return policies\n  - Warranty claims\n  - Product exchanges\n  - Refund procedures\n  - RMA processes\n  - *Even if they use different wording*\n\n### Implementing Vector Database\n\n**Step 1: Configure Unstructured Data Sources**\n\n```\nSetup  Data Cloud  Data Sources  Create\n- Source Type: Unstructured Data\n- Options:\n   Salesforce Knowledge\n   EmailMessage object\n   External documents (S3, Azure Blob, Google Drive)\n   API-based ingestion\n   ContentDocument/File objects\n```\n\n**Step 2: Enable Vector Indexing**\n\n```apex\n// API to index unstructured content\npublic class VectorDatabaseService {\n    public static void indexDocument(String documentId, String content, Map<String, Object> metadata) {\n        // Create vector embedding request\n        HttpRequest req = new HttpRequest();\n        req.setEndpoint('callout:DataCloud/v1/vector/index');\n        req.setMethod('POST');\n        req.setHeader('Content-Type', 'application/json');\n\n        Map<String, Object> payload = new Map<String, Object>{\n            'documentId' => documentId,\n            'content' => content,\n            'metadata' => metadata,\n            'source' => 'Salesforce',\n            'timestamp' => System.now().getTime()\n        };\n\n        req.setBody(JSON.serialize(payload));\n\n        Http http = new Http();\n        HttpResponse res = http.send(req);\n\n        if (res.getStatusCode() == 200) {\n            System.debug('Document indexed: ' + documentId);\n        } else {\n            System.debug('Indexing failed: ' + res.getBody());\n        }\n    }\n}\n\n// Trigger to auto-index Knowledge articles\ntrigger KnowledgeArticleTrigger on Knowledge__kav (after insert, after update) {\n    for (Knowledge__kav article : Trigger.new) {\n        if (article.PublishStatus == 'Online') {\n            Map<String, Object> metadata = new Map<String, Object>{\n                'articleNumber' => article.ArticleNumber,\n                'title' => article.Title,\n                'category' => article.Category__c,\n                'language' => article.Language\n            };\n\n            VectorDatabaseService.indexDocument(\n                article.Id,\n                article.Body__c,\n                metadata\n            );\n        }\n    }\n}\n```\n\n**Step 3: Perform Semantic Search**\n\n```apex\npublic class SemanticSearchService {\n    @InvocableMethod(label='Semantic Search' description='Search unstructured data semantically')\n    public static List<SearchResult> semanticSearch(List<SearchRequest> requests) {\n        List<SearchResult> results = new List<SearchResult>();\n\n        for (SearchRequest req : requests) {\n            HttpRequest httpReq = new HttpRequest();\n            httpReq.setEndpoint('callout:DataCloud/v1/vector/search');\n            httpReq.setMethod('POST');\n            httpReq.setHeader('Content-Type', 'application/json');\n\n            Map<String, Object> payload = new Map<String, Object>{\n                'query' => req.query,\n                'topK' => req.maxResults,\n                'filters' => req.filters,\n                'includeMetadata' => true\n            };\n\n            httpReq.setBody(JSON.serialize(payload));\n\n            Http http = new Http();\n            HttpResponse httpRes = http.send(httpReq);\n\n            if (httpRes.getStatusCode() == 200) {\n                Map<String, Object> response = (Map<String, Object>)\n                    JSON.deserializeUntyped(httpRes.getBody());\n\n                List<Object> hits = (List<Object>)response.get('results');\n\n                SearchResult result = new SearchResult();\n                result.query = req.query;\n                result.matches = new List<String>();\n\n                for (Object hit : hits) {\n                    Map<String, Object> doc = (Map<String, Object>)hit;\n                    result.matches.add((String)doc.get('content'));\n                }\n\n                results.add(result);\n            }\n        }\n\n        return results;\n    }\n\n    public class SearchRequest {\n        @InvocableVariable(required=true)\n        public String query;\n        @InvocableVariable\n        public Integer maxResults = 10;\n        @InvocableVariable\n        public Map<String, String> filters;\n    }\n\n    public class SearchResult {\n        @InvocableVariable\n        public String query;\n        @InvocableVariable\n        public List<String> matches;\n    }\n}\n```\n\n### Hybrid Search (Pilot 2025)\n\nHybrid search combines semantic search with traditional keyword search for improved accuracy:\n\n**Benefits**:\n- Understands semantic similarities and context (semantic search)\n- Recognizes company-specific words and concepts (keyword search)\n- Higher accuracy than either method alone\n- Handles acronyms, product codes, and technical terms better\n\n**Use Case Example**:\n```\nService agent searches: \"customer wants refund for SKU-12345\"\n\nSemantic Search finds:\n- Return policy documents\n- Refund procedures\n- Customer satisfaction articles\n\nKeyword Search finds:\n- Specific SKU-12345 product documentation\n- Previous cases mentioning SKU-12345\n- Product-specific return windows\n\nHybrid Search combines both:\n- Return procedures specifically for SKU-12345\n- Previous refund cases for this product\n- Product warranty terms\n```\n\n**Implementation**:\n```apex\npublic class HybridSearchService {\n    public static List<Map<String, Object>> hybridSearch(String query, Map<String, Object> filters) {\n        HttpRequest req = new HttpRequest();\n        req.setEndpoint('callout:DataCloud/v1/search/hybrid');\n        req.setMethod('POST');\n        req.setHeader('Content-Type', 'application/json');\n\n        Map<String, Object> payload = new Map<String, Object>{\n            'query' => query,\n            'semantic' => new Map<String, Object>{\n                'enabled' => true,\n                'weight' => 0.6  // 60% semantic\n            },\n            'keyword' => new Map<String, Object>{\n                'enabled' => true,\n                'weight' => 0.4  // 40% keyword\n            },\n            'filters' => filters,\n            'topK' => 20\n        };\n\n        req.setBody(JSON.serialize(payload));\n\n        Http http = new Http();\n        HttpResponse res = http.send(req);\n\n        if (res.getStatusCode() == 200) {\n            Map<String, Object> response = (Map<String, Object>)JSON.deserializeUntyped(res.getBody());\n            return (List<Map<String, Object>>)response.get('results');\n        }\n\n        return new List<Map<String, Object>>();\n    }\n}\n```\n\n### Multi-Language Semantic Search\n\nVector database supports cross-language semantic search:\n\n**Example**:\n- Service agent types case subject in French: \"Problme de connexion\"\n- Semantic search finds similar cases in English:\n  - \"Login issues\"\n  - \"Connection problems\"\n  - \"Unable to access account\"\n- Returns relevant solutions regardless of language\n\n**Configuration**:\n```\nData Cloud  Vector Database  Settings\n- Enable multi-language support\n- Supported languages: 100+ languages via multilingual embeddings\n- Automatic language detection\n- Cross-language similarity matching\n```\n\n### Use Cases for Vector Database\n\n**1. Customer Service Knowledge Retrieval**\n```apex\n// Agentforce action: Find relevant knowledge articles\n@InvocableMethod(label='Find Relevant Articles')\npublic static List<String> findRelevantArticles(List<String> customerQueries) {\n    List<String> articles = new List<String>();\n\n    for (String query : customerQueries) {\n        // Semantic search finds conceptually similar articles\n        List<SearchResult> results = SemanticSearchService.semanticSearch(\n            new List<SearchRequest>{new SearchRequest(query, 5)}\n        );\n\n        if (!results.isEmpty()) {\n            articles.addAll(results[0].matches);\n        }\n    }\n\n    return articles;\n}\n```\n\n**2. Case Similarity Detection**\n```apex\n// Find similar past cases to suggest solutions\npublic class CaseSimilarityService {\n    public static List<Case> findSimilarCases(String caseDescription) {\n        // Semantic search in past cases\n        List<SearchResult> results = SemanticSearchService.semanticSearch(\n            new List<SearchRequest>{new SearchRequest(caseDescription, 10)}\n        );\n\n        // Extract case IDs from metadata\n        Set<Id> caseIds = new Set<Id>();\n        // ... extract IDs from results\n\n        return [SELECT Id, Subject, Description, Status, Resolution__c\n                FROM Case\n                WHERE Id IN :caseIds\n                AND Status = 'Closed'\n                ORDER BY ClosedDate DESC];\n    }\n}\n```\n\n**3. Lead Scoring from Unstructured Data**\n```apex\n// Analyze email content and web behavior for lead scoring\npublic class LeadScoringService {\n    public static Decimal scoreLeadFromContent(Id leadId) {\n        // Get all email interactions\n        List<EmailMessage> emails = [SELECT Id, TextBody\n                                      FROM EmailMessage\n                                      WHERE RelatedToId = :leadId];\n\n        Decimal score = 0;\n\n        // Semantic search for buying intent keywords\n        String allContent = '';\n        for (EmailMessage email : emails) {\n            allContent += email.TextBody + ' ';\n        }\n\n        // Check semantic similarity to high-intent phrases\n        List<String> intentPhrases = new List<String>{\n            'ready to purchase',\n            'need pricing quote',\n            'schedule demo',\n            'implementation timeline'\n        };\n\n        for (String phrase : intentPhrases) {\n            // Semantic similarity score\n            Decimal similarity = calculateSemanticSimilarity(allContent, phrase);\n            score += similarity * 10;\n        }\n\n        return score;\n    }\n}\n```\n\n## Data Cloud SQL (ANSI SQL Support)\n\nQuery Data Cloud using standard SQL:\n\n```sql\n-- Complex analytical query across multiple sources\nSELECT\n    c.customer_id,\n    c.name,\n    c.segment,\n    COUNT(DISTINCT o.order_id) as total_orders,\n    SUM(o.order_total) as revenue,\n    AVG(s.satisfaction_score) as avg_satisfaction,\n    MAX(o.order_date) as last_order_date\nFROM\n    unified_customer c\n    INNER JOIN unified_orders o ON c.customer_id = o.customer_id\n    LEFT JOIN support_interactions s ON c.customer_id = s.customer_id\nWHERE\n    o.order_date >= CURRENT_DATE - INTERVAL '90' DAY\nGROUP BY\n    c.customer_id, c.name, c.segment\nHAVING\n    COUNT(DISTINCT o.order_id) >= 3\nORDER BY\n    revenue DESC\nLIMIT 100\n```\n\n## Authentication Patterns\n\n### OAuth 2.0 JWT Bearer Flow (Server-to-Server)\n\n```python\n# External system  Data Cloud authentication\nimport jwt\nimport time\nimport requests\n\ndef get_data_cloud_access_token(client_id, private_key, username, instance_url):\n    \"\"\"Get access token for Data Cloud API\"\"\"\n\n    # Create JWT\n    payload = {\n        'iss': client_id,\n        'sub': username,\n        'aud': instance_url,\n        'exp': int(time.time()) + 180  # 3 minutes\n    }\n\n    encoded_jwt = jwt.encode(payload, private_key, algorithm='RS256')\n\n    # Exchange JWT for access token\n    token_url = f\"{instance_url}/services/oauth2/token\"\n    response = requests.post(token_url, data={\n        'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n        'assertion': encoded_jwt\n    })\n\n    return response.json()['access_token']\n```\n\n## Best Practices\n\n### Performance\n- **Use Zero Copy** for large datasets (>10M records)\n- **Batch imports** outside business hours\n- **Index frequently queried fields** in Data Cloud\n- **Limit real-time triggers** to critical events\n- **Cache unified profiles** when possible\n\n### Security\n- **Field-level security** applies to Data Cloud queries from Salesforce\n- **Data masking** for PII in non-production environments\n- **Encryption at rest** and in transit (TLS 1.2+)\n- **Audit logging** for all data access\n- **Role-based access control** (RBAC) for Data Cloud users\n\n### Data Quality\n- **Data validation** before ingestion\n- **Deduplication rules** at source and in Data Cloud\n- **Data lineage tracking** (know source of each field)\n- **Quality scores** for unified profiles\n- **Regular data audits** and cleansing\n\n## Resources\n\n- **Data Cloud Documentation**: https://developer.salesforce.com/docs/data/data-cloud-int/guide\n- **Zero Copy Partner Network**: https://www.salesforce.com/data/zero-copy/\n- **Data Cloud Pricing**: Part of Customer 360 platform, usage-based pricing\n- **Trailhead**: \"Data Cloud Basics\" and \"Data Cloud for Developers\""
              },
              {
                "name": "flow-orchestrator-2025",
                "description": "Salesforce Flow Orchestrator multi-user automation best practices (2025)",
                "path": "plugins/salesforce-master/skills/flow-orchestrator-2025/SKILL.md",
                "frontmatter": {
                  "name": "flow-orchestrator-2025",
                  "description": "Salesforce Flow Orchestrator multi-user automation best practices (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Salesforce Flow Orchestrator (2025)\n\n## What is Flow Orchestrator?\n\nFlow Orchestrator enables you to orchestrate multi-user, multi-step, and multi-stage business processes without code. It allows different users to complete sequential tasks within a unified workflow, with built-in approvals, conditional logic, and error handling.\n\n**Key Capabilities**:\n- **Multi-User Workflows**: Assign tasks to different users/teams across stages\n- **Stage-Based Execution**: Organize work into logical stages\n- **Background Automation**: Combine user tasks with automated steps\n- **Visual Progress Tracking**: Users see their position in the workflow\n- **Fault Paths**: Handle errors gracefully (Summer '25)\n- **No-Code**: Build complex processes without Apex\n\n## When to Use Flow Orchestrator\n\n| Use Case | Flow Orchestrator? | Why |\n|----------|-------------------|-----|\n| Employee Onboarding (HR  IT  Manager) |  Yes | Multi-user, sequential stages |\n| Quote-to-Cash (Sales  Finance  Operations) |  Yes | Cross-functional approval process |\n| Case Escalation (L1  L2  L3 Support) |  Yes | Tiered assignment with SLAs |\n| Simple record automation (create/update) |  No | Use Record-Triggered Flow |\n| Single-user process |  No | Use Screen Flow |\n| Batch data processing |  No | Use Scheduled Flow or Apex Batch |\n\n## Orchestration Architecture\n\n```\nOrchestration = Stages  Steps  Background Automations\n\nStage 1: \"HR Review\"\n Step 1.1: Interactive Step (HR Manager reviews)\n Step 1.2: Background Automation (create records)\n Decision: Approved?  Next Stage : End\n\nStage 2: \"IT Provisioning\"\n Step 2.1: Interactive Step (IT assigns equipment)\n Step 2.2: Background Automation (provision accounts)\n Step 2.3: Interactive Step (IT confirms completion)\n\nStage 3: \"Manager Onboarding\"\n Step 3.1: Interactive Step (Manager schedules 1:1)\n Step 3.2: Background Automation (send welcome email)\n```\n\n## Building an Orchestration (Step-by-Step)\n\n### Example: Employee Onboarding Process\n\n**Requirements**:\n- HR reviews new hire documents  Approved/Rejected\n- If approved, IT provisions accounts and equipment\n- Manager schedules first day and assigns mentor\n- System sends notifications at each stage\n\n### Step 1: Create Orchestration\n\n```\nSetup  Flows  New Flow  Orchestration\nName: Employee_Onboarding\nObject: Employee__c (custom object)\nTrigger: Record Created, Status = 'Pending Onboarding'\n```\n\n### Step 2: Design Stages\n\n**Stage 1: HR Document Review**\n```\nStage Name: HR_Document_Review\nStage Description: HR verifies employee documentation\nRun Mode: One at a Time (sequential)\n\nStep 1.1 (Interactive):\n- Name: Review_Documents\n- Assigned To: Queue \"HR_Onboarding_Queue\"\n- Due Date: 2 days from start\n- Screen Flow: HR_Document_Review_Screen\n- Inputs: Employee__c.Id\n\nStep 1.2 (Background - Decision):\n- If HR_Approved = true  Next Stage\n- If HR_Approved = false  End + Send Rejection Email\n```\n\n**Stage 2: IT Provisioning**\n```\nStage Name: IT_Provisioning\nCondition: Runs only if Stage 1 approved\n\nStep 2.1 (Interactive):\n- Name: Assign_Equipment\n- Assigned To: Queue \"IT_Provisioning_Queue\"\n- Due Date: 3 days from stage start\n- Screen Flow: IT_Equipment_Assignment\n- Inputs: Employee__c.Id\n\nStep 2.2 (Background):\n- Name: Create_AD_Account\n- Autolaunched Flow: Create_Active_Directory_Account\n- Inputs: Employee__c.Email, Employee__c.FirstName\n\nStep 2.3 (Background):\n- Name: Send_IT_Confirmation\n- Action: Send Email Template\n- Recipient: Employee__c.Email\n- Template: Welcome_Email\n```\n\n**Stage 3: Manager Setup**\n```\nStage Name: Manager_Setup\nDepends On: Stage 2 complete\n\nStep 3.1 (Interactive):\n- Name: Schedule_First_Day\n- Assigned To: Employee__c.Manager__c\n- Due Date: 1 day from stage start\n- Screen Flow: Manager_Onboarding_Tasks\n\nStep 3.2 (Background):\n- Name: Update_Status\n- Record Update: Employee__c.Status = 'Onboarding Complete'\n```\n\n### Step 3: Implement Fault Paths (Summer '25)\n\n**Fault Path on IT Provisioning Failure**:\n```\nIf Step 2.2 (Create_AD_Account) fails:\n Retry Step (1 attempt after 10 minutes)\n If still fails:\n   Send email to IT Manager with error details\n   Create Task for manual provisioning\n   Assign Interactive Step to IT Manager for resolution\n Continue to Stage 3 (don't block entire process)\n```\n\n**Configuration**:\n```\nStep 2.2: Create_AD_Account\n Fault Path Enabled: true\n Retry Attempts: 1\n Retry Delay: 10 minutes\n On Final Failure:\n    Create Task\n      Subject: \"Manual AD Account Creation Needed\"\n      Assigned To: IT_Manager_Queue\n      Priority: High\n    Send Email Notification\n       Template: IT_Provisioning_Failure\n       Recipients: IT Managers\n```\n\n## Interactive Steps vs Background Steps\n\n### Interactive Steps\n\n**Use for**: Actions requiring human judgment or input\n\n```\nInteractive Step Configuration:\n Screen Flow: Define UI for user input\n Assigned To: User, Queue, or Role\n Due Date: Formula (TODAY() + 2 for 2 days)\n Instructions: What user should do\n Input Variables: Data passed to screen flow\n Output Variables: Data returned from user\n```\n\n**Example Screen Flow** (HR Review):\n```\nScreen: Review Documents\n Display: Employee Name, Position, Documents Uploaded\n Input: Radio Button (Approve / Reject)\n Input: Text Area (Comments - required if reject)\n Action: Save & Submit\n\nOutput Variables:\n- HR_Approved (Boolean)\n- HR_Comments (Text)\n```\n\n### Background Steps\n\n**Use for**: Automated actions without user interaction\n\n```\nBackground Step Types:\n Autolaunched Flow: Call another flow\n Apex Action: Invoke Apex method\n Send Email: Email template or custom\n Post to Chatter: Notify users\n Create Records: DML operations\n Update Records: Field updates\n External Service: REST callout\n Wait: Pause for duration or until condition\n```\n\n## Advanced Patterns\n\n### Pattern 1: Conditional Stage Execution\n\n**Use Case**: Skip stages based on criteria\n\n```\nStage 2: Manager Approval\nCondition: Order_Total__c > 10000\n\nEntry Criteria Formula:\n{!$Record.Order_Total__c} > 10000\n\nResult:\n- If order <= $10,000  Skip Stage 2, go to Stage 3\n- If order > $10,000  Execute Stage 2 (manager approval required)\n```\n\n### Pattern 2: Parallel Steps Within Stage\n\n**Use Case**: Multiple teams work simultaneously\n\n```\nStage 3: Parallel Provisioning\nRun Mode: All at Once (parallel)\n\nStep 3.1 (Interactive): IT assigns laptop [Assigned to IT Queue]\nStep 3.2 (Interactive): Facilities assigns desk [Assigned to Facilities Queue]\nStep 3.3 (Interactive): HR orders business cards [Assigned to HR Queue]\n\nStage completes when: All steps complete\n```\n\n### Pattern 3: Dynamic Assignment\n\n**Use Case**: Assign to different users based on record data\n\n```\nStep Assignment Formula:\nIF(\n  {!$Record.Region__c} = 'West',\n  {!$User.WestCoastManager},\n  IF(\n    {!$Record.Region__c} = 'East',\n    {!$User.EastCoastManager},\n    {!$User.DefaultManager}\n  )\n)\n```\n\n### Pattern 4: SLA Monitoring\n\n**Use Case**: Escalate if step not completed on time\n\n```\nStep Due Date: {!$Flow.CurrentDate} + 2 (2 days)\n\nScheduled Flow: Check_Overdue_Steps\n- Schedule: Daily at 8 AM\n- Query: OrchestrationWorkItem where DueDate < TODAY AND Status = 'In Progress'\n- Action: Send escalation email to manager\n```\n\n**Monitor with SOQL**:\n```apex\n// Query overdue orchestration steps\nList<FlowOrchestrationWorkItem> overdueItems = [\n    SELECT Id, Label, StepDefinitionName, AssignedToId,\n           RelatedRecordId, DueDate, Status\n    FROM FlowOrchestrationWorkItem\n    WHERE DueDate < TODAY\n      AND Status = 'InProgress'\n    ORDER BY DueDate ASC\n];\n\n// Send escalation notifications\nfor (FlowOrchestrationWorkItem item : overdueItems) {\n    sendEscalationEmail(item);\n}\n```\n\n## Monitoring and Reporting\n\n### FlowOrchestrationWorkItem Object\n\n**Query work items for reporting**:\n```apex\n// Active orchestrations\nList<FlowOrchestrationWorkItem> activeItems = [\n    SELECT Id, Label, StepDefinitionName, AssignedToId,\n           CreatedDate, LastModifiedDate, DueDate,\n           Status, RelatedRecordId\n    FROM FlowOrchestrationWorkItem\n    WHERE Status = 'InProgress'\n    ORDER BY DueDate ASC\n];\n\n// Calculate time spent in each step\nfor (FlowOrchestrationWorkItem item : activeItems) {\n    Long milliseconds = item.LastModifiedDate.getTime() - item.CreatedDate.getTime();\n    Decimal hours = milliseconds / (1000.0 * 60 * 60);\n    System.debug('Step: ' + item.Label + ', Time: ' + hours + ' hours');\n}\n```\n\n### Dashboard Metrics\n\n**Key Metrics to Track**:\n```\n1. Average Time per Stage\n   SELECT StepDefinitionName,\n          AVG(LastModifiedDate - CreatedDate) as AvgDuration\n   FROM FlowOrchestrationWorkItem\n   WHERE Status = 'Completed'\n   GROUP BY StepDefinitionName\n\n2. Completion Rate by Assignee\n   SELECT AssignedToId,\n          COUNT(CASE WHEN Status = 'Completed' THEN 1 END) as Completed,\n          COUNT(CASE WHEN DueDate < TODAY AND Status = 'InProgress' THEN 1 END) as Overdue\n   FROM FlowOrchestrationWorkItem\n   GROUP BY AssignedToId\n\n3. Bottleneck Identification\n   - Which steps take longest?\n   - Which steps have highest rejection/failure rate?\n   - Which assignees have most overdue items?\n```\n\n### Custom Dashboard Component\n\n```apex\npublic class OrchestrationMetrics {\n    @AuraEnabled(cacheable=true)\n    public static Map<String, Object> getMetrics() {\n        Map<String, Object> metrics = new Map<String, Object>();\n\n        // Total active orchestrations\n        Integer active = [SELECT COUNT() FROM FlowOrchestrationWorkItem WHERE Status = 'InProgress'];\n        metrics.put('active', active);\n\n        // Overdue count\n        Integer overdue = [SELECT COUNT() FROM FlowOrchestrationWorkItem\n                          WHERE Status = 'InProgress' AND DueDate < TODAY];\n        metrics.put('overdue', overdue);\n\n        // Average completion time (last 30 days)\n        AggregateResult[] avgTime = [\n            SELECT AVG(LastModifiedDate - CreatedDate) avgDuration\n            FROM FlowOrchestrationWorkItem\n            WHERE Status = 'Completed'\n              AND CreatedDate = LAST_N_DAYS:30\n        ];\n        metrics.put('avgCompletionHours', (Decimal)avgTime[0].get('avgDuration') / (1000.0 * 60 * 60));\n\n        return metrics;\n    }\n}\n```\n\n## Integration with Other Features\n\n### Pattern: Orchestration + Approval Process\n\n```\nStage 2: Manager Approval\n Step 2.1 (Interactive): Manager reviews\n   Screen Flow with Approve/Reject buttons\n Background Automation: Update approval status\n If Approved: Proceed to Stage 3\n   If Rejected: Send rejection email, End orchestration\n```\n\n### Pattern: Orchestration + Platform Events\n\n**Publish events at stage transitions**:\n```apex\ntrigger OrchestrationStageTrigger on FlowOrchestrationStageInstance (after insert, after update) {\n    List<OrchestrationStageEvent__e> events = new List<OrchestrationStageEvent__e>();\n\n    for (FlowOrchestrationStageInstance stage : Trigger.new) {\n        if (stage.Status == 'Completed') {\n            events.add(new OrchestrationStageEvent__e(\n                OrchestrationId__c = stage.OrchestrationInstanceId,\n                StageName__c = stage.StepDefinitionName,\n                CompletedDate__c = System.now()\n            ));\n        }\n    }\n\n    if (!events.isEmpty()) {\n        EventBus.publish(events);\n    }\n}\n```\n\n**Subscribe externally**:\n```javascript\n// External system tracks orchestration progress\nclient.subscribe('/event/OrchestrationStageEvent__e', (message) => {\n    const { OrchestrationId__c, StageName__c } = message.data.payload;\n    console.log(`Stage ${StageName__c} completed for ${OrchestrationId__c}`);\n\n    // Update external dashboard\n    updateOrchestrationStatus(OrchestrationId__c, StageName__c);\n});\n```\n\n### Pattern: Orchestration + Agentforce\n\n**AI agent handles certain steps**:\n```\nStage 2: Document Verification\n Step 2.1 (Background): AI agent verifies documents\n   Agentforce Action: Verify_Document_Compliance\n     - Uses Einstein OCR to extract text\n     - Uses LLM to validate against compliance rules\n     - Returns: Compliant (true/false) + Confidence score\n Decision: If confidence < 90%  Human review\n Step 2.2 (Interactive - Conditional): Human verifies (if AI uncertain)\n```\n\n## Best Practices\n\n### Design\n- **Plan stages before building**: Sketch workflow on paper/whiteboard\n- **One business process per orchestration**: Don't combine unrelated processes\n- **Meaningful stage/step names**: Use business terminology, not technical jargon\n- **Clear instructions**: Tell users exactly what to do in each step\n- **Appropriate due dates**: Balance urgency with realistic timelines\n\n### Performance\n- **Minimize steps per stage**: <10 steps per stage for maintainability\n- **Avoid unnecessary waits**: Only pause when truly needed\n- **Bulkify background automations**: Process multiple records efficiently\n- **Use decision logic wisely**: Skip unnecessary stages with conditions\n- **Monitor active orchestrations**: Archive completed ones regularly\n\n### Error Handling\n- **Always implement fault paths** (Summer '25): Don't let failures block entire process\n- **Retry transient errors**: Network issues, temporary API failures\n- **Escalate permanent errors**: Create tasks for manual intervention\n- **Log failures**: Track what went wrong for troubleshooting\n- **Test error scenarios**: Intentionally trigger failures in sandbox\n\n### Security\n- **Respect sharing rules**: Use \"with sharing\" in Apex called by orchestrations\n- **Field-level security**: Ensure users have access to fields in screen flows\n- **Queue membership**: Verify users in queues have necessary permissions\n- **Sensitive data**: Mask or encrypt PII in screen flows and work items\n\n### User Experience\n- **Mobile-friendly screens**: Many users work on mobile devices\n- **Progress indicators**: Show users where they are in process\n- **Clear next steps**: Always tell users what happens after they complete a step\n- **Timely notifications**: Send reminders before due dates\n- **Feedback on submission**: Confirm action was successful\n\n## Troubleshooting\n\n### Common Issues\n\n**Issue 1: Work items not appearing for users**\n```\nCauses:\n- User not in assigned queue\n- User lacks permission to object\n- Filter criteria on view excludes item\n\nSolution:\n1. Check queue membership: Setup  Queues\n2. Verify object permissions: User profile/permission set\n3. Review work item list view filters\n```\n\n**Issue 2: Background step failing silently**\n```\nCauses:\n- Apex error in called flow/action\n- Required field missing\n- Governor limit exceeded\n\nSolution:\n1. Enable debug logs: Setup  Debug Logs\n2. Check Flow error emails: Setup  Process Automation Settings\n3. Implement fault path to catch and handle errors\n```\n\n**Issue 3: Orchestration not triggering**\n```\nCauses:\n- Trigger criteria not met\n- Record not updated properly\n- Orchestration inactive\n\nSolution:\n1. Verify record meets entry criteria\n2. Check orchestration activation status\n3. Review audit trail for record updates\n```\n\n### Debug with Apex\n\n```apex\n// Query orchestration details for debugging\npublic class OrchestrationDebugger {\n    public static void debugOrchestration(Id recordId) {\n        // Find orchestration instances for record\n        List<FlowOrchestrationInstance> instances = [\n            SELECT Id, Label, Status, CreatedDate\n            FROM FlowOrchestrationInstance\n            WHERE RelatedRecordId = :recordId\n            ORDER BY CreatedDate DESC\n        ];\n\n        for (FlowOrchestrationInstance instance : instances) {\n            System.debug('Orchestration: ' + instance.Label);\n            System.debug('Status: ' + instance.Status);\n\n            // Get work items\n            List<FlowOrchestrationWorkItem> items = [\n                SELECT Id, Label, Status, AssignedToId, DueDate\n                FROM FlowOrchestrationWorkItem\n                WHERE OrchestrationInstanceId = :instance.Id\n                ORDER BY CreatedDate\n            ];\n\n            for (FlowOrchestrationWorkItem item : items) {\n                System.debug('  Step: ' + item.Label + ', Status: ' + item.Status);\n            }\n        }\n    }\n}\n```\n\n## Pricing and Availability\n\n- **Editions**: Enterprise, Performance, Unlimited, Developer\n- **Included Runs**: 600 orchestration runs/year (no charge)\n- **Additional Runs**: Purchase in blocks for high-volume use\n- **Permissions Required**:\n  - **Create/Edit**: Manage Flows permission\n  - **View**: View Orchestration in Automation App (Winter '26)\n  - **Approve Flows**: Approval Designer system permission (Winter '26)\n\n## Resources\n\n- **Flow Orchestrator Guide**: https://help.salesforce.com/s/articleView?id=platform.orchestrator_flow_orchestrator.htm\n- **Trailhead**: \"Flow Orchestration Basics\"\n- **Release Notes**: Summer '25 and Winter '26 for latest features\n- **FlowOrchestrationWorkItem Reference**: Salesforce Object Reference documentation\n\n## Migration from Process Builder\n\n**Process Builder  Flow Orchestrator**:\n```\nProcess Builder supports only simple automation\nFlow Orchestrator adds:\n- Multi-user coordination\n- Interactive steps\n- Stage-based organization\n- Visual progress tracking\n- Fault handling\n\nWhen to migrate:\n- Process involves multiple users\n- Need stage-based workflow\n- Want user interface for steps\n- Require better error handling\n```\n\nFlow Orchestrator transforms complex, cross-functional business processes into visual, manageable workflows that scale across your organization."
              },
              {
                "name": "hyperforce-2025",
                "description": "Salesforce Hyperforce public cloud infrastructure and architecture (2025)",
                "path": "plugins/salesforce-master/skills/hyperforce-2025/SKILL.md",
                "frontmatter": {
                  "name": "hyperforce-2025",
                  "description": "Salesforce Hyperforce public cloud infrastructure and architecture (2025)"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Salesforce Hyperforce Architecture (2025)\n\n## What is Hyperforce?\n\nHyperforce is Salesforce's next-generation infrastructure architecture built on public cloud platforms (AWS, Azure, Google Cloud). It represents a complete re-architecture of Salesforce from data center-based infrastructure to cloud-native, containerized microservices.\n\n**Key Innovation**: Infrastructure as code that can be deployed anywhere, giving customers choice, control, and data residency compliance.\n\n## Five Architectural Principles\n\n### 1. Immutable Infrastructure\n\n**Traditional**: Patch and update existing servers\n**Hyperforce**: Destroy and recreate servers with each deployment\n\n```\nOld Architecture:\nServer  Patch  Patch  Patch  Configuration Drift\n\nHyperforce:\nContainer Image v1  Deploy\nNew Code  Build Container Image v2  Replace v1 with v2\nResult: Every deployment is identical, reproducible\n```\n\n**Benefits**:\n- No configuration drift\n- Consistent environments (dev = prod)\n- Fast rollback (redeploy previous image)\n- Security patches applied immediately\n\n### 2. Multi-Availability Zone Design\n\n**Architecture**:\n```\nRegion: US-East (Virginia)\n Availability Zone A (Data Center 1)\n   App Servers (Kubernetes pods)\n   Database Primary\n   Load Balancer\n Availability Zone B (Data Center 2)\n   App Servers (Kubernetes pods)\n   Database Replica\n   Load Balancer\n Availability Zone C (Data Center 3)\n    App Servers (Kubernetes pods)\n    Database Replica\n    Load Balancer\n\nTraffic Distribution: Round-robin across all AZs\nFailure Handling: If AZ fails, traffic routes to remaining AZs\nRTO (Recovery Time Objective): <5 minutes\nRPO (Recovery Point Objective): <30 seconds\n```\n\n**Impact on Developers**:\n- Higher availability (99.95%+ SLA)\n- Transparent failover (no code changes)\n- Regional data residency guaranteed\n\n### 3. Zero Trust Security\n\n**Traditional**: Perimeter security (firewall protects everything inside)\n**Hyperforce**: No implicit trust - verify everything, always\n\n```\nZero Trust Model:\n Identity Verification (MFA required for all users by 2025)\n Device Trust (managed devices only)\n Network Segmentation (micro-segmentation between services)\n Least Privilege Access (minimal permissions by default)\n Continuous Monitoring (real-time threat detection)\n Encryption Everywhere (TLS 1.3, data at rest encryption)\n```\n\n**Code Impact**:\n```apex\n// OLD: Assume internal traffic is safe\npublic without sharing class InternalService {\n    // No auth checks - trusted network\n}\n\n// HYPERFORCE: Always verify, never trust\npublic with sharing class InternalService {\n    // Always enforce sharing rules\n    // Always validate session\n    // Always check field-level security\n\n    public List<Account> getAccounts() {\n        // WITH SECURITY_ENFORCED prevents data leaks\n        return [SELECT Id, Name FROM Account WITH SECURITY_ENFORCED];\n    }\n}\n```\n\n**2025 Requirements**:\n- **MFA Mandatory**: All users must enable MFA\n- **Session Security**: Shorter session timeouts, IP restrictions\n- **API Security**: JWT with short expiration (15 minutes)\n\n### 4. Infrastructure as Code (IaC)\n\n**Everything defined as code, version-controlled**:\n\n```yaml\n# Hyperforce deployment manifest (conceptual)\napiVersion: hyperforce.salesforce.com/v1\nkind: SalesforceOrg\nmetadata:\n  name: production-org\n  region: aws-us-east-1\nspec:\n  edition: enterprise\n  features:\n    - agentforce\n    - dataCloud\n    - einstein\n  compute:\n    pods: 50\n    autoScaling:\n      min: 10\n      max: 100\n      targetCPU: 70%\n  storage:\n    size: 500GB\n    replication: 3\n  backup:\n    frequency: hourly\n    retention: 30days\n  networking:\n    privateLink: enabled\n    ipWhitelist:\n      - 203.0.113.0/24\n```\n\n**Benefits for Developers**:\n- **Reproducible**: Recreate exact environment anytime\n- **Version Controlled**: Track all infrastructure changes in Git\n- **Testable**: Validate infrastructure before deployment\n- **Automated**: No manual configuration, eliminates human error\n\n### 5. Clean Slate (No Legacy Constraints)\n\n**Hyperforce rebuilt from scratch**:\n- Modern Kubernetes orchestration\n- Cloud-native services (managed databases, object storage)\n- API-first design (everything accessible via API)\n- Microservices architecture (independent scaling)\n- No legacy code or technical debt\n\n## Public Cloud Integration\n\n### AWS Hyperforce Architecture\n\n```\n\n                  AWS Region (us-east-1)                \n\n  VPC (Virtual Private Cloud)                           \n   Public Subnets (3 AZs)                             \n     Application Load Balancer (ALB)                 \n   Private Subnets (3 AZs)                            \n     EKS Cluster (Kubernetes)                        \n       Salesforce App Pods (autoscaling)            \n       Metadata Service Pods                        \n       API Gateway Pods                             \n       Background Job Pods (Batch, Scheduled)       \n     RDS Aurora PostgreSQL (multi-AZ)                \n     ElastiCache Redis (session storage)             \n     S3 Buckets (attachments, documents)             \n   Database Subnets (3 AZs)                           \n      Aurora Database Cluster                         \n\n  Additional Services                                   \n   CloudWatch (monitoring, logs)                      \n   CloudTrail (audit logs)                            \n   AWS Shield (DDoS protection)                       \n   AWS WAF (web application firewall)                 \n   KMS (encryption key management)                    \n   PrivateLink (secure connectivity)                  \n\n```\n\n**AWS Services Used**:\n- **Compute**: EKS (Elastic Kubernetes Service)\n- **Database**: Aurora PostgreSQL (multi-master)\n- **Storage**: S3 (object storage), EBS (block storage)\n- **Networking**: VPC, ALB, Route 53, CloudFront CDN\n- **Security**: IAM, KMS, Shield, WAF, Certificate Manager\n\n### Azure Hyperforce Architecture\n\n```\nAzure Region (East US)\n Virtual Network (VNet)\n   AKS (Azure Kubernetes Service)\n     Salesforce workloads\n   Azure Database for PostgreSQL (Hyperscale)\n   Azure Cache for Redis\n   Azure Blob Storage\n Azure Front Door (CDN + Load Balancer)\n Azure Monitor (logging, metrics)\n Azure Active Directory (identity)\n Azure Key Vault (secrets, encryption)\n```\n\n### Google Cloud Hyperforce Architecture\n\n```\nGCP Region (us-central1)\n VPC Network\n   GKE (Google Kubernetes Engine)\n   Cloud SQL (PostgreSQL)\n   Memorystore (Redis)\n   Cloud Storage (GCS)\n Cloud Load Balancing\n Cloud Armor (DDoS protection)\n Cloud Monitoring (Stackdriver)\n Cloud KMS (encryption)\n```\n\n## Data Residency and Compliance\n\n### Geographic Regions (2025)\n\n**Available Hyperforce Regions**:\n```\nAmericas:\n US East (Virginia) - AWS, Azure\n US West (Oregon) - AWS\n US Central (Iowa) - GCP\n Canada (Toronto) - AWS\n Brazil (So Paulo) - AWS\n\nEurope:\n UK (London) - AWS\n Germany (Frankfurt) - AWS, Azure\n France (Paris) - AWS\n Ireland (Dublin) - AWS\n Switzerland (Zurich) - AWS\n\nAsia Pacific:\n Japan (Tokyo) - AWS\n Australia (Sydney) - AWS\n Singapore - AWS\n India (Mumbai) - AWS\n South Korea (Seoul) - AWS\n\nMiddle East:\n UAE (Dubai) - AWS\n```\n\n### Data Residency Guarantees\n\n**What stays in region**:\n- All customer data (records, attachments, metadata)\n- Database backups\n- Transaction logs\n- Audit logs\n\n**What may leave region**:\n- Telemetry data (anonymized performance metrics)\n- Security threat intelligence\n- Platform health monitoring\n\n**Code Implication**:\n```apex\n// Data residency automatically enforced\n// No code changes needed - Hyperforce handles it\n\n// Example: File stored in org's region\nContentVersion cv = new ContentVersion(\n    Title = 'Customer Contract',\n    PathOnClient = 'contract.pdf',\n    VersionData = Blob.valueOf('contract data')\n);\ninsert cv;\n\n// File automatically stored in:\n// - AWS S3 in org's region\n// - Encrypted at rest (AES-256)\n// - Replicated across 3 AZs in region\n// - Never leaves region boundary\n```\n\n### Compliance Certifications\n\n**Hyperforce maintains**:\n- **SOC 2 Type II**: Security, availability, confidentiality\n- **ISO 27001**: Information security management\n- **GDPR**: EU data protection compliance\n- **HIPAA**: Healthcare data protection (BAA available)\n- **PCI DSS**: Payment card data security\n- **FedRAMP**: US government cloud security (select regions)\n\n## Performance Improvements\n\n### Latency Reduction\n\n**Old Architecture** (data center-based):\n```\nUser (Germany)  Transatlantic cable  US Data Center  Response\nLatency: 150-200ms\n```\n\n**Hyperforce**:\n```\nUser (Germany)  Frankfurt Hyperforce Region  Response\nLatency: 10-30ms\n\nResult: 5-10x faster for regional users\n```\n\n### Auto-Scaling\n\n**Traditional**: Fixed capacity, must provision for peak load\n**Hyperforce**: Dynamic scaling based on demand\n\n```\nBusiness Hours (9 AM - 5 PM):\n High user load\n Kubernetes scales up pods: 50  150\n Response times maintained\n\nOff Hours (6 PM - 8 AM):\n Low user load\n Kubernetes scales down pods: 150  30\n Cost savings (pay for what you use)\n\nBlack Friday (peak event):\n Extreme load\n Kubernetes scales to maximum: 30  500 pods in minutes\n No downtime, no performance degradation\n```\n\n**Governor Limits - No Change**:\n```apex\n// Hyperforce does NOT change governor limits\n// Limits remain the same as classic Salesforce:\n// - 100 SOQL queries per transaction\n// - 150 DML statements\n// - 6 MB heap size (sync), 12 MB (async)\n\n// But: Infrastructure scales to handle more concurrent users\n```\n\n## Migration to Hyperforce\n\n### Migration Process\n\n**Salesforce handles migration** (no customer action required):\n\n```\nPhase 1: Assessment (Salesforce internal)\n Analyze org size, customizations\n Identify any incompatible features\n Plan migration window\n\nPhase 2: Pre-Migration (Customer notified)\n Salesforce sends notification (90 days notice)\n Customer tests in sandbox (migrated first)\n Customer validates functionality\n\nPhase 3: Migration (Weekend maintenance window)\n Backup all data\n Replicate data to Hyperforce\n Cutover DNS (redirect traffic)\n Validate migration success\n\nPhase 4: Post-Migration\n Monitor performance\n Support customer issues\n Decommission old infrastructure\n\nDowntime: Typically <2 hours\n```\n\n### What Changes for Developers?\n\n**No Code Changes Required**:\n```apex\n// Your Apex code works identically on Hyperforce\npublic class MyController {\n    public List<Account> getAccounts() {\n        return [SELECT Id, Name FROM Account LIMIT 10];\n    }\n}\n\n// No changes needed\n// Same APIs, same limits, same behavior\n```\n\n**Potential Performance Improvements**:\n- Faster API responses (lower latency)\n- Better handling of concurrent users\n- Improved batch job processing (parallel execution)\n\n**Backward Compatibility**: 100% compatible with existing code\n\n### Testing Pre-Migration\n\n**Use Sandbox Migration**:\n```\n1. Salesforce migrates your sandbox first\n2. Test all critical functionality:\n    Custom Apex classes\n    Triggers and workflows\n    Integrations (API callouts)\n    Lightning components\n    Reports and dashboards\n\n3. Validate performance:\n    Run load tests\n    Check API response times\n    Verify batch jobs complete\n\n4. Report any issues to Salesforce\n5. Production migration scheduled after sandbox validated\n```\n\n## Hyperforce for Developers\n\n### Enhanced APIs\n\n**Hyperforce exposes infrastructure APIs**:\n\n```apex\n// Query org's Hyperforce region (API 62.0+)\nOrganization org = [SELECT Id, InstanceName, InfrastructureRegion__c FROM Organization LIMIT 1];\nSystem.debug('Region: ' + org.InfrastructureRegion__c); // 'aws-us-east-1'\n\n// Check if org is on Hyperforce\nSystem.debug('Is Hyperforce: ' + org.IsHyperforce__c); // true\n```\n\n### Private Connectivity\n\n**AWS PrivateLink / Azure Private Link**:\n```\nTraditional: Salesforce API  Public Internet  Your API\nSecurity: TLS encryption, but still public internet\n\nHyperforce PrivateLink: Salesforce API  Private Network  Your API\nSecurity: Never touches public internet, lower latency\n\nSetup:\n1. Create VPC Endpoint (AWS) or Private Endpoint (Azure)\n2. Salesforce provides service endpoint name\n3. Configure Named Credential in Salesforce with private endpoint\n4. API calls route over private network\n```\n\n**Configuration**:\n```apex\n// Named Credential uses PrivateLink endpoint\n// Setup  Named Credentials  External API (PrivateLink)\n// URL: https://api.internal.example.com (private endpoint)\n\n// Apex callout\nHttpRequest req = new HttpRequest();\nreq.setEndpoint('callout:ExternalAPIPrivateLink/data');\nreq.setMethod('GET');\n\nHttp http = new Http();\nHttpResponse res = http.send(req);\n\n// Callout never leaves private network\n// Lower latency, higher security\n```\n\n### Monitoring\n\n**CloudWatch / Azure Monitor Integration**:\n```\nSalesforce publishes metrics to your cloud account:\n API request volume\n API response times\n Error rates\n Governor limit usage\n Batch job completion times\n\nBenefits:\n- Unified monitoring (Salesforce + your apps)\n- Custom alerting (CloudWatch Alarms)\n- Cost attribution (AWS Cost Explorer)\n```\n\n## Best Practices for Hyperforce\n\n### Security\n- **Enable MFA**: Required for all users in 2025\n- **Use WITH SECURITY_ENFORCED**: Field-level security in SOQL\n- **Implement IP whitelisting**: Restrict access to known IPs\n- **Monitor audit logs**: Setup  Event Monitoring\n- **Rotate credentials**: API keys, certificates, passwords regularly\n\n### Performance\n- **Leverage caching**: Platform Cache for frequently accessed data\n- **Optimize queries**: Use indexed fields, selective queries\n- **Async processing**: Use @future, Queueable for non-critical work\n- **Bulkification**: Always design for 200+ records\n- **Monitor limits**: Use Limits class to track governor limit usage\n\n### Data Residency\n- **Understand requirements**: Know your compliance obligations\n- **Choose correct region**: Select region meeting your needs\n- **Validate configurations**: Ensure integrations respect boundaries\n- **Document decisions**: Maintain records of data residency choices\n\n### Cost Optimization\n- **Right-size storage**: Archive old data, delete unnecessary records\n- **Optimize API calls**: Batch API calls, use composite APIs\n- **Schedule batch jobs efficiently**: Run during off-peak hours\n- **Monitor usage**: Track API calls, storage, compute usage\n\n## Resources\n\n- **Hyperforce Trust Site**: https://trust.salesforce.com/en/infrastructure/hyperforce/\n- **Hyperforce FAQ**: Salesforce Help documentation\n- **Available Regions**: https://help.salesforce.com/s/articleView?id=sf.getstart_domain_overview.htm\n- **Migration Guide**: Provided by Salesforce 90 days before migration\n- **Trust & Compliance**: https://compliance.salesforce.com/\n\n## Future Roadmap (2025+)\n\n**Expected Enhancements**:\n- More regions (Africa, additional Asia Pacific)\n- Bring Your Own Cloud (BYOC) - use your own AWS/Azure account\n- Multi-region active-active (write to multiple regions simultaneously)\n- Edge computing (Salesforce at CDN edge locations)\n- Kubernetes cluster API (direct pod management for enterprises)\n\nHyperforce represents Salesforce's commitment to modern, cloud-native infrastructure that scales globally while meeting the most stringent compliance and performance requirements."
              },
              {
                "name": "lightning-2025-features",
                "description": "Salesforce Lightning Web Components Winter '26 and 2025 features",
                "path": "plugins/salesforce-master/skills/lightning-2025-features/SKILL.md",
                "frontmatter": {
                  "name": "lightning-2025-features",
                  "description": "Salesforce Lightning Web Components Winter '26 and 2025 features"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Lightning Web Components 2025 Features\n\n## lightning/graphql Module (Winter '26)\n\nNew module replaces deprecated `lightning/uiGraphQLApi`:\n\n### Migration\n\n```javascript\n//  Old (deprecated)\nimport { gql, graphql } from 'lightning/uiGraphQLApi';\n\n//  New (Winter '26)\nimport { gql, graphql } from 'lightning/graphql';\n\nexport default class MyComponent extends LightningElement {\n  @wire(graphql, {\n    query: gql`\n      query getAccount($id: ID!) {\n        uiapi {\n          query {\n            Account(where: { Id: { eq: $id } }) {\n              edges {\n                node {\n                  Id\n                  Name\n                  Industry\n                }\n              }\n            }\n          }\n        }\n      }\n    `,\n    variables: '$variables'\n  })\n  results;\n\n  get variables() {\n    return { id: this.recordId };\n  }\n}\n```\n\n### Benefits\n\n- Improved performance\n- Better error handling\n- Enhanced type safety\n- Future-proof API\n\n## Local Development (sf lightning dev component)\n\nRun LWC components locally without deploying:\n\n```bash\n# Start local dev server\nsf lightning dev component\n\n# Opens browser at http://localhost:3333\n# Live reload on file changes\n# No deployment needed\n# Faster development cycle\n\n# Specify component\nsf lightning dev component -n myComponent\n\n# Specify target org\nsf lightning dev component -o myOrg@example.com\n```\n\n**Benefits:**\n- Instant feedback (no deployment wait)\n- Debug in real browser DevTools\n- Hot module replacement\n- Work offline\n\n## Platform Module Access in Component Preview\n\nAccess platform modules in single component preview:\n\n```javascript\n// Works in local preview now\nimport { getRecord } from 'lightning/uiRecordApi';\nimport { ShowToastEvent } from 'lightning/platformShowToastEvent';\nimport { NavigationMixin } from 'lightning/navigation';\n\n// Previously required full org deployment\n// Now works in sf lightning dev component\n```\n\n## Agentforce Targets (Winter '26)\n\nNew LWC targets for AI agents:\n\n```xml\n<!-- meta.xml -->\n<targets>\n  <!-- Input component for Agentforce -->\n  <target>lightning__AgentforceInput</target>\n\n  <!-- Output component for Agentforce -->\n  <target>lightning__AgentforceOutput</target>\n</targets>\n```\n\n```javascript\n// agentInputComponent.js\nimport { LightningElement, api } from 'lwc';\n\nexport default class AgentInputComponent extends LightningElement {\n  @api agentContext;  // Provided by Agentforce\n\n  handleSubmit() {\n    const userInput = this.template.querySelector('input').value;\n    // Send to Agentforce\n    this.dispatchEvent(new CustomEvent('agentinput', {\n      detail: { input: userInput }\n    }));\n  }\n}\n```\n\n## Lightning Out 2.0 (GA Winter '26)\n\nRe-imagined embedding with web components:\n\n### Traditional Lightning Out (Legacy)\n\n```html\n<script src=\"https://MyDomain.lightning.force.com/lightning/lightning.out.js\"></script>\n<script>\n  $Lightning.use(\"c:myApp\", function() {\n    $Lightning.createComponent(\"c:myComponent\",\n      { recordId: \"001...\" },\n      \"lightningContainer\",\n      function(cmp) { /* callback */ }\n    );\n  });\n</script>\n<div id=\"lightningContainer\"></div>\n```\n\n### Lightning Out 2.0 (Modern)\n\n```html\n<!-- Standards-based web components -->\n<script src=\"https://MyDomain.lightning.force.com/c/myComponent.js\" type=\"module\"></script>\n\n<!-- Use as web component -->\n<c-my-component record-id=\"001...\" ></c-my-component>\n\n<!-- No Aura dependency -->\n<!-- Powered by LWR (Lightning Web Runtime) -->\n<!-- Lighter and faster -->\n```\n\n**Benefits:**\n- 50-70% smaller bundle size\n- Faster load times\n- Standards-based (no proprietary framework)\n- Better browser compatibility\n- Simplified integration\n\n## SLDS 2.0 with Dark Mode (GA Winter '26)\n\nSalesforce Lightning Design System 2.0:\n\n```css\n/* Dark mode support in custom themes */\n:host([data-theme=\"dark\"]) {\n  --lwc-colorBackground: #16325c;\n  --lwc-colorTextPrimary: #ffffff;\n  --lwc-brandPrimary: #1589ee;\n}\n\n/* Light mode */\n:host([data-theme=\"light\"]) {\n  --lwc-colorBackground: #ffffff;\n  --lwc-colorTextPrimary: #181818;\n  --lwc-brandPrimary: #0176d3;\n}\n```\n\n```javascript\n// Toggle dark mode\nexport default class ThemeToggle extends LightningElement {\n  handleThemeChange(event) {\n    const theme = event.target.checked ? 'dark' : 'light';\n    this.template.host.setAttribute('data-theme', theme);\n  }\n}\n```\n\n### SLDS Linter with Fix Option\n\n```bash\n# Install SLDS linter\nnpm install -D @salesforce-ux/slds-linter\n\n# Lint with auto-fix\nnpx slds-linter --fix src/\n\n# CI/CD integration\nnpx slds-linter src/ --format json > slds-report.json\n```\n\n## Unified Testing APIs (Winter '26)\n\nTest Discovery and Test Runner APIs unify Apex and Flow testing:\n\n```apex\n// Discover all tests\nTest.DiscoveryResult discovery = Test.discoverTests();\n\n// Get Apex tests\nList<Test.ApexTestInfo> apexTests = discovery.getApexTests();\n\n// Get Flow tests\nList<Test.FlowTestInfo> flowTests = discovery.getFlowTests();\n\n// Run all tests from single location\nTest.RunResult result = Test.runTests(discovery);\n\n// Check results\nSystem.debug('Apex passed: ' + result.getApexTestsPassed());\nSystem.debug('Flow passed: ' + result.getFlowTestsPassed());\n```\n\n**Benefits:**\n- Unified test execution\n- Single test report\n- Simplified CI/CD\n- Better test orchestration\n\n## Lightning Web Security Enhancements\n\nNew security protections with API distortions:\n\n```javascript\n// Additional secure protections automatically applied\nexport default class SecureComponent extends LightningElement {\n  connectedCallback() {\n    // Web APIs now include security distortions\n    // ESLint rules validate distortion compliance\n\n    // Example: Secure window access\n    const secureWindow = window;  // LWS-secured reference\n  }\n}\n```\n\n### ESLint Rules for Security\n\n```json\n// .eslintrc.json\n{\n  \"extends\": [\"@salesforce/eslint-config-lwc/recommended\"],\n  \"rules\": {\n    \"@lwc/lwc/no-inner-html\": \"error\",\n    \"@lwc/lwc/no-document-query\": \"error\",\n    \"@salesforce/lwc-security/no-unsafe-references\": \"error\"\n  }\n}\n```\n\n## Practical Migration Examples\n\n### GraphQL Module Update\n\n```javascript\n// Before (Winter '25)\nimport { gql, graphql } from 'lightning/uiGraphQLApi';\n\n@wire(graphql, { query: gql`...` })\nresults;\n\n// After (Winter '26)\nimport { gql, graphql } from 'lightning/graphql';\n\n@wire(graphql, { query: gql`...` })\nresults;\n```\n\n### Local Development Workflow\n\n```bash\n# Old workflow (deploy every change)\nsf project deploy start\n# Wait 30-60 seconds\n# Test in org\n# Repeat...\n\n# New workflow (instant feedback)\nsf lightning dev component\n# Changes reflect immediately\n# No deployment\n# Test in local browser\n# Deploy only when ready\n```\n\n### Embedding with Lightning Out 2.0\n\n```html\n<!-- Old (Lightning Out 1.0) -->\n<script src=\"/lightning/lightning.out.js\"></script>\n<script>\n  $Lightning.use(\"c:app\", function() {\n    $Lightning.createComponent(\"c:comp\", {}, \"div\", callback);\n  });\n</script>\n\n<!-- New (Lightning Out 2.0) -->\n<script src=\"/c/comp.js\" type=\"module\"></script>\n<c-comp></c-comp>\n```\n\n## Resources\n\n- [Lightning Web Components Dev Guide](https://developer.salesforce.com/docs/platform/lwc/guide)\n- [Winter '26 Release Notes](https://help.salesforce.com/s/articleView?id=release-notes.salesforce_release_notes.htm)\n- [SLDS 2.0](https://www.lightningdesignsystem.com)\n- [Lightning Out 2.0](https://developer.salesforce.com/docs/platform/lwc/guide/use-lightning-out.html)"
              }
            ]
          },
          {
            "name": "azure-master",
            "description": "Complete Azure cloud expertise system with 2025 features including AKS Automatic, Container Apps GPU support, and Deployment Stacks. PROACTIVELY activate for: (1) ANY Azure resource provisioning or management, (2) AKS Automatic with Karpenter autoscaling, (3) Container Apps with serverless GPU and Dapr integration, (4) Azure OpenAI GPT-5 and reasoning models (o4-mini, o3), (5) Deployment Stacks for infrastructure lifecycle management, (6) Bicep v0.37+ with externalInput() and custom extensions, (7) Azure CLI 2.79+ with latest breaking changes, (8) SRE Agent integration for monitoring and incident response, (9) Azure AI Foundry model deployment, (10) Security, networking, and cost optimization. Provides: AKS Automatic GA features, Container Apps GPU workloads, Deployment Stacks best practices, latest Azure OpenAI models, Bicep 2025 patterns, Azure CLI expertise, comprehensive service configurations (compute, networking, storage, databases, AI/ML), Well-Architected Framework guidance, high availability patterns, security hardening, cost optimization strategies, and production-ready configurations. Ensures enterprise-ready, secure, scalable Azure infrastructure following Microsoft 2025 standards.",
            "source": "./plugins/azure-master",
            "category": null,
            "version": "1.1.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install azure-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "aks-automatic-2025",
                "description": "Azure Kubernetes Service Automatic mode GA 2025 features including Karpenter, auto-scaling, and zero operational overhead",
                "path": "plugins/azure-master/skills/aks-automatic-2025/SKILL.md",
                "frontmatter": {
                  "name": "aks-automatic-2025",
                  "description": "Azure Kubernetes Service Automatic mode GA 2025 features including Karpenter, auto-scaling, and zero operational overhead"
                },
                "content": "# AKS Automatic - 2025 GA Features\n\nComplete knowledge base for Azure Kubernetes Service Automatic mode (GA October 2025).\n\n## Overview\n\nAKS Automatic is a fully-managed Kubernetes offering that eliminates operational overhead through intelligent automation and built-in best practices.\n\n## Key Features (GA October 2025)\n\n### 1. Zero Operational Overhead\n- Fully-managed control plane and worker nodes\n- Automatic OS patching and security updates\n- Built-in monitoring and diagnostics\n- Integrated security and compliance\n\n### 2. Karpenter Integration\n- Dynamic node provisioning based on real-time demand\n- Intelligent bin-packing for cost optimization\n- Automatic node consolidation and deprovisioning\n- Support for multiple node pools and instance types\n\n### 3. Auto-Scaling (Enabled by Default)\n- **Horizontal Pod Autoscaler (HPA)**: Scale pods based on CPU/memory\n- **Vertical Pod Autoscaler (VPA)**: Adjust pod resource requests/limits\n- **KEDA**: Event-driven autoscaling for external triggers\n\n### 4. Enhanced Security\n- Microsoft Entra ID integration for authentication\n- Azure RBAC for Kubernetes authorization\n- Network policies enabled by default\n- Automatic security patches\n- Workload identity for pod-level authentication\n\n### 5. Advanced Networking\n- Azure CNI Overlay for efficient IP usage\n- Cilium dataplane for high-performance networking\n- Network policies for microsegmentation\n- Private clusters supported\n\n### 6. New Billing Model (Effective October 19, 2025)\n- Hosted control plane fee: **$0.16/cluster/hour**\n- Compute charges based on actual node usage\n- No separate cluster management fee\n- Cost savings from Karpenter optimization\n\n### 7. Node Operating System\n- Ubuntu 22.04 for Kubernetes < 1.34\n- Ubuntu 24.04 for Kubernetes >= 1.34\n- Automatic OS upgrades with node image channel\n\n## Creating AKS Automatic Cluster\n\n### Basic Creation\n\n```bash\naz aks create \\\n  --resource-group MyRG \\\n  --name MyAKSAutomatic \\\n  --sku automatic \\\n  --kubernetes-version 1.34 \\\n  --location eastus\n```\n\n### Production-Ready Configuration\n\n```bash\naz aks create \\\n  --resource-group MyRG \\\n  --name MyAKSAutomatic \\\n  --location eastus \\\n  --sku automatic \\\n  --tier standard \\\n  \\\n  # Kubernetes version\n  --kubernetes-version 1.34 \\\n  \\\n  # Karpenter (default in automatic mode)\n  --enable-karpenter \\\n  \\\n  # Networking\n  --network-plugin azure \\\n  --network-plugin-mode overlay \\\n  --network-dataplane cilium \\\n  --service-cidr 10.0.0.0/16 \\\n  --dns-service-ip 10.0.0.10 \\\n  --load-balancer-sku standard \\\n  \\\n  # Use custom VNet (optional)\n  --vnet-subnet-id /subscriptions/<sub-id>/resourceGroups/MyRG/providers/Microsoft.Network/virtualNetworks/MyVNet/subnets/AKSSubnet \\\n  \\\n  # Availability zones\n  --zones 1 2 3 \\\n  \\\n  # Authentication and authorization\n  --enable-managed-identity \\\n  --enable-aad \\\n  --enable-azure-rbac \\\n  --aad-admin-group-object-ids <group-object-id> \\\n  \\\n  # Auto-upgrade\n  --auto-upgrade-channel stable \\\n  --node-os-upgrade-channel NodeImage \\\n  \\\n  # Security\n  --enable-defender \\\n  --enable-workload-identity \\\n  --enable-oidc-issuer \\\n  \\\n  # Monitoring\n  --enable-addons monitoring \\\n  --workspace-resource-id /subscriptions/<sub-id>/resourceGroups/MyRG/providers/Microsoft.OperationalInsights/workspaces/MyWorkspace \\\n  \\\n  # Tags\n  --tags Environment=Production ManagedBy=AKSAutomatic\n```\n\n### With Azure Policy Add-on\n\n```bash\naz aks create \\\n  --resource-group MyRG \\\n  --name MyAKSAutomatic \\\n  --sku automatic \\\n  --enable-addons azure-policy \\\n  --kubernetes-version 1.34\n```\n\n## Karpenter Configuration\n\nAKS Automatic uses Karpenter for intelligent node provisioning. Customize node provisioning with AKSNodeClass and NodePool CRDs.\n\n### Default AKSNodeClass\n\n```yaml\napiVersion: karpenter.azure.com/v1alpha1\nkind: AKSNodeClass\nmetadata:\n  name: default\nspec:\n  # OS Image - Ubuntu 24.04 for K8s 1.34+\n  osImage:\n    sku: Ubuntu\n    version: \"24.04\"\n\n  # VM Series\n  vmSeries:\n    - Standard_D\n    - Standard_E\n\n  # Max pods per node\n  maxPodsPerNode: 110\n\n  # Security\n  securityProfile:\n    sshAccess: Disabled\n    securityType: Standard\n```\n\n### Custom NodePool\n\n```yaml\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: general-purpose\nspec:\n  # Constraints\n  template:\n    spec:\n      requirements:\n        - key: kubernetes.io/arch\n          operator: In\n          values: [\"amd64\"]\n        - key: karpenter.sh/capacity-type\n          operator: In\n          values: [\"on-demand\"]\n        - key: kubernetes.azure.com/agentpool\n          operator: In\n          values: [\"general\"]\n\n      # Node labels\n      labels:\n        workload-type: general\n\n      # Taints (optional)\n      taints:\n        - key: \"dedicated\"\n          value: \"general\"\n          effect: \"NoSchedule\"\n\n      # NodeClass reference\n      nodeClassRef:\n        group: karpenter.azure.com\n        kind: AKSNodeClass\n        name: default\n\n  # Limits\n  limits:\n    cpu: \"1000\"\n    memory: 4000Gi\n\n  # Disruption budget\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 30s\n    expireAfter: 720h # 30 days\n    budgets:\n      - nodes: \"10%\"\n        duration: 5m\n```\n\n### GPU NodePool for AI Workloads\n\n```yaml\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: gpu-workloads\nspec:\n  template:\n    spec:\n      requirements:\n        - key: kubernetes.io/arch\n          operator: In\n          values: [\"amd64\"]\n        - key: karpenter.sh/capacity-type\n          operator: In\n          values: [\"on-demand\"]\n        - key: node.kubernetes.io/instance-type\n          operator: In\n          values: [\"Standard_NC6s_v3\", \"Standard_NC12s_v3\", \"Standard_NC24s_v3\"]\n\n      labels:\n        workload-type: gpu\n        gpu-type: nvidia-v100\n\n      taints:\n        - key: \"nvidia.com/gpu\"\n          value: \"true\"\n          effect: \"NoSchedule\"\n\n      nodeClassRef:\n        group: karpenter.azure.com\n        kind: AKSNodeClass\n        name: gpu-nodeclass\n\n  limits:\n    cpu: \"200\"\n    memory: 800Gi\n    nvidia.com/gpu: \"16\"\n\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 300s\n```\n\n## Autoscaling with HPA, VPA, and KEDA\n\n### Horizontal Pod Autoscaler (HPA)\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 50\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n        - type: Percent\n          value: 100\n          periodSeconds: 15\n        - type: Pods\n          value: 4\n          periodSeconds: 15\n      selectPolicy: Max\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n        - type: Percent\n          value: 50\n          periodSeconds: 15\n```\n\n### Vertical Pod Autoscaler (VPA)\n\n```yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: myapp-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  updatePolicy:\n    updateMode: \"Auto\"  # Auto, Recreate, Initial, Off\n  resourcePolicy:\n    containerPolicies:\n      - containerName: \"*\"\n        minAllowed:\n          cpu: 100m\n          memory: 128Mi\n        maxAllowed:\n          cpu: 4\n          memory: 8Gi\n        controlledResources: [\"cpu\", \"memory\"]\n        controlledValues: RequestsAndLimits\n```\n\n### KEDA ScaledObject (Event-Driven)\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: myapp-queue-scaler\nspec:\n  scaleTargetRef:\n    name: myapp\n  minReplicaCount: 0  # Scale to zero\n  maxReplicaCount: 100\n  pollingInterval: 30\n  cooldownPeriod: 300\n  triggers:\n    # Azure Service Bus Queue\n    - type: azure-servicebus\n      metadata:\n        queueName: myqueue\n        namespace: myservicebus\n        messageCount: \"5\"\n      authenticationRef:\n        name: azure-servicebus-auth\n\n    # Azure Storage Queue\n    - type: azure-queue\n      metadata:\n        queueName: myqueue\n        queueLength: \"10\"\n        accountName: mystorageaccount\n      authenticationRef:\n        name: azure-storage-auth\n\n    # Prometheus metrics\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n        metricName: http_requests_per_second\n        threshold: \"100\"\n        query: sum(rate(http_requests_total[2m]))\n```\n\n## Workload Identity (Replaces AAD Pod Identity)\n\n### Setup\n\n```bash\n# Workload identity is enabled by default in AKS Automatic\n\n# Create managed identity\naz identity create \\\n  --name myapp-identity \\\n  --resource-group MyRG\n\n# Get identity details\nexport IDENTITY_CLIENT_ID=$(az identity show -g MyRG -n myapp-identity --query clientId -o tsv)\nexport IDENTITY_OBJECT_ID=$(az identity show -g MyRG -n myapp-identity --query principalId -o tsv)\n\n# Assign role to identity\naz role assignment create \\\n  --assignee $IDENTITY_OBJECT_ID \\\n  --role \"Storage Blob Data Contributor\" \\\n  --scope /subscriptions/<sub-id>/resourceGroups/MyRG/providers/Microsoft.Storage/storageAccounts/mystorage\n\n# Create federated credential\nexport AKS_OIDC_ISSUER=$(az aks show -g MyRG -n MyAKSAutomatic --query oidcIssuerProfile.issuerUrl -o tsv)\n\naz identity federated-credential create \\\n  --name myapp-federated-credential \\\n  --identity-name myapp-identity \\\n  --resource-group MyRG \\\n  --issuer $AKS_OIDC_ISSUER \\\n  --subject system:serviceaccount:default:myapp-sa\n```\n\n### Kubernetes Resources\n\n```yaml\n# Service Account\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: myapp-sa\n  namespace: default\n  annotations:\n    azure.workload.identity/client-id: \"<IDENTITY_CLIENT_ID>\"\n\n---\n# Deployment using workload identity\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n        azure.workload.identity/use: \"true\"  # Enable workload identity\n    spec:\n      serviceAccountName: myapp-sa\n      containers:\n        - name: myapp\n          image: myregistry.azurecr.io/myapp:latest\n          env:\n            - name: AZURE_CLIENT_ID\n              value: \"<IDENTITY_CLIENT_ID>\"\n            - name: AZURE_TENANT_ID\n              value: \"<TENANT_ID>\"\n            - name: AZURE_FEDERATED_TOKEN_FILE\n              value: /var/run/secrets/azure/tokens/azure-identity-token\n          volumeMounts:\n            - name: azure-identity-token\n              mountPath: /var/run/secrets/azure/tokens\n              readOnly: true\n      volumes:\n        - name: azure-identity-token\n          projected:\n            sources:\n              - serviceAccountToken:\n                  path: azure-identity-token\n                  expirationSeconds: 3600\n                  audience: api://AzureADTokenExchange\n```\n\n## Monitoring and Observability\n\n### Enable Container Insights\n\n```bash\n# Already enabled with --enable-addons monitoring\n# Query logs using Azure Monitor\n\n# Get cluster logs\naz monitor log-analytics query \\\n  --workspace <workspace-id> \\\n  --analytics-query \"KubePodInventory | where ClusterName == 'MyAKSAutomatic' | take 10\" \\\n  --output table\n\n# Get Karpenter logs\nkubectl logs -n kube-system -l app.kubernetes.io/name=karpenter\n```\n\n### Prometheus and Grafana\n\n```bash\n# Enable managed Prometheus\naz aks update \\\n  --resource-group MyRG \\\n  --name MyAKSAutomatic \\\n  --enable-azure-monitor-metrics\n\n# Access Grafana dashboards through Azure Portal\n```\n\n## Cost Optimization\n\n### Billing Model (October 2025)\n- **Control plane**: $0.16/hour per cluster\n- **Compute**: Pay for actual node usage\n- **Karpenter**: Automatic bin-packing and consolidation\n- **Scale-to-zero**: Possible with KEDA and Karpenter\n\n### Cost-Saving Tips\n\n1. **Use Spot Instances for Non-Critical Workloads**\n```yaml\n- key: karpenter.sh/capacity-type\n  operator: In\n  values: [\"spot\"]\n```\n\n2. **Configure Aggressive Consolidation**\n```yaml\ndisruption:\n  consolidationPolicy: WhenUnderutilized\n  consolidateAfter: 30s\n```\n\n3. **Implement Pod Disruption Budgets**\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: myapp-pdb\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n4. **Use VPA for Right-Sizing**\n- VPA automatically adjusts resource requests based on actual usage\n\n## Migration from Standard AKS to Automatic\n\nAKS Automatic is a new cluster mode - in-place migration is not supported. Follow these steps:\n\n1. **Create new AKS Automatic cluster**\n2. **Install workloads in new cluster**\n3. **Validate functionality**\n4. **Switch traffic** (DNS, load balancer)\n5. **Decommission old cluster**\n\n## Best Practices\n\n Use AKS Automatic for new production clusters\n Enable workload identity for pod authentication\n Configure custom NodePools for specific workload types\n Implement HPA, VPA, and KEDA for comprehensive scaling\n Use spot instances for batch and fault-tolerant workloads\n Enable Container Insights and Managed Prometheus\n Configure Pod Disruption Budgets for critical apps\n Use network policies for microsegmentation\n Enable Azure Policy add-on for compliance\n Implement GitOps with Flux or Argo CD\n\n## Troubleshooting\n\n### Check Karpenter Status\n```bash\nkubectl logs -n kube-system -l app.kubernetes.io/name=karpenter --tail=100\nkubectl get nodepools\nkubectl get nodeclaims\n```\n\n### View Node Provisioning Events\n```bash\nkubectl get events --field-selector involvedObject.kind=NodePool -A\n```\n\n### Debug Workload Identity Issues\n```bash\n# Check service account annotation\nkubectl get sa myapp-sa -o yaml\n\n# Check pod labels\nkubectl get pod <pod-name> -o yaml | grep azure.workload.identity\n\n# Check federated credential\naz identity federated-credential show \\\n  --identity-name myapp-identity \\\n  --resource-group MyRG \\\n  --name myapp-federated-credential\n```\n\n## References\n\n- [AKS Automatic Documentation](https://learn.microsoft.com/en-us/azure/aks/automatic)\n- [Karpenter on Azure](https://karpenter.sh)\n- [Workload Identity](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview)\n- [AKS Release Notes](https://github.com/Azure/AKS/releases)\n\nAKS Automatic represents the future of managed Kubernetes on Azure - zero operational overhead with maximum automation!"
              },
              {
                "name": "azure-openai-2025",
                "description": "Azure OpenAI Service 2025 models including GPT-5, GPT-4.1, reasoning models, and Azure AI Foundry integration",
                "path": "plugins/azure-master/skills/azure-openai-2025/SKILL.md",
                "frontmatter": {
                  "name": "azure-openai-2025",
                  "description": "Azure OpenAI Service 2025 models including GPT-5, GPT-4.1, reasoning models, and Azure AI Foundry integration"
                },
                "content": "# Azure OpenAI Service - 2025 Models and Features\n\nComplete knowledge base for Azure OpenAI Service with latest 2025 models including GPT-5, GPT-4.1, reasoning models, and Azure AI Foundry integration.\n\n## Overview\n\nAzure OpenAI Service provides REST API access to OpenAI's most powerful models with enterprise-grade security, compliance, and regional availability.\n\n## Latest Models (2025)\n\n### GPT-5 Series (GA August 2025)\n\n**Registration Required Models:**\n- `gpt-5-pro`: Highest capability, complex reasoning\n- `gpt-5`: Balanced performance and cost\n- `gpt-5-codex`: Optimized for code generation\n\n**No Registration Required:**\n- `gpt-5-mini`: Faster, more affordable\n- `gpt-5-nano`: Ultra-fast for simple tasks\n- `gpt-5-chat`: Optimized for conversational use\n\n### GPT-4.1 Series\n\n- `gpt-4.1`: 1 million token context window\n- `gpt-4.1-mini`: Efficient version with 1M context\n- `gpt-4.1-nano`: Fastest variant\n\n**Key Improvements:**\n- 1,000,000 token context (vs 128K in GPT-4 Turbo)\n- Better instruction following\n- Reduced hallucinations\n- Improved multilingual support\n\n### Reasoning Models\n\n**o4-mini**: Lightweight reasoning model\n- Faster inference\n- Lower cost\n- Suitable for structured reasoning tasks\n\n**o3**: Advanced reasoning model\n- Complex problem solving\n- Mathematical reasoning\n- Scientific analysis\n\n**o1**: Original reasoning model\n- General-purpose reasoning\n- Step-by-step explanations\n\n**o1-mini**: Efficient reasoning\n- Balanced cost and performance\n\n### Image Generation\n\n**GPT-image-1 (2025-04-15)**\n- DALL-E 3 successor\n- Higher quality images\n- Better prompt understanding\n- Improved safety filters\n\n### Video Generation\n\n**Sora (2025-05-02)**\n- Text-to-video generation\n- Realistic and imaginative scenes\n- Up to 60 seconds of video\n- Multiple camera angles and styles\n\n### Audio Models\n\n**gpt-4o-transcribe**: Speech-to-text powered by GPT-4o\n- High accuracy transcription\n- Multiple languages\n- Speaker diarization\n\n**gpt-4o-mini-transcribe**: Faster, more affordable transcription\n- Good accuracy\n- Lower latency\n- Cost-effective\n\n## Deploying Azure OpenAI\n\n### Create Azure OpenAI Resource\n\n```bash\n# Create OpenAI account\naz cognitiveservices account create \\\n  --name myopenai \\\n  --resource-group MyRG \\\n  --kind OpenAI \\\n  --sku S0 \\\n  --location eastus \\\n  --custom-domain myopenai \\\n  --public-network-access Disabled \\\n  --identity-type SystemAssigned\n\n# Get endpoint and key\naz cognitiveservices account show \\\n  --name myopenai \\\n  --resource-group MyRG \\\n  --query \"properties.endpoint\" \\\n  --output tsv\n\naz cognitiveservices account keys list \\\n  --name myopenai \\\n  --resource-group MyRG \\\n  --query \"key1\" \\\n  --output tsv\n```\n\n### Deploy GPT-5 Model\n\n```bash\n# Deploy gpt-5\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name gpt-5 \\\n  --model-name gpt-5 \\\n  --model-version latest \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 100 \\\n  --scale-type Standard\n\n# Deploy gpt-5-pro (requires registration)\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name gpt-5-pro \\\n  --model-name gpt-5-pro \\\n  --model-version latest \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 50\n```\n\n### Deploy Reasoning Models\n\n```bash\n# Deploy o3 reasoning model\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name o3-reasoning \\\n  --model-name o3 \\\n  --model-version latest \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 50\n\n# Deploy o4-mini\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name o4-mini \\\n  --model-name o4-mini \\\n  --model-version latest \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 100\n```\n\n### Deploy GPT-4.1 with 1M Context\n\n```bash\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name gpt-4-1 \\\n  --model-name gpt-4.1 \\\n  --model-version latest \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 100\n```\n\n### Deploy Image Generation Model\n\n```bash\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name image-gen \\\n  --model-name gpt-image-1 \\\n  --model-version 2025-04-15 \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 10\n```\n\n### Deploy Sora Video Generation\n\n```bash\naz cognitiveservices account deployment create \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name sora \\\n  --model-name sora \\\n  --model-version 2025-05-02 \\\n  --model-format OpenAI \\\n  --sku-name Standard \\\n  --sku-capacity 5\n```\n\n## Using Azure OpenAI Models\n\n### Python SDK (GPT-5)\n\n```python\nfrom openai import AzureOpenAI\nimport os\n\n# Initialize client\nclient = AzureOpenAI(\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    api_version=\"2025-02-01-preview\",\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n)\n\n# GPT-5 completion\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",  # deployment name\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n    ],\n    max_tokens=1000,\n    temperature=0.7,\n    top_p=0.95\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Python SDK (o3 Reasoning Model)\n\n```python\n# o3 reasoning with chain-of-thought\nresponse = client.chat.completions.create(\n    model=\"o3-reasoning\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an expert problem solver. Show your reasoning step-by-step.\"},\n        {\"role\": \"user\", \"content\": \"If a train travels 120 km in 2 hours, then speeds up to travel 180 km in the next 2 hours, what is the average speed for the entire journey?\"}\n    ],\n    max_tokens=2000,\n    temperature=0.2  # Lower temperature for reasoning tasks\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Python SDK (GPT-4.1 with 1M Context)\n\n```python\n# Read a large document\nwith open('large_document.txt', 'r') as f:\n    document = f.read()\n\n# GPT-4.1 can handle up to 1M tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4-1\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a document analysis expert.\"},\n        {\"role\": \"user\", \"content\": f\"Analyze this document and provide key insights:\\n\\n{document}\"}\n    ],\n    max_tokens=4000\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Image Generation (GPT-image-1)\n\n```python\n# Generate image with DALL-E 3 successor\nresponse = client.images.generate(\n    model=\"image-gen\",\n    prompt=\"A futuristic city with flying cars and vertical gardens, cyberpunk style, highly detailed, 4K\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    n=1\n)\n\nimage_url = response.data[0].url\nprint(f\"Generated image: {image_url}\")\n```\n\n### Video Generation (Sora)\n\n```python\n# Generate video with Sora\nresponse = client.videos.generate(\n    model=\"sora\",\n    prompt=\"A serene lakeside at sunset with birds flying overhead and gentle waves on the shore\",\n    duration=10,  # seconds\n    resolution=\"1080p\",\n    fps=30\n)\n\nvideo_url = response.data[0].url\nprint(f\"Generated video: {video_url}\")\n```\n\n### Audio Transcription\n\n```python\n# Transcribe audio file\naudio_file = open(\"meeting_recording.mp3\", \"rb\")\n\nresponse = client.audio.transcriptions.create(\n    model=\"gpt-4o-transcribe\",\n    file=audio_file,\n    language=\"en\",\n    response_format=\"verbose_json\"\n)\n\nprint(f\"Transcription: {response.text}\")\nprint(f\"Duration: {response.duration}s\")\n\n# Speaker diarization\nfor segment in response.segments:\n    print(f\"[{segment.start}s - {segment.end}s] {segment.text}\")\n```\n\n## Azure AI Foundry Integration\n\n### Model Router (Automatic Model Selection)\n\n```python\nfrom azure.ai.foundry import ModelRouter\n\n# Initialize model router\nrouter = ModelRouter(\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    credential=os.getenv(\"AZURE_OPENAI_API_KEY\")\n)\n\n# Automatically select optimal model\nresponse = router.complete(\n    prompt=\"Analyze this complex scientific paper...\",\n    optimization_goals=[\"quality\", \"cost\"],\n    available_models=[\"gpt-5\", \"gpt-5-mini\", \"gpt-4-1\"]\n)\n\nprint(f\"Selected model: {response.model_used}\")\nprint(f\"Response: {response.content}\")\nprint(f\"Cost: ${response.cost}\")\n```\n\n**Benefits:**\n- Automatic model selection based on prompt complexity\n- Balance quality vs cost\n- Reduce costs by up to 40% while maintaining quality\n\n### Agentic Retrieval (Azure AI Search Integration)\n\n```python\nfrom azure.search.documents import SearchClient\nfrom azure.core.credentials import AzureKeyCredential\n\n# Initialize search client\nsearch_client = SearchClient(\n    endpoint=os.getenv(\"SEARCH_ENDPOINT\"),\n    index_name=\"documents\",\n    credential=AzureKeyCredential(os.getenv(\"SEARCH_KEY\"))\n)\n\n# Agentic retrieval with Azure OpenAI\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You have access to a document search system.\"},\n        {\"role\": \"user\", \"content\": \"What are the company's revenue projections for Q3?\"}\n    ],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search_documents\",\n            \"description\": \"Search company documents\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }],\n    tool_choice=\"auto\"\n)\n\n# Process tool calls\nif response.choices[0].message.tool_calls:\n    for tool_call in response.choices[0].message.tool_calls:\n        if tool_call.function.name == \"search_documents\":\n            query = json.loads(tool_call.function.arguments)[\"query\"]\n            results = search_client.search(query)\n            # Feed results back to model for final answer\n```\n\n**Improvements:**\n- 40% better on complex, multi-part questions\n- Automatic query decomposition\n- Relevance ranking\n- Citation generation\n\n### Foundry Observability (Preview)\n\n```python\nfrom azure.ai.foundry import FoundryObservability\n\n# Enable observability\nobservability = FoundryObservability(\n    workspace_id=os.getenv(\"AI_FOUNDRY_WORKSPACE_ID\"),\n    enable_tracing=True,\n    enable_metrics=True\n)\n\n# Monitor agent execution\nwith observability.trace_agent(\"customer_support_agent\") as trace:\n    response = client.chat.completions.create(\n        model=\"gpt-5\",\n        messages=messages\n    )\n\n    trace.log_tool_call(\"search_kb\", {\"query\": \"refund policy\"})\n    trace.log_reasoning_step(\"Retrieved refund policy document\")\n    trace.log_token_usage(response.usage.total_tokens)\n\n# View in Azure AI Foundry portal:\n# - End-to-end trace logs\n# - Reasoning steps and tool calls\n# - Performance metrics\n# - Cost analysis\n```\n\n## Capacity and Quota Management\n\n### Check Quota\n\n```bash\n# List deployments with usage\naz cognitiveservices account deployment list \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --output table\n\n# Check usage metrics\naz monitor metrics list \\\n  --resource $(az cognitiveservices account show -g MyRG -n myopenai --query id -o tsv) \\\n  --metric \"TokenTransaction\" \\\n  --start-time 2025-01-01T00:00:00Z \\\n  --end-time 2025-01-31T23:59:59Z \\\n  --interval PT1H \\\n  --aggregation Total\n```\n\n### Update Capacity\n\n```bash\n# Scale up deployment capacity\naz cognitiveservices account deployment update \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name gpt-5 \\\n  --sku-capacity 200\n\n# Scale down during off-peak\naz cognitiveservices account deployment update \\\n  --resource-group MyRG \\\n  --name myopenai \\\n  --deployment-name gpt-5 \\\n  --sku-capacity 50\n```\n\n### Request Quota Increase\n\n1. Navigate to Azure Portal  Azure OpenAI resource\n2. Go to \"Quotas\" blade\n3. Select model and region\n4. Click \"Request quota increase\"\n5. Provide justification and target capacity\n\n## Security and Networking\n\n### Private Endpoint\n\n```bash\n# Create private endpoint\naz network private-endpoint create \\\n  --name openai-private-endpoint \\\n  --resource-group MyRG \\\n  --vnet-name MyVNet \\\n  --subnet PrivateEndpointSubnet \\\n  --private-connection-resource-id $(az cognitiveservices account show -g MyRG -n myopenai --query id -o tsv) \\\n  --group-id account \\\n  --connection-name openai-connection\n\n# Create private DNS zone\naz network private-dns zone create \\\n  --resource-group MyRG \\\n  --name privatelink.openai.azure.com\n\n# Link to VNet\naz network private-dns link vnet create \\\n  --resource-group MyRG \\\n  --zone-name privatelink.openai.azure.com \\\n  --name openai-dns-link \\\n  --virtual-network MyVNet \\\n  --registration-enabled false\n\n# Create DNS zone group\naz network private-endpoint dns-zone-group create \\\n  --resource-group MyRG \\\n  --endpoint-name openai-private-endpoint \\\n  --name default \\\n  --private-dns-zone privatelink.openai.azure.com \\\n  --zone-name privatelink.openai.azure.com\n```\n\n### Managed Identity Access\n\n```bash\n# Enable system-assigned identity\naz cognitiveservices account identity assign \\\n  --name myopenai \\\n  --resource-group MyRG\n\n# Grant role to managed identity\nPRINCIPAL_ID=$(az cognitiveservices account show -g MyRG -n myopenai --query identity.principalId -o tsv)\n\naz role assignment create \\\n  --assignee $PRINCIPAL_ID \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope /subscriptions/<sub-id>/resourceGroups/MyRG\n```\n\n### Content Filtering\n\n```bash\n# Configure content filtering\naz cognitiveservices account update \\\n  --name myopenai \\\n  --resource-group MyRG \\\n  --set properties.customContentFilter='{\n    \"hate\": {\"severity\": \"medium\", \"enabled\": true},\n    \"violence\": {\"severity\": \"medium\", \"enabled\": true},\n    \"sexual\": {\"severity\": \"medium\", \"enabled\": true},\n    \"selfHarm\": {\"severity\": \"high\", \"enabled\": true}\n  }'\n```\n\n## Cost Optimization\n\n### Model Selection Strategy\n\n**Use GPT-5-mini or GPT-5-nano for:**\n- Simple questions\n- Classification tasks\n- Content moderation\n- Summarization\n\n**Use GPT-5 or GPT-4.1 for:**\n- Complex reasoning\n- Long-form content generation\n- Document analysis\n- Code generation\n\n**Use Reasoning Models (o3, o4-mini) for:**\n- Mathematical problems\n- Scientific analysis\n- Step-by-step reasoning\n- Logic puzzles\n\n### Implement Caching\n\n```python\n# Use semantic cache to reduce duplicate requests\nfrom azure.ai.cache import SemanticCache\n\ncache = SemanticCache(\n    similarity_threshold=0.95,\n    ttl_seconds=3600\n)\n\n# Check cache before API call\ncached_response = cache.get(user_query)\nif cached_response:\n    return cached_response\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=messages\n)\n\ncache.set(user_query, response)\n```\n\n### Token Management\n\n```python\nimport tiktoken\n\n# Count tokens before API call\nencoding = tiktoken.get_encoding(\"cl100k_base\")\ntokens = len(encoding.encode(prompt))\n\nif tokens > 100000:\n    print(f\"Warning: Prompt has {tokens} tokens, this will be expensive!\")\n\n# Use shorter max_tokens when appropriate\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=messages,\n    max_tokens=500  # Limit output tokens\n)\n```\n\n## Monitoring and Alerts\n\n### Set Up Cost Alerts\n\n```bash\n# Create budget alert\naz consumption budget create \\\n  --budget-name openai-monthly-budget \\\n  --resource-group MyRG \\\n  --amount 1000 \\\n  --category Cost \\\n  --time-grain Monthly \\\n  --start-date 2025-01-01 \\\n  --end-date 2025-12-31 \\\n  --notifications '{\n    \"actual_GreaterThan_80_Percent\": {\n      \"enabled\": true,\n      \"operator\": \"GreaterThan\",\n      \"threshold\": 80,\n      \"contactEmails\": [\"billing@example.com\"]\n    }\n  }'\n```\n\n### Application Insights Integration\n\n```python\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\nimport logging\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(\n    connection_string=os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n))\n\n# Log API calls\nlogger.info(\"OpenAI API call\", extra={\n    \"custom_dimensions\": {\n        \"model\": \"gpt-5\",\n        \"tokens\": response.usage.total_tokens,\n        \"cost\": calculate_cost(response.usage.total_tokens),\n        \"latency_ms\": response.response_ms\n    }\n})\n```\n\n## Best Practices\n\n **Use Model Router** for automatic cost optimization\n **Implement caching** to reduce duplicate requests\n **Monitor token usage** and set budgets\n **Use private endpoints** for production workloads\n **Enable managed identity** instead of API keys\n **Configure content filtering** for safety\n **Right-size capacity** based on actual demand\n **Use Foundry Observability** for monitoring\n **Implement retry logic** with exponential backoff\n **Choose appropriate models** for task complexity\n\n## References\n\n- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/)\n- [What's New in Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/whats-new)\n- [GPT-5 Announcement](https://azure.microsoft.com/en-us/blog/gpt-5-azure/)\n- [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/)\n- [Model Pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)\n\nAzure OpenAI Service with GPT-5 and reasoning models brings enterprise-grade AI to your applications!"
              },
              {
                "name": "azure-well-architected-framework",
                "description": "Comprehensive Azure Well-Architected Framework knowledge covering the five pillars: Reliability, Security, Cost Optimization, Operational Excellence, and Performance Efficiency. Provides design principles, best practices, and implementation guidance for building robust Azure solutions.",
                "path": "plugins/azure-master/skills/azure-well-architected-framework/SKILL.md",
                "frontmatter": {
                  "name": "azure-well-architected-framework",
                  "description": "Comprehensive Azure Well-Architected Framework knowledge covering the five pillars: Reliability, Security, Cost Optimization, Operational Excellence, and Performance Efficiency. Provides design principles, best practices, and implementation guidance for building robust Azure solutions."
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Azure Well-Architected Framework\n\nThe Azure Well-Architected Framework is a set of guiding tenets for building high-quality cloud solutions. It consists of five pillars of architectural excellence.\n\n## Overview\n\n**Purpose**: Help architects and engineers build secure, high-performing, resilient, and efficient infrastructure for applications.\n\n**The Five Pillars**:\n1. Reliability\n2. Security\n3. Cost Optimization\n4. Operational Excellence\n5. Performance Efficiency\n\n## Pillar 1: Reliability\n\n**Definition**: The ability of a system to recover from failures and continue to function.\n\n**Key Principles**:\n- Design for failure\n- Use availability zones and regions\n- Implement redundancy\n- Monitor and respond to failures\n- Test disaster recovery\n\n**Best Practices**:\n\n**Availability Zones:**\n```bash\n# Deploy VM across availability zones\naz vm create \\\n  --resource-group MyRG \\\n  --name MyVM \\\n  --zone 1 \\\n  --image Ubuntu2204 \\\n  --size Standard_D2s_v3\n\n# Availability SLAs:\n# - Single VM (Premium SSD): 99.9%\n# - Availability Set: 99.95%\n# - Availability Zones: 99.99%\n```\n\n**Backup and Disaster Recovery:**\n```bash\n# Enable Azure Backup\naz backup protection enable-for-vm \\\n  --resource-group MyRG \\\n  --vault-name MyVault \\\n  --vm MyVM \\\n  --policy-name DefaultPolicy\n\n# Recovery Point Objective (RPO): How much data loss is acceptable\n# Recovery Time Objective (RTO): How long can system be down\n```\n\n**Health Probes:**\n- Application Gateway health probes\n- Load Balancer probes\n- Traffic Manager endpoint monitoring\n\n## Pillar 2: Security\n\n**Definition**: Protecting applications and data from threats.\n\n**Key Principles**:\n- Defense in depth\n- Least privilege access\n- Secure the network\n- Protect data at rest and in transit\n- Monitor and audit\n\n**Best Practices**:\n\n**Identity and Access:**\n```bash\n# Use managed identities (no credentials in code)\naz vm identity assign \\\n  --resource-group MyRG \\\n  --name MyVM\n\n# RBAC assignment\naz role assignment create \\\n  --assignee <principal-id> \\\n  --role \"Contributor\" \\\n  --scope /subscriptions/<subscription-id>/resourceGroups/MyRG\n```\n\n**Network Security:**\n- Use Network Security Groups (NSGs)\n- Implement Azure Firewall or Application Gateway WAF\n- Use Private Endpoints for PaaS services\n- Enable DDoS Protection Standard for public-facing apps\n\n**Data Protection:**\n```bash\n# Enable encryption at rest (automatic for most services)\n# Enable TLS 1.2+ for data in transit\n\n# Azure Storage encryption\naz storage account update \\\n  --name mystorageaccount \\\n  --resource-group MyRG \\\n  --min-tls-version TLS1_2 \\\n  --https-only true\n```\n\n**Security Monitoring:**\n```bash\n# Enable Microsoft Defender for Cloud\naz security pricing create \\\n  --name VirtualMachines \\\n  --tier Standard\n\n# Enable Azure Sentinel\naz sentinel onboard \\\n  --resource-group MyRG \\\n  --workspace-name MyWorkspace\n```\n\n## Pillar 3: Cost Optimization\n\n**Definition**: Managing costs to maximize the value delivered.\n\n**Key Principles**:\n- Plan and estimate costs\n- Provision with optimization\n- Use monitoring and analytics\n- Maximize efficiency of cloud spend\n\n**Best Practices**:\n\n**Right-Sizing:**\n```bash\n# Use Azure Advisor recommendations\naz advisor recommendation list \\\n  --category Cost \\\n  --output table\n\n# Common optimizations:\n# 1. Shutdown dev/test VMs when not in use\n# 2. Use Azure Hybrid Benefit for Windows/SQL\n# 3. Purchase reservations for consistent workloads\n# 4. Use autoscaling to match demand\n```\n\n**Reserved Instances:**\n- 1-year or 3-year commitment\n- Save up to 72% vs pay-as-you-go\n- Available for VMs, SQL Database, Cosmos DB, Synapse, Storage\n\n**Azure Hybrid Benefit:**\n```bash\n# Apply Windows license to VM\naz vm update \\\n  --resource-group MyRG \\\n  --name MyVM \\\n  --license-type Windows_Server\n\n# SQL Server Hybrid Benefit\naz sql vm create \\\n  --resource-group MyRG \\\n  --name MySQLVM \\\n  --license-type AHUB\n```\n\n**Cost Management:**\n```bash\n# Create budget\naz consumption budget create \\\n  --budget-name MyBudget \\\n  --category cost \\\n  --amount 1000 \\\n  --time-grain monthly \\\n  --start-date 2025-01-01 \\\n  --end-date 2025-12-31\n\n# Set up alerts at 80%, 100%, 120% of budget\n```\n\n## Pillar 4: Operational Excellence\n\n**Definition**: Operations processes that keep a system running in production.\n\n**Key Principles**:\n- Automate operations\n- Monitor and gain insights\n- Refine operations procedures\n- Anticipate failure\n- Stay current with updates\n\n**Best Practices**:\n\n**Infrastructure as Code:**\n```bash\n# Use ARM, Bicep, or Terraform\n# Version control all infrastructure\n# Implement CI/CD for infrastructure\n\n# Example: Bicep deployment\naz deployment group create \\\n  --resource-group MyRG \\\n  --template-file main.bicep \\\n  --parameters @parameters.json\n```\n\n**Monitoring and Alerting:**\n```bash\n# Application Insights for apps\naz monitor app-insights component create \\\n  --app MyApp \\\n  --location eastus \\\n  --resource-group MyRG\n\n# Log Analytics for infrastructure\naz monitor log-analytics workspace create \\\n  --resource-group MyRG \\\n  --workspace-name MyWorkspace\n\n# Create alerts\naz monitor metrics alert create \\\n  --name HighCPU \\\n  --resource-group MyRG \\\n  --scopes <vm-id> \\\n  --condition \"avg Percentage CPU > 80\" \\\n  --description \"CPU usage is above 80%\"\n```\n\n**DevOps Practices:**\n- Continuous Integration/Continuous Deployment (CI/CD)\n- Blue-green deployments\n- Canary releases\n- Feature flags\n- Automated testing\n\n## Pillar 5: Performance Efficiency\n\n**Definition**: The ability of a system to adapt to changes in load.\n\n**Key Principles**:\n- Scale horizontally\n- Choose the right resources\n- Monitor performance\n- Optimize network and data access\n\n**Best Practices**:\n\n**Scaling:**\n```bash\n# Horizontal scaling (preferred)\n# VM Scale Sets\naz vmss create \\\n  --resource-group MyRG \\\n  --name MyVMSS \\\n  --image Ubuntu2204 \\\n  --instance-count 3 \\\n  --vm-sku Standard_D2s_v3\n\n# Autoscaling\naz monitor autoscale create \\\n  --resource-group MyRG \\\n  --resource MyVMSS \\\n  --resource-type Microsoft.Compute/virtualMachineScaleSets \\\n  --name MyAutoscale \\\n  --min-count 2 \\\n  --max-count 10\n```\n\n**Caching:**\n- Azure Cache for Redis\n- Azure CDN for static content\n- Application-level caching\n\n**Data Access:**\n- Use indexes on databases\n- Implement caching strategies\n- Use CDN for global content delivery\n- Optimize queries (SQL, Cosmos DB)\n\n**Networking:**\n```bash\n# Use Azure Front Door for global apps\naz afd profile create \\\n  --profile-name MyFrontDoor \\\n  --resource-group MyRG \\\n  --sku Premium_AzureFrontDoor\n\n# Features:\n# - Global load balancing\n# - CDN capabilities\n# - Web Application Firewall\n# - SSL offloading\n# - Caching\n```\n\n## Assessment and Tools\n\n**Azure Well-Architected Review:**\n```bash\n# Self-assessment tool in Azure Portal\n# Generates recommendations per pillar\n# Provides actionable guidance\n```\n\n**Azure Advisor:**\n```bash\n# Get recommendations\naz advisor recommendation list --output table\n\n# Categories:\n# - Reliability (High Availability)\n# - Security\n# - Performance\n# - Cost\n# - Operational Excellence\n```\n\n## Implementation Checklist\n\n**Reliability:**\n- [ ] Deploy across availability zones\n- [ ] Implement backup strategy\n- [ ] Define RTO and RPO\n- [ ] Test disaster recovery\n- [ ] Implement health monitoring\n\n**Security:**\n- [ ] Enable Azure AD authentication\n- [ ] Implement RBAC (least privilege)\n- [ ] Encrypt data at rest and in transit\n- [ ] Enable Microsoft Defender for Cloud\n- [ ] Implement network segmentation (NSGs, Firewall)\n- [ ] Use Key Vault for secrets\n\n**Cost Optimization:**\n- [ ] Right-size resources\n- [ ] Purchase reservations for predictable workloads\n- [ ] Enable autoscaling\n- [ ] Use Azure Hybrid Benefit\n- [ ] Implement budget alerts\n- [ ] Review Azure Advisor cost recommendations\n\n**Operational Excellence:**\n- [ ] Implement Infrastructure as Code\n- [ ] Set up CI/CD pipelines\n- [ ] Enable comprehensive monitoring\n- [ ] Create operational runbooks\n- [ ] Implement automated alerting\n- [ ] Use tags for resource organization\n\n**Performance Efficiency:**\n- [ ] Choose appropriate resource SKUs\n- [ ] Implement autoscaling\n- [ ] Use caching (Redis, CDN)\n- [ ] Optimize database queries\n- [ ] Implement load balancing\n- [ ] Monitor performance metrics\n\n## Common Patterns\n\n**Highly Available Web Application:**\n- Application Gateway (WAF enabled)\n- App Service (Premium tier, multiple instances)\n- Azure SQL Database (Zone-redundant)\n- Azure Cache for Redis\n- Application Insights\n- Azure Front Door (global distribution)\n\n**Mission-Critical Application:**\n- Multi-region deployment\n- Traffic Manager or Front Door (global routing)\n- Availability Zones in each region\n- Geo-redundant storage (GRS or RA-GRS)\n- Automated backups with geo-replication\n- Comprehensive monitoring and alerting\n\n**Cost-Optimized Dev/Test:**\n- Auto-shutdown for VMs\n- B-series (burstable) VMs\n- Dev/Test pricing tiers\n- Shared App Service plans\n- Azure DevTest Labs\n\n## References\n\n- **Official Framework**: https://learn.microsoft.com/en-us/azure/well-architected/\n- **Azure Advisor**: https://portal.azure.com/#blade/Microsoft_Azure_Expert/AdvisorMenuBlade/overview\n- **Well-Architected Review**: https://learn.microsoft.com/en-us/assessments/azure-architecture-review/\n- **Architecture Center**: https://learn.microsoft.com/en-us/azure/architecture/\n\n## Key Takeaways\n\n1. **Balance the Pillars**: Trade-offs exist between pillars (e.g., cost vs. reliability)\n2. **Continuous Improvement**: Architecture is not static, revisit regularly\n3. **Measure and Monitor**: Use data to drive decisions\n4. **Automation**: Automate repetitive tasks to improve reliability and reduce costs\n5. **Security First**: Integrate security into every layer of architecture\n\nThe Well-Architected Framework provides a consistent approach to evaluating architectures and implementing designs that scale over time."
              },
              {
                "name": "container-apps-gpu-2025",
                "description": "Azure Container Apps GPU support 2025 features including serverless GPU, Dapr integration, and scale-to-zero",
                "path": "plugins/azure-master/skills/container-apps-gpu-2025/SKILL.md",
                "frontmatter": {
                  "name": "container-apps-gpu-2025",
                  "description": "Azure Container Apps GPU support 2025 features including serverless GPU, Dapr integration, and scale-to-zero"
                },
                "content": "# Azure Container Apps GPU Support - 2025 Features\n\nComplete knowledge base for Azure Container Apps with GPU support, serverless capabilities, and Dapr integration (2025 GA features).\n\n## Overview\n\nAzure Container Apps is a serverless container platform with native GPU support, Dapr integration, and scale-to-zero capabilities for cost-efficient AI/ML workloads.\n\n## Key 2025 Features (Build Announcements)\n\n### 1. Serverless GPU (GA)\n- **Automatic scaling**: Scale GPU workloads based on demand\n- **Scale-to-zero**: Pay only when GPU is actively used\n- **Per-second billing**: Granular cost control\n- **Optimized cold start**: Fast initialization for AI models\n- **Reduced operational overhead**: No infrastructure management\n\n### 2. Dedicated GPU (GA)\n- **Consistent performance**: Dedicated GPU resources\n- **Simplified AI deployment**: Easy model hosting\n- **Long-running workloads**: Ideal for training and continuous inference\n- **Multiple GPU types**: NVIDIA A100, T4, and more\n\n### 3. Dynamic Sessions with GPU (Early Access)\n- **Sandboxed execution**: Run untrusted AI-generated code\n- **Hyper-V isolation**: Enhanced security\n- **GPU-powered Python interpreter**: Handle compute-intensive AI workloads\n- **Scale at runtime**: Dynamic resource allocation\n\n### 4. Foundry Models Integration\n- **Deploy AI models directly**: During container app creation\n- **Ready-to-use models**: Pre-configured inference endpoints\n- **Azure AI Foundry**: Seamless integration\n\n### 5. Workflow with Durable Task Scheduler (Preview)\n- **Long-running workflows**: Reliable orchestration\n- **State management**: Automatic persistence\n- **Event-driven**: Trigger workflows from events\n\n### 6. Native Azure Functions Support\n- **Functions runtime**: Run Azure Functions in Container Apps\n- **Consistent development**: Same code, serverless execution\n- **Event triggers**: All Functions triggers supported\n\n### 7. Dapr Integration (GA)\n- **Service discovery**: Built-in DNS-based discovery\n- **State management**: Distributed state stores\n- **Pub/sub messaging**: Reliable messaging patterns\n- **Service invocation**: Resilient service-to-service calls\n- **Observability**: Integrated tracing and metrics\n\n## Creating Container Apps with GPU\n\n### Basic Container App with Serverless GPU\n\n```bash\n# Create Container Apps environment\naz containerapp env create \\\n  --name myenv \\\n  --resource-group MyRG \\\n  --location eastus \\\n  --logs-workspace-id <workspace-id> \\\n  --logs-workspace-key <workspace-key>\n\n# Create Container App with GPU\naz containerapp create \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/ai-model:latest \\\n  --cpu 4 \\\n  --memory 8Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 1 \\\n  --min-replicas 0 \\\n  --max-replicas 10 \\\n  --ingress external \\\n  --target-port 8080\n```\n\n### Production-Ready Container App with GPU\n\n```bash\naz containerapp create \\\n  --name myapp-gpu-prod \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  \\\n  # Container configuration\n  --image myregistry.azurecr.io/ai-model:latest \\\n  --registry-server myregistry.azurecr.io \\\n  --registry-identity system \\\n  \\\n  # Resources\n  --cpu 4 \\\n  --memory 8Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 1 \\\n  \\\n  # Scaling\n  --min-replicas 0 \\\n  --max-replicas 20 \\\n  --scale-rule-name http-scaling \\\n  --scale-rule-type http \\\n  --scale-rule-http-concurrency 10 \\\n  \\\n  # Networking\n  --ingress external \\\n  --target-port 8080 \\\n  --transport http2 \\\n  --exposed-port 8080 \\\n  \\\n  # Security\n  --registry-identity system \\\n  --env-vars \"AZURE_CLIENT_ID=secretref:client-id\" \\\n  \\\n  # Monitoring\n  --dapr-app-id myapp \\\n  --dapr-app-port 8080 \\\n  --dapr-app-protocol http \\\n  --enable-dapr \\\n  \\\n  # Identity\n  --system-assigned\n```\n\n## Container Apps Environment Configuration\n\n### Environment with Zone Redundancy\n\n```bash\naz containerapp env create \\\n  --name myenv-prod \\\n  --resource-group MyRG \\\n  --location eastus \\\n  --logs-workspace-id <workspace-id> \\\n  --logs-workspace-key <workspace-key> \\\n  --zone-redundant true \\\n  --enable-workload-profiles true\n```\n\n### Workload Profiles (Dedicated GPU)\n\n```bash\n# Create environment with workload profiles\naz containerapp env create \\\n  --name myenv-gpu \\\n  --resource-group MyRG \\\n  --location eastus \\\n  --enable-workload-profiles true\n\n# Add GPU workload profile\naz containerapp env workload-profile add \\\n  --name myenv-gpu \\\n  --resource-group MyRG \\\n  --workload-profile-name gpu-profile \\\n  --workload-profile-type GPU-A100 \\\n  --min-nodes 0 \\\n  --max-nodes 10\n\n# Create container app with GPU profile\naz containerapp create \\\n  --name myapp-dedicated-gpu \\\n  --resource-group MyRG \\\n  --environment myenv-gpu \\\n  --workload-profile-name gpu-profile \\\n  --image myregistry.azurecr.io/training-job:latest \\\n  --cpu 8 \\\n  --memory 16Gi \\\n  --min-replicas 1 \\\n  --max-replicas 5\n```\n\n## GPU Scaling Rules\n\n### Custom Prometheus Scaling\n\n```bash\naz containerapp create \\\n  --name myapp-gpu-prometheus \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/ai-model:latest \\\n  --cpu 4 \\\n  --memory 8Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 1 \\\n  --min-replicas 0 \\\n  --max-replicas 10 \\\n  --scale-rule-name gpu-utilization \\\n  --scale-rule-type custom \\\n  --scale-rule-custom-type prometheus \\\n  --scale-rule-metadata \\\n    serverAddress=http://prometheus.monitoring.svc.cluster.local:9090 \\\n    metricName=gpu_utilization \\\n    threshold=80 \\\n    query=\"avg(nvidia_gpu_utilization{app='myapp'})\"\n```\n\n### Queue-Based Scaling (Azure Service Bus)\n\n```bash\naz containerapp create \\\n  --name myapp-queue-processor \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/batch-processor:latest \\\n  --cpu 4 \\\n  --memory 8Gi \\\n  --gpu-type nvidia-t4 \\\n  --gpu-count 1 \\\n  --min-replicas 0 \\\n  --max-replicas 50 \\\n  --scale-rule-name queue-scaling \\\n  --scale-rule-type azure-servicebus \\\n  --scale-rule-metadata \\\n    queueName=ai-jobs \\\n    namespace=myservicebus \\\n    messageCount=5 \\\n  --scale-rule-auth connection=servicebus-connection\n```\n\n## Dapr Integration\n\n### Enable Dapr on Container App\n\n```bash\naz containerapp create \\\n  --name myapp-dapr \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/myapp:latest \\\n  --enable-dapr \\\n  --dapr-app-id myapp \\\n  --dapr-app-port 8080 \\\n  --dapr-app-protocol http \\\n  --dapr-http-max-request-size 4 \\\n  --dapr-http-read-buffer-size 4 \\\n  --dapr-log-level info \\\n  --dapr-enable-api-logging true\n```\n\n### Dapr State Store (Azure Cosmos DB)\n\n```yaml\n# Create Dapr component for state store\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: statestore\nspec:\n  type: state.azure.cosmosdb\n  version: v1\n  metadata:\n    - name: url\n      value: \"https://mycosmosdb.documents.azure.com:443/\"\n    - name: masterKey\n      secretRef: cosmosdb-key\n    - name: database\n      value: \"mydb\"\n    - name: collection\n      value: \"state\"\n```\n\n```bash\n# Create the component\naz containerapp env dapr-component set \\\n  --name myenv \\\n  --resource-group MyRG \\\n  --dapr-component-name statestore \\\n  --yaml component.yaml\n```\n\n### Dapr Pub/Sub (Azure Service Bus)\n\n```yaml\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: pubsub\nspec:\n  type: pubsub.azure.servicebus.topics\n  version: v1\n  metadata:\n    - name: connectionString\n      secretRef: servicebus-connection\n    - name: consumerID\n      value: \"myapp\"\n```\n\n### Service-to-Service Invocation\n\n```python\n# Python example using Dapr SDK\nfrom dapr.clients import DaprClient\n\nwith DaprClient() as client:\n    # Invoke another service\n    response = client.invoke_method(\n        app_id='other-service',\n        method_name='process',\n        data='{\"input\": \"data\"}'\n    )\n\n    # Save state\n    client.save_state(\n        store_name='statestore',\n        key='mykey',\n        value='myvalue'\n    )\n\n    # Publish message\n    client.publish_event(\n        pubsub_name='pubsub',\n        topic_name='orders',\n        data='{\"orderId\": \"123\"}'\n    )\n```\n\n## AI Model Deployment Patterns\n\n### OpenAI-Compatible Endpoint\n\n```dockerfile\n# Dockerfile for vLLM model serving\nFROM vllm/vllm-openai:latest\n\nENV MODEL_NAME=\"meta-llama/Llama-3.1-8B-Instruct\"\nENV GPU_MEMORY_UTILIZATION=0.9\nENV MAX_MODEL_LEN=4096\n\nCMD [\"--model\", \"${MODEL_NAME}\", \\\n     \"--gpu-memory-utilization\", \"${GPU_MEMORY_UTILIZATION}\", \\\n     \"--max-model-len\", \"${MAX_MODEL_LEN}\", \\\n     \"--port\", \"8080\"]\n```\n\n```bash\n# Deploy vLLM model\naz containerapp create \\\n  --name llama-inference \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image vllm/vllm-openai:latest \\\n  --cpu 8 \\\n  --memory 32Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 1 \\\n  --min-replicas 1 \\\n  --max-replicas 5 \\\n  --target-port 8080 \\\n  --ingress external \\\n  --env-vars \\\n    MODEL_NAME=\"meta-llama/Llama-3.1-8B-Instruct\" \\\n    GPU_MEMORY_UTILIZATION=\"0.9\" \\\n    HF_TOKEN=secretref:huggingface-token\n```\n\n### Stable Diffusion Image Generation\n\n```bash\naz containerapp create \\\n  --name stable-diffusion \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/stable-diffusion:latest \\\n  --cpu 4 \\\n  --memory 16Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 1 \\\n  --min-replicas 0 \\\n  --max-replicas 10 \\\n  --target-port 7860 \\\n  --ingress external \\\n  --scale-rule-name http-scaling \\\n  --scale-rule-type http \\\n  --scale-rule-http-concurrency 1\n```\n\n### Batch Processing Job\n\n```bash\naz containerapp job create \\\n  --name batch-training-job \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --trigger-type Manual \\\n  --image myregistry.azurecr.io/training:latest \\\n  --cpu 8 \\\n  --memory 32Gi \\\n  --gpu-type nvidia-a100 \\\n  --gpu-count 2 \\\n  --parallelism 1 \\\n  --replica-timeout 7200 \\\n  --replica-retry-limit 3 \\\n  --env-vars \\\n    DATASET_URL=\"https://mystorage.blob.core.windows.net/datasets/train.csv\" \\\n    MODEL_OUTPUT=\"https://mystorage.blob.core.windows.net/models/\" \\\n    EPOCHS=\"100\"\n\n# Execute job\naz containerapp job start \\\n  --name batch-training-job \\\n  --resource-group MyRG\n```\n\n## Monitoring and Observability\n\n### Application Insights Integration\n\n```bash\naz containerapp create \\\n  --name myapp-monitored \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/myapp:latest \\\n  --env-vars \\\n    APPLICATIONINSIGHTS_CONNECTION_STRING=secretref:appinsights-connection\n```\n\n### Query Logs\n\n```bash\n# Stream logs\naz containerapp logs show \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --follow\n\n# Query with Log Analytics\naz monitor log-analytics query \\\n  --workspace <workspace-id> \\\n  --analytics-query \"ContainerAppConsoleLogs_CL | where ContainerAppName_s == 'myapp-gpu' | take 100\"\n```\n\n### Metrics and Alerts\n\n```bash\n# Create metric alert for GPU usage\naz monitor metrics alert create \\\n  --name high-gpu-usage \\\n  --resource-group MyRG \\\n  --scopes $(az containerapp show -g MyRG -n myapp-gpu --query id -o tsv) \\\n  --condition \"avg Requests > 100\" \\\n  --window-size 5m \\\n  --evaluation-frequency 1m \\\n  --action <action-group-id>\n```\n\n## Security Best Practices\n\n### Managed Identity\n\n```bash\n# Create with system-assigned identity\naz containerapp create \\\n  --name myapp-identity \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --system-assigned \\\n  --image myregistry.azurecr.io/myapp:latest\n\n# Get identity principal ID\nIDENTITY_ID=$(az containerapp show -g MyRG -n myapp-identity --query identity.principalId -o tsv)\n\n# Assign role to access Key Vault\naz role assignment create \\\n  --assignee $IDENTITY_ID \\\n  --role \"Key Vault Secrets User\" \\\n  --scope /subscriptions/<sub-id>/resourceGroups/MyRG/providers/Microsoft.KeyVault/vaults/mykeyvault\n\n# Use user-assigned identity\naz identity create --name myapp-identity --resource-group MyRG\nIDENTITY_RESOURCE_ID=$(az identity show -g MyRG -n myapp-identity --query id -o tsv)\n\naz containerapp create \\\n  --name myapp-user-identity \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --user-assigned $IDENTITY_RESOURCE_ID \\\n  --image myregistry.azurecr.io/myapp:latest\n```\n\n### Secret Management\n\n```bash\n# Add secrets\naz containerapp secret set \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --secrets \\\n    huggingface-token=\"<token>\" \\\n    api-key=\"<key>\"\n\n# Reference secrets in environment variables\naz containerapp update \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --set-env-vars \\\n    HF_TOKEN=secretref:huggingface-token \\\n    API_KEY=secretref:api-key\n```\n\n## Cost Optimization\n\n### Scale-to-Zero Configuration\n\n```bash\naz containerapp create \\\n  --name myapp-scale-zero \\\n  --resource-group MyRG \\\n  --environment myenv \\\n  --image myregistry.azurecr.io/myapp:latest \\\n  --min-replicas 0 \\\n  --max-replicas 10 \\\n  --scale-rule-name http-scaling \\\n  --scale-rule-type http \\\n  --scale-rule-http-concurrency 10\n```\n\n**Cost savings**: Pay only when requests are being processed. GPU costs are per-second when active.\n\n### Right-Sizing Resources\n\n```bash\n# Start with minimal resources\n--cpu 2 --memory 4Gi --gpu-count 1\n\n# Monitor and adjust based on actual usage\naz monitor metrics list \\\n  --resource $(az containerapp show -g MyRG -n myapp-gpu --query id -o tsv) \\\n  --metric \"CpuPercentage,MemoryPercentage\"\n```\n\n### Use Spot/Preemptible GPUs (Future Feature)\n\nWhen available, configure spot instances for non-critical workloads to save up to 80% on GPU costs.\n\n## Troubleshooting\n\n### Check Revision Status\n\n```bash\naz containerapp revision list \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --output table\n```\n\n### View Revision Details\n\n```bash\naz containerapp revision show \\\n  --name <revision-name> \\\n  --app myapp-gpu \\\n  --resource-group MyRG\n```\n\n### Restart Container App\n\n```bash\naz containerapp update \\\n  --name myapp-gpu \\\n  --resource-group MyRG \\\n  --force-restart\n```\n\n### GPU Not Available\n\nIf GPU is not provisioning:\n1. Check region availability: Not all regions support GPU\n2. Verify quota: Request quota increase if needed\n3. Check workload profile: Ensure GPU workload profile is created\n\n## Best Practices\n\n Use scale-to-zero for intermittent workloads\n Implement health probes (liveness and readiness)\n Use managed identities for authentication\n Store secrets in Azure Key Vault\n Enable Dapr for microservices patterns\n Configure appropriate scaling rules\n Monitor GPU utilization and adjust resources\n Use Container Apps jobs for batch processing\n Implement retry logic for transient failures\n Use Application Insights for observability\n\n## References\n\n- [Container Apps GPU Documentation](https://learn.microsoft.com/en-us/azure/container-apps/gpu-support)\n- [Dapr Integration](https://learn.microsoft.com/en-us/azure/container-apps/dapr-overview)\n- [Scaling Rules](https://learn.microsoft.com/en-us/azure/container-apps/scale-app)\n- [Build 2025 Announcements](https://azure.microsoft.com/en-us/blog/container-apps-build-2025/)\n\nAzure Container Apps with GPU support provides the ultimate serverless platform for AI/ML workloads!"
              },
              {
                "name": "deployment-stacks-2025",
                "description": "Azure Deployment Stacks GA 2025 features for unified resource management, deny settings, and lifecycle management",
                "path": "plugins/azure-master/skills/deployment-stacks-2025/SKILL.md",
                "frontmatter": {
                  "name": "deployment-stacks-2025",
                  "description": "Azure Deployment Stacks GA 2025 features for unified resource management, deny settings, and lifecycle management"
                },
                "content": "# Azure Deployment Stacks - 2025 GA Features\n\nComplete knowledge base for Azure Deployment Stacks, the successor to Azure Blueprints (GA 2024, best practices 2025).\n\n## Overview\n\nAzure Deployment Stacks is a resource type for managing a collection of Azure resources as a single, atomic unit. It provides unified lifecycle management, resource protection, and automatic cleanup capabilities.\n\n## Key Features\n\n### 1. Unified Resource Management\n- Manage multiple resources as a single entity\n- Update, export, and delete operations on the entire stack\n- Track all managed resources in one place\n- Consistent deployment across environments\n\n### 2. Deny Settings (Resource Protection)\nPrevent unauthorized modifications to managed resources:\n- **None**: No restrictions (default)\n- **DenyDelete**: Prevent resource deletion\n- **DenyWriteAndDelete**: Prevent updates and deletions\n\n### 3. ActionOnUnmanage (Cleanup Policies)\nControl what happens to resources no longer in template:\n- **detachAll**: Remove from stack management, keep resources\n- **deleteAll**: Delete resources not in template\n- **deleteResources**: Delete unmanaged resources, keep resource groups\n\n### 4. Scope Flexibility\nDeploy stacks at:\n- Resource group scope\n- Subscription scope\n- Management group scope\n\n### 5. Replaces Azure Blueprints\nAzure Blueprints will be deprecated in **July 2026**. Deployment Stacks is the recommended replacement.\n\n## Prerequisites\n\n### Azure CLI Version\n```bash\n# Requires Azure CLI 2.61.0 or later\naz version\n\n# Upgrade if needed\naz upgrade\n```\n\n### Azure PowerShell Version\n```bash\n# Requires Azure PowerShell 12.0.0 or later\nGet-InstalledModule -Name Az\nUpdate-Module -Name Az\n```\n\n## Creating Deployment Stacks\n\n### Subscription Scope Stack\n\n```bash\n# Create deployment stack at subscription level\naz stack sub create \\\n  --name MyProductionStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --parameters @parameters.json \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --deny-settings-excluded-principals <devops-service-principal-id> <admin-group-id> \\\n  --action-on-unmanage deleteAll \\\n  --description \"Production infrastructure managed by deployment stack\" \\\n  --tags Environment=Production ManagedBy=DeploymentStack CostCenter=Engineering\n\n# What-if analysis before deployment\naz stack sub what-if \\\n  --name MyProductionStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --parameters @parameters.json\n\n# Create with confirmation prompt disabled\naz stack sub create \\\n  --name MyDevStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --deny-settings-mode None \\\n  --action-on-unmanage detachAll \\\n  --yes\n```\n\n### Resource Group Scope Stack\n\n```bash\n# Create resource group\naz group create \\\n  --name MyRG \\\n  --location eastus \\\n  --tags Environment=Production\n\n# Create deployment stack\naz stack group create \\\n  --name MyAppStack \\\n  --resource-group MyRG \\\n  --template-file main.bicep \\\n  --parameters environment=production \\\n  --deny-settings-mode DenyDelete \\\n  --action-on-unmanage deleteAll \\\n  --description \"Application infrastructure stack\"\n```\n\n### Management Group Scope Stack\n\n```bash\n# Create stack at management group level\naz stack mg create \\\n  --name MyEnterpriseStack \\\n  --management-group-id MyMgmtGroup \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --action-on-unmanage detachAll\n```\n\n## Bicep Template for Deployment Stack\n\n### Production Stack Template\n\n```bicep\n// main.bicep\ntargetScope = 'subscription'\n\n@description('Environment name')\n@allowed([\n  'dev'\n  'staging'\n  'production'\n])\nparam environment string = 'production'\n\n@description('Primary location')\nparam location string = 'eastus'\n\n@description('Secondary location for geo-replication')\nparam secondaryLocation string = 'westus'\n\n// Resource naming\nvar namingPrefix = 'myapp-${environment}'\n\n// Resource Group for core infrastructure\nresource coreRG 'Microsoft.Resources/resourceGroups@2024-03-01' = {\n  name: '${namingPrefix}-core-rg'\n  location: location\n  tags: {\n    Environment: environment\n    ManagedBy: 'DeploymentStack'\n    Purpose: 'Core Infrastructure'\n  }\n}\n\n// Resource Group for data services\nresource dataRG 'Microsoft.Resources/resourceGroups@2024-03-01' = {\n  name: '${namingPrefix}-data-rg'\n  location: location\n  tags: {\n    Environment: environment\n    ManagedBy: 'DeploymentStack'\n    Purpose: 'Data Services'\n  }\n}\n\n// Log Analytics Workspace\nmodule logAnalytics 'modules/log-analytics.bicep' = {\n  name: 'logAnalyticsDeploy'\n  scope: coreRG\n  params: {\n    name: '${namingPrefix}-logs'\n    location: location\n    retentionInDays: environment == 'production' ? 90 : 30\n  }\n}\n\n// AKS Automatic Cluster\nmodule aksCluster 'modules/aks-automatic.bicep' = {\n  name: 'aksClusterDeploy'\n  scope: coreRG\n  params: {\n    name: '${namingPrefix}-aks'\n    location: location\n    kubernetesVersion: '1.34'\n    workspaceId: logAnalytics.outputs.workspaceId\n    enableZoneRedundancy: environment == 'production'\n  }\n}\n\n// Container Apps Environment\nmodule containerEnv 'modules/container-env.bicep' = {\n  name: 'containerEnvDeploy'\n  scope: coreRG\n  params: {\n    name: '${namingPrefix}-containerenv'\n    location: location\n    workspaceId: logAnalytics.outputs.workspaceId\n    zoneRedundant: environment == 'production'\n  }\n}\n\n// Azure OpenAI\nmodule openAI 'modules/openai.bicep' = {\n  name: 'openAIDeploy'\n  scope: dataRG\n  params: {\n    name: '${namingPrefix}-openai'\n    location: location\n    deployGPT5: environment == 'production'\n  }\n}\n\n// Cosmos DB with geo-replication\nmodule cosmosDB 'modules/cosmos-db.bicep' = {\n  name: 'cosmosDBDeploy'\n  scope: dataRG\n  params: {\n    name: '${namingPrefix}-cosmos'\n    primaryLocation: location\n    secondaryLocation: secondaryLocation\n    enableAutomaticFailover: environment == 'production'\n  }\n}\n\n// Key Vault\nmodule keyVault 'modules/key-vault.bicep' = {\n  name: 'keyVaultDeploy'\n  scope: coreRG\n  params: {\n    name: '${namingPrefix}-kv'\n    location: location\n    enablePurgeProtection: environment == 'production'\n  }\n}\n\n// Outputs\noutput aksClusterName string = aksCluster.outputs.clusterName\noutput containerEnvId string = containerEnv.outputs.environmentId\noutput openAIEndpoint string = openAI.outputs.endpoint\noutput cosmosDBEndpoint string = cosmosDB.outputs.endpoint\noutput keyVaultUri string = keyVault.outputs.vaultUri\n```\n\n### AKS Automatic Module\n\n```bicep\n// modules/aks-automatic.bicep\n@description('Cluster name')\nparam name string\n\n@description('Location')\nparam location string\n\n@description('Kubernetes version')\nparam kubernetesVersion string = '1.34'\n\n@description('Log Analytics workspace ID')\nparam workspaceId string\n\n@description('Enable zone redundancy')\nparam enableZoneRedundancy bool = true\n\nresource aksCluster 'Microsoft.ContainerService/managedClusters@2025-01-01' = {\n  name: name\n  location: location\n  sku: {\n    name: 'Automatic'\n    tier: 'Standard'\n  }\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    kubernetesVersion: kubernetesVersion\n    dnsPrefix: '${name}-dns'\n    enableRBAC: true\n    aadProfile: {\n      managed: true\n      enableAzureRBAC: true\n    }\n    networkProfile: {\n      networkPlugin: 'azure'\n      networkPluginMode: 'overlay'\n      networkDataplane: 'cilium'\n      serviceCidr: '10.0.0.0/16'\n      dnsServiceIP: '10.0.0.10'\n    }\n    autoScalerProfile: {\n      'balance-similar-node-groups': 'true'\n      expander: 'least-waste'\n    }\n    autoUpgradeProfile: {\n      upgradeChannel: 'stable'\n      nodeOSUpgradeChannel: 'NodeImage'\n    }\n    securityProfile: {\n      defender: {\n        securityMonitoring: {\n          enabled: true\n        }\n      }\n      workloadIdentity: {\n        enabled: true\n      }\n    }\n    oidcIssuerProfile: {\n      enabled: true\n    }\n    addonProfiles: {\n      omsagent: {\n        enabled: true\n        config: {\n          logAnalyticsWorkspaceResourceID: workspaceId\n        }\n      }\n      azurePolicy: {\n        enabled: true\n      }\n    }\n  }\n  zones: enableZoneRedundancy ? ['1', '2', '3'] : null\n}\n\noutput clusterName string = aksCluster.name\noutput clusterId string = aksCluster.id\noutput oidcIssuerUrl string = aksCluster.properties.oidcIssuerProfile.issuerUrl\noutput kubeletIdentity string = aksCluster.properties.identityProfile.kubeletidentity.objectId\n```\n\n## Managing Deployment Stacks\n\n### Update Stack\n\n```bash\n# Update with new template version\naz stack sub update \\\n  --name MyProductionStack \\\n  --template-file main.bicep \\\n  --parameters @parameters.json \\\n  --action-on-unmanage deleteAll\n\n# Update deny settings\naz stack sub update \\\n  --name MyProductionStack \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --deny-settings-excluded-principals <new-principal-id>\n```\n\n### View Stack Details\n\n```bash\n# Show stack information\naz stack sub show \\\n  --name MyProductionStack \\\n  --output json\n\n# List all stacks in subscription\naz stack sub list --output table\n\n# List stacks in resource group\naz stack group list \\\n  --resource-group MyRG \\\n  --output table\n```\n\n### Export Stack Template\n\n```bash\n# Export template from deployed stack\naz stack sub export \\\n  --name MyProductionStack \\\n  --output-file exported-stack.json\n\n# Export and save parameters\naz stack sub show \\\n  --name MyProductionStack \\\n  --query \"parameters\" \\\n  --output json > parameters-backup.json\n```\n\n### Delete Stack\n\n```bash\n# Delete stack and all managed resources\naz stack sub delete \\\n  --name MyProductionStack \\\n  --action-on-unmanage deleteAll \\\n  --yes\n\n# Delete stack but keep resources\naz stack sub delete \\\n  --name MyProductionStack \\\n  --action-on-unmanage detachAll \\\n  --yes\n\n# Delete with confirmation prompt\naz stack sub delete --name MyProductionStack\n```\n\n## Deny Settings in Detail\n\n### DenyDelete Mode\n\nPrevents deletion but allows updates:\n\n```bash\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --deny-settings-mode DenyDelete \\\n  --deny-settings-excluded-principals \\\n    <emergency-access-principal-id> \\\n    <devops-service-principal-id>\n```\n\n**Use cases:**\n- Protect production databases\n- Prevent accidental resource deletion\n- Allow configuration updates\n\n### DenyWriteAndDelete Mode\n\nPrevents both updates and deletions:\n\n```bash\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --deny-settings-excluded-principals <break-glass-principal-id>\n```\n\n**Use cases:**\n- Immutable infrastructure\n- Compliance requirements\n- Critical production workloads\n\n### Excluded Principals\n\nBypass deny settings for specific identities:\n\n```bash\n# Get principal IDs\nSERVICE_PRINCIPAL_ID=$(az ad sp show --id <app-id> --query id -o tsv)\nADMIN_GROUP_ID=$(az ad group show --group \"Cloud Admins\" --query id -o tsv)\n\n# Apply with exclusions\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --deny-settings-excluded-principals $SERVICE_PRINCIPAL_ID $ADMIN_GROUP_ID\n```\n\n## ActionOnUnmanage Policies\n\n### detachAll\n\nResources are removed from stack management but not deleted:\n\n```bash\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --action-on-unmanage detachAll\n```\n\n**Use when:**\n- Testing deployment changes\n- Migrating resources to another stack\n- Temporary stack management\n\n### deleteAll\n\nAll unmanaged resources are deleted:\n\n```bash\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --action-on-unmanage deleteAll\n```\n\n**Use when:**\n- Ephemeral environments (dev, test)\n- Clean slate deployments\n- Strict infrastructure-as-code enforcement\n\n### deleteResources\n\nDelete resources but keep resource groups:\n\n```bash\naz stack sub create \\\n  --name MyStack \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --action-on-unmanage deleteResources\n```\n\n## RBAC for Deployment Stacks\n\n### Built-in Roles\n\n**Azure Deployment Stack Contributor**\n- Manage deployment stacks\n- Cannot create or delete deny-assignments\n\n**Azure Deployment Stack Owner**\n- Full stack management\n- Can create and delete deny-assignments\n\n### Assign Roles\n\n```bash\n# Assign Stack Contributor role\naz role assignment create \\\n  --assignee <user-or-service-principal-id> \\\n  --role \"Azure Deployment Stack Contributor\" \\\n  --scope /subscriptions/<subscription-id>\n\n# Assign Stack Owner role\naz role assignment create \\\n  --assignee <admin-principal-id> \\\n  --role \"Azure Deployment Stack Owner\" \\\n  --scope /subscriptions/<subscription-id>\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: Deploy Deployment Stack\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Azure Login\n        uses: azure/login@v2\n        with:\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n\n      - name: What-if Analysis\n        run: |\n          az stack sub what-if \\\n            --name MyProductionStack \\\n            --location eastus \\\n            --template-file main.bicep \\\n            --parameters @parameters.json\n\n      - name: Deploy Stack\n        run: |\n          az stack sub create \\\n            --name MyProductionStack \\\n            --location eastus \\\n            --template-file main.bicep \\\n            --parameters @parameters.json \\\n            --deny-settings-mode DenyWriteAndDelete \\\n            --deny-settings-excluded-principals ${{ secrets.DEVOPS_PRINCIPAL_ID }} \\\n            --action-on-unmanage deleteAll \\\n            --yes\n```\n\n### Azure DevOps Pipeline\n\n```yaml\ntrigger:\n  branches:\n    include:\n      - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nvariables:\n  azureSubscription: 'MyAzureConnection'\n  stackName: 'MyProductionStack'\n  location: 'eastus'\n\nsteps:\n  - task: AzureCLI@2\n    displayName: 'What-if Analysis'\n    inputs:\n      azureSubscription: $(azureSubscription)\n      scriptType: 'bash'\n      scriptLocation: 'inlineScript'\n      inlineScript: |\n        az stack sub what-if \\\n          --name $(stackName) \\\n          --location $(location) \\\n          --template-file main.bicep \\\n          --parameters @parameters.json\n\n  - task: AzureCLI@2\n    displayName: 'Deploy Stack'\n    inputs:\n      azureSubscription: $(azureSubscription)\n      scriptType: 'bash'\n      scriptLocation: 'inlineScript'\n      inlineScript: |\n        az stack sub create \\\n          --name $(stackName) \\\n          --location $(location) \\\n          --template-file main.bicep \\\n          --parameters @parameters.json \\\n          --deny-settings-mode DenyWriteAndDelete \\\n          --action-on-unmanage deleteAll \\\n          --yes\n```\n\n## Monitoring and Auditing\n\n### View Stack Events\n\n```bash\n# Get deployment operations\naz stack sub show \\\n  --name MyProductionStack \\\n  --query \"deploymentId\" \\\n  --output tsv | \\\n  xargs -I {} az deployment sub show --name {}\n\n# List managed resources\naz stack sub show \\\n  --name MyProductionStack \\\n  --query \"resources[].id\" \\\n  --output table\n```\n\n### Activity Logs\n\n```bash\n# Query stack operations\naz monitor activity-log list \\\n  --resource-group MyRG \\\n  --namespace Microsoft.Resources \\\n  --start-time 2025-01-01T00:00:00Z \\\n  --query \"[?contains(authorization.action, 'Microsoft.Resources/deploymentStacks')]\" \\\n  --output table\n```\n\n## Migration from Azure Blueprints\n\n### Assessment\n\n1. **Inventory Blueprints**: List all blueprints and assignments\n2. **Document Parameters**: Export parameters and configurations\n3. **Plan Conversion**: Map blueprints to deployment stacks\n4. **Test in Dev**: Validate converted templates\n\n### Conversion Steps\n\n```bash\n# 1. Export Blueprint as ARM template\n# (Use Azure Portal or PowerShell)\n\n# 2. Convert ARM to Bicep\naz bicep decompile --file blueprint-template.json\n\n# 3. Create Deployment Stack\naz stack sub create \\\n  --name ConvertedFromBlueprint \\\n  --location eastus \\\n  --template-file converted.bicep \\\n  --parameters @blueprint-parameters.json \\\n  --deny-settings-mode DenyWriteAndDelete \\\n  --action-on-unmanage detachAll\n\n# 4. Validate resources\naz stack sub show --name ConvertedFromBlueprint\n\n# 5. Delete Blueprint assignment (after validation)\n# Remove-AzBlueprintAssignment -Name MyBlueprintAssignment\n```\n\n## Best Practices\n\n **Use Deployment Stacks for all new infrastructure**\n **Always run what-if analysis before deployment**\n **Use DenyWriteAndDelete for production stacks**\n **Exclude break-glass principals from deny settings**\n **Tag stacks with Environment, CostCenter, Owner**\n **Use deleteAll for ephemeral environments**\n **Use detachAll for migration scenarios**\n **Implement CI/CD pipelines for stack deployment**\n **Monitor stack operations via activity logs**\n **Document stack architecture and dependencies**\n\n## Troubleshooting\n\n### Stack Creation Fails\n\n```bash\n# Check deployment errors\naz stack sub show \\\n  --name MyStack \\\n  --query \"error\" \\\n  --output json\n\n# Validate template\naz deployment sub validate \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --parameters @parameters.json\n```\n\n### Deny Settings Blocking Operations\n\n```bash\n# Check deny assignments\naz role assignment list \\\n  --scope /subscriptions/<subscription-id> \\\n  --include-inherited \\\n  --query \"[?type=='Microsoft.Authorization/denyAssignments']\"\n\n# Add principal to exclusions\naz stack sub update \\\n  --name MyStack \\\n  --deny-settings-excluded-principals <new-principal-id>\n```\n\n### Resources Not Deleted\n\n```bash\n# Check action-on-unmanage setting\naz stack sub show \\\n  --name MyStack \\\n  --query \"actionOnUnmanage\" \\\n  --output tsv\n\n# Update to deleteAll\naz stack sub update \\\n  --name MyStack \\\n  --action-on-unmanage deleteAll\n```\n\n## References\n\n- [Deployment Stacks Documentation](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/deployment-stacks)\n- [Deployment Stacks Quickstart](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/quickstart-create-deployment-stacks)\n- [Migrate from Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/how-to/migrate-to-deployment-stacks)\n\nDeployment Stacks represents the future of Azure infrastructure lifecycle management!"
              }
            ]
          },
          {
            "name": "azure-to-docker-master",
            "description": "Complete Azure-to-Docker migration system for local development with 2025 features. PROACTIVELY activate for: (1) ANY Azure-to-Docker migration task, (2) Azure infrastructure extraction and Docker Compose generation, (3) Azure service emulator setup (Azurite 2025-11-05 API, SQL Server 2025 latest, Cosmos DB vnext Linux, Service Bus official emulator), (4) Local development with Docker Compose Watch mode (hot reload), (5) Database export from Azure SQL/PostgreSQL/MySQL to Docker, (6) Dockerfile generation from Azure App Service configurations, (7) Multi-container orchestration with proper networking and dependencies, (8) Production-ready Docker Compose with health checks and runtime secrets, (9) Azure service mapping (App Service/SQL/Storage/Redis/Cosmos/Service Bus), (10) Development-to-production parity with Azure emulators. Provides: Azure resource extraction and analysis, complete Docker Compose generation with 2025 best practices, Azure emulator configuration (Azurite with latest API, SQL Server 2025 with Vector Search, Cosmos DB vnext Linux-based, official Service Bus emulator), Docker Compose Watch mode for hot reload, database export automation, App Service to Dockerfile conversion, service dependency mapping, network isolation patterns, volume management strategies, environment variable templating, health check implementation, resource limit configuration, security hardening (non-root users, read-only filesystems, capability drops, runtime-only secrets), development override patterns with watch mode, and Azure-to-Docker best practices. Ensures production-ready local development environments that mirror Azure infrastructure with instant hot reload capabilities.",
            "source": "./plugins/azure-to-docker-master",
            "category": null,
            "version": "1.1.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install azure-to-docker-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/export-database",
                "description": "Export Azure SQL/PostgreSQL/MySQL databases for local Docker containers",
                "path": "plugins/azure-to-docker-master/commands/export-database.md",
                "frontmatter": {
                  "description": "Export Azure SQL/PostgreSQL/MySQL databases for local Docker containers"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Export Azure Databases to Docker\n\n## Purpose\nExport databases from Azure (SQL Database, PostgreSQL, MySQL) and import them into local Docker containers for development.\n\n## Prerequisites\n\n**Required tools:**\n- Azure CLI (`az`) installed and authenticated\n- Docker Desktop 4.40+ with Compose v2.42+\n- Database-specific CLI tools:\n  - SQL Server: `sqlcmd` (mssql-tools18)\n  - PostgreSQL: `psql`, `pg_dump`\n  - MySQL: `mysql`, `mysqldump`\n\n**Azure access:**\n- Read permissions on databases\n- Network access (firewall rules configured)\n- Valid credentials\n\n## Step 1: Configure Azure Firewall Rules\n\n**Add your IP to Azure SQL firewall:**\n```bash\n# Get your public IP\nMY_IP=$(curl -s ifconfig.me)\n\n# Add firewall rule (SQL Server)\naz sql server firewall-rule create \\\n  --resource-group <resource-group> \\\n  --server <server-name> \\\n  --name AllowMyIP \\\n  --start-ip-address $MY_IP \\\n  --end-ip-address $MY_IP\n\n# PostgreSQL\naz postgres flexible-server firewall-rule create \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --rule-name AllowMyIP \\\n  --start-ip-address $MY_IP \\\n  --end-ip-address $MY_IP\n\n# MySQL\naz mysql flexible-server firewall-rule create \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --rule-name AllowMyIP \\\n  --start-ip-address $MY_IP\n```\n\n## Step 2: Get Connection Information\n\n**Azure SQL Database:**\n```bash\n# Get connection string\naz sql db show-connection-string \\\n  --client sqlcmd \\\n  --name <database-name> \\\n  --server <server-name>\n\n# Output format:\n# sqlcmd -S <server-name>.database.windows.net -d <database-name> -U <username> -P <password> -N -l 30\n```\n\n**PostgreSQL:**\n```bash\n# Get server details\naz postgres flexible-server show \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --query \"{fqdn:fullyQualifiedDomainName, version:version}\" \\\n  --output table\n\n# Connection string format:\n# postgresql://<username>:<password>@<server-name>.postgres.database.azure.com:5432/<database-name>?sslmode=require\n```\n\n**MySQL:**\n```bash\n# Get server details\naz mysql flexible-server show \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --query \"{fqdn:fullyQualifiedDomainName, version:version}\" \\\n  --output table\n\n# Connection string format:\n# mysql://<username>:<password>@<server-name>.mysql.database.azure.com:3306/<database-name>?ssl-mode=REQUIRED\n```\n\n## Step 3: Export Database\n\n### Azure SQL Database\n\n**Option 1: Using Azure CLI (BACPAC):**\n```bash\n# Export to Azure Storage\naz sql db export \\\n  --resource-group <resource-group> \\\n  --server <server-name> \\\n  --name <database-name> \\\n  --admin-user <username> \\\n  --admin-password <password> \\\n  --storage-key-type StorageAccessKey \\\n  --storage-key <storage-key> \\\n  --storage-uri https://<storage-account>.blob.core.windows.net/<container>/<database-name>.bacpac\n\n# Download BACPAC file\naz storage blob download \\\n  --account-name <storage-account> \\\n  --container-name <container> \\\n  --name <database-name>.bacpac \\\n  --file ./<database-name>.bacpac\n```\n\n**Option 2: Using sqlcmd (SQL script):**\n```bash\n# Install mssql-tools18 if needed\n# Windows: winget install Microsoft.SqlCmd\n# Linux: https://learn.microsoft.com/en-us/sql/linux/sql-server-linux-setup-tools\n\n# Generate schema + data script\nsqlcmd -S <server-name>.database.windows.net \\\n  -d <database-name> \\\n  -U <username> \\\n  -P <password> \\\n  -C \\\n  -Q \"SELECT * FROM INFORMATION_SCHEMA.TABLES\" \\\n  -o schema-info.txt\n\n# For full export, use SQL Server Management Studio or Azure Data Studio\n# Export wizard: Tasks  Generate Scripts  Script entire database\n```\n\n**Option 3: Using SqlPackage (recommended for large databases):**\n```bash\n# Install SqlPackage: https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-download\n\n# Export as BACPAC\nsqlpackage /Action:Export \\\n  /SourceServerName:<server-name>.database.windows.net \\\n  /SourceDatabaseName:<database-name> \\\n  /SourceUser:<username> \\\n  /SourcePassword:<password> \\\n  /SourceTrustServerCertificate:True \\\n  /TargetFile:./<database-name>.bacpac\n\n# Or export as DACPAC (schema only)\nsqlpackage /Action:Extract \\\n  /SourceServerName:<server-name>.database.windows.net \\\n  /SourceDatabaseName:<database-name> \\\n  /SourceUser:<username> \\\n  /SourcePassword:<password> \\\n  /SourceTrustServerCertificate:True \\\n  /TargetFile:./<database-name>.dacpac\n```\n\n### PostgreSQL\n\n**Using pg_dump:**\n```bash\n# Export entire database (schema + data)\npg_dump -h <server-name>.postgres.database.azure.com \\\n  -U <username> \\\n  -d <database-name> \\\n  -F c \\\n  -f <database-name>.dump\n\n# Or as SQL script\npg_dump -h <server-name>.postgres.database.azure.com \\\n  -U <username> \\\n  -d <database-name> \\\n  --clean \\\n  --if-exists \\\n  -f <database-name>.sql\n\n# Schema only\npg_dump -h <server-name>.postgres.database.azure.com \\\n  -U <username> \\\n  -d <database-name> \\\n  --schema-only \\\n  -f <database-name>-schema.sql\n\n# Data only\npg_dump -h <server-name>.postgres.database.azure.com \\\n  -U <username> \\\n  -d <database-name> \\\n  --data-only \\\n  -f <database-name>-data.sql\n```\n\n### MySQL\n\n**Using mysqldump:**\n```bash\n# Export entire database\nmysqldump -h <server-name>.mysql.database.azure.com \\\n  -u <username> \\\n  -p<password> \\\n  --ssl-mode=REQUIRED \\\n  --databases <database-name> \\\n  --single-transaction \\\n  --routines \\\n  --triggers \\\n  > <database-name>.sql\n\n# Schema only\nmysqldump -h <server-name>.mysql.database.azure.com \\\n  -u <username> \\\n  -p<password> \\\n  --ssl-mode=REQUIRED \\\n  --no-data \\\n  --databases <database-name> \\\n  > <database-name>-schema.sql\n\n# Data only\nmysqldump -h <server-name>.mysql.database.azure.com \\\n  -u <username> \\\n  -p<password> \\\n  --ssl-mode=REQUIRED \\\n  --no-create-info \\\n  --databases <database-name> \\\n  > <database-name>-data.sql\n```\n\n## Step 4: Prepare Local Docker Containers\n\nEnsure Docker Compose is configured with database services.\n\n**SQL Server container:**\n```yaml\nservices:\n  sqlserver:\n    image: mcr.microsoft.com/mssql/server:2025-latest\n    environment:\n      - ACCEPT_EULA=Y\n      - MSSQL_PID=Developer\n      - MSSQL_SA_PASSWORD=YourStrong!Passw0rd\n    ports:\n      - \"1433:1433\"\n    volumes:\n      - sqlserver-data:/var/opt/mssql\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P $$MSSQL_SA_PASSWORD -Q 'SELECT 1' -C || exit 1\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n```\n\n**PostgreSQL container:**\n```yaml\nservices:\n  postgres:\n    image: postgres:16-alpine\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres123\n      - POSTGRES_DB=myapp\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n```\n\n**MySQL container:**\n```yaml\nservices:\n  mysql:\n    image: mysql:8.4\n    environment:\n      - MYSQL_ROOT_PASSWORD=mysql123\n      - MYSQL_DATABASE=myapp\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \"-p$$MYSQL_ROOT_PASSWORD\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n```\n\n## Step 5: Start Docker Containers\n\n```bash\n# Start database containers\ndocker compose up -d sqlserver postgres mysql\n\n# Wait for health checks to pass\ndocker compose ps\n\n# Check logs\ndocker compose logs sqlserver\n```\n\n## Step 6: Import Data into Docker Containers\n\n### SQL Server\n\n**Using sqlcmd:**\n```bash\n# Import SQL script\nsqlcmd -S localhost,1433 \\\n  -U sa \\\n  -P YourStrong!Passw0rd \\\n  -C \\\n  -i <database-name>.sql\n\n# Or execute via docker exec\ndocker compose exec sqlserver /opt/mssql-tools18/bin/sqlcmd \\\n  -S localhost \\\n  -U sa \\\n  -P YourStrong!Passw0rd \\\n  -C \\\n  -i /tmp/<database-name>.sql\n```\n\n**Using SqlPackage (BACPAC):**\n```bash\n# Import BACPAC\nsqlpackage /Action:Import \\\n  /SourceFile:./<database-name>.bacpac \\\n  /TargetServerName:localhost \\\n  /TargetDatabaseName:<database-name> \\\n  /TargetUser:sa \\\n  /TargetPassword:YourStrong!Passw0rd \\\n  /TargetTrustServerCertificate:True\n\n# Or via docker exec\ndocker cp <database-name>.bacpac sqlserver:/tmp/\ndocker compose exec sqlserver /opt/sqlpackage/sqlpackage \\\n  /Action:Import \\\n  /SourceFile:/tmp/<database-name>.bacpac \\\n  /TargetServerName:localhost \\\n  /TargetDatabaseName:<database-name> \\\n  /TargetUser:sa \\\n  /TargetPassword:YourStrong!Passw0rd \\\n  /TargetTrustServerCertificate:True\n```\n\n### PostgreSQL\n\n**Using psql:**\n```bash\n# Import SQL script\npsql -h localhost \\\n  -U postgres \\\n  -d myapp \\\n  -f <database-name>.sql\n\n# Or with custom-format dump\npg_restore -h localhost \\\n  -U postgres \\\n  -d myapp \\\n  -v \\\n  <database-name>.dump\n\n# Via docker exec\ndocker cp <database-name>.sql postgres:/tmp/\ndocker compose exec postgres psql \\\n  -U postgres \\\n  -d myapp \\\n  -f /tmp/<database-name>.sql\n```\n\n### MySQL\n\n**Using mysql:**\n```bash\n# Import SQL script\nmysql -h localhost \\\n  -u root \\\n  -pmysql123 \\\n  < <database-name>.sql\n\n# Via docker exec\ndocker cp <database-name>.sql mysql:/tmp/\ndocker compose exec mysql mysql \\\n  -u root \\\n  -pmysql123 \\\n  < /tmp/<database-name>.sql\n```\n\n## Step 7: Verify Import\n\n**SQL Server:**\n```bash\nsqlcmd -S localhost,1433 -U sa -P YourStrong!Passw0rd -C -Q \"SELECT name FROM sys.databases\"\nsqlcmd -S localhost,1433 -U sa -P YourStrong!Passw0rd -C -Q \"USE <database-name>; SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES\"\n```\n\n**PostgreSQL:**\n```bash\ndocker compose exec postgres psql -U postgres -d myapp -c \"\\dt\"\ndocker compose exec postgres psql -U postgres -d myapp -c \"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'\"\n```\n\n**MySQL:**\n```bash\ndocker compose exec mysql mysql -u root -pmysql123 -e \"SHOW DATABASES\"\ndocker compose exec mysql mysql -u root -pmysql123 myapp -e \"SHOW TABLES\"\n```\n\n## Step 8: Automate with Init Scripts\n\nPlace SQL files in `./init/` directory for automatic import on container startup.\n\n**SQL Server init script (init/01-create-database.sql):**\n```sql\n-- Wait for SQL Server to be ready\nWAITFOR DELAY '00:00:10';\nGO\n\n-- Create database if not exists\nIF NOT EXISTS (SELECT * FROM sys.databases WHERE name = 'MyApp')\nBEGIN\n    CREATE DATABASE MyApp;\nEND\nGO\n\nUSE MyApp;\nGO\n\n-- Your schema and data here\nCREATE TABLE Users (\n    Id INT PRIMARY KEY IDENTITY(1,1),\n    Username NVARCHAR(100) NOT NULL,\n    Email NVARCHAR(255) NOT NULL\n);\nGO\n```\n\n**PostgreSQL init script (init/01-init.sql):**\n```sql\n-- Runs automatically on first container start\nCREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(100) NOT NULL,\n    email VARCHAR(255) NOT NULL\n);\n\nINSERT INTO users (username, email) VALUES\n    ('admin', 'admin@example.com'),\n    ('user', 'user@example.com');\n```\n\n**MySQL init script (init/01-init.sql):**\n```sql\n-- Runs automatically on first container start\nCREATE TABLE IF NOT EXISTS users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    username VARCHAR(100) NOT NULL,\n    email VARCHAR(255) NOT NULL\n);\n\nINSERT INTO users (username, email) VALUES\n    ('admin', 'admin@example.com'),\n    ('user', 'user@example.com');\n```\n\n## Step 9: Handle Large Databases\n\nFor databases > 10GB:\n\n1. **Use compression:**\n   ```bash\n   pg_dump -h <server> -U <user> -d <db> -F c -Z 9 -f <db>.dump.gz\n   ```\n\n2. **Export schema separately:**\n   ```bash\n   pg_dump -h <server> -U <user> -d <db> --schema-only -f schema.sql\n   ```\n\n3. **Export data in chunks:**\n   ```bash\n   pg_dump -h <server> -U <user> -d <db> -t table1 --data-only -f table1.sql\n   pg_dump -h <server> -U <user> -d <db> -t table2 --data-only -f table2.sql\n   ```\n\n4. **Use parallel export (PostgreSQL):**\n   ```bash\n   pg_dump -h <server> -U <user> -d <db> -F d -j 4 -f ./dump_directory\n   ```\n\n5. **Consider subset of data for development:**\n   ```sql\n   -- Export last 6 months only\n   pg_dump -h <server> -U <user> -d <db> \\\n     -t table1 \\\n     --where=\"created_at > NOW() - INTERVAL '6 months'\" \\\n     -f subset.sql\n   ```\n\n## Step 10: Clean Up Azure Resources\n\nRemove firewall rules after export:\n\n```bash\n# SQL Server\naz sql server firewall-rule delete \\\n  --resource-group <resource-group> \\\n  --server <server-name> \\\n  --name AllowMyIP\n\n# PostgreSQL\naz postgres flexible-server firewall-rule delete \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --rule-name AllowMyIP\n\n# MySQL\naz mysql flexible-server firewall-rule delete \\\n  --resource-group <resource-group> \\\n  --name <server-name> \\\n  --rule-name AllowMyIP\n```\n\n## Common Issues and Solutions\n\n**Connection timeout:**\n- Verify firewall rules include your IP\n- Check network connectivity to Azure\n- Ensure NSG rules allow database traffic\n\n**Authentication failed:**\n- Verify username format (PostgreSQL/MySQL require `user@server-name`)\n- Check password for special characters (escape in shell)\n- Ensure Azure AD authentication is not required\n\n**BACPAC import fails:**\n- Check SQL Server version compatibility\n- Ensure sufficient disk space in Docker volume\n- Review error messages for missing dependencies\n\n**Large file transfer fails:**\n- Use compression\n- Split into multiple files\n- Consider Azure Data Factory for large datasets\n\n**Schema compatibility issues:**\n- Azure SQL  SQL Server 2025: Generally compatible\n- Check for Azure-specific features (elastic pools, etc.)\n- Test import in non-production environment first\n\n## Best Practices\n\n1. **Use separate init scripts for schema and data**\n2. **Version control schema scripts**\n3. **Exclude sensitive data from exports**\n4. **Test import process before full migration**\n5. **Document any manual adjustments needed**\n6. **Use environment variables for credentials**\n7. **Automate with CI/CD pipelines**\n8. **Keep export files secure (gitignore)**\n9. **Regularly refresh local data from Azure**\n10. **Consider using sample data for local development**\n\n## Automation Script Example\n\nCreate `scripts/export-and-import.sh`:\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\n# Configuration\nAZURE_SERVER=\"myserver.database.windows.net\"\nAZURE_DB=\"myapp\"\nAZURE_USER=\"admin\"\nAZURE_PASS=\"${AZURE_SQL_PASSWORD}\"\n\necho \"Exporting from Azure...\"\npg_dump -h \"$AZURE_SERVER\" -U \"$AZURE_USER\" -d \"$AZURE_DB\" -F c -f ./dump.sql\n\necho \"Starting Docker container...\"\ndocker compose up -d postgres\nsleep 10\n\necho \"Importing to Docker...\"\ndocker cp ./dump.sql postgres:/tmp/\ndocker compose exec -T postgres pg_restore -U postgres -d myapp -v /tmp/dump.sql\n\necho \"Verifying import...\"\ndocker compose exec -T postgres psql -U postgres -d myapp -c \"\\dt\"\n\necho \"Done! Database ready for local development.\"\n```\n\n## Output Deliverables\n\nProvide:\n1. Database export files (.sql, .bacpac, .dump)\n2. Import scripts for Docker containers\n3. Init directory structure for auto-loading\n4. Verification queries\n5. Documentation of any schema changes needed\n6. Connection string examples for local development\n\n## Next Steps\n\nAfter importing databases:\n1. Update application connection strings\n2. Test application against local databases\n3. Verify all tables and data imported correctly\n4. Document any Azure-specific features not available locally\n5. Set up regular refresh process from Azure"
              },
              {
                "name": "/extract-infrastructure",
                "description": "Extract Azure infrastructure and generate Docker Compose stack for local development",
                "path": "plugins/azure-to-docker-master/commands/extract-infrastructure.md",
                "frontmatter": {
                  "description": "Extract Azure infrastructure and generate Docker Compose stack for local development"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Extract Azure Infrastructure to Docker Compose\n\n## Purpose\nAnalyze existing Azure infrastructure and generate a complete Docker Compose stack with Azure service emulators for local development.\n\n## Prerequisites\n\n**Required tools:**\n- Azure CLI (`az`) installed and configured\n- Docker Desktop 4.40+ with Compose v2.42+\n- Sufficient local resources (minimum 8GB RAM for full Azure stack)\n\n**Azure access:**\n- Authenticated with `az login`\n- Appropriate RBAC permissions to read resources\n- Access to target resource group\n\n## Step 1: Authenticate with Azure\n\n```bash\n# Login to Azure\naz login\n\n# List available subscriptions\naz account list --output table\n\n# Set target subscription\naz account set --subscription \"subscription-name-or-id\"\n\n# Verify current subscription\naz account show\n```\n\n## Step 2: Extract Azure Resources\n\n### List Resources in Resource Group\n\n```bash\n# List all resources in resource group\naz resource list \\\n  --resource-group <resource-group-name> \\\n  --output table\n\n# Get detailed JSON output for analysis\naz resource list \\\n  --resource-group <resource-group-name> \\\n  --output json > azure-resources.json\n```\n\n### Extract Specific Service Configurations\n\n**App Services:**\n```bash\n# List App Services\naz webapp list \\\n  --resource-group <resource-group-name> \\\n  --output json > app-services.json\n\n# Get detailed configuration for each app\naz webapp show \\\n  --name <app-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > app-<app-name>.json\n\n# Get application settings (environment variables)\naz webapp config appsettings list \\\n  --name <app-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > app-<app-name>-settings.json\n\n# Get connection strings\naz webapp config connection-string list \\\n  --name <app-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > app-<app-name>-connections.json\n```\n\n**Azure SQL Databases:**\n```bash\n# List SQL servers\naz sql server list \\\n  --resource-group <resource-group-name> \\\n  --output json > sql-servers.json\n\n# List databases on server\naz sql db list \\\n  --server <server-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > sql-databases.json\n\n# Get database details\naz sql db show \\\n  --name <database-name> \\\n  --server <server-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > sql-db-<database-name>.json\n```\n\n**PostgreSQL/MySQL:**\n```bash\n# PostgreSQL\naz postgres flexible-server list \\\n  --resource-group <resource-group-name> \\\n  --output json > postgres-servers.json\n\naz postgres flexible-server db list \\\n  --server-name <server-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > postgres-databases.json\n\n# MySQL\naz mysql flexible-server list \\\n  --resource-group <resource-group-name> \\\n  --output json > mysql-servers.json\n```\n\n**Redis Cache:**\n```bash\naz redis list \\\n  --resource-group <resource-group-name> \\\n  --output json > redis-caches.json\n\naz redis show \\\n  --name <redis-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > redis-<redis-name>.json\n```\n\n**Storage Accounts:**\n```bash\naz storage account list \\\n  --resource-group <resource-group-name> \\\n  --output json > storage-accounts.json\n\naz storage account show \\\n  --name <storage-account-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > storage-<storage-account-name>.json\n```\n\n**Cosmos DB:**\n```bash\naz cosmosdb list \\\n  --resource-group <resource-group-name> \\\n  --output json > cosmosdb-accounts.json\n\naz cosmosdb show \\\n  --name <cosmosdb-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > cosmosdb-<cosmosdb-name>.json\n```\n\n**Service Bus:**\n```bash\naz servicebus namespace list \\\n  --resource-group <resource-group-name> \\\n  --output json > servicebus-namespaces.json\n\naz servicebus queue list \\\n  --namespace-name <namespace-name> \\\n  --resource-group <resource-group-name> \\\n  --output json > servicebus-queues.json\n```\n\n## Step 3: Analyze Extracted Resources\n\nRead all JSON files and identify:\n\n1. **Service Types and Counts**\n   - How many App Services?\n   - Database types (SQL Server, PostgreSQL, MySQL)?\n   - Cache services (Redis)?\n   - Storage requirements (Blob, Queue, Table)?\n   - NoSQL databases (Cosmos DB)?\n   - Message queues (Service Bus)?\n\n2. **Service Dependencies**\n   - Which apps connect to which databases?\n   - Connection strings and relationships\n   - Network configurations\n   - Authentication methods\n\n3. **Configuration Requirements**\n   - Environment variables from app settings\n   - Connection strings\n   - Feature flags\n   - Secrets (need local equivalents)\n\n4. **Resource Sizing**\n   - Database SKUs  Docker resource limits\n   - App Service plans  Container CPU/memory\n   - Storage capacity  Volume sizing\n\n## Step 4: Map Azure Services to Docker\n\nUse this mapping table:\n\n| Azure Service | Docker Image | Configuration Notes |\n|---------------|--------------|---------------------|\n| App Service (Windows) | Custom build | Extract runtime stack from config |\n| App Service (Linux) | Custom build | Use specified container image |\n| Azure SQL Database | `mcr.microsoft.com/mssql/server:2025-latest` | Use Developer edition |\n| PostgreSQL Flexible Server | `postgres:16-alpine` | Match version from Azure |\n| MySQL Flexible Server | `mysql:8.4` | Match version from Azure |\n| Redis Cache | `redis:7.4-alpine` | Configure persistence |\n| Storage Account (Blob/Queue/Table) | `mcr.microsoft.com/azure-storage/azurite` | All storage types in one |\n| Cosmos DB | `mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator` | NoSQL emulator |\n| Service Bus | Custom or `rabbitmq:3.14-alpine` | Limited emulator support |\n| Application Insights | `jaegertracing/all-in-one` | OpenTelemetry compatible |\n\n## Step 5: Generate Docker Compose Structure\n\nCreate `docker-compose.yml` with this structure:\n\n```yaml\n# Modern Compose format (no version field for v2.40+)\n\nservices:\n  # Frontend App Services\n  # Backend App Services\n  # Databases (SQL Server, PostgreSQL, MySQL)\n  # Cache (Redis)\n  # Storage (Azurite)\n  # NoSQL (Cosmos DB)\n  # Monitoring (Jaeger, Grafana)\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n  monitoring:\n    driver: bridge\n\nvolumes:\n  # Named volumes for each database\n  # Named volumes for storage emulators\n\nsecrets:\n  # Database passwords\n  # Connection strings\n```\n\n### Service Generation Rules\n\n**For each App Service:**\n```yaml\nservice-name:\n  build:\n    context: ./path-to-app\n    dockerfile: Dockerfile\n  ports:\n    - \"PORT:PORT\"\n  depends_on:\n    database-service:\n      condition: service_healthy\n  environment:\n    # Map from Azure app settings\n  networks:\n    - frontend\n    - backend\n  restart: unless-stopped\n  user: \"1000:1000\"\n  read_only: true\n  tmpfs:\n    - /tmp\n  cap_drop:\n    - ALL\n  cap_add:\n    - NET_BIND_SERVICE\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:PORT/health\"]\n    interval: 30s\n    timeout: 3s\n    retries: 3\n    start_period: 40s\n  deploy:\n    resources:\n      limits:\n        cpus: 'X'\n        memory: XG\n```\n\n**For Azure SQL Database:**\n```yaml\nsqlserver:\n  image: mcr.microsoft.com/mssql/server:2025-latest\n  environment:\n    - ACCEPT_EULA=Y\n    - MSSQL_PID=Developer\n    - MSSQL_SA_PASSWORD_FILE=/run/secrets/sa_password\n  secrets:\n    - sa_password\n  ports:\n    - \"1433:1433\"\n  volumes:\n    - sqlserver-data:/var/opt/mssql\n  networks:\n    - backend\n  healthcheck:\n    test: [\"CMD-SHELL\", \"/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P $$MSSQL_SA_PASSWORD -Q 'SELECT 1' -C || exit 1\"]\n    interval: 10s\n    timeout: 3s\n    retries: 3\n    start_period: 10s\n  deploy:\n    resources:\n      limits:\n        cpus: '2'\n        memory: 4G\n      reservations:\n        cpus: '1'\n        memory: 2G\n  security_opt:\n    - no-new-privileges:true\n```\n\n**For Storage Account:**\n```yaml\nazurite:\n  image: mcr.microsoft.com/azure-storage/azurite:latest\n  command: azurite --blobHost 0.0.0.0 --queueHost 0.0.0.0 --tableHost 0.0.0.0 --loose\n  ports:\n    - \"10000:10000\"  # Blob\n    - \"10001:10001\"  # Queue\n    - \"10002:10002\"  # Table\n  volumes:\n    - azurite-data:/data\n  networks:\n    - backend\n  healthcheck:\n    test: [\"CMD\", \"nc\", \"-z\", \"localhost\", \"10000\"]\n    interval: 30s\n    timeout: 3s\n    retries: 3\n  restart: unless-stopped\n```\n\n**For Redis Cache:**\n```yaml\nredis:\n  image: redis:7.4-alpine\n  command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}\n  ports:\n    - \"6379:6379\"\n  volumes:\n    - redis-data:/data\n  networks:\n    - backend\n  healthcheck:\n    test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\n    interval: 10s\n    timeout: 3s\n    retries: 3\n  security_opt:\n    - no-new-privileges:true\n```\n\n**For Cosmos DB:**\n```yaml\ncosmosdb:\n  image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest\n  environment:\n    - AZURE_COSMOS_EMULATOR_PARTITION_COUNT=10\n    - AZURE_COSMOS_EMULATOR_ENABLE_DATA_PERSISTENCE=true\n  ports:\n    - \"8081:8081\"\n    - \"10251-10254:10251-10254\"\n  volumes:\n    - cosmos-data:/data/db\n  networks:\n    - backend\n  deploy:\n    resources:\n      limits:\n        cpus: '2'\n        memory: 4G\n```\n\n## Step 6: Generate Environment Files\n\nCreate `.env.template`:\n\n```bash\n# SQL Server\nMSSQL_SA_PASSWORD=YourStrong!Passw0rd\n\n# PostgreSQL (if used)\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres123\nPOSTGRES_DB=myapp\n\n# MySQL (if used)\nMYSQL_ROOT_PASSWORD=mysql123\nMYSQL_DATABASE=myapp\n\n# Redis\nREDIS_PASSWORD=redis123\n\n# Application Settings\n# (Map from Azure app settings JSON)\nASPNETCORE_ENVIRONMENT=Development\nNODE_ENV=development\n\n# Azure Storage Emulator (Standard Development Connection String)\nAZURITE_CONNECTION_STRING=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite:10000/devstoreaccount1;QueueEndpoint=http://azurite:10001/devstoreaccount1;TableEndpoint=http://azurite:10002/devstoreaccount1;\n\n# Cosmos DB Emulator\nCOSMOS_EMULATOR_ENDPOINT=https://cosmosdb:8081\nCOSMOS_EMULATOR_KEY=C2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==\n\n# Feature Flags\nENABLE_MONITORING=true\n```\n\n## Step 7: Create Supporting Files\n\n**Makefile:**\n```makefile\n.PHONY: up down logs health restart clean\n\nup:\n\t@docker compose up -d\n\t@echo \" Services started. Access at:\"\n\t@echo \"  - Frontend: http://localhost:3000\"\n\t@echo \"  - Backend: http://localhost:8080\"\n\t@echo \"  - Azurite: http://localhost:10000\"\n\t@echo \"  - Cosmos DB: https://localhost:8081/_explorer/index.html\"\n\ndown:\n\t@docker compose down\n\nlogs:\n\t@docker compose logs -f\n\nhealth:\n\t@docker compose ps\n\nrestart:\n\t@docker compose restart\n\nclean:\n\t@docker compose down -v\n\t@echo \" Cleaned all volumes\"\n\ninit:\n\t@cp .env.template .env\n\t@echo \" Created .env file. Please update passwords!\"\n```\n\n**README.md:**\nInclude:\n- Architecture diagram of services\n- Service mapping (Azure  Docker)\n- Port mappings\n- Connection strings for local development\n- How to start/stop\n- Health check verification\n- Troubleshooting guide\n\n**docker-compose.override.yml (for development):**\n```yaml\nservices:\n  frontend:\n    volumes:\n      - ./frontend/src:/app/src:cached\n    environment:\n      - HOT_RELOAD=true\n\n  backend:\n    volumes:\n      - ./backend/src:/app/src:cached\n    ports:\n      - \"9229:9229\"  # Node.js debugger\n```\n\n## Step 8: Validation\n\nBefore finalizing, validate:\n\n1. **Syntax validation:**\n   ```bash\n   docker compose config\n   ```\n\n2. **Service startup order:**\n   - Databases start first\n   - Health checks complete before dependent services start\n   - Apps start after all dependencies are healthy\n\n3. **Network isolation:**\n   - Databases only on backend network\n   - Frontend services can't directly access databases\n   - Proper communication paths\n\n4. **Resource limits:**\n   - Total CPU allocation < host CPUs\n   - Total memory allocation < host memory\n   - Leave headroom for host OS\n\n5. **Security checks:**\n   - No hardcoded secrets in docker-compose.yml\n   - All services run as non-root where possible\n   - Read-only filesystems enabled\n   - Capabilities dropped\n\n## Output Deliverables\n\nProvide the following files:\n\n1. `docker-compose.yml` - Main compose file\n2. `docker-compose.override.yml` - Development overrides\n3. `.env.template` - Environment variable template\n4. `Makefile` - Common operations\n5. `README.md` - Setup and usage documentation\n6. `.dockerignore` - Files to exclude from builds\n7. `secrets/` directory structure (gitignored)\n\n## Common Azure Patterns\n\n### Pattern 1: Simple Web + Database\n- 1 App Service  web container\n- 1 Azure SQL  SQL Server 2025 container\n- 1 Storage Account  Azurite\n\n### Pattern 2: Three-Tier Application\n- Frontend App Service  React/Angular container\n- Backend App Service  API container\n- Azure SQL  SQL Server 2025 container\n- Redis Cache  Redis container\n- Storage Account  Azurite\n\n### Pattern 3: Microservices\n- Multiple App Services  Multiple containers\n- Azure SQL + Cosmos DB  SQL Server + Cosmos emulator\n- Service Bus  RabbitMQ\n- Application Insights  Jaeger\n- API Management  Nginx gateway\n\n### Pattern 4: Full Azure Stack\n- Multiple App Services (frontend/backend/admin)\n- Azure SQL + PostgreSQL + MySQL\n- Redis Cache\n- Storage Account  Azurite\n- Cosmos DB  Cosmos emulator\n- Service Bus  Custom emulator\n- Application Insights  Jaeger + Grafana\n\n## Tips and Best Practices\n\n1. **Start Simple:** Extract minimal viable stack first, add services incrementally\n2. **Health Checks:** Ensure every service has working health checks\n3. **Dependencies:** Use `depends_on` with `condition: service_healthy`\n4. **Secrets Management:** Never commit .env files, provide .env.template\n5. **Resource Limits:** Set realistic limits based on local development machine\n6. **Network Design:** Isolate backend services from direct external access\n7. **Documentation:** Document AzureDocker mapping for team reference\n8. **Version Control:** Exclude .env, secrets/, and volumes/ from git\n\n## Troubleshooting\n\n**Services fail to start:**\n- Check Docker Desktop resource allocation\n- Verify no port conflicts with other local services\n- Review logs: `docker compose logs <service-name>`\n\n**Database connection issues:**\n- Verify connection strings use service names (not localhost)\n- Check network configuration\n- Ensure health checks pass before apps start\n\n**Performance issues:**\n- Increase Docker Desktop memory allocation\n- Reduce number of services running simultaneously\n- Use volume caching for macOS (`:cached`)\n\n**Azurite connection failures:**\n- Use standard development account key\n- Ensure ports 10000-10002 are available\n- Verify `--loose` flag for compatibility\n\n## Next Steps\n\nAfter generating Docker Compose stack:\n1. Test with `docker compose up`\n2. Verify health checks: `docker compose ps`\n3. Export databases using `/export-database` command\n4. Generate Dockerfiles using `/generate-dockerfile` command\n5. Document any Azure-specific features not replicated locally"
              }
            ],
            "skills": [
              {
                "name": "azure-emulators-2025",
                "description": "Azure service emulators for local development including Azurite, Cosmos DB, and Event Hub Docker containers",
                "path": "plugins/azure-to-docker-master/skills/azure-emulators-2025/SKILL.md",
                "frontmatter": {
                  "name": "azure-emulators-2025",
                  "description": "Azure service emulators for local development including Azurite, Cosmos DB, and Event Hub Docker containers"
                },
                "content": "# Azure Service Emulators for Local Development (2025)\n\n## Overview\n\nThis skill provides comprehensive knowledge of Azure service emulators available as Docker containers for local development in 2025.\n\n## Available Azure Emulators\n\n### 1. Azurite (Azure Storage Emulator)\n\n**Official replacement for Azure Storage Emulator (deprecated)**\n\n**Image:** `mcr.microsoft.com/azure-storage/azurite:latest`\n\n**Supported Services:**\n- Blob Storage\n- Queue Storage\n- Table Storage\n\n**Configuration:**\n```yaml\nservices:\n  azurite:\n    image: mcr.microsoft.com/azure-storage/azurite:latest\n    container_name: azurite\n    command: azurite --blobHost 0.0.0.0 --queueHost 0.0.0.0 --tableHost 0.0.0.0 --loose\n    ports:\n      - \"10000:10000\"  # Blob service\n      - \"10001:10001\"  # Queue service\n      - \"10002:10002\"  # Table service\n    volumes:\n      - azurite-data:/data\n    restart: unless-stopped\n```\n\n**Standard Development Connection String:**\n```\nDefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite:10000/devstoreaccount1;QueueEndpoint=http://azurite:10001/devstoreaccount1;TableEndpoint=http://azurite:10002/devstoreaccount1;\n```\n\n**Features:**\n- Cross-platform (Windows, Linux, macOS)\n- Written in Node.js/JavaScript\n- Supports latest Azure Storage APIs\n- Persistence to disk\n- Compatible with Azure Storage Explorer\n- `--loose` flag for relaxed validation (useful for local development)\n\n\n**2025 API Version Support:**\n- Latest release (v3.35.0) targets 2025-11-05 API version\n- Blob service: 2025-11-05\n- Queue service: 2025-11-05\n- Table service: 2025-11-05 (preview)\n- RA-GRS support for geo-redundant replication testing\n\n**CLI Usage:**\n```bash\n# Install via npm (alternative to Docker)\nnpm install -g azurite\n\n# Run with custom location\nazurite --location /path/to/data --debug /path/to/debug.log\n```\n\n**Application Integration:**\n```javascript\n// Node.js with @azure/storage-blob\nconst { BlobServiceClient } = require('@azure/storage-blob');\n\nconst connectionString = process.env.AZURITE_CONNECTION_STRING;\nconst blobServiceClient = BlobServiceClient.fromConnectionString(connectionString);\n```\n\n```csharp\n// .NET with Azure.Storage.Blobs\nusing Azure.Storage.Blobs;\n\nvar connectionString = Environment.GetEnvironmentVariable(\"AZURITE_CONNECTION_STRING\");\nvar blobServiceClient = new BlobServiceClient(connectionString);\n```\n\n### 2. Azure SQL Server 2025\n\n**Latest SQL Server with AI features**\n\n**Image:** `mcr.microsoft.com/mssql/server:2025-latest`\n\n**2025 Features:**\n- Built-in Vector Search for AI similarity queries\n- Semantic Queries alongside traditional full-text search\n- Optimized Locking (TID Locking, LAQ)\n- Native JSON and RegEx support\n- Fabric Mirroring integration\n- REST API support\n\n**Configuration:**\n```yaml\nservices:\n  sqlserver:\n    image: mcr.microsoft.com/mssql/server:2025-latest\n    container_name: sqlserver\n    environment:\n      - ACCEPT_EULA=Y\n      - MSSQL_PID=Developer\n      - MSSQL_SA_PASSWORD_FILE=/run/secrets/sa_password\n    secrets:\n      - sa_password\n    ports:\n      - \"1433:1433\"\n    volumes:\n      - sqlserver-data:/var/opt/mssql\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P $$MSSQL_SA_PASSWORD -Q 'SELECT 1' -C || exit 1\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n      start_period: 10s\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n        reservations:\n          cpus: '1'\n          memory: 2G\n    networks:\n      - backend\n    security_opt:\n      - no-new-privileges:true\n    restart: unless-stopped\n\nsecrets:\n  sa_password:\n    file: ./secrets/sa_password.txt\n\nvolumes:\n  sqlserver-data:\n    driver: local\n```\n\n**Connection String:**\n```\nServer=sqlserver;Database=MyApp;User Id=sa;Password=YourStrong!Passw0rd;TrustServerCertificate=True;\n```\n\n**Important Notes:**\n- **Azure SQL Edge retired September 30, 2025** - Use SQL Server 2025 instead\n- Use `mssql-tools18` for TLS 1.3 support (`-C` flag trusts certificate)\n- Developer edition is free for non-production use\n- Supports arm64 via Rosetta 2 on macOS\n\n**Vector Search Example (2025 Feature):**\n```sql\n-- Create vector index for AI similarity search\nCREATE TABLE Documents (\n    Id INT PRIMARY KEY,\n    Content NVARCHAR(MAX),\n    ContentVector VECTOR(1536)\n);\n\n-- Perform similarity search\nSELECT TOP 10 Id, Content\nFROM Documents\nORDER BY VECTOR_DISTANCE(ContentVector, @queryVector);\n```\n\n**2025 vnext-preview Features:**\n- Entirely Linux-based emulator (cross-platform: x64, ARM64, Apple Silicon)\n- No virtual machines required on Apple Silicon or Microsoft ARM\n- Changefeed support (April 2025+)\n- Document TTL (Time-to-Live) support\n- OpenTelemetry V2 support\n- API for NoSQL in gateway mode\n- Currently in preview (active development)\n\n**Image:** `mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview`\n\n### 3. Cosmos DB Emulator\n\n**NoSQL database emulator**\n\n**Image:** `mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview`\n\n**Configuration:**\n```yaml\nservices:\n  cosmosdb:\n    image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview\n    container_name: cosmosdb\n    environment:\n      - AZURE_COSMOS_EMULATOR_PARTITION_COUNT=10\n      - AZURE_COSMOS_EMULATOR_ENABLE_DATA_PERSISTENCE=true\n      - AZURE_COSMOS_EMULATOR_IP_ADDRESS_OVERRIDE=127.0.0.1\n    ports:\n      - \"8081:8081\"       # Data Explorer\n      - \"10251:10251\"\n      - \"10252:10252\"\n      - \"10253:10253\"\n      - \"10254:10254\"\n    volumes:\n      - cosmos-data:/data/db\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n    networks:\n      - backend\n    restart: unless-stopped\n```\n\n**Emulator Endpoint:**\n```\nhttps://localhost:8081\n```\n\n**Emulator Key (Standard):**\n```\nC2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==\n```\n\n**Data Explorer:**\nAccess at `https://localhost:8081/_explorer/index.html`\n\n**Application Integration:**\n```javascript\n// Node.js with @azure/cosmos\nconst { CosmosClient } = require('@azure/cosmos');\n\nconst endpoint = 'https://localhost:8081';\nconst key = process.env.COSMOS_EMULATOR_KEY;\nconst client = new CosmosClient({ endpoint, key });\n```\n\n```csharp\n// .NET with Microsoft.Azure.Cosmos\nusing Microsoft.Azure.Cosmos;\n\nvar endpoint = \"https://localhost:8081\";\nvar key = Environment.GetEnvironmentVariable(\"COSMOS_EMULATOR_KEY\");\nvar client = new CosmosClient(endpoint, key);\n```\n\n**Limitations:**\n- Performance not representative of production\n- Limited to single partition for local development\n- Certificate trust required (self-signed)\n\n**2025 Official Azure Service Bus Emulator:**\n- Released as official Docker container\n- Linux-based emulator with cross-platform support\n- Requires SQL Server Linux as dependency\n- Supports AMQP protocol (port 5672)\n- Connection string format with UseDevelopmentEmulator=true\n- Current limitations: No JMS protocol, no partitioned entities, no AMQP Web Sockets\n\n**Official Emulator Image:** `mcr.microsoft.com/azure-messaging/servicebus-emulator:latest`\n\n**Connection String:**\n```\nEndpoint=sb://host.docker.internal;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=SAS_KEY_VALUE;UseDevelopmentEmulator=true;\n```\n\n### 4. Azure Service Bus Emulator\n\n**Message queue emulator**\n\n**Note:** Official emulator has limited Docker support. Use RabbitMQ as alternative.\n\n**Official Emulator (Preview):**\n```yaml\nservices:\n  servicebus-sql:\n    image: mcr.microsoft.com/mssql/server:2025-latest\n    environment:\n      - ACCEPT_EULA=Y\n      - MSSQL_SA_PASSWORD=ServiceBus123!\n    volumes:\n      - servicebus-sql-data:/var/opt/mssql\n\n  servicebus:\n    image: mcr.microsoft.com/azure-messaging/servicebus-emulator:latest\n    depends_on:\n      - servicebus-sql\n    environment:\n      - ACCEPT_EULA=Y\n      - SQL_SERVER=servicebus-sql\n      - SQL_SA_PASSWORD=ServiceBus123!\n    ports:\n      - \"5672:5672\"  # AMQP\n    networks:\n      - backend\n```\n\n**RabbitMQ Alternative (Recommended):**\n```yaml\nservices:\n  rabbitmq:\n    image: rabbitmq:3.14-alpine\n    container_name: rabbitmq\n    ports:\n      - \"5672:5672\"   # AMQP\n      - \"15672:15672\" # Management UI\n    environment:\n      - RABBITMQ_DEFAULT_USER=admin\n      - RABBITMQ_DEFAULT_PASS=admin123\n    volumes:\n      - rabbitmq-data:/var/lib/rabbitmq\n    healthcheck:\n      test: [\"CMD\", \"rabbitmq-diagnostics\", \"ping\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    networks:\n      - backend\n    restart: unless-stopped\n```\n\n### 5. PostgreSQL (Azure Database for PostgreSQL)\n\n**Image:** `postgres:16.6-alpine`\n\n**Configuration:**\n```yaml\nservices:\n  postgres:\n    image: postgres:16.6-alpine\n    container_name: postgres\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n      - POSTGRES_DB=myapp\n    secrets:\n      - postgres_password\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 2G\n    networks:\n      - backend\n    security_opt:\n      - no-new-privileges:true\n    restart: unless-stopped\n```\n\n**Extensions:**\nMatch Azure PostgreSQL Flexible Server extensions:\n```sql\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\nCREATE EXTENSION IF NOT EXISTS \"pg_trgm\";\nCREATE EXTENSION IF NOT EXISTS \"pgcrypto\";\n```\n\n### 6. MySQL (Azure Database for MySQL)\n\n**Image:** `mysql:9.2`\n\n**Configuration:**\n```yaml\nservices:\n  mysql:\n    image: mysql:9.2\n    container_name: mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mysql_root_password\n      - MYSQL_DATABASE=myapp\n      - MYSQL_USER=appuser\n      - MYSQL_PASSWORD_FILE=/run/secrets/mysql_password\n    secrets:\n      - mysql_root_password\n      - mysql_password\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n      - ./init:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \"-p$$MYSQL_ROOT_PASSWORD\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 2G\n    networks:\n      - backend\n    security_opt:\n      - no-new-privileges:true\n    restart: unless-stopped\n```\n\n### 7. Redis (Azure Cache for Redis)\n\n**Image:** `redis:7.4-alpine`\n\n**Configuration:**\n```yaml\nservices:\n  redis:\n    image: redis:7.4-alpine\n    container_name: redis\n    command: >\n      redis-server\n      --appendonly yes\n      --requirepass ${REDIS_PASSWORD}\n      --maxmemory 512mb\n      --maxmemory-policy allkeys-lru\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n    networks:\n      - backend\n    security_opt:\n      - no-new-privileges:true\n    restart: unless-stopped\n```\n\n### 8. Application Insights Alternative (Jaeger + Grafana)\n\n**OpenTelemetry-compatible observability stack**\n\n```yaml\nservices:\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    container_name: jaeger\n    ports:\n      - \"16686:16686\"  # UI\n      - \"14268:14268\"  # HTTP collector\n      - \"4317:4317\"    # OTLP gRPC\n      - \"4318:4318\"    # OTLP HTTP\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n    networks:\n      - monitoring\n    restart: unless-stopped\n\n  grafana:\n    image: grafana/grafana:latest\n    container_name: grafana\n    ports:\n      - \"3001:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana-data:/var/lib/grafana\n    networks:\n      - monitoring\n    restart: unless-stopped\n```\n\n## Complete Azure Stack Example\n\n```yaml\nservices:\n  # Application Services\n  frontend:\n    build: ./frontend\n    ports: [\"3000:3000\"]\n    networks: [frontend]\n    depends_on:\n      backend:\n        condition: service_healthy\n\n  backend:\n    build: ./backend\n    ports: [\"8080:8080\"]\n    networks: [frontend, backend]\n    depends_on:\n      sqlserver:\n        condition: service_healthy\n      redis:\n        condition: service_started\n      azurite:\n        condition: service_started\n\n  # Databases\n  sqlserver:\n    image: mcr.microsoft.com/mssql/server:2025-latest\n    environment:\n      - ACCEPT_EULA=Y\n      - MSSQL_PID=Developer\n      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD}\n    ports: [\"1433:1433\"]\n    volumes: [sqlserver-data:/var/opt/mssql]\n    networks: [backend]\n    healthcheck:\n      test: [\"CMD-SHELL\", \"/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P $$MSSQL_SA_PASSWORD -Q 'SELECT 1' -C\"]\n      interval: 10s\n\n  postgres:\n    image: postgres:16.6-alpine\n    environment:\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n    ports: [\"5432:5432\"]\n    volumes: [postgres-data:/var/lib/postgresql/data]\n    networks: [backend]\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n\n  # Cache\n  redis:\n    image: redis:7.4-alpine\n    command: redis-server --requirepass ${REDIS_PASSWORD}\n    ports: [\"6379:6379\"]\n    volumes: [redis-data:/data]\n    networks: [backend]\n\n  # Storage\n  azurite:\n    image: mcr.microsoft.com/azure-storage/azurite:latest\n    command: azurite --blobHost 0.0.0.0 --queueHost 0.0.0.0 --tableHost 0.0.0.0 --loose\n    ports: [\"10000:10000\", \"10001:10001\", \"10002:10002\"]\n    volumes: [azurite-data:/data]\n    networks: [backend]\n\n  # NoSQL\n  cosmosdb:\n    image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:vnext-preview\n    ports: [\"8081:8081\"]\n    volumes: [cosmos-data:/data/db]\n    networks: [backend]\n\n  # Monitoring\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports: [\"16686:16686\", \"4317:4317\"]\n    networks: [monitoring]\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n  monitoring:\n    driver: bridge\n\nvolumes:\n  sqlserver-data:\n  postgres-data:\n  redis-data:\n  azurite-data:\n  cosmos-data:\n```\n\n## Best Practices\n\n1. **Use health checks** - Ensure dependencies are ready before app starts\n2. **Network isolation** - Keep databases on internal backend network\n3. **Resource limits** - Prevent emulators from consuming all system resources\n4. **Persistence** - Use named volumes for data that should survive restarts\n5. **Secrets management** - Use Docker secrets or environment files\n6. **Version pinning** - Use specific tags, not `latest`\n7. **Security hardening** - Drop capabilities, run as non-root where possible\n8. **Documentation** - Document differences between emulators and Azure services\n\n## Known Limitations\n\n- **Performance**: Emulators are slower than Azure services\n- **Scale**: Single-instance only, no clustering\n- **Features**: Some Azure-specific features unavailable locally\n- **SSL/TLS**: Self-signed certificates require trust configuration\n- **Azure AD**: Authentication not replicated locally\n- **Networking**: VNets, Private Endpoints not available\n\n## Migration Checklist\n\nWhen moving from Azure to Docker emulators:\n\n- [ ] Replace Azure Storage connection strings with Azurite\n- [ ] Update SQL Server connection strings (remove Azure-specific options)\n- [ ] Configure Cosmos DB with emulator endpoint and key\n- [ ] Replace Service Bus with RabbitMQ or emulator\n- [ ] Set up alternative for Application Insights\n- [ ] Update authentication (remove Azure AD dependencies)\n- [ ] Configure network isolation\n- [ ] Test all integrations\n- [ ] Document feature parity gaps\n- [ ] Create init scripts for sample data\n\n## References\n\n- [Azurite Documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite)\n- [SQL Server 2025 in Docker](https://learn.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker)\n- [Cosmos DB Emulator](https://learn.microsoft.com/en-us/azure/cosmos-db/local-emulator)\n- [Service Bus Emulator](https://learn.microsoft.com/en-us/azure/service-bus-messaging/test-locally-with-service-bus-emulator)"
              },
              {
                "name": "compose-patterns-2025",
                "description": "Docker Compose production patterns 2025 including multi-environment strategies, health checks, and modern compose features",
                "path": "plugins/azure-to-docker-master/skills/compose-patterns-2025/SKILL.md",
                "frontmatter": {
                  "name": "compose-patterns-2025",
                  "description": "Docker Compose production patterns 2025 including multi-environment strategies, health checks, and modern compose features"
                },
                "content": "# Docker Compose Patterns for Production (2025)\n\n## Overview\n\nThis skill documents production-ready Docker Compose patterns and best practices for 2025, based on official Docker documentation and industry standards.\n\n## File Format Changes (2025)\n\n**IMPORTANT:** The `version` field is now **obsolete** in Docker Compose v2.42+.\n\n**Correct (2025):**\n```yaml\nservices:\n  app:\n    image: myapp:latest\n```\n\n**Incorrect (deprecated):**\n```yaml\nversion: '3.8'  # DO NOT USE\nservices:\n  app:\n    image: myapp:latest\n```\n\n## Multiple Environment Strategy\n\n### Pattern: Base + Environment Overrides\n\n**compose.yaml (base):**\n```yaml\nservices:\n  app:\n    build:\n      context: ./app\n      dockerfile: Dockerfile\n    environment:\n      - NODE_ENV=production\n    restart: unless-stopped\n```\n\n**compose.override.yaml (development - auto-loaded):**\n```yaml\nservices:\n  app:\n    build:\n      target: development\n    volumes:\n      - ./app/src:/app/src:cached\n    environment:\n      - NODE_ENV=development\n      - DEBUG=*\n    ports:\n      - \"9229:9229\"  # Debugger\n```\n\n**compose.prod.yaml (production - explicit):**\n```yaml\nservices:\n  app:\n    build:\n      target: production\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '1'\n          memory: 512M\n      restart_policy:\n        condition: on-failure\n        max_attempts: 3\n```\n\n**Usage:**\n```bash\n# Development (auto-loads compose.override.yaml)\ndocker compose up\n\n# Production\ndocker compose -f compose.yaml -f compose.prod.yaml up -d\n\n# CI/CD\ndocker compose -f compose.yaml -f compose.ci.yaml up --abort-on-container-exit\n```\n\n## Environment Variable Management\n\n### Pattern: .env Files per Environment\n\n**.env.template (committed to git):**\n```bash\n# Database\nDB_HOST=sqlserver\nDB_PORT=1433\nDB_NAME=myapp\nDB_USER=sa\n# DB_PASSWORD= (set in actual .env)\n\n# Redis\nREDIS_HOST=redis\nREDIS_PORT=6379\n# REDIS_PASSWORD= (set in actual .env)\n\n# Application\nNODE_ENV=production\nLOG_LEVEL=info\n```\n\n**.env.dev:**\n```bash\nDB_PASSWORD=Dev!Pass123\nREDIS_PASSWORD=redis-dev-123\nNODE_ENV=development\nLOG_LEVEL=debug\n```\n\n**.env.prod:**\n```bash\nDB_PASSWORD=${PROD_DB_PASSWORD}  # From CI/CD\nREDIS_PASSWORD=${PROD_REDIS_PASSWORD}\nNODE_ENV=production\nLOG_LEVEL=info\n```\n\n**Load specific environment:**\n```bash\ndocker compose --env-file .env.dev up\n```\n\n## Security Patterns\n\n### Pattern: Run as Non-Root User\n\n```yaml\nservices:\n  app:\n    image: node:20-alpine\n    user: \"1000:1000\"  # UID:GID\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /app/.cache\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE  # Only if binding to ports < 1024\n    security_opt:\n      - no-new-privileges:true\n```\n\n**Create user in Dockerfile:**\n```dockerfile\nFROM node:20-alpine\n\n# Create app user\nRUN addgroup -g 1000 appuser && \\\n    adduser -D -u 1000 -G appuser appuser\n\n# Set ownership\nWORKDIR /app\nCOPY --chown=appuser:appuser . .\n\nUSER appuser\n```\n\n### Pattern: Secrets Management\n\n**Docker Swarm secrets (production):**\n```yaml\nservices:\n  app:\n    secrets:\n      - db_password\n      - api_key\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n  api_key:\n    external: true  # Managed by Swarm\n```\n\n**Access secrets in application:**\n```javascript\n// Read from /run/secrets/\nconst fs = require('fs');\nconst dbPassword = fs.readFileSync('/run/secrets/db_password', 'utf8').trim();\n```\n\n**Development alternative (environment):**\n```yaml\nservices:\n  app:\n    environment:\n      - DB_PASSWORD_FILE=/run/secrets/db_password\n```\n\n## Health Check Patterns\n\n### Pattern: Comprehensive Health Checks\n\n**HTTP endpoint:**\n```yaml\nservices:\n  web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n      start_period: 40s\n```\n\n**Database ping:**\n```yaml\nservices:\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U $$POSTGRES_USER\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n```\n\n**Custom script:**\n```yaml\nservices:\n  app:\n    healthcheck:\n      test: [\"CMD\", \"node\", \"/app/scripts/healthcheck.js\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n      start_period: 40s\n```\n\n**healthcheck.js:**\n```javascript\nconst http = require('http');\n\nconst options = {\n  hostname: 'localhost',\n  port: 8080,\n  path: '/health',\n  timeout: 2000\n};\n\nconst req = http.request(options, (res) => {\n  process.exit(res.statusCode === 200 ? 0 : 1);\n});\n\nreq.on('error', () => process.exit(1));\nreq.on('timeout', () => {\n  req.destroy();\n  process.exit(1);\n});\nreq.end();\n```\n\n## Dependency Management\n\n### Pattern: Ordered Startup with Conditions\n\n```yaml\nservices:\n  web:\n    depends_on:\n      database:\n        condition: service_healthy\n      redis:\n        condition: service_started\n      migration:\n        condition: service_completed_successfully\n\n  database:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n\n  redis:\n    # No health check needed, just wait for start\n\n  migration:\n    image: myapp:latest\n    command: npm run migrate\n    restart: \"no\"  # Run once\n    depends_on:\n      database:\n        condition: service_healthy\n```\n\n## Network Isolation Patterns\n\n### Pattern: Three-Tier Network Architecture\n\n```yaml\nservices:\n  nginx:\n    image: nginx:alpine\n    networks:\n      - frontend\n    ports:\n      - \"80:80\"\n\n  api:\n    build: ./api\n    networks:\n      - frontend\n      - backend\n\n  database:\n    image: postgres:16-alpine\n    networks:\n      - backend  # No frontend access\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true  # No external access\n```\n\n### Pattern: Service-Specific Networks\n\n```yaml\nservices:\n  web-app:\n    networks:\n      - public\n      - app-network\n\n  api:\n    networks:\n      - app-network\n      - data-network\n\n  postgres:\n    networks:\n      - data-network\n\n  redis:\n    networks:\n      - data-network\n\nnetworks:\n  public:\n    driver: bridge\n  app-network:\n    driver: bridge\n    internal: true\n  data-network:\n    driver: bridge\n    internal: true\n```\n\n## Volume Patterns\n\n### Pattern: Named Volumes for Persistence\n\n```yaml\nservices:\n  database:\n    volumes:\n      - db-data:/var/lib/postgresql/data  # Persistent data\n      - ./init:/docker-entrypoint-initdb.d:ro  # Init scripts (read-only)\n      - db-logs:/var/log/postgresql  # Logs\n\nvolumes:\n  db-data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /mnt/data/postgres  # Host path\n  db-logs:\n    driver: local\n```\n\n### Pattern: Development Bind Mounts\n\n```yaml\nservices:\n  app:\n    volumes:\n      - ./src:/app/src:cached  # macOS optimization\n      - /app/node_modules  # Don't overwrite installed modules\n      - app-cache:/app/.cache  # Named volume for cache\n```\n\n**Volume mount options:**\n- `:ro` - Read-only\n- `:rw` - Read-write (default)\n- `:cached` - macOS performance optimization (host authoritative)\n- `:delegated` - macOS performance optimization (container authoritative)\n- `:z` - SELinux single container\n- `:Z` - SELinux multi-container\n\n## Resource Management Patterns\n\n### Pattern: CPU and Memory Limits\n\n```yaml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 512M\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n```\n\n**Calculate total resources:**\n```yaml\n# 3 app replicas + database + redis\nservices:\n  app:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '0.5'      # 3 x 0.5 = 1.5 CPUs\n          memory: 512M     # 3 x 512M = 1.5GB\n\n  database:\n    deploy:\n      resources:\n        limits:\n          cpus: '2'        # 2 CPUs\n          memory: 4G       # 4GB\n\n  redis:\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'      # 0.5 CPUs\n          memory: 512M     # 512MB\n\n# Total: 4 CPUs, 6GB RAM minimum\n```\n\n## Logging Patterns\n\n### Pattern: Centralized Logging\n\n```yaml\nservices:\n  app:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n        compress: \"true\"\n        labels: \"app,environment\"\n```\n\n**Alternative: Log to stdout/stderr (12-factor):**\n```yaml\nservices:\n  app:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n```\n\n**View logs:**\n```bash\ndocker compose logs -f app\ndocker compose logs --since 30m app\ndocker compose logs --tail 100 app\n```\n\n## Init Container Pattern\n\n### Pattern: Database Migration\n\n```yaml\nservices:\n  migration:\n    image: myapp:latest\n    command: npm run migrate\n    depends_on:\n      database:\n        condition: service_healthy\n    restart: \"no\"  # Run once\n    networks:\n      - backend\n\n  app:\n    image: myapp:latest\n    depends_on:\n      migration:\n        condition: service_completed_successfully\n    networks:\n      - backend\n```\n\n## YAML Anchors and Aliases\n\n### Pattern: Reusable Configuration\n\n```yaml\nx-common-app-config: &common-app\n  restart: unless-stopped\n  logging:\n    driver: \"json-file\"\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n  security_opt:\n    - no-new-privileges:true\n  cap_drop:\n    - ALL\n  cap_add:\n    - NET_BIND_SERVICE\n\nservices:\n  app1:\n    <<: *common-app\n    build: ./app1\n    ports:\n      - \"8001:8080\"\n\n  app2:\n    <<: *common-app\n    build: ./app2\n    ports:\n      - \"8002:8080\"\n\n  app3:\n    <<: *common-app\n    build: ./app3\n    ports:\n      - \"8003:8080\"\n```\n\n### Pattern: Environment-Specific Overrides\n\n```yaml\nx-logging: &default-logging\n  driver: \"json-file\"\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n\nx-resources: &default-resources\n  limits:\n    cpus: '1'\n    memory: 512M\n  reservations:\n    cpus: '0.5'\n    memory: 256M\n\nservices:\n  app:\n    logging: *default-logging\n    deploy:\n      resources: *default-resources\n```\n\n## Port Binding Patterns\n\n### Pattern: Security-First Port Binding\n\n```yaml\nservices:\n  # Public services\n  web:\n    ports:\n      - \"80:8080\"\n      - \"443:8443\"\n\n  # Development only (localhost binding)\n  debug:\n    ports:\n      - \"127.0.0.1:9229:9229\"  # Debugger only accessible from host\n\n  # Environment-based binding\n  app:\n    ports:\n      - \"${DOCKER_WEB_PORT_FORWARD:-127.0.0.1:8000}:8000\"\n```\n\n**Environment control:**\n```bash\n# Development (.env.dev)\nDOCKER_WEB_PORT_FORWARD=127.0.0.1:8000  # Localhost only\n\n# Production (.env.prod)\nDOCKER_WEB_PORT_FORWARD=8000  # All interfaces\n```\n\n## Restart Policy Patterns\n\n```yaml\nservices:\n  # Always restart (production services)\n  app:\n    restart: always\n\n  # Restart unless manually stopped (most common)\n  database:\n    restart: unless-stopped\n\n  # Never restart (one-time tasks)\n  migration:\n    restart: \"no\"\n\n  # Restart on failure only (with Swarm)\n  worker:\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n```\n\n## Validation and Testing\n\n### Pattern: Pre-Deployment Validation\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\necho \"Validating Compose syntax...\"\ndocker compose config > /dev/null\n\necho \"Building images...\"\ndocker compose build\n\necho \"Running security scan...\"\nfor service in $(docker compose config --services); do\n  image=$(docker compose config | yq \".services.$service.image\")\n  if [ -n \"$image\" ]; then\n    docker scout cves \"$image\" || true\n  fi\ndone\n\necho \"Starting services...\"\ndocker compose up -d\n\necho \"Checking health...\"\nsleep 10\ndocker compose ps\n\necho \"Running smoke tests...\"\ncurl -f http://localhost:8080/health || exit 1\n\necho \" All checks passed\"\n```\n\n## Complete Production Example\n\n```yaml\n# Modern Compose format (no version field for v2.40+)\n\nx-common-service: &common-service\n  restart: unless-stopped\n  logging:\n    driver: \"json-file\"\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n  security_opt:\n    - no-new-privileges:true\n\nservices:\n  nginx:\n    <<: *common-service\n    image: nginxinc/nginx-unprivileged:alpine\n    ports:\n      - \"80:8080\"\n    volumes:\n      - ./nginx/conf.d:/etc/nginx/conf.d:ro\n    networks:\n      - frontend\n    depends_on:\n      api:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--quiet\", \"--tries=1\", \"--spider\", \"http://localhost:8080/health\"]\n      interval: 30s\n\n  api:\n    <<: *common-service\n    build:\n      context: ./api\n      dockerfile: Dockerfile\n      target: production\n    user: \"1000:1000\"\n    read_only: true\n    tmpfs:\n      - /tmp\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n    networks:\n      - frontend\n      - backend\n    depends_on:\n      migration:\n        condition: service_completed_successfully\n      redis:\n        condition: service_started\n    env_file:\n      - .env\n    healthcheck:\n      test: [\"CMD\", \"node\", \"healthcheck.js\"]\n      interval: 30s\n      start_period: 40s\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 512M\n\n  migration:\n    image: myapp:latest\n    command: npm run migrate\n    restart: \"no\"\n    networks:\n      - backend\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  postgres:\n    <<: *common-service\n    image: postgres:16-alpine\n    environment:\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n    secrets:\n      - postgres_password\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 2G\n\n  redis:\n    <<: *common-service\n    image: redis:7.4-alpine\n    command: redis-server --requirepass ${REDIS_PASSWORD}\n    volumes:\n      - redis-data:/data\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n\nvolumes:\n  postgres-data:\n    driver: local\n  redis-data:\n    driver: local\n\nsecrets:\n  postgres_password:\n    file: ./secrets/postgres_password.txt\n```\n\n## Common Mistakes to Avoid\n\n1. **Using `version` field** - Obsolete in 2025\n2. **No health checks** - Leads to race conditions\n3. **Running as root** - Security risk\n4. **No resource limits** - Can exhaust host resources\n5. **Hardcoded secrets** - Use secrets or environment variables\n6. **No logging limits** - Disk space issues\n7. **Bind mounts in production** - Use named volumes\n8. **Missing restart policies** - Services don't recover\n9. **No network isolation** - All services can talk to each other\n10. **Not using .dockerignore** - Larger build contexts\n\n## Troubleshooting Commands\n\n```bash\n# Validate syntax\ndocker compose config\n\n# View merged configuration\ndocker compose config --services\n\n# Check which file is being used\ndocker compose config --files\n\n# View environment interpolation\ndocker compose config --no-interpolate\n\n# Check service dependencies\ndocker compose config | yq '.services.*.depends_on'\n\n# View resource usage\ndocker stats $(docker compose ps -q)\n\n# Debug startup issues\ndocker compose up --no-deps service-name\n\n# Force recreate\ndocker compose up --force-recreate service-name\n```\n\n## References\n\n- [Docker Compose Documentation](https://docs.docker.com/compose/)\n- [Compose v2.42+ Release Notes](https://github.com/docker/compose/releases)\n- [Best Practices](https://docs.docker.com/compose/how-tos/production/)"
              },
              {
                "name": "docker-watch-mode-2025",
                "description": "Docker Compose Watch mode for automatic hot reload during local development with sync, rebuild, and restart actions",
                "path": "plugins/azure-to-docker-master/skills/docker-watch-mode-2025/SKILL.md",
                "frontmatter": {
                  "name": "docker-watch-mode-2025",
                  "description": "Docker Compose Watch mode for automatic hot reload during local development with sync, rebuild, and restart actions"
                },
                "content": "# Docker Compose Watch Mode (2025 GA)\n\nDocker Compose Watch enables automatic hot reload during local development by synchronizing file changes instantly without manual container restarts.\n\n## Three Watch Actions\n\n### 1. sync - Hot Reload\nFor frameworks with hot reload (React, Next.js, Node.js, Flask).\nCopies changed files directly into running container.\n\n### 2. rebuild - Compilation\nFor compiled languages (Go, Rust, Java) or dependency changes.\nRebuilds image and recreates container when files change.\n\n### 3. sync+restart - Config Changes\nFor configuration files requiring restart.\nSyncs files and restarts container.\n\n## Usage\n\n```yaml\nservices:\n  frontend:\n    build: ./frontend\n    develop:\n      watch:\n        - action: sync\n          path: ./frontend/src\n          target: /app/src\n          ignore: [node_modules/, .git/]\n        - action: rebuild\n          path: ./frontend/package.json\n```\n\nStart with: `docker compose up --watch`\n\n## Benefits\n- Better performance than bind mounts\n- No file permission issues\n- Intelligent syncing\n- Supports rebuild capability\n- Works on all platforms"
              }
            ]
          },
          {
            "name": "windows-path-master",
            "description": "Windows path resolution and Git Bash compatibility expert for Claude Code. PROACTIVELY activate for: (1) File path errors on Windows (backslash issues), (2) Git Bash MINGW path resolution, (3) Edit/Write tool failures with Windows paths, (4) Cross-platform path conversion, (5) Windows file system navigation in Claude Code, (6) Path format detection and conversion, (7) Windows-specific file operation troubleshooting. Provides: automatic path format detection, backslash conversion guidance, Git Bash compatibility fixes, Windows-specific file operation patterns, MINGW path troubleshooting, real-time path validation, and production-ready Windows file handling. Ensures correct file operations on Windows following Claude Code best practices.",
            "source": "./plugins/windows-path-master",
            "category": null,
            "version": "1.0.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install windows-path-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/path-fix",
                "description": "Interactively fix Windows file path issues and convert paths from Git Bash MINGW format to Windows format",
                "path": "plugins/windows-path-master/commands/path-fix.md",
                "frontmatter": {
                  "description": "Interactively fix Windows file path issues and convert paths from Git Bash MINGW format to Windows format"
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Windows Path Fix Command\n\n## Purpose\n\nThis command provides interactive assistance for fixing Windows file path issues in Claude Code, especially when working with Git Bash. It helps detect and convert paths from MINGW/POSIX format to Windows-native format required by Claude Code's file operation tools.\n\n## When to Use This Command\n\n**PROACTIVELY use this command when:**\n1. A file operation (Edit/Write/Read) fails with \"file not found\" or \"ENOENT\" error on Windows\n2. User is working on Windows with Git Bash\n3. User provides a path that looks like MINGW format (e.g., `/s/repos/`, `/c/Users/`)\n4. User reports file path issues on Windows\n5. You need to verify a path format before using Edit/Write/Read tools\n6. User asks how to format paths for Claude Code on Windows\n\n## Instructions\n\n### Step 1: Identify the Problem\n\n**Check for these indicators:**\n- User is on Windows (check environment or user mentions it)\n- Path starts with `/` followed by single letter (e.g., `/s/`, `/c/`, `/d/`)\n- Path uses forward slashes instead of backslashes\n- Recent Edit/Write/Read tool failure with path-related error\n\n### Step 2: Gather Path Information\n\n**Ask the user (if path is not already provided):**\n\n```\nI can help fix the file path format for Windows. Could you provide the file path you're trying to access?\n\nIf you're in Git Bash, you can get the full path by running:\n  pwd -W\nor for a specific file:\n  realpath filename.txt\n```\n\n### Step 3: Analyze the Path Format\n\n**Determine the path type:**\n\n**MINGW Path (Git Bash format):**\n- Pattern: `/x/path/to/file` where `x` is a single letter\n- Example: `/s/repos/project/file.tsx`\n- Example: `/c/Users/name/Documents/file.txt`\n- **Action:** Convert to Windows format\n\n**Windows Path with Forward Slashes:**\n- Pattern: `X:/path/to/file` where `X` is drive letter\n- Example: `S:/repos/project/file.tsx`\n- **Action:** Replace forward slashes with backslashes\n\n**Windows Path (Correct):**\n- Pattern: `X:\\path\\to\\file` where `X` is drive letter\n- Example: `S:\\repos\\project\\file.tsx`\n- **Action:** No conversion needed\n\n**Relative Path:**\n- Pattern: `./path/to/file` or `../path/to/file`\n- Example: `./src/components/Button.tsx`\n- **Action:** Request full path or current directory\n\n**WSL Path:**\n- Pattern: `/mnt/x/path/to/file` where `x` is drive letter\n- Example: `/mnt/c/repos/project/file.tsx`\n- **Action:** Convert to Windows format\n\n### Step 4: Convert the Path\n\n**Use the appropriate conversion algorithm:**\n\n**For MINGW paths (`/x/...`):**\n```\nInput:  /s/repos/myproject/file.tsx\n\nStep 1: Extract drive letter from first segment  \"s\"\nStep 2: Uppercase the drive letter  \"S\"\nStep 3: Add colon  \"S:\"\nStep 4: Replace remaining forward slashes with backslashes  \\repos\\myproject\\file.tsx\nStep 5: Combine  S:\\repos\\myproject\\file.tsx\n\nOutput: S:\\repos\\myproject\\file.tsx\n```\n\n**For Windows paths with forward slashes (`X:/...`):**\n```\nInput:  S:/repos/project/file.tsx\n\nStep 1: Drive letter already present  \"S:\"\nStep 2: Replace all forward slashes with backslashes  \\repos\\project\\file.tsx\nStep 3: Combine  S:\\repos\\project\\file.tsx\n\nOutput: S:\\repos\\project\\file.tsx\n```\n\n**For relative paths:**\n```\nInput:  ./src/components/Button.tsx\nCurrent directory (ask user for `pwd -W` output): S:/repos/my-project\n\nStep 1: Get current directory in Windows format  S:\\repos\\my-project\nStep 2: Remove ./ prefix from relative path  src/components/Button.tsx\nStep 3: Replace forward slashes with backslashes  src\\components\\Button.tsx\nStep 4: Combine  S:\\repos\\my-project\\src\\components\\Button.tsx\n\nOutput: S:\\repos\\my-project\\src\\components\\Button.tsx\n```\n\n**For WSL paths (`/mnt/x/...`):**\n```\nInput:  /mnt/c/Users/name/project/file.tsx\n\nStep 1: Extract drive letter after /mnt/  \"c\"\nStep 2: Uppercase  \"C\"\nStep 3: Add colon  \"C:\"\nStep 4: Remove /mnt/c/ prefix  Users/name/project/file.tsx\nStep 5: Replace forward slashes with backslashes  Users\\name\\project\\file.tsx\nStep 6: Combine  C:\\Users\\name\\project\\file.tsx\n\nOutput: C:\\Users\\name\\project\\file.tsx\n```\n\n### Step 5: Present the Conversion\n\n**Show the conversion clearly:**\n\n```\n Path Conversion Complete\n\n**Original Path (Git Bash/MINGW format):**\n/s/repos/myproject/src/components/Button.tsx\n\n**Converted Path (Windows format for Claude Code):**\nS:\\repos\\myproject\\src\\components\\Button.tsx\n\n**What Changed:**\n- Converted /s/  S:\n- Replaced forward slashes (/) with backslashes (\\)\n- Now compatible with Claude Code's Edit/Write/Read tools\n```\n\n### Step 6: Verify and Retry\n\n**If the original operation failed, retry with the converted path:**\n\n```\nI'll now retry the [Edit/Write/Read] operation with the correct Windows path format...\n```\n\nThen execute the intended file operation using the converted path.\n\n### Step 7: Educate the User\n\n**Explain why the conversion was necessary:**\n\n```\n Why This Matters:\n\n**Git Bash displays paths in POSIX/MINGW format** (e.g., /s/repos/file.tsx), but\n**Claude Code's file tools require Windows native format** (e.g., S:\\repos\\file.tsx).\n\n**Key Points:**\n1. Always use backslashes (\\) in file paths for Claude Code on Windows\n2. Use drive letter format (C:, D:, S:) not MINGW format (/c/, /d/, /s/)\n3. Run `pwd -W` in Git Bash to get Windows-formatted paths\n\n**This conversion is automatic when you use the /path-fix command!**\n```\n\n### Step 8: Prevent Automatic Path Conversion (Advanced)\n\n**For command-line operations that need POSIX paths:**\n\nWhen running commands in Git Bash that interact with tools expecting Unix-style paths (like Docker, WSL tools, or certain CLIs), Git Bash may automatically convert paths, which can cause issues.\n\n**Use MSYS_NO_PATHCONV=1 to disable automatic conversion:**\n\n```bash\n# Problem: Git Bash converts /app to C:/Program Files/Git/app\ndocker run -v /app:/app myimage\n\n# Solution: Disable path conversion for this command\nMSYS_NO_PATHCONV=1 docker run -v /app:/app myimage\n```\n\n**Common scenarios where MSYS_NO_PATHCONV=1 is needed:**\n\n1. **Docker volume mounts:**\n   ```bash\n   #  Correct\n   MSYS_NO_PATHCONV=1 docker run -v /app:/app nginx\n\n   #  Wrong - Git Bash converts /app to C:/Program Files/Git/app\n   docker run -v /app:/app nginx\n   ```\n\n2. **Docker exec commands:**\n   ```bash\n   MSYS_NO_PATHCONV=1 docker exec container ls /app\n   ```\n\n3. **Azure CLI with paths:**\n   ```bash\n   MSYS_NO_PATHCONV=1 az storage blob upload --file /path/to/file\n   ```\n\n4. **Terraform with module sources:**\n   ```bash\n   MSYS_NO_PATHCONV=1 terraform init\n   ```\n\n**When to recommend MSYS_NO_PATHCONV=1:**\n- User reports Docker commands failing with weird paths\n- Commands expecting Unix paths get Windows paths instead\n- Azure CLI, AWS CLI, or other cloud tools show path errors\n- Terraform or other IaC tools fail with path conversion issues\n\n**Example explanation for users:**\n```\n Pro Tip for Git Bash Users:\n\nGit Bash automatically converts Unix-style paths to Windows paths, which can\nbreak Docker and other tools. Use MSYS_NO_PATHCONV=1 to prevent this:\n\nMSYS_NO_PATHCONV=1 docker run -v /app:/app myimage\n\nOr set it globally for your Git Bash session:\nexport MSYS_NO_PATHCONV=1\n```\n\n## Common Scenarios\n\n### Scenario 1: User Reports \"File Not Found\" Error\n\n**User:** \"I'm trying to edit a file but getting 'file not found' error\"\n\n**Response:**\n```\nI can help fix that! This is likely a path format issue on Windows.\n\nCould you share the file path you're trying to access? If you're using Git Bash,\nrun this command to get the Windows-formatted path:\n\npwd -W\n\nThen share the full path to the file you want to edit.\n```\n\n**After receiving path `/s/repos/project/file.tsx`:**\n```\nI see the issue - this is Git Bash MINGW format. I'll convert it to Windows format:\n\nOriginal:  /s/repos/project/file.tsx\nConverted: S:\\repos\\project\\file.tsx\n\nNow retrying the edit operation with the correct Windows path...\n```\n\n### Scenario 2: Proactive Path Conversion\n\n**User:** \"Please edit /c/Users/john/Documents/notes.txt\"\n\n**Response (proactive conversion):**\n```\nI notice you're using Git Bash path format. I'll convert this to Windows format\nfor Claude Code's Edit tool:\n\nGit Bash format:  /c/Users/john/Documents/notes.txt\nWindows format:   C:\\Users\\john\\Documents\\notes.txt\n\nEditing the file now...\n```\n\n### Scenario 3: Relative Path Resolution\n\n**User:** \"Edit the file ./src/components/Button.tsx\"\n\n**Response:**\n```\nTo work with this relative path on Windows, I need the full Windows path.\n\nCould you run this command in Git Bash and share the output?\n\npwd -W\n\nThis will give me your current directory in Windows format, so I can construct\nthe full path.\n```\n\n**After receiving `S:/repos/my-project`:**\n```\nGot it! Here's the conversion:\n\nCurrent directory: S:\\repos\\my-project\nRelative path: ./src/components/Button.tsx\nFull Windows path: S:\\repos\\my-project\\src\\components\\Button.tsx\n\nEditing the file now...\n```\n\n### Scenario 4: Multiple Paths Need Conversion\n\n**User:** \"Edit these files: /s/repos/file1.ts, /s/repos/file2.ts, /s/repos/file3.ts\"\n\n**Response:**\n```\nI'll convert all three paths from Git Bash to Windows format:\n\n1. /s/repos/file1.ts  S:\\repos\\file1.ts\n2. /s/repos/file2.ts  S:\\repos\\file2.ts\n3. /s/repos/file3.ts  S:\\repos\\file3.ts\n\nEditing all three files with the correct Windows paths...\n```\n\n## Validation Steps\n\n**Before using a converted path, verify:**\n\n- [ ] Path starts with uppercase drive letter and colon (e.g., `C:`, `S:`)\n- [ ] Path uses backslashes (`\\`) not forward slashes (`/`)\n- [ ] Path is absolute (starts with drive letter), not relative\n- [ ] No MINGW format indicators (`/c/`, `/s/`, `/mnt/`)\n- [ ] No environment variables unexpanded (`$HOME`, `%USERPROFILE%`)\n- [ ] No trailing spaces or hidden characters\n\n## Error Handling\n\n**If path conversion still results in errors:**\n\n### Error: File still not found after conversion\n\n**Possible causes:**\n1. File doesn't actually exist at that path\n2. Path has typos\n3. File extension is missing or incorrect\n4. Case sensitivity issue (rare on Windows but possible)\n\n**Actions:**\n```\nThe converted path appears correct, but the file might not exist. Let's verify:\n\nCould you run this command in Git Bash to check if the file exists?\n\nls -la [path to file]\n\nThis will show if the file exists and its exact name (including extension).\n```\n\n### Error: Access denied or permission error\n\n**Possible causes:**\n1. File is locked by another process\n2. User doesn't have read/write permissions\n3. File is in a protected directory\n\n**Actions:**\n```\nThe path format is correct, but there's a permission issue.\n\nCould you check:\n1. Is the file currently open in another program?\n2. Do you have read/write permissions for this file?\n3. Try running Git Bash as Administrator if needed\n```\n\n## Quick Reference\n\n### Path Conversion Patterns\n\n| Input Format | Output Format | Example |\n|--------------|---------------|---------|\n| `/s/repos/file.txt` | `S:\\repos\\file.txt` | MINGW  Windows |\n| `S:/repos/file.txt` | `S:\\repos\\file.txt` | Windows with `/`  Windows with `\\` |\n| `/mnt/c/Users/file.txt` | `C:\\Users\\file.txt` | WSL  Windows |\n| `./src/file.txt` | `[CWD]\\src\\file.txt` | Relative  Absolute |\n| `/c/Program Files/app/file.txt` | `C:\\Program Files\\app\\file.txt` | MINGW with spaces  Windows |\n\n### Git Bash Commands for Path Discovery\n\n```bash\n# Get current directory in Windows format\npwd -W\n\n# Get absolute path of a file in Windows format\nrealpath -W filename.txt\n\n# List files with full details\nls -la\n\n# Show file with full path\nreadlink -f filename.txt\n```\n\n## Success Criteria\n\nThe command is successful when:\n\n1.  Path is correctly identified as MINGW, WSL, or Windows format\n2.  Conversion algorithm produces valid Windows path\n3.  Converted path uses backslashes throughout\n4.  File operation succeeds with converted path\n5.  User understands why conversion was necessary\n6.  User knows how to provide Windows-formatted paths in future\n\n## Related Commands\n\n- **Windows Path Troubleshooting Skill**: Comprehensive path format knowledge\n- **Windows Path Expert Agent**: For complex path issues and debugging\n- **File operations**: Edit, Write, Read tools that require proper Windows paths\n\n## Best Practices\n\n1. **Proactive Detection**: Don't wait for errors - convert paths immediately when you see MINGW format\n2. **Clear Communication**: Always show both original and converted paths\n3. **User Education**: Explain why conversion is needed\n4. **Validation**: Verify path format before using file tools\n5. **Helpful Guidance**: Provide `pwd -W` command for users to get Windows paths themselves"
              }
            ],
            "skills": [
              {
                "name": "windows-path-troubleshooting",
                "description": "Complete Windows file path troubleshooting knowledge for Claude Code on Git Bash and Windows environments. PROACTIVELY activate for: (1) File path errors on Windows, (2) Backslash vs forward slash issues, (3) Edit/Write/Read tool failures, (4) MINGW path resolution, (5) Cross-platform path conversion.",
                "path": "plugins/windows-path-master/skills/windows-path-troubleshooting/SKILL.md",
                "frontmatter": {
                  "name": "windows-path-troubleshooting",
                  "description": "Complete Windows file path troubleshooting knowledge for Claude Code on Git Bash and Windows environments. PROACTIVELY activate for: (1) File path errors on Windows, (2) Backslash vs forward slash issues, (3) Edit/Write/Read tool failures, (4) MINGW path resolution, (5) Cross-platform path conversion."
                },
                "content": "##  CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n**Examples:**\n-  WRONG: `D:/repos/project/file.tsx`\n-  CORRECT: `D:\\repos\\project\\file.tsx`\n\nThis applies to:\n- Edit tool file_path parameter\n- Write tool file_path parameter\n- All file operations on Windows systems\n\n\n### Documentation Guidelines\n\n**NEVER create new documentation files unless explicitly requested by the user.**\n\n- **Priority**: Update existing README.md files rather than creating new documentation\n- **Repository cleanliness**: Keep repository root clean - only README.md unless user requests otherwise\n- **Style**: Documentation should be concise, direct, and professional - avoid AI-generated tone\n- **User preference**: Only create additional .md files when user specifically asks for documentation\n\n\n---\n\n# Windows Path Troubleshooting for Claude Code\n\n##  CRITICAL: Always Use Backslashes on Windows for File Paths\n\n**MANDATORY: When using Edit, Write, or Read tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).**\n\n### The Rule\n\n**Windows File Path Requirements:**\n-  **CORRECT**: `D:\\repos\\project\\file.tsx`\n-  **WRONG**: `D:/repos/project/file.tsx`\n\n**This applies to:**\n- Edit tool `file_path` parameter\n- Write tool `file_path` parameter\n- Read tool `file_path` parameter\n- All file operations on Windows systems\n\n### Why This Matters\n\n**Common error message when using forward slashes on Windows:**\n```\nError: ENOENT: no such file or directory\n```\n\n**Root cause:**\n- Windows native file system uses backslashes (`\\`) as path separators\n- Forward slashes (`/`) work in some Windows contexts but NOT in Claude Code file tools\n- Git Bash displays paths with forward slashes but Windows APIs require backslashes\n- Claude Code's Read/Write/Edit tools use Windows native APIs that expect backslashes\n\n##  Common Windows Path Issues in Claude Code\n\n### Issue 1: Forward Slashes in Tool Calls\n\n**Symptom:**\n```\nEdit tool fails with \"file not found\" or \"no such file or directory\"\n```\n\n**Cause:**\nUsing forward slashes copied from Git Bash output:\n```bash\n# Git Bash shows:\n/s/repos/claude-plugin-marketplace/file.tsx\n```\n\n**Incorrect usage:**\n```\nEdit(file_path=\"/s/repos/myproject/file.tsx\")\n```\n\n**Correct usage:**\n```\nEdit(file_path=\"S:\\repos\\myproject\\file.tsx\")\n```\n\n**Solution steps:**\n1. Identify the Windows drive letter (e.g., `/s/`  `S:`)\n2. Replace forward slashes with backslashes\n3. Add drive letter with colon\n4. Use absolute Windows path format\n\n### Issue 2: Git Bash MINGW Path Format\n\n**Symptom:**\nPaths like `/s/repos/` or `/c/Users/` don't work in Edit/Write/Read tools\n\n**MINGW path format explained:**\n- Git Bash uses POSIX-style paths on Windows\n- Drive letters are represented as `/c/`, `/d/`, `/s/`, etc.\n- These are MINGW virtual paths, not Windows paths\n- Claude Code tools need Windows-native paths\n\n**Conversion table:**\n| Git Bash (MINGW) | Windows Native |\n|------------------|----------------|\n| `/c/Users/name/` | `C:\\Users\\name\\` |\n| `/d/repos/project/` | `D:\\repos\\project\\` |\n| `/s/work/code/` | `S:\\work\\code\\` |\n| `/mnt/c/Windows/` | `C:\\Windows\\` |\n\n**Conversion algorithm:**\n1. Extract drive letter from first path segment (e.g., `/s/`  `S`)\n2. Add colon to drive letter (e.g., `S`  `S:`)\n3. Replace remaining forward slashes with backslashes\n4. Combine: `S:` + `\\repos\\project\\file.tsx`\n\n### Issue 3: Relative Paths in Git Bash\n\n**Symptom:**\nRelative paths from Git Bash don't resolve correctly in Claude Code tools\n\n**Cause:**\nGit Bash current working directory uses MINGW format, but Claude Code tools use Windows format\n\n**Example scenario:**\n```bash\n# In Git Bash:\npwd\n# Shows: /s/repos/my-project\n\n# User provides relative path:\n./src/components/Button.tsx\n```\n\n**Problem:**\nClaude Code can't resolve `./src/` from MINGW `/s/repos/my-project`\n\n**Solution:**\n1. Get the full Windows path from Git Bash:\n   ```bash\n   pwd -W\n   # Shows: S:/repos/my-project (Windows format with forward slashes)\n   ```\n2. Convert to proper Windows path: `S:\\repos\\my-project`\n3. Append relative path with backslashes: `S:\\repos\\my-project\\src\\components\\Button.tsx`\n\n### Issue 4: Environment Variable Expansion\n\n**Symptom:**\nPaths with environment variables like `$HOME` or `%USERPROFILE%` fail\n\n**Git Bash environment variables:**\n```bash\necho $HOME\n# Shows: /c/Users/username (MINGW format)\n```\n\n**Windows environment variables:**\n```cmd\necho %USERPROFILE%\n# Shows: C:\\Users\\username (Windows format)\n```\n\n**Best practice:**\n- Avoid environment variables in file paths for Claude Code tools\n- Use absolute Windows paths instead\n- If user provides `$HOME`, ask them to run `echo $HOME` and convert the result\n\n### Issue 5: Spaces in File Paths\n\n**Symptom:**\nPaths with spaces break or cause \"file not found\" errors\n\n**Correct handling:**\n```\n CORRECT: Edit(file_path=\"C:\\Program Files\\My App\\config.json\")\n CORRECT: Edit(file_path=\"D:\\My Documents\\project\\file.txt\")\n```\n\n**Notes:**\n- Do NOT add quotes around the path in the parameter\n- The tool call itself handles escaping\n- Spaces are fine in Windows paths when using backslashes\n\n### Issue 6: UNC Network Paths\n\n**Symptom:**\nNetwork paths like `\\\\server\\share\\file.txt` fail\n\n**Windows UNC format:**\n```\n\\\\server\\share\\folder\\file.txt\n```\n\n**Git Bash representation:**\n```\n//server/share/folder/file.txt\n```\n\n**Correct usage in Claude Code:**\n```\nEdit(file_path=\"\\\\\\\\server\\\\share\\\\folder\\\\file.txt\")\n```\n\n**Note:** Backslashes must be doubled in some contexts due to escaping, but Claude Code tools handle this automatically.\n\n##  Path Detection and Conversion Algorithm\n\nWhen a user provides a file path, follow this decision tree:\n\n### Step 1: Identify Path Format\n\n**MINGW Path (Git Bash):**\n- Starts with `/` followed by single letter and `/` (e.g., `/c/`, `/s/`)\n- Example: `/s/repos/project/file.tsx`\n- **Action:** Convert to Windows format\n\n**Windows Path:**\n- Starts with drive letter and colon (e.g., `C:`, `D:`)\n- Uses backslashes or forward slashes\n- Example: `S:\\repos\\project\\file.tsx` or `S:/repos/project/file.tsx`\n- **Action:** Ensure backslashes are used\n\n**Relative Path:**\n- Starts with `./` or `../` or just filename\n- Example: `./src/components/Button.tsx`\n- **Action:** Request full path from user or detect current directory\n\n**UNC Path:**\n- Starts with `\\\\` or `//`\n- Example: `\\\\server\\share\\file.txt`\n- **Action:** Ensure backslashes are used\n\n### Step 2: Conversion Process\n\n**For MINGW paths (`/x/...`):**\n```\nInput: /s/repos/myproject/src/components/Button.tsx\n\nProcess:\n1. Extract drive letter: \"s\"\n2. Uppercase: \"S\"\n3. Add colon: \"S:\"\n4. Replace remaining slashes: \\repos\\myproject\\src\\components\\Button.tsx\n5. Combine: S:\\repos\\myproject\\src\\components\\Button.tsx\n\nOutput: S:\\repos\\myproject\\src\\components\\Button.tsx\n```\n\n**For Windows paths with forward slashes (`X:/...`):**\n```\nInput: S:/repos/project/file.tsx\n\nProcess:\n1. Detect drive letter already present: \"S:\"\n2. Replace forward slashes with backslashes: \\repos\\project\\file.tsx\n3. Combine: S:\\repos\\project\\file.tsx\n\nOutput: S:\\repos\\project\\file.tsx\n```\n\n**For relative paths:**\n```\nInput: ./src/components/Button.tsx\nCurrent directory (from user or detection): S:\\repos\\my-project\n\nProcess:\n1. Remove ./ prefix\n2. Replace forward slashes: src\\components\\Button.tsx\n3. Combine with current directory: S:\\repos\\my-project\\src\\components\\Button.tsx\n\nOutput: S:\\repos\\my-project\\src\\components\\Button.tsx\n```\n\n##  Interactive Path Fixing Workflow\n\nWhen you encounter a file path error on Windows:\n\n### Step 1: Detect the Error\n\n**Error indicators:**\n- \"ENOENT: no such file or directory\"\n- \"file not found\"\n- Edit/Write/Read tool failure\n- User mentions \"Windows\" or \"Git Bash\"\n\n### Step 2: Analyze the Path\n\n**Ask yourself:**\n1. Was the path provided by the user in MINGW format?\n2. Does the path use forward slashes?\n3. Is it a relative path?\n4. Did I receive the path from a Git Bash command output?\n\n### Step 3: Request Clarification (If Needed)\n\n**If the path is ambiguous, ask:**\n```\nI see you're working on Windows with Git Bash. To ensure I use the correct path format,\ncould you run this command and share the output?\n\npwd -W\n\nThis will give me the Windows-formatted path.\n```\n\n### Step 4: Convert and Retry\n\n**Conversion template:**\n```\nI'll convert the path from Git Bash format to Windows format:\n- Git Bash: /s/repos/project/file.tsx\n- Windows: S:\\repos\\project\\file.tsx\n\nRetrying with the correct Windows path...\n```\n\n### Step 5: Verify Success\n\nAfter conversion, verify the operation succeeded and explain what was fixed:\n```\n Successfully edited the file using the Windows path format (S:\\repos\\...).\n\nNote: On Windows with Git Bash, always use backslashes (\\) in file paths for\nClaude Code's Edit/Write/Read tools, even though Git Bash displays paths with\nforward slashes (/).\n```\n\n##  Troubleshooting Checklist\n\nWhen file operations fail on Windows:\n\n- [ ] **Check path separator**: Are backslashes (`\\`) used instead of forward slashes (`/`)?\n- [ ] **Check drive letter format**: Is it `C:` not `/c/`?\n- [ ] **Check MINGW conversion**: Did you convert `/x/path` to `X:\\path`?\n- [ ] **Check relative vs absolute**: Is the path absolute starting with drive letter?\n- [ ] **Check environment variables**: Did you expand `$HOME` or `%USERPROFILE%`?\n- [ ] **Check spaces**: Are spaces in the path handled correctly?\n- [ ] **Check UNC paths**: Are network paths using `\\\\server\\share` format?\n- [ ] **Check file existence**: Does the file actually exist at that path?\n\n##  Quick Reference: Path Conversion Examples\n\n| Context | Path Format | Claude Code Tool Format |\n|---------|-------------|-------------------------|\n| Git Bash pwd | `/s/repos/project` | `S:\\repos\\project` |\n| Git Bash relative | `./src/file.tsx` | `S:\\repos\\project\\src\\file.tsx` |\n| Windows Explorer | `S:\\repos\\project\\file.tsx` | `S:\\repos\\project\\file.tsx`  |\n| Windows with `/` | `S:/repos/project/file.tsx` | `S:\\repos\\project\\file.tsx` |\n| MINGW full path | `/c/Users/name/file.txt` | `C:\\Users\\name\\file.txt` |\n| Network share (Git Bash) | `//server/share/file.txt` | `\\\\server\\share\\file.txt` |\n| WSL path | `/mnt/c/repos/project` | `C:\\repos\\project` |\n\n##  Best Practices for Windows File Operations\n\n### 1. Always Convert Paths Proactively\n\n**Don't wait for errors** - If you see a path that looks like MINGW format, convert it immediately:\n\n```\nUser provides: /s/repos/project/file.tsx\nYou think: \"This is MINGW format, I need to convert it to S:\\repos\\project\\file.tsx\"\nYou do: Convert before calling Edit/Write/Read tool\n```\n\n### 2. Use pwd -W in Git Bash\n\n**When you need current directory on Windows:**\n```bash\n# Instead of:\npwd                    # Shows: /s/repos/project (MINGW format)\n\n# Use:\npwd -W                 # Shows: S:/repos/project (Windows format with /)\n```\n\nThen convert the forward slashes to backslashes.\n\n### 3. Communicate Path Format Changes\n\n**Always explain when you convert paths:**\n```\nI'll convert the Git Bash path to Windows format for the Edit tool:\n- From: /s/repos/project/file.tsx\n- To: S:\\repos\\project\\file.tsx\n```\n\nThis helps users understand the requirement and learn for future interactions.\n\n### 4. Validate Before Tool Use\n\n**Before calling Edit/Write/Read tools on Windows:**\n```\nPre-flight checklist:\n Path starts with drive letter and colon (e.g., C:, S:)\n Path uses backslashes (\\) not forward slashes (/)\n Path is absolute, not relative\n No MINGW format (no /c/, /s/, etc.)\n```\n\n### 5. Handle User-Provided Paths Carefully\n\n**User might provide paths in various formats:**\n- Copy-pasted from Git Bash (MINGW format)\n- Copy-pasted from Windows Explorer (Windows format)\n- Typed manually (could be either)\n- From command output (varies by tool)\n\n**Always detect and convert as needed.**\n\n##  Common Error Messages and Solutions\n\n### Error: \"ENOENT: no such file or directory\"\n\n**Most likely cause:** Forward slashes instead of backslashes\n\n**Solution:**\n1. Check if path uses forward slashes\n2. Convert to backslashes\n3. Verify drive letter format\n4. Retry operation\n\n### Error: \"Invalid file path\"\n\n**Most likely cause:** MINGW path format\n\n**Solution:**\n1. Detect `/x/` pattern at start\n2. Convert to `X:` format\n3. Replace all forward slashes with backslashes\n4. Retry operation\n\n### Error: \"Access denied\" or \"Permission denied\"\n\n**Most likely cause:** Path is correct but permissions issue\n\n**Solution:**\n1. Verify file exists and is accessible\n2. Check if file is locked by another process\n3. Verify user has read/write permissions\n4. Consider running Git Bash as administrator\n\n### Error: \"File not found\" but path looks correct\n\n**Possible causes:**\n1. Path has hidden characters (copy-paste issue)\n2. File extension is hidden in Windows\n3. Path has trailing spaces\n4. Case sensitivity (some tools are case-sensitive)\n\n**Solution:**\n1. Ask user to run `ls -la` in Git Bash to verify exact filename\n2. Check for file extensions\n3. Trim whitespace from path\n4. Match exact case of filename\n\n##  Platform-Specific Knowledge\n\n### Windows File System Characteristics\n\n**Path characteristics:**\n- Drive letters: A-Z (typically C: for system, D-Z for additional drives)\n- Path separator: Backslash (`\\`)\n- Case insensitive: `File.txt` same as `file.txt`\n- Special characters: Avoid `< > : \" | ? *` in filenames\n- Maximum path length: 260 characters (legacy limit, can be increased)\n\n### Git Bash on Windows\n\n**Git Bash is a POSIX-compatible environment:**\n- Uses MINGW (Minimalist GNU for Windows)\n- Translates POSIX paths to Windows paths internally\n- Commands like `ls`, `pwd`, `cd` use POSIX format\n- Native Windows programs need Windows format paths\n\n**Key insight:** Git Bash displays and accepts POSIX paths, but Windows APIs (used by Claude Code) require Windows paths.\n\n### WSL (Windows Subsystem for Linux)\n\n**WSL path mounting:**\n- Windows drives mounted at `/mnt/c/`, `/mnt/d/`, etc.\n- WSL path: `/mnt/c/Users/name/project`\n- Windows path: `C:\\Users\\name\\project`\n\n**Conversion:**\n1. Replace `/mnt/x/` with `X:`\n2. Replace forward slashes with backslashes\n\n##  Teaching Users\n\nWhen explaining path issues to users, use this template:\n\n```\nI encountered a path format issue. Here's what happened:\n\n**The Problem:**\nClaude Code's file tools (Edit, Write, Read) on Windows require paths in Windows\nnative format with backslashes (\\), but Git Bash displays paths in POSIX format\nwith forward slashes (/).\n\n**The Path Formats:**\n- Git Bash shows: /s/repos/project/file.tsx\n- Windows needs: S:\\repos\\project\\file.tsx\n\n**The Solution:**\nI've converted your path to Windows format. For future reference, when working\nwith Claude Code on Windows with Git Bash:\n1. Use backslashes (\\) in file paths\n2. Use drive letter format (C:, D:, S:) not MINGW format (/c/, /d/, /s/)\n3. Run `pwd -W` in Git Bash to get Windows-formatted paths\n\n**The Fix:**\n Now using: S:\\repos\\project\\file.tsx\n```\n\n##  Advanced Scenarios\n\n### Scenario 1: Mixed Path Contexts\n\n**User is working with both WSL and Git Bash:**\n- Ask which environment they're in\n- Use appropriate conversion\n- Document the choice\n\n### Scenario 2: Symbolic Links\n\n**Windows symbolic links:**\n```\nmklink /D C:\\link C:\\target\n```\n\n**Handling:**\n- Follow the link to actual path\n- Use actual path in tool calls\n- Inform user if link resolution needed\n\n### Scenario 3: Docker Volumes\n\n**Docker volume mounts on Windows:**\n```\ndocker run -v C:\\repos:/app\n```\n\n**Path translation:**\n- Outside container: `C:\\repos\\file.txt`\n- Inside container: `/app/file.txt`\n- Use context-appropriate format\n\n##  Success Criteria\n\nYou've successfully handled Windows paths when:\n\n1.  All Edit/Write/Read tool calls use backslashes on Windows\n2.  MINGW paths are converted before tool use\n3.  Relative paths are resolved to absolute Windows paths\n4.  User understands why conversion was necessary\n5.  File operations succeed without path-related errors\n6.  Path format is consistent throughout the session\n\n##  When to Use This Skill\n\n**PROACTIVELY apply this knowledge when:**\n1. User mentions they're on Windows\n2. User mentions Git Bash or MINGW\n3. You see paths starting with `/c/`, `/s/`, etc.\n4. Edit/Write/Read tool fails with \"file not found\"\n5. User provides paths with forward slashes on Windows\n6. You need to read/edit/write files on Windows system\n\n**This skill is CRITICAL for Windows users** - Path format errors are the #1 cause of file operation failures on Windows with Git Bash."
              }
            ]
          },
          {
            "name": "dotnet-microservices-master",
            "description": "Expert agent on .NET microservices architecture, containerization, Docker, DDD, CQRS, and cloud-native patterns based on Microsoft's official guide. PROACTIVELY activate for: (1) ANY .NET microservices architecture task, (2) Docker containerization for .NET applications, (3) Domain-Driven Design (DDD) tactical and strategic patterns, (4) CQRS and Event Sourcing implementation, (5) Microservices communication patterns (sync/async), (6) API Gateway design with Ocelot, (7) Event-driven architecture with message brokers, (8) Resilience patterns (Circuit Breaker, Retry, Bulkhead) with Polly, (9) Entity Framework Core and Dapper for CQRS, (10) Kubernetes and Azure deployment strategies, (11) eShopOnContainers reference application guidance. Provides: comprehensive Microsoft .NET Microservices Architecture guide (v7.0), 350-page reference material via Agent Skill, Docker and container fundamentals, .NET 7+ best practices, microservices design patterns, DDD aggregate and repository patterns, CQRS read/write model separation, integration events and event bus patterns, security with IdentityServer and OAuth 2.0, health checks and monitoring, production-ready architecture guidance. Ensures scalable, resilient, maintainable microservices systems using .NET and Docker following Microsoft official guidance.",
            "source": "./plugins/dotnet-microservices-master",
            "category": null,
            "version": "1.0.2",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install dotnet-microservices-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "microsoft-guide",
                "description": "Complete Microsoft .NET Microservices Architecture guide (v7.0) - 350 pages of comprehensive reference material",
                "path": "plugins/dotnet-microservices-master/skills/microsoft-guide/SKILL.md",
                "frontmatter": {
                  "name": "microsoft-guide",
                  "description": "Complete Microsoft .NET Microservices Architecture guide (v7.0) - 350 pages of comprehensive reference material"
                },
                "content": "# Microsoft .NET Microservices Architecture Guide (v7.0)\n\nThis skill provides access to the complete official Microsoft guide: \".NET Microservices Architecture for Containerized .NET Applications\" (Edition v7.0 - Updated to ASP.NET Core 7.0).\n\n## When to Use This Skill\n\nInvoke this skill when you need:\n- **Detailed technical information** from the official Microsoft guide\n- **Specific implementation patterns** or code examples\n- **In-depth explanations** beyond your summarized knowledge\n- **Official Microsoft recommendations** for microservices architecture\n- **Reference to eShopOnContainers** implementation details\n- **Comprehensive coverage** of specific topics (DDD, CQRS, Docker, Kubernetes, etc.)\n\n## What This Skill Contains\n\nThe complete 350-page Microsoft guide (text content only) including:\n- Container fundamentals and Docker concepts\n- Choosing between .NET and .NET Framework\n- Microservices architecture principles\n- API Gateway patterns\n- Event-driven architecture\n- Domain-Driven Design (DDD) tactical and strategic patterns\n- CQRS (Command Query Responsibility Segregation)\n- Data management in microservices\n- Resilience patterns (Circuit Breaker, Retry, Bulkhead)\n- Security best practices\n- Kubernetes and orchestration\n- eShopOnContainers reference application details\n- Complete code examples and implementation guidance\n- All architectural concepts explained in text (diagrams removed for size optimization)\n\n## How to Use\n\nWhen a user asks about:\n1. Specific technical details you need to verify\n2. Code implementation examples\n3. eShopOnContainers architecture details\n4. Official Microsoft recommendations\n5. Complex patterns requiring detailed explanations\n\nInvoke this skill and reference the full guide content below.\n\n---\n\n## Full Microsoft Guide Content\n\nThe complete guide is available in this skill directory at:\n`NET-Microservices-Architecture.md`\n\nWhen you invoke this skill, you have access to read this file which contains the full 350-page Microsoft guide with all technical details, code examples, architecture patterns, and implementation guidance.\n\n## Usage Instructions\n\nWhen this skill is invoked:\n1. Read the NET-Microservices-Architecture.md file in this directory\n2. Use Grep to search for specific topics the user is asking about\n3. Provide detailed, accurate answers based on the official Microsoft guide content\n4. Reference specific sections and code examples from the guide\n5. Cite the guide as the source of information\n\n**Note**: The guide contains comprehensive text explanations of all concepts, patterns, and implementations. Image references have been removed to optimize plugin size (reduced from 18MB to ~800KB)."
              }
            ]
          },
          {
            "name": "ffmpeg-master",
            "description": "Complete FFmpeg expertise system with comprehensive coverage of FFmpeg 8.0.1 (2025-11-20) features, hardware acceleration (NVIDIA NVENC, Intel QSV, AMD AMF, VAAPI, Vulkan), Docker/container usage, Modal.com serverless containers (CPU/GPU), Cloudflare Workers/WebAssembly (ffmpeg.wasm), CI/CD runner optimization (GitHub Actions), live streaming (RTMP/HLS/DASH/SRT/WebRTC/WHIP), audio processing and normalization, encoding optimization (CRF, presets, quality tuning), and common gotchas/troubleshooting. PROACTIVELY activate for: (1) ANY FFmpeg task (encoding, transcoding, streaming, filtering), (2) Video format conversion (MP4, WebM, MKV, AVI), (3) Hardware acceleration setup (NVENC, CUDA, QSV, VAAPI, AMF, Vulkan), (4) Docker container FFmpeg usage (jrottenberg/ffmpeg, linuxserver/ffmpeg), (5) Modal.com serverless video processing (GPU/CPU containers, parallel processing, volumes), (6) Browser/WebAssembly FFmpeg (ffmpeg.wasm), (7) Cloudflare Workers video processing, (8) GitHub Actions/CI pipeline video processing, (9) Live streaming setup (RTMP ingest, HLS/DASH/WHIP/WebRTC delivery), (10) Audio encoding/normalization (AAC, MP3, loudnorm, EBU R128), (11) Quality optimization (CRF, bitrate, presets), (12) FFmpeg debugging and error resolution, (13) VVC/H.266, AV1, HEVC encoding, (14) Filter graphs and complex filtergraphs, (15) Batch processing and automation, (16) Whisper AI subtitle generation. Provides: FFmpeg 8.0.1 patch release features, FFmpeg 8.0 Huffman features (Whisper AI filter, Vulkan compute codecs for FFv1/ProRes RAW/AV1/VP9, APV codec, VVC VA-API/QSV decoding, WHIP muxer for WebRTC, colordetect/pad_cuda/scale_d3d11 filters, ProRes RAW decoder, RealVideo 6.0, G.728), FFmpeg 7.1 LTS features (VVC decoder, MV-HEVC for Apple Vision Pro, xHE-AAC, Vulkan H.264/H.265), hardware acceleration guides, Docker best practices, Modal.com parallel processing patterns, WebAssembly limitations and workarounds, CI/CD optimization strategies, streaming protocol guidance, audio processing workflows, encoding presets and recommendations, systematic debugging approaches, and production-ready 2025 configurations for all platforms.",
            "source": "./plugins/ffmpeg-master",
            "category": null,
            "version": "3.2.0",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install ffmpeg-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/ffmpeg-audio",
                "description": null,
                "path": "plugins/ffmpeg-master/commands/ffmpeg-audio.md",
                "frontmatter": null,
                "content": "---\nname: Process Audio\ndescription: Process audio with FFmpeg - extraction, conversion, normalization, and professional audio workflows\nargument-hint: <input-file> [operation: extract|convert|normalize]\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Audio Processing\n\n## Purpose\nExtract, convert, normalize, and process audio using FFmpeg with professional-grade settings.\n\n## Workflow\n\n### 1. Analyze Audio\n```bash\n# Get audio stream info\nffprobe -v error -select_streams a -show_entries stream=codec_name,sample_rate,channels,bit_rate -of default=noprint_wrappers=1 INPUT\n```\n\n### 2. Common Operations\n\n#### Extract Audio\n```bash\n# Copy without re-encoding\nffmpeg -i video.mp4 -vn -c:a copy audio.m4a\n\n# Convert to MP3\nffmpeg -i video.mp4 -vn -c:a libmp3lame -b:a 320k audio.mp3\n\n# Convert to high-quality AAC\nffmpeg -i video.mp4 -vn -c:a aac -b:a 256k audio.m4a\n```\n\n#### Convert Formats\n```bash\n# FLAC to MP3\nffmpeg -i input.flac -c:a libmp3lame -b:a 320k output.mp3\n\n# WAV to AAC\nffmpeg -i input.wav -c:a aac -b:a 256k output.m4a\n\n# MP3 to Opus (for WebM)\nffmpeg -i input.mp3 -c:a libopus -b:a 128k output.opus\n```\n\n#### Normalize Audio (EBU R128)\n```bash\n# Single-pass (quick)\nffmpeg -i input.mp3 -af \"loudnorm=I=-16:TP=-1.5:LRA=11\" -ar 48000 output.mp3\n\n# Two-pass (best quality) - see ffmpeg-audio-processing skill for script\n```\n\n### 3. Audio Filters\n\n#### Volume Adjustment\n```bash\n# Increase by 50%\nffmpeg -i input.mp3 -af \"volume=1.5\" output.mp3\n\n# Increase by 6dB\nffmpeg -i input.mp3 -af \"volume=6dB\" output.mp3\n```\n\n#### Fade Effects\n```bash\n# Fade in 3s, fade out 3s (60s audio)\nffmpeg -i input.mp3 -af \"afade=t=in:d=3,afade=t=out:st=57:d=3\" output.mp3\n```\n\n#### Noise Reduction\n```bash\nffmpeg -i input.mp3 -af \"afftdn=nf=-25\" output.mp3\n```\n\n#### Compression\n```bash\nffmpeg -i input.mp3 -af \"acompressor=threshold=-20dB:ratio=4:attack=5:release=50\" output.mp3\n```\n\n### 4. Bitrate Reference\n\n| Codec | Speech | Music (Good) | Music (High) |\n|-------|--------|--------------|--------------|\n| AAC | 64-96k | 128-192k | 256-320k |\n| MP3 | 96-128k | 192-256k | 320k |\n| Opus | 32-64k | 96-128k | 160-256k |\n\n### 5. Podcast Processing Chain\n```bash\nffmpeg -i raw_podcast.wav \\\n  -af \"highpass=f=80,acompressor=threshold=-20dB:ratio=4,loudnorm=I=-16:TP=-1.5:LRA=11\" \\\n  -c:a aac -b:a 96k \\\n  podcast.m4a\n```\n\n## Output\n\nProvide:\n1. Complete FFmpeg command for the audio operation\n2. Codec and bitrate recommendations for use case\n3. Quality comparison if re-encoding\n4. Batch processing command if multiple files\n5. Verification command to check output\n"
              },
              {
                "name": "/ffmpeg-batch-social",
                "description": "Export video content for multiple social platforms simultaneously (TikTok, YouTube Shorts, Instagram Reels, Twitter/X) with platform-optimized encoding",
                "path": "plugins/ffmpeg-master/commands/ffmpeg-batch-social.md",
                "frontmatter": {
                  "name": "Batch Social Export",
                  "description": "Export video content for multiple social platforms simultaneously (TikTok, YouTube Shorts, Instagram Reels, Twitter/X) with platform-optimized encoding",
                  "argument-hint": "<input-file> [--platforms] [--captions] [--clips]"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Batch Social Media Export\n\n## Purpose\n\nProcess a single video into multiple platform-optimized outputs simultaneously:\n- **TikTok** (9:16, H.264, 60s max)\n- **YouTube Shorts** (9:16, H.264/VP9, 59s max)\n- **Instagram Reels** (9:16, H.264, 90s max)\n- **Facebook Reels** (9:16, H.264, 90s max)\n- **Twitter/X** (9:16, H.264, 140s max)\n- **Snapchat Spotlight** (9:16, H.264, 60s max)\n\nAlso includes:\n- Automatic caption generation\n- Thumbnail extraction\n- Content calendar-ready file organization\n\n## Quick Start\n\n### Export to All Platforms\n\n```bash\n# Basic batch export\n./batch_social.sh input.mp4\n\n# With captions\n./batch_social.sh input.mp4 --captions\n\n# Specific platforms only\n./batch_social.sh input.mp4 --platforms tiktok,shorts,reels\n```\n\n## Complete Batch Export Script\n\n```bash\n#!/bin/bash\n# batch_social.sh - Export video to all social platforms simultaneously\n#\n# Usage: ./batch_social.sh <input_video> [options]\n# Options:\n#   --captions      Generate and burn auto-captions\n#   --platforms     Comma-separated list: tiktok,shorts,reels,facebook,twitter,snapchat\n#   --output-dir    Output directory (default: ./social_exports)\n#   --clips N       Extract N clips from longer video\n#   --hook TEXT     Add text hook overlay to first 2 seconds\n\nset -e\n\nINPUT=\"$1\"\nshift\n\n# Default settings\nCAPTIONS=false\nPLATFORMS=\"tiktok,shorts,reels,twitter\"\nOUTPUT_DIR=\"./social_exports\"\nCLIP_COUNT=0\nHOOK_TEXT=\"\"\n\n# Parse arguments\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        --captions)\n            CAPTIONS=true\n            shift\n            ;;\n        --platforms)\n            PLATFORMS=\"$2\"\n            shift 2\n            ;;\n        --output-dir)\n            OUTPUT_DIR=\"$2\"\n            shift 2\n            ;;\n        --clips)\n            CLIP_COUNT=\"$2\"\n            shift 2\n            ;;\n        --hook)\n            HOOK_TEXT=\"$2\"\n            shift 2\n            ;;\n        *)\n            echo \"Unknown option: $1\"\n            exit 1\n            ;;\n    esac\ndone\n\n# Validate input\nif [ -z \"$INPUT\" ] || [ ! -f \"$INPUT\" ]; then\n    echo \"Error: Input file not found: $INPUT\"\n    exit 1\nfi\n\nBASENAME=$(basename \"$INPUT\" .mp4)\nmkdir -p \"$OUTPUT_DIR\"\n\necho \"===========================================\"\necho \"   BATCH SOCIAL MEDIA EXPORT\"\necho \"===========================================\"\necho \"Input: $INPUT\"\necho \"Platforms: $PLATFORMS\"\necho \"Captions: $CAPTIONS\"\necho \"Output: $OUTPUT_DIR\"\necho \"===========================================\"\n\n# Get video duration\nDURATION=$(ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$INPUT\")\necho \"Video duration: ${DURATION}s\"\n\n# Generate captions if requested\nCAPTION_FILTER=\"\"\nif [ \"$CAPTIONS\" = true ]; then\n    echo \"\"\n    echo \"[STEP] Generating captions with Whisper...\"\n    ffmpeg -y -i \"$INPUT\" -vn \\\n        -af \"whisper=model=ggml-base.bin:language=auto:format=srt\" \\\n        \"${OUTPUT_DIR}/captions.srt\" 2>/dev/null || echo \"Warning: Whisper failed, continuing without captions\"\n\n    if [ -f \"${OUTPUT_DIR}/captions.srt\" ]; then\n        CAPTION_FILTER=\",subtitles=${OUTPUT_DIR}/captions.srt:force_style='FontName=Arial Black,FontSize=52,PrimaryColour=&HFFFFFF,OutlineColour=&H000000,Outline=4,Bold=1,Alignment=2,MarginV=200'\"\n    fi\nfi\n\n# Hook filter\nHOOK_FILTER=\"\"\nif [ -n \"$HOOK_TEXT\" ]; then\n    HOOK_FILTER=\",drawtext=text='${HOOK_TEXT}':fontsize=64:fontcolor=yellow:borderw=4:bordercolor=black:x=(w-tw)/2:y=h*0.12:enable='between(t,0,2.5)'\"\nfi\n\n# Function to export for a platform\nexport_platform() {\n    local PLATFORM=\"$1\"\n    local OUTPUT=\"${OUTPUT_DIR}/${BASENAME}_${PLATFORM}.mp4\"\n\n    case $PLATFORM in\n        tiktok)\n            local MAX_DURATION=60\n            local AUDIO_RATE=44100\n            local AUDIO_BITRATE=128k\n            local CRF=23\n            ;;\n        shorts)\n            local MAX_DURATION=59\n            local AUDIO_RATE=48000\n            local AUDIO_BITRATE=192k\n            local CRF=22\n            ;;\n        reels|facebook)\n            local MAX_DURATION=90\n            local AUDIO_RATE=44100\n            local AUDIO_BITRATE=128k\n            local CRF=23\n            ;;\n        twitter)\n            local MAX_DURATION=140\n            local AUDIO_RATE=44100\n            local AUDIO_BITRATE=128k\n            local CRF=23\n            ;;\n        snapchat)\n            local MAX_DURATION=60\n            local AUDIO_RATE=44100\n            local AUDIO_BITRATE=128k\n            local CRF=24\n            ;;\n        *)\n            echo \"Unknown platform: $PLATFORM\"\n            return 1\n            ;;\n    esac\n\n    # Calculate actual duration to use\n    local USE_DURATION=$(echo \"$DURATION $MAX_DURATION\" | awk '{if($1<$2) print $1; else print $2}')\n\n    echo \"[${PLATFORM^^}] Exporting (max ${MAX_DURATION}s)...\"\n\n    ffmpeg -y -i \"$INPUT\" \\\n        -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1${HOOK_FILTER}${CAPTION_FILTER}\" \\\n        -c:v libx264 -preset fast -crf $CRF -profile:v high -level 4.1 \\\n        -c:a aac -b:a $AUDIO_BITRATE -ar $AUDIO_RATE -ac 2 \\\n        -pix_fmt yuv420p \\\n        -movflags +faststart \\\n        -t $USE_DURATION \\\n        \"$OUTPUT\" 2>/dev/null\n\n    # Extract thumbnail\n    ffmpeg -y -i \"$OUTPUT\" -ss 00:00:03 -vframes 1 -q:v 2 \\\n        \"${OUTPUT_DIR}/${BASENAME}_${PLATFORM}_thumb.jpg\" 2>/dev/null\n\n    local SIZE=$(ls -lh \"$OUTPUT\" | awk '{print $5}')\n    echo \"[${PLATFORM^^}] Done: $OUTPUT ($SIZE)\"\n}\n\n# Run exports in parallel\necho \"\"\necho \"[STEP] Starting platform exports...\"\n\nIFS=',' read -ra PLATFORM_ARRAY <<< \"$PLATFORMS\"\nPIDS=()\n\nfor PLATFORM in \"${PLATFORM_ARRAY[@]}\"; do\n    export_platform \"$PLATFORM\" &\n    PIDS+=($!)\ndone\n\n# Wait for all exports to complete\nfor PID in \"${PIDS[@]}\"; do\n    wait $PID\ndone\n\necho \"\"\necho \"===========================================\"\necho \"   EXPORT COMPLETE\"\necho \"===========================================\"\necho \"\"\nls -lh \"$OUTPUT_DIR\"/${BASENAME}_*\necho \"\"\necho \"Files ready for upload in: $OUTPUT_DIR\"\n```\n\n## Individual Platform Commands\n\n### TikTok Export\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1\" \\\n  -c:v libx264 -preset fast -crf 23 -profile:v high -level 4.1 \\\n  -c:a aac -b:a 128k -ar 44100 -ac 2 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 60 \\\n  output_tiktok.mp4\n```\n\n### YouTube Shorts Export\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1\" \\\n  -c:v libx264 -preset fast -crf 22 -profile:v high -level 4.2 \\\n  -c:a aac -b:a 192k -ar 48000 -ac 2 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 59 \\\n  output_shorts.mp4\n```\n\n### Instagram Reels Export\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1,fps=30\" \\\n  -c:v libx264 -preset fast -crf 23 -profile:v high -level 4.1 \\\n  -c:a aac -b:a 128k -ar 44100 -ac 2 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 90 \\\n  output_reels.mp4\n```\n\n### Twitter/X Export\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1,fps=30\" \\\n  -c:v libx264 -preset fast -crf 23 -profile:v high -level 4.1 \\\n  -c:a aac -b:a 128k -ar 44100 -ac 2 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 140 \\\n  output_twitter.mp4\n```\n\n## Content Calendar Workflow\n\n### Weekly Content Generation Script\n\n```bash\n#!/bin/bash\n# weekly_content.sh - Generate a week's worth of social content from source videos\n#\n# Usage: ./weekly_content.sh source_videos_dir output_dir\n\nSOURCE_DIR=\"$1\"\nOUTPUT_DIR=\"${2:-./weekly_content}\"\nWEEK=$(date +%Y-W%V)\n\nmkdir -p \"$OUTPUT_DIR/$WEEK\"/{monday,tuesday,wednesday,thursday,friday,saturday,sunday}\n\nDAYS=(monday tuesday wednesday thursday friday saturday sunday)\nDAY_INDEX=0\n\nfor VIDEO in \"$SOURCE_DIR\"/*.mp4; do\n    [ -f \"$VIDEO\" ] || continue\n\n    BASENAME=$(basename \"$VIDEO\" .mp4)\n    DAY=\"${DAYS[$DAY_INDEX]}\"\n    DAY_DIR=\"$OUTPUT_DIR/$WEEK/$DAY\"\n\n    echo \"Processing $BASENAME for $DAY...\"\n\n    # Export to all platforms\n    ffmpeg -y -i \"$VIDEO\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 23 \\\n      -c:a aac -b:a 128k \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      -t 60 \\\n      \"$DAY_DIR/${BASENAME}_tiktok.mp4\" &\n\n    ffmpeg -y -i \"$VIDEO\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 22 \\\n      -c:a aac -b:a 192k -ar 48000 \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      -t 59 \\\n      \"$DAY_DIR/${BASENAME}_shorts.mp4\" &\n\n    ffmpeg -y -i \"$VIDEO\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,fps=30\" \\\n      -c:v libx264 -preset fast -crf 23 \\\n      -c:a aac -b:a 128k \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      -t 90 \\\n      \"$DAY_DIR/${BASENAME}_reels.mp4\" &\n\n    wait\n\n    # Extract thumbnails\n    ffmpeg -y -i \"$DAY_DIR/${BASENAME}_tiktok.mp4\" -ss 3 -vframes 1 \"$DAY_DIR/${BASENAME}_thumb.jpg\" 2>/dev/null\n\n    DAY_INDEX=$(( (DAY_INDEX + 1) % 7 ))\ndone\n\necho \"\"\necho \"Weekly content generated in: $OUTPUT_DIR/$WEEK\"\nfind \"$OUTPUT_DIR/$WEEK\" -name \"*.mp4\" | wc -l\necho \"videos created\"\n```\n\n## Long Video to Clips Workflow\n\n### Extract Multiple Clips from Long Video\n\n```bash\n#!/bin/bash\n# extract_clips.sh - Extract viral clips from long-form content\n#\n# Usage: ./extract_clips.sh long_video.mp4 timestamps.txt output_dir\n#\n# timestamps.txt format (one per line):\n# 00:01:30-00:02:15 Hook about topic A\n# 00:05:00-00:05:45 Best moment\n# 00:10:20-00:11:10 Viral potential clip\n\nLONG_VIDEO=\"$1\"\nTIMESTAMPS=\"$2\"\nOUTPUT_DIR=\"${3:-./clips}\"\n\nmkdir -p \"$OUTPUT_DIR\"\n\nCLIP_NUM=1\nwhile IFS= read -r LINE; do\n    # Skip empty lines and comments\n    [[ -z \"$LINE\" || \"$LINE\" =~ ^# ]] && continue\n\n    # Parse timestamp and description\n    TIMERANGE=$(echo \"$LINE\" | awk '{print $1}')\n    DESCRIPTION=$(echo \"$LINE\" | cut -d' ' -f2-)\n\n    START=$(echo \"$TIMERANGE\" | cut -d'-' -f1)\n    END=$(echo \"$TIMERANGE\" | cut -d'-' -f2)\n\n    # Calculate duration\n    START_SEC=$(echo \"$START\" | awk -F: '{print ($1*3600)+($2*60)+$3}')\n    END_SEC=$(echo \"$END\" | awk -F: '{print ($1*3600)+($2*60)+$3}')\n    DURATION=$((END_SEC - START_SEC))\n\n    OUTPUT_NAME=$(printf \"clip_%02d\" $CLIP_NUM)\n\n    echo \"Extracting clip $CLIP_NUM: $DESCRIPTION ($DURATION seconds)\"\n\n    # Extract and process for all platforms\n    ffmpeg -y -ss \"$START\" -i \"$LONG_VIDEO\" -t \"$DURATION\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 23 \\\n      -c:a aac -b:a 128k \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      \"$OUTPUT_DIR/${OUTPUT_NAME}_tiktok.mp4\" &\n\n    ffmpeg -y -ss \"$START\" -i \"$LONG_VIDEO\" -t \"$DURATION\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 22 \\\n      -c:a aac -b:a 192k -ar 48000 \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      \"$OUTPUT_DIR/${OUTPUT_NAME}_shorts.mp4\" &\n\n    wait\n\n    CLIP_NUM=$((CLIP_NUM + 1))\ndone < \"$TIMESTAMPS\"\n\necho \"\"\necho \"Extracted $((CLIP_NUM - 1)) clips to: $OUTPUT_DIR\"\nls -lh \"$OUTPUT_DIR\"\n```\n\n## PowerShell Version (Windows)\n\n```powershell\n# batch_social.ps1 - Windows PowerShell version\n# Usage: .\\batch_social.ps1 -Input \"input.mp4\" -Platforms \"tiktok,shorts,reels\"\n\nparam(\n    [Parameter(Mandatory=$true)]\n    [string]$Input,\n\n    [string]$Platforms = \"tiktok,shorts,reels,twitter\",\n\n    [string]$OutputDir = \".\\social_exports\",\n\n    [switch]$Captions\n)\n\n$BaseName = [System.IO.Path]::GetFileNameWithoutExtension($Input)\n\nif (-not (Test-Path $OutputDir)) {\n    New-Item -ItemType Directory -Path $OutputDir | Out-Null\n}\n\nWrite-Host \"===========================================\"\nWrite-Host \"   BATCH SOCIAL MEDIA EXPORT (Windows)\"\nWrite-Host \"===========================================\"\nWrite-Host \"Input: $Input\"\nWrite-Host \"Platforms: $Platforms\"\nWrite-Host \"Output: $OutputDir\"\nWrite-Host \"===========================================\"\n\n$PlatformList = $Platforms -split \",\"\n\n$Jobs = @()\n\nforeach ($Platform in $PlatformList) {\n    $Output = \"$OutputDir\\${BaseName}_${Platform}.mp4\"\n\n    switch ($Platform) {\n        \"tiktok\" {\n            $MaxDuration = 60\n            $AudioRate = 44100\n            $AudioBitrate = \"128k\"\n            $CRF = 23\n        }\n        \"shorts\" {\n            $MaxDuration = 59\n            $AudioRate = 48000\n            $AudioBitrate = \"192k\"\n            $CRF = 22\n        }\n        \"reels\" {\n            $MaxDuration = 90\n            $AudioRate = 44100\n            $AudioBitrate = \"128k\"\n            $CRF = 23\n        }\n        \"twitter\" {\n            $MaxDuration = 140\n            $AudioRate = 44100\n            $AudioBitrate = \"128k\"\n            $CRF = 23\n        }\n    }\n\n    Write-Host \"[$($Platform.ToUpper())] Starting export...\"\n\n    $Job = Start-Job -ScriptBlock {\n        param($Input, $Output, $MaxDuration, $AudioRate, $AudioBitrate, $CRF)\n\n        ffmpeg -y -i $Input `\n            -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1\" `\n            -c:v libx264 -preset fast -crf $CRF -profile:v high -level 4.1 `\n            -c:a aac -b:a $AudioBitrate -ar $AudioRate -ac 2 `\n            -pix_fmt yuv420p `\n            -movflags +faststart `\n            -t $MaxDuration `\n            $Output\n\n    } -ArgumentList $Input, $Output, $MaxDuration, $AudioRate, $AudioBitrate, $CRF\n\n    $Jobs += $Job\n}\n\n# Wait for all jobs\n$Jobs | Wait-Job | Out-Null\n\n# Get results\nforeach ($Job in $Jobs) {\n    Receive-Job -Job $Job\n    Remove-Job -Job $Job\n}\n\nWrite-Host \"\"\nWrite-Host \"===========================================\"\nWrite-Host \"   EXPORT COMPLETE\"\nWrite-Host \"===========================================\"\nGet-ChildItem \"$OutputDir\\${BaseName}_*\" | Format-Table Name, Length\n```\n\n## Verification and Quality Check\n\n```bash\n#!/bin/bash\n# verify_exports.sh - Verify all exported files meet platform requirements\n\nOUTPUT_DIR=\"${1:-./social_exports}\"\n\necho \"Verifying exports in: $OUTPUT_DIR\"\necho \"\"\n\nfor FILE in \"$OUTPUT_DIR\"/*.mp4; do\n    [ -f \"$FILE\" ] || continue\n\n    FILENAME=$(basename \"$FILE\")\n\n    # Get video info\n    INFO=$(ffprobe -v error -select_streams v:0 \\\n        -show_entries stream=width,height,codec_name,pix_fmt \\\n        -show_entries format=duration,size \\\n        -of csv=p=0 \"$FILE\")\n\n    WIDTH=$(echo \"$INFO\" | cut -d',' -f1)\n    HEIGHT=$(echo \"$INFO\" | cut -d',' -f2)\n    CODEC=$(echo \"$INFO\" | cut -d',' -f3)\n    PIX_FMT=$(echo \"$INFO\" | cut -d',' -f4)\n    DURATION=$(echo \"$INFO\" | cut -d',' -f5)\n    SIZE=$(echo \"$INFO\" | cut -d',' -f6)\n\n    SIZE_MB=$(echo \"scale=2; $SIZE / 1048576\" | bc)\n\n    # Check requirements\n    STATUS=\"\"\n    ISSUES=\"\"\n\n    if [ \"$WIDTH\" != \"1080\" ] || [ \"$HEIGHT\" != \"1920\" ]; then\n        STATUS=\"\"\n        ISSUES+=\" Wrong resolution\"\n    fi\n\n    if [ \"$CODEC\" != \"h264\" ]; then\n        STATUS=\"\"\n        ISSUES+=\" Wrong codec\"\n    fi\n\n    if [ \"$PIX_FMT\" != \"yuv420p\" ]; then\n        STATUS=\"\"\n        ISSUES+=\" Check pixel format\"\n    fi\n\n    printf \"%-40s %s %.1fs %.1fMB %s\\n\" \"$FILENAME\" \"$STATUS\" \"$DURATION\" \"$SIZE_MB\" \"$ISSUES\"\ndone\n```\n\n## Output Structure\n\nAfter running batch export, you'll have:\n\n```\nsocial_exports/\n video_tiktok.mp4        # TikTok optimized (60s max)\n video_tiktok_thumb.jpg  # TikTok thumbnail\n video_shorts.mp4        # YouTube Shorts (59s max)\n video_shorts_thumb.jpg  # Shorts thumbnail\n video_reels.mp4         # Instagram Reels (90s max)\n video_reels_thumb.jpg   # Reels thumbnail\n video_twitter.mp4       # Twitter/X (140s max)\n video_twitter_thumb.jpg # Twitter thumbnail\n captions.srt            # Generated captions (if enabled)\n```\n\n## Related Skills\n\n- `viral-video-platform-specs` - Platform requirements reference\n- `ffmpeg-viral-tiktok` - TikTok-specific optimization\n- `ffmpeg-viral-shorts` - YouTube Shorts optimization\n- `viral-video-animated-captions` - Advanced caption styling"
              },
              {
                "name": "/ffmpeg-color",
                "description": null,
                "path": "plugins/ffmpeg-master/commands/ffmpeg-color.md",
                "frontmatter": null,
                "content": "---\nname: Apply Color Effects\ndescription: Apply color grading, LUTs, chromakey (green screen), and cinematic color effects\nargument-hint: <effect: lut|chromakey|grade|cinematic> [input-file]\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Color Effects\n\n## Purpose\nApply professional color grading, LUTs (Look-Up Tables), chromakey (green screen removal), and cinematic color effects to video.\n\n## Workflow\n\n### 1. Analyze Input\n```bash\nffprobe -v error -show_entries stream=width,height,pix_fmt -of default=noprint_wrappers=1 INPUT\n```\n\n### 2. Color Effect Presets\n\n#### LUT Application (3D Look-Up Tables)\n```bash\n# Apply .cube LUT file\nffmpeg -i INPUT -vf \"lut3d=file='cinematic.cube'\" OUTPUT\n\n# Apply with intensity control (0-1)\nffmpeg -i INPUT -vf \"lut3d=file='vintage.cube':interp=tetrahedral\" OUTPUT\n```\n\n#### Chromakey (Green Screen Removal)\n```bash\n# Standard green screen\nffmpeg -i INPUT -vf \"chromakey=0x00FF00:0.3:0.1\" -c:v libx264 OUTPUT\n\n# Blue screen\nffmpeg -i INPUT -vf \"chromakey=0x0000FF:0.3:0.1\" OUTPUT\n\n# Fine-tuned green (studio green)\nffmpeg -i INPUT -vf \"chromakey=0x00B140:0.25:0.05\" OUTPUT\n```\n\n#### Colorkey (Non-chroma keying)\n```bash\n# Remove specific color\nffmpeg -i INPUT -vf \"colorkey=0xFFFFFF:0.3:0.2\" OUTPUT\n```\n\n#### Green Screen Composite\n```bash\n# Composite foreground over background\nffmpeg -i foreground.mp4 -i background.mp4 \\\n  -filter_complex \"[0:v]chromakey=0x00FF00:0.3:0.1[fg];[1:v][fg]overlay=format=auto\" \\\n  -c:v libx264 -crf 18 OUTPUT\n```\n\n### 3. Cinematic Color Presets\n\n#### Teal and Orange (Blockbuster Look)\n```bash\nffmpeg -i INPUT \\\n  -vf \"curves=b='0/0.1 0.5/0.4 1/0.8':r='0/0 0.5/0.6 1/1',eq=saturation=1.2:contrast=1.1\" \\\n  OUTPUT\n```\n\n#### Film Emulation (Vintage)\n```bash\nffmpeg -i INPUT \\\n  -vf \"curves=preset=vintage,eq=saturation=0.9:brightness=0.02\" \\\n  OUTPUT\n```\n\n#### High Contrast Noir\n```bash\nffmpeg -i INPUT \\\n  -vf \"eq=contrast=1.5:brightness=-0.1:saturation=0.1\" \\\n  OUTPUT\n```\n\n#### Warm/Golden Hour\n```bash\nffmpeg -i INPUT \\\n  -vf \"colorbalance=rs=0.15:gs=0.05:bs=-0.1:rm=0.1:gm=0.05:bm=-0.05\" \\\n  OUTPUT\n```\n\n#### Cool/Moonlight\n```bash\nffmpeg -i INPUT \\\n  -vf \"colorbalance=rs=-0.1:bs=0.2:rm=-0.1:bm=0.15,eq=brightness=-0.1\" \\\n  OUTPUT\n```\n\n### 4. Color Adjustment Filters\n\n#### Color Balance\n```bash\n# Shadows (s), Midtones (m), Highlights (h)\nffmpeg -i INPUT -vf \"colorbalance=rs=0.1:gs=0:bs=-0.1:rm=0.05:gm=0:bm=-0.05\" OUTPUT\n```\n\n#### Curves Adjustment\n```bash\n# Boost contrast with S-curve\nffmpeg -i INPUT -vf \"curves=all='0/0 0.25/0.15 0.5/0.5 0.75/0.85 1/1'\" OUTPUT\n\n# RGB channel manipulation\nffmpeg -i INPUT -vf \"curves=r='0/0 0.5/0.6 1/1':g='0/0 0.5/0.5 1/1':b='0/0.1 0.5/0.4 1/0.9'\" OUTPUT\n```\n\n#### Saturation and Vibrance\n```bash\n# Increase saturation\nffmpeg -i INPUT -vf \"eq=saturation=1.5\" OUTPUT\n\n# Selective color boost (vibrance-like)\nffmpeg -i INPUT -vf \"hue=s=1.3\" OUTPUT\n```\n\n#### White Balance / Color Temperature\n```bash\n# Warm (increase color temperature)\nffmpeg -i INPUT -vf \"colortemperature=temperature=6500\" OUTPUT\n\n# Cool (decrease color temperature)\nffmpeg -i INPUT -vf \"colortemperature=temperature=4500\" OUTPUT\n```\n\n### 5. Professional Workflows\n\n#### Spill Suppression (After Chromakey)\n```bash\nffmpeg -i INPUT \\\n  -filter_complex \"[0:v]chromakey=0x00FF00:0.3:0.1,despill=type=green:mix=0.5[fg]\" \\\n  OUTPUT\n```\n\n#### Color Match Between Clips\n```bash\nffmpeg -i source.mp4 -i reference.mp4 \\\n  -filter_complex \"[0:v][1:v]colorcorrect=rl=0.1:bl=-0.1:rh=0.05:bh=-0.05[out]\" \\\n  -map \"[out]\" OUTPUT\n```\n\n### 6. Parameter Reference\n\n| Filter | Key Parameters |\n|--------|----------------|\n| chromakey | color, similarity (0-1), blend (0-1) |\n| lut3d | file, interp (nearest, trilinear, tetrahedral) |\n| curves | preset, r/g/b/all channel curves |\n| colorbalance | rs/gs/bs (shadows), rm/gm/bm (mids), rh/gh/bh (highs) |\n| eq | contrast, brightness, saturation, gamma |\n| colortemperature | temperature (Kelvin value) |\n\n### 7. Testing Tips\n\n- **Preview first**: Use `-t 5` to test on 5 seconds\n- **Check histogram**: Add `split[a][b];[b]histogram,format=yuva444p[h];[a][h]overlay` to see color distribution\n- **A/B comparison**: Use `hstack` to compare original vs processed\n\n## Output\n\nProvide:\n1. Complete FFmpeg command for the requested color effect\n2. Explanation of color parameters and their visual impact\n3. Suggestions for fine-tuning the effect\n4. Alternative presets or looks to try\n5. Tips for matching footage or maintaining consistency\n"
              },
              {
                "name": "/ffmpeg-debug",
                "description": "Debug FFmpeg issues - analyze errors, validate files, troubleshoot encoding problems",
                "path": "plugins/ffmpeg-master/commands/ffmpeg-debug.md",
                "frontmatter": {
                  "name": "Debug FFmpeg",
                  "description": "Debug FFmpeg issues - analyze errors, validate files, troubleshoot encoding problems",
                  "argument-hint": [
                    "file-or-error-message"
                  ]
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Debugging\n\n## Purpose\nDiagnose and resolve FFmpeg errors, validate media files, and troubleshoot encoding issues.\n\n## Workflow\n\n### 1. Validate Media File\n```bash\n# Check if file is valid\nffprobe -v error INPUT && echo \"Valid\" || echo \"Invalid/Corrupt\"\n\n# Get detailed error info\nffprobe -v verbose INPUT 2>&1 | head -50\n\n# Full validation (slow but thorough)\nffmpeg -v error -i INPUT -f null - && echo \"Playable\" || echo \"Has errors\"\n```\n\n### 2. Get Detailed File Info\n```bash\n# Comprehensive JSON info\nffprobe -v quiet -print_format json -show_format -show_streams INPUT\n\n# Summary info\nffprobe -v error -show_entries format=duration,size,bit_rate:stream=codec_name,width,height,r_frame_rate -of default=noprint_wrappers=1 INPUT\n```\n\n### 3. Common Errors & Solutions\n\n#### \"Invalid data found when processing input\"\n```bash\n# Try with error recovery\nffmpeg -err_detect ignore_err -i INPUT -c copy OUTPUT\n\n# Force format detection\nffmpeg -f FORMAT -i INPUT -c copy OUTPUT\n```\n\n#### \"No such file or directory\"\n```bash\n# Check path (Windows Git Bash issue)\nMSYS_NO_PATHCONV=1 ffmpeg -i INPUT OUTPUT\n\n# Use quotes for paths with spaces\nffmpeg -i \"path with spaces/file.mp4\" output.mp4\n```\n\n#### \"Avi/codec not found\"\n```bash\n# Check available codecs\nffmpeg -encoders | grep -i CODEC_NAME\nffmpeg -decoders | grep -i CODEC_NAME\n\n# Install missing codec or use alternative\nffmpeg -i INPUT -c:v libx264 OUTPUT.mp4\n```\n\n#### \"Hardware acceleration failed\"\n```bash\n# Check available hardware accelerators\nffmpeg -hwaccels\n\n# Check GPU encoders\nffmpeg -encoders | grep nvenc\nffmpeg -encoders | grep qsv\nffmpeg -encoders | grep vaapi\n\n# Fallback to software\nffmpeg -i INPUT -c:v libx264 OUTPUT.mp4\n```\n\n#### \"Buffer overflow\" / \"Queue full\"\n```bash\n# Increase buffer size\nffmpeg -thread_queue_size 1024 -i INPUT OUTPUT\n\n# For streaming\nffmpeg -i INPUT -bufsize 10000k OUTPUT\n```\n\n### 4. Generate Debug Log\n```bash\n# Create detailed report\nffmpeg -report -i INPUT -c copy OUTPUT\n\n# Or set log level\nffmpeg -v verbose -i INPUT OUTPUT 2>&1 | tee ffmpeg.log\n```\n\n### 5. Check FFmpeg Capabilities\n```bash\n# Version and build info\nffmpeg -version\n\n# All encoders\nffmpeg -encoders\n\n# All decoders\nffmpeg -decoders\n\n# All formats\nffmpeg -formats\n\n# All filters\nffmpeg -filters\n\n# Specific encoder options\nffmpeg -h encoder=libx264\n```\n\n### 6. Test Commands\n```bash\n# Test input without processing\nffmpeg -i INPUT -t 5 -c copy /dev/null\n\n# Generate test pattern\nffmpeg -f lavfi -i testsrc=size=1280x720:rate=30 -t 10 test.mp4\n\n# Test encoding speed\nffmpeg -benchmark -i INPUT -c:v libx264 -f null -\n```\n\n## Output\n\nProvide:\n1. Diagnosis of the specific error\n2. Root cause explanation\n3. Solution command(s)\n4. Prevention tips for future\n5. Alternative approaches if primary fix doesn't work"
              },
              {
                "name": "/ffmpeg-effects",
                "description": null,
                "path": "plugins/ffmpeg-master/commands/ffmpeg-effects.md",
                "frontmatter": null,
                "content": "---\nname: Apply Video Effects\ndescription: Apply creative video effects - glitch, datamosh, VHS, chromatic aberration, distortion, and artistic filters\nargument-hint: <effect: glitch|vhs|datamosh|chromatic|trails|distortion> [input-file]\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Creative Effects\n\n## Purpose\nApply creative video effects including glitch art, VHS simulation, datamosh, chromatic aberration, motion trails, and distortion effects.\n\n## Workflow\n\n### 1. Analyze Input\n```bash\nffprobe -v error -show_entries stream=width,height,r_frame_rate,duration -of default=noprint_wrappers=1 INPUT\n```\n\n### 2. Effect Presets\n\n#### Glitch (Combined)\n```bash\nffmpeg -i INPUT \\\n  -vf \"\\\n    minterpolate='mi_mode=mci:mc_mode=aobmc':enable='lt(mod(t,3),0.2)',\\\n    rgbashift=rh='3*sin(t*10)':bh='-3*sin(t*10)',\\\n    noise=c0s=10:c0f=t:enable='lt(mod(t,2),0.1)'\" \\\n  -c:v libx264 -crf 18 OUTPUT\n```\n\n#### VHS/Analog\n```bash\nffmpeg -i INPUT \\\n  -vf \"\\\n    noise=c0s=15:c0f=t:c1s=10:c1f=t,\\\n    eq=saturation=1.4:contrast=1.1:brightness=-0.02,\\\n    chromashift=cbh=3:crh=-3,\\\n    rgbashift=rh=2:bh=-2,\\\n    drawgrid=w=iw:h=2:t=1:c=black@0.3,\\\n    curves=preset=vintage\" \\\n  -c:v libx264 -crf 20 OUTPUT\n```\n\n#### Datamosh (Pixel Bleeding)\n```bash\nffmpeg -i INPUT \\\n  -vf \"minterpolate='mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1'\" \\\n  -c:v libx264 -crf 18 OUTPUT\n```\n\n#### Chromatic Aberration\n```bash\n# Static\nffmpeg -i INPUT -vf \"rgbashift=rh=-5:bh=5\" OUTPUT\n\n# Pulsing (animated)\nffmpeg -i INPUT -vf \"rgbashift=rh='5*sin(t*10)':bh='-5*sin(t*10)'\" OUTPUT\n```\n\n#### Motion Trails\n```bash\nffmpeg -i INPUT -vf \"lagfun=decay=0.95\" OUTPUT\n```\n\n#### Wave Distortion\n```bash\nffmpeg -i INPUT \\\n  -vf \"geq=lum='lum(X+10*sin(Y/20+T*5),Y)':cb='cb(X+10*sin(Y/20+T*5),Y)':cr='cr(X+10*sin(Y/20+T*5),Y)'\" \\\n  OUTPUT\n```\n\n#### Barrel/Fisheye Distortion\n```bash\nffmpeg -i INPUT \\\n  -vf \"lenscorrection=cx=0.5:cy=0.5:k1=0.5:k2=0.5\" \\\n  OUTPUT\n```\n\n### 3. Timing Effects\n\nApply effects only during specific time ranges:\n```bash\n# Effect between 5-10 seconds\n-vf \"effect_filter:enable='between(t,5,10)'\"\n\n# Effect at regular intervals\n-vf \"effect_filter:enable='lt(mod(t,4),0.3)'\"\n```\n\n### 4. Effect Parameters\n\n| Effect | Key Parameters |\n|--------|----------------|\n| Datamosh | mi_mode, mc_mode, scd (scene change) |\n| Chromatic | rh, bh, rv, bv (shift amounts) |\n| VHS noise | c0s, c0f (strength, flags) |\n| Trails | decay (0-1, higher = longer trails) |\n| Distortion | k1, k2 (barrel/pincushion) |\n\n### 5. Combining Effects\n\nChain multiple effects:\n```bash\nffmpeg -i INPUT \\\n  -vf \"effect1,effect2,effect3\" \\\n  OUTPUT\n```\n\n### 6. Performance Tips\n\n- **Datamosh is CPU-intensive**: Consider processing at lower resolution\n- **Test on short clips**: Use `-t 5` to process only 5 seconds\n- **Hardware encode final**: After effects, use NVENC/QSV for encoding\n\n## Output\n\nProvide:\n1. Complete FFmpeg command for the requested effect\n2. Explanation of effect parameters\n3. Options for timing/triggering the effect\n4. Combination suggestions for more complex looks\n5. Performance optimization tips\n"
              },
              {
                "name": "/ffmpeg-kinetic",
                "description": null,
                "path": "plugins/ffmpeg-master/commands/ffmpeg-kinetic.md",
                "frontmatter": null,
                "content": "---\nname: Create Kinetic Captions\ndescription: Create animated kinetic captions with word-grow, bounce, pop, and karaoke effects for viral videos\nargument-hint: <effect: pop|grow|bounce|elastic|karaoke-grow> [input-file] [--platform tiktok|youtube|instagram]\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Edit\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Kinetic Captions\n\n## Purpose\nCreate animated kinetic captions with professional effects like word-grow karaoke, bouncy text, elastic effects, and CapCut-style word pop animations for viral social media videos.\n\n## Workflow\n\n### 1. Quick Effect Reference\n\n| Effect | Description | Best For |\n|--------|-------------|----------|\n| `pop` | Words pop in from small to normal with overshoot | TikTok, fast-paced content |\n| `grow` | Words smoothly scale up when highlighted | Karaoke, music videos |\n| `bounce` | Elastic bounce with multiple oscillations | YouTube Shorts, emphasis |\n| `elastic` | Spring physics with natural damping | Professional, premium feel |\n| `karaoke-grow` | Combined karaoke fill + grow animation | Lyric videos, singalongs |\n\n### 2. Generate Kinetic ASS Subtitles\n\n#### Using the Script\n```bash\n# Basic kinetic captions\n./generate-karaoke.sh kinetic lyrics.txt output.ass pop\n\n# Platform-optimized\n./generate-karaoke.sh kinetic lyrics.txt output.ass bounce tiktok\n./generate-karaoke.sh kinetic lyrics.txt output.ass elastic youtube\n./generate-karaoke.sh kinetic lyrics.txt output.ass karaoke-grow instagram\n```\n\n#### Manual ASS Creation\n\n**Pop Effect (CapCut-Style):**\n```ass\n[Script Info]\nScriptType: v4.00+\nPlayResX: 1080\nPlayResY: 1920\n\n[V4+ Styles]\nStyle: Kinetic,Montserrat,72,&H00FFFFFF,&H000000FF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,3,0,2,50,50,30,1\n\n[Events]\nDialogue: 0,0:00:01.00,0:00:02.00,Kinetic,,0,0,0,,{\\fscx50\\fscy50\\t(0,100,\\fscx115\\fscy115)\\t(100,200,\\fscx100\\fscy100)}Hello\nDialogue: 0,0:00:02.00,0:00:03.00,Kinetic,,0,0,0,,{\\fscx50\\fscy50\\t(0,100,\\fscx115\\fscy115)\\t(100,200,\\fscx100\\fscy100)}World\n```\n\n**Word-Grow Karaoke Effect:**\n```ass\nDialogue: 0,0:00:01.00,0:00:05.00,Kinetic,,0,0,0,,{\\k80\\t(0,200,\\fscx115\\fscy115)\\t(200,400,\\fscx100\\fscy100)}First {\\k60\\t(0,200,\\fscx115\\fscy115)\\t(200,400,\\fscx100\\fscy100)}word {\\k100\\t(0,200,\\fscx115\\fscy115)\\t(200,400,\\fscx100\\fscy100)}grows\n```\n\n**Bounce Effect:**\n```ass\nDialogue: 0,0:00:01.00,0:00:02.00,Kinetic,,0,0,0,,{\\fscx80\\fscy80\\t(0,100,\\fscx120\\fscy120)\\t(100,200,\\fscx95\\fscy95)\\t(200,300,\\fscx100\\fscy100)}Bounce\n```\n\n**Elastic Effect:**\n```ass\nDialogue: 0,0:00:01.00,0:00:02.00,Kinetic,,0,0,0,,{\\fscx60\\fscy60\\t(0,80,\\fscx125\\fscy125)\\t(80,160,\\fscx92\\fscy92)\\t(160,240,\\fscx105\\fscy105)\\t(240,320,\\fscx98\\fscy98)\\t(320,400,\\fscx100\\fscy100)}Elastic\n```\n\n### 3. Apply to Video\n\n```bash\n# Burn subtitles into video\nffmpeg -i input.mp4 -vf \"ass=kinetic.ass\" -c:v libx264 -crf 18 -c:a copy output.mp4\n\n# With hardware acceleration (NVENC)\nffmpeg -i input.mp4 -vf \"ass=kinetic.ass\" -c:v h264_nvenc -preset p4 -cq 20 -c:a copy output.mp4\n\n# 9:16 vertical with kinetic captions\nffmpeg -i input.mp4 -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2,ass=kinetic.ass\" -c:v libx264 -crf 18 output.mp4\n```\n\n### 4. Platform-Specific Timing\n\n| Platform | Animation Speed | Recommended Effect |\n|----------|----------------|-------------------|\n| TikTok | Fast (100-200ms) | pop, bounce |\n| YouTube Shorts | Medium (150-300ms) | grow, elastic |\n| Instagram Reels | Stylish (200-400ms) | elastic, karaoke-grow |\n\n### 5. Critical ASS Timing Notes\n\n**IMPORTANT: ASS uses TWO different time units:**\n- **Karaoke tags (`\\k`, `\\kf`, `\\ko`)**: Centiseconds (100 = 1 second)\n- **Animation tags (`\\t`)**: Milliseconds (1000 = 1 second)\n\nExample combining both:\n```ass\n{\\k80\\t(0,200,\\fscx115\\fscy115)}Word\n```\n- `\\k80` = word highlighted for 0.8 seconds (80 centiseconds)\n- `\\t(0,200,...)` = animation over 200 milliseconds\n\n### 6. Color Variations\n\n```ass\n# Yellow highlight on grow\n{\\k80\\t(0,200,\\fscx115\\fscy115\\c&H00FFFF&)\\t(200,400,\\fscx100\\fscy100)}Word\n\n# Gradient glow effect\n{\\k80\\t(0,200,\\fscx115\\fscy115\\3c&H0000FF&)\\t(200,400,\\fscx100\\fscy100\\3c&H000000&)}Word\n```\n\n### 7. Advanced: Python Generator\n\nFor complex projects, use the Python generator:\n\n```python\nimport json\n\ndef generate_kinetic_ass(words, effect=\"pop\", platform=\"tiktok\"):\n    \"\"\"Generate kinetic ASS subtitle file\"\"\"\n\n    effects = {\n        \"pop\": r\"{\\fscx50\\fscy50\\t(0,100,\\fscx115\\fscy115)\\t(100,200,\\fscx100\\fscy100)}\",\n        \"grow\": r\"{\\fscx90\\fscy90\\t(0,150,\\fscx110\\fscy110)\\t(150,300,\\fscx100\\fscy100)}\",\n        \"bounce\": r\"{\\fscx80\\fscy80\\t(0,100,\\fscx120\\fscy120)\\t(100,200,\\fscx95\\fscy95)\\t(200,300,\\fscx100\\fscy100)}\",\n        \"elastic\": r\"{\\fscx60\\fscy60\\t(0,80,\\fscx125\\fscy125)\\t(80,160,\\fscx92\\fscy92)\\t(160,240,\\fscx105\\fscy105)\\t(240,320,\\fscx98\\fscy98)\\t(320,400,\\fscx100\\fscy100)}\"\n    }\n\n    header = \"\"\"[Script Info]\nScriptType: v4.00+\nPlayResX: 1080\nPlayResY: 1920\n\n[V4+ Styles]\nStyle: Kinetic,Montserrat,72,&H00FFFFFF,&H000000FF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,3,0,2,50,50,30,1\n\n[Events]\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n\"\"\"\n\n    effect_code = effects.get(effect, effects[\"pop\"])\n    lines = []\n\n    for word_data in words:\n        word = word_data[\"word\"]\n        start = word_data[\"start\"]\n        end = word_data[\"end\"]\n        lines.append(f\"Dialogue: 0,{start},{end},Kinetic,,0,0,0,,{effect_code}{word}\")\n\n    return header + \"\\n\".join(lines)\n\n# Usage\nwords = [\n    {\"word\": \"Hello\", \"start\": \"0:00:01.00\", \"end\": \"0:00:02.00\"},\n    {\"word\": \"World\", \"start\": \"0:00:02.00\", \"end\": \"0:00:03.00\"}\n]\nass_content = generate_kinetic_ass(words, effect=\"bounce\", platform=\"tiktok\")\n```\n\n## Output\n\nProvide:\n1. Complete ASS subtitle file with kinetic effects\n2. FFmpeg command to apply to video\n3. Platform-specific optimization recommendations\n4. Timing adjustments for audio sync\n5. Color and style customization options\n"
              },
              {
                "name": "/ffmpeg-stream",
                "description": null,
                "path": "plugins/ffmpeg-master/commands/ffmpeg-stream.md",
                "frontmatter": null,
                "content": "---\nname: Setup Streaming\ndescription: Set up live streaming with FFmpeg - RTMP, HLS, DASH with optimal encoding settings\nargument-hint: <platform: twitch|youtube|hls|rtmp> [source]\n---\n\n## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Streaming Setup\n\n## Purpose\nConfigure FFmpeg for live streaming to platforms (Twitch, YouTube, Facebook) or custom RTMP/HLS/DASH endpoints.\n\n## Workflow\n\n### 1. Determine Streaming Requirements\n\nAsk user about:\n- **Platform**: Twitch, YouTube, Facebook, custom server\n- **Input source**: Webcam, screen capture, file, RTMP input\n- **Resolution**: 1080p60, 1080p30, 720p60, 720p30\n- **Bitrate limits**: Platform restrictions or bandwidth constraints\n- **Hardware acceleration**: Available GPU\n\n### 2. Platform-Specific Settings\n\n#### Twitch (Recommended)\n```bash\nffmpeg -re -i INPUT \\\n  -c:v libx264 -preset veryfast -b:v 6000k -maxrate 6000k -bufsize 12000k \\\n  -g 60 -keyint_min 60 \\\n  -c:a aac -b:a 160k -ar 44100 \\\n  -f flv rtmp://live.twitch.tv/app/STREAM_KEY\n```\n\n#### YouTube\n```bash\nffmpeg -re -i INPUT \\\n  -c:v libx264 -preset veryfast -b:v 4500k -maxrate 4500k -bufsize 9000k \\\n  -g 60 -keyint_min 60 \\\n  -c:a aac -b:a 128k -ar 44100 \\\n  -f flv rtmp://a.rtmp.youtube.com/live2/STREAM_KEY\n```\n\n#### Facebook\n```bash\nffmpeg -re -i INPUT \\\n  -c:v libx264 -preset veryfast -b:v 4000k -maxrate 4000k -bufsize 8000k \\\n  -g 60 \\\n  -c:a aac -b:a 128k -ar 44100 \\\n  -f flv rtmps://live-api-s.facebook.com:443/rtmp/STREAM_KEY\n```\n\n### 3. Input Sources\n\n#### Webcam + Microphone (Linux)\n```bash\nffmpeg -f v4l2 -framerate 30 -video_size 1920x1080 -i /dev/video0 \\\n  -f alsa -i default \\\n  -c:v libx264 -preset ultrafast -tune zerolatency -b:v 4000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n#### Webcam + Microphone (macOS)\n```bash\nffmpeg -f avfoundation -framerate 30 -video_size 1920x1080 -i \"0:0\" \\\n  -c:v libx264 -preset ultrafast -tune zerolatency -b:v 4000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n#### Screen Capture (Linux)\n```bash\nffmpeg -f x11grab -framerate 30 -video_size 1920x1080 -i :0.0 \\\n  -f pulse -i default \\\n  -c:v libx264 -preset ultrafast -tune zerolatency -b:v 4000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n#### Screen Capture (Windows)\n```bash\nffmpeg -f gdigrab -framerate 30 -i desktop \\\n  -c:v libx264 -preset ultrafast -tune zerolatency -b:v 4000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n### 4. GPU-Accelerated Streaming\n\n#### NVIDIA NVENC\n```bash\nffmpeg -re -i INPUT \\\n  -c:v h264_nvenc -preset p3 -tune ll -b:v 6000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n#### Intel QSV\n```bash\nffmpeg -re -init_hw_device qsv=hw -filter_hw_device hw \\\n  -i INPUT \\\n  -c:v h264_qsv -preset fast -b:v 6000k \\\n  -c:a aac -b:a 128k \\\n  -f flv rtmp://server/live/stream\n```\n\n### 5. HLS Output\n\n```bash\nffmpeg -re -i INPUT \\\n  -c:v libx264 -preset veryfast -b:v 3000k \\\n  -c:a aac -b:a 128k \\\n  -hls_time 4 \\\n  -hls_list_size 10 \\\n  -hls_flags delete_segments \\\n  -hls_segment_filename \"segment_%03d.ts\" \\\n  stream.m3u8\n```\n\n### 6. Multi-Bitrate ABR\n\n```bash\nffmpeg -re -i INPUT \\\n  -filter_complex \"[0:v]split=3[v1][v2][v3]; \\\n    [v1]scale=1920:1080[v1out]; \\\n    [v2]scale=1280:720[v2out]; \\\n    [v3]scale=854:480[v3out]\" \\\n  -map \"[v1out]\" -c:v:0 libx264 -b:v:0 5000k \\\n  -map \"[v2out]\" -c:v:1 libx264 -b:v:1 2500k \\\n  -map \"[v3out]\" -c:v:2 libx264 -b:v:2 1000k \\\n  -map a:0 -c:a aac -b:a 128k \\\n  -f hls -hls_time 4 \\\n  -var_stream_map \"v:0,a:0 v:1,a:0 v:2,a:0\" \\\n  -master_pl_name master.m3u8 \\\n  stream_%v.m3u8\n```\n\n### 7. Bitrate Recommendations\n\n| Resolution | Framerate | Min Bitrate | Recommended | Max Bitrate |\n|------------|-----------|-------------|-------------|-------------|\n| 1080p | 60 | 4500 kbps | 6000 kbps | 9000 kbps |\n| 1080p | 30 | 3000 kbps | 4500 kbps | 6000 kbps |\n| 720p | 60 | 2500 kbps | 4000 kbps | 6000 kbps |\n| 720p | 30 | 1500 kbps | 2500 kbps | 4000 kbps |\n| 480p | 30 | 500 kbps | 1000 kbps | 2000 kbps |\n\n## Output\n\nProvide:\n1. Complete FFmpeg streaming command for the platform\n2. Explanation of key parameters (bitrate, keyframe, etc.)\n3. Hardware acceleration options if available\n4. Troubleshooting tips for common issues\n5. Test command to verify stream before going live\n"
              },
              {
                "name": "/ffmpeg-transcode",
                "description": "Transcode video/audio with FFmpeg using optimal settings for target format, quality, and compatibility",
                "path": "plugins/ffmpeg-master/commands/ffmpeg-transcode.md",
                "frontmatter": {
                  "name": "Transcode Video",
                  "description": "Transcode video/audio with FFmpeg using optimal settings for target format, quality, and compatibility",
                  "argument-hint": "<input-file> [target-format] [quality]"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# FFmpeg Transcode\n\n## Purpose\nAnalyze input media and generate optimal FFmpeg commands for transcoding to target format with best quality/size/compatibility balance.\n\n## Workflow\n\n### 1. Analyze Input\nBefore transcoding, analyze the source file:\n\n```bash\n# Get comprehensive media info\nffprobe -v quiet -print_format json -show_format -show_streams INPUT_FILE\n```\n\nKey information to extract:\n- Video codec, resolution, frame rate, bitrate\n- Audio codec, sample rate, channels, bitrate\n- Duration and container format\n\n### 2. Determine Target Requirements\n\nAsk user about:\n- **Target format**: MP4, WebM, MKV, etc.\n- **Use case**: Web delivery, archival, editing, streaming\n- **Quality preference**: Best quality, balanced, smaller size\n- **Compatibility**: Universal, modern browsers, specific devices\n- **Hardware acceleration**: Available GPU (NVIDIA, Intel, AMD)\n\n### 3. Generate Transcode Command\n\n#### Web Delivery (MP4)\n```bash\nffmpeg -i INPUT \\\n  -c:v libx264 -preset medium -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -movflags +faststart \\\n  -pix_fmt yuv420p \\\n  OUTPUT.mp4\n```\n\n#### High Quality Archival\n```bash\nffmpeg -i INPUT \\\n  -c:v libx265 -preset slow -crf 20 \\\n  -c:a flac \\\n  -tag:v hvc1 \\\n  OUTPUT.mkv\n```\n\n#### WebM for Web\n```bash\nffmpeg -i INPUT \\\n  -c:v libvpx-vp9 -crf 30 -b:v 0 \\\n  -c:a libopus -b:a 128k \\\n  OUTPUT.webm\n```\n\n#### With Hardware Acceleration (NVIDIA)\n```bash\nffmpeg -hwaccel cuda -hwaccel_output_format cuda \\\n  -i INPUT \\\n  -c:v h264_nvenc -preset p4 -cq 23 \\\n  -c:a aac -b:a 128k \\\n  OUTPUT.mp4\n```\n\n### 4. Quality Settings Reference\n\n| Quality | CRF (x264) | CRF (x265) | CRF (VP9) | Use Case |\n|---------|------------|------------|-----------|----------|\n| Lossless | 0 | 0 | 0 | Editing |\n| Very High | 18 | 20 | 15 | Archival |\n| High | 20-23 | 22-25 | 23-28 | General |\n| Medium | 24-26 | 26-28 | 30-35 | Streaming |\n| Low | 28+ | 30+ | 40+ | Preview |\n\n### 5. Preset Reference\n\n| Preset | Speed | Quality | Use Case |\n|--------|-------|---------|----------|\n| ultrafast | 10x | Lower | Live/preview |\n| veryfast | 5x | Low | Quick encode |\n| faster | 3x | Medium-low | Draft |\n| fast | 2x | Medium | General |\n| medium | 1x | **Balanced** | **Default** |\n| slow | 0.5x | High | Quality focus |\n| slower | 0.3x | Higher | Final render |\n| veryslow | 0.1x | Highest | Archival |\n\n### 6. Resolution & Scaling\n\n```bash\n# Scale to 1080p (maintain aspect ratio)\n-vf \"scale=1920:-2\"\n\n# Scale to 720p\n-vf \"scale=-2:720\"\n\n# Fit within bounds\n-vf \"scale='min(1920,iw)':'min(1080,ih)':force_original_aspect_ratio=decrease\"\n```\n\n### 7. Verify Output\n\n```bash\n# Compare before/after\nffprobe -v error -show_entries format=duration,size,bit_rate -of default=noprint_wrappers=1 INPUT\nffprobe -v error -show_entries format=duration,size,bit_rate -of default=noprint_wrappers=1 OUTPUT\n\n# Check for errors\nffmpeg -v error -i OUTPUT -f null - && echo \"Valid\" || echo \"Error\"\n```\n\n## Output\n\nProvide:\n1. Complete FFmpeg command with explanation of each option\n2. Expected output size estimate\n3. Encoding time estimate based on preset\n4. Compatibility notes for target use case\n5. Alternative commands for different quality/speed tradeoffs\n6. Verification commands"
              },
              {
                "name": "/ffmpeg-viral-shorts",
                "description": "Create YouTube Shorts-optimized viral video with 9:16 aspect ratio, auto-captions, retention optimization, and thumbnail extraction",
                "path": "plugins/ffmpeg-master/commands/ffmpeg-viral-shorts.md",
                "frontmatter": {
                  "name": "YouTube Shorts Viral Video",
                  "description": "Create YouTube Shorts-optimized viral video with 9:16 aspect ratio, auto-captions, retention optimization, and thumbnail extraction",
                  "argument-hint": "<input-file> [--caption] [--thumbnail-time] [--output]"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# YouTube Shorts Viral Video Creator\n\n## Purpose\n\nTransform any video into a YouTube Shorts-optimized viral format with:\n- **9:16 vertical aspect ratio** (1080x1920)\n- **Auto-generated captions** with engaging styling\n- **Optimal duration** (50-60 seconds for max retention)\n- **Thumbnail extraction** for custom cover selection\n- **YouTube-compliant encoding** (VP9/H.264, HDR support)\n- **Loop-friendly endings** for increased watch time\n\n## Quick Start\n\n### Basic Shorts Conversion\n\n```bash\n# Simple conversion to YouTube Shorts format\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k -ar 48000 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 59 \\\n  output_shorts.mp4\n```\n\n## Complete YouTube Shorts Workflow\n\n### Step 1: Analyze Input\n\n```bash\n# Check source video properties\nffprobe -v quiet -print_format json -show_format -show_streams input.mp4\n```\n\nKey metrics:\n- Duration (Shorts max: 60s, optimal: 50-60s)\n- Resolution (need 9:16 vertical)\n- Audio quality (YouTube supports higher bitrate than TikTok)\n\n### Step 2: Create 9:16 Vertical Format\n\n#### From Horizontal (16:9) Video - Smart Cropping\n\n```bash\n# Option A: Intelligent center crop with face detection zone\nffmpeg -i horizontal.mp4 \\\n  -vf \"crop=ih*9/16:ih:(iw-ih*9/16)/2:0,scale=1080:1920\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_shorts.mp4\n\n# Option B: Cinematic letterbox with gradient bars\nffmpeg -i horizontal.mp4 \\\n  -filter_complex \"\n    [0:v]scale=1080:-2[main];\n    color=c=black:s=1080x1920:d=1[bg];\n    [bg][main]overlay=0:(H-h)/2:shortest=1,\n    drawbox=x=0:y=0:w=1080:h=300:c=black@0.7:t=fill,\n    drawbox=x=0:y=1620:w=1080:h=300:c=black@0.7:t=fill\n  \" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_shorts.mp4\n\n# Option C: Picture-in-picture with blurred background\nffmpeg -i horizontal.mp4 \\\n  -filter_complex \"\n    [0:v]scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920,boxblur=25:25[bg];\n    [0:v]scale=1080:-2[fg];\n    [bg][fg]overlay=0:(H-h)/2\n  \" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_shorts.mp4\n```\n\n#### From Long-Form YouTube Video (Clip Extraction)\n\n```bash\n# Extract viral clip from longer video\nffmpeg -i long_video.mp4 \\\n  -ss 00:05:30 -t 00:00:58 \\\n  -vf \"crop=ih*9/16:ih,scale=1080:1920\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  shorts_clip.mp4\n```\n\n### Step 3: Optimize Duration for Algorithm\n\nYouTube Shorts algorithm favors videos that are watched to completion.\n\n```bash\n# Trim to optimal 50-58 second range\nffmpeg -i input.mp4 \\\n  -ss 00:00:00 -t 00:00:55 \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_optimal_length.mp4\n```\n\n### Step 4: Add Captions for Retention\n\n#### Auto-Generate with Whisper (FFmpeg 8.0+)\n\n```bash\n# Generate captions (FFmpeg 8.0+ with Whisper)\nffmpeg -i input.mp4 -vn \\\n  -af \"whisper=model=ggml-medium.bin:language=auto:format=srt\" \\\n  captions.srt\n\n# Burn captions with YouTube-optimized style\nffmpeg -i shorts_base.mp4 \\\n  -vf \"subtitles=captions.srt:force_style='FontName=Montserrat,FontSize=52,PrimaryColour=&HFFFFFF,OutlineColour=&H000000,BackColour=&H40000000,Outline=3,Shadow=2,Bold=1,Alignment=2,MarginV=250'\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a copy \\\n  shorts_with_captions.mp4\n```\n\n#### Caption Styles for Different Content Types\n\n```bash\n# Gaming/Energetic style\n-vf \"subtitles=caps.srt:force_style='FontName=Impact,FontSize=56,PrimaryColour=&H00FFFF,OutlineColour=&H000000,Outline=4,Bold=1,Alignment=2,MarginV=200'\"\n\n# Educational/Professional style\n-vf \"subtitles=caps.srt:force_style='FontName=Roboto,FontSize=44,PrimaryColour=&HFFFFFF,OutlineColour=&H333333,Outline=2,Shadow=1,Alignment=2,MarginV=280'\"\n\n# Storytelling/Dramatic style\n-vf \"subtitles=caps.srt:force_style='FontName=Georgia,FontSize=48,PrimaryColour=&HFFD700,OutlineColour=&H000000,Outline=3,Italic=1,Alignment=2,MarginV=240'\"\n```\n\n### Step 5: Add Hook Elements\n\n#### Opening Text Hook (First 2 Seconds)\n\n```bash\n# Curiosity gap hook\nffmpeg -i input.mp4 \\\n  -vf \"drawtext=text='This changed everything...':fontsize=60:fontcolor=white:borderw=4:bordercolor=black:x=(w-tw)/2:y=h*0.15:enable='between(t,0,2.5)',drawtext=text='Watch till the end':fontsize=36:fontcolor=yellow:borderw=2:bordercolor=black:x=(w-tw)/2:y=h*0.22:enable='between(t,0,2.5)'\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a copy \\\n  output_with_hook.mp4\n```\n\n#### Visual Pattern Interrupt\n\n```bash\n# Flash/brightness pulse at start\nffmpeg -i input.mp4 \\\n  -vf \"eq=brightness=0.1*sin(t*12)*between(t,0,0.5)\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a copy \\\n  output_flash_hook.mp4\n```\n\n### Step 6: Create Loop-Friendly Ending\n\n```bash\n# Crossfade last 1 second with first 1 second for smooth loop\nffmpeg -i input.mp4 \\\n  -filter_complex \"\n    [0:v]trim=0:1,setpts=PTS-STARTPTS[start];\n    [0:v]trim=54:55,setpts=PTS-STARTPTS[end];\n    [end][start]xfade=transition=fade:duration=0.5:offset=0.5[loop];\n    [0:v]trim=0:54,setpts=PTS-STARTPTS[main];\n    [main][loop]concat=n=2:v=1:a=0\n  \" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  output_looped.mp4\n```\n\n### Step 7: Extract Thumbnails\n\n```bash\n# Extract multiple thumbnail options\nffmpeg -i output_shorts.mp4 -vf \"select='eq(n,0)+eq(n,60)+eq(n,120)+eq(n,180)'\" -vsync vfr thumbnail_%02d.jpg\n\n# Extract at specific timestamp with text overlay\nffmpeg -i output_shorts.mp4 -ss 00:00:15 -vframes 1 \\\n  -vf \"drawtext=text='YOUR TITLE':fontsize=72:fontcolor=white:borderw=5:bordercolor=black:x=(w-tw)/2:y=h*0.4\" \\\n  thumbnail_with_text.jpg\n\n# High-quality thumbnail\nffmpeg -i output_shorts.mp4 -ss 00:00:10 -vframes 1 \\\n  -q:v 2 \\\n  thumbnail_hq.jpg\n```\n\n### Step 8: Encode for Quality\n\n#### Standard Quality (H.264)\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n  -c:v libx264 -preset slow -crf 20 \\\n  -c:a aac -b:a 192k -ar 48000 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  output_hq.mp4\n```\n\n#### Premium Quality (VP9 - YouTube's Preferred)\n\n```bash\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n  -c:v libvpx-vp9 -crf 25 -b:v 0 -deadline good \\\n  -c:a libopus -b:a 192k \\\n  -row-mt 1 \\\n  output_vp9.webm\n```\n\n#### HDR Content (If Source is HDR)\n\n```bash\nffmpeg -i hdr_input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n  -c:v libx265 -preset slow -crf 22 \\\n  -tag:v hvc1 \\\n  -color_primaries bt2020 -color_trc smpte2084 -colorspace bt2020nc \\\n  -c:a aac -b:a 192k \\\n  output_hdr.mp4\n```\n\n## Complete One-Command Preset\n\n```bash\n# Full YouTube Shorts optimization pipeline\nffmpeg -i input.mp4 \\\n  -filter_complex \"\n    [0:v]scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black[v1];\n    [v1]drawtext=text='YOUR HOOK':fontsize=56:fontcolor=white:borderw=4:bordercolor=black:x=(w-tw)/2:y=h*0.12:enable='between(t,0,2.5)'[v2];\n    [0:a]loudnorm=I=-14:TP=-1.5:LRA=11[a]\n  \" \\\n  -map \"[v2]\" -map \"[a]\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a aac -b:a 192k -ar 48000 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 59 \\\n  -metadata title=\"Your Shorts Title\" \\\n  -metadata comment=\"#Shorts\" \\\n  output_shorts_final.mp4\n```\n\n## YouTube Shorts Technical Requirements (2025-2026)\n\n| Specification | Requirement | Optimal Value |\n|---------------|-------------|---------------|\n| **Aspect Ratio** | 9:16 (vertical) | Required for Shorts shelf |\n| **Resolution** | 1080x1920 (recommended) | 1080x1920 (limited to 1080p playback) |\n| **Video Codec** | H.264, VP9, HEVC | H.264 (compatibility), VP9 (quality) |\n| **Audio Codec** | AAC, Opus | AAC-LC (most compatible) |\n| **Audio Bitrate** | 192-384 kbps | 192 kbps minimum, 384 kbps YouTube recommendation |\n| **Audio Loudness** | -13 to -15 LUFS | -14 LUFS (YouTube normalizes to this) |\n| **True Peak** | -1 to -3 dB | -1.5 dBTP recommended |\n| **Frame Rate** | 24-60 fps | 30 fps (upload at source frame rate) |\n| **Bitrate (1080p30)** | 8 Mbps | 8 Mbps (YouTube official) |\n| **Bitrate (1080p60)** | 12 Mbps | 12 Mbps |\n| **CRF (H.264)** | 18-23 | CRF 20 (high quality), CRF 22 (standard) |\n| **Preset** | slow, medium, fast | slow (best quality for uploads) |\n| **Max File Size** | 256 GB | Practical: 20-100 MB for Shorts |\n| **Max Duration** | 60 seconds (HARD LIMIT) | Strict enforcement |\n| **Optimal Duration** | 50-60 seconds | Maximizes watch time metric |\n| **Min Duration** | ~15 seconds recommended | Shorter = harder to rank |\n| **Pixel Format** | yuv420p (SDR) | yuv420p10le for HDR |\n| **Color Space** | Rec.709 (SDR) | Rec.2100 for HDR only |\n| **Audio Sample Rate** | 48000 Hz preferred | YouTube production standard |\n\n## Algorithm Optimization Tips\n\n### Duration Strategy\n\n| Length | Performance |\n|--------|-------------|\n| 15-30s | Good for simple concepts, high completion |\n| 30-45s | Balanced, good for tutorials |\n| **50-60s** | **Algorithm favorite**, max watch time |\n\n### Retention Techniques\n\n```bash\n# Add progress bar (increases completion rate)\nffmpeg -i input.mp4 \\\n  -vf \"drawbox=x=0:y=1900:w='(t/60)*1080':h=20:c=red:t=fill\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a copy \\\n  output_with_progress.mp4\n\n# Subtle continuous zoom - 0.15%/sec for YouTube (larger screens need more movement)\n# YouTube Shorts viewers often watch on TVs, requiring 1.5x TikTok's zoom rate\nffmpeg -i input.mp4 \\\n  -vf \"zoompan=z='1+0.0015*t':d=1:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  output_slow_zoom.mp4\n```\n\n### Audio Optimization (2025-2026 Research)\n\n**YouTube Shorts Audio Targets**:\n- **Loudness**: -13 to -15 LUFS (YouTube normalizes to -14 LUFS)\n- **True Peak**: -1 to -3 dBTP (recommended: -1.5 dBTP)\n- **Strategy**: Upload at -14 LUFS for minimal platform processing\n\n```bash\n# Normalize audio for YouTube Shorts - -14 LUFS target\nffmpeg -i input.mp4 \\\n  -af \"loudnorm=I=-14:TP=-1.5:LRA=11\" \\\n  -c:v copy \\\n  output_normalized_shorts.mp4\n\n# Voice clarity with YouTube-optimized loudness\nffmpeg -i input.mp4 \\\n  -af \"highpass=f=80,lowpass=f=12000,compand=attacks=0:points=-80/-900|-45/-15|-27/-9|0/-7|20/-7:gain=3,loudnorm=I=-14:TP=-1.5\" \\\n  -c:v copy \\\n  output_voice_optimized.mp4\n```\n\n### End Screen Call-to-Action\n\n```bash\n# Add subscribe reminder in last 5 seconds\nffmpeg -i input.mp4 \\\n  -vf \"drawtext=text='SUBSCRIBE':fontsize=48:fontcolor=red:borderw=3:bordercolor=white:x=(w-tw)/2:y=h*0.85:enable='gte(t,55)'\" \\\n  -c:v libx264 -preset fast -crf 22 \\\n  -c:a copy \\\n  output_with_cta.mp4\n```\n\n## Batch Processing for Content Repurposing\n\n```bash\n#!/bin/bash\n# batch_shorts.sh - Convert multiple clips to Shorts format\n\nfor input in *.mp4; do\n    output=\"shorts_${input}\"\n\n    # Get duration\n    duration=$(ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$input\")\n\n    # Cap at 59 seconds\n    if (( $(echo \"$duration > 59\" | bc -l) )); then\n        trim=\"-t 59\"\n    else\n        trim=\"\"\n    fi\n\n    ffmpeg -i \"$input\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 22 \\\n      -c:a aac -b:a 192k \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      $trim \\\n      \"$output\"\n\n    # Extract thumbnail\n    ffmpeg -i \"$output\" -ss 00:00:05 -vframes 1 \"thumb_${input%.mp4}.jpg\"\ndone\n```\n\n## Output Verification\n\n```bash\n# Verify Shorts compliance\nffprobe -v error -select_streams v:0 \\\n  -show_entries stream=width,height,codec_name,pix_fmt,r_frame_rate \\\n  -show_entries format=duration \\\n  -of csv=p=0 output_shorts.mp4\n\n# Expected: 1080,1920,h264,yuv420p,30/1 and duration < 60\n\n# Check file integrity\nffmpeg -v error -i output_shorts.mp4 -f null - && echo \"Valid\" || echo \"Error\"\n```\n\n## Related Skills\n\n- `viral-video-platform-specs` - All platform upload requirements\n- `viral-video-hook-templates` - 10 proven hook patterns\n- `viral-video-animated-captions` - CapCut-style word highlighting\n- `ffmpeg-captions-subtitles` - Full caption system"
              },
              {
                "name": "/ffmpeg-viral-tiktok",
                "description": "Create TikTok-optimized viral video with 9:16 aspect ratio, auto-captions, hook optimization, and platform-specific encoding",
                "path": "plugins/ffmpeg-master/commands/ffmpeg-viral-tiktok.md",
                "frontmatter": {
                  "name": "TikTok Viral Video",
                  "description": "Create TikTok-optimized viral video with 9:16 aspect ratio, auto-captions, hook optimization, and platform-specific encoding",
                  "argument-hint": "<input-file> [--caption] [--hook-style] [--output]"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# TikTok Viral Video Creator\n\n## Purpose\n\nTransform any video into a TikTok-optimized viral format with:\n- **9:16 vertical aspect ratio** (1080x1920)\n- **Auto-generated captions** with CapCut-style word highlighting\n- **Hook optimization** for the critical first 1-3 seconds\n- **Platform-compliant encoding** (H.264, yuv420p, max 287MB)\n- **Optimal duration** guidance (15-60 seconds sweet spot)\n\n## Quick Start\n\n### Basic TikTok Conversion\n\n```bash\n# Simple conversion to TikTok format\nffmpeg -i input.mp4 \\\n  -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black,setsar=1\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k -ar 44100 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 60 \\\n  output_tiktok.mp4\n```\n\n## Complete TikTok Workflow\n\n### Step 1: Analyze Input\n\n```bash\n# Check source video properties\nffprobe -v quiet -print_format json -show_format -show_streams input.mp4\n```\n\nKey metrics to evaluate:\n- Duration (TikTok optimal: 15-60s, max 10 min)\n- Resolution (need to crop/scale for 9:16)\n- Audio quality (TikTok requires good audio)\n\n### Step 2: Create 9:16 Vertical Format\n\n#### From Horizontal (16:9) Video\n\n```bash\n# Option A: Center crop (best for talking head/single subject)\nffmpeg -i horizontal.mp4 \\\n  -vf \"crop=ih*9/16:ih,scale=1080:1920\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_tiktok.mp4\n\n# Option B: Letterbox with blur background (preserves full frame)\nffmpeg -i horizontal.mp4 \\\n  -filter_complex \"[0:v]scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2[fg];[0:v]scale=1080:1920,boxblur=20:20[bg];[bg][fg]overlay=(W-w)/2:(H-h)/2\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_tiktok.mp4\n\n# Option C: Split screen (original top + zoomed bottom)\nffmpeg -i horizontal.mp4 \\\n  -filter_complex \"[0:v]scale=1080:-2[top];[0:v]crop=ih*9/16:ih,scale=1080:-2[bottom];[top][bottom]vstack=inputs=2,scale=1080:1920\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_tiktok.mp4\n```\n\n#### From Square (1:1) Video\n\n```bash\n# Extend vertically with blur background\nffmpeg -i square.mp4 \\\n  -filter_complex \"[0:v]scale=1080:1080[fg];[0:v]scale=1080:1920,boxblur=15:15[bg];[bg][fg]overlay=0:(H-h)/2\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_tiktok.mp4\n```\n\n### Step 3: Add Viral Captions\n\n#### Auto-Generate Captions with Whisper (FFmpeg 8.0+)\n\n```bash\n# Generate SRT from video (FFmpeg 8.0+ with Whisper)\nffmpeg -i input.mp4 -vn \\\n  -af \"whisper=model=ggml-base.bin:language=auto:format=srt\" \\\n  captions.srt\n\n# Burn captions with TikTok-style appearance\nffmpeg -i input_tiktok.mp4 \\\n  -vf \"subtitles=captions.srt:force_style='FontName=Arial Black,FontSize=48,PrimaryColour=&HFFFFFF,OutlineColour=&H000000,Outline=4,Shadow=0,Bold=1,Alignment=2,MarginV=200'\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a copy \\\n  output_with_captions.mp4\n```\n\n#### Animated Word-by-Word Captions (CapCut Style)\n\nSee the `viral-video-animated-captions` skill for advanced word-level highlighting similar to CapCut's auto-captions.\n\n### Step 4: Add Hook Elements (First 1-3 Seconds)\n\n#### Text Hook Overlay\n\n```bash\n# Add attention-grabbing text in first 3 seconds\nffmpeg -i input.mp4 \\\n  -vf \"drawtext=text='WAIT FOR IT...':fontfile=/path/to/bold-font.ttf:fontsize=72:fontcolor=yellow:borderw=4:bordercolor=black:x=(w-tw)/2:y=h*0.15:enable='between(t,0,3)'\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a copy \\\n  output_with_hook.mp4\n```\n\n#### Visual Hook Effects\n\n```bash\n# Zoom pulse effect - 60fps for smooth motion, optimized timing for 1.3s attention window\n# 12% amplitude at ~1.3Hz (sin(t*8) = 8 rad/s) creates highly visible pulsing on mobile\nffmpeg -i input.mp4 \\\n  -vf \"fps=60,zoompan=z='if(lt(t,1.5),1+0.12*sin(t*8),1)':d=1:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  output_hook_zoom.mp4\n```\n\n### Step 5: Optimize Duration\n\n```bash\n# Trim to optimal TikTok length (15-60s)\nffmpeg -i input.mp4 \\\n  -ss 00:00:00 -t 00:00:45 \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k \\\n  -pix_fmt yuv420p -movflags +faststart \\\n  output_trimmed.mp4\n```\n\n### Step 6: Extract Cover Frame\n\n```bash\n# Extract thumbnail at peak moment (adjust time)\nffmpeg -i output_tiktok.mp4 -ss 00:00:03 -vframes 1 \\\n  -vf \"scale=1080:1920\" \\\n  cover_image.jpg\n```\n\n## Complete One-Command Preset\n\n```bash\n# Full TikTok optimization pipeline\nffmpeg -i input.mp4 \\\n  -filter_complex \"\n    [0:v]scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black[v1];\n    [v1]drawtext=text='YOUR HOOK TEXT':fontsize=64:fontcolor=white:borderw=3:bordercolor=black:x=(w-tw)/2:y=h*0.12:enable='between(t,0,2.5)'[v2];\n    [0:a]loudnorm=I=-16:TP=-1.5:LRA=11[a]\n  \" \\\n  -map \"[v2]\" -map \"[a]\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  -c:a aac -b:a 128k -ar 44100 \\\n  -pix_fmt yuv420p \\\n  -movflags +faststart \\\n  -t 60 \\\n  -metadata title=\"Your Video Title\" \\\n  output_tiktok_final.mp4\n```\n\n## TikTok Technical Requirements (2025-2026)\n\n| Specification | Requirement | Optimal Value |\n|---------------|-------------|---------------|\n| **Aspect Ratio** | 9:16 (vertical) | Required |\n| **Resolution** | 1080x1920 (recommended) | 1080x1920 (TikTok compresses 4K) |\n| **Video Codec** | H.264 (required) | H.264 High Profile, Level 4.1 |\n| **Audio Codec** | AAC | AAC-LC, 192 kbps |\n| **Audio Loudness** | -10 to -12 LUFS | -11 LUFS (mobile-optimized) |\n| **True Peak** | -1.5 dBTP max | Prevents distortion |\n| **Frame Rate** | 24-60 fps | 30 fps (60 fps for sports/action only) |\n| **Bitrate (30fps)** | 6-8.5 Mbps | 7 Mbps VBR optimal |\n| **CRF** | 21-23 | CRF 22 (best balance) |\n| **Keyframe Interval** | 2-3 seconds | Every 60-90 frames at 30fps |\n| **Max File Size** | 287 MB (iOS), 72 MB (Android web) | Target: <100 MB |\n| **Max Duration** | 10 minutes | Optimal: 21-34 seconds |\n| **Optimal Duration** | 21-34 seconds | Highest completion rate |\n| **Pixel Format** | yuv420p | Required, Rec.709 |\n\n## Viral Optimization Tips\n\n### Hook Strategies (First 1-3 Seconds)\n\n1. **Pattern Interrupt**: Unexpected visual/sound\n2. **Curiosity Gap**: \"You won't believe what happens...\"\n3. **Direct Address**: \"Stop scrolling if you...\"\n4. **Controversy**: Bold claim or opinion\n5. **Transformation**: Before/after tease\n\n### Retention Boosters\n\n```bash\n# Add subtle zoom throughout video - 0.2%/sec is minimum perceptible on mobile\n# Keeps viewers engaged with subconscious motion without being distracting\nffmpeg -i input.mp4 \\\n  -vf \"zoompan=z='1+0.002*t':d=1:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  output_slow_zoom.mp4\n\n# Add subtle shake/movement\nffmpeg -i input.mp4 \\\n  -vf \"crop=1060:1900:(10+5*sin(t*8)):(10+5*cos(t*6)),scale=1080:1920\" \\\n  -c:v libx264 -preset fast -crf 23 \\\n  output_subtle_shake.mp4\n```\n\n### Audio Optimization (2025-2026 Research)\n\n**TikTok Audio Targets**:\n- **Loudness**: -10 to -12 LUFS (louder wins on mobile speakers)\n- **True Peak**: -1.5 dBTP maximum (prevent distortion)\n- **Strategy**: Upload hotter than platform target for better mobile experience\n\n```bash\n# Normalize audio for TikTok - optimized for mobile playback\n# -11 LUFS target: louder than YouTube (-14) but prevents over-compression\nffmpeg -i input.mp4 \\\n  -af \"loudnorm=I=-11:TP=-1.5:LRA=11\" \\\n  -c:v copy \\\n  output_normalized_tiktok.mp4\n\n# Boost voice clarity with compression\nffmpeg -i input.mp4 \\\n  -af \"highpass=f=100,lowpass=f=8000,compand=attacks=0:points=-80/-900|-45/-15|-27/-9|0/-7|20/-7:gain=5,loudnorm=I=-11:TP=-1.5\" \\\n  -c:v copy \\\n  output_voice_boost.mp4\n\n# Bass boost for music content\nffmpeg -i input.mp4 \\\n  -af \"bass=g=6:f=100,loudnorm=I=-11:TP=-1.5:LRA=11\" \\\n  -c:v copy \\\n  output_bass_boost.mp4\n```\n\n## Batch Processing\n\n```bash\n#!/bin/bash\n# batch_tiktok.sh - Convert multiple videos to TikTok format\n\nfor input in *.mp4; do\n    output=\"tiktok_${input}\"\n    ffmpeg -i \"$input\" \\\n      -vf \"scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2:black\" \\\n      -c:v libx264 -preset fast -crf 23 \\\n      -c:a aac -b:a 128k \\\n      -pix_fmt yuv420p -movflags +faststart \\\n      -t 60 \\\n      \"$output\"\ndone\n```\n\n## Output Checklist\n\nBefore uploading, verify:\n\n```bash\n# Verify TikTok compliance\nffprobe -v error -select_streams v:0 \\\n  -show_entries stream=width,height,codec_name,pix_fmt,r_frame_rate \\\n  -of csv=p=0 output_tiktok.mp4\n\n# Expected: 1080,1920,h264,yuv420p,30/1\n\n# Check file size (must be under 287MB)\nls -lh output_tiktok.mp4\n```\n\n## Related Skills\n\n- `viral-video-platform-specs` - All platform upload requirements\n- `viral-video-hook-templates` - 10 proven hook patterns\n- `viral-video-animated-captions` - CapCut-style word highlighting\n- `ffmpeg-captions-subtitles` - Full caption system"
              }
            ],
            "skills": []
          },
          {
            "name": "tailwindcss-master",
            "description": "Complete Tailwind CSS v4 expertise system with 2025 best practices. PROACTIVELY activate for: (1) ANY Tailwind CSS task, (2) Tailwind v4 CSS-first configuration (@theme, @utility, @custom-variant), (3) Responsive design and dark mode implementation, (4) Performance optimization and JIT, (5) Plugin integration (@tailwindcss/typography, @tailwindcss/forms), (6) Framework integration (React, Vue, Next.js, Vite), (7) Migration from v3 to v4, (8) Custom utility and component creation, (9) Debugging and troubleshooting, (10) Production build optimization. Provides: Tailwind v4.1 complete features, CSS-first @theme configuration, @utility and @custom-variant directives, PostCSS and Vite plugin setup, responsive breakpoints, dark mode patterns (media/selector strategies), typography and forms plugins, animation/transition utilities, framework-specific integration patterns, performance optimization (tree-shaking, JIT), debugging techniques, VS Code extension setup, Prettier plugin configuration, and production-ready 2025 patterns. Ensures modern, performant, maintainable Tailwind CSS implementations.",
            "source": "./plugins/tailwindcss-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install tailwindcss-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/tailwind-component",
                "description": "Create reusable Tailwind CSS components with proper patterns",
                "path": "plugins/tailwindcss-master/commands/tailwind-component.md",
                "frontmatter": {
                  "name": "tailwind-component",
                  "description": "Create reusable Tailwind CSS components with proper patterns",
                  "argument-hint": "<component-type> e.g., 'button', 'card', 'modal', 'form input'"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Tailwind CSS Component Creation\n\n## Purpose\nCreate well-structured, reusable components using Tailwind CSS best practices.\n\n## Workflow\n\n### 1. Gather Requirements\n\nAsk user about:\n- **Component type**: Button, Card, Modal, Form, Navigation, etc.\n- **Variants**: Primary/secondary, sizes, states\n- **Framework**: React, Vue, Svelte, or vanilla HTML\n- **Accessibility needs**: ARIA labels, keyboard navigation\n\n### 2. Component Patterns\n\n#### Button Component (React)\n```tsx\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary' | 'outline' | 'ghost'\n  size?: 'sm' | 'md' | 'lg'\n  disabled?: boolean\n  children: React.ReactNode\n  onClick?: () => void\n}\n\nconst variants = {\n  primary: 'bg-blue-600 text-white hover:bg-blue-700 focus:ring-blue-500',\n  secondary: 'bg-gray-200 text-gray-900 hover:bg-gray-300 focus:ring-gray-500',\n  outline: 'border-2 border-blue-600 text-blue-600 hover:bg-blue-50 focus:ring-blue-500',\n  ghost: 'text-gray-600 hover:bg-gray-100 focus:ring-gray-500'\n}\n\nconst sizes = {\n  sm: 'px-3 py-1.5 text-sm',\n  md: 'px-4 py-2 text-base',\n  lg: 'px-6 py-3 text-lg'\n}\n\nexport function Button({\n  variant = 'primary',\n  size = 'md',\n  disabled = false,\n  children,\n  onClick\n}: ButtonProps) {\n  return (\n    <button\n      onClick={onClick}\n      disabled={disabled}\n      className={`\n        inline-flex items-center justify-center\n        font-medium rounded-lg\n        transition-colors duration-200\n        focus:outline-none focus:ring-2 focus:ring-offset-2\n        disabled:opacity-50 disabled:cursor-not-allowed\n        ${variants[variant]}\n        ${sizes[size]}\n      `}\n    >\n      {children}\n    </button>\n  )\n}\n```\n\n#### Card Component\n```tsx\ninterface CardProps {\n  title?: string\n  children: React.ReactNode\n  footer?: React.ReactNode\n}\n\nexport function Card({ title, children, footer }: CardProps) {\n  return (\n    <div className=\"bg-white dark:bg-gray-800 rounded-xl shadow-sm border border-gray-200 dark:border-gray-700 overflow-hidden\">\n      {title && (\n        <div className=\"px-6 py-4 border-b border-gray-200 dark:border-gray-700\">\n          <h3 className=\"text-lg font-semibold text-gray-900 dark:text-white\">\n            {title}\n          </h3>\n        </div>\n      )}\n      <div className=\"p-6\">\n        {children}\n      </div>\n      {footer && (\n        <div className=\"px-6 py-4 bg-gray-50 dark:bg-gray-900 border-t border-gray-200 dark:border-gray-700\">\n          {footer}\n        </div>\n      )}\n    </div>\n  )\n}\n```\n\n#### Input Component\n```tsx\ninterface InputProps extends React.InputHTMLAttributes<HTMLInputElement> {\n  label?: string\n  error?: string\n  hint?: string\n}\n\nexport function Input({ label, error, hint, id, ...props }: InputProps) {\n  const inputId = id || label?.toLowerCase().replace(/\\s/g, '-')\n\n  return (\n    <div className=\"space-y-1\">\n      {label && (\n        <label\n          htmlFor={inputId}\n          className=\"block text-sm font-medium text-gray-700 dark:text-gray-300\"\n        >\n          {label}\n        </label>\n      )}\n      <input\n        id={inputId}\n        className={`\n          block w-full rounded-lg border px-3 py-2\n          text-gray-900 dark:text-white\n          placeholder-gray-400 dark:placeholder-gray-500\n          bg-white dark:bg-gray-800\n          focus:outline-none focus:ring-2 focus:ring-offset-0\n          transition-colors\n          ${error\n            ? 'border-red-500 focus:border-red-500 focus:ring-red-500'\n            : 'border-gray-300 dark:border-gray-600 focus:border-blue-500 focus:ring-blue-500'\n          }\n        `}\n        {...props}\n      />\n      {error && (\n        <p className=\"text-sm text-red-600 dark:text-red-400\">{error}</p>\n      )}\n      {hint && !error && (\n        <p className=\"text-sm text-gray-500 dark:text-gray-400\">{hint}</p>\n      )}\n    </div>\n  )\n}\n```\n\n### 3. Use clsx/tailwind-merge\n\n```bash\nnpm install clsx tailwind-merge\n```\n\n```tsx\nimport { clsx, type ClassValue } from 'clsx'\nimport { twMerge } from 'tailwind-merge'\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs))\n}\n\n// Usage\n<button className={cn(\n  \"base-styles\",\n  variant === 'primary' && \"primary-styles\",\n  disabled && \"disabled-styles\",\n  className // Allow overrides\n)}>\n```\n\n### 4. Accessibility Checklist\n\n- [ ] Semantic HTML elements\n- [ ] Focus states visible\n- [ ] ARIA labels where needed\n- [ ] Keyboard navigation\n- [ ] Color contrast (4.5:1 for text)\n- [ ] Reduced motion support\n\n### 5. Dark Mode Support\n\n```tsx\n// Always include dark mode variants\nclassName=\"\n  bg-white dark:bg-gray-800\n  text-gray-900 dark:text-white\n  border-gray-200 dark:border-gray-700\n\"\n```\n\n## Output\n\nProvide:\n1. Complete component code with TypeScript types\n2. All variants and sizes\n3. Dark mode support\n4. Accessibility features\n5. Usage examples\n6. Props documentation"
              },
              {
                "name": "/tailwind-debug",
                "description": "Debug Tailwind CSS issues, missing styles, and configuration problems",
                "path": "plugins/tailwindcss-master/commands/tailwind-debug.md",
                "frontmatter": {
                  "name": "tailwind-debug",
                  "description": "Debug Tailwind CSS issues, missing styles, and configuration problems",
                  "argument-hint": "[issue] e.g., 'classes not working', 'dark mode broken', 'build error'"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Tailwind CSS Debugging\n\n## Purpose\nDiagnose and resolve Tailwind CSS issues including missing styles, configuration problems, and build errors.\n\n## Workflow\n\n### 1. Identify the Problem\n\nCommon issues:\n- Styles not applying\n- Build errors\n- Missing classes\n- Dark mode not working\n- Plugins not loading\n- Slow builds\n\n### 2. Quick Diagnostic Checks\n\n#### Check Installation\n```bash\n# Verify packages installed\nnpm ls tailwindcss\nnpm ls @tailwindcss/postcss\nnpm ls @tailwindcss/vite\n```\n\n#### Check CSS Import\n```css\n/* Must have this import */\n@import \"tailwindcss\";\n```\n\n#### Verify Build is Running\n```bash\n# Restart dev server\nnpm run dev\n```\n\n### 3. Styles Not Applying\n\n#### A. Check Class Name Completeness\n```javascript\n// BAD - Dynamic classes won't be detected\nconst color = 'blue'\nclassName={`bg-${color}-500`}\n\n// GOOD - Complete class names\nconst colorClasses = {\n  blue: 'bg-blue-500',\n  red: 'bg-red-500'\n}\nclassName={colorClasses[color]}\n```\n\n#### B. Check Content Detection\n```css\n/* Add explicit sources if needed */\n@source \"./src/**/*.{html,js,jsx,ts,tsx,vue,svelte}\";\n@source \"./components/**/*.{js,jsx,ts,tsx}\";\n```\n\n#### C. Check CSS Specificity\n```html\n<!-- Use !important if needed -->\n<div class=\"!mt-0\">\n\n<!-- Or check for conflicting styles in DevTools -->\n```\n\n#### D. Clear Caches\n```bash\n# Vite\nrm -rf node_modules/.vite\n\n# Next.js\nrm -rf .next\n\n# General\nrm -rf node_modules && npm install\n```\n\n### 4. v4 Migration Issues\n\n#### PostCSS Plugin\n```javascript\n// OLD (v3)\nplugins: {\n  tailwindcss: {},\n  autoprefixer: {}\n}\n\n// NEW (v4)\nplugins: {\n  '@tailwindcss/postcss': {}\n}\n```\n\n#### Configuration Location\n```css\n/* v4 - Configure in CSS */\n@import \"tailwindcss\";\n\n@theme {\n  --color-primary: oklch(0.6 0.2 250);\n}\n```\n\n#### Dark Mode\n```css\n/* Add for selector strategy */\n@custom-variant dark (&:where(.dark, .dark *));\n```\n\n### 5. Plugin Issues\n\n#### Typography Not Working\n```css\n/* Verify plugin is loaded */\n@plugin \"@tailwindcss/typography\";\n```\n\n```html\n<!-- Use prose class -->\n<article class=\"prose dark:prose-invert\">\n  Content\n</article>\n```\n\n#### Forms Not Styled\n```html\n<!-- Forms plugin requires type attribute -->\n<input type=\"text\" />  <!--  Works -->\n<input />              <!--  No styling -->\n```\n\n### 6. Build Errors\n\n#### \"Cannot find module\"\n```bash\nnpm install -D tailwindcss @tailwindcss/postcss\n```\n\n#### \"Unknown at-rule @theme\"\n- Using v3 tooling with v4 syntax\n- Update packages to latest versions\n\n#### Large Bundle Size\n- Check for dynamic class generation\n- Review safelisted classes\n- Remove unused plugins\n\n### 7. Browser DevTools Debugging\n\n1. **Right-click** element  **Inspect**\n2. **Styles panel**  See applied rules\n3. Look for **crossed-out** styles (overridden)\n4. Check **Computed** tab for final values\n5. Search for specific classes\n\n### 8. Generate Debug Output\n\n```bash\n# Output compiled CSS\nnpx tailwindcss -o output.css\n\n# Verbose logging\nDEBUG=tailwindcss:* npm run build\n```\n\n### 9. Common Fixes\n\n| Problem | Solution |\n|---------|----------|\n| Styles missing | Check content sources, restart dev |\n| Wrong colors | Check @theme, dark mode config |\n| Layout broken | Check flex/grid classes |\n| Slow builds | Ensure using v4, check globs |\n| Plugins fail | Update to v4-compatible versions |\n\n### 10. Verification\n\n```html\n<!-- Add this to verify Tailwind is working -->\n<div class=\"p-4 m-4 bg-blue-500 text-white rounded-lg\">\n  If this is blue with white text, Tailwind works!\n</div>\n```\n\n## Output\n\nProvide:\n1. Specific diagnosis of the issue\n2. Root cause explanation\n3. Step-by-step fix\n4. Prevention tips\n5. Verification steps"
              },
              {
                "name": "/tailwind-responsive",
                "description": "Implement responsive designs with breakpoints and mobile-first patterns",
                "path": "plugins/tailwindcss-master/commands/tailwind-responsive.md",
                "frontmatter": {
                  "name": "tailwind-responsive",
                  "description": "Implement responsive designs with breakpoints and mobile-first patterns",
                  "argument-hint": "[layout] e.g., 'card grid', 'sidebar layout', 'hero section'"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Tailwind CSS Responsive Design\n\n## Purpose\nImplement responsive layouts using Tailwind's mobile-first breakpoint system.\n\n## Workflow\n\n### 1. Understand Requirements\n\nAsk user about:\n- **Breakpoints needed**: Mobile, tablet, desktop, wide\n- **Layout type**: Grid, flexbox, container\n- **Content priority**: What shows/hides at each size\n- **Custom breakpoints**: Any non-standard sizes\n\n### 2. Default Breakpoints\n\n| Prefix | Min Width | Target Devices |\n|--------|-----------|----------------|\n| (none) | 0px | Mobile (default) |\n| `sm:` | 640px | Large phones, small tablets |\n| `md:` | 768px | Tablets |\n| `lg:` | 1024px | Laptops |\n| `xl:` | 1280px | Desktops |\n| `2xl:` | 1536px | Large desktops |\n\n### 3. Common Responsive Patterns\n\n#### Responsive Grid\n```html\n<div class=\"grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4 md:gap-6\">\n  <div>Item 1</div>\n  <div>Item 2</div>\n  <div>Item 3</div>\n  <div>Item 4</div>\n</div>\n```\n\n#### Responsive Navigation\n```html\n<nav class=\"flex flex-col md:flex-row md:items-center md:justify-between p-4\">\n  <div class=\"flex items-center justify-between\">\n    <a href=\"/\" class=\"text-xl font-bold\">Logo</a>\n    <button class=\"md:hidden p-2\">\n      <!-- Mobile menu button -->\n    </button>\n  </div>\n  <div class=\"hidden md:flex md:items-center md:space-x-4\">\n    <a href=\"/about\">About</a>\n    <a href=\"/contact\">Contact</a>\n  </div>\n</nav>\n```\n\n#### Responsive Hero\n```html\n<section class=\"py-12 md:py-16 lg:py-24 px-4 md:px-8\">\n  <div class=\"max-w-4xl mx-auto text-center\">\n    <h1 class=\"text-3xl sm:text-4xl md:text-5xl lg:text-6xl font-bold\">\n      Headline\n    </h1>\n    <p class=\"mt-4 text-lg md:text-xl text-gray-600 max-w-2xl mx-auto\">\n      Description text\n    </p>\n    <div class=\"mt-8 flex flex-col sm:flex-row gap-4 justify-center\">\n      <button class=\"px-6 py-3 bg-blue-600 text-white rounded-lg\">\n        Primary\n      </button>\n      <button class=\"px-6 py-3 border border-gray-300 rounded-lg\">\n        Secondary\n      </button>\n    </div>\n  </div>\n</section>\n```\n\n#### Responsive Sidebar Layout\n```html\n<div class=\"flex flex-col lg:flex-row min-h-screen\">\n  <!-- Sidebar -->\n  <aside class=\"w-full lg:w-64 lg:flex-shrink-0 bg-gray-100\">\n    <nav class=\"p-4\">\n      <!-- Sidebar content -->\n    </nav>\n  </aside>\n\n  <!-- Main content -->\n  <main class=\"flex-1 p-4 lg:p-8\">\n    <!-- Main content -->\n  </main>\n</div>\n```\n\n#### Responsive Card Grid\n```html\n<div class=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\">\n  <article class=\"bg-white rounded-lg shadow-sm overflow-hidden\">\n    <img class=\"w-full h-48 object-cover\" src=\"...\" alt=\"\">\n    <div class=\"p-4 md:p-6\">\n      <h3 class=\"text-lg md:text-xl font-semibold\">Title</h3>\n      <p class=\"mt-2 text-gray-600 text-sm md:text-base\">Description</p>\n    </div>\n  </article>\n</div>\n```\n\n### 4. Show/Hide Patterns\n\n```html\n<!-- Mobile only -->\n<div class=\"block md:hidden\">Mobile content</div>\n\n<!-- Desktop only -->\n<div class=\"hidden md:block\">Desktop content</div>\n\n<!-- Different content per breakpoint -->\n<div class=\"md:hidden\">Mobile nav</div>\n<div class=\"hidden md:flex lg:hidden\">Tablet nav</div>\n<div class=\"hidden lg:flex\">Desktop nav</div>\n```\n\n### 5. Responsive Typography\n\n```html\n<h1 class=\"\n  text-2xl sm:text-3xl md:text-4xl lg:text-5xl xl:text-6xl\n  font-bold leading-tight\n\">\n  Responsive Heading\n</h1>\n\n<p class=\"\n  text-base md:text-lg\n  leading-relaxed md:leading-loose\n\">\n  Responsive paragraph\n</p>\n```\n\n### 6. Responsive Spacing\n\n```html\n<section class=\"\n  py-8 md:py-12 lg:py-16 xl:py-24\n  px-4 md:px-8 lg:px-12\n\">\n  <div class=\"\n    max-w-7xl mx-auto\n    space-y-6 md:space-y-8 lg:space-y-12\n  \">\n    Content\n  </div>\n</section>\n```\n\n### 7. Custom Breakpoints\n\n```css\n@theme {\n  /* Add custom breakpoints */\n  --breakpoint-xs: 475px;\n  --breakpoint-3xl: 1920px;\n}\n```\n\n```html\n<div class=\"grid xs:grid-cols-2 3xl:grid-cols-6\">\n  Custom breakpoints\n</div>\n```\n\n### 8. Container Queries\n\n```css\n@plugin \"@tailwindcss/container-queries\";\n```\n\n```html\n<!-- Respond to container size, not viewport -->\n<div class=\"@container\">\n  <div class=\"flex flex-col @md:flex-row @lg:gap-8\">\n    Component content\n  </div>\n</div>\n```\n\n## Testing Checklist\n\n- [ ] Test at each breakpoint\n- [ ] Check intermediate sizes\n- [ ] Verify touch targets on mobile (min 44px)\n- [ ] Test with actual content\n- [ ] Check landscape orientation\n\n## Output\n\nProvide:\n1. Complete responsive HTML structure\n2. Explanation of breakpoint choices\n3. Show/hide logic for each screen size\n4. Mobile-first progressive enhancement\n5. Testing recommendations"
              },
              {
                "name": "/tailwind-setup",
                "description": "Set up Tailwind CSS v4 with optimal configuration for your project and framework",
                "path": "plugins/tailwindcss-master/commands/tailwind-setup.md",
                "frontmatter": {
                  "name": "tailwind-setup",
                  "description": "Set up Tailwind CSS v4 with optimal configuration for your project and framework",
                  "argument-hint": "[framework] e.g., 'nextjs', 'vite react', 'vue', 'astro'"
                },
                "content": "## CRITICAL GUIDELINES\n\n### Windows File Path Requirements\n\n**MANDATORY: Always Use Backslashes on Windows for File Paths**\n\nWhen using Edit or Write tools on Windows, you MUST use backslashes (`\\`) in file paths, NOT forward slashes (`/`).\n\n---\n\n# Tailwind CSS Setup\n\n## Purpose\nConfigure Tailwind CSS v4 for a new or existing project with the optimal setup for your framework.\n\n## Workflow\n\n### 1. Determine Project Type\n\nAsk user about:\n- **Framework**: React (Vite), Next.js, Vue, Nuxt, Svelte, Astro, or vanilla\n- **TypeScript**: Yes/No\n- **Package Manager**: npm, pnpm, yarn, bun\n\n### 2. Install Dependencies\n\n#### Vite Projects (React, Vue, Svelte)\n```bash\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n#### Next.js / PostCSS Projects\n```bash\nnpm install -D tailwindcss @tailwindcss/postcss\n```\n\n### 3. Configure Build Tool\n\n#### Vite Configuration\n```javascript\n// vite.config.ts\nimport tailwindcss from '@tailwindcss/vite'\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  plugins: [tailwindcss()]\n})\n```\n\n#### PostCSS Configuration\n```javascript\n// postcss.config.mjs\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {}\n  }\n}\n```\n\n### 4. Create CSS Entry Point\n\n```css\n/* src/index.css or app/globals.css */\n@import \"tailwindcss\";\n\n@theme {\n  /* Custom design tokens */\n  --color-primary: oklch(0.6 0.2 250);\n  --color-secondary: oklch(0.7 0.15 180);\n  --font-sans: \"Inter\", system-ui, sans-serif;\n}\n```\n\n### 5. Add Dark Mode (Optional)\n\nFor selector-based dark mode:\n```css\n@import \"tailwindcss\";\n@custom-variant dark (&:where(.dark, .dark *));\n```\n\n### 6. Add Plugins (Optional)\n\n```css\n@import \"tailwindcss\";\n\n/* Typography for Markdown content */\n@plugin \"@tailwindcss/typography\";\n\n/* Form element resets */\n@plugin \"@tailwindcss/forms\";\n\n/* Container queries */\n@plugin \"@tailwindcss/container-queries\";\n```\n\n### 7. Import CSS in Entry Point\n\n```javascript\n// main.ts / main.tsx\nimport './index.css'\n```\n\n### 8. Install Developer Tools\n\n```bash\n# VS Code extension\ncode --install-extension bradlc.vscode-tailwindcss\n\n# Prettier plugin for class sorting\nnpm install -D prettier prettier-plugin-tailwindcss\n```\n\nCreate `.prettierrc`:\n```json\n{\n  \"plugins\": [\"prettier-plugin-tailwindcss\"]\n}\n```\n\n### 9. Verify Setup\n\n```html\n<!-- Test in any component -->\n<div class=\"p-4 bg-blue-500 text-white rounded-lg\">\n  Tailwind is working!\n</div>\n```\n\n## Framework-Specific Notes\n\n### Next.js\n- CSS file goes in `app/globals.css`\n- Import in `app/layout.tsx`\n- Use `suppressHydrationWarning` on `<html>` for dark mode\n\n### Vue with Scoped Styles\n- Use `@reference` directive in scoped styles\n- Or use CSS variables directly\n\n### Astro\n- Add plugin to `astro.config.mjs` vite plugins\n- Import CSS in layout files\n\n## Output\n\nProvide:\n1. Complete installation commands for chosen setup\n2. Configuration files with explanations\n3. CSS entry point with recommended @theme setup\n4. Framework-specific import instructions\n5. Verification steps"
              }
            ],
            "skills": [
              {
                "name": "tailwindcss-accessibility",
                "description": "Tailwind CSS accessibility patterns including focus management and ARIA support",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-accessibility/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-accessibility",
                  "description": "Tailwind CSS accessibility patterns including focus management and ARIA support"
                },
                "content": "# Tailwind CSS Accessibility Patterns\n\n## Focus Management\n\n### Focus Ring Utilities\n\n```html\n<!-- Default focus ring -->\n<button class=\"focus:outline-none focus:ring-2 focus:ring-brand-500 focus:ring-offset-2\">\n  Button\n</button>\n\n<!-- Focus-visible for keyboard users only -->\n<button class=\"focus:outline-none focus-visible:ring-2 focus-visible:ring-brand-500\">\n  Only shows ring for keyboard focus\n</button>\n\n<!-- Focus-within for parent containers -->\n<div class=\"focus-within:ring-2 focus-within:ring-brand-500 rounded-lg p-1\">\n  <input type=\"text\" class=\"border-none focus:outline-none\" />\n</div>\n\n<!-- Custom focus ring component -->\n```\n\n```css\n@layer components {\n  .focus-ring {\n    @apply focus:outline-none focus-visible:ring-2 focus-visible:ring-brand-500 focus-visible:ring-offset-2;\n  }\n\n  .focus-ring-inset {\n    @apply focus:outline-none focus-visible:ring-2 focus-visible:ring-brand-500 focus-visible:ring-inset;\n  }\n}\n```\n\n### Skip Links\n\n```html\n<!-- Skip to main content -->\n<a\n  href=\"#main-content\"\n  class=\"\n    sr-only focus:not-sr-only\n    focus:absolute focus:top-4 focus:left-4 focus:z-50\n    focus:bg-white focus:px-4 focus:py-2 focus:rounded-md focus:shadow-lg\n    focus:ring-2 focus:ring-brand-500\n  \"\n>\n  Skip to main content\n</a>\n\n<header>Navigation...</header>\n<main id=\"main-content\" tabindex=\"-1\">\n  Main content\n</main>\n```\n\n### Focus Trap Pattern\n\n```html\n<!-- Modal with focus management -->\n<div\n  role=\"dialog\"\n  aria-modal=\"true\"\n  aria-labelledby=\"modal-title\"\n  class=\"fixed inset-0 z-50 flex items-center justify-center\"\n>\n  <div class=\"fixed inset-0 bg-black/50\" aria-hidden=\"true\"></div>\n  <div\n    class=\"relative bg-white rounded-xl p-6 max-w-md w-full\"\n    role=\"document\"\n  >\n    <h2 id=\"modal-title\" class=\"text-lg font-semibold\">Modal Title</h2>\n    <p>Modal content</p>\n    <button class=\"focus-ring\">Close</button>\n  </div>\n</div>\n```\n\n## Screen Reader Utilities\n\n### Visually Hidden Content\n\n```html\n<!-- Hidden visually but available to screen readers -->\n<span class=\"sr-only\">Additional context for screen readers</span>\n\n<!-- Show on focus (skip links) -->\n<a href=\"#main\" class=\"sr-only focus:not-sr-only\">Skip to main</a>\n\n<!-- Icon with accessible label -->\n<button>\n  <svg aria-hidden=\"true\">...</svg>\n  <span class=\"sr-only\">Close menu</span>\n</button>\n\n<!-- Form labels -->\n<label>\n  <span class=\"sr-only\">Search</span>\n  <input type=\"search\" placeholder=\"Search...\" />\n</label>\n```\n\n### Announcing Dynamic Content\n\n```html\n<!-- Live region for announcements -->\n<div\n  role=\"status\"\n  aria-live=\"polite\"\n  aria-atomic=\"true\"\n  class=\"sr-only\"\n>\n  <!-- Dynamic content announced to screen readers -->\n  3 items added to cart\n</div>\n\n<!-- Alert for important messages -->\n<div\n  role=\"alert\"\n  aria-live=\"assertive\"\n  class=\"bg-red-100 text-red-800 p-4 rounded-lg\"\n>\n  Error: Please correct the form\n</div>\n```\n\n## Color Contrast\n\n### High Contrast Patterns\n\n```html\n<!-- Ensure sufficient contrast -->\n<p class=\"text-gray-700 bg-white\">4.5:1 contrast ratio</p>\n<p class=\"text-gray-500 bg-white\">May not meet WCAG AA (3:1 min for large text)</p>\n\n<!-- Large text (18pt+) needs 3:1 -->\n<h1 class=\"text-4xl text-gray-600 bg-white\">Large text - 3:1 ratio OK</h1>\n\n<!-- Interactive elements need 3:1 against adjacent colors -->\n<button class=\"\n  bg-brand-500 text-white\n  border-2 border-brand-500\n  focus:ring-2 focus:ring-brand-500 focus:ring-offset-2\n\">\n  Accessible Button\n</button>\n```\n\n### Dark Mode Contrast\n\n```html\n<!-- Maintain contrast in both modes -->\n<p class=\"text-gray-900 dark:text-gray-100\">\n  High contrast text\n</p>\n\n<p class=\"text-gray-600 dark:text-gray-400\">\n  Secondary text with adequate contrast\n</p>\n\n<!-- Avoid low contrast combinations -->\n<p class=\"text-gray-400 dark:text-gray-600\">\n   May have contrast issues in dark mode\n</p>\n```\n\n### Focus Indicator Contrast\n\n```css\n@theme {\n  /* High contrast focus ring */\n  --color-focus: oklch(0.55 0.25 250);\n  --color-focus-offset: oklch(1 0 0);\n}\n```\n\n```html\n<button class=\"\n  focus:outline-none\n  focus-visible:ring-2\n  focus-visible:ring-[var(--color-focus)]\n  focus-visible:ring-offset-2\n  focus-visible:ring-offset-[var(--color-focus-offset)]\n\">\n  High contrast focus\n</button>\n```\n\n## Motion and Animation\n\n### Reduced Motion\n\n```html\n<!-- Respect user's motion preferences -->\n<div class=\"\n  animate-bounce\n  motion-reduce:animate-none\n\">\n  Bouncing element (static for motion-sensitive users)\n</div>\n\n<!-- Safer alternative animations -->\n<div class=\"\n  transition-opacity duration-300\n  motion-reduce:transition-none\n\">\n  Fades in (instant for motion-sensitive)\n</div>\n\n<!-- Use opacity instead of movement -->\n<div class=\"\n  transition-all\n  hover:scale-105 hover:shadow-lg\n  motion-reduce:hover:scale-100 motion-reduce:hover:shadow-md\n\">\n  Scales on hover (shadow only for motion-sensitive)\n</div>\n```\n\n### Safe Animation Patterns\n\n```css\n@layer components {\n  /* Animations that respect reduced motion */\n  .animate-fade-in {\n    @apply animate-in fade-in duration-300;\n    @apply motion-reduce:animate-none motion-reduce:opacity-100;\n  }\n\n  .animate-slide-up {\n    @apply animate-in slide-in-from-bottom-4 duration-300;\n    @apply motion-reduce:animate-none motion-reduce:translate-y-0;\n  }\n}\n```\n\n### Pause Animation on Hover\n\n```html\n<!-- Allow users to pause animations -->\n<div class=\"\n  animate-spin\n  hover:animate-pause\n  motion-reduce:animate-none\n\">\n  Loading spinner\n</div>\n```\n\n## Form Accessibility\n\n### Accessible Form Fields\n\n```html\n<div class=\"space-y-4\">\n  <!-- Text input with label -->\n  <div>\n    <label for=\"email\" class=\"block text-sm font-medium text-gray-700\">\n      Email address\n      <span class=\"text-red-500\" aria-hidden=\"true\">*</span>\n    </label>\n    <input\n      type=\"email\"\n      id=\"email\"\n      name=\"email\"\n      required\n      aria-required=\"true\"\n      aria-describedby=\"email-hint email-error\"\n      class=\"\n        mt-1 block w-full rounded-md border-gray-300\n        focus:border-brand-500 focus:ring-brand-500\n        aria-invalid:border-red-500 aria-invalid:ring-red-500\n      \"\n    />\n    <p id=\"email-hint\" class=\"mt-1 text-sm text-gray-500\">\n      We'll never share your email\n    </p>\n    <p id=\"email-error\" class=\"mt-1 text-sm text-red-600 hidden\" role=\"alert\">\n      Please enter a valid email\n    </p>\n  </div>\n\n  <!-- Checkbox with accessible label -->\n  <div class=\"flex items-start gap-3\">\n    <input\n      type=\"checkbox\"\n      id=\"terms\"\n      name=\"terms\"\n      class=\"\n        h-4 w-4 rounded border-gray-300 text-brand-500\n        focus:ring-brand-500\n      \"\n    />\n    <label for=\"terms\" class=\"text-sm text-gray-700\">\n      I agree to the\n      <a href=\"/terms\" class=\"text-brand-500 underline\">terms and conditions</a>\n    </label>\n  </div>\n\n  <!-- Radio group -->\n  <fieldset>\n    <legend class=\"text-sm font-medium text-gray-700\">Notification preference</legend>\n    <div class=\"mt-2 space-y-2\">\n      <div class=\"flex items-center gap-3\">\n        <input type=\"radio\" id=\"email-pref\" name=\"notification\" value=\"email\" class=\"h-4 w-4\" />\n        <label for=\"email-pref\">Email</label>\n      </div>\n      <div class=\"flex items-center gap-3\">\n        <input type=\"radio\" id=\"sms-pref\" name=\"notification\" value=\"sms\" class=\"h-4 w-4\" />\n        <label for=\"sms-pref\">SMS</label>\n      </div>\n    </div>\n  </fieldset>\n</div>\n```\n\n### Error States\n\n```html\n<!-- Input with error -->\n<div>\n  <label for=\"password\" class=\"block text-sm font-medium text-gray-700\">\n    Password\n  </label>\n  <input\n    type=\"password\"\n    id=\"password\"\n    aria-invalid=\"true\"\n    aria-describedby=\"password-error\"\n    class=\"\n      mt-1 block w-full rounded-md\n      border-red-500 text-red-900\n      focus:border-red-500 focus:ring-red-500\n    \"\n  />\n  <p id=\"password-error\" class=\"mt-1 text-sm text-red-600\" role=\"alert\">\n    <span class=\"sr-only\">Error:</span>\n    Password must be at least 8 characters\n  </p>\n</div>\n```\n\n### Form Validation Feedback\n\n```css\n/* Style based on aria-invalid attribute */\n@custom-variant aria-invalid (&[aria-invalid=\"true\"]);\n```\n\n```html\n<input\n  class=\"\n    border-gray-300\n    aria-invalid:border-red-500\n    aria-invalid:text-red-900\n    aria-invalid:focus:ring-red-500\n  \"\n  aria-invalid=\"true\"\n/>\n```\n\n## Interactive Components\n\n### Accessible Buttons\n\n```html\n<!-- Button with loading state -->\n<button\n  type=\"submit\"\n  aria-busy=\"true\"\n  aria-disabled=\"true\"\n  class=\"\n    relative\n    aria-busy:cursor-wait\n    aria-disabled:opacity-50 aria-disabled:cursor-not-allowed\n  \"\n>\n  <span class=\"aria-busy:invisible\">Submit</span>\n  <span class=\"absolute inset-0 flex items-center justify-center aria-busy:visible invisible\">\n    <svg class=\"animate-spin h-5 w-5\" aria-hidden=\"true\">...</svg>\n    <span class=\"sr-only\">Loading...</span>\n  </span>\n</button>\n\n<!-- Icon button -->\n<button\n  type=\"button\"\n  aria-label=\"Close dialog\"\n  class=\"rounded-full p-2 hover:bg-gray-100 focus-ring\"\n>\n  <svg aria-hidden=\"true\" class=\"h-5 w-5\">...</svg>\n</button>\n\n<!-- Toggle button -->\n<button\n  type=\"button\"\n  aria-pressed=\"false\"\n  class=\"\n    px-4 py-2 rounded-lg border\n    aria-pressed:bg-brand-500 aria-pressed:text-white aria-pressed:border-brand-500\n  \"\n>\n  <span class=\"sr-only\">Toggle feature</span>\n  Feature\n</button>\n```\n\n### Accessible Dropdowns\n\n```html\n<div class=\"relative\">\n  <button\n    type=\"button\"\n    aria-haspopup=\"menu\"\n    aria-expanded=\"false\"\n    aria-controls=\"dropdown-menu\"\n    class=\"flex items-center gap-2 px-4 py-2 rounded-lg border focus-ring\"\n  >\n    Options\n    <svg aria-hidden=\"true\" class=\"h-4 w-4\">...</svg>\n  </button>\n\n  <ul\n    id=\"dropdown-menu\"\n    role=\"menu\"\n    aria-labelledby=\"dropdown-button\"\n    class=\"\n      absolute top-full mt-1 w-48 rounded-lg bg-white shadow-lg border\n      hidden aria-expanded:block\n    \"\n  >\n    <li role=\"none\">\n      <a\n        href=\"#\"\n        role=\"menuitem\"\n        tabindex=\"-1\"\n        class=\"block px-4 py-2 hover:bg-gray-100 focus:bg-gray-100 focus:outline-none\"\n      >\n        Edit\n      </a>\n    </li>\n    <li role=\"none\">\n      <a\n        href=\"#\"\n        role=\"menuitem\"\n        tabindex=\"-1\"\n        class=\"block px-4 py-2 hover:bg-gray-100 focus:bg-gray-100 focus:outline-none\"\n      >\n        Delete\n      </a>\n    </li>\n  </ul>\n</div>\n```\n\n### Accessible Tabs\n\n```html\n<div>\n  <div role=\"tablist\" aria-label=\"Account settings\" class=\"flex border-b\">\n    <button\n      role=\"tab\"\n      aria-selected=\"true\"\n      aria-controls=\"panel-1\"\n      id=\"tab-1\"\n      class=\"\n        px-4 py-2 border-b-2\n        aria-selected:border-brand-500 aria-selected:text-brand-500\n        hover:text-gray-700\n        focus-visible:ring-2 focus-visible:ring-inset\n      \"\n    >\n      Profile\n    </button>\n    <button\n      role=\"tab\"\n      aria-selected=\"false\"\n      aria-controls=\"panel-2\"\n      id=\"tab-2\"\n      tabindex=\"-1\"\n      class=\"px-4 py-2 border-b-2 border-transparent\"\n    >\n      Settings\n    </button>\n  </div>\n\n  <div\n    role=\"tabpanel\"\n    id=\"panel-1\"\n    aria-labelledby=\"tab-1\"\n    tabindex=\"0\"\n    class=\"p-4 focus:outline-none focus-visible:ring-2 focus-visible:ring-inset\"\n  >\n    Profile content\n  </div>\n\n  <div\n    role=\"tabpanel\"\n    id=\"panel-2\"\n    aria-labelledby=\"tab-2\"\n    tabindex=\"0\"\n    hidden\n    class=\"p-4\"\n  >\n    Settings content\n  </div>\n</div>\n```\n\n## Touch Targets\n\n### Minimum Touch Target Size\n\n```html\n<!-- Minimum 44x44px for touch targets -->\n<button class=\"min-h-[44px] min-w-[44px] p-2\">\n  <svg class=\"h-6 w-6\">...</svg>\n</button>\n\n<!-- Extend touch target beyond visible element -->\n<a href=\"#\" class=\"relative inline-block\">\n  Small Link\n  <span class=\"absolute -inset-2\" aria-hidden=\"true\"></span>\n</a>\n\n<!-- Icon button with adequate touch target -->\n<button class=\"relative p-2 -m-2\">\n  <svg class=\"h-5 w-5\" aria-hidden=\"true\">...</svg>\n  <span class=\"sr-only\">Action</span>\n</button>\n```\n\n### Spacing Between Interactive Elements\n\n```html\n<!-- Adequate spacing between touch targets -->\n<div class=\"flex gap-4\">\n  <button class=\"min-h-[44px] px-4\">Button 1</button>\n  <button class=\"min-h-[44px] px-4\">Button 2</button>\n</div>\n\n<!-- Stacked links with spacing -->\n<nav class=\"flex flex-col\">\n  <a href=\"#\" class=\"py-3 px-4\">Link 1</a>\n  <a href=\"#\" class=\"py-3 px-4\">Link 2</a>\n  <a href=\"#\" class=\"py-3 px-4\">Link 3</a>\n</nav>\n```\n\n## Text Accessibility\n\n### Readable Text\n\n```html\n<!-- Adequate line height for body text -->\n<p class=\"leading-relaxed\">\n  Long form content with comfortable line height\n</p>\n\n<!-- Limit line length for readability -->\n<article class=\"max-w-prose\">\n  <p class=\"leading-relaxed\">\n    Content with optimal line length (45-75 characters)\n  </p>\n</article>\n\n<!-- Adequate paragraph spacing -->\n<div class=\"space-y-6\">\n  <p>Paragraph 1</p>\n  <p>Paragraph 2</p>\n</div>\n```\n\n### Text Resizing\n\n```html\n<!-- Use relative units for text -->\n<p class=\"text-base\">Scales with user's font size preferences</p>\n\n<!-- Don't use fixed pixel values for text -->\n<p class=\"text-[14px]\"> Won't scale with browser zoom</p>\n\n<!-- Container that doesn't break on text zoom -->\n<div class=\"min-h-[auto]\">\n  Content height adjusts with text size\n</div>\n```\n\n## Semantic HTML with Tailwind\n\n### Landmark Regions\n\n```html\n<body class=\"min-h-screen flex flex-col\">\n  <header class=\"sticky top-0 bg-white shadow z-50\">\n    <nav aria-label=\"Main navigation\">...</nav>\n  </header>\n\n  <main id=\"main-content\" class=\"flex-1\">\n    <article>\n      <h1>Page Title</h1>\n      <section aria-labelledby=\"section-1\">\n        <h2 id=\"section-1\">Section Title</h2>\n        <p>Content...</p>\n      </section>\n    </article>\n\n    <aside aria-label=\"Related content\" class=\"hidden lg:block\">\n      Sidebar content\n    </aside>\n  </main>\n\n  <footer class=\"bg-gray-800 text-white\">\n    <nav aria-label=\"Footer navigation\">...</nav>\n  </footer>\n</body>\n```\n\n### Heading Hierarchy\n\n```html\n<article class=\"prose\">\n  <h1 class=\"text-4xl font-bold\">Main Title (H1)</h1>\n\n  <section>\n    <h2 class=\"text-2xl font-semibold\">Section (H2)</h2>\n\n    <section>\n      <h3 class=\"text-xl font-medium\">Subsection (H3)</h3>\n      <p>Content...</p>\n    </section>\n  </section>\n</article>\n```\n\n## Testing Accessibility\n\n### Browser DevTools Checklist\n\n1. **Color contrast**: Use contrast checker\n2. **Focus order**: Tab through the page\n3. **Zoom**: Test at 200% zoom\n4. **Reduced motion**: Enable in OS settings\n\n### Automated Testing\n\n```javascript\n// axe-core integration\nimport { axe, toHaveNoViolations } from 'jest-axe';\n\nexpect.extend(toHaveNoViolations);\n\ntest('component is accessible', async () => {\n  const { container } = render(<Component />);\n  const results = await axe(container);\n  expect(results).toHaveNoViolations();\n});\n```\n\n## Best Practices Summary\n\n| Pattern | Implementation |\n|---------|---------------|\n| Focus visible | `focus-visible:ring-2` |\n| Screen reader only | `sr-only` |\n| Skip links | `sr-only focus:not-sr-only` |\n| Reduced motion | `motion-reduce:animate-none` |\n| Touch targets | `min-h-[44px] min-w-[44px]` |\n| Text contrast | Test with WCAG tools |\n| Form errors | `aria-invalid` + `role=\"alert\"` |"
              },
              {
                "name": "tailwindcss-advanced-components",
                "description": "Tailwind CSS advanced component patterns with CVA integration and variant management",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-components/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-advanced-components",
                  "description": "Tailwind CSS advanced component patterns with CVA integration and variant management"
                },
                "content": "# Tailwind CSS Advanced Component Patterns\n\n## Component Variants with CVA\n\n### Class Variance Authority Integration\n\n```bash\nnpm install class-variance-authority\n```\n\n```typescript\n// components/Button.tsx\nimport { cva, type VariantProps } from 'class-variance-authority';\nimport { cn } from '@/lib/utils';\n\nconst buttonVariants = cva(\n  // Base styles\n  'inline-flex items-center justify-center rounded-lg font-medium transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50',\n  {\n    variants: {\n      variant: {\n        default: 'bg-brand-500 text-white hover:bg-brand-600 focus-visible:ring-brand-500',\n        secondary: 'bg-gray-100 text-gray-900 hover:bg-gray-200 focus-visible:ring-gray-500',\n        outline: 'border border-gray-300 bg-transparent hover:bg-gray-100 focus-visible:ring-gray-500',\n        ghost: 'hover:bg-gray-100 focus-visible:ring-gray-500',\n        destructive: 'bg-red-500 text-white hover:bg-red-600 focus-visible:ring-red-500',\n        link: 'text-brand-500 underline-offset-4 hover:underline',\n      },\n      size: {\n        sm: 'h-8 px-3 text-sm',\n        md: 'h-10 px-4 text-sm',\n        lg: 'h-12 px-6 text-base',\n        xl: 'h-14 px-8 text-lg',\n        icon: 'h-10 w-10',\n      },\n      fullWidth: {\n        true: 'w-full',\n      },\n    },\n    compoundVariants: [\n      {\n        variant: 'outline',\n        size: 'sm',\n        className: 'border',\n      },\n      {\n        variant: 'outline',\n        size: ['md', 'lg', 'xl'],\n        className: 'border-2',\n      },\n    ],\n    defaultVariants: {\n      variant: 'default',\n      size: 'md',\n    },\n  }\n);\n\nexport interface ButtonProps\n  extends React.ButtonHTMLAttributes<HTMLButtonElement>,\n    VariantProps<typeof buttonVariants> {\n  asChild?: boolean;\n}\n\nexport function Button({\n  className,\n  variant,\n  size,\n  fullWidth,\n  asChild = false,\n  ...props\n}: ButtonProps) {\n  return (\n    <button\n      className={cn(buttonVariants({ variant, size, fullWidth, className }))}\n      {...props}\n    />\n  );\n}\n```\n\n### Usage\n\n```tsx\n<Button>Default</Button>\n<Button variant=\"secondary\" size=\"lg\">Large Secondary</Button>\n<Button variant=\"destructive\" fullWidth>Delete</Button>\n<Button variant=\"ghost\" size=\"icon\"><IconMenu /></Button>\n```\n\n## Compound Components Pattern\n\n### Context-Based Component System\n\n```tsx\n// components/Card/index.tsx\nimport { createContext, useContext, forwardRef } from 'react';\nimport { cn } from '@/lib/utils';\n\n// Context for shared state\nconst CardContext = createContext<{ variant?: 'default' | 'elevated' | 'outline' }>({});\n\n// Root component\ninterface CardProps extends React.HTMLAttributes<HTMLDivElement> {\n  variant?: 'default' | 'elevated' | 'outline';\n}\n\nconst Card = forwardRef<HTMLDivElement, CardProps>(\n  ({ className, variant = 'default', children, ...props }, ref) => {\n    const variants = {\n      default: 'bg-white border border-gray-200',\n      elevated: 'bg-white shadow-lg',\n      outline: 'bg-transparent border-2 border-gray-300',\n    };\n\n    return (\n      <CardContext.Provider value={{ variant }}>\n        <div\n          ref={ref}\n          className={cn(\n            'rounded-xl',\n            variants[variant],\n            className\n          )}\n          {...props}\n        >\n          {children}\n        </div>\n      </CardContext.Provider>\n    );\n  }\n);\n\n// Sub-components\nconst CardHeader = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div\n      ref={ref}\n      className={cn('flex flex-col gap-1.5 p-6 pb-0', className)}\n      {...props}\n    />\n  )\n);\n\nconst CardTitle = forwardRef<HTMLHeadingElement, React.HTMLAttributes<HTMLHeadingElement>>(\n  ({ className, ...props }, ref) => (\n    <h3\n      ref={ref}\n      className={cn('text-xl font-semibold leading-none tracking-tight', className)}\n      {...props}\n    />\n  )\n);\n\nconst CardDescription = forwardRef<HTMLParagraphElement, React.HTMLAttributes<HTMLParagraphElement>>(\n  ({ className, ...props }, ref) => (\n    <p\n      ref={ref}\n      className={cn('text-sm text-gray-500', className)}\n      {...props}\n    />\n  )\n);\n\nconst CardContent = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div ref={ref} className={cn('p-6', className)} {...props} />\n  )\n);\n\nconst CardFooter = forwardRef<HTMLDivElement, React.HTMLAttributes<HTMLDivElement>>(\n  ({ className, ...props }, ref) => (\n    <div\n      ref={ref}\n      className={cn('flex items-center p-6 pt-0', className)}\n      {...props}\n    />\n  )\n);\n\n// Named exports\nCard.displayName = 'Card';\nCardHeader.displayName = 'CardHeader';\nCardTitle.displayName = 'CardTitle';\nCardDescription.displayName = 'CardDescription';\nCardContent.displayName = 'CardContent';\nCardFooter.displayName = 'CardFooter';\n\nexport { Card, CardHeader, CardTitle, CardDescription, CardContent, CardFooter };\n```\n\n### Usage\n\n```tsx\n<Card variant=\"elevated\">\n  <CardHeader>\n    <CardTitle>Account Settings</CardTitle>\n    <CardDescription>Manage your account preferences</CardDescription>\n  </CardHeader>\n  <CardContent>\n    <form>...</form>\n  </CardContent>\n  <CardFooter className=\"justify-between\">\n    <Button variant=\"ghost\">Cancel</Button>\n    <Button>Save Changes</Button>\n  </CardFooter>\n</Card>\n```\n\n## Data Attribute Variants\n\n### CSS-Based State Management\n\n```css\n/* Define data attribute variants */\n@custom-variant data-state-open (&[data-state=\"open\"]);\n@custom-variant data-state-closed (&[data-state=\"closed\"]);\n@custom-variant data-side-top (&[data-side=\"top\"]);\n@custom-variant data-side-bottom (&[data-side=\"bottom\"]);\n@custom-variant data-side-left (&[data-side=\"left\"]);\n@custom-variant data-side-right (&[data-side=\"right\"]);\n@custom-variant data-highlighted (&[data-highlighted]);\n@custom-variant data-disabled (&[data-disabled]);\n```\n\n```tsx\n// Dropdown component using data attributes\nfunction DropdownContent({ children, ...props }) {\n  return (\n    <div\n      data-state={isOpen ? 'open' : 'closed'}\n      data-side={side}\n      className=\"\n        absolute z-50 min-w-[8rem] overflow-hidden rounded-md\n        border border-gray-200 bg-white p-1 shadow-lg\n\n        data-state-open:animate-in\n        data-state-open:fade-in-0\n        data-state-open:zoom-in-95\n\n        data-state-closed:animate-out\n        data-state-closed:fade-out-0\n        data-state-closed:zoom-out-95\n\n        data-side-top:slide-in-from-bottom-2\n        data-side-bottom:slide-in-from-top-2\n        data-side-left:slide-in-from-right-2\n        data-side-right:slide-in-from-left-2\n      \"\n      {...props}\n    >\n      {children}\n    </div>\n  );\n}\n\nfunction DropdownItem({ children, disabled, ...props }) {\n  return (\n    <div\n      data-highlighted={isHighlighted || undefined}\n      data-disabled={disabled || undefined}\n      className=\"\n        relative flex cursor-pointer select-none items-center\n        rounded-sm px-2 py-1.5 text-sm outline-none\n\n        data-highlighted:bg-gray-100\n        data-disabled:pointer-events-none\n        data-disabled:opacity-50\n      \"\n      {...props}\n    >\n      {children}\n    </div>\n  );\n}\n```\n\n## Group and Peer Patterns\n\n### Complex State Propagation\n\n```html\n<!-- Group pattern: Parent hover affects children -->\n<div class=\"group relative overflow-hidden rounded-xl\">\n  <img\n    src=\"image.jpg\"\n    class=\"transition-transform duration-300 group-hover:scale-110\"\n  />\n  <div class=\"\n    absolute inset-0 bg-gradient-to-t from-black/80 to-transparent\n    opacity-0 transition-opacity group-hover:opacity-100\n  \">\n    <div class=\"\n      absolute bottom-0 left-0 right-0 p-6\n      translate-y-4 transition-transform group-hover:translate-y-0\n    \">\n      <h3 class=\"text-xl font-bold text-white\">Title</h3>\n      <p class=\"text-gray-200\">Description</p>\n    </div>\n  </div>\n</div>\n\n<!-- Named groups for nested components -->\n<div class=\"group/card\">\n  <div class=\"group/header\">\n    <button class=\"group-hover/header:text-blue-500\">\n      Header Action\n    </button>\n  </div>\n  <div class=\"group-hover/card:bg-gray-50\">\n    Card content\n  </div>\n</div>\n\n<!-- Peer pattern: Sibling state affects elements -->\n<div class=\"relative\">\n  <input\n    type=\"email\"\n    class=\"peer w-full border rounded-lg px-4 py-2 placeholder-transparent\"\n    placeholder=\"Email\"\n  />\n  <label class=\"\n    absolute left-4 top-2 text-gray-500\n    transition-all\n    peer-placeholder-shown:top-2 peer-placeholder-shown:text-base\n    peer-focus:-top-6 peer-focus:text-sm peer-focus:text-blue-500\n    peer-[:not(:placeholder-shown)]:-top-6 peer-[:not(:placeholder-shown)]:text-sm\n  \">\n    Email address\n  </label>\n</div>\n\n<!-- Peer for form validation -->\n<div>\n  <input\n    type=\"email\"\n    class=\"peer\"\n    required\n  />\n  <p class=\"hidden text-red-500 peer-invalid:block\">\n    Please enter a valid email\n  </p>\n</div>\n```\n\n## Slot Pattern with @apply\n\n### Reusable Component Slots\n\n```css\n@layer components {\n  /* Base dialog structure */\n  .dialog {\n    @apply fixed inset-0 z-50 flex items-center justify-center;\n  }\n\n  .dialog-overlay {\n    @apply fixed inset-0 bg-black/50 backdrop-blur-sm;\n    @apply data-state-open:animate-in data-state-open:fade-in-0;\n    @apply data-state-closed:animate-out data-state-closed:fade-out-0;\n  }\n\n  .dialog-content {\n    @apply relative z-50 w-full max-w-lg rounded-xl bg-white p-6 shadow-xl;\n    @apply data-state-open:animate-in data-state-open:fade-in-0 data-state-open:zoom-in-95;\n    @apply data-state-closed:animate-out data-state-closed:fade-out-0 data-state-closed:zoom-out-95;\n  }\n\n  .dialog-header {\n    @apply flex flex-col gap-1.5 text-center sm:text-left;\n  }\n\n  .dialog-title {\n    @apply text-lg font-semibold leading-none tracking-tight;\n  }\n\n  .dialog-description {\n    @apply text-sm text-gray-500;\n  }\n\n  .dialog-footer {\n    @apply flex flex-col-reverse gap-2 sm:flex-row sm:justify-end;\n  }\n\n  .dialog-close {\n    @apply absolute right-4 top-4 rounded-sm opacity-70 hover:opacity-100;\n    @apply focus:outline-none focus:ring-2 focus:ring-offset-2;\n  }\n}\n```\n\n## Polymorphic Components\n\n### \"as\" Prop Pattern\n\n```tsx\nimport { forwardRef, ElementType, ComponentPropsWithoutRef } from 'react';\nimport { cn } from '@/lib/utils';\n\ntype PolymorphicRef<C extends ElementType> = ComponentPropsWithoutRef<C>['ref'];\n\ntype PolymorphicComponentProps<C extends ElementType, Props = {}> = Props & {\n  as?: C;\n  className?: string;\n  children?: React.ReactNode;\n} & Omit<ComponentPropsWithoutRef<C>, 'as' | 'className' | keyof Props>;\n\n// Text component that can be any element\ninterface TextProps {\n  size?: 'xs' | 'sm' | 'base' | 'lg' | 'xl' | '2xl';\n  weight?: 'normal' | 'medium' | 'semibold' | 'bold';\n  color?: 'default' | 'muted' | 'accent';\n}\n\ntype TextComponent = <C extends ElementType = 'span'>(\n  props: PolymorphicComponentProps<C, TextProps> & { ref?: PolymorphicRef<C> }\n) => React.ReactElement | null;\n\nexport const Text: TextComponent = forwardRef(\n  <C extends ElementType = 'span'>(\n    {\n      as,\n      size = 'base',\n      weight = 'normal',\n      color = 'default',\n      className,\n      children,\n      ...props\n    }: PolymorphicComponentProps<C, TextProps>,\n    ref?: PolymorphicRef<C>\n  ) => {\n    const Component = as || 'span';\n\n    const sizes = {\n      xs: 'text-xs',\n      sm: 'text-sm',\n      base: 'text-base',\n      lg: 'text-lg',\n      xl: 'text-xl',\n      '2xl': 'text-2xl',\n    };\n\n    const weights = {\n      normal: 'font-normal',\n      medium: 'font-medium',\n      semibold: 'font-semibold',\n      bold: 'font-bold',\n    };\n\n    const colors = {\n      default: 'text-gray-900',\n      muted: 'text-gray-500',\n      accent: 'text-brand-500',\n    };\n\n    return (\n      <Component\n        ref={ref}\n        className={cn(sizes[size], weights[weight], colors[color], className)}\n        {...props}\n      >\n        {children}\n      </Component>\n    );\n  }\n);\n```\n\n### Usage\n\n```tsx\n<Text>Default span</Text>\n<Text as=\"p\" size=\"lg\" color=\"muted\">Large muted paragraph</Text>\n<Text as=\"h1\" size=\"2xl\" weight=\"bold\">Bold heading</Text>\n<Text as=\"a\" href=\"/link\" color=\"accent\">Accent link</Text>\n```\n\n## Headless Component Integration\n\n### Headless UI with Tailwind\n\n```tsx\nimport { Dialog, Transition } from '@headlessui/react';\nimport { Fragment } from 'react';\n\nfunction Modal({ isOpen, onClose, title, children }) {\n  return (\n    <Transition appear show={isOpen} as={Fragment}>\n      <Dialog as=\"div\" className=\"relative z-50\" onClose={onClose}>\n        {/* Backdrop */}\n        <Transition.Child\n          as={Fragment}\n          enter=\"ease-out duration-300\"\n          enterFrom=\"opacity-0\"\n          enterTo=\"opacity-100\"\n          leave=\"ease-in duration-200\"\n          leaveFrom=\"opacity-100\"\n          leaveTo=\"opacity-0\"\n        >\n          <div className=\"fixed inset-0 bg-black/50 backdrop-blur-sm\" />\n        </Transition.Child>\n\n        <div className=\"fixed inset-0 overflow-y-auto\">\n          <div className=\"flex min-h-full items-center justify-center p-4\">\n            <Transition.Child\n              as={Fragment}\n              enter=\"ease-out duration-300\"\n              enterFrom=\"opacity-0 scale-95\"\n              enterTo=\"opacity-100 scale-100\"\n              leave=\"ease-in duration-200\"\n              leaveFrom=\"opacity-100 scale-100\"\n              leaveTo=\"opacity-0 scale-95\"\n            >\n              <Dialog.Panel className=\"\n                w-full max-w-md transform overflow-hidden rounded-2xl\n                bg-white p-6 text-left align-middle shadow-xl transition-all\n              \">\n                <Dialog.Title className=\"text-lg font-medium leading-6 text-gray-900\">\n                  {title}\n                </Dialog.Title>\n                <div className=\"mt-2\">\n                  {children}\n                </div>\n              </Dialog.Panel>\n            </Transition.Child>\n          </div>\n        </div>\n      </Dialog>\n    </Transition>\n  );\n}\n```\n\n### Radix UI with Tailwind\n\n```tsx\nimport * as DropdownMenu from '@radix-ui/react-dropdown-menu';\n\nfunction Dropdown() {\n  return (\n    <DropdownMenu.Root>\n      <DropdownMenu.Trigger className=\"\n        inline-flex items-center justify-center rounded-md\n        bg-white px-4 py-2 text-sm font-medium\n        border border-gray-300 hover:bg-gray-50\n        focus:outline-none focus:ring-2 focus:ring-brand-500\n      \">\n        Options\n        <ChevronDownIcon className=\"ml-2 h-4 w-4\" />\n      </DropdownMenu.Trigger>\n\n      <DropdownMenu.Portal>\n        <DropdownMenu.Content\n          className=\"\n            min-w-[200px] rounded-md bg-white p-1 shadow-lg\n            border border-gray-200\n            animate-in fade-in-0 zoom-in-95\n            data-[side=bottom]:slide-in-from-top-2\n            data-[side=top]:slide-in-from-bottom-2\n          \"\n          sideOffset={5}\n        >\n          <DropdownMenu.Item className=\"\n            relative flex cursor-pointer select-none items-center\n            rounded-sm px-2 py-2 text-sm outline-none\n            data-[highlighted]:bg-gray-100\n          \">\n            Profile\n          </DropdownMenu.Item>\n\n          <DropdownMenu.Separator className=\"my-1 h-px bg-gray-200\" />\n\n          <DropdownMenu.Item className=\"\n            relative flex cursor-pointer select-none items-center\n            rounded-sm px-2 py-2 text-sm text-red-600 outline-none\n            data-[highlighted]:bg-red-50\n          \">\n            Delete\n          </DropdownMenu.Item>\n        </DropdownMenu.Content>\n      </DropdownMenu.Portal>\n    </DropdownMenu.Root>\n  );\n}\n```\n\n## Animation Patterns\n\n### Staggered Animations\n\n```tsx\nfunction StaggeredList({ items }) {\n  return (\n    <ul className=\"space-y-2\">\n      {items.map((item, index) => (\n        <li\n          key={item.id}\n          className=\"animate-in fade-in-0 slide-in-from-left-4\"\n          style={{ animationDelay: `${index * 100}ms` }}\n        >\n          {item.content}\n        </li>\n      ))}\n    </ul>\n  );\n}\n```\n\n### Skeleton Loading\n\n```tsx\nfunction Skeleton({ className, ...props }) {\n  return (\n    <div\n      className={cn(\n        'animate-pulse rounded-md bg-gray-200',\n        className\n      )}\n      {...props}\n    />\n  );\n}\n\nfunction CardSkeleton() {\n  return (\n    <div className=\"rounded-xl border border-gray-200 p-6\">\n      <div className=\"flex items-center gap-4\">\n        <Skeleton className=\"h-12 w-12 rounded-full\" />\n        <div className=\"space-y-2\">\n          <Skeleton className=\"h-4 w-[200px]\" />\n          <Skeleton className=\"h-3 w-[150px]\" />\n        </div>\n      </div>\n      <div className=\"mt-4 space-y-2\">\n        <Skeleton className=\"h-3 w-full\" />\n        <Skeleton className=\"h-3 w-full\" />\n        <Skeleton className=\"h-3 w-3/4\" />\n      </div>\n    </div>\n  );\n}\n```\n\n## Best Practices\n\n### 1. Use cn() Utility for Class Merging\n\n```typescript\n// lib/utils.ts\nimport { clsx, type ClassValue } from 'clsx';\nimport { twMerge } from 'tailwind-merge';\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs));\n}\n```\n\n### 2. Extract Common Patterns\n\n```css\n@layer components {\n  /* Consistent focus ring */\n  .focus-ring {\n    @apply focus:outline-none focus-visible:ring-2 focus-visible:ring-brand-500 focus-visible:ring-offset-2;\n  }\n\n  /* Consistent disabled state */\n  .disabled-state {\n    @apply disabled:pointer-events-none disabled:opacity-50;\n  }\n\n  /* Truncate text */\n  .truncate-lines-2 {\n    display: -webkit-box;\n    -webkit-line-clamp: 2;\n    -webkit-box-orient: vertical;\n    overflow: hidden;\n  }\n}\n```\n\n### 3. Document Component APIs\n\n```tsx\n/**\n * Button component with multiple variants\n *\n * @example\n * <Button variant=\"primary\" size=\"lg\">Click me</Button>\n *\n * @example\n * <Button variant=\"ghost\" size=\"icon\">\n *   <IconMenu />\n * </Button>\n */\nexport function Button({ ... }) { ... }\n```\n\n### 4. Test Component Variants\n\n```tsx\n// Button.test.tsx\ndescribe('Button', () => {\n  it('renders all variants correctly', () => {\n    const variants = ['default', 'secondary', 'outline', 'ghost', 'destructive'];\n\n    variants.forEach(variant => {\n      render(<Button variant={variant}>Test</Button>);\n      // Assert classes are applied correctly\n    });\n  });\n});\n```"
              },
              {
                "name": "tailwindcss-advanced-design-systems",
                "description": "Tailwind CSS advanced design systems with design tokens and @theme configuration",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-design-systems/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-advanced-design-systems",
                  "description": "Tailwind CSS advanced design systems with design tokens and @theme configuration"
                },
                "content": "# Tailwind CSS Advanced Design Systems\n\n## Building Design Tokens with @theme\n\n### Complete Design Token System\n\n```css\n@import \"tailwindcss\";\n\n@theme {\n  /* ===== COLOR SYSTEM ===== */\n\n  /* Disable all defaults for full control */\n  --color-*: initial;\n\n  /* Semantic color tokens */\n  --color-surface-primary: oklch(1 0 0);\n  --color-surface-secondary: oklch(0.98 0.002 250);\n  --color-surface-tertiary: oklch(0.95 0.004 250);\n  --color-surface-inverse: oklch(0.15 0.02 250);\n\n  --color-text-primary: oklch(0.15 0.02 250);\n  --color-text-secondary: oklch(0.4 0.02 250);\n  --color-text-tertiary: oklch(0.55 0.015 250);\n  --color-text-inverse: oklch(0.98 0 0);\n  --color-text-disabled: oklch(0.7 0.01 250);\n\n  --color-border-default: oklch(0.85 0.01 250);\n  --color-border-subtle: oklch(0.92 0.005 250);\n  --color-border-strong: oklch(0.7 0.02 250);\n\n  /* Brand colors with full scale */\n  --color-brand-50: oklch(0.97 0.02 250);\n  --color-brand-100: oklch(0.93 0.04 250);\n  --color-brand-200: oklch(0.87 0.08 250);\n  --color-brand-300: oklch(0.78 0.12 250);\n  --color-brand-400: oklch(0.68 0.16 250);\n  --color-brand-500: oklch(0.58 0.2 250);\n  --color-brand-600: oklch(0.5 0.2 250);\n  --color-brand-700: oklch(0.42 0.18 250);\n  --color-brand-800: oklch(0.35 0.15 250);\n  --color-brand-900: oklch(0.28 0.12 250);\n  --color-brand-950: oklch(0.2 0.08 250);\n\n  /* Status colors */\n  --color-success: oklch(0.6 0.18 145);\n  --color-success-subtle: oklch(0.95 0.04 145);\n  --color-warning: oklch(0.75 0.18 85);\n  --color-warning-subtle: oklch(0.95 0.06 85);\n  --color-error: oklch(0.55 0.22 25);\n  --color-error-subtle: oklch(0.95 0.04 25);\n  --color-info: oklch(0.6 0.18 250);\n  --color-info-subtle: oklch(0.95 0.04 250);\n\n  /* ===== TYPOGRAPHY SYSTEM ===== */\n\n  --font-*: initial;\n\n  --font-display: \"Cal Sans\", \"Inter\", system-ui, sans-serif;\n  --font-body: \"Inter\", system-ui, sans-serif;\n  --font-mono: \"JetBrains Mono\", \"Fira Code\", monospace;\n\n  /* Type scale (Major Third - 1.25) */\n  --text-xs: 0.64rem;\n  --text-sm: 0.8rem;\n  --text-base: 1rem;\n  --text-lg: 1.25rem;\n  --text-xl: 1.563rem;\n  --text-2xl: 1.953rem;\n  --text-3xl: 2.441rem;\n  --text-4xl: 3.052rem;\n  --text-5xl: 3.815rem;\n\n  /* Line heights */\n  --leading-none: 1;\n  --leading-tight: 1.15;\n  --leading-snug: 1.3;\n  --leading-normal: 1.5;\n  --leading-relaxed: 1.625;\n  --leading-loose: 2;\n\n  /* Letter spacing */\n  --tracking-tighter: -0.05em;\n  --tracking-tight: -0.025em;\n  --tracking-normal: 0;\n  --tracking-wide: 0.025em;\n  --tracking-wider: 0.05em;\n  --tracking-widest: 0.1em;\n\n  /* ===== SPACING SYSTEM ===== */\n\n  --spacing-*: initial;\n\n  /* 4px base unit */\n  --spacing-0: 0;\n  --spacing-px: 1px;\n  --spacing-0_5: 0.125rem;\n  --spacing-1: 0.25rem;\n  --spacing-1_5: 0.375rem;\n  --spacing-2: 0.5rem;\n  --spacing-2_5: 0.625rem;\n  --spacing-3: 0.75rem;\n  --spacing-3_5: 0.875rem;\n  --spacing-4: 1rem;\n  --spacing-5: 1.25rem;\n  --spacing-6: 1.5rem;\n  --spacing-7: 1.75rem;\n  --spacing-8: 2rem;\n  --spacing-9: 2.25rem;\n  --spacing-10: 2.5rem;\n  --spacing-11: 2.75rem;\n  --spacing-12: 3rem;\n  --spacing-14: 3.5rem;\n  --spacing-16: 4rem;\n  --spacing-20: 5rem;\n  --spacing-24: 6rem;\n  --spacing-28: 7rem;\n  --spacing-32: 8rem;\n  --spacing-36: 9rem;\n  --spacing-40: 10rem;\n  --spacing-44: 11rem;\n  --spacing-48: 12rem;\n  --spacing-52: 13rem;\n  --spacing-56: 14rem;\n  --spacing-60: 15rem;\n  --spacing-64: 16rem;\n  --spacing-72: 18rem;\n  --spacing-80: 20rem;\n  --spacing-96: 24rem;\n\n  /* ===== EFFECTS ===== */\n\n  --shadow-*: initial;\n\n  --shadow-xs: 0 1px 2px 0 oklch(0 0 0 / 0.05);\n  --shadow-sm: 0 1px 3px 0 oklch(0 0 0 / 0.1), 0 1px 2px -1px oklch(0 0 0 / 0.1);\n  --shadow-md: 0 4px 6px -1px oklch(0 0 0 / 0.1), 0 2px 4px -2px oklch(0 0 0 / 0.1);\n  --shadow-lg: 0 10px 15px -3px oklch(0 0 0 / 0.1), 0 4px 6px -4px oklch(0 0 0 / 0.1);\n  --shadow-xl: 0 20px 25px -5px oklch(0 0 0 / 0.1), 0 8px 10px -6px oklch(0 0 0 / 0.1);\n  --shadow-2xl: 0 25px 50px -12px oklch(0 0 0 / 0.25);\n  --shadow-inner: inset 0 2px 4px 0 oklch(0 0 0 / 0.05);\n\n  /* Colored shadows */\n  --shadow-brand: 0 4px 14px 0 oklch(0.58 0.2 250 / 0.3);\n  --shadow-success: 0 4px 14px 0 oklch(0.6 0.18 145 / 0.3);\n  --shadow-error: 0 4px 14px 0 oklch(0.55 0.22 25 / 0.3);\n\n  --radius-*: initial;\n\n  --radius-none: 0;\n  --radius-sm: 0.125rem;\n  --radius-default: 0.25rem;\n  --radius-md: 0.375rem;\n  --radius-lg: 0.5rem;\n  --radius-xl: 0.75rem;\n  --radius-2xl: 1rem;\n  --radius-3xl: 1.5rem;\n  --radius-full: 9999px;\n\n  /* ===== MOTION ===== */\n\n  --ease-*: initial;\n  --animate-*: initial;\n\n  --ease-linear: linear;\n  --ease-in: cubic-bezier(0.4, 0, 1, 1);\n  --ease-out: cubic-bezier(0, 0, 0.2, 1);\n  --ease-in-out: cubic-bezier(0.4, 0, 0.2, 1);\n  --ease-bounce: cubic-bezier(0.68, -0.55, 0.265, 1.55);\n  --ease-spring: cubic-bezier(0.175, 0.885, 0.32, 1.275);\n\n  --duration-75: 75ms;\n  --duration-100: 100ms;\n  --duration-150: 150ms;\n  --duration-200: 200ms;\n  --duration-300: 300ms;\n  --duration-500: 500ms;\n  --duration-700: 700ms;\n  --duration-1000: 1000ms;\n\n  /* ===== BREAKPOINTS ===== */\n\n  --breakpoint-*: initial;\n\n  --breakpoint-xs: 475px;\n  --breakpoint-sm: 640px;\n  --breakpoint-md: 768px;\n  --breakpoint-lg: 1024px;\n  --breakpoint-xl: 1280px;\n  --breakpoint-2xl: 1536px;\n  --breakpoint-3xl: 1920px;\n\n  /* ===== Z-INDEX ===== */\n\n  --z-auto: auto;\n  --z-0: 0;\n  --z-10: 10;\n  --z-20: 20;\n  --z-30: 30;\n  --z-40: 40;\n  --z-50: 50;\n  --z-dropdown: 100;\n  --z-sticky: 200;\n  --z-fixed: 300;\n  --z-modal-backdrop: 400;\n  --z-modal: 500;\n  --z-popover: 600;\n  --z-tooltip: 700;\n  --z-toast: 800;\n}\n```\n\n## Dark Mode Design Tokens\n\n### Automatic Dark Mode with CSS Variables\n\n```css\n@import \"tailwindcss\";\n\n@theme {\n  /* Light mode tokens (default) */\n  --color-surface: oklch(1 0 0);\n  --color-surface-raised: oklch(0.98 0 0);\n  --color-text: oklch(0.15 0 0);\n  --color-text-muted: oklch(0.45 0 0);\n  --color-border: oklch(0.9 0 0);\n}\n\n/* Dark mode overrides using native CSS */\n@media (prefers-color-scheme: dark) {\n  :root {\n    --color-surface: oklch(0.12 0.02 260);\n    --color-surface-raised: oklch(0.18 0.02 260);\n    --color-text: oklch(0.95 0 0);\n    --color-text-muted: oklch(0.65 0 0);\n    --color-border: oklch(0.28 0.02 260);\n  }\n}\n\n/* Selector-based dark mode */\n@custom-variant dark (&:where(.dark, .dark *));\n\n.dark {\n  --color-surface: oklch(0.12 0.02 260);\n  --color-surface-raised: oklch(0.18 0.02 260);\n  --color-text: oklch(0.95 0 0);\n  --color-text-muted: oklch(0.65 0 0);\n  --color-border: oklch(0.28 0.02 260);\n}\n```\n\n### Usage\n\n```html\n<!-- These classes work in both light and dark automatically -->\n<div class=\"bg-[var(--color-surface)] text-[var(--color-text)]\">\n  <p class=\"text-[var(--color-text-muted)]\">Muted text</p>\n  <div class=\"border border-[var(--color-border)]\">Bordered</div>\n</div>\n\n<!-- Or create semantic utility classes -->\n```\n\n```css\n@utility bg-surface {\n  background-color: var(--color-surface);\n}\n\n@utility bg-surface-raised {\n  background-color: var(--color-surface-raised);\n}\n\n@utility text-default {\n  color: var(--color-text);\n}\n\n@utility text-muted {\n  color: var(--color-text-muted);\n}\n\n@utility border-default {\n  border-color: var(--color-border);\n}\n```\n\n## Multi-Theme Systems\n\n### Theme Switching with Data Attributes\n\n```css\n@import \"tailwindcss\";\n\n@custom-variant theme-ocean (&:where([data-theme=\"ocean\"], [data-theme=\"ocean\"] *));\n@custom-variant theme-forest (&:where([data-theme=\"forest\"], [data-theme=\"forest\"] *));\n@custom-variant theme-sunset (&:where([data-theme=\"sunset\"], [data-theme=\"sunset\"] *));\n\n@theme {\n  /* Default theme */\n  --color-primary: oklch(0.6 0.2 250);\n  --color-secondary: oklch(0.7 0.15 200);\n  --color-accent: oklch(0.75 0.18 30);\n}\n\n[data-theme=\"ocean\"] {\n  --color-primary: oklch(0.55 0.2 220);\n  --color-secondary: oklch(0.65 0.15 200);\n  --color-accent: oklch(0.7 0.18 180);\n}\n\n[data-theme=\"forest\"] {\n  --color-primary: oklch(0.5 0.18 145);\n  --color-secondary: oklch(0.6 0.12 120);\n  --color-accent: oklch(0.75 0.15 85);\n}\n\n[data-theme=\"sunset\"] {\n  --color-primary: oklch(0.6 0.22 25);\n  --color-secondary: oklch(0.7 0.2 45);\n  --color-accent: oklch(0.8 0.18 65);\n}\n```\n\n```html\n<html data-theme=\"ocean\">\n  <body>\n    <button class=\"bg-primary theme-forest:bg-primary\">\n      Uses ocean primary, unless explicitly overridden\n    </button>\n  </body>\n</html>\n```\n\n### JavaScript Theme Switcher\n\n```javascript\nconst themes = ['default', 'ocean', 'forest', 'sunset'];\n\nfunction setTheme(theme) {\n  if (theme === 'default') {\n    document.documentElement.removeAttribute('data-theme');\n  } else {\n    document.documentElement.setAttribute('data-theme', theme);\n  }\n  localStorage.setItem('theme', theme);\n}\n\nfunction initTheme() {\n  const saved = localStorage.getItem('theme') || 'default';\n  setTheme(saved);\n}\n\n// Theme picker component\nfunction ThemePicker() {\n  return `\n    <select onchange=\"setTheme(this.value)\">\n      ${themes.map(t => `<option value=\"${t}\">${t}</option>`).join('')}\n    </select>\n  `;\n}\n\ninitTheme();\n```\n\n## Component Token System\n\n### Design Tokens for Components\n\n```css\n@theme {\n  /* Button tokens */\n  --button-padding-x: var(--spacing-4);\n  --button-padding-y: var(--spacing-2);\n  --button-radius: var(--radius-lg);\n  --button-font-weight: 500;\n  --button-transition: all 150ms ease;\n\n  /* Input tokens */\n  --input-padding-x: var(--spacing-3);\n  --input-padding-y: var(--spacing-2);\n  --input-radius: var(--radius-md);\n  --input-border-width: 1px;\n  --input-focus-ring-width: 2px;\n  --input-focus-ring-offset: 2px;\n\n  /* Card tokens */\n  --card-padding: var(--spacing-6);\n  --card-radius: var(--radius-xl);\n  --card-shadow: var(--shadow-md);\n  --card-border-width: 1px;\n\n  /* Modal tokens */\n  --modal-padding: var(--spacing-6);\n  --modal-radius: var(--radius-2xl);\n  --modal-max-width: 32rem;\n  --modal-backdrop-opacity: 0.5;\n}\n```\n\n```css\n@layer components {\n  .btn {\n    padding: var(--button-padding-y) var(--button-padding-x);\n    border-radius: var(--button-radius);\n    font-weight: var(--button-font-weight);\n    transition: var(--button-transition);\n  }\n\n  .input {\n    padding: var(--input-padding-y) var(--input-padding-x);\n    border-radius: var(--input-radius);\n    border-width: var(--input-border-width);\n  }\n\n  .input:focus {\n    outline: none;\n    ring-width: var(--input-focus-ring-width);\n    ring-offset: var(--input-focus-ring-offset);\n  }\n\n  .card {\n    padding: var(--card-padding);\n    border-radius: var(--card-radius);\n    box-shadow: var(--card-shadow);\n    border-width: var(--card-border-width);\n  }\n}\n```\n\n## Responsive Design Tokens\n\n### Fluid Typography\n\n```css\n@theme {\n  /* Fluid type scale using clamp() */\n  --text-fluid-xs: clamp(0.64rem, 0.5rem + 0.5vw, 0.75rem);\n  --text-fluid-sm: clamp(0.8rem, 0.7rem + 0.5vw, 0.875rem);\n  --text-fluid-base: clamp(1rem, 0.9rem + 0.5vw, 1.125rem);\n  --text-fluid-lg: clamp(1.25rem, 1rem + 1vw, 1.5rem);\n  --text-fluid-xl: clamp(1.5rem, 1.2rem + 1.5vw, 2rem);\n  --text-fluid-2xl: clamp(2rem, 1.5rem + 2vw, 3rem);\n  --text-fluid-3xl: clamp(2.5rem, 1.8rem + 3vw, 4rem);\n  --text-fluid-4xl: clamp(3rem, 2rem + 4vw, 5rem);\n}\n\n@utility text-fluid-xs { font-size: var(--text-fluid-xs); }\n@utility text-fluid-sm { font-size: var(--text-fluid-sm); }\n@utility text-fluid-base { font-size: var(--text-fluid-base); }\n@utility text-fluid-lg { font-size: var(--text-fluid-lg); }\n@utility text-fluid-xl { font-size: var(--text-fluid-xl); }\n@utility text-fluid-2xl { font-size: var(--text-fluid-2xl); }\n@utility text-fluid-3xl { font-size: var(--text-fluid-3xl); }\n@utility text-fluid-4xl { font-size: var(--text-fluid-4xl); }\n```\n\n### Fluid Spacing\n\n```css\n@theme {\n  /* Fluid spacing using clamp() */\n  --space-fluid-xs: clamp(0.25rem, 0.2rem + 0.25vw, 0.5rem);\n  --space-fluid-sm: clamp(0.5rem, 0.4rem + 0.5vw, 1rem);\n  --space-fluid-md: clamp(1rem, 0.8rem + 1vw, 2rem);\n  --space-fluid-lg: clamp(2rem, 1.5rem + 2vw, 4rem);\n  --space-fluid-xl: clamp(4rem, 3rem + 4vw, 8rem);\n  --space-fluid-2xl: clamp(6rem, 4rem + 6vw, 12rem);\n}\n\n@utility p-fluid-xs { padding: var(--space-fluid-xs); }\n@utility p-fluid-sm { padding: var(--space-fluid-sm); }\n@utility p-fluid-md { padding: var(--space-fluid-md); }\n@utility p-fluid-lg { padding: var(--space-fluid-lg); }\n@utility p-fluid-xl { padding: var(--space-fluid-xl); }\n\n@utility gap-fluid-xs { gap: var(--space-fluid-xs); }\n@utility gap-fluid-sm { gap: var(--space-fluid-sm); }\n@utility gap-fluid-md { gap: var(--space-fluid-md); }\n@utility gap-fluid-lg { gap: var(--space-fluid-lg); }\n```\n\n## Brand Color Generation\n\n### Generating Color Scales from Brand Color\n\n```css\n@theme {\n  /* Start with your brand color */\n  --brand-hue: 250;\n  --brand-chroma: 0.2;\n\n  /* Generate full scale */\n  --color-brand-50: oklch(0.97 calc(var(--brand-chroma) * 0.1) var(--brand-hue));\n  --color-brand-100: oklch(0.93 calc(var(--brand-chroma) * 0.2) var(--brand-hue));\n  --color-brand-200: oklch(0.87 calc(var(--brand-chroma) * 0.4) var(--brand-hue));\n  --color-brand-300: oklch(0.78 calc(var(--brand-chroma) * 0.6) var(--brand-hue));\n  --color-brand-400: oklch(0.68 calc(var(--brand-chroma) * 0.8) var(--brand-hue));\n  --color-brand-500: oklch(0.58 var(--brand-chroma) var(--brand-hue));\n  --color-brand-600: oklch(0.5 var(--brand-chroma) var(--brand-hue));\n  --color-brand-700: oklch(0.42 calc(var(--brand-chroma) * 0.9) var(--brand-hue));\n  --color-brand-800: oklch(0.35 calc(var(--brand-chroma) * 0.75) var(--brand-hue));\n  --color-brand-900: oklch(0.28 calc(var(--brand-chroma) * 0.6) var(--brand-hue));\n  --color-brand-950: oklch(0.2 calc(var(--brand-chroma) * 0.4) var(--brand-hue));\n}\n```\n\n## Monorepo Design System\n\n### Shared Design Tokens Package\n\n```\npackages/\n  design-tokens/\n    tokens.css\n    package.json\n  ui-components/\n    src/\n    package.json\napps/\n  web/\n  mobile/\n```\n\n**packages/design-tokens/tokens.css:**\n```css\n/* Exportable design tokens */\n@theme {\n  /* All your design tokens here */\n  --color-brand-500: oklch(0.58 0.2 250);\n  /* ... */\n}\n```\n\n**packages/design-tokens/package.json:**\n```json\n{\n  \"name\": \"@mycompany/design-tokens\",\n  \"version\": \"1.0.0\",\n  \"exports\": {\n    \".\": \"./tokens.css\"\n  }\n}\n```\n\n**apps/web/app.css:**\n```css\n@import \"tailwindcss\";\n@import \"@mycompany/design-tokens\";\n```\n\n## Best Practices\n\n### 1. Use Semantic Naming\n\n```css\n/* Good - Semantic names */\n--color-text-primary\n--color-surface-elevated\n--color-action-primary\n\n/* Avoid - Raw values */\n--color-gray-900\n--color-white\n--color-blue-500\n```\n\n### 2. Document Token Purpose\n\n```css\n@theme {\n  /*\n   * Surface colors for backgrounds\n   * primary: Main page background\n   * raised: Cards and elevated elements\n   * sunken: Inset areas like inputs\n   */\n  --color-surface-primary: oklch(1 0 0);\n  --color-surface-raised: oklch(0.98 0 0);\n  --color-surface-sunken: oklch(0.96 0 0);\n}\n```\n\n### 3. Create Token Aliases\n\n```css\n@theme {\n  /* Base tokens */\n  --color-blue-500: oklch(0.58 0.2 250);\n\n  /* Semantic aliases */\n  --color-primary: var(--color-blue-500);\n  --color-link: var(--color-blue-500);\n  --color-focus-ring: var(--color-blue-500);\n}\n```\n\n### 4. Version Your Design System\n\nKeep design tokens in version control and document changes:\n\n```css\n/*\n * Design System v2.0.0\n * Breaking changes:\n * - Renamed --color-gray-* to --color-neutral-*\n * - Updated primary color to new brand guidelines\n */\n```"
              },
              {
                "name": "tailwindcss-advanced-layouts",
                "description": "Tailwind CSS advanced layout techniques including CSS Grid and Flexbox patterns",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-advanced-layouts/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-advanced-layouts",
                  "description": "Tailwind CSS advanced layout techniques including CSS Grid and Flexbox patterns"
                },
                "content": "# Tailwind CSS Advanced Layout Techniques\n\n## CSS Grid Mastery\n\n### Complex Grid Layouts\n\n```html\n<!-- Holy Grail Layout -->\n<div class=\"grid min-h-screen grid-rows-[auto_1fr_auto]\">\n  <header class=\"bg-white shadow\">Header</header>\n  <div class=\"grid grid-cols-[250px_1fr_300px]\">\n    <aside class=\"bg-gray-50 p-4\">Sidebar</aside>\n    <main class=\"p-6\">Main Content</main>\n    <aside class=\"bg-gray-50 p-4\">Right Sidebar</aside>\n  </div>\n  <footer class=\"bg-gray-800 text-white\">Footer</footer>\n</div>\n\n<!-- Responsive Holy Grail -->\n<div class=\"grid min-h-screen grid-rows-[auto_1fr_auto]\">\n  <header>Header</header>\n  <div class=\"grid grid-cols-1 md:grid-cols-[250px_1fr] lg:grid-cols-[250px_1fr_300px]\">\n    <aside class=\"order-2 md:order-1\">Sidebar</aside>\n    <main class=\"order-1 md:order-2\">Main</main>\n    <aside class=\"order-3 hidden lg:block\">Right</aside>\n  </div>\n  <footer>Footer</footer>\n</div>\n```\n\n### Grid Template Areas\n\n```css\n@utility grid-areas-dashboard {\n  grid-template-areas:\n    \"header header header\"\n    \"nav main aside\"\n    \"nav footer footer\";\n}\n\n@utility area-header { grid-area: header; }\n@utility area-nav { grid-area: nav; }\n@utility area-main { grid-area: main; }\n@utility area-aside { grid-area: aside; }\n@utility area-footer { grid-area: footer; }\n```\n\n```html\n<div class=\"grid grid-areas-dashboard grid-cols-[200px_1fr_250px] grid-rows-[60px_1fr_40px] min-h-screen\">\n  <header class=\"area-header bg-white shadow\">Header</header>\n  <nav class=\"area-nav bg-gray-100\">Navigation</nav>\n  <main class=\"area-main p-6\">Main Content</main>\n  <aside class=\"area-aside bg-gray-50 p-4\">Sidebar</aside>\n  <footer class=\"area-footer bg-gray-800 text-white\">Footer</footer>\n</div>\n```\n\n### Auto-Fill and Auto-Fit Grids\n\n```html\n<!-- Auto-fill: Creates as many tracks as fit, even empty ones -->\n<div class=\"grid grid-cols-[repeat(auto-fill,minmax(250px,1fr))] gap-6\">\n  <div class=\"bg-white rounded-lg shadow p-4\">Card 1</div>\n  <div class=\"bg-white rounded-lg shadow p-4\">Card 2</div>\n  <div class=\"bg-white rounded-lg shadow p-4\">Card 3</div>\n</div>\n\n<!-- Auto-fit: Collapses empty tracks -->\n<div class=\"grid grid-cols-[repeat(auto-fit,minmax(250px,1fr))] gap-6\">\n  <!-- Cards stretch to fill available space -->\n</div>\n\n<!-- With arbitrary values -->\n<div class=\"grid grid-cols-[repeat(auto-fill,minmax(min(100%,300px),1fr))] gap-4\">\n  <!-- Handles edge case where container is smaller than minmax min -->\n</div>\n```\n\n### Subgrid\n\n```css\n/* Enable subgrid in v4 */\n@utility subgrid-cols {\n  grid-template-columns: subgrid;\n}\n\n@utility subgrid-rows {\n  grid-template-rows: subgrid;\n}\n```\n\n```html\n<div class=\"grid grid-cols-4 gap-4\">\n  <!-- Span 2 columns but align children to parent grid -->\n  <div class=\"col-span-2 grid subgrid-cols gap-4\">\n    <div>Aligned to parent column 1</div>\n    <div>Aligned to parent column 2</div>\n  </div>\n</div>\n```\n\n## Advanced Flexbox Patterns\n\n### Space Distribution\n\n```html\n<!-- Equal spacing with first/last at edges -->\n<div class=\"flex justify-between\">\n  <div>First</div>\n  <div>Second</div>\n  <div>Third</div>\n</div>\n\n<!-- Equal spacing everywhere including edges -->\n<div class=\"flex justify-around\">\n  <div>Item</div>\n  <div>Item</div>\n  <div>Item</div>\n</div>\n\n<!-- Double space between items vs edges -->\n<div class=\"flex justify-evenly\">\n  <div>Item</div>\n  <div>Item</div>\n  <div>Item</div>\n</div>\n```\n\n### Flexible Item Sizing\n\n```html\n<!-- Items share space equally -->\n<div class=\"flex\">\n  <div class=\"flex-1\">1/3</div>\n  <div class=\"flex-1\">1/3</div>\n  <div class=\"flex-1\">1/3</div>\n</div>\n\n<!-- First item takes 2x space -->\n<div class=\"flex\">\n  <div class=\"flex-[2]\">2/4</div>\n  <div class=\"flex-1\">1/4</div>\n  <div class=\"flex-1\">1/4</div>\n</div>\n\n<!-- Fixed + flexible -->\n<div class=\"flex\">\n  <div class=\"w-64 shrink-0\">Fixed 256px</div>\n  <div class=\"flex-1 min-w-0\">Flexible (can shrink)</div>\n</div>\n\n<!-- Prevent shrinking with text overflow -->\n<div class=\"flex min-w-0\">\n  <div class=\"shrink-0\">Icon</div>\n  <div class=\"min-w-0 truncate\">Very long text that should truncate</div>\n</div>\n```\n\n### Masonry-Like with Flexbox\n\n```html\n<!-- Column-based masonry -->\n<div class=\"flex flex-col flex-wrap h-[800px] gap-4\">\n  <div class=\"w-[calc(33.333%-1rem)] h-48\">Item 1</div>\n  <div class=\"w-[calc(33.333%-1rem)] h-64\">Item 2</div>\n  <div class=\"w-[calc(33.333%-1rem)] h-32\">Item 3</div>\n  <!-- Items flow vertically then wrap to next column -->\n</div>\n```\n\n## Container Queries\n\n### Basic Container Queries\n\n```css\n@plugin \"@tailwindcss/container-queries\";\n```\n\n```html\n<!-- Define container -->\n<div class=\"@container\">\n  <!-- Respond to container width -->\n  <div class=\"flex flex-col @md:flex-row @lg:grid @lg:grid-cols-3 gap-4\">\n    <div>Item 1</div>\n    <div>Item 2</div>\n    <div>Item 3</div>\n  </div>\n</div>\n```\n\n### Named Containers\n\n```html\n<!-- Multiple named containers -->\n<div class=\"@container/sidebar\">\n  <nav class=\"@[200px]/sidebar:flex-col @[300px]/sidebar:flex-row\">\n    Navigation\n  </nav>\n</div>\n\n<div class=\"@container/main\">\n  <article class=\"@[600px]/main:prose-lg @[900px]/main:prose-xl\">\n    Content\n  </article>\n</div>\n```\n\n### Container Query Units\n\n```html\n<!-- Size relative to container -->\n<div class=\"@container\">\n  <h1 class=\"text-[5cqw]\">Scales with container width</h1>\n  <p class=\"text-[3cqi]\">Scales with container inline size</p>\n</div>\n```\n\n## Position and Layering\n\n### Sticky Positioning\n\n```html\n<!-- Sticky header -->\n<header class=\"sticky top-0 z-50 bg-white/80 backdrop-blur-sm border-b\">\n  Navigation\n</header>\n\n<!-- Sticky sidebar -->\n<aside class=\"sticky top-20 h-[calc(100vh-5rem)] overflow-auto\">\n  Sidebar content\n</aside>\n\n<!-- Sticky table header -->\n<div class=\"overflow-auto max-h-96\">\n  <table>\n    <thead class=\"sticky top-0 bg-white shadow\">\n      <tr>\n        <th class=\"sticky left-0 bg-white z-10\">Corner cell</th>\n        <th>Column 2</th>\n      </tr>\n    </thead>\n    <tbody>...</tbody>\n  </table>\n</div>\n```\n\n### Fixed Elements\n\n```html\n<!-- Fixed bottom navigation (mobile) -->\n<nav class=\"fixed bottom-0 inset-x-0 z-50 bg-white border-t md:hidden\">\n  <div class=\"flex justify-around py-2\">\n    <a href=\"#\">Home</a>\n    <a href=\"#\">Search</a>\n    <a href=\"#\">Profile</a>\n  </div>\n</nav>\n\n<!-- Fixed action button -->\n<button class=\"fixed bottom-6 right-6 z-40 rounded-full bg-brand-500 p-4 shadow-lg\">\n  <PlusIcon />\n</button>\n```\n\n### Z-Index Management\n\n```css\n@theme {\n  --z-dropdown: 100;\n  --z-sticky: 200;\n  --z-fixed: 300;\n  --z-modal-backdrop: 400;\n  --z-modal: 500;\n  --z-popover: 600;\n  --z-tooltip: 700;\n  --z-toast: 800;\n}\n\n@utility z-dropdown { z-index: var(--z-dropdown); }\n@utility z-sticky { z-index: var(--z-sticky); }\n@utility z-fixed { z-index: var(--z-fixed); }\n@utility z-modal-backdrop { z-index: var(--z-modal-backdrop); }\n@utility z-modal { z-index: var(--z-modal); }\n@utility z-popover { z-index: var(--z-popover); }\n@utility z-tooltip { z-index: var(--z-tooltip); }\n@utility z-toast { z-index: var(--z-toast); }\n```\n\n## Overflow and Scrolling\n\n### Custom Scrollbars\n\n```css\n@utility scrollbar-thin {\n  scrollbar-width: thin;\n}\n\n@utility scrollbar-none {\n  scrollbar-width: none;\n  -ms-overflow-style: none;\n}\n\n@utility scrollbar-none::-webkit-scrollbar {\n  display: none;\n}\n\n/* Custom scrollbar styling */\n@utility scrollbar-custom {\n  scrollbar-color: oklch(0.7 0 0) oklch(0.95 0 0);\n}\n\n@utility scrollbar-custom::-webkit-scrollbar {\n  width: 8px;\n  height: 8px;\n}\n\n@utility scrollbar-custom::-webkit-scrollbar-track {\n  background: oklch(0.95 0 0);\n  border-radius: 4px;\n}\n\n@utility scrollbar-custom::-webkit-scrollbar-thumb {\n  background: oklch(0.7 0 0);\n  border-radius: 4px;\n}\n\n@utility scrollbar-custom::-webkit-scrollbar-thumb:hover {\n  background: oklch(0.5 0 0);\n}\n```\n\n### Scroll Snap\n\n```html\n<!-- Horizontal carousel -->\n<div class=\"flex snap-x snap-mandatory overflow-x-auto gap-4 pb-4\">\n  <div class=\"snap-start shrink-0 w-80\">Card 1</div>\n  <div class=\"snap-start shrink-0 w-80\">Card 2</div>\n  <div class=\"snap-start shrink-0 w-80\">Card 3</div>\n</div>\n\n<!-- Full-page sections -->\n<div class=\"h-screen snap-y snap-mandatory overflow-y-auto\">\n  <section class=\"h-screen snap-start\">Section 1</section>\n  <section class=\"h-screen snap-start\">Section 2</section>\n  <section class=\"h-screen snap-start\">Section 3</section>\n</div>\n\n<!-- Snap with padding -->\n<div class=\"snap-x scroll-pl-6 overflow-x-auto\">\n  <div class=\"snap-start\">...</div>\n</div>\n```\n\n### Scroll Margin for Anchors\n\n```html\n<!-- Offset for fixed header -->\n<section id=\"about\" class=\"scroll-mt-20\">\n  <!-- Content appears below fixed header when linked -->\n</section>\n```\n\n## Aspect Ratio and Object Fit\n\n### Responsive Aspect Ratios\n\n```html\n<!-- Fixed aspect ratio container -->\n<div class=\"aspect-video bg-gray-100\">\n  <video class=\"h-full w-full object-cover\">...</video>\n</div>\n\n<div class=\"aspect-square rounded-full overflow-hidden\">\n  <img src=\"avatar.jpg\" class=\"h-full w-full object-cover\" />\n</div>\n\n<!-- Custom aspect ratio -->\n<div class=\"aspect-[4/3]\">4:3 content</div>\n<div class=\"aspect-[21/9]\">Ultra-wide content</div>\n```\n\n### Object Positioning\n\n```html\n<!-- Focus on specific part of image -->\n<div class=\"h-64 overflow-hidden\">\n  <img\n    src=\"portrait.jpg\"\n    class=\"h-full w-full object-cover object-top\"\n  />\n</div>\n\n<!-- Arbitrary object position -->\n<img class=\"object-cover object-[25%_75%]\" src=\"...\" />\n```\n\n## Advanced Spacing\n\n### Logical Properties\n\n```html\n<!-- Works for LTR and RTL -->\n<div class=\"ps-4 pe-6 ms-auto\">\n  Padding and margin that respect text direction\n</div>\n\n<!-- Block direction (vertical in horizontal writing modes) -->\n<div class=\"pbs-4 pbe-6 mbs-auto\">\n  Block-direction spacing\n</div>\n```\n\n### Space Between with Dividers\n\n```html\n<!-- Dividers between items -->\n<ul class=\"divide-y divide-gray-200\">\n  <li class=\"py-4\">Item 1</li>\n  <li class=\"py-4\">Item 2</li>\n  <li class=\"py-4\">Item 3</li>\n</ul>\n\n<!-- Horizontal dividers -->\n<div class=\"flex divide-x divide-gray-200\">\n  <div class=\"px-4\">Section 1</div>\n  <div class=\"px-4\">Section 2</div>\n  <div class=\"px-4\">Section 3</div>\n</div>\n```\n\n### Negative Margins for Bleeds\n\n```html\n<!-- Full-bleed image in padded container -->\n<article class=\"px-6\">\n  <p>Padded content</p>\n  <img src=\"hero.jpg\" class=\"-mx-6 w-[calc(100%+3rem)]\" />\n  <p>More padded content</p>\n</article>\n\n<!-- Pull quote that breaks out -->\n<div class=\"max-w-prose mx-auto px-4\">\n  <p>Normal content...</p>\n  <blockquote class=\"-mx-8 md:-mx-16 px-8 md:px-16 py-8 bg-gray-100\">\n    Featured quote that extends beyond content width\n  </blockquote>\n</div>\n```\n\n## Multi-Column Layout\n\n### Text Columns\n\n```html\n<!-- Responsive columns -->\n<div class=\"columns-1 sm:columns-2 lg:columns-3 gap-8\">\n  <p>Content flows across columns...</p>\n</div>\n\n<!-- Fixed-width columns -->\n<div class=\"columns-[300px] gap-6\">\n  <p>Creates as many 300px columns as fit</p>\n</div>\n\n<!-- Prevent breaks inside elements -->\n<div class=\"columns-2\">\n  <div class=\"break-inside-avoid mb-4\">\n    Card that stays together\n  </div>\n</div>\n```\n\n## Responsive Patterns\n\n### Container Queries + Media Queries\n\n```html\n<div class=\"@container\">\n  <div class=\"\n    /* Container query for component-level responsiveness */\n    @md:flex @md:gap-4\n\n    /* Media query for page-level responsiveness */\n    lg:grid lg:grid-cols-2\n  \">\n    Content\n  </div>\n</div>\n```\n\n### Breakpoint-Based Visibility\n\n```html\n<!-- Show different content per breakpoint -->\n<nav>\n  <!-- Mobile menu button -->\n  <button class=\"md:hidden\">Menu</button>\n\n  <!-- Desktop navigation -->\n  <ul class=\"hidden md:flex gap-4\">\n    <li>Home</li>\n    <li>About</li>\n    <li>Contact</li>\n  </ul>\n</nav>\n```\n\n### Fluid Sizing with Clamp\n\n```html\n<!-- Fluid padding -->\n<section class=\"py-[clamp(2rem,5vw,6rem)] px-[clamp(1rem,3vw,4rem)]\">\n  Content with responsive padding\n</section>\n\n<!-- Fluid max-width -->\n<div class=\"mx-auto w-full max-w-[clamp(300px,90vw,1200px)]\">\n  Responsive container\n</div>\n```\n\n## Print Styles\n\n```html\n<!-- Hide elements when printing -->\n<nav class=\"print:hidden\">Navigation</nav>\n\n<!-- Show only when printing -->\n<div class=\"hidden print:block\">Print-only content</div>\n\n<!-- Print-specific styles -->\n<article class=\"print:text-black print:bg-white\">\n  <h1 class=\"text-2xl print:text-xl\">Heading</h1>\n  <a href=\"...\" class=\"text-blue-500 print:text-black print:underline\">\n    Link (shows as text when printed)\n  </a>\n</article>\n\n<!-- Prevent page breaks -->\n<div class=\"print:break-inside-avoid\">\n  Keep this content together on one page\n</div>\n\n<!-- Force page break -->\n<div class=\"print:break-before-page\">\n  Start on new page\n</div>\n```\n\n## Best Practices\n\n### 1. Use Modern Layout Methods\n\n```html\n<!-- Prefer Grid for 2D layouts -->\n<div class=\"grid grid-cols-3 gap-4\">\n\n<!-- Prefer Flexbox for 1D layouts -->\n<div class=\"flex items-center gap-2\">\n```\n\n### 2. Handle Edge Cases\n\n```html\n<!-- Prevent flex item from overflowing -->\n<div class=\"flex min-w-0\">\n  <div class=\"min-w-0 truncate\">Long text</div>\n</div>\n\n<!-- Prevent grid blowout -->\n<div class=\"grid grid-cols-1 min-w-0\">\n  <div class=\"overflow-hidden\">Content that might overflow</div>\n</div>\n```\n\n### 3. Use Semantic Sizing\n\n```html\n<!-- Prefer max-w-prose for reading content -->\n<article class=\"max-w-prose mx-auto\">\n\n<!-- Use container for page sections -->\n<div class=\"container mx-auto px-4\">\n```\n\n### 4. Test All Breakpoints\n\nCreate systematic tests for all responsive layouts to ensure they work at every breakpoint."
              },
              {
                "name": "tailwindcss-animations",
                "description": "Tailwind CSS animations and transitions including built-in utilities and custom animation patterns",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-animations/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-animations",
                  "description": "Tailwind CSS animations and transitions including built-in utilities and custom animation patterns"
                },
                "content": "# Tailwind CSS Animations & Transitions\n\n## Built-in Animations\n\n### Spin\n\nContinuous rotation for loading indicators:\n\n```html\n<svg class=\"animate-spin h-5 w-5 text-blue-500\" viewBox=\"0 0 24 24\">\n  <circle class=\"opacity-25\" cx=\"12\" cy=\"12\" r=\"10\" stroke=\"currentColor\" stroke-width=\"4\" />\n  <path class=\"opacity-75\" fill=\"currentColor\" d=\"M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z\" />\n</svg>\n```\n\n### Ping\n\nRadar-style pulse for notifications:\n\n```html\n<span class=\"relative flex h-3 w-3\">\n  <span class=\"animate-ping absolute inline-flex h-full w-full rounded-full bg-sky-400 opacity-75\"></span>\n  <span class=\"relative inline-flex rounded-full h-3 w-3 bg-sky-500\"></span>\n</span>\n```\n\n### Pulse\n\nSubtle fade for skeleton loaders:\n\n```html\n<div class=\"animate-pulse flex space-x-4\">\n  <div class=\"rounded-full bg-slate-200 h-10 w-10\"></div>\n  <div class=\"flex-1 space-y-2 py-1\">\n    <div class=\"h-2 bg-slate-200 rounded\"></div>\n    <div class=\"h-2 bg-slate-200 rounded w-5/6\"></div>\n  </div>\n</div>\n```\n\n### Bounce\n\nAttention-grabbing vertical bounce:\n\n```html\n<svg class=\"animate-bounce w-6 h-6 text-blue-500\" fill=\"none\" viewBox=\"0 0 24 24\">\n  <path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M19 14l-7 7m0 0l-7-7m7 7V3\" />\n</svg>\n```\n\n## Transitions\n\n### Transition Properties\n\n| Class | CSS Property |\n|-------|--------------|\n| `transition-none` | None |\n| `transition-all` | All properties |\n| `transition` | Common properties |\n| `transition-colors` | Colors only |\n| `transition-opacity` | Opacity only |\n| `transition-shadow` | Shadow only |\n| `transition-transform` | Transform only |\n\n### Transition Duration\n\n| Class | Duration |\n|-------|----------|\n| `duration-75` | 75ms |\n| `duration-100` | 100ms |\n| `duration-150` | 150ms |\n| `duration-200` | 200ms |\n| `duration-300` | 300ms |\n| `duration-500` | 500ms |\n| `duration-700` | 700ms |\n| `duration-1000` | 1000ms |\n\n### Transition Timing Functions\n\n| Class | Easing |\n|-------|--------|\n| `ease-linear` | Linear |\n| `ease-in` | Accelerate |\n| `ease-out` | Decelerate |\n| `ease-in-out` | Accelerate then decelerate |\n\n### Transition Delay\n\n| Class | Delay |\n|-------|-------|\n| `delay-75` | 75ms |\n| `delay-100` | 100ms |\n| `delay-150` | 150ms |\n| `delay-200` | 200ms |\n| `delay-300` | 300ms |\n| `delay-500` | 500ms |\n\n### Basic Transition Examples\n\n```html\n<!-- Color transition -->\n<button class=\"bg-blue-500 hover:bg-blue-700 transition-colors duration-200\">\n  Hover me\n</button>\n\n<!-- Scale on hover -->\n<div class=\"transform hover:scale-105 transition-transform duration-200\">\n  Card\n</div>\n\n<!-- Opacity transition -->\n<div class=\"opacity-100 hover:opacity-75 transition-opacity duration-150\">\n  Fade\n</div>\n\n<!-- Multiple properties -->\n<button class=\"\n  bg-blue-500 hover:bg-blue-700\n  transform hover:scale-105\n  shadow-md hover:shadow-lg\n  transition-all duration-200\n\">\n  Full transition\n</button>\n```\n\n## Transform Utilities\n\n### Scale\n\n```html\n<div class=\"transform scale-100 hover:scale-110 transition-transform\">\n  Scale up\n</div>\n\n<div class=\"transform scale-100 hover:scale-90 transition-transform\">\n  Scale down\n</div>\n\n<!-- Specific axes -->\n<div class=\"transform hover:scale-x-110\">Horizontal</div>\n<div class=\"transform hover:scale-y-110\">Vertical</div>\n```\n\n### Rotate\n\n```html\n<div class=\"transform hover:rotate-12 transition-transform\">\n  Rotate right\n</div>\n\n<div class=\"transform hover:-rotate-12 transition-transform\">\n  Rotate left\n</div>\n\n<div class=\"transform hover:rotate-180 transition-transform duration-500\">\n  Flip\n</div>\n```\n\n### Translate\n\n```html\n<div class=\"transform hover:translate-x-2 transition-transform\">\n  Move right\n</div>\n\n<div class=\"transform hover:-translate-y-2 transition-transform\">\n  Move up\n</div>\n```\n\n### Skew\n\n```html\n<div class=\"transform hover:skew-x-3 transition-transform\">\n  Skew horizontal\n</div>\n\n<div class=\"transform hover:skew-y-3 transition-transform\">\n  Skew vertical\n</div>\n```\n\n### Transform Origin\n\n```html\n<div class=\"origin-center transform hover:rotate-45\">Center (default)</div>\n<div class=\"origin-top-left transform hover:rotate-45\">Top left</div>\n<div class=\"origin-bottom-right transform hover:rotate-45\">Bottom right</div>\n```\n\n## Custom Animations (v4)\n\n### Define in @theme\n\n```css\n@theme {\n  /* Custom keyframes */\n  --animate-fade-in: fade-in 0.5s ease-out;\n  --animate-slide-up: slide-up 0.3s ease-out;\n  --animate-shake: shake 0.5s ease-in-out;\n}\n\n@keyframes fade-in {\n  from { opacity: 0; }\n  to { opacity: 1; }\n}\n\n@keyframes slide-up {\n  from {\n    opacity: 0;\n    transform: translateY(20px);\n  }\n  to {\n    opacity: 1;\n    transform: translateY(0);\n  }\n}\n\n@keyframes shake {\n  0%, 100% { transform: translateX(0); }\n  25% { transform: translateX(-5px); }\n  75% { transform: translateX(5px); }\n}\n```\n\n### Usage\n\n```html\n<div class=\"animate-fade-in\">Fades in</div>\n<div class=\"animate-slide-up\">Slides up</div>\n<div class=\"animate-shake\">Shakes on error</div>\n```\n\n### Custom Easing\n\n```css\n@theme {\n  --ease-in-expo: cubic-bezier(0.95, 0.05, 0.795, 0.035);\n  --ease-out-expo: cubic-bezier(0.19, 1, 0.22, 1);\n  --ease-bounce: cubic-bezier(0.68, -0.55, 0.265, 1.55);\n}\n```\n\n```html\n<div class=\"transition-transform ease-bounce duration-300 hover:scale-110\">\n  Bouncy scale\n</div>\n```\n\n## Accessibility: Reduced Motion\n\n### motion-safe / motion-reduce\n\n```html\n<!-- Only animate if user prefers motion -->\n<div class=\"motion-safe:animate-bounce motion-reduce:animate-none\">\n  Respects preferences\n</div>\n\n<!-- Alternative: reduced animation -->\n<button class=\"\n  motion-safe:transition-all motion-safe:duration-200\n  motion-reduce:transition-none\n  hover:bg-blue-600\n\">\n  Accessible button\n</button>\n```\n\n### Reduced Motion Pattern\n\n```css\n/* Apply globally */\n@media (prefers-reduced-motion: reduce) {\n  *,\n  ::before,\n  ::after {\n    animation-duration: 0.01ms !important;\n    animation-iteration-count: 1 !important;\n    transition-duration: 0.01ms !important;\n  }\n}\n```\n\n## Common Animation Patterns\n\n### Hover Card Lift\n\n```html\n<div class=\"\n  transform transition-all duration-200\n  hover:-translate-y-1 hover:shadow-lg\n  bg-white rounded-lg p-6 shadow\n\">\n  Card content\n</div>\n```\n\n### Button Press Effect\n\n```html\n<button class=\"\n  transform transition-transform duration-100\n  active:scale-95\n  bg-blue-500 text-white px-4 py-2 rounded\n\">\n  Click me\n</button>\n```\n\n### Loading Dots\n\n```html\n<div class=\"flex space-x-1\">\n  <div class=\"w-2 h-2 bg-blue-500 rounded-full animate-bounce [animation-delay:-0.3s]\"></div>\n  <div class=\"w-2 h-2 bg-blue-500 rounded-full animate-bounce [animation-delay:-0.15s]\"></div>\n  <div class=\"w-2 h-2 bg-blue-500 rounded-full animate-bounce\"></div>\n</div>\n```\n\n### Fade In on Scroll (with JS)\n\n```html\n<div class=\"opacity-0 translate-y-4 transition-all duration-500\" data-animate>\n  Content that fades in\n</div>\n```\n\n```javascript\n// Intersection Observer to trigger animation\nconst observer = new IntersectionObserver((entries) => {\n  entries.forEach(entry => {\n    if (entry.isIntersecting) {\n      entry.target.classList.remove('opacity-0', 'translate-y-4')\n    }\n  })\n})\n\ndocument.querySelectorAll('[data-animate]').forEach(el => observer.observe(el))\n```\n\n### Skeleton Loader\n\n```html\n<div class=\"animate-pulse\">\n  <div class=\"h-4 bg-gray-200 rounded w-3/4 mb-4\"></div>\n  <div class=\"h-4 bg-gray-200 rounded w-1/2 mb-4\"></div>\n  <div class=\"h-4 bg-gray-200 rounded w-5/6\"></div>\n</div>\n```\n\n### Hamburger to X Animation\n\n```html\n<button class=\"group\" aria-label=\"Toggle menu\">\n  <span class=\"block w-6 h-0.5 bg-black transition-all duration-200 group-open:rotate-45 group-open:translate-y-1.5\"></span>\n  <span class=\"block w-6 h-0.5 bg-black mt-1 transition-opacity duration-200 group-open:opacity-0\"></span>\n  <span class=\"block w-6 h-0.5 bg-black mt-1 transition-all duration-200 group-open:-rotate-45 group-open:-translate-y-1.5\"></span>\n</button>\n```\n\n## Transition on Enter/Leave (with JS)\n\nFor complex enter/leave transitions, use a library like Headless UI:\n\n```jsx\nimport { Transition } from '@headlessui/react'\n\nfunction Modal({ isOpen, children }) {\n  return (\n    <Transition\n      show={isOpen}\n      enter=\"transition-opacity duration-300\"\n      enterFrom=\"opacity-0\"\n      enterTo=\"opacity-100\"\n      leave=\"transition-opacity duration-200\"\n      leaveFrom=\"opacity-100\"\n      leaveTo=\"opacity-0\"\n    >\n      {children}\n    </Transition>\n  )\n}\n```\n\n## Performance Tips\n\n### 1. Prefer GPU-Accelerated Properties\n\n```html\n<!-- GOOD - GPU accelerated -->\n<div class=\"transform hover:translate-x-2 transition-transform\">\n\n<!-- AVOID - May cause repaints -->\n<div class=\"hover:left-2 transition-all\">\n```\n\n### 2. Use Specific Transitions\n\n```html\n<!-- GOOD - Only transitions what changes -->\n<div class=\"transition-colors duration-200 hover:bg-blue-500\">\n\n<!-- AVOID - Transitions everything -->\n<div class=\"transition-all duration-200 hover:bg-blue-500\">\n```\n\n### 3. Keep Animations Short\n\n```html\n<!-- GOOD - Snappy feedback -->\n<button class=\"transition-colors duration-150\">\n\n<!-- AVOID - Too slow for UI feedback -->\n<button class=\"transition-colors duration-1000\">\n```\n\n### 4. Use will-change Sparingly\n\n```html\n<!-- Only for complex, frequently animated elements -->\n<div class=\"will-change-transform animate-spin\">\n  Loading spinner\n</div>\n```"
              },
              {
                "name": "tailwindcss-debugging",
                "description": "Tailwind CSS debugging and troubleshooting guide for common issues and solutions",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-debugging/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-debugging",
                  "description": "Tailwind CSS debugging and troubleshooting guide for common issues and solutions"
                },
                "content": "# Tailwind CSS Debugging & Troubleshooting\n\n## Common Issues & Solutions\n\n### 1. Styles Not Applying\n\n#### Check Content Detection\n\nv4 automatically detects content, but if styles are missing:\n\n```css\n/* Explicitly specify sources */\n@import \"tailwindcss\";\n@source \"./src/**/*.{html,js,jsx,ts,tsx,vue,svelte}\";\n```\n\n#### Verify Class Names\n\n```html\n<!-- WRONG - Dynamic class won't be detected -->\n<div class={`text-${color}-500`}>\n\n<!-- CORRECT - Use complete class names -->\n<div class={color === 'blue' ? 'text-blue-500' : 'text-red-500'}>\n```\n\n#### Check Build Process\n\n```bash\n# Restart dev server\nnpm run dev\n\n# Clear cache and rebuild\nrm -rf node_modules/.vite\nnpm run build\n```\n\n### 2. v4 Migration Issues\n\n#### PostCSS Plugin Changed\n\n```javascript\n// OLD (v3)\nexport default {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {}\n  }\n}\n\n// NEW (v4)\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {}\n  }\n}\n```\n\n#### Configuration Moved to CSS\n\n```css\n/* v4 - Configure in CSS, not JS */\n@import \"tailwindcss\";\n\n@theme {\n  --color-primary: oklch(0.6 0.2 250);\n}\n```\n\n#### Dark Mode Variant\n\n```css\n/* v4 - Add if using selector strategy */\n@custom-variant dark (&:where(.dark, .dark *));\n```\n\n### 3. Classes Being Overridden\n\n#### Check Specificity\n\n```css\n/* Browser DevTools: Inspect element  Styles panel */\n/* Look for crossed-out styles */\n```\n\n#### Solutions\n\n```html\n<!-- Use !important (last resort) -->\n<div class=\"!mt-0\">\n\n<!-- Or increase specificity with variants -->\n<div class=\"[&]:mt-0\">\n```\n\n#### Check Import Order\n\n```css\n/* Your custom CSS should come after Tailwind */\n@import \"tailwindcss\";\n@import \"./custom.css\";  /* After Tailwind */\n```\n\n### 4. Typography Plugin Issues\n\n#### Styles Not Applied\n\n```css\n/* Ensure plugin is loaded */\n@plugin \"@tailwindcss/typography\";\n```\n\n#### Utilities Overridden by Prose\n\n```html\n<!-- Use element modifiers -->\n<article class=\"prose prose-h1:text-4xl prose-a:text-blue-600\">\n\n<!-- Or escape prose entirely -->\n<article class=\"prose\">\n  <div class=\"not-prose\">\n    <CustomComponent />\n  </div>\n</article>\n```\n\n### 5. Forms Plugin Issues\n\n#### Styles Not Applied to Plain Inputs\n\n```html\n<!-- Forms plugin only styles inputs with type attribute -->\n<input type=\"text\" />  <!--  Styled -->\n<input />              <!--  Not styled -->\n```\n\n#### Using Class Strategy\n\n```css\n@plugin \"@tailwindcss/forms\" {\n  strategy: class;\n}\n```\n\n```html\n<!-- Now explicitly opt-in -->\n<input type=\"text\" class=\"form-input\" />\n```\n\n## Debugging Tools\n\n### VS Code Extension\n\n```bash\n# Install Tailwind CSS IntelliSense\ncode --install-extension bradlc.vscode-tailwindcss\n```\n\nFeatures:\n- Autocomplete for class names\n- Hover previews showing CSS\n- Linting for errors\n- Color decorators\n\n### Debug Screens Plugin\n\n```bash\nnpm install -D @tailwindcss/debug-screens\n```\n\n```css\n@plugin \"@tailwindcss/debug-screens\";\n```\n\n```html\n<!-- Shows current breakpoint in corner -->\n<body class=\"debug-screens\">\n```\n\n### Browser DevTools\n\n1. **Inspect Element**  See computed styles\n2. **Styles Panel**  See which rules apply\n3. **Filter**  Search for Tailwind classes\n4. **Computed Tab**  See final computed values\n\n### Check Generated CSS\n\n```bash\n# Output CSS to file for inspection\nnpx tailwindcss -o output.css --content './src/**/*.{html,js}'\n\n# With verbose logging\nDEBUG=tailwindcss:* npm run build\n```\n\n## v4 Specific Debugging\n\n### Check Plugin Loading\n\n```bash\n# Look for plugin-related errors\nnpm run build 2>&1 | grep -i plugin\n```\n\n### Verify CSS Variable Output\n\n```css\n/* In browser DevTools, check :root for variables */\n:root {\n  --color-blue-500: oklch(...);\n  --spacing-4: 1rem;\n}\n```\n\n### Content Detection Issues\n\n```css\n/* Add explicit sources if auto-detection fails */\n@source \"./src/**/*.tsx\";\n@source \"./components/**/*.tsx\";\n\n/* Exclude paths */\n@source not \"./src/generated/**\";\n```\n\n## Common Error Messages\n\n### \"Cannot find module '@tailwindcss/postcss'\"\n\n```bash\nnpm install -D @tailwindcss/postcss\n```\n\n### \"Unknown at-rule @theme\"\n\nUsing v3 tooling with v4 syntax. Update your build setup:\n\n```bash\nnpm install -D tailwindcss@latest @tailwindcss/postcss@latest\n```\n\n### \"Class 'X' doesn't exist\"\n\nDynamic class generation issue:\n\n```javascript\n// BAD\nconst classes = `bg-${dynamic}-500`\n\n// GOOD\nconst colorMap = {\n  primary: 'bg-blue-500',\n  danger: 'bg-red-500'\n}\nconst classes = colorMap[dynamic]\n```\n\n### \"Styles not updating in development\"\n\n```bash\n# Restart dev server\nnpm run dev\n\n# Clear Vite cache\nrm -rf node_modules/.vite\n\n# Clear Next.js cache\nrm -rf .next\n```\n\n## Performance Debugging\n\n### Large CSS Output\n\n```bash\n# Check CSS file size\nls -lh dist/assets/*.css\n\n# If too large, check for:\n# 1. Dynamic class generation\n# 2. Unnecessary safelisting\n# 3. Unused plugins\n```\n\n### Slow Builds\n\n```bash\n# Time the build\ntime npm run build\n\n# v4 should be very fast\n# Full build: <1s\n# Incremental: microseconds\n```\n\n## Debugging Checklist\n\n### Initial Setup\n\n- [ ] Correct import: `@import \"tailwindcss\";`\n- [ ] PostCSS plugin: `@tailwindcss/postcss` (not `tailwindcss`)\n- [ ] Vite plugin: `@tailwindcss/vite` (if using Vite)\n- [ ] CSS file imported in entry point\n- [ ] Development server restarted after changes\n\n### Styles Not Applying\n\n- [ ] Class name is complete (no dynamic generation)\n- [ ] File is in content path\n- [ ] Browser cache cleared\n- [ ] No CSS specificity conflicts\n- [ ] Check DevTools for overridden styles\n\n### After Migration\n\n- [ ] tailwind.config.js removed or converted\n- [ ] @theme directive used for customization\n- [ ] PostCSS config updated\n- [ ] Dark mode variant added if using selector strategy\n- [ ] Plugins updated to v4-compatible versions\n\n### Production Issues\n\n- [ ] NODE_ENV=production\n- [ ] Build output includes styles\n- [ ] CSS file linked correctly\n- [ ] No dynamic class generation issues\n\n## Getting Help\n\n### Create Minimal Reproduction\n\n```bash\n# Create fresh project\nnpm create vite@latest repro -- --template react-ts\ncd repro\nnpm install -D tailwindcss @tailwindcss/vite\n\n# Add minimal code that shows the issue\n# Share on GitHub Issues or Discord\n```\n\n### Resources\n\n- [Official Docs](https://tailwindcss.com/docs)\n- [GitHub Issues](https://github.com/tailwindlabs/tailwindcss/issues)\n- [Tailwind CSS Discord](https://tailwindcss.com/discord)\n- [Stack Overflow](https://stackoverflow.com/questions/tagged/tailwind-css)"
              },
              {
                "name": "tailwindcss-framework-integration",
                "description": "Tailwind CSS integration guide for React, Vue, Next.js, and other frameworks",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-framework-integration/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-framework-integration",
                  "description": "Tailwind CSS integration guide for React, Vue, Next.js, and other frameworks"
                },
                "content": "# Tailwind CSS Framework Integration\n\n## React with Vite\n\n### Setup\n\n```bash\n# Create React + Vite project\nnpm create vite@latest my-app -- --template react-ts\ncd my-app\n\n# Install Tailwind CSS\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n### Configuration\n\n```javascript\n// vite.config.ts\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\nimport tailwindcss from '@tailwindcss/vite'\n\nexport default defineConfig({\n  plugins: [react(), tailwindcss()]\n})\n```\n\n```css\n/* src/index.css */\n@import \"tailwindcss\";\n```\n\n```tsx\n// src/main.tsx\nimport React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport App from './App'\nimport './index.css'\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>\n)\n```\n\n### Component Example\n\n```tsx\n// src/components/Button.tsx\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary' | 'outline'\n  size?: 'sm' | 'md' | 'lg'\n  children: React.ReactNode\n  onClick?: () => void\n}\n\nconst variants = {\n  primary: 'bg-blue-600 text-white hover:bg-blue-700',\n  secondary: 'bg-gray-200 text-gray-900 hover:bg-gray-300',\n  outline: 'border-2 border-blue-600 text-blue-600 hover:bg-blue-50'\n}\n\nconst sizes = {\n  sm: 'px-3 py-1.5 text-sm',\n  md: 'px-4 py-2 text-base',\n  lg: 'px-6 py-3 text-lg'\n}\n\nexport function Button({\n  variant = 'primary',\n  size = 'md',\n  children,\n  onClick\n}: ButtonProps) {\n  return (\n    <button\n      onClick={onClick}\n      className={`\n        inline-flex items-center justify-center\n        font-medium rounded-lg\n        transition-colors duration-200\n        focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2\n        disabled:opacity-50 disabled:cursor-not-allowed\n        ${variants[variant]}\n        ${sizes[size]}\n      `}\n    >\n      {children}\n    </button>\n  )\n}\n```\n\n## Next.js\n\n### Setup (App Router)\n\n```bash\n# Create Next.js project (Tailwind included by default)\nnpx create-next-app@latest my-app\ncd my-app\n```\n\nIf adding to existing project:\n\n```bash\nnpm install -D tailwindcss @tailwindcss/postcss\n```\n\n### Configuration\n\n```javascript\n// postcss.config.mjs\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {}\n  }\n}\n```\n\n```css\n/* app/globals.css */\n@import \"tailwindcss\";\n\n@theme {\n  --color-primary: oklch(0.6 0.2 250);\n}\n```\n\n```tsx\n// app/layout.tsx\nimport './globals.css'\n\nexport default function RootLayout({\n  children\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <html lang=\"en\">\n      <body className=\"bg-white dark:bg-gray-900\">{children}</body>\n    </html>\n  )\n}\n```\n\n### Server Components\n\nTailwind works seamlessly with React Server Components:\n\n```tsx\n// app/page.tsx (Server Component)\nexport default function Home() {\n  return (\n    <main className=\"container mx-auto px-4 py-8\">\n      <h1 className=\"text-4xl font-bold text-gray-900 dark:text-white\">\n        Welcome\n      </h1>\n      <p className=\"mt-4 text-gray-600 dark:text-gray-300\">\n        Server-rendered with Tailwind\n      </p>\n    </main>\n  )\n}\n```\n\n### Dark Mode with next-themes\n\n```bash\nnpm install next-themes\n```\n\n```tsx\n// app/providers.tsx\n'use client'\n\nimport { ThemeProvider } from 'next-themes'\n\nexport function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <ThemeProvider attribute=\"class\" defaultTheme=\"system\" enableSystem>\n      {children}\n    </ThemeProvider>\n  )\n}\n```\n\n```tsx\n// app/layout.tsx\nimport { Providers } from './providers'\nimport './globals.css'\n\nexport default function RootLayout({\n  children\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <html lang=\"en\" suppressHydrationWarning>\n      <body>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  )\n}\n```\n\n```css\n/* globals.css - Add dark mode variant */\n@import \"tailwindcss\";\n@custom-variant dark (&:where(.dark, .dark *));\n```\n\n## Vue 3\n\n### Setup with Vite\n\n```bash\nnpm create vue@latest my-app\ncd my-app\n\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n### Configuration\n\n```javascript\n// vite.config.ts\nimport { defineConfig } from 'vite'\nimport vue from '@vitejs/plugin-vue'\nimport tailwindcss from '@tailwindcss/vite'\n\nexport default defineConfig({\n  plugins: [vue(), tailwindcss()]\n})\n```\n\n```css\n/* src/assets/main.css */\n@import \"tailwindcss\";\n```\n\n```typescript\n// src/main.ts\nimport { createApp } from 'vue'\nimport App from './App.vue'\nimport './assets/main.css'\n\ncreateApp(App).mount('#app')\n```\n\n### Component Example\n\n```vue\n<!-- src/components/Button.vue -->\n<script setup lang=\"ts\">\ndefineProps<{\n  variant?: 'primary' | 'secondary'\n}>()\n</script>\n\n<template>\n  <button\n    class=\"px-4 py-2 rounded-lg font-medium transition-colors\"\n    :class=\"{\n      'bg-blue-600 text-white hover:bg-blue-700': variant === 'primary',\n      'bg-gray-200 text-gray-900 hover:bg-gray-300': variant === 'secondary'\n    }\"\n  >\n    <slot />\n  </button>\n</template>\n```\n\n### Scoped Styles with @reference\n\n```vue\n<template>\n  <h1>Hello world!</h1>\n</template>\n\n<style scoped>\n/* Reference global Tailwind definitions */\n@reference \"../../assets/main.css\";\n\nh1 {\n  @apply text-2xl font-bold text-blue-600;\n}\n</style>\n```\n\n### Alternative: CSS Variables\n\n```vue\n<template>\n  <h1>Hello world!</h1>\n</template>\n\n<style scoped>\nh1 {\n  /* Use CSS variables directly - better performance */\n  font-size: var(--text-2xl);\n  font-weight: var(--font-bold);\n  color: var(--color-blue-600);\n}\n</style>\n```\n\n## Nuxt 3\n\n### Setup\n\n```bash\nnpx nuxi init my-app\ncd my-app\n\nnpm install -D tailwindcss @tailwindcss/postcss\n```\n\n### Configuration\n\n```javascript\n// postcss.config.mjs\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {}\n  }\n}\n```\n\n```css\n/* assets/css/main.css */\n@import \"tailwindcss\";\n```\n\n```typescript\n// nuxt.config.ts\nexport default defineNuxtConfig({\n  css: ['~/assets/css/main.css']\n})\n```\n\n## Svelte / SvelteKit\n\n### Setup\n\n```bash\nnpm create svelte@latest my-app\ncd my-app\n\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n### Configuration\n\n```javascript\n// vite.config.js\nimport { sveltekit } from '@sveltejs/kit/vite'\nimport tailwindcss from '@tailwindcss/vite'\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  plugins: [sveltekit(), tailwindcss()]\n})\n```\n\n```css\n/* src/app.css */\n@import \"tailwindcss\";\n```\n\n```svelte\n<!-- src/routes/+layout.svelte -->\n<script>\n  import '../app.css'\n</script>\n\n<slot />\n```\n\n### Component Example\n\n```svelte\n<!-- src/lib/Button.svelte -->\n<script lang=\"ts\">\n  export let variant: 'primary' | 'secondary' = 'primary'\n</script>\n\n<button\n  class=\"px-4 py-2 rounded-lg font-medium transition-colors\n    {variant === 'primary' ? 'bg-blue-600 text-white hover:bg-blue-700' : ''}\n    {variant === 'secondary' ? 'bg-gray-200 text-gray-900 hover:bg-gray-300' : ''}\"\n>\n  <slot />\n</button>\n```\n\n## Astro\n\n### Setup\n\n```bash\nnpm create astro@latest my-app\ncd my-app\n\nnpx astro add tailwind\n```\n\n### Manual Setup\n\n```bash\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n```javascript\n// astro.config.mjs\nimport { defineConfig } from 'astro/config'\nimport tailwindcss from '@tailwindcss/vite'\n\nexport default defineConfig({\n  vite: {\n    plugins: [tailwindcss()]\n  }\n})\n```\n\n```css\n/* src/styles/global.css */\n@import \"tailwindcss\";\n```\n\n```astro\n<!-- src/layouts/Layout.astro -->\n---\nimport '../styles/global.css'\n---\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <title>My Site</title>\n  </head>\n  <body class=\"bg-white dark:bg-gray-900\">\n    <slot />\n  </body>\n</html>\n```\n\n## Remix\n\n### Setup\n\n```bash\nnpx create-remix@latest my-app\ncd my-app\n\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n### Configuration\n\n```javascript\n// vite.config.ts\nimport { vitePlugin as remix } from '@remix-run/dev'\nimport { defineConfig } from 'vite'\nimport tailwindcss from '@tailwindcss/vite'\n\nexport default defineConfig({\n  plugins: [remix(), tailwindcss()]\n})\n```\n\n```css\n/* app/tailwind.css */\n@import \"tailwindcss\";\n```\n\n```tsx\n// app/root.tsx\nimport stylesheet from './tailwind.css?url'\nimport { Links } from '@remix-run/react'\n\nexport const links = () => [\n  { rel: 'stylesheet', href: stylesheet }\n]\n\nexport default function App() {\n  return (\n    <html>\n      <head>\n        <Links />\n      </head>\n      <body className=\"bg-white dark:bg-gray-900\">\n        <Outlet />\n      </body>\n    </html>\n  )\n}\n```\n\n## Best Practices for All Frameworks\n\n### 1. Use the Prettier Plugin\n\n```bash\nnpm install -D prettier prettier-plugin-tailwindcss\n```\n\n```json\n// .prettierrc\n{\n  \"plugins\": [\"prettier-plugin-tailwindcss\"]\n}\n```\n\n### 2. Install VS Code Extension\n\n```bash\ncode --install-extension bradlc.vscode-tailwindcss\n```\n\n### 3. Create Reusable Components\n\nDon't repeat long class stringsextract components:\n\n```tsx\n// Instead of repeating this everywhere:\n<button className=\"px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors\">\n\n// Create a component:\n<Button variant=\"primary\">Click me</Button>\n```\n\n### 4. Use clsx or tailwind-merge for Conditional Classes\n\n```bash\nnpm install clsx tailwind-merge\n```\n\n```tsx\nimport { clsx } from 'clsx'\nimport { twMerge } from 'tailwind-merge'\n\nfunction cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs))\n}\n\n// Usage\n<div className={cn(\n  \"base-class\",\n  isActive && \"active-class\",\n  disabled && \"opacity-50\"\n)} />\n```\n\n### 5. Configure Path Aliases\n\n```json\n// tsconfig.json\n{\n  \"compilerOptions\": {\n    \"paths\": {\n      \"@/*\": [\"./src/*\"],\n      \"@/components/*\": [\"./src/components/*\"]\n    }\n  }\n}\n```\n\n```tsx\nimport { Button } from '@/components/Button'\n```"
              },
              {
                "name": "tailwindcss-fundamentals-v4",
                "description": "Tailwind CSS v4 fundamentals covering installation, CSS-first configuration, and the new Rust-based engine",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-fundamentals-v4/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-fundamentals-v4",
                  "description": "Tailwind CSS v4 fundamentals covering installation, CSS-first configuration, and the new Rust-based engine"
                },
                "content": "# Tailwind CSS v4 Fundamentals (2025)\n\n## Overview\n\nTailwind CSS v4.0 was released January 22, 2025, featuring a complete rewrite with a Rust-based engine, CSS-first configuration, and significant performance improvements.\n\n## Installation\n\n### Vite Projects (Recommended)\n\n```bash\nnpm install -D tailwindcss @tailwindcss/vite\n```\n\n```javascript\n// vite.config.js\nimport tailwindcss from '@tailwindcss/vite'\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  plugins: [tailwindcss()]\n})\n```\n\n### PostCSS Projects\n\n```bash\nnpm install -D tailwindcss @tailwindcss/postcss\n```\n\n```javascript\n// postcss.config.mjs\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {}\n  }\n}\n```\n\n### CSS Entry Point\n\n```css\n/* app.css - The only required CSS file */\n@import \"tailwindcss\";\n```\n\n## CSS-First Configuration\n\n### The @theme Directive\n\nReplace tailwind.config.js with CSS-based configuration:\n\n```css\n@import \"tailwindcss\";\n\n@theme {\n  /* Colors using modern oklch */\n  --color-primary: oklch(0.6 0.2 250);\n  --color-secondary: oklch(0.7 0.15 180);\n  --color-accent: oklch(0.8 0.2 30);\n\n  /* Typography */\n  --font-display: \"Satoshi\", sans-serif;\n  --font-body: \"Inter\", sans-serif;\n\n  /* Custom spacing */\n  --spacing-xs: 0.25rem;\n  --spacing-sm: 0.5rem;\n  --spacing-md: 1rem;\n  --spacing-lg: 2rem;\n  --spacing-xl: 4rem;\n\n  /* Custom breakpoints */\n  --breakpoint-xs: 475px;\n  --breakpoint-3xl: 1920px;\n}\n```\n\n### Theme Variables Reference\n\n| Category | Variable Pattern | Example |\n|----------|-----------------|---------|\n| Colors | `--color-*` | `--color-brand-500` |\n| Fonts | `--font-*` | `--font-heading` |\n| Spacing | `--spacing-*` | `--spacing-4` |\n| Breakpoints | `--breakpoint-*` | `--breakpoint-3xl` |\n| Radius | `--radius-*` | `--radius-lg` |\n| Shadows | `--shadow-*` | `--shadow-xl` |\n| Animations | `--animate-*` | `--animate-fade-in` |\n| Easing | `--ease-*` | `--ease-bounce` |\n\n### Disabling Defaults\n\n```css\n@theme {\n  /* Start fresh - disable all default theme values */\n  --*: initial;\n\n  /* Define only what you need */\n  --spacing: 4px;\n  --font-body: Inter, sans-serif;\n  --color-primary: oklch(0.72 0.11 221.19);\n  --color-secondary: oklch(0.74 0.17 40.24);\n}\n```\n\n## Custom Utilities\n\n### Static Utilities\n\n```css\n/* Define custom utility in CSS */\n@utility content-auto {\n  content-visibility: auto;\n}\n\n@utility text-balance {\n  text-wrap: balance;\n}\n\n@utility scrollbar-hide {\n  -ms-overflow-style: none;\n  scrollbar-width: none;\n}\n\n@utility scrollbar-hide::-webkit-scrollbar {\n  display: none;\n}\n```\n\nUsage:\n```html\n<div class=\"content-auto scrollbar-hide\">\n  <h1 class=\"text-balance\">Long headline that should balance nicely</h1>\n</div>\n```\n\n### Functional Utilities\n\n```css\n/* Utility that accepts values */\n@utility tab-* {\n  tab-size: --value(integer);\n}\n\n@utility text-shadow-* {\n  text-shadow: 0 0 --value([length]) currentColor;\n}\n\n/* With theme reference */\n@utility gap-safe-* {\n  gap: max(--value(--spacing-*), env(safe-area-inset-bottom));\n}\n```\n\nUsage:\n```html\n<pre class=\"tab-4\">Code with 4-space tabs</pre>\n<h1 class=\"text-shadow-[2px]\">Glowing text</h1>\n<div class=\"gap-safe-4\">Safe area aware gap</div>\n```\n\n## Custom Variants\n\n### The @custom-variant Directive\n\n```css\n/* Dark mode with selector */\n@custom-variant dark (&:where(.dark, .dark *));\n\n/* Custom state variants */\n@custom-variant hocus (&:hover, &:focus);\n@custom-variant group-hocus (:merge(.group):hover &, :merge(.group):focus &);\n\n/* Data attribute variants */\n@custom-variant data-loading (&[data-loading=\"true\"]);\n@custom-variant data-active (&[data-state=\"active\"]);\n\n/* Child selectors */\n@custom-variant children (& > *);\n@custom-variant not-first (& > *:not(:first-child));\n```\n\nUsage:\n```html\n<button class=\"hocus:bg-blue-600\">Hover or focus</button>\n<div class=\"data-loading:opacity-50\" data-loading=\"true\">Loading...</div>\n<ul class=\"children:border-b not-first:pt-2\">\n  <li>Item 1</li>\n  <li>Item 2</li>\n</ul>\n```\n\n## Loading Plugins\n\n### The @plugin Directive\n\n```css\n@import \"tailwindcss\";\n\n/* Load official plugins */\n@plugin \"@tailwindcss/typography\";\n@plugin \"@tailwindcss/forms\";\n@plugin \"@tailwindcss/container-queries\";\n\n/* Load local plugin */\n@plugin \"./my-plugin.js\";\n```\n\n### Plugin with Options\n\n```css\n@plugin \"@tailwindcss/typography\" {\n  className: wysiwyg;\n}\n\n@plugin \"@tailwindcss/forms\" {\n  strategy: class;\n}\n```\n\n## Using Prefixes\n\n```css\n@import \"tailwindcss\" prefix(tw);\n\n@theme {\n  /* Define without prefix */\n  --font-display: \"Satoshi\", sans-serif;\n}\n```\n\n```html\n<!-- Use with prefix -->\n<div class=\"tw:flex tw:bg-red-500 tw:hover:bg-red-600\">\n  Content\n</div>\n```\n\nGenerated CSS variables include prefix:\n```css\n:root {\n  --tw-font-display: \"Satoshi\", sans-serif;\n}\n```\n\n## CSS Variables in Code\n\n### Replace theme() with var()\n\n```css\n/* v3 approach (deprecated) */\n.old-way {\n  background-color: theme(colors.red.500);\n}\n\n/* v4 approach */\n.new-way {\n  background-color: var(--color-red-500);\n}\n\n/* In media queries, use CSS variable names */\n@media (width >= theme(--breakpoint-xl)) {\n  /* styles */\n}\n```\n\n## Core Utility Categories\n\n### Layout\n\n```html\n<!-- Flexbox -->\n<div class=\"flex flex-col md:flex-row items-center justify-between gap-4\">\n\n<!-- Grid -->\n<div class=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\">\n\n<!-- Container -->\n<div class=\"container mx-auto px-4\">\n```\n\n### Spacing\n\n```html\n<!-- Padding -->\n<div class=\"p-4 px-6 py-2\">\n\n<!-- Margin -->\n<div class=\"m-4 mx-auto my-8\">\n\n<!-- Gap -->\n<div class=\"gap-4 gap-x-6 gap-y-2\">\n```\n\n### Typography\n\n```html\n<!-- Font size -->\n<p class=\"text-sm md:text-base lg:text-lg\">\n\n<!-- Font weight -->\n<h1 class=\"font-bold\">\n\n<!-- Text color -->\n<p class=\"text-gray-600 dark:text-gray-300\">\n\n<!-- Line height -->\n<p class=\"leading-relaxed\">\n```\n\n### Colors\n\n```html\n<!-- Background -->\n<div class=\"bg-white dark:bg-gray-900\">\n\n<!-- Text -->\n<p class=\"text-blue-600\">\n\n<!-- Border -->\n<div class=\"border border-gray-200\">\n\n<!-- Ring -->\n<button class=\"focus:ring-2 focus:ring-blue-500\">\n```\n\n### Sizing\n\n```html\n<!-- Width -->\n<div class=\"w-full md:w-1/2 lg:w-1/3\">\n\n<!-- Height -->\n<div class=\"h-screen min-h-[500px]\">\n\n<!-- Max width -->\n<div class=\"max-w-xl mx-auto\">\n```\n\n## Arbitrary Values\n\n```html\n<!-- Use any CSS value -->\n<div class=\"top-[117px] left-[calc(50%-4rem)]\">\n\n<!-- With CSS variables -->\n<div class=\"bg-[var(--my-color)]\">\n\n<!-- Complex values -->\n<div class=\"grid-cols-[1fr_500px_2fr]\">\n```\n\n## Important Modifier\n\n```html\n<!-- Force important -->\n<div class=\"!mt-0\">\n\n<!-- With variants -->\n<div class=\"hover:!bg-red-500\">\n```\n\n## Built-in Features (No Config Needed)\n\n| Feature | v3 Requirement | v4 |\n|---------|---------------|-----|\n| @import handling | postcss-import | Built-in |\n| Vendor prefixing | autoprefixer | Built-in |\n| CSS nesting | postcss-nested | Built-in |\n| Content detection | content config | Automatic |\n\n## Layers\n\n```css\n/* Use native CSS layers */\n@layer base {\n  h1 {\n    @apply text-2xl font-bold;\n  }\n}\n\n@layer components {\n  .btn {\n    @apply px-4 py-2 rounded font-medium;\n  }\n}\n\n@layer utilities {\n  /* Custom utilities via @utility directive instead */\n}\n```\n\n## Best Practices\n\n### 1. Use CSS Variables Consistently\n\n```css\n@theme {\n  --color-brand: oklch(0.6 0.2 250);\n}\n\n/* In custom CSS */\n.custom-element {\n  border-color: var(--color-brand);\n}\n```\n\n### 2. Organize Theme by Category\n\n```css\n@theme {\n  /* Colors first */\n  --color-primary: oklch(0.6 0.2 250);\n  --color-secondary: oklch(0.7 0.15 180);\n\n  /* Then typography */\n  --font-sans: Inter, sans-serif;\n  --font-mono: \"Fira Code\", monospace;\n\n  /* Then spacing */\n  --spacing-page: 2rem;\n\n  /* Then other tokens */\n  --radius-default: 0.5rem;\n}\n```\n\n### 3. Keep @utility Definitions Simple\n\n```css\n/* Good - single purpose */\n@utility truncate-2 {\n  display: -webkit-box;\n  -webkit-line-clamp: 2;\n  -webkit-box-orient: vertical;\n  overflow: hidden;\n}\n\n/* Avoid - too complex */\n@utility card-fancy {\n  /* Too many properties - use a component instead */\n}\n```\n\n## Common Migration Issues\n\n### Border Color Default\n\n```css\n/* v3: border used gray-200 by default */\n/* v4: border uses currentColor */\n\n/* Fix: explicitly set color */\n@theme {\n  --default-border-color: var(--color-gray-200);\n}\n```\n\n### Ring Default\n\n```css\n/* v3: ring was 3px blue-500 */\n/* v4: ring is 1px currentColor */\n\n/* Fix: restore v3 behavior */\n@theme {\n  --default-ring-width: 3px;\n  --default-ring-color: var(--color-blue-500);\n}\n```\n\n### Button Cursor\n\n```css\n/* v4: buttons use cursor: default */\n/* Fix: add pointer globally if needed */\nbutton {\n  cursor: pointer;\n}\n```"
              },
              {
                "name": "tailwindcss-performance",
                "description": "Tailwind CSS performance optimization including v4 improvements and best practices",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-performance/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-performance",
                  "description": "Tailwind CSS performance optimization including v4 improvements and best practices"
                },
                "content": "# Tailwind CSS Performance Optimization\n\n## v4 Performance Improvements\n\nTailwind CSS v4 features a completely rewritten engine in Rust:\n\n| Metric | v3 | v4 |\n|--------|----|----|\n| Full builds | Baseline | Up to 5x faster |\n| Incremental builds | Milliseconds | Microseconds (100x+) |\n| Engine | JavaScript | Rust |\n\n## JIT (Just-In-Time) Compilation\n\n### How JIT Works\n\nJIT generates styles on-demand as classes are discovered in your files:\n\n1. Scans source files for class names\n2. Generates only the CSS you use\n3. Produces minimal, optimized output\n\n### v4: Always JIT\n\nUnlike v3, JIT is always enabled in v4no configuration needed:\n\n```css\n@import \"tailwindcss\";\n/* JIT is automatic */\n```\n\n## Content Detection\n\n### Automatic Detection (v4)\n\nv4 automatically detects template filesno content configuration required:\n\n```css\n/* v4 - Works automatically */\n@import \"tailwindcss\";\n```\n\n### Explicit Content (v4)\n\nIf automatic detection fails, specify sources explicitly:\n\n```css\n@import \"tailwindcss\";\n@source \"./src/**/*.{html,js,jsx,ts,tsx,vue,svelte}\";\n@source \"./components/**/*.{js,jsx,ts,tsx}\";\n```\n\n### Excluding Paths\n\n```css\n@source not \"./src/legacy/**\";\n```\n\n## Tree Shaking\n\n### How It Works\n\nTailwind's build process removes unused CSS:\n\n```\nSource: All possible utilities (~15MB+)\n\nScan: Find used class names\n\nOutput: Only used styles (~10-50KB typical)\n```\n\n### Production Build\n\n```bash\n# Vite - automatically optimized for production\nnpm run build\n\n# PostCSS - ensure NODE_ENV is set\nNODE_ENV=production npx postcss input.css -o output.css\n```\n\n## Dynamic Class Names\n\n### The Problem\n\nTailwind can't detect dynamically constructed class names:\n\n```javascript\n// BAD - Classes won't be generated\nconst color = 'blue'\nclassName={`text-${color}-500`}  //  Not detected\n\nconst size = 'lg'\nclassName={`text-${size}`}  //  Not detected\n```\n\n### Solutions\n\n#### 1. Use Complete Class Names\n\n```javascript\n// GOOD - Full class names\nconst colorClasses = {\n  blue: 'text-blue-500',\n  red: 'text-red-500',\n  green: 'text-green-500',\n}\nclassName={colorClasses[color]}  //  Detected\n```\n\n#### 2. Use Data Attributes\n\n```javascript\n// GOOD - Style based on data attributes\n<div data-color={color} className=\"data-[color=blue]:text-blue-500 data-[color=red]:text-red-500\">\n```\n\n#### 3. Safelist Classes\n\n```css\n/* In your CSS for v4 */\n@source inline(\"text-blue-500 text-red-500 text-green-500\");\n```\n\n#### 4. CSS Variables\n\n```css\n@theme {\n  --color-dynamic: oklch(0.6 0.2 250);\n}\n```\n\n```html\n<div class=\"text-[var(--color-dynamic)]\">Dynamic color</div>\n```\n\n## Optimizing Transitions\n\n### Use Specific Transitions\n\n```html\n<!-- SLOW - Transitions all properties -->\n<button class=\"transition-all duration-200\">\n\n<!-- FAST - Only transitions specific properties -->\n<button class=\"transition-colors duration-200\">\n<button class=\"transition-transform duration-200\">\n<button class=\"transition-opacity duration-200\">\n```\n\n### GPU-Accelerated Properties\n\nPrefer `transform` and `opacity` for smooth animations:\n\n```html\n<!-- GOOD - GPU accelerated -->\n<div class=\"transform hover:scale-105 transition-transform\">\n\n<!-- GOOD - GPU accelerated -->\n<div class=\"opacity-100 hover:opacity-80 transition-opacity\">\n\n<!-- SLOW - May cause repaints -->\n<div class=\"left-0 hover:left-4 transition-all\">\n```\n\n## CSS Variable Usage\n\n### Prefer Native Variables\n\nIn v4, use CSS variables directly instead of `theme()`:\n\n```css\n/* v3 - Uses theme() function */\n.element {\n  color: theme(colors.blue.500);\n}\n\n/* v4 - Use CSS variables (faster) */\n.element {\n  color: var(--color-blue-500);\n}\n```\n\n### Static Theme Values\n\nFor performance-critical paths:\n\n```css\n@import \"tailwindcss/theme.css\" theme(static);\n```\n\nThis inlines theme values instead of using CSS variables.\n\n## Build Optimization\n\n### Vite Configuration\n\n```javascript\n// vite.config.js\nimport tailwindcss from '@tailwindcss/vite'\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  plugins: [tailwindcss()],\n  build: {\n    // Minify CSS\n    cssMinify: 'lightningcss',\n    // Optimize chunks\n    rollupOptions: {\n      output: {\n        manualChunks: {\n          // Split vendor CSS if needed\n        }\n      }\n    }\n  }\n})\n```\n\n### PostCSS with cssnano\n\n```javascript\n// postcss.config.mjs\nexport default {\n  plugins: {\n    '@tailwindcss/postcss': {},\n    cssnano: process.env.NODE_ENV === 'production' ? {} : false\n  }\n}\n```\n\n## Reducing Bundle Size\n\n### 1. Avoid Unused Plugins\n\n```css\n/* Only load what you need */\n@plugin \"@tailwindcss/typography\";\n/* Don't load unused plugins */\n```\n\n### 2. Limit Color Palette\n\n```css\n@theme {\n  /* Disable default colors */\n  --color-*: initial;\n\n  /* Define only needed colors */\n  --color-primary: oklch(0.6 0.2 250);\n  --color-secondary: oklch(0.7 0.15 180);\n  --color-gray-100: oklch(0.95 0 0);\n  --color-gray-900: oklch(0.15 0 0);\n}\n```\n\n### 3. Limit Breakpoints\n\n```css\n@theme {\n  /* Remove unused breakpoints */\n  --breakpoint-2xl: initial;\n\n  /* Keep only what you use */\n  --breakpoint-sm: 640px;\n  --breakpoint-md: 768px;\n  --breakpoint-lg: 1024px;\n}\n```\n\n## Caching Strategies\n\n### Development\n\n- v4's incremental builds are already extremely fast\n- No additional caching needed in most cases\n\n### CI/CD\n\n```yaml\n# GitHub Actions example\n- name: Cache node_modules\n  uses: actions/cache@v4\n  with:\n    path: node_modules\n    key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}\n\n- name: Build\n  run: npm run build\n```\n\n## Measuring Performance\n\n### Build Time Analysis\n\n```bash\n# Time your build\ntime npm run build\n\n# Verbose output\nDEBUG=tailwindcss:* npm run build\n```\n\n### Bundle Analysis\n\n```bash\n# Install analyzer\nnpm install -D vite-bundle-analyzer\n\n# Analyze bundle\nnpm run build -- --analyze\n```\n\n### CSS Size Check\n\n```bash\n# Check output CSS size\nls -lh dist/assets/*.css\n\n# Gzipped size\ngzip -c dist/assets/main.css | wc -c\n```\n\n## Performance Checklist\n\n### Development\n\n- [ ] JIT is working (styles update instantly)\n- [ ] No console warnings about large files\n- [ ] Hot reload is fast\n\n### Production\n\n- [ ] `NODE_ENV=production` is set\n- [ ] CSS is minified\n- [ ] Unused CSS is removed\n- [ ] No dynamic class name issues\n- [ ] CSS size is reasonable (<50KB typical)\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Large CSS output | Check for dynamic classes, safelist issues |\n| Slow builds | Ensure v4, check file globs |\n| Missing styles | Check content detection, class names |\n| Slow animations | Use GPU-accelerated properties |\n\n## Lazy Loading CSS\n\nFor very large apps, consider code-splitting CSS:\n\n```javascript\n// Dynamically import CSS for routes\nconst AdminPage = lazy(() =>\n  import('./admin.css').then(() => import('./AdminPage'))\n)\n```\n\n## Best Practices Summary\n\n1. **Let JIT do its work** - Don't safelist unnecessarily\n2. **Use complete class names** - Avoid dynamic concatenation\n3. **Specific transitions** - Not `transition-all`\n4. **GPU properties** - Prefer `transform` and `opacity`\n5. **Minimal theme** - Only define what you use\n6. **Production builds** - Always use production mode\n7. **Measure** - Check your actual CSS size"
              },
              {
                "name": "tailwindcss-plugins",
                "description": "Tailwind CSS plugins including official plugins and custom plugin development",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-plugins/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-plugins",
                  "description": "Tailwind CSS plugins including official plugins and custom plugin development"
                },
                "content": "# Tailwind CSS Plugins\n\n## Official Plugins\n\n### @tailwindcss/typography\n\nBeautiful typographic defaults for content you don't control (Markdown, CMS content).\n\n#### Installation\n\n```bash\nnpm install -D @tailwindcss/typography\n```\n\n```css\n@import \"tailwindcss\";\n@plugin \"@tailwindcss/typography\";\n```\n\n#### Basic Usage\n\n```html\n<article class=\"prose\">\n  <h1>Article Title</h1>\n  <p>This content gets beautiful default styles...</p>\n</article>\n```\n\n#### Size Modifiers\n\n| Class | Description |\n|-------|-------------|\n| `prose-sm` | Smaller text (14px base) |\n| `prose` | Default (16px base) |\n| `prose-lg` | Larger text (18px base) |\n| `prose-xl` | Extra large (20px base) |\n| `prose-2xl` | Huge (24px base) |\n\n```html\n<article class=\"prose md:prose-lg lg:prose-xl\">\n  <!-- Responsive sizing -->\n</article>\n```\n\n#### Color Themes\n\n```html\n<article class=\"prose prose-slate\">Gray theme</article>\n<article class=\"prose prose-zinc\">Zinc theme</article>\n<article class=\"prose prose-neutral\">Neutral theme</article>\n<article class=\"prose prose-stone\">Stone theme</article>\n```\n\n#### Dark Mode\n\n```html\n<article class=\"prose dark:prose-invert\">\n  <!-- Automatically inverts for dark mode -->\n</article>\n```\n\n#### Element Modifiers\n\nOverride specific elements:\n\n```html\n<article class=\"\n  prose\n  prose-headings:text-blue-600\n  prose-a:text-blue-500\n  prose-a:no-underline\n  prose-code:text-pink-500\n  prose-img:rounded-lg\n  prose-strong:text-gray-900\n  prose-blockquote:border-blue-500\n\">\n  Content\n</article>\n```\n\n#### Max Width Control\n\n```html\n<!-- Remove max-width constraint -->\n<article class=\"prose max-w-none\">\n  Full width content\n</article>\n```\n\n#### Escaping Prose Styles\n\n```html\n<article class=\"prose\">\n  <h1>Styled heading</h1>\n  <p>Styled paragraph</p>\n\n  <div class=\"not-prose\">\n    <!-- This div and its children escape prose styles -->\n    <CustomComponent />\n  </div>\n\n  <p>Back to prose styles</p>\n</article>\n```\n\n#### Custom Class Name\n\n```css\n@plugin \"@tailwindcss/typography\" {\n  className: wysiwyg;\n}\n```\n\n```html\n<article class=\"wysiwyg\">Content</article>\n```\n\n### @tailwindcss/forms\n\nResets form elements to a consistent, easily-styleable baseline.\n\n#### Installation\n\n```bash\nnpm install -D @tailwindcss/forms\n```\n\n```css\n@import \"tailwindcss\";\n@plugin \"@tailwindcss/forms\";\n```\n\n#### Styled Elements\n\nThe plugin applies styles to:\n- `input[type='text']`\n- `input[type='email']`\n- `input[type='password']`\n- `input[type='number']`\n- `input[type='url']`\n- `input[type='date']`\n- `input[type='datetime-local']`\n- `input[type='month']`\n- `input[type='week']`\n- `input[type='time']`\n- `input[type='search']`\n- `input[type='tel']`\n- `input[type='checkbox']`\n- `input[type='radio']`\n- `select`\n- `select[multiple]`\n- `textarea`\n\n#### Basic Usage\n\n```html\n<!-- Text input -->\n<input type=\"email\" class=\"rounded-lg border-gray-300 focus:border-blue-500 focus:ring-blue-500\">\n\n<!-- Select -->\n<select class=\"rounded-lg border-gray-300\">\n  <option>Option 1</option>\n  <option>Option 2</option>\n</select>\n\n<!-- Checkbox -->\n<input type=\"checkbox\" class=\"rounded text-blue-500 focus:ring-blue-500\">\n\n<!-- Radio -->\n<input type=\"radio\" class=\"text-blue-500 focus:ring-blue-500\">\n\n<!-- Textarea -->\n<textarea class=\"rounded-lg border-gray-300\" rows=\"4\"></textarea>\n```\n\n#### Strategy: Class-Based\n\nFor opt-in styling (doesn't apply global resets):\n\n```css\n@plugin \"@tailwindcss/forms\" {\n  strategy: class;\n}\n```\n\n```html\n<!-- Explicitly opt-in to form styles -->\n<input type=\"text\" class=\"form-input rounded-lg\">\n<select class=\"form-select rounded-lg\">\n<textarea class=\"form-textarea rounded-lg\">\n<input type=\"checkbox\" class=\"form-checkbox rounded\">\n<input type=\"radio\" class=\"form-radio\">\n```\n\n#### Form Classes Reference\n\n| Class | Element |\n|-------|---------|\n| `form-input` | Text inputs |\n| `form-textarea` | Textareas |\n| `form-select` | Selects |\n| `form-multiselect` | Multiple selects |\n| `form-checkbox` | Checkboxes |\n| `form-radio` | Radio buttons |\n\n#### Styling Checkboxes/Radios\n\n```html\n<!-- Colored checkbox -->\n<input type=\"checkbox\" class=\"rounded text-pink-500\">\n\n<!-- Accent color (native) -->\n<input type=\"checkbox\" class=\"accent-purple-600\">\n\n<!-- Custom size -->\n<input type=\"checkbox\" class=\"h-6 w-6 rounded text-blue-500\">\n```\n\n### @tailwindcss/container-queries\n\nStyle elements based on their container's size instead of the viewport.\n\n#### Installation\n\n```bash\nnpm install -D @tailwindcss/container-queries\n```\n\n```css\n@import \"tailwindcss\";\n@plugin \"@tailwindcss/container-queries\";\n```\n\n#### Basic Usage\n\n```html\n<!-- Define a container -->\n<div class=\"@container\">\n  <!-- Use container query breakpoints -->\n  <div class=\"flex flex-col @md:flex-row @lg:gap-8\">\n    <div class=\"@sm:text-lg @md:text-xl\">\n      Responsive to container, not viewport\n    </div>\n  </div>\n</div>\n```\n\n#### Container Breakpoints\n\n| Prefix | Width |\n|--------|-------|\n| `@xs` | 320px |\n| `@sm` | 384px |\n| `@md` | 448px |\n| `@lg` | 512px |\n| `@xl` | 576px |\n| `@2xl` | 672px |\n| `@3xl` | 768px |\n| `@4xl` | 896px |\n| `@5xl` | 1024px |\n| `@6xl` | 1152px |\n| `@7xl` | 1280px |\n\n#### Named Containers\n\n```html\n<!-- Name the container -->\n<div class=\"@container/sidebar\">\n  <!-- Reference by name -->\n  <div class=\"@md/sidebar:flex\">\n    Only flex when sidebar container is md\n  </div>\n</div>\n\n<div class=\"@container/main\">\n  <div class=\"@lg/main:grid-cols-3\">\n    Grid when main container is lg\n  </div>\n</div>\n```\n\n#### Arbitrary Container Values\n\n```html\n<div class=\"@container\">\n  <div class=\"@[400px]:flex @[600px]:grid\">\n    Arbitrary breakpoint values\n  </div>\n</div>\n```\n\n## Creating Custom Plugins (v4)\n\n### CSS-Only Utilities\n\nFor simple utilities, use the `@utility` directive instead of a JavaScript plugin:\n\n```css\n/* In your CSS file */\n@utility content-auto {\n  content-visibility: auto;\n}\n\n@utility text-balance {\n  text-wrap: balance;\n}\n\n@utility scrollbar-none {\n  scrollbar-width: none;\n  -ms-overflow-style: none;\n}\n\n@utility scrollbar-none::-webkit-scrollbar {\n  display: none;\n}\n```\n\n### JavaScript Plugins\n\nFor complex plugins requiring JavaScript:\n\n```javascript\n// plugins/my-plugin.js\nimport plugin from 'tailwindcss/plugin'\n\nexport default plugin(function({ addUtilities, addComponents, matchUtilities, theme }) {\n  // Add static utilities\n  addUtilities({\n    '.content-auto': {\n      'content-visibility': 'auto',\n    },\n    '.text-balance': {\n      'text-wrap': 'balance',\n    },\n  })\n\n  // Add components\n  addComponents({\n    '.btn': {\n      padding: theme('spacing.2') + ' ' + theme('spacing.4'),\n      borderRadius: theme('borderRadius.lg'),\n      fontWeight: theme('fontWeight.medium'),\n    },\n    '.btn-primary': {\n      backgroundColor: theme('colors.blue.500'),\n      color: theme('colors.white'),\n      '&:hover': {\n        backgroundColor: theme('colors.blue.600'),\n      },\n    },\n  })\n\n  // Add dynamic utilities\n  matchUtilities(\n    {\n      'text-shadow': (value) => ({\n        textShadow: value,\n      }),\n    },\n    { values: theme('textShadow') }\n  )\n})\n```\n\nLoad in CSS:\n\n```css\n@import \"tailwindcss\";\n@plugin \"./plugins/my-plugin.js\";\n```\n\n### Plugin with Theme Extension\n\n```javascript\n// plugins/gradients.js\nimport plugin from 'tailwindcss/plugin'\n\nexport default plugin(\n  function({ matchUtilities, theme }) {\n    matchUtilities(\n      {\n        'text-gradient': (value) => ({\n          backgroundImage: value,\n          backgroundClip: 'text',\n          color: 'transparent',\n        }),\n      },\n      { values: theme('textGradient') }\n    )\n  },\n  {\n    theme: {\n      textGradient: {\n        primary: 'linear-gradient(to right, #667eea, #764ba2)',\n        secondary: 'linear-gradient(to right, #f093fb, #f5576c)',\n        sunset: 'linear-gradient(to right, #fa709a, #fee140)',\n      },\n    },\n  }\n)\n```\n\n### Adding Custom Variants\n\n```javascript\n// plugins/variants.js\nimport plugin from 'tailwindcss/plugin'\n\nexport default plugin(function({ addVariant }) {\n  // Peer states\n  addVariant('peer-checked', ':merge(.peer):checked ~ &')\n\n  // Group states\n  addVariant('group-focus-visible', ':merge(.group):focus-visible &')\n\n  // Data attributes\n  addVariant('data-active', '&[data-active=\"true\"]')\n  addVariant('data-loading', '&[data-loading]')\n\n  // Custom selectors\n  addVariant('hocus', ['&:hover', '&:focus'])\n  addVariant('not-first', '&:not(:first-child)')\n  addVariant('not-last', '&:not(:last-child)')\n})\n```\n\n## Community Plugins\n\n### Popular Plugins\n\n| Plugin | Description |\n|--------|-------------|\n| `tailwindcss-animate` | Animation utilities |\n| `tailwindcss-motion` | Advanced motion/animation |\n| `@headlessui/tailwindcss` | Headless UI variants |\n| `tailwind-scrollbar` | Scrollbar styling |\n| `tailwindcss-3d` | 3D transform utilities |\n| `tailwindcss-fluid-type` | Fluid typography |\n\n### tailwindcss-animate\n\n```bash\nnpm install -D tailwindcss-animate\n```\n\n```css\n@plugin \"tailwindcss-animate\";\n```\n\n```html\n<div class=\"animate-in fade-in slide-in-from-bottom-4 duration-500\">\n  Animated content\n</div>\n\n<div class=\"animate-out fade-out slide-out-to-top-4 duration-300\">\n  Exiting content\n</div>\n```\n\n## Best Practices\n\n### 1. Load Only What You Need\n\n```css\n/* Only load plugins you actually use */\n@plugin \"@tailwindcss/typography\";\n/* @plugin \"@tailwindcss/forms\"; -- commented out if not using */\n```\n\n### 2. Use CSS Utilities First\n\nBefore creating a JavaScript plugin, try the `@utility` directive:\n\n```css\n/* Simple custom utility - no JS needed */\n@utility glass {\n  backdrop-filter: blur(10px);\n  background: rgba(255, 255, 255, 0.1);\n}\n```\n\n### 3. Document Custom Plugins\n\n```javascript\n/**\n * @name Text Gradient Plugin\n * @description Adds text gradient utilities\n * @usage class=\"text-gradient-primary\"\n */\nexport default plugin(...)\n```\n\n### 4. Test Plugin Compatibility\n\nWhen upgrading Tailwind, verify all plugins work with the new version:\n\n```bash\nnpm outdated | grep tailwind\nnpm update @tailwindcss/typography @tailwindcss/forms\n```"
              },
              {
                "name": "tailwindcss-responsive-darkmode",
                "description": "Tailwind CSS responsive design and dark mode implementation patterns",
                "path": "plugins/tailwindcss-master/skills/tailwindcss-responsive-darkmode/SKILL.md",
                "frontmatter": {
                  "name": "tailwindcss-responsive-darkmode",
                  "description": "Tailwind CSS responsive design and dark mode implementation patterns"
                },
                "content": "# Tailwind CSS Responsive Design & Dark Mode\n\n## Responsive Design\n\n### Mobile-First Approach\n\nTailwind uses a mobile-first breakpoint system. Unprefixed utilities apply to all screen sizes, while prefixed utilities apply at that breakpoint and above.\n\n### Default Breakpoints\n\n| Prefix | Min Width | CSS Media Query |\n|--------|-----------|-----------------|\n| (none) | 0px | All sizes |\n| `sm:` | 640px | `@media (min-width: 640px)` |\n| `md:` | 768px | `@media (min-width: 768px)` |\n| `lg:` | 1024px | `@media (min-width: 1024px)` |\n| `xl:` | 1280px | `@media (min-width: 1280px)` |\n| `2xl:` | 1536px | `@media (min-width: 1536px)` |\n\n### Custom Breakpoints\n\n```css\n@theme {\n  /* Add custom breakpoints */\n  --breakpoint-xs: 475px;\n  --breakpoint-3xl: 1920px;\n\n  /* Override existing breakpoint */\n  --breakpoint-sm: 600px;\n}\n```\n\nUsage:\n```html\n<div class=\"grid xs:grid-cols-2 3xl:grid-cols-6\">\n  <!-- Custom breakpoints work like built-in ones -->\n</div>\n```\n\n### Responsive Examples\n\n#### Responsive Grid\n\n```html\n<div class=\"grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4\">\n  <div>Item 1</div>\n  <div>Item 2</div>\n  <div>Item 3</div>\n  <div>Item 4</div>\n</div>\n```\n\n#### Responsive Typography\n\n```html\n<h1 class=\"text-2xl sm:text-3xl md:text-4xl lg:text-5xl font-bold\">\n  Responsive Heading\n</h1>\n\n<p class=\"text-sm md:text-base lg:text-lg leading-relaxed\">\n  Responsive paragraph text\n</p>\n```\n\n#### Responsive Spacing\n\n```html\n<section class=\"py-8 md:py-12 lg:py-16 px-4 md:px-8 lg:px-12\">\n  <div class=\"max-w-4xl mx-auto\">\n    Content with responsive padding\n  </div>\n</section>\n```\n\n#### Responsive Navigation\n\n```html\n<nav class=\"flex flex-col md:flex-row items-center justify-between\">\n  <div class=\"hidden md:flex gap-4\">\n    <!-- Desktop navigation -->\n  </div>\n  <button class=\"md:hidden\">\n    <!-- Mobile menu button -->\n  </button>\n</nav>\n```\n\n#### Show/Hide Based on Screen Size\n\n```html\n<!-- Hidden on mobile, visible on desktop -->\n<div class=\"hidden md:block\">Desktop only</div>\n\n<!-- Visible on mobile, hidden on desktop -->\n<div class=\"block md:hidden\">Mobile only</div>\n\n<!-- Different content per breakpoint -->\n<span class=\"sm:hidden\">XS</span>\n<span class=\"hidden sm:inline md:hidden\">SM</span>\n<span class=\"hidden md:inline lg:hidden\">MD</span>\n<span class=\"hidden lg:inline xl:hidden\">LG</span>\n<span class=\"hidden xl:inline 2xl:hidden\">XL</span>\n<span class=\"hidden 2xl:inline\">2XL</span>\n```\n\n### Container Queries (v4)\n\nUse container queries for component-based responsive design:\n\n```css\n@plugin \"@tailwindcss/container-queries\";\n```\n\n```html\n<div class=\"@container\">\n  <div class=\"flex flex-col @md:flex-row @lg:gap-8\">\n    <!-- Responds to container size, not viewport -->\n  </div>\n</div>\n```\n\n### Max-Width Breakpoints\n\nTarget screens below a certain size:\n\n```html\n<!-- Only on screens smaller than md (< 768px) -->\n<div class=\"md:hidden\">Small screens only</div>\n\n<!-- Custom max-width media query -->\n<div class=\"[@media(max-width:600px)]:text-sm\">\n  Custom max-width\n</div>\n```\n\n## Dark Mode\n\n### Strategy: Media (Default)\n\nDark mode follows the user's operating system preference using `prefers-color-scheme`:\n\n```css\n@import \"tailwindcss\";\n/* No additional configuration needed */\n```\n\n```html\n<div class=\"bg-white dark:bg-gray-900\">\n  <h1 class=\"text-gray-900 dark:text-white\">Title</h1>\n  <p class=\"text-gray-600 dark:text-gray-300\">Content</p>\n</div>\n```\n\n### Strategy: Selector (Manual Toggle)\n\nControl dark mode with a CSS class:\n\n```css\n@import \"tailwindcss\";\n\n@custom-variant dark (&:where(.dark, .dark *));\n```\n\n```html\n<!-- Add .dark class to html or body to enable dark mode -->\n<html class=\"dark\">\n  <body>\n    <div class=\"bg-white dark:bg-gray-900\">\n      Content\n    </div>\n  </body>\n</html>\n```\n\n### JavaScript Toggle\n\n```javascript\n// Simple toggle\nfunction toggleDarkMode() {\n  document.documentElement.classList.toggle('dark');\n}\n\n// With localStorage persistence\nfunction initDarkMode() {\n  const isDark = localStorage.getItem('darkMode') === 'true' ||\n    (!localStorage.getItem('darkMode') &&\n     window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n  document.documentElement.classList.toggle('dark', isDark);\n}\n\nfunction toggleDarkMode() {\n  const isDark = document.documentElement.classList.toggle('dark');\n  localStorage.setItem('darkMode', isDark);\n}\n\n// Initialize on page load\ninitDarkMode();\n```\n\n### Three-Way Toggle (Light/Dark/System)\n\n```javascript\nconst themes = ['light', 'dark', 'system'];\n\nfunction setTheme(theme) {\n  localStorage.setItem('theme', theme);\n  applyTheme();\n}\n\nfunction applyTheme() {\n  const theme = localStorage.getItem('theme') || 'system';\n  const isDark = theme === 'dark' ||\n    (theme === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches);\n\n  document.documentElement.classList.toggle('dark', isDark);\n}\n\n// Listen for system preference changes\nwindow.matchMedia('(prefers-color-scheme: dark)')\n  .addEventListener('change', () => {\n    if (localStorage.getItem('theme') === 'system') {\n      applyTheme();\n    }\n  });\n\napplyTheme();\n```\n\n### Data Attribute Strategy\n\n```css\n@custom-variant dark (&:where([data-theme=\"dark\"], [data-theme=\"dark\"] *));\n```\n\n```html\n<html data-theme=\"dark\">\n  <body>\n    <div class=\"bg-white dark:bg-gray-900\">Content</div>\n  </body>\n</html>\n```\n\n### Dark Mode with Next.js (next-themes)\n\n```bash\nnpm install next-themes\n```\n\n```jsx\n// app/providers.tsx\n'use client';\n\nimport { ThemeProvider } from 'next-themes';\n\nexport function Providers({ children }) {\n  return (\n    <ThemeProvider attribute=\"class\" defaultTheme=\"system\">\n      {children}\n    </ThemeProvider>\n  );\n}\n```\n\n```jsx\n// app/layout.tsx\nimport { Providers } from './providers';\n\nexport default function RootLayout({ children }) {\n  return (\n    <html lang=\"en\" suppressHydrationWarning>\n      <body>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}\n```\n\n```jsx\n// components/ThemeToggle.tsx\n'use client';\n\nimport { useTheme } from 'next-themes';\n\nexport function ThemeToggle() {\n  const { theme, setTheme } = useTheme();\n\n  return (\n    <button onClick={() => setTheme(theme === 'dark' ? 'light' : 'dark')}>\n      Toggle Theme\n    </button>\n  );\n}\n```\n\n### Dark Mode Color Palette\n\n```html\n<!-- Text colors -->\n<p class=\"text-gray-900 dark:text-gray-100\">Primary text</p>\n<p class=\"text-gray-600 dark:text-gray-400\">Secondary text</p>\n<p class=\"text-gray-400 dark:text-gray-500\">Muted text</p>\n\n<!-- Background colors -->\n<div class=\"bg-white dark:bg-gray-900\">Page background</div>\n<div class=\"bg-gray-50 dark:bg-gray-800\">Card background</div>\n<div class=\"bg-gray-100 dark:bg-gray-700\">Elevated background</div>\n\n<!-- Border colors -->\n<div class=\"border border-gray-200 dark:border-gray-700\">Bordered element</div>\n\n<!-- Interactive elements -->\n<button class=\"bg-blue-500 hover:bg-blue-600 dark:bg-blue-600 dark:hover:bg-blue-700\">\n  Button\n</button>\n```\n\n### Dark Mode with CSS Variables\n\n```css\n@theme {\n  /* Light mode colors (default) */\n  --color-bg-primary: oklch(1 0 0);\n  --color-bg-secondary: oklch(0.98 0 0);\n  --color-text-primary: oklch(0.15 0 0);\n  --color-text-secondary: oklch(0.4 0 0);\n}\n\n/* Dark mode overrides */\n@media (prefers-color-scheme: dark) {\n  :root {\n    --color-bg-primary: oklch(0.15 0 0);\n    --color-bg-secondary: oklch(0.2 0 0);\n    --color-text-primary: oklch(0.95 0 0);\n    --color-text-secondary: oklch(0.7 0 0);\n  }\n}\n```\n\n```html\n<div class=\"bg-[var(--color-bg-primary)] text-[var(--color-text-primary)]\">\n  Semantic colors\n</div>\n```\n\n### Typography Plugin Dark Mode\n\n```html\n<article class=\"prose dark:prose-invert\">\n  <!-- Markdown content automatically adapts to dark mode -->\n</article>\n```\n\n## Combining Responsive and Dark Mode\n\n```html\n<!-- Different layouts AND colors based on screen size and theme -->\n<div class=\"\n  grid grid-cols-1 md:grid-cols-2\n  bg-white dark:bg-gray-900\n  p-4 md:p-8\n  text-gray-900 dark:text-white\n\">\n  <div class=\"hidden dark:md:block\">\n    Only visible on md+ screens in dark mode\n  </div>\n</div>\n```\n\n## Best Practices\n\n### 1. Start Mobile, Then Enhance\n\n```html\n<!-- Good: Mobile-first -->\n<div class=\"text-sm md:text-base lg:text-lg\">\n\n<!-- Avoid: Desktop-first thinking -->\n<div class=\"lg:text-lg md:text-base text-sm\">\n```\n\n### 2. Use Semantic Dark Mode Colors\n\n```css\n@theme {\n  /* Instead of raw colors, use semantic names */\n  --color-surface: oklch(1 0 0);\n  --color-surface-dark: oklch(0.15 0 0);\n  --color-on-surface: oklch(0.1 0 0);\n  --color-on-surface-dark: oklch(0.95 0 0);\n}\n```\n\n### 3. Test All Breakpoints\n\nUse the debug-screens plugin during development:\n\n```bash\nnpm install -D @tailwindcss/debug-screens\n```\n\n```css\n@plugin \"@tailwindcss/debug-screens\";\n```\n\n### 4. Reduce Repetition with Components\n\n```css\n/* components.css */\n@layer components {\n  .card {\n    @apply bg-white dark:bg-gray-800 rounded-lg p-6 shadow-sm;\n  }\n\n  .section {\n    @apply py-12 md:py-16 lg:py-24;\n  }\n}\n```\n\n### 5. Consider Color Contrast\n\nEnsure sufficient contrast in both light and dark modes:\n\n```html\n<!-- Good contrast in both modes -->\n<button class=\"\n  bg-blue-600 text-white\n  dark:bg-blue-500 dark:text-white\n  hover:bg-blue-700 dark:hover:bg-blue-400\n\">\n  Action\n</button>\n```"
              }
            ]
          },
          {
            "name": "react-master",
            "description": "Comprehensive React 19 expertise system covering Server Components, Client Components, hooks, state management, patterns, and testing. PROACTIVELY activate for: (1) ANY React task, (2) React 19 Server/Client Components, (3) Server Actions with 'use server', (4) React hooks (useState, useEffect, useRef, useMemo, useCallback, useTransition, useDeferredValue, useActionState, useOptimistic, useFormStatus), (5) State management (Context, Zustand, Jotai, TanStack Query, SWR), (6) Component patterns (compound, render props, HOC, custom hooks), (7) TypeScript with React, (8) Form handling (React Hook Form, Zod), (9) Performance optimization (React.memo, code splitting, virtualization), (10) Testing with Vitest/Jest and React Testing Library. Provides: React 19 complete features, use() hook for promises, Server Components architecture, Client Component patterns, all hooks with TypeScript, state management solutions, component patterns, forms and validation, performance optimization techniques, comprehensive testing patterns.",
            "source": "./plugins/react-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install react-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/react-component",
                "description": "Create a new React component following best practices for React 19",
                "path": "plugins/react-master/commands/react-component.md",
                "frontmatter": {
                  "name": "react-component",
                  "description": "Create a new React component following best practices for React 19",
                  "argument-hint": "ComponentName [with props: prop1, prop2] [--client] [--server]"
                },
                "content": "# Generate React Component\n\nCreate a new React component following best practices for React 19.\n\n## Arguments\n- `$ARGUMENTS` - Component name and optional specifications (e.g., \"UserCard with props for name, email, avatar\" or \"ProductList --client\")\n\n## Instructions\n\nCreate a React component based on the provided name and specifications:\n\n1. **Analyze Requirements**\n   - Parse component name from `$ARGUMENTS`\n   - Identify required props and their types\n   - Determine if it should be a Server or Client Component:\n     - `--client` flag: Force Client Component\n     - `--server` flag: Force Server Component\n     - No flag: Auto-detect based on needs\n   - Check for existing patterns in the codebase\n\n2. **Component Structure**\n   - Use function component syntax\n   - Add proper TypeScript interface for props\n   - Include 'use client' directive only if needed (useState, useEffect, event handlers)\n   - Follow project naming conventions\n\n3. **Implementation Checklist**\n   - [ ] Proper prop types with TypeScript\n   - [ ] Destructured props with defaults where appropriate\n   - [ ] Proper accessibility attributes (aria-*, role, etc.)\n   - [ ] Error boundaries if needed\n   - [ ] Loading/error states if applicable\n   - [ ] Proper key usage for lists\n\n4. **File Location**\n   - Check project structure for component directories\n   - Place in appropriate folder (components/, features/, etc.)\n   - Use consistent file naming (PascalCase.tsx)\n\n## Example Output\n\n```tsx\n// components/UserCard.tsx\ninterface UserCardProps {\n  name: string;\n  email: string;\n  avatar?: string;\n  onEdit?: (id: string) => void;\n}\n\nexport function UserCard({ name, email, avatar, onEdit }: UserCardProps) {\n  return (\n    <article className=\"user-card\">\n      {avatar && <img src={avatar} alt={`${name}'s avatar`} />}\n      <h3>{name}</h3>\n      <p>{email}</p>\n      {onEdit && (\n        <button onClick={() => onEdit(email)} aria-label={`Edit ${name}`}>\n          Edit\n        </button>\n      )}\n    </article>\n  );\n}\n```\n\n## Client Component Example\n\n```tsx\n// components/Counter.tsx\n'use client';\n\nimport { useState } from 'react';\n\ninterface CounterProps {\n  initialCount?: number;\n  step?: number;\n  onChange?: (count: number) => void;\n}\n\nexport function Counter({ initialCount = 0, step = 1, onChange }: CounterProps) {\n  const [count, setCount] = useState(initialCount);\n\n  const handleIncrement = () => {\n    const newCount = count + step;\n    setCount(newCount);\n    onChange?.(newCount);\n  };\n\n  return (\n    <div className=\"counter\">\n      <span aria-live=\"polite\">{count}</span>\n      <button onClick={handleIncrement} aria-label=\"Increment count\">\n        +{step}\n      </button>\n    </div>\n  );\n}\n```\n\n## When to Use 'use client'\n\nOnly add 'use client' when the component:\n- Uses hooks like useState, useEffect, useRef\n- Has event handlers (onClick, onChange, etc.)\n- Uses browser-only APIs (window, document, localStorage)\n- Needs to be interactive\n\nServer Components (no 'use client') when:\n- Only displaying data\n- Fetching data directly\n- No interactivity needed\n- Rendering static content"
              },
              {
                "name": "/react-hook",
                "description": "Create a custom React hook following best practices",
                "path": "plugins/react-master/commands/react-hook.md",
                "frontmatter": {
                  "name": "react-hook",
                  "description": "Create a custom React hook following best practices",
                  "argument-hint": "useHookName - description of functionality"
                },
                "content": "# Generate React Custom Hook\n\nCreate a custom React hook following best practices.\n\n## Arguments\n- `$ARGUMENTS` - Hook name and description (e.g., \"useDebounce - debounce a value with delay\" or \"useLocalStorage - persist state to localStorage\")\n\n## Instructions\n\nCreate a custom hook based on the provided name and description:\n\n1. **Analyze Requirements**\n   - Parse hook name from `$ARGUMENTS` (must start with \"use\")\n   - Identify the core functionality\n   - Determine parameters and return types\n   - Check for existing similar hooks in the codebase\n\n2. **Hook Structure**\n   - Name must start with \"use\"\n   - Proper TypeScript generics if needed\n   - Clear parameter and return type definitions\n   - JSDoc comments for documentation\n\n3. **Implementation Checklist**\n   - [ ] Follows Rules of Hooks\n   - [ ] Proper dependency arrays\n   - [ ] Cleanup in useEffect if needed\n   - [ ] Memoization where appropriate\n   - [ ] Handles edge cases\n   - [ ] TypeScript types for all parameters and returns\n   - [ ] SSR-safe (check for `window`/`document`)\n\n4. **File Location**\n   - Place in hooks/ directory\n   - Use consistent naming (useHookName.ts)\n   - Export from index if using barrel exports\n\n## Example Output\n\n```tsx\n// hooks/useDebounce.ts\nimport { useState, useEffect } from 'react';\n\n/**\n * Debounce a value by the specified delay\n * @param value - The value to debounce\n * @param delay - Delay in milliseconds (default: 500)\n * @returns The debounced value\n */\nexport function useDebounce<T>(value: T, delay = 500): T {\n  const [debouncedValue, setDebouncedValue] = useState<T>(value);\n\n  useEffect(() => {\n    const timer = setTimeout(() => {\n      setDebouncedValue(value);\n    }, delay);\n\n    return () => {\n      clearTimeout(timer);\n    };\n  }, [value, delay]);\n\n  return debouncedValue;\n}\n```\n\n## Common Hook Patterns\n\n### State + Actions Pattern\n```tsx\nfunction useCounter(initial = 0) {\n  const [count, setCount] = useState(initial);\n\n  const increment = useCallback(() => setCount(c => c + 1), []);\n  const decrement = useCallback(() => setCount(c => c - 1), []);\n  const reset = useCallback(() => setCount(initial), [initial]);\n\n  return { count, increment, decrement, reset };\n}\n```\n\n### Async Data Pattern\n```tsx\nfunction useFetch<T>(url: string) {\n  const [data, setData] = useState<T | null>(null);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState<Error | null>(null);\n\n  useEffect(() => {\n    const controller = new AbortController();\n\n    fetch(url, { signal: controller.signal })\n      .then(res => res.json())\n      .then(setData)\n      .catch(setError)\n      .finally(() => setLoading(false));\n\n    return () => controller.abort();\n  }, [url]);\n\n  return { data, loading, error };\n}\n```\n\n### Subscription Pattern\n```tsx\nfunction useMediaQuery(query: string) {\n  const [matches, setMatches] = useState(false);\n\n  useEffect(() => {\n    const mediaQuery = window.matchMedia(query);\n    setMatches(mediaQuery.matches);\n\n    const handler = (e: MediaQueryListEvent) => setMatches(e.matches);\n    mediaQuery.addEventListener('change', handler);\n\n    return () => mediaQuery.removeEventListener('change', handler);\n  }, [query]);\n\n  return matches;\n}\n```\n\n### Ref-Based Pattern\n```tsx\nfunction useClickOutside<T extends HTMLElement>(handler: () => void) {\n  const ref = useRef<T>(null);\n\n  useEffect(() => {\n    const listener = (event: MouseEvent | TouchEvent) => {\n      if (!ref.current || ref.current.contains(event.target as Node)) {\n        return;\n      }\n      handler();\n    };\n\n    document.addEventListener('mousedown', listener);\n    document.addEventListener('touchstart', listener);\n\n    return () => {\n      document.removeEventListener('mousedown', listener);\n      document.removeEventListener('touchstart', listener);\n    };\n  }, [handler]);\n\n  return ref;\n}\n```\n\n### Storage Pattern (SSR-Safe)\n```tsx\nfunction useLocalStorage<T>(key: string, initialValue: T) {\n  const [storedValue, setStoredValue] = useState<T>(() => {\n    if (typeof window === 'undefined') {\n      return initialValue;\n    }\n    try {\n      const item = window.localStorage.getItem(key);\n      return item ? JSON.parse(item) : initialValue;\n    } catch {\n      return initialValue;\n    }\n  });\n\n  const setValue = useCallback((value: T | ((prev: T) => T)) => {\n    setStoredValue((prev) => {\n      const newValue = value instanceof Function ? value(prev) : value;\n      window.localStorage.setItem(key, JSON.stringify(newValue));\n      return newValue;\n    });\n  }, [key]);\n\n  return [storedValue, setValue] as const;\n}\n```\n\n## Rules of Hooks Reminder\n\n1. **Only call hooks at the top level** - Never inside loops, conditions, or nested functions\n2. **Only call hooks from React functions** - Function components or custom hooks\n3. **Consistent order** - Hooks must be called in the same order every render"
              },
              {
                "name": "/react-optimize",
                "description": "Analyze and optimize a React component for better performance",
                "path": "plugins/react-master/commands/react-optimize.md",
                "frontmatter": {
                  "name": "react-optimize",
                  "description": "Analyze and optimize a React component for better performance",
                  "argument-hint": "ComponentName or path/to/Component.tsx"
                },
                "content": "# Optimize React Component Performance\n\nAnalyze and optimize a React component for better performance.\n\n## Arguments\n- `$ARGUMENTS` - Component name or file path to optimize\n\n## Instructions\n\nAnalyze and optimize the specified React component:\n\n1. **Performance Analysis**\n   - Read the component file specified in `$ARGUMENTS`\n   - Identify unnecessary re-renders\n   - Find expensive computations\n   - Check for missing memoization\n   - Identify bundle size concerns\n\n2. **Common Issues to Check**\n   - Inline object/function creation in JSX\n   - Missing React.memo on pure components\n   - Missing useMemo for expensive calculations\n   - Missing useCallback for callbacks passed to children\n   - Large component bundles (code splitting opportunities)\n   - Missing keys or unstable keys in lists\n\n3. **Optimization Techniques**\n   - Add React.memo for components that render often with same props\n   - Use useMemo for expensive computations\n   - Use useCallback for stable callback references\n   - Implement code splitting with React.lazy\n   - Add virtualization for long lists\n   - Use useTransition for non-urgent updates\n\n4. **Report Format**\n   - List identified issues\n   - Provide optimized code\n   - Explain each optimization\n   - Note any trade-offs\n\n## Analysis Checklist\n\n### Re-render Prevention\n```tsx\n// Before - new object every render\n<Child style={{ color: 'red' }} />\n\n// After - stable reference\nconst style = useMemo(() => ({ color: 'red' }), []);\n<Child style={style} />\n```\n\n### Callback Stability\n```tsx\n// Before - new function every render\n<List onItemClick={(id) => handleClick(id)} />\n\n// After - stable callback\nconst handleItemClick = useCallback((id: string) => {\n  handleClick(id);\n}, [handleClick]);\n<List onItemClick={handleItemClick} />\n```\n\n### Expensive Computation\n```tsx\n// Before - computed every render\nconst sortedItems = items.sort((a, b) => a.name.localeCompare(b.name));\n\n// After - memoized\nconst sortedItems = useMemo(\n  () => [...items].sort((a, b) => a.name.localeCompare(b.name)),\n  [items]\n);\n```\n\n### Component Memoization\n```tsx\n// Before\nfunction ExpensiveList({ items }: Props) { ... }\n\n// After\nconst ExpensiveList = memo(function ExpensiveList({ items }: Props) { ... });\n```\n\n### Code Splitting\n```tsx\n// Before - imported directly\nimport HeavyEditor from './HeavyEditor';\n\n// After - lazy loaded\nconst HeavyEditor = lazy(() => import('./HeavyEditor'));\n\nfunction App() {\n  return (\n    <Suspense fallback={<Loading />}>\n      <HeavyEditor />\n    </Suspense>\n  );\n}\n```\n\n### List Virtualization\n```tsx\n// Before - renders all items\n{items.map(item => <Item key={item.id} {...item} />)}\n\n// After - virtualized\nimport { FixedSizeList } from 'react-window';\n\n<FixedSizeList height={400} itemCount={items.length} itemSize={50}>\n  {({ index, style }) => (\n    <Item style={style} {...items[index]} />\n  )}\n</FixedSizeList>\n```\n\n### Non-Urgent Updates\n```tsx\n// Before - blocks UI\nsetSearchResults(results);\n\n// After - non-blocking\nstartTransition(() => {\n  setSearchResults(results);\n});\n```\n\n## Output Format\n\nAfter analysis, provide:\n\n1. **Issues Found**\n   - List each performance issue with severity (High/Medium/Low)\n\n2. **Optimized Code**\n   - Show the optimized component code\n\n3. **Changes Made**\n   - Explain each optimization applied\n\n4. **Additional Recommendations**\n   - Suggest further improvements if applicable"
              },
              {
                "name": "/react-test",
                "description": "Create comprehensive tests for a React component using React Testing Library",
                "path": "plugins/react-master/commands/react-test.md",
                "frontmatter": {
                  "name": "react-test",
                  "description": "Create comprehensive tests for a React component using React Testing Library",
                  "argument-hint": "ComponentName or path/to/Component.tsx"
                },
                "content": "# Generate React Component Tests\n\nCreate comprehensive tests for a React component using React Testing Library.\n\n## Arguments\n- `$ARGUMENTS` - Component name or file path to test (e.g., \"UserCard\" or \"src/components/UserCard.tsx\")\n\n## Instructions\n\nGenerate tests for the specified React component:\n\n1. **Analyze Component**\n   - Read the component file specified in `$ARGUMENTS`\n   - Identify all props and their types\n   - Find all interactive elements (buttons, inputs, links)\n   - Identify conditional rendering logic\n   - Check for async operations or side effects\n\n2. **Test Categories to Cover**\n   - Rendering tests (component renders without errors)\n   - Props tests (handles different prop values correctly)\n   - Interaction tests (clicks, typing, form submission)\n   - Accessibility tests (roles, labels, keyboard navigation)\n   - Edge cases (empty states, error states, loading states)\n\n3. **Implementation Checklist**\n   - [ ] Import component and testing utilities\n   - [ ] Set up necessary providers/mocks\n   - [ ] Test default render\n   - [ ] Test all prop variations\n   - [ ] Test user interactions\n   - [ ] Test accessibility\n   - [ ] Test error handling\n\n4. **File Location**\n   - Place test file next to component: `Component.test.tsx`\n   - Or in __tests__ folder: `__tests__/Component.test.tsx`\n\n## Example Output\n\n```tsx\n// components/UserCard.test.tsx\nimport { render, screen } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport { describe, it, expect, vi } from 'vitest';\nimport { UserCard } from './UserCard';\n\ndescribe('UserCard', () => {\n  const defaultProps = {\n    name: 'John Doe',\n    email: 'john@example.com',\n  };\n\n  it('renders user name and email', () => {\n    render(<UserCard {...defaultProps} />);\n\n    expect(screen.getByText('John Doe')).toBeInTheDocument();\n    expect(screen.getByText('john@example.com')).toBeInTheDocument();\n  });\n\n  it('renders avatar when provided', () => {\n    render(<UserCard {...defaultProps} avatar=\"/avatar.jpg\" />);\n\n    const avatar = screen.getByRole('img', { name: /john doe's avatar/i });\n    expect(avatar).toHaveAttribute('src', '/avatar.jpg');\n  });\n\n  it('does not render avatar when not provided', () => {\n    render(<UserCard {...defaultProps} />);\n\n    expect(screen.queryByRole('img')).not.toBeInTheDocument();\n  });\n\n  it('calls onEdit when edit button is clicked', async () => {\n    const user = userEvent.setup();\n    const handleEdit = vi.fn();\n\n    render(<UserCard {...defaultProps} onEdit={handleEdit} />);\n\n    await user.click(screen.getByRole('button', { name: /edit john doe/i }));\n\n    expect(handleEdit).toHaveBeenCalledWith('john@example.com');\n  });\n\n  it('does not render edit button when onEdit is not provided', () => {\n    render(<UserCard {...defaultProps} />);\n\n    expect(screen.queryByRole('button')).not.toBeInTheDocument();\n  });\n});\n```\n\n## Testing Patterns\n\n### Testing with Providers\n```tsx\nfunction renderWithProviders(ui: React.ReactElement) {\n  return render(\n    <QueryClientProvider client={new QueryClient()}>\n      <ThemeProvider>\n        {ui}\n      </ThemeProvider>\n    </QueryClientProvider>\n  );\n}\n```\n\n### Testing Async Components\n```tsx\nit('loads and displays data', async () => {\n  render(<UserList />);\n\n  expect(screen.getByText('Loading...')).toBeInTheDocument();\n\n  await waitFor(() => {\n    expect(screen.getByText('John Doe')).toBeInTheDocument();\n  });\n});\n```\n\n### Testing Forms\n```tsx\nit('submits form with entered data', async () => {\n  const user = userEvent.setup();\n  const handleSubmit = vi.fn();\n\n  render(<ContactForm onSubmit={handleSubmit} />);\n\n  await user.type(screen.getByLabelText('Name'), 'John');\n  await user.type(screen.getByLabelText('Email'), 'john@example.com');\n  await user.click(screen.getByRole('button', { name: /submit/i }));\n\n  expect(handleSubmit).toHaveBeenCalledWith({\n    name: 'John',\n    email: 'john@example.com',\n  });\n});\n```\n\n### Testing Custom Hooks\n```tsx\nimport { renderHook, act } from '@testing-library/react';\n\nit('increments counter', () => {\n  const { result } = renderHook(() => useCounter());\n\n  act(() => {\n    result.current.increment();\n  });\n\n  expect(result.current.count).toBe(1);\n});\n```\n\n## Query Priority (Best Practices)\n\n1. **Accessible queries (preferred)**\n   - `getByRole` - Most semantic\n   - `getByLabelText` - Form elements\n   - `getByPlaceholderText` - Input placeholders\n   - `getByText` - Non-interactive elements\n   - `getByDisplayValue` - Current input value\n\n2. **Semantic queries**\n   - `getByAltText` - Images\n   - `getByTitle` - Title attribute\n\n3. **Test IDs (last resort)**\n   - `getByTestId` - When no semantic option exists\n\n## Mocking Best Practices\n\n```tsx\n// Mock fetch\nglobal.fetch = vi.fn().mockResolvedValue({\n  ok: true,\n  json: () => Promise.resolve({ data: 'test' }),\n});\n\n// Mock modules\nvi.mock('@/lib/api', () => ({\n  fetchUsers: vi.fn(),\n}));\n\n// Mock router\nvi.mock('next/navigation', () => ({\n  useRouter: () => ({ push: vi.fn() }),\n  useParams: () => ({ id: '123' }),\n}));\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "nextjs-master",
            "description": "Comprehensive Next.js 15 expertise system covering App Router, Server Components, Server Actions, caching, middleware, authentication, and deployment. PROACTIVELY activate for: (1) ANY Next.js task, (2) App Router (layouts, pages, loading, error), (3) Server Components and Client Components, (4) Server Actions for mutations, (5) Data fetching and caching strategies, (6) Route Handlers (API routes), (7) Middleware for auth/i18n/redirects, (8) Dynamic and parallel routes, (9) Authentication with NextAuth.js, (10) Deployment (Vercel, Docker, static export). Provides: Next.js 15 complete features, App Router file conventions, Server/Client Component patterns, Server Actions with useActionState, caching (Data Cache, Full Route Cache, Router Cache), revalidatePath/revalidateTag, middleware patterns, advanced routing (parallel, intercepting), NextAuth.js authentication, deployment strategies.",
            "source": "./plugins/nextjs-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Claude Marketplace"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install nextjs-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/nextjs-api-route",
                "description": "Create a Next.js Route Handler with proper HTTP methods and error handling",
                "path": "plugins/nextjs-master/commands/nextjs-api-route.md",
                "frontmatter": {
                  "description": "Create a Next.js Route Handler with proper HTTP methods and error handling",
                  "argument-hint": "API route path and methods (e.g., 'posts with GET,POST', 'users/[id] with CRUD')",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# Generate Next.js API Route\n\nCreate a Next.js Route Handler with proper HTTP methods and error handling.\n\n## Arguments\n- `$ARGUMENTS` - API route path and methods (e.g., \"posts with GET, POST, PUT, DELETE\")\n\n## Instructions\n\nCreate a Route Handler based on the provided path and methods:\n\n1. **Analyze Requirements**\n   - Parse route path from `$ARGUMENTS`\n   - Identify required HTTP methods\n   - Determine if route is dynamic ([id], [slug])\n   - Check if authentication is needed\n\n2. **Implementation**\n   - Create route.ts in app/api directory\n   - Implement requested HTTP methods\n   - Add proper TypeScript types\n   - Include error handling\n\n3. **Best Practices**\n   - Validate input data\n   - Return appropriate status codes\n   - Handle errors gracefully\n   - Add proper headers when needed\n\n## Example Output\n\n### Basic CRUD Route Handler\n```tsx\n// app/api/posts/route.ts\nimport { NextResponse } from 'next/server';\nimport { z } from 'zod';\n\nconst PostSchema = z.object({\n  title: z.string().min(1).max(200),\n  content: z.string().min(1),\n  published: z.boolean().optional(),\n});\n\nexport async function GET(request: Request) {\n  try {\n    const { searchParams } = new URL(request.url);\n    const page = parseInt(searchParams.get('page') || '1');\n    const limit = parseInt(searchParams.get('limit') || '10');\n\n    const posts = await db.posts.findMany({\n      skip: (page - 1) * limit,\n      take: limit,\n      orderBy: { createdAt: 'desc' },\n    });\n\n    const total = await db.posts.count();\n\n    return NextResponse.json({\n      data: posts,\n      pagination: {\n        page,\n        limit,\n        total,\n        totalPages: Math.ceil(total / limit),\n      },\n    });\n  } catch (error) {\n    console.error('GET /api/posts error:', error);\n    return NextResponse.json(\n      { error: 'Failed to fetch posts' },\n      { status: 500 }\n    );\n  }\n}\n\nexport async function POST(request: Request) {\n  try {\n    const body = await request.json();\n    const validatedData = PostSchema.parse(body);\n\n    const post = await db.posts.create({\n      data: validatedData,\n    });\n\n    return NextResponse.json(post, { status: 201 });\n  } catch (error) {\n    if (error instanceof z.ZodError) {\n      return NextResponse.json(\n        { error: 'Validation failed', details: error.errors },\n        { status: 400 }\n      );\n    }\n\n    console.error('POST /api/posts error:', error);\n    return NextResponse.json(\n      { error: 'Failed to create post' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n### Dynamic Route Handler (Next.js 16 - Async Params)\n```tsx\n// app/api/posts/[id]/route.ts\nimport { NextResponse } from 'next/server';\n\ninterface RouteContext {\n  params: Promise<{ id: string }>;\n}\n\nexport async function GET(request: Request, context: RouteContext) {\n  try {\n    const { id } = await context.params;\n\n    const post = await db.posts.findUnique({\n      where: { id },\n    });\n\n    if (!post) {\n      return NextResponse.json(\n        { error: 'Post not found' },\n        { status: 404 }\n      );\n    }\n\n    return NextResponse.json(post);\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to fetch post' },\n      { status: 500 }\n    );\n  }\n}\n\nexport async function PUT(request: Request, context: RouteContext) {\n  try {\n    const { id } = await context.params;\n    const body = await request.json();\n\n    const post = await db.posts.update({\n      where: { id },\n      data: body,\n    });\n\n    return NextResponse.json(post);\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to update post' },\n      { status: 500 }\n    );\n  }\n}\n\nexport async function DELETE(request: Request, context: RouteContext) {\n  try {\n    const { id } = await context.params;\n\n    await db.posts.delete({\n      where: { id },\n    });\n\n    return new NextResponse(null, { status: 204 });\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to delete post' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n### Authenticated Route\n```tsx\n// app/api/user/route.ts\nimport { NextResponse } from 'next/server';\nimport { auth } from '@/auth';\n\nexport async function GET() {\n  const session = await auth();\n\n  if (!session) {\n    return NextResponse.json(\n      { error: 'Unauthorized' },\n      { status: 401 }\n    );\n  }\n\n  const user = await db.users.findUnique({\n    where: { id: session.user.id },\n    select: {\n      id: true,\n      name: true,\n      email: true,\n      role: true,\n    },\n  });\n\n  return NextResponse.json(user);\n}\n```\n\n### Streaming Response\n```tsx\n// app/api/stream/route.ts\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const encoder = new TextEncoder();\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      for (let i = 0; i < 10; i++) {\n        const data = JSON.stringify({ count: i, timestamp: Date.now() });\n        controller.enqueue(encoder.encode(`data: ${data}\\n\\n`));\n        await new Promise(resolve => setTimeout(resolve, 1000));\n      }\n      controller.close();\n    },\n  });\n\n  return new NextResponse(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive',\n    },\n  });\n}\n```\n\n### Route with Cache Headers\n```tsx\n// app/api/products/route.ts\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const products = await db.products.findMany();\n\n  return NextResponse.json(products, {\n    headers: {\n      'Cache-Control': 'public, s-maxage=60, stale-while-revalidate=300',\n    },\n  });\n}\n```\n\n### File Upload Route\n```tsx\n// app/api/upload/route.ts\nimport { NextResponse } from 'next/server';\nimport { writeFile } from 'fs/promises';\nimport { join } from 'path';\n\nexport async function POST(request: Request) {\n  try {\n    const formData = await request.formData();\n    const file = formData.get('file') as File;\n\n    if (!file) {\n      return NextResponse.json(\n        { error: 'No file provided' },\n        { status: 400 }\n      );\n    }\n\n    const bytes = await file.arrayBuffer();\n    const buffer = Buffer.from(bytes);\n\n    const filename = `${Date.now()}-${file.name}`;\n    const path = join(process.cwd(), 'public/uploads', filename);\n\n    await writeFile(path, buffer);\n\n    return NextResponse.json({\n      url: `/uploads/${filename}`,\n      filename,\n    });\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Upload failed' },\n      { status: 500 }\n    );\n  }\n}\n\nexport const config = {\n  api: {\n    bodyParser: false,\n  },\n};\n```"
              },
              {
                "name": "/nextjs-middleware",
                "description": "Create Next.js middleware or proxy for authentication, redirects, or headers",
                "path": "plugins/nextjs-master/commands/nextjs-middleware.md",
                "frontmatter": {
                  "description": "Create Next.js middleware or proxy for authentication, redirects, or headers",
                  "argument-hint": "Middleware purpose (e.g., 'auth for dashboard', 'i18n', 'rate-limiting')",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# Generate Next.js Middleware\n\nCreate middleware with common patterns for authentication, redirects, or headers.\n\n## Arguments\n- `$ARGUMENTS` - Middleware purpose (e.g., \"authentication for dashboard routes\" or \"internationalization\")\n\n## Instructions\n\nCreate middleware based on the provided purpose:\n\n1. **Analyze Requirements**\n   - Parse middleware purpose from `$ARGUMENTS`\n   - Identify routes to protect/modify\n   - Determine required logic\n\n2. **Implementation**\n   - Create middleware.ts (Edge) or proxy.ts (Node.js - Next.js 16)\n   - Configure matcher for target routes\n   - Implement required logic\n\n3. **Common Patterns**\n   - Authentication protection\n   - Internationalization\n   - Redirects/rewrites\n   - Custom headers\n   - Rate limiting\n\n## Middleware vs Proxy (Next.js 16)\n\n| Use Case | File | Runtime |\n|----------|------|---------|\n| Simple redirects/rewrites | middleware.ts | Edge |\n| Basic auth checks | middleware.ts | Edge |\n| Database queries | proxy.ts | Node.js |\n| Full JWT verification | proxy.ts | Node.js |\n| Complex business logic | proxy.ts | Node.js |\n\n## Example Output\n\n### Authentication Middleware (Edge)\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\nimport { getToken } from 'next-auth/jwt';\n\nconst publicPaths = [\n  '/',\n  '/login',\n  '/register',\n  '/forgot-password',\n  '/api/auth',\n];\n\nconst adminPaths = ['/admin'];\n\nexport async function middleware(request: NextRequest) {\n  const { pathname } = request.nextUrl;\n\n  // Check if path is public\n  const isPublicPath = publicPaths.some(\n    (path) => pathname === path || pathname.startsWith(`${path}/`)\n  );\n\n  if (isPublicPath) {\n    return NextResponse.next();\n  }\n\n  // Get session token\n  const token = await getToken({\n    req: request,\n    secret: process.env.NEXTAUTH_SECRET,\n  });\n\n  // Redirect to login if not authenticated\n  if (!token) {\n    const loginUrl = new URL('/login', request.url);\n    loginUrl.searchParams.set('callbackUrl', pathname);\n    return NextResponse.redirect(loginUrl);\n  }\n\n  // Check admin access\n  const isAdminPath = adminPaths.some((path) => pathname.startsWith(path));\n  if (isAdminPath && token.role !== 'admin') {\n    return NextResponse.redirect(new URL('/unauthorized', request.url));\n  }\n\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: ['/((?!_next/static|_next/image|favicon.ico|public).*)'],\n};\n```\n\n### Node.js Proxy with Database Access (Next.js 16)\n```tsx\n// proxy.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\nimport { verify } from 'jsonwebtoken';\nimport { db } from './lib/db';\n\nexport async function proxy(request: NextRequest) {\n  const { pathname } = request.nextUrl;\n\n  // Skip public paths\n  if (pathname.startsWith('/api/public') || pathname === '/login') {\n    return NextResponse.next();\n  }\n\n  const token = request.cookies.get('auth-token')?.value;\n\n  if (!token) {\n    return NextResponse.redirect(new URL('/login', request.url));\n  }\n\n  try {\n    const decoded = verify(token, process.env.JWT_SECRET!) as { userId: string };\n\n    // Query database for user (only possible in proxy.ts, not middleware.ts)\n    const user = await db.user.findUnique({\n      where: { id: decoded.userId },\n      select: { id: true, role: true, isActive: true },\n    });\n\n    if (!user || !user.isActive) {\n      return NextResponse.redirect(new URL('/login', request.url));\n    }\n\n    // Admin routes check\n    if (pathname.startsWith('/admin') && user.role !== 'admin') {\n      return NextResponse.redirect(new URL('/unauthorized', request.url));\n    }\n\n    // Add user context to headers\n    const response = NextResponse.next();\n    response.headers.set('x-user-id', user.id);\n    response.headers.set('x-user-role', user.role);\n    return response;\n\n  } catch (error) {\n    return NextResponse.redirect(new URL('/login', request.url));\n  }\n}\n\nexport const config = {\n  matcher: ['/((?!_next/static|_next/image|favicon.ico).*)'],\n};\n```\n\n### Internationalization Middleware\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nconst locales = ['en', 'es', 'fr', 'de'];\nconst defaultLocale = 'en';\n\nfunction getLocale(request: NextRequest): string {\n  // Check cookie\n  const cookieLocale = request.cookies.get('NEXT_LOCALE')?.value;\n  if (cookieLocale && locales.includes(cookieLocale)) {\n    return cookieLocale;\n  }\n\n  // Check Accept-Language header\n  const acceptLanguage = request.headers.get('accept-language');\n  if (acceptLanguage) {\n    const preferred = acceptLanguage\n      .split(',')\n      .map((lang) => lang.split(';')[0].trim().split('-')[0])\n      .find((lang) => locales.includes(lang));\n\n    if (preferred) return preferred;\n  }\n\n  return defaultLocale;\n}\n\nexport function middleware(request: NextRequest) {\n  const { pathname } = request.nextUrl;\n\n  // Check if pathname has locale\n  const pathnameHasLocale = locales.some(\n    (locale) => pathname.startsWith(`/${locale}/`) || pathname === `/${locale}`\n  );\n\n  if (pathnameHasLocale) {\n    return NextResponse.next();\n  }\n\n  // Redirect to localized path\n  const locale = getLocale(request);\n  const url = new URL(`/${locale}${pathname}`, request.url);\n\n  return NextResponse.redirect(url);\n}\n\nexport const config = {\n  matcher: ['/((?!api|_next/static|_next/image|favicon.ico).*)'],\n};\n```\n\n### Security Headers Middleware\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport function middleware(request: NextRequest) {\n  const response = NextResponse.next();\n\n  // Security headers\n  response.headers.set('X-Frame-Options', 'DENY');\n  response.headers.set('X-Content-Type-Options', 'nosniff');\n  response.headers.set('X-XSS-Protection', '1; mode=block');\n  response.headers.set('Referrer-Policy', 'strict-origin-when-cross-origin');\n  response.headers.set(\n    'Permissions-Policy',\n    'camera=(), microphone=(), geolocation=()'\n  );\n\n  // CSP\n  const csp = [\n    \"default-src 'self'\",\n    \"script-src 'self' 'unsafe-inline' 'unsafe-eval'\",\n    \"style-src 'self' 'unsafe-inline'\",\n    \"img-src 'self' data: https:\",\n    \"font-src 'self'\",\n    \"connect-src 'self'\",\n  ].join('; ');\n\n  response.headers.set('Content-Security-Policy', csp);\n\n  return response;\n}\n\nexport const config = {\n  matcher: ['/((?!api|_next/static|_next/image|favicon.ico).*)'],\n};\n```\n\n### Rate Limiting Middleware\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nconst rateLimit = new Map<string, { count: number; resetTime: number }>();\n\nconst WINDOW_MS = 60 * 1000; // 1 minute\nconst MAX_REQUESTS = 100;\n\nexport function middleware(request: NextRequest) {\n  // Only rate limit API routes\n  if (!request.nextUrl.pathname.startsWith('/api')) {\n    return NextResponse.next();\n  }\n\n  const ip = request.ip || request.headers.get('x-forwarded-for') || 'unknown';\n  const now = Date.now();\n\n  const record = rateLimit.get(ip);\n\n  if (!record || now > record.resetTime) {\n    rateLimit.set(ip, { count: 1, resetTime: now + WINDOW_MS });\n    return NextResponse.next();\n  }\n\n  if (record.count >= MAX_REQUESTS) {\n    return NextResponse.json(\n      { error: 'Too many requests' },\n      {\n        status: 429,\n        headers: {\n          'Retry-After': String(Math.ceil((record.resetTime - now) / 1000)),\n        },\n      }\n    );\n  }\n\n  record.count++;\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: '/api/:path*',\n};\n```\n\n### Geolocation-Based Routing\n```tsx\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nconst EU_COUNTRIES = ['DE', 'FR', 'IT', 'ES', 'NL', 'BE', 'AT', 'PL'];\n\nexport function middleware(request: NextRequest) {\n  const country = request.geo?.country || 'US';\n\n  // Redirect EU users to EU-specific pages\n  if (EU_COUNTRIES.includes(country)) {\n    if (request.nextUrl.pathname === '/pricing') {\n      return NextResponse.rewrite(new URL('/pricing/eu', request.url));\n    }\n\n    if (request.nextUrl.pathname === '/privacy') {\n      return NextResponse.rewrite(new URL('/privacy/gdpr', request.url));\n    }\n  }\n\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: ['/pricing', '/privacy'],\n};\n```"
              },
              {
                "name": "/nextjs-page",
                "description": "Create a new Next.js App Router page with proper structure",
                "path": "plugins/nextjs-master/commands/nextjs-page.md",
                "frontmatter": {
                  "description": "Create a new Next.js App Router page with proper structure",
                  "argument-hint": "Page path (e.g., 'dashboard/settings', 'posts/[slug]', 'products/[[...category]]')",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# Generate Next.js Page\n\nCreate a new Next.js App Router page with proper structure.\n\n## Arguments\n- `$ARGUMENTS` - Page path and optional specifications (e.g., \"dashboard/settings with user data fetching\")\n\n## Instructions\n\nCreate a Next.js page based on the provided path and specifications:\n\n1. **Analyze Requirements**\n   - Parse page path from `$ARGUMENTS`\n   - Determine if page needs data fetching\n   - Check if it requires dynamic routing ([slug], [...slug], [[...slug]])\n   - Identify if page should be static or dynamic\n\n2. **File Structure**\n   - Create page.tsx in correct directory\n   - Add loading.tsx for loading state\n   - Add error.tsx for error handling\n   - Consider layout.tsx if needed\n\n3. **Component Type**\n   - Default to Server Component (no 'use client')\n   - Add 'use client' only if interactive\n   - Use async function for data fetching\n\n4. **Implementation Checklist**\n   - [ ] Proper TypeScript types for params/searchParams (Promise in Next.js 16)\n   - [ ] generateMetadata for SEO\n   - [ ] generateStaticParams if applicable\n   - [ ] Loading and error states\n   - [ ] Proper data fetching pattern\n\n## Example Output\n\n### Static Page\n```tsx\n// app/about/page.tsx\nimport type { Metadata } from 'next';\n\nexport const metadata: Metadata = {\n  title: 'About Us',\n  description: 'Learn more about our company',\n};\n\nexport default function AboutPage() {\n  return (\n    <div>\n      <h1>About Us</h1>\n      <p>Content here...</p>\n    </div>\n  );\n}\n```\n\n### Dynamic Page with Data Fetching (Next.js 16)\n```tsx\n// app/posts/[slug]/page.tsx\nimport { notFound } from 'next/navigation';\nimport type { Metadata } from 'next';\n\ninterface PageProps {\n  params: Promise<{ slug: string }>;\n  searchParams: Promise<{ [key: string]: string | string[] | undefined }>;\n}\n\nexport async function generateMetadata({ params }: PageProps): Promise<Metadata> {\n  const { slug } = await params;\n  const post = await getPost(slug);\n\n  return {\n    title: post?.title || 'Post Not Found',\n    description: post?.excerpt,\n  };\n}\n\nexport async function generateStaticParams() {\n  const posts = await getPosts();\n  return posts.map((post) => ({ slug: post.slug }));\n}\n\nexport default async function PostPage({ params, searchParams }: PageProps) {\n  const { slug } = await params;\n  const { page } = await searchParams;\n\n  const post = await getPost(slug);\n\n  if (!post) {\n    notFound();\n  }\n\n  return (\n    <article>\n      <h1>{post.title}</h1>\n      <div dangerouslySetInnerHTML={{ __html: post.content }} />\n    </article>\n  );\n}\n```\n\n### Loading State\n```tsx\n// app/posts/[slug]/loading.tsx\nexport default function Loading() {\n  return (\n    <div className=\"animate-pulse\">\n      <div className=\"h-8 bg-gray-200 rounded w-3/4 mb-4\" />\n      <div className=\"h-4 bg-gray-200 rounded w-full mb-2\" />\n      <div className=\"h-4 bg-gray-200 rounded w-5/6\" />\n    </div>\n  );\n}\n```\n\n### Error Boundary\n```tsx\n// app/posts/[slug]/error.tsx\n'use client';\n\nexport default function Error({\n  error,\n  reset,\n}: {\n  error: Error;\n  reset: () => void;\n}) {\n  return (\n    <div>\n      <h2>Something went wrong!</h2>\n      <button onClick={() => reset()}>Try again</button>\n    </div>\n  );\n}\n```\n\n### Cached Page with use cache (Next.js 16)\n```tsx\n// app/products/page.tsx\n'use cache'\n\nimport { cacheLife, cacheTag } from 'next/cache';\nimport type { Metadata } from 'next';\n\nexport const metadata: Metadata = {\n  title: 'Products',\n  description: 'Browse our products',\n};\n\nexport default async function ProductsPage() {\n  cacheLife('hours');\n  cacheTag('products');\n\n  const products = await db.products.findMany({\n    orderBy: { createdAt: 'desc' },\n  });\n\n  return (\n    <div>\n      <h1>Products</h1>\n      <ProductGrid products={products} />\n    </div>\n  );\n}\n```"
              },
              {
                "name": "/nextjs-server-action",
                "description": "Create a Next.js Server Action with validation and proper error handling",
                "path": "plugins/nextjs-master/commands/nextjs-server-action.md",
                "frontmatter": {
                  "description": "Create a Next.js Server Action with validation and proper error handling",
                  "argument-hint": "Action name and purpose (e.g., 'createPost with validation', 'deleteUser with auth')",
                  "allowed-tools": [
                    "Read",
                    "Write",
                    "Edit",
                    "Glob",
                    "Grep"
                  ]
                },
                "content": "# Generate Next.js Server Action\n\nCreate a Server Action with validation and proper error handling.\n\n## Arguments\n- `$ARGUMENTS` - Action name and description (e.g., \"createPost with title and content validation\")\n\n## Instructions\n\nCreate a Server Action based on the provided specifications:\n\n1. **Analyze Requirements**\n   - Parse action name from `$ARGUMENTS`\n   - Identify required input fields\n   - Determine validation requirements\n   - Check if revalidation is needed\n\n2. **Implementation**\n   - Create in actions.ts or separate file\n   - Add 'use server' directive\n   - Implement Zod validation\n   - Handle errors properly\n\n3. **Features to Include**\n   - Input validation with Zod\n   - Type-safe return values\n   - Proper error handling\n   - Cache revalidation\n   - Redirect if needed\n\n## Example Output\n\n### Basic Server Action\n```tsx\n// app/actions.ts\n'use server';\n\nimport { revalidatePath } from 'next/cache';\nimport { redirect } from 'next/navigation';\nimport { z } from 'zod';\n\nconst CreatePostSchema = z.object({\n  title: z.string().min(1, 'Title is required').max(200, 'Title too long'),\n  content: z.string().min(1, 'Content is required'),\n  published: z.boolean().default(false),\n});\n\nexport type CreatePostState = {\n  errors?: {\n    title?: string[];\n    content?: string[];\n    published?: string[];\n  };\n  message?: string;\n  success?: boolean;\n};\n\nexport async function createPost(\n  prevState: CreatePostState,\n  formData: FormData\n): Promise<CreatePostState> {\n  // Validate input\n  const validatedFields = CreatePostSchema.safeParse({\n    title: formData.get('title'),\n    content: formData.get('content'),\n    published: formData.get('published') === 'true',\n  });\n\n  if (!validatedFields.success) {\n    return {\n      errors: validatedFields.error.flatten().fieldErrors,\n      message: 'Validation failed',\n    };\n  }\n\n  try {\n    // Create post in database\n    await db.posts.create({\n      data: validatedFields.data,\n    });\n  } catch (error) {\n    return {\n      message: 'Database error: Failed to create post',\n    };\n  }\n\n  // Revalidate and redirect\n  revalidatePath('/posts');\n  redirect('/posts');\n}\n```\n\n### Type-Safe Action with next-safe-action (Recommended 2025+)\n```tsx\n// lib/safe-action.ts\nimport { createSafeActionClient } from 'next-safe-action';\nimport { auth } from '@/auth';\n\nexport const actionClient = createSafeActionClient({\n  handleServerError: (e) => {\n    console.error('Action error:', e);\n    return 'Something went wrong';\n  },\n});\n\nexport const authActionClient = actionClient.use(async ({ next }) => {\n  const session = await auth();\n  if (!session?.user) {\n    throw new Error('Unauthorized');\n  }\n  return next({ ctx: { user: session.user } });\n});\n```\n\n```tsx\n// app/posts/actions.ts\n'use server';\n\nimport { z } from 'zod';\nimport { authActionClient } from '@/lib/safe-action';\nimport { revalidatePath } from 'next/cache';\n\nconst createPostSchema = z.object({\n  title: z.string().min(1, 'Title required').max(200),\n  content: z.string().min(1, 'Content required'),\n  published: z.boolean().default(false),\n});\n\nexport const createPost = authActionClient\n  .schema(createPostSchema)\n  .action(async ({ parsedInput, ctx }) => {\n    const { title, content, published } = parsedInput;\n    const { user } = ctx;\n\n    const post = await db.posts.create({\n      data: {\n        title,\n        content,\n        published,\n        authorId: user.id,\n      },\n    });\n\n    revalidatePath('/posts');\n    return { success: true, postId: post.id };\n  });\n```\n\n### Action with Authentication\n```tsx\n// app/actions.ts\n'use server';\n\nimport { auth } from '@/auth';\nimport { revalidateTag } from 'next/cache';\n\nexport async function deletePost(postId: string) {\n  const session = await auth();\n\n  if (!session) {\n    return { error: 'Not authenticated' };\n  }\n\n  const post = await db.posts.findUnique({\n    where: { id: postId },\n  });\n\n  if (!post) {\n    return { error: 'Post not found' };\n  }\n\n  if (post.authorId !== session.user.id && session.user.role !== 'admin') {\n    return { error: 'Not authorized' };\n  }\n\n  try {\n    await db.posts.delete({ where: { id: postId } });\n    revalidateTag('posts');\n    return { success: true };\n  } catch (error) {\n    return { error: 'Failed to delete post' };\n  }\n}\n```\n\n### Action with Bound Arguments\n```tsx\n// app/actions.ts\n'use server';\n\nexport async function updatePostStatus(\n  postId: string,\n  status: 'draft' | 'published',\n  formData: FormData\n) {\n  await db.posts.update({\n    where: { id: postId },\n    data: { status },\n  });\n\n  revalidatePath(`/posts/${postId}`);\n  return { success: true };\n}\n```\n\n```tsx\n// components/PublishButton.tsx\n'use client';\n\nimport { updatePostStatus } from '@/app/actions';\n\nexport function PublishButton({ postId }: { postId: string }) {\n  const publishPost = updatePostStatus.bind(null, postId, 'published');\n\n  return (\n    <form action={publishPost}>\n      <button type=\"submit\">Publish</button>\n    </form>\n  );\n}\n```\n\n### Using the Action in a Form\n```tsx\n// app/posts/new/page.tsx\n'use client';\n\nimport { useActionState } from 'react';\nimport { createPost, CreatePostState } from '@/app/actions';\n\nconst initialState: CreatePostState = {};\n\nexport default function NewPostPage() {\n  const [state, formAction, isPending] = useActionState(createPost, initialState);\n\n  return (\n    <form action={formAction}>\n      <div>\n        <label htmlFor=\"title\">Title</label>\n        <input id=\"title\" name=\"title\" required />\n        {state.errors?.title && (\n          <p className=\"error\">{state.errors.title[0]}</p>\n        )}\n      </div>\n\n      <div>\n        <label htmlFor=\"content\">Content</label>\n        <textarea id=\"content\" name=\"content\" required />\n        {state.errors?.content && (\n          <p className=\"error\">{state.errors.content[0]}</p>\n        )}\n      </div>\n\n      {state.message && <p className=\"message\">{state.message}</p>}\n\n      <button type=\"submit\" disabled={isPending}>\n        {isPending ? 'Creating...' : 'Create Post'}\n      </button>\n    </form>\n  );\n}\n```\n\n### Using with next-safe-action Hook\n```tsx\n// components/CreatePostForm.tsx\n'use client';\n\nimport { useAction } from 'next-safe-action/hooks';\nimport { createPost } from '@/app/posts/actions';\n\nexport function CreatePostForm() {\n  const { execute, result, isExecuting } = useAction(createPost, {\n    onSuccess: ({ data }) => {\n      toast.success(`Post created: ${data?.postId}`);\n    },\n    onError: ({ error }) => {\n      toast.error(error.serverError || 'Failed to create post');\n    },\n  });\n\n  return (\n    <form action={(formData) => {\n      execute({\n        title: formData.get('title') as string,\n        content: formData.get('content') as string,\n        published: formData.get('published') === 'on',\n      });\n    }}>\n      <input name=\"title\" required />\n      {result.validationErrors?.title && (\n        <span className=\"error\">{result.validationErrors.title[0]}</span>\n      )}\n\n      <textarea name=\"content\" required />\n\n      <button type=\"submit\" disabled={isExecuting}>\n        {isExecuting ? 'Creating...' : 'Create Post'}\n      </button>\n    </form>\n  );\n}\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "python-master",
            "description": "Comprehensive Python 3.13+ expertise system covering modern Python development, async programming, type hints, performance optimization, and cloud deployment. PROACTIVELY activate for: (1) ANY Python task, (2) Python 3.13+ features (free-threading, JIT), (3) asyncio and concurrent programming, (4) Type hints and mypy/pyright, (5) Package management (uv, pip, poetry), (6) Project structure (src layout, pyproject.toml), (7) FastAPI and web development, (8) Pydantic data validation, (9) Testing (pytest, coverage), (10) Cloudflare Workers/Containers Python, (11) GitHub Actions Python workflows, (12) Performance optimization and profiling, (13) Security best practices (OWASP), (14) Ruff linting and formatting. Provides: Python 3.13 free-threading and JIT, asyncio patterns, comprehensive type hints, uv package management, FastAPI production patterns, Pydantic vs dataclass guidance, pytest testing, Cloudflare Python optimization, GitHub Actions caching, memory profiling, security hardening.",
            "source": "./plugins/python-master",
            "category": null,
            "version": "2.3.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install python-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/python-init",
                "description": "Initialize a modern Python project with best practices configuration",
                "path": "plugins/python-master/commands/python-init.md",
                "frontmatter": {
                  "name": "python-init",
                  "description": "Initialize a modern Python project with best practices configuration",
                  "argument-hint": "[project-name] [--fastapi|--cli|--lib] [--minimal]"
                },
                "content": "# Initialize Python Project\n\nInitialize a modern Python project with best practices configuration.\n\n## What This Command Does\n\nCreates a production-ready Python project structure with:\n\n1. **Project Structure** (src layout)\n   ```\n   project/\n    src/\n       package_name/\n           __init__.py\n           main.py\n    tests/\n       conftest.py\n       test_main.py\n    pyproject.toml\n    README.md\n    .gitignore\n   ```\n\n2. **Configuration Files**\n   - `pyproject.toml` with uv, ruff, mypy, pytest settings\n   - `.gitignore` for Python projects\n   - Pre-commit hooks (optional)\n\n3. **Development Tools**\n   - Ruff for linting and formatting\n   - Mypy for type checking\n   - Pytest for testing\n   - uv for dependency management\n\n## Usage\n\nWhen running this command, I will:\n\n1. Ask for project name and description\n2. Create the directory structure\n3. Generate pyproject.toml with modern configuration\n4. Initialize git repository\n5. Create virtual environment with uv\n6. Install development dependencies\n\n## Example pyproject.toml\n\n```toml\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"My Python project\"\nrequires-python = \">=3.11\"\ndependencies = []\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=8.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"ruff>=0.4.0\",\n    \"mypy>=1.9.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.ruff]\ntarget-version = \"py311\"\nline-length = 88\nsrc = [\"src\", \"tests\"]\n\n[tool.ruff.lint]\nselect = [\"E\", \"W\", \"F\", \"I\", \"B\", \"C4\", \"UP\"]\n\n[tool.mypy]\npython_version = \"3.11\"\nstrict = true\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = [\"-ra\", \"-q\", \"--cov=src\"]\n```\n\n## Next Steps\n\nAfter initialization:\n\n```bash\n# Activate environment\nsource .venv/bin/activate  # or just use `uv run`\n\n# Run tests\nuv run pytest\n\n# Check code\nuv run ruff check .\nuv run mypy src\n\n# Add dependencies\nuv add requests fastapi\n```"
              },
              {
                "name": "/python-test",
                "description": "Run tests with pytest and display coverage report",
                "path": "plugins/python-master/commands/python-test.md",
                "frontmatter": {
                  "name": "python-test",
                  "description": "Run tests with pytest and display coverage report",
                  "argument-hint": "[test-path] [-k pattern] [-x] [--cov] [--parallel]"
                },
                "content": "# Run Python Tests\n\nRun tests with pytest and display coverage report.\n\n## What This Command Does\n\n1. Runs pytest with coverage enabled\n2. Shows test results and coverage summary\n3. Identifies uncovered code\n4. Suggests improvements\n\n## Usage\n\nI will execute:\n\n```bash\nuv run pytest -v --cov --cov-report=term-missing\n```\n\nOr if uv is not available:\n\n```bash\npytest -v --cov --cov-report=term-missing\n```\n\n## Options\n\nYou can specify:\n- **Test path**: `tests/unit/test_users.py`\n- **Test pattern**: `-k \"test_create\"`\n- **Parallel**: `-n auto` (with pytest-xdist)\n- **Stop on failure**: `-x`\n- **Show locals**: `-l`\n\n## Coverage Report\n\nThe command displays:\n- Files and their coverage percentage\n- Missing lines that need tests\n- Branch coverage (if enabled)\n\n## Example Output\n\n```\ntests/test_users.py::test_create_user PASSED\ntests/test_users.py::test_get_user PASSED\ntests/test_users.py::test_delete_user PASSED\n\n---------- coverage: platform linux, python 3.12 ----------\nName                    Stmts   Miss  Cover   Missing\n-----------------------------------------------------\nsrc/mypackage/users.py     45      5    89%   23-25, 42-43\nsrc/mypackage/utils.py     12      0   100%\n-----------------------------------------------------\nTOTAL                      57      5    91%\n\n3 passed in 0.45s\n```\n\n## Troubleshooting\n\nIf tests fail, I will:\n1. Analyze the failure message\n2. Identify the root cause\n3. Suggest fixes\n4. Help implement corrections"
              }
            ],
            "skills": []
          },
          {
            "name": "fal-ai-master",
            "description": "Expert fal.ai generative media platform system with 600+ AI models (FLUX.2, GPT-Image 1.5, Veo 3.1, Sora 2, Kling 2.6, Recraft V3), client libraries (@fal-ai/client, fal-client), and serverless deployment. PROACTIVELY activate for: (1) ANY fal.ai task, (2) Image generation (FLUX.2, GPT-Image 1/1.5, SDXL, Recraft V3), (3) FLUX Kontext instruction-based image editing, (4) Video generation (Veo 3/3.1, Sora 2/Pro, Kling 2.6 Pro, LTX-2, Runway Gen-3), (5) Native audio in video (Veo 3, Kling 2.6), (6) Image-to-video animation pipelines, (7) Real-time WebSocket streaming (@fal.realtime.connect), (8) Queue-based execution (fal.subscribe, fal.queue), (9) Serverless deployment (fal.App, machine_type, keep_alive, min/max_concurrency), (10) GPU compute (T4/A10G/A100/H100/H200/B200), (11) JavaScript/TypeScript integration (@fal-ai/client), (12) Python integration (fal-client, fal_client.subscribe), (13) Storage (fal.Volume, fal.storage.upload), (14) Cost optimization and 2025 pricing ($0.025/image to $0.45/sec video). Provides: Complete model catalog, client library patterns, real-time streaming, webhook notifications, rate limiting strategies, error handling, and production deployment patterns.",
            "source": "./plugins/fal-ai-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install fal-ai-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/fal-debug",
                "description": "Diagnose and fix common issues with fal.ai integrations including authentication, API errors, and timeouts",
                "path": "plugins/fal-ai-master/commands/fal-debug.md",
                "frontmatter": {
                  "name": "Debug Integration",
                  "description": "Diagnose and fix common issues with fal.ai integrations including authentication, API errors, and timeouts",
                  "argument-hint": [
                    {
                      "error-type": "auth|rate-limit|timeout|validation|upload|websocket"
                    }
                  ]
                },
                "content": "# Debug fal.ai Integration Issues\n\nDiagnose and fix common issues with fal.ai integrations, including authentication, API errors, timeouts, and deployment problems.\n\n## What This Command Does\n\nHelps troubleshoot fal.ai integration issues by:\n- Identifying common error patterns\n- Providing diagnostic steps\n- Suggesting fixes for specific errors\n- Offering best practices to prevent issues\n\n## Common Issues and Solutions\n\n### 1. Authentication Errors\n\n**Symptoms:**\n- `401 Unauthorized`\n- `Invalid API key`\n- `Authentication required`\n\n**Diagnostic Steps:**\n```bash\n# Check if FAL_KEY is set\necho $FAL_KEY\n\n# Verify key format (should start with specific prefix)\n# Keys typically look like: fal_xxx...\n\n# Test authentication\ncurl -H \"Authorization: Key $FAL_KEY\" https://fal.run/fal-ai/flux/dev\n```\n\n**Solutions:**\n\n```typescript\n// JavaScript - Ensure proper configuration\nimport { fal } from \"@fal-ai/client\";\n\n// Method 1: Environment variable (recommended)\n// Set FAL_KEY in your environment\n// The client automatically reads process.env.FAL_KEY\n\n// Method 2: Explicit configuration\nfal.config({\n  credentials: process.env.FAL_KEY  // Don't hardcode!\n});\n\n// Method 3: Per-request credentials\nconst result = await fal.subscribe(\"fal-ai/flux/dev\", {\n  input: { prompt: \"test\" },\n  credentials: process.env.FAL_KEY\n});\n```\n\n```python\n# Python - Set environment variable\nimport os\nos.environ[\"FAL_KEY\"] = \"your-key\"  # Before importing fal_client\n\n# Or use .env file\n# FAL_KEY=your-key\n```\n\n**Browser/Client-Side Security:**\n```typescript\n// NEVER expose FAL_KEY in browser code!\n// Use a server-side proxy instead\n\n// API Route (Next.js example)\n// app/api/generate/route.ts\nimport { fal } from \"@fal-ai/client\";\n\nfal.config({ credentials: process.env.FAL_KEY });\n\nexport async function POST(request: Request) {\n  const { prompt } = await request.json();\n  const result = await fal.subscribe(\"fal-ai/flux/dev\", {\n    input: { prompt }\n  });\n  return Response.json(result);\n}\n\n// Client component calls your API route, not fal directly\n```\n\n### 2. Rate Limiting (429 Errors)\n\n**Symptoms:**\n- `429 Too Many Requests`\n- `Rate limit exceeded`\n- Requests failing after many successful ones\n\n**Solutions:**\n\n```typescript\n// Implement exponential backoff\nasync function generateWithRetry(\n  prompt: string,\n  maxRetries = 3\n): Promise<any> {\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await fal.subscribe(\"fal-ai/flux/dev\", {\n        input: { prompt }\n      });\n    } catch (error) {\n      if (error.status === 429 && i < maxRetries - 1) {\n        const delay = Math.pow(2, i) * 1000; // 1s, 2s, 4s\n        console.log(`Rate limited, retrying in ${delay}ms`);\n        await new Promise(r => setTimeout(r, delay));\n      } else {\n        throw error;\n      }\n    }\n  }\n}\n\n// Use queue for high-volume\nasync function batchGenerate(prompts: string[]) {\n  const results = [];\n  for (const prompt of prompts) {\n    results.push(await generateWithRetry(prompt));\n    // Add small delay between requests\n    await new Promise(r => setTimeout(r, 100));\n  }\n  return results;\n}\n```\n\n```python\nimport time\nimport fal_client\nfrom fal_client import FalClientError\n\ndef generate_with_retry(prompt: str, max_retries: int = 3):\n    for i in range(max_retries):\n        try:\n            return fal_client.subscribe(\n                \"fal-ai/flux/dev\",\n                arguments={\"prompt\": prompt}\n            )\n        except FalClientError as e:\n            if e.status == 429 and i < max_retries - 1:\n                delay = 2 ** i\n                print(f\"Rate limited, retrying in {delay}s\")\n                time.sleep(delay)\n            else:\n                raise\n```\n\n### 3. Timeout Errors\n\n**Symptoms:**\n- `Request timeout`\n- `Gateway timeout`\n- Long-running requests failing\n\n**Solutions:**\n\n```typescript\n// Use subscribe instead of run for long operations\n// subscribe handles queue automatically\n\nconst result = await fal.subscribe(\"fal-ai/flux/dev\", {\n  input: { prompt: \"test\" },\n  // Optional: configure polling\n  pollInterval: 1000,  // Poll every second\n  logs: true,\n  onQueueUpdate: (update) => {\n    console.log(\"Status:\", update.status);\n    if (update.status === \"IN_PROGRESS\") {\n      console.log(\"Progress:\", update.logs);\n    }\n  }\n});\n\n// For very long operations, use webhooks\nconst result = await fal.subscribe(\"fal-ai/flux/dev\", {\n  input: { prompt: \"test\" },\n  webhookUrl: \"https://your-server.com/webhook\"\n});\n```\n\n```python\n# Use subscribe for long-running tasks\nresult = fal_client.subscribe(\n    \"fal-ai/flux/dev\",\n    arguments={\"prompt\": \"test\"},\n    with_logs=True,\n    on_queue_update=lambda u: print(f\"Status: {u}\")\n)\n\n# Or manual queue management with custom timeout handling\nimport asyncio\n\nasync def generate_with_timeout(prompt: str, timeout: int = 300):\n    handler = await fal_client.submit_async(\n        \"fal-ai/flux/dev\",\n        arguments={\"prompt\": prompt}\n    )\n\n    start = time.time()\n    while time.time() - start < timeout:\n        status = await handler.status_async()\n        if status.status == \"COMPLETED\":\n            return await handler.get_async()\n        await asyncio.sleep(1)\n\n    raise TimeoutError(\"Generation timed out\")\n```\n\n### 4. Validation Errors (400 Bad Request)\n\n**Symptoms:**\n- `400 Bad Request`\n- `Validation error`\n- `Invalid input`\n\n**Common Causes and Fixes:**\n\n```typescript\n// Issue: Invalid image_size\n// Wrong\n{ image_size: \"1920x1080\" }\n// Right\n{ image_size: \"landscape_16_9\" }\n// Or custom dimensions\n{ image_size: { width: 1920, height: 1080 } }\n\n// Issue: Invalid URL for image_url\n// Wrong - local file path\n{ image_url: \"/path/to/image.jpg\" }\n// Right - upload first\nconst url = await fal.storage.upload(file);\n{ image_url: url }\n\n// Issue: Out of range parameters\n// Wrong\n{ num_inference_steps: 100 }  // Max is usually 50\n// Right\n{ num_inference_steps: 50 }\n\n// Issue: Missing required parameters\n// Wrong\n{ image_size: \"square_hd\" }  // Missing prompt\n// Right\n{ prompt: \"A beautiful landscape\", image_size: \"square_hd\" }\n```\n\n**Validate before sending:**\n```typescript\nfunction validateFluxInput(input: any): void {\n  if (!input.prompt || typeof input.prompt !== 'string') {\n    throw new Error(\"prompt is required and must be a string\");\n  }\n  if (input.num_inference_steps && (input.num_inference_steps < 1 || input.num_inference_steps > 50)) {\n    throw new Error(\"num_inference_steps must be between 1 and 50\");\n  }\n  if (input.guidance_scale && (input.guidance_scale < 1 || input.guidance_scale > 20)) {\n    throw new Error(\"guidance_scale must be between 1 and 20\");\n  }\n}\n```\n\n### 5. File Upload Issues\n\n**Symptoms:**\n- `Invalid image URL`\n- `Failed to fetch image`\n- `Unsupported file type`\n\n**Solutions:**\n\n```typescript\n// Upload files to fal CDN before using\nimport { fal } from \"@fal-ai/client\";\n\n// From File object (browser)\nconst file = document.querySelector('input[type=\"file\"]').files[0];\nconst url = await fal.storage.upload(file);\n\n// From Blob\nconst blob = new Blob([data], { type: \"image/png\" });\nconst file = new File([blob], \"image.png\", { type: \"image/png\" });\nconst url = await fal.storage.upload(file);\n\n// From URL (fetch and re-upload)\nconst response = await fetch(\"https://example.com/image.jpg\");\nconst blob = await response.blob();\nconst file = new File([blob], \"image.jpg\", { type: \"image/jpeg\" });\nconst url = await fal.storage.upload(file);\n```\n\n```python\nimport fal_client\n\n# Upload local file\nurl = fal_client.upload_file(\"path/to/image.png\")\n\n# Upload bytes\nwith open(\"image.png\", \"rb\") as f:\n    url = fal_client.upload(f.read(), \"image/png\")\n\n# For small files, use data URL\ndata_url = fal_client.encode_file(\"small_image.png\")\n# Use in request\nresult = fal_client.run(\n    \"fal-ai/flux/dev/image-to-image\",\n    arguments={\"image_url\": data_url, \"prompt\": \"enhance\"}\n)\n```\n\n### 6. Serverless Deployment Errors\n\n**Symptoms:**\n- Deployment fails\n- Container crashes\n- Out of memory errors\n\n**Diagnostic Steps:**\n```bash\n# Check deployment logs\nfal logs <app-id>\n\n# List deployments\nfal list\n\n# Check status\nfal status <app-id>\n```\n\n**Common Fixes:**\n\n```python\n# Issue: Out of memory during model loading\n# Solution: Use appropriate machine type\nclass MyApp(fal.App):\n    machine_type = \"GPU-A100\"  # Upgrade from T4/A10G\n    num_gpus = 1\n\n# Issue: Missing dependencies\n# Solution: List all requirements\nclass MyApp(fal.App):\n    requirements = [\n        \"torch>=2.0.0\",\n        \"transformers>=4.35.0\",\n        \"accelerate\",\n        \"safetensors\",\n        \"sentencepiece\",  # Often forgotten\n    ]\n\n# Issue: Slow cold starts\n# Solution: Use persistent storage and keep_alive\nclass MyApp(fal.App):\n    keep_alive = 600  # 10 minutes\n    min_concurrency = 1  # Always keep one warm\n\n    volumes = {\n        \"/data\": fal.Volume(\"model-cache\")\n    }\n\n    def setup(self):\n        # Models cached in persistent volume\n        self.model = load_model(cache_dir=\"/data/models\")\n\n# Issue: Request timeout\n# Solution: Move heavy work to setup()\nclass MyApp(fal.App):\n    def setup(self):\n        # Load model once, not per request\n        self.model = load_large_model()\n\n    @fal.endpoint(\"/predict\")\n    def predict(self, request):\n        # Fast inference only\n        return self.model(request.input)\n```\n\n### 7. WebSocket/Realtime Issues\n\n**Symptoms:**\n- Connection drops\n- No results received\n- Throttling issues\n\n**Solutions:**\n\n```typescript\n// Proper WebSocket handling\nconst connection = fal.realtime.connect(\"fal-ai/lcm-sd15-i2i\", {\n  connectionKey: \"unique-key-per-user\",  // Important!\n  throttleInterval: 128,  // Adjust based on needs\n\n  onResult: (result) => {\n    console.log(\"Result:\", result);\n  },\n\n  onError: (error) => {\n    console.error(\"WebSocket error:\", error);\n    // Implement reconnection logic\n  },\n\n  onOpen: () => {\n    console.log(\"Connected\");\n  },\n\n  onClose: () => {\n    console.log(\"Disconnected\");\n    // Implement reconnection if needed\n  }\n});\n\n// Clean up on unmount\nuseEffect(() => {\n  return () => {\n    connection.close();\n  };\n}, []);\n\n// Handle connection state\nif (!connection.connected) {\n  // Show reconnecting UI\n}\n```\n\n### 8. Debugging Checklist\n\n```markdown\n## Pre-flight Checks\n\n[ ] FAL_KEY environment variable is set\n[ ] Key has correct permissions for the model\n[ ] Using latest version of client library\n[ ] Network connectivity to fal.ai endpoints\n\n## Request Checks\n\n[ ] Required parameters are provided\n[ ] Parameter values are within valid ranges\n[ ] Image URLs are publicly accessible\n[ ] File uploads completed successfully\n\n## Response Checks\n\n[ ] Checking correct response field (images vs image)\n[ ] Handling all possible status codes\n[ ] Implementing proper error handling\n[ ] Not assuming response structure\n\n## Deployment Checks (Serverless)\n\n[ ] All dependencies listed in requirements\n[ ] Machine type has sufficient memory\n[ ] setup() loads models, not request handlers\n[ ] Secrets are set via fal secrets\n[ ] Volume paths exist before use\n```\n\n## Getting Help\n\n1. **Check Documentation:** https://docs.fal.ai\n2. **Model Explorer:** https://fal.ai/models (see model-specific parameters)\n3. **Discord Community:** https://discord.gg/fal-ai\n4. **GitHub Issues:** https://github.com/fal-ai\n\n**When reporting issues, include:**\n- Full error message and stack trace\n- Request payload (without API key)\n- Client library version\n- Language/runtime version\n- Steps to reproduce"
              },
              {
                "name": "/fal-deploy",
                "description": "Deploy custom ML models to fal.ai's serverless infrastructure with automatic scaling and GPU support",
                "path": "plugins/fal-ai-master/commands/fal-deploy.md",
                "frontmatter": {
                  "name": "Deploy Model",
                  "description": "Deploy custom ML models to fal.ai's serverless infrastructure with automatic scaling and GPU support",
                  "argument-hint": "<app-file.py> [--gpu T4|A10G|A100|H100|H200|B200]"
                },
                "content": "# Deploy Custom Model to fal Serverless\n\nDeploy your own ML models to fal.ai's serverless infrastructure with automatic scaling and GPU support.\n\n## What This Command Does\n\nGuides you through deploying custom models to fal serverless, including:\n- Setting up the fal.App structure\n- Configuring GPU requirements\n- Managing dependencies\n- Handling persistent storage\n- Setting up endpoints\n\n## Steps\n\n1. **Analyze Requirements**\n   - Model type and size\n   - GPU memory needs\n   - Expected concurrency\n   - Storage requirements\n\n2. **Design App Structure**\n   - Machine type selection\n   - Endpoint definitions\n   - Setup/teardown logic\n\n3. **Generate Code**\n   - Complete fal.App class\n   - Deployment commands\n   - Health checks\n\n4. **Deploy and Monitor**\n   - Deployment commands\n   - Log access\n   - Scaling configuration\n\n## Quick Start Template\n\n### Basic Model Deployment\n\n```python\nimport fal\nfrom pydantic import BaseModel\n\nclass PredictRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 100\n\nclass PredictResponse(BaseModel):\n    text: str\n    tokens_used: int\n\nclass MyModel(fal.App):\n    # Infrastructure configuration\n    machine_type = \"GPU-A100\"\n    num_gpus = 1\n    requirements = [\n        \"torch>=2.0.0\",\n        \"transformers>=4.35.0\",\n        \"accelerate>=0.24.0\"\n    ]\n\n    # Scaling configuration\n    keep_alive = 300      # Keep warm for 5 minutes\n    min_concurrency = 0   # Scale to zero when idle\n    max_concurrency = 4   # Max concurrent requests\n\n    def setup(self):\n        \"\"\"Called once when container starts - load model here\"\"\"\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\"model-name\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"model-name\",\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n\n    @fal.endpoint(\"/predict\")\n    def predict(self, request: PredictRequest) -> PredictResponse:\n        \"\"\"Main inference endpoint\"\"\"\n        inputs = self.tokenizer(request.prompt, return_tensors=\"pt\").to(self.device)\n\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=request.max_tokens\n        )\n\n        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return PredictResponse(\n            text=text,\n            tokens_used=len(outputs[0])\n        )\n\n    @fal.endpoint(\"/health\")\n    def health(self):\n        \"\"\"Health check endpoint\"\"\"\n        return {\"status\": \"healthy\", \"gpu\": self.device}\n```\n\n### Deploy Command\n\n```bash\n# Install fal CLI\npip install fal\n\n# Authenticate\nfal auth login\n\n# Deploy\nfal deploy app.py::MyModel\n\n# Deploy with specific configuration\nfal deploy app.py::MyModel \\\n  --machine-type GPU-A100 \\\n  --num-gpus 1 \\\n  --min-concurrency 1 \\\n  --max-concurrency 4\n```\n\n## Machine Types\n\n| Type | GPU | VRAM | Monthly Cost* | Best For |\n|------|-----|------|---------------|----------|\n| `CPU` | None | - | $ | Preprocessing |\n| `GPU-T4` | NVIDIA T4 | 16GB | $$ | Development |\n| `GPU-A10G` | NVIDIA A10G | 24GB | $$$ | Small models |\n| `GPU-A100` | NVIDIA A100 | 40/80GB | $$$$ | Large models |\n| `GPU-H100` | NVIDIA H100 | 80GB | $$$$$ | Cutting edge |\n| `GPU-H200` | NVIDIA H200 | 141GB | $$$$$$ | Very large |\n| `GPU-B200` | NVIDIA B200 | 192GB | $$$$$$$ | Frontier |\n\n*Pricing is per-second for compute used\n\n### GPU Selection Guide\n\n```python\n# Small models (< 7B params)\nmachine_type = \"GPU-T4\"\n\n# Medium models (7B-13B params)\nmachine_type = \"GPU-A10G\"\n\n# Large models (13B-70B params)\nmachine_type = \"GPU-A100\"\nnum_gpus = 1  # or 2 for very large\n\n# Very large models (70B+ params)\nmachine_type = \"GPU-H100\"\nnum_gpus = 2  # or more\n```\n\n## Persistent Storage\n\n```python\nclass ModelWithStorage(fal.App):\n    machine_type = \"GPU-A100\"\n    requirements = [\"torch\", \"transformers\"]\n\n    # Define persistent volume\n    volumes = {\n        \"/data\": fal.Volume(\"model-cache\")\n    }\n\n    def setup(self):\n        import os\n        from transformers import AutoModel\n\n        cache_dir = \"/data/models\"\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Models persist across cold starts\n        self.model = AutoModel.from_pretrained(\n            \"large-model-name\",\n            cache_dir=cache_dir\n        )\n```\n\n## Secrets Management\n\n```bash\n# Set secrets via CLI\nfal secrets set HF_TOKEN=hf_xxx OPENAI_KEY=sk_xxx\n\n# List secrets\nfal secrets list\n\n# Delete secret\nfal secrets delete HF_TOKEN\n```\n\n```python\nimport os\n\nclass SecureApp(fal.App):\n    def setup(self):\n        # Access secrets as environment variables\n        hf_token = os.environ.get(\"HF_TOKEN\")\n\n        from huggingface_hub import login\n        login(token=hf_token)\n\n        # Load gated model\n        self.model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n```\n\n## Advanced Patterns\n\n### Image Generation App\n\n```python\nimport fal\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = None\n    width: int = 1024\n    height: int = 1024\n    steps: int = 28\n    guidance_scale: float = 3.5\n    seed: Optional[int] = None\n\nclass GenerateResponse(BaseModel):\n    image_url: str\n    seed: int\n\nclass ImageGenerator(fal.App):\n    machine_type = \"GPU-A100\"\n    num_gpus = 1\n    requirements = [\n        \"torch\",\n        \"diffusers\",\n        \"transformers\",\n        \"accelerate\",\n        \"safetensors\"\n    ]\n    keep_alive = 600\n    max_concurrency = 2\n\n    volumes = {\n        \"/data\": fal.Volume(\"diffusion-cache\")\n    }\n\n    def setup(self):\n        import torch\n        from diffusers import StableDiffusionXLPipeline\n\n        self.pipe = StableDiffusionXLPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-base-1.0\",\n            torch_dtype=torch.float16,\n            cache_dir=\"/data/models\"\n        ).to(\"cuda\")\n\n        # Optimize for inference\n        self.pipe.enable_model_cpu_offload()\n\n    @fal.endpoint(\"/generate\")\n    def generate(self, request: GenerateRequest) -> GenerateResponse:\n        import random\n\n        seed = request.seed or random.randint(0, 2**32 - 1)\n        generator = torch.Generator(\"cuda\").manual_seed(seed)\n\n        image = self.pipe(\n            prompt=request.prompt,\n            negative_prompt=request.negative_prompt,\n            width=request.width,\n            height=request.height,\n            num_inference_steps=request.steps,\n            guidance_scale=request.guidance_scale,\n            generator=generator\n        ).images[0]\n\n        # Save and upload\n        path = f\"/tmp/output_{seed}.png\"\n        image.save(path)\n        url = fal.upload_file(path)\n\n        return GenerateResponse(image_url=url, seed=seed)\n```\n\n### Streaming Response\n\n```python\nimport fal\nfrom typing import Generator\n\nclass StreamingApp(fal.App):\n    machine_type = \"GPU-A100\"\n    requirements = [\"torch\", \"transformers\"]\n\n    def setup(self):\n        from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n        from threading import Thread\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"model-name\")\n        self.model = AutoModelForCausalLM.from_pretrained(\"model-name\")\n\n    @fal.endpoint(\"/stream\")\n    def stream(self, prompt: str) -> Generator[str, None, None]:\n        \"\"\"Stream tokens as they're generated\"\"\"\n        from transformers import TextIteratorStreamer\n        from threading import Thread\n\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n\n        generation_kwargs = {\n            **inputs,\n            \"streamer\": streamer,\n            \"max_new_tokens\": 256\n        }\n\n        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        for text in streamer:\n            yield text\n```\n\n### Multi-Endpoint App\n\n```python\nimport fal\nfrom pydantic import BaseModel\n\nclass TextRequest(BaseModel):\n    text: str\n\nclass ImageRequest(BaseModel):\n    image_url: str\n\nclass MultiModalApp(fal.App):\n    machine_type = \"GPU-A100\"\n    requirements = [\"torch\", \"transformers\", \"Pillow\"]\n\n    def setup(self):\n        # Load multiple models\n        self.text_model = load_text_model()\n        self.vision_model = load_vision_model()\n\n    @fal.endpoint(\"/analyze-text\")\n    def analyze_text(self, request: TextRequest):\n        return self.text_model(request.text)\n\n    @fal.endpoint(\"/analyze-image\")\n    def analyze_image(self, request: ImageRequest):\n        return self.vision_model(request.image_url)\n\n    @fal.endpoint(\"/health\")\n    def health(self):\n        return {\"status\": \"ok\"}\n```\n\n## Deployment Commands\n\n```bash\n# Deploy application\nfal deploy app.py::MyModel\n\n# View deployment status\nfal list\n\n# View logs\nfal logs <app-id>\n\n# Delete deployment\nfal delete <app-id>\n\n# Update deployment\nfal deploy app.py::MyModel  # Redeploy with changes\n```\n\n## Calling Your Deployed Model\n\n### JavaScript/TypeScript\n\n```typescript\nimport { fal } from \"@fal-ai/client\";\n\nfal.config({ credentials: process.env.FAL_KEY });\n\nconst result = await fal.subscribe(\"your-username/your-app\", {\n  input: {\n    prompt: \"Hello world\",\n    max_tokens: 100\n  }\n});\n```\n\n### Python\n\n```python\nimport fal_client\n\nresult = fal_client.subscribe(\n    \"your-username/your-app\",\n    arguments={\n        \"prompt\": \"Hello world\",\n        \"max_tokens\": 100\n    }\n)\n```\n\n### cURL\n\n```bash\ncurl -X POST \"https://queue.fal.run/your-username/your-app/predict\" \\\n  -H \"Authorization: Key $FAL_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"Hello world\", \"max_tokens\": 100}'\n```\n\n## Best Practices\n\n1. **Model Loading in setup()**\n   - Load models once in setup(), not per request\n   - Use persistent volumes for large model weights\n\n2. **Error Handling**\n   ```python\n   @fal.endpoint(\"/predict\")\n   def predict(self, request: Request):\n       try:\n           result = self.model(request.input)\n           return {\"result\": result}\n       except RuntimeError as e:\n           raise fal.HTTPException(500, str(e))\n       except ValueError as e:\n           raise fal.HTTPException(400, str(e))\n   ```\n\n3. **Resource Cleanup**\n   ```python\n   def teardown(self):\n       \"\"\"Called when container shuts down\"\"\"\n       if hasattr(self, 'model'):\n           del self.model\n       torch.cuda.empty_cache()\n   ```\n\n4. **Concurrency Settings**\n   - `min_concurrency=1` for always-warm endpoints\n   - `max_concurrency` based on GPU memory per request\n   - `keep_alive` to reduce cold starts\n\n5. **Testing Locally**\n   ```bash\n   # Run locally before deploying\n   fal run app.py::MyModel\n\n   # Test endpoint\n   curl http://localhost:8000/predict -d '{\"prompt\": \"test\"}'\n   ```"
              },
              {
                "name": "/fal-generate-image",
                "description": "Generate AI images using fal.ai models like FLUX.2, GPT-Image 1, Recraft, and more",
                "path": "plugins/fal-ai-master/commands/fal-generate-image.md",
                "frontmatter": {
                  "name": "Generate Image",
                  "description": "Generate AI images using fal.ai models like FLUX.2, GPT-Image 1, Recraft, and more",
                  "argument-hint": "<prompt> [--model flux|gpt-image|recraft|sdxl]"
                },
                "content": "# Generate Image with fal.ai\n\nGenerate AI images using fal.ai models like FLUX, Stable Diffusion, and more.\n\n## What This Command Does\n\nGuides you through generating images with fal.ai, selecting the optimal model and parameters for your use case.\n\n## Steps\n\n1. **Gather Requirements**\n   - Prompt description\n   - Image size and aspect ratio\n   - Quality vs speed preference\n   - Output format requirements\n\n2. **Select Model**\n   - FLUX.1 [dev] - High quality, open-source\n   - FLUX.2 [pro] - Best quality, production\n   - FLUX Schnell - Fast generation\n   - Fast SDXL - Speed optimized\n   - Recraft V3 - Design assets\n\n3. **Configure Parameters**\n   - num_inference_steps (quality)\n   - guidance_scale (prompt adherence)\n   - seed (reproducibility)\n   - safety checker settings\n\n4. **Generate Code**\n   - JavaScript/TypeScript or Python\n   - Error handling\n   - Progress feedback\n\n## Quick Reference\n\n### Model Selection Guide\n\n| Need | Recommended Model | Why |\n|------|------------------|-----|\n| Best quality | `fal-ai/flux-2-pro` | Latest FLUX, highest fidelity |\n| Fast iteration | `fal-ai/flux/schnell` | 4-step generation |\n| Open source | `fal-ai/flux/dev` | 12B parameter model |\n| Budget friendly | `fal-ai/fast-sdxl` | Lower cost per image |\n| Custom styles | `fal-ai/flux-lora` | Fine-tuned models |\n| Design work | `fal-ai/recraft-v3` | Vector-style outputs |\n\n### Image Sizes\n\n| Preset | Dimensions | Use Case |\n|--------|------------|----------|\n| `square` | 512x512 | Thumbnails, icons |\n| `square_hd` | 1024x1024 | Social media |\n| `portrait_4_3` | 768x1024 | Portraits |\n| `portrait_16_9` | 576x1024 | Mobile wallpapers |\n| `landscape_4_3` | 1024x768 | Presentations |\n| `landscape_16_9` | 1024x576 | Widescreen |\n\n### JavaScript/TypeScript Example\n\n```typescript\nimport { fal } from \"@fal-ai/client\";\n\nfal.config({ credentials: process.env.FAL_KEY });\n\nasync function generateImage(prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/flux/dev\", {\n    input: {\n      prompt,\n      image_size: \"landscape_16_9\",\n      num_inference_steps: 28,\n      guidance_scale: 3.5,\n      num_images: 1,\n      enable_safety_checker: true,\n      output_format: \"jpeg\"\n    },\n    logs: true,\n    onQueueUpdate: (update) => {\n      if (update.status === \"IN_PROGRESS\") {\n        console.log(\"Generating...\", update.logs);\n      }\n    }\n  });\n\n  return result.images[0].url;\n}\n```\n\n### Python Example\n\n```python\nimport fal_client\n\ndef generate_image(prompt: str) -> str:\n    result = fal_client.subscribe(\n        \"fal-ai/flux/dev\",\n        arguments={\n            \"prompt\": prompt,\n            \"image_size\": \"landscape_16_9\",\n            \"num_inference_steps\": 28,\n            \"guidance_scale\": 3.5,\n            \"num_images\": 1,\n            \"enable_safety_checker\": True,\n            \"output_format\": \"jpeg\"\n        },\n        with_logs=True,\n        on_queue_update=lambda update: print(f\"Status: {update}\")\n    )\n    return result[\"images\"][0][\"url\"]\n```\n\n### Advanced Features\n\n**Image-to-Image:**\n```typescript\nconst result = await fal.subscribe(\"fal-ai/flux/dev/image-to-image\", {\n  input: {\n    image_url: \"https://example.com/input.jpg\",\n    prompt: \"Transform into oil painting style\",\n    strength: 0.75\n  }\n});\n```\n\n**Inpainting:**\n```typescript\nconst result = await fal.subscribe(\"fal-ai/flux/dev/inpainting\", {\n  input: {\n    image_url: \"https://example.com/photo.jpg\",\n    mask_url: \"https://example.com/mask.png\",\n    prompt: \"A golden retriever\"\n  }\n});\n```\n\n**ControlNet:**\n```typescript\nconst result = await fal.subscribe(\"fal-ai/flux/dev/controlnet\", {\n  input: {\n    prompt: \"Modern architecture\",\n    control_image_url: \"https://example.com/edges.png\",\n    controlnet_conditioning_scale: 0.8\n  }\n});\n```\n\n## Parameter Tuning\n\n### num_inference_steps\n- **Low (4-10):** Fast, lower quality - use with Schnell\n- **Medium (20-30):** Balanced - recommended default\n- **High (40-50):** Maximum quality, slower\n\n### guidance_scale\n- **Low (1-3):** More creative, less prompt adherence\n- **Medium (3.5-7):** Balanced - recommended\n- **High (8-15):** Strict prompt following\n\n### Seed\n- Use same seed for reproducible results\n- Combine with identical parameters for variations\n\n## Error Handling\n\n```typescript\nimport { fal, ValidationError, ApiError } from \"@fal-ai/client\";\n\ntry {\n  const result = await fal.subscribe(\"fal-ai/flux/dev\", {\n    input: { prompt: \"Your prompt\" }\n  });\n} catch (error) {\n  if (error instanceof ValidationError) {\n    console.error(\"Invalid parameters:\", error.body);\n  } else if (error instanceof ApiError) {\n    if (error.status === 429) {\n      console.error(\"Rate limited, retry later\");\n    } else {\n      console.error(\"API error:\", error.message);\n    }\n  }\n}\n```\n\n## Cost Optimization\n\n1. **Start with Schnell** for iteration, switch to dev/pro for final\n2. **Use appropriate sizes** - don't generate larger than needed\n3. **Batch similar requests** when possible\n4. **Cache results** by seed for reproducible outputs"
              },
              {
                "name": "/fal-generate-video",
                "description": "Generate AI videos using fal.ai models like Veo 3, Sora 2, Kling 2.6, and more",
                "path": "plugins/fal-ai-master/commands/fal-generate-video.md",
                "frontmatter": {
                  "name": "Generate Video",
                  "description": "Generate AI videos using fal.ai models like Veo 3, Sora 2, Kling 2.6, and more",
                  "argument-hint": "<prompt> [--model veo|sora|kling|ltx] [--duration 5|10|20]"
                },
                "content": "# Generate Video with fal.ai\n\nGenerate AI videos using fal.ai models like Kling, Sora, LTX, and more.\n\n## What This Command Does\n\nGuides you through generating videos with fal.ai, selecting the optimal model and parameters for your use case.\n\n## Steps\n\n1. **Gather Requirements**\n   - Text-to-video or image-to-video\n   - Duration needed\n   - Aspect ratio\n   - Quality requirements\n   - Audio needs\n\n2. **Select Model**\n   - Kling 2.6 Pro - Cinematic with audio\n   - Sora 2 - OpenAI advanced\n   - LTX-2 Pro - High fidelity\n   - MiniMax Hailuo - Image-to-video\n   - Runway Gen-3 - Fast generation\n\n3. **Configure Parameters**\n   - Duration\n   - Resolution/aspect ratio\n   - Motion settings\n   - Audio options\n\n4. **Generate Code**\n   - JavaScript/TypeScript or Python\n   - Progress tracking\n   - Error handling\n\n## Model Selection Guide\n\n| Model | Endpoint | Best For | Audio | Duration |\n|-------|----------|----------|-------|----------|\n| Kling 2.6 Pro | `fal-ai/kling-video/v2.6/pro` | Cinematic quality | Native | 5-10s |\n| Kling O1 | `fal-ai/kling-video/o1` | Video editing | Yes | 5s |\n| Sora 2 | `fal-ai/sora` | High quality | Optional | 5-20s |\n| LTX-2 Pro | `fal-ai/ltx-video-2-pro` | Fast, high fidelity | Yes | 5s |\n| MiniMax Hailuo | `fal-ai/minimax/video-01` | Image animation | No | 6s |\n| Runway Gen-3 | `fal-ai/runway/gen3/turbo` | Fast iteration | No | 5-10s |\n| Luma | `fal-ai/luma-dream-machine` | Creative | No | 5s |\n| CogVideoX | `fal-ai/cogvideox` | Open source | No | 6s |\n\n## Text-to-Video Examples\n\n### JavaScript/TypeScript\n\n```typescript\nimport { fal } from \"@fal-ai/client\";\n\nfal.config({ credentials: process.env.FAL_KEY });\n\n// Kling 2.6 Pro - Best quality\nasync function generateWithKling(prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/kling-video/v2.6/pro\", {\n    input: {\n      prompt,\n      duration: 5,\n      aspect_ratio: \"16:9\",\n      negative_prompt: \"blurry, low quality, distorted\",\n      cfg_scale: 0.5\n    },\n    logs: true,\n    onQueueUpdate: (update) => {\n      console.log(\"Status:\", update.status);\n    }\n  });\n\n  return {\n    videoUrl: result.video.url,\n    audioUrl: result.audio?.url\n  };\n}\n\n// LTX-2 Pro - Fast with audio\nasync function generateWithLTX(prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/ltx-video-2-pro\", {\n    input: {\n      prompt,\n      negative_prompt: \"worst quality, inconsistent motion\",\n      num_inference_steps: 30,\n      guidance_scale: 3.5,\n      resolution: \"720p\",\n      enable_audio: true\n    }\n  });\n\n  return result.video.url;\n}\n\n// Runway Gen-3 Turbo - Fast iteration\nasync function generateWithRunway(prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/runway/gen3/turbo\", {\n    input: {\n      prompt,\n      duration: 5,\n      ratio: \"16:9\"\n    }\n  });\n\n  return result.video.url;\n}\n```\n\n### Python\n\n```python\nimport fal_client\n\ndef generate_video_kling(prompt: str) -> dict:\n    \"\"\"Generate video with Kling 2.6 Pro\"\"\"\n    result = fal_client.subscribe(\n        \"fal-ai/kling-video/v2.6/pro\",\n        arguments={\n            \"prompt\": prompt,\n            \"duration\": 5,\n            \"aspect_ratio\": \"16:9\",\n            \"negative_prompt\": \"blurry, low quality\",\n            \"cfg_scale\": 0.5\n        },\n        with_logs=True,\n        on_queue_update=lambda u: print(f\"Status: {u}\")\n    )\n    return {\n        \"video_url\": result[\"video\"][\"url\"],\n        \"audio_url\": result.get(\"audio\", {}).get(\"url\")\n    }\n\ndef generate_video_ltx(prompt: str) -> str:\n    \"\"\"Generate video with LTX-2 Pro\"\"\"\n    result = fal_client.subscribe(\n        \"fal-ai/ltx-video-2-pro\",\n        arguments={\n            \"prompt\": prompt,\n            \"negative_prompt\": \"worst quality\",\n            \"num_inference_steps\": 30,\n            \"guidance_scale\": 3.5,\n            \"resolution\": \"720p\",\n            \"enable_audio\": True\n        }\n    )\n    return result[\"video\"][\"url\"]\n```\n\n## Image-to-Video Examples\n\n### Animate an Image\n\n```typescript\n// MiniMax Hailuo - Image to video\nasync function animateImage(imageUrl: string, prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/minimax/video-01\", {\n    input: {\n      image_url: imageUrl,\n      prompt,\n      prompt_optimizer: true\n    }\n  });\n\n  return result.video.url;\n}\n\n// Kling Image-to-Video\nasync function animateWithKling(imageUrl: string, prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/kling-video/v2.6/pro/image-to-video\", {\n    input: {\n      image_url: imageUrl,\n      prompt,\n      duration: 5,\n      aspect_ratio: \"16:9\"\n    }\n  });\n\n  return result.video.url;\n}\n\n// Luma Dream Machine\nasync function animateWithLuma(imageUrl: string, prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/luma-dream-machine\", {\n    input: {\n      image_url: imageUrl,\n      prompt,\n      loop: false,\n      aspect_ratio: \"16:9\"\n    }\n  });\n\n  return result.video.url;\n}\n```\n\n### Python Image-to-Video\n\n```python\ndef animate_image(image_url: str, prompt: str) -> str:\n    \"\"\"Animate image with MiniMax\"\"\"\n    result = fal_client.subscribe(\n        \"fal-ai/minimax/video-01\",\n        arguments={\n            \"image_url\": image_url,\n            \"prompt\": prompt,\n            \"prompt_optimizer\": True\n        }\n    )\n    return result[\"video\"][\"url\"]\n```\n\n## Video-to-Video (Editing)\n\n```typescript\n// Kling O1 for video editing\nasync function editVideo(videoUrl: string, prompt: string) {\n  const result = await fal.subscribe(\"fal-ai/kling-video/o1\", {\n    input: {\n      video_url: videoUrl,\n      prompt,\n      negative_prompt: \"distorted, glitchy\"\n    }\n  });\n\n  return result.video.url;\n}\n```\n\n## Parameter Reference\n\n### Common Parameters\n\n| Parameter | Description | Typical Values |\n|-----------|-------------|----------------|\n| `prompt` | Description of video | Detailed, cinematic |\n| `negative_prompt` | What to avoid | \"blurry, low quality\" |\n| `duration` | Length in seconds | 5, 10, 20 |\n| `aspect_ratio` | Video dimensions | \"16:9\", \"9:16\", \"1:1\" |\n| `cfg_scale` | Prompt adherence | 0.3-0.7 |\n| `num_inference_steps` | Quality steps | 20-50 |\n\n### Aspect Ratios\n\n| Ratio | Use Case |\n|-------|----------|\n| `16:9` | Widescreen, YouTube |\n| `9:16` | Vertical, TikTok/Reels |\n| `1:1` | Square, Instagram |\n| `4:3` | Traditional video |\n| `21:9` | Cinematic ultrawide |\n\n## Prompt Tips for Video\n\n1. **Be Specific About Motion**\n   - Bad: \"A person walking\"\n   - Good: \"A woman walking gracefully through a sunlit forest, camera tracking alongside her\"\n\n2. **Describe Camera Movement**\n   - \"Slow zoom in on...\"\n   - \"Panning shot across...\"\n   - \"Drone shot flying over...\"\n\n3. **Include Temporal Details**\n   - \"Starting with... then transitioning to...\"\n   - \"The sun slowly sets as...\"\n\n4. **Specify Style/Mood**\n   - \"Cinematic, film grain, warm colors\"\n   - \"Hyper-realistic, 4K, sharp focus\"\n\n## Error Handling\n\n```typescript\ntry {\n  const result = await fal.subscribe(\"fal-ai/kling-video/v2.6/pro\", {\n    input: { prompt: \"Your video prompt\" }\n  });\n} catch (error) {\n  if (error.status === 429) {\n    console.error(\"Rate limited - implement backoff\");\n  } else if (error.status === 400) {\n    console.error(\"Invalid parameters:\", error.body);\n  } else {\n    console.error(\"Generation failed:\", error.message);\n  }\n}\n```\n\n## Cost Optimization\n\n1. **Use shorter durations** for testing\n2. **Lower resolution** for iterations\n3. **Runway Gen-3 Turbo** for quick previews\n4. **Kling Pro** only for final renders\n5. **Batch process** during off-peak hours\n\n## Workflow Example\n\n```typescript\n// Production workflow: iterate fast, render high quality\nasync function productionWorkflow(prompt: string) {\n  // 1. Quick preview with fast model\n  const preview = await fal.subscribe(\"fal-ai/runway/gen3/turbo\", {\n    input: { prompt, duration: 5, ratio: \"16:9\" }\n  });\n  console.log(\"Preview:\", preview.video.url);\n\n  // 2. Review and refine prompt...\n\n  // 3. Final render with Kling Pro\n  const final = await fal.subscribe(\"fal-ai/kling-video/v2.6/pro\", {\n    input: {\n      prompt,\n      duration: 10,\n      aspect_ratio: \"16:9\",\n      cfg_scale: 0.5\n    }\n  });\n\n  return final;\n}\n```"
              }
            ],
            "skills": []
          },
          {
            "name": "modal-master",
            "description": "Expert Modal.com serverless cloud platform system with comprehensive Modal 1.0 SDK (May 2025) features, GPU functions (T4/L4/A10G/L40S/A100/H100/H200/B200), autoscaler configuration, @modal.concurrent/@modal.batched decorators, Sandboxes for isolated code execution, CloudBucketMount for S3/GCS, and production deployment patterns. PROACTIVELY activate for: (1) ANY Modal.com task, (2) GPU configuration with fallbacks and multi-GPU, (3) Autoscaler settings (min/max/buffer containers, scaledown_window), (4) Web endpoints (FastAPI, ASGI, WSGI, custom servers), (5) @modal.concurrent for request concurrency, (6) @modal.batched for dynamic batching, (7) Sandboxes for untrusted code execution, (8) Scheduling (Cron with timezone, Period), (9) Storage (Volumes with commit(), Dict with TTL, Queue, CloudBucketMount), (10) Parallel processing (.map(), .starmap(), .spawn(), .for_each()), (11) Container lifecycle (@modal.enter, @modal.method, @modal.exit), (12) Image building (uv_pip_install, run_function for model downloads), (13) Secrets and environment management, (14) Deployment and CI/CD with GitHub Actions, (15) Cost optimization and 2025 pricing. Provides: Modal 1.0 stable API patterns, GPU selection guide with per-second pricing, autoscaler tuning strategies, concurrency and batching for ML inference, Sandbox security patterns, CloudBucketMount for external data, complete CLI reference, debugging workflows, and production-ready configurations.",
            "source": "./plugins/modal-master",
            "category": null,
            "version": "3.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install modal-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/modal-debug",
                "description": "Debug Modal issues including container failures, GPU errors, and deployment problems",
                "path": "plugins/modal-master/commands/modal-debug.md",
                "frontmatter": {
                  "name": "Debug App",
                  "description": "Debug Modal issues including container failures, GPU errors, and deployment problems",
                  "argument-hint": "[app-name-or-issue]"
                },
                "content": "# Modal Debug Command\n\nDebug Modal issues including container failures, GPU errors, and deployment problems.\n\n## Task\n\nHelp diagnose and resolve Modal issues:\n\n1. **Error Identification**\n   - Parse error messages\n   - Identify error category\n   - Determine root cause\n\n2. **Common Issues**\n   - Container startup failures\n   - GPU availability issues\n   - Memory errors\n   - Timeout problems\n   - Secret/volume access errors\n\n3. **Debugging Tools**\n   - Interactive shell\n   - Log analysis\n   - Local testing\n\n## Debugging Commands\n\n```bash\n# Interactive shell in container\nmodal shell app.py\n\n# Shell with GPU\nmodal shell app.py --gpu A100\n\n# View logs\nmodal app logs my-app\nmodal app logs my-app --follow\n\n# List deployments\nmodal app list\n\n# Check function status\nmodal app show my-app\n```\n\n## Common Errors and Solutions\n\n### 1. Container Startup Failures\n\n**Error:** `Container failed to start`\n\n**Causes:**\n- Bad import at module level\n- Missing dependencies\n- Image build failure\n\n**Solutions:**\n```python\n# Test image locally\nmodal shell app.py\n\n# Check imports work\npython -c \"import your_module\"\n\n# Simplify image for debugging\nimage = modal.Image.debian_slim().pip_install(\"package\")\n```\n\n### 2. Import Errors\n\n**Error:** `ModuleNotFoundError: No module named 'xxx'`\n\n**Solutions:**\n```python\n# Add missing package\nimage = modal.Image.debian_slim().pip_install(\"missing_package\")\n\n# Use uv for faster installs\nimage = modal.Image.debian_slim().uv_pip_install(\"package\")\n\n# For local modules\nimage = image.add_local_python_source(\"my_module\")\n\n# Or use include_source\n@app.function(include_source=True)\ndef my_func():\n    pass\n```\n\n### 3. GPU Not Available\n\n**Error:** `GPU type xxx not available`\n\n**Solutions:**\n```python\n# Add fallbacks\n@app.function(gpu=[\"H100\", \"A100-80GB\", \"A100\", \"any\"])\ndef gpu_func():\n    pass\n\n# Use \"any\" for flexibility\n@app.function(gpu=\"any\")  # L4, A10G, or T4\ndef inference():\n    pass\n```\n\n### 4. CUDA Out of Memory\n\n**Error:** `CUDA out of memory`\n\n**Solutions:**\n```python\n# Use larger GPU\n@app.function(gpu=\"A100-80GB\")  # Instead of A100-40GB\n\n# Reduce batch size in code\nbatch_size = 8  # Instead of 32\n\n# Clear cache in exit handler\n@modal.exit()\ndef cleanup(self):\n    import torch\n    torch.cuda.empty_cache()\n\n# Use gradient checkpointing for training\nmodel.gradient_checkpointing_enable()\n```\n\n### 5. Timeout Errors\n\n**Error:** `Function timed out after xxx seconds`\n\n**Solutions:**\n```python\n# Increase timeout\n@app.function(timeout=3600)  # 1 hour\ndef long_running():\n    pass\n\n# For web endpoints (max 150s)\n@app.function(timeout=150)\n@modal.asgi_app()\ndef web():\n    return app\n```\n\n### 6. Memory Errors\n\n**Error:** `Out of memory` (CPU memory)\n\n**Solutions:**\n```python\n# Increase memory\n@app.function(memory=16384)  # 16 GB\ndef memory_intensive():\n    pass\n\n# Process in chunks\ndef process_large_file(path):\n    for chunk in pd.read_csv(path, chunksize=10000):\n        process(chunk)\n```\n\n### 7. Secret Not Found\n\n**Error:** `Secret 'xxx' not found`\n\n**Solutions:**\n```bash\n# Check secret exists\nmodal secret list\n\n# Create secret\nmodal secret create my-secret KEY=value\n\n# Check environment\nMODAL_ENVIRONMENT=prod modal secret list\n```\n\n```python\n# Verify secret name matches\n@app.function(secrets=[modal.Secret.from_name(\"exact-name\")])\ndef func():\n    pass\n```\n\n### 8. Volume Not Found\n\n**Error:** `Volume 'xxx' not found`\n\n**Solutions:**\n```bash\n# Create volume\nmodal volume create my-volume\n\n# List volumes\nmodal volume list\n```\n\n```python\n# Create if missing\nvol = modal.Volume.from_name(\"my-volume\", create_if_missing=True)\n```\n\n### 9. Network/DNS Errors\n\n**Error:** `Name resolution failed` or connection errors\n\n**Solutions:**\n```python\n# Add retries\n@app.function(retries=3)\ndef network_func():\n    pass\n\n# Increase timeout for slow APIs\nimport requests\nresponse = requests.get(url, timeout=30)\n```\n\n### 10. Image Build Failures\n\n**Error:** `Image build failed`\n\n**Solutions:**\n```python\n# Check package compatibility\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .apt_install(\"build-essential\")  # For compiled packages\n    .pip_install(\"package\")\n)\n\n# Build in stages for debugging\nbase_image = modal.Image.debian_slim()\nwith_deps = base_image.pip_install(\"dep1\", \"dep2\")\nfinal = with_deps.pip_install(\"main_package\")\n```\n\n## Debugging Workflow\n\n### Step 1: Reproduce Locally\n\n```bash\n# Test the function\nmodal run app.py::function_name --arg value\n\n# Get interactive shell\nmodal shell app.py\n```\n\n### Step 2: Check Logs\n\n```bash\n# View recent logs\nmodal app logs my-app\n\n# Follow logs in real-time\nmodal app logs my-app --follow\n```\n\n### Step 3: Simplify\n\n```python\n# Minimal reproduction\n@app.function()\ndef debug_func():\n    # Add one thing at a time\n    import torch\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n```\n\n### Step 4: Check Resources\n\n```python\n# Log resource usage\n@app.function(gpu=\"A100\")\ndef check_resources():\n    import torch\n    import psutil\n\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(f\"CPU Memory: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n```\n\n## Debug Logging Pattern\n\n```python\nimport modal\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = modal.App(\"debug-app\")\n\n@app.function()\ndef debuggable_func(data):\n    logger.info(f\"Starting with data: {data}\")\n\n    try:\n        logger.info(\"Step 1: Processing\")\n        result = process(data)\n        logger.info(f\"Step 1 complete: {len(result)} items\")\n\n        logger.info(\"Step 2: Transforming\")\n        transformed = transform(result)\n        logger.info(f\"Step 2 complete\")\n\n        return transformed\n\n    except Exception as e:\n        logger.error(f\"Error: {e}\", exc_info=True)\n        raise\n```\n\n## Performance Debugging\n\n```python\nimport time\n\n@app.function()\ndef profile_func():\n    start = time.time()\n\n    # Phase 1\n    t1 = time.time()\n    step1()\n    print(f\"Step 1: {time.time() - t1:.2f}s\")\n\n    # Phase 2\n    t2 = time.time()\n    step2()\n    print(f\"Step 2: {time.time() - t2:.2f}s\")\n\n    print(f\"Total: {time.time() - start:.2f}s\")\n```\n\n## Getting Help\n\n1. **Modal Discord**: Active community support\n2. **Modal Docs**: docs.modal.com\n3. **GitHub Issues**: github.com/modal-labs/modal-client\n4. **Error Messages**: Usually contain helpful context\n\n## Checklist\n\n- [ ] Function runs locally with `modal run`?\n- [ ] Image builds successfully?\n- [ ] Secrets exist in correct environment?\n- [ ] Volumes exist and are accessible?\n- [ ] GPU type is available?\n- [ ] Sufficient memory allocated?\n- [ ] Timeout is adequate?\n- [ ] Dependencies are installed correctly?"
              },
              {
                "name": "/modal-deploy",
                "description": "Deploy a Modal application with environment configuration and CI/CD setup",
                "path": "plugins/modal-master/commands/modal-deploy.md",
                "frontmatter": {
                  "name": "Deploy App",
                  "description": "Deploy a Modal application with environment configuration and CI/CD setup",
                  "argument-hint": "<app-file> [--env staging|prod]"
                },
                "content": "# Modal Deploy Command\n\nDeploy a Modal application with environment configuration and CI/CD setup.\n\n## Task\n\nHelp deploy a Modal application to production with:\n\n1. **Pre-deployment Checks**\n   - Verify app runs locally with `modal run`\n   - Check all secrets are configured\n   - Verify volumes exist\n   - Review resource configuration (GPU, memory, timeout)\n\n2. **Environment Configuration**\n   - Set up Modal environments (dev, staging, prod)\n   - Configure environment-specific secrets\n   - Set appropriate concurrency limits\n\n3. **Deployment Execution**\n   - Deploy using `modal deploy`\n   - Verify deployment success\n   - Test deployed endpoints\n\n4. **CI/CD Setup (Optional)**\n   - GitHub Actions workflow\n   - Environment variables for tokens\n   - Branch-based deployments\n\n## Deployment Commands\n\n```bash\n# Test locally first\nmodal run app.py\n\n# Deploy to default environment\nmodal deploy app.py\n\n# Deploy to specific environment\nMODAL_ENVIRONMENT=prod modal deploy app.py\n\n# Deploy specific module\nmodal deploy -m mypackage.module\n\n# List deployments\nmodal app list\n\n# View logs\nmodal app logs my-app\n```\n\n## GitHub Actions Workflow\n\n```yaml\nname: Deploy to Modal\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install modal\n          pip install -r requirements.txt\n\n      - name: Deploy to Modal\n        run: modal deploy app.py\n        env:\n          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}\n          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}\n          MODAL_ENVIRONMENT: ${{ github.ref == 'refs/heads/main' && 'prod' || 'staging' }}\n```\n\n## Environment Setup\n\n```bash\n# Create environments\nmodal environment create dev\nmodal environment create staging\nmodal environment create prod\n\n# Create environment-specific secrets\nMODAL_ENVIRONMENT=prod modal secret create api-keys \\\n  OPENAI_API_KEY=sk-prod-xxx \\\n  DATABASE_URL=postgres://prod...\n\nMODAL_ENVIRONMENT=staging modal secret create api-keys \\\n  OPENAI_API_KEY=sk-staging-xxx \\\n  DATABASE_URL=postgres://staging...\n```\n\n## Pre-deployment Checklist\n\n- [ ] App runs successfully with `modal run`\n- [ ] All secrets created in Modal dashboard\n- [ ] Volumes created and accessible\n- [ ] Appropriate GPU/memory configured\n- [ ] Timeout values set appropriately\n- [ ] Concurrency limits configured\n- [ ] Environment variables set\n- [ ] CI/CD tokens stored as GitHub secrets\n\n## Post-deployment Verification\n\n```bash\n# Check deployment status\nmodal app list\n\n# View app details\nmodal app show my-app\n\n# Test web endpoint\ncurl https://your-app--endpoint.modal.run/\n\n# Check logs for errors\nmodal app logs my-app --follow\n```\n\n## Rollback (Team Plan Required)\n\n```bash\n# List deployment history\nmodal app history my-app\n\n# Rollback to previous version\nmodal app rollback my-app --version <version-id>\n```\n\n## Troubleshooting\n\n1. **Deployment fails**: Check `modal run` works locally first\n2. **Secrets not found**: Verify secrets exist in correct environment\n3. **GPU not available**: Add GPU fallbacks `gpu=[\"H100\", \"A100\", \"any\"]`\n4. **Timeout errors**: Increase `timeout` parameter\n5. **Out of memory**: Increase `memory` parameter"
              },
              {
                "name": "/modal-gpu",
                "description": "Configure GPU functions with optimal settings, fallbacks, and cost optimization",
                "path": "plugins/modal-master/commands/modal-gpu.md",
                "frontmatter": {
                  "name": "Configure GPU",
                  "description": "Configure GPU functions with optimal settings, fallbacks, and cost optimization",
                  "argument-hint": "<gpu-type: T4|L4|A10G|L40S|A100|H100|H200|B200> [workload: inference|training]"
                },
                "content": "# Modal GPU Command\n\nConfigure GPU functions with optimal settings, fallbacks, and cost optimization.\n\n## Task\n\nHelp configure GPU-accelerated Modal functions with:\n\n1. **GPU Selection**\n   - Match GPU to workload requirements\n   - Configure appropriate memory\n   - Set up multi-GPU if needed\n\n2. **Fallback Configuration**\n   - Define GPU fallback chain\n   - Handle availability issues\n\n3. **Cost Optimization**\n   - Choose cost-effective GPU for workload\n   - Configure container idle timeout\n   - Optimize cold start times\n\n4. **Performance Patterns**\n   - Use `@modal.enter()` for model loading\n   - Configure concurrency appropriately\n   - Set up warm containers\n\n## Available GPUs\n\n| GPU | Memory | Best For | ~Cost/hr |\n|-----|--------|----------|----------|\n| T4 | 16 GB | Small models, inference | $0.59 |\n| L4 | 24 GB | Medium inference | $0.80 |\n| A10G | 24 GB | Inference, fine-tuning | $1.10 |\n| L40S | 48 GB | Heavy inference | $1.50 |\n| A100-40GB | 40 GB | Training, large models | $2.00 |\n| A100-80GB | 80 GB | Very large models | $3.00 |\n| H100 | 80 GB | Cutting-edge training | $5.00 |\n| H200 | 141 GB | Largest models | $5.00 |\n| B200 | 180+ GB | Latest generation | $6.25 |\n\n## GPU Configuration Examples\n\n### Single GPU\n\n```python\n# Basic GPU\n@app.function(gpu=\"T4\")\ndef inference():\n    pass\n\n# Specific model\n@app.function(gpu=\"A100\")\ndef training():\n    pass\n\n# High memory variant\n@app.function(gpu=\"A100-80GB\")\ndef large_model():\n    pass\n```\n\n### Multi-GPU\n\n```python\n# 2 GPUs\n@app.function(gpu=\"A100:2\")\ndef multi_gpu_training():\n    import torch\n    assert torch.cuda.device_count() == 2\n\n# 4 GPUs for distributed training\n@app.function(gpu=\"H100:4\")\ndef distributed_training():\n    pass\n```\n\n### GPU Fallbacks\n\n```python\n# Try H100 first, then A100, then any available\n@app.function(gpu=[\"H100\", \"A100-80GB\", \"A100\", \"any\"])\ndef flexible_workload():\n    pass\n\n# \"any\" = L4, A10G, or T4\n@app.function(gpu=\"any\")\ndef simple_inference():\n    pass\n```\n\n## Optimal GPU Patterns\n\n### ML Inference Server (Recommended Pattern)\n\n```python\nimport modal\n\napp = modal.App(\"inference-server\")\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .uv_pip_install(\"torch\", \"transformers\", \"accelerate\")\n)\n\n@app.cls(\n    gpu=\"A10G\",  # Good balance for inference\n    image=image,\n    container_idle_timeout=300,  # Keep warm for 5 minutes\n    timeout=120,\n)\nclass ModelServer:\n\n    @modal.enter()\n    def load_model(self):\n        \"\"\"Load model once when container starts\"\"\"\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"microsoft/DialoGPT-medium\"\n        ).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"microsoft/DialoGPT-medium\"\n        )\n\n    @modal.method()\n    def generate(self, prompt: str) -> str:\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        outputs = self.model.generate(inputs, max_length=100)\n        return self.tokenizer.decode(outputs[0])\n\n    @modal.exit()\n    def cleanup(self):\n        \"\"\"Clean up GPU memory\"\"\"\n        import torch\n        del self.model\n        torch.cuda.empty_cache()\n```\n\n### Download Model During Build\n\n```python\ndef download_model():\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        \"meta-llama/Llama-2-7b-chat-hf\",\n        local_dir=\"/models/llama\"\n    )\n\nimage = (\n    modal.Image.debian_slim()\n    .uv_pip_install(\"huggingface_hub\", \"torch\", \"transformers\")\n    .run_function(\n        download_model,\n        secrets=[modal.Secret.from_name(\"huggingface\")]\n    )\n)\n```\n\n## Cost Optimization Tips\n\n1. **Right-size GPU**: Don't use A100 for inference that fits on T4\n2. **Use fallbacks**: Increase availability, potentially get better GPU\n3. **Set `container_idle_timeout`**: Balance cold starts vs idle costs\n4. **Use `@modal.enter()`**: Move model loading to warmup\n5. **Monitor usage**: Check Modal dashboard for actual costs\n\n## GPU Selection Guide\n\n| Use Case | Recommended GPU |\n|----------|-----------------|\n| Small model inference (<7B) | T4 or L4 |\n| Medium model inference (7B-13B) | A10G or L40S |\n| Large model inference (13B-70B) | A100-80GB or H100 |\n| Fine-tuning small models | A10G |\n| Fine-tuning large models | A100-80GB |\n| Distributed training | H100:4 or H100:8 |\n| vLLM inference | A100 or H100 |\n| Image generation (SD) | T4 or A10G |\n| Video processing | T4 or L4 |\n\n## Troubleshooting GPU Issues\n\n1. **GPU not available**: Add fallbacks, try different regions\n2. **CUDA out of memory**: Use larger GPU or reduce batch size\n3. **Slow startup**: Move model loading to `@modal.enter()`\n4. **High costs**: Review GPU selection, reduce idle timeout\n5. **Inconsistent performance**: Pin to specific GPU type"
              },
              {
                "name": "/modal-schedule",
                "description": "Set up scheduled and cron functions with timezone support",
                "path": "plugins/modal-master/commands/modal-schedule.md",
                "frontmatter": {
                  "name": "Setup Schedule",
                  "description": "Set up scheduled and cron functions with timezone support",
                  "argument-hint": "<cron-expression|period> [timezone]"
                },
                "content": "# Modal Schedule Command\n\nSet up scheduled and cron functions with timezone support.\n\n## Task\n\nHelp configure scheduled Modal functions with:\n\n1. **Schedule Type Selection**\n   - Cron expressions for specific times\n   - Period-based for intervals\n\n2. **Timezone Configuration**\n   - UTC vs local timezone\n   - Daylight saving handling\n\n3. **Best Practices**\n   - Error handling and retries\n   - Monitoring and logging\n   - Resource configuration\n\n## Schedule Types\n\n### Cron Expressions\n\n```python\nimport modal\n\napp = modal.App(\"scheduled-app\")\n\n# Daily at 8 AM UTC\n@app.function(schedule=modal.Cron(\"0 8 * * *\"))\ndef daily_job():\n    print(\"Running daily job\")\n\n# With timezone\n@app.function(schedule=modal.Cron(\"0 6 * * *\", timezone=\"America/New_York\"))\ndef daily_eastern():\n    print(\"Running at 6 AM Eastern\")\n\n# Every Monday at 9 AM\n@app.function(schedule=modal.Cron(\"0 9 * * 1\"))\ndef weekly_report():\n    generate_report()\n\n# First day of month at midnight\n@app.function(schedule=modal.Cron(\"0 0 1 * *\"))\ndef monthly_cleanup():\n    cleanup_old_data()\n\n# Every 15 minutes\n@app.function(schedule=modal.Cron(\"*/15 * * * *\"))\ndef frequent_check():\n    check_status()\n\n# Weekdays at 9 AM\n@app.function(schedule=modal.Cron(\"0 9 * * 1-5\"))\ndef weekday_job():\n    pass\n\n# Multiple times per day\n@app.function(schedule=modal.Cron(\"0 8,12,18 * * *\"))\ndef three_times_daily():\n    pass\n```\n\n### Period-Based Scheduling\n\n```python\n# Every 5 hours\n@app.function(schedule=modal.Period(hours=5))\ndef periodic_job():\n    pass\n\n# Every 2 days\n@app.function(schedule=modal.Period(days=2))\ndef bi_daily_job():\n    pass\n\n# Every 30 minutes\n@app.function(schedule=modal.Period(minutes=30))\ndef half_hourly():\n    pass\n```\n\n## Cron Syntax Reference\n\n```\n minute (0-59)\n  hour (0-23)\n   day of month (1-31)\n    month (1-12)\n     day of week (0-6, Sunday=0)\n    \n* * * * *\n```\n\n**Special Characters:**\n- `*` - Any value\n- `,` - Value list (1,3,5)\n- `-` - Range (1-5)\n- `/` - Step (*/15 = every 15)\n\n**Common Patterns:**\n| Pattern | Meaning |\n|---------|---------|\n| `0 * * * *` | Every hour |\n| `0 0 * * *` | Daily at midnight |\n| `0 0 * * 0` | Weekly on Sunday |\n| `0 0 1 * *` | Monthly on 1st |\n| `*/5 * * * *` | Every 5 minutes |\n| `0 9-17 * * 1-5` | Hourly, 9-5, Mon-Fri |\n\n## Complete Example: Data Pipeline\n\n```python\nimport modal\nfrom datetime import datetime\n\napp = modal.App(\"data-pipeline\")\n\nvol = modal.Volume.from_name(\"pipeline-data\", create_if_missing=True)\n\n@app.function(\n    schedule=modal.Cron(\"0 6 * * *\", timezone=\"America/New_York\"),\n    secrets=[modal.Secret.from_name(\"database\")],\n    volumes={\"/data\": vol},\n    timeout=3600,  # 1 hour max\n    retries=3,\n)\ndef daily_etl():\n    import os\n\n    print(f\"Starting ETL at {datetime.now()}\")\n\n    try:\n        # Extract\n        db_url = os.environ[\"DATABASE_URL\"]\n        data = extract_from_database(db_url)\n        print(f\"Extracted {len(data)} records\")\n\n        # Transform\n        transformed = transform_data(data)\n        print(f\"Transformed {len(transformed)} records\")\n\n        # Load\n        output_path = f\"/data/output_{datetime.now().strftime('%Y%m%d')}.parquet\"\n        save_to_parquet(transformed, output_path)\n        vol.commit()\n\n        # Notify success\n        send_notification(f\"ETL completed: {len(transformed)} records\")\n\n        return {\"status\": \"success\", \"records\": len(transformed)}\n\n    except Exception as e:\n        send_alert(f\"ETL failed: {str(e)}\")\n        raise\n```\n\n## Example: Health Monitoring\n\n```python\nimport modal\n\napp = modal.App(\"health-monitor\")\n\n@app.function(\n    schedule=modal.Cron(\"*/5 * * * *\"),  # Every 5 minutes\n    secrets=[modal.Secret.from_name(\"monitoring\")],\n)\ndef health_check():\n    import os\n    import requests\n\n    endpoints = [\n        \"https://api.example.com/health\",\n        \"https://app.example.com/health\",\n    ]\n\n    results = []\n    for endpoint in endpoints:\n        try:\n            response = requests.get(endpoint, timeout=10)\n            results.append({\n                \"endpoint\": endpoint,\n                \"status\": response.status_code,\n                \"healthy\": response.status_code == 200\n            })\n        except Exception as e:\n            results.append({\n                \"endpoint\": endpoint,\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"healthy\": False\n            })\n\n    # Alert on failures\n    unhealthy = [r for r in results if not r[\"healthy\"]]\n    if unhealthy:\n        send_alert(unhealthy)\n\n    return results\n```\n\n## Example: Cleanup Job\n\n```python\nimport modal\nfrom datetime import datetime, timedelta\n\napp = modal.App(\"cleanup\")\n\nvol = modal.Volume.from_name(\"data-volume\")\n\n@app.function(\n    schedule=modal.Cron(\"0 2 * * *\"),  # 2 AM daily\n    volumes={\"/data\": vol},\n)\ndef cleanup_old_files():\n    import os\n\n    cutoff = datetime.now() - timedelta(days=30)\n    deleted = 0\n\n    for root, dirs, files in os.walk(\"/data\"):\n        for file in files:\n            path = os.path.join(root, file)\n            mtime = datetime.fromtimestamp(os.path.getmtime(path))\n            if mtime < cutoff:\n                os.remove(path)\n                deleted += 1\n\n    vol.commit()\n    print(f\"Deleted {deleted} files older than 30 days\")\n    return {\"deleted\": deleted}\n```\n\n## Important Notes\n\n1. **Deployment Required**: Scheduled functions only run when deployed with `modal deploy`, not with `modal run`\n\n2. **Test First**: Always test with `modal run app.py::function_name` before deploying\n\n3. **Timezone Handling**: Use explicit timezone to avoid DST issues\n\n4. **Error Handling**: Always wrap in try/except and send alerts on failure\n\n5. **Idempotency**: Design jobs to be safely re-runnable\n\n6. **Logging**: Include timestamps and progress logging\n\n7. **Timeouts**: Set appropriate timeout for long-running jobs\n\n## Deployment\n\n```bash\n# Test the function manually\nmodal run app.py::daily_etl\n\n# Deploy for scheduled execution\nmodal deploy app.py\n\n# Check scheduled functions\nmodal app list\n\n# View execution logs\nmodal app logs data-pipeline --follow\n```\n\n## Monitoring\n\n- Check Modal dashboard for execution history\n- Set up alerts for failed runs\n- Monitor execution duration\n- Track error rates\n\n## Troubleshooting\n\n1. **Job not running**: Verify deployed with `modal deploy`\n2. **Wrong time**: Check timezone configuration\n3. **Timeout**: Increase timeout parameter\n4. **Failures**: Check logs with `modal app logs`\n5. **Missed runs**: Review execution history in dashboard"
              },
              {
                "name": "/modal-setup",
                "description": "Initialize a Modal project with proper structure, configuration, and best practices",
                "path": "plugins/modal-master/commands/modal-setup.md",
                "frontmatter": {
                  "name": "Setup Project",
                  "description": "Initialize a Modal project with proper structure, configuration, and best practices",
                  "argument-hint": "[project-type: api|batch|ml|scheduled]"
                },
                "content": "# Modal Setup Command\n\nInitialize a Modal project with proper structure, configuration, and best practices.\n\n## Task\n\nSet up a new Modal project with the following:\n\n1. **Project Structure Analysis**\n   - Determine project type (API server, batch processing, ML inference, scheduled tasks)\n   - Identify existing dependencies and requirements\n   - Check for existing Modal configuration\n\n2. **Create Core Files**\n   - Main Modal app file with proper structure\n   - Requirements/dependencies file\n   - Environment configuration\n\n3. **Configure Based on Use Case**\n   - For ML/AI: GPU configuration, model loading patterns\n   - For APIs: Web endpoint setup with FastAPI\n   - For Batch: Volume configuration, parallel processing\n   - For Scheduled: Cron/Period configuration\n\n4. **Best Practices Implementation**\n   - Use `@modal.enter()` for initialization\n   - Use `uv_pip_install` for faster builds\n   - Set appropriate timeouts and retries\n   - Configure secrets management\n\n## Project Structure Template\n\n```\nproject/\n modal_app.py          # Main Modal application\n requirements.txt      # Python dependencies\n .env.example          # Environment variables template\n src/                  # Source code\n    __init__.py\n README.md             # Project documentation\n```\n\n## Questions to Ask\n\n1. What type of Modal application? (API, batch processing, ML inference, scheduled tasks)\n2. Do you need GPU support? If yes, what workload type?\n3. What Python packages are required?\n4. Do you need persistent storage (Volumes)?\n5. Are there secrets/credentials to manage?\n\n## Example Starter Code\n\n```python\nimport modal\n\napp = modal.App(\"my-app\")\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .uv_pip_install(\"fastapi\", \"uvicorn\")\n)\n\n@app.function(image=image)\ndef hello(name: str = \"World\") -> str:\n    return f\"Hello, {name}!\"\n\n@app.local_entrypoint()\ndef main():\n    result = hello.remote()\n    print(result)\n```\n\n## CLI Commands to Run\n\n```bash\n# Install Modal CLI\npip install modal\n\n# Authenticate (opens browser)\nmodal setup\n\n# Test the app\nmodal run modal_app.py\n\n# Start development server\nmodal serve modal_app.py\n```\n\n## Checklist\n\n- [ ] Modal CLI installed and authenticated\n- [ ] App file created with proper structure\n- [ ] Dependencies specified\n- [ ] Secrets configured (if needed)\n- [ ] Volumes configured (if needed)\n- [ ] Local test successful with `modal run`"
              },
              {
                "name": "/modal-web",
                "description": "Create web endpoints using FastAPI, ASGI, WSGI, or custom web servers",
                "path": "plugins/modal-master/commands/modal-web.md",
                "frontmatter": {
                  "name": "Create Endpoint",
                  "description": "Create web endpoints using FastAPI, ASGI, WSGI, or custom web servers",
                  "argument-hint": "<endpoint-type: fastapi|asgi|wsgi|server> [route]"
                },
                "content": "# Modal Web Command\n\nCreate web endpoints using FastAPI, ASGI, WSGI, or custom web servers.\n\n## Task\n\nHelp create web endpoints for Modal applications with:\n\n1. **Endpoint Type Selection**\n   - Simple endpoints with `@modal.fastapi_endpoint()`\n   - Full FastAPI apps with `@modal.asgi_app()`\n   - Flask/Django with `@modal.wsgi_app()`\n   - Custom servers with `@modal.web_server()`\n\n2. **Configuration**\n   - Request/response handling\n   - Authentication and security\n   - Custom domains\n   - WebSocket support\n\n3. **Performance**\n   - Container warmup for low latency\n   - Concurrency settings\n   - Timeout configuration\n\n## Endpoint Types\n\n### Simple FastAPI Endpoint\n\nBest for single-function APIs:\n\n```python\nimport modal\n\napp = modal.App(\"simple-api\")\n\n@app.function()\n@modal.fastapi_endpoint()\ndef hello(name: str = \"World\"):\n    return {\"message\": f\"Hello, {name}!\"}\n```\n\n### Full FastAPI Application\n\nBest for complex APIs with multiple routes:\n\n```python\nimport modal\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = modal.App(\"full-api\")\n\nweb_app = FastAPI(title=\"My API\", version=\"1.0.0\")\n\nclass PredictRequest(BaseModel):\n    text: str\n    max_length: int = 100\n\nclass PredictResponse(BaseModel):\n    result: str\n    tokens: int\n\n@web_app.get(\"/health\")\ndef health():\n    return {\"status\": \"healthy\"}\n\n@web_app.post(\"/predict\", response_model=PredictResponse)\ndef predict(request: PredictRequest):\n    # Your logic here\n    result = process(request.text)\n    return PredictResponse(result=result, tokens=len(result.split()))\n\n@web_app.get(\"/items/{item_id}\")\ndef get_item(item_id: int):\n    if item_id < 0:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"item_id\": item_id}\n\n@app.function()\n@modal.asgi_app()\ndef fastapi_app():\n    return web_app\n```\n\n### FastAPI with GPU (ML Inference)\n\n```python\nimport modal\n\napp = modal.App(\"ml-api\")\n\nimage = (\n    modal.Image.debian_slim()\n    .uv_pip_install(\"torch\", \"transformers\", \"fastapi\")\n)\n\n@app.cls(gpu=\"A10G\", image=image, container_idle_timeout=300)\nclass InferenceServer:\n\n    @modal.enter()\n    def load_model(self):\n        from transformers import pipeline\n        self.pipe = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n\n    @modal.asgi_app()\n    def web(self):\n        from fastapi import FastAPI\n\n        web_app = FastAPI()\n\n        @web_app.post(\"/generate\")\n        def generate(prompt: str, max_length: int = 50):\n            result = self.pipe(prompt, max_length=max_length)\n            return {\"generated\": result[0][\"generated_text\"]}\n\n        return web_app\n```\n\n### Flask Application (WSGI)\n\n```python\nimport modal\nfrom flask import Flask, request, jsonify\n\napp = modal.App(\"flask-api\")\n\nflask_app = Flask(__name__)\n\n@flask_app.route(\"/\")\ndef home():\n    return jsonify({\"message\": \"Hello from Flask!\"})\n\n@flask_app.route(\"/predict\", methods=[\"POST\"])\ndef predict():\n    data = request.json\n    result = process(data[\"text\"])\n    return jsonify({\"result\": result})\n\n@app.function()\n@modal.wsgi_app()\ndef flask_endpoint():\n    return flask_app\n```\n\n### Custom Web Server\n\nFor any server listening on a port:\n\n```python\nimport modal\n\napp = modal.App(\"custom-server\")\n\n@app.function()\n@modal.web_server(port=8080)\ndef gradio_server():\n    import subprocess\n    subprocess.run([\"python\", \"gradio_app.py\"])\n```\n\n### WebSocket Support\n\n```python\nimport modal\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\n\napp = modal.App(\"websocket-api\")\n\nweb_app = FastAPI()\n\n@web_app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await websocket.send_text(f\"Echo: {data}\")\n    except WebSocketDisconnect:\n        print(\"Client disconnected\")\n\n@app.function()\n@modal.asgi_app()\ndef ws_app():\n    return web_app\n```\n\n## Custom Domains\n\n```python\n# Configure custom domain\n@app.function()\n@modal.asgi_app(custom_domains=[\"api.example.com\"])\ndef production_api():\n    return web_app\n```\n\n**Setup Steps:**\n1. Add domain in Modal dashboard\n2. Configure DNS records as instructed\n3. Add `custom_domains` parameter to decorator\n4. Deploy with `modal deploy`\n\n## Authentication\n\n### Proxy Auth Token\n\n```python\n# Protected endpoint (requires token)\n@app.function()\n@modal.asgi_app()\ndef protected_api():\n    # Modal provides proxy auth token\n    # Clients must include Authorization header\n    return web_app\n```\n\n### Custom Authentication\n\n```python\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nweb_app = FastAPI()\nsecurity = HTTPBearer()\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    if credentials.credentials != os.environ[\"API_TOKEN\"]:\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n    return credentials\n\n@web_app.get(\"/protected\")\ndef protected_route(token: str = Depends(verify_token)):\n    return {\"message\": \"Authenticated!\"}\n```\n\n## Configuration Options\n\n```python\n@app.function(\n    timeout=150,  # HTTP timeout is 150s max\n    memory=2048,  # Memory in MB\n    cpu=2.0,  # CPU cores\n    container_idle_timeout=60,  # Keep warm\n    concurrency_limit=100,  # Max concurrent requests\n)\n@modal.asgi_app()\ndef configured_api():\n    return web_app\n```\n\n## Development Workflow\n\n```bash\n# Hot-reload development\nmodal serve app.py\n\n# Test locally (auto-generates URL)\n# Output: https://your-workspace--app-name-fastapi-app-dev.modal.run\n\n# Deploy to production\nmodal deploy app.py\n\n# View logs\nmodal app logs app-name --follow\n```\n\n## Best Practices\n\n1. **Use `@modal.asgi_app()`** for complex FastAPI apps\n2. **Use `@modal.fastapi_endpoint()`** for simple single-function endpoints\n3. **Set `container_idle_timeout`** to reduce cold starts\n4. **Use class pattern with `@modal.enter()`** for ML models\n5. **Add health check endpoint** for monitoring\n6. **Configure appropriate timeout** (max 150s for HTTP)\n7. **Use custom domains** for production APIs\n8. **Implement proper error handling** with HTTPException"
              }
            ],
            "skills": [
              {
                "name": "modal-knowledge",
                "description": "Comprehensive Modal.com platform knowledge covering all features, pricing, and best practices",
                "path": "plugins/modal-master/skills/modal-knowledge/SKILL.md",
                "frontmatter": {
                  "name": "modal-knowledge",
                  "description": "Comprehensive Modal.com platform knowledge covering all features, pricing, and best practices"
                },
                "content": "# Modal Knowledge Skill\n\nComprehensive Modal.com platform knowledge covering all features, pricing, and best practices. Activate this skill when users need detailed information about Modal's serverless cloud platform.\n\n## Activation Triggers\n\nActivate this skill when users ask about:\n- Modal.com platform features and capabilities\n- GPU-accelerated Python functions\n- Serverless container configuration\n- Modal pricing and billing\n- Modal CLI commands\n- Web endpoints and APIs on Modal\n- Scheduled/cron jobs on Modal\n- Modal volumes, secrets, and storage\n- Parallel processing with Modal\n- Modal deployment and CI/CD\n\n---\n\n## Platform Overview\n\nModal is a serverless cloud platform for running Python code, optimized for AI/ML workloads with:\n\n- **Zero Configuration**: Everything defined in Python code\n- **Fast GPU Startup**: ~1 second container spin-up\n- **Automatic Scaling**: Scale to zero, scale to thousands\n- **Per-Second Billing**: Only pay for active compute\n- **Multi-Cloud**: AWS, GCP, Oracle Cloud Infrastructure\n\n---\n\n## Core Components Reference\n\n### Apps and Functions\n\n```python\nimport modal\n\napp = modal.App(\"app-name\")\n\n@app.function()\ndef basic_function(arg: str) -> str:\n    return f\"Result: {arg}\"\n\n@app.local_entrypoint()\ndef main():\n    result = basic_function.remote(\"test\")\n    print(result)\n```\n\n### Function Decorator Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `image` | Image | Container image configuration |\n| `gpu` | str/list | GPU type(s): \"T4\", \"A100\", [\"H100\", \"A100\"] |\n| `cpu` | float | CPU cores (0.125 to 64) |\n| `memory` | int | Memory in MB (128 to 262144) |\n| `timeout` | int | Max execution seconds |\n| `retries` | int | Retry attempts on failure |\n| `secrets` | list | Secrets to inject |\n| `volumes` | dict | Volume mount points |\n| `schedule` | Cron/Period | Scheduled execution |\n| `concurrency_limit` | int | Max concurrent executions |\n| `container_idle_timeout` | int | Seconds to keep warm |\n| `include_source` | bool | Auto-sync source code |\n\n---\n\n## GPU Reference\n\n### Available GPUs\n\n| GPU | Memory | Use Case | ~Cost/hr |\n|-----|--------|----------|----------|\n| T4 | 16 GB | Small inference | $0.59 |\n| L4 | 24 GB | Medium inference | $0.80 |\n| A10G | 24 GB | Inference/fine-tuning | $1.10 |\n| L40S | 48 GB | Heavy inference | $1.50 |\n| A100-40GB | 40 GB | Training | $2.00 |\n| A100-80GB | 80 GB | Large models | $3.00 |\n| H100 | 80 GB | Cutting-edge | $5.00 |\n| H200 | 141 GB | Largest models | $5.00 |\n| B200 | 180+ GB | Latest gen | $6.25 |\n\n### GPU Configuration\n\n```python\n# Single GPU\n@app.function(gpu=\"A100\")\n\n# Specific memory variant\n@app.function(gpu=\"A100-80GB\")\n\n# Multi-GPU\n@app.function(gpu=\"H100:4\")\n\n# Fallbacks (tries in order)\n@app.function(gpu=[\"H100\", \"A100\", \"any\"])\n\n# \"any\" = L4, A10G, or T4\n@app.function(gpu=\"any\")\n```\n\n---\n\n## Image Building\n\n### Base Images\n\n```python\n# Debian slim (recommended)\nmodal.Image.debian_slim(python_version=\"3.11\")\n\n# From Dockerfile\nmodal.Image.from_dockerfile(\"./Dockerfile\")\n\n# From Docker registry\nmodal.Image.from_registry(\"nvidia/cuda:12.1.0-base-ubuntu22.04\")\n```\n\n### Package Installation\n\n```python\n# pip (standard)\nimage.pip_install(\"torch\", \"transformers\")\n\n# uv (FASTER - 10-100x)\nimage.uv_pip_install(\"torch\", \"transformers\")\n\n# System packages\nimage.apt_install(\"ffmpeg\", \"libsm6\")\n\n# Shell commands\nimage.run_commands(\"apt-get update\", \"make install\")\n```\n\n### Adding Files\n\n```python\n# Single file\nimage.add_local_file(\"./config.json\", \"/app/config.json\")\n\n# Directory\nimage.add_local_dir(\"./models\", \"/app/models\")\n\n# Python source\nimage.add_local_python_source(\"my_module\")\n\n# Environment variables\nimage.env({\"VAR\": \"value\"})\n```\n\n### Build-Time Function\n\n```python\ndef download_model():\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"model-name\")\n\nimage.run_function(download_model, secrets=[...])\n```\n\n---\n\n## Storage\n\n### Volumes\n\n```python\n# Create/reference volume\nvol = modal.Volume.from_name(\"my-vol\", create_if_missing=True)\n\n# Mount in function\n@app.function(volumes={\"/data\": vol})\ndef func():\n    # Read/write to /data\n    vol.commit()  # Persist changes\n```\n\n### Secrets\n\n```python\n# From dashboard (recommended)\nmodal.Secret.from_name(\"secret-name\")\n\n# From dictionary\nmodal.Secret.from_dict({\"KEY\": \"value\"})\n\n# From local env\nmodal.Secret.from_local_environ([\"KEY1\", \"KEY2\"])\n\n# From .env file\nmodal.Secret.from_dotenv()\n\n# Usage\n@app.function(secrets=[modal.Secret.from_name(\"api-keys\")])\ndef func():\n    import os\n    key = os.environ[\"API_KEY\"]\n```\n\n### Dict and Queue\n\n```python\n# Distributed dict\nd = modal.Dict.from_name(\"cache\", create_if_missing=True)\nd[\"key\"] = \"value\"\nd.put(\"key\", \"value\", ttl=3600)\n\n# Distributed queue\nq = modal.Queue.from_name(\"jobs\", create_if_missing=True)\nq.put(\"task\")\nitem = q.get()\n```\n\n---\n\n## Web Endpoints\n\n### FastAPI Endpoint (Simple)\n\n```python\n@app.function()\n@modal.fastapi_endpoint()\ndef hello(name: str = \"World\"):\n    return {\"message\": f\"Hello, {name}!\"}\n```\n\n### ASGI App (Full FastAPI)\n\n```python\nfrom fastapi import FastAPI\nweb_app = FastAPI()\n\n@web_app.post(\"/predict\")\ndef predict(text: str):\n    return {\"result\": process(text)}\n\n@app.function()\n@modal.asgi_app()\ndef fastapi_app():\n    return web_app\n```\n\n### WSGI App (Flask)\n\n```python\nfrom flask import Flask\nflask_app = Flask(__name__)\n\n@app.function()\n@modal.wsgi_app()\ndef flask_endpoint():\n    return flask_app\n```\n\n### Custom Web Server\n\n```python\n@app.function()\n@modal.web_server(port=8000)\ndef custom_server():\n    subprocess.run([\"python\", \"-m\", \"http.server\", \"8000\"])\n```\n\n### Custom Domains\n\n```python\n@modal.asgi_app(custom_domains=[\"api.example.com\"])\n```\n\n---\n\n## Scheduling\n\n### Cron\n\n```python\n# Daily at 8 AM UTC\n@app.function(schedule=modal.Cron(\"0 8 * * *\"))\n\n# With timezone\n@app.function(schedule=modal.Cron(\"0 6 * * *\", timezone=\"America/New_York\"))\n```\n\n### Period\n\n```python\n@app.function(schedule=modal.Period(hours=5))\n@app.function(schedule=modal.Period(days=1))\n```\n\n**Note:** Scheduled functions only run with `modal deploy`, not `modal run`.\n\n---\n\n## Parallel Processing\n\n### Map\n\n```python\n# Parallel execution (up to 1000 concurrent)\nresults = list(func.map(items))\n\n# Unordered (faster)\nresults = list(func.map(items, order_outputs=False))\n```\n\n### Starmap\n\n```python\n# Spread args\npairs = [(1, 2), (3, 4)]\nresults = list(add.starmap(pairs))\n```\n\n### Spawn\n\n```python\n# Async job (returns immediately)\ncall = func.spawn(data)\nresult = call.get()  # Get result later\n\n# Spawn many\ncalls = [func.spawn(item) for item in items]\nresults = [call.get() for call in calls]\n```\n\n---\n\n## Container Lifecycle (Classes)\n\n```python\n@app.cls(gpu=\"A100\", container_idle_timeout=300)\nclass Server:\n\n    @modal.enter()\n    def load(self):\n        self.model = load_model()\n\n    @modal.method()\n    def predict(self, text):\n        return self.model(text)\n\n    @modal.exit()\n    def cleanup(self):\n        del self.model\n```\n\n### Concurrency\n\n```python\n@modal.concurrent(max_inputs=100, target_inputs=80)\n@modal.method()\ndef batched(self, item):\n    pass\n```\n\n---\n\n## CLI Commands\n\n### Development\n\n```bash\nmodal run app.py              # Run function\nmodal serve app.py            # Hot-reload dev server\nmodal shell app.py            # Interactive shell\nmodal shell app.py --gpu A100 # Shell with GPU\n```\n\n### Deployment\n\n```bash\nmodal deploy app.py           # Deploy\nmodal app list                # List apps\nmodal app logs app-name       # View logs\nmodal app stop app-name       # Stop app\n```\n\n### Resources\n\n```bash\n# Volumes\nmodal volume create name\nmodal volume list\nmodal volume put name local remote\nmodal volume get name remote local\n\n# Secrets\nmodal secret create name KEY=value\nmodal secret list\n\n# Environments\nmodal environment create staging\n```\n\n---\n\n## Pricing (2025)\n\n### Plans\n\n| Plan | Price | Containers | GPU Concurrency |\n|------|-------|------------|-----------------|\n| Starter | Free ($30 credits) | 100 | 10 |\n| Team | $250/month | 1000 | 50 |\n| Enterprise | Custom | Unlimited | Custom |\n\n### Compute\n\n- **CPU**: $0.0000131/core/sec\n- **Memory**: $0.00000222/GiB/sec\n- **GPUs**: See GPU table above\n\n### Special Programs\n\n- Startups: Up to $25k credits\n- Researchers: Up to $10k credits\n\n---\n\n## Best Practices\n\n1. **Use `@modal.enter()`** for model loading\n2. **Use `uv_pip_install`** for faster builds\n3. **Use GPU fallbacks** for availability\n4. **Set appropriate timeouts** and retries\n5. **Use environments** (dev/staging/prod)\n6. **Download models during build**, not runtime\n7. **Use `order_outputs=False`** when order doesn't matter\n8. **Set `container_idle_timeout`** to balance cost/latency\n9. **Monitor costs** in Modal dashboard\n10. **Test with `modal run`** before `modal deploy`\n\n---\n\n## Common Patterns\n\n### LLM Inference\n\n```python\n@app.cls(gpu=\"A100\", container_idle_timeout=300)\nclass LLM:\n    @modal.enter()\n    def load(self):\n        from vllm import LLM\n        self.llm = LLM(model=\"...\")\n\n    @modal.method()\n    def generate(self, prompt):\n        return self.llm.generate([prompt])\n```\n\n### Batch Processing\n\n```python\n@app.function(volumes={\"/data\": vol})\ndef process(file):\n    # Process file\n    vol.commit()\n\n# Parallel\nresults = list(process.map(files))\n```\n\n### Scheduled ETL\n\n```python\n@app.function(\n    schedule=modal.Cron(\"0 6 * * *\"),\n    secrets=[modal.Secret.from_name(\"db\")]\n)\ndef daily_etl():\n    extract()\n    transform()\n    load()\n```\n\n---\n\n## Quick Reference\n\n| Task | Code |\n|------|------|\n| Create app | `app = modal.App(\"name\")` |\n| Basic function | `@app.function()` |\n| With GPU | `@app.function(gpu=\"A100\")` |\n| With image | `@app.function(image=img)` |\n| Web endpoint | `@modal.asgi_app()` |\n| Scheduled | `schedule=modal.Cron(\"...\")` |\n| Mount volume | `volumes={\"/path\": vol}` |\n| Use secret | `secrets=[modal.Secret.from_name(\"x\")]` |\n| Parallel map | `func.map(items)` |\n| Async spawn | `func.spawn(arg)` |\n| Class pattern | `@app.cls()` with `@modal.enter()` |"
              }
            ]
          },
          {
            "name": "viral-video-master",
            "description": "Complete viral video optimization system with comprehensive 2025-2026 research-backed strategies for maximizing video virality across all platforms. PROACTIVELY activate for: (1) ANY viral video strategy (TikTok, YouTube, Instagram, Facebook), (2) Short-form content optimization (Reels, Shorts, TikTok), (3) Long-form YouTube optimization, (4) Video hook and opening optimization (1.3-second attention capture), (5) Caption and subtitle strategy (80% engagement boost), (6) Thumbnail design and CTR optimization, (7) Hashtag and SEO strategy, (8) Posting schedule optimization, (9) Content repurposing strategies, (10) Audience retention improvement, (11) Trending audio and sound selection, (12) Video editing pacing and transitions, (13) Call-to-action optimization (380% conversion increase), (14) UGC and authenticity strategies, (15) Platform algorithm understanding (TikTok, YouTube, Instagram 2025-2026), (16) Faceless content creation (340% subscriber growth), (17) Community engagement and growth, (18) Shoppable video and social commerce (TikTok Shop $26.2B GMV), (19) Collaboration and cross-promotion, (20) Analytics and metrics interpretation. Provides: Research-backed viral video formulas, platform-specific optimization guides, hook templates with psychological triggers, retention strategies, trending content patterns, algorithm insights (YouTube March 2025 view counting, Instagram Sends Per Reach, TikTok cold start), production best practices, utility scripts for content calendars and metrics analysis, and comprehensive 2025-2026 patterns.",
            "source": "./plugins/viral-video-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install viral-video-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/viral-audit",
                "description": "Audit a video strategy for viral potential with platform-specific recommendations",
                "path": "plugins/viral-video-master/commands/viral-audit.md",
                "frontmatter": {
                  "name": "viral-audit",
                  "description": "Audit a video strategy for viral potential with platform-specific recommendations",
                  "argument-hint": "<platform> <content-type> [video-url-or-description]",
                  "allowed-tools": [
                    "Read",
                    "WebFetch",
                    "WebSearch"
                  ]
                },
                "content": "# Viral Video Audit\n\n## Purpose\n\nPerform a comprehensive audit of a video or video strategy to identify opportunities for improved viral potential and engagement.\n\n## Arguments\n\n- `$ARGUMENTS`: Platform target, content type, and optionally a video URL or description\n  - Example: `tiktok fitness-tips`\n  - Example: `youtube-shorts cooking demo at https://youtu.be/example`\n  - Example: `instagram-reels lifestyle my current video gets 200 views`\n\n## Workflow\n\n### 1. Gather Information\n\nIf not provided in arguments, ask about:\n- Platform target (TikTok, YouTube, Instagram, Facebook)\n- Content type (short-form or long-form)\n- Video topic/niche\n- Current video URL or description\n- Current metrics (views, engagement, retention if available)\n\n### 2. Audit Framework\n\nEvaluate across these dimensions:\n\n#### Hook Analysis (Weight: 25%)\n\n| Element | Check |\n|---------|-------|\n| First 1.3 seconds | Does it grab attention immediately? |\n| First 3 seconds | Is there a clear hook? |\n| Psychological trigger | Curiosity gap, pattern interrupt, or open loop? |\n| Value proposition | Is the payoff clear? |\n\n**Benchmark**: 65% who watch first 3 seconds will watch 10+ seconds\n\n#### Retention Strategy (Weight: 25%)\n\n| Element | Check |\n|---------|-------|\n| Pacing | Is it appropriate for platform? |\n| Pattern interrupts | Are there visual/audio changes every 30-60 seconds? |\n| Story structure | Beginning, middle, end? |\n| Filler content | Any unnecessary segments? |\n\n**Benchmark**: 80-90% completion rate for top performers\n\n#### Visual Optimization (Weight: 20%)\n\n| Element | Check |\n|---------|-------|\n| Thumbnail (YouTube) | Emotional face, <5 words, high contrast? |\n| Captions | Present, styled, synchronized? (12-40% watch time boost) |\n| Aspect ratio | Correct for platform (9:16 for short-form)? |\n| Color grading | Consistent, on-trend? |\n\n#### Audio Strategy (Weight: 15%)\n\n| Element | Check |\n|---------|-------|\n| Trending sound | Using current trends? (88% say sound essential) |\n| Voice quality | Clear, engaging, human? (35% drop with AI narration) |\n| Music sync | Beats aligned with cuts? |\n| Sound design | Enhance or distract? |\n\n#### SEO & Discovery (Weight: 15%)\n\n| Element | Check |\n|---------|-------|\n| Title | Keywords, compelling, optimal length? |\n| Description | First 25 words optimized, CTAs? |\n| Hashtags | 3-5 relevant, niche + broad mix? |\n| Tags | Platform-appropriate? |\n\n### 3. Scoring System\n\nRate each dimension 1-10 and calculate weighted score:\n\n```\nHook Score: __/10 x 0.25 = __\nRetention Score: __/10 x 0.25 = __\nVisual Score: __/10 x 0.20 = __\nAudio Score: __/10 x 0.15 = __\nSEO Score: __/10 x 0.15 = __\n---\nVIRAL POTENTIAL SCORE: __/10\n```\n\n### 4. Platform-Specific Benchmarks\n\n**TikTok:**\n- Target retention: 60%+ completion rate (80%+ for viral)\n- Hook window: 1.3 seconds\n- Optimal length: 15-30 seconds\n- Trending sounds: Essential (88% say sound matters)\n- Cold start test: First 200-500 viewers determine reach\n\n**YouTube Shorts:**\n- Target retention: 70%+ in first 30 seconds\n- Optimal length: 50-60 seconds (3x more views than 15-second)\n- Freshness factor: Content older than 30 days rarely pushed\n- CTA impact: 22% more engagement with strong CTA\n\n**YouTube Long-form:**\n- Target retention: 50%+ average (23.7% is average)\n- Optimal length: 8-15 minutes\n- Watch time: Primary metric\n- Thumbnail: 60-70% CTR impact\n\n**Instagram Reels:**\n- Target retention: 80%+ for viral\n- Optimal length: 7-30 seconds (viral), 30-90 seconds (engagement)\n- 3/8/12 rule: 3s hook, 8s deepen, 12s+ deliver\n- Sends Per Reach: Most powerful signal for new audience reach\n\n### 5. Generate Recommendations\n\n**Priority 1 (Immediate Impact):**\n- List 2-3 changes with biggest effect\n- Focus on hook and retention first\n\n**Priority 2 (Short-term Improvements):**\n- Visual and audio optimization\n- SEO improvements\n\n**Priority 3 (Long-term Strategy):**\n- Content strategy refinements\n- Audience building tactics\n\n## Output Format\n\nProvide:\n1. **Overall Viral Potential Score** with breakdown\n2. **Top 3 Strengths** of current approach\n3. **Top 3 Weaknesses** limiting viral potential\n4. **Prioritized Recommendations** with specific actions\n5. **Platform-Specific Tips** tailored to target platform\n6. **Benchmark Comparison** showing where content falls vs top performers\n7. **Quick Win Actions** that can be implemented immediately"
              },
              {
                "name": "/viral-hook",
                "description": "Generate viral hook ideas for video content using proven psychological triggers",
                "path": "plugins/viral-video-master/commands/viral-hook.md",
                "frontmatter": {
                  "name": "viral-hook",
                  "description": "Generate viral hook ideas for video content using proven psychological triggers",
                  "argument-hint": "<topic> [platform] [content-type]",
                  "allowed-tools": [
                    "Read"
                  ]
                },
                "content": "# Viral Hook Generator\n\n## Purpose\n\nGenerate attention-grabbing hooks for video content using proven psychological triggers and 2025-2026 viral patterns.\n\n## Arguments\n\n- `$ARGUMENTS`: Video topic, optionally platform and content type\n  - Example: `morning routine`\n  - Example: `productivity tips tiktok`\n  - Example: `home workout youtube-shorts educational`\n  - Example: `cooking pasta instagram`\n\n## Hook Psychology Framework\n\n### The 1.3-Second Rule (2026)\n\nYou have **1.3 seconds** to capture attention. The hook must:\n- Interrupt the scroll\n- Create immediate curiosity\n- Promise value\n\n### Hook Success Statistics\n\n| Statistic | Value |\n|-----------|-------|\n| 3-second viewers watching 10+ seconds | 65% |\n| 3-second viewers watching 30+ seconds | 45% |\n| TikTok scroll-past in first 3 seconds | 33% |\n| Instagram engagement drop without hook | 60% |\n| Drop-off with slow intros | 33%+ in first 30 seconds |\n\n## Psychological Triggers\n\n| Trigger | How It Works | Example |\n|---------|--------------|---------|\n| **Curiosity Gap** | Creates information gap that must be filled | \"The reason no one tells you about...\" |\n| **Pattern Interrupt** | Breaks expected patterns (triggers orienting response) | Start mid-action, unexpected visual |\n| **Open Loop (Zeigarnik)** | Incomplete stories demand completion | \"What happened next changed everything...\" |\n| **Loss Aversion/FOMO** | Humans feel losses 2x more than gains | \"If you're not doing this, you're losing...\" |\n| **Self-Identification** | Immediate relevance to viewer | \"If you're someone who struggles with...\" |\n| **Controversy** | Challenges beliefs | \"Everything you know about X is wrong\" |\n| **Specificity** | Numbers create credibility | \"3 exact steps\" vs \"some tips\" |\n| **Time Sensitivity** | Urgency drives action | \"Before this gets taken down...\" |\n\n## Hook Templates by Category\n\n### Curiosity Hooks\n- \"Here's why [common belief] is completely wrong...\"\n- \"I discovered something that changed everything about...\"\n- \"Most people don't know this about [topic]...\"\n- \"The [industry] doesn't want you to know...\"\n- \"This is what they don't teach you about...\"\n\n### Pattern Interrupt Hooks\n- [Start mid-sentence] \"...and that's why I stopped doing it\"\n- [Unexpected visual] Show the result first\n- [Controversial statement] \"I quit [popular thing]\"\n- [Action hook] Start with the most dramatic moment\n\n### Open Loop Hooks\n- \"By the end of this video, you'll understand why...\"\n- \"What happened next shocked everyone...\"\n- \"The third tip is the one that actually works...\"\n- \"Wait until you see what happens at the end...\"\n- \"I didn't believe it until I tried it myself...\"\n\n### FOMO/Urgency Hooks\n- \"If you're not doing this, you're losing...\"\n- \"This trend is about to explode...\"\n- \"Before everyone else figures this out...\"\n- \"Stop scrolling if you want to...\"\n- \"This is going away soon...\"\n\n### Self-Identification Hooks\n- \"If you're someone who [struggle], this is for you...\"\n- \"POV: You're a [audience type] who...\"\n- \"Only [niche] people will understand this...\"\n- \"This is for my [audience] who...\"\n\n### Educational Hooks\n- \"Here's exactly how to [result] in [timeframe]...\"\n- \"[Number] things I wish I knew before...\"\n- \"The simple trick that [achievement]...\"\n- \"Why [common approach] doesn't work...\"\n\n### Story Hooks\n- \"I was about to give up when...\"\n- \"Nobody believed me until...\"\n- \"This one decision changed my life...\"\n- \"The moment I realized...\"\n\n## Platform-Specific Variations\n\n**TikTok:**\n- Shorter, punchier hooks\n- Leverage trending sounds\n- Text overlay as hook\n- First frame must stop the scroll\n\n**YouTube Shorts:**\n- Slightly more context allowed\n- Thumbnail preview matters\n- Can tease longer content\n- 50-60 seconds optimal\n\n**YouTube Long-form:**\n- Can be longer (5-10 seconds)\n- Preview the payoff\n- Hook + roadmap works well\n- \"In this video, you'll learn...\"\n\n**Instagram Reels:**\n- 3/8/12 rule: Hook in 3 seconds\n- Visual-first platform\n- Aesthetic matters more\n- Text overlay hooks popular\n\n## Hook Generation Process\n\n1. **Identify the core value** - What will viewers get?\n2. **Find the tension** - What problem/curiosity exists?\n3. **Choose trigger(s)** - Which psychology applies?\n4. **Draft 5+ variations** - Different angles and triggers\n5. **Add visual component** - What supports the verbal hook?\n6. **Test with 3-second rule** - Does it work in 3 seconds?\n\n## Output Format\n\nGenerate:\n1. **5-10 Hook Variations** using different psychological triggers\n2. **Visual Hook Suggestions** - What to show in first 1.3 seconds\n3. **Audio/Music Recommendations** - Sound that amplifies the hook\n4. **Opening Text Overlay** - On-screen text to reinforce\n5. **Hook + Follow-up Flow** - How to transition from hook to content\n6. **A/B Testing Suggestions** - Which hooks to test against each other\n7. **Platform Variations** - How to adapt for different platforms"
              },
              {
                "name": "/viral-optimize",
                "description": "Optimize video for specific platform with tailored recommendations for maximum viral potential",
                "path": "plugins/viral-video-master/commands/viral-optimize.md",
                "frontmatter": {
                  "name": "viral-optimize",
                  "description": "Optimize video for specific platform with tailored recommendations for maximum viral potential",
                  "argument-hint": "<platform> [content-type] [optimization-focus]",
                  "allowed-tools": [
                    "Read",
                    "WebSearch"
                  ]
                },
                "content": "# Platform-Specific Viral Optimization\n\n## Purpose\n\nOptimize a video for maximum viral potential on a specific platform with tailored, actionable recommendations.\n\n## Arguments\n\n- `$ARGUMENTS`: Platform, optionally content type and focus area\n  - Example: `tiktok`\n  - Example: `youtube-shorts tutorial`\n  - Example: `instagram-reels product-demo hook-optimization`\n  - Example: `youtube-longform educational retention`\n\n## Platform Optimization Guides\n\n---\n\n## TikTok Optimization\n\n### Algorithm Signals (2025-2026)\n\n| Signal | Weight | Action |\n|--------|--------|--------|\n| Watch time | Highest | Optimize first 3 seconds |\n| Completion rate | Very High | Keep under 60 seconds |\n| Replays | High (10% = boost) | Create loop-worthy content |\n| Shares | High | Add share triggers |\n| Comments | Medium | Include conversation starters |\n| Likes | Lower (2025 update) | Still important, not primary |\n\n### TikTok Checklist\n\n- [ ] Video length: 15-30 seconds (peak) or under 60 seconds\n- [ ] Aspect ratio: 9:16 (1080x1920)\n- [ ] Hook in first 1.3 seconds\n- [ ] Trending sound used\n- [ ] Captions added and styled\n- [ ] 3-5 relevant hashtags\n- [ ] Micro-niche targeting\n- [ ] Loop potential considered\n- [ ] CTA included\n\n### TikTok-Specific Tips\n\n- **Cold Start**: First 200-500 viewers determine distribution\n- **Sound**: 88% say sound is essential - never skip audio\n- **Niche**: Algorithm favors micro-niche over broad appeal (2.5x better distribution)\n- **Posting**: 1-4 times daily; 68% of views in first 24 hours; 2-4x daily = 2.5x follower growth\n- **Hashtags**: 3-5 relevant; trending hashtags = 23% more engagement\n\n---\n\n## YouTube Shorts Optimization\n\n### Algorithm Signals\n\n| Signal | Weight | Action |\n|--------|--------|--------|\n| Watch time | Highest | Maximize retention |\n| Completion rate | Critical | Target 80-90% |\n| Replay rate | Very High | 10% replay = significant boost |\n| CTR | High | Thumbnail and title matter |\n| Engagement | High | Likes, comments, shares |\n\n### YouTube Shorts Checklist\n\n- [ ] Video length: 50-60 seconds (3x more views than 15-second)\n- [ ] Aspect ratio: 9:16 (1080x1920)\n- [ ] Hook in first 3 seconds\n- [ ] Title optimized (max 100 chars, keyword-rich)\n- [ ] Engaging thumbnail\n- [ ] Captions/subtitles added\n- [ ] End with CTA (22% more engagement)\n- [ ] Teases long-form content if applicable\n\n### YouTube Shorts-Specific Tips\n\n- **March 2025 Update**: Any start/replay = view, but only \"Engaged Views\" count for monetization\n- **Freshness**: Shorts older than 30 days rarely pushed\n- **Length**: 55-second versions get 3x more views than 15-second\n- **Long-form Combo**: Shorts + long-form = 41% faster channel growth\n- **Retention benchmark**: 80-90% for top performers, below 50% = \"skippable\"\n\n---\n\n## YouTube Long-form Optimization\n\n### Algorithm Signals\n\n| Signal | Weight | Action |\n|--------|--------|--------|\n| Watch time | Highest | Most critical metric |\n| CTR | Very High | Thumbnail and title essential |\n| Audience retention | Very High | Minimize drop-offs |\n| Session time | High | Keep viewers on YouTube |\n| Engagement | Medium | Comments, likes, shares |\n\n### YouTube Long-form Checklist\n\n- [ ] Video length: 8-15 minutes (sweet spot)\n- [ ] Aspect ratio: 16:9 (1920x1080 or 4K)\n- [ ] Hook in first 5-10 seconds\n- [ ] Custom thumbnail with face, emotion, <5 words\n- [ ] Title: Primary keyword + compelling copy (<60 chars)\n- [ ] Description: 150-250 words, keyword in first 25\n- [ ] Chapters/timestamps added\n- [ ] End screen and cards added\n- [ ] Closed captions (94% of top videos have them)\n- [ ] Playlist placement considered\n\n### YouTube Long-form Tips\n\n- **Thumbnail**: Test up to 3 variants with Test & Compare; faces with emotion = 20-30% higher CTR\n- **8+ minutes**: Enables more ad placements\n- **Connected TV**: YouTube is most-watched streaming service on TVs; long-form excels\n- **Retention**: 70%+ in first 30 seconds = much higher ranking; 10% improvement = 25%+ impressions\n- **SEO**: Videos can get 2-5x more views from Google ranking\n\n---\n\n## Instagram Reels Optimization\n\n### Algorithm Signals (December 2025+)\n\n| Signal | Weight | Action |\n|--------|--------|--------|\n| Sends Per Reach | Very High | DM shares = biggest boost |\n| Topic clarity | Very High | Algorithm needs to categorize |\n| Engagement | High | Saves, shares, comments |\n| Watch time | High | Completion rate matters |\n| Follows from Reel | High | Content discovery |\n\n### Instagram Reels Checklist\n\n- [ ] Video length: 7-30 seconds (viral) or 30-90 seconds (engagement)\n- [ ] Aspect ratio: 9:16 (1080x1920)\n- [ ] 3/8/12 rule: Hook (3s), deepen (8s), deliver (12s+)\n- [ ] Captions for sound-off viewing (50% watch without sound)\n- [ ] 3-5 highly relevant hashtags\n- [ ] Keyword-optimized caption\n- [ ] Clear topic/niche focus\n- [ ] CTA included\n\n### Instagram Reels Tips\n\n- **Trial Reels**: Test content without risking engagement metrics\n- **\"Your Algorithm\" Feature**: Users can see/customize topics - niche focus critical\n- **Sends Per Reach**: Most powerful signal for reaching new audiences\n- **Sound**: 50% watch without sound - captions essential\n- **Best Times**: Tuesday-Thursday, 11 AM - 6 PM\n\n---\n\n## Facebook Reels Optimization\n\n### Algorithm Signals\n\n| Signal | Weight | Action |\n|--------|--------|--------|\n| Shares | Very High | Facebook prioritizes sharing |\n| Watch time | High | Completion rate |\n| Comments | High | Conversation drivers |\n| Reactions | Medium | Emotional responses |\n\n### Facebook Reels Checklist\n\n- [ ] Video length: 15-30 seconds (optimal)\n- [ ] Aspect ratio: 9:16 (70% brand lift vs square)\n- [ ] Hook in first 3 seconds\n- [ ] Share-worthy content\n- [ ] Captions for sound-off viewing\n- [ ] Native upload (not cross-posted link)\n\n### Facebook Reels Tips\n\n- **Best Time**: Late Friday night/midnight into Saturday\n- **First Hours**: 1-2 hours after posting are critical\n- **Vertical**: 70% brand lift vs square format\n- **Sharing**: Prioritize content that drives shares\n\n---\n\n## Cross-Platform Optimization\n\n### Repurposing Workflow\n\n1. **Create primary version** for main platform\n2. **Adjust length** for each platform's sweet spot\n3. **Modify captions** for platform norms\n4. **Update hashtags** per platform strategy\n5. **Adjust aspect ratio** if needed\n\n### Platform Priority by Content Type\n\n| Content Type | Primary | Secondary | Tertiary |\n|--------------|---------|-----------|----------|\n| Entertainment | TikTok | Instagram | YouTube Shorts |\n| Educational | YouTube Long | YouTube Shorts | TikTok |\n| Product/Commerce | TikTok | Instagram | Facebook |\n| Professional | LinkedIn | YouTube | Instagram |\n| News/Trending | TikTok | Instagram | Twitter |\n\n## Output Format\n\nProvide:\n1. **Platform-Specific Checklist** with completion status\n2. **Optimization Score** current vs optimized\n3. **Technical Specs** exact dimensions, length, format\n4. **Content Adjustments** hook, pacing, CTA modifications\n5. **Metadata Optimization** title, description, hashtags, tags\n6. **Visual Recommendations** thumbnail, captions, graphics\n7. **Posting Strategy** best times, frequency, cross-posting plan\n8. **A/B Testing Plan** what to test for improvement"
              }
            ],
            "skills": []
          },
          {
            "name": "tsql-master",
            "description": "Comprehensive T-SQL and SQL Server expertise for query optimization, performance tuning, and Azure SQL Database. PROACTIVELY activate for: (1) T-SQL query optimization and SARGability analysis, (2) SQL Server performance tuning, (3) Index design and strategy, (4) Execution plan analysis, (5) Parameter sniffing solutions, (6) Azure SQL Database optimization, (7) Window functions and advanced patterns, (8) Columnstore and In-Memory OLTP, (9) Query Store and IQP features. Includes: tsql-expert agent, 5 progressive disclosure skills, 3 optimization commands, diagnostic scripts.",
            "source": "./plugins/tsql-master",
            "category": null,
            "version": "2.0.1",
            "author": {
              "name": "Josiah Siegel"
            },
            "install_commands": [
              "/plugin marketplace add JosiahSiegel/claude-plugin-marketplace",
              "/plugin install tsql-master@claude-plugin-marketplace"
            ],
            "signals": {
              "stars": 5,
              "forks": 1,
              "pushed_at": "2026-01-10T20:58:53Z",
              "created_at": "2025-10-22T23:30:38Z",
              "license": "MIT"
            },
            "commands": [
              {
                "name": "/analyze-query",
                "description": "Analyze a T-SQL query for optimization opportunities, SARGability issues, and index recommendations",
                "path": "plugins/tsql-master/commands/analyze-query.md",
                "frontmatter": {
                  "description": "Analyze a T-SQL query for optimization opportunities, SARGability issues, and index recommendations",
                  "argument-hint": "[paste query or file path]"
                },
                "content": "# Analyze Query\n\nSystematically analyze a T-SQL query for optimization opportunities.\n\n## Process\n\n### Step 1: Get Query\n\nIf user provided a file path:\n```bash\n# Read the query from file\ncat \"$1\"\n```\n\nIf user pasted query directly, use that.\n\n### Step 2: SARGability Analysis\n\nCheck for non-SARGable patterns:\n\n| Pattern | Issue | Fix |\n|---------|-------|-----|\n| `YEAR(DateCol) = 2024` | Function on column | `DateCol >= '2024-01-01' AND DateCol < '2025-01-01'` |\n| `LEFT(Col, 3) = 'ABC'` | Function on column | `Col LIKE 'ABC%'` |\n| `Col * 1.1 > 100` | Arithmetic on column | `Col > 100 / 1.1` |\n| `ISNULL(Col, 0) = 5` | Function on column | `(Col = 5 OR Col IS NULL)` |\n| `@Var = Col` | Variable on left | `Col = @Var` |\n\n### Step 3: Implicit Conversion Check\n\nLook for potential type mismatches:\n- VARCHAR column compared to INT value\n- NVARCHAR vs VARCHAR comparisons\n- Date strings without explicit CAST/CONVERT\n\n### Step 4: Join Analysis\n\nFor each JOIN:\n- Check if join columns should be indexed\n- Identify potential Cartesian products\n- Look for inefficient OR conditions across tables\n\n### Step 5: Subquery Analysis\n\nCheck for:\n- Correlated subqueries that could be JOINs\n- IN vs EXISTS optimization opportunities\n- Subqueries in SELECT that could use APPLY\n\n### Step 6: Index Recommendations\n\nBased on:\n- WHERE clause columns (equality first, then inequality)\n- JOIN columns\n- ORDER BY columns\n- SELECT columns (for covering index)\n\nSuggest CREATE INDEX statement.\n\n### Step 7: Generate Report\n\n```\n================================\nQuery Analysis Report\n================================\n\nQUERY:\n[First 200 chars of query...]\n\nSARGABILITY ISSUES:\n[ ] WHERE YEAR(OrderDate) = 2024\n     Change to: WHERE OrderDate >= '2024-01-01' AND OrderDate < '2025-01-01'\n\nIMPLICIT CONVERSIONS:\n[ ] Line 5: VarcharColumn = 123\n     Change to: VarcharColumn = '123'\n\nJOIN OPTIMIZATION:\n[] CustomerID join column should have index\n[ ] Consider EXISTS instead of IN for large subquery\n\nINDEX RECOMMENDATIONS:\nCREATE NONCLUSTERED INDEX IX_Orders_CustomerDate\nON Orders(CustomerID, OrderDate)\nINCLUDE (Amount, Status);\n\nESTIMATED IMPACT: High\n================================\n```\n\n## Usage\n\n```bash\n# Analyze pasted query\n/analyze-query SELECT * FROM Orders WHERE YEAR(OrderDate) = 2024\n\n# Analyze query from file\n/analyze-query queries/slow_report.sql\n```\n\n## Tips\n\n- For complex queries, break into CTEs for analysis\n- Consider execution plan for actual performance data\n- Test changes in non-production first\n- Update statistics after adding indexes"
              },
              {
                "name": "/explain-plan",
                "description": "Help interpret SQL Server execution plan operators and identify performance issues",
                "path": "plugins/tsql-master/commands/explain-plan.md",
                "frontmatter": {
                  "description": "Help interpret SQL Server execution plan operators and identify performance issues",
                  "argument-hint": "[describe the plan or paste XML]"
                },
                "content": "# Explain Execution Plan\n\nHelp understand and optimize SQL Server execution plans.\n\n## Process\n\n### Step 1: Get Plan Information\n\nUser can provide:\n- Description of operators they see\n- Execution plan XML\n- Screenshot (describe what you see)\n\n### Step 2: Identify Key Operators\n\nExplain each significant operator:\n\n#### Data Access Operators\n\n| Operator | Description | Performance |\n|----------|-------------|-------------|\n| **Clustered Index Scan** | Reads entire table | Poor for large tables |\n| **Clustered Index Seek** | Direct row lookup | Excellent |\n| **Index Scan** | Reads entire index | Poor for large indexes |\n| **Index Seek** | Range or point lookup | Good |\n| **Key Lookup** | Follows pointer to get extra columns | Overhead - add INCLUDE |\n| **RID Lookup** | Heap row lookup | Consider adding clustered index |\n| **Table Scan** | Heap full scan | Add index |\n\n#### Join Operators\n\n| Operator | Best For | Characteristics |\n|----------|----------|-----------------|\n| **Nested Loop** | Small outer, indexed inner | Low memory, many seeks |\n| **Merge Join** | Pre-sorted large datasets | Needs sorted input |\n| **Hash Match** | Large unsorted datasets | High memory usage |\n| **Adaptive Join** | Variable cardinality | IQP feature, auto-selects |\n\n#### Other Important Operators\n\n| Operator | Concern | Solution |\n|----------|---------|----------|\n| **Sort** | Memory/tempdb spills | Add index with ORDER BY cols |\n| **Hash Match (Aggregate)** | Large grouping | Normal for large GROUP BY |\n| **Filter** | Late filtering | Move to WHERE if possible |\n| **Compute Scalar** | Usually fine | Watch for in loops |\n| **Spool (Eager/Lazy)** | Repeated scans | Restructure query |\n| **Parallelism** | Check for skew | CXPACKET waits if issues |\n\n### Step 3: Warning Signs\n\nCheck for these issues:\n\n**Yellow Warning Icons:**\n- Missing index hint\n- Implicit conversion\n- No join predicate\n- Residual predicate\n\n**Thick Arrows:**\n- Large row counts flowing between operators\n- Compare Estimated vs Actual rows\n\n**Red Indicators:**\n- Missing statistics\n- Memory grant warnings\n- Spills to tempdb\n\n### Step 4: Estimated vs Actual Analysis\n\n```\nEstimated Rows: 100\nActual Rows: 100,000\n\nProblem: Statistics are outdated or misleading\nSolution: UPDATE STATISTICS TableName WITH FULLSCAN\n```\n\n### Step 5: Cost Analysis\n\n- Find highest cost operators (% in graphical plan)\n- These are optimization targets\n- 0% cost doesn't mean free - check actual metrics\n\n## Common Patterns and Fixes\n\n### Pattern: Scan + Key Lookup\n\n```\n[Index Scan] --> [Key Lookup] --> [Nested Loop]\n```\n\n**Problem:** Index doesn't cover all columns\n**Fix:** Add missing columns to INCLUDE\n\n### Pattern: Sort Operator\n\n```\n[Index Seek] --> [Sort] --> [Top]\n```\n\n**Problem:** ORDER BY columns not in index\n**Fix:** Add ORDER BY columns to index key\n\n### Pattern: Hash Match Join\n\n```\n[Table Scan] --> [Hash Match] <-- [Table Scan]\n```\n\n**Problem:** No indexes on join columns\n**Fix:** Add indexes on join columns\n\n### Pattern: Parallelism with Repartition\n\n```\n[Parallelism] --> [Repartition Streams] --> [Hash Match]\n```\n\n**Problem:** Data redistribution overhead\n**Fix:** May need MAXDOP hint or different approach\n\n## Output Format\n\n```\n================================\nExecution Plan Analysis\n================================\n\nPLAN SUMMARY:\n- Estimated Cost: 2.45\n- Actual Duration: 1,234 ms\n- Parallelism: DOP 4\n\nKEY OPERATORS:\n\n1. [PROBLEM] Clustered Index Scan on Orders (45% cost)\n   Estimated: 1,000 rows | Actual: 50,000 rows\n    Large scan indicates missing index or non-SARGable predicate\n\n2. [OK] Index Seek on Customers (5% cost)\n   Estimated: 100 rows | Actual: 98 rows\n    Good cardinality estimation\n\n3. [WARNING] Key Lookup on Orders (35% cost)\n    Missing columns: Amount, Status\n    Solution: Add INCLUDE (Amount, Status) to existing index\n\n4. [WARNING] Sort operator present (10% cost)\n    2MB memory grant, no spill\n    Consider index to eliminate sort\n\nRECOMMENDATIONS:\n\n1. Create covering index:\n   CREATE INDEX IX_Orders_Customer_Cover\n   ON Orders(CustomerID)\n   INCLUDE (Amount, Status, OrderDate);\n\n2. Update statistics:\n   UPDATE STATISTICS Orders WITH FULLSCAN;\n\nEXPECTED IMPROVEMENT:\n- Eliminate key lookup (35% cost reduction)\n- Better cardinality estimates\n\n================================\n```\n\n## Usage\n\n```bash\n# Describe plan\n/explain-plan I see a clustered index scan on Orders taking 80% of the cost\n\n# Paste XML\n/explain-plan <ShowPlanXML xmlns=...>\n\n# Ask about specific operator\n/explain-plan What does Hash Match Aggregate mean?\n```\n\n## Tips\n\n- Enable \"Include Actual Execution Plan\" in SSMS\n- Compare estimated vs actual row counts\n- Look at wait stats in SQL 2016+ plans\n- Use Query Store for plan history"
              },
              {
                "name": "/suggest-indexes",
                "description": "Suggest optimal indexes for a T-SQL query or table based on query patterns",
                "path": "plugins/tsql-master/commands/suggest-indexes.md",
                "frontmatter": {
                  "description": "Suggest optimal indexes for a T-SQL query or table based on query patterns",
                  "argument-hint": "[query or table name]"
                },
                "content": "# Suggest Indexes\n\nGenerate index recommendations for T-SQL queries or tables.\n\n## Process\n\n### Step 1: Parse Input\n\nIf input is a query:\n- Extract table names from FROM/JOIN clauses\n- Identify WHERE clause predicates\n- Note ORDER BY columns\n- List SELECT columns\n\nIf input is a table name:\n- Ask for typical query patterns or\n- Use DMV missing index recommendations\n\n### Step 2: Analyze Query Patterns\n\nFor each table in query:\n\n**Equality Predicates (=)**\n- First candidates for index key columns\n- Most selective columns first\n\n**Inequality Predicates (<, >, BETWEEN, LIKE 'x%')**\n- After equality columns in key\n- Only one inequality benefits from index\n\n**JOIN Columns**\n- Foreign key columns\n- Should match data types exactly\n\n**ORDER BY Columns**\n- Can eliminate sort operation\n- Consider ASC/DESC requirements\n\n**SELECT Columns**\n- Candidates for INCLUDE clause\n- Creates covering index\n\n### Step 3: Generate Recommendations\n\nFor each recommended index:\n\n```sql\n-- Purpose: [What queries this index supports]\n-- Expected benefit: [Seek vs scan, eliminated lookup, etc.]\nCREATE NONCLUSTERED INDEX IX_TableName_Columns\nON SchemaName.TableName (KeyColumn1, KeyColumn2)\nINCLUDE (IncludeColumn1, IncludeColumn2)\nWHERE [FilterCondition];  -- If filtered index recommended\n```\n\n### Step 4: Consider Existing Indexes\n\nQuery to check existing indexes:\n```sql\nSELECT\n    i.name AS IndexName,\n    i.type_desc,\n    STUFF((\n        SELECT ', ' + c.name + CASE WHEN ic.is_descending_key = 1 THEN ' DESC' ELSE '' END\n        FROM sys.index_columns ic\n        JOIN sys.columns c ON ic.object_id = c.object_id AND ic.column_id = c.column_id\n        WHERE ic.object_id = i.object_id AND ic.index_id = i.index_id AND ic.is_included_column = 0\n        ORDER BY ic.key_ordinal\n        FOR XML PATH('')\n    ), 1, 2, '') AS KeyColumns,\n    STUFF((\n        SELECT ', ' + c.name\n        FROM sys.index_columns ic\n        JOIN sys.columns c ON ic.object_id = c.object_id AND ic.column_id = c.column_id\n        WHERE ic.object_id = i.object_id AND ic.index_id = i.index_id AND ic.is_included_column = 1\n        ORDER BY ic.key_ordinal\n        FOR XML PATH('')\n    ), 1, 2, '') AS IncludedColumns\nFROM sys.indexes i\nWHERE i.object_id = OBJECT_ID('TableName')\nORDER BY i.index_id;\n```\n\n### Step 5: DMV Missing Index Check\n\n```sql\nSELECT\n    migs.avg_user_impact AS ImpactPercent,\n    mid.statement AS TableName,\n    mid.equality_columns,\n    mid.inequality_columns,\n    mid.included_columns,\n    migs.user_seeks,\n    migs.user_scans\nFROM sys.dm_db_missing_index_groups mig\nJOIN sys.dm_db_missing_index_group_stats migs ON mig.index_group_handle = migs.group_handle\nJOIN sys.dm_db_missing_index_details mid ON mig.index_handle = mid.index_handle\nWHERE mid.database_id = DB_ID()\n  AND OBJECT_NAME(mid.object_id) = 'TableName'\nORDER BY migs.avg_user_impact DESC;\n```\n\n## Output Format\n\n```\n================================\nIndex Recommendations\n================================\n\nTABLE: dbo.Orders\n\nQUERY PATTERN ANALYSIS:\n- Filters: CustomerID (=), OrderDate (range)\n- Joins: CustomerID -> Customers.CustomerID\n- Order: OrderDate DESC\n- Selects: OrderID, Amount, Status\n\nRECOMMENDED INDEXES:\n\n1. PRIMARY INDEX (High Impact)\n   CREATE NONCLUSTERED INDEX IX_Orders_Customer_Date\n   ON dbo.Orders (CustomerID, OrderDate DESC)\n   INCLUDE (Amount, Status);\n\n   Benefit: Covers filter + order + select columns\n   Eliminates: Table scan, sort operation, key lookup\n\n2. FILTERED INDEX (Medium Impact)\n   CREATE NONCLUSTERED INDEX IX_Orders_Active_Customer\n   ON dbo.Orders (CustomerID, OrderDate)\n   WHERE Status = 'Active';\n\n   Benefit: Smaller index for common filter pattern\n\nEXISTING INDEX CONFLICTS:\n- IX_Orders_CustomerID overlaps with recommendation 1\n  Consider: DROP or keep if other queries need it\n\nMAINTENANCE IMPACT:\n- New indexes will slow INSERT/UPDATE by ~5-10%\n- Recommended maintenance: Weekly REORGANIZE\n\n================================\n```\n\n## Usage\n\n```bash\n# From query\n/suggest-indexes SELECT * FROM Orders WHERE CustomerID = @ID ORDER BY OrderDate\n\n# From table name\n/suggest-indexes Orders\n\n# Multiple tables\n/suggest-indexes \"Orders JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID\"\n```\n\n## Best Practices\n\n1. **Don't over-index** - Each index adds write overhead\n2. **Consider covering indexes** - Eliminate key lookups\n3. **Use filtered indexes** - For well-known subsets\n4. **Check existing indexes** - Consolidate similar indexes\n5. **Test before production** - Verify with actual workload"
              }
            ],
            "skills": [
              {
                "name": "advanced-patterns",
                "description": "Advanced T-SQL patterns and techniques for SQL Server. Use this skill when: (1) User needs help with CTEs or recursive queries, (2) User asks about APPLY operator, (3) User wants MERGE or OUTPUT clause help, (4) User works with temporal tables, (5) User needs In-Memory OLTP guidance, (6) User asks about advanced grouping (ROLLUP, CUBE, GROUPING SETS).\n",
                "path": "plugins/tsql-master/skills/advanced-patterns/SKILL.md",
                "frontmatter": {
                  "name": "advanced-patterns",
                  "description": "Advanced T-SQL patterns and techniques for SQL Server. Use this skill when: (1) User needs help with CTEs or recursive queries, (2) User asks about APPLY operator, (3) User wants MERGE or OUTPUT clause help, (4) User works with temporal tables, (5) User needs In-Memory OLTP guidance, (6) User asks about advanced grouping (ROLLUP, CUBE, GROUPING SETS).\n"
                },
                "content": "# Advanced T-SQL Patterns\n\nAdvanced techniques for complex SQL Server scenarios.\n\n## Quick Reference\n\n### Pattern Selection Guide\n\n| Task | Pattern |\n|------|---------|\n| Hierarchical data | Recursive CTE |\n| Top N per group | ROW_NUMBER + CTE |\n| Correlated subquery alternative | CROSS/OUTER APPLY |\n| Upsert (insert or update) | MERGE |\n| Capture modified rows | OUTPUT clause |\n| Historical data tracking | Temporal tables |\n| High-throughput OLTP | In-Memory OLTP |\n| Multiple aggregation levels | ROLLUP/CUBE/GROUPING SETS |\n\n## Common Table Expressions (CTEs)\n\n### Basic CTE\n```sql\nWITH RecentOrders AS (\n    SELECT CustomerID, OrderDate, Amount\n    FROM Orders\n    WHERE OrderDate >= DATEADD(month, -3, GETDATE())\n)\nSELECT c.CustomerName, r.Amount\nFROM Customers c\nJOIN RecentOrders r ON c.CustomerID = r.CustomerID;\n```\n\n### Multiple CTEs\n```sql\nWITH\nSales AS (\n    SELECT ProductID, SUM(Amount) AS TotalSales FROM Orders GROUP BY ProductID\n),\nInventory AS (\n    SELECT ProductID, SUM(Quantity) AS TotalInventory FROM Stock GROUP BY ProductID\n)\nSELECT p.ProductName, s.TotalSales, i.TotalInventory\nFROM Products p\nLEFT JOIN Sales s ON p.ProductID = s.ProductID\nLEFT JOIN Inventory i ON p.ProductID = i.ProductID;\n```\n\n### Recursive CTE (Hierarchies)\n```sql\nWITH OrgChart AS (\n    -- Anchor: Top-level (no manager)\n    SELECT EmployeeID, Name, ManagerID, 0 AS Level,\n           CAST(Name AS VARCHAR(1000)) AS Path\n    FROM Employees\n    WHERE ManagerID IS NULL\n\n    UNION ALL\n\n    -- Recursive: Subordinates\n    SELECT e.EmployeeID, e.Name, e.ManagerID, oc.Level + 1,\n           CAST(oc.Path + ' > ' + e.Name AS VARCHAR(1000))\n    FROM Employees e\n    JOIN OrgChart oc ON e.ManagerID = oc.EmployeeID\n)\nSELECT * FROM OrgChart\nOPTION (MAXRECURSION 100);  -- Default is 100, max is 32767\n```\n\n### CTE for Deleting Duplicates\n```sql\nWITH Duplicates AS (\n    SELECT *,\n           ROW_NUMBER() OVER (\n               PARTITION BY Email\n               ORDER BY CreatedDate DESC\n           ) AS RowNum\n    FROM Users\n)\nDELETE FROM Duplicates WHERE RowNum > 1;\n```\n\n## APPLY Operator\n\n### CROSS APPLY (Inner Join Behavior)\n```sql\n-- Top 3 orders per customer\nSELECT c.CustomerID, c.Name, o.OrderID, o.Amount\nFROM Customers c\nCROSS APPLY (\n    SELECT TOP 3 OrderID, Amount\n    FROM Orders\n    WHERE CustomerID = c.CustomerID\n    ORDER BY OrderDate DESC\n) o;\n```\n\n### OUTER APPLY (Left Join Behavior)\n```sql\n-- Last order per customer (including customers with no orders)\nSELECT c.CustomerID, c.Name, o.LastOrderDate, o.LastOrderAmount\nFROM Customers c\nOUTER APPLY (\n    SELECT TOP 1 OrderDate AS LastOrderDate, Amount AS LastOrderAmount\n    FROM Orders\n    WHERE CustomerID = c.CustomerID\n    ORDER BY OrderDate DESC\n) o;\n```\n\n### APPLY with Table-Valued Function\n```sql\n-- Call function for each row\nSELECT c.CustomerID, f.MonthlyTotal, f.OrderCount\nFROM Customers c\nCROSS APPLY dbo.GetCustomerMonthlyStats(c.CustomerID) f;\n```\n\n### APPLY to Unpivot Columns\n```sql\n-- Transform columns to rows\nSELECT ID, AttributeName, AttributeValue\nFROM Products\nCROSS APPLY (\n    VALUES\n        ('Color', Color),\n        ('Size', Size),\n        ('Weight', CAST(Weight AS VARCHAR))\n) AS Unpivoted(AttributeName, AttributeValue)\nWHERE AttributeValue IS NOT NULL;\n```\n\n## MERGE Statement\n\n### Basic Upsert\n```sql\nMERGE INTO TargetTable AS t\nUSING SourceTable AS s\nON t.ID = s.ID\nWHEN MATCHED THEN\n    UPDATE SET t.Name = s.Name, t.Value = s.Value, t.UpdatedAt = GETDATE()\nWHEN NOT MATCHED BY TARGET THEN\n    INSERT (ID, Name, Value, CreatedAt)\n    VALUES (s.ID, s.Name, s.Value, GETDATE())\nWHEN NOT MATCHED BY SOURCE THEN\n    DELETE\nOUTPUT $action, inserted.*, deleted.*;\n```\n\n### MERGE with Conditions\n```sql\nMERGE INTO Products AS t\nUSING StagingProducts AS s\nON t.ProductID = s.ProductID\nWHEN MATCHED AND s.Price <> t.Price THEN\n    UPDATE SET t.Price = s.Price, t.LastModified = GETDATE()\nWHEN MATCHED AND s.Discontinued = 1 THEN\n    DELETE\nWHEN NOT MATCHED THEN\n    INSERT (ProductID, Name, Price) VALUES (s.ProductID, s.Name, s.Price);\n```\n\n## OUTPUT Clause\n\n### Capture Inserted Rows\n```sql\nDECLARE @InsertedRows TABLE (ID INT, Name VARCHAR(100));\n\nINSERT INTO Customers (Name, Email)\nOUTPUT inserted.CustomerID, inserted.Name INTO @InsertedRows\nVALUES ('John', 'john@email.com'), ('Jane', 'jane@email.com');\n\nSELECT * FROM @InsertedRows;\n```\n\n### Capture Updated Rows (Before and After)\n```sql\nUPDATE Products\nSET Price = Price * 1.1\nOUTPUT deleted.ProductID, deleted.Price AS OldPrice, inserted.Price AS NewPrice\nWHERE Category = 'Electronics';\n```\n\n### Capture Deleted Rows\n```sql\nDELETE FROM ExpiredOrders\nOUTPUT deleted.*\nINTO OrderArchive\nWHERE ExpiryDate < DATEADD(year, -1, GETDATE());\n```\n\n## Advanced Grouping\n\n### ROLLUP (Hierarchical Subtotals)\n```sql\nSELECT\n    COALESCE(Region, 'Total') AS Region,\n    COALESCE(Product, 'All Products') AS Product,\n    SUM(Sales) AS TotalSales\nFROM SalesData\nGROUP BY ROLLUP (Region, Product);\n-- Groups: (Region, Product), (Region), ()\n```\n\n### CUBE (All Combinations)\n```sql\nSELECT Region, Product, SUM(Sales) AS TotalSales\nFROM SalesData\nGROUP BY CUBE (Region, Product);\n-- Groups: (Region, Product), (Region), (Product), ()\n```\n\n### GROUPING SETS (Custom Combinations)\n```sql\nSELECT Region, Product, Year, SUM(Sales)\nFROM SalesData\nGROUP BY GROUPING SETS (\n    (Region, Product),\n    (Region, Year),\n    (Product),\n    ()\n);\n```\n\n### Identify Grouping Level\n```sql\nSELECT\n    CASE WHEN GROUPING(Region) = 1 THEN 'All' ELSE Region END AS Region,\n    CASE WHEN GROUPING(Product) = 1 THEN 'All' ELSE Product END AS Product,\n    SUM(Sales) AS TotalSales,\n    GROUPING_ID(Region, Product) AS GroupLevel\n    -- GroupLevel: 0 = both, 1 = Product rolled up, 2 = Region rolled up, 3 = both\nFROM SalesData\nGROUP BY ROLLUP (Region, Product);\n```\n\n## Temporal Tables (SQL 2016+)\n\n### Create System-Versioned Table\n```sql\nCREATE TABLE Products (\n    ProductID INT PRIMARY KEY,\n    Name NVARCHAR(100),\n    Price DECIMAL(18,2),\n    ValidFrom DATETIME2 GENERATED ALWAYS AS ROW START,\n    ValidTo DATETIME2 GENERATED ALWAYS AS ROW END,\n    PERIOD FOR SYSTEM_TIME (ValidFrom, ValidTo)\n)\nWITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.ProductsHistory));\n```\n\n### Query Historical Data\n```sql\n-- Point in time\nSELECT * FROM Products\nFOR SYSTEM_TIME AS OF '2024-01-01 12:00:00';\n\n-- Time range\nSELECT * FROM Products\nFOR SYSTEM_TIME BETWEEN '2024-01-01' AND '2024-06-30';\n\n-- All history\nSELECT * FROM Products\nFOR SYSTEM_TIME ALL;\n```\n\n## In-Memory OLTP\n\n### Create Memory-Optimized Table\n```sql\n-- First add filegroup\nALTER DATABASE YourDB\nADD FILEGROUP MemOptFG CONTAINS MEMORY_OPTIMIZED_DATA;\n\nALTER DATABASE YourDB\nADD FILE (NAME = 'MemOptFile', FILENAME = 'C:\\Data\\MemOpt') TO FILEGROUP MemOptFG;\n\n-- Create table\nCREATE TABLE OrdersMemOpt (\n    OrderID INT NOT NULL PRIMARY KEY NONCLUSTERED HASH WITH (BUCKET_COUNT = 1000000),\n    CustomerID INT NOT NULL INDEX IX_Customer NONCLUSTERED HASH WITH (BUCKET_COUNT = 100000),\n    OrderDate DATETIME2 NOT NULL,\n    Amount DECIMAL(18,2) NOT NULL,\n    INDEX IX_Date NONCLUSTERED (OrderDate)\n) WITH (MEMORY_OPTIMIZED = ON, DURABILITY = SCHEMA_AND_DATA);\n```\n\n### Natively Compiled Procedure\n```sql\nCREATE PROCEDURE InsertOrderFast\n    @CustomerID INT,\n    @Amount DECIMAL(18,2)\nWITH NATIVE_COMPILATION, SCHEMABINDING\nAS\nBEGIN ATOMIC WITH (TRANSACTION ISOLATION LEVEL = SNAPSHOT, LANGUAGE = N'English')\n    INSERT INTO dbo.OrdersMemOpt (OrderID, CustomerID, OrderDate, Amount)\n    VALUES (NEXT VALUE FOR dbo.OrderSeq, @CustomerID, SYSDATETIME(), @Amount);\nEND;\n```\n\n## Table-Valued Constructor\n\n### VALUES as Table\n```sql\nSELECT * FROM (\n    VALUES\n        (1, 'Apple', 1.50),\n        (2, 'Banana', 0.75),\n        (3, 'Orange', 2.00)\n) AS Products(ID, Name, Price);\n```\n\n### Use in MERGE\n```sql\nMERGE INTO Products AS t\nUSING (VALUES\n    (1, 'Apple', 1.60),\n    (2, 'Banana', 0.80)\n) AS s(ID, Name, Price)\nON t.ID = s.ID\nWHEN MATCHED THEN UPDATE SET Price = s.Price\nWHEN NOT MATCHED THEN INSERT VALUES (s.ID, s.Name, s.Price);\n```\n\n## Sequences\n\n### Create and Use Sequence\n```sql\nCREATE SEQUENCE OrderSeq\n    AS INT START WITH 1 INCREMENT BY 1;\n\n-- Get next value\nSELECT NEXT VALUE FOR OrderSeq;\n\n-- Use in INSERT\nINSERT INTO Orders (OrderID, CustomerID)\nVALUES (NEXT VALUE FOR OrderSeq, @CustomerID);\n\n-- Use as default\nALTER TABLE Orders\nADD CONSTRAINT DF_OrderID DEFAULT NEXT VALUE FOR OrderSeq FOR OrderID;\n```"
              },
              {
                "name": "azure-sql-optimization",
                "description": "Azure SQL Database optimization and platform-specific features. Use this skill when: (1) User asks about Azure SQL Database optimization, (2) User needs DTU/vCore guidance, (3) User wants to use automatic tuning, (4) User asks about Hyperscale or serverless, (5) User needs Azure SQL performance monitoring.\n",
                "path": "plugins/tsql-master/skills/azure-sql-optimization/SKILL.md",
                "frontmatter": {
                  "name": "azure-sql-optimization",
                  "description": "Azure SQL Database optimization and platform-specific features. Use this skill when: (1) User asks about Azure SQL Database optimization, (2) User needs DTU/vCore guidance, (3) User wants to use automatic tuning, (4) User asks about Hyperscale or serverless, (5) User needs Azure SQL performance monitoring.\n"
                },
                "content": "# Azure SQL Database Optimization\n\nPlatform-specific optimization for Azure SQL Database.\n\n## Quick Reference\n\n### Service Tier Comparison\n\n| Tier | Best For | Max Size | Key Features |\n|------|----------|----------|--------------|\n| Basic | Dev/test, light workloads | 2 GB | Low cost |\n| Standard | General workloads | 1 TB | S0-S12 DTUs |\n| Premium | High I/O, low latency | 4 TB | P1-P15 DTUs |\n| General Purpose (vCore) | Most workloads | 16 TB | Serverless option |\n| Business Critical | High availability | 4 TB | In-memory, read replicas |\n| Hyperscale | Large databases | 100 TB | Auto-scaling storage |\n\n### DTU vs vCore\n\n| Aspect | DTU | vCore |\n|--------|-----|-------|\n| Pricing | Bundled resources | Separate compute/storage |\n| Control | Limited | Fine-grained |\n| Reserved capacity | No | Yes (up to 72% savings) |\n| Serverless | No | Yes (General Purpose) |\n| Best for | Simple workloads | Predictable, migrated workloads |\n\n## Performance Monitoring\n\n### Resource Consumption\n```sql\n-- Last 15 minutes (avg 15-second intervals)\nSELECT\n    end_time,\n    avg_cpu_percent,\n    avg_data_io_percent,\n    avg_log_write_percent,\n    avg_memory_usage_percent,\n    max_worker_percent,\n    max_session_percent\nFROM sys.dm_db_resource_stats\nORDER BY end_time DESC;\n\n-- Historical (last 14 days, hourly)\nSELECT\n    start_time,\n    end_time,\n    avg_cpu_percent,\n    avg_data_io_percent,\n    avg_log_write_percent\nFROM sys.resource_stats\nWHERE database_name = DB_NAME()\nORDER BY start_time DESC;\n```\n\n### Query Performance Insight\n```sql\n-- Top CPU consumers last hour\nSELECT TOP 20\n    qt.query_sql_text,\n    rs.avg_cpu_time / 1000 AS avg_cpu_ms,\n    rs.count_executions,\n    rs.avg_cpu_time * rs.count_executions / 1000 AS total_cpu_ms\nFROM sys.query_store_query q\nJOIN sys.query_store_query_text qt ON q.query_text_id = qt.query_text_id\nJOIN sys.query_store_plan p ON q.query_id = p.query_id\nJOIN sys.query_store_runtime_stats rs ON p.plan_id = rs.plan_id\nJOIN sys.query_store_runtime_stats_interval rsi ON rs.runtime_stats_interval_id = rsi.runtime_stats_interval_id\nWHERE rsi.start_time >= DATEADD(hour, -1, GETUTCDATE())\nORDER BY rs.avg_cpu_time * rs.count_executions DESC;\n```\n\n## Automatic Tuning\n\n### Enable Automatic Tuning\n```sql\n-- Enable all auto-tuning options\nALTER DATABASE current\nSET AUTOMATIC_TUNING (\n    FORCE_LAST_GOOD_PLAN = ON,\n    CREATE_INDEX = ON,\n    DROP_INDEX = ON\n);\n\n-- Check current settings\nSELECT * FROM sys.database_automatic_tuning_options;\n```\n\n### View Tuning Recommendations\n```sql\n-- Current recommendations\nSELECT\n    name,\n    reason,\n    score,\n    state_desc,\n    is_revertable_action,\n    is_executable_action,\n    details\nFROM sys.dm_db_tuning_recommendations;\n```\n\n### Apply Recommendations\n```sql\n-- Force a specific query plan\nEXEC sp_query_store_force_plan @query_id = 12345, @plan_id = 67890;\n\n-- Unforce plan\nEXEC sp_query_store_unforce_plan @query_id = 12345, @plan_id = 67890;\n```\n\n## Hyperscale Features\n\n### Storage Auto-Scaling\n- Automatically grows up to 128 TB\n- No need to pre-provision storage\n- Pay only for storage used\n\n### Read Scale-Out\n```sql\n-- Connection string option\nApplicationIntent=ReadOnly\n\n-- In application code\n\"Server=myserver.database.windows.net;Database=mydb;ApplicationIntent=ReadOnly;...\"\n```\n\n### Named Replicas\n```sql\n-- Create named replica\nALTER DATABASE MyDatabase\nADD SECONDARY ON SERVER MySecondaryServer\nWITH (SERVICE_OBJECTIVE = 'HS_Gen5_2', SECONDARY_TYPE = Named, NAME = N'MyReadReplica');\n```\n\n## Serverless Configuration\n\n### Configure Auto-Pause\n```sql\n-- Via Azure Portal, CLI, or PowerShell\n-- Set auto-pause delay (minutes), min/max vCores\n\n-- Check current usage\nSELECT\n    cpu_percent,\n    auto_pause_delay_in_minutes_configured\nFROM sys.dm_db_resource_stats_serverless;\n```\n\n### Serverless Best Practices\n1. **Use for intermittent workloads** - Saves cost during idle periods\n2. **Set appropriate min vCores** - Prevents cold starts for time-sensitive apps\n3. **Monitor auto-pause** - Auto-resume adds latency\n4. **Consider always-on for consistent workloads** - Provisioned may be cheaper\n\n## Connection Optimization\n\n### Connection Pooling\n```csharp\n// .NET connection string\n\"Server=tcp:myserver.database.windows.net,1433;Database=mydb;\n Min Pool Size=10;Max Pool Size=100;Connection Timeout=30;\"\n```\n\n### Retry Logic\n```csharp\n// Azure SQL requires retry logic for transient faults\nvar options = new SqlRetryLogicOption()\n{\n    NumberOfTries = 5,\n    DeltaTime = TimeSpan.FromSeconds(1),\n    MaxTimeInterval = TimeSpan.FromSeconds(30)\n};\n```\n\n### Connection Best Practices\n1. **Use connection pooling** - Reduce connection overhead\n2. **Implement retry logic** - Handle transient faults (error 40613, 40197)\n3. **Use redirect connection mode** - Better performance after initial connection\n4. **Close connections promptly** - Don't hold connections unnecessarily\n\n## Azure-Specific Limitations\n\n### Not Supported\n- SQL Agent (use Azure Functions, Logic Apps)\n- BULK INSERT from files (use Blob Storage)\n- Linked servers (use Elastic Query)\n- FILESTREAM\n- Cross-database queries in same server (use Elastic Query)\n\n### Workarounds\n\n#### Bulk Insert from Blob Storage\n```sql\n-- Create credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your_sas_token';\n\n-- Create external data source\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = BLOB_STORAGE,\n    LOCATION = 'https://youraccount.blob.core.windows.net/container',\n    CREDENTIAL = BlobCredential\n);\n\n-- Bulk insert\nBULK INSERT MyTable\nFROM 'data.csv'\nWITH (DATA_SOURCE = 'BlobStorage', FORMAT = 'CSV', FIRSTROW = 2);\n```\n\n#### Elastic Query for Cross-Database\n```sql\n-- On target database\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'password';\n\nCREATE DATABASE SCOPED CREDENTIAL ElasticCredential\nWITH IDENTITY = 'username', SECRET = 'password';\n\nCREATE EXTERNAL DATA SOURCE RemoteDB\nWITH (\n    TYPE = RDBMS,\n    LOCATION = 'remote-server.database.windows.net',\n    DATABASE_NAME = 'RemoteDatabase',\n    CREDENTIAL = ElasticCredential\n);\n\nCREATE EXTERNAL TABLE dbo.RemoteTable (...)\nWITH (DATA_SOURCE = RemoteDB);\n```\n\n## Cost Optimization\n\n### Reserved Capacity\n- Up to 72% savings vs pay-as-you-go\n- 1-year or 3-year terms\n- Exchange/refund flexibility\n\n### Right-Sizing\n```sql\n-- Check if over-provisioned\nSELECT\n    AVG(avg_cpu_percent) AS avg_cpu,\n    MAX(avg_cpu_percent) AS max_cpu,\n    AVG(avg_data_io_percent) AS avg_io,\n    MAX(avg_data_io_percent) AS max_io\nFROM sys.dm_db_resource_stats\nWHERE end_time >= DATEADD(day, -7, GETUTCDATE());\n\n-- If avg < 40% consistently, consider downsizing\n```\n\n### Hyperscale Cost Considerations\n- Compute: Per-second billing\n- Storage: Per-hour billing for used space\n- Read replicas: Additional compute cost\n- Memory not automatically released (monitor and scale appropriately)"
              },
              {
                "name": "index-strategies",
                "description": "SQL Server index design and optimization strategies. Use this skill when: (1) User needs help designing indexes, (2) User asks about clustered vs nonclustered indexes, (3) User wants to optimize columnstore indexes, (4) User needs filtered or covering indexes, (5) User asks about index maintenance and fragmentation.\n",
                "path": "plugins/tsql-master/skills/index-strategies/SKILL.md",
                "frontmatter": {
                  "name": "index-strategies",
                  "description": "SQL Server index design and optimization strategies. Use this skill when: (1) User needs help designing indexes, (2) User asks about clustered vs nonclustered indexes, (3) User wants to optimize columnstore indexes, (4) User needs filtered or covering indexes, (5) User asks about index maintenance and fragmentation.\n"
                },
                "content": "# Index Strategies\n\nComprehensive guide to SQL Server index design and optimization.\n\n## Quick Reference\n\n### Index Types\n\n| Type | Description | Best For |\n|------|-------------|----------|\n| Clustered | Table data order | Primary access path, range scans |\n| Nonclustered | Separate structure | Specific query patterns |\n| Columnstore | Column-based storage | Analytics, aggregations |\n| Filtered | Partial index | Well-known subsets |\n| Covering | All columns needed | Avoiding key lookups |\n\n### Clustered Index Guidelines\n\n**Ideal Clustered Key:**\n- Narrow (small data type)\n- Unique or mostly unique\n- Ever-increasing (identity, sequential GUID)\n- Static (rarely updated)\n\n```sql\n-- Good: Identity column\nCREATE CLUSTERED INDEX CIX_Orders ON Orders(OrderID);\n\n-- Good: Sequential GUID\nCREATE TABLE Orders (\n    OrderID UNIQUEIDENTIFIER DEFAULT NEWSEQUENTIALID() PRIMARY KEY CLUSTERED\n);\n\n-- Avoid: Wide composite keys, frequently updated columns, GUIDs (NEWID)\n```\n\n### Nonclustered Index Design\n\n```sql\n-- Basic index\nCREATE NONCLUSTERED INDEX IX_Orders_CustomerID\nON Orders(CustomerID);\n\n-- Covering index (avoids key lookup)\nCREATE NONCLUSTERED INDEX IX_Orders_CustomerID_Cover\nON Orders(CustomerID)\nINCLUDE (OrderDate, TotalAmount, Status);\n\n-- Filtered index (partial)\nCREATE NONCLUSTERED INDEX IX_Orders_Active\nON Orders(CustomerID, OrderDate)\nWHERE Status = 'Active';\n\n-- Descending order\nCREATE NONCLUSTERED INDEX IX_Orders_DateDesc\nON Orders(OrderDate DESC, OrderID DESC);\n```\n\n## Index Selection Guide\n\n### By Query Pattern\n\n| Pattern | Recommended Index |\n|---------|-------------------|\n| `WHERE Col = value` | Nonclustered on Col |\n| `WHERE Col = v1 AND Col2 = v2` | Nonclustered on (Col, Col2) |\n| `WHERE Col = v ORDER BY Col2` | Nonclustered on (Col, Col2) |\n| `WHERE Col BETWEEN x AND y` | Col as leftmost key |\n| `SELECT * WHERE Col = v` | Clustered or covering NC |\n| Large aggregations | Columnstore |\n| Specific subset queries | Filtered index |\n\n### Column Order in Composite Keys\n\n```sql\n-- Order matters! Left-to-right matching\nCREATE INDEX IX_Example ON Table(A, B, C);\n\n-- These queries CAN use the index:\nWHERE A = 1\nWHERE A = 1 AND B = 2\nWHERE A = 1 AND B = 2 AND C = 3\nWHERE A = 1 AND B > 5 ORDER BY B\n\n-- These queries CANNOT use index seek:\nWHERE B = 2                    -- A not specified\nWHERE B = 2 AND C = 3          -- A not specified\nWHERE A = 1 AND C = 3          -- B skipped (partial match only)\n```\n\n## Columnstore Indexes\n\n### Clustered Columnstore\n```sql\n-- Best for data warehousing\nCREATE CLUSTERED COLUMNSTORE INDEX CCI_FactSales\nON FactSales;\n\n-- Ordered columnstore (SQL 2022+)\nCREATE CLUSTERED COLUMNSTORE INDEX CCI_FactSales\nON FactSales\nORDER (DateKey, ProductKey);\n```\n\n### Nonclustered Columnstore\n```sql\n-- Hybrid OLTP/OLAP\nCREATE NONCLUSTERED COLUMNSTORE INDEX NCCI_Orders_Analysis\nON Orders(OrderDate, ProductID, Quantity, Amount)\nWHERE Status = 'Completed';\n```\n\n### Columnstore Best Practices\n1. **Load batches >= 102,400 rows** - Creates compressed segments\n2. **Order data by filtered columns** - Better segment elimination\n3. **Use REORGANIZE, not REBUILD** - More efficient maintenance\n4. **Avoid frequent small updates** - Causes deltastore fragmentation\n5. **Partition by date** - Enables partition elimination\n\n```sql\n-- Maintenance\nALTER INDEX CCI_FactSales ON FactSales REORGANIZE;\n\n-- Check fragmentation\nSELECT\n    object_name(object_id) AS TableName,\n    index_id,\n    avg_fragmentation_in_percent,\n    fragment_count\nFROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'LIMITED');\n```\n\n## Filtered Indexes\n\n```sql\n-- Index active orders only\nCREATE NONCLUSTERED INDEX IX_Orders_Active\nON Orders(CustomerID, OrderDate)\nWHERE Status = 'Active';\n\n-- Index non-NULL values\nCREATE UNIQUE INDEX IX_Users_Email\nON Users(Email)\nWHERE Email IS NOT NULL;\n\n-- Constraints:\n-- - Cannot use variable in filter\n-- - Query WHERE must match or be subset of filter WHERE\n-- - May cause parameter sniffing issues\n```\n\n## Covering Indexes\n\n```sql\n-- Eliminate key lookups\n-- Original: Index on CustomerID, query selects OrderDate, Amount\n-- Execution plan shows Key Lookup\n\n-- Solution: Covering index\nCREATE INDEX IX_Orders_CustomerID_Cover\nON Orders(CustomerID)\nINCLUDE (OrderDate, Amount, Status);\n\n-- INCLUDE columns:\n-- - Not in key (not sorted)\n-- - Stored at leaf level only\n-- - Don't contribute to 900-byte key limit\n-- - Perfect for frequently selected columns\n```\n\n## Index Maintenance\n\n### Fragmentation Guidelines\n\n| Fragmentation % | Action |\n|-----------------|--------|\n| < 5% | None needed |\n| 5-30% | REORGANIZE |\n| > 30% | REBUILD |\n\n```sql\n-- Reorganize (online, minimal locking)\nALTER INDEX IX_Orders_CustomerID ON Orders REORGANIZE;\n\n-- Rebuild (offline by default, more thorough)\nALTER INDEX IX_Orders_CustomerID ON Orders REBUILD;\n\n-- Online rebuild (Enterprise Edition)\nALTER INDEX IX_Orders_CustomerID ON Orders\nREBUILD WITH (ONLINE = ON);\n\n-- Resumable rebuild (SQL 2017+)\nALTER INDEX IX_Orders_CustomerID ON Orders\nREBUILD WITH (ONLINE = ON, RESUMABLE = ON, MAX_DURATION = 60);\n\n-- Resume interrupted rebuild\nALTER INDEX IX_Orders_CustomerID ON Orders RESUME;\n```\n\n### Statistics Update\n```sql\n-- Update after index changes\nUPDATE STATISTICS Orders;\n\n-- Full scan for accurate stats\nUPDATE STATISTICS Orders WITH FULLSCAN;\n\n-- Check last update\nSELECT\n    OBJECT_NAME(object_id) AS TableName,\n    name AS StatsName,\n    STATS_DATE(object_id, stats_id) AS LastUpdated\nFROM sys.stats\nWHERE object_id = OBJECT_ID('Orders');\n```\n\n## Performance Monitoring\n\n### Index Usage Stats\n```sql\nSELECT\n    OBJECT_NAME(i.object_id) AS TableName,\n    i.name AS IndexName,\n    ius.user_seeks,\n    ius.user_scans,\n    ius.user_lookups,\n    ius.user_updates\nFROM sys.indexes i\nLEFT JOIN sys.dm_db_index_usage_stats ius\n    ON i.object_id = ius.object_id\n    AND i.index_id = ius.index_id\nWHERE OBJECTPROPERTY(i.object_id, 'IsUserTable') = 1\nORDER BY ius.user_seeks + ius.user_scans DESC;\n```\n\n### Missing Index Recommendations\n```sql\nSELECT\n    migs.avg_user_impact AS ImpactPercent,\n    mid.statement AS TableName,\n    mid.equality_columns,\n    mid.inequality_columns,\n    mid.included_columns\nFROM sys.dm_db_missing_index_groups mig\nJOIN sys.dm_db_missing_index_group_stats migs\n    ON mig.index_group_handle = migs.group_handle\nJOIN sys.dm_db_missing_index_details mid\n    ON mig.index_handle = mid.index_handle\nORDER BY migs.avg_user_impact DESC;\n```"
              },
              {
                "name": "query-optimization",
                "description": "T-SQL query optimization techniques for SQL Server and Azure SQL Database. Use this skill when: (1) User needs to optimize slow queries, (2) User asks about SARGability or index seeks, (3) User needs help with query hints, (4) User has parameter sniffing issues, (5) User needs to understand execution plans, (6) User asks about statistics and cardinality estimation.\n",
                "path": "plugins/tsql-master/skills/query-optimization/SKILL.md",
                "frontmatter": {
                  "name": "query-optimization",
                  "description": "T-SQL query optimization techniques for SQL Server and Azure SQL Database. Use this skill when: (1) User needs to optimize slow queries, (2) User asks about SARGability or index seeks, (3) User needs help with query hints, (4) User has parameter sniffing issues, (5) User needs to understand execution plans, (6) User asks about statistics and cardinality estimation.\n"
                },
                "content": "# Query Optimization\n\nComprehensive guide to T-SQL query optimization techniques.\n\n## Quick Reference\n\n### SARGable vs Non-SARGable Patterns\n\n| Non-SARGable (Bad) | SARGable (Good) |\n|-------------------|-----------------|\n| `WHERE YEAR(Date) = 2024` | `WHERE Date >= '2024-01-01' AND Date < '2025-01-01'` |\n| `WHERE LEFT(Name, 3) = 'ABC'` | `WHERE Name LIKE 'ABC%'` |\n| `WHERE Amount * 1.1 > 1000` | `WHERE Amount > 1000 / 1.1` |\n| `WHERE ISNULL(Col, 0) = 5` | `WHERE Col = 5 OR Col IS NULL` |\n| `WHERE VarcharCol = 123` | `WHERE VarcharCol = '123'` |\n\n### Join Types Performance\n\n| Join Type | Best For | Characteristics |\n|-----------|----------|-----------------|\n| Nested Loop | Small outer, indexed inner | Low memory, good for small sets |\n| Merge Join | Sorted inputs, similar sizes | Efficient for sorted data |\n| Hash Join | Large unsorted inputs | High memory, good for large sets |\n\n### Query Hints Quick Reference\n\n| Hint | Purpose |\n|------|---------|\n| `OPTION (RECOMPILE)` | Fresh plan each execution |\n| `OPTION (OPTIMIZE FOR (@p = value))` | Optimize for specific value |\n| `OPTION (OPTIMIZE FOR UNKNOWN)` | Use average statistics |\n| `OPTION (MAXDOP n)` | Limit parallelism |\n| `OPTION (FORCE ORDER)` | Use exact join order |\n| `WITH (NOLOCK)` | Read uncommitted (dirty reads) |\n| `WITH (FORCESEEK)` | Force index seek |\n\n## Core Optimization Principles\n\n### 1. SARGability\n\nSARG = Search ARGument. SARGable queries can use index seeks:\n\n```sql\n-- Non-SARGable: Function on column\nWHERE DATEPART(year, OrderDate) = 2024\nWHERE UPPER(CustomerName) = 'JOHN'\nWHERE OrderAmount + 100 > 500\n\n-- SARGable: Preserve column\nWHERE OrderDate >= '2024-01-01' AND OrderDate < '2025-01-01'\nWHERE CustomerName = 'john' COLLATE SQL_Latin1_General_CP1_CI_AS\nWHERE OrderAmount > 400\n```\n\n### 2. Implicit Conversions\n\nAvoid data type mismatches:\n\n```sql\n-- Bad: Implicit conversion (varchar column compared to int)\nWHERE VarcharColumn = 12345\n\n-- Good: Match types exactly\nWHERE VarcharColumn = '12345'\n\n-- Check for implicit conversions in execution plan\n-- Look for CONVERT_IMPLICIT warnings\n```\n\n### 3. OR Optimization\n\nOR on different columns prevents seek:\n\n```sql\n-- Inefficient: OR on different columns\nSELECT * FROM Orders\nWHERE CustomerID = 1 OR ProductID = 2\n\n-- Better: UNION for OR optimization\nSELECT * FROM Orders WHERE CustomerID = 1\nUNION ALL\nSELECT * FROM Orders WHERE ProductID = 2 AND CustomerID <> 1\n```\n\n### 4. EXISTS vs IN vs JOIN\n\n```sql\n-- EXISTS: Best for semi-joins (checking existence)\nSELECT * FROM Customers c\nWHERE EXISTS (SELECT 1 FROM Orders o WHERE o.CustomerID = c.CustomerID)\n\n-- IN: Good for small static lists\nSELECT * FROM Products WHERE CategoryID IN (1, 2, 3)\n\n-- JOIN: Best when you need data from both tables\nSELECT c.*, o.OrderDate\nFROM Customers c\nJOIN Orders o ON c.CustomerID = o.CustomerID\n```\n\n## Parameter Sniffing Solutions\n\n### Problem\n```sql\n-- First execution with CustomerID=1 (10 rows) creates plan\n-- Subsequent execution with CustomerID=999 (1M rows) uses same plan\nCREATE PROCEDURE GetOrders @CustomerID INT AS\n    SELECT * FROM Orders WHERE CustomerID = @CustomerID\n```\n\n### Solution 1: OPTION (RECOMPILE)\n```sql\nCREATE PROCEDURE GetOrders @CustomerID INT AS\n    SELECT * FROM Orders\n    WHERE CustomerID = @CustomerID\n    OPTION (RECOMPILE)\n-- Best for: Infrequent queries, highly variable data distribution\n```\n\n### Solution 2: OPTIMIZE FOR\n```sql\n-- Optimize for specific value\nOPTION (OPTIMIZE FOR (@CustomerID = 1))\n\n-- Optimize for unknown (average statistics)\nOPTION (OPTIMIZE FOR UNKNOWN)\n```\n\n### Solution 3: Local Variables\n```sql\nCREATE PROCEDURE GetOrders @CustomerID INT AS\nBEGIN\n    DECLARE @LocalID INT = @CustomerID\n    SELECT * FROM Orders WHERE CustomerID = @LocalID\nEND\n-- Hides parameter from optimizer, similar to OPTIMIZE FOR UNKNOWN\n```\n\n### Solution 4: Query Store Hints (SQL 2022+)\n```sql\nEXEC sys.sp_query_store_set_hints\n    @query_id = 12345,\n    @hints = N'OPTION (RECOMPILE)'\n-- Apply hints without code changes\n```\n\n### Solution 5: PSP Optimization (SQL 2022+)\n```sql\n-- Enable Parameter Sensitive Plan optimization\nALTER DATABASE YourDB SET COMPATIBILITY_LEVEL = 160\n-- Automatically creates multiple plans based on parameter values\n```\n\n## Execution Plan Analysis\n\n### Key Operators to Watch\n\n| Operator | Warning Sign | Action |\n|----------|--------------|--------|\n| Table Scan | Missing index | Add appropriate index |\n| Index Scan | Non-SARGable predicate | Rewrite query |\n| Key Lookup | Missing covering index | Add INCLUDE columns |\n| Sort | Missing index for ORDER BY | Add sorted index |\n| Hash Match | Large memory grant | Consider index |\n| Spools | Repeated scans | Restructure query |\n\n### Estimated vs Actual Rows\n```sql\n-- Large difference indicates statistics problem\n-- Check if stats need updating:\nUPDATE STATISTICS TableName WITH FULLSCAN\n\n-- Or enable auto-update:\nALTER DATABASE YourDB SET AUTO_UPDATE_STATISTICS ON\n```\n\n### Finding Missing Indexes\n```sql\nSELECT\n    CONVERT(DECIMAL(18,2), migs.avg_user_impact) AS AvgImpact,\n    mid.statement AS TableName,\n    mid.equality_columns,\n    mid.inequality_columns,\n    mid.included_columns\nFROM sys.dm_db_missing_index_groups mig\nJOIN sys.dm_db_missing_index_group_stats migs ON mig.index_group_handle = migs.group_handle\nJOIN sys.dm_db_missing_index_details mid ON mig.index_handle = mid.index_handle\nWHERE mid.database_id = DB_ID()\nORDER BY migs.avg_user_impact DESC\n```\n\n## Statistics Management\n\n### View Statistics Info\n```sql\nDBCC SHOW_STATISTICS('TableName', 'IndexName')\n```\n\n### Update Statistics\n```sql\n-- Update all statistics on table\nUPDATE STATISTICS TableName\n\n-- Update with full scan (most accurate)\nUPDATE STATISTICS TableName WITH FULLSCAN\n\n-- Update specific statistics\nUPDATE STATISTICS TableName StatisticsName\n```\n\n### Auto-Update Settings\n```sql\n-- Enable async auto-update (better for OLTP)\nALTER DATABASE YourDB SET AUTO_UPDATE_STATISTICS_ASYNC ON\n```\n\n## Additional References\n\nFor deeper coverage of performance diagnostics, see:\n\n- `references/dmv-diagnostic-queries.md` - DMV queries for performance analysis"
              },
              {
                "name": "tsql-functions",
                "description": "Complete T-SQL function reference for SQL Server and Azure SQL Database. Use this skill when: (1) User asks about T-SQL string, date, math, or conversion functions, (2) User needs help with window/ranking functions, (3) User works with JSON or XML in T-SQL, (4) User asks about aggregate functions or GROUP BY, (5) User needs system or metadata functions.\n",
                "path": "plugins/tsql-master/skills/tsql-functions/SKILL.md",
                "frontmatter": {
                  "name": "tsql-functions",
                  "description": "Complete T-SQL function reference for SQL Server and Azure SQL Database. Use this skill when: (1) User asks about T-SQL string, date, math, or conversion functions, (2) User needs help with window/ranking functions, (3) User works with JSON or XML in T-SQL, (4) User asks about aggregate functions or GROUP BY, (5) User needs system or metadata functions.\n"
                },
                "content": "# T-SQL Functions Reference\n\nComplete reference for all T-SQL function categories with version-specific availability.\n\n## Quick Reference\n\n### String Functions\n| Function | Description | Version |\n|----------|-------------|---------|\n| `CONCAT(str1, str2, ...)` | NULL-safe concatenation | 2012+ |\n| `CONCAT_WS(sep, str1, ...)` | Concatenate with separator | 2017+ |\n| `STRING_AGG(expr, sep)` | Aggregate strings | 2017+ |\n| `STRING_SPLIT(str, sep)` | Split to rows | 2016+ |\n| `STRING_SPLIT(str, sep, 1)` | With ordinal column | 2022+ |\n| `TRIM([chars FROM] str)` | Remove leading/trailing | 2017+ |\n| `TRANSLATE(str, from, to)` | Character replacement | 2017+ |\n| `FORMAT(value, format)` | .NET format strings | 2012+ |\n\n### Date/Time Functions\n| Function | Description | Version |\n|----------|-------------|---------|\n| `DATEADD(part, n, date)` | Add interval | All |\n| `DATEDIFF(part, start, end)` | Difference (int) | All |\n| `DATEDIFF_BIG(part, s, e)` | Difference (bigint) | 2016+ |\n| `EOMONTH(date, [offset])` | Last day of month | 2012+ |\n| `DATETRUNC(part, date)` | Truncate to precision | 2022+ |\n| `DATE_BUCKET(part, n, date)` | Group into buckets | 2022+ |\n| `AT TIME ZONE 'tz'` | Timezone conversion | 2016+ |\n\n### Window Functions\n| Function | Description | Version |\n|----------|-------------|---------|\n| `ROW_NUMBER()` | Sequential unique numbers | 2005+ |\n| `RANK()` | Rank with gaps for ties | 2005+ |\n| `DENSE_RANK()` | Rank without gaps | 2005+ |\n| `NTILE(n)` | Distribute into n groups | 2005+ |\n| `LAG(col, n, default)` | Previous row value | 2012+ |\n| `LEAD(col, n, default)` | Next row value | 2012+ |\n| `FIRST_VALUE(col)` | First in window | 2012+ |\n| `LAST_VALUE(col)` | Last in window | 2012+ |\n| `IGNORE NULLS` | Skip NULLs in offset funcs | 2022+ |\n\n### SQL Server 2022 New Functions\n| Function | Description |\n|----------|-------------|\n| `GREATEST(v1, v2, ...)` | Maximum of values |\n| `LEAST(v1, v2, ...)` | Minimum of values |\n| `DATETRUNC(part, date)` | Truncate date |\n| `GENERATE_SERIES(start, stop, [step])` | Number sequence |\n| `JSON_OBJECT('key': val)` | Create JSON object |\n| `JSON_ARRAY(v1, v2, ...)` | Create JSON array |\n| `JSON_PATH_EXISTS(json, path)` | Check path exists |\n| `IS [NOT] DISTINCT FROM` | NULL-safe comparison |\n\n## Core Patterns\n\n### String Manipulation\n```sql\n-- Concatenate with separator (NULL-safe)\nSELECT CONCAT_WS(', ', FirstName, MiddleName, LastName) AS FullName\n\n-- Split string to rows with ordinal\nSELECT value, ordinal\nFROM STRING_SPLIT('apple,banana,cherry', ',', 1)\n\n-- Aggregate strings with ordering\nSELECT DeptID,\n       STRING_AGG(EmployeeName, ', ') WITHIN GROUP (ORDER BY HireDate)\nFROM Employees\nGROUP BY DeptID\n```\n\n### Date Operations\n```sql\n-- Truncate to first of month\nSELECT DATETRUNC(month, OrderDate) AS MonthStart\n\n-- Group by week buckets\nSELECT DATE_BUCKET(week, 1, OrderDate) AS WeekBucket,\n       COUNT(*) AS OrderCount\nFROM Orders\nGROUP BY DATE_BUCKET(week, 1, OrderDate)\n\n-- Generate date series\nSELECT CAST(value AS date) AS Date\nFROM GENERATE_SERIES(\n    CAST('2024-01-01' AS date),\n    CAST('2024-12-31' AS date),\n    1\n)\n```\n\n### Window Functions\n```sql\n-- Running total with partitioning\nSELECT OrderID, CustomerID, Amount,\n       SUM(Amount) OVER (\n           PARTITION BY CustomerID\n           ORDER BY OrderDate\n           ROWS UNBOUNDED PRECEDING\n       ) AS RunningTotal\nFROM Orders\n\n-- Get previous non-NULL value (SQL 2022+)\nSELECT Date, Value,\n       LAST_VALUE(Value) IGNORE NULLS OVER (\n           ORDER BY Date\n           ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n       ) AS PreviousNonNull\nFROM Measurements\n```\n\n### JSON Operations\n```sql\n-- Extract scalar value\nSELECT JSON_VALUE(JsonColumn, '$.customer.name') AS CustomerName\n\n-- Parse JSON array to rows\nSELECT j.ProductID, j.Quantity\nFROM Orders\nCROSS APPLY OPENJSON(OrderDetails)\nWITH (\n    ProductID INT '$.productId',\n    Quantity INT '$.qty'\n) AS j\n\n-- Build JSON object (SQL 2022+)\nSELECT JSON_OBJECT('id': CustomerID, 'name': CustomerName) AS CustomerJson\nFROM Customers\n```\n\n## Additional References\n\nFor deeper coverage of specific function categories, see:\n\n- `references/string-functions.md` - Complete string function reference with examples\n- `references/window-functions.md` - Window and ranking functions with frame specifications"
              }
            ]
          }
        ]
      }
    }
  ]
}