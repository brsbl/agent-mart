{
  "owner": {
    "id": "srnnkls",
    "display_name": "Sören Nikolaus",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/31724172?u=051826a0199cc719d80f59472db1b73ee9b6a38c&v=4",
    "url": "https://github.com/srnnkls",
    "bio": null,
    "stats": {
      "total_repos": 1,
      "total_plugins": 4,
      "total_commands": 0,
      "total_skills": 44,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "srnnkls/tropos",
      "url": "https://github.com/srnnkls/tropos",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T17:40:51Z",
        "created_at": "2026-01-12T07:48:16Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1033
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1714
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 159
        },
        {
          "path": "agents/task-implementer.md",
          "type": "blob",
          "size": 2206
        },
        {
          "path": "agents/task-reviewer.md",
          "type": "blob",
          "size": 1519
        },
        {
          "path": "agents/task-tester.md",
          "type": "blob",
          "size": 591
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 170
        },
        {
          "path": "commands/code.review.md",
          "type": "blob",
          "size": 163
        },
        {
          "path": "commands/debate.md",
          "type": "blob",
          "size": 634
        },
        {
          "path": "commands/hint.focus-leakage.md",
          "type": "blob",
          "size": 1311
        },
        {
          "path": "commands/implement.md",
          "type": "blob",
          "size": 1382
        },
        {
          "path": "commands/pr.review.md",
          "type": "blob",
          "size": 156
        },
        {
          "path": "commands/spec.archive.md",
          "type": "blob",
          "size": 154
        },
        {
          "path": "commands/spec.clarify.md",
          "type": "blob",
          "size": 468
        },
        {
          "path": "commands/spec.create.md",
          "type": "blob",
          "size": 489
        },
        {
          "path": "commands/spec.issues.md",
          "type": "blob",
          "size": 164
        },
        {
          "path": "commands/spec.promote.md",
          "type": "blob",
          "size": 159
        },
        {
          "path": "commands/spec.review.md",
          "type": "blob",
          "size": 304
        },
        {
          "path": "commands/spec.update.md",
          "type": "blob",
          "size": 268
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 154
        },
        {
          "path": "skills/code-debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-debug/SKILL.md",
          "type": "blob",
          "size": 4948
        },
        {
          "path": "skills/code-debug/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-debug/reference/defense-in-depth.md",
          "type": "blob",
          "size": 3081
        },
        {
          "path": "skills/code-debug/reference/root-cause-tracing.md",
          "type": "blob",
          "size": 3342
        },
        {
          "path": "skills/code-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-implement/SKILL.md",
          "type": "blob",
          "size": 2210
        },
        {
          "path": "skills/code-review-receive",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review-receive/SKILL.md",
          "type": "blob",
          "size": 3780
        },
        {
          "path": "skills/code-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review/SKILL.md",
          "type": "blob",
          "size": 2263
        },
        {
          "path": "skills/code-review/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-review/resources/checklist.md",
          "type": "blob",
          "size": 1491
        },
        {
          "path": "skills/code-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/code-test/SKILL.md",
          "type": "blob",
          "size": 3843
        },
        {
          "path": "skills/debate-start",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debate-start/SKILL.md",
          "type": "blob",
          "size": 4828
        },
        {
          "path": "skills/debate-start/reference.md",
          "type": "blob",
          "size": 6588
        },
        {
          "path": "skills/debate-start/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/debate-start/templates/debate-scratchpad.md",
          "type": "blob",
          "size": 659
        },
        {
          "path": "skills/docs-implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/docs-implement/SKILL.md",
          "type": "blob",
          "size": 806
        },
        {
          "path": "skills/dotfiles-manage",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/dotfiles-manage/SKILL.md",
          "type": "blob",
          "size": 3189
        },
        {
          "path": "skills/git-worktree-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/git-worktree-use/SKILL.md",
          "type": "blob",
          "size": 3832
        },
        {
          "path": "skills/hooks-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hooks-test/SKILL.md",
          "type": "blob",
          "size": 7142
        },
        {
          "path": "skills/hooks-test/reference.md",
          "type": "blob",
          "size": 5199
        },
        {
          "path": "skills/hooks-test/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hooks-test/templates/test-harness.py",
          "type": "blob",
          "size": 6714
        },
        {
          "path": "skills/pr-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pr-review/SKILL.md",
          "type": "blob",
          "size": 4466
        },
        {
          "path": "skills/skill-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/skill-create/SKILL.md",
          "type": "blob",
          "size": 4168
        },
        {
          "path": "skills/skill-create/best-practices.md",
          "type": "blob",
          "size": 11182
        },
        {
          "path": "skills/skill-create/reference.md",
          "type": "blob",
          "size": 5510
        },
        {
          "path": "skills/spec-archive",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-archive/SKILL.md",
          "type": "blob",
          "size": 4417
        },
        {
          "path": "skills/spec-archive/reference.md",
          "type": "blob",
          "size": 2542
        },
        {
          "path": "skills/spec-archive/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-archive/templates/archive-index.md",
          "type": "blob",
          "size": 307
        },
        {
          "path": "skills/spec-archive/templates/archive-notes.md",
          "type": "blob",
          "size": 203
        },
        {
          "path": "skills/spec-clarify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-clarify/SKILL.md",
          "type": "blob",
          "size": 5222
        },
        {
          "path": "skills/spec-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/SKILL.md",
          "type": "blob",
          "size": 10266
        },
        {
          "path": "skills/spec-create/examples.md",
          "type": "blob",
          "size": 1569
        },
        {
          "path": "skills/spec-create/guidelines.md",
          "type": "blob",
          "size": 5503
        },
        {
          "path": "skills/spec-create/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/resources/README.md",
          "type": "blob",
          "size": 13695
        },
        {
          "path": "skills/spec-create/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-create/templates/context.md",
          "type": "blob",
          "size": 2433
        },
        {
          "path": "skills/spec-create/templates/dependencies.yaml",
          "type": "blob",
          "size": 2296
        },
        {
          "path": "skills/spec-create/templates/spec.md",
          "type": "blob",
          "size": 2362
        },
        {
          "path": "skills/spec-create/templates/tasks.yaml",
          "type": "blob",
          "size": 1354
        },
        {
          "path": "skills/spec-create/templates/validation.yaml",
          "type": "blob",
          "size": 5432
        },
        {
          "path": "skills/spec-issues-create",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-issues-create/SKILL.md",
          "type": "blob",
          "size": 4864
        },
        {
          "path": "skills/spec-issues-create/mapping-guide.md",
          "type": "blob",
          "size": 3465
        },
        {
          "path": "skills/spec-issues-create/reference.md",
          "type": "blob",
          "size": 1150
        },
        {
          "path": "skills/spec-issues-create/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-issues-create/scripts/create-issue.sh",
          "type": "blob",
          "size": 1416
        },
        {
          "path": "skills/spec-issues-create/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-issues-create/templates/feature.md",
          "type": "blob",
          "size": 342
        },
        {
          "path": "skills/spec-issues-create/templates/initiative.md",
          "type": "blob",
          "size": 435
        },
        {
          "path": "skills/spec-issues-create/templates/task.md",
          "type": "blob",
          "size": 279
        },
        {
          "path": "skills/spec-promote",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-promote/SKILL.md",
          "type": "blob",
          "size": 3449
        },
        {
          "path": "skills/spec-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/SKILL.md",
          "type": "blob",
          "size": 8330
        },
        {
          "path": "skills/spec-review/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/reference/playbook.md",
          "type": "blob",
          "size": 3936
        },
        {
          "path": "skills/spec-review/reference/report-format.md",
          "type": "blob",
          "size": 3486
        },
        {
          "path": "skills/spec-review/reference/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-review/reference/roles/claude-reviewer.md",
          "type": "blob",
          "size": 1197
        },
        {
          "path": "skills/spec-review/reference/roles/opencode-reviewer.md",
          "type": "blob",
          "size": 1366
        },
        {
          "path": "skills/spec-update",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-update/SKILL.md",
          "type": "blob",
          "size": 5472
        },
        {
          "path": "skills/spec-validate",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-validate/SKILL.md",
          "type": "blob",
          "size": 9953
        },
        {
          "path": "skills/spec-validate/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spec-validate/reference/issue-types.md",
          "type": "blob",
          "size": 4503
        },
        {
          "path": "skills/spec-validate/reference/question-taxonomy.md",
          "type": "blob",
          "size": 11181
        },
        {
          "path": "skills/spec-validate/reference/sdd-gates.md",
          "type": "blob",
          "size": 1845
        },
        {
          "path": "skills/task-completion-verify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-completion-verify/SKILL.md",
          "type": "blob",
          "size": 3350
        },
        {
          "path": "skills/task-dispatch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/SKILL.md",
          "type": "blob",
          "size": 7930
        },
        {
          "path": "skills/task-dispatch/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/reference/parallel-detection.md",
          "type": "blob",
          "size": 3769
        },
        {
          "path": "skills/task-dispatch/reference/report-format.md",
          "type": "blob",
          "size": 5900
        },
        {
          "path": "skills/task-dispatch/reference/roles",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/task-dispatch/reference/roles/implementer.md",
          "type": "blob",
          "size": 3152
        },
        {
          "path": "skills/task-dispatch/reference/roles/reviewer.md",
          "type": "blob",
          "size": 3345
        },
        {
          "path": "skills/task-dispatch/reference/roles/tester.md",
          "type": "blob",
          "size": 2554
        },
        {
          "path": "skills/task-dispatch/reference/subagent-workflow.md",
          "type": "blob",
          "size": 7219
        }
      ],
      "marketplace": {
        "name": "tropos",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "srnnkls"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "skills",
            "description": "24 skills: spec pipeline, code quality, task execution",
            "source": "./skills",
            "category": "workflow",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add srnnkls/tropos",
              "/plugin install skills@tropos"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T17:40:51Z",
              "created_at": "2026-01-12T07:48:16Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "code-debug",
                "description": "Systematic debugging with root cause tracing. Use when encountering bugs, test failures, or unexpected behavior - find root cause before attempting fixes, trace backward through call chain.",
                "path": "skills/code-debug/SKILL.md",
                "frontmatter": {
                  "name": "code-debug",
                  "description": "Systematic debugging with root cause tracing. Use when encountering bugs, test failures, or unexpected behavior - find root cause before attempting fixes, trace backward through call chain."
                },
                "content": "# Systematic Debugging\n\nRandom fixes waste time and create new bugs.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n---\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n---\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n\n**Use ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- You don't fully understand the issue\n\n---\n\n## The Four Phases\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - If not reproducible, gather more data\n\n3. **Check Recent Changes**\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Trace Data Flow Backward**\n   - Where does the bad value originate?\n   - What called this with the bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n5. **Multi-Component Systems**\n   Add diagnostic instrumentation at each boundary:\n   - Log what enters/exits each component\n   - Verify environment/config propagation\n   - Run once to gather evidence WHERE it breaks\n\n### Phase 2: Pattern Analysis\n\n1. **Find Working Examples** - Similar working code in same codebase\n2. **Compare Against References** - Read reference implementations completely\n3. **Identify Differences** - List every difference, however small\n4. **Understand Dependencies** - Settings, config, environment, assumptions\n\n### Phase 3: Hypothesis and Testing\n\n1. **Form Single Hypothesis** - \"I think X is the root cause because Y\"\n2. **Test Minimally** - Smallest possible change, one variable at a time\n3. **Verify Before Continuing** - Worked? Phase 4. Didn't work? New hypothesis.\n4. **When You Don't Know** - Say so. Ask for help. Research more.\n\n### Phase 4: Implementation\n\n1. **Create Failing Test Case** - Simplest possible reproduction\n2. **Implement Single Fix** - ONE change at a time, no bundled improvements\n3. **Verify Fix** - Test passes? No regressions?\n\n**If fix doesn't work:**\n- Count: How many fixes have you tried?\n- If < 3: Return to Phase 1 with new information\n- If >= 3: STOP and question the architecture\n\n### When 3+ Fixes Fail\n\nPattern indicating architectural problem:\n- Each fix reveals new shared state/coupling\n- Fixes require \"massive refactoring\"\n- Each fix creates new symptoms elsewhere\n\n**STOP and question fundamentals:**\n- Is this pattern fundamentally sound?\n- Should we refactor architecture vs. continue fixing symptoms?\n- Discuss with user before attempting more fixes\n\n---\n\n## Root Cause Tracing\n\nWhen bugs manifest deep in the call stack:\n\n1. **Observe the Symptom** - What error occurred?\n2. **Find Immediate Cause** - What code directly causes this?\n3. **Ask: What Called This?** - Trace up the call chain\n4. **Keep Tracing Up** - What value was passed? Where did it come from?\n5. **Find Original Trigger** - The source, not the symptom\n\n**Adding Stack Traces:**\n```\nstack = capture_stack_trace()\nlog(\"DEBUG operation:\", {\n  input_value,\n  current_directory,\n  environment,\n  stack\n})\n```\n\n**NEVER fix just where the error appears.** Trace back to find the original trigger.\n\n---\n\n## Red Flags - STOP and Follow Process\n\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"I don't fully understand but this might work\"\n- Proposing solutions before tracing data flow\n- \"One more fix attempt\" (when already tried 2+)\n\n**ALL of these mean:** STOP. Return to Phase 1.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple\" | Simple issues have root causes too. |\n| \"Emergency, no time\" | Systematic is FASTER than thrashing. |\n| \"Just try this first\" | First fix sets the pattern. Do it right. |\n| \"I see the problem\" | Seeing symptoms != understanding root cause. |\n| \"One more fix attempt\" | 3+ failures = architectural problem. |\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Write failing test to reproduce bug before fixing\n- `completion-verify` - Verify fix actually worked before claiming done\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Detailed tracing techniques"
              },
              {
                "name": "code-implement",
                "description": "Language-specific coding guidelines. Use when implementing code in Python or other supported languages.",
                "path": "skills/code-implement/SKILL.md",
                "frontmatter": {
                  "name": "code-implement",
                  "description": "Language-specific coding guidelines. Use when implementing code in Python or other supported languages."
                },
                "content": "# Code Implement Skill\n\nLanguage-specific patterns, anti-patterns, and best practices for writing code.\n\n---\n\n## When to Use\n\n- Writing code in supported languages\n- Deciding on code structure, patterns, or style\n- Designing domain models or data structures\n- Organizing code into modules\n\n**IMPORTANT - Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill instead (it will invoke this skill per task with quality gates)\n- **Single implementation task or asking about patterns?** → Use this skill directly\n\n---\n\n## Supported Languages\n\nLanguage-specific guidelines are in `~/.claude/skills/code-implement/resources/loqui/languages/{language}/`.\n\n**Use Read tool** (not Glob) to access resources - paths outside cwd require direct reads.\n\nEach language directory follows this structure:\n\n```\n~/.claude/skills/code-implement/resources/loqui/languages/{language}/\n├── README.md              # Overview, core principles, anti-patterns checklist\n├── quality.md             # Naming, comments, documentation conventions\n├── composition.md         # Structuring behavior (classes/functions/modules)\n├── modules.md             # Package structure, organization, public APIs\n├── errors.md              # Error handling patterns and practices\n└── ...                    # Additional language-specific resources as needed\n```\n\n**Start with the language README** for quick reference and core principles, then dive into specific topic files as needed.\n\n---\n\n## Related Skills\n\n- **task-dispatch**: Use for multiple independent implementation tasks (invokes this skill per task)\n- **code-test**: Use for TDD workflow (write test first, then implement)\n- **code-review**: Review methodology (delegates here for language specifics)\n- **pr-review**: GitHub PR workflow (delegates to code-review)\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Tracing bugs to their source"
              },
              {
                "name": "code-review-receive",
                "description": "Technical evaluation of code review feedback. Use when receiving review feedback - requires verification before implementing, no performative agreement, push back when technically wrong.",
                "path": "skills/code-review-receive/SKILL.md",
                "frontmatter": {
                  "name": "code-review-receive",
                  "description": "Technical evaluation of code review feedback. Use when receiving review feedback - requires verification before implementing, no performative agreement, push back when technically wrong."
                },
                "content": "# Code Review Reception\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n---\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n---\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\"\n- \"Great point!\" / \"Excellent feedback!\"\n- \"Let me implement that now\" (before verification)\n- Any performative agreement\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n---\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nFeedback: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\nWRONG: Implement 1,2,3,6 now, ask about 4,5 later\nRIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5.\"\n```\n\n---\n\n## YAGNI Check\n\nWhen reviewer suggests adding features:\n\n```\nIF reviewer suggests \"implementing properly\":\n  Search codebase for actual usage\n\n  IF unused: \"This isn't called anywhere. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n---\n\n## When to Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with prior architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Escalate if architectural\n\n---\n\n## Implementation Order\n\nFor multi-item feedback:\n\n1. Clarify anything unclear FIRST\n2. Then implement in order:\n   - Blocking issues (breaks, security)\n   - Simple fixes (typos, imports)\n   - Complex fixes (refactoring, logic)\n3. Test each fix individually\n4. Verify no regressions\n\n---\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n\n```\nDO:    \"Fixed. [Brief description of what changed]\"\nDO:    \"Good catch - [specific issue]. Fixed in [location].\"\nDO:    [Just fix it and show in the code]\n\nDON'T: \"You're absolutely right!\"\nDON'T: \"Thanks for catching that!\"\nDON'T: ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it.\n\n---\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n\n```\nDO:    \"Verified and you're correct. My understanding was wrong. Fixing.\"\nDON'T: Long apology or defending why you pushed back\n```\n\nState the correction factually and move on.\n\n---\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n\n---\n\n## Integration\n\n**Use with:**\n- `task-dispatch` - Handle review between tasks\n- `code-test` - Test each fix individually\n- `completion-verify` - Verify fixes actually work"
              },
              {
                "name": "code-review",
                "description": "Code review methodology. Use when reviewing code locally or preparing for a PR review.",
                "path": "skills/code-review/SKILL.md",
                "frontmatter": {
                  "name": "code-review",
                  "description": "Code review methodology. Use when reviewing code locally or preparing for a PR review."
                },
                "content": "# Code Review Skill\n\nGeneric code review process and methodology. Language-agnostic review guidelines.\n\n---\n\n## When to Use\n\n- Reviewing code changes locally\n- Preparing review feedback before a PR\n- Understanding what to look for in code\n- Categorizing and prioritizing issues\n\n---\n\n## Review Process\n\n### Step 1: Understand Context\n\nBefore reviewing code, understand:\n- What problem is being solved?\n- What are the requirements or acceptance criteria?\n- Are there related issues or prior discussions?\n\n### Step 2: Detect Language and Load Guidelines\n\nIdentify the primary language and load appropriate guidelines from the `code-implement` skill\n\n### Step 3: Review by Category\n\nReview code across these focus areas:\n\n| Category | What to Check |\n|----------|---------------|\n| **Correctness** | Logic errors, edge cases, error handling, type safety |\n| **Style** | Naming, formatting, code organization, idioms |\n| **Performance** | Efficiency, data structures, unnecessary work |\n| **Security** | Input validation, secrets, injection risks |\n| **Architecture** | Design patterns, coupling, separation of concerns |\n\n### Step 4: Categorize by Severity\n\nAssign severity to each issue:\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| **Critical** | Blocks merge - bugs, security issues, data corruption | Must fix |\n| **High** | Should fix - significant issues, unclear behavior | Fix before merge |\n| **Medium** | Nice to fix - style, minor improvements | Can merge, follow-up |\n\n### Step 5: Provide Actionable Feedback\n\nFor each issue:\n1. Identify the specific location\n2. Explain what's wrong (reference guidelines if applicable)\n3. Suggest a concrete fix or alternative\n\n---\n\n## Review Checklist\n\nSee [resources/checklist.md](resources/checklist.md) for a generic review checklist.\n\n---\n\n## Language-Specific Guidelines\n\nFor language-specific patterns and anti-patterns, delegate to:\n\n- **code-implement**: Language guidelines and implementation patterns\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific coding guidelines\n- **pr-review**: GitHub PR workflow (delegates here for methodology)"
              },
              {
                "name": "code-test",
                "description": "Enforce test-driven development (RED-GREEN-REFACTOR). Use when implementing features, fixing bugs, or changing behavior - write failing test first, then minimal code to pass.",
                "path": "skills/code-test/SKILL.md",
                "frontmatter": {
                  "name": "code-test",
                  "description": "Enforce test-driven development (RED-GREEN-REFACTOR). Use when implementing features, fixing bugs, or changing behavior - write failing test first, then minimal code to pass."
                },
                "content": "# Test-Driven Development\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n---\n\n## When to Use\n\n**Always use for:**\n- New features\n- Bug fixes\n- Behavior changes\n- Refactoring\n\n**Exceptions (confirm with user):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\n**Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill (it enforces TDD per task with quality gates)\n- **Single implementation task?** → Use this skill directly for TDD workflow\n\n---\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrote code before the test? Delete it. Start over. No exceptions.\n\n---\n\n## Red-Green-Refactor Cycle\n\n### 1. RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n**Requirements:**\n- One behavior per test\n- Clear name describing behavior\n- Real code (avoid mocks unless unavoidable)\n\n### 2. Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\nRun the test. Confirm:\n- Test fails (not errors from typos)\n- Failure message matches expectation\n- Fails because feature is missing\n\n**Test passes immediately?** You're testing existing behavior. Fix the test.\n\n### 3. GREEN - Minimal Code\n\nWrite the simplest code to pass the test.\n\n**DO:** Just enough to pass, simple implementation\n**DON'T:** Add features, refactor other code, add configurability\n\n### 4. Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\nRun the test. Confirm:\n- Test passes\n- Other tests still pass\n- No errors or warnings\n\n### 5. REFACTOR - Clean Up\n\nOnly after green:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### 6. Repeat\n\nNext failing test for next behavior.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Already manually tested\" | Ad-hoc is not systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Unverified code is debt. |\n| \"Need to explore first\" | Fine. Throw away exploration, then TDD. |\n| \"Test hard to write\" | Hard to test = hard to use. Simplify design. |\n\n---\n\n## Red Flags - Stop and Start Over\n\n- Code written before test\n- Test passes immediately\n- Can't explain why test failed\n- \"Just this once\" rationalization\n- Keeping code \"as reference\"\n\n**All of these mean:** Delete code. Start with TDD.\n\n---\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] No errors or warnings in output\n\n---\n\n## TDD Evidence Format (For Subagent Verification)\n\nWhen implementing as a subagent, you MUST output this evidence block:\n\n```yaml\ntdd_evidence:\n  tests_written:\n    - name: \"test_feature_x\"\n      file: \"tests/test_x.py\"\n      red_output: \"FAILED - [actual failure message]\"\n      green_output: \"PASSED - 1 passed in 0.05s\"\n  implementation_files:\n    - path: \"src/feature.py\"\n  all_tests_pass: true\n  test_command: \"pytest tests/test_x.py -v\"\n  final_output: \"[full test output]\"\n```\n\n**This is REQUIRED for SubagentStop hook verification.**\n\n---\n\n## Integration\n\n**Use with:**\n- `code-debug` - Write failing test to reproduce bug before fixing\n- `task-dispatch` - Subagents follow TDD for each task\n- `completion-verify` - Run tests before claiming completion"
              },
              {
                "name": "debate-start",
                "description": "Start structured red vs. blue team debates via subagents. Use when exploring a topic from multiple adversarial perspectives.",
                "path": "skills/debate-start/SKILL.md",
                "frontmatter": {
                  "name": "debate-start",
                  "description": "Start structured red vs. blue team debates via subagents. Use when exploring a topic from multiple adversarial perspectives."
                },
                "content": "# Start Debate Skill\n\nOrchestrate multi-perspective debates on a topic using color-coded team subagents.\n\n> **Reference**: See [reference.md](reference.md) for moderation guidelines and intervention patterns.\n\n---\n\n## When to Use\n\n- Exploring trade-offs in architectural decisions\n- Evaluating competing approaches or technologies\n- Risk analysis requiring devil's advocate perspectives\n- Any topic benefiting from structured adversarial review\n\n---\n\n## Workflow\n\n### Step 1: Initialize Debate\n\n1. Parse topic from user input\n2. Create slugs from topic and context (e.g., \"API Design\" → `api-design`, context max 3 words)\n3. Ensure `./debates/` directory exists\n4. Create scratchpad from template: `./debates/{topic}_{context}.md`\n\n### Step 2: Configure Teams\n\nUse **AskUserQuestion** to gather team configuration:\n\n**Question 1: Optional Teams (multiSelect: true)**\n```\nWhich additional teams should participate beyond Red and Blue?\n- None: Just Red and Blue\n- Green Team: Pragmatic/implementation focus\n- Yellow Team: Risk/safety analysis\n- Purple Team: Synthesis/integration bridge\n```\n\n**Question 2: Red Team Stance**\n```\nWhat position should Red Team (challenger/skeptic) argue?\n```\n\n**Question 3: Blue Team Stance**\n```\nWhat position should Blue Team (defender/advocate) argue?\n```\n\n**Questions 4-6: Additional team stances** (if selected)\n\nWrite all stances to the scratchpad's Team Positions section.\n\n### Step 3: Spawn Opening Arguments (Parallel)\n\nLaunch all team subagents **simultaneously** using the Task tool:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nYou are the {COLOR} TEAM in a debate on: {topic}\n\nYour stance: {stance}\n\n## Research Phase\nGather evidence before writing using read-only tools.\n\n**Codebase research:**\n- Glob/Grep/Read: Find relevant code, patterns, prior decisions\n\n**External research (encouraged):**\n- WebSearch: Find industry practices, benchmarks, expert opinions, case studies\n- WebFetch: Retrieve specific documentation, articles, or technical references\n\nFor deep research questions, spawn focused subagents:\n  Task(subagent_type=\"general-purpose\", prompt=\"Research {specific question}...\")\n\n## Writing Phase\n1. Read ./debates/{topic}_{context}.md\n2. Edit your section: ### [{COLOR}]\n3. Structure: Position → Evidence → Implications\n4. Cite sources (files, URLs) for claims\n\n## Constraints\n- Read-only tools only (no code modifications)\n- Stay on assigned perspective\n- Arguments must be evidence-backed\n\"\"\")\n```\n\n### Step 4: Monitor and Moderate\n\nAfter subagents complete, the main agent:\n\n1. **Read scratchpad** and summarize key points to user\n2. **Assess debate health:**\n   - Progress: Are teams making new points?\n   - Balance: Is one team dominating?\n   - Relevance: Staying on topic?\n   - Depth: Avoiding superficial arguments?\n\n3. **Intervene if needed** - write to Moderator Notes section:\n   - `[MODERATOR] Stuck:` \"Team X, consider addressing Y\"\n   - `[MODERATOR] Tunnel:` \"Team X, you've repeated Z\"\n   - `[MODERATOR] Astray:` \"Refocus on core question\"\n   - `[MODERATOR] Disconnected:` \"Team X, respond to Team Y's point\"\n\n4. **Ask user** for next action:\n   - \"Advance to rebuttals?\"\n   - \"Request synthesis round?\"\n   - \"Conclude debate?\"\n\n### Step 5: Rebuttal Round (Sequential)\n\nSpawn teams **sequentially** for direct responses:\n\nOrder: Red → Blue → Green → Yellow → Purple (active teams only)\n\nEach team's prompt includes instruction to read and respond to specific opposing arguments.\n\n### Step 6: Synthesis Round (Optional)\n\nIf requested, spawn Purple Team (or all teams) to find:\n- Common ground\n- Irreconcilable differences\n- Potential compromises\n\n### Step 7: Conclude Debate\n\nMain agent writes Conclusion section:\n- **Summary:** Key positions from each team\n- **Agreements:** Points of consensus\n- **Disagreements:** Unresolved tensions\n- **Recommendations:** Suggested path forward (if applicable)\n\nUpdate scratchpad status to \"Completed\".\n\n---\n\n## Templates\n\n- [templates/debate-scratchpad.md](templates/debate-scratchpad.md) - Debate file template\n\n---\n\n## Success Criteria\n\n- Scratchpad created at `./debates/{topic}_{context}.md`\n- All active teams contributed arguments\n- Moderator interventions documented transparently\n- User controlled round progression\n- Debate concluded with synthesis\n\n---\n\n## Integration\n\n**Command:** `/debate {topic}`\n\n**Related:**\n- Tools: Task (subagents), AskUserQuestion (configuration), Edit (scratchpad)\n- Pattern: Document-centric coordination via shared scratchpad\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for:\n- Team perspective definitions\n- Intervention decision tree\n- Example debate flows\n- Common failure modes"
              },
              {
                "name": "docs-implement",
                "description": "General reference documents by domain. Use when creating architecture docs, strategy documents, specs, or guides.",
                "path": "skills/docs-implement/SKILL.md",
                "frontmatter": {
                  "name": "docs-implement",
                  "description": "General reference documents by domain. Use when creating architecture docs, strategy documents, specs, or guides."
                },
                "content": "# Docs Implement Skill\n\nDomain-specific reference documents for non-code artifacts.\n\n---\n\n## When to Use\n\n- Creating architecture documentation\n- Writing strategy or decision documents\n- Drafting specifications\n- Authoring guides or playbooks\n\n---\n\n## Domains\n\n| Domain | Purpose |\n|--------|---------|\n| architecture | System design, component diagrams, data flow |\n| strategy | Decision frameworks, trade-off analysis |\n| specs | API contracts, interface definitions |\n| guides | How-to documentation, playbooks |\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific coding guidelines\n- **spec-create**: Spec documents with validation"
              },
              {
                "name": "dotfiles-manage",
                "description": "Manage dotfiles using dotter (symlink manager and templater). Use when deploying, adding, removing, or organizing configuration files in ~/dotfiles.",
                "path": "skills/dotfiles-manage/SKILL.md",
                "frontmatter": {
                  "name": "dotfiles-manage",
                  "description": "Manage dotfiles using dotter (symlink manager and templater). Use when deploying, adding, removing, or organizing configuration files in ~/dotfiles."
                },
                "content": "# Manage Dotfiles Skill\n\nManage dotfiles using [dotter](https://github.com/SuperCuber/dotter) - a dotfile manager and templater.\n\n## Environment\n\n- **Dotfiles repo**: `~/dotfiles`\n- **Dotter config**: `~/dotfiles/.dotter/`\n  - `global.toml`: Package definitions (files to deploy)\n  - `local.toml`: Machine-specific package selection\n  - `cache.toml`: Deployment state cache\n\n## Core Commands\n\n```bash\n# Deploy all configured files\ndotter deploy\n\n# Preview changes without applying\ndotter deploy --dry-run\n\n# Undeploy all managed files\ndotter undeploy\n\n# Watch for changes and auto-deploy\ndotter watch\n```\n\n## Workflow: Add New Dotfile\n\n### Step 1: Add Source File\n\nPlace the configuration file in `~/dotfiles`:\n\n```bash\n# Example: adding a new config\ncp ~/.config/app/config.toml ~/dotfiles/.config/app/config.toml\n```\n\n### Step 2: Define in global.toml\n\nAdd a new package or extend existing one in `~/dotfiles/.dotter/global.toml`:\n\n```toml\n# New package\n[myapp.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n\n# Or extend existing package\n[existing-package.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n```\n\n**File mapping format**: `\"source\" = \"target\"`\n- Source: relative path from dotfiles repo root\n- Target: absolute path or `~` for home directory\n\n### Step 3: Enable Package (if new)\n\nAdd package to `~/dotfiles/.dotter/local.toml`:\n\n```toml\npackages = [\"doom\", \"myapp\"]\n```\n\n### Step 4: Deploy\n\n```bash\ncd ~/dotfiles && dotter deploy\n```\n\n## Workflow: Remove Dotfile\n\n1. **Undeploy first**: `dotter undeploy`\n2. **Remove from global.toml**: Delete the file mapping\n3. **Remove package from local.toml** (if removing entire package)\n4. **Redeploy**: `dotter deploy`\n5. **Clean up source** (optional): Remove file from dotfiles repo\n\n## Package Organization\n\nGroup related files into packages:\n\n```toml\n# Shell configuration\n[shell.files]\n\".zshrc\" = \"~/.zshrc\"\n\".zprofile\" = \"~/.zprofile\"\n\".config/starship.toml\" = \"~/.config/starship.toml\"\n\n# Editor configuration\n[nvim.files]\n\".config/nvim\" = \"~/.config/nvim\"\n\n# Git configuration\n[git.files]\n\".gitconfig\" = \"~/.gitconfig\"\n\".gitignore_global\" = \"~/.gitignore_global\"\n```\n\n## Templating\n\nDotter supports Handlebars templating for machine-specific values:\n\n```toml\n# In global.toml - define variables\n[package.variables]\nemail = \"default@example.com\"\n\n# In local.toml - override per machine\n[variables]\nemail = \"work@company.com\"\n```\n\nIn template files, use `\\{{email}}` syntax.\n\n## Troubleshooting\n\n**Conflict with existing file:**\n```bash\n# Force overwrite (use with caution)\ndotter deploy --force\n```\n\n**Check deployment status:**\n```bash\ndotter deploy --dry-run --verbose\n```\n\n**View what's currently deployed:**\n```bash\ncat ~/dotfiles/.dotter/cache.toml\n```\n\n## Best Practices\n\n- Keep packages granular and focused\n- Use descriptive package names\n- Commit changes to dotfiles repo after modifications\n- Test with `--dry-run` before deploying\n- Use templating for machine-specific values (email, paths)"
              },
              {
                "name": "git-worktree-use",
                "description": "Create isolated git worktrees with safety verification. Use when starting feature work needing isolation or before executing plans - systematic directory selection and baseline verification.",
                "path": "skills/git-worktree-use/SKILL.md",
                "frontmatter": {
                  "name": "git-worktree-use",
                  "description": "Create isolated git worktrees with safety verification. Use when starting feature work needing isolation or before executing plans - systematic directory selection and baseline verification."
                },
                "content": "# Using Git Worktrees\n\nGit worktrees create isolated workspaces sharing the same repository.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n---\n\n## When to Use\n\n**Use for:**\n- Feature work needing isolation from current workspace\n- Parallel work on multiple branches\n- Before executing implementation plans\n\n**Don't use for:**\n- Quick fixes on current branch\n- Single-file changes\n- When isolation isn't needed\n\n---\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check Project Config\n\nLook for worktree directory preference in project documentation (CLAUDE.md, README, etc.).\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no preference found:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n---\n\n## Safety Verification\n\n### For Project-Local Directories\n\n**MUST verify .gitignore before creating worktree:**\n\n```bash\ngrep -q \"^\\.worktrees/$\" .gitignore || grep -q \"^worktrees/$\" .gitignore\n```\n\n**If NOT in .gitignore:**\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents.\n\n### For Global Directory\n\nNo .gitignore verification needed - outside project entirely.\n\n---\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Detect project type and install dependencies\nif [ -f package.json ]; then npm install; fi\nif [ -f Cargo.toml ]; then cargo build; fi\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then pip install -e .; fi\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Use project-appropriate command\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n---\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify .gitignore) |\n| `worktrees/` exists | Use it (verify .gitignore) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check config then ask user |\n| Not in .gitignore | Add it immediately + commit |\n| Tests fail | Report failures + ask |\n\n---\n\n## Red Flags\n\n**Never:**\n- Create worktree without .gitignore verification (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n\n**Always:**\n- Follow directory priority: existing > config > ask\n- Verify .gitignore for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n---\n\n## Integration\n\n**Use with:**\n- `brainstorm` - After design approval, set up workspace\n- `task-dispatch` - Work happens in this worktree\n- `completion-verify` - Verify baseline before and after"
              },
              {
                "name": "hooks-test",
                "description": "Test Claude Code hooks in isolation and via integration. Use when developing, debugging, or validating hook behavior.",
                "path": "skills/hooks-test/SKILL.md",
                "frontmatter": {
                  "name": "hooks-test",
                  "description": "Test Claude Code hooks in isolation and via integration. Use when developing, debugging, or validating hook behavior."
                },
                "content": "# Hooks Test\n\nTest hooks at three levels: unit tests (Python), direct invocation, and headless Claude.\n\n> **Reference**: See [reference.md](reference.md) for complete payload schemas.\n\n---\n\n## When to Use\n\n- Developing new hooks\n- Debugging hook failures\n- Validating hook behavior before deployment\n- Regression testing after hook changes\n\n---\n\n## Level 1: Unit Tests (Python)\n\nTest hook logic in isolation using the test harness with pytest.\n\n### Test Harness\n\nUse [templates/test-harness.py](templates/test-harness.py):\n\n```python\nfrom test_harness import create_payload, run_hook, assert_blocked, assert_allowed\n\ndef test_blocks_dangerous_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"rm -rf /\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_blocked(result, \"dangerous\")\n\ndef test_allows_safe_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"echo hello\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_allowed(result)\n\ndef test_modifies_input():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Write\",\n                             tool_input={\"file_path\": \"/tmp/test.txt\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_modified_input(result, \"file_path\", \"/safe/path/test.txt\")\n```\n\nRun with pytest:\n\n```bash\npytest test_my_hook.py -v\n```\n\n### Assertions\n\n| Function | Checks |\n|----------|--------|\n| `assert_blocked(result, msg)` | Exit code 2, stderr contains msg |\n| `assert_allowed(result)` | Exit code 0, no block decision |\n| `assert_modified_input(result, field, value)` | Exit 0, updatedInput has field |\n| `assert_context_added(result, contains)` | Exit 0, context includes text |\n\n---\n\n## Level 2: Direct Invocation (Shell)\n\nTest hooks by calling them directly with JSON payloads.\n\n### Basic Pattern\n\n```bash\necho '{\"hook_event_name\": \"PreToolUse\", \"tool_name\": \"Bash\", ...}' | ./hook.py\necho \"Exit: $?\"\n```\n\n### With Payload File\n\n```bash\ncat > /tmp/payload.json << 'EOF'\n{\n  \"session_id\": \"test-123\",\n  \"transcript_path\": \"/tmp/test.jsonl\",\n  \"cwd\": \"/path/to/project\",\n  \"permission_mode\": \"default\",\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Bash\",\n  \"tool_input\": {\"command\": \"rm -rf /\"},\n  \"tool_use_id\": \"toolu_01ABC\"\n}\nEOF\n\ncat /tmp/payload.json | ./my-hook.py\n```\n\n### Exit Code Reference\n\n| Exit Code | Meaning | Check |\n|-----------|---------|-------|\n| 0 | Success/allow | stdout contains valid JSON or context |\n| 2 | Block action | stderr contains reason for Claude |\n| Other | Non-blocking error | stderr contains user message |\n\n---\n\n## Level 3: Headless Claude (End-to-End)\n\nTest hooks end-to-end by invoking Claude in headless mode. Claude executes normally,\ntriggers hooks through its behavior, and you verify the outcome.\n\n### Headless Claude Flags\n\n```bash\nclaude -p \"prompt here\" --debug\n```\n\n| Flag | Purpose |\n|------|---------|\n| `-p` / `--print` | Headless mode - prints response and exits |\n| `--debug` | Shows hook execution details in stderr |\n| `--allowedTools` | Limit which tools Claude can use |\n| `--permission-mode` | Control permission behavior |\n\n### Test Patterns\n\n**Test PreToolUse blocks dangerous commands:**\n\n```bash\n# Prompt that would trigger dangerous Bash command\noutput=$(claude -p \"delete everything in /tmp\" --debug 2>&1)\n\n# Check hook blocked it (look for your hook's block message)\nif echo \"$output\" | grep -q \"blocked\"; then\n    echo \"PASS: Hook blocked dangerous command\"\nfi\n```\n\n**Test PostToolUse reacts to failures:**\n\n```bash\n# Prompt that triggers a command expected to fail\noutput=$(claude -p \"run: exit 1\" --debug 2>&1)\n\n# Check hook's reaction appears in output\necho \"$output\" | grep -q \"Hook command completed\"\n```\n\n**Test UserPromptSubmit adds context:**\n\n```bash\n# Any prompt triggers UserPromptSubmit\noutput=$(claude -p \"hello\" --debug 2>&1)\n\n# Verify hook ran\necho \"$output\" | grep \"UserPromptSubmit\"\n```\n\n**Test Stop hook continues execution:**\n\n```bash\n# Prompt that completes, triggering Stop hook\noutput=$(claude -p \"what is 2+2\" --debug 2>&1)\n\n# Check if hook caused continuation\necho \"$output\" | grep \"Stop\"\n```\n\n### Debug Output Format\n\n```\n[DEBUG] Executing hooks for PreToolUse:Bash\n[DEBUG] Found 1 hook matchers in settings\n[DEBUG] Matched 1 hooks for query \"Bash\"\n[DEBUG] Executing hook command: ./my-hook.py with timeout 60000ms\n[DEBUG] Hook command completed with status 0: <stdout content>\n```\n\n### Automated Test Script\n\n```bash\n#!/bin/bash\n# integration-test.sh - Run from project root with hook configured\nset -e\n\necho \"Test 1: PreToolUse blocks rm -rf\"\nif claude -p \"run: rm -rf /\" --debug 2>&1 | grep -qi \"block\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"Test 2: Safe commands allowed\"\nif claude -p \"run: echo hello\" --debug 2>&1 | grep -q \"hello\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"All tests passed\"\n```\n\n### Isolate Test Configuration\n\nUse `.claude/settings.local.json` for test hooks (not committed):\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\".claude/hooks/test-blocker.py\"]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Payload Examples by Event\n\n### PreToolUse\n\n```python\npayload = create_payload(\"PreToolUse\",\n    tool_name=\"Write\",\n    tool_input={\"file_path\": \"/etc/passwd\", \"content\": \"...\"})\n```\n\nTest: block dangerous paths, allow safe ops, modify input via `updatedInput`\n\n### PostToolUse\n\n```python\npayload = create_payload(\"PostToolUse\",\n    tool_name=\"Bash\",\n    tool_input={\"command\": \"npm test\"},\n    tool_response={\"stdout\": \"FAILED\", \"exit_code\": 1})\n```\n\nTest: react to failures, add context, log operations\n\n### UserPromptSubmit\n\n```python\npayload = create_payload(\"UserPromptSubmit\", prompt=\"delete all files\")\n```\n\nTest: block prohibited prompts, add context, transform input\n\n### Stop/SubagentStop\n\n```python\npayload = create_payload(\"Stop\", stop_hook_active=False)\n```\n\nTest: continue on conditions, verify TDD evidence, prevent loops when `stop_hook_active=True`\n\n---\n\n## Debugging Tips\n\n1. **Hook not running?** Check `/hooks` menu, verify matcher syntax\n2. **JSON parse error?** Validate hook outputs valid JSON on exit 0\n3. **Timeout?** Default is 60s, increase with `\"timeout\": 120000`\n4. **Wrong exit code?** Use `sys.exit(2)` to block, `sys.exit(0)` to allow\n5. **Stderr not showing?** Only displayed in verbose mode (`ctrl+o`)\n\n---\n\n## Success Criteria\n\n- [ ] Level 1: Unit tests pass with pytest\n- [ ] Level 2: Direct invocation returns expected exit codes\n- [ ] Level 3: Headless Claude triggers hook and behaves correctly\n- [ ] Exit codes match behavior (0=allow, 2=block)\n- [ ] Blocking responses include reason in stderr\n- [ ] JSON output is valid and complete\n\n---\n\n## Integration\n\n**Related:**\n- Docs: `claude --help` for CLI flags\n- Command: `/hooks` to manage hooks\n- Config: `.claude/settings.json`, `.claude/settings.local.json`"
              },
              {
                "name": "pr-review",
                "description": "Review GitHub PRs with inline comments and structured summaries. Use when reviewing PRs via gh CLI.",
                "path": "skills/pr-review/SKILL.md",
                "frontmatter": {
                  "name": "pr-review",
                  "description": "Review GitHub PRs with inline comments and structured summaries. Use when reviewing PRs via gh CLI."
                },
                "content": "# PR Review Skill\n\nInteractive GitHub PR review workflow: fetch PR, iterate findings with clarification, manage draft comments, submit when ready.\n\n**Delegates to:** `code-review` for methodology, `code-implement` for language guidelines.\n\n**Requires:** `gh-review` extension (`gh extension install srnnkls/gh-review`)\n\n---\n\n## When to Use\n\n- Reviewing PRs on GitHub repositories\n- When asked to review `owner/repo#N` or just `#N`\n- Posting structured feedback with inline code comments\n\n---\n\n## Workflow\n\n### Step 1: Configure Review\n\nUse **AskUserQuestion** to gather configuration (see `code-review` for details):\n- Focus areas: correctness, style, performance, security\n- Severity threshold: blocking, high, all\n- Scope: full, quick\n\n### Step 2: Fetch PR Context\n\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json title,body,files,commits,additions,deletions\ngh pr diff {pr} --repo {owner}/{repo}\ngh api repos/{owner}/{repo}/pulls/{pr} --jq '.head.sha'  # Get commit SHA\n```\n\n### Step 3: Check Existing Drafts\n\nFetch any pending review from previous session:\n\n```bash\ngh review comments {pr} -R {owner}/{repo} --mine --states=pending\n```\n\nIf drafts exist, display them and offer to continue or discard.\n\n### Step 4: Check Related Issues\n\n```bash\ngh issue view {issue} --repo {owner}/{repo} --json title,body\n```\n\n### Step 5: Delegate to Review Skills\n\n**Detect language** from file extensions:\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json files --jq '.files[].path' | \\\n  sed 's/.*\\.//' | sort | uniq -c | sort -rn\n```\n\n**Load review methodology:**\n- `code-review` - Generic review process and checklist\n\n**Load language guidelines:**\n- `~/.claude/skills/code-implement/resources/loqui/languages/{language}/*` - Load language-specific resources based on file extensions\n\n### Step 6: Reference Style Guides\n\nCompare against:\n- Project style guides (STYLE.md, CLAUDE.md)\n- Established patterns from related issues\n\n### Step 7: Iterate Findings with Clarification\n\nFor each potential issue identified:\n\n1. **Present finding** - Show code context and concern\n2. **Ask context-aware questions** via `AskUserQuestion` with `multiSelect`:\n   - Type changes → \"Is type widening intentional?\"\n   - Error handling removed → \"Was this error path obsolete?\"\n   - Performance-sensitive code → \"Acceptable trade-off?\"\n   - General → \"Flag this?\", \"Severity level?\"\n3. **Batch decisions** - Allow selecting multiple findings to flag/dismiss\n\n### Step 8: Create/Update Draft Comments\n\nFor each confirmed finding, add to pending review:\n\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {line} -b \"{comment_body}\"\n```\n\nFor multi-line comments:\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {end_line} --start-line {start_line} -b \"{comment_body}\"\n```\n\nTo update an existing draft:\n```bash\ngh review edit {pr} -R {owner}/{repo} -c {comment_id} -b \"{updated_body}\"\n```\n\nTo remove a draft:\n```bash\ngh review delete {pr} -R {owner}/{repo} -c {comment_id}\n```\n\n### Step 9: Submit or Keep Draft\n\nUse **AskUserQuestion** to determine action:\n\n**Options:**\n- **Submit now** → Choose verdict (approve, request_changes, comment)\n- **Keep as draft** → Leave pending for later continuation\n\nTo submit:\n```bash\ngh review submit {pr} -R {owner}/{repo} -v {approve|request_changes|comment} -b \"{summary}\"\n```\n\nTo discard and start fresh:\n```bash\ngh review discard {pr} -R {owner}/{repo}\n```\n\n---\n\n## Key Gotchas\n\n- **Line numbers**: Use actual file line numbers (not diff positions) with `gh review`\n- **Comment IDs**: GraphQL node IDs (e.g., `PRRC_kwDOABC123`) - get with `--ids` flag\n- **Comment format**: Supports markdown, suggestion blocks with triple backticks\n\n---\n\n## gh-review Commands\n\n| Command | Description |\n|---------|-------------|\n| `comments` | List PR comments with filtering (`--mine`, `--states`) |\n| `view` | View review threads hierarchically |\n| `add` | Add draft comment to pending review |\n| `edit` | Update existing draft comment |\n| `delete` | Remove draft comment |\n| `submit` | Submit review with verdict |\n| `discard` | Delete pending review |\n\n---\n\n## Related Skills\n\n- **code-review**: Review methodology (focus, severity, checklist)\n- **code-implement**: Language guidelines and implementation patterns\n\n---\n\n## Reference\n\n- `gh review --help`\n- `gh pr review --help`"
              },
              {
                "name": "skill-create",
                "description": "Create new Claude Code skills following project patterns and best practices. Use when building new skills, extracting reusable capabilities, or converting commands to skills.",
                "path": "skills/skill-create/SKILL.md",
                "frontmatter": {
                  "name": "skill-create",
                  "description": "Create new Claude Code skills following project patterns and best practices. Use when building new skills, extracting reusable capabilities, or converting commands to skills."
                },
                "content": "# Skill Creation\n\nCreate well-structured skills using progressive disclosure and project conventions.\n\n> **Reference:** [best-practices.md](best-practices.md) for comprehensive guidance, [reference.md](reference.md) for project patterns and frontmatter specs.\n\n---\n\n## Workflow\n\n### Step 1: Understand Use Cases\n\nGather concrete examples of how the skill will be used:\n\n- What tasks will it handle?\n- What would users say to trigger it?\n- What variations exist?\n\nSkip this step only when usage patterns are already clearly understood.\n\n### Step 2: Plan Contents\n\nAnalyze each use case to identify reusable resources:\n\n| Resource Type | When to Use | Example |\n|---------------|-------------|---------|\n| `scripts/` | Same code rewritten repeatedly | `rotate_pdf.py` |\n| `references/` | Domain knowledge Claude needs | `schema.md`, `api.md` |\n| `assets/` | Files used in output | `template.html`, `logo.png` |\n| `templates/` | Document structure patterns | `report.md` |\n\n### Step 3: Choose Frontmatter\n\n**Required fields:**\n\n```yaml\nname: skill-name          # Lowercase, hyphens, max 64 chars\ndescription: |            # Max 1024 chars\n  [What it does]. Use when [context].\n```\n\n**Optional fields:**\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `context` | Run in forked sub-agent | `context: fork` |\n| `agent` | Specify agent type | `agent: haiku` |\n| `user-invocable` | Hide from slash menu | `user-invocable: false` |\n| `allowed-tools` | Restrict available tools | See reference.md |\n| `hooks` | Lifecycle hooks (PreToolUse, PostToolUse, Stop) | See reference.md |\n\n**Naming pattern:** `<namespace>[-<subnamespace>]-<action>`\n- `code-debug`, `spec-create`, `git-worktree-use`\n\n**Description format:** Third person, what + when.\n- \"Generate GitHub issue drafts from spec directories. Use when converting specs to GitHub issues.\"\n\n### Step 4: Create Structure\n\n```bash\nmkdir -p .claude/skills/{skill-name}\n```\n\n**Standard structure:**\n\n```\n.claude/skills/{skill-name}/\n├── SKILL.md              # Main instructions (<500 lines)\n├── templates/            # Document templates (.md)\n├── scripts/              # Executable code (.sh, .py)\n└── references/           # Extended documentation\n```\n\n### Step 5: Implement & Test\n\n**Write SKILL.md:**\n- Keep under 200 lines (500 max)\n- Progressive disclosure: SKILL.md → references/\n- Include concrete examples, no emojis\n- Reference authoritative docs (don't duplicate)\n\n**Test with real tasks:**\n1. Does the description trigger correctly?\n2. Can Claude find bundled resources?\n3. Does the workflow complete successfully?\n\n---\n\n## Degrees of Freedom\n\nMatch specificity to task fragility:\n\n**High freedom** - Multiple approaches valid, context-dependent:\n```markdown\n## Code review\n1. Analyze structure and organization\n2. Check for bugs and edge cases\n3. Suggest improvements\n```\n\n**Low freedom** - Operations fragile, consistency critical:\n```markdown\n## Database migration\nRun exactly: `python scripts/migrate.py --verify --backup`\nDo not modify flags.\n```\n\n---\n\n## Skill Types\n\n| Type | Characteristics | Examples |\n|------|-----------------|----------|\n| **Operational** | Multi-step workflow, state changes, document templates | `spec-create`, `spec-archive` |\n| **Generation** | Transform input → structured output, format templates | `spec-issues-create` |\n| **Guidance** | Imperative instructions, code patterns | `code-implement`, `code-debug` |\n\n---\n\n## Success Criteria\n\n- Name follows `<namespace>[-<subnamespace>]-<action>`\n- Description is third person with what + when\n- SKILL.md under 200 lines (500 max)\n- Workflow steps numbered and actionable\n- Templates extracted to separate files\n- References point to authoritative sources\n- No emojis (text markers only)\n\n---\n\n## Reference\n\n- [best-practices.md](best-practices.md) - Core principles, patterns, checklist\n- [reference.md](reference.md) - Project patterns, frontmatter specs, anti-patterns"
              },
              {
                "name": "spec-archive",
                "description": "Archive completed development specs from ./specs/active/ to ./specs/archive/, updating documents with completion status and maintaining archive index. Use when finishing tasks or moving completed work to archive.",
                "path": "skills/spec-archive/SKILL.md",
                "frontmatter": {
                  "name": "spec-archive",
                  "description": "Archive completed development specs from ./specs/active/ to ./specs/archive/, updating documents with completion status and maintaining archive index. Use when finishing tasks or moving completed work to archive."
                },
                "content": "# Spec Archive Skill\n\nArchive completed development specs with proper documentation and index maintenance.\n\n> **Reference**: See [reference.md](reference.md).\n\n---\n\n## When to Use\n\nArchive a spec when:\n- All success criteria met\n- Tests passing and code merged\n- Ready to start new work\n- Spec is \"done enough\" and blocking new work\n- User explicitly requests archival\n\nDon't archive when:\n- Spec is actively in progress\n- Blocking issues remain unresolved\n- Branch has unmerged changes\n- Critical checklist items incomplete (unless user confirms)\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/active/add-temporal-joins/\n```\n\nIf not found, list available specs and ask which to archive.\n\n### Step 2: Verify Completion Status\n\nBefore archiving, check:\n- Read `spec.md` - status should be \"Complete\"\n- Read `tasks.yaml` - verify all tasks have `status: completed`\n- If incomplete, ask: \"This spec appears incomplete. Archive anyway? (y/n)\"\n\n### Step 3: Pre-Archive Updates\n\nUpdate documents before moving:\n\n**In `spec.md`:**\n- Set status to `Complete`\n- Add `**Completed**: [Today's date]` after Started date\n- Ensure all success criteria marked complete\n\n**In `tasks.yaml`:**\n- Update `meta.progress` to final count (e.g., \"45/45\")\n- Update `meta.last_updated` to today\n- Ensure all tasks have `status: completed`\n- Mark any phase checkpoints as `verified: true`\n\n**In `context.md`:**\n- Update \"Last Updated\" to today\n- Add Archive Notes section using [templates/archive-notes.md](templates/archive-notes.md)\n\n### Step 4: Archive the Spec\n\n```bash\nmkdir -p ./specs/archive/\nmv ./specs/active/{spec-name} ./specs/archive/{spec-name}\n```\n\n### Step 5: Git Operations (Optional)\n\nIf spec has associated branch:\n1. Check current branch\n2. If on spec branch, ask: \"Switch back to main? (y/n)\"\n3. Ask: \"Delete spec branch `{branch}` (only if merged)? (y/n)\"\n\n### Step 6: Create/Update Archive Index\n\nCreate `./specs/archive/README.md` if doesn't exist using [templates/archive-index.md](templates/archive-index.md).\n\nAdd entry at top of Archive Index section:\n\n```markdown\n### [Spec Name] - [Today's Date]\n- **Duration**: [Started] → [Completed]\n- **Branch**: [branch-name if applicable]\n- **Summary**: [One sentence from spec.md Overview]\n- **Location**: `./specs/archive/{spec-name}/`\n\n---\n```\n\n### Step 7: Confirm Completion\n\nReport to user:\n```\nArchived: {spec-name}\n\n  From: ./specs/active/{spec-name}/\n  To:   ./specs/archive/{spec-name}/\n\n  Files archived:\n  - spec.md (Complete)\n  - context.md (Final notes added)\n  - tasks.yaml (X/Y tasks complete)\n\n  Archive index updated: ./specs/archive/README.md\n\n  [If branch operations performed]:\n  Git: Switched to main, deleted branch {branch}\n```\n\n---\n\n## Partial Completion Handling\n\nIf archiving incomplete spec (user confirmed):\n- Mark status as `Incomplete - [Reason]` not `Complete`\n- In Archive Notes, clearly document what was NOT completed\n- Explain why spec was abandoned/deferred\n- Consider creating new spec for remaining work\n\n---\n\n## Archive Notes Template\n\nSee [templates/archive-notes.md](templates/archive-notes.md) for full template including:\n- Summary (2-3 sentences)\n- Key Outcomes (deliverables)\n- Technical Debt / Future Work\n- Lessons Learned\n\n---\n\n## Templates\n\n- [templates/archive-notes.md](templates/archive-notes.md) - Archive Notes section for context.md\n- [templates/archive-index.md](templates/archive-index.md) - Archive README template\n\n---\n\n## Success Criteria\n\n- Spec moved from `./specs/active/` to `./specs/archive/`\n- All three documents updated with completion status\n- Archive Notes added to context.md\n- Archive index entry created\n- User informed of next steps\n\n---\n\n## Integration\n\n**Workflow:**\n- During task: Use `/spec.update` to sync status\n- After task: Use this skill to archive\n- Result: Clean `./specs/active/`, historical record preserved\n\n**Related:**\n- Command: `/spec.archive`\n- Skills: `spec-create` (creation), `spec-update` (reminder)\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for archive organization guidelines and troubleshooting."
              },
              {
                "name": "spec-clarify",
                "description": "Resolve ambiguities and markers in validation.yaml interactively. Updates spec docs directly with full audit trail.",
                "path": "skills/spec-clarify/SKILL.md",
                "frontmatter": {
                  "name": "spec-clarify",
                  "description": "Resolve ambiguities and markers in validation.yaml interactively. Updates spec docs directly with full audit trail."
                },
                "content": "# Spec Clarify Skill\n\nResolve ambiguities and markers by updating spec documents directly with tracked changes.\n\n---\n\n## When to Use\n\n**Use for:**\n- Resolving markers before task-dispatch (especially for Initiatives)\n- Clarifying ambiguous requirements discovered post-validation\n- Addressing gaps identified in ambiguity_scan\n\n**Don't use for:**\n- Initial validation (use spec-validate)\n- Changing fundamental scope (re-run spec-validate)\n\n---\n\n## Workflow\n\n### Step 1: Load and Scan\n\n1. Find active spec in `./specs/active/*/`\n2. Read `validation.yaml` from spec directory\n3. Run ambiguity scan:\n   - Check `ambiguity_scan` section for areas with `status: partial` or `status: missing`\n   - These become clarification candidates alongside open markers\n4. Collect open markers where `status: open`\n5. Merge candidates: ambiguity gaps + open markers (deduplicate by area)\n6. If no candidates: report \"No unresolved items\" and exit\n\n### Step 2: Present Candidates\n\nFor each candidate (prioritized by impact), use AskUserQuestion:\n\n```\nHeader: ${AREA}\nQuestion: ${DESCRIPTION}\nmultiSelect: false\nOptions:\n- [Generated options based on context]\n- Defer: Skip for now\n```\n\n**Prioritization:** Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 3: Update Spec Documents Directly\n\nWhen a clarification is resolved, update the relevant section in the source document:\n\n| Clarification Area | Target Document | Target Section |\n|--------------------|-----------------|----------------|\n| Scope | spec.md | Requirements, Scope |\n| Behavior | spec.md | Requirements, Behavior |\n| Data Model | context.md | Data Model |\n| Constraints | spec.md | Constraints |\n| Edge Cases | spec.md | Edge Cases |\n| Integration | context.md | Integration Points |\n| Terminology | context.md | Terminology |\n\n**Update approach:**\n1. Read the target section\n2. Integrate the clarification naturally into existing content\n3. Do NOT create a separate \"## Clarifications\" section\n\n### Step 4: Record Clarification Session\n\nCreate a new session entry in `clarification_sessions`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n```\n\n**doc_updates** tracks exactly which files/sections changed for audit trail.\n\n### Step 5: Update Ambiguity Scan Status\n\nFor each resolved clarification from ambiguity gaps:\n\n1. Update `ambiguity_scan.${area}.status` to `clear`\n2. Remove the resolved gap from `ambiguity_scan.${area}.gaps`\n\n### Step 6: Update Markers\n\nFor each resolved marker:\n\n1. Change `status: open` to `status: resolved`\n2. Add `resolution: \"${USER_ANSWER}\"`\n\n### Step 7: Re-check Gates (Initiatives Only)\n\nFor Initiative specs:\n\n1. Re-evaluate gates in validation.yaml\n2. Update gate status if resolution changes assessment\n3. Report gate status\n\n---\n\n## Doc Update Mapping\n\n| Source | Target File | Target Section |\n|--------|-------------|----------------|\n| Scope gap | spec.md | ## Requirements or ## Scope |\n| Behavior gap | spec.md | ## Requirements / Behavior subsection |\n| Data Model gap | context.md | ## Data Model |\n| Constraints gap | spec.md | ## Constraints |\n| Edge Cases gap | spec.md | ## Edge Cases |\n| Integration gap | context.md | ## Integration Points |\n| Terminology gap | context.md | ## Terminology |\n\n---\n\n## Example Session\n\n```\n[Load validation.yaml]\n[Run ambiguity scan]\n- scope: partial (1 gap)\n- data_model: missing (2 gaps)\n[Check markers]\n- M001 (Constraints): open\n\nCandidates:\n1. Scope: \"User role boundaries unclear\"\n2. Data Model: \"Schema for notifications not defined\"\n3. Data Model: \"Retention policy not specified\"\n4. Constraints: \"Authentication method not specified\"\n\n---\nHeader: Scope\nQuestion: What user roles exist and what are their boundaries?\n\nOptions:\n- Admin/User: Two-tier with admin full access\n- Role-based: Granular permissions per feature\n- Defer: Skip for now\n\nUser selects: Admin/User\n\n[Update spec.md#requirements]\nAdded: \"Two-tier role system: Admin (full access), User (standard permissions)\"\n\n[Record session]\nclarification_sessions:\n  - id: S001\n    timestamp: 2025-01-15T10:30:00Z\n    questions:\n      - id: Q001\n        question: \"What user roles exist and what are their boundaries?\"\n        answer: \"Two-tier: Admin (full access), User (standard permissions)\"\n        area: scope\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n\n[Update ambiguity_scan]\nscope:\n  status: clear\n  gaps: []\n\n---\nHeader: Constraints\nQuestion: Which authentication method should be used?\n...\n```\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.clarify` command\n\n**Related skills:**\n- `spec-validate` - Initial validation (creates ambiguity_scan and markers)\n- `spec-create` - Document creation (references markers)\n- `task-dispatch` - Checks for blocking markers before dispatch"
              },
              {
                "name": "spec-create",
                "description": "Create spec documents (spec.md, context.md, tasks.yaml, dependencies.yaml, validation.yaml). Receives validation data from spec-validate.",
                "path": "skills/spec-create/SKILL.md",
                "frontmatter": {
                  "name": "spec-create",
                  "description": "Create spec documents (spec.md, context.md, tasks.yaml, dependencies.yaml, validation.yaml). Receives validation data from spec-validate."
                },
                "content": "# Spec Creation Skill\n\nCreates structured tracking documents for complex development tasks.\n\n---\n\n## When to Use\n\n**DO use for:**\n- Complex multi-step tasks (3+ distinct phases)\n- Non-trivial features requiring careful planning\n- After ExitPlanMode when user accepts a plan\n- Multi-phase implementation work\n\n**DON'T use for:**\n- Single-file changes\n- Trivial refactorings\n- Simple bug fixes\n- Tasks completable in < 30 minutes\n\n---\n\n## Workflow\n\n### Step 1: Check for Validation Data\n\n**If invoked after spec-validate:**\n- Issue type, taxonomy coverage, and clarification log are available\n- Use this data to populate frontmatter and validation.yaml\n- Proceed to Step 2\n\n### Step 2: Determine Task Name\n\nDerive from: git branch name, ExitPlanMode plan name, or user-provided argument.\nFormat: `kebab-case` (e.g., `add-temporal-joins`). If unclear, ask user.\n\n### Step 3: Gather Context\n\nRead relevant files to understand: task goal, key files, central types, architectural decisions, current vs target state.\n\n### Step 3.5: Detect and Extract Code Artifacts\n\n**If input contains code blocks** (```python, ```rust, etc.):\n\n1. **Extract code blocks** with their language tags\n2. **Create resources directory:**\n   ```bash\n   mkdir -p ./specs/draft/[task-name]/resources/\n   ```\n3. **Stage extracted content** (held in memory until user chooses):\n   - Code blocks with language fences\n   - Schema definitions (OpenAPI, JSON Schema, type definitions)\n   - Config examples (YAML, TOML, env files)\n   - Integration/test patterns\n\n4. **Ask user which resources to create** (use AskUserQuestion with multiSelect):\n   ```\n   Header: Resources\n   Question: Which implementation artifacts should be preserved?\n   multiSelect: true\n   Options:\n   - implementation: Code sketches/examples - patterns to follow (loqui-validated)\n   - schemas: API contracts, data models, type definitions\n   - config: Configuration examples\n   - patterns: Integration and test patterns\n   - assets: Diagrams, screenshots, other media\n   - none: Skip resources, spec only\n   ```\n\n5. **Create selected resources:**\n   - Only create directories for selected options\n   - Run loqui validation on code (if selected and language has guidelines)\n   - Report violations in validation.yaml under `loqui_check` section\n\n6. **Continue with spec generation** using extracted requirements (not code details)\n\n**Resources directory structure** (all subdirectories optional):\n```\nspecs/draft/{spec-name}/\n├── spec.md\n├── context.md\n├── tasks.yaml\n├── ...\n└── resources/           # Only if user selected any\n    ├── implementation.md   # If \"implementation\" selected\n    ├── schemas/         # If \"schemas\" selected\n    ├── config/          # If \"config\" selected\n    ├── patterns/        # If \"patterns\" selected\n    └── assets/          # If \"assets\" selected\n```\n\n### Step 4: Create Directory and Documents\n\n```bash\nmkdir -p ./specs/draft/[task-name]/\n```\n\nGenerate these files:\n\n1. **`spec.md`** - Strategic spec (review this)\n2. **`context.md`** - Implementation context (update as you work)\n3. **`tasks.yaml`** - Work checklist (dignity CLI, TodoWrite sync)\n4. **`dependencies.yaml`** - Task dependency graph (parallel dispatch)\n5. **`validation.yaml`** - Audit trail and gate checks\n\n**Document scaling by issue type:**\n\n| Document | Initiative | Feature | Task |\n|----------|-----------|---------|------|\n| spec.md | Full strategic | Standard | Lightweight |\n| context.md | High-level | Standard | Skip |\n| tasks.yaml | Feature breakdown + phases | Task breakdown | Single task |\n| dependencies.yaml | Full DAG | Phase-based | Skip |\n| validation.yaml | Full (7 areas + gates) | Full (7 areas) | Skip |\n\n**Task output = 2 files:** spec.md (lightweight) + tasks.yaml\n\n**Frontmatter for spec.md:**\n\n```yaml\n---\nissue_type: [Initiative|Feature|Task]\ncreated: [Date]\nstatus: Draft\nstage: draft\nclaude_plan: [path to native Claude plan file, if exists]\n---\n```\n\nIf a native Claude plan exists (from EnterPlanMode/ExitPlanMode), include its path in the `claude_plan` field. This links the detailed spec to its originating design document.\n\n### Conditional Sections\n\nBased on validation data and issue type, include optional sections:\n\n**spec.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| User Stories (P1/P2/P3) | Include | Skip | Skip |\n| Given/When/Then Acceptance | Full | Standard | Simple |\n| API Contract | Include if API work | Opt-in | Skip |\n| Implementation Strategy | Include | Skip | Skip |\n\n**context.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Tech Decisions | Include | Opt-in | (file skipped) |\n| Data Model | Include | Opt-in | (file skipped) |\n\n**validation.yaml:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Gates | Evaluate all | n/a | (file skipped) |\n| Complexity Tracking | If violations | If violations | (file skipped) |\n| Markers | Full | Full | (file skipped) |\n\n**tasks.yaml:**\n\n| Element | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Phases with checkpoints | Full (gates between phases) | Simple (optional checkpoints) | Skip |\n| Evidence tracking | Full | Opt-in | Skip |\n| Dependencies | Full | Opt-in | Skip |\n\n**tasks.yaml structure:**\n\n```yaml\nspec: ${SPEC_NAME}\ncode: ${CODE}           # Prefix for task IDs (e.g., FEAT → FEAT-001)\nnext_id: 1\ntasks:\n  - id: ${CODE}-001\n    content: Task description\n    status: pending     # pending | in_progress | completed\n    active_form: Doing task description\nmeta:\n  created: ${DATE}\n  last_updated: ${DATE}\n  progress: 0/N\nphases:                 # Optional: organize tasks with checkpoints\n  - name: \"Phase 1: Setup\"\n    task_ids: [${CODE}-001, ${CODE}-002]\n    checkpoint:\n      description: Setup complete\n      criteria: [...]\n      verified: false\n```\n\nUsed by dignity for task management operations. The `code` field generates sequential IDs (e.g., AUT-001, AUT-002).\n\n### Step 5: Populate TodoWrite from tasks.yaml\n\nParse the just-created `tasks.yaml` and populate TodoWrite:\n\n1. Read tasks from `tasks.yaml`\n2. Create TodoWrite with up to 10 tasks:\n   - status: map from tasks.yaml status\n   - content: task content field\n   - activeForm: task active_form field\n\n**Example:**\n```yaml\n# tasks.yaml:\ntasks:\n  - id: IMPL-001\n    content: Create implement.md command\n    status: pending\n    active_form: Creating implement.md command\n  - id: IMPL-002\n    content: Create docs-implement SKILL.md\n    status: pending\n    active_form: Creating docs-implement SKILL.md\n```\n\n```json\n// TodoWrite:\n[\n  {\"content\": \"Create implement.md command\", \"status\": \"pending\", \"activeForm\": \"Creating implement.md command\"},\n  {\"content\": \"Create docs-implement SKILL.md\", \"status\": \"pending\", \"activeForm\": \"Creating docs-implement SKILL.md\"}\n]\n```\n\n### Step 6: Present Summary\n\nShow user:\n- Directory created: `./specs/draft/[task-name]/`\n- Human-facing docs: spec.md, context.md (brief overview of each)\n- Tooling artifacts: tasks.yaml, dependencies.yaml, validation.yaml (note these are for automation)\n- Validation coverage (if from spec-validate)\n- Next action: \"Run `/spec.review` or `/spec.promote` when ready to activate\"\n\n### Step 7: Offer Comprehensive Review (Optional)\n\nAfter presenting summary, offer spec review:\n\nUse AskUserQuestion:\n```\nHeader: Review\nQuestion: Would you like a comprehensive spec review before implementation?\nmultiSelect: false\nOptions:\n- Yes: Run multi-agent review (Claude + OpenCode)\n- Later: Skip for now, use /spec.review when ready\n- Skip: Proceed without review\n```\n\nIf \"Yes\": Invoke `spec-review` skill with the just-created spec name.\n\n---\n\n## Output Artifacts\n\n**For humans (review these):**\n```\n├── spec.md      # WHY & WHAT - Strategic requirements, acceptance criteria\n├── context.md   # WHAT WE LEARNED - Key files, decisions, gotchas\n└── resources/   # HOW TO BUILD - Implementation details (when provided)\n```\n\n**For tooling (infrastructure):**\n```\n├── tasks.yaml        # Progress tracking, TodoWrite sync\n├── dependencies.yaml # Parallel dispatch DAG\n└── validation.yaml   # Audit trail, gate checks, loqui validation\n```\n\nThe YAML files are infrastructure - humans *can* read them for debugging or auditing, but the primary consumers are tooling and automation.\n\n### The resources/ Directory\n\nOnly created when implementation details are provided in input. Contains structured artifacts:\n\n| Subdirectory | Content |\n|-------------|---------|\n| `implementation.md` | Code sketches and examples - patterns to follow, not tested/final (loqui-validated) |\n| `schemas/` | API contracts, data models, type definitions |\n| `config/` | Configuration examples |\n| `patterns/` | Integration and test patterns |\n| `assets/` | Diagrams, screenshots, other media |\n\n**Review burden:** 2-3 documents (spec.md, context.md, resources/ when present), not 8+.\n\nSee [guidelines.md](guidelines.md) for detailed breakdown.\n\n### Native Plan Integration\n\nIf a native Claude plan exists (from EnterPlanMode), add a **Native Plan** section to `context.md`:\n\n```markdown\n## Native Plan\n\n**Source:** `/path/to/.claude/specs/spec-name.md`\n\nSummary of the original design:\n- Goal: [brief goal from native plan]\n- Approach: [key approach decisions]\n- Open questions resolved: [any clarifications made]\n```\n\nThis preserves the connection to the original design discussion and rationale.\n\n---\n\n## Templates\n\nLocated in `templates/` directory:\n- [validation.yaml](templates/validation.yaml)\n- [dependencies.yaml](templates/dependencies.yaml)\n- [tasks.yaml](templates/tasks.yaml)\n- [spec.md](templates/spec.md)\n- [context.md](templates/context.md)\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command\n- Runs `spec-validate` first, then this skill\n\n**Related commands:**\n- `/spec.review` - Review spec with multiple AI reviewers\n- `/spec.update` - Sync spec with git history\n- `/spec.archive` - Archive completed spec\n- `/spec.issues` - Generate GitHub issues from spec"
              },
              {
                "name": "spec-issues-create",
                "description": "Generate GitHub issue drafts from spec directories, creating initiative/feature/task markdown files with gh CLI commands. Use when converting specs to GitHub issues or setting up issue tracking for features.",
                "path": "skills/spec-issues-create/SKILL.md",
                "frontmatter": {
                  "name": "spec-issues-create",
                  "description": "Generate GitHub issue drafts from spec directories, creating initiative/feature/task markdown files with gh CLI commands. Use when converting specs to GitHub issues or setting up issue tracking for features."
                },
                "content": "# Spec Issues Skill\n\nGenerate GitHub issue drafts from spec documents following the project's issue management framework.\n\n---\n\n## When to Use\n\nUse when you need to create GitHub issues from:\n- Spec documents (`./specs/`)\n- Any structured work breakdown ready for issue tracking\n\n---\n\n## Workflow\n\n### Step 1: Validate Input Path\n\nParse path from command argument and verify:\n\n```bash\n# User provides path like:\n# ./specs/active/my-feature\n# ./specs/archive/nested-view-refactor\n```\n\nValidate directory exists and contains appropriate files.\n\n### Step 2: Detect Issue Type\n\n**Spec indicators:**\n- Has `spec.md`\n- Has `tasks.md`\n- Has `context.md`\n\n**Issue type detection:**\n\n1. Read spec.md frontmatter for `issue_type` field:\n   ```yaml\n   ---\n   issue_type: [Initiative|Feature|Task]\n   created: [Date]\n   status: Active\n   ---\n   ```\n\n2. If `issue_type` present in frontmatter:\n   - Skip type selection question\n   - Use frontmatter value directly for template selection\n\n3. If `issue_type` absent:\n   - Fall back to current detection logic (spec overview analysis)\n   - Consider asking user to classify\n\n**Template selection based on issue_type:**\n- `Initiative` → `templates/initiative.md`\n- `Feature` → `templates/feature.md`\n- `Task` → `templates/task.md`\n\nRead appropriate files based on detected type.\n\n### Step 3: Create Issue Drafts Directory\n\n```bash\nmkdir -p \"$SOURCE_DIR/drafts/issues\"\n```\n\n**Structure created:**\n```\n./specs/archive/{spec-name}/drafts/issues/\n├── initiative-{spec-name}.md\n├── issue-001-{short-description}.md\n├── issue-002-{short-description}.md\n└── ...\n```\n\n### Step 4: Generate Issue Draft Files\n\nCreate issue markdown files using templates:\n\n**Initiative file**: `initiative-{name}.md`\n- Use [templates/initiative.md](templates/initiative.md)\n- Map spec overview → initiative overview\n- Map success criteria → acceptance criteria\n- Include YAML frontmatter with metadata\n\n**Feature/Task files**: `issue-{NNN}-{description}.md`\n- Use [templates/feature.md](templates/feature.md) or [templates/task.md](templates/task.md)\n- Extract from spec phases or tasks\n- Sequential numbering: 001, 002, 003\n- Include YAML frontmatter\n\n**Key points:**\n- YAML frontmatter contains: title, labels, milestone, assignees\n- Frontmatter serves as reference metadata\n- Content extracted from spec documents\n\n### Step 5: Generate gh CLI Script\n\nCreate bash script using [scripts/create-issue.sh](scripts/create-issue.sh):\n\n- Parse YAML frontmatter from draft files\n- Generate `gh issue create` commands\n- Capture issue numbers for sub-issue linking\n- Use `gh sub-issue add` for hierarchy\n- Include helper function to extract YAML fields\n\n**Script structure:**\n```bash\n#!/bin/bash\n# Extract YAML → gh issue create → capture numbers → link sub-issues\n```\n\n### Step 6: Provide Metadata Configuration Checklist\n\nInclude instructions for setting fields via GitHub web UI:\n- Priority (Critical/High/Medium/Low)\n- Status (Backlog/To Do/In Progress/etc.)\n- Sprint/Iteration\n- Complexity estimation\n- Milestone/Release\n\n---\n\n## Content Transformation\n\nSee [mapping-guide.md](mapping-guide.md) for detailed rules on:\n- Mapping spec content to issue hierarchy (Initiative → Feature → Task)\n- Issue title patterns for each type\n- Component label detection from file paths\n- Priority and complexity mapping\n- Cross-linking patterns\n\nQuick summary:\n- Specs: phases → Features, task breakdown → Tasks\n- Titles follow verb-noun pattern, increasing specificity by level\n\n---\n\n## Templates\n\n- [templates/initiative.md](templates/initiative.md) - Initiative issue template with YAML\n- [templates/feature.md](templates/feature.md) - Feature issue template with YAML\n- [templates/task.md](templates/task.md) - Task issue template with YAML\n\n## Scripts\n\n**generate-issues.py**: Automates draft generation from specs. Parses content, creates issue files with YAML frontmatter.\n**create-issue.sh**: Creates actual GitHub issues from drafts using `gh` CLI.\n\n**Requirement**: Install `gh sub-issue` extension: `gh extension install yahsan2/gh-sub-issue`\n\n---\n\n## Success Criteria\n\n- Drafts directory created: `{source}/drafts/issues/`\n- Initiative file with complete overview\n- Feature/task files with proper templates\n- gh CLI script with issue creation commands\n- Metadata checklist provided\n\n---\n\n## Integration\n\n**Workflow:**\n1. Complete spec\n2. Invoke this skill with source path\n3. Review generated draft files\n4. Run gh CLI script to create issues\n5. Set metadata via GitHub web UI\n\n**Related:**\n- Command: `/spec.issues`\n- Tools: `gh` CLI, `gh sub-issue` extension"
              },
              {
                "name": "spec-promote",
                "description": "Promote spec from draft to active stage. Use after spec review passes or when ready to begin implementation.",
                "path": "skills/spec-promote/SKILL.md",
                "frontmatter": {
                  "name": "spec-promote",
                  "description": "Promote spec from draft to active stage. Use after spec review passes or when ready to begin implementation."
                },
                "content": "# Spec Promote Skill\n\nPromote validated specs from draft to active stage for implementation.\n\n---\n\n## When to Use\n\nPromote a spec when:\n- Spec validation passes (no open markers or user override)\n- Ready to begin implementation\n- Spec review approved\n- User explicitly requests promotion\n\nDon't promote when:\n- Open markers exist without user override\n- Initiative has failed gates\n- Spec is incomplete or missing required documents\n- Draft directory doesn't exist\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/draft/add-temporal-joins/\n```\n\nIf not found, list available draft specs and ask which to promote.\n\n### Step 2: Verify Readiness\n\nBefore promoting, check:\n\n**Read `validation.yaml`** (if exists):\n- Check markers section for `status: open`\n- If open markers exist, warn user and ask to proceed anyway\n- For Initiatives: check gates section for `status: failed`\n- If failed gates exist, block promotion (require user to fix)\n\n**Read `spec.md`**:\n- Verify required sections exist\n- Note current status for update\n\nIf validation.yaml doesn't exist (e.g., Task issue type), proceed without marker checks.\n\n### Step 3: Move to Active\n\n```bash\nmkdir -p ./specs/active/\nmv ./specs/draft/{spec-name} ./specs/active/{spec-name}\n```\n\n### Step 4: Update Metadata\n\nUpdate `spec.md` frontmatter:\n\n**Change:**\n```yaml\nstatus: Draft\nstage: draft\n```\n\n**To:**\n```yaml\nstatus: Active\nstage: active\npromoted: {TODAY'S DATE}\n```\n\nIf `stage` field doesn't exist, add it. Preserve all other frontmatter fields.\n\n### Step 5: Report Success\n\nReport to user:\n```\nPromoted: {spec-name}\n\n  From: ./specs/draft/{spec-name}/\n  To:   ./specs/active/{spec-name}/\n\n  Status: Active\n  Promoted: {DATE}\n\n  [If open markers were overridden]:\n  Warning: {N} open markers remain unresolved\n\nNext steps:\n  - Run /implement to begin task execution\n  - Or use task-dispatch skill for parallel work\n```\n\n---\n\n## Readiness Checks\n\n### Marker Status Check\n\n```yaml\n# In validation.yaml\nmarkers:\n  - id: M001\n    status: open    # WARN: ask user to proceed\n  - id: M002\n    status: resolved  # OK\n```\n\nOpen markers indicate unresolved ambiguities. Warn but allow override.\n\n### Gate Status Check (Initiatives Only)\n\n```yaml\n# In validation.yaml\ngates:\n  simplicity:\n    status: failed  # BLOCK: cannot promote\n  anti_abstraction:\n    status: passed  # OK\n```\n\nFailed gates block promotion. User must resolve before promoting.\n\n---\n\n## Error Handling\n\n| Condition | Action |\n|-----------|--------|\n| Spec not found in draft | List available drafts, ask user |\n| Open markers | Warn, ask to proceed (y/n) |\n| Failed gates | Block, explain which gates failed |\n| Missing spec.md | Error: \"Invalid spec directory\" |\n| Already in active | Error: \"Spec already active\" |\n\n---\n\n## Integration\n\n**Workflow:**\n- Create: `spec-validate` -> `spec-create` (creates in draft/)\n- Review: Manual review or `/spec.clarify`\n- Promote: This skill (draft/ -> active/)\n- Execute: `task-dispatch` or `/implement`\n- Archive: `spec-archive` (active/ -> archive/)\n\n**Related:**\n- Command: `/spec.promote`\n- Skills: `spec-create` (creates drafts), `spec-archive` (archives active)\n- Skill: `spec-clarify` (resolve open markers before promoting)"
              },
              {
                "name": "spec-review",
                "description": "Multi-agent spec review with parallel Claude/OpenCode reviewers. Use after spec-create or standalone via /spec.review.",
                "path": "skills/spec-review/SKILL.md",
                "frontmatter": {
                  "name": "spec-review",
                  "description": "Multi-agent spec review with parallel Claude/OpenCode reviewers. Use after spec-create or standalone via /spec.review."
                },
                "content": "# Spec Review Skill\n\nMulti-perspective spec review using parallel subagent dispatch for comprehensive validation.\n\n> **Reference:** See [reference/roles/](reference/roles/) for reviewer personas, [reference/report-format.md](reference/report-format.md) for YAML schemas, [reference/playbook.md](reference/playbook.md) for edge case handling.\n\n---\n\n## When to Use\n\n- After `spec-create` to validate before implementation\n- When spec feels incomplete or ambiguous\n- Before `task-dispatch` for Initiatives (catches gate issues early)\n- Standalone review of existing specs\n\n---\n\n## Workflow\n\n### Step 1: Identify Spec\n\n1. Parse spec name from argument (e.g., `/spec.review auth-system`)\n2. If no argument: find most recent in `./specs/draft/`\n3. If no specs in draft: check `./specs/active/`\n4. Read spec documents: `spec.md`, `context.md`, `tasks.yaml`, `validation.yaml`\n\n### Step 2: Select Reviewers\n\nUse **AskUserQuestion** with multiSelect:\n\n```\nHeader: Reviewers\nQuestion: Which reviewers should analyze this spec?\nmultiSelect: true\nOptions:\n- claude-opus: Native subagent - comprehensive, context-aware, follows project patterns\n- opencode-gpt5.2: External perspective - fresh eyes, different reasoning patterns\n```\n\nDefault: Both selected for maximum coverage.\n\n### Step 3: Dispatch Reviewers in Parallel\n\n**CRITICAL:** Dispatch both reviewers in the same message for true parallelism.\n\n**Claude Reviewer (Task tool):**\n\n```\nTask(subagent_type=\"general-purpose\", model=\"opus\", prompt=\"\"\"\nYou are reviewing a spec for completeness and feasibility.\n\n## Spec Documents\n[Include spec.md, context.md, tasks.yaml content]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Completeness** - Are all requirements specified? Missing behaviors?\n2. **Consistency** - Do documents contradict each other? Ambiguous terms?\n3. **Feasibility** - Can tasks be implemented as described? Missing dependencies?\n4. **Clarity** - Would a fresh developer understand what to build?\n\n## Output Format\nReturn a YAML report:\n\n```yaml\nreviewer_report:\n  reviewer: claude-opus\n  gates:\n    completeness:\n      status: pass | fail\n      issues: []\n    consistency:\n      status: pass | fail\n      issues: []\n    feasibility:\n      status: pass | fail\n      issues: []\n    clarity:\n      status: pass | fail\n      issues: []\n  issues:\n    - severity: critical | high | medium\n      gate: completeness\n      area: ${TAXONOMY_AREA}\n      description: \"Clear description\"\n      suggestion: \"How to fix\"\n  clarifying_questions:\n    - area: ${TAXONOMY_AREA}\n      question: \"What needs clarification?\"\n  strengths:\n    - \"Positive observation\"\n```\n\"\"\")\n```\n\n**OpenCode Reviewer (Bash tool, background):**\n\n```bash\ntimeout 300 opencode run --model openai/gpt-5.2 --print-last \"\nYou are reviewing a spec for completeness and feasibility.\n\n## Spec Documents\n[Include spec.md, context.md, tasks.yaml content]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Completeness** - Are all requirements specified? Missing behaviors?\n2. **Consistency** - Do documents contradict each other? Ambiguous terms?\n3. **Feasibility** - Can tasks be implemented as described? Missing dependencies?\n4. **Clarity** - Would a fresh developer understand what to build?\n\n## Output Format\nReturn a YAML report:\n\n\\`\\`\\`yaml\nreviewer_report:\n  reviewer: opencode-gpt5.2\n  gates:\n    completeness:\n      status: pass | fail\n      issues: []\n    consistency:\n      status: pass | fail\n      issues: []\n    feasibility:\n      status: pass | fail\n      issues: []\n    clarity:\n      status: pass | fail\n      issues: []\n  issues:\n    - severity: critical | high | medium\n      gate: completeness\n      area: \\${TAXONOMY_AREA}\n      description: \\\"Clear description\\\"\n      suggestion: \\\"How to fix\\\"\n  clarifying_questions:\n    - area: \\${TAXONOMY_AREA}\n      question: \\\"What needs clarification?\\\"\n  strengths:\n    - \\\"Positive observation\\\"\n\\`\\`\\`\n\"\n```\n\n### Step 4: Synthesize Reviews\n\nAfter both reviewers complete:\n\n1. **Parse reports** - Extract YAML from both outputs\n2. **Merge issues:**\n   - Deduplicate by description similarity\n   - Combine issues flagged by both reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if EITHER reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Prioritize questions:**\n   - Group by taxonomy area\n   - Rank: Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 5: Present Review\n\n**Gate Summary Table:**\n\n```\n| Gate         | Status | Claude | OpenCode |\n|--------------|--------|--------|----------|\n| Completeness | FAIL   | fail   | pass     |\n| Consistency  | PASS   | pass   | pass     |\n| Feasibility  | FAIL   | fail   | fail     |\n| Clarity      | PASS   | pass   | pass     |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (must fix before implementation)\n- [C1] Missing error handling for auth timeout (Completeness)\n  Found by: claude-opus, opencode-gpt5.2\n  Suggestion: Add error case to spec.md#edge-cases\n\n## High (should fix)\n- [H1] Task T003 depends on undefined API contract (Feasibility)\n  Found by: claude-opus\n  Suggestion: Define API in context.md or defer task\n\n## Medium (consider)\n- [M1] Term \"session\" used inconsistently (Consistency)\n  Found by: opencode-gpt5.2\n  Suggestion: Add to terminology section\n```\n\n### Step 6: Clarifying Questions\n\nUse **AskUserQuestion** with questions grouped by taxonomy area:\n\n```\nHeader: Scope\nQuestion: The spec mentions \"user roles\" but doesn't define them. What roles exist?\nmultiSelect: false\nOptions:\n- Admin/User: Two-tier system\n- Role-based: Granular permissions\n- Defer: Address later\n```\n\nRecord answers for validation.yaml update.\n\n### Step 7: Update Validation\n\nAdd clarification session to `validation.yaml`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    source: spec-review\n    reviewers: [claude-opus, opencode-gpt5.2]\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: ${SECTION}\n            action: modified\n```\n\nUpdate `markers` section:\n- Close resolved markers (`status: resolved`)\n- Add new markers for deferred questions (`status: open`)\n\n### Step 8: Recommend Action\n\n**All gates pass:**\n```\nReview complete. All gates passed.\n\nRecommendation: Ready for /spec.promote or /implement\n```\n\n**Issues found:**\n```\nReview complete. 2 gates failed.\n\nRecommendation:\n1. Address critical/high issues\n2. Re-run /spec.review\n```\n\n---\n\n## Gates\n\n| Gate | What It Checks |\n|------|----------------|\n| **Completeness** | All requirements specified, no missing behaviors |\n| **Consistency** | Documents align, no contradictions, terms used consistently |\n| **Feasibility** | Tasks implementable, dependencies available, no blockers |\n| **Clarity** | Unambiguous, fresh developer can understand scope |\n\n---\n\n## Edge Cases\n\n**OpenCode timeout (> 5 minutes):**\n- Continue with Claude-only results\n- Note in output: \"OpenCode review timed out, partial results\"\n- Still usable but recommend re-running with single reviewer\n\n**One reviewer fails:**\n- Parse what you can\n- Report partial results with clear indication\n- \"Claude review: complete, OpenCode review: failed to parse\"\n\n**No reviewers selected:**\n- Default to claude-opus only\n- Warn: \"Consider adding external reviewer for fresh perspective\"\n\n**Spec not found:**\n- List available specs in `./specs/draft/` and `./specs/active/`\n- Ask user to specify\n\n---\n\n## Integration\n\n**Command:** `/spec.review [spec-name]`\n\n**Related skills:**\n- `spec-create` - Creates specs to review\n- `spec-clarify` - Resolves markers found during review\n- `task-dispatch` - Next step after review passes\n\n---\n\n## Reference\n\n- [reference/roles/claude-reviewer.md](reference/roles/claude-reviewer.md) - Claude reviewer persona\n- [reference/roles/opencode-reviewer.md](reference/roles/opencode-reviewer.md) - OpenCode reviewer persona\n- [reference/report-format.md](reference/report-format.md) - YAML report schemas\n- [reference/playbook.md](reference/playbook.md) - Edge case handling"
              },
              {
                "name": "spec-update",
                "description": "Update spec documents by analyzing git history to sync task status with reality.",
                "path": "skills/spec-update/SKILL.md",
                "frontmatter": {
                  "name": "spec-update",
                  "description": "Update spec documents by analyzing git history to sync task status with reality."
                },
                "content": "# Spec Update Skill\n\nSynchronize spec documents with actual project state by analyzing git commits and working directory changes.\n\n## Command Syntax\n\n```\n/spec.update [spec-path] [--mode=status|content|full] [--context=\"user instructions\"]\n```\n\n**Arguments:**\n- `spec-path` (optional): Path to spec file. If omitted, finds most recent in `./specs/active/*/`\n- `--mode` (optional):\n  - `status` (default): Update completion status only\n  - `content`: Update spec structure based on learnings\n  - `full`: Both status and content updates\n- `--context` (optional): Manual overrides/clarifications\n\n## Core Workflow\n\n### Step 1: Locate and Parse Spec\n\n1. **Find spec file:**\n   - If path provided: use that\n   - Else: search `./specs/active/**/spec.md` for most recent\n\n2. **Parse structure:**\n   - Identify task format (checkboxes, numbered lists, sections)\n   - Extract tasks with current status\n   - Preserve formatting\n\n3. **Determine baseline:**\n   - Use file creation time or first commit mentioning spec\n\n### Step 2: Analyze Current State\n\nRun git commands to gather evidence:\n\n```bash\n# Commits since plan creation\ngit log --oneline --since=\"<plan-creation-time>\" --all\ngit log --stat --since=\"<plan-creation-time>\" --all\n\n# Current state\ngit status --short\ngit branch -vv\n\n# Files changed since baseline\ngit diff <baseline>..HEAD --name-status\n```\n\n**Collect:**\n- Commits that map to plan tasks\n- Files created/modified/deleted\n- Working directory changes\n- Branch/sync status\n\n### Step 2.5: Sync TodoWrite to tasks.yaml (if active)\n\nIf TodoWrite has entries matching spec tasks:\n\n1. For each \"completed\" todo, update corresponding task in tasks.yaml to `status: completed`\n2. For each \"in_progress\" todo, update to `status: in_progress`\n3. Update `meta.last_updated` and `meta.progress` fields\n\nThis catches any completions that weren't synced immediately during task execution.\n\n### Step 3: Map Evidence to Tasks\n\nFor each task:\n\n1. **Search for evidence:**\n   - Commit messages mentioning task\n   - Files mentioned in task were modified\n   - Tests exist if task mentions testing\n   - Dependencies met\n\n2. **Determine status:**\n   - `completed`: Clear evidence in commits + files exist\n   - `in_progress`: Working directory changes or partial completion\n   - `pending`: No evidence\n   - `blocked`: Explicit context or dependencies not met (add to task's `blocked_by` field)\n\n3. **Collect evidence notes:**\n   - Which commits\n   - Which files changed\n   - Test/build results\n\n### Step 4: Update tasks.yaml\n\n**Status mode** (`--mode=status`):\n\nUpdate task statuses and add evidence:\n\n```yaml\ntasks:\n  - id: PROJ-001\n    content: Set up project structure\n    status: completed\n    active_form: Setting up project structure\n    evidence:\n      commits: [c228fea, 2f069d7]\n      files: [src/feature_link/temporal.py, tests/test_errata_example.py]\n\n  - id: PROJ-002\n    content: Implement core logic\n    status: in_progress\n    active_form: Implementing core logic\n    evidence:\n      notes: \"3 files changed in working directory\"\n\n  - id: PROJ-003\n    content: Write integration tests\n    status: pending\n    active_form: Writing integration tests\n\nmeta:\n  last_updated: 2025-12-17\n  progress: 1/3\n```\n\n**Content mode** (`--mode=content`):\n\nAlso update structure:\n- Add new tasks via dignity's `add_task()` function\n- Update task content/active_form via `update_task()`\n- Remove obsolete tasks via `discard_task()`\n- Update phases and checkpoints as needed\n\n**User context**: If `--context` provided, apply manual overrides (takes precedence over auto-detection).\n\n### Step 5: Present Summary\n\n```\n## Spec Update Summary\n\nSpec: ./specs/active/refactor/\nTasks: tasks.yaml (progress: 5/10)\nBaseline: c228fea (2025-11-06)\n\nStatus:\n  ✓ Completed: 5 tasks\n  • In Progress: 2 tasks\n  ○ Pending: 3 tasks\n\nRecent Activity:\n  - 8 commits since plan creation\n  - 12 files modified\n\nNext actions:\n  1. REFAC-006: Implement validation (ready)\n  2. REFAC-007: Add error handling (ready)\n```\n\n---\n\n## Matching Heuristics\n\n**Strong evidence (auto-mark complete):**\n- Commit message explicitly references task\n- Commit modifies exact files mentioned\n- All acceptance criteria met\n\n**Weak evidence (mark in-progress):**\n- Commit touches related files\n- Working directory has related changes\n- Partial completion of multi-step task\n\n**Conservative approach:** When uncertain, prefer in-progress over completed\n\n## Best Practices\n\n### Preserve Structure\n- Keep all non-task content unchanged\n- Maintain indentation and formatting\n- Append evidence (don't remove existing notes)\n- Don't remove user comments\n\n### Handle tasks.yaml Structure\n- Core fields: spec, code, next_id, tasks\n- Task statuses: pending, in_progress, completed\n- Optional: phases with checkpoints\n- Optional: evidence (commits, files, notes)\n\n## Examples\n\n```bash\n# Update most recent spec status\n/spec.update\n\n# Full update with content changes\n/spec.update --mode=full\n\n# Manual override for blocker\n/spec.update --context=\"Task 5 blocked waiting for API docs\"\n\n# Specific spec\n/spec.update ./specs/active/auth/spec.md --mode=full\n```\n\n---\n\n## Integration\n\n- Created by `/spec.create`\n- Updated regularly as work progresses\n- Provides visibility into completion status\n- Informs `/spec.archive` decision\n\n**Best practice:** Run at end of each work session to keep synchronized with reality."
              },
              {
                "name": "spec-validate",
                "description": "Validation loop for speccing. Clarifies requirements through structured questioning before document creation.",
                "path": "skills/spec-validate/SKILL.md",
                "frontmatter": {
                  "name": "spec-validate",
                  "description": "Validation loop for speccing. Clarifies requirements through structured questioning before document creation."
                },
                "content": "# Spec Validate Skill\n\nValidate requirements through structured clarification. Produces validation data for spec-create.\n\n> **Reference:** See [reference/issue-types.md](reference/issue-types.md) for type definitions, [reference/question-taxonomy.md](reference/question-taxonomy.md) for question templates, and [reference/sdd-gates.md](reference/sdd-gates.md) for pre-implementation gate definitions.\n\n---\n\n## When to Use\n\n**Use for:**\n- New features before implementation\n- Unclear requirements needing refinement\n- Complex changes needing design exploration\n- When multiple approaches seem viable\n\n**Don't use for:**\n- Simple bug fixes\n- Well-specified changes\n- Clear mechanical tasks (use Claude's native planning)\n\n---\n\n## Workflow\n\n### Step 0: Check Native Plan Context\n\nBefore starting validation, check if the user has existing context from Claude's native `/plan`:\n\n1. **Check for context:** Ask if `/plan` was used or if there's existing plan context\n2. **If present:** Extract key elements:\n   - Goal/objective → Seeds **Scope** taxonomy area\n   - Approach/strategy → Seeds **Integration/Architecture** areas\n   - Open questions → Become priority clarification targets\n3. **If absent:** Proceed directly to Step 1\n\nThis step bridges native Claude planning with structured validation.\n\n### Step 0.5: Constitution Check (Initiatives Only)\n\nFor Initiative-type work, verify alignment with constitution before proceeding:\n\n1. Read `.claude/constitution.md`\n2. Check if proposed work aligns with core principles\n3. If conflict detected:\n   - Flag the conflict\n   - Ask user to resolve or justify exception\n   - Document exception in validation data\n\n**Skip for:** Features, Tasks (constitution checked during task-dispatch for Initiatives)\n\n### Step 1: Issue Type Selection\n\n**FIRST QUESTION (Always)** - Use AskUserQuestion:\n\n```\nHeader: Work type\nQuestion: What type of work is this?\nmultiSelect: false\nOptions:\n- Initiative: Strategic coordination (months) - Multiple features toward business goal\n- Feature: User-facing capability (weeks) - Deliverable value, multiple tasks\n- Task: Implementation item (days) - Single concrete deliverable\n- Exploratory: Not sure yet - Gather context first, then classify\n```\n\nThis selection determines:\n- **Question limit:** Tasks get 3, Features/Initiatives get 5\n- **Taxonomy areas:** Tasks get minimal (3), Features/Initiatives get full (7)\n\n**If Exploratory:** Gather context, ask 3 questions to understand scope, present classification recommendation, restart with correct type.\n\n### Step 2: Gather Context\n\n1. Examine relevant files, docs, recent commits\n2. Understand existing patterns and constraints\n3. Initialize taxonomy tracking based on issue type\n\n**Taxonomy by type:**\n\n| Type | Areas to Cover |\n|------|----------------|\n| Initiative | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Feature | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Task | Scope, Behavior, Integration |\n\n### Step 2.5: Ambiguity Scan\n\nAutomatically scan gathered context for specification gaps across taxonomy areas.\n\n**Process:**\n\n1. For each taxonomy area (based on issue type), evaluate:\n   - **clear**: Fully specified, no questions needed\n   - **partial**: Some information present, gaps remain\n   - **missing**: Not addressed at all\n\n2. Populate `ambiguity_scan` section in validation data:\n   ```yaml\n   ambiguity_scan:\n     scope:\n       status: clear | partial | missing\n       gaps: [\"gap description if partial/missing\"]\n     behavior:\n       status: clear | partial | missing\n       gaps: []\n     # ... remaining areas\n   ```\n\n3. Route based on scan results:\n   - **No gaps (all clear):** Proceed silently to Step 4 (skip validation loop)\n   - **Gaps found:** Areas with `partial` or `missing` status become priority candidates for clarification questions in Step 3\n\n**Evaluation criteria per area:**\n\n| Area | Clear | Partial | Missing |\n|------|-------|---------|---------|\n| Scope | Goals, boundaries, success criteria defined | Some elements unclear | No scope information |\n| Behavior | User flows, system responses specified | Some paths undefined | No behavior described |\n| Data Model | Entities, relationships, formats clear | Schema gaps exist | No data model |\n| Constraints | Performance, security, compatibility stated | Some constraints unclear | No constraints |\n| Edge Cases | Error handling, limits documented | Some cases unaddressed | No edge cases |\n| Integration | Dependencies, APIs, interfaces identified | Some touchpoints unclear | No integration info |\n| Terminology | Domain terms defined consistently | Some ambiguous terms | No definitions |\n\n### Step 3: Validation Loop\n\nAsk clarifying questions in taxonomy-based batches, with re-evaluation between rounds.\n\n**Process:**\n1. Identify uncovered taxonomy areas\n2. Prioritize areas by (Impact × Uncertainty)\n3. Group questions by taxonomy area into batches\n4. Use AskUserQuestion with multiple questions from the same area\n5. Receive answers\n6. Re-evaluate remaining questions for relevance (skip questions invalidated by answers)\n7. Update taxonomy coverage\n8. Repeat with next taxonomy area until limit reached or all Primary areas covered\n\n**Batch format:**\n\n```\nAskUserQuestion with questions array (1-4 questions per batch):\n\nQuestion 1:\n  Header: [Area, max 12 chars]\n  Question: [Clear question ending with ?]\n  multiSelect: false\n  Options: [2-4 options with implications]\n\nQuestion 2:\n  Header: [Same area]\n  Question: [Related question]\n  ...\n```\n\n**Single question format (when only one question in area):**\n\n```\nHeader: [Area, max 12 chars]\nQuestion: [Clear question ending with ?]\nmultiSelect: false\nOptions:\n- Option A: [choice] - [implication]\n- Option B: [choice] - [implication]\n- Option C: [choice] - [implication]\n- None: [default/skip]\n```\n\n**Batching rules:**\n- One batch per taxonomy area (questions grouped by area)\n- Multiple trigger fields within a batch combine with OR (any match)\n\n**Batch counts:**\n- Tasks: up to 3 batches (Scope, Behavior, Integration)\n- Features/Initiatives: up to 7 batches (all taxonomy areas)\n- Plus: additional clarification batches if \"Other\" answers need follow-up\n\n**Batch size limits:**\n- Tasks: up to 3 questions per batch\n- Features/Initiatives: up to 4 questions per batch (AskUserQuestion max)\n\n**Re-evaluation between batches:**\n- After receiving answers, check if pending questions are still relevant\n- Skip questions invalidated by previous answers\n- If user provides ambiguous \"Other\" answer spanning areas, add follow-up to next batch\n\n### Step 3.5: Initiative-Specific Validation (Initiatives Only)\n\nFor Initiatives, ask about user story prioritization and implementation strategy:\n\n```\nHeader: User Stories\nQuestion: How should user stories be prioritized?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): P1 stories deliver standalone value, P2/P3 are incremental\n- Parallel Tracks: Stories can be developed independently by different teams\n- Sequential: Stories have strict dependencies, must complete in order\n```\n\n```\nHeader: Strategy\nQuestion: What implementation approach fits best?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): Ship P1, iterate on P2/P3 based on feedback\n- Incremental: Each phase adds value, all planned upfront\n- Parallel Team: Multiple workstreams, integration points defined\n```\n\nTrack selections in validation data for spec-create (populates Implementation Strategy section).\n\n### Step 3.6: SDD Section Opt-ins (Features Only)\n\nFor Features, offer opt-in for detailed SDD sections:\n\n```\nHeader: SDD sections\nQuestion: Which detailed sections do you want in the spec?\nmultiSelect: true\nOptions:\n- Tech Decisions: Document technology choices and rationale\n- API Contract: Define API endpoints and schemas\n- Data Model: Document entities and relationships\n- None: Keep spec lightweight\n```\n\nTrack selections in validation data for spec-create.\n\n### Step 4: Propose Approaches\n\nUse AskUserQuestion to present options:\n\n```\nHeader: Approach\nQuestion: Which approach should we take?\nmultiSelect: false\nOptions:\n- Approach A: [brief] - Trade-off: [X]\n- Approach B: [brief] - Trade-off: [Y] (Recommended)\n- Approach C: [brief] - Trade-off: [Z]\n```\n\nLead with your recommendation. Apply YAGNI ruthlessly.\n\n### Step 5: Generate Validation Data\n\nCompile validation data for spec-create:\n\n- **Issue type:** Selected in Step 1\n- **Ambiguity scan:** Status per area (clear / partial / missing) with gap descriptions\n- **Questions used:** N / limit\n- **Taxonomy coverage:** Status per area (Covered / Gap / N/A)\n- **Clarification log:** Question → Answer → Integration point\n- **Approach selected:** From Step 4\n- **SDD opt-ins:** Which sections selected (Features only)\n- **Gates status:** For Initiatives: run gate checks; for Features/Tasks: mark n/a\n- **Markers:** Any unresolved items identified during validation\n\nThis data passes to spec-create for validation.yaml.\n\n---\n\n## Key Principles\n\n| Principle | Why |\n|-----------|-----|\n| **Issue type first** | Branches workflow, sets question limit |\n| **Ambiguity scan** | Identifies gaps early, skips validation if all clear |\n| **MultiSelect always** | Structured options, faster iteration |\n| **Question limits** | Forces prioritization |\n| **Taxonomy tracking** | Ensures coverage of important areas |\n| **YAGNI ruthlessly** | Remove unnecessary features |\n\n---\n\n## Output\n\nThis skill produces **validation data**, not documents. The data flows to `spec-create` which generates:\n- spec.md, context.md, tasks.md, dependencies.yaml, validation.yaml\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command (default behavior)\n\n**Standalone use:** Can be invoked directly via `spec-validate` skill for validation without document creation."
              },
              {
                "name": "task-completion-verify",
                "description": "Evidence-based completion claims. Use before claiming work is complete, fixed, or passing - requires running verification commands and confirming output before any success claims.",
                "path": "skills/task-completion-verify/SKILL.md",
                "frontmatter": {
                  "name": "task-completion-verify",
                  "description": "Evidence-based completion claims. Use before claiming work is complete, fixed, or passing - requires running verification commands and confirming output before any success claims."
                },
                "content": "# Verification Before Completion\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n---\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n---\n\n## The Gate Function\n\n```\nBEFORE claiming any status:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n---\n\n## Common Verification Requirements\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check |\n| Build succeeds | Build command: exit 0 | Linter passing |\n| Bug fixed | Test symptom: passes | Code changed |\n| Regression test | Red-green verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n---\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- ANY wording implying success without running verification\n\n---\n\n## Key Patterns\n\n**Tests:**\n```\nDO:   [Run test] [See: 34/34 pass] \"All tests pass\"\nDON'T: \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\nDO:   Write -> Run (pass) -> Revert fix -> Run (MUST FAIL) -> Restore -> Run (pass)\nDON'T: \"I've written a regression test\" (without red-green)\n```\n\n**Build:**\n```\nDO:   [Run build] [See: exit 0] \"Build passes\"\nDON'T: \"Linter passed\" (linter != compiler)\n```\n\n**Requirements:**\n```\nDO:   Re-read plan -> Create checklist -> Verify each -> Report\nDON'T: \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\nDO:   Agent reports -> Check VCS diff -> Verify changes -> Report\nDON'T: Trust agent report\n```\n\n---\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence != evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter != compiler |\n| \"Agent said success\" | Verify independently |\n| \"Partial check is enough\" | Partial proves nothing |\n\n---\n\n## When to Apply\n\n**ALWAYS before:**\n- ANY success/completion claims\n- ANY expression of satisfaction\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Run tests before claiming they pass\n- `code-debug` - Verify fix before claiming bug resolved\n- `task-dispatch` - Verify each task before marking complete\n- `git-worktree-use` - Verify baseline before and after"
              },
              {
                "name": "task-dispatch",
                "description": "Subagent-driven task execution with TDD workflow. Dispatches tester subagent (writes failing tests) then implementer subagent (makes tests pass), with batch review.",
                "path": "skills/task-dispatch/SKILL.md",
                "frontmatter": {
                  "name": "task-dispatch",
                  "description": "Subagent-driven task execution with TDD workflow. Dispatches tester subagent (writes failing tests) then implementer subagent (makes tests pass), with batch review."
                },
                "content": "# Subagent-Driven Task Execution\n\nExecute specs with proper TDD: tester writes failing tests, implementer makes them pass.\n\n**Core principle:** Separate test-writing from implementation. Fresh subagents + opus model + skill activation = high quality.\n\n---\n\n## When to Use\n\n**Use when:**\n- Executing an implementation spec (created with `spec-create`)\n- Tasks are mostly independent\n- Want TDD enforcement with quality gates\n\n**Don't use when:**\n- No spec exists yet (use `spec-validate` → `spec-create` first)\n- Tasks are tightly coupled (manual execution better)\n- Single small task (just do it directly)\n- Initiative spec has failed gates (resolve first via /spec.clarify)\n\n---\n\n## The Process\n\n### 1. Load Spec and Populate TodoWrite\n\n1. Find most recent spec in `./specs/active/*/`\n2. Read `tasks.yaml` from that directory\n3. Parse tasks with `status: pending` or `status: in_progress`\n4. Create TodoWrite with ALL uncompleted tasks:\n   - First uncompleted task: \"in_progress\"\n   - Others: \"pending\"\n   - content: task text\n   - activeForm: present continuous form\n\n**CRITICAL:** Always populate TodoWrite before dispatching any subagents.\n\n### 1.5. Pre-Implementation Gate Check\n\nBefore dispatching any tasks, verify validation.yaml gates:\n\n1. Read `validation.yaml` from spec directory\n2. Check `metadata.issue_type`\n3. **If Initiative:**\n   - Check all gates in `gates` section\n   - If any gate has `status: failed`:\n     - Report which gates failed with reasons\n     - Prompt: \"Resolve via /spec.clarify or proceed anyway?\"\n     - If user chooses to proceed: document override in validation.yaml\n   - Check `markers` section for `status: open`\n   - If blocking markers exist:\n     - Report marker count and summaries\n     - Prompt: \"Resolve markers first or proceed?\"\n4. **If Feature/Task:** Skip gate check (gates marked n/a)\n\n**Gate check failure response:**\n\n```\nPre-implementation gate check failed:\n\nGates:\n- ❌ Simplicity: [reason from validation.yaml]\n- ✓ Anti-Abstraction: passed\n- ❌ Integration-First: [reason]\n\nOpen Markers: 3\n- M001 (Constraints): Authentication method not specified\n- M002 (Edge Cases): Error handling for timeout\n- M003 (Integration): External API contract undefined\n\nOptions:\n1. Run /spec.clarify to resolve\n2. Proceed anyway (document override)\n3. Abort\n```\n\n### 2. Analyze Task Dependencies\n\nParse `dependencies.yaml` to identify execution batches:\n\n**Dependency rules:**\n- Tasks in Phase N depend on Phase N-1 completion\n- Tasks with `[P]` marker AND different file paths can run in parallel\n- Tasks with same file path must run sequentially\n- Phase boundaries force batch breaks\n\n### 3. Execute Tasks (Two-Phase TDD)\n\n**Per task, dispatch TWO subagents:**\n\n```\nPhase A: TESTER (opus)\n├── Invokes code-test skill\n├── Writes failing tests (RED)\n└── Reports: test paths, failure output\n\nPhase B: IMPLEMENTER (opus)\n├── Receives test paths from tester\n├── Invokes code-implement skill\n├── Makes tests pass (GREEN)\n└── Reports: impl files, test pass output\n```\n\n**Key constraint:** Implementer for task X needs tester X's report. But testers for different tasks are independent.\n\n**For single task:**\n1. Dispatch tester subagent (opus) → wait for completion\n2. Dispatch implementer subagent (opus) with tester's report → wait for completion\n\n**For parallel batch (N independent tasks):**\n1. Dispatch N tester subagents in single message → wait for ALL testers\n2. Dispatch N implementer subagents in single message → wait for ALL implementers\n\nEach implementer receives its corresponding tester's report. This maximizes parallelism:\n- All testers run concurrently (different test files)\n- All implementers run concurrently (different impl files)\n\n> **Reference:** See [reference/subagent-workflow.md](reference/subagent-workflow.md) for dispatch templates.\n\n### 4. Handle Tester Gaps\n\nIf tester reports `status: gap` (cannot write meaningful tests):\n1. Read the gap_reason from tester's YAML report\n2. Consult the spec for clarification\n3. If still unclear, use AskUserQuestion\n4. Re-dispatch tester with clarified requirements\n\n### 5. Review Batch Work\n\nAfter ALL implementers in a batch complete, dispatch single reviewer:\n\n- Reviews all changes from the batch together\n- Checks against spec requirements\n- Identifies issues by severity:\n  - **Critical** - Blocks progress, must fix immediately\n  - **Important** - Fix before next batch\n  - **Minor** - Note for later\n\n### 6. Apply Review Feedback\n\n**If issues found:**\n- Fix Critical issues immediately (dispatch fix subagent)\n- Fix Important issues before next batch\n- Note Minor issues\n\n### 7. Mark Complete and Sync to tasks.yaml\n\nWhen marking a task complete:\n\n1. Update TodoWrite (mark as \"completed\")\n2. Edit tasks.yaml: Change `status: in_progress` to `status: completed`\n3. Move to next task (mark as \"in_progress\")\n\n### 8. Final Review\n\nAfter all tasks complete:\n- Review entire implementation\n- Check all spec requirements met\n- Validate overall architecture\n\n---\n\n## Subagent Configuration\n\n| Role | Subagent Type | Model | Skill |\n|------|---------------|-------|-------|\n| Tester | task-tester | opus | code-test |\n| Implementer | task-implementer | opus | code-implement |\n| Reviewer | task-reviewer | opus | code-review |\n\n**CRITICAL:** Always specify `model: opus` in Task tool calls.\n\n---\n\n## Quality Gates\n\n| Gate | When | Action if Failed |\n|------|------|------------------|\n| Pre-impl gate | Before any dispatch | Block if Initiative gates failed |\n| RED verification | After tester | Verify tests actually fail |\n| GREEN verification | After implementer | Verify tests pass |\n| Batch review | After all implementers | Fix before next batch |\n| Final review | After all tasks | Address gaps |\n\n---\n\n## Red Flags\n\n**Never:**\n- Skip the tester phase (implementer must receive failing tests)\n- Use sonnet for subagents (always opus)\n- Skip review between batches\n- Dispatch parallel subagents on same file\n- Let implementer write tests (tester's job)\n- Ignore failed pre-impl gates for Initiatives (gates exist for a reason)\n\n**If tester can't write tests:**\n- Don't skip to implementer\n- Handle the gap (consult spec, ask user)\n- Re-dispatch tester with clarification\n\n---\n\n## Example Workflow\n\n```\n[Load spec, create TodoWrite]\n\nTask 1: Add caching\n[Dispatch tester (opus)]\nTester: Wrote 3 tests, all failing (RED)\n  - test_cache_hit, test_cache_miss, test_ttl_expiry\n  - Files: tests/test_cache.py\n[Dispatch implementer (opus) with test paths]\nImplementer: Made tests pass (GREEN)\n  - Files: src/cache.py\n[Review - no issues]\n[Mark Task 1 complete]\n\nTask 2, 3, 4: [P] parallel batch\n[Dispatch 3 testers in single message]\nAll testers complete with failing tests\n[Dispatch 3 implementers in single message]\nAll implementers complete, tests passing\n[Single review for entire batch]\n[Mark Tasks 2, 3, 4 complete]\n\n...\n\n[Final review]\nAll requirements met\n```\n\n---\n\n## Integration\n\n**Use with:**\n- `spec-validate` → `spec-create` - Create spec before dispatch\n- `spec-clarify` - Resolve markers/gates before dispatch\n- `code-test` - Tester invokes for TDD methodology\n- `code-implement` - Implementer invokes for language guidelines\n- `code-review` - Reviewer invokes for review methodology\n- `task-completion-verify` - Verify before claiming done\n\n---\n\n## Reference\n\n- [subagent-workflow.md](reference/subagent-workflow.md) - Dispatch templates and YAML reports\n- [report-format.md](reference/report-format.md) - YAML report schemas\n- [roles/tester.md](reference/roles/tester.md) - Test-writing subagent\n- [roles/implementer.md](reference/roles/implementer.md) - Implementation subagent\n- [roles/reviewer.md](reference/roles/reviewer.md) - Review subagent"
              }
            ]
          },
          {
            "name": "commands",
            "description": "Slash commands for specs, reviews, implementation",
            "source": "./commands",
            "category": "commands",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add srnnkls/tropos",
              "/plugin install commands@tropos"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T17:40:51Z",
              "created_at": "2026-01-12T07:48:16Z",
              "license": null
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "agents",
            "description": "Task agents: implementer, reviewer, tester",
            "source": "./agents",
            "category": "agents",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add srnnkls/tropos",
              "/plugin install agents@tropos"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T17:40:51Z",
              "created_at": "2026-01-12T07:48:16Z",
              "license": null
            },
            "commands": [],
            "skills": []
          },
          {
            "name": "loqui",
            "description": "Additional skills, rules, commands",
            "source": {
              "source": "github",
              "repo": "srnnkls/loqui"
            },
            "category": "external",
            "version": "1.0.0",
            "author": null,
            "install_commands": [
              "/plugin marketplace add srnnkls/tropos",
              "/plugin install loqui@tropos"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-12T17:40:51Z",
              "created_at": "2026-01-12T07:48:16Z",
              "license": null
            },
            "commands": [],
            "skills": [
              {
                "name": "code-debug",
                "description": "Systematic debugging with root cause tracing. Use when encountering bugs, test failures, or unexpected behavior - find root cause before attempting fixes, trace backward through call chain.",
                "path": "skills/code-debug/SKILL.md",
                "frontmatter": {
                  "name": "code-debug",
                  "description": "Systematic debugging with root cause tracing. Use when encountering bugs, test failures, or unexpected behavior - find root cause before attempting fixes, trace backward through call chain."
                },
                "content": "# Systematic Debugging\n\nRandom fixes waste time and create new bugs.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n---\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n---\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n\n**Use ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- You don't fully understand the issue\n\n---\n\n## The Four Phases\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - If not reproducible, gather more data\n\n3. **Check Recent Changes**\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Trace Data Flow Backward**\n   - Where does the bad value originate?\n   - What called this with the bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n5. **Multi-Component Systems**\n   Add diagnostic instrumentation at each boundary:\n   - Log what enters/exits each component\n   - Verify environment/config propagation\n   - Run once to gather evidence WHERE it breaks\n\n### Phase 2: Pattern Analysis\n\n1. **Find Working Examples** - Similar working code in same codebase\n2. **Compare Against References** - Read reference implementations completely\n3. **Identify Differences** - List every difference, however small\n4. **Understand Dependencies** - Settings, config, environment, assumptions\n\n### Phase 3: Hypothesis and Testing\n\n1. **Form Single Hypothesis** - \"I think X is the root cause because Y\"\n2. **Test Minimally** - Smallest possible change, one variable at a time\n3. **Verify Before Continuing** - Worked? Phase 4. Didn't work? New hypothesis.\n4. **When You Don't Know** - Say so. Ask for help. Research more.\n\n### Phase 4: Implementation\n\n1. **Create Failing Test Case** - Simplest possible reproduction\n2. **Implement Single Fix** - ONE change at a time, no bundled improvements\n3. **Verify Fix** - Test passes? No regressions?\n\n**If fix doesn't work:**\n- Count: How many fixes have you tried?\n- If < 3: Return to Phase 1 with new information\n- If >= 3: STOP and question the architecture\n\n### When 3+ Fixes Fail\n\nPattern indicating architectural problem:\n- Each fix reveals new shared state/coupling\n- Fixes require \"massive refactoring\"\n- Each fix creates new symptoms elsewhere\n\n**STOP and question fundamentals:**\n- Is this pattern fundamentally sound?\n- Should we refactor architecture vs. continue fixing symptoms?\n- Discuss with user before attempting more fixes\n\n---\n\n## Root Cause Tracing\n\nWhen bugs manifest deep in the call stack:\n\n1. **Observe the Symptom** - What error occurred?\n2. **Find Immediate Cause** - What code directly causes this?\n3. **Ask: What Called This?** - Trace up the call chain\n4. **Keep Tracing Up** - What value was passed? Where did it come from?\n5. **Find Original Trigger** - The source, not the symptom\n\n**Adding Stack Traces:**\n```\nstack = capture_stack_trace()\nlog(\"DEBUG operation:\", {\n  input_value,\n  current_directory,\n  environment,\n  stack\n})\n```\n\n**NEVER fix just where the error appears.** Trace back to find the original trigger.\n\n---\n\n## Red Flags - STOP and Follow Process\n\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"I don't fully understand but this might work\"\n- Proposing solutions before tracing data flow\n- \"One more fix attempt\" (when already tried 2+)\n\n**ALL of these mean:** STOP. Return to Phase 1.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple\" | Simple issues have root causes too. |\n| \"Emergency, no time\" | Systematic is FASTER than thrashing. |\n| \"Just try this first\" | First fix sets the pattern. Do it right. |\n| \"I see the problem\" | Seeing symptoms != understanding root cause. |\n| \"One more fix attempt\" | 3+ failures = architectural problem. |\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Write failing test to reproduce bug before fixing\n- `completion-verify` - Verify fix actually worked before claiming done\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Detailed tracing techniques"
              },
              {
                "name": "code-implement",
                "description": "Language-specific coding guidelines. Use when implementing code in Python or other supported languages.",
                "path": "skills/code-implement/SKILL.md",
                "frontmatter": {
                  "name": "code-implement",
                  "description": "Language-specific coding guidelines. Use when implementing code in Python or other supported languages."
                },
                "content": "# Code Implement Skill\n\nLanguage-specific patterns, anti-patterns, and best practices for writing code.\n\n---\n\n## When to Use\n\n- Writing code in supported languages\n- Deciding on code structure, patterns, or style\n- Designing domain models or data structures\n- Organizing code into modules\n\n**IMPORTANT - Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill instead (it will invoke this skill per task with quality gates)\n- **Single implementation task or asking about patterns?** → Use this skill directly\n\n---\n\n## Supported Languages\n\nLanguage-specific guidelines are in `~/.claude/skills/code-implement/resources/loqui/languages/{language}/`.\n\n**Use Read tool** (not Glob) to access resources - paths outside cwd require direct reads.\n\nEach language directory follows this structure:\n\n```\n~/.claude/skills/code-implement/resources/loqui/languages/{language}/\n├── README.md              # Overview, core principles, anti-patterns checklist\n├── quality.md             # Naming, comments, documentation conventions\n├── composition.md         # Structuring behavior (classes/functions/modules)\n├── modules.md             # Package structure, organization, public APIs\n├── errors.md              # Error handling patterns and practices\n└── ...                    # Additional language-specific resources as needed\n```\n\n**Start with the language README** for quick reference and core principles, then dive into specific topic files as needed.\n\n---\n\n## Related Skills\n\n- **task-dispatch**: Use for multiple independent implementation tasks (invokes this skill per task)\n- **code-test**: Use for TDD workflow (write test first, then implement)\n- **code-review**: Review methodology (delegates here for language specifics)\n- **pr-review**: GitHub PR workflow (delegates to code-review)\n\n---\n\n## Reference\n\n- [defense-in-depth.md](reference/defense-in-depth.md) - Multi-layer validation patterns\n- [root-cause-tracing.md](reference/root-cause-tracing.md) - Tracing bugs to their source"
              },
              {
                "name": "code-review-receive",
                "description": "Technical evaluation of code review feedback. Use when receiving review feedback - requires verification before implementing, no performative agreement, push back when technically wrong.",
                "path": "skills/code-review-receive/SKILL.md",
                "frontmatter": {
                  "name": "code-review-receive",
                  "description": "Technical evaluation of code review feedback. Use when receiving review feedback - requires verification before implementing, no performative agreement, push back when technically wrong."
                },
                "content": "# Code Review Reception\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n---\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n---\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\"\n- \"Great point!\" / \"Excellent feedback!\"\n- \"Let me implement that now\" (before verification)\n- Any performative agreement\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n---\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nFeedback: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\nWRONG: Implement 1,2,3,6 now, ask about 4,5 later\nRIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5.\"\n```\n\n---\n\n## YAGNI Check\n\nWhen reviewer suggests adding features:\n\n```\nIF reviewer suggests \"implementing properly\":\n  Search codebase for actual usage\n\n  IF unused: \"This isn't called anywhere. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n---\n\n## When to Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with prior architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Escalate if architectural\n\n---\n\n## Implementation Order\n\nFor multi-item feedback:\n\n1. Clarify anything unclear FIRST\n2. Then implement in order:\n   - Blocking issues (breaks, security)\n   - Simple fixes (typos, imports)\n   - Complex fixes (refactoring, logic)\n3. Test each fix individually\n4. Verify no regressions\n\n---\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n\n```\nDO:    \"Fixed. [Brief description of what changed]\"\nDO:    \"Good catch - [specific issue]. Fixed in [location].\"\nDO:    [Just fix it and show in the code]\n\nDON'T: \"You're absolutely right!\"\nDON'T: \"Thanks for catching that!\"\nDON'T: ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it.\n\n---\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n\n```\nDO:    \"Verified and you're correct. My understanding was wrong. Fixing.\"\nDON'T: Long apology or defending why you pushed back\n```\n\nState the correction factually and move on.\n\n---\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n\n---\n\n## Integration\n\n**Use with:**\n- `task-dispatch` - Handle review between tasks\n- `code-test` - Test each fix individually\n- `completion-verify` - Verify fixes actually work"
              },
              {
                "name": "code-review",
                "description": "Code review methodology. Use when reviewing code locally or preparing for a PR review.",
                "path": "skills/code-review/SKILL.md",
                "frontmatter": {
                  "name": "code-review",
                  "description": "Code review methodology. Use when reviewing code locally or preparing for a PR review."
                },
                "content": "# Code Review Skill\n\nGeneric code review process and methodology. Language-agnostic review guidelines.\n\n---\n\n## When to Use\n\n- Reviewing code changes locally\n- Preparing review feedback before a PR\n- Understanding what to look for in code\n- Categorizing and prioritizing issues\n\n---\n\n## Review Process\n\n### Step 1: Understand Context\n\nBefore reviewing code, understand:\n- What problem is being solved?\n- What are the requirements or acceptance criteria?\n- Are there related issues or prior discussions?\n\n### Step 2: Detect Language and Load Guidelines\n\nIdentify the primary language and load appropriate guidelines from the `code-implement` skill\n\n### Step 3: Review by Category\n\nReview code across these focus areas:\n\n| Category | What to Check |\n|----------|---------------|\n| **Correctness** | Logic errors, edge cases, error handling, type safety |\n| **Style** | Naming, formatting, code organization, idioms |\n| **Performance** | Efficiency, data structures, unnecessary work |\n| **Security** | Input validation, secrets, injection risks |\n| **Architecture** | Design patterns, coupling, separation of concerns |\n\n### Step 4: Categorize by Severity\n\nAssign severity to each issue:\n\n| Severity | Definition | Action |\n|----------|------------|--------|\n| **Critical** | Blocks merge - bugs, security issues, data corruption | Must fix |\n| **High** | Should fix - significant issues, unclear behavior | Fix before merge |\n| **Medium** | Nice to fix - style, minor improvements | Can merge, follow-up |\n\n### Step 5: Provide Actionable Feedback\n\nFor each issue:\n1. Identify the specific location\n2. Explain what's wrong (reference guidelines if applicable)\n3. Suggest a concrete fix or alternative\n\n---\n\n## Review Checklist\n\nSee [resources/checklist.md](resources/checklist.md) for a generic review checklist.\n\n---\n\n## Language-Specific Guidelines\n\nFor language-specific patterns and anti-patterns, delegate to:\n\n- **code-implement**: Language guidelines and implementation patterns\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific coding guidelines\n- **pr-review**: GitHub PR workflow (delegates here for methodology)"
              },
              {
                "name": "code-test",
                "description": "Enforce test-driven development (RED-GREEN-REFACTOR). Use when implementing features, fixing bugs, or changing behavior - write failing test first, then minimal code to pass.",
                "path": "skills/code-test/SKILL.md",
                "frontmatter": {
                  "name": "code-test",
                  "description": "Enforce test-driven development (RED-GREEN-REFACTOR). Use when implementing features, fixing bugs, or changing behavior - write failing test first, then minimal code to pass."
                },
                "content": "# Test-Driven Development\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n---\n\n## When to Use\n\n**Always use for:**\n- New features\n- Bug fixes\n- Behavior changes\n- Refactoring\n\n**Exceptions (confirm with user):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\n**Workflow Integration:**\n- **Multiple independent tasks from a plan?** → Use `task-dispatch` skill (it enforces TDD per task with quality gates)\n- **Single implementation task?** → Use this skill directly for TDD workflow\n\n---\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrote code before the test? Delete it. Start over. No exceptions.\n\n---\n\n## Red-Green-Refactor Cycle\n\n### 1. RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n**Requirements:**\n- One behavior per test\n- Clear name describing behavior\n- Real code (avoid mocks unless unavoidable)\n\n### 2. Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\nRun the test. Confirm:\n- Test fails (not errors from typos)\n- Failure message matches expectation\n- Fails because feature is missing\n\n**Test passes immediately?** You're testing existing behavior. Fix the test.\n\n### 3. GREEN - Minimal Code\n\nWrite the simplest code to pass the test.\n\n**DO:** Just enough to pass, simple implementation\n**DON'T:** Add features, refactor other code, add configurability\n\n### 4. Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\nRun the test. Confirm:\n- Test passes\n- Other tests still pass\n- No errors or warnings\n\n### 5. REFACTOR - Clean Up\n\nOnly after green:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### 6. Repeat\n\nNext failing test for next behavior.\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Already manually tested\" | Ad-hoc is not systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Unverified code is debt. |\n| \"Need to explore first\" | Fine. Throw away exploration, then TDD. |\n| \"Test hard to write\" | Hard to test = hard to use. Simplify design. |\n\n---\n\n## Red Flags - Stop and Start Over\n\n- Code written before test\n- Test passes immediately\n- Can't explain why test failed\n- \"Just this once\" rationalization\n- Keeping code \"as reference\"\n\n**All of these mean:** Delete code. Start with TDD.\n\n---\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] No errors or warnings in output\n\n---\n\n## TDD Evidence Format (For Subagent Verification)\n\nWhen implementing as a subagent, you MUST output this evidence block:\n\n```yaml\ntdd_evidence:\n  tests_written:\n    - name: \"test_feature_x\"\n      file: \"tests/test_x.py\"\n      red_output: \"FAILED - [actual failure message]\"\n      green_output: \"PASSED - 1 passed in 0.05s\"\n  implementation_files:\n    - path: \"src/feature.py\"\n  all_tests_pass: true\n  test_command: \"pytest tests/test_x.py -v\"\n  final_output: \"[full test output]\"\n```\n\n**This is REQUIRED for SubagentStop hook verification.**\n\n---\n\n## Integration\n\n**Use with:**\n- `code-debug` - Write failing test to reproduce bug before fixing\n- `task-dispatch` - Subagents follow TDD for each task\n- `completion-verify` - Run tests before claiming completion"
              },
              {
                "name": "debate-start",
                "description": "Start structured red vs. blue team debates via subagents. Use when exploring a topic from multiple adversarial perspectives.",
                "path": "skills/debate-start/SKILL.md",
                "frontmatter": {
                  "name": "debate-start",
                  "description": "Start structured red vs. blue team debates via subagents. Use when exploring a topic from multiple adversarial perspectives."
                },
                "content": "# Start Debate Skill\n\nOrchestrate multi-perspective debates on a topic using color-coded team subagents.\n\n> **Reference**: See [reference.md](reference.md) for moderation guidelines and intervention patterns.\n\n---\n\n## When to Use\n\n- Exploring trade-offs in architectural decisions\n- Evaluating competing approaches or technologies\n- Risk analysis requiring devil's advocate perspectives\n- Any topic benefiting from structured adversarial review\n\n---\n\n## Workflow\n\n### Step 1: Initialize Debate\n\n1. Parse topic from user input\n2. Create slugs from topic and context (e.g., \"API Design\" → `api-design`, context max 3 words)\n3. Ensure `./debates/` directory exists\n4. Create scratchpad from template: `./debates/{topic}_{context}.md`\n\n### Step 2: Configure Teams\n\nUse **AskUserQuestion** to gather team configuration:\n\n**Question 1: Optional Teams (multiSelect: true)**\n```\nWhich additional teams should participate beyond Red and Blue?\n- None: Just Red and Blue\n- Green Team: Pragmatic/implementation focus\n- Yellow Team: Risk/safety analysis\n- Purple Team: Synthesis/integration bridge\n```\n\n**Question 2: Red Team Stance**\n```\nWhat position should Red Team (challenger/skeptic) argue?\n```\n\n**Question 3: Blue Team Stance**\n```\nWhat position should Blue Team (defender/advocate) argue?\n```\n\n**Questions 4-6: Additional team stances** (if selected)\n\nWrite all stances to the scratchpad's Team Positions section.\n\n### Step 3: Spawn Opening Arguments (Parallel)\n\nLaunch all team subagents **simultaneously** using the Task tool:\n\n```\nTask(subagent_type=\"general-purpose\", prompt=\"\"\"\nYou are the {COLOR} TEAM in a debate on: {topic}\n\nYour stance: {stance}\n\n## Research Phase\nGather evidence before writing using read-only tools.\n\n**Codebase research:**\n- Glob/Grep/Read: Find relevant code, patterns, prior decisions\n\n**External research (encouraged):**\n- WebSearch: Find industry practices, benchmarks, expert opinions, case studies\n- WebFetch: Retrieve specific documentation, articles, or technical references\n\nFor deep research questions, spawn focused subagents:\n  Task(subagent_type=\"general-purpose\", prompt=\"Research {specific question}...\")\n\n## Writing Phase\n1. Read ./debates/{topic}_{context}.md\n2. Edit your section: ### [{COLOR}]\n3. Structure: Position → Evidence → Implications\n4. Cite sources (files, URLs) for claims\n\n## Constraints\n- Read-only tools only (no code modifications)\n- Stay on assigned perspective\n- Arguments must be evidence-backed\n\"\"\")\n```\n\n### Step 4: Monitor and Moderate\n\nAfter subagents complete, the main agent:\n\n1. **Read scratchpad** and summarize key points to user\n2. **Assess debate health:**\n   - Progress: Are teams making new points?\n   - Balance: Is one team dominating?\n   - Relevance: Staying on topic?\n   - Depth: Avoiding superficial arguments?\n\n3. **Intervene if needed** - write to Moderator Notes section:\n   - `[MODERATOR] Stuck:` \"Team X, consider addressing Y\"\n   - `[MODERATOR] Tunnel:` \"Team X, you've repeated Z\"\n   - `[MODERATOR] Astray:` \"Refocus on core question\"\n   - `[MODERATOR] Disconnected:` \"Team X, respond to Team Y's point\"\n\n4. **Ask user** for next action:\n   - \"Advance to rebuttals?\"\n   - \"Request synthesis round?\"\n   - \"Conclude debate?\"\n\n### Step 5: Rebuttal Round (Sequential)\n\nSpawn teams **sequentially** for direct responses:\n\nOrder: Red → Blue → Green → Yellow → Purple (active teams only)\n\nEach team's prompt includes instruction to read and respond to specific opposing arguments.\n\n### Step 6: Synthesis Round (Optional)\n\nIf requested, spawn Purple Team (or all teams) to find:\n- Common ground\n- Irreconcilable differences\n- Potential compromises\n\n### Step 7: Conclude Debate\n\nMain agent writes Conclusion section:\n- **Summary:** Key positions from each team\n- **Agreements:** Points of consensus\n- **Disagreements:** Unresolved tensions\n- **Recommendations:** Suggested path forward (if applicable)\n\nUpdate scratchpad status to \"Completed\".\n\n---\n\n## Templates\n\n- [templates/debate-scratchpad.md](templates/debate-scratchpad.md) - Debate file template\n\n---\n\n## Success Criteria\n\n- Scratchpad created at `./debates/{topic}_{context}.md`\n- All active teams contributed arguments\n- Moderator interventions documented transparently\n- User controlled round progression\n- Debate concluded with synthesis\n\n---\n\n## Integration\n\n**Command:** `/debate {topic}`\n\n**Related:**\n- Tools: Task (subagents), AskUserQuestion (configuration), Edit (scratchpad)\n- Pattern: Document-centric coordination via shared scratchpad\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for:\n- Team perspective definitions\n- Intervention decision tree\n- Example debate flows\n- Common failure modes"
              },
              {
                "name": "docs-implement",
                "description": "General reference documents by domain. Use when creating architecture docs, strategy documents, specs, or guides.",
                "path": "skills/docs-implement/SKILL.md",
                "frontmatter": {
                  "name": "docs-implement",
                  "description": "General reference documents by domain. Use when creating architecture docs, strategy documents, specs, or guides."
                },
                "content": "# Docs Implement Skill\n\nDomain-specific reference documents for non-code artifacts.\n\n---\n\n## When to Use\n\n- Creating architecture documentation\n- Writing strategy or decision documents\n- Drafting specifications\n- Authoring guides or playbooks\n\n---\n\n## Domains\n\n| Domain | Purpose |\n|--------|---------|\n| architecture | System design, component diagrams, data flow |\n| strategy | Decision frameworks, trade-off analysis |\n| specs | API contracts, interface definitions |\n| guides | How-to documentation, playbooks |\n\n---\n\n## Related Skills\n\n- **code-implement**: Language-specific coding guidelines\n- **spec-create**: Spec documents with validation"
              },
              {
                "name": "dotfiles-manage",
                "description": "Manage dotfiles using dotter (symlink manager and templater). Use when deploying, adding, removing, or organizing configuration files in ~/dotfiles.",
                "path": "skills/dotfiles-manage/SKILL.md",
                "frontmatter": {
                  "name": "dotfiles-manage",
                  "description": "Manage dotfiles using dotter (symlink manager and templater). Use when deploying, adding, removing, or organizing configuration files in ~/dotfiles."
                },
                "content": "# Manage Dotfiles Skill\n\nManage dotfiles using [dotter](https://github.com/SuperCuber/dotter) - a dotfile manager and templater.\n\n## Environment\n\n- **Dotfiles repo**: `~/dotfiles`\n- **Dotter config**: `~/dotfiles/.dotter/`\n  - `global.toml`: Package definitions (files to deploy)\n  - `local.toml`: Machine-specific package selection\n  - `cache.toml`: Deployment state cache\n\n## Core Commands\n\n```bash\n# Deploy all configured files\ndotter deploy\n\n# Preview changes without applying\ndotter deploy --dry-run\n\n# Undeploy all managed files\ndotter undeploy\n\n# Watch for changes and auto-deploy\ndotter watch\n```\n\n## Workflow: Add New Dotfile\n\n### Step 1: Add Source File\n\nPlace the configuration file in `~/dotfiles`:\n\n```bash\n# Example: adding a new config\ncp ~/.config/app/config.toml ~/dotfiles/.config/app/config.toml\n```\n\n### Step 2: Define in global.toml\n\nAdd a new package or extend existing one in `~/dotfiles/.dotter/global.toml`:\n\n```toml\n# New package\n[myapp.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n\n# Or extend existing package\n[existing-package.files]\n\".config/app/config.toml\" = \"~/.config/app/config.toml\"\n```\n\n**File mapping format**: `\"source\" = \"target\"`\n- Source: relative path from dotfiles repo root\n- Target: absolute path or `~` for home directory\n\n### Step 3: Enable Package (if new)\n\nAdd package to `~/dotfiles/.dotter/local.toml`:\n\n```toml\npackages = [\"doom\", \"myapp\"]\n```\n\n### Step 4: Deploy\n\n```bash\ncd ~/dotfiles && dotter deploy\n```\n\n## Workflow: Remove Dotfile\n\n1. **Undeploy first**: `dotter undeploy`\n2. **Remove from global.toml**: Delete the file mapping\n3. **Remove package from local.toml** (if removing entire package)\n4. **Redeploy**: `dotter deploy`\n5. **Clean up source** (optional): Remove file from dotfiles repo\n\n## Package Organization\n\nGroup related files into packages:\n\n```toml\n# Shell configuration\n[shell.files]\n\".zshrc\" = \"~/.zshrc\"\n\".zprofile\" = \"~/.zprofile\"\n\".config/starship.toml\" = \"~/.config/starship.toml\"\n\n# Editor configuration\n[nvim.files]\n\".config/nvim\" = \"~/.config/nvim\"\n\n# Git configuration\n[git.files]\n\".gitconfig\" = \"~/.gitconfig\"\n\".gitignore_global\" = \"~/.gitignore_global\"\n```\n\n## Templating\n\nDotter supports Handlebars templating for machine-specific values:\n\n```toml\n# In global.toml - define variables\n[package.variables]\nemail = \"default@example.com\"\n\n# In local.toml - override per machine\n[variables]\nemail = \"work@company.com\"\n```\n\nIn template files, use `\\{{email}}` syntax.\n\n## Troubleshooting\n\n**Conflict with existing file:**\n```bash\n# Force overwrite (use with caution)\ndotter deploy --force\n```\n\n**Check deployment status:**\n```bash\ndotter deploy --dry-run --verbose\n```\n\n**View what's currently deployed:**\n```bash\ncat ~/dotfiles/.dotter/cache.toml\n```\n\n## Best Practices\n\n- Keep packages granular and focused\n- Use descriptive package names\n- Commit changes to dotfiles repo after modifications\n- Test with `--dry-run` before deploying\n- Use templating for machine-specific values (email, paths)"
              },
              {
                "name": "git-worktree-use",
                "description": "Create isolated git worktrees with safety verification. Use when starting feature work needing isolation or before executing plans - systematic directory selection and baseline verification.",
                "path": "skills/git-worktree-use/SKILL.md",
                "frontmatter": {
                  "name": "git-worktree-use",
                  "description": "Create isolated git worktrees with safety verification. Use when starting feature work needing isolation or before executing plans - systematic directory selection and baseline verification."
                },
                "content": "# Using Git Worktrees\n\nGit worktrees create isolated workspaces sharing the same repository.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n---\n\n## When to Use\n\n**Use for:**\n- Feature work needing isolation from current workspace\n- Parallel work on multiple branches\n- Before executing implementation plans\n\n**Don't use for:**\n- Quick fixes on current branch\n- Single-file changes\n- When isolation isn't needed\n\n---\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check Project Config\n\nLook for worktree directory preference in project documentation (CLAUDE.md, README, etc.).\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no preference found:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n---\n\n## Safety Verification\n\n### For Project-Local Directories\n\n**MUST verify .gitignore before creating worktree:**\n\n```bash\ngrep -q \"^\\.worktrees/$\" .gitignore || grep -q \"^worktrees/$\" .gitignore\n```\n\n**If NOT in .gitignore:**\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents.\n\n### For Global Directory\n\nNo .gitignore verification needed - outside project entirely.\n\n---\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Detect project type and install dependencies\nif [ -f package.json ]; then npm install; fi\nif [ -f Cargo.toml ]; then cargo build; fi\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then pip install -e .; fi\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Use project-appropriate command\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n---\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify .gitignore) |\n| `worktrees/` exists | Use it (verify .gitignore) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check config then ask user |\n| Not in .gitignore | Add it immediately + commit |\n| Tests fail | Report failures + ask |\n\n---\n\n## Red Flags\n\n**Never:**\n- Create worktree without .gitignore verification (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n\n**Always:**\n- Follow directory priority: existing > config > ask\n- Verify .gitignore for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n---\n\n## Integration\n\n**Use with:**\n- `brainstorm` - After design approval, set up workspace\n- `task-dispatch` - Work happens in this worktree\n- `completion-verify` - Verify baseline before and after"
              },
              {
                "name": "hooks-test",
                "description": "Test Claude Code hooks in isolation and via integration. Use when developing, debugging, or validating hook behavior.",
                "path": "skills/hooks-test/SKILL.md",
                "frontmatter": {
                  "name": "hooks-test",
                  "description": "Test Claude Code hooks in isolation and via integration. Use when developing, debugging, or validating hook behavior."
                },
                "content": "# Hooks Test\n\nTest hooks at three levels: unit tests (Python), direct invocation, and headless Claude.\n\n> **Reference**: See [reference.md](reference.md) for complete payload schemas.\n\n---\n\n## When to Use\n\n- Developing new hooks\n- Debugging hook failures\n- Validating hook behavior before deployment\n- Regression testing after hook changes\n\n---\n\n## Level 1: Unit Tests (Python)\n\nTest hook logic in isolation using the test harness with pytest.\n\n### Test Harness\n\nUse [templates/test-harness.py](templates/test-harness.py):\n\n```python\nfrom test_harness import create_payload, run_hook, assert_blocked, assert_allowed\n\ndef test_blocks_dangerous_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"rm -rf /\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_blocked(result, \"dangerous\")\n\ndef test_allows_safe_commands():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Bash\",\n                             tool_input={\"command\": \"echo hello\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_allowed(result)\n\ndef test_modifies_input():\n    payload = create_payload(\"PreToolUse\", tool_name=\"Write\",\n                             tool_input={\"file_path\": \"/tmp/test.txt\"})\n    result = run_hook(\"./my-hook.py\", payload)\n    assert_modified_input(result, \"file_path\", \"/safe/path/test.txt\")\n```\n\nRun with pytest:\n\n```bash\npytest test_my_hook.py -v\n```\n\n### Assertions\n\n| Function | Checks |\n|----------|--------|\n| `assert_blocked(result, msg)` | Exit code 2, stderr contains msg |\n| `assert_allowed(result)` | Exit code 0, no block decision |\n| `assert_modified_input(result, field, value)` | Exit 0, updatedInput has field |\n| `assert_context_added(result, contains)` | Exit 0, context includes text |\n\n---\n\n## Level 2: Direct Invocation (Shell)\n\nTest hooks by calling them directly with JSON payloads.\n\n### Basic Pattern\n\n```bash\necho '{\"hook_event_name\": \"PreToolUse\", \"tool_name\": \"Bash\", ...}' | ./hook.py\necho \"Exit: $?\"\n```\n\n### With Payload File\n\n```bash\ncat > /tmp/payload.json << 'EOF'\n{\n  \"session_id\": \"test-123\",\n  \"transcript_path\": \"/tmp/test.jsonl\",\n  \"cwd\": \"/path/to/project\",\n  \"permission_mode\": \"default\",\n  \"hook_event_name\": \"PreToolUse\",\n  \"tool_name\": \"Bash\",\n  \"tool_input\": {\"command\": \"rm -rf /\"},\n  \"tool_use_id\": \"toolu_01ABC\"\n}\nEOF\n\ncat /tmp/payload.json | ./my-hook.py\n```\n\n### Exit Code Reference\n\n| Exit Code | Meaning | Check |\n|-----------|---------|-------|\n| 0 | Success/allow | stdout contains valid JSON or context |\n| 2 | Block action | stderr contains reason for Claude |\n| Other | Non-blocking error | stderr contains user message |\n\n---\n\n## Level 3: Headless Claude (End-to-End)\n\nTest hooks end-to-end by invoking Claude in headless mode. Claude executes normally,\ntriggers hooks through its behavior, and you verify the outcome.\n\n### Headless Claude Flags\n\n```bash\nclaude -p \"prompt here\" --debug\n```\n\n| Flag | Purpose |\n|------|---------|\n| `-p` / `--print` | Headless mode - prints response and exits |\n| `--debug` | Shows hook execution details in stderr |\n| `--allowedTools` | Limit which tools Claude can use |\n| `--permission-mode` | Control permission behavior |\n\n### Test Patterns\n\n**Test PreToolUse blocks dangerous commands:**\n\n```bash\n# Prompt that would trigger dangerous Bash command\noutput=$(claude -p \"delete everything in /tmp\" --debug 2>&1)\n\n# Check hook blocked it (look for your hook's block message)\nif echo \"$output\" | grep -q \"blocked\"; then\n    echo \"PASS: Hook blocked dangerous command\"\nfi\n```\n\n**Test PostToolUse reacts to failures:**\n\n```bash\n# Prompt that triggers a command expected to fail\noutput=$(claude -p \"run: exit 1\" --debug 2>&1)\n\n# Check hook's reaction appears in output\necho \"$output\" | grep -q \"Hook command completed\"\n```\n\n**Test UserPromptSubmit adds context:**\n\n```bash\n# Any prompt triggers UserPromptSubmit\noutput=$(claude -p \"hello\" --debug 2>&1)\n\n# Verify hook ran\necho \"$output\" | grep \"UserPromptSubmit\"\n```\n\n**Test Stop hook continues execution:**\n\n```bash\n# Prompt that completes, triggering Stop hook\noutput=$(claude -p \"what is 2+2\" --debug 2>&1)\n\n# Check if hook caused continuation\necho \"$output\" | grep \"Stop\"\n```\n\n### Debug Output Format\n\n```\n[DEBUG] Executing hooks for PreToolUse:Bash\n[DEBUG] Found 1 hook matchers in settings\n[DEBUG] Matched 1 hooks for query \"Bash\"\n[DEBUG] Executing hook command: ./my-hook.py with timeout 60000ms\n[DEBUG] Hook command completed with status 0: <stdout content>\n```\n\n### Automated Test Script\n\n```bash\n#!/bin/bash\n# integration-test.sh - Run from project root with hook configured\nset -e\n\necho \"Test 1: PreToolUse blocks rm -rf\"\nif claude -p \"run: rm -rf /\" --debug 2>&1 | grep -qi \"block\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"Test 2: Safe commands allowed\"\nif claude -p \"run: echo hello\" --debug 2>&1 | grep -q \"hello\"; then\n    echo \"  PASS\"\nelse\n    echo \"  FAIL\"\n    exit 1\nfi\n\necho \"All tests passed\"\n```\n\n### Isolate Test Configuration\n\nUse `.claude/settings.local.json` for test hooks (not committed):\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\".claude/hooks/test-blocker.py\"]\n      }\n    ]\n  }\n}\n```\n\n---\n\n## Payload Examples by Event\n\n### PreToolUse\n\n```python\npayload = create_payload(\"PreToolUse\",\n    tool_name=\"Write\",\n    tool_input={\"file_path\": \"/etc/passwd\", \"content\": \"...\"})\n```\n\nTest: block dangerous paths, allow safe ops, modify input via `updatedInput`\n\n### PostToolUse\n\n```python\npayload = create_payload(\"PostToolUse\",\n    tool_name=\"Bash\",\n    tool_input={\"command\": \"npm test\"},\n    tool_response={\"stdout\": \"FAILED\", \"exit_code\": 1})\n```\n\nTest: react to failures, add context, log operations\n\n### UserPromptSubmit\n\n```python\npayload = create_payload(\"UserPromptSubmit\", prompt=\"delete all files\")\n```\n\nTest: block prohibited prompts, add context, transform input\n\n### Stop/SubagentStop\n\n```python\npayload = create_payload(\"Stop\", stop_hook_active=False)\n```\n\nTest: continue on conditions, verify TDD evidence, prevent loops when `stop_hook_active=True`\n\n---\n\n## Debugging Tips\n\n1. **Hook not running?** Check `/hooks` menu, verify matcher syntax\n2. **JSON parse error?** Validate hook outputs valid JSON on exit 0\n3. **Timeout?** Default is 60s, increase with `\"timeout\": 120000`\n4. **Wrong exit code?** Use `sys.exit(2)` to block, `sys.exit(0)` to allow\n5. **Stderr not showing?** Only displayed in verbose mode (`ctrl+o`)\n\n---\n\n## Success Criteria\n\n- [ ] Level 1: Unit tests pass with pytest\n- [ ] Level 2: Direct invocation returns expected exit codes\n- [ ] Level 3: Headless Claude triggers hook and behaves correctly\n- [ ] Exit codes match behavior (0=allow, 2=block)\n- [ ] Blocking responses include reason in stderr\n- [ ] JSON output is valid and complete\n\n---\n\n## Integration\n\n**Related:**\n- Docs: `claude --help` for CLI flags\n- Command: `/hooks` to manage hooks\n- Config: `.claude/settings.json`, `.claude/settings.local.json`"
              },
              {
                "name": "pr-review",
                "description": "Review GitHub PRs with inline comments and structured summaries. Use when reviewing PRs via gh CLI.",
                "path": "skills/pr-review/SKILL.md",
                "frontmatter": {
                  "name": "pr-review",
                  "description": "Review GitHub PRs with inline comments and structured summaries. Use when reviewing PRs via gh CLI."
                },
                "content": "# PR Review Skill\n\nInteractive GitHub PR review workflow: fetch PR, iterate findings with clarification, manage draft comments, submit when ready.\n\n**Delegates to:** `code-review` for methodology, `code-implement` for language guidelines.\n\n**Requires:** `gh-review` extension (`gh extension install srnnkls/gh-review`)\n\n---\n\n## When to Use\n\n- Reviewing PRs on GitHub repositories\n- When asked to review `owner/repo#N` or just `#N`\n- Posting structured feedback with inline code comments\n\n---\n\n## Workflow\n\n### Step 1: Configure Review\n\nUse **AskUserQuestion** to gather configuration (see `code-review` for details):\n- Focus areas: correctness, style, performance, security\n- Severity threshold: blocking, high, all\n- Scope: full, quick\n\n### Step 2: Fetch PR Context\n\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json title,body,files,commits,additions,deletions\ngh pr diff {pr} --repo {owner}/{repo}\ngh api repos/{owner}/{repo}/pulls/{pr} --jq '.head.sha'  # Get commit SHA\n```\n\n### Step 3: Check Existing Drafts\n\nFetch any pending review from previous session:\n\n```bash\ngh review comments {pr} -R {owner}/{repo} --mine --states=pending\n```\n\nIf drafts exist, display them and offer to continue or discard.\n\n### Step 4: Check Related Issues\n\n```bash\ngh issue view {issue} --repo {owner}/{repo} --json title,body\n```\n\n### Step 5: Delegate to Review Skills\n\n**Detect language** from file extensions:\n```bash\ngh pr view {pr} --repo {owner}/{repo} --json files --jq '.files[].path' | \\\n  sed 's/.*\\.//' | sort | uniq -c | sort -rn\n```\n\n**Load review methodology:**\n- `code-review` - Generic review process and checklist\n\n**Load language guidelines:**\n- `~/.claude/skills/code-implement/resources/loqui/languages/{language}/*` - Load language-specific resources based on file extensions\n\n### Step 6: Reference Style Guides\n\nCompare against:\n- Project style guides (STYLE.md, CLAUDE.md)\n- Established patterns from related issues\n\n### Step 7: Iterate Findings with Clarification\n\nFor each potential issue identified:\n\n1. **Present finding** - Show code context and concern\n2. **Ask context-aware questions** via `AskUserQuestion` with `multiSelect`:\n   - Type changes → \"Is type widening intentional?\"\n   - Error handling removed → \"Was this error path obsolete?\"\n   - Performance-sensitive code → \"Acceptable trade-off?\"\n   - General → \"Flag this?\", \"Severity level?\"\n3. **Batch decisions** - Allow selecting multiple findings to flag/dismiss\n\n### Step 8: Create/Update Draft Comments\n\nFor each confirmed finding, add to pending review:\n\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {line} -b \"{comment_body}\"\n```\n\nFor multi-line comments:\n```bash\ngh review add {pr} -R {owner}/{repo} -p {file_path} -l {end_line} --start-line {start_line} -b \"{comment_body}\"\n```\n\nTo update an existing draft:\n```bash\ngh review edit {pr} -R {owner}/{repo} -c {comment_id} -b \"{updated_body}\"\n```\n\nTo remove a draft:\n```bash\ngh review delete {pr} -R {owner}/{repo} -c {comment_id}\n```\n\n### Step 9: Submit or Keep Draft\n\nUse **AskUserQuestion** to determine action:\n\n**Options:**\n- **Submit now** → Choose verdict (approve, request_changes, comment)\n- **Keep as draft** → Leave pending for later continuation\n\nTo submit:\n```bash\ngh review submit {pr} -R {owner}/{repo} -v {approve|request_changes|comment} -b \"{summary}\"\n```\n\nTo discard and start fresh:\n```bash\ngh review discard {pr} -R {owner}/{repo}\n```\n\n---\n\n## Key Gotchas\n\n- **Line numbers**: Use actual file line numbers (not diff positions) with `gh review`\n- **Comment IDs**: GraphQL node IDs (e.g., `PRRC_kwDOABC123`) - get with `--ids` flag\n- **Comment format**: Supports markdown, suggestion blocks with triple backticks\n\n---\n\n## gh-review Commands\n\n| Command | Description |\n|---------|-------------|\n| `comments` | List PR comments with filtering (`--mine`, `--states`) |\n| `view` | View review threads hierarchically |\n| `add` | Add draft comment to pending review |\n| `edit` | Update existing draft comment |\n| `delete` | Remove draft comment |\n| `submit` | Submit review with verdict |\n| `discard` | Delete pending review |\n\n---\n\n## Related Skills\n\n- **code-review**: Review methodology (focus, severity, checklist)\n- **code-implement**: Language guidelines and implementation patterns\n\n---\n\n## Reference\n\n- `gh review --help`\n- `gh pr review --help`"
              },
              {
                "name": "skill-create",
                "description": "Create new Claude Code skills following project patterns and best practices. Use when building new skills, extracting reusable capabilities, or converting commands to skills.",
                "path": "skills/skill-create/SKILL.md",
                "frontmatter": {
                  "name": "skill-create",
                  "description": "Create new Claude Code skills following project patterns and best practices. Use when building new skills, extracting reusable capabilities, or converting commands to skills."
                },
                "content": "# Skill Creation\n\nCreate well-structured skills using progressive disclosure and project conventions.\n\n> **Reference:** [best-practices.md](best-practices.md) for comprehensive guidance, [reference.md](reference.md) for project patterns and frontmatter specs.\n\n---\n\n## Workflow\n\n### Step 1: Understand Use Cases\n\nGather concrete examples of how the skill will be used:\n\n- What tasks will it handle?\n- What would users say to trigger it?\n- What variations exist?\n\nSkip this step only when usage patterns are already clearly understood.\n\n### Step 2: Plan Contents\n\nAnalyze each use case to identify reusable resources:\n\n| Resource Type | When to Use | Example |\n|---------------|-------------|---------|\n| `scripts/` | Same code rewritten repeatedly | `rotate_pdf.py` |\n| `references/` | Domain knowledge Claude needs | `schema.md`, `api.md` |\n| `assets/` | Files used in output | `template.html`, `logo.png` |\n| `templates/` | Document structure patterns | `report.md` |\n\n### Step 3: Choose Frontmatter\n\n**Required fields:**\n\n```yaml\nname: skill-name          # Lowercase, hyphens, max 64 chars\ndescription: |            # Max 1024 chars\n  [What it does]. Use when [context].\n```\n\n**Optional fields:**\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `context` | Run in forked sub-agent | `context: fork` |\n| `agent` | Specify agent type | `agent: haiku` |\n| `user-invocable` | Hide from slash menu | `user-invocable: false` |\n| `allowed-tools` | Restrict available tools | See reference.md |\n| `hooks` | Lifecycle hooks (PreToolUse, PostToolUse, Stop) | See reference.md |\n\n**Naming pattern:** `<namespace>[-<subnamespace>]-<action>`\n- `code-debug`, `spec-create`, `git-worktree-use`\n\n**Description format:** Third person, what + when.\n- \"Generate GitHub issue drafts from spec directories. Use when converting specs to GitHub issues.\"\n\n### Step 4: Create Structure\n\n```bash\nmkdir -p .claude/skills/{skill-name}\n```\n\n**Standard structure:**\n\n```\n.claude/skills/{skill-name}/\n├── SKILL.md              # Main instructions (<500 lines)\n├── templates/            # Document templates (.md)\n├── scripts/              # Executable code (.sh, .py)\n└── references/           # Extended documentation\n```\n\n### Step 5: Implement & Test\n\n**Write SKILL.md:**\n- Keep under 200 lines (500 max)\n- Progressive disclosure: SKILL.md → references/\n- Include concrete examples, no emojis\n- Reference authoritative docs (don't duplicate)\n\n**Test with real tasks:**\n1. Does the description trigger correctly?\n2. Can Claude find bundled resources?\n3. Does the workflow complete successfully?\n\n---\n\n## Degrees of Freedom\n\nMatch specificity to task fragility:\n\n**High freedom** - Multiple approaches valid, context-dependent:\n```markdown\n## Code review\n1. Analyze structure and organization\n2. Check for bugs and edge cases\n3. Suggest improvements\n```\n\n**Low freedom** - Operations fragile, consistency critical:\n```markdown\n## Database migration\nRun exactly: `python scripts/migrate.py --verify --backup`\nDo not modify flags.\n```\n\n---\n\n## Skill Types\n\n| Type | Characteristics | Examples |\n|------|-----------------|----------|\n| **Operational** | Multi-step workflow, state changes, document templates | `spec-create`, `spec-archive` |\n| **Generation** | Transform input → structured output, format templates | `spec-issues-create` |\n| **Guidance** | Imperative instructions, code patterns | `code-implement`, `code-debug` |\n\n---\n\n## Success Criteria\n\n- Name follows `<namespace>[-<subnamespace>]-<action>`\n- Description is third person with what + when\n- SKILL.md under 200 lines (500 max)\n- Workflow steps numbered and actionable\n- Templates extracted to separate files\n- References point to authoritative sources\n- No emojis (text markers only)\n\n---\n\n## Reference\n\n- [best-practices.md](best-practices.md) - Core principles, patterns, checklist\n- [reference.md](reference.md) - Project patterns, frontmatter specs, anti-patterns"
              },
              {
                "name": "spec-archive",
                "description": "Archive completed development specs from ./specs/active/ to ./specs/archive/, updating documents with completion status and maintaining archive index. Use when finishing tasks or moving completed work to archive.",
                "path": "skills/spec-archive/SKILL.md",
                "frontmatter": {
                  "name": "spec-archive",
                  "description": "Archive completed development specs from ./specs/active/ to ./specs/archive/, updating documents with completion status and maintaining archive index. Use when finishing tasks or moving completed work to archive."
                },
                "content": "# Spec Archive Skill\n\nArchive completed development specs with proper documentation and index maintenance.\n\n> **Reference**: See [reference.md](reference.md).\n\n---\n\n## When to Use\n\nArchive a spec when:\n- All success criteria met\n- Tests passing and code merged\n- Ready to start new work\n- Spec is \"done enough\" and blocking new work\n- User explicitly requests archival\n\nDon't archive when:\n- Spec is actively in progress\n- Blocking issues remain unresolved\n- Branch has unmerged changes\n- Critical checklist items incomplete (unless user confirms)\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/active/add-temporal-joins/\n```\n\nIf not found, list available specs and ask which to archive.\n\n### Step 2: Verify Completion Status\n\nBefore archiving, check:\n- Read `spec.md` - status should be \"Complete\"\n- Read `tasks.yaml` - verify all tasks have `status: completed`\n- If incomplete, ask: \"This spec appears incomplete. Archive anyway? (y/n)\"\n\n### Step 3: Pre-Archive Updates\n\nUpdate documents before moving:\n\n**In `spec.md`:**\n- Set status to `Complete`\n- Add `**Completed**: [Today's date]` after Started date\n- Ensure all success criteria marked complete\n\n**In `tasks.yaml`:**\n- Update `meta.progress` to final count (e.g., \"45/45\")\n- Update `meta.last_updated` to today\n- Ensure all tasks have `status: completed`\n- Mark any phase checkpoints as `verified: true`\n\n**In `context.md`:**\n- Update \"Last Updated\" to today\n- Add Archive Notes section using [templates/archive-notes.md](templates/archive-notes.md)\n\n### Step 4: Archive the Spec\n\n```bash\nmkdir -p ./specs/archive/\nmv ./specs/active/{spec-name} ./specs/archive/{spec-name}\n```\n\n### Step 5: Git Operations (Optional)\n\nIf spec has associated branch:\n1. Check current branch\n2. If on spec branch, ask: \"Switch back to main? (y/n)\"\n3. Ask: \"Delete spec branch `{branch}` (only if merged)? (y/n)\"\n\n### Step 6: Create/Update Archive Index\n\nCreate `./specs/archive/README.md` if doesn't exist using [templates/archive-index.md](templates/archive-index.md).\n\nAdd entry at top of Archive Index section:\n\n```markdown\n### [Spec Name] - [Today's Date]\n- **Duration**: [Started] → [Completed]\n- **Branch**: [branch-name if applicable]\n- **Summary**: [One sentence from spec.md Overview]\n- **Location**: `./specs/archive/{spec-name}/`\n\n---\n```\n\n### Step 7: Confirm Completion\n\nReport to user:\n```\nArchived: {spec-name}\n\n  From: ./specs/active/{spec-name}/\n  To:   ./specs/archive/{spec-name}/\n\n  Files archived:\n  - spec.md (Complete)\n  - context.md (Final notes added)\n  - tasks.yaml (X/Y tasks complete)\n\n  Archive index updated: ./specs/archive/README.md\n\n  [If branch operations performed]:\n  Git: Switched to main, deleted branch {branch}\n```\n\n---\n\n## Partial Completion Handling\n\nIf archiving incomplete spec (user confirmed):\n- Mark status as `Incomplete - [Reason]` not `Complete`\n- In Archive Notes, clearly document what was NOT completed\n- Explain why spec was abandoned/deferred\n- Consider creating new spec for remaining work\n\n---\n\n## Archive Notes Template\n\nSee [templates/archive-notes.md](templates/archive-notes.md) for full template including:\n- Summary (2-3 sentences)\n- Key Outcomes (deliverables)\n- Technical Debt / Future Work\n- Lessons Learned\n\n---\n\n## Templates\n\n- [templates/archive-notes.md](templates/archive-notes.md) - Archive Notes section for context.md\n- [templates/archive-index.md](templates/archive-index.md) - Archive README template\n\n---\n\n## Success Criteria\n\n- Spec moved from `./specs/active/` to `./specs/archive/`\n- All three documents updated with completion status\n- Archive Notes added to context.md\n- Archive index entry created\n- User informed of next steps\n\n---\n\n## Integration\n\n**Workflow:**\n- During task: Use `/spec.update` to sync status\n- After task: Use this skill to archive\n- Result: Clean `./specs/active/`, historical record preserved\n\n**Related:**\n- Command: `/spec.archive`\n- Skills: `spec-create` (creation), `spec-update` (reminder)\n\n---\n\n## Reference\n\nSee [reference.md](reference.md) for archive organization guidelines and troubleshooting."
              },
              {
                "name": "spec-clarify",
                "description": "Resolve ambiguities and markers in validation.yaml interactively. Updates spec docs directly with full audit trail.",
                "path": "skills/spec-clarify/SKILL.md",
                "frontmatter": {
                  "name": "spec-clarify",
                  "description": "Resolve ambiguities and markers in validation.yaml interactively. Updates spec docs directly with full audit trail."
                },
                "content": "# Spec Clarify Skill\n\nResolve ambiguities and markers by updating spec documents directly with tracked changes.\n\n---\n\n## When to Use\n\n**Use for:**\n- Resolving markers before task-dispatch (especially for Initiatives)\n- Clarifying ambiguous requirements discovered post-validation\n- Addressing gaps identified in ambiguity_scan\n\n**Don't use for:**\n- Initial validation (use spec-validate)\n- Changing fundamental scope (re-run spec-validate)\n\n---\n\n## Workflow\n\n### Step 1: Load and Scan\n\n1. Find active spec in `./specs/active/*/`\n2. Read `validation.yaml` from spec directory\n3. Run ambiguity scan:\n   - Check `ambiguity_scan` section for areas with `status: partial` or `status: missing`\n   - These become clarification candidates alongside open markers\n4. Collect open markers where `status: open`\n5. Merge candidates: ambiguity gaps + open markers (deduplicate by area)\n6. If no candidates: report \"No unresolved items\" and exit\n\n### Step 2: Present Candidates\n\nFor each candidate (prioritized by impact), use AskUserQuestion:\n\n```\nHeader: ${AREA}\nQuestion: ${DESCRIPTION}\nmultiSelect: false\nOptions:\n- [Generated options based on context]\n- Defer: Skip for now\n```\n\n**Prioritization:** Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 3: Update Spec Documents Directly\n\nWhen a clarification is resolved, update the relevant section in the source document:\n\n| Clarification Area | Target Document | Target Section |\n|--------------------|-----------------|----------------|\n| Scope | spec.md | Requirements, Scope |\n| Behavior | spec.md | Requirements, Behavior |\n| Data Model | context.md | Data Model |\n| Constraints | spec.md | Constraints |\n| Edge Cases | spec.md | Edge Cases |\n| Integration | context.md | Integration Points |\n| Terminology | context.md | Terminology |\n\n**Update approach:**\n1. Read the target section\n2. Integrate the clarification naturally into existing content\n3. Do NOT create a separate \"## Clarifications\" section\n\n### Step 4: Record Clarification Session\n\nCreate a new session entry in `clarification_sessions`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n```\n\n**doc_updates** tracks exactly which files/sections changed for audit trail.\n\n### Step 5: Update Ambiguity Scan Status\n\nFor each resolved clarification from ambiguity gaps:\n\n1. Update `ambiguity_scan.${area}.status` to `clear`\n2. Remove the resolved gap from `ambiguity_scan.${area}.gaps`\n\n### Step 6: Update Markers\n\nFor each resolved marker:\n\n1. Change `status: open` to `status: resolved`\n2. Add `resolution: \"${USER_ANSWER}\"`\n\n### Step 7: Re-check Gates (Initiatives Only)\n\nFor Initiative specs:\n\n1. Re-evaluate gates in validation.yaml\n2. Update gate status if resolution changes assessment\n3. Report gate status\n\n---\n\n## Doc Update Mapping\n\n| Source | Target File | Target Section |\n|--------|-------------|----------------|\n| Scope gap | spec.md | ## Requirements or ## Scope |\n| Behavior gap | spec.md | ## Requirements / Behavior subsection |\n| Data Model gap | context.md | ## Data Model |\n| Constraints gap | spec.md | ## Constraints |\n| Edge Cases gap | spec.md | ## Edge Cases |\n| Integration gap | context.md | ## Integration Points |\n| Terminology gap | context.md | ## Terminology |\n\n---\n\n## Example Session\n\n```\n[Load validation.yaml]\n[Run ambiguity scan]\n- scope: partial (1 gap)\n- data_model: missing (2 gaps)\n[Check markers]\n- M001 (Constraints): open\n\nCandidates:\n1. Scope: \"User role boundaries unclear\"\n2. Data Model: \"Schema for notifications not defined\"\n3. Data Model: \"Retention policy not specified\"\n4. Constraints: \"Authentication method not specified\"\n\n---\nHeader: Scope\nQuestion: What user roles exist and what are their boundaries?\n\nOptions:\n- Admin/User: Two-tier with admin full access\n- Role-based: Granular permissions per feature\n- Defer: Skip for now\n\nUser selects: Admin/User\n\n[Update spec.md#requirements]\nAdded: \"Two-tier role system: Admin (full access), User (standard permissions)\"\n\n[Record session]\nclarification_sessions:\n  - id: S001\n    timestamp: 2025-01-15T10:30:00Z\n    questions:\n      - id: Q001\n        question: \"What user roles exist and what are their boundaries?\"\n        answer: \"Two-tier: Admin (full access), User (standard permissions)\"\n        area: scope\n        doc_updates:\n          - file: spec.md\n            section: Requirements\n            action: modified\n\n[Update ambiguity_scan]\nscope:\n  status: clear\n  gaps: []\n\n---\nHeader: Constraints\nQuestion: Which authentication method should be used?\n...\n```\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.clarify` command\n\n**Related skills:**\n- `spec-validate` - Initial validation (creates ambiguity_scan and markers)\n- `spec-create` - Document creation (references markers)\n- `task-dispatch` - Checks for blocking markers before dispatch"
              },
              {
                "name": "spec-create",
                "description": "Create spec documents (spec.md, context.md, tasks.yaml, dependencies.yaml, validation.yaml). Receives validation data from spec-validate.",
                "path": "skills/spec-create/SKILL.md",
                "frontmatter": {
                  "name": "spec-create",
                  "description": "Create spec documents (spec.md, context.md, tasks.yaml, dependencies.yaml, validation.yaml). Receives validation data from spec-validate."
                },
                "content": "# Spec Creation Skill\n\nCreates structured tracking documents for complex development tasks.\n\n---\n\n## When to Use\n\n**DO use for:**\n- Complex multi-step tasks (3+ distinct phases)\n- Non-trivial features requiring careful planning\n- After ExitPlanMode when user accepts a plan\n- Multi-phase implementation work\n\n**DON'T use for:**\n- Single-file changes\n- Trivial refactorings\n- Simple bug fixes\n- Tasks completable in < 30 minutes\n\n---\n\n## Workflow\n\n### Step 1: Check for Validation Data\n\n**If invoked after spec-validate:**\n- Issue type, taxonomy coverage, and clarification log are available\n- Use this data to populate frontmatter and validation.yaml\n- Proceed to Step 2\n\n### Step 2: Determine Task Name\n\nDerive from: git branch name, ExitPlanMode plan name, or user-provided argument.\nFormat: `kebab-case` (e.g., `add-temporal-joins`). If unclear, ask user.\n\n### Step 3: Gather Context\n\nRead relevant files to understand: task goal, key files, central types, architectural decisions, current vs target state.\n\n### Step 3.5: Detect and Extract Code Artifacts\n\n**If input contains code blocks** (```python, ```rust, etc.):\n\n1. **Extract code blocks** with their language tags\n2. **Create resources directory:**\n   ```bash\n   mkdir -p ./specs/draft/[task-name]/resources/\n   ```\n3. **Stage extracted content** (held in memory until user chooses):\n   - Code blocks with language fences\n   - Schema definitions (OpenAPI, JSON Schema, type definitions)\n   - Config examples (YAML, TOML, env files)\n   - Integration/test patterns\n\n4. **Ask user which resources to create** (use AskUserQuestion with multiSelect):\n   ```\n   Header: Resources\n   Question: Which implementation artifacts should be preserved?\n   multiSelect: true\n   Options:\n   - implementation: Code sketches/examples - patterns to follow (loqui-validated)\n   - schemas: API contracts, data models, type definitions\n   - config: Configuration examples\n   - patterns: Integration and test patterns\n   - assets: Diagrams, screenshots, other media\n   - none: Skip resources, spec only\n   ```\n\n5. **Create selected resources:**\n   - Only create directories for selected options\n   - Run loqui validation on code (if selected and language has guidelines)\n   - Report violations in validation.yaml under `loqui_check` section\n\n6. **Continue with spec generation** using extracted requirements (not code details)\n\n**Resources directory structure** (all subdirectories optional):\n```\nspecs/draft/{spec-name}/\n├── spec.md\n├── context.md\n├── tasks.yaml\n├── ...\n└── resources/           # Only if user selected any\n    ├── implementation.md   # If \"implementation\" selected\n    ├── schemas/         # If \"schemas\" selected\n    ├── config/          # If \"config\" selected\n    ├── patterns/        # If \"patterns\" selected\n    └── assets/          # If \"assets\" selected\n```\n\n### Step 4: Create Directory and Documents\n\n```bash\nmkdir -p ./specs/draft/[task-name]/\n```\n\nGenerate these files:\n\n1. **`spec.md`** - Strategic spec (review this)\n2. **`context.md`** - Implementation context (update as you work)\n3. **`tasks.yaml`** - Work checklist (dignity CLI, TodoWrite sync)\n4. **`dependencies.yaml`** - Task dependency graph (parallel dispatch)\n5. **`validation.yaml`** - Audit trail and gate checks\n\n**Document scaling by issue type:**\n\n| Document | Initiative | Feature | Task |\n|----------|-----------|---------|------|\n| spec.md | Full strategic | Standard | Lightweight |\n| context.md | High-level | Standard | Skip |\n| tasks.yaml | Feature breakdown + phases | Task breakdown | Single task |\n| dependencies.yaml | Full DAG | Phase-based | Skip |\n| validation.yaml | Full (7 areas + gates) | Full (7 areas) | Skip |\n\n**Task output = 2 files:** spec.md (lightweight) + tasks.yaml\n\n**Frontmatter for spec.md:**\n\n```yaml\n---\nissue_type: [Initiative|Feature|Task]\ncreated: [Date]\nstatus: Draft\nstage: draft\nclaude_plan: [path to native Claude plan file, if exists]\n---\n```\n\nIf a native Claude plan exists (from EnterPlanMode/ExitPlanMode), include its path in the `claude_plan` field. This links the detailed spec to its originating design document.\n\n### Conditional Sections\n\nBased on validation data and issue type, include optional sections:\n\n**spec.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| User Stories (P1/P2/P3) | Include | Skip | Skip |\n| Given/When/Then Acceptance | Full | Standard | Simple |\n| API Contract | Include if API work | Opt-in | Skip |\n| Implementation Strategy | Include | Skip | Skip |\n\n**context.md:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Tech Decisions | Include | Opt-in | (file skipped) |\n| Data Model | Include | Opt-in | (file skipped) |\n\n**validation.yaml:**\n\n| Section | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Gates | Evaluate all | n/a | (file skipped) |\n| Complexity Tracking | If violations | If violations | (file skipped) |\n| Markers | Full | Full | (file skipped) |\n\n**tasks.yaml:**\n\n| Element | Initiative | Feature | Task |\n|---------|------------|---------|------|\n| Phases with checkpoints | Full (gates between phases) | Simple (optional checkpoints) | Skip |\n| Evidence tracking | Full | Opt-in | Skip |\n| Dependencies | Full | Opt-in | Skip |\n\n**tasks.yaml structure:**\n\n```yaml\nspec: ${SPEC_NAME}\ncode: ${CODE}           # Prefix for task IDs (e.g., FEAT → FEAT-001)\nnext_id: 1\ntasks:\n  - id: ${CODE}-001\n    content: Task description\n    status: pending     # pending | in_progress | completed\n    active_form: Doing task description\nmeta:\n  created: ${DATE}\n  last_updated: ${DATE}\n  progress: 0/N\nphases:                 # Optional: organize tasks with checkpoints\n  - name: \"Phase 1: Setup\"\n    task_ids: [${CODE}-001, ${CODE}-002]\n    checkpoint:\n      description: Setup complete\n      criteria: [...]\n      verified: false\n```\n\nUsed by dignity for task management operations. The `code` field generates sequential IDs (e.g., AUT-001, AUT-002).\n\n### Step 5: Populate TodoWrite from tasks.yaml\n\nParse the just-created `tasks.yaml` and populate TodoWrite:\n\n1. Read tasks from `tasks.yaml`\n2. Create TodoWrite with up to 10 tasks:\n   - status: map from tasks.yaml status\n   - content: task content field\n   - activeForm: task active_form field\n\n**Example:**\n```yaml\n# tasks.yaml:\ntasks:\n  - id: IMPL-001\n    content: Create implement.md command\n    status: pending\n    active_form: Creating implement.md command\n  - id: IMPL-002\n    content: Create docs-implement SKILL.md\n    status: pending\n    active_form: Creating docs-implement SKILL.md\n```\n\n```json\n// TodoWrite:\n[\n  {\"content\": \"Create implement.md command\", \"status\": \"pending\", \"activeForm\": \"Creating implement.md command\"},\n  {\"content\": \"Create docs-implement SKILL.md\", \"status\": \"pending\", \"activeForm\": \"Creating docs-implement SKILL.md\"}\n]\n```\n\n### Step 6: Present Summary\n\nShow user:\n- Directory created: `./specs/draft/[task-name]/`\n- Human-facing docs: spec.md, context.md (brief overview of each)\n- Tooling artifacts: tasks.yaml, dependencies.yaml, validation.yaml (note these are for automation)\n- Validation coverage (if from spec-validate)\n- Next action: \"Run `/spec.review` or `/spec.promote` when ready to activate\"\n\n### Step 7: Offer Comprehensive Review (Optional)\n\nAfter presenting summary, offer spec review:\n\nUse AskUserQuestion:\n```\nHeader: Review\nQuestion: Would you like a comprehensive spec review before implementation?\nmultiSelect: false\nOptions:\n- Yes: Run multi-agent review (Claude + OpenCode)\n- Later: Skip for now, use /spec.review when ready\n- Skip: Proceed without review\n```\n\nIf \"Yes\": Invoke `spec-review` skill with the just-created spec name.\n\n---\n\n## Output Artifacts\n\n**For humans (review these):**\n```\n├── spec.md      # WHY & WHAT - Strategic requirements, acceptance criteria\n├── context.md   # WHAT WE LEARNED - Key files, decisions, gotchas\n└── resources/   # HOW TO BUILD - Implementation details (when provided)\n```\n\n**For tooling (infrastructure):**\n```\n├── tasks.yaml        # Progress tracking, TodoWrite sync\n├── dependencies.yaml # Parallel dispatch DAG\n└── validation.yaml   # Audit trail, gate checks, loqui validation\n```\n\nThe YAML files are infrastructure - humans *can* read them for debugging or auditing, but the primary consumers are tooling and automation.\n\n### The resources/ Directory\n\nOnly created when implementation details are provided in input. Contains structured artifacts:\n\n| Subdirectory | Content |\n|-------------|---------|\n| `implementation.md` | Code sketches and examples - patterns to follow, not tested/final (loqui-validated) |\n| `schemas/` | API contracts, data models, type definitions |\n| `config/` | Configuration examples |\n| `patterns/` | Integration and test patterns |\n| `assets/` | Diagrams, screenshots, other media |\n\n**Review burden:** 2-3 documents (spec.md, context.md, resources/ when present), not 8+.\n\nSee [guidelines.md](guidelines.md) for detailed breakdown.\n\n### Native Plan Integration\n\nIf a native Claude plan exists (from EnterPlanMode), add a **Native Plan** section to `context.md`:\n\n```markdown\n## Native Plan\n\n**Source:** `/path/to/.claude/specs/spec-name.md`\n\nSummary of the original design:\n- Goal: [brief goal from native plan]\n- Approach: [key approach decisions]\n- Open questions resolved: [any clarifications made]\n```\n\nThis preserves the connection to the original design discussion and rationale.\n\n---\n\n## Templates\n\nLocated in `templates/` directory:\n- [validation.yaml](templates/validation.yaml)\n- [dependencies.yaml](templates/dependencies.yaml)\n- [tasks.yaml](templates/tasks.yaml)\n- [spec.md](templates/spec.md)\n- [context.md](templates/context.md)\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command\n- Runs `spec-validate` first, then this skill\n\n**Related commands:**\n- `/spec.review` - Review spec with multiple AI reviewers\n- `/spec.update` - Sync spec with git history\n- `/spec.archive` - Archive completed spec\n- `/spec.issues` - Generate GitHub issues from spec"
              },
              {
                "name": "spec-issues-create",
                "description": "Generate GitHub issue drafts from spec directories, creating initiative/feature/task markdown files with gh CLI commands. Use when converting specs to GitHub issues or setting up issue tracking for features.",
                "path": "skills/spec-issues-create/SKILL.md",
                "frontmatter": {
                  "name": "spec-issues-create",
                  "description": "Generate GitHub issue drafts from spec directories, creating initiative/feature/task markdown files with gh CLI commands. Use when converting specs to GitHub issues or setting up issue tracking for features."
                },
                "content": "# Spec Issues Skill\n\nGenerate GitHub issue drafts from spec documents following the project's issue management framework.\n\n---\n\n## When to Use\n\nUse when you need to create GitHub issues from:\n- Spec documents (`./specs/`)\n- Any structured work breakdown ready for issue tracking\n\n---\n\n## Workflow\n\n### Step 1: Validate Input Path\n\nParse path from command argument and verify:\n\n```bash\n# User provides path like:\n# ./specs/active/my-feature\n# ./specs/archive/nested-view-refactor\n```\n\nValidate directory exists and contains appropriate files.\n\n### Step 2: Detect Issue Type\n\n**Spec indicators:**\n- Has `spec.md`\n- Has `tasks.md`\n- Has `context.md`\n\n**Issue type detection:**\n\n1. Read spec.md frontmatter for `issue_type` field:\n   ```yaml\n   ---\n   issue_type: [Initiative|Feature|Task]\n   created: [Date]\n   status: Active\n   ---\n   ```\n\n2. If `issue_type` present in frontmatter:\n   - Skip type selection question\n   - Use frontmatter value directly for template selection\n\n3. If `issue_type` absent:\n   - Fall back to current detection logic (spec overview analysis)\n   - Consider asking user to classify\n\n**Template selection based on issue_type:**\n- `Initiative` → `templates/initiative.md`\n- `Feature` → `templates/feature.md`\n- `Task` → `templates/task.md`\n\nRead appropriate files based on detected type.\n\n### Step 3: Create Issue Drafts Directory\n\n```bash\nmkdir -p \"$SOURCE_DIR/drafts/issues\"\n```\n\n**Structure created:**\n```\n./specs/archive/{spec-name}/drafts/issues/\n├── initiative-{spec-name}.md\n├── issue-001-{short-description}.md\n├── issue-002-{short-description}.md\n└── ...\n```\n\n### Step 4: Generate Issue Draft Files\n\nCreate issue markdown files using templates:\n\n**Initiative file**: `initiative-{name}.md`\n- Use [templates/initiative.md](templates/initiative.md)\n- Map spec overview → initiative overview\n- Map success criteria → acceptance criteria\n- Include YAML frontmatter with metadata\n\n**Feature/Task files**: `issue-{NNN}-{description}.md`\n- Use [templates/feature.md](templates/feature.md) or [templates/task.md](templates/task.md)\n- Extract from spec phases or tasks\n- Sequential numbering: 001, 002, 003\n- Include YAML frontmatter\n\n**Key points:**\n- YAML frontmatter contains: title, labels, milestone, assignees\n- Frontmatter serves as reference metadata\n- Content extracted from spec documents\n\n### Step 5: Generate gh CLI Script\n\nCreate bash script using [scripts/create-issue.sh](scripts/create-issue.sh):\n\n- Parse YAML frontmatter from draft files\n- Generate `gh issue create` commands\n- Capture issue numbers for sub-issue linking\n- Use `gh sub-issue add` for hierarchy\n- Include helper function to extract YAML fields\n\n**Script structure:**\n```bash\n#!/bin/bash\n# Extract YAML → gh issue create → capture numbers → link sub-issues\n```\n\n### Step 6: Provide Metadata Configuration Checklist\n\nInclude instructions for setting fields via GitHub web UI:\n- Priority (Critical/High/Medium/Low)\n- Status (Backlog/To Do/In Progress/etc.)\n- Sprint/Iteration\n- Complexity estimation\n- Milestone/Release\n\n---\n\n## Content Transformation\n\nSee [mapping-guide.md](mapping-guide.md) for detailed rules on:\n- Mapping spec content to issue hierarchy (Initiative → Feature → Task)\n- Issue title patterns for each type\n- Component label detection from file paths\n- Priority and complexity mapping\n- Cross-linking patterns\n\nQuick summary:\n- Specs: phases → Features, task breakdown → Tasks\n- Titles follow verb-noun pattern, increasing specificity by level\n\n---\n\n## Templates\n\n- [templates/initiative.md](templates/initiative.md) - Initiative issue template with YAML\n- [templates/feature.md](templates/feature.md) - Feature issue template with YAML\n- [templates/task.md](templates/task.md) - Task issue template with YAML\n\n## Scripts\n\n**generate-issues.py**: Automates draft generation from specs. Parses content, creates issue files with YAML frontmatter.\n**create-issue.sh**: Creates actual GitHub issues from drafts using `gh` CLI.\n\n**Requirement**: Install `gh sub-issue` extension: `gh extension install yahsan2/gh-sub-issue`\n\n---\n\n## Success Criteria\n\n- Drafts directory created: `{source}/drafts/issues/`\n- Initiative file with complete overview\n- Feature/task files with proper templates\n- gh CLI script with issue creation commands\n- Metadata checklist provided\n\n---\n\n## Integration\n\n**Workflow:**\n1. Complete spec\n2. Invoke this skill with source path\n3. Review generated draft files\n4. Run gh CLI script to create issues\n5. Set metadata via GitHub web UI\n\n**Related:**\n- Command: `/spec.issues`\n- Tools: `gh` CLI, `gh sub-issue` extension"
              },
              {
                "name": "spec-promote",
                "description": "Promote spec from draft to active stage. Use after spec review passes or when ready to begin implementation.",
                "path": "skills/spec-promote/SKILL.md",
                "frontmatter": {
                  "name": "spec-promote",
                  "description": "Promote spec from draft to active stage. Use after spec review passes or when ready to begin implementation."
                },
                "content": "# Spec Promote Skill\n\nPromote validated specs from draft to active stage for implementation.\n\n---\n\n## When to Use\n\nPromote a spec when:\n- Spec validation passes (no open markers or user override)\n- Ready to begin implementation\n- Spec review approved\n- User explicitly requests promotion\n\nDon't promote when:\n- Open markers exist without user override\n- Initiative has failed gates\n- Spec is incomplete or missing required documents\n- Draft directory doesn't exist\n\n---\n\n## Workflow\n\n### Step 1: Validate Spec Argument\n\nParse spec name from command argument:\n\n```bash\n# User provides: add-temporal-joins\n# Look for: ./specs/draft/add-temporal-joins/\n```\n\nIf not found, list available draft specs and ask which to promote.\n\n### Step 2: Verify Readiness\n\nBefore promoting, check:\n\n**Read `validation.yaml`** (if exists):\n- Check markers section for `status: open`\n- If open markers exist, warn user and ask to proceed anyway\n- For Initiatives: check gates section for `status: failed`\n- If failed gates exist, block promotion (require user to fix)\n\n**Read `spec.md`**:\n- Verify required sections exist\n- Note current status for update\n\nIf validation.yaml doesn't exist (e.g., Task issue type), proceed without marker checks.\n\n### Step 3: Move to Active\n\n```bash\nmkdir -p ./specs/active/\nmv ./specs/draft/{spec-name} ./specs/active/{spec-name}\n```\n\n### Step 4: Update Metadata\n\nUpdate `spec.md` frontmatter:\n\n**Change:**\n```yaml\nstatus: Draft\nstage: draft\n```\n\n**To:**\n```yaml\nstatus: Active\nstage: active\npromoted: {TODAY'S DATE}\n```\n\nIf `stage` field doesn't exist, add it. Preserve all other frontmatter fields.\n\n### Step 5: Report Success\n\nReport to user:\n```\nPromoted: {spec-name}\n\n  From: ./specs/draft/{spec-name}/\n  To:   ./specs/active/{spec-name}/\n\n  Status: Active\n  Promoted: {DATE}\n\n  [If open markers were overridden]:\n  Warning: {N} open markers remain unresolved\n\nNext steps:\n  - Run /implement to begin task execution\n  - Or use task-dispatch skill for parallel work\n```\n\n---\n\n## Readiness Checks\n\n### Marker Status Check\n\n```yaml\n# In validation.yaml\nmarkers:\n  - id: M001\n    status: open    # WARN: ask user to proceed\n  - id: M002\n    status: resolved  # OK\n```\n\nOpen markers indicate unresolved ambiguities. Warn but allow override.\n\n### Gate Status Check (Initiatives Only)\n\n```yaml\n# In validation.yaml\ngates:\n  simplicity:\n    status: failed  # BLOCK: cannot promote\n  anti_abstraction:\n    status: passed  # OK\n```\n\nFailed gates block promotion. User must resolve before promoting.\n\n---\n\n## Error Handling\n\n| Condition | Action |\n|-----------|--------|\n| Spec not found in draft | List available drafts, ask user |\n| Open markers | Warn, ask to proceed (y/n) |\n| Failed gates | Block, explain which gates failed |\n| Missing spec.md | Error: \"Invalid spec directory\" |\n| Already in active | Error: \"Spec already active\" |\n\n---\n\n## Integration\n\n**Workflow:**\n- Create: `spec-validate` -> `spec-create` (creates in draft/)\n- Review: Manual review or `/spec.clarify`\n- Promote: This skill (draft/ -> active/)\n- Execute: `task-dispatch` or `/implement`\n- Archive: `spec-archive` (active/ -> archive/)\n\n**Related:**\n- Command: `/spec.promote`\n- Skills: `spec-create` (creates drafts), `spec-archive` (archives active)\n- Skill: `spec-clarify` (resolve open markers before promoting)"
              },
              {
                "name": "spec-review",
                "description": "Multi-agent spec review with parallel Claude/OpenCode reviewers. Use after spec-create or standalone via /spec.review.",
                "path": "skills/spec-review/SKILL.md",
                "frontmatter": {
                  "name": "spec-review",
                  "description": "Multi-agent spec review with parallel Claude/OpenCode reviewers. Use after spec-create or standalone via /spec.review."
                },
                "content": "# Spec Review Skill\n\nMulti-perspective spec review using parallel subagent dispatch for comprehensive validation.\n\n> **Reference:** See [reference/roles/](reference/roles/) for reviewer personas, [reference/report-format.md](reference/report-format.md) for YAML schemas, [reference/playbook.md](reference/playbook.md) for edge case handling.\n\n---\n\n## When to Use\n\n- After `spec-create` to validate before implementation\n- When spec feels incomplete or ambiguous\n- Before `task-dispatch` for Initiatives (catches gate issues early)\n- Standalone review of existing specs\n\n---\n\n## Workflow\n\n### Step 1: Identify Spec\n\n1. Parse spec name from argument (e.g., `/spec.review auth-system`)\n2. If no argument: find most recent in `./specs/draft/`\n3. If no specs in draft: check `./specs/active/`\n4. Read spec documents: `spec.md`, `context.md`, `tasks.yaml`, `validation.yaml`\n\n### Step 2: Select Reviewers\n\nUse **AskUserQuestion** with multiSelect:\n\n```\nHeader: Reviewers\nQuestion: Which reviewers should analyze this spec?\nmultiSelect: true\nOptions:\n- claude-opus: Native subagent - comprehensive, context-aware, follows project patterns\n- opencode-gpt5.2: External perspective - fresh eyes, different reasoning patterns\n```\n\nDefault: Both selected for maximum coverage.\n\n### Step 3: Dispatch Reviewers in Parallel\n\n**CRITICAL:** Dispatch both reviewers in the same message for true parallelism.\n\n**Claude Reviewer (Task tool):**\n\n```\nTask(subagent_type=\"general-purpose\", model=\"opus\", prompt=\"\"\"\nYou are reviewing a spec for completeness and feasibility.\n\n## Spec Documents\n[Include spec.md, context.md, tasks.yaml content]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Completeness** - Are all requirements specified? Missing behaviors?\n2. **Consistency** - Do documents contradict each other? Ambiguous terms?\n3. **Feasibility** - Can tasks be implemented as described? Missing dependencies?\n4. **Clarity** - Would a fresh developer understand what to build?\n\n## Output Format\nReturn a YAML report:\n\n```yaml\nreviewer_report:\n  reviewer: claude-opus\n  gates:\n    completeness:\n      status: pass | fail\n      issues: []\n    consistency:\n      status: pass | fail\n      issues: []\n    feasibility:\n      status: pass | fail\n      issues: []\n    clarity:\n      status: pass | fail\n      issues: []\n  issues:\n    - severity: critical | high | medium\n      gate: completeness\n      area: ${TAXONOMY_AREA}\n      description: \"Clear description\"\n      suggestion: \"How to fix\"\n  clarifying_questions:\n    - area: ${TAXONOMY_AREA}\n      question: \"What needs clarification?\"\n  strengths:\n    - \"Positive observation\"\n```\n\"\"\")\n```\n\n**OpenCode Reviewer (Bash tool, background):**\n\n```bash\ntimeout 300 opencode run --model openai/gpt-5.2 --print-last \"\nYou are reviewing a spec for completeness and feasibility.\n\n## Spec Documents\n[Include spec.md, context.md, tasks.yaml content]\n\n## Review Focus\nEvaluate against these gates:\n\n1. **Completeness** - Are all requirements specified? Missing behaviors?\n2. **Consistency** - Do documents contradict each other? Ambiguous terms?\n3. **Feasibility** - Can tasks be implemented as described? Missing dependencies?\n4. **Clarity** - Would a fresh developer understand what to build?\n\n## Output Format\nReturn a YAML report:\n\n\\`\\`\\`yaml\nreviewer_report:\n  reviewer: opencode-gpt5.2\n  gates:\n    completeness:\n      status: pass | fail\n      issues: []\n    consistency:\n      status: pass | fail\n      issues: []\n    feasibility:\n      status: pass | fail\n      issues: []\n    clarity:\n      status: pass | fail\n      issues: []\n  issues:\n    - severity: critical | high | medium\n      gate: completeness\n      area: \\${TAXONOMY_AREA}\n      description: \\\"Clear description\\\"\n      suggestion: \\\"How to fix\\\"\n  clarifying_questions:\n    - area: \\${TAXONOMY_AREA}\n      question: \\\"What needs clarification?\\\"\n  strengths:\n    - \\\"Positive observation\\\"\n\\`\\`\\`\n\"\n```\n\n### Step 4: Synthesize Reviews\n\nAfter both reviewers complete:\n\n1. **Parse reports** - Extract YAML from both outputs\n2. **Merge issues:**\n   - Deduplicate by description similarity\n   - Combine issues flagged by both reviewers (higher confidence)\n   - Note which reviewer(s) found each issue\n3. **Aggregate gates:**\n   - Gate fails if EITHER reviewer fails it\n   - Record which reviewer(s) failed each gate\n4. **Prioritize questions:**\n   - Group by taxonomy area\n   - Rank: Scope > Behavior > Data Model > Constraints > Edge Cases > Integration > Terminology\n\n### Step 5: Present Review\n\n**Gate Summary Table:**\n\n```\n| Gate         | Status | Claude | OpenCode |\n|--------------|--------|--------|----------|\n| Completeness | FAIL   | fail   | pass     |\n| Consistency  | PASS   | pass   | pass     |\n| Feasibility  | FAIL   | fail   | fail     |\n| Clarity      | PASS   | pass   | pass     |\n```\n\n**Issues by Severity:**\n\n```\n## Critical (must fix before implementation)\n- [C1] Missing error handling for auth timeout (Completeness)\n  Found by: claude-opus, opencode-gpt5.2\n  Suggestion: Add error case to spec.md#edge-cases\n\n## High (should fix)\n- [H1] Task T003 depends on undefined API contract (Feasibility)\n  Found by: claude-opus\n  Suggestion: Define API in context.md or defer task\n\n## Medium (consider)\n- [M1] Term \"session\" used inconsistently (Consistency)\n  Found by: opencode-gpt5.2\n  Suggestion: Add to terminology section\n```\n\n### Step 6: Clarifying Questions\n\nUse **AskUserQuestion** with questions grouped by taxonomy area:\n\n```\nHeader: Scope\nQuestion: The spec mentions \"user roles\" but doesn't define them. What roles exist?\nmultiSelect: false\nOptions:\n- Admin/User: Two-tier system\n- Role-based: Granular permissions\n- Defer: Address later\n```\n\nRecord answers for validation.yaml update.\n\n### Step 7: Update Validation\n\nAdd clarification session to `validation.yaml`:\n\n```yaml\nclarification_sessions:\n  - id: S00${N}\n    timestamp: ${ISO_TIMESTAMP}\n    source: spec-review\n    reviewers: [claude-opus, opencode-gpt5.2]\n    questions:\n      - id: Q001\n        question: \"${QUESTION}\"\n        answer: \"${ANSWER}\"\n        area: ${TAXONOMY_AREA}\n        doc_updates:\n          - file: spec.md\n            section: ${SECTION}\n            action: modified\n```\n\nUpdate `markers` section:\n- Close resolved markers (`status: resolved`)\n- Add new markers for deferred questions (`status: open`)\n\n### Step 8: Recommend Action\n\n**All gates pass:**\n```\nReview complete. All gates passed.\n\nRecommendation: Ready for /spec.promote or /implement\n```\n\n**Issues found:**\n```\nReview complete. 2 gates failed.\n\nRecommendation:\n1. Address critical/high issues\n2. Re-run /spec.review\n```\n\n---\n\n## Gates\n\n| Gate | What It Checks |\n|------|----------------|\n| **Completeness** | All requirements specified, no missing behaviors |\n| **Consistency** | Documents align, no contradictions, terms used consistently |\n| **Feasibility** | Tasks implementable, dependencies available, no blockers |\n| **Clarity** | Unambiguous, fresh developer can understand scope |\n\n---\n\n## Edge Cases\n\n**OpenCode timeout (> 5 minutes):**\n- Continue with Claude-only results\n- Note in output: \"OpenCode review timed out, partial results\"\n- Still usable but recommend re-running with single reviewer\n\n**One reviewer fails:**\n- Parse what you can\n- Report partial results with clear indication\n- \"Claude review: complete, OpenCode review: failed to parse\"\n\n**No reviewers selected:**\n- Default to claude-opus only\n- Warn: \"Consider adding external reviewer for fresh perspective\"\n\n**Spec not found:**\n- List available specs in `./specs/draft/` and `./specs/active/`\n- Ask user to specify\n\n---\n\n## Integration\n\n**Command:** `/spec.review [spec-name]`\n\n**Related skills:**\n- `spec-create` - Creates specs to review\n- `spec-clarify` - Resolves markers found during review\n- `task-dispatch` - Next step after review passes\n\n---\n\n## Reference\n\n- [reference/roles/claude-reviewer.md](reference/roles/claude-reviewer.md) - Claude reviewer persona\n- [reference/roles/opencode-reviewer.md](reference/roles/opencode-reviewer.md) - OpenCode reviewer persona\n- [reference/report-format.md](reference/report-format.md) - YAML report schemas\n- [reference/playbook.md](reference/playbook.md) - Edge case handling"
              },
              {
                "name": "spec-update",
                "description": "Update spec documents by analyzing git history to sync task status with reality.",
                "path": "skills/spec-update/SKILL.md",
                "frontmatter": {
                  "name": "spec-update",
                  "description": "Update spec documents by analyzing git history to sync task status with reality."
                },
                "content": "# Spec Update Skill\n\nSynchronize spec documents with actual project state by analyzing git commits and working directory changes.\n\n## Command Syntax\n\n```\n/spec.update [spec-path] [--mode=status|content|full] [--context=\"user instructions\"]\n```\n\n**Arguments:**\n- `spec-path` (optional): Path to spec file. If omitted, finds most recent in `./specs/active/*/`\n- `--mode` (optional):\n  - `status` (default): Update completion status only\n  - `content`: Update spec structure based on learnings\n  - `full`: Both status and content updates\n- `--context` (optional): Manual overrides/clarifications\n\n## Core Workflow\n\n### Step 1: Locate and Parse Spec\n\n1. **Find spec file:**\n   - If path provided: use that\n   - Else: search `./specs/active/**/spec.md` for most recent\n\n2. **Parse structure:**\n   - Identify task format (checkboxes, numbered lists, sections)\n   - Extract tasks with current status\n   - Preserve formatting\n\n3. **Determine baseline:**\n   - Use file creation time or first commit mentioning spec\n\n### Step 2: Analyze Current State\n\nRun git commands to gather evidence:\n\n```bash\n# Commits since plan creation\ngit log --oneline --since=\"<plan-creation-time>\" --all\ngit log --stat --since=\"<plan-creation-time>\" --all\n\n# Current state\ngit status --short\ngit branch -vv\n\n# Files changed since baseline\ngit diff <baseline>..HEAD --name-status\n```\n\n**Collect:**\n- Commits that map to plan tasks\n- Files created/modified/deleted\n- Working directory changes\n- Branch/sync status\n\n### Step 2.5: Sync TodoWrite to tasks.yaml (if active)\n\nIf TodoWrite has entries matching spec tasks:\n\n1. For each \"completed\" todo, update corresponding task in tasks.yaml to `status: completed`\n2. For each \"in_progress\" todo, update to `status: in_progress`\n3. Update `meta.last_updated` and `meta.progress` fields\n\nThis catches any completions that weren't synced immediately during task execution.\n\n### Step 3: Map Evidence to Tasks\n\nFor each task:\n\n1. **Search for evidence:**\n   - Commit messages mentioning task\n   - Files mentioned in task were modified\n   - Tests exist if task mentions testing\n   - Dependencies met\n\n2. **Determine status:**\n   - `completed`: Clear evidence in commits + files exist\n   - `in_progress`: Working directory changes or partial completion\n   - `pending`: No evidence\n   - `blocked`: Explicit context or dependencies not met (add to task's `blocked_by` field)\n\n3. **Collect evidence notes:**\n   - Which commits\n   - Which files changed\n   - Test/build results\n\n### Step 4: Update tasks.yaml\n\n**Status mode** (`--mode=status`):\n\nUpdate task statuses and add evidence:\n\n```yaml\ntasks:\n  - id: PROJ-001\n    content: Set up project structure\n    status: completed\n    active_form: Setting up project structure\n    evidence:\n      commits: [c228fea, 2f069d7]\n      files: [src/feature_link/temporal.py, tests/test_errata_example.py]\n\n  - id: PROJ-002\n    content: Implement core logic\n    status: in_progress\n    active_form: Implementing core logic\n    evidence:\n      notes: \"3 files changed in working directory\"\n\n  - id: PROJ-003\n    content: Write integration tests\n    status: pending\n    active_form: Writing integration tests\n\nmeta:\n  last_updated: 2025-12-17\n  progress: 1/3\n```\n\n**Content mode** (`--mode=content`):\n\nAlso update structure:\n- Add new tasks via dignity's `add_task()` function\n- Update task content/active_form via `update_task()`\n- Remove obsolete tasks via `discard_task()`\n- Update phases and checkpoints as needed\n\n**User context**: If `--context` provided, apply manual overrides (takes precedence over auto-detection).\n\n### Step 5: Present Summary\n\n```\n## Spec Update Summary\n\nSpec: ./specs/active/refactor/\nTasks: tasks.yaml (progress: 5/10)\nBaseline: c228fea (2025-11-06)\n\nStatus:\n  ✓ Completed: 5 tasks\n  • In Progress: 2 tasks\n  ○ Pending: 3 tasks\n\nRecent Activity:\n  - 8 commits since plan creation\n  - 12 files modified\n\nNext actions:\n  1. REFAC-006: Implement validation (ready)\n  2. REFAC-007: Add error handling (ready)\n```\n\n---\n\n## Matching Heuristics\n\n**Strong evidence (auto-mark complete):**\n- Commit message explicitly references task\n- Commit modifies exact files mentioned\n- All acceptance criteria met\n\n**Weak evidence (mark in-progress):**\n- Commit touches related files\n- Working directory has related changes\n- Partial completion of multi-step task\n\n**Conservative approach:** When uncertain, prefer in-progress over completed\n\n## Best Practices\n\n### Preserve Structure\n- Keep all non-task content unchanged\n- Maintain indentation and formatting\n- Append evidence (don't remove existing notes)\n- Don't remove user comments\n\n### Handle tasks.yaml Structure\n- Core fields: spec, code, next_id, tasks\n- Task statuses: pending, in_progress, completed\n- Optional: phases with checkpoints\n- Optional: evidence (commits, files, notes)\n\n## Examples\n\n```bash\n# Update most recent spec status\n/spec.update\n\n# Full update with content changes\n/spec.update --mode=full\n\n# Manual override for blocker\n/spec.update --context=\"Task 5 blocked waiting for API docs\"\n\n# Specific spec\n/spec.update ./specs/active/auth/spec.md --mode=full\n```\n\n---\n\n## Integration\n\n- Created by `/spec.create`\n- Updated regularly as work progresses\n- Provides visibility into completion status\n- Informs `/spec.archive` decision\n\n**Best practice:** Run at end of each work session to keep synchronized with reality."
              },
              {
                "name": "spec-validate",
                "description": "Validation loop for speccing. Clarifies requirements through structured questioning before document creation.",
                "path": "skills/spec-validate/SKILL.md",
                "frontmatter": {
                  "name": "spec-validate",
                  "description": "Validation loop for speccing. Clarifies requirements through structured questioning before document creation."
                },
                "content": "# Spec Validate Skill\n\nValidate requirements through structured clarification. Produces validation data for spec-create.\n\n> **Reference:** See [reference/issue-types.md](reference/issue-types.md) for type definitions, [reference/question-taxonomy.md](reference/question-taxonomy.md) for question templates, and [reference/sdd-gates.md](reference/sdd-gates.md) for pre-implementation gate definitions.\n\n---\n\n## When to Use\n\n**Use for:**\n- New features before implementation\n- Unclear requirements needing refinement\n- Complex changes needing design exploration\n- When multiple approaches seem viable\n\n**Don't use for:**\n- Simple bug fixes\n- Well-specified changes\n- Clear mechanical tasks (use Claude's native planning)\n\n---\n\n## Workflow\n\n### Step 0: Check Native Plan Context\n\nBefore starting validation, check if the user has existing context from Claude's native `/plan`:\n\n1. **Check for context:** Ask if `/plan` was used or if there's existing plan context\n2. **If present:** Extract key elements:\n   - Goal/objective → Seeds **Scope** taxonomy area\n   - Approach/strategy → Seeds **Integration/Architecture** areas\n   - Open questions → Become priority clarification targets\n3. **If absent:** Proceed directly to Step 1\n\nThis step bridges native Claude planning with structured validation.\n\n### Step 0.5: Constitution Check (Initiatives Only)\n\nFor Initiative-type work, verify alignment with constitution before proceeding:\n\n1. Read `.claude/constitution.md`\n2. Check if proposed work aligns with core principles\n3. If conflict detected:\n   - Flag the conflict\n   - Ask user to resolve or justify exception\n   - Document exception in validation data\n\n**Skip for:** Features, Tasks (constitution checked during task-dispatch for Initiatives)\n\n### Step 1: Issue Type Selection\n\n**FIRST QUESTION (Always)** - Use AskUserQuestion:\n\n```\nHeader: Work type\nQuestion: What type of work is this?\nmultiSelect: false\nOptions:\n- Initiative: Strategic coordination (months) - Multiple features toward business goal\n- Feature: User-facing capability (weeks) - Deliverable value, multiple tasks\n- Task: Implementation item (days) - Single concrete deliverable\n- Exploratory: Not sure yet - Gather context first, then classify\n```\n\nThis selection determines:\n- **Question limit:** Tasks get 3, Features/Initiatives get 5\n- **Taxonomy areas:** Tasks get minimal (3), Features/Initiatives get full (7)\n\n**If Exploratory:** Gather context, ask 3 questions to understand scope, present classification recommendation, restart with correct type.\n\n### Step 2: Gather Context\n\n1. Examine relevant files, docs, recent commits\n2. Understand existing patterns and constraints\n3. Initialize taxonomy tracking based on issue type\n\n**Taxonomy by type:**\n\n| Type | Areas to Cover |\n|------|----------------|\n| Initiative | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Feature | Scope, Behavior, Data Model, Constraints, Edge Cases, Integration, Terminology |\n| Task | Scope, Behavior, Integration |\n\n### Step 2.5: Ambiguity Scan\n\nAutomatically scan gathered context for specification gaps across taxonomy areas.\n\n**Process:**\n\n1. For each taxonomy area (based on issue type), evaluate:\n   - **clear**: Fully specified, no questions needed\n   - **partial**: Some information present, gaps remain\n   - **missing**: Not addressed at all\n\n2. Populate `ambiguity_scan` section in validation data:\n   ```yaml\n   ambiguity_scan:\n     scope:\n       status: clear | partial | missing\n       gaps: [\"gap description if partial/missing\"]\n     behavior:\n       status: clear | partial | missing\n       gaps: []\n     # ... remaining areas\n   ```\n\n3. Route based on scan results:\n   - **No gaps (all clear):** Proceed silently to Step 4 (skip validation loop)\n   - **Gaps found:** Areas with `partial` or `missing` status become priority candidates for clarification questions in Step 3\n\n**Evaluation criteria per area:**\n\n| Area | Clear | Partial | Missing |\n|------|-------|---------|---------|\n| Scope | Goals, boundaries, success criteria defined | Some elements unclear | No scope information |\n| Behavior | User flows, system responses specified | Some paths undefined | No behavior described |\n| Data Model | Entities, relationships, formats clear | Schema gaps exist | No data model |\n| Constraints | Performance, security, compatibility stated | Some constraints unclear | No constraints |\n| Edge Cases | Error handling, limits documented | Some cases unaddressed | No edge cases |\n| Integration | Dependencies, APIs, interfaces identified | Some touchpoints unclear | No integration info |\n| Terminology | Domain terms defined consistently | Some ambiguous terms | No definitions |\n\n### Step 3: Validation Loop\n\nAsk clarifying questions in taxonomy-based batches, with re-evaluation between rounds.\n\n**Process:**\n1. Identify uncovered taxonomy areas\n2. Prioritize areas by (Impact × Uncertainty)\n3. Group questions by taxonomy area into batches\n4. Use AskUserQuestion with multiple questions from the same area\n5. Receive answers\n6. Re-evaluate remaining questions for relevance (skip questions invalidated by answers)\n7. Update taxonomy coverage\n8. Repeat with next taxonomy area until limit reached or all Primary areas covered\n\n**Batch format:**\n\n```\nAskUserQuestion with questions array (1-4 questions per batch):\n\nQuestion 1:\n  Header: [Area, max 12 chars]\n  Question: [Clear question ending with ?]\n  multiSelect: false\n  Options: [2-4 options with implications]\n\nQuestion 2:\n  Header: [Same area]\n  Question: [Related question]\n  ...\n```\n\n**Single question format (when only one question in area):**\n\n```\nHeader: [Area, max 12 chars]\nQuestion: [Clear question ending with ?]\nmultiSelect: false\nOptions:\n- Option A: [choice] - [implication]\n- Option B: [choice] - [implication]\n- Option C: [choice] - [implication]\n- None: [default/skip]\n```\n\n**Batching rules:**\n- One batch per taxonomy area (questions grouped by area)\n- Multiple trigger fields within a batch combine with OR (any match)\n\n**Batch counts:**\n- Tasks: up to 3 batches (Scope, Behavior, Integration)\n- Features/Initiatives: up to 7 batches (all taxonomy areas)\n- Plus: additional clarification batches if \"Other\" answers need follow-up\n\n**Batch size limits:**\n- Tasks: up to 3 questions per batch\n- Features/Initiatives: up to 4 questions per batch (AskUserQuestion max)\n\n**Re-evaluation between batches:**\n- After receiving answers, check if pending questions are still relevant\n- Skip questions invalidated by previous answers\n- If user provides ambiguous \"Other\" answer spanning areas, add follow-up to next batch\n\n### Step 3.5: Initiative-Specific Validation (Initiatives Only)\n\nFor Initiatives, ask about user story prioritization and implementation strategy:\n\n```\nHeader: User Stories\nQuestion: How should user stories be prioritized?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): P1 stories deliver standalone value, P2/P3 are incremental\n- Parallel Tracks: Stories can be developed independently by different teams\n- Sequential: Stories have strict dependencies, must complete in order\n```\n\n```\nHeader: Strategy\nQuestion: What implementation approach fits best?\nmultiSelect: false\nOptions:\n- MVP First (Recommended): Ship P1, iterate on P2/P3 based on feedback\n- Incremental: Each phase adds value, all planned upfront\n- Parallel Team: Multiple workstreams, integration points defined\n```\n\nTrack selections in validation data for spec-create (populates Implementation Strategy section).\n\n### Step 3.6: SDD Section Opt-ins (Features Only)\n\nFor Features, offer opt-in for detailed SDD sections:\n\n```\nHeader: SDD sections\nQuestion: Which detailed sections do you want in the spec?\nmultiSelect: true\nOptions:\n- Tech Decisions: Document technology choices and rationale\n- API Contract: Define API endpoints and schemas\n- Data Model: Document entities and relationships\n- None: Keep spec lightweight\n```\n\nTrack selections in validation data for spec-create.\n\n### Step 4: Propose Approaches\n\nUse AskUserQuestion to present options:\n\n```\nHeader: Approach\nQuestion: Which approach should we take?\nmultiSelect: false\nOptions:\n- Approach A: [brief] - Trade-off: [X]\n- Approach B: [brief] - Trade-off: [Y] (Recommended)\n- Approach C: [brief] - Trade-off: [Z]\n```\n\nLead with your recommendation. Apply YAGNI ruthlessly.\n\n### Step 5: Generate Validation Data\n\nCompile validation data for spec-create:\n\n- **Issue type:** Selected in Step 1\n- **Ambiguity scan:** Status per area (clear / partial / missing) with gap descriptions\n- **Questions used:** N / limit\n- **Taxonomy coverage:** Status per area (Covered / Gap / N/A)\n- **Clarification log:** Question → Answer → Integration point\n- **Approach selected:** From Step 4\n- **SDD opt-ins:** Which sections selected (Features only)\n- **Gates status:** For Initiatives: run gate checks; for Features/Tasks: mark n/a\n- **Markers:** Any unresolved items identified during validation\n\nThis data passes to spec-create for validation.yaml.\n\n---\n\n## Key Principles\n\n| Principle | Why |\n|-----------|-----|\n| **Issue type first** | Branches workflow, sets question limit |\n| **Ambiguity scan** | Identifies gaps early, skips validation if all clear |\n| **MultiSelect always** | Structured options, faster iteration |\n| **Question limits** | Forces prioritization |\n| **Taxonomy tracking** | Ensures coverage of important areas |\n| **YAGNI ruthlessly** | Remove unnecessary features |\n\n---\n\n## Output\n\nThis skill produces **validation data**, not documents. The data flows to `spec-create` which generates:\n- spec.md, context.md, tasks.md, dependencies.yaml, validation.yaml\n\n---\n\n## Integration\n\n**Invoked by:** `/spec.create` command (default behavior)\n\n**Standalone use:** Can be invoked directly via `spec-validate` skill for validation without document creation."
              },
              {
                "name": "task-completion-verify",
                "description": "Evidence-based completion claims. Use before claiming work is complete, fixed, or passing - requires running verification commands and confirming output before any success claims.",
                "path": "skills/task-completion-verify/SKILL.md",
                "frontmatter": {
                  "name": "task-completion-verify",
                  "description": "Evidence-based completion claims. Use before claiming work is complete, fixed, or passing - requires running verification commands and confirming output before any success claims."
                },
                "content": "# Verification Before Completion\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n---\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n---\n\n## The Gate Function\n\n```\nBEFORE claiming any status:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n---\n\n## Common Verification Requirements\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check |\n| Build succeeds | Build command: exit 0 | Linter passing |\n| Bug fixed | Test symptom: passes | Code changed |\n| Regression test | Red-green verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n---\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- ANY wording implying success without running verification\n\n---\n\n## Key Patterns\n\n**Tests:**\n```\nDO:   [Run test] [See: 34/34 pass] \"All tests pass\"\nDON'T: \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\nDO:   Write -> Run (pass) -> Revert fix -> Run (MUST FAIL) -> Restore -> Run (pass)\nDON'T: \"I've written a regression test\" (without red-green)\n```\n\n**Build:**\n```\nDO:   [Run build] [See: exit 0] \"Build passes\"\nDON'T: \"Linter passed\" (linter != compiler)\n```\n\n**Requirements:**\n```\nDO:   Re-read plan -> Create checklist -> Verify each -> Report\nDON'T: \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\nDO:   Agent reports -> Check VCS diff -> Verify changes -> Report\nDON'T: Trust agent report\n```\n\n---\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence != evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter != compiler |\n| \"Agent said success\" | Verify independently |\n| \"Partial check is enough\" | Partial proves nothing |\n\n---\n\n## When to Apply\n\n**ALWAYS before:**\n- ANY success/completion claims\n- ANY expression of satisfaction\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n---\n\n## Integration\n\n**Use with:**\n- `code-test` - Run tests before claiming they pass\n- `code-debug` - Verify fix before claiming bug resolved\n- `task-dispatch` - Verify each task before marking complete\n- `git-worktree-use` - Verify baseline before and after"
              },
              {
                "name": "task-dispatch",
                "description": "Subagent-driven task execution with TDD workflow. Dispatches tester subagent (writes failing tests) then implementer subagent (makes tests pass), with batch review.",
                "path": "skills/task-dispatch/SKILL.md",
                "frontmatter": {
                  "name": "task-dispatch",
                  "description": "Subagent-driven task execution with TDD workflow. Dispatches tester subagent (writes failing tests) then implementer subagent (makes tests pass), with batch review."
                },
                "content": "# Subagent-Driven Task Execution\n\nExecute specs with proper TDD: tester writes failing tests, implementer makes them pass.\n\n**Core principle:** Separate test-writing from implementation. Fresh subagents + opus model + skill activation = high quality.\n\n---\n\n## When to Use\n\n**Use when:**\n- Executing an implementation spec (created with `spec-create`)\n- Tasks are mostly independent\n- Want TDD enforcement with quality gates\n\n**Don't use when:**\n- No spec exists yet (use `spec-validate` → `spec-create` first)\n- Tasks are tightly coupled (manual execution better)\n- Single small task (just do it directly)\n- Initiative spec has failed gates (resolve first via /spec.clarify)\n\n---\n\n## The Process\n\n### 1. Load Spec and Populate TodoWrite\n\n1. Find most recent spec in `./specs/active/*/`\n2. Read `tasks.yaml` from that directory\n3. Parse tasks with `status: pending` or `status: in_progress`\n4. Create TodoWrite with ALL uncompleted tasks:\n   - First uncompleted task: \"in_progress\"\n   - Others: \"pending\"\n   - content: task text\n   - activeForm: present continuous form\n\n**CRITICAL:** Always populate TodoWrite before dispatching any subagents.\n\n### 1.5. Pre-Implementation Gate Check\n\nBefore dispatching any tasks, verify validation.yaml gates:\n\n1. Read `validation.yaml` from spec directory\n2. Check `metadata.issue_type`\n3. **If Initiative:**\n   - Check all gates in `gates` section\n   - If any gate has `status: failed`:\n     - Report which gates failed with reasons\n     - Prompt: \"Resolve via /spec.clarify or proceed anyway?\"\n     - If user chooses to proceed: document override in validation.yaml\n   - Check `markers` section for `status: open`\n   - If blocking markers exist:\n     - Report marker count and summaries\n     - Prompt: \"Resolve markers first or proceed?\"\n4. **If Feature/Task:** Skip gate check (gates marked n/a)\n\n**Gate check failure response:**\n\n```\nPre-implementation gate check failed:\n\nGates:\n- ❌ Simplicity: [reason from validation.yaml]\n- ✓ Anti-Abstraction: passed\n- ❌ Integration-First: [reason]\n\nOpen Markers: 3\n- M001 (Constraints): Authentication method not specified\n- M002 (Edge Cases): Error handling for timeout\n- M003 (Integration): External API contract undefined\n\nOptions:\n1. Run /spec.clarify to resolve\n2. Proceed anyway (document override)\n3. Abort\n```\n\n### 2. Analyze Task Dependencies\n\nParse `dependencies.yaml` to identify execution batches:\n\n**Dependency rules:**\n- Tasks in Phase N depend on Phase N-1 completion\n- Tasks with `[P]` marker AND different file paths can run in parallel\n- Tasks with same file path must run sequentially\n- Phase boundaries force batch breaks\n\n### 3. Execute Tasks (Two-Phase TDD)\n\n**Per task, dispatch TWO subagents:**\n\n```\nPhase A: TESTER (opus)\n├── Invokes code-test skill\n├── Writes failing tests (RED)\n└── Reports: test paths, failure output\n\nPhase B: IMPLEMENTER (opus)\n├── Receives test paths from tester\n├── Invokes code-implement skill\n├── Makes tests pass (GREEN)\n└── Reports: impl files, test pass output\n```\n\n**Key constraint:** Implementer for task X needs tester X's report. But testers for different tasks are independent.\n\n**For single task:**\n1. Dispatch tester subagent (opus) → wait for completion\n2. Dispatch implementer subagent (opus) with tester's report → wait for completion\n\n**For parallel batch (N independent tasks):**\n1. Dispatch N tester subagents in single message → wait for ALL testers\n2. Dispatch N implementer subagents in single message → wait for ALL implementers\n\nEach implementer receives its corresponding tester's report. This maximizes parallelism:\n- All testers run concurrently (different test files)\n- All implementers run concurrently (different impl files)\n\n> **Reference:** See [reference/subagent-workflow.md](reference/subagent-workflow.md) for dispatch templates.\n\n### 4. Handle Tester Gaps\n\nIf tester reports `status: gap` (cannot write meaningful tests):\n1. Read the gap_reason from tester's YAML report\n2. Consult the spec for clarification\n3. If still unclear, use AskUserQuestion\n4. Re-dispatch tester with clarified requirements\n\n### 5. Review Batch Work\n\nAfter ALL implementers in a batch complete, dispatch single reviewer:\n\n- Reviews all changes from the batch together\n- Checks against spec requirements\n- Identifies issues by severity:\n  - **Critical** - Blocks progress, must fix immediately\n  - **Important** - Fix before next batch\n  - **Minor** - Note for later\n\n### 6. Apply Review Feedback\n\n**If issues found:**\n- Fix Critical issues immediately (dispatch fix subagent)\n- Fix Important issues before next batch\n- Note Minor issues\n\n### 7. Mark Complete and Sync to tasks.yaml\n\nWhen marking a task complete:\n\n1. Update TodoWrite (mark as \"completed\")\n2. Edit tasks.yaml: Change `status: in_progress` to `status: completed`\n3. Move to next task (mark as \"in_progress\")\n\n### 8. Final Review\n\nAfter all tasks complete:\n- Review entire implementation\n- Check all spec requirements met\n- Validate overall architecture\n\n---\n\n## Subagent Configuration\n\n| Role | Subagent Type | Model | Skill |\n|------|---------------|-------|-------|\n| Tester | task-tester | opus | code-test |\n| Implementer | task-implementer | opus | code-implement |\n| Reviewer | task-reviewer | opus | code-review |\n\n**CRITICAL:** Always specify `model: opus` in Task tool calls.\n\n---\n\n## Quality Gates\n\n| Gate | When | Action if Failed |\n|------|------|------------------|\n| Pre-impl gate | Before any dispatch | Block if Initiative gates failed |\n| RED verification | After tester | Verify tests actually fail |\n| GREEN verification | After implementer | Verify tests pass |\n| Batch review | After all implementers | Fix before next batch |\n| Final review | After all tasks | Address gaps |\n\n---\n\n## Red Flags\n\n**Never:**\n- Skip the tester phase (implementer must receive failing tests)\n- Use sonnet for subagents (always opus)\n- Skip review between batches\n- Dispatch parallel subagents on same file\n- Let implementer write tests (tester's job)\n- Ignore failed pre-impl gates for Initiatives (gates exist for a reason)\n\n**If tester can't write tests:**\n- Don't skip to implementer\n- Handle the gap (consult spec, ask user)\n- Re-dispatch tester with clarification\n\n---\n\n## Example Workflow\n\n```\n[Load spec, create TodoWrite]\n\nTask 1: Add caching\n[Dispatch tester (opus)]\nTester: Wrote 3 tests, all failing (RED)\n  - test_cache_hit, test_cache_miss, test_ttl_expiry\n  - Files: tests/test_cache.py\n[Dispatch implementer (opus) with test paths]\nImplementer: Made tests pass (GREEN)\n  - Files: src/cache.py\n[Review - no issues]\n[Mark Task 1 complete]\n\nTask 2, 3, 4: [P] parallel batch\n[Dispatch 3 testers in single message]\nAll testers complete with failing tests\n[Dispatch 3 implementers in single message]\nAll implementers complete, tests passing\n[Single review for entire batch]\n[Mark Tasks 2, 3, 4 complete]\n\n...\n\n[Final review]\nAll requirements met\n```\n\n---\n\n## Integration\n\n**Use with:**\n- `spec-validate` → `spec-create` - Create spec before dispatch\n- `spec-clarify` - Resolve markers/gates before dispatch\n- `code-test` - Tester invokes for TDD methodology\n- `code-implement` - Implementer invokes for language guidelines\n- `code-review` - Reviewer invokes for review methodology\n- `task-completion-verify` - Verify before claiming done\n\n---\n\n## Reference\n\n- [subagent-workflow.md](reference/subagent-workflow.md) - Dispatch templates and YAML reports\n- [report-format.md](reference/report-format.md) - YAML report schemas\n- [roles/tester.md](reference/roles/tester.md) - Test-writing subagent\n- [roles/implementer.md](reference/roles/implementer.md) - Implementation subagent\n- [roles/reviewer.md](reference/roles/reviewer.md) - Review subagent"
              }
            ]
          }
        ]
      }
    }
  ]
}