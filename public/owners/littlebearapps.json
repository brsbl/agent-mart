{
  "owner": {
    "id": "littlebearapps",
    "display_name": "Little Bear Apps",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/220834681?v=4",
    "url": "https://github.com/littlebearapps",
    "bio": "Simple tools. Serious privacy.",
    "stats": {
      "total_repos": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 10,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "littlebearapps/cloudflare-engineer",
      "url": "https://github.com/littlebearapps/cloudflare-engineer",
      "description": "Claude Code plugin: Senior Cloudflare Systems Engineer capabilities for architecture, cost optimization, security, and implementation",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-08T09:31:44Z",
        "created_at": "2026-01-07T07:04:42Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1111
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 901
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE/bug_report.md",
          "type": "blob",
          "size": 1000
        },
        {
          "path": ".github/ISSUE_TEMPLATE/config.yml",
          "type": "blob",
          "size": 28
        },
        {
          "path": ".github/ISSUE_TEMPLATE/feature_request.md",
          "type": "blob",
          "size": 1055
        },
        {
          "path": ".github/PULL_REQUEST_TEMPLATE.md",
          "type": "blob",
          "size": 1178
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/claude.yml",
          "type": "blob",
          "size": 1812
        },
        {
          "path": ".github/workflows/release.yml",
          "type": "blob",
          "size": 1209
        },
        {
          "path": ".github/workflows/validate.yml",
          "type": "blob",
          "size": 1626
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 35
        },
        {
          "path": ".markdownlint.json",
          "type": "blob",
          "size": 149
        },
        {
          "path": "CHANGELOG.md",
          "type": "blob",
          "size": 4224
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 6410
        },
        {
          "path": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3190
        },
        {
          "path": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7630
        },
        {
          "path": "COST_SENSITIVE_RESOURCES.md",
          "type": "blob",
          "size": 11920
        },
        {
          "path": "LICENSE",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 12477
        },
        {
          "path": "SECURITY.md",
          "type": "blob",
          "size": 1905
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/architecture-reviewer.md",
          "type": "blob",
          "size": 11048
        },
        {
          "path": "agents/cost-analyzer.md",
          "type": "blob",
          "size": 9736
        },
        {
          "path": "agents/security-auditor.md",
          "type": "blob",
          "size": 8453
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/cf-audit.md",
          "type": "blob",
          "size": 11746
        },
        {
          "path": "commands/cf-costs.md",
          "type": "blob",
          "size": 8089
        },
        {
          "path": "commands/cf-design.md",
          "type": "blob",
          "size": 6018
        },
        {
          "path": "commands/cf-pattern.md",
          "type": "blob",
          "size": 7289
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 398
        },
        {
          "path": "hooks/pre-deploy-check.py",
          "type": "blob",
          "size": 17118
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/architect/SKILL.md",
          "type": "blob",
          "size": 15238
        },
        {
          "path": "skills/custom-hostnames",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/custom-hostnames/SKILL.md",
          "type": "blob",
          "size": 11800
        },
        {
          "path": "skills/guardian",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/guardian/SKILL.md",
          "type": "blob",
          "size": 13454
        },
        {
          "path": "skills/implement",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/implement/SKILL.md",
          "type": "blob",
          "size": 14548
        },
        {
          "path": "skills/media-streaming",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/media-streaming/SKILL.md",
          "type": "blob",
          "size": 16741
        },
        {
          "path": "skills/optimize-costs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/optimize-costs/SKILL.md",
          "type": "blob",
          "size": 6071
        },
        {
          "path": "skills/patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/patterns/SKILL.md",
          "type": "blob",
          "size": 4941
        },
        {
          "path": "skills/patterns/circuit-breaker.md",
          "type": "blob",
          "size": 12937
        },
        {
          "path": "skills/patterns/d1-batching.md",
          "type": "blob",
          "size": 8744
        },
        {
          "path": "skills/patterns/service-bindings.md",
          "type": "blob",
          "size": 9191
        },
        {
          "path": "skills/probes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/probes/SKILL.md",
          "type": "blob",
          "size": 10665
        },
        {
          "path": "skills/scale",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/scale/SKILL.md",
          "type": "blob",
          "size": 12015
        },
        {
          "path": "skills/zero-trust",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/zero-trust/SKILL.md",
          "type": "blob",
          "size": 8212
        }
      ],
      "marketplace": {
        "name": "littlebearapps-cloudflare",
        "version": null,
        "description": null,
        "owner_info": {
          "name": "Little Bear Apps",
          "email": "hello@littlebearapps.com"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "cloudflare-engineer",
            "description": "Senior Cloudflare Systems Engineer capabilities - architecture design, cost optimization, security auditing, and implementation patterns",
            "source": "./",
            "category": "cloud-infrastructure",
            "version": "1.1.0",
            "author": {
              "name": "Little Bear Apps",
              "email": "hello@littlebearapps.com"
            },
            "install_commands": [
              "/plugin marketplace add littlebearapps/cloudflare-engineer",
              "/plugin install cloudflare-engineer@littlebearapps-cloudflare"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-08T09:31:44Z",
              "created_at": "2026-01-07T07:04:42Z",
              "license": "MIT"
            },
            "commands": [],
            "skills": [
              {
                "name": "architect",
                "description": "Design industry-grade Cloudflare architectures with wrangler.toml generation, Mermaid diagrams, AND Edge-Native Constraint validation. Use this skill when designing new systems, planning migrations, evaluating architecture options, or when the user mentions Node.js libraries that may not work on Workers.",
                "path": "skills/architect/SKILL.md",
                "frontmatter": {
                  "name": "architect",
                  "description": "Design industry-grade Cloudflare architectures with wrangler.toml generation, Mermaid diagrams, AND Edge-Native Constraint validation. Use this skill when designing new systems, planning migrations, evaluating architecture options, or when the user mentions Node.js libraries that may not work on Workers."
                },
                "content": "# Cloudflare Architect Skill\n\nDesign production-ready Cloudflare architectures with proper service selection, wrangler configuration generation, visual diagrams, **and Edge-Native Constraint enforcement**.\n\n## Core Capabilities\n\n### 1. Architecture Design\n- Service selection based on use case requirements\n- Data flow design between components\n- Scalability and cost trade-off analysis\n- Security boundary definition\n\n### 2. Configuration Generation\n- Complete wrangler.toml/wrangler.jsonc files\n- Environment-specific configurations (dev/staging/prod)\n- Binding declarations with proper naming\n- Trigger configuration (routes, crons, queues)\n\n### 3. Visual Documentation\n- Mermaid architecture diagrams\n- Data flow sequence diagrams\n- Component relationship graphs\n\n### 4. Edge-Native Constraint Validation (NEW)\n- Cross-reference proposed code against Workers runtime compatibility\n- Flag non-standard Node.js libraries\n- Suggest `node:` polyfills or Cloudflare alternatives\n- Verify compatibility flags in wrangler config\n\n### 5. Wrangler Health Check\n\nBefore designing or deploying, verify the local wrangler installation is current.\n\n**Check Command**:\n```bash\nnpx wrangler --version\n```\n\n**Version Advisory Table** (as of 2026-01):\n\n| Installed Version | Status | Recommendation |\n|-------------------|--------|----------------|\n| 3.100+ | Current | Good to go |\n| 3.80-3.99 | Acceptable | Update when convenient |\n| 3.50-3.79 | Outdated | Update recommended: `npm install -g wrangler@latest` |\n| <3.50 | Critical | Update required - missing important features |\n\n**Key Version Features**:\n- 3.100+: Enhanced D1 Time Travel, improved `wrangler dev`\n- 3.80+: Vectorize GA support, better error messages\n- 3.60+: Queues improvements, better observability\n- 3.50+: nodejs_compat_v2 flag support\n\n**Wrangler Health Check Output**:\n```markdown\n## Wrangler Health Check\n\n**Installed**: wrangler 3.95.0\n**Status**: ✅ Acceptable (current is 3.102.0)\n**Recommendation**: Update when convenient for latest features\n\nTo update: `npm install -g wrangler@latest`\n```\n\n**When to Run**:\n- Before `/cf-design` architecture sessions\n- Before first deployment to new environment\n- When encountering unexpected wrangler errors\n\n## Service Selection Matrix\n\n### Storage Selection\n\n| Need | Service | Limits | Cost |\n|------|---------|--------|------|\n| Relational queries | D1 | 10GB, 128MB memory | $0.25/B reads, $1/M writes |\n| Key-value lookups | KV | 25MB/value, 1 write/sec/key | $0.50/M reads, $5/M writes |\n| Large files/blobs | R2 | 5TB/object | $0.36/M reads, $4.50/M writes |\n| Coordination/locks | Durable Objects | Per-object isolation | CPU time based |\n| Time-series metrics | Analytics Engine | Adaptive sampling | FREE |\n| Vector similarity | Vectorize | 1536 dims, 5M vectors | $0.01/M queries |\n\n### Compute Selection\n\n| Need | Service | Limits | Best For |\n|------|---------|--------|----------|\n| HTTP handlers | Workers | 128MB, 30s/req | API endpoints |\n| Background jobs | Queues | 128KB/msg, batches ≤100 | Async processing |\n| Long-running tasks | Workflows | 1024 steps, 1GB state | Multi-step pipelines |\n| Stateful coordination | Durable Objects | Per-object | Sessions, locks |\n| Scheduled jobs | Cron Triggers | 1-minute minimum | Periodic tasks |\n\n### AI/ML Selection\n\n| Need | Service | Cost | Best For |\n|------|---------|------|----------|\n| LLM inference | Workers AI | $0.011/1K neurons | Serverless AI |\n| LLM caching/logging | AI Gateway | Free tier + $0.10/M | Production AI |\n| Embeddings + search | Vectorize | Per-dimension | RAG, semantic search |\n\n## Edge-Native Constraints\n\n**IMPORTANT**: Cloudflare Workers use a V8 isolate runtime, NOT Node.js. Always validate proposed code against these constraints.\n\n### Node.js API Compatibility\n\nWorkers supports many Node.js APIs via `node:` prefixed imports when `nodejs_compat` or `nodejs_compat_v2` flag is enabled.\n\n| Node.js Module | Workers Support | Required Flag | Notes |\n|----------------|-----------------|---------------|-------|\n| `node:assert` | Partial | `nodejs_compat` | Basic assertions |\n| `node:async_hooks` | Yes | `nodejs_compat` | AsyncLocalStorage supported |\n| `node:buffer` | Yes | `nodejs_compat` | Full Buffer API |\n| `node:crypto` | Yes | `nodejs_compat` | Prefer Web Crypto when possible |\n| `node:events` | Yes | `nodejs_compat` | EventEmitter |\n| `node:path` | Yes | `nodejs_compat` | Path manipulation |\n| `node:stream` | Partial | `nodejs_compat` | Web Streams preferred |\n| `node:url` | Yes | `nodejs_compat` | URL parsing |\n| `node:util` | Partial | `nodejs_compat` | Common utilities |\n| `node:zlib` | Yes | `nodejs_compat` | Compression |\n| `node:fs` | NO | - | **Use R2 instead** |\n| `node:net` | NO | - | **Use TCP sockets (beta)** |\n| `node:child_process` | NO | - | **Not available** |\n| `node:cluster` | NO | - | **Edge inherently distributed** |\n| `node:http` | NO | - | **Use fetch()** |\n| `node:https` | NO | - | **Use fetch()** |\n\n### Common Library Compatibility\n\n| Library | Works? | Alternative | Notes |\n|---------|--------|-------------|-------|\n| `express` | NO | **Hono, itty-router** | Hono is recommended |\n| `axios` | Partial | **fetch()** | Native fetch preferred |\n| `lodash` | Yes | - | Works, but increases bundle |\n| `moment` | Yes | **dayjs, date-fns** | Moment is heavy |\n| `uuid` | Yes | **crypto.randomUUID()** | Native is better |\n| `bcrypt` | NO | **bcryptjs** | Pure JS version works |\n| `sharp` | NO | **Cloudflare Images** | Use Images API |\n| `puppeteer` | NO | **Browser Rendering API** | Cloudflare has native |\n| `pg` | NO | **Hyperdrive** | Use Hyperdrive for Postgres |\n| `mysql2` | NO | **Hyperdrive** | Use Hyperdrive for MySQL |\n| `mongodb` | Partial | - | Use fetch to Atlas API |\n| `redis` | NO | **KV, Durable Objects** | Use native services |\n| `aws-sdk` | Partial | **R2 S3 API** | R2 is S3-compatible |\n| `@prisma/client` | Yes | - | D1 adapter available |\n| `drizzle-orm` | Yes | - | Recommended for D1 |\n\n### Compatibility Flag Configuration\n\nWhen using Node.js APIs, add to wrangler config:\n\n```jsonc\n{\n  \"compatibility_flags\": [\"nodejs_compat_v2\"],  // Recommended: latest Node.js compat\n  // OR for legacy:\n  // \"compatibility_flags\": [\"nodejs_compat\"]\n}\n```\n\n### Edge-Native Validation Workflow\n\nWhen reviewing proposed architecture or dependencies:\n\n```\n1. Scan package.json for known incompatible libraries\n2. Flag any `require('fs')` or `require('net')` patterns\n3. Check for `node:` imports without compat flags\n4. Suggest Cloudflare-native alternatives:\n   - fs → R2\n   - net/http → fetch()\n   - express → Hono\n   - redis → KV/DO\n   - postgres → Hyperdrive\n   - image processing → Images API\n5. Verify wrangler.toml has appropriate compat flags\n```\n\n### Runtime Limits to Consider\n\n| Limit | Free | Standard | Unbound |\n|-------|------|----------|---------|\n| Request timeout | 10ms CPU | 30s wall | 30s wall |\n| Memory | 128MB | 128MB | 128MB |\n| Bundle size | 1MB | 10MB | 10MB |\n| Subrequests | 50 | 1000 | 1000 |\n| Environment vars | 64 | 128 | 128 |\n| Cron triggers | 3 | 5 | 5 |\n\n### Example: Edge-Native Migration\n\n**Before (Node.js pattern):**\n```javascript\nconst express = require('express');\nconst fs = require('fs');\nconst Redis = require('ioredis');\n\nconst app = express();\napp.get('/file/:id', async (req, res) => {\n  const content = fs.readFileSync(`./uploads/${req.params.id}`);\n  await redis.set(`cache:${req.params.id}`, content);\n  res.send(content);\n});\n```\n\n**After (Edge-Native):**\n```typescript\nimport { Hono } from 'hono';\n\nconst app = new Hono<{ Bindings: Env }>();\n\napp.get('/file/:id', async (c) => {\n  const obj = await c.env.R2_BUCKET.get(c.req.param('id'));\n  if (!obj) return c.notFound();\n\n  // Cache in KV for fast reads\n  await c.env.KV_CACHE.put(`file:${c.req.param('id')}`, await obj.text(), { expirationTtl: 3600 });\n\n  return new Response(obj.body);\n});\n\nexport default app;\n```\n\n## Architecture Templates\n\n### Template 1: API Gateway\n\n**Use Case**: REST/GraphQL API with database backend\n\n```mermaid\ngraph LR\n    subgraph \"Edge\"\n        W[Worker<br/>Hono Router]\n    end\n    subgraph \"Storage\"\n        D1[(D1<br/>Primary DB)]\n        KV[(KV<br/>Cache)]\n    end\n    subgraph \"Auth\"\n        Access[CF Access]\n    end\n\n    Client --> Access --> W\n    W --> KV\n    KV -.->|miss| D1\n    W --> D1\n```\n\n**Wrangler Config**:\n```jsonc\n{\n  \"name\": \"api-gateway\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n  \"placement\": { \"mode\": \"smart\" },\n  \"observability\": { \"logs\": { \"enabled\": true } },\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"api-db\", \"database_id\": \"...\" }\n  ],\n  \"kv_namespaces\": [\n    { \"binding\": \"CACHE\", \"id\": \"...\" }\n  ],\n  \"routes\": [\n    { \"pattern\": \"api.example.com/*\", \"zone_name\": \"example.com\" }\n  ]\n}\n```\n\n### Template 2: Event Pipeline\n\n**Use Case**: Ingest events, process async, store results\n\n```mermaid\ngraph LR\n    subgraph \"Ingest\"\n        I[Ingest Worker]\n    end\n    subgraph \"Processing\"\n        Q1[Queue]\n        P[Processor]\n        DLQ[Dead Letter]\n    end\n    subgraph \"Storage\"\n        D1[(D1)]\n        R2[(R2<br/>Raw Data)]\n        AE[Analytics Engine]\n    end\n\n    Client --> I\n    I --> Q1 --> P\n    P --> D1\n    P --> R2\n    P --> AE\n    P -.->|failed| DLQ\n```\n\n**Wrangler Config**:\n```jsonc\n{\n  \"name\": \"event-pipeline\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n  \"observability\": { \"logs\": { \"enabled\": true } },\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"events-db\", \"database_id\": \"...\" }\n  ],\n  \"r2_buckets\": [\n    { \"binding\": \"RAW_DATA\", \"bucket_name\": \"events-raw\" }\n  ],\n  \"analytics_engine_datasets\": [\n    { \"binding\": \"METRICS\", \"dataset\": \"event_metrics\" }\n  ],\n  \"queues\": {\n    \"producers\": [\n      { \"binding\": \"EVENTS_QUEUE\", \"queue\": \"events\" }\n    ],\n    \"consumers\": [\n      {\n        \"queue\": \"events\",\n        \"max_batch_size\": 100,\n        \"max_retries\": 1,\n        \"dead_letter_queue\": \"events-dlq\",\n        \"max_concurrency\": 10\n      }\n    ]\n  }\n}\n```\n\n### Template 3: AI Application\n\n**Use Case**: LLM-powered application with RAG\n\n```mermaid\ngraph LR\n    subgraph \"Edge\"\n        W[Worker]\n    end\n    subgraph \"AI\"\n        GW[AI Gateway]\n        WAI[Workers AI]\n    end\n    subgraph \"Storage\"\n        V[(Vectorize)]\n        KV[(KV<br/>Prompt Cache)]\n        D1[(D1<br/>Conversations)]\n    end\n\n    Client --> W\n    W --> KV\n    W --> V\n    W --> GW --> WAI\n    W --> D1\n```\n\n**Wrangler Config**:\n```jsonc\n{\n  \"name\": \"ai-app\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n  \"placement\": { \"mode\": \"smart\" },\n  \"observability\": { \"logs\": { \"enabled\": true } },\n  \"ai\": { \"binding\": \"AI\" },\n  \"vectorize\": [\n    { \"binding\": \"VECTORS\", \"index_name\": \"knowledge-base\" }\n  ],\n  \"kv_namespaces\": [\n    { \"binding\": \"PROMPT_CACHE\", \"id\": \"...\" }\n  ],\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"conversations\", \"database_id\": \"...\" }\n  ],\n  \"vars\": {\n    \"AI_GATEWAY_SLUG\": \"ai-app-gateway\"\n  }\n}\n```\n\n### Template 4: Static Site with Functions\n\n**Use Case**: Marketing site with API endpoints\n\n```mermaid\ngraph LR\n    subgraph \"Static\"\n        Assets[R2<br/>Static Assets]\n    end\n    subgraph \"Dynamic\"\n        W[Worker<br/>API Routes]\n        D1[(D1)]\n    end\n\n    Client --> Assets\n    Client --> W --> D1\n```\n\n**Wrangler Config**:\n```jsonc\n{\n  \"name\": \"marketing-site\",\n  \"main\": \"src/worker.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n  \"assets\": {\n    \"directory\": \"./dist\",\n    \"binding\": \"ASSETS\"\n  },\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"site-db\", \"database_id\": \"...\" }\n  ],\n  \"routes\": [\n    { \"pattern\": \"example.com/*\", \"zone_name\": \"example.com\" }\n  ]\n}\n```\n\n## Design Workflow\n\n### Step 1: Requirements Gathering\n\nAsk about:\n1. **Traffic patterns**: Requests/second, geographic distribution\n2. **Data characteristics**: Size, structure, access patterns\n3. **Processing needs**: Sync vs async, latency requirements\n4. **Budget constraints**: Target monthly cost\n5. **Compliance**: Data residency, encryption requirements\n\n### Step 2: Service Selection\n\nFor each requirement, select appropriate service:\n- High read:write ratio → KV for reads, D1 for writes\n- Large files → R2 with presigned URLs\n- Time-series data → Analytics Engine (free)\n- Search/similarity → Vectorize\n- Long processes → Workflows or Queues\n- Real-time coordination → Durable Objects\n\n### Step 3: Architecture Draft\n\nCreate initial Mermaid diagram showing:\n- All Workers and their responsibilities\n- Storage bindings and data flow\n- Queue topology (if async processing)\n- External service integrations\n\n### Step 4: Configuration Generation\n\nGenerate wrangler.jsonc with:\n- All bindings properly named\n- Environment-specific overrides\n- Proper placement mode\n- Observability enabled\n- Queue DLQs configured\n\n### Step 5: Cost Estimation\n\nCalculate monthly costs using:\n- Request volume × $0.30/M (Workers)\n- Storage GB × service rates\n- Operation counts × service rates\n- AI usage × neuron/token rates\n\n### Step 6: Review Checklist\n\nBefore finalizing, verify:\n- [ ] All queues have DLQs\n- [ ] D1 has appropriate indexes planned\n- [ ] Smart placement enabled for latency-sensitive\n- [ ] Observability configured\n- [ ] Secrets use wrangler secret (not vars)\n- [ ] Rate limiting for public APIs\n\n## Mermaid Diagram Patterns\n\n### Basic Worker Flow\n```mermaid\ngraph LR\n    A[Client] --> B[Worker]\n    B --> C[(D1)]\n    B --> D[(KV)]\n```\n\n### Queue Processing\n```mermaid\ngraph LR\n    A[Producer] --> B[Queue]\n    B --> C[Consumer]\n    C --> D[(Storage)]\n    C -.->|failed| E[DLQ]\n```\n\n### Service Bindings\n```mermaid\ngraph LR\n    A[Gateway Worker] -->|RPC| B[Auth Worker]\n    A -->|RPC| C[Data Worker]\n    B --> D[(KV)]\n    C --> E[(D1)]\n```\n\n### Multi-Region\n```mermaid\ngraph TB\n    subgraph \"Region A\"\n        WA[Worker A]\n        DA[(D1 Primary)]\n    end\n    subgraph \"Region B\"\n        WB[Worker B]\n        DB[(D1 Replica)]\n    end\n    WA --> DA\n    WB --> DB\n    DA -.->|sync| DB\n```\n\n## Output Format\n\nWhen designing an architecture, provide:\n\n1. **Requirements Summary** - Confirmed requirements\n2. **Architecture Diagram** - Mermaid visualization\n3. **Service Justification** - Why each service was chosen\n4. **Wrangler Configuration** - Complete, deployable config\n5. **Cost Estimate** - Monthly projection with breakdown\n6. **Migration Path** - If replacing existing system\n7. **Next Steps** - Implementation order\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| HTTP between Workers | 1000 subrequest limit | Service Bindings RPC |\n| D1 as queue | Expensive, no guarantees | Use Queues |\n| KV for large files | 25MB limit, expensive | Use R2 |\n| Polling for events | Wasteful, slow | Queues or WebSocket |\n| Per-request AI calls | Expensive, slow | Cache with KV |\n| No DLQ | Lost messages | Always add DLQ |"
              },
              {
                "name": "custom-hostnames",
                "description": "Manage Custom Hostnames and SSL for SaaS applications on Cloudflare. Use this skill when building white-label platforms, implementing vanity domains for customers, managing SSL certificate lifecycle, or troubleshooting custom hostname verification issues.",
                "path": "skills/custom-hostnames/SKILL.md",
                "frontmatter": {
                  "name": "custom-hostnames",
                  "description": "Manage Custom Hostnames and SSL for SaaS applications on Cloudflare. Use this skill when building white-label platforms, implementing vanity domains for customers, managing SSL certificate lifecycle, or troubleshooting custom hostname verification issues."
                },
                "content": "# Cloudflare Custom Hostnames Skill (SaaS for SaaS)\n\nDesign and implement custom hostname solutions for SaaS platforms that allow customers to bring their own domains. Handle SSL certificate provisioning, domain verification, and DNS configuration.\n\n## SaaS Custom Hostname Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Customer Domain\"\n        CD[customer.com]\n        CNAME[CNAME → proxy.saas.com]\n    end\n    subgraph \"Cloudflare for SaaS\"\n        CH[Custom Hostname<br/>customer.com]\n        SSL[SSL Certificate<br/>Auto-provisioned]\n        FO[Fallback Origin<br/>app.saas.com]\n    end\n    subgraph \"SaaS Platform\"\n        W[Worker<br/>Multi-tenant Router]\n        DB[(Database<br/>Tenant Config)]\n    end\n\n    CD --> CNAME --> CH\n    CH --> SSL --> FO --> W\n    W --> DB\n```\n\n## Custom Hostname Lifecycle\n\n| Stage | Status | Action Required |\n|-------|--------|-----------------|\n| 1. Created | `pending` | Customer adds CNAME record |\n| 2. Verification | `pending_validation` | DNS TXT record verification |\n| 3. SSL Issuance | `active` (pending SSL) | Certificate provisioning |\n| 4. Active | `active` | Fully operational |\n| 5. Renewal | `active` | Auto-renewal (90 days before expiry) |\n\n## Implementation Patterns\n\n### Pattern 1: Worker-Based Routing\n\nRoute requests based on Host header to tenant-specific configuration:\n\n```typescript\n// src/index.ts - Multi-tenant SaaS Worker\nimport { Hono } from 'hono';\n\ninterface Tenant {\n  id: string;\n  customHostname: string;\n  config: Record<string, unknown>;\n}\n\nconst app = new Hono<{ Bindings: Env }>();\n\n// Middleware: Resolve tenant from hostname\napp.use('*', async (c, next) => {\n  const hostname = c.req.header('host') || '';\n\n  // Check cache first\n  let tenant = await c.env.KV_TENANTS.get<Tenant>(`host:${hostname}`, 'json');\n\n  if (!tenant) {\n    // Query database for tenant config\n    const result = await c.env.DB.prepare(\n      'SELECT * FROM tenants WHERE custom_hostname = ? OR hostname = ?'\n    ).bind(hostname, hostname).first<Tenant>();\n\n    if (result) {\n      tenant = result;\n      // Cache for 5 minutes\n      await c.env.KV_TENANTS.put(`host:${hostname}`, JSON.stringify(tenant), {\n        expirationTtl: 300\n      });\n    }\n  }\n\n  if (!tenant) {\n    return c.text('Unknown domain', 404);\n  }\n\n  c.set('tenant', tenant);\n  await next();\n});\n\n// Tenant-aware routes\napp.get('/', (c) => {\n  const tenant = c.get('tenant') as Tenant;\n  return c.json({ tenant: tenant.id, message: 'Welcome!' });\n});\n\nexport default app;\n```\n\n### Pattern 2: Custom Hostname API Integration\n\n```typescript\n// api/hostnames.ts - Custom hostname management\ninterface CreateHostnameRequest {\n  tenantId: string;\n  hostname: string;\n}\n\nexport async function createCustomHostname(\n  env: Env,\n  request: CreateHostnameRequest\n): Promise<CustomHostnameResult> {\n  const { tenantId, hostname } = request;\n\n  // 1. Create custom hostname via Cloudflare API\n  const response = await fetch(\n    `https://api.cloudflare.com/client/v4/zones/${env.ZONE_ID}/custom_hostnames`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${env.CF_API_TOKEN}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        hostname,\n        ssl: {\n          method: 'http',  // or 'cname', 'txt', 'email'\n          type: 'dv',      // Domain Validation\n          settings: {\n            http2: 'on',\n            min_tls_version: '1.2',\n            tls_1_3: 'on',\n          },\n        },\n        custom_metadata: {\n          tenant_id: tenantId,\n        },\n      }),\n    }\n  );\n\n  const result = await response.json();\n\n  if (!result.success) {\n    throw new Error(result.errors[0]?.message || 'Failed to create hostname');\n  }\n\n  // 2. Store in database\n  await env.DB.prepare(\n    `INSERT INTO custom_hostnames (id, tenant_id, hostname, status, cf_id)\n     VALUES (?, ?, ?, ?, ?)`\n  ).bind(\n    crypto.randomUUID(),\n    tenantId,\n    hostname,\n    'pending',\n    result.result.id\n  ).run();\n\n  // 3. Return verification instructions\n  return {\n    id: result.result.id,\n    hostname,\n    status: result.result.status,\n    verification: {\n      type: result.result.ssl.method,\n      // For HTTP validation\n      http_url: result.result.ssl.http_url,\n      http_body: result.result.ssl.http_body,\n      // For CNAME/TXT validation\n      cname_target: result.result.ssl.cname_target,\n      txt_name: result.result.ssl.txt_name,\n      txt_value: result.result.ssl.txt_value,\n    },\n    instructions: generateInstructions(result.result),\n  };\n}\n\nfunction generateInstructions(hostname: any): string {\n  return `\n## Setup Instructions for ${hostname.hostname}\n\n### Step 1: Add DNS Record\nAdd a CNAME record pointing to your SaaS proxy:\n\n| Type  | Name                | Target                |\n|-------|---------------------|----------------------|\n| CNAME | ${hostname.hostname} | proxy.your-saas.com |\n\n### Step 2: Verify Ownership (if required)\n${hostname.ssl.method === 'txt' ? `\nAdd this TXT record:\n| Type | Name | Value |\n|------|------|-------|\n| TXT  | ${hostname.ssl.txt_name} | ${hostname.ssl.txt_value} |\n` : ''}\n\n### Step 3: Wait for SSL\nSSL certificate will be issued automatically once DNS propagates (usually 5-15 minutes).\n\nYou can check status at: /api/hostnames/${hostname.id}/status\n  `;\n}\n```\n\n### Pattern 3: Hostname Status Webhook\n\n```typescript\n// api/webhooks/hostname-status.ts\nexport async function handleHostnameWebhook(\n  request: Request,\n  env: Env\n): Promise<Response> {\n  const payload = await request.json();\n\n  // Cloudflare webhook payload\n  const { data } = payload;\n  const { hostname, status, ssl } = data;\n\n  // Update database\n  await env.DB.prepare(\n    `UPDATE custom_hostnames SET status = ?, ssl_status = ?, updated_at = ?\n     WHERE hostname = ?`\n  ).bind(status, ssl?.status, new Date().toISOString(), hostname).run();\n\n  // Clear cache\n  await env.KV_TENANTS.delete(`host:${hostname}`);\n\n  // Notify tenant if status changed to active\n  if (status === 'active' && ssl?.status === 'active') {\n    await notifyTenant(env, hostname, 'Your custom domain is now active!');\n  }\n\n  return new Response('OK');\n}\n```\n\n## SSL Certificate Options\n\n### Validation Methods\n\n| Method | Description | Best For |\n|--------|-------------|----------|\n| `http` | HTTP file validation | Most automated; works behind proxies |\n| `cname` | CNAME delegation | When customer controls DNS |\n| `txt` | TXT record validation | One-time verification |\n| `email` | Domain admin email | Legacy; not recommended |\n\n### SSL Types\n\n| Type | Certificate Authority | Cost | Use Case |\n|------|----------------------|------|----------|\n| `dv` | Let's Encrypt / Google Trust | Free | Standard SaaS |\n| `ev` | DigiCert | $$$ | Enterprise (rare) |\n\n### Recommended SSL Configuration\n\n```json\n{\n  \"ssl\": {\n    \"method\": \"http\",\n    \"type\": \"dv\",\n    \"settings\": {\n      \"http2\": \"on\",\n      \"min_tls_version\": \"1.2\",\n      \"tls_1_3\": \"on\",\n      \"early_hints\": \"on\"\n    },\n    \"bundle_method\": \"ubiquitous\",\n    \"wildcard\": false\n  }\n}\n```\n\n## Troubleshooting Guide\n\n### Common Issues\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| `pending` forever | DNS not configured | Verify CNAME record exists |\n| `pending_validation` | TXT/HTTP validation failing | Check validation path accessibility |\n| SSL errors | CAA records blocking | Add `0 issue \"letsencrypt.org\"` |\n| 522 errors | Origin not reachable | Check fallback origin configuration |\n| 403 Forbidden | Origin hostname mismatch | Set Host header override |\n\n### Debugging Workflow\n\n```javascript\n// Check hostname status via MCP\nmcp__cloudflare-bindings__custom_hostname_get({\n  zone_id: \"...\",\n  hostname_id: \"...\"\n})\n\n// List all custom hostnames\nmcp__cloudflare-bindings__custom_hostnames_list({\n  zone_id: \"...\"\n})\n```\n\n### CAA Record Requirements\n\nIf customer has CAA records, they need:\n```\n0 issue \"letsencrypt.org\"\n0 issue \"pki.goog\"\n0 issue \"digicert.com\"\n0 issuewild \"letsencrypt.org\"\n```\n\n## Database Schema\n\n```sql\n-- D1 schema for custom hostnames\nCREATE TABLE tenants (\n  id TEXT PRIMARY KEY,\n  name TEXT NOT NULL,\n  hostname TEXT NOT NULL,  -- Default hostname\n  custom_hostname TEXT,    -- Customer's domain\n  plan TEXT DEFAULT 'free',\n  created_at TEXT DEFAULT (datetime('now'))\n);\n\nCREATE TABLE custom_hostnames (\n  id TEXT PRIMARY KEY,\n  tenant_id TEXT NOT NULL,\n  hostname TEXT NOT NULL UNIQUE,\n  status TEXT DEFAULT 'pending',\n  ssl_status TEXT DEFAULT 'pending',\n  cf_id TEXT,  -- Cloudflare custom hostname ID\n  verified_at TEXT,\n  expires_at TEXT,\n  created_at TEXT DEFAULT (datetime('now')),\n  updated_at TEXT,\n  FOREIGN KEY (tenant_id) REFERENCES tenants(id)\n);\n\nCREATE INDEX idx_hostnames_tenant ON custom_hostnames(tenant_id);\nCREATE INDEX idx_hostnames_hostname ON custom_hostnames(hostname);\nCREATE INDEX idx_hostnames_status ON custom_hostnames(status);\n```\n\n## Wrangler Configuration\n\n```jsonc\n{\n  \"name\": \"saas-platform\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n  \"compatibility_flags\": [\"nodejs_compat_v2\"],\n\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"saas-db\", \"database_id\": \"...\" }\n  ],\n  \"kv_namespaces\": [\n    { \"binding\": \"KV_TENANTS\", \"id\": \"...\" }\n  ],\n\n  \"vars\": {\n    \"ZONE_ID\": \"your-zone-id\"\n  },\n\n  // Fallback origin for custom hostnames\n  \"routes\": [\n    { \"pattern\": \"app.your-saas.com/*\", \"zone_name\": \"your-saas.com\" }\n  ]\n}\n```\n\n## Cost Considerations\n\n| Feature | Cost | Included With |\n|---------|------|---------------|\n| Custom Hostnames | $2/hostname/month | SSL for SaaS |\n| Wildcard Hostnames | $2/hostname/month | SSL for SaaS |\n| SSL Certificates | Free (DV) | All plans |\n| SSL for SaaS Plan | $200/month base | 100 hostnames included |\n| Additional Hostnames | $0.10/hostname/month | After included quota |\n\n### Cost Optimization Tips\n\n- **Batch hostname creation**: Create during off-peak hours\n- **Use wildcards**: `*.customer.example.com` for subdomains\n- **Monitor usage**: Track active vs inactive hostnames\n- **Clean up**: Remove hostnames for churned customers\n- **Consider enterprise**: 10K+ hostnames? Contact CF sales\n\n## Security Considerations\n\n- **Hostname takeover**: Validate customer owns domain before creating\n- **Certificate transparency**: Monitor CT logs for unauthorized certs\n- **Origin protection**: Only allow traffic from Cloudflare IPs\n- **Tenant isolation**: Ensure hostname-to-tenant mapping is secure\n- **Rate limiting**: Limit hostname creation per tenant\n\n## Output Format\n\n```markdown\n# Custom Hostname Report\n\n**Tenant**: [Tenant Name]\n**Total Hostnames**: X\n\n## Active Hostnames\n\n| Hostname | SSL Status | Expires | Traffic (30d) |\n|----------|------------|---------|---------------|\n| app.customer.com | Active | 2025-03-15 | 1.2M requests |\n\n## Pending Hostnames\n\n| Hostname | Status | Action Required |\n|----------|--------|-----------------|\n| new.customer.com | pending_validation | Add TXT record |\n\n## Issues Detected\n\n### [CH001] Hostname approaching SSL expiry\n- **Hostname**: old.customer.com\n- **Expires**: 2025-01-10\n- **Fix**: Renew or remove if unused\n```\n\n## Tips\n\n- **Wildcard support**: Use `*.example.com` for unlimited subdomains\n- **Fallback handling**: Always have a catch-all route for unknown hostnames\n- **Status polling**: Implement webhook or poll every 30s during setup\n- **Customer communication**: Send email when hostname becomes active\n- **Monitoring**: Track hostname verification success rates\n- **API token scope**: Use minimal permissions (Zone:Custom Hostnames:Edit)"
              },
              {
                "name": "guardian",
                "description": "Proactively audit Cloudflare configurations for security vulnerabilities, resilience gaps, cost traps, AND budget/privacy risks. Use this skill when reviewing wrangler configs, before deployments, investigating issues, or when ANY architecture decision involves Durable Objects, R2, Workers AI, or high-volume operations. This skill PROACTIVELY warns about cost impacts before users ask.",
                "path": "skills/guardian/SKILL.md",
                "frontmatter": {
                  "name": "guardian",
                  "description": "Proactively audit Cloudflare configurations for security vulnerabilities, resilience gaps, cost traps, AND budget/privacy risks. Use this skill when reviewing wrangler configs, before deployments, investigating issues, or when ANY architecture decision involves Durable Objects, R2, Workers AI, or high-volume operations. This skill PROACTIVELY warns about cost impacts before users ask."
                },
                "content": "# Cloudflare Guardian Skill\n\nAudit wrangler configurations for security vulnerabilities, performance issues, cost traps, resilience gaps, **and proactively enforce budget/privacy constraints**. Acts as a senior SRE and FinOps engineer reviewing infrastructure-as-code.\n\n## Cost Watchlist Reference\n\n**IMPORTANT**: For detailed cost trap documentation, reference `${CLAUDE_PLUGIN_ROOT}/COST_SENSITIVE_RESOURCES.md`.\n\nWhen issuing cost warnings, use provenance tags:\n- `[STATIC:COST_WATCHLIST]` - Pattern detected via code analysis\n- `[LIVE-VALIDATED:COST_WATCHLIST]` - Confirmed by observability data\n- `[REFUTED:COST_WATCHLIST]` - Pattern exists but not hitting thresholds\n\n## Budget Whisperer Behavior\n\n**CRITICAL**: When Claude suggests ANY code change involving the following, the guardian skill MUST trigger proactive checks:\n\n### D1 Write Operations\nIf suggesting code that includes `.run()`, `.first()`, or database writes:\n1. **Search for `.batch()`** - If missing, warn about per-row insert costs\n2. **Search for `CREATE INDEX`** - If querying unindexed columns, warn about scan costs\n3. **Cite**: `TRAP-D1-001` or `TRAP-D1-002` from COST_SENSITIVE_RESOURCES.md\n\n```\nBudget Whisperer Check:\n- Detected: D1 write operation in proposed code\n- Searched for: db.batch() usage\n- Found: ❌ Missing batch operations\n- Warning: [STATIC:COST_WATCHLIST] TRAP-D1-001\n  Per-row INSERT detected. At 10K rows, this costs $0.01 vs $0.00001 batched.\n  Recommendation: Wrap in db.batch() with max 1000 statements per batch.\n```\n\n### R2 Write Operations\nIf suggesting code that includes `.put()`:\n1. **Check loop context** - Is `.put()` inside a loop or frequently called handler?\n2. **Search for buffering** - Any aggregation before write?\n3. **Cite**: `TRAP-R2-001` from COST_SENSITIVE_RESOURCES.md\n\n### Durable Objects Usage\nIf suggesting DO architecture:\n1. **Check use case** - Is coordination/locking actually needed?\n2. **Suggest alternatives** - KV for simple storage, D1 for relational\n3. **Cite**: `TRAP-DO-001` from COST_SENSITIVE_RESOURCES.md\n\n## Vibecoder Proactive Safeguards\n\n**IMPORTANT**: This skill should proactively warn users about cost and privacy impacts BEFORE they deploy or even ask about costs. When reviewing ANY architecture that includes the following, immediately surface budget/privacy alerts:\n\n### Budget Enforcement Triggers\n\n| Service/Pattern | Threshold | Proactive Warning |\n|-----------------|-----------|-------------------|\n| Durable Objects | Any usage | \"DO charges ~$0.15/GB-month storage + $0.50/M requests. Consider KV for simple key-value.\" |\n| R2 Class A ops | >1M/month | \"R2 writes cost $4.50/M. Buffer writes or use presigned URLs for client uploads.\" |\n| D1 Writes | >10M/month | \"D1 writes cost $1/M. Detected pattern suggests >$10/mo. Batch to ≤1,000 rows.\" |\n| Workers AI (>8B) | Any usage | \"Large models (Llama 11B+) cost $0.68/M tokens. Use 8B or smaller for bulk.\" |\n| Vectorize | >1M vectors | \"Approaching 5M vector limit. Plan sharding strategy.\" |\n| KV Writes | >5M/month | \"KV writes cost $5/M (10× reads). Consider D1 or R2 for write-heavy.\" |\n\n### Privacy Enforcement Triggers\n\n| Pattern | Severity | Proactive Warning |\n|---------|----------|-------------------|\n| PII in logs | CRITICAL | \"Detected potential PII logging. Use structured logging with redaction.\" |\n| User data in KV keys | HIGH | \"KV keys with user IDs may leak via Workers dashboard. Hash or encrypt.\" |\n| AI prompts with PII | HIGH | \"AI Gateway logs may contain user data. Enable prompt redaction.\" |\n| R2 public buckets | HIGH | \"R2 bucket appears public. Verify intentional or add authentication.\" |\n| Analytics with user IDs | MEDIUM | \"User IDs in Analytics Engine may persist. Use anonymized identifiers.\" |\n\n## Audit Categories\n\n### Security Audit Rules\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| SEC001 | Secrets in plaintext | CRITICAL | `vars.*` contains API_KEY, SECRET, PASSWORD, TOKEN patterns |\n| SEC002 | Missing route auth | HIGH | Routes without `cf.access` or auth middleware |\n| SEC003 | CORS wildcard | MEDIUM | `cors.origins` includes `*` |\n| SEC004 | Exposed admin routes | HIGH | `/admin/*` routes without auth |\n| SEC005 | Missing rate limiting | MEDIUM | No rate limit bindings for public APIs |\n| SEC006 | Debug mode enabled | LOW | `ENVIRONMENT` or `DEBUG` set to development/true |\n\n### Performance Audit Rules\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| PERF001 | Missing Smart Placement | LOW | `placement.mode` not set |\n| PERF002 | D1 without indexes | MEDIUM | D1 bindings but no CREATE INDEX in migrations |\n| PERF003 | Large bundled dependencies | MEDIUM | Bundle >10MB (check `main` entry) |\n| PERF004 | Missing observability | LOW | No `observability` config block |\n| PERF005 | Frequent cron | LOW | Cron more often than every 5 minutes |\n\n### Cost Audit Rules\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| COST001 | Queue retries high | MEDIUM | `max_retries > 1` for potentially idempotent consumers |\n| COST002 | No cron batching | LOW | Multiple crons that could be combined |\n| COST003 | AI without caching | MEDIUM | AI bindings but no AI Gateway |\n| COST004 | Large model usage | LOW | Workers AI with >8B parameter models |\n| COST005 | Missing Analytics Engine | INFO | Using D1/KV for metrics instead of free AE |\n\n### Resilience Audit Rules\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| RES001 | Missing DLQ | HIGH | Queues without `dead_letter_queue` binding |\n| RES002 | No concurrency limit | MEDIUM | `max_concurrency` not set for queue consumers |\n| RES003 | Single region | LOW | No `cf.smart_placement` for latency-sensitive |\n| RES004 | Missing retry config | MEDIUM | Queue consumer without explicit retry config |\n| RES005 | No circuit breaker | LOW | External API calls without timeout/fallback |\n\n### Budget Audit Rules (Proactive)\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| BUDGET001 | Durable Objects usage | INFO | Any DO binding - proactively explain cost model |\n| BUDGET002 | R2 write-heavy pattern | MEDIUM | Frequent R2 Class A ops without buffering |\n| BUDGET003 | D1 per-row inserts | HIGH | Loop-based INSERTs instead of batch |\n| BUDGET004 | Large AI model | MEDIUM | Workers AI with >8B parameter model |\n| BUDGET005 | KV write-heavy | MEDIUM | >5M KV writes/month pattern |\n| BUDGET006 | Vectorize scaling | INFO | >1M vectors - warn about 5M limit |\n\n### Privacy Audit Rules\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| PRIV001 | PII in logs | CRITICAL | console.log with user data patterns |\n| PRIV002 | User IDs in KV keys | HIGH | KV key patterns containing user/email/phone |\n| PRIV003 | AI prompts PII | HIGH | AI bindings without redaction middleware |\n| PRIV004 | R2 public access | HIGH | R2 bucket without authentication |\n| PRIV005 | Analytics PII | MEDIUM | User identifiers in Analytics Engine writes |\n\n## Audit Workflow\n\n### Step 1: Parse Wrangler Config\n\nSupport both TOML and JSONC formats:\n```\n1. Read wrangler.toml or wrangler.jsonc\n2. Parse into structured format\n3. Extract: name, bindings, routes, triggers, vars\n```\n\n### Step 2: Run Security Checks\n\n```\nFor each security rule:\n1. Check if pattern exists in config\n2. If violation found:\n   - Record rule ID, severity, location\n   - Generate specific recommendation\n   - Include docs URL if available\n```\n\n### Step 3: Run Performance Checks\n\n```\nFor each performance rule:\n1. Check config for anti-patterns\n2. Cross-reference with migrations (for D1 index checks)\n3. Record findings with optimization recommendations\n```\n\n### Step 4: Run Cost Checks\n\n```\nFor each cost rule:\n1. Identify cost-amplifying patterns\n2. Estimate impact if possible\n3. Provide specific fixes\n```\n\n### Step 5: Run Resilience Checks\n\n```\nFor each resilience rule:\n1. Check for missing failure handling\n2. Identify single points of failure\n3. Recommend redundancy patterns\n```\n\n### Step 5b: Run Budget Enforcement Checks (Proactive)\n\n```\nFor bindings that trigger budget warnings:\n1. Detect Durable Objects → Explain cost model proactively\n2. Detect R2 writes → Check for buffering patterns\n3. Detect D1 writes → Check for batch vs per-row\n4. Detect Workers AI → Check model size selection\n5. Detect high-volume KV → Suggest alternatives\n```\n\n**Key principle**: Surface budget impacts BEFORE the user asks about costs.\n\n### Step 5c: Run Privacy Checks\n\n```\nFor privacy-sensitive patterns:\n1. Scan code for console.log with user data patterns\n2. Check KV key naming for PII patterns\n3. Verify AI prompts have redaction middleware\n4. Check R2 bucket access controls\n5. Review Analytics Engine write patterns\n```\n\n### Step 6: Calculate Score\n\n```\nscore = 100 - (critical × 25) - (high × 15) - (medium × 5) - (low × 2)\n```\n\nGrades:\n- 90-100: A (Production ready)\n- 80-89: B (Minor issues)\n- 70-79: C (Address before deployment)\n- 60-69: D (Significant issues)\n- <60: F (Critical problems)\n\n## Output Format\n\n```markdown\n# Cloudflare Configuration Audit\n\n**Score**: XX/100 (Grade: X)\n**File**: wrangler.jsonc\n\n## Proactive Budget & Privacy Alerts\n\n> **Budget Impact Detected**: [List any BUDGET* findings with cost estimates]\n> **Privacy Concern**: [List any PRIV* findings requiring attention]\n\n## Summary\n\n| Category | Critical | High | Medium | Low | Info |\n|----------|----------|------|--------|-----|------|\n| Security | X | X | X | X | - |\n| Performance | X | X | X | X | - |\n| Cost | X | X | X | X | - |\n| Resilience | X | X | X | X | - |\n| Budget | - | X | X | - | X |\n| Privacy | X | X | X | - | - |\n\n## Critical Issues (Must Fix)\n\n### SEC001: Secrets in plaintext\n- **Location**: `vars.API_KEY`\n- **Issue**: Plaintext API key in configuration\n- **Fix**: Use `wrangler secret put API_KEY`\n- **Docs**: https://developers.cloudflare.com/workers/configuration/secrets/\n\n## High Priority Issues\n\n### RES001: Missing dead letter queue\n- **Location**: `queues[0]` (harvest-queue)\n- **Issue**: No DLQ for failed message inspection\n- **Fix**: Add `dead_letter_queue = \"harvest-dlq\"`\n\n## Medium Priority Issues\n\n[List all medium issues]\n\n## Low Priority Issues\n\n[List all low issues]\n\n## Recommendations\n\n1. [ ] Move secrets to wrangler secret\n2. [ ] Add DLQ for all production queues\n3. [ ] Enable Smart Placement\n4. [ ] Consider Analytics Engine for metrics\n```\n\n## Migration Checks\n\nWhen D1 bindings exist, also scan migration files:\n\n```sql\n-- Good: Has index\nCREATE INDEX idx_projects_source ON projects(source);\n\n-- Bad: Missing index for common query pattern\nSELECT * FROM projects WHERE source = ? ORDER BY created_at DESC;\n```\n\nFlag missing indexes for:\n- Columns in WHERE clauses\n- Columns in ORDER BY\n- Compound queries (need compound indexes)\n\n## Wrangler Config Patterns\n\n### Good Patterns to Recognize\n\n```jsonc\n{\n  // Smart Placement enabled\n  \"placement\": { \"mode\": \"smart\" },\n\n  // Observability configured\n  \"observability\": { \"logs\": { \"enabled\": true } },\n\n  // Queue with DLQ\n  \"queues\": {\n    \"consumers\": [{\n      \"queue\": \"my-queue\",\n      \"dead_letter_queue\": \"my-dlq\",\n      \"max_retries\": 1,\n      \"max_concurrency\": 10\n    }]\n  }\n}\n```\n\n### Bad Patterns to Flag\n\n```jsonc\n{\n  // Secrets in vars\n  \"vars\": { \"API_KEY\": \"sk-xxxxx\" },\n\n  // No DLQ\n  \"queues\": { \"consumers\": [{ \"queue\": \"my-queue\" }] },\n\n  // High retries\n  \"queues\": { \"consumers\": [{ \"max_retries\": 10 }] }\n}\n```\n\n## Live Validation with Probes\n\nWhen MCP tools are available (via `--validate` mode in `/cf-audit`), enhance static findings with live data.\n\nReference @skills/probes/SKILL.md for detailed query patterns.\n\n### Security Validation\n- **Error rate analysis**: High errors on specific paths may indicate attacks\n- **Request patterns**: Verify authentication is actually enforced\n- **Resource exposure**: Check KV/R2 for public access settings\n\n### Performance Validation\n- **EXPLAIN QUERY PLAN**: Verify D1 index usage\n- **Latency percentiles**: P50/P95/P99 analysis\n- **CPU time analysis**: Identify hotspots\n\n### Resilience Validation\n- **Queue health**: Check DLQ depth and retry rates\n- **Error patterns**: Identify cascading failures\n\n## Provenance Tagging\n\nTag findings based on data source:\n- `[STATIC]` - Inferred from code/config analysis only\n- `[LIVE-VALIDATED]` - Confirmed by observability data\n- `[LIVE-REFUTED]` - Code smell not observed in production\n- `[INCOMPLETE]` - MCP tools unavailable for verification\n\n## Pattern Recommendations\n\nWhen issues are found, recommend applicable patterns from @skills/patterns/:\n\n| Finding | Recommended Pattern |\n|---------|-------------------|\n| Per-row D1 inserts | `d1-batching` |\n| External API issues | `circuit-breaker` |\n| Monolithic Worker | `service-bindings` |\n\n## Tips\n\n- Run before every deployment\n- Use `--validate` for production-ready verification\n- Focus on CRITICAL and HIGH first\n- Use `--fix` suggestions to auto-generate patches\n- Compare scores over time to track improvements\n- `[LIVE-REFUTED]` findings may still be worth fixing proactively"
              },
              {
                "name": "implement",
                "description": "Scaffold Cloudflare Workers with Hono, Drizzle ORM, and TypeScript best practices. Use this skill when implementing new Workers, adding endpoints, or setting up database schemas.",
                "path": "skills/implement/SKILL.md",
                "frontmatter": {
                  "name": "implement",
                  "description": "Scaffold Cloudflare Workers with Hono, Drizzle ORM, and TypeScript best practices. Use this skill when implementing new Workers, adding endpoints, or setting up database schemas."
                },
                "content": "# Cloudflare Implementation Skill\n\nScaffold production-ready Cloudflare Workers following modern patterns with Hono, Drizzle ORM, and TypeScript.\n\n## Technology Stack\n\n| Layer | Technology | Purpose |\n|-------|-----------|---------|\n| Router | Hono v4+ | Lightweight, fast, TypeScript-first |\n| ORM | Drizzle | Type-safe D1 queries, migrations |\n| Validation | Zod | Request/response validation |\n| Runtime | Workers | Edge compute |\n\n## Project Structure\n\n```\nworker/\n├── src/\n│   ├── index.ts          # Hono app entry\n│   ├── routes/           # Route handlers\n│   │   ├── api.ts\n│   │   └── health.ts\n│   ├── middleware/       # Hono middleware\n│   │   ├── auth.ts\n│   │   └── errors.ts\n│   ├── services/         # Business logic\n│   │   └── users.ts\n│   ├── db/               # Drizzle schema + queries\n│   │   ├── schema.ts\n│   │   └── queries.ts\n│   └── types.ts          # Shared types\n├── migrations/           # D1 migrations\n│   └── 0001_initial.sql\n├── wrangler.jsonc\n├── drizzle.config.ts\n├── package.json\n└── tsconfig.json\n```\n\n## Code Templates\n\n### Entry Point (src/index.ts)\n\n```typescript\nimport { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { logger } from 'hono/logger';\nimport { timing } from 'hono/timing';\nimport { errorHandler } from './middleware/errors';\nimport { apiRoutes } from './routes/api';\nimport { healthRoutes } from './routes/health';\nimport type { Bindings } from './types';\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\n// Middleware\napp.use('*', timing());\napp.use('*', logger());\napp.use('*', cors());\napp.onError(errorHandler);\n\n// Routes\napp.route('/health', healthRoutes);\napp.route('/api', apiRoutes);\n\n// 404 handler\napp.notFound((c) => c.json({ error: 'Not found' }, 404));\n\nexport default app;\n```\n\n### Type Definitions (src/types.ts)\n\n```typescript\nexport interface Bindings {\n  // D1 Database\n  DB: D1Database;\n\n  // KV Namespace\n  CACHE: KVNamespace;\n\n  // R2 Bucket\n  STORAGE: R2Bucket;\n\n  // Queue Producer\n  QUEUE: Queue<QueueMessage>;\n\n  // AI\n  AI: Ai;\n\n  // Environment variables\n  ENVIRONMENT: 'development' | 'staging' | 'production';\n}\n\nexport interface QueueMessage {\n  type: string;\n  payload: unknown;\n  timestamp: number;\n}\n\n// Hono context helper\nexport type AppContext = Context<{ Bindings: Bindings }>;\n```\n\n### Drizzle Schema (src/db/schema.ts)\n\n```typescript\nimport { sqliteTable, text, integer, real } from 'drizzle-orm/sqlite-core';\nimport { sql } from 'drizzle-orm';\n\nexport const users = sqliteTable('users', {\n  id: text('id').primaryKey(),\n  email: text('email').notNull().unique(),\n  name: text('name'),\n  createdAt: text('created_at')\n    .notNull()\n    .default(sql`CURRENT_TIMESTAMP`),\n  updatedAt: text('updated_at')\n    .notNull()\n    .default(sql`CURRENT_TIMESTAMP`),\n});\n\nexport const projects = sqliteTable('projects', {\n  id: text('id').primaryKey(),\n  userId: text('user_id')\n    .notNull()\n    .references(() => users.id),\n  title: text('title').notNull(),\n  status: text('status', { enum: ['draft', 'active', 'archived'] })\n    .notNull()\n    .default('draft'),\n  metadata: text('metadata', { mode: 'json' }).$type<Record<string, unknown>>(),\n  createdAt: text('created_at')\n    .notNull()\n    .default(sql`CURRENT_TIMESTAMP`),\n});\n\n// Type exports for queries\nexport type User = typeof users.$inferSelect;\nexport type NewUser = typeof users.$inferInsert;\nexport type Project = typeof projects.$inferSelect;\nexport type NewProject = typeof projects.$inferInsert;\n```\n\n### Drizzle Config (drizzle.config.ts)\n\n```typescript\nimport type { Config } from 'drizzle-kit';\n\nexport default {\n  schema: './src/db/schema.ts',\n  out: './migrations',\n  dialect: 'sqlite',\n} satisfies Config;\n```\n\n### Database Queries (src/db/queries.ts)\n\n```typescript\nimport { drizzle } from 'drizzle-orm/d1';\nimport { eq, desc, and, sql } from 'drizzle-orm';\nimport * as schema from './schema';\n\nexport function getDb(d1: D1Database) {\n  return drizzle(d1, { schema });\n}\n\nexport async function getUserById(db: D1Database, id: string) {\n  const d = getDb(db);\n  return d.query.users.findFirst({\n    where: eq(schema.users.id, id),\n  });\n}\n\nexport async function createUser(db: D1Database, user: schema.NewUser) {\n  const d = getDb(db);\n  return d.insert(schema.users).values(user).returning().get();\n}\n\nexport async function getProjectsByUser(\n  db: D1Database,\n  userId: string,\n  options: { limit?: number; offset?: number } = {}\n) {\n  const d = getDb(db);\n  const { limit = 20, offset = 0 } = options;\n\n  return d.query.projects.findMany({\n    where: eq(schema.projects.userId, userId),\n    orderBy: desc(schema.projects.createdAt),\n    limit,\n    offset,\n  });\n}\n\n// Batch insert pattern (≤1000 rows)\nexport async function batchInsertProjects(\n  db: D1Database,\n  projects: schema.NewProject[]\n) {\n  const d = getDb(db);\n  const BATCH_SIZE = 1000;\n\n  for (let i = 0; i < projects.length; i += BATCH_SIZE) {\n    const batch = projects.slice(i, i + BATCH_SIZE);\n    await d.insert(schema.projects).values(batch);\n  }\n}\n```\n\n### Route Handler (src/routes/api.ts)\n\n```typescript\nimport { Hono } from 'hono';\nimport { zValidator } from '@hono/zod-validator';\nimport { z } from 'zod';\nimport type { Bindings } from '../types';\nimport * as queries from '../db/queries';\n\nconst api = new Hono<{ Bindings: Bindings }>();\n\n// Validation schemas\nconst createUserSchema = z.object({\n  email: z.string().email(),\n  name: z.string().optional(),\n});\n\nconst paginationSchema = z.object({\n  limit: z.coerce.number().min(1).max(100).default(20),\n  offset: z.coerce.number().min(0).default(0),\n});\n\n// GET /api/users/:id\napi.get('/users/:id', async (c) => {\n  const id = c.req.param('id');\n  const user = await queries.getUserById(c.env.DB, id);\n\n  if (!user) {\n    return c.json({ error: 'User not found' }, 404);\n  }\n\n  return c.json(user);\n});\n\n// POST /api/users\napi.post('/users', zValidator('json', createUserSchema), async (c) => {\n  const data = c.req.valid('json');\n  const id = crypto.randomUUID();\n\n  const user = await queries.createUser(c.env.DB, {\n    id,\n    ...data,\n  });\n\n  return c.json(user, 201);\n});\n\n// GET /api/users/:id/projects\napi.get(\n  '/users/:id/projects',\n  zValidator('query', paginationSchema),\n  async (c) => {\n    const userId = c.req.param('id');\n    const { limit, offset } = c.req.valid('query');\n\n    const projects = await queries.getProjectsByUser(c.env.DB, userId, {\n      limit,\n      offset,\n    });\n\n    return c.json({ projects, limit, offset });\n  }\n);\n\nexport { api as apiRoutes };\n```\n\n### Error Middleware (src/middleware/errors.ts)\n\n```typescript\nimport type { ErrorHandler } from 'hono';\nimport type { Bindings } from '../types';\n\nexport const errorHandler: ErrorHandler<{ Bindings: Bindings }> = (\n  err,\n  c\n) => {\n  console.error('Unhandled error:', err);\n\n  // Don't leak internal errors in production\n  if (c.env.ENVIRONMENT === 'production') {\n    return c.json({ error: 'Internal server error' }, 500);\n  }\n\n  return c.json(\n    {\n      error: err.message,\n      stack: err.stack,\n    },\n    500\n  );\n};\n```\n\n### Auth Middleware (src/middleware/auth.ts)\n\n```typescript\nimport { createMiddleware } from 'hono/factory';\nimport type { Bindings } from '../types';\n\ninterface AuthVariables {\n  userId: string;\n}\n\nexport const requireAuth = createMiddleware<{\n  Bindings: Bindings;\n  Variables: AuthVariables;\n}>(async (c, next) => {\n  const authHeader = c.req.header('Authorization');\n\n  if (!authHeader?.startsWith('Bearer ')) {\n    return c.json({ error: 'Missing authorization' }, 401);\n  }\n\n  const token = authHeader.slice(7);\n\n  // Validate token (replace with your auth logic)\n  try {\n    const userId = await validateToken(token, c.env);\n    c.set('userId', userId);\n    await next();\n  } catch {\n    return c.json({ error: 'Invalid token' }, 401);\n  }\n});\n\nasync function validateToken(token: string, env: Bindings): Promise<string> {\n  // Implement your token validation\n  // e.g., JWT verification, database lookup, etc.\n  throw new Error('Not implemented');\n}\n```\n\n### Queue Consumer (src/queue.ts)\n\n```typescript\nimport type { Bindings, QueueMessage } from './types';\n\nexport default {\n  async queue(\n    batch: MessageBatch<QueueMessage>,\n    env: Bindings\n  ): Promise<void> {\n    // Process in batches for efficiency\n    const messages = batch.messages;\n\n    for (const msg of messages) {\n      try {\n        await processMessage(msg.body, env);\n        msg.ack();\n      } catch (error) {\n        console.error('Failed to process message:', error);\n        // Will retry or go to DLQ based on wrangler config\n        msg.retry();\n      }\n    }\n  },\n};\n\nasync function processMessage(\n  message: QueueMessage,\n  env: Bindings\n): Promise<void> {\n  switch (message.type) {\n    case 'user.created':\n      await handleUserCreated(message.payload, env);\n      break;\n    case 'project.updated':\n      await handleProjectUpdated(message.payload, env);\n      break;\n    default:\n      console.warn('Unknown message type:', message.type);\n  }\n}\n\nasync function handleUserCreated(payload: unknown, env: Bindings) {\n  // Process user creation event\n}\n\nasync function handleProjectUpdated(payload: unknown, env: Bindings) {\n  // Process project update event\n}\n```\n\n### Health Check (src/routes/health.ts)\n\n```typescript\nimport { Hono } from 'hono';\nimport type { Bindings } from '../types';\n\nconst health = new Hono<{ Bindings: Bindings }>();\n\nhealth.get('/', async (c) => {\n  const checks: Record<string, 'ok' | 'error'> = {};\n\n  // Check D1\n  try {\n    await c.env.DB.prepare('SELECT 1').first();\n    checks.d1 = 'ok';\n  } catch {\n    checks.d1 = 'error';\n  }\n\n  // Check KV\n  try {\n    await c.env.CACHE.get('health-check');\n    checks.kv = 'ok';\n  } catch {\n    checks.kv = 'error';\n  }\n\n  const healthy = Object.values(checks).every((v) => v === 'ok');\n\n  return c.json(\n    {\n      status: healthy ? 'healthy' : 'degraded',\n      checks,\n      timestamp: new Date().toISOString(),\n    },\n    healthy ? 200 : 503\n  );\n});\n\nexport { health as healthRoutes };\n```\n\n### Migration Template (migrations/0001_initial.sql)\n\n```sql\n-- Migration: Initial schema\n-- Created: YYYY-MM-DD\n\nCREATE TABLE users (\n    id TEXT PRIMARY KEY,\n    email TEXT NOT NULL UNIQUE,\n    name TEXT,\n    created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE projects (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id),\n    title TEXT NOT NULL,\n    status TEXT NOT NULL DEFAULT 'draft' CHECK(status IN ('draft', 'active', 'archived')),\n    metadata TEXT,\n    created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Always create indexes for WHERE and ORDER BY columns\nCREATE INDEX idx_projects_user_id ON projects(user_id);\nCREATE INDEX idx_projects_status ON projects(status);\nCREATE INDEX idx_projects_user_status ON projects(user_id, status);\nCREATE INDEX idx_projects_created_at ON projects(created_at DESC);\n```\n\n### Package.json Template\n\n```json\n{\n  \"name\": \"worker-name\",\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"wrangler dev\",\n    \"deploy\": \"wrangler deploy\",\n    \"db:generate\": \"drizzle-kit generate\",\n    \"db:migrate\": \"wrangler d1 migrations apply DB\",\n    \"db:migrate:local\": \"wrangler d1 migrations apply DB --local\",\n    \"typecheck\": \"tsc --noEmit\"\n  },\n  \"dependencies\": {\n    \"hono\": \"^4.0.0\",\n    \"@hono/zod-validator\": \"^0.2.0\",\n    \"drizzle-orm\": \"^0.29.0\",\n    \"zod\": \"^3.22.0\"\n  },\n  \"devDependencies\": {\n    \"@cloudflare/workers-types\": \"^4.20240000.0\",\n    \"drizzle-kit\": \"^0.20.0\",\n    \"typescript\": \"^5.3.0\",\n    \"wrangler\": \"^3.0.0\"\n  }\n}\n```\n\n### TSConfig Template\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"lib\": [\"ES2022\"],\n    \"types\": [\"@cloudflare/workers-types\"],\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"noEmit\": true,\n    \"esModuleInterop\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"react-jsx\",\n    \"jsxImportSource\": \"hono/jsx\"\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\n## Best Practices\n\n### D1 Query Patterns\n\n```typescript\n// GOOD: Batch inserts\nconst BATCH_SIZE = 1000;\nfor (let i = 0; i < items.length; i += BATCH_SIZE) {\n  await db.insert(table).values(items.slice(i, i + BATCH_SIZE));\n}\n\n// BAD: Per-row inserts\nfor (const item of items) {\n  await db.insert(table).values(item); // N statements = N × cost\n}\n```\n\n### KV Caching Pattern\n\n```typescript\nasync function getCached<T>(\n  kv: KVNamespace,\n  key: string,\n  fetcher: () => Promise<T>,\n  ttl: number = 3600\n): Promise<T> {\n  const cached = await kv.get(key, 'json');\n  if (cached) return cached as T;\n\n  const fresh = await fetcher();\n  await kv.put(key, JSON.stringify(fresh), { expirationTtl: ttl });\n  return fresh;\n}\n```\n\n### Queue Publishing Pattern\n\n```typescript\n// Publish to queue with type safety\nasync function enqueue<T extends QueueMessage['type']>(\n  queue: Queue<QueueMessage>,\n  type: T,\n  payload: Extract<QueueMessage, { type: T }>['payload']\n) {\n  await queue.send({\n    type,\n    payload,\n    timestamp: Date.now(),\n  });\n}\n```\n\n### Error Handling Pattern\n\n```typescript\n// Custom error classes\nclass NotFoundError extends Error {\n  status = 404;\n}\n\nclass ValidationError extends Error {\n  status = 400;\n}\n\n// Error handler catches and formats\nconst errorHandler: ErrorHandler = (err, c) => {\n  const status = 'status' in err ? (err.status as number) : 500;\n  return c.json({ error: err.message }, status);\n};\n```\n\n## Service Bindings RPC\n\nFor Worker-to-Worker calls, use Service Bindings instead of HTTP:\n\n```typescript\n// In wrangler.jsonc\n{\n  \"services\": [\n    { \"binding\": \"AUTH_SERVICE\", \"service\": \"auth-worker\" }\n  ]\n}\n\n// In code\ninterface AuthService {\n  validateToken(token: string): Promise<{ userId: string }>;\n}\n\n// Call via RPC (no HTTP overhead)\nconst result = await c.env.AUTH_SERVICE.validateToken(token);\n```\n\n## Commands\n\n```bash\n# Generate migration from schema changes\nnpm run db:generate\n\n# Apply migrations locally\nnpm run db:migrate:local\n\n# Apply migrations to remote D1\nnpm run db:migrate\n\n# Development\nnpm run dev\n\n# Deploy\nnpm run deploy\n```"
              },
              {
                "name": "media-streaming",
                "description": "Implement Cloudflare Stream for video delivery and Images for image transformations. Use this skill when building media platforms, implementing video players, generating signed URLs, or optimizing image delivery with transformations.",
                "path": "skills/media-streaming/SKILL.md",
                "frontmatter": {
                  "name": "media-streaming",
                  "description": "Implement Cloudflare Stream for video delivery and Images for image transformations. Use this skill when building media platforms, implementing video players, generating signed URLs, or optimizing image delivery with transformations."
                },
                "content": "# Cloudflare Media & Streaming Skill\n\nBuild media-rich applications using Cloudflare Stream for video and Images for image transformations. Includes patterns for signed URLs, adaptive bitrate streaming, and responsive images.\n\n## Service Overview\n\n### Cloudflare Stream\n\n| Feature | Description | Pricing (2026) |\n|---------|-------------|----------------|\n| Storage | $5/1,000 min stored | Per minute |\n| Encoding | Included | Free |\n| Delivery | $1/1,000 min viewed | Per minute watched |\n| Live | $1/1,000 min live | Per minute streamed |\n| Signed URLs | Included | Free |\n\n### Cloudflare Images\n\n| Feature | Description | Pricing (2026) |\n|---------|-------------|----------------|\n| Storage | $5/100K images | Per image stored |\n| Transformations | $0.50/1,000 unique | Per unique transform |\n| Delivery | $1/100K images | Per image served |\n| Variants | 100 named variants | Included |\n\n## Cloudflare Stream Patterns\n\n### Pattern 1: Video Upload with Signed URL\n\n```typescript\n// api/videos/upload.ts - Generate upload URL\ninterface UploadRequest {\n  userId: string;\n  maxDurationSeconds?: number;\n  meta?: Record<string, string>;\n}\n\nexport async function createUploadUrl(\n  env: Env,\n  request: UploadRequest\n): Promise<{ uploadUrl: string; videoId: string }> {\n  const response = await fetch(\n    `https://api.cloudflare.com/client/v4/accounts/${env.CF_ACCOUNT_ID}/stream/direct_upload`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${env.CF_API_TOKEN}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        maxDurationSeconds: request.maxDurationSeconds || 3600,  // 1 hour default\n        expiry: new Date(Date.now() + 30 * 60 * 1000).toISOString(),  // 30 min\n        requireSignedURLs: true,\n        allowedOrigins: ['https://your-app.com'],\n        meta: {\n          userId: request.userId,\n          ...request.meta,\n        },\n        thumbnailTimestampPct: 0.5,\n      }),\n    }\n  );\n\n  const result = await response.json();\n\n  if (!result.success) {\n    throw new Error(result.errors[0]?.message || 'Upload creation failed');\n  }\n\n  return {\n    uploadUrl: result.result.uploadURL,\n    videoId: result.result.uid,\n  };\n}\n```\n\n### Pattern 2: Signed Video Playback URL\n\n```typescript\n// api/videos/playback.ts - Generate signed playback URL\nimport { base64url } from 'rfc4648';\n\ninterface SignedUrlOptions {\n  videoId: string;\n  expiresIn?: number;  // seconds\n  accessRules?: AccessRule[];\n}\n\ninterface AccessRule {\n  type: 'ip.src' | 'ip.geoip.country' | 'any';\n  action: 'allow' | 'block';\n  value?: string[];\n  country?: string[];\n}\n\nexport async function createSignedPlaybackUrl(\n  env: Env,\n  options: SignedUrlOptions\n): Promise<string> {\n  const { videoId, expiresIn = 3600, accessRules } = options;\n\n  // Token creation using Stream's signing key\n  const expiry = Math.floor(Date.now() / 1000) + expiresIn;\n\n  // Build token payload\n  const tokenPayload = {\n    sub: videoId,\n    kid: env.STREAM_SIGNING_KEY_ID,\n    exp: expiry,\n    accessRules: accessRules || [{ type: 'any', action: 'allow' }],\n  };\n\n  // Sign with RSA-256 or use Cloudflare's token endpoint\n  const signedToken = await signStreamToken(env, tokenPayload);\n\n  // Return signed URL\n  return `https://customer-${env.CF_CUSTOMER_SUBDOMAIN}.cloudflarestream.com/${videoId}/manifest/video.m3u8?token=${signedToken}`;\n}\n\n// Alternative: Use Cloudflare API to generate token\nasync function signStreamToken(env: Env, payload: any): Promise<string> {\n  const response = await fetch(\n    `https://api.cloudflare.com/client/v4/accounts/${env.CF_ACCOUNT_ID}/stream/${payload.sub}/token`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${env.CF_API_TOKEN}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        id: payload.kid,\n        exp: payload.exp,\n        accessRules: payload.accessRules,\n      }),\n    }\n  );\n\n  const result = await response.json();\n  return result.result.token;\n}\n```\n\n### Pattern 3: HLS.js Player Integration\n\n```html\n<!-- Video player with Stream -->\n<video id=\"player\" controls></video>\n\n<script src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\"></script>\n<script>\nasync function loadVideo(videoId) {\n  // Get signed URL from your API\n  const response = await fetch(`/api/videos/${videoId}/playback`);\n  const { playbackUrl } = await response.json();\n\n  const video = document.getElementById('player');\n\n  if (Hls.isSupported()) {\n    const hls = new Hls();\n    hls.loadSource(playbackUrl);\n    hls.attachMedia(video);\n    hls.on(Hls.Events.MANIFEST_PARSED, () => video.play());\n  } else if (video.canPlayType('application/vnd.apple.mpegurl')) {\n    // Safari native HLS\n    video.src = playbackUrl;\n    video.addEventListener('loadedmetadata', () => video.play());\n  }\n}\n</script>\n```\n\n### Pattern 4: Stream Webhook Handler\n\n```typescript\n// api/webhooks/stream.ts - Handle video processing events\nexport async function handleStreamWebhook(\n  request: Request,\n  env: Env\n): Promise<Response> {\n  // Verify webhook signature\n  const signature = request.headers.get('Webhook-Signature');\n  const body = await request.text();\n\n  if (!verifySignature(body, signature, env.STREAM_WEBHOOK_SECRET)) {\n    return new Response('Invalid signature', { status: 401 });\n  }\n\n  const event = JSON.parse(body);\n\n  switch (event.type) {\n    case 'ready':\n      // Video is ready for playback\n      await handleVideoReady(env, event.payload);\n      break;\n\n    case 'error':\n      // Video processing failed\n      await handleVideoError(env, event.payload);\n      break;\n\n    case 'live_input.connected':\n      // Live stream started\n      await handleLiveStart(env, event.payload);\n      break;\n\n    case 'live_input.disconnected':\n      // Live stream ended\n      await handleLiveEnd(env, event.payload);\n      break;\n  }\n\n  return new Response('OK');\n}\n\nasync function handleVideoReady(env: Env, payload: any) {\n  const { uid, duration, meta, thumbnail } = payload;\n\n  await env.DB.prepare(\n    `UPDATE videos SET status = 'ready', duration = ?, thumbnail_url = ?\n     WHERE stream_id = ?`\n  ).bind(duration, thumbnail, uid).run();\n\n  // Notify user\n  if (meta?.userId) {\n    await sendNotification(env, meta.userId, 'Your video is ready!');\n  }\n}\n```\n\n## Cloudflare Images Patterns\n\n### Pattern 1: Image Upload API\n\n```typescript\n// api/images/upload.ts\nexport async function uploadImage(\n  env: Env,\n  file: File,\n  metadata: Record<string, string>\n): Promise<{ imageId: string; url: string }> {\n  const formData = new FormData();\n  formData.append('file', file);\n  formData.append('metadata', JSON.stringify(metadata));\n  formData.append('requireSignedURLs', 'false');\n\n  const response = await fetch(\n    `https://api.cloudflare.com/client/v4/accounts/${env.CF_ACCOUNT_ID}/images/v1`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${env.CF_API_TOKEN}`,\n      },\n      body: formData,\n    }\n  );\n\n  const result = await response.json();\n\n  if (!result.success) {\n    throw new Error(result.errors[0]?.message || 'Upload failed');\n  }\n\n  return {\n    imageId: result.result.id,\n    url: result.result.variants[0],\n  };\n}\n```\n\n### Pattern 2: Image URL Transformations\n\n```typescript\n// utils/images.ts - Build transformation URLs\n\ntype ImageFit = 'scale-down' | 'contain' | 'cover' | 'crop' | 'pad';\ntype ImageFormat = 'webp' | 'avif' | 'jpeg' | 'png' | 'gif';\ntype ImageGravity = 'auto' | 'face' | 'top' | 'bottom' | 'left' | 'right' | 'center';\n\ninterface ImageTransformOptions {\n  width?: number;\n  height?: number;\n  fit?: ImageFit;\n  format?: ImageFormat;\n  quality?: number;\n  gravity?: ImageGravity;\n  blur?: number;  // 1-250\n  sharpen?: number;  // 0-10\n  brightness?: number;  // -1 to 1\n  contrast?: number;  // -1 to 1\n  dpr?: number;  // Device pixel ratio\n  background?: string;  // For 'pad' fit\n}\n\nexport function buildImageUrl(\n  accountHash: string,\n  imageId: string,\n  options: ImageTransformOptions\n): string {\n  const transforms: string[] = [];\n\n  if (options.width) transforms.push(`w=${options.width}`);\n  if (options.height) transforms.push(`h=${options.height}`);\n  if (options.fit) transforms.push(`fit=${options.fit}`);\n  if (options.format) transforms.push(`f=${options.format}`);\n  if (options.quality) transforms.push(`q=${options.quality}`);\n  if (options.gravity) transforms.push(`g=${options.gravity}`);\n  if (options.blur) transforms.push(`blur=${options.blur}`);\n  if (options.sharpen) transforms.push(`sharpen=${options.sharpen}`);\n  if (options.brightness) transforms.push(`brightness=${options.brightness}`);\n  if (options.contrast) transforms.push(`contrast=${options.contrast}`);\n  if (options.dpr) transforms.push(`dpr=${options.dpr}`);\n  if (options.background) transforms.push(`background=${options.background}`);\n\n  const transformString = transforms.join(',');\n\n  return `https://imagedelivery.net/${accountHash}/${imageId}/${transformString || 'public'}`;\n}\n\n// Usage examples\nconst thumbnailUrl = buildImageUrl(ACCOUNT_HASH, imageId, {\n  width: 300,\n  height: 200,\n  fit: 'cover',\n  format: 'webp',\n  quality: 80,\n});\n\nconst avatarUrl = buildImageUrl(ACCOUNT_HASH, imageId, {\n  width: 128,\n  height: 128,\n  fit: 'cover',\n  gravity: 'face',\n  format: 'webp',\n});\n```\n\n### Pattern 3: Named Variants\n\nDefine reusable transformation presets via Cloudflare Dashboard or API:\n\n```typescript\n// Create named variants via API\nconst variants = [\n  { id: 'thumbnail', fit: 'cover', width: 300, height: 200 },\n  { id: 'avatar', fit: 'cover', width: 128, height: 128 },\n  { id: 'hero', fit: 'cover', width: 1920, height: 1080 },\n  { id: 'og', fit: 'cover', width: 1200, height: 630 },  // Open Graph\n];\n\n// Usage with named variant\nconst url = `https://imagedelivery.net/${ACCOUNT_HASH}/${imageId}/thumbnail`;\n```\n\n### Pattern 4: Responsive Images with srcset\n\n```typescript\n// components/ResponsiveImage.tsx\ninterface ResponsiveImageProps {\n  imageId: string;\n  alt: string;\n  sizes: string;\n  className?: string;\n}\n\nexport function ResponsiveImage({ imageId, alt, sizes, className }: ResponsiveImageProps) {\n  const accountHash = process.env.CF_IMAGES_HASH;\n\n  const srcset = [320, 640, 960, 1280, 1920]\n    .map(w => `https://imagedelivery.net/${accountHash}/${imageId}/w=${w},f=auto ${w}w`)\n    .join(', ');\n\n  return (\n    <img\n      src={`https://imagedelivery.net/${accountHash}/${imageId}/w=960,f=auto`}\n      srcSet={srcset}\n      sizes={sizes}\n      alt={alt}\n      className={className}\n      loading=\"lazy\"\n      decoding=\"async\"\n    />\n  );\n}\n\n// Usage\n<ResponsiveImage\n  imageId=\"abc123\"\n  alt=\"Product image\"\n  sizes=\"(max-width: 768px) 100vw, 50vw\"\n/>\n```\n\n### Pattern 5: Image Transform via Worker\n\nUse R2 + Image Resizing for on-the-fly transforms:\n\n```typescript\n// workers/image-transform.ts\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const url = new URL(request.url);\n    const path = url.pathname;\n\n    // Parse transform options from URL\n    // Format: /transform/w=300,h=200,fit=cover/{imagePath}\n    const match = path.match(/^\\/transform\\/([^/]+)\\/(.+)$/);\n    if (!match) {\n      return new Response('Not found', { status: 404 });\n    }\n\n    const [, optionsStr, imagePath] = match;\n    const options = parseTransformOptions(optionsStr);\n\n    // Fetch original from R2\n    const object = await env.R2_IMAGES.get(imagePath);\n    if (!object) {\n      return new Response('Image not found', { status: 404 });\n    }\n\n    // Apply transformations via cf.image\n    return fetch(request.url, {\n      cf: {\n        image: {\n          width: options.width,\n          height: options.height,\n          fit: options.fit || 'cover',\n          format: 'auto',  // Auto-detect WebP/AVIF support\n          quality: options.quality || 85,\n        },\n      },\n    });\n  },\n};\n\nfunction parseTransformOptions(str: string): Record<string, any> {\n  const options: Record<string, any> = {};\n  str.split(',').forEach(part => {\n    const [key, value] = part.split('=');\n    options[key] = isNaN(Number(value)) ? value : Number(value);\n  });\n  return options;\n}\n```\n\n## Live Streaming Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Broadcaster\"\n        OBS[OBS/Encoder]\n    end\n    subgraph \"Cloudflare Stream\"\n        RTMPS[RTMPS Ingest]\n        Encode[Real-time Encoding]\n        HLS[HLS/DASH Output]\n    end\n    subgraph \"Viewers\"\n        P1[Player 1]\n        P2[Player 2]\n        PN[Player N]\n    end\n\n    OBS -->|RTMPS| RTMPS --> Encode --> HLS\n    HLS --> P1\n    HLS --> P2\n    HLS --> PN\n```\n\n### Live Input Configuration\n\n```typescript\n// Create live input\nasync function createLiveInput(env: Env, name: string): Promise<LiveInput> {\n  const response = await fetch(\n    `https://api.cloudflare.com/client/v4/accounts/${env.CF_ACCOUNT_ID}/stream/live_inputs`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${env.CF_API_TOKEN}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        meta: { name },\n        recording: {\n          mode: 'automatic',  // or 'off'\n          timeoutSeconds: 0,  // No timeout\n          requireSignedURLs: true,\n        },\n      }),\n    }\n  );\n\n  const result = await response.json();\n\n  return {\n    uid: result.result.uid,\n    rtmps: result.result.rtmps,\n    srt: result.result.srt,\n    webRTC: result.result.webRTC,\n  };\n}\n\n// RTMPS URL format:\n// rtmps://live.cloudflare.com:443/live/{streamKey}\n```\n\n## Security Best Practices\n\n### Signed URL Requirements\n\n| Use Case | Signed URL | Expiration | Notes |\n|----------|------------|------------|-------|\n| Paid content | Required | 1-4 hours | Short expiry for VOD |\n| User uploads | Required | 30 minutes | For upload URL only |\n| Live streams | Recommended | Per-session | Regenerate on refresh |\n| Public content | Optional | N/A | For analytics tracking |\n\n### Access Control Rules\n\n```typescript\nconst accessRules = [\n  // Allow from specific countries\n  {\n    type: 'ip.geoip.country',\n    action: 'allow',\n    country: ['US', 'CA', 'GB'],\n  },\n  // Block specific IPs (abuse prevention)\n  {\n    type: 'ip.src',\n    action: 'block',\n    value: ['192.168.1.1'],\n  },\n  // Require referrer (hotlink protection)\n  {\n    type: 'any',\n    action: 'allow',\n    // Combined with allowedOrigins in upload config\n  },\n];\n```\n\n## Wrangler Configuration\n\n```jsonc\n{\n  \"name\": \"media-platform\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2025-01-01\",\n\n  \"d1_databases\": [\n    { \"binding\": \"DB\", \"database_name\": \"media-db\", \"database_id\": \"...\" }\n  ],\n\n  \"r2_buckets\": [\n    { \"binding\": \"R2_IMAGES\", \"bucket_name\": \"images\" },\n    { \"binding\": \"R2_VIDEOS\", \"bucket_name\": \"videos-raw\" }\n  ],\n\n  \"vars\": {\n    \"CF_ACCOUNT_ID\": \"your-account-id\",\n    \"CF_IMAGES_HASH\": \"your-images-hash\",\n    \"STREAM_SIGNING_KEY_ID\": \"key-id\"\n  }\n}\n```\n\n## Cost Optimization\n\n### Stream\n\n- **Encode once, deliver many**: Re-encode only when quality issues\n- **Set max duration**: Prevent infinite uploads\n- **Use signed URLs**: Prevent bandwidth abuse\n- **Monitor viewer minutes**: Primary cost driver\n\n### Images\n\n- **Use format=auto**: Let Cloudflare choose optimal format\n- **Cache transforms**: Same transform = 1 unique transform charge\n- **Batch uploads**: Reduce API calls\n- **Clean up unused**: Delete orphaned images monthly\n\n## Output Format\n\n```markdown\n# Media Delivery Report\n\n## Stream Statistics\n\n| Metric | Value | Cost Estimate |\n|--------|-------|---------------|\n| Videos stored | 1,500 min | $7.50/month |\n| Minutes viewed (30d) | 50,000 min | $50/month |\n| Unique videos | 45 | - |\n\n## Images Statistics\n\n| Metric | Value | Cost Estimate |\n|--------|-------|---------------|\n| Images stored | 25,000 | $1.25/month |\n| Unique transforms | 75,000 | $37.50/month |\n| Images delivered | 2M | $20/month |\n\n## Optimization Opportunities\n\n| Issue | Current | Optimized | Savings |\n|-------|---------|-----------|---------|\n| Unused transforms | 500 variants | 50 variants | ~$22/mo |\n| Oversized images | avg 2000px | max 1920px | ~$5/mo |\n```\n\n## Tips\n\n- **Auto-detect format**: Always use `format=auto` for best compression\n- **Lazy loading**: Add `loading=\"lazy\"` to images below fold\n- **Preload critical**: Use `<link rel=\"preload\">` for hero images\n- **Stream analytics**: Use webhooks to track engagement\n- **Thumbnail timing**: Set `thumbnailTimestampPct` for better previews\n- **Regional delivery**: Stream uses Cloudflare's global network automatically"
              },
              {
                "name": "optimize-costs",
                "description": "Analyze Cloudflare architecture and predict monthly costs with optimization recommendations. Use this skill when the user asks about costs, billing, pricing, or wants to understand their Cloudflare spend. Works with wrangler configs, observability data, and AI Gateway logs.",
                "path": "skills/optimize-costs/SKILL.md",
                "frontmatter": {
                  "name": "optimize-costs",
                  "description": "Analyze Cloudflare architecture and predict monthly costs with optimization recommendations. Use this skill when the user asks about costs, billing, pricing, or wants to understand their Cloudflare spend. Works with wrangler configs, observability data, and AI Gateway logs."
                },
                "content": "# Cloudflare Cost Optimization Skill\n\nAnalyze Cloudflare architectures and predict monthly costs with actionable optimization recommendations. This skill provides engineering-grade cost estimates based on 2026 Cloudflare pricing.\n\n## Pricing Reference (2026)\n\n### Workers\n- **Requests**: $0.30/million (after 10M free)\n- **CPU Time**: $0.02/million GB-seconds\n- **Unbound**: $12.50/million duration-ms (15M free)\n- **Subrequests**: Count against request limits\n\n### D1 (SQLite)\n- **Reads**: $0.25/billion rows\n- **Writes**: $1.00/million rows (4x more expensive than reads!)\n- **Storage**: $0.75/GB/month\n- **Cost Trap**: `for(row){db.insert()}` = N× writes. Always batch ≤1,000.\n\n### R2 (Object Storage)\n- **Class A (writes)**: $4.50/million\n- **Class B (reads)**: $0.36/million\n- **Storage**: $0.015/GB/month\n- **Egress**: FREE (major advantage)\n\n### KV (Key-Value)\n- **Reads**: $0.50/million\n- **Writes**: $5.00/million (10x more than reads!)\n- **Storage**: $0.50/GB/month\n- **Rate Limit**: 1 write/sec/key\n\n### Queues\n- **Standard**: $0.40/million messages\n- **Batch**: $0.40/million batches\n- **Cost Trap**: `max_retries: 3` = up to 3× message cost\n\n### Vectorize\n- **Queries**: $0.01/million\n- **Stored Vectors**: $0.05/100M dimensions×vectors\n- **Hard Limit**: 5M vectors/index, 1,536 dimensions\n\n### Workers AI\n- **Neurons**: $0.011/1,000 (reset daily UTC)\n- **Large Models** (Llama 11B+): $0.68/M output tokens - expensive!\n- **Recommendation**: Use smaller models (1B-8B) or Gemini Flash for bulk\n\n### AI Gateway\n- **Caching**: Only caches IDENTICAL prompts (no semantic)\n- **Logs**: 10M free, then $0.10/million\n- **Cost Trap**: Forgetting cache = paying full LLM cost every time\n\n### Analytics Engine\n- **Essentially FREE** - no per-write charges\n- **Note**: Use `SUM(_sample_interval)` at scale (adaptive sampling)\n\n## Analysis Workflow\n\n### Step 1: Gather Architecture Data\n\nUse MCP tools to collect current usage:\n\n```\n1. Read wrangler.toml/wrangler.jsonc for bindings\n2. Query cloudflare-observability for Worker metrics\n3. Query cloudflare-ai-gateway for AI costs\n4. Check cloudflare-bindings for resource lists\n```\n\n### Step 2: Calculate Per-Service Costs\n\nFor each service bound in wrangler config:\n\n**Workers:**\n```\nmonthly_cost = (requests - 10M) / 1M * $0.30\n            + cpu_gb_seconds / 1M * $0.02\n```\n\n**D1:**\n```\nmonthly_cost = reads / 1B * $0.25\n            + writes / 1M * $1.00\n            + storage_gb * $0.75\n```\n\n**R2:**\n```\nmonthly_cost = class_a_ops / 1M * $4.50\n            + class_b_ops / 1M * $0.36\n            + storage_gb * $0.015\n```\n\n**KV:**\n```\nmonthly_cost = reads / 1M * $0.50\n            + writes / 1M * $5.00\n            + storage_gb * $0.50\n```\n\n**Queues:**\n```\nmonthly_cost = messages / 1M * $0.40 * (1 + avg_retries)\n```\n\n### Step 3: Identify Cost Drivers\n\nFlag any service that's >20% of total cost. Common patterns:\n\n| Cost Driver | Typical Cause | Fix |\n|-------------|--------------|-----|\n| D1 writes dominating | Per-row inserts | Batch to ≤1,000 |\n| Queue costs high | Retries enabled | Set `max_retries: 1` if idempotent |\n| AI Gateway expensive | No caching | Enable cache, deduplicate prompts |\n| Workers AI | Large model | Switch to smaller model or external LLM |\n| R2 Class A | Frequent writes | Buffer writes, use R2 presigned |\n\n### Step 4: Generate Recommendations\n\nFor each optimization opportunity, provide:\n\n1. **Current**: What it costs now\n2. **Optimized**: What it could cost\n3. **Savings**: Monthly/annual savings\n4. **Trade-off**: What changes in behavior\n5. **Implementation**: Specific code/config change\n\n## Output Format\n\n```markdown\n# Cloudflare Cost Analysis\n\n## Monthly Cost Estimate: $X.XX\n\n### Breakdown by Service\n\n| Service | Cost | % of Total | Status |\n|---------|------|------------|--------|\n| D1 | $X.XX | X% | ⚠️ Cost driver |\n| Workers | $X.XX | X% | ✅ Normal |\n| R2 | $X.XX | X% | ✅ Normal |\n\n### Cost Drivers Identified\n\n1. **D1 Writes** (80% of total)\n   - Current: 50M writes/month = $50\n   - Pattern detected: Per-row inserts in cron job\n   - Fix: Batch inserts to ≤1,000 rows\n\n### Optimization Opportunities\n\n| Opportunity | Current | Optimized | Savings | Effort |\n|-------------|---------|-----------|---------|--------|\n| Batch D1 writes | $50/mo | $5/mo | $45/mo ($540/yr) | Low |\n| Reduce queue retries | $10/mo | $3/mo | $7/mo ($84/yr) | Trivial |\n\n### Warnings\n\n- ⚠️ D1 writes >50M/day is a red flag\n- ⚠️ Workers AI Llama 11B is expensive for high-volume\n\n### Action Items\n\n1. [ ] Change `for(row){insert()}` to `db.batch()` in `processor.ts`\n2. [ ] Set `max_retries: 1` for `layer2-queue` in wrangler.jsonc\n3. [ ] Consider switching AI model from llama-3-11b to llama-3-8b\n```\n\n## MCP Tools to Use\n\n- `mcp__cloudflare-observability__query_worker_observability` - Worker request/duration metrics\n- `mcp__cloudflare-ai-gateway__list_logs` - AI request costs\n- `mcp__cloudflare-bindings__workers_get_worker` - Worker details\n- `mcp__cloudflare-bindings__d1_databases_list` - D1 databases\n- `mcp__cloudflare-bindings__r2_buckets_list` - R2 buckets\n- `mcp__cloudflare-bindings__kv_namespaces_list` - KV namespaces\n\n## Tips\n\n- **D1 is usually the culprit**: Writes are 4× more expensive than reads\n- **Queue retries multiply costs**: Each retry = another message charge\n- **Analytics Engine is nearly free**: Use it heavily for metrics\n- **R2 egress is free**: Use R2 over S3 when possible\n- **AI caching only works for identical prompts**: Deduplicate inputs\n\n## Example Usage\n\nWhen user asks:\n- \"How much is this costing me?\"\n- \"Optimize my Cloudflare costs\"\n- \"Why is my D1 bill so high?\"\n- \"Estimate monthly costs for this architecture\"\n\nInvoke this skill to provide detailed cost analysis with actionable recommendations."
              },
              {
                "name": "patterns",
                "description": "Cloudflare architecture patterns for common problems. Reference when decomposing monolithic Workers, optimizing D1 writes, or adding external API resilience.",
                "path": "skills/patterns/SKILL.md",
                "frontmatter": {
                  "name": "patterns",
                  "description": "Cloudflare architecture patterns for common problems. Reference when decomposing monolithic Workers, optimizing D1 writes, or adding external API resilience.",
                  "triggers": [
                    "monolithic Worker",
                    "subrequest limits",
                    "D1 write costs",
                    "batch inserts",
                    "external API failures",
                    "circuit breaker",
                    "resilience patterns",
                    "decomposing Workers"
                  ]
                },
                "content": "# Cloudflare Architecture Patterns\n\nProven patterns for solving common Cloudflare architecture challenges. Each pattern includes symptoms (when to apply), detection methods, implementation steps, and trade-offs.\n\n## Pattern Catalog\n\n| Pattern | Problem | Solution | Effort |\n|---------|---------|----------|--------|\n| [service-bindings](#service-bindings) | Monolithic Worker hitting subrequest limits | Decompose with RPC | Medium |\n| [d1-batching](#d1-batching) | High D1 write costs from per-row inserts | Batch writes | Low |\n| [circuit-breaker](#circuit-breaker) | External API failures cascading | Fail-fast with fallback | Medium |\n\n---\n\n## When to Apply Patterns\n\n### Service Bindings Pattern\n\n**Trigger Conditions**:\n- Subrequest count approaching 1,000/request limit\n- Single Worker handling unrelated domains (auth, data, notifications)\n- HTTP `fetch()` between internal Workers\n- Worker file > 1MB after bundling\n- Multiple teams need to deploy independently\n\n**Detection** (static):\n```javascript\n// Anti-pattern: HTTP fetch to internal service\nconst response = await fetch(`${AUTH_SERVICE_URL}/validate`, {\n  headers: { Authorization: token }\n});\n```\n\n**Detection** (live):\n- Check observability for high subrequest counts\n- Look for latency patterns indicating network hops\n\n### D1 Batching Pattern\n\n**Trigger Conditions**:\n- D1 writes represent > 80% of costs\n- Per-row INSERT in loops detected in code\n- Cron jobs with unbatched writes\n- Webhook handlers inserting one record at a time\n\n**Detection** (static):\n```javascript\n// Anti-pattern: Per-row inserts in loop\nfor (const item of items) {\n  await db.run('INSERT INTO items (name) VALUES (?)', [item.name]);\n}\n```\n\n**Detection** (live):\n- D1 write count >> expected record count\n- High write costs relative to data volume\n\n### Circuit Breaker Pattern\n\n**Trigger Conditions**:\n- External API calls without timeout\n- No fallback for third-party failures\n- Error rate spikes correlating with upstream issues\n- User-facing errors caused by backend service outages\n\n**Detection** (static):\n```javascript\n// Anti-pattern: Unbounded external fetch\nconst data = await fetch('https://external-api.com/data');\n// No timeout, no fallback, no retry logic\n```\n\n**Detection** (live):\n- Error rate spikes in observability\n- Latency P99 >> P50 (indicating timeouts)\n- Correlation between external service status and Worker errors\n\n---\n\n## Pattern Details\n\nDetailed implementation guides are in separate files:\n\n- @service-bindings.md - Decompose monolithic Workers into service-bound microservices\n- @d1-batching.md - Optimize D1 write costs with batch operations\n- @circuit-breaker.md - Add resilience for external API dependencies\n\n---\n\n## Pattern Selection Guide\n\n```\nIs your Worker > 500 lines or hitting subrequest limits?\n├─ Yes → Consider SERVICE-BINDINGS pattern\n└─ No\n   │\n   Is D1 your primary cost driver (>50%)?\n   ├─ Yes → Apply D1-BATCHING pattern\n   └─ No\n      │\n      Do you call external APIs?\n      ├─ Yes → Apply CIRCUIT-BREAKER pattern\n      └─ No → Review other optimization opportunities\n```\n\n---\n\n## Combining Patterns\n\nPatterns can be combined. Common combinations:\n\n1. **Service Bindings + D1 Batching**: Decompose monolith AND optimize data layer\n2. **Circuit Breaker + Service Bindings**: Each microservice has its own resilience\n3. **All Three**: Large applications benefit from all patterns\n\n**Implementation Order** (recommended):\n1. Circuit Breaker first (lowest risk, immediate resilience benefit)\n2. D1 Batching second (cost savings without architecture change)\n3. Service Bindings last (largest change, needs careful planning)\n\n---\n\n## Pattern Anti-Patterns\n\nThings to avoid when implementing patterns:\n\n| Pattern | Anti-Pattern | Why It's Bad |\n|---------|--------------|--------------|\n| Service Bindings | Too many small Workers | Complexity overhead exceeds benefit |\n| Service Bindings | Circular dependencies | Deadlock risk, hard to reason about |\n| D1 Batching | Batches too large | Memory pressure, timeout risk |\n| D1 Batching | No error handling per batch | One bad record fails entire batch |\n| Circuit Breaker | Circuit never closes | Permanent degraded mode |\n| Circuit Breaker | No fallback defined | Fail-fast with no alternative |\n\n---\n\n## Future Patterns (Roadmap)\n\nPatterns to be added:\n\n- **read-replication**: Durable Objects for regional caching\n- **event-sourcing**: Queues + D1 for audit trails\n- **rate-limiting**: KV-based distributed rate limiting\n- **edge-caching**: Cache API patterns for dynamic content"
              },
              {
                "name": "probes",
                "description": "Pre-built audit probes for Cloudflare services. Reference these query patterns when validating D1 indexes, observability metrics, AI Gateway costs, and queue health via MCP tools.",
                "path": "skills/probes/SKILL.md",
                "frontmatter": {
                  "name": "probes",
                  "description": "Pre-built audit probes for Cloudflare services. Reference these query patterns when validating D1 indexes, observability metrics, AI Gateway costs, and queue health via MCP tools.",
                  "triggers": [
                    "validating D1 queries",
                    "checking observability data",
                    "auditing AI Gateway costs",
                    "queue health checks",
                    "MCP tool orchestration",
                    "EXPLAIN QUERY PLAN"
                  ]
                },
                "content": "# Cloudflare Audit Probes\n\nPre-built diagnostic queries for live validation. Use these probes when `--validate` mode is enabled to compare static analysis findings against actual production data.\n\n## MCP Availability Check\n\nBefore running any probes, verify MCP tool availability:\n\n```javascript\n// Lightweight probe to test MCP connectivity\nmcp__cloudflare-bindings__workers_list()\n\n// Expected: Returns array of workers\n// Failure: MCP tools unavailable, fall back to static analysis\n```\n\n**Graceful Degradation**:\n- If MCP call succeeds: Proceed with live validation\n- If MCP call fails/times out: Note \"MCP tools unavailable\" and continue with static analysis\n- Tag all findings appropriately: `[STATIC]`, `[LIVE-VALIDATED]`, `[LIVE-REFUTED]`, `[INCOMPLETE]`\n\n---\n\n## D1 Database Probes\n\n### Schema Discovery\n\nList all tables in the database:\n\n```sql\nSELECT name, sql\nFROM sqlite_master\nWHERE type='table'\nORDER BY name;\n```\n\n**MCP Call**:\n```javascript\nmcp__cloudflare-bindings__d1_database_query({\n  database_id: \"your-database-id\",\n  sql: \"SELECT name, sql FROM sqlite_master WHERE type='table' ORDER BY name\"\n})\n```\n\n### Index Inventory\n\nList all indexes with their definitions:\n\n```sql\nSELECT name, tbl_name, sql\nFROM sqlite_master\nWHERE type='index'\nAND name NOT LIKE 'sqlite_%'\nORDER BY tbl_name, name;\n```\n\n**Interpretation**:\n- Missing indexes on foreign keys = potential performance issue\n- Missing indexes on frequently filtered columns = query scan risk\n\n### Query Plan Analysis\n\nValidate query efficiency:\n\n```sql\nEXPLAIN QUERY PLAN SELECT * FROM users WHERE email = ?;\n```\n\n**MCP Call**:\n```javascript\nmcp__cloudflare-bindings__d1_database_query({\n  database_id: \"your-database-id\",\n  sql: \"EXPLAIN QUERY PLAN SELECT * FROM users WHERE email = ?\"\n})\n```\n\n**Interpretation**:\n| Output Contains | Meaning | Action |\n|----------------|---------|--------|\n| `SCAN TABLE` | Full table scan, no index | Add index on filtered column |\n| `SEARCH USING INDEX` | Index used | Good performance |\n| `COVERING INDEX` | Index contains all needed data | Optimal |\n| `USING TEMP B-TREE` | Temporary sort required | Consider index on ORDER BY column |\n\n### Table Row Counts\n\nGet approximate table sizes:\n\n```sql\nSELECT name,\n       (SELECT COUNT(*) FROM pragma_table_info(name)) as columns,\n       (SELECT seq FROM sqlite_sequence WHERE name = m.name) as approx_rows\nFROM sqlite_master m\nWHERE type='table'\nAND name NOT LIKE 'sqlite_%';\n```\n\n---\n\n## Observability Probes\n\n### Error Rate by Worker (7 days)\n\n```javascript\nmcp__cloudflare-observability__query_worker_observability({\n  view: \"calculations\",\n  parameters: {\n    calculations: [\n      { operator: \"count\", as: \"total_requests\" },\n      {\n        operator: \"countIf\",\n        as: \"errors\",\n        condition: {\n          field: \"$metadata.outcome\",\n          operator: \"eq\",\n          value: \"exception\"\n        }\n      }\n    ],\n    groupBys: [\n      { type: \"string\", value: \"$metadata.service\" }\n    ]\n  },\n  timeframe: {\n    reference: \"now\",\n    offset: \"-7d\"\n  }\n})\n```\n\n**Interpretation**:\n- Error rate > 1%: Investigate immediately\n- Error rate > 0.1%: Monitor closely\n- Compare static findings against actual error patterns\n\n### Latency Percentiles by Endpoint\n\n```javascript\nmcp__cloudflare-observability__query_worker_observability({\n  view: \"calculations\",\n  parameters: {\n    calculations: [\n      { operator: \"p50\", field: \"$metadata.duration\", as: \"p50_ms\" },\n      { operator: \"p95\", field: \"$metadata.duration\", as: \"p95_ms\" },\n      { operator: \"p99\", field: \"$metadata.duration\", as: \"p99_ms\" }\n    ],\n    groupBys: [\n      { type: \"string\", value: \"$metadata.path\" }\n    ]\n  },\n  timeframe: {\n    reference: \"now\",\n    offset: \"-24h\"\n  }\n})\n```\n\n**Interpretation**:\n- P99 > 10s: Likely timeout issues\n- P95/P50 ratio > 10: High variance, investigate outliers\n- Compare against D1 query patterns identified in static analysis\n\n### Request Volume by Endpoint\n\n```javascript\nmcp__cloudflare-observability__query_worker_observability({\n  view: \"calculations\",\n  parameters: {\n    calculations: [\n      { operator: \"count\", as: \"requests\" }\n    ],\n    groupBys: [\n      { type: \"string\", value: \"$metadata.path\" },\n      { type: \"time\", interval: \"1h\" }\n    ]\n  },\n  timeframe: {\n    reference: \"now\",\n    offset: \"-7d\"\n  }\n})\n```\n\n**Use Case**: Identify high-traffic endpoints for cost optimization focus.\n\n### CPU Time Analysis\n\n```javascript\nmcp__cloudflare-observability__query_worker_observability({\n  view: \"calculations\",\n  parameters: {\n    calculations: [\n      { operator: \"sum\", field: \"$metadata.cpuTime\", as: \"total_cpu_ms\" },\n      { operator: \"avg\", field: \"$metadata.cpuTime\", as: \"avg_cpu_ms\" }\n    ],\n    groupBys: [\n      { type: \"string\", value: \"$metadata.service\" }\n    ]\n  },\n  timeframe: {\n    reference: \"now\",\n    offset: \"-30d\"\n  }\n})\n```\n\n**Cost Impact**: CPU time directly impacts Workers billing on paid plans.\n\n---\n\n## AI Gateway Probes\n\n### Cost by Model (30 days)\n\n```javascript\nconst logs = await mcp__cloudflare-ai-gateway__list_logs({\n  gateway_id: \"your-gateway-id\",\n  per_page: 1000,\n  order_by: \"created_at\",\n  direction: \"desc\"\n});\n\n// Aggregate by model\nconst costByModel = {};\nfor (const log of logs.result) {\n  const model = log.model;\n  const tokens = log.tokens_in + log.tokens_out;\n  costByModel[model] = (costByModel[model] || 0) + tokens;\n}\n```\n\n**Pricing Reference** (per 1K tokens):\n| Model | Input | Output |\n|-------|-------|--------|\n| @cf/meta/llama-3-8b-instruct | $0.00019 | $0.00019 |\n| @cf/mistral/mistral-7b-instruct | $0.00011 | $0.00011 |\n| @hf/thebloke/deepseek-coder-6.7b-instruct | $0.00013 | $0.00013 |\n\n### Cache Hit Rate\n\n```javascript\nconst logs = await mcp__cloudflare-ai-gateway__list_logs({\n  gateway_id: \"your-gateway-id\",\n  per_page: 1000\n});\n\nconst total = logs.result.length;\nconst cached = logs.result.filter(l => l.cached).length;\nconst cacheHitRate = (cached / total * 100).toFixed(1);\n```\n\n**Interpretation**:\n- Cache hit rate < 10%: Significant cost savings opportunity\n- Review request patterns for cacheable queries\n- Consider increasing cache TTL for stable prompts\n\n### Token Usage Distribution\n\n```javascript\nconst logs = await mcp__cloudflare-ai-gateway__list_logs({\n  gateway_id: \"your-gateway-id\",\n  per_page: 1000\n});\n\n// Analyze token distribution\nconst tokenBuckets = { small: 0, medium: 0, large: 0, xlarge: 0 };\nfor (const log of logs.result) {\n  const total = log.tokens_in + log.tokens_out;\n  if (total < 100) tokenBuckets.small++;\n  else if (total < 500) tokenBuckets.medium++;\n  else if (total < 2000) tokenBuckets.large++;\n  else tokenBuckets.xlarge++;\n}\n```\n\n**Use Case**: Identify if smaller models could handle high-volume, low-token requests.\n\n---\n\n## Queue Probes\n\n### Queue List and Status\n\n```javascript\nmcp__cloudflare-bindings__queues_list()\n```\n\n**Check For**:\n- Queues without corresponding DLQ (dead-letter queue)\n- Queue naming patterns (should have `-dlq` suffix for DLQs)\n\n### Consumer Configuration\n\nFrom wrangler config, verify:\n\n```jsonc\n{\n  \"queues\": {\n    \"consumers\": [{\n      \"queue\": \"my-queue\",\n      \"max_batch_size\": 10,      // 1-100, default 10\n      \"max_batch_timeout\": 5,    // 0-30 seconds\n      \"max_retries\": 2,          // 0-100, recommend <= 2\n      \"dead_letter_queue\": \"my-queue-dlq\",  // REQUIRED for resilience\n      \"max_concurrency\": 10      // 1-20\n    }]\n  }\n}\n```\n\n**Audit Checks**:\n| Setting | Risk | Recommendation |\n|---------|------|----------------|\n| `max_retries > 2` | Cost multiplication | Reduce to 2, use DLQ |\n| Missing `dead_letter_queue` | Message loss | Always configure DLQ |\n| `max_batch_size = 1` | Inefficient processing | Increase for throughput |\n\n### DLQ Depth (Manual Check)\n\nCurrently no direct MCP tool for queue depth. Check via:\n1. Cloudflare Dashboard > Queues\n2. wrangler CLI: `wrangler queues list`\n\n**Warning Signs**:\n- DLQ with messages > 0: Processing failures occurring\n- DLQ message age > 24h: Failures not being addressed\n\n---\n\n## R2 Probes\n\n### Bucket List\n\n```javascript\nmcp__cloudflare-bindings__r2_buckets_list()\n```\n\n### Storage Class Distribution\n\nFrom R2 metrics (when available):\n- Standard storage: Most expensive\n- Infrequent Access: 80% cheaper storage, higher retrieval cost\n\n**Cost Optimization**:\n- Objects not accessed in 30+ days: Consider lifecycle rules\n- Large objects with frequent partial reads: Enable range requests\n\n---\n\n## KV Probes\n\n### Namespace List\n\n```javascript\nmcp__cloudflare-bindings__kv_namespaces_list()\n```\n\n### Key Pattern Analysis\n\nKV doesn't support listing all keys efficiently. Instead, analyze code for:\n- Key naming patterns (prefix-based for organization)\n- TTL usage (expiring keys save storage)\n- Value sizes (KV has 25MB limit, prefer R2 for large values)\n\n**Cost Note**: KV writes are 10x more expensive than reads ($5/M vs $0.50/M).\n\n---\n\n## Vectorize Probes\n\n### Index List\n\n```javascript\nmcp__cloudflare-bindings__vectorize_indexes_list()\n```\n\n### Index Statistics\n\nFor each index, check:\n- Dimension count (affects query cost)\n- Vector count (affects storage cost)\n- Metadata fields (affects query flexibility)\n\n---\n\n## Probe Output Format\n\nWhen reporting probe results, use provenance tags:\n\n```markdown\n### [LIVE-VALIDATED] COST001: D1 writes at 85% of projected\n- **Probe**: D1 write volume query\n- **Expected** (static): 50M writes/month\n- **Actual** (live): 42.5M writes/month\n- **Evidence**: Observability data, 30-day window\n\n### [LIVE-REFUTED] PERF001: Missing indexes on users.email\n- **Probe**: EXPLAIN QUERY PLAN\n- **Static Finding**: No index detected in migrations\n- **Live Result**: `SEARCH USING COVERING INDEX idx_users_email`\n- **Conclusion**: Index exists, not in tracked migrations\n\n### [INCOMPLETE] SEC001: Rate limiting status\n- **Probe**: Could not verify (MCP tool unavailable)\n- **Static Finding**: No rate limiting detected in code\n- **Action**: Manual verification required\n```\n\n---\n\n## Best Practices\n\n1. **Always check MCP availability first** before running probes\n2. **Use bounded time ranges** (7 days, 30 days) to limit data volume\n3. **Compare static vs live** findings and tag appropriately\n4. **Note when probes are incomplete** due to MCP unavailability\n5. **Aggregate expensive queries** - don't run per-endpoint probes individually"
              },
              {
                "name": "scale",
                "description": "Recommend sharding, caching strategies, and read-replication patterns for Cloudflare architectures. Use this skill when preparing for growth, hitting limits, or optimizing for high traffic.",
                "path": "skills/scale/SKILL.md",
                "frontmatter": {
                  "name": "scale",
                  "description": "Recommend sharding, caching strategies, and read-replication patterns for Cloudflare architectures. Use this skill when preparing for growth, hitting limits, or optimizing for high traffic."
                },
                "content": "# Cloudflare Scaling Skill\n\nStrategies for scaling Cloudflare architectures beyond default limits while maintaining cost efficiency.\n\n## Scaling Decision Matrix\n\n| Bottleneck | Symptom | Solution |\n|------------|---------|----------|\n| D1 read latency | >50ms queries | Add KV cache layer |\n| D1 write throughput | Queue backlog | Batch writes, add queue buffer |\n| D1 storage | Approaching 10GB | Archive to R2, partition tables |\n| KV read latency | Cache misses | Key prefixing, predictable keys |\n| KV write rate | 1 write/sec/key limit | Shard keys, batch writes |\n| R2 throughput | Slow uploads | Presigned URLs, multipart |\n| Worker memory | 128MB limit | Streaming, chunked processing |\n| Worker CPU | 30s timeout | Queues, Workflows, DO |\n| Subrequests | 1000/request limit | Service Bindings RPC |\n| Queue throughput | Consumer lag | Increase concurrency, batch size |\n\n## Caching Strategies\n\n### Cache Hierarchy\n\n```\nRequest → Edge Cache → KV Cache → D1 → Origin\n           (Tiered)    (Global)   (Primary)\n```\n\n### KV Cache Patterns\n\n#### Write-Through Cache\n```typescript\nasync function getWithCache<T>(\n  kv: KVNamespace,\n  db: D1Database,\n  key: string,\n  query: () => Promise<T>,\n  ttl: number = 3600\n): Promise<T> {\n  // Try cache first\n  const cached = await kv.get(key, 'json');\n  if (cached !== null) {\n    return cached as T;\n  }\n\n  // Cache miss - fetch from D1\n  const fresh = await query();\n\n  // Write to cache (non-blocking)\n  kv.put(key, JSON.stringify(fresh), { expirationTtl: ttl });\n\n  return fresh;\n}\n```\n\n#### Cache Invalidation\n```typescript\n// Pattern 1: TTL-based (simple, eventual consistency)\nawait kv.put(key, value, { expirationTtl: 300 }); // 5 min\n\n// Pattern 2: Version-based (immediate, more complex)\nconst version = await kv.get('cache:version');\nconst key = `data:${id}:v${version}`;\n\n// Invalidate by incrementing version\nawait kv.put('cache:version', String(Number(version) + 1));\n\n// Pattern 3: Tag-based (flexible, requires cleanup)\nawait kv.put(`user:${userId}:profile`, data);\nawait kv.put(`user:${userId}:settings`, settings);\n\n// Invalidate all user data\nconst keys = await kv.list({ prefix: `user:${userId}:` });\nfor (const key of keys.keys) {\n  await kv.delete(key.name);\n}\n```\n\n### Tiered Cache (Cloudflare CDN)\n\nEnable in Worker for static-like responses:\n```typescript\n// Cache API for fine-grained control\nconst cache = caches.default;\n\napp.get('/api/products/:id', async (c) => {\n  const cacheKey = new Request(c.req.url);\n\n  // Check cache\n  const cached = await cache.match(cacheKey);\n  if (cached) {\n    return cached;\n  }\n\n  // Fetch fresh data\n  const product = await getProduct(c.env.DB, c.req.param('id'));\n\n  const response = c.json(product);\n  response.headers.set('Cache-Control', 's-maxage=300');\n\n  // Store in cache\n  c.executionCtx.waitUntil(cache.put(cacheKey, response.clone()));\n\n  return response;\n});\n```\n\n## Sharding Strategies\n\n### Key-Based Sharding (KV)\n\nWhen hitting 1 write/sec/key limit:\n\n```typescript\n// Problem: High-frequency counter\nawait kv.put('page:views', views); // Limited to 1/sec\n\n// Solution: Shard across multiple keys\nconst SHARD_COUNT = 10;\n\nasync function incrementCounter(kv: KVNamespace, key: string) {\n  const shard = Math.floor(Math.random() * SHARD_COUNT);\n  const shardKey = `${key}:shard:${shard}`;\n\n  const current = Number(await kv.get(shardKey)) || 0;\n  await kv.put(shardKey, String(current + 1));\n}\n\nasync function getCounter(kv: KVNamespace, key: string): Promise<number> {\n  let total = 0;\n  for (let i = 0; i < SHARD_COUNT; i++) {\n    const value = await kv.get(`${key}:shard:${i}`);\n    total += Number(value) || 0;\n  }\n  return total;\n}\n```\n\n### Time-Based Sharding (D1)\n\nFor high-volume time-series data:\n\n```sql\n-- Partition by month\nCREATE TABLE events_2025_01 (\n    id TEXT PRIMARY KEY,\n    timestamp TEXT NOT NULL,\n    data TEXT\n);\n\nCREATE TABLE events_2025_02 (\n    id TEXT PRIMARY KEY,\n    timestamp TEXT NOT NULL,\n    data TEXT\n);\n\n-- Query router in code\nfunction getEventsTable(date: Date): string {\n  const year = date.getFullYear();\n  const month = String(date.getMonth() + 1).padStart(2, '0');\n  return `events_${year}_${month}`;\n}\n```\n\n### Entity-Based Sharding (D1)\n\nFor multi-tenant applications:\n\n```typescript\n// Tenant-specific D1 databases\ninterface Bindings {\n  DB_TENANT_A: D1Database;\n  DB_TENANT_B: D1Database;\n  // Or use Hyperdrive for external Postgres\n}\n\nfunction getDbForTenant(env: Bindings, tenantId: string): D1Database {\n  const dbMapping: Record<string, D1Database> = {\n    'tenant-a': env.DB_TENANT_A,\n    'tenant-b': env.DB_TENANT_B,\n  };\n  return dbMapping[tenantId] ?? env.DB_DEFAULT;\n}\n```\n\n## Read Replication Patterns\n\n### D1 Read Replicas\n\nD1 automatically creates read replicas. Optimize access:\n\n```typescript\n// Enable Smart Placement in wrangler.jsonc\n{\n  \"placement\": { \"mode\": \"smart\" }\n}\n\n// Worker runs near data, reducing latency\n```\n\n### Multi-Region with Durable Objects\n\nFor global coordination with regional caching:\n\n```typescript\n// Durable Object for region-local state\nexport class RegionalCache {\n  private state: DurableObjectState;\n  private cache: Map<string, { value: unknown; expires: number }>;\n\n  constructor(state: DurableObjectState) {\n    this.state = state;\n    this.cache = new Map();\n  }\n\n  async fetch(request: Request): Promise<Response> {\n    const url = new URL(request.url);\n    const key = url.searchParams.get('key');\n\n    if (request.method === 'GET' && key) {\n      const cached = this.cache.get(key);\n      if (cached && cached.expires > Date.now()) {\n        return Response.json({ value: cached.value, source: 'cache' });\n      }\n      return Response.json({ value: null, source: 'miss' });\n    }\n\n    if (request.method === 'PUT' && key) {\n      const { value, ttl } = await request.json();\n      this.cache.set(key, {\n        value,\n        expires: Date.now() + (ttl * 1000),\n      });\n      return Response.json({ success: true });\n    }\n\n    return Response.json({ error: 'Invalid request' }, { status: 400 });\n  }\n}\n```\n\n### Eventual Consistency Pattern\n\nFor data that can tolerate staleness:\n\n```typescript\ninterface CacheEntry<T> {\n  data: T;\n  cachedAt: number;\n  staleAfter: number;\n  expireAfter: number;\n}\n\nasync function getWithStaleWhileRevalidate<T>(\n  kv: KVNamespace,\n  key: string,\n  fetcher: () => Promise<T>,\n  options: {\n    staleAfter: number;  // Serve stale, revalidate in background\n    expireAfter: number; // Force fresh fetch\n  }\n): Promise<T> {\n  const cached = await kv.get<CacheEntry<T>>(key, 'json');\n  const now = Date.now();\n\n  if (cached) {\n    // Fresh - return immediately\n    if (now < cached.staleAfter) {\n      return cached.data;\n    }\n\n    // Stale but not expired - return stale, revalidate async\n    if (now < cached.expireAfter) {\n      // Background revalidation\n      kv.put(key, JSON.stringify(await buildCacheEntry(fetcher, options)));\n      return cached.data;\n    }\n  }\n\n  // Expired or missing - must fetch fresh\n  const entry = await buildCacheEntry(fetcher, options);\n  await kv.put(key, JSON.stringify(entry));\n  return entry.data;\n}\n\nasync function buildCacheEntry<T>(\n  fetcher: () => Promise<T>,\n  options: { staleAfter: number; expireAfter: number }\n): Promise<CacheEntry<T>> {\n  const now = Date.now();\n  return {\n    data: await fetcher(),\n    cachedAt: now,\n    staleAfter: now + options.staleAfter,\n    expireAfter: now + options.expireAfter,\n  };\n}\n```\n\n## Queue Scaling\n\n### Horizontal Scaling (Concurrency)\n\n```jsonc\n{\n  \"queues\": {\n    \"consumers\": [\n      {\n        \"queue\": \"events\",\n        \"max_batch_size\": 100,     // Max messages per invocation\n        \"max_concurrency\": 20,     // Parallel consumer instances\n        \"max_retries\": 1,\n        \"dead_letter_queue\": \"events-dlq\"\n      }\n    ]\n  }\n}\n```\n\n### Batch Processing Optimization\n\n```typescript\nexport default {\n  async queue(batch: MessageBatch, env: Bindings) {\n    // Group messages for efficient D1 batching\n    const byType = new Map<string, unknown[]>();\n\n    for (const msg of batch.messages) {\n      const type = msg.body.type;\n      if (!byType.has(type)) byType.set(type, []);\n      byType.get(type)!.push(msg.body.payload);\n    }\n\n    // Process each type as a batch\n    for (const [type, payloads] of byType) {\n      await processBatch(type, payloads, env);\n    }\n\n    // Ack all at once\n    batch.ackAll();\n  },\n};\n\nasync function processBatch(\n  type: string,\n  payloads: unknown[],\n  env: Bindings\n) {\n  // Batch insert to D1 (≤1000 at a time)\n  const BATCH_SIZE = 1000;\n  for (let i = 0; i < payloads.length; i += BATCH_SIZE) {\n    const chunk = payloads.slice(i, i + BATCH_SIZE);\n    await insertBatch(env.DB, type, chunk);\n  }\n}\n```\n\n## Memory Management\n\n### Streaming for Large Payloads\n\n```typescript\n// Problem: Loading entire file into memory\nconst data = await r2.get(key);\nconst json = await data.json(); // May exceed 128MB\n\n// Solution: Stream processing\napp.get('/export/:key', async (c) => {\n  const object = await c.env.R2.get(c.req.param('key'));\n  if (!object) return c.json({ error: 'Not found' }, 404);\n\n  return new Response(object.body, {\n    headers: {\n      'Content-Type': object.httpMetadata?.contentType ?? 'application/octet-stream',\n      'Content-Length': String(object.size),\n    },\n  });\n});\n```\n\n### Chunked Processing with Durable Objects\n\n```typescript\n// For files >50MB, process in chunks with checkpointing\nexport class ChunkedProcessor {\n  private state: DurableObjectState;\n\n  async processFile(r2Key: string, chunkSize: number = 1024 * 1024) {\n    // Get checkpoint\n    let offset = (await this.state.storage.get<number>('offset')) ?? 0;\n\n    const object = await this.env.R2.get(r2Key, {\n      range: { offset, length: chunkSize },\n    });\n\n    if (!object) {\n      // Processing complete\n      await this.state.storage.delete('offset');\n      return { complete: true };\n    }\n\n    // Process chunk\n    await this.processChunk(await object.arrayBuffer());\n\n    // Save checkpoint\n    await this.state.storage.put('offset', offset + chunkSize);\n\n    // Schedule next chunk via alarm\n    await this.state.storage.setAlarm(Date.now() + 100);\n\n    return { complete: false, offset: offset + chunkSize };\n  }\n}\n```\n\n## Scaling Checklist\n\n### Before Launch\n- [ ] KV cache layer for hot data\n- [ ] D1 indexes on all query columns\n- [ ] Queue DLQs configured\n- [ ] Smart Placement enabled\n- [ ] Batch size ≤1000 for D1 writes\n\n### At 10K req/day\n- [ ] Monitor D1 query performance\n- [ ] Review cache hit rates\n- [ ] Check queue consumer lag\n\n### At 100K req/day\n- [ ] Add Analytics Engine for metrics (free)\n- [ ] Consider key sharding for hot KV keys\n- [ ] Review batch processing efficiency\n\n### At 1M req/day\n- [ ] Implement Tiered Cache\n- [ ] Add read-through caching\n- [ ] Consider time-based D1 partitioning\n- [ ] Review and optimize indexes\n\n### At 10M req/day\n- [ ] Multi-region strategy\n- [ ] Entity-based sharding\n- [ ] Durable Objects for coordination\n- [ ] Custom rate limiting\n\n## Cost Implications\n\n| Scaling Strategy | Additional Cost | When to Use |\n|------------------|-----------------|-------------|\n| KV caching | $0.50/M reads | D1 read heavy |\n| Key sharding | More KV reads | >1 write/sec/key |\n| Time partitioning | None (same D1) | >10GB data |\n| Tiered Cache | None (CDN) | Cacheable responses |\n| DO coordination | CPU time | Global state |\n| Queue scaling | Per message | High throughput |\n\n## Anti-Patterns\n\n| Pattern | Problem | Solution |\n|---------|---------|----------|\n| Cache everything | KV costs add up | Cache hot data only |\n| Shard too early | Complexity without benefit | Monitor first |\n| Ignore TTLs | Stale data | Set appropriate TTLs |\n| Skip DLQ | Lost messages | Always add DLQ |\n| Over-replicate | Cost multiplication | Right-size replication |"
              },
              {
                "name": "zero-trust",
                "description": "Identify and remediate Zero Trust security gaps in Cloudflare deployments. Use this skill when auditing Access policies, checking staging/dev environment protection, detecting unprotected admin routes, or implementing mTLS and service tokens for machine-to-machine auth.",
                "path": "skills/zero-trust/SKILL.md",
                "frontmatter": {
                  "name": "zero-trust",
                  "description": "Identify and remediate Zero Trust security gaps in Cloudflare deployments. Use this skill when auditing Access policies, checking staging/dev environment protection, detecting unprotected admin routes, or implementing mTLS and service tokens for machine-to-machine auth."
                },
                "content": "# Cloudflare Zero Trust Skill\n\nAudit and implement Zero Trust security policies using Cloudflare Access, service tokens, and mTLS. Ensure all environments (production, staging, dev) have appropriate access controls.\n\n## Environment Protection Matrix\n\n### Risk Assessment by Environment\n\n| Environment | Expected Protection | Common Gap | Risk Level |\n|-------------|--------------------|-----------| -----------|\n| Production | CF Access + WAF + Rate Limiting | Usually protected | LOW |\n| Staging | CF Access (should mirror prod) | Often missing Access | HIGH |\n| Development | CF Access or IP restrictions | Frequently exposed | CRITICAL |\n| Preview (PR deploys) | CF Access or time-limited | Often public | HIGH |\n| Admin/Internal APIs | Service Tokens + mTLS | Basic auth only | CRITICAL |\n\n## Zero Trust Audit Workflow\n\n### Step 1: Environment Discovery\n\n```\n1. List all Workers in account via MCP\n2. Identify environment patterns:\n   - *-staging, *-dev, *-preview\n   - staging.*, dev.*, preview.*\n   - Feature branch deployments\n3. Check route configurations\n```\n\n### Step 2: Access Policy Verification\n\nFor each environment, verify:\n\n```javascript\n// Query Access applications\nmcp__cloudflare-access__list_applications()\n\n// For each route/hostname, check if Access policy exists:\n// - Authentication requirement\n// - Allow/Block rules\n// - Session duration\n// - Geographic restrictions\n```\n\n### Step 3: Audit Findings\n\n| ID | Name | Severity | Check |\n|----|------|----------|-------|\n| ZT001 | Staging without Access | CRITICAL | staging.* routes without Access policy |\n| ZT002 | Dev environment exposed | CRITICAL | dev.* publicly accessible |\n| ZT003 | Preview deploys public | HIGH | *.pages.dev or preview.* without Access |\n| ZT004 | Admin routes unprotected | CRITICAL | /admin/* without Access or auth middleware |\n| ZT005 | Internal APIs no service token | HIGH | Internal service routes without mTLS/tokens |\n| ZT006 | Weak session duration | MEDIUM | Access session > 24h for sensitive routes |\n| ZT007 | No geographic restriction | LOW | Admin access from any country |\n| ZT008 | Missing bypass audit | MEDIUM | Bypass rules without justification |\n\n## Access Policy Patterns\n\n### Pattern 1: Environment-Based Access\n\n```jsonc\n// wrangler.jsonc with Access-protected routes\n{\n  \"routes\": [\n    {\n      \"pattern\": \"api.example.com/*\",\n      \"zone_name\": \"example.com\"\n    },\n    {\n      \"pattern\": \"staging.example.com/*\",\n      \"zone_name\": \"example.com\"\n      // Access policy should protect this route\n    }\n  ]\n}\n```\n\n**Recommended Access Policy for Staging:**\n```json\n{\n  \"name\": \"Staging Environment\",\n  \"domain\": \"staging.example.com\",\n  \"type\": \"self_hosted\",\n  \"session_duration\": \"12h\",\n  \"policies\": [\n    {\n      \"name\": \"Team Access\",\n      \"decision\": \"allow\",\n      \"include\": [\n        { \"email_domain\": { \"domain\": \"company.com\" } }\n      ],\n      \"require\": [\n        { \"login_method\": { \"id\": \"google\" } }\n      ]\n    }\n  ]\n}\n```\n\n### Pattern 2: Service Token for Machine Auth\n\nFor Worker-to-Worker or CI/CD access:\n\n```typescript\n// Verify service token in Worker\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    // Service token headers set by Cloudflare Access\n    const cfAccessClientId = request.headers.get('CF-Access-Client-Id');\n    const cfAccessClientSecret = request.headers.get('CF-Access-Client-Secret');\n\n    if (!cfAccessClientId || cfAccessClientId !== env.EXPECTED_CLIENT_ID) {\n      return new Response('Unauthorized', { status: 401 });\n    }\n\n    // Process authenticated request\n    return handleRequest(request, env);\n  }\n};\n```\n\n### Pattern 3: mTLS for High-Security APIs\n\n```jsonc\n// wrangler.jsonc with mTLS binding\n{\n  \"mtls_certificates\": [\n    {\n      \"binding\": \"MY_CERT\",\n      \"certificate_id\": \"...\"\n    }\n  ]\n}\n```\n\n```typescript\n// Verify client certificate\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const tlsClientAuth = request.cf?.tlsClientAuth;\n\n    if (!tlsClientAuth || tlsClientAuth.certVerified !== 'SUCCESS') {\n      return new Response('Certificate required', { status: 403 });\n    }\n\n    // Additional verification\n    if (!tlsClientAuth.certIssuerDN.includes('O=MyCompany')) {\n      return new Response('Invalid certificate issuer', { status: 403 });\n    }\n\n    return handleRequest(request, env);\n  }\n};\n```\n\n## Environment Detection Heuristics\n\n### Staging/Dev Indicators\n\n```\nHostname patterns:\n- staging.*, stage.*, stg.*\n- dev.*, development.*\n- preview.*, pr-*.*, branch-*.*\n- *.pages.dev (Cloudflare Pages previews)\n- localhost:*, 127.0.0.1:*\n\nWrangler config indicators:\n- env.staging, env.development\n- name: \"*-staging\", \"*-dev\"\n- vars.ENVIRONMENT: \"staging\" | \"development\"\n```\n\n### Admin Route Indicators\n\n```\nPath patterns requiring protection:\n- /admin/*\n- /api/admin/*\n- /internal/*\n- /dashboard/*\n- /manage/*\n- /config/*\n- /_debug/*\n- /metrics, /health (depends on sensitivity)\n```\n\n## Output Format\n\n```markdown\n# Zero Trust Audit Report\n\n**Scope**: [Account/Zone]\n**Environments Scanned**: X\n\n## Critical Gaps (Immediate Action Required)\n\n### [ZT001] Staging Environment Exposed\n- **Route**: staging.example.com/*\n- **Status**: No Access policy detected\n- **Risk**: Staging data/functionality exposed to internet\n- **Fix**: Create Access application with team email domain restriction\n- **Provenance**: `[LIVE-VALIDATED]` via cloudflare-access MCP\n\n### [ZT004] Admin Routes Unprotected\n- **Route**: api.example.com/admin/*\n- **Status**: No authentication middleware or Access policy\n- **Risk**: Admin functions accessible without auth\n- **Fix**: Add Access policy OR implement auth middleware\n- **Provenance**: `[STATIC]` - code analysis\n\n## High Priority\n\n[List HIGH severity findings]\n\n## Recommendations\n\n1. [ ] Create Access application for `staging.example.com`\n2. [ ] Implement service token auth for CI/CD access\n3. [ ] Add mTLS for internal service-to-service calls\n4. [ ] Review and reduce session durations\n\n## Access Policy Suggestions\n\n[Generated Access policy configurations]\n```\n\n## MCP Tools for Zero Trust\n\n```javascript\n// List Access applications\nmcp__cloudflare-access__list_applications()\n\n// Get application details\nmcp__cloudflare-access__get_application({ app_id: \"...\" })\n\n// List Access policies\nmcp__cloudflare-access__list_policies({ app_id: \"...\" })\n\n// Verify route protection\nmcp__cloudflare-bindings__workers_list()\n```\n\n## Tips\n\n- **Preview deploys**: Always protect with Access; use time-limited URLs\n- **Service tokens**: Rotate quarterly; scope to specific applications\n- **mTLS**: Required for PCI-DSS/HIPAA compliance scenarios\n- **Session duration**: Shorter for admin (1-4h), longer for general access (24h)\n- **Bypass rules**: Document and audit regularly; set expiration\n- **Geographic restrictions**: Consider for admin access\n- **Device posture**: Enable for high-security environments (requires WARP)\n\n## Quick Fixes\n\n### Add Access to Staging (via Terraform)\n\n```hcl\nresource \"cloudflare_access_application\" \"staging\" {\n  zone_id          = var.zone_id\n  name             = \"Staging Environment\"\n  domain           = \"staging.example.com\"\n  type             = \"self_hosted\"\n  session_duration = \"12h\"\n}\n\nresource \"cloudflare_access_policy\" \"staging_team\" {\n  application_id = cloudflare_access_application.staging.id\n  zone_id        = var.zone_id\n  name           = \"Team Access\"\n  precedence     = 1\n  decision       = \"allow\"\n\n  include {\n    email_domain = [\"company.com\"]\n  }\n}\n```\n\n### Add Service Token Auth to Worker\n\n```typescript\n// middleware/serviceToken.ts\nexport function requireServiceToken(env: Env) {\n  return async (c: Context, next: () => Promise<void>) => {\n    const clientId = c.req.header('CF-Access-Client-Id');\n    if (clientId !== env.EXPECTED_SERVICE_TOKEN_ID) {\n      return c.json({ error: 'Unauthorized' }, 401);\n    }\n    await next();\n  };\n}\n```"
              }
            ]
          }
        ]
      }
    }
  ]
}