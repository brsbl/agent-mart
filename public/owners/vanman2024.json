{
  "owner": {
    "id": "vanman2024",
    "display_name": "vanman2024",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/183324231?v=4",
    "url": "https://github.com/vanman2024",
    "bio": null,
    "stats": {
      "total_repos": 3,
      "total_plugins": 21,
      "total_commands": 244,
      "total_skills": 105,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "repos": [
    {
      "full_name": "vanman2024/mcp-servers-marketplace",
      "url": "https://github.com/vanman2024/mcp-servers-marketplace",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-25T21:18:14Z",
        "created_at": "2025-10-13T00:59:40Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 886
        },
        {
          "path": ".git-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": ".git-hooks/pre-commit",
          "type": "blob",
          "size": 658
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/security-scan.yml",
          "type": "blob",
          "size": 3838
        },
        {
          "path": ".github/workflows/sync-airtable-mcp.yml",
          "type": "blob",
          "size": 1457
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 429
        },
        {
          "path": ".gitleaks.toml",
          "type": "blob",
          "size": 1690
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 1728
        },
        {
          "path": "DEPLOYED_SERVERS.md",
          "type": "blob",
          "size": 4252
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 6241
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/AIRTABLE-MCP-SYNC.md",
          "type": "blob",
          "size": 6291
        },
        {
          "path": "docs/CREDENTIAL-SAFETY.md",
          "type": "blob",
          "size": 3495
        },
        {
          "path": "docs/security",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/security/SECURITY-RULES.md",
          "type": "blob",
          "size": 6366
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 799
        },
        {
          "path": "plugins/fastmcp/.gitignore",
          "type": "blob",
          "size": 416
        },
        {
          "path": "plugins/fastmcp/CHANGELOG.md",
          "type": "blob",
          "size": 636
        },
        {
          "path": "plugins/fastmcp/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/fastmcp/README.md",
          "type": "blob",
          "size": 16740
        },
        {
          "path": "plugins/fastmcp/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-api-wrapper.md",
          "type": "blob",
          "size": 8377
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-client-setup-ts.md",
          "type": "blob",
          "size": 7108
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-client-setup.md",
          "type": "blob",
          "size": 8059
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-deployment.md",
          "type": "blob",
          "size": 21784
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-setup-ts.md",
          "type": "blob",
          "size": 8971
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-setup.md",
          "type": "blob",
          "size": 6508
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-tester.md",
          "type": "blob",
          "size": 9980
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-verifier-py.md",
          "type": "blob",
          "size": 6364
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-verifier-ts.md",
          "type": "blob",
          "size": 7227
        },
        {
          "path": "plugins/fastmcp/agents/fastmcp-verifier.md",
          "type": "blob",
          "size": 11120
        },
        {
          "path": "plugins/fastmcp/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/commands/add-api-wrapper.md",
          "type": "blob",
          "size": 10538
        },
        {
          "path": "plugins/fastmcp/commands/add-auth.md",
          "type": "blob",
          "size": 6208
        },
        {
          "path": "plugins/fastmcp/commands/add-components.md",
          "type": "blob",
          "size": 5425
        },
        {
          "path": "plugins/fastmcp/commands/add-deployment.md",
          "type": "blob",
          "size": 3636
        },
        {
          "path": "plugins/fastmcp/commands/add-integration.md",
          "type": "blob",
          "size": 4835
        },
        {
          "path": "plugins/fastmcp/commands/fastmcp-verify.md",
          "type": "blob",
          "size": 6659
        },
        {
          "path": "plugins/fastmcp/commands/new-client.md",
          "type": "blob",
          "size": 5150
        },
        {
          "path": "plugins/fastmcp/commands/new-server.md",
          "type": "blob",
          "size": 9938
        },
        {
          "path": "plugins/fastmcp/commands/test.md",
          "type": "blob",
          "size": 4548
        },
        {
          "path": "plugins/fastmcp/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/docs/DEPLOYMENT.md",
          "type": "blob",
          "size": 4956
        },
        {
          "path": "plugins/fastmcp/docs/VALIDATION_REPORT.md",
          "type": "blob",
          "size": 7469
        },
        {
          "path": "plugins/fastmcp/docs/fastmcp-documentation.md",
          "type": "blob",
          "size": 49680
        },
        {
          "path": "plugins/fastmcp/docs/fastmcp-links-organized.md",
          "type": "blob",
          "size": 17125
        },
        {
          "path": "plugins/fastmcp/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/scripts/extract-server-to-repo.sh",
          "type": "blob",
          "size": 4973
        },
        {
          "path": "plugins/fastmcp/scripts/manage-github-secrets.sh",
          "type": "blob",
          "size": 4146
        },
        {
          "path": "plugins/fastmcp/scripts/setup_venvs.sh",
          "type": "blob",
          "size": 5177
        },
        {
          "path": "plugins/fastmcp/scripts/sync-to-standalone-secure.sh",
          "type": "blob",
          "size": 5430
        },
        {
          "path": "plugins/fastmcp/scripts/sync-to-standalone.sh",
          "type": "blob",
          "size": 2341
        },
        {
          "path": "plugins/fastmcp/scripts/update-deployed-servers.sh",
          "type": "blob",
          "size": 621
        },
        {
          "path": "plugins/fastmcp/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/README.md",
          "type": "blob",
          "size": 4312
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/SKILL.md",
          "type": "blob",
          "size": 14630
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/examples/successful-deployment.md",
          "type": "blob",
          "size": 10716
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/examples/troubleshooting.md",
          "type": "blob",
          "size": 12540
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts/README.md",
          "type": "blob",
          "size": 5933
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts/check-env-vars.sh",
          "type": "blob",
          "size": 8373
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts/test-local.sh",
          "type": "blob",
          "size": 9058
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts/validate-server.sh",
          "type": "blob",
          "size": 8294
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/scripts/verify-deployment.sh",
          "type": "blob",
          "size": 8308
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/templates/.fastmcp-deployments.json",
          "type": "blob",
          "size": 2965
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/templates/deployment-checklist.md",
          "type": "blob",
          "size": 5795
        },
        {
          "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/templates/env-var-template.md",
          "type": "blob",
          "size": 5997
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/sanitize-env-for-docs.sh",
          "type": "blob",
          "size": 2088
        },
        {
          "path": "scripts/sync-airtable-mcp.py",
          "type": "blob",
          "size": 12330
        }
      ],
      "marketplace": {
        "name": "mcp-servers-marketplace",
        "version": "1.0.0",
        "description": "Marketplace for FastMCP plugin - used to build and deploy MCP servers",
        "owner_info": {
          "name": "MCP Servers Project",
          "email": "noreply@mcp-servers.dev"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "fastmcp",
            "description": "FastMCP SDK plugin for building MCP servers with Python/TypeScript. Layered commands for server development, authentication, deployment, and integrations.",
            "source": "./plugins/fastmcp",
            "category": null,
            "version": "1.0.0",
            "author": {
              "name": "ai-dev-marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/mcp-servers-marketplace",
              "/plugin install fastmcp@mcp-servers-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-25T21:18:14Z",
              "created_at": "2025-10-13T00:59:40Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-api-wrapper",
                "description": "Generate MCP tools from Postman collections to wrap existing APIs. Falls back to WebFetch/Playwright if Postman unavailable. Uses Postman MCP server and Newman to analyze API structure and create FastMCP tool wrappers.",
                "path": "plugins/fastmcp/commands/add-api-wrapper.md",
                "frontmatter": {
                  "description": "Generate MCP tools from Postman collections to wrap existing APIs. Falls back to WebFetch/Playwright if Postman unavailable. Uses Postman MCP server and Newman to analyze API structure and create FastMCP tool wrappers.",
                  "argument-hint": "<collection-name-or-id> [--server-path=path]",
                  "mcp-servers": "postman"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Automatically generate FastMCP tools that wrap existing REST APIs. Uses Postman collections as primary source, but falls back to WebFetch/Playwright if Postman MCP is unavailable or collection doesn't exist.\n\n## Load Skills\n\nINVOKE skills from quality plugin to load all API analysis patterns, templates, and examples:\n\n!{skill quality:api-schema-analyzer}\n!{skill quality:newman-runner}\n!{skill quality:postman-collection-manager}\n\nThese skills provide:\n- OpenAPI/Swagger parsing scripts and templates\n- Newman collection analysis and test patterns\n- Postman collection management utilities\n- Tool signature generation examples\n- API endpoint mapping templates\n\nPhase 1: Discovery & API Source Determination\nGoal: Find API structure from best available source\n\n**Primary Strategy: Postman/Newman**\n\nActions:\n- Parse $ARGUMENTS for collection name/ID and server path\n- Check if Postman MCP server is available\n- If available:\n  - Use mcp__postman tools to list collections and get collection details\n  - Interactive conversation:\n    - \"Which Postman collection contains the API you want to wrap?\"\n    - \"Do you want to wrap ALL endpoints or select specific ones?\"\n    - Show collection structure (folders, requests)\n    - Explain what MCP tools will be generated\n\n**Fallback Strategy: WebFetch/Playwright**\n\nIf Postman MCP not available OR collection doesn't exist:\n- Use WebSearch to find \"{API_NAME} API documentation\"\n- Use WebFetch to retrieve:\n  - Official API documentation pages\n  - OpenAPI/Swagger specifications\n  - API reference guides\n  - Developer portal endpoints\n- Check common documentation paths:\n  - `/api/docs`\n  - `/swagger`\n  - `/api-docs`\n  - `/docs/api`\n- If interactive discovery needed, document that Playwright could be used\n- If NO documentation found, use generic REST patterns with clear warnings\n\nPhase 2: API Structure Analysis & Architecture Decision\nGoal: Extract endpoint information and determine server architecture\n\n**If using Postman/Newman:**\n- Use skills loaded above (postman-collection-manager, newman-runner, api-schema-analyzer)\n- Refer to scripts from newman-runner skill for collection analysis\n- Refer to templates from api-schema-analyzer skill for endpoint extraction\n- Export collection to temporary JSON file\n- Run Newman to validate collection and extract:\n  - **Total endpoint count** (CRITICAL for architecture decision)\n  - Endpoint paths and HTTP methods\n  - Request parameters (path, query, body)\n  - Response schemas\n  - Authentication requirements\n  - Error responses\n  - Folder structure (for natural toolset groupings)\n- Analyze API patterns: RESTful conventions, pagination, error handling, authentication flow\n- **Count endpoints by resource/folder** to identify toolset groupings\n\n**If using WebFetch/Playwright:**\n- Use api-schema-analyzer skill (loaded above) for OpenAPI/Swagger parsing\n- Refer to scripts and templates from skill for parsing patterns\n- Parse documentation to extract:\n  - **Total endpoint count**\n  - Endpoint paths and HTTP methods\n  - Required and optional parameters\n  - Authentication methods (Bearer token, API key, OAuth, etc.)\n  - Response formats and schemas\n  - Rate limiting information\n  - Error response codes\n  - Resource groupings (from table of contents/sections)\n- Look for OpenAPI/Swagger specs that contain structured endpoint data\n- Document any assumptions made due to incomplete documentation\n\n**Architecture Decision (NEW - Critical Step):**\n\nBased on total endpoint count, determine server architecture:\n\n1. **<30 endpoints:** Single server, all tools directly in main file\n   - Simple, straightforward implementation\n   - No toolsets needed\n   - Estimated file size: 400-900 lines\n\n2. **30-80 endpoints:** Ask user to choose:\n   - **Option A (Recommended):** Toolsets pattern\n     - 5-8 toolsets with default subset\n     - CLI: `--toolsets <list>`\n     - Estimated: 800-2,000 lines\n   - **Option B:** Hybrid approach\n     - 20 common tools + generic request tool\n     - Simpler but less flexible\n     - Estimated: 600-1,200 lines\n\n3. **80-150 endpoints (e.g., GitHub 103, CATS 162):** Toolsets pattern (REQUIRED)\n   - 10-20 toolsets organized by resource\n   - Default: 5 core toolsets (~30-40% of API)\n   - Optional: 10-15 additional toolsets\n   - CLI flags: `--toolsets candidates,jobs,companies` or `--toolsets all`\n   - Environment: `{API_NAME}_TOOLSETS=\"core,extended\"`\n   - **Key benefit:** Agents load only needed toolsets (token efficient)\n   - Estimated: 3,000-6,000 lines (but organized into clear sections)\n\n4. **150+ endpoints:** Multiple domain servers\n   - Split into 3-5 specialized servers\n   - Each server: 30-50 tools\n   - Better permission isolation\n\n**Toolset Organization (for Large APIs):**\n- Analyze Postman folder structure or API documentation sections\n- Identify natural resource groupings\n- Calculate tool count per grouping\n- Designate 5 most-used groupings as DEFAULT_TOOLSETS\n- Remaining groupings become OPTIONAL_TOOLSETS\n- Document use cases for each toolset combination\n\n**Present Architecture Recommendation:**\n- Show total endpoint count\n- Show recommended architecture with reasoning\n- If toolsets: show proposed DEFAULT and OPTIONAL toolsets with tool counts\n- Explain token efficiency benefits (agents load ~30-40% of tools by default)\n- Get user confirmation before proceeding\n\nPhase 3: Planning & User Confirmation\nGoal: Design the MCP tool wrappers and confirm approach\n\nActions:\n- For each API endpoint, design an MCP tool (convert endpoint to function name, extract parameters, define return types)\n- Present plan: number of tools, example signatures, authentication strategy, error handling approach\n- Reference FastMCP docs: https://gofastmcp.com/servers/tools, /servers/auth/token-verification\n- Confirm before generating code\n\nPhase 4: Implementation\nGoal: Generate FastMCP server with API wrapper tools\n\nInvoke the fastmcp-api-wrapper agent to create the API wrapper tools.\n\n**Provide the agent with:**\n- **Architecture:** Which pattern to use (simple, toolsets, or multi-server)\n- **Endpoint count:** Total number of endpoints\n- **Toolset breakdown** (if using toolsets):\n  - DEFAULT_TOOLSETS list with tool counts\n  - OPTIONAL_TOOLSETS list with tool counts\n  - Toolset groupings (which endpoints belong to which toolset)\n- **Context:** Collection structure and API analysis from Phase 2\n- **Endpoints:** List of endpoints to wrap with full specifications\n- **Server path:** Where to add/create the server\n- **Expected output:** Complete working FastMCP server application\n\n**The agent should generate:**\n\nFor **Small/Medium APIs (<80 endpoints):**\n- Single server file with all tools directly registered\n- Standard structure: imports, config, helper functions, tools, main block\n\nFor **Large APIs (80+ endpoints) - Toolsets Pattern:**\n- Toolset registration functions (one per resource group)\n- CLI argument parser with `--toolsets` flag\n- Environment variable support (`{API_NAME}_TOOLSETS`)\n- Toolset loading logic\n- Default toolsets vs optional toolsets\n- `--list-toolsets` command to show available toolsets\n- Clear section headers separating toolsets\n\n**All servers should include:**\n- @mcp.tool() decorator for each endpoint\n- Proper function naming (HTTP method + resource)\n- Type hints for parameters and returns\n- HTTP client code (httpx for Python, fetch for TypeScript)\n- Authentication headers\n- Error handling (map HTTP status ‚Üí appropriate exceptions)\n- Comprehensive docstrings with endpoint reference\n- Helper functions: make_request(), auth headers, error handling\n- .env.example with API_BASE_URL, API_KEY\n- Dependencies: httpx (Python), node-fetch (TypeScript)\n\n**Documentation to generate:**\n- README section explaining toolset usage (if applicable)\n- Example .mcp.json configuration\n- Common toolset combinations by use case\n- Token efficiency explanation\n\nPhase 5: Verification\nGoal: Validate the generated code works\n\nActions:\n- Run syntax check on generated code\n- Verify all dependencies are listed\n- Test that server can start\n- Optionally run test request with Newman\n\nPhase 6: Documentation & Next Steps\nGoal: Guide user on using the API wrapper\n\nActions:\n- Show generated tool list with signatures\n- Explain how to configure API credentials\n- **Document transport configuration**:\n  - STDIO mode (default) for Claude Desktop/Code integration via `.mcp.json`\n  - HTTP mode for remote services/cloud deployment\n  - Show `.mcp.json` example with all environment variables\n  - Provide HTTP deployment examples (Docker, cloud platforms)\n- Show example of calling tools from Claude Desktop\n- Provide FastMCP testing guidance\n- Suggest enhancements: rate limiting, caching, batch operations, webhooks\n\nSuccess Criteria:\n- ‚úÖ Postman collection successfully analyzed\n- ‚úÖ Newman validation passed\n- ‚úÖ MCP tools generated for all selected endpoints\n- ‚úÖ Type hints and documentation complete\n- ‚úÖ Authentication handling implemented (if needed)\n- ‚úÖ Error handling covers common API errors\n- ‚úÖ Server can start without errors\n- ‚úÖ Environment variables documented in .env.example"
              },
              {
                "name": "/add-auth",
                "description": null,
                "path": "plugins/fastmcp/commands/add-auth.md",
                "frontmatter": null,
                "content": "---\ndescription: Add authentication to FastMCP server (OAuth 2.1, JWT, Bearer Token, all providers)\nargument-hint: [auth-type] [--server-path=path]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add authentication and authorization to an existing FastMCP server. Supports OAuth 2.1 (all providers), JWT token verification, Bearer tokens, and authorization middleware.\n\nCore Principles:\n- Ask which authentication method to use\n- Support all OAuth providers (Google, GitHub, Azure, WorkOS, Auth0, AWS Cognito, Descope, Scalekit, Supabase)\n- Handle both server-side and remote OAuth\n- Secure credential management\n- Follow FastMCP auth patterns\n\nPhase 1: Discovery\nGoal: Understand authentication requirements\n\nActions:\n- Parse $ARGUMENTS for auth type and server path\n- Use AskUserQuestion to gather:\n  - Authentication method:\n    - OAuth 2.1 Providers (select which):\n      - Google\n      - GitHub\n      - Azure (Microsoft Entra)\n      - WorkOS / AuthKit\n      - Auth0\n      - AWS Cognito\n      - Descope\n      - Scalekit\n      - Supabase\n    - JWT Token Verification\n    - Bearer Token\n    - Remote OAuth (proxy)\n  - OAuth configuration (if OAuth selected):\n    - Client ID source (env variable name)\n    - Client secret source (env variable name)\n    - Scopes needed\n    - Redirect URI\n    - Token storage strategy\n  - JWT configuration (if JWT selected):\n    - JWT secret source\n    - Token location (header, cookie)\n    - Claims to validate\n    - Issuer/audience validation\n  - Bearer token configuration (if Bearer selected):\n    - Token storage method (env, file, database)\n    - Token format\n  - Server file location\n- Load FastMCP auth documentation:\n  @plugins/fastmcp/docs/fastmcp-documentation.md\n\nPhase 2: Analysis\nGoal: Understand existing server and language\n\nActions:\n- Find server file\n- Read existing code to determine:\n  - Language (Python or TypeScript)\n  - Existing authentication (if any)\n  - Import patterns\n  - Middleware configuration\n- Check for conflicts with existing auth\n\nPhase 3: Implementation\nGoal: Add authentication\n\nActions:\n\n**Use Task tool NOW to implement authentication:**\n\n```\nTask(\n  subagent_type=\"general-purpose\", description=\"Add authentication to FastMCP server\", prompt=\"Add {auth-type} authentication to FastMCP server at {server-path}.\n\n**Authentication Type:** {selected-auth-type}\n**Server Language:** {detected-language}\n**Configuration:** {auth-config from Phase 1}\n\nWebFetch relevant authentication documentation:\n  - OAuth Proxy: https://gofastmcp.com/servers/auth/oauth-proxy\n  - Remote OAuth: https://gofastmcp.com/servers/auth/remote-oauth\n  - Token Verification: https://gofastmcp.com/servers/auth/token-verification\n  - Provider-specific docs:\n    - Google: https://gofastmcp.com/integrations/google\n    - GitHub: https://gofastmcp.com/integrations/github\n    - Azure: https://gofastmcp.com/integrations/azure\n    - WorkOS: https://gofastmcp.com/integrations/workos + https://gofastmcp.com/integrations/authkit\n    - Auth0: https://gofastmcp.com/integrations/auth0\n    - AWS Cognito: https://gofastmcp.com/integrations/aws-cognito\n    - Descope: https://gofastmcp.com/integrations/descope\n    - Scalekit: https://gofastmcp.com/integrations/scalekit\n- Add provider-specific imports\n- Configure provider with credentials from environment\n- Add authentication middleware to server\n- Update .env.example with required variables\n- Add redirect URI configuration (for OAuth)\n- Include error handling for auth failures\n- Add authorization middleware if requested:\n  - Permit.io: https://gofastmcp.com/integrations/permit\n  - Eunomia: https://gofastmcp.com/integrations/eunomia-authorization\n\nProvide the agent with:\n- Context: Auth method and configuration from Phase 1\n- Target: Server file path\n- Language: Python or TypeScript\n- Expected output: Authentication configured and working\n\nPhase 4: Configuration Files\nGoal: Update environment and documentation\n\nActions:\n- Ensure .env.example has all required variables:\n  - OAuth: CLIENT_ID, CLIENT_SECRET, REDIRECT_URI\n  - JWT: JWT_SECRET, JWT_ISSUER, JWT_AUDIENCE\n  - Bearer: BEARER_TOKEN or token file path\n- Add .gitignore entry for .env if missing\n- Update README with:\n  - How to get OAuth credentials\n  - Environment variables needed\n  - Authentication flow explanation\n  - Testing instructions\n\nPhase 5: Verification\nGoal: Verify authentication works\n\nActions:\n- Run syntax check\n- Verify imports are correct\n- Check .env.example is complete\n- Verify .env is in .gitignore\n- Scan for hardcoded credentials (should be none)\n\nPhase 6: Summary\nGoal: Show configuration and next steps\n\nActions:\n- Display authentication setup\n- Show .env variables needed\n- Explain how to obtain credentials:\n  - Google: https://console.cloud.google.com\n  - GitHub: https://github.com/settings/developers\n  - Azure: https://portal.azure.com\n  - WorkOS: https://workos.com\n  - Auth0: https://auth0.com\n  - AWS Cognito: https://aws.amazon.com/cognito\n  - Descope: https://app.descope.com\n  - Scalekit: https://app.scalekit.com\n- Provide testing examples\n- Suggest adding authorization if needed (Permit.io, Eunomia)\n"
              },
              {
                "name": "/add-components",
                "description": null,
                "path": "plugins/fastmcp/commands/add-components.md",
                "frontmatter": null,
                "content": "---\ndescription: Add MCP components to existing FastMCP server (tools, resources, prompts, middleware, context, dependencies)\nargument-hint: [component-type] [--server-path=path]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add MCP protocol components to an existing FastMCP server. Supports tools, resources, prompts, middleware, context management, dependencies, and elicitation.\n\nCore Principles:\n- Ask which component type to add\n- Support multiple components in one invocation\n- Follow FastMCP SDK patterns from documentation\n- Use proper decorators and type hints\n\nPhase 1: Discovery\nGoal: Understand which components to add\n\nActions:\n- Parse $ARGUMENTS for component type and server path\n- Use AskUserQuestion to gather:\n  - Which components to add (can select multiple):\n    - Tools (@mcp.tool() - executable functions)\n    - Resources (@mcp.resource() - data access with URI templates)\n    - Prompts (@mcp.prompt() - LLM interaction templates)\n    - Middleware (logging, caching, error handling, timing)\n    - Context (server context management)\n    - Dependencies (dependency injection)\n    - Elicitation (user input from server)\n  - Server file location (if not provided)\n  - For each selected component, ask specific details:\n    - Tools: Function name, parameters, return type, description\n    - Resources: URI template, data source, caching strategy\n    - Prompts: Template name, variables, use case\n    - Middleware: Type (logging, caching, error, timing, custom)\n    - Context: State management needs\n    - Dependencies: Services to inject\n- Load FastMCP documentation:\n  @plugins/fastmcp/docs/fastmcp-documentation.md\n\nPhase 2: Analysis\nGoal: Understand existing server structure\n\nActions:\n- Find server file (server.py, main.py, src/server.ts, src/index.ts)\n- Read existing code to understand:\n  - Current language (Python or TypeScript)\n  - Existing components\n  - Import patterns\n  - Code style\n- Identify where to add new components\n- Check for conflicts with existing names\n\nPhase 3: Implementation\nGoal: Add selected components\n\nActions:\n\nDetermine complexity:\n- Count total components to add\n- If adding 10 or fewer components: Implement directly (no agent needed)\n- If adding 11-50 components: Optionally use Task with general-purpose agent\n- If adding 50+ components OR wrapping API endpoints: Use Task with general-purpose agent\n\nFor simple cases (<=10 components), implement directly:\n- WebFetch relevant documentation based on component type:\n  - Tools: https://gofastmcp.com/servers/tools\n  - Resources: https://gofastmcp.com/servers/resources\n  - Prompts: https://gofastmcp.com/servers/prompts\n  - Middleware: https://gofastmcp.com/servers/middleware\n  - Context: https://gofastmcp.com/servers/context\n  - Dependencies: https://gofastmcp.com/servers/dependencies\n  - Elicitation: https://gofastmcp.com/servers/elicitation\n- Read existing server code\n- Add component following fetched documentation patterns\n- Use Edit tool to add imports and component code\n- Add proper type hints/annotations (Python) or TypeScript types\n- Include comprehensive docstrings/comments\n- Add error handling\n- Follow existing code style\n\nFor complex cases (>50 components), use Task tool NOW:\n\n```\nTask(\n  subagent_type=\"general-purpose\", description=\"Add MCP components to FastMCP server\", prompt=\"Add {count} MCP components to FastMCP server at {server-path}.\n\n**Component Specifications:** {specs from Phase 1}\n**Server Language:** {detected-language}\n**Target File:** {server-file-path}\n- Expected output: Component(s) added to server\n\nPhase 4: Verification\nGoal: Verify components work\n\nActions:\n- Run syntax check based on language:\n  - Python: `python -m py_compile <file>`\n  - TypeScript: `npx tsc --noEmit`\n- Verify imports are correct\n- Check that server can start\n- Display added components summary\n\nPhase 5: Summary\nGoal: Show what was added and next steps\n\nActions:\n- List all components added with their signatures\n- Show how to test each component:\n  - Tools: How to call from client\n  - Resources: URI pattern to access\n  - Prompts: How to use in LLM interactions\n  - Middleware: Execution order and effects\n- Suggest next steps:\n  - Add authentication if handling sensitive data\n  - Configure deployment for production\n  - Add testing for new components\n  - Consider adding related components\n"
              },
              {
                "name": "/add-deployment",
                "description": null,
                "path": "plugins/fastmcp/commands/add-deployment.md",
                "frontmatter": null,
                "content": "---\ndescription: Configure deployment for FastMCP server (HTTP, STDIO, FastMCP Cloud, production config)\nargument-hint: [deployment-type] [--server-path=path]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure deployment and transport for an existing FastMCP server. Supports HTTP, STDIO (Claude Desktop/Cursor/Claude Code), FastMCP Cloud, and production configuration.\n\n## Overview\n\nThis command delegates to the specialized FastMCP deployment agent which handles:\n- Transport configuration (STDIO, HTTP, FastMCP Cloud)\n- IDE integration (Claude Desktop, Cursor, Claude Code)\n- Production features (monitoring, logging, error handling, rate limiting)\n- Security setup (CORS, SSL/TLS, authentication)\n- Deployment scripts and documentation\n\n## Reference Documentation\n\nThe deployment agent references:\n- Deployment strategy: @plugins/fastmcp/docs/DEPLOYMENT.md\n- FastMCP documentation: @plugins/fastmcp/docs/fastmcp-documentation.md\n- Official FastMCP docs via WebFetch:\n  - https://gofastmcp.com/deployment/running-server\n  - https://gofastmcp.com/deployment/http\n  - https://gofastmcp.com/deployment/fastmcp-cloud\n  - https://gofastmcp.com/deployment/server-configuration\n\n## Implementation\n\nUse Task tool to launch the fastmcp-deployment agent:\n\n```\nTask(\n  subagent_type: \"fastmcp:fastmcp-deployment\",\n  description: \"Configure FastMCP server deployment\",\n  prompt: \"Use the @plugins/fastmcp/agents/fastmcp-deployment.md agent to configure deployment for the FastMCP server.\n\n  Arguments provided: $ARGUMENTS\n\n  The agent should:\n  1. Discover current server configuration\n  2. Gather deployment requirements from the user\n  3. Configure requested transport protocols (STDIO, HTTP, FastMCP Cloud)\n  4. Generate IDE configuration files as needed\n  5. Add production features (logging, monitoring, error handling)\n  6. Create deployment scripts and documentation\n  7. Verify the deployment configuration works\n\n  Follow all phases in the fastmcp-deployment agent systematically.\"\n)\n```\n\n## Expected Outputs\n\nAfter the agent completes, the server should have:\n- ‚úÖ Configured transport protocols based on requirements\n- ‚úÖ IDE configuration files for selected targets\n- ‚úÖ Production middleware (logging, monitoring, error handling)\n- ‚úÖ Environment-specific configs (.env.development, .env.production)\n- ‚úÖ Security features (CORS, SSL/TLS, rate limiting)\n- ‚úÖ Health check endpoint\n- ‚úÖ Deployment scripts (start.sh, deploy.sh)\n- ‚úÖ Updated README with deployment instructions\n- ‚úÖ Documented environment variables\n"
              },
              {
                "name": "/add-integration",
                "description": null,
                "path": "plugins/fastmcp/commands/add-integration.md",
                "frontmatter": null,
                "content": "---\ndescription: Add integrations to FastMCP server (FastAPI, OpenAPI, LLM platforms, IDEs, authorization)\nargument-hint: [integration-type] [--server-path=path]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate existing FastMCP server with external platforms. Supports API frameworks (FastAPI, OpenAPI), LLM platforms (Anthropic, OpenAI, Gemini, ChatGPT), IDEs (Claude Desktop, Cursor, Claude Code), and authorization (Permit.io, Eunomia).\n\nPhase 1: Discovery\n\nActions:\n- Parse $ARGUMENTS for integration type and server path\n- Use AskUserQuestion to gather integration categories (can select multiple):\n  - API Frameworks: FastAPI (app path, mount path), OpenAPI (spec path, route mapping)\n  - LLM Platforms: Anthropic/OpenAI/Gemini/ChatGPT (API key source, features)\n  - IDEs: Claude Desktop/Cursor/Claude Code (installation method, config)\n  - Authorization: Permit.io/Eunomia (policy config, resources to protect)\n- Load FastMCP integrations docs: @plugins/fastmcp/docs/fastmcp-documentation.md\n\nPhase 2: Analysis\n\nActions:\n- Find server file and determine language (Python/TypeScript)\n- Read existing integrations\n- For FastAPI: find and read FastAPI app file\n- For OpenAPI: read OpenAPI specification\n- Check for conflicts\n\nPhase 3: Implementation\n\nActions:\n\nImplement directly or use Task with general-purpose agent for complex cases for each integration.\n\nAgent should WebFetch based on integration type:\n\nAPI Frameworks:\n- FastAPI: https://gofastmcp.com/integrations/fastapi\n- OpenAPI: https://gofastmcp.com/integrations/openapi\n\nLLM Platforms:\n- Anthropic: https://gofastmcp.com/integrations/anthropic\n- OpenAI: https://gofastmcp.com/integrations/openai\n- Gemini: https://gofastmcp.com/integrations/gemini\n- ChatGPT: https://gofastmcp.com/integrations/chatgpt\n\nIDEs:\n- Claude Desktop: https://gofastmcp.com/integrations/claude-desktop\n- Claude Code: https://gofastmcp.com/integrations/claude-code\n- Cursor: https://gofastmcp.com/integrations/cursor\n\nAuthorization:\n- Permit.io: https://gofastmcp.com/integrations/permit\n- Eunomia: https://gofastmcp.com/integrations/eunomia-authorization\n\nFastAPI: Import MCP server, mount at path, configure CORS, add middleware\nOpenAPI: Parse spec, generate MCP tools from endpoints, add validation, configure route mapping\nLLM Platform: Add client library, configure API key, setup MCP connector, add sampling support\nIDE: Generate config files, add installation scripts, configure server command\nAuthorization: Add middleware, configure policy engine, define resources/permissions, add checks, setup policy files\n\nPhase 4: Configuration & Dependencies\n\nActions:\n- Update .env.example with integration-specific variables (API keys, URLs)\n- Add dependencies to requirements.txt/package.json (fastapi, uvicorn, openapi-parser, anthropic, openai, google-generativeai, permit-fastmcp, eunomia)\n- Install new dependencies\n\nPhase 5: Documentation & Verification\n\nActions:\n- Add integration section to README (what integrated, how to configure, usage examples)\n- For FastAPI: combined endpoint structure\n- For OpenAPI: generated tools list\n- For LLM: MCP connector usage\n- For IDEs: installation steps\n- For Authorization: policy configuration\n- Run syntax check, verify imports\n- Test integration (start combined app, test endpoints, verify tools, test API connection, verify config files, test policy enforcement)\n- Check environment variables documented\n\nPhase 6: Summary\n\nActions:\n- Display integration summary (what integrated, configuration needed, testing instructions)\n- Show specific results (FastAPI endpoints, OpenAPI tools, connector usage, config locations, policy examples)\n- Suggest next steps (test thoroughly, add error handling, add monitoring, update documentation)\n"
              },
              {
                "name": "/fastmcp-verify",
                "description": "Verify MCP servers follow proper FastMCP framework structure and conventions",
                "path": "plugins/fastmcp/commands/fastmcp-verify.md",
                "frontmatter": {
                  "description": "Verify MCP servers follow proper FastMCP framework structure and conventions",
                  "argument-hint": [
                    "server-path or \"all\""
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Analyze Python-based MCP servers to verify they follow FastMCP framework conventions, identifying non-compliant servers and providing detailed remediation recommendations.\n\nCore Principles:\n- Detect actual server structure, don't assume\n- Provide actionable remediation steps\n- Support both single-server and bulk verification\n- Report compliance status clearly\n\nPhase 1: Discovery\nGoal: Identify which MCP servers need to be verified\n\nActions:\n- Parse $ARGUMENTS to determine scope:\n  - If \"all\" or empty: Verify all Python servers in marketplace\n  - If path provided: Verify specific server\n  - If category: Verify all servers in that category\n- Use TodoWrite to create task list with identified servers\n- List all Python-based MCP servers to verify:\n  !{bash find /home/gotime2022/.claude/plugins/marketplaces/mcp-servers/servers -name \"server.py\" -o -name \"*.py\" -path \"*/src/*\" | grep -v venv | grep -v \".venv\"}\n\nPhase 2: Initial Analysis\nGoal: Quick scan to categorize servers\n\nActions:\n- For each server identified, check:\n  - Uses FastMCP framework (imports `from fastmcp import FastMCP`)\n  - Has proper server initialization (`mcp = FastMCP(...)`)\n  - Has lifespan management (`@asynccontextmanager async def lifespan()`)\n  - Uses modern FastMCP 2.x patterns\n- Create summary of:\n  - Compliant servers\n  - Non-compliant servers\n  - Servers needing updates\n- Update todos with categorization\n\nPhase 3: Detailed Verification\nGoal: Run comprehensive FastMCP framework validation\n\nActions:\n\nLaunch the fastmcp-verifier agent to perform deep analysis:\n\nTask(description=\"Verify FastMCP compliance\", subagent_type=\"fastmcp-verifier\", prompt=\"You are the fastmcp-verifier agent. Analyze the MCP servers identified to verify they follow proper FastMCP framework structure and conventions.\n\nServers to verify: $ARGUMENTS\n\nFor each server, validate:\n\n1. **Framework Import & Initialization**\n   - Imports FastMCP from `fastmcp` package\n   - Creates server instance: `mcp = FastMCP(name, instructions, version)`\n   - Uses proper FastMCP constructor parameters\n\n2. **Lifecycle Management (FastMCP 2.x)**\n   - Has `@asynccontextmanager async def lifespan()` function\n   - Lifespan contains startup and shutdown logic\n   - Server initialization includes `lifespan=lifespan` parameter\n   - No deprecated patterns (old-style `with_context` or manual server start)\n\n3. **Tool Definitions**\n   - Uses `@mcp.tool` decorator for all tools\n   - Proper type hints with `Annotated[type, Field(description=...)]`\n   - Includes `Context` parameter for logging/progress\n   - Uses `ctx.info()`, `ctx.warning()`, `ctx.report_progress()` appropriately\n\n4. **Resource Definitions**\n   - Uses `@mcp.resource(uri)` decorator\n   - Proper URI patterns (e.g., `servername://resource/path`)\n   - Returns appropriate data structures\n\n5. **Prompt Definitions**\n   - Uses `@mcp.prompt` decorator\n   - Takes parameters for dynamic prompt generation\n   - Returns helpful guidance strings\n\n6. **Server Execution**\n   - Has `if __name__ == \\\"__main__\\\":` block\n   - Calls `mcp.run()` for STDIO mode\n   - No custom server runners (unless specifically needed)\n\n7. **Environment & Configuration**\n   - Loads `.env` from server directory\n   - Uses `load_dotenv()` properly\n   - Has proper environment variable handling\n\n8. **Code Quality**\n   - Proper async/await usage\n   - Type hints throughout\n   - Docstrings for tools/resources/prompts\n   - Error handling with meaningful messages\n\nFor each server analyzed, provide:\n\n**Compliance Report Structure:**\n```\nServer: [name]\nPath: [path]\nStatus: [COMPLIANT / NEEDS_UPDATES / NON_COMPLIANT]\n\n‚úÖ Passing Checks:\n- [List what's correct]\n\n‚ùå Failing Checks:\n- [List violations]\n\n‚ö†Ô∏è  Warnings:\n- [List concerns]\n\nüìù Remediation Steps:\n1. [Step-by-step fixes needed]\n2. [Include code examples]\n3. [Reference FastMCP docs]\n\nPriority: [HIGH / MEDIUM / LOW]\n```\n\nExpected output:\n- Comprehensive compliance report for all servers\n- Categorized by compliance status\n- Prioritized remediation plan\n- Example code fixes for common issues\n- Summary statistics (X of Y servers compliant)\")\n\nUpdate todos as verification progresses.\n\nPhase 4: Generate Remediation Plan\nGoal: Create actionable next steps\n\nActions:\n- Review agent's verification report\n- Identify highest priority fixes\n- Group fixes by type:\n  - Quick wins (simple updates)\n  - Medium effort (pattern refactoring)\n  - Major rewrites (non-FastMCP servers)\n- Create prioritized task list\n- Update todos\n\nPhase 5: Summary\nGoal: Present verification results and recommendations\n\nActions:\n- Mark all todos complete\n- Display comprehensive summary:\n  - **Overall Statistics**\n    - Total servers analyzed: X\n    - Fully compliant: X\n    - Need updates: X\n    - Non-compliant: X\n  - **By Priority**\n    - High priority fixes: [list]\n    - Medium priority fixes: [list]\n    - Low priority improvements: [list]\n  - **Quick Wins** (can fix now)\n    - [List servers with simple fixes]\n  - **Major Updates Needed**\n    - [List servers requiring significant work]\n  - **Recommended Next Steps**\n    1. Fix high priority issues first\n    2. Update servers to FastMCP 2.x patterns\n    3. Add missing lifecycle management\n    4. Improve type hints and documentation\n\n- Offer to:\n  - Fix specific servers automatically\n  - Generate migration guides for non-compliant servers\n  - Create tracking issues for remediation work"
              },
              {
                "name": "/new-client",
                "description": "Create and setup a new FastMCP client project with Python or TypeScript for connecting to MCP servers",
                "path": "plugins/fastmcp/commands/new-client.md",
                "frontmatter": {
                  "description": "Create and setup a new FastMCP client project with Python or TypeScript for connecting to MCP servers",
                  "argument-hint": "<client-name>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a FastMCP client project for connecting to and interacting with MCP servers. Supports both Python and TypeScript.\n\nCore Principles:\n- Ask language preference first (Python or TypeScript)\n- Route to correct setup agent based on language\n- Follow FastMCP Client SDK patterns\n- Create functional client code with proper transport configuration\n\nPhase 1: Discovery & Education\nGoal: Understand what the user wants to build through interactive conversation\n\nActions:\n- Parse $ARGUMENTS for project name\n- Load FastMCP documentation:\n  @plugins/fastmcp/docs/fastmcp-documentation.md\n- Have an interactive conversation to understand the client:\n\n  **Start with Purpose:**\n  - \"What MCP server(s) will this client connect to?\"\n  - \"What will this client do? (e.g., 'call tools', 'fetch resources', 'automated workflows')\"\n\n  **Explain Transport Types:**\n  - **STDIO**: For local MCP servers (like Claude Desktop uses)\n    - Reference: https://gofastmcp.com/clients/client-transports\n  - **HTTP**: For remote MCP servers over HTTP/HTTPS\n  - **In-Memory**: For testing or embedded servers\n  - Ask which transport they need and explain the differences\n\n  **Explain Client Capabilities:**\n  - Show them how clients call tools: `client.call_tool(\"tool_name\", {...})`\n  - Show them how to fetch resources: `client.read_resource(\"uri://resource\")`\n  - Explain callback handlers for server notifications\n  - Reference: https://gofastmcp.com/clients/client-usage\n\n  **Language Selection:**\n  - Use AskUserQuestion for language (Python or TypeScript)\n  - Explain: Python for scripts/automation, TypeScript for applications\n\n  **Connection Strategy:**\n  - Will they connect to multiple servers? Explain connection management\n  - Do they need authentication? Explain client-side auth\n\n- Store all choices for Phase 3\n\nPhase 2: Validation\nGoal: Verify project doesn't exist and environment is ready\n\nActions:\n- Check if directory already exists\n- If Python chosen:\n  - Verify Python is installed (Python 3.10+)\n  - Check if uv or pip is available\n- If TypeScript chosen:\n  - Verify Node.js is installed (Node 18+)\n  - Check if npm/yarn/pnpm is available\n- Confirm project location\n\nPhase 3: Implementation\nGoal: Create client project with language-specific agent\n\nActions:\n\n**If Python was chosen in Phase 1:**\n\nInvoke the fastmcp-client-setup agent to create the FastMCP client application.\n\nThe agent should:\n- Create Python project structure\n- Set up pyproject.toml with FastMCP client dependencies\n- Generate starter client code with transport configuration\n- Include examples for tool calls, resource fetching\n- Create README with setup instructions\n- Add .gitignore for Python projects\n- Create .env.example if needed\n\nProvide the agent with:\n- Context: Client requirements and transport type\n- Target: $ARGUMENTS (project name)\n- Expected output: Working FastMCP client application\n\n**If TypeScript was chosen in Phase 1:**\n\nInvoke the fastmcp-client-setup-ts agent to create the FastMCP client application.\n\nThe agent should:\n- Create Node.js/TypeScript project structure\n- Set up package.json with FastMCP client TypeScript dependencies\n- Create tsconfig.json with ES modules support\n- Generate starter client code with TypeScript types and transport configuration\n- Include examples for tool calls, resource fetching\n- Create README with setup and TypeScript build instructions\n- Add .gitignore for Node.js/TypeScript projects\n- Create .env.example if needed\n\nProvide the agent with:\n- Context: Client requirements and transport type\n- Target: $ARGUMENTS (project name)\n- Expected output: Working FastMCP client application with TypeScript\n\nPhase 4: Summary\nGoal: Guide user on next steps\n\nActions:\n- Show project location and structure\n- Display how to connect to MCP servers\n- Explain transport configuration\n- Provide FastMCP Client documentation links"
              },
              {
                "name": "/new-server",
                "description": "Create complete FastMCP server with all features - orchestrates setup, components, auth, deployment, verification, and testing",
                "path": "plugins/fastmcp/commands/new-server.md",
                "frontmatter": {
                  "description": "Create complete FastMCP server with all features - orchestrates setup, components, auth, deployment, verification, and testing",
                  "argument-hint": "<server-name> [--language=python|typescript] [--purpose=\"description\"] [--collection=path] [--auth=type] [--deployment=type] [--skip-questions]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n## Command Purpose\n\n`/fastmcp:new-server` is the MAIN ORCHESTRATOR for building complete FastMCP servers.\n\nIt chains multiple slash commands sequentially to build a production-ready server:\n1. Creates base server structure (Python or TypeScript)\n2. Adds API wrapper tools (if Postman collection provided)\n3. Adds additional components (if specified)\n4. Configures authentication (if specified)\n5. Sets up deployment (if specified)\n6. Verifies server structure and compliance\n7. Generates and runs comprehensive tests\n8. Provides complete summary and next steps\n\n**This command ORCHESTRATES other commands - it does NOT invoke agents directly!**\n\n## Phase 1: Parse Arguments and Gather Requirements\n\n**Parse $ARGUMENTS to extract:**\n- `server-name` - First positional argument\n- `--language=python|typescript` - Language choice\n- `--purpose=\"description\"` - Server purpose\n- `--collection=/path/to/collection.json` - Optional Postman collection\n- `--auth=oauth|jwt|bearer` - Optional authentication type\n- `--deployment=stdio|http|cloud` - Optional deployment type\n- `--skip-questions` - Skip interactive questions\n\n**If `--skip-questions` provided OR all parameters present:**\n- Skip to Phase 2 immediately\n- Use provided parameters\n\n**If parameters missing:**\n- Use TodoWrite to track requirements gathering\n- Ask interactive questions:\n  - Language: \"Which language: Python or TypeScript?\"\n  - Purpose: \"What will this MCP server do?\"\n  - Components: \"What features do you need? (tools, resources, prompts)\"\n  - Authentication: \"Do you need authentication? (OAuth, JWT, Bearer, none)\"\n  - Deployment: \"Where will you deploy? (STDIO/local, HTTP/remote, FastMCP Cloud)\"\n\n**Update TodoWrite with detected/gathered parameters**\n\n## Phase 2: Create Base Server Structure\n\n**Action: Invoke new-server-base setup agent**\n\n**For Python servers:**\n\nUse Task tool NOW to invoke the fastmcp-setup agent:\n\n```\nTask(\n  subagent_type=\"fastmcp:fastmcp-setup\",\n  description=\"Create Python FastMCP server structure\",\n  prompt=\"Create FastMCP Python server with these requirements:\n\n**Project name:** {server-name}\n**Purpose:** {purpose from Phase 1}\n**Location:** {current-directory}/{server-name}\n\nCreate complete Python FastMCP server with:\n- Project directory structure\n- pyproject.toml with fastmcp dependency\n- server.py with FastMCP initialization\n- .env.example with placeholders\n- .gitignore for Python projects\n- README.md with setup instructions\n\nFollow FastMCP SDK best practices. Generate functional starter code, not placeholders.\"\n)\n```\n\n**For TypeScript servers:**\n\nUse Task tool NOW to invoke the fastmcp-setup-ts agent:\n\n```\nTask(\n  subagent_type=\"fastmcp:fastmcp-setup-ts\",\n  description=\"Create TypeScript FastMCP server structure\",\n  prompt=\"Create FastMCP TypeScript server with these requirements:\n\n**Project name:** {server-name}\n**Purpose:** {purpose from Phase 1}\n**Location:** {current-directory}/{server-name}\n\nCreate complete TypeScript FastMCP server with:\n- Project directory structure\n- package.json with fastmcp dependency\n- tsconfig.json with proper configuration\n- src/server.ts with FastMCP initialization\n- .env.example with placeholders\n- .gitignore for Node.js/TypeScript\n- README.md with setup instructions\n\nFollow FastMCP SDK best practices. Generate functional starter code, not placeholders.\"\n)\n```\n\n**WAIT for agent completion.**\n\n**After completion:**\n- Verify server directory was created\n- Confirm server files exist\n- Update TodoWrite: mark \"Create base server\" as completed\n- Capture server path for next phases\n\n**If agent failed:**\n- Report error to user\n- STOP workflow - do not proceed to Phase 3\n\n## Phase 3: Add API Wrapper Tools (if --collection provided)\n\n**Check if `--collection` parameter was provided in Phase 1.**\n\n**If YES:**\n\nUse SlashCommand tool NOW to invoke add-api-wrapper command:\n\n```\nSlashCommand(command=\"/fastmcp:add-api-wrapper {server-name} --collection={collection-path}\")\n```\n\n**WAIT for command completion.**\n\n**After completion:**\n- Verify tools were generated\n- Update TodoWrite: mark \"Add API wrapper tools\" as completed\n\n**If NO collection provided:**\n- Skip to Phase 4\n\n## Phase 4: Add Additional Components (if needed)\n\n**Check if additional components needed beyond API wrapper.**\n\nCommon scenarios:\n- User wants custom tools beyond API wrapper\n- User wants resources for data access\n- User wants prompts for LLM interactions\n\n**If additional components needed:**\n\nUse SlashCommand tool NOW:\n\n```\nSlashCommand(command=\"/fastmcp:add-components {component-types} --server-path={detected-path}\")\n```\n\nWhere `{component-types}` might be: `tools`, `resources`, `prompts`, or combinations.\n\n**WAIT for command completion.**\n\n**After completion:**\n- Update TodoWrite: mark \"Add components\" as completed\n\n**If no additional components needed:**\n- Skip to Phase 5\n\n## Phase 5: Configure Authentication (if --auth provided)\n\n**Check if `--auth` parameter was provided in Phase 1.**\n\n**If YES:**\n\nUse SlashCommand tool NOW:\n\n```\nSlashCommand(command=\"/fastmcp:add-auth {auth-type} --server-path={detected-path}\")\n```\n\nWhere `{auth-type}` is: `oauth`, `jwt`, or `bearer`\n\n**WAIT for command completion.**\n\n**After completion:**\n- Verify authentication was configured\n- Update TodoWrite: mark \"Configure authentication\" as completed\n\n**If NO auth specified:**\n- Skip to Phase 6\n\n## Phase 6: Set Up Deployment (if --deployment provided)\n\n**Check if `--deployment` parameter was provided in Phase 1.**\n\n**If YES:**\n\nUse SlashCommand tool NOW:\n\n```\nSlashCommand(command=\"/fastmcp:add-deployment {deployment-type} --server-path={detected-path}\")\n```\n\nWhere `{deployment-type}` is: `stdio`, `http`, or `cloud`\n\n**WAIT for command completion.**\n\n**After completion:**\n- Verify deployment configuration created\n- Update TodoWrite: mark \"Set up deployment\" as completed\n\n**If NO deployment specified:**\n- Default to STDIO (already in base server)\n- Skip to Phase 7\n\n## Phase 7: Verify Server Structure\n\n**Action: Verify server compliance**\n\nUse SlashCommand tool NOW:\n\n```\nSlashCommand(command=\"/fastmcp:fastmcp-verify {detected-path}\")\n```\n\n**WAIT for command completion.**\n\n**After completion:**\n- Review verification report\n- Check for compliance issues\n- Update TodoWrite: mark \"Verify server\" as completed\n\n**If verification found critical issues:**\n- Report issues to user\n- Suggest fixes\n- Allow user to decide whether to continue\n\n## Phase 8: Generate and Run Tests\n\n**Action: Create comprehensive test suite**\n\nUse SlashCommand tool NOW:\n\n```\nSlashCommand(command=\"/fastmcp:test --server-path={detected-path} --run --coverage\")\n```\n\n**WAIT for command completion.**\n\n**After completion:**\n- Review test results\n- Check test coverage\n- Update TodoWrite: mark \"Run tests\" as completed\n\n**If tests failed:**\n- Report failures to user\n- Suggest fixes\n- Server is created but may need adjustments\n\n## Phase 9: Complete Summary\n\n**Display comprehensive summary:**\n\n```\n‚úÖ FastMCP Server Created Successfully!\n\n**Server Details:**\n- Name: {server-name}\n- Language: {Python|TypeScript}\n- Location: {full-path}\n- Purpose: {purpose}\n\n**Components Added:**\n- Base server structure ‚úì\n- API wrapper tools (if applicable) ‚úì\n- Additional components (if applicable) ‚úì\n- Authentication (if applicable) ‚úì\n- Deployment configuration (if applicable) ‚úì\n\n**Verification:**\n- Structure validation: {PASSED|FAILED}\n- Test results: {PASSED|FAILED}\n- Test coverage: {percentage}%\n\n**Next Steps:**\n1. Navigate to server: cd {server-name}\n2. Review and customize server.py (or src/server.ts)\n3. Run server locally: {command based on language}\n4. Test with MCP client\n5. Deploy to {deployment-target}\n\n**Documentation:**\n- FastMCP Docs: https://gofastmcp.com\n- Server README: {server-name}/README.md\n```\n\n**Update TodoWrite: mark all tasks completed**\n\n## Error Handling\n\nAt each phase, if a command or agent fails:\n1. Report the specific error to user\n2. Explain what went wrong\n3. Suggest remediation steps\n4. Ask if they want to:\n   - Retry the failed step\n   - Skip the step and continue\n   - Abort the workflow\n\nDo NOT silently continue past errors.\n\n## Command Execution Rules\n\n**CRITICAL:**\n- Use SlashCommand tool for invoking commands\n- Use Task tool for invoking agents\n- WAIT for each step to complete before proceeding\n- Do NOT run commands in parallel\n- Do NOT describe what \"will happen\" - actually INVOKE the tools\n- Update TodoWrite after each phase\n\nThis ensures the orchestration actually executes, not just describes the workflow."
              },
              {
                "name": "/test",
                "description": null,
                "path": "plugins/fastmcp/commands/test.md",
                "frontmatter": null,
                "content": "---\ndescription: Generate and run comprehensive test suite for FastMCP server using in-memory testing pattern\nargument-hint: [--server-path=path] [--run] [--coverage]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate comprehensive pytest-based test suite for FastMCP server and optionally run tests with coverage reporting\n\nCore Principles:\n- Detect server architecture (simple vs toolsets)\n- Use FastMCP in-memory testing pattern\n- Generate parametrized tests for all components\n- Validate before running\n\nPhase 1: Discovery\nGoal: Analyze server structure and components\n\nActions:\n- Parse $ARGUMENTS for:\n  - `--server-path=<path>` (default: current directory)\n  - `--run` flag (execute tests after generation)\n  - `--coverage` flag (include coverage reporting)\n- Find server file (server.py or main.py)\n- Detect architecture pattern (simple vs toolsets)\n- Count tools, resources, prompts using Grep:\n  - !{bash grep -r \"@mcp.tool()\" --include=\"*.py\" | wc -l}\n  - !{bash grep -r \"@mcp.resource()\" --include=\"*.py\" | wc -l}\n  - !{bash grep -r \"@mcp.prompt()\" --include=\"*.py\" | wc -l}\n\nPhase 2: Analysis\nGoal: Understand what needs testing\n\nActions:\n- Identify all toolset files (toolset_*.py pattern)\n- List all tools/resources/prompts by file\n- Check for existing tests/ directory\n- Determine test dependencies needed\n\nPhase 3: Planning\nGoal: Present testing strategy\n\nActions:\n- Display server analysis:\n  - Server type (simple/toolsets)\n  - Component counts\n  - Files to test\n- Show test suite structure:\n  - tests/conftest.py (shared fixtures)\n  - tests/test_*.py (one per toolset/module)\n  - tests/pytest.ini (configuration)\n- List dependencies to add:\n  - pytest>=8.0.0\n  - pytest-asyncio>=0.23.0\n  - inline-snapshot>=0.13.0\n  - dirty-equals>=0.7.0\n  - pytest-cov (if --coverage)\n- Confirm with user before generating\n\nPhase 4: Test Generation\nGoal: Create comprehensive test suite\n\nActions:\n\nInvoke the fastmcp-tester agent to generate test suite.\n\nProvide the agent with:\n- Server structure analysis from Phase 1\n- Component counts and locations\n- Architecture type (simple/toolsets)\n- List of all tools/resources/prompts to test\n- Flags: --run and --coverage status\n\nThe agent should:\n- Create tests/ directory structure\n- Generate conftest.py with mcp_client fixture\n- Generate test_*.py for each toolset/module\n- Include parametrized tests for edge cases\n- Add error handling tests\n- Create pytest.ini configuration\n- Update requirements.txt or pyproject.toml\n- Follow FastMCP in-memory testing pattern\n\nPhase 5: Verification\nGoal: Validate generated tests\n\nActions:\n- Check test syntax: !{bash python -m py_compile tests/*.py}\n- Verify imports: !{bash python -c \"from tests.conftest import mcp_client\"}\n- Check pytest can collect: !{bash pytest tests/ --collect-only}\n- If errors found, ask agent to fix\n\nPhase 6: Execution (Optional)\nGoal: Run tests if --run flag provided\n\nActions:\n- If --run flag:\n  - Install dependencies: !{bash pip install -r requirements.txt}\n  - Run tests with appropriate flags\n  - If --coverage: !{bash pytest tests/ -v --cov=. --cov-report=html --cov-report=term}\n  - Else: !{bash pytest tests/ -v}\n  - Display results summary\n\nPhase 7: Summary\nGoal: Report what was accomplished\n\nActions:\n- List generated files\n- Show test coverage (if applicable)\n- Display test results (if --run)\n- Provide next steps:\n  - How to run tests manually\n  - How to add new tests\n  - Link to TESTING.md documentation\n"
              }
            ],
            "skills": [
              {
                "name": "fastmcp-cloud-deployment",
                "description": "FastMCP Cloud deployment validation, testing, and verification patterns. Use when deploying MCP servers, validating deployments, testing server configurations, checking environment variables, verifying deployment health, tracking deployments, or when user mentions FastMCP Cloud, deployment validation, pre-deployment checks, post-deployment verification, deployment troubleshooting, or deployment lifecycle management.",
                "path": "plugins/fastmcp/skills/fastmcp-cloud-deployment/SKILL.md",
                "frontmatter": {
                  "name": "fastmcp-cloud-deployment",
                  "description": "FastMCP Cloud deployment validation, testing, and verification patterns. Use when deploying MCP servers, validating deployments, testing server configurations, checking environment variables, verifying deployment health, tracking deployments, or when user mentions FastMCP Cloud, deployment validation, pre-deployment checks, post-deployment verification, deployment troubleshooting, or deployment lifecycle management.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# FastMCP Cloud Deployment Skill\n\nThis skill provides comprehensive deployment lifecycle management for FastMCP servers, including pre-deployment validation, local testing, post-deployment verification, environment variable checking, and deployment tracking.\n\n## Overview\n\nThe deployment lifecycle consists of five phases:\n1. **Pre-Deployment Validation** - Syntax, dependencies, configuration\n2. **Local Testing** - STDIO and HTTP transport testing\n3. **Environment Verification** - Environment variable validation\n4. **Deployment** - To FastMCP Cloud, HTTP, or STDIO\n5. **Post-Deployment Verification** - Health checks, endpoint testing\n\n## Available Scripts\n\n### 1. Pre-Deployment Validation\n\n**Script**: `scripts/validate-server.sh <server-path>`\n\n**Purpose**: Validates server is ready for deployment\n\n**Checks**:\n- Server file exists (server.py, server.ts, index.ts)\n- Syntax validation (Python/TypeScript)\n- Dependencies declared (requirements.txt or package.json)\n- FastMCP dependency included\n- fastmcp.json configuration valid\n- No hardcoded secrets\n- Environment configuration present\n- .gitignore properly configured\n\n**Usage**:\n```bash\n# Validate current directory\n./scripts/validate-server.sh .\n\n# Validate specific server\n./scripts/validate-server.sh /path/to/server\n\n# Verbose mode\nVERBOSE=1 ./scripts/validate-server.sh .\n```\n\n**Exit Codes**:\n- `0`: Validation passed (may have warnings)\n- `1`: Validation failed (must fix before deployment)\n\n**Example Output**:\n```\n=== FastMCP Server Pre-Deployment Validation ===\n‚úì Found Python server file: server.py\n‚úì Python syntax is valid\n‚úì FastMCP dependency declared in requirements.txt\n‚úì fastmcp.json has valid JSON syntax\n‚úì Server name: my-server\n‚ö† Found 2 unpinned dependencies\n‚úì No obvious hardcoded secrets detected\n\nResults: 12 passed, 0 failed, 1 warnings\n‚úì Server passed validation - ready for deployment\n```\n\n### 2. Local Testing\n\n**Script**: `scripts/test-local.sh <server-path>`\n\n**Purpose**: Tests server locally before deployment\n\n**Tests**:\n- Module imports successful\n- STDIO transport working\n- HTTP transport responding\n- Environment variables configured\n- Health endpoints (if configured)\n- Server stability (runs for 5+ seconds)\n\n**Usage**:\n```bash\n# Test with default transport (STDIO)\n./scripts/test-local.sh .\n\n# Test both STDIO and HTTP\nTRANSPORT=both ./scripts/test-local.sh .\n\n# Test HTTP only on custom port\nTRANSPORT=http PORT=3000 ./scripts/test-local.sh .\n\n# Longer test duration\nTEST_DURATION=30 ./scripts/test-local.sh .\n```\n\n**Environment Variables**:\n- `TRANSPORT`: `stdio`, `http`, or `both` (default: `stdio`)\n- `PORT`: Port for HTTP testing (default: `8000`)\n- `TEST_DURATION`: Test duration in seconds (default: `10`)\n\n**Example Output**:\n```\n=== FastMCP Server Local Testing ===\n‚úì Python imports successful\n‚úì Server started successfully (PID: 12345)\n‚úì Server is producing MCP protocol messages\n‚úì HTTP server started (PID: 12346)\n‚úì Health endpoint responding\n\nResults: 12 passed, 0 failed, 0 warnings\n‚úì Server tests passed - ready for deployment testing\n```\n\n### 3. Environment Variable Check\n\n**Script**: `scripts/check-env-vars.sh <server-path>`\n\n**Purpose**: Validates environment variables are properly configured\n\n**Checks**:\n- .env.example exists and documents variables\n- Required vs optional variables identified\n- Local .env file has all required variables\n- fastmcp.json env declarations match .env.example\n- .env in .gitignore (security)\n- No placeholder values in production\n\n**Usage**:\n```bash\n# Check default .env file\n./scripts/check-env-vars.sh .\n\n# Check specific env file\nENV_FILE=.env.production ./scripts/check-env-vars.sh .\n\n# Check only required variables\nCHECK_MODE=required ./scripts/check-env-vars.sh .\n```\n\n**Environment Variables**:\n- `ENV_FILE`: Environment file to check (default: `.env`)\n- `CHECK_MODE`: `all`, `required`, or `optional` (default: `all`)\n\n**Example Output**:\n```\n=== FastMCP Server Environment Variable Check ===\n‚úì Found .env.example template\n‚Ñπ Required: API_KEY\n‚Ñπ Optional: LOG_LEVEL (default: INFO)\n‚úì All required variables are set\n‚úì All fastmcp.json variables documented in .env.example\n‚úì .env files properly excluded from git\n\nResults: 6 passed, 0 failed, 0 warnings\n‚úì Environment configuration validated\n```\n\n### 4. Post-Deployment Verification\n\n**Script**: `scripts/verify-deployment.sh <deployment-url>`\n\n**Purpose**: Verifies deployed server is accessible and functioning\n\n**Checks**:\n- DNS resolution working\n- Server is reachable\n- Health endpoint responding\n- MCP endpoint accepting requests\n- Valid JSON-RPC responses\n- Tools are available\n- SSL/TLS certificate valid (HTTPS)\n- Response time acceptable\n\n**Usage**:\n```bash\n# Verify FastMCP Cloud deployment\n./scripts/verify-deployment.sh https://my-server.fastmcp.app/mcp\n\n# Verify HTTP deployment\n./scripts/verify-deployment.sh https://my-server.example.com/mcp\n\n# Custom timeout and retries\nMAX_RETRIES=10 TIMEOUT=60 ./scripts/verify-deployment.sh https://my-server.com/mcp\n\n# Verbose output\nVERBOSE=1 ./scripts/verify-deployment.sh https://my-server.com/mcp\n```\n\n**Environment Variables**:\n- `MAX_RETRIES`: Maximum retry attempts (default: `5`)\n- `RETRY_DELAY`: Seconds between retries (default: `10`)\n- `TIMEOUT`: Request timeout in seconds (default: `30`)\n- `VERBOSE`: Show detailed output (default: `0`)\n\n**Example Output**:\n```\n=== FastMCP Server Deployment Verification ===\n‚úì DNS resolved: my-server.fastmcp.app -> 104.21.45.123\n‚úì Server is reachable\n‚úì Health endpoint available at /health (HTTP 200)\n‚úì MCP endpoint responding (HTTP 200)\n‚úì Valid JSON-RPC response received\n‚úì Server provides 3 tool(s)\n‚úì Valid SSL/TLS certificate\n‚úì Response time excellent (<1s)\n\nResults: 11 passed, 0 failed, 0 warnings\n‚úì Deployment verified successfully\n```\n\n## Templates\n\n### Deployment Tracking Template\n\n**File**: `templates/.fastmcp-deployments.json`\n\n**Purpose**: Track all server deployments with metadata\n\n**Structure**:\n```json\n{\n  \"version\": \"1.0.0\",\n  \"deployments\": [\n    {\n      \"id\": \"deployment-uuid\",\n      \"serverName\": \"my-server\",\n      \"environment\": \"production\",\n      \"target\": \"fastmcp-cloud\",\n      \"url\": \"https://my-server.fastmcp.app/mcp\",\n      \"status\": \"active\",\n      \"deployedAt\": \"2025-01-15T10:30:00Z\",\n      \"version\": \"1.0.0\",\n      \"validationResults\": {\n        \"preDeployment\": {...},\n        \"postDeployment\": {...}\n      }\n    }\n  ]\n}\n```\n\n**Usage**: Copy template and update with actual deployment details\n\n### Deployment Checklist\n\n**File**: `templates/deployment-checklist.md`\n\n**Purpose**: Step-by-step checklist for successful deployments\n\n**Sections**:\n- Pre-Deployment Checklist (code quality, dependencies, config)\n- Deployment Checklist (by target: FastMCP Cloud, HTTP, STDIO)\n- Post-Deployment Verification (accessibility, functionality, performance)\n- Deployment Tracking (record keeping)\n- Rollback Plan (if issues occur)\n\n**Usage**: Copy checklist for each deployment, check off items as completed\n\n### Environment Variables Documentation\n\n**File**: `templates/env-var-template.md`\n\n**Purpose**: Template for documenting all environment variables\n\n**Sections**:\n- Required Variables (must be set)\n- Optional Variables (have defaults)\n- Development-Only Variables\n- Environment-Specific Configurations\n- Security Best Practices\n- Troubleshooting\n\n**Usage**: Copy template to server repo as `ENV_VARS.md`, customize with actual variables\n\n## Examples\n\n### Successful Deployment Workflow\n\n**File**: `examples/successful-deployment.md`\n\n**Contents**: Complete end-to-end deployment example including:\n- Pre-deployment validation output\n- Environment variable checking\n- Local testing results\n- Deployment process\n- Post-deployment verification\n- Deployment tracking record\n- Post-deployment monitoring\n- Lessons learned\n\n**Use Case**: Reference for first-time deployments or training\n\n### Troubleshooting Guide\n\n**File**: `examples/troubleshooting.md`\n\n**Contents**: Common issues and solutions including:\n- Pre-deployment validation failures\n- Environment variable issues\n- Local testing problems\n- FastMCP Cloud deployment errors\n- HTTP deployment issues\n- SSL/TLS certificate problems\n- Post-deployment verification failures\n- Performance issues\n- Runtime crashes\n- Debugging tools and techniques\n\n**Use Case**: Reference when deployments fail or issues occur\n\n## Deployment Workflow\n\n### Standard Deployment Process\n\n**Step 1: Pre-Deployment Validation**\n```bash\ncd /path/to/server\n./scripts/validate-server.sh .\n```\n\n**Expected**: All checks pass, address any warnings\n\n**Step 2: Environment Variable Check**\n```bash\n./scripts/check-env-vars.sh .\n```\n\n**Expected**: All required variables set, no security issues\n\n**Step 3: Local Testing**\n```bash\nTRANSPORT=both ./scripts/test-local.sh .\n```\n\n**Expected**: Server runs successfully in both STDIO and HTTP modes\n\n**Step 4: Deploy**\n\nFor **FastMCP Cloud**:\n1. Create separate GitHub repository\n2. Push code\n3. Connect to FastMCP Cloud\n4. Set environment variables in dashboard\n5. Trigger deployment\n\nFor **HTTP** (your infrastructure):\n1. Deploy to server (VPS, container, etc.)\n2. Configure environment variables\n3. Set up reverse proxy (nginx/caddy)\n4. Configure SSL/TLS\n5. Start server\n\nFor **STDIO** (local/IDE):\n1. Update `.mcp.json` or IDE config\n2. Ensure `.env` file has required variables\n3. Restart IDE\n\n**Step 5: Post-Deployment Verification**\n```bash\n./scripts/verify-deployment.sh https://your-deployment-url/mcp\n```\n\n**Expected**: All checks pass, server responding correctly\n\n**Step 6: Track Deployment**\n\nUpdate `.fastmcp-deployments.json` with:\n- Deployment timestamp\n- Git commit hash\n- Version number\n- Validation results\n- Environment variables used\n\n### Emergency Rollback\n\nIf deployment fails:\n\n1. Check deployment logs for errors\n2. Run verification script to identify issues\n3. Review troubleshooting guide for solutions\n4. If critical: Rollback to previous version\n5. Fix issues locally\n6. Re-run validation scripts\n7. Redeploy\n\n## Integration with Other Skills\n\nThis skill complements:\n\n- **mcp-server-config**: Uses config templates for deployment setup\n- **newman-runner**: Can integrate API testing before deployment\n- **api-schema-analyzer**: Validates API schemas match deployment\n\n## Best Practices\n\n### Before Every Deployment\n\n1. Run all validation scripts in order\n2. Test locally in target transport mode\n3. Verify all environment variables\n4. Review deployment checklist\n5. Have rollback plan ready\n\n### Security\n\n1. Never commit `.env` files\n2. Use `.env.example` for documentation only\n3. Rotate secrets regularly\n4. Use different values per environment\n5. Store production secrets in secrets manager\n\n### Monitoring\n\n1. Check health endpoint immediately after deployment\n2. Monitor logs for first 24 hours\n3. Set up alerts for failures\n4. Track performance metrics\n5. Document any issues encountered\n\n### Documentation\n\n1. Keep `.env.example` up to date\n2. Document all environment variables\n3. Track deployments in `.fastmcp-deployments.json`\n4. Update troubleshooting guide with new issues\n5. Maintain deployment checklist\n\n## Common Use Cases\n\n### Use Case 1: First Production Deployment\n\n```bash\n# Validate server is ready\n./scripts/validate-server.sh .\n\n# Check environment variables\n./scripts/check-env-vars.sh .\n\n# Test locally\nTRANSPORT=both ./scripts/test-local.sh .\n\n# Deploy to FastMCP Cloud\n# (via dashboard or CLI)\n\n# Verify deployment\n./scripts/verify-deployment.sh https://my-server.fastmcp.app/mcp\n```\n\n### Use Case 2: Staging Environment Testing\n\n```bash\n# Use staging environment file\nENV_FILE=.env.staging ./scripts/check-env-vars.sh .\n\n# Test with staging config\ncp .env.staging .env\nTRANSPORT=http ./scripts/test-local.sh .\n\n# Deploy to staging\n# ...\n\n# Verify staging deployment\n./scripts/verify-deployment.sh https://staging.example.com/mcp\n```\n\n### Use Case 3: Multi-Environment Deployment\n\n```bash\n# Validate once\n./scripts/validate-server.sh .\n\n# Check each environment's variables\nfor env in development staging production; do\n    echo \"Checking $env...\"\n    ENV_FILE=.env.$env ./scripts/check-env-vars.sh .\ndone\n\n# Deploy to each environment\n# ...\n\n# Verify each deployment\n./scripts/verify-deployment.sh https://dev.example.com/mcp\n./scripts/verify-deployment.sh https://staging.example.com/mcp\n./scripts/verify-deployment.sh https://my-server.fastmcp.app/mcp\n```\n\n### Use Case 4: Continuous Deployment Pipeline\n\n```bash\n#!/bin/bash\n# .github/workflows/deploy.sh\n\nset -e\n\n# Validation\n./scripts/validate-server.sh . || exit 1\n\n# Environment check\nENV_FILE=.env.production ./scripts/check-env-vars.sh . || exit 1\n\n# Local testing\nTRANSPORT=both ./scripts/test-local.sh . || exit 1\n\n# Deploy (example using FastMCP Cloud CLI)\nfastmcp deploy --env production\n\n# Wait for deployment\nsleep 30\n\n# Verify\n./scripts/verify-deployment.sh https://my-server.fastmcp.app/mcp || exit 1\n\necho \"Deployment successful!\"\n```\n\n## Troubleshooting Quick Reference\n\n**Script fails with permission denied**:\n```bash\nchmod +x scripts/*.sh\n```\n\n**Python not found**:\n```bash\n# Install Python 3\nsudo apt install python3 python3-pip\n```\n\n**jq not found**:\n```bash\n# Install jq for JSON parsing\nsudo apt install jq\n```\n\n**curl not found**:\n```bash\n# Install curl\nsudo apt install curl\n```\n\n**Script hangs during testing**:\n```bash\n# Reduce test duration\nTEST_DURATION=5 ./scripts/test-local.sh .\n```\n\n**Verification fails immediately**:\n```bash\n# Increase timeout\nTIMEOUT=60 ./scripts/verify-deployment.sh <url>\n```\n\n## Success Criteria\n\nA deployment is successful when:\n\n- ‚úÖ All validation scripts pass\n- ‚úÖ Local tests complete successfully\n- ‚úÖ Environment variables properly configured\n- ‚úÖ Deployment completes without errors\n- ‚úÖ Post-deployment verification passes\n- ‚úÖ Health endpoint returns 200 OK\n- ‚úÖ MCP endpoint responds to JSON-RPC\n- ‚úÖ All tools are available\n- ‚úÖ Response time is acceptable\n- ‚úÖ No errors in first 24 hours of logs\n- ‚úÖ Deployment tracked in `.fastmcp-deployments.json`\n\n---\n\n**Skill Version**: 1.0.0\n**Last Updated**: 2025-01-15\n**Maintained By**: FastMCP Plugin Team"
              }
            ]
          }
        ]
      }
    },
    {
      "full_name": "vanman2024/planning-marketplace",
      "url": "https://github.com/vanman2024/planning-marketplace",
      "description": "Planning tools for feature specifications, architecture design, ADRs, and project roadmaps",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-06T07:17:42Z",
        "created_at": "2026-01-06T06:58:01Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 822
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1768
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/CHANGELOG.md",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "plugins/planning/README.md",
          "type": "blob",
          "size": 15093
        },
        {
          "path": "plugins/planning/_archive",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/docs/DOCUMENTATION-SYNC.md",
          "type": "blob",
          "size": 12787
        },
        {
          "path": "plugins/planning/_archive/docs/WIZARD-TESTING.md",
          "type": "blob",
          "size": 31095
        },
        {
          "path": "plugins/planning/_archive/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/hooks/hooks.json",
          "type": "blob",
          "size": 108
        },
        {
          "path": "plugins/planning/_archive/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/SKILL.md",
          "type": "blob",
          "size": 7602
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples/example-ai-rag-architecture.md",
          "type": "blob",
          "size": 13027
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples/example-fastapi-architecture.md",
          "type": "blob",
          "size": 19510
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples/example-fullstack-architecture.md",
          "type": "blob",
          "size": 11011
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples/example-microservices-architecture.md",
          "type": "blob",
          "size": 14455
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/examples/example-nextjs-architecture.md",
          "type": "blob",
          "size": 13263
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts/create-architecture.sh",
          "type": "blob",
          "size": 5951
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts/export-diagrams.sh",
          "type": "blob",
          "size": 3600
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts/generate-diagrams.sh",
          "type": "blob",
          "size": 9681
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts/update-architecture.sh",
          "type": "blob",
          "size": 7705
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/scripts/validate-mermaid.sh",
          "type": "blob",
          "size": 4739
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/api-architecture.md",
          "type": "blob",
          "size": 14795
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/architecture-overview.md",
          "type": "blob",
          "size": 9172
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/component-diagram.md",
          "type": "blob",
          "size": 11341
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/data-flow-diagram.md",
          "type": "blob",
          "size": 11585
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/deployment-diagram.md",
          "type": "blob",
          "size": 12739
        },
        {
          "path": "plugins/planning/_archive/skills/architecture-patterns/templates/security-architecture.md",
          "type": "blob",
          "size": 18179
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/SKILL.md",
          "type": "blob",
          "size": 5040
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/examples/BUILD-GUIDE-with-gaps.json",
          "type": "blob",
          "size": 1120
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/examples/BUILD-GUIDE.json",
          "type": "blob",
          "size": 6857
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/examples/BUILD-GUIDE.md",
          "type": "blob",
          "size": 1074
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/scripts/detect-tech-stack.sh",
          "type": "blob",
          "size": 1078
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/scripts/generate-manifest.py",
          "type": "blob",
          "size": 13551
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/scripts/validate-manifest.sh",
          "type": "blob",
          "size": 1013
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/templates/BUILD-GUIDE-minimal.json.template",
          "type": "blob",
          "size": 314
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/templates/BUILD-GUIDE.json.template",
          "type": "blob",
          "size": 4133
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/templates/BUILD-GUIDE.md.template",
          "type": "blob",
          "size": 2987
        },
        {
          "path": "plugins/planning/_archive/skills/build-manifest/templates/layer-template.json",
          "type": "blob",
          "size": 248
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/SKILL.md",
          "type": "blob",
          "size": 8236
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples/example-adr-architecture.md",
          "type": "blob",
          "size": 21171
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples/example-adr-index.md",
          "type": "blob",
          "size": 13855
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples/example-adr-security.md",
          "type": "blob",
          "size": 23810
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples/example-adr-superseded.md",
          "type": "blob",
          "size": 10513
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/examples/example-adr-technology.md",
          "type": "blob",
          "size": 15707
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts/create-adr.sh",
          "type": "blob",
          "size": 4890
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts/list-adrs.sh",
          "type": "blob",
          "size": 5441
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts/search-adrs.sh",
          "type": "blob",
          "size": 5728
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts/supersede-adr.sh",
          "type": "blob",
          "size": 9219
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/scripts/update-adr-index.sh",
          "type": "blob",
          "size": 6720
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates/adr-frontmatter.yaml",
          "type": "blob",
          "size": 4397
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates/adr-index-template.md",
          "type": "blob",
          "size": 10755
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates/adr-template.md",
          "type": "blob",
          "size": 7550
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates/consequences-template.md",
          "type": "blob",
          "size": 13770
        },
        {
          "path": "plugins/planning/_archive/skills/decision-tracking/templates/decision-matrix.md",
          "type": "blob",
          "size": 9870
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/AGENT-INTEGRATION.md",
          "type": "blob",
          "size": 8504
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/SKILL.md",
          "type": "blob",
          "size": 6846
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/requirements.txt",
          "type": "blob",
          "size": 273
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/bulk-register-worktrees.py",
          "type": "blob",
          "size": 12789
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/full-registry.py",
          "type": "blob",
          "size": 16531
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/query-docs.py",
          "type": "blob",
          "size": 3636
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/register-worktree.py",
          "type": "blob",
          "size": 20336
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/serve-viewer.py",
          "type": "blob",
          "size": 8809
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/sync-to-mem0.py",
          "type": "blob",
          "size": 13538
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/update-relationships.py",
          "type": "blob",
          "size": 11300
        },
        {
          "path": "plugins/planning/_archive/skills/doc-sync/scripts/view-docs.sh",
          "type": "blob",
          "size": 1275
        },
        {
          "path": "plugins/planning/_archive/skills/feature-workflow-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/feature-workflow-generation/SKILL.md",
          "type": "blob",
          "size": 7693
        },
        {
          "path": "plugins/planning/_archive/skills/feature-workflow-generation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/_archive/skills/feature-workflow-generation/scripts/generate-feature-workflow.py",
          "type": "blob",
          "size": 7506
        },
        {
          "path": "plugins/planning/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/agents/architecture-designer.md",
          "type": "blob",
          "size": 11685
        },
        {
          "path": "plugins/planning/agents/build-manifest-generator.md",
          "type": "blob",
          "size": 6515
        },
        {
          "path": "plugins/planning/agents/cost-validator.md",
          "type": "blob",
          "size": 11031
        },
        {
          "path": "plugins/planning/agents/cto-reviewer.md",
          "type": "blob",
          "size": 9117
        },
        {
          "path": "plugins/planning/agents/decision-documenter.md",
          "type": "blob",
          "size": 10410
        },
        {
          "path": "plugins/planning/agents/doc-analyzer.md",
          "type": "blob",
          "size": 10476
        },
        {
          "path": "plugins/planning/agents/doc-consolidator.md",
          "type": "blob",
          "size": 10793
        },
        {
          "path": "plugins/planning/agents/doc-executor.md",
          "type": "blob",
          "size": 11261
        },
        {
          "path": "plugins/planning/agents/doc-reviewer.md",
          "type": "blob",
          "size": 11111
        },
        {
          "path": "plugins/planning/agents/feature-analyzer.md",
          "type": "blob",
          "size": 20517
        },
        {
          "path": "plugins/planning/agents/feature-spec-writer.md",
          "type": "blob",
          "size": 16057
        },
        {
          "path": "plugins/planning/agents/requirements-processor.md",
          "type": "blob",
          "size": 7674
        },
        {
          "path": "plugins/planning/agents/roadmap-planner.md",
          "type": "blob",
          "size": 9594
        },
        {
          "path": "plugins/planning/agents/spec-analyzer.md",
          "type": "blob",
          "size": 9399
        },
        {
          "path": "plugins/planning/agents/sync-validator.md",
          "type": "blob",
          "size": 3033
        },
        {
          "path": "plugins/planning/agents/technical-validator.md",
          "type": "blob",
          "size": 10156
        },
        {
          "path": "plugins/planning/agents/timeline-validator.md",
          "type": "blob",
          "size": 9699
        },
        {
          "path": "plugins/planning/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/commands/add-feature.md",
          "type": "blob",
          "size": 4954
        },
        {
          "path": "plugins/planning/commands/add-spec.md",
          "type": "blob",
          "size": 2991
        },
        {
          "path": "plugins/planning/commands/analyze-project.md",
          "type": "blob",
          "size": 7063
        },
        {
          "path": "plugins/planning/commands/architecture.md",
          "type": "blob",
          "size": 8490
        },
        {
          "path": "plugins/planning/commands/clarify.md",
          "type": "blob",
          "size": 6239
        },
        {
          "path": "plugins/planning/commands/consolidate-docs.md",
          "type": "blob",
          "size": 3933
        },
        {
          "path": "plugins/planning/commands/decide.md",
          "type": "blob",
          "size": 7338
        },
        {
          "path": "plugins/planning/commands/extract-config.md",
          "type": "blob",
          "size": 15839
        },
        {
          "path": "plugins/planning/commands/generate-feature-workflow.md",
          "type": "blob",
          "size": 8438
        },
        {
          "path": "plugins/planning/commands/init-project.md",
          "type": "blob",
          "size": 14775
        },
        {
          "path": "plugins/planning/commands/init-website.md",
          "type": "blob",
          "size": 5164
        },
        {
          "path": "plugins/planning/commands/notes.md",
          "type": "blob",
          "size": 4379
        },
        {
          "path": "plugins/planning/commands/roadmap.md",
          "type": "blob",
          "size": 6324
        },
        {
          "path": "plugins/planning/commands/spec.md",
          "type": "blob",
          "size": 7327
        },
        {
          "path": "plugins/planning/commands/sync-all.md",
          "type": "blob",
          "size": 8130
        },
        {
          "path": "plugins/planning/commands/update-feature.md",
          "type": "blob",
          "size": 5904
        },
        {
          "path": "plugins/planning/commands/view-docs.md",
          "type": "blob",
          "size": 2981
        },
        {
          "path": "plugins/planning/commands/wizard.md",
          "type": "blob",
          "size": 8684
        },
        {
          "path": "plugins/planning/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management/README.md",
          "type": "blob",
          "size": 4297
        },
        {
          "path": "plugins/planning/skills/spec-management/SKILL.md",
          "type": "blob",
          "size": 8151
        },
        {
          "path": "plugins/planning/skills/spec-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management/examples/example-spec-ai-feature.md",
          "type": "blob",
          "size": 18642
        },
        {
          "path": "plugins/planning/skills/spec-management/examples/example-spec-complex.md",
          "type": "blob",
          "size": 25338
        },
        {
          "path": "plugins/planning/skills/spec-management/examples/example-spec-list.md",
          "type": "blob",
          "size": 15887
        },
        {
          "path": "plugins/planning/skills/spec-management/examples/example-spec-simple.md",
          "type": "blob",
          "size": 5539
        },
        {
          "path": "plugins/planning/skills/spec-management/examples/example-validation-report.md",
          "type": "blob",
          "size": 11524
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/consolidate-specs.sh",
          "type": "blob",
          "size": 2342
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/create-spec-structure.sh",
          "type": "blob",
          "size": 1972
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/create-spec.sh",
          "type": "blob",
          "size": 5894
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/generate-json-output.sh",
          "type": "blob",
          "size": 1390
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/list-specs.sh",
          "type": "blob",
          "size": 6602
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/search-specs.sh",
          "type": "blob",
          "size": 8394
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/update-status.sh",
          "type": "blob",
          "size": 6702
        },
        {
          "path": "plugins/planning/skills/spec-management/scripts/validate-spec.sh",
          "type": "blob",
          "size": 9237
        },
        {
          "path": "plugins/planning/skills/spec-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/feature-spec-minimal.md",
          "type": "blob",
          "size": 3110
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/feature-tasks-minimal.md",
          "type": "blob",
          "size": 6304
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/plan-template.md",
          "type": "blob",
          "size": 6742
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/project-overview-template.md",
          "type": "blob",
          "size": 7439
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/requirements-template.md",
          "type": "blob",
          "size": 10869
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/spec-metadata.yaml",
          "type": "blob",
          "size": 2638
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/spec-simple-template.md",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/spec-template.md",
          "type": "blob",
          "size": 5733
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/success-criteria-template.md",
          "type": "blob",
          "size": 12301
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/task-breakdown-template.md",
          "type": "blob",
          "size": 6514
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/archive/tasks-template.md",
          "type": "blob",
          "size": 5927
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/features-json-schema.json",
          "type": "blob",
          "size": 2894
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/infrastructure-json-schema.json",
          "type": "blob",
          "size": 3934
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/infrastructure-template.md",
          "type": "blob",
          "size": 2037
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/spec-template.md",
          "type": "blob",
          "size": 2101
        },
        {
          "path": "plugins/planning/skills/spec-management/templates/tasks-template.md",
          "type": "blob",
          "size": 6056
        }
      ],
      "marketplace": {
        "name": "planning-marketplace",
        "version": "1.0.0",
        "description": "Planning tools for feature specifications, architecture design, ADRs, and project roadmaps",
        "owner_info": {
          "name": "vanman2024",
          "email": "noreply@planning-marketplace.dev"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "planning",
            "description": "Feature specification, architecture design, decision documentation, and roadmap planning with multi-agent coordination and template-driven workflows",
            "source": "./plugins/planning",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "vanman2024"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/planning-marketplace",
              "/plugin install planning@planning-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2026-01-06T07:17:42Z",
              "created_at": "2026-01-06T06:58:01Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-feature",
                "description": "Add complete feature with roadmap, spec, ADR, and architecture updates",
                "path": "plugins/planning/commands/add-feature.md",
                "frontmatter": {
                  "description": "Add complete feature with roadmap, spec, ADR, and architecture updates",
                  "argument-hint": "<feature-description> OR --doc=<path/to/document.md>",
                  "allowed-tools": "Read, Bash, Task, TodoWrite, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add a new feature with complete planning documentation. Delegates to feature-analyzer agent for heavy lifting.\n\nPhase 1: Parse Input\nGoal: Determine input mode and basic context\n\nActions:\n- Create todo: \"Add feature to project\"\n- Parse $ARGUMENTS:\n  * If contains \"--doc=\": MODE = \"document\", extract DOC_PATH\n  * Otherwise: MODE = \"text\", DESCRIPTION = $ARGUMENTS\n- If MODE = \"document\":\n  * Validate file exists: !{bash test -f \"$DOC_PATH\" && echo \"exists\" || echo \"missing\"}\n  * If missing: Error and exit\n- Display: \"Mode: [MODE]\"\n\nPhase 2: Launch Feature Analyzer\nGoal: Analyze context and determine what to create\n\nActions:\n- Launch feature-analyzer agent:\n\n```\nTask(\n  description=\"Analyze feature and context\",\n  subagent_type=\"planning:feature-analyzer\",\n  prompt=\"Analyze this feature request and project context.\n\n  Input Mode: [MODE]\n  Description: $ARGUMENTS\n  Document Path: [DOC_PATH if applicable]\n\n  Read schema templates:\n  - @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/foundation/skills/project-detection/templates/project-json-schema.json\n  - @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/spec-management/templates/features-json-schema.json\n\n  Analyze:\n  1. Read roadmap/project.json for tech stack and infrastructure\n  2. Read roadmap/features.json for existing features\n  3. Check for similar existing specs (>70% similarity ‚Üí redirect to update-feature)\n  4. Identify infrastructure dependencies (I0XX IDs)\n  5. Calculate infrastructure_phase from dependencies (numeric 0-5)\n  6. Determine phase milestone (MVP/Beta Launch/Post-MVP based on priority)\n  7. Determine if ADR needed (new tech/architecture)\n  8. Determine priority (P0/P1/P2)\n\n  Phase mapping:\n  - P0 (Critical) ‚Üí 'MVP'\n  - P1 (High) ‚Üí 'Beta Launch'\n  - P2 (Medium) ‚Üí 'Post-MVP'\n\n  Return JSON:\n  {\n    'next_number': 'F0XX',\n    'name': 'feature-name',\n    'phase': 'MVP/Beta Launch/Post-MVP',\n    'infrastructure_phase': N,\n    'priority': 'P0/P1/P2',\n    'infrastructure_dependencies': ['I001', 'I010'],\n    'feature_dependencies': ['F001'],\n    'needs_adr': true/false,\n    'needs_architecture_update': true/false,\n    'similar_spec': null or 'F0XX',\n    'description': 'extracted description'\n  }\"\n)\n```\n\n- Parse agent response\n- If similar_spec found:\n  * Display: \"Found similar spec [similar_spec]. Redirecting to update-feature.\"\n  * Exit - user should run /planning:update-feature instead\n\nPhase 3: Update roadmap/features.json\nGoal: Add feature entry before generating docs\n\nActions:\n- Read roadmap/features.json (or create if missing)\n- Add feature entry with:\n  * id, name, description, status: \"planned\"\n  * priority (P0/P1/P2)\n  * phase (MVP/Beta Launch/Post-MVP - string milestone)\n  * infrastructure_phase (0-5 - numeric build order)\n  * infrastructure_dependencies, dependencies\n  * created date\n- Update phases summary array (by infrastructure_phase)\n- Write roadmap/features.json\n- Display: \"‚úÖ Added F[NUMBER] to roadmap/features.json ([PHASE] milestone, infrastructure phase [INFRASTRUCTURE_PHASE])\"\n\nPhase 4: Generate Documentation in Parallel\nGoal: Create all docs simultaneously\n\nActions:\n- Launch ALL applicable agents in ONE message:\n\n```\nTask(\n  description=\"Generate feature spec\",\n  subagent_type=\"planning:feature-spec-writer\",\n  prompt=\"Create spec for F[NUMBER]: [DESCRIPTION].\n  Phase milestone: [PHASE]. Infrastructure phase: [INFRASTRUCTURE_PHASE]. Priority: [PRIORITY].\n  Infrastructure deps: [IDS]. Feature deps: [IDS].\n  Create: specs/phase-[INFRASTRUCTURE_PHASE]/F[NUMBER]-[slug]/spec.md and tasks.md\"\n)\n\nTask(\n  description=\"Update roadmap\",\n  subagent_type=\"planning:roadmap-planner\",\n  prompt=\"Add F[NUMBER] to ROADMAP.md: [DESCRIPTION].\n  Priority: [PRIORITY]. Phase: [PHASE] (milestone). Dependencies: [list].\"\n)\n```\n\n- IF needs_adr:\n```\nTask(\n  description=\"Create ADR\",\n  subagent_type=\"planning:decision-documenter\",\n  prompt=\"Create ADR for F[NUMBER]: [DESCRIPTION].\n  Document decision, alternatives, consequences.\"\n)\n```\n\n- IF needs_architecture_update:\n```\nTask(\n  description=\"Update architecture\",\n  subagent_type=\"planning:architecture-designer\",\n  prompt=\"Update docs/architecture/ for F[NUMBER]: [DESCRIPTION].\"\n)\n```\n\nPhase 5: Summary\nGoal: Report results and next steps\n\nActions:\n- Mark todo complete\n- Display: \"‚úÖ Created:\"\n  * Spec: specs/phase-[INFRASTRUCTURE_PHASE]/F[NUMBER]-[slug]/\n  * Milestone: [PHASE]\n  * Roadmap: Updated\n  * ADR: (if created)\n  * Architecture: (if updated)\n- Next steps:\n  * Review spec in specs/phase-[INFRASTRUCTURE_PHASE]/F[NUMBER]-[slug]/\n  * Run /implementation:execute F[NUMBER] to build the feature\n  * Or run /implementation:execute to auto-continue"
              },
              {
                "name": "/add-spec",
                "description": "[DEPRECATED] Use /planning:add-feature instead - adds spec with similarity checking and complete planning sync",
                "path": "plugins/planning/commands/add-spec.md",
                "frontmatter": {
                  "description": "[DEPRECATED] Use /planning:add-feature instead - adds spec with similarity checking and complete planning sync",
                  "argument-hint": "<feature-description>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\n‚ö†Ô∏è **DEPRECATED COMMAND**\n\nThis command has been deprecated in favor of `/planning:add-feature`.\n\n## Why This Changed\n\n**The Problem:**\n- `/planning:add-spec` created specs without checking for duplicates\n- No similarity detection ‚Üí overlapping/duplicate specs\n- No roadmap sync ‚Üí planning docs out of sync\n- No ADR creation ‚Üí architecture decisions not tracked\n\n**The Solution:**\nUse `/planning:add-feature` instead, which:\n- ‚úÖ Similarity checking (prevents duplicates)\n- ‚úÖ Updates ROADMAP.md automatically\n- ‚úÖ Creates ADRs for architecture decisions\n- ‚úÖ Updates architecture docs\n- ‚úÖ Keeps all planning in sync\n\n## Migration\n\n**Old way (DEPRECATED):**\n```bash\n/planning:add-spec \"email notifications\"\n‚Üí Blindly creates spec 021\n‚Üí Might duplicate existing notification spec\n```\n\n**New way (RECOMMENDED):**\n```bash\n/planning:add-feature \"email notifications\"\n‚Üí Checks similarity with existing specs\n‚Üí Finds spec 003 \"notification system\" (87% match)\n‚Üí Asks: New feature or enhancement?\n‚Üí Routes correctly, no duplicates\n```\n\n## Automatic Redirect\n\nThis command will automatically redirect you to `/planning:add-feature`.\n\nPhase 1: Deprecation Warning\nGoal: Inform user and redirect\n\nActions:\n- Display deprecation warning\n- Explain why `/planning:add-feature` is better\n- Ask user confirmation to proceed with redirect\n\nPhase 2: Redirect\nGoal: Route to correct command\n\nActions:\n- Display: \"Redirecting to /planning:add-feature with your description...\"\n- Display: \"Please run: /planning:add-feature $ARGUMENTS\"\n- Explain what the new command will do:\n  - Check for similar existing specs\n  - Ask priority, phase, dependencies\n  - Update roadmap automatically\n  - Create ADRs if needed\n  - Keep all docs in sync\n- Exit (user should run /planning:add-feature)\n\n## For Documentation\n\n**Command Status:** DEPRECATED as of 2025-11-05\n**Replacement:** `/planning:add-feature`\n**Reason:** Similarity checking required to prevent duplicate specs\n**Breaking Change:** No - command still exists but redirects"
              },
              {
                "name": "/analyze-project",
                "description": "Analyze existing project specs for completeness and identify gaps",
                "path": "plugins/planning/commands/analyze-project.md",
                "frontmatter": {
                  "description": "Analyze existing project specs for completeness and identify gaps",
                  "argument-hint": "(optional)"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Comprehensively analyze all existing project specification files to identify gaps, incomplete sections, missing features, and provide actionable recommendations for improvement.\n\nCore Principles:\n- Discover all specs systematically\n- Analyze each spec independently in parallel\n- Consolidate findings into actionable gaps\n- Provide clear recommendations\n\nPhase 1: Discovery\nGoal: Identify all existing specification files\n\nActions:\n- Create todo list for tracking analysis progress\n- Search for all specification files following naming convention\n- Count total specs found\n- Validate specs directory exists\n\n!{bash if [ -d \"specs\" ]; then echo \"Specs directory found\"; else echo \"ERROR: specs directory not found\"; exit 1; fi}\n\n!{bash find specs -type f -name '[0-9][0-9][0-9]-*.md' 2>/dev/null | sort}\n\nParse discovered specs and prepare for parallel analysis.\n\nIf no specs found, report that project has no specs to analyze.\n\nPhase 2: Parallel Analysis\nGoal: Launch spec-analyzer agent for each discovered spec file\n\nActions:\n\nFor EACH spec file discovered in Phase 1, launch a spec-analyzer agent in PARALLEL.\n\nTask(description=\"Analyze spec completeness\", subagent_type=\"planning:spec-analyzer\", prompt=\"You are the spec-analyzer agent. Analyze the specification file for completeness and quality.\n\nTarget: $ARGUMENTS\n\nYour analysis should evaluate:\n- Completeness: Are all sections filled out? Any TODO or placeholder text?\n- Clarity: Is the spec clear and unambiguous?\n- Technical detail: Sufficient implementation guidance?\n- Requirements coverage: Are acceptance criteria defined?\n- Dependencies: Are dependencies on other specs documented?\n- Testability: Can this spec be validated/tested?\n\nDeliverable: Return JSON analysis:\n{\n  \\\"spec_file\\\": \\\"filename\\\",\n  \\\"completeness_score\\\": 0-100,\n  \\\"missing_sections\\\": [list],\n  \\\"incomplete_sections\\\": [list with details],\n  \\\"clarity_issues\\\": [list],\n  \\\"technical_gaps\\\": [list],\n  \\\"recommendations\\\": [list]\n}\")\n\nWait for ALL spec-analyzer agents to complete before proceeding.\n\nUpdate todos as each analysis completes.\n\nPhase 3: Consolidation\nGoal: Aggregate all analysis results and identify patterns\n\nActions:\n- Collect all JSON results from spec-analyzer agents\n- Calculate aggregate metrics:\n  - Average completeness score across all specs\n  - Total missing sections\n  - Total incomplete sections\n  - Common clarity issues\n  - Common technical gaps\n- Identify specs requiring immediate attention (score < 60)\n- Identify specs that are well-documented (score >= 80)\n- Cross-reference dependencies between specs\n- Prioritize gaps by impact\n\nPhase 4: Gap Analysis Report\nGoal: Generate comprehensive gap analysis document\n\nActions:\n- Create gaps-analysis.json with structure:\n  {\n    \"analysis_date\": \"YYYY-MM-DD\",\n    \"total_specs\": N,\n    \"avg_completeness\": X,\n    \"critical_gaps\": [],\n    \"incomplete_specs\": [],\n    \"well_documented_specs\": [],\n    \"missing_features\": [],\n    \"recommendations\": []\n  }\n- Write report to project root or specs directory\n- Include severity levels: CRITICAL, HIGH, MEDIUM, LOW\n\n!{bash echo \"Gap analysis saved to gaps-analysis.json\"}\n\nPhase 5: Summary and Recommendations\nGoal: Present actionable findings to user\n\nActions:\n- Mark all todos complete\n- Display comprehensive summary:\n\n**Analysis Complete**\n\nTotal Specs Analyzed: [N]\nAverage Completeness: [X%]\n\n**Critical Issues** (requires immediate attention):\n- [List specs with score < 60]\n- [Key missing sections]\n\n**Incomplete Specs** (needs work):\n- [List specs with score 60-79]\n\n**Well-Documented Specs** (reference examples):\n- [List specs with score >= 80]\n\n**Top Missing Features/Gaps**:\n1. [Gap 1 with affected specs]\n2. [Gap 2 with affected specs]\n3. [Gap 3 with affected specs]\n\n**Recommendations**:\n1. [Priority 1 action]\n2. [Priority 2 action]\n3. [Priority 3 action]\n\n**Next Steps**:\n- Review gaps-analysis.json for detailed breakdown\n- Prioritize specs needing completion\n- Consider creating new specs for missing features\n- Update incomplete sections following well-documented examples"
              },
              {
                "name": "/architecture",
                "description": "Design and document system architecture",
                "path": "plugins/planning/commands/architecture.md",
                "frontmatter": {
                  "description": "Design and document system architecture",
                  "argument-hint": "<action> [architecture-name] [--sync-specs]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Design and document system architecture including component diagrams, data flows, infrastructure, and technical decisions\n\nCore Principles:\n- Framework-agnostic - works with any detected tech stack\n- Comprehensive - covers all architectural aspects\n- Visual - includes diagrams and flow charts\n- Adaptable - aligns with detected stack from roadmap/project.json\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Discovery\n\nGoal: Understand the architecture request and current project state\n\nActions:\n- Parse $ARGUMENTS for:\n  - Action (design, update, diagram, review)\n  - Architecture name (optional)\n  - Flags: --sync-specs (auto-update affected specs after architecture changes)\n- Load detected tech stack: @roadmap/project.json\n- Check for existing architecture documentation\n- Example: !{bash find docs -name \"architecture*.md\" 2>/dev/null}\n- Identify architecture scope (frontend, backend, database, infrastructure, all)\n\n## Phase 2: Analysis\n\nGoal: Analyze project structure and determine architectural needs\n\nActions:\n- Check for wizard requirements (if /planning:wizard was run):\n  - Load: @docs/requirements/*/01-initial-request.md\n  - Load: @docs/requirements/*/.wizard/extracted-requirements.json\n  - Load: @docs/requirements/*/02-wizard-qa.md\n  - These contain: project description, multimodal inputs, structured Q&A\n- Review project structure and components\n- Example: !{bash find . -type d -name \"src\" -o -name \"app\" -o -name \"api\" | head -10}\n- Identify key architectural areas:\n  - Frontend architecture (if detected)\n  - Backend/API architecture (if detected)\n  - Database schema and relationships\n  - Infrastructure and deployment\n  - Integration points\n- Load any existing specs for context\n- Example: @specs/*/README.md\n\n## Phase 3: Planning\n\nGoal: Outline architectural approach\n\nActions:\n- If action unclear, use AskUserQuestion to ask:\n  - What architectural aspect to focus on?\n  - High-level or detailed design?\n  - Any specific concerns (scalability, security, performance)?\n- Determine documentation structure:\n  - System overview\n  - Component architecture\n  - Data architecture\n  - Infrastructure architecture\n  - Security architecture\n  - Integration architecture\n\n## Phase 4: Implementation\n\nGoal: Execute architecture design with agent\n\nActions:\n\nTask(description=\"Design system architecture\", subagent_type=\"planning:architecture-designer\", prompt=\"You are the architecture-designer agent. Create system architecture for $ARGUMENTS.\n\nContext:\n- Detected tech stack: roadmap/project.json\n- Wizard requirements (if available): docs/requirements/*/01-initial-request.md, docs/requirements/*/.wizard/extracted-requirements.json, docs/requirements/*/02-wizard-qa.md\n- Action: $ARGUMENTS (design, update, diagram, review)\n\nRequirements:\n  - Read wizard requirements first (if they exist) to understand project goals\n  - Create comprehensive architecture documentation including:\n    - System overview and goals\n    - Component diagrams\n    - Data flow diagrams\n    - Database schema design\n    - API architecture\n    - Infrastructure design\n    - Security architecture\n    - Deployment architecture\n    - Integration patterns\n  - Adapt to detected stack (Next.js, FastAPI, AI SDKs, etc.)\n  - Use architecture-patterns skill templates\n\nDeliverable: Complete architecture documentation with mermaid diagrams in docs/architecture/\")\n\n## Phase 5: Review\n\nGoal: Verify architecture documentation\n\nActions:\n- Check agent's output for completeness\n- Verify architecture file created/updated\n- Example: @docs/architecture/README.md\n- Ensure all key areas covered:\n  - Components ‚úì\n  - Data flows ‚úì\n  - Infrastructure ‚úì\n  - Security ‚úì\n\n## Phase 6: Documentation Sync & Impact Analysis\n\nGoal: Register architecture changes and identify affected specs\n\nActions:\n- If action was 'design' or 'update':\n  - Sync architecture to Mem0 documentation registry:\n    !{source /tmp/mem0-env/bin/activate && python plugins/planning/skills/doc-sync/scripts/sync-to-mem0.py --quiet 2>/dev/null && echo \"‚úÖ Architecture registered in documentation system\" || echo \"‚ö†Ô∏è  Doc sync unavailable (mem0 not installed)\"}\n\n  - Query which specs are affected by architecture changes:\n    !{bash if [ -f /tmp/mem0-env/bin/activate ]; then source /tmp/mem0-env/bin/activate && python plugins/planning/skills/doc-sync/scripts/query-docs.py \"What specs reference architecture documents?\" 2>/dev/null | grep -E \"Specification|references\" | head -10 || echo \"‚ö†Ô∏è  No specs found referencing architecture\"; fi}\n\n  - Display affected specs for review\n  - This identifies:\n    - Which specs reference changed architecture docs\n    - What features are impacted\n    - Where implementation plans need review\n\n  - If --sync-specs flag present:\n    - Display: \"üîÑ Auto-updating affected specs based on architecture changes\"\n    - For each affected spec, invoke:\n      SlashCommand(/planning:update-feature F00X \"Updated to align with new architecture\")\n    - Display: \"‚úÖ All affected specs updated\"\n\n- If action was 'diagram' or 'review':\n  - Skip sync (no changes made)\n\n## Phase 7: Summary\n\nGoal: Report architecture design results\n\nActions:\n- Display summary:\n  - \"Architecture documented: docs/architecture/\"\n  - List main sections created\n  - Highlight key architectural decisions\n- Show next steps:\n  - \"Review architecture with team\"\n  - \"Use /planning:decide to document key decisions as ADRs\"\n  - \"Create specs based on architectural components\"\n  - \"Use architecture to guide /iterate:tasks assignments\""
              },
              {
                "name": "/clarify",
                "description": "Gather clarification on ambiguous requirements, specs, or tasks through structured questions. Helps resolve uncertainty before implementation.",
                "path": "plugins/planning/commands/clarify.md",
                "frontmatter": {
                  "description": "Gather clarification on ambiguous requirements, specs, or tasks through structured questions. Helps resolve uncertainty before implementation.",
                  "argument-hint": [
                    "spec-name or topic"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Arguments**: $ARGUMENTS\n\nGoal: Resolve ambiguity and gather missing information through structured clarification questions\n\nCore Principles:\n- Ask before assuming - get clarity on uncertain requirements\n- Structured questions - use AskUserQuestion for clear options\n- Document answers - update specs with clarifications\n- Prevent rework - resolve uncertainty upfront\n\nPhase 1: Identify Ambiguities\nGoal: Find what needs clarification\n\nActions:\n- Parse $ARGUMENTS for spec name or topic to clarify\n- If spec provided, load it: @specs/$SPEC_NAME/spec.md\n- If no spec provided, ask what needs clarification\n- Analyze for ambiguities:\n  - Vague requirements (\"user-friendly\", \"fast\", \"secure\")\n  - Missing acceptance criteria\n  - Unclear technical decisions\n  - Multiple interpretations possible\n  - Dependencies or constraints not specified\n- List all ambiguous items found\n- Prioritize by impact on implementation\n\nPhase 2: Structure Clarification Questions\nGoal: Prepare clear, actionable questions\n\nActions:\n- For each ambiguity, formulate structured question\n- Use AskUserQuestion with specific options:\n  - Question: Clear, specific question about the ambiguity\n  - Header: Short label (max 12 chars)\n  - Options: 2-4 concrete choices with descriptions\n  - MultiSelect: true if multiple choices applicable\n- Group related questions together\n- Limit to 4 questions per batch (tool constraint)\n- Example question structure:\n  - Header: \"Auth method\"\n  - Question: \"Which authentication method should we use?\"\n  - Options:\n    - JWT: \"Stateless tokens, good for APIs\"\n    - Sessions: \"Server-side state, traditional web apps\"\n    - OAuth: \"Third-party login (Google, GitHub)\"\n\nPhase 3: Gather Clarifications\nGoal: Get answers from user through structured questions\n\nActions:\n- Use AskUserQuestion to present questions\n- Ask up to 4 questions at a time (tool limit)\n- If more than 4 ambiguities, batch them\n- Capture all responses\n- For \"Other\" responses, ask follow-up for details\n- Confirm understanding of answers\n\nPhase 4: Document Clarifications\nGoal: Update specs with resolved information\n\nActions:\n- If spec file exists, update it with clarifications:\n  - Add resolved details to appropriate sections\n  - Update acceptance criteria with specifics\n  - Add technical decisions to plan.md\n  - Mark ambiguities as resolved\n- If no spec file, create clarification summary document\n- Use Write or Edit to update files\n- Format clarifications clearly:\n  - **Question**: Original ambiguity\n  - **Answer**: User's response\n  - **Impact**: What this clarifies for implementation\n\nPhase 5: Validate Completeness\nGoal: Ensure no critical ambiguities remain\n\nActions:\n- Review updated spec or topic\n- Check for remaining uncertainties\n- List any follow-up questions needed\n- Verify acceptance criteria are now clear\n- Confirm technical approach is defined\n\nPhase 6: Summary\nGoal: Report clarification results\n\nActions:\n- Display summary:\n  - **Topic**: What was clarified\n  - **Ambiguities Resolved**: Count\n  - **Key Decisions**:\n    - List each clarification with answer\n  - **Files Updated**: Specs or docs modified\n  - **Remaining Questions**: Any unresolved items\n\n- If spec was updated:\n  - Show what changed\n  - Recommend next steps: /planning:spec validate\n\n- If creating new spec:\n  - Suggest: /planning:spec create with clarified requirements\n\n- Provide implementation guidance based on clarifications"
              },
              {
                "name": "/consolidate-docs",
                "description": "Consolidate auto-generated documentation into proper locations (specs, architecture, ADRs, contracts)",
                "path": "plugins/planning/commands/consolidate-docs.md",
                "frontmatter": {
                  "description": "Consolidate auto-generated documentation into proper locations (specs, architecture, ADRs, contracts)",
                  "argument-hint": [
                    "target-directory"
                  ],
                  "allowed-tools": "Task, Read, Bash(*), Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Discover, classify, and organize scattered auto-generated documentation files into their proper locations within the project structure (specs/, docs/architecture/, docs/adrs/, contracts/).\n\nCore Principles:\n- Scan comprehensively - find all markdown files\n- Classify accurately - determine proper location for each doc\n- Consolidate intelligently - merge duplicates, split massive files\n- Preserve carefully - ask before deleting anything important\n- Track systematically - use TodoWrite to show progress\n\nPhase 1: Discovery\nGoal: Understand the scope of documentation to consolidate\n\nActions:\n- Create todo list for consolidation workflow\n- If $ARGUMENTS provided, use as target directory\n- If no arguments, default to current project root\n- Scan for all markdown files:\n  !{bash find ${ARGUMENTS:-.} -name \"*.md\" -not -path \"*/node_modules/*\" -not -path \"*/.git/*\" -not -path \"*/vendor/*\" -type f}\n- List existing documentation directories:\n  !{bash ls -la specs/ docs/architecture/ docs/adrs/ contracts/ 2>/dev/null || echo \"No standard doc directories found\"}\n- Show count of files found and directories that exist\n- Update todos\n\nPhase 2: Analysis\nGoal: Classify documentation and identify consolidation needs\n\nActions:\n\nLaunch the doc-consolidator agent to analyze and classify all discovered documentation.\n\nProvide the agent with:\n- Target directory from $ARGUMENTS or current directory\n- List of markdown files found in Phase 1\n- Existing documentation structure\n\nThe agent will:\n- Read and classify each markdown file by content type\n- Identify duplicates and overlapping documentation\n- Detect gaps in documentation\n- Create consolidation plan with file operations\n\nExpected output:\n- Classification report (specs, architecture, ADRs, contracts, general)\n- List of duplicates to merge\n- List of files to move/reorganize\n- Recommended new documentation to create\n- Detailed consolidation plan\n\nPhase 3: Review and Approval\nGoal: Present plan and get user confirmation\n\nActions:\n- Display the consolidation plan from doc-consolidator agent\n- Show:\n  - Files to be consolidated (merged)\n  - Files to be moved to proper locations\n  - Files to be archived\n  - Files to be deleted (if any)\n  - New documentation to create\n- Ask user to review and approve plan before proceeding\n- If user wants changes, allow them to specify modifications\n- Update todos\n\nPhase 4: Summary\nGoal: Report consolidation results\n\nActions:\n- Mark all todos complete\n- Summarize consolidation actions taken:\n  - Number of files processed\n  - Files moved to specs/\n  - Files moved to docs/architecture/\n  - Files moved to docs/adrs/\n  - Files moved to contracts/\n  - Files archived\n  - New documentation created\n- Show before/after organization structure\n- Suggest next steps:\n  - Review consolidated documentation\n  - Update cross-references\n  - Run /planning:spec list to verify specs recognized\n  - Consider running documentation validation"
              },
              {
                "name": "/decide",
                "description": null,
                "path": "plugins/planning/commands/decide.md",
                "frontmatter": null,
                "content": "---\ndescription: Create Architecture Decision Records (ADRs)\nargument-hint: [decision-title] [--supersede ADR-XXX]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Document architectural decisions as ADRs with proper numbering, context, and rationale\n\nCore Principles:\n- Structured format - consistent ADR template\n- Numbered sequence - automatic ADR numbering\n- Immutable - decisions are recorded, not changed\n- Searchable - easy to find past decisions\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Discovery\nGoal: Understand decision to document\n\nActions:\n- Parse $ARGUMENTS for:\n  - Decision title\n  - Flags: --supersede ADR-XXX (mark this ADR as superseding an older one)\n- Check for existing ADRs directory\n- Example: !{bash ls docs/adr/ 2>/dev/null | wc -l}\n- Determine next ADR number\n- Example: !{bash ls docs/adr/*.md 2>/dev/null | tail -1}\n- If --supersede flag present:\n  - Validate superseded ADR exists: !{bash find docs/adr -name \"*$SUPERSEDE_NUMBER*\" 2>/dev/null}\n  - Load superseded ADR for context: @docs/adr/*$SUPERSEDE_NUMBER*.md\n\n## Phase 2: Analysis\nGoal: Gather decision context\n\nActions:\n- Check for wizard requirements (if /planning:wizard was run):\n  - Load: @docs/requirements/*/01-initial-request.md\n  - Load: @docs/requirements/*/.wizard/extracted-requirements.json\n  - Load: @docs/requirements/*/02-wizard-qa.md\n  - These may identify architectural decisions to document\n- If decision unclear, use AskUserQuestion to ask:\n  - What decision was made?\n  - What were the alternatives considered?\n  - Why was this chosen?\n- Load project context: @roadmap/project.json\n- Review related architecture: @docs/architecture/\n\n## Phase 3: Planning\nGoal: Structure ADR content\n\nActions:\n- Outline ADR sections:\n  - Title and status\n  - Context and problem\n  - Decision made\n  - Alternatives considered\n  - Consequences\n  - References\n\n## Phase 4: Implementation\nGoal: Create ADR with agent\n\nActions:\n\nTask(description=\"Create ADR\", subagent_type=\"planning:decision-documenter\", prompt=\"You are the decision-documenter agent. Create Architecture Decision Record for $ARGUMENTS.\n\nContext:\n- Project stack: roadmap/project.json\n- Architecture docs: docs/architecture/\n- Wizard requirements (if available): docs/requirements/*/01-initial-request.md, docs/requirements/*/.wizard/extracted-requirements.json, docs/requirements/*/02-wizard-qa.md\n- Decision: $ARGUMENTS\n- Supersedes (if --supersede flag): ADR-XXX (context from docs/adr/)\n\nRequirements:\n  - Read wizard requirements first (if they exist) to understand project context\n  - Follow ADR template format\n  - Number sequentially (ADR-XXXX)\n  - Include all required sections\n  - Link to related specs/architecture\n  - If superseding another ADR:\n    - Add 'Supersedes: ADR-XXX' in frontmatter\n    - Explain why previous decision changed\n    - Mark superseded ADR as deprecated in its status\n  - Use decision-tracking skill templates\n\nDeliverable: docs/adr/XXXX-decision-title.md\")\n\n## Phase 5: Review\nGoal: Verify ADR created\n\nActions:\n- Check ADR file exists and is complete\n- Example: @docs/adr/XXXX-*.md\n- Verify all sections present\n- Update ADR index if exists\n\n## Phase 6: Documentation Sync & Implementation Tracking\n\nGoal: Register ADR and identify implementing specs\n\nActions:\n- Sync ADR to Mem0 documentation registry:\n  !{source /tmp/mem0-env/bin/activate && python plugins/planning/skills/doc-sync/scripts/sync-to-mem0.py --quiet 2>/dev/null && echo \"‚úÖ ADR registered in documentation system\" || echo \"‚ö†Ô∏è  Doc sync unavailable (mem0 not installed)\"}\n\n- Query which specs implement this ADR:\n  !{bash if [ -f /tmp/mem0-env/bin/activate ]; then ADR_NUM=$(ls -1 docs/adr/*.md 2>/dev/null | tail -1 | grep -oP '\\d+' | head -1); if [ -n \"$ADR_NUM\" ]; then source /tmp/mem0-env/bin/activate && python plugins/planning/skills/doc-sync/scripts/query-docs.py \"What specs implement ADR-$ADR_NUM?\" 2>/dev/null | grep -E \"Specification|implement\" | head -10 || echo \"‚ÑπÔ∏è  No specs yet implement this ADR\"; fi; fi}\n\n- Display implementing specs (if any)\n- This tracks:\n  - Which specs are implementing this decision\n  - Where this ADR is being applied\n  - What features are affected\n\n## Phase 7: Summary\nGoal: Report ADR creation\n\nActions:\n- Display: \"‚úÖ Created ADR-XXXX: {title}\"\n- Show file location\n- If --supersede flag was used:\n  - Display: \"üîÑ Supersedes: ADR-{old_number}\"\n  - Display: \"üìù Updated superseded ADR status to 'Deprecated'\"\n- Else:\n  - Suggest: \"ADRs are immutable - use --supersede ADR-XXX to create superseding ADR\"\n"
              },
              {
                "name": "/extract-config",
                "description": "Extract project.json, features.json, application-design.json, and website-design.json from architecture docs",
                "path": "plugins/planning/commands/extract-config.md",
                "frontmatter": {
                  "description": "Extract project.json, features.json, application-design.json, and website-design.json from architecture docs",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n**Arguments**: None required\n\nGoal: Extract comprehensive roadmap/project.json, roadmap/features.json, roadmap/application-design.json, and roadmap/website-design.json from all generated architecture documentation with full context.\n\nCore Principles:\n- Read ALL architecture docs before generating config\n- Cross-reference for completeness and consistency\n- Extract tech stack from multiple sources (backend.md, frontend.md, data.md, ai.md, infrastructure.md)\n- Extract features from ROADMAP.md and architecture analysis\n- Extract application pages (Next.js) and website pages (Astro) from frontend.md\n- Generate comprehensive configuration files ready for init-project\n\nPhase 1: Validate Architecture Exists\nGoal: Ensure all required architecture files exist before extraction\n\nActions:\n- Create todo list tracking extraction phases\n- Check for required architecture files:\n  - docs/architecture/README.md\n  - docs/architecture/backend.md\n  - docs/architecture/frontend.md\n  - docs/architecture/data.md\n  - docs/architecture/ai.md\n  - docs/architecture/infrastructure.md\n  - docs/architecture/security.md\n  - docs/architecture/integrations.md\n  - docs/architecture/application-pages.md\n  - docs/architecture/website-pages.md\n  - docs/ROADMAP.md\n- If any files missing, display error and exit:\n  ```\n  ‚ùå Missing architecture files. Run /planning:wizard first to generate architecture documentation.\n  \n  Missing files:\n  - docs/architecture/backend.md\n  - docs/ROADMAP.md\n  ```\n- Update todos\n\nPhase 2: Read All Architecture Documentation\nGoal: Load complete context from all architecture files\n\nActions:\n- Read all 10 architecture files into memory:\n  - README.md (system overview, project description)\n  - backend.md (backend framework, API design, MCP servers)\n  - frontend.md (frontend framework, UI libraries, component patterns)\n  - data.md (database type, schema, relationships)\n  - ai.md (AI SDKs, providers, memory systems, MCP integrations)\n  - infrastructure.md (deployment targets, containerization, CI/CD)\n  - security.md (authentication, authorization, secrets management)\n  - integrations.md (external services, webhooks, APIs)\n  - application-pages.md (interactive app pages: dashboard, settings, chat, admin, auth)\n  - website-pages.md (static/marketing pages: landing, pricing, about, blog, docs)\n- Read ROADMAP.md (features, milestones, timeline)\n- Read docs/FINAL-APPROVAL.md if exists (validation results)\n- Store content for cross-referencing\n- Update todos\n\nPhase 3: Extract Tech Stack (project.json)\nGoal: Generate comprehensive roadmap/project.json from architecture docs\n\nActions:\n- Extract project name from README.md\n- Extract framework from backend.md and frontend.md:\n  - Backend: FastAPI, Django, Express, Go, Rust, etc.\n  - Frontend: Next.js, React, Vue, Svelte, etc.\n- Extract languages from all docs (TypeScript, Python, Go, Rust, etc.)\n- Extract AI stack from ai.md:\n  - SDKs: Vercel AI SDK, Claude Agent SDK, LangChain, etc.\n  - Providers: Anthropic, OpenAI, Google, etc.\n  - Memory: Mem0, custom implementations\n  - MCP servers: List from ai.md and backend.md\n- Extract database from data.md:\n  - Type: PostgreSQL, MongoDB, MySQL, etc.\n  - Provider: Supabase, raw, cloud-hosted\n  - ORM: Prisma, SQLAlchemy, Drizzle, etc.\n  - Extensions: pgvector, etc.\n- Extract testing frameworks from architecture docs\n- Extract deployment targets from infrastructure.md:\n  - Frontend: Vercel, Netlify, Cloudflare Pages\n  - Backend: Railway, DigitalOcean, AWS\n  - MCP: FastMCP Cloud, self-hosted\n- Extract infrastructure components from infrastructure.md:\n  - Authentication (Clerk, Supabase Auth, Auth0, NextAuth)\n  - Caching (Redis, Memcached, in-memory)\n  - Monitoring (Sentry, DataDog, New Relic)\n  - Error handling (Sentry, custom)\n  - Rate limiting (express-rate-limit, Redis-based)\n  - CI/CD (GitHub Actions, GitLab CI)\n- Cross-reference all sources for consistency\n- Update todos\n\nPhase 4: Generate project.json\nGoal: Write comprehensive roadmap/project.json\n\nActions:\n- Create roadmap/ directory if not exists\n- Generate project.json with structure:\n  ```json\n  {\n    \"name\": \"project-name\",\n    \"framework\": \"Next.js 15\",\n    \"languages\": [\"TypeScript\", \"Python\"],\n    \"ai_stack\": {\n      \"sdks\": [\"Vercel AI SDK\", \"Claude Agent SDK\"],\n      \"providers\": [\"Anthropic\", \"OpenAI\"],\n      \"memory\": \"Mem0\",\n      \"mcp_servers\": [\"supabase\", \"playwright\", \"context7\"]\n    },\n    \"database\": {\n      \"type\": \"PostgreSQL\",\n      \"provider\": \"Supabase\",\n      \"orm\": \"Prisma\",\n      \"extensions\": [\"pgvector\"]\n    },\n    \"testing\": {\n      \"unit\": \"Jest\",\n      \"e2e\": \"Playwright\",\n      \"api\": \"Supertest\"\n    },\n    \"deployment\": {\n      \"frontend\": \"Vercel\",\n      \"backend\": \"Railway\",\n      \"mcp\": \"FastMCP Cloud\"\n    },\n    \"infrastructure\": {\n      \"authentication\": {\n        \"provider\": \"Clerk\",\n        \"features\": [\"JWT validation\", \"Session management\"],\n        \"integration\": \"Supabase RLS sync\"\n      },\n      \"caching\": {\n        \"provider\": \"Redis\",\n        \"strategy\": \"query caching\",\n        \"use_cases\": [\"API responses\", \"embeddings\"]\n      },\n      \"monitoring\": {\n        \"provider\": \"Sentry\",\n        \"features\": [\"error tracking\", \"performance monitoring\"]\n      },\n      \"error_handling\": {\n        \"provider\": \"Sentry\",\n        \"features\": [\"error aggregation\", \"alert rules\"]\n      },\n      \"rate_limiting\": {\n        \"provider\": \"express-rate-limit\",\n        \"strategy\": \"sliding window\"\n      },\n      \"ci_cd\": {\n        \"platform\": \"GitHub Actions\",\n        \"workflows\": [\"test\", \"deploy\", \"security-scan\"]\n      }\n    },\n    \"extracted_at\": \"2025-01-XX\",\n    \"extracted_from\": \"architecture docs via /planning:extract-config\"\n  }\n  ```\n- Write to roadmap/project.json\n- Validate JSON syntax\n- Update todos\n\nPhase 5: Extract Features (features.json)\nGoal: Generate comprehensive roadmap/features.json from ROADMAP.md and architecture\n\nActions:\n- Parse ROADMAP.md for feature list:\n  - Feature names and descriptions\n  - Priority levels (P0, P1, P2)\n  - Dependencies between features\n  - Estimated effort\n- Cross-reference with architecture docs for feature details:\n  - ai.md for AI-related features\n  - frontend.md for UI features\n  - backend.md for API features\n  - data.md for data features\n- Extract shared context from architecture:\n  - Tech stack references\n  - Common dependencies\n  - Infrastructure requirements\n- **CRITICAL: Analyze feature dependencies and determine build order**:\n  - Infrastructure must come first (always dependency for features)\n  - Foundation features before dependent features\n  - Core services before UI features\n  - Data models before features using them\n  - API endpoints before frontend consuming them\n- Number features sequentially (F001, F002, etc.)\n- **Order features by build_order (not just priority)**\n- Group features by priority AND dependencies\n- Update todos\n\nPhase 6: Generate features.json\nGoal: Write comprehensive roadmap/features.json with dependency-based ordering\n\nActions:\n- Generate features.json with structure:\n  ```json\n  {\n    \"features\": [\n      {\n        \"id\": \"F001\",\n        \"name\": \"Feature Name\",\n        \"description\": \"Detailed description from ROADMAP and architecture\",\n        \"priority\": \"P0\",\n        \"status\": \"not_started\",\n        \"estimated_effort\": \"3-5 days\",\n        \"build_order\": 1,\n        \"dependencies\": [\"infrastructure\"],\n        \"blocks\": [\"F003\", \"F005\"],\n        \"architecture_refs\": [\n          \"docs/architecture/ai.md#rag-system\",\n          \"docs/architecture/backend.md#api-endpoints\"\n        ]\n      }\n    ],\n    \"build_order_explanation\": {\n      \"1\": \"Foundation features (no feature dependencies, only infrastructure)\",\n      \"2\": \"Core services (depend on foundation)\",\n      \"3\": \"Secondary features (depend on core services)\",\n      \"4\": \"UI features (depend on backend/core services)\",\n      \"5\": \"Integration features (depend on multiple features)\"\n    },\n    \"shared_context\": {\n      \"tech_stack\": \"Next.js 15 + FastAPI + Supabase\",\n      \"ai_stack\": \"Claude Agent SDK + Vercel AI SDK\",\n      \"authentication\": \"Clerk with Supabase RLS\",\n      \"deployment\": \"Vercel (frontend) + Railway (backend)\"\n    },\n    \"extracted_at\": \"2025-01-XX\",\n    \"extracted_from\": \"ROADMAP.md + architecture docs via /planning:extract-config\"\n  }\n  ```\n- **CRITICAL: Features MUST be ordered by build_order in the array**\n  - Feature with build_order: 1 comes first\n  - Features with same build_order can be built in parallel\n  - This ensures /planning:init-project creates specs in correct order\n- Write to roadmap/features.json\n- Validate JSON syntax\n- Update todos\n\nPhase 7: Extract Application Pages (application-design.json)\nGoal: Generate roadmap/application-design.json from application-pages.md\n\nActions:\n- Parse application-pages.md for all application pages:\n  * Dashboard pages (main dashboard, analytics, reports)\n  * Settings pages (user settings, preferences, profile)\n  * Chat/AI pages (chat interface, AI generation)\n  * Admin pages (user management, system config)\n  * Auth pages (login, signup, password reset)\n- Detect page characteristics:\n  * Route paths (/ dashboard, /settings, /chat)\n  * Route groups ((app), (auth), (admin))\n  * Layouts (dashboard_layout, auth_layout, minimal_layout)\n  * Rendering strategy (server | client | hybrid)\n  * Components needed (sidebar, header, forms, tables)\n  * Data sources (supabase, API endpoints)\n  * AI features (chat streaming, generation)\n- Number pages sequentially (A001, A002, etc.)\n- Determine dependencies and phases\n- Update todos\n\nPhase 8: Generate application-design.json\nGoal: Write comprehensive roadmap/application-design.json for Next.js application pages\n\nActions:\n- Generate roadmap/application-design.json using schema template\n- Include all application pages with full details\n- Add layout definitions (dashboard, auth, minimal)\n- Reference design-system.md for UI enforcement\n- Write to roadmap/application-design.json\n- Validate JSON syntax\n- Update todos\n\nPhase 9: Extract Website Pages (website-design.json)\nGoal: Generate roadmap/website-design.json from website-pages.md\n\nActions:\n- Parse website-pages.md for all marketing/content pages:\n  * Landing pages (main landing, product pages)\n  * Marketing pages (pricing, about, features)\n  * Blog pages (blog index, post template)\n  * Documentation pages (docs structure)\n- Detect page characteristics:\n  * Route paths (/, /pricing, /about, /blog)\n  * Sections (hero, features, pricing, testimonials, CTA, FAQ)\n  * Content type (static | collection | CMS)\n  * AI features (content generation, image generation, SEO)\n  * SEO requirements (meta tags, structured data, OG images)\n- Number pages sequentially (W001, W002, etc.)\n- Determine dependencies and phases\n- Update todos\n\nPhase 10: Generate website-design.json\nGoal: Write comprehensive roadmap/website-design.json for Astro marketing/content pages\n\nActions:\n- Generate roadmap/website-design.json using schema template\n- Include all website pages with full details\n- Add content collections (blog posts, docs)\n- Add CMS integration if specified\n- Add AI generation capabilities\n- Write to roadmap/website-design.json\n- Validate JSON syntax\n- Update todos\n\nPhase 11: Validation\nGoal: Verify extracted configuration is complete and consistent\n\nActions:\n- Validate project.json:\n  - All required fields present\n  - Tech stack consistent across architecture docs\n  - Infrastructure section matches infrastructure.md\n  - No placeholder values (all real detections)\n- Validate features.json:\n  - All features from ROADMAP included\n  - Feature descriptions are comprehensive\n  - Dependencies correctly identified\n  - Priority levels assigned\n  - Architecture references valid\n  - **CRITICAL: Build order is correct**:\n    - Features ordered by build_order field\n    - No circular dependencies\n    - Dependencies have lower build_order than dependents\n    - Features with same build_order can be built in parallel\n- Validate application-design.json:\n  - All application pages from frontend.md included\n  - Page routes are valid Next.js App Router routes\n  - Route groups are correct\n  - Layouts match design-system.md\n  - Dependencies correctly identified\n  - Phase ordering is correct\n- Validate website-design.json:\n  - All marketing/content pages from frontend.md included\n  - Page routes are valid Astro routes\n  - Sections match marketing page patterns\n  - Content collections properly defined\n  - AI generation capabilities specified\n  - SEO requirements complete\n- Check for inconsistencies between files\n- Display validation results including:\n  - Feature build order summary (X features at order 1, Y at order 2, etc.)\n  - Application pages summary (X pages, Y layouts)\n  - Website pages summary (X pages, Y with AI generation)\n  - Dependency graph validation\n- Update todos\n\nPhase 12: Summary\nGoal: Display results and next steps\n\nActions:\n- Display completion message:\n  ```\n  ‚úÖ Configuration Extraction Complete!\n\n  Generated Files:\n  - roadmap/project.json (tech stack and infrastructure)\n  - roadmap/features.json (feature breakdown with build order)\n  - roadmap/application-design.json (Next.js application pages)\n  - roadmap/website-design.json (Astro marketing/content pages)\n\n  Feature Build Order:\n  - Build Order 1: X features (foundation - can build in parallel)\n  - Build Order 2: Y features (core services - can build in parallel)\n  - Build Order 3: Z features (secondary - can build in parallel)\n  - Build Order 4: N features (UI - can build in parallel)\n  - Build Order 5: M features (integration - can build in parallel)\n\n  Application Pages (Next.js):\n  - Phase 0: X pages (can build in parallel)\n  - Phase 1: Y pages (can build in parallel)\n  - Phase 2: Z pages (can build in parallel)\n  - Total Layouts: N\n\n  Website Pages (Astro):\n  - Phase 0: X pages (can build in parallel)\n  - Phase 1: Y pages (can build in parallel)\n  - Phase 2: Z pages (can build in parallel)\n  - AI Content Generation: N pages\n  - AI Image Generation: M pages\n\n  Extracted From:\n  - docs/architecture/README.md\n  - docs/architecture/backend.md\n  - docs/architecture/frontend.md (component patterns)\n  - docs/architecture/data.md\n  - docs/architecture/ai.md\n  - docs/architecture/infrastructure.md\n  - docs/architecture/security.md\n  - docs/architecture/integrations.md\n  - docs/architecture/application-pages.md (app page inventory)\n  - docs/architecture/website-pages.md (marketing page inventory)\n  - docs/ROADMAP.md\n\n  Next Steps:\n  1. Run /planning:init-project to generate feature specs (creates specs in build order)\n  2. Run /foundation:generate-infrastructure-specs to generate infrastructure specs\n  3. Build application pages: /implementation:execute --application\n  4. Build website pages: /implementation:execute --website\n  5. Build features in order (build_order: 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5)\n  6. Features with same build_order can be built in parallel\n  ```\n- Mark all todos completed"
              },
              {
                "name": "/generate-feature-workflow",
                "description": null,
                "path": "plugins/planning/commands/generate-feature-workflow.md",
                "frontmatter": null,
                "content": "---\ndescription: Generate feature implementation workflow from features.json and specs\nargument-hint: [project-path] [--feature <id>|--priority <P0|P1|P2>|--status <status>|--split]\nallowed-tools: Read(*), Write, Bash(*), Glob, Grep, TodoWrite, mcp__airtable__search_records, mcp__airtable__get_record\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate ongoing feature implementation workflow from features.json and specs/ directory. This workflow guides feature-by-feature implementation with tech-stack-aware commands.\n\nCore Principles:\n- roadmap/features.json is source of truth for planned features\n- Specs provide detailed requirements\n- Workflow includes feature-specific commands\n- Separate from foundation infrastructure workflow\n\n**Flags**:\n- `--feature <id>`: Generate workflow for specific feature only (e.g., F001)\n- `--priority <level>`: Filter by priority (P0, P1, P2)\n- `--status <status>`: Filter by status (planned, in-progress, completed)\n- `--split`: Generate separate files per feature (F001-WORKFLOW.md, F002-WORKFLOW.md, etc.)\n- Default (no flags): All features in one FEATURE-IMPLEMENTATION-WORKFLOW.md\n\nPhase 0.5: Parse Flags\nGoal: Parse command arguments and determine filtering scope\n\nActions:\n- Create todo list using TodoWrite\n- Parse $ARGUMENTS for flags:\n  * Extract `--feature <id>`: FEATURE_FILTER=\"<id>\"/null\n  * Extract `--priority <level>`: PRIORITY_FILTER=\"<level>\"/null\n  * Extract `--status <status>`: STATUS_FILTER=\"<status>\"/null\n  * Extract `--split`: SPLIT_MODE=true/false\n  * Extract remaining as PROJECT_PATH (if provided)\n- Store parsed values:\n  * FEATURE_FILTER (string or null)\n  * PRIORITY_FILTER (string or null)\n  * STATUS_FILTER (string or null)\n  * SPLIT_MODE (boolean)\n  * PROJECT_PATH (string or current directory)\n- Display: \"Scope: {All features|Feature: <id>|Priority: <level>|Status: <status>} {Split mode: {yes|no}}\"\n\nPhase 1: Discovery\nGoal: Read features.json and specs/ to understand what needs to be built\n\nActions:\n- Change to PROJECT_PATH directory (from Phase 0.5)\n- Check if features.json exists:\n  !{bash test -f roadmap/features.json && echo \"exists\" || echo \"missing\"}\n- If missing: Display error and exit\n- Read features.json:\n  @roadmap/features.json\n- Extract all feature IDs, names, status, priority, dependencies\n- **Apply filters** (from Phase 0.5):\n  * If FEATURE_FILTER set: Only include matching feature ID\n  * If PRIORITY_FILTER set: Only include matching priority (P0, P1, P2)\n  * If STATUS_FILTER set: Only include matching status\n  * Store filtered features in FILTERED_FEATURES array\n- Count filtered features: Display \"Found {N} features matching criteria\"\n- If no features match filters: Display error and exit\n\nPhase 2: Load Specifications\nGoal: Read detailed specs for each filtered feature\n\nActions:\n- List all spec directories:\n  !{bash ls -d specs/features/[0-9][0-9][0-9]-*/ 2>/dev/null}\n- **For each feature in FILTERED_FEATURES** (not all features):\n  - Check if spec directory exists\n  - If exists: Read spec.md and tasks.md\n  - Extract: Requirements, tech stack components used, implementation notes\n  - Store per-feature context\n- Display: \"Loaded {N} specifications for filtered features\"\n\nPhase 3: Fetch Available Commands from Airtable\nGoal: Query tech stack in Airtable to get ALL available commands for implementation\n\nActions:\n- Navigate to planning skill directory:\n  !{cd ~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/feature-workflow-generation}\n- Execute Python script to query Airtable:\n  !{python3 scripts/generate-feature-workflow.py}\n- Script returns JSON with:\n  * tech_stack: Tech stack name from project.json\n  * features: Array of features from features.json with spec content\n  * available_commands: All commands available for the tech stack\n  * plugins: Plugin details organized by lifecycle phase\n- Parse JSON output and extract:\n  * AVAILABLE_COMMANDS: Map of commands organized by plugin\n  * FEATURES_DATA: Features with spec content\n  * TECH_STACK_NAME: Tech stack being used\n- Handle errors:\n  * If \"error\" in JSON: Display error message and exit\n  * If missing features.json: Suggest running /planning:add-feature\n  * If missing project.json: Suggest running /foundation:detect\n  * If Airtable fails: Fall back to filesystem-based command discovery\n- Display: \"Found [N] commands across [M] plugins for [TECH_STACK_NAME]\"\n\nPhase 4: Generate Workflow Document\nGoal: Create feature workflow document(s) based on SPLIT_MODE\n\nActions:\n- Determine workflow file strategy:\n  * If SPLIT_MODE=true: Generate separate files per feature ({FEATURE_ID}-WORKFLOW.md)\n  * If SPLIT_MODE=false: Single file (FEATURE-IMPLEMENTATION-WORKFLOW.md)\n  * If FEATURE_FILTER set: Single file ({FEATURE_ID}-WORKFLOW.md)\n\n- **If SPLIT_MODE=true**:\n  - For each feature in FILTERED_FEATURES:\n    * Create file: {FEATURE_ID}-WORKFLOW.md\n    * Generate workflow for THAT feature only\n    * Include: Feature header, requirements, matched commands, validation\n  - Write each file separately\n  - Track created files in CREATED_FILES array\n\n- **If SPLIT_MODE=false** (default):\n  - Create single file: FEATURE-IMPLEMENTATION-WORKFLOW.md\n  - For each feature in FILTERED_FEATURES (ordered by priority and dependencies):\n    * Create section: Feature [ID]: [Name]\n    * Add status, priority, dependencies, spec path\n    * Extract requirements from spec.md\n    * Match requirements to AVAILABLE_COMMANDS from Phase 3:\n      - If feature needs database ‚Üí Use /supabase:* commands\n      - If feature needs auth ‚Üí Use /clerk:* commands\n      - If feature needs memory ‚Üí Use /mem0:* commands\n      - If feature needs backend ‚Üí Use /fastapi-backend:* commands\n      - If feature needs frontend ‚Üí Use /nextjs-frontend:* commands\n    * Layer commands by phase:\n      - Setup: /iterate:tasks [ID]\n      - Implementation: Matched commands from AVAILABLE_COMMANDS\n      - Validation: /quality:validate-code [ID], /testing:test, /iterate:sync [ID]\n  - Include summary: Total features, status breakdown, available commands\n  - Write workflow document\n\n- Display: \"Generated {N} workflow file(s)\"\n\nPhase 5: Summary\nGoal: Report what was generated\n\nActions:\n- Mark all todos complete\n- Display summary based on what was generated:\n\n**If SPLIT_MODE=true**:\n  ```\n  **‚úÖ Generated {N} workflow files:**\n  {List each file created}\n\n  **Filtering:**\n  - Feature filter: {FEATURE_FILTER or \"None\"}\n  - Priority filter: {PRIORITY_FILTER or \"None\"}\n  - Status filter: {STATUS_FILTER or \"None\"}\n\n  **Contents per file:**\n  - 1 feature documented\n  - Tech-stack-aware commands\n  - Implementation steps\n  ```\n\n**If SPLIT_MODE=false**:\n  ```\n  **‚úÖ Generated: FEATURE-IMPLEMENTATION-WORKFLOW.md**\n\n  **Filtering:**\n  - Feature filter: {FEATURE_FILTER or \"None\"}\n  - Priority filter: {PRIORITY_FILTER or \"None\"}\n  - Status filter: {STATUS_FILTER or \"None\"}\n\n  **Contents:**\n  - [N] features documented\n  - [X] complete, [Y] in-progress, [Z] planned\n  - Tech-stack-aware commands for each feature\n  ```\n\n**Always display**:\n  ```\n  **Next Steps:**\n  1. Review workflow document(s)\n  2. Follow feature-by-feature implementation commands\n  3. Update roadmap/features.json status as you progress\n  4. Re-run this command when adding new features\n\n  **Difference from Foundation Workflow:**\n  - `/foundation:generate-workflow` = Infrastructure setup (one-time)\n  - `/planning:generate-feature-workflow` = Feature implementation (ongoing)\n\n  **Examples:**\n  - /planning:generate-feature-workflow --feature F001\n  - /planning:generate-feature-workflow --priority P0\n  - /planning:generate-feature-workflow --status in-progress\n  - /planning:generate-feature-workflow --split\n  ```\n"
              },
              {
                "name": "/init-project",
                "description": "Create ALL project specs in one shot from massive description using parallel agents",
                "path": "plugins/planning/commands/init-project.md",
                "frontmatter": {
                  "description": "Create ALL project specs in one shot from massive description using parallel agents",
                  "argument-hint": "<project-description>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Rapidly generate complete project specifications from features.json (or massive project description if features.json missing)\n\nCore Principles:\n- **Prefer features.json if it exists** (source of truth)\n- Read project.json for tech stack context\n- Generate specs ONLY for features without specs/ directories\n- Process 3-5 features at a time (batching)\n- Use structured JSON to coordinate agents\n- Provide comprehensive summary with paths\n\nPhase 0: Check Existing Project Data\nGoal: Check if features.json and project.json already exist\n\nActions:\n- Check for features.json: !{bash test -f roadmap/features.json && echo \"‚úÖ EXISTS\" || echo \"‚ö†Ô∏è MISSING\"}\n- Check for project.json: !{bash test -f roadmap/project.json && echo \"‚úÖ EXISTS\" || echo \"‚ö†Ô∏è MISSING\"}\n\n**If BOTH exist**:\n  - Read features.json: @roadmap/features.json\n  - Read project.json: @roadmap/project.json\n  - Extract feature list from features.json\n  - Check which features already have specs (phase-nested or legacy):\n    !{bash for f in $(jq -r '.features | keys[]' roadmap/features.json 2>/dev/null); do phase=$(jq -r \".features[\\\"$f\\\"].phase // 0\" roadmap/features.json); if [ -d \"specs/phase-$phase/$f-\"* ] || [ -d \"specs/$f\" ]; then echo \"$f: ‚úÖ HAS SPEC\"; else echo \"$f: ‚ö†Ô∏è NEEDS SPEC\"; fi; done}\n  - Filter to features WITHOUT specs\n  - Display: \"Found [X] features, [Y] need specs\"\n  - **SKIP to Phase 4** (use existing roadmap/features.json)\n\n**If roadmap/features.json MISSING**:\n  - Display: \"roadmap/features.json not found - will create from architecture docs\"\n  - Continue to Phase 1 (architecture analysis)\n\nPhase 1: Verify Architecture Documentation\nGoal: Check for architecture docs created by /planning:architecture, /planning:decide, /planning:roadmap\n\nActions:\n- Check if architecture docs exist: !{bash test -d docs/architecture && echo \"‚úÖ Found\" || echo \"‚ö†Ô∏è Missing\"}\n- If docs/architecture/ exists:\n  - List architecture files: !{bash ls -1 docs/architecture/*.md 2>/dev/null | wc -l}\n  - List ADR files: !{bash ls -1 docs/adr/*.md 2>/dev/null | wc -l}\n  - Verify ROADMAP: !{bash test -f docs/ROADMAP.md && echo \"‚úÖ Found\" || echo \"‚ö†Ô∏è Missing\"}\n  - Architecture files will be passed directly to agents via @ references:\n    - @docs/architecture/frontend.md\n    - @docs/architecture/backend.md\n    - @docs/architecture/data.md\n    - @docs/architecture/ai.md\n    - @docs/architecture/infrastructure.md\n    - @docs/architecture/security.md\n    - @docs/architecture/integrations.md\n    - @docs/adr/*.md\n    - @docs/ROADMAP.md\n- If not exists:\n  - Note: Will use $ARGUMENTS only (architecture docs recommended)\n- No temporary files needed\n\nPhase 2: Parse Project Description\nGoal: Save the project description and validate input\n\nActions:\n- Parse $ARGUMENTS to extract project description\n- Save description to temporary file for analysis\n- Example: !{bash echo \"$ARGUMENTS\" > /tmp/project-description.txt}\n- Verify description is substantial (>100 words) OR architecture docs exist\n- Count words: !{bash wc -w < /tmp/project-description.txt}\n\nPhase 3: Feature Analysis\nGoal: Break massive description into discrete features with dependencies\n\nActions:\n\nTask(description=\"Analyze architecture and break into features\", subagent_type=\"planning:feature-analyzer\", prompt=\"You are the feature-analyzer agent.\n\nINPUT SOURCES:\n\nArchitecture Documentation (read directly from source):\n@docs/architecture/frontend.md\n@docs/architecture/backend.md\n@docs/architecture/data.md\n@docs/architecture/ai.md\n@docs/architecture/infrastructure.md\n@docs/architecture/security.md\n@docs/architecture/integrations.md\n@docs/adr/*.md\n@docs/ROADMAP.md\n\nProject Description: $ARGUMENTS\n\nYOUR TASK:\nBreak this into AS MANY focused features as needed. NO ARTIFICIAL LIMITS.\n\nCRITICAL: Each feature should be:\n- Implementable in 2-3 days (if >3 days, SPLIT IT)\n- Result in 200-300 line specs (NOT 647!)\n- Have 15-25 tasks (NOT 45!)\n- Single responsibility\n- Reference architecture docs for details (don't duplicate)\n\nSIZING RULE: If a feature needs >3 days or >25 tasks, it's TOO LARGE - split it.\n\nExample: DON'T create large features like 'User Authentication'\nInstead, create focused features:\n- Feature 1: Basic Auth (email/password) - 2 days, 18 tasks\n- Feature 2: OAuth Integration - 2 days, 15 tasks\n- Feature 3: MFA - 1 day, 12 tasks\n- Feature 4: Password Reset - 1 day, 10 tasks\n\nThe project might have 10 features, 50 features, or 200 features - THAT'S OK.\nWhat matters: Each feature is small, focused, and implementable in 2-3 days.\n\nDeliverable: JSON output with:\n- features array (AS MANY AS NEEDED - no artificial limit):\n  - number, name, shortName, focus\n  - dependencies (feature numbers this depends on)\n  - estimatedDays (2-3 typical, MAX 3)\n  - complexity (low/medium/high)\n  - architectureReferences (which docs/architecture/*.md sections to reference)\n- sharedContext (techStack, userTypes, dataEntities, integrations)\n\nSave JSON to: /tmp/feature-breakdown.json\")\n\nWait for feature-analyzer to complete and generate JSON.\n\nPhase 4: Prepare Feature List for Spec Generation\nGoal: Get final list of features that need specs (from roadmap/features.json OR feature-breakdown.json)\n\nActions:\n**If came from Phase 0** (roadmap/features.json exists):\n  - Features list already loaded from roadmap/features.json\n  - Filter to features WITHOUT specs/ directories (from Phase 0)\n  - Use roadmap/project.json for tech stack context\n  - Display: \"Generating specs for [X] features from roadmap/features.json\"\n\n**If came from Phase 3** (created feature-breakdown.json):\n  - Load the generated JSON: @/tmp/feature-breakdown.json\n  - Extract feature list from JSON\n  - Count total features: !{bash jq '.features | length' /tmp/feature-breakdown.json}\n  - Display feature list for user visibility\n  - Example: !{bash jq -r '.features[] | \"\\(.number) - \\(.name): \\(.focus)\"' /tmp/feature-breakdown.json}\n  - Display: \"Generating specs for [X] features from architecture analysis\"\n\n**Batching Strategy**:\n  - Total features to generate: [X]\n  - Batch size: 3-5 features at a time\n  - Number of batches: [X/5 rounded up]\n  - Display: \"Will generate in [Y] batches of 3-5 features\"\n\nPhase 5: Parallel Spec Generation (Batch 1)\nGoal: Generate specs for first 3-5 features in parallel\n\nActions:\n- Select first 3-5 features from list (features without specs)\n- Display: \"Batch 1: Generating specs for features [F001, F002, F003...]\"\n\n**Data Sources for Spec Writer**:\n- If from features.json: Use feature data from features.json + project.json for tech stack\n- If from feature-breakdown.json: Use feature data from /tmp/feature-breakdown.json + architecture docs\n\n**Launch parallel spec-writer agents** (3-5 at a time):\n\nFor each feature in BATCH 1, launch a parallel Task:\n\nTask(description=\"Generate spec for feature 001\", subagent_type=\"planning:spec-writer\", prompt=\"You are the spec-writer agent. Create complete specifications (spec.md, plan.md, tasks.md) for this feature.\n\nArchitecture Documentation (read directly from source):\n@docs/architecture/frontend.md\n@docs/architecture/backend.md\n@docs/architecture/data.md\n@docs/architecture/ai.md\n@docs/architecture/infrastructure.md\n@docs/architecture/security.md\n@docs/architecture/integrations.md\n@docs/adr/*.md\n@docs/ROADMAP.md\n\nFull Project Context:\n$ARGUMENTS\n\nYour Feature Assignment:\n- Feature: Extract from JSON /tmp/feature-breakdown.json feature 001\n- Phase: Extract phase from JSON (calculated from dependencies)\n- Focus: Extract focus from JSON\n- Dependencies: Extract dependencies from JSON\n- Integrations: Extract integrations from JSON\n- Shared Context: Extract sharedContext from JSON\n\nDeliverable: Three files in phase-nested directory specs/phase-{phase}/F{number}-{name}/:\n- spec.md (user requirements, tech-agnostic)\n- plan.md (technical design with database schema, API contracts)\n- tasks.md (implementation tasks, 5 phases, numbered)\")\n\nTask(description=\"Generate spec for feature 002\", subagent_type=\"planning:spec-writer\", prompt=\"You are the spec-writer agent. Create complete specifications (spec.md, plan.md, tasks.md) for this feature.\n\nArchitecture Documentation (read directly from source):\n@docs/architecture/frontend.md\n@docs/architecture/backend.md\n@docs/architecture/data.md\n@docs/architecture/ai.md\n@docs/architecture/infrastructure.md\n@docs/architecture/security.md\n@docs/architecture/integrations.md\n@docs/adr/*.md\n@docs/ROADMAP.md\n\nFull Project Context:\n$ARGUMENTS\n\nYour Feature Assignment:\n- Feature: Extract from JSON /tmp/feature-breakdown.json feature 002\n- Phase: Extract phase from JSON (calculated from dependencies)\n- Focus: Extract focus from JSON\n- Dependencies: Extract dependencies from JSON\n- Integrations: Extract integrations from JSON\n- Shared Context: Extract sharedContext from JSON\n\nDeliverable: Three files in phase-nested directory specs/phase-{phase}/F{number}-{name}/:\n- spec.md (user requirements, tech-agnostic)\n- plan.md (technical design with database schema, API contracts)\n- tasks.md (implementation tasks, 5 phases, numbered)\")\n\nContinue launching Task() calls for ALL features in parallel (one Task per feature).\n\nNOTE: In actual execution, the command orchestrator will read the JSON and dynamically create N Task() calls based on feature count.\n\nWait for ALL spec-writer agents to complete before proceeding.\n\nPhase 6: Project Overview\nGoal: Create high-level project overview with build phases and dependency graph\n\nActions:\n- Create overview directory: !{bash mkdir -p specs/000-project-overview}\n- Load template:\n  - @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/spec-management/templates/project-overview-template.md\n- Parse feature analysis JSON for:\n  - Project name and description\n  - All features with buildPhase, dependencies, sharedEntities\n  - Shared context (tech stack, user types, data entities, entity ownership)\n- Generate README.md with populated template:\n  - **Features table** with build phase column, dependencies, status\n  - **Tech stack** from sharedContext\n  - **User types** from sharedContext\n  - **Data Architecture** showing entity ownership (who owns what)\n  - **Build Order & Phases** grouped by phase (1=Foundation, 2=Core, 3=Integration)\n  - **Dependency graph** (mermaid) showing all feature relationships\n  - **Critical path** (longest dependency chain)\n  - **Parallel work opportunities** (which can build simultaneously)\n  - **Integration map** (how features connect)\n- Write: specs/000-project-overview/README.md\n- This file provides the bird's-eye view of entire project with phase organization\n\nPhase 7: Consolidation\nGoal: Generate consolidated project-specs.json from all specs\n\nActions:\n- Run consolidation script to generate JSON output\n- Example: !{bash bash ~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/spec-management/scripts/consolidate-specs.sh}\n- Verify JSON was created: !{bash test -f .planning/project-specs.json && echo \"Generated\" || echo \"Missing\"}\n- Count total specs created across all phases: !{bash find specs/phase-* -name \"spec.md\" 2>/dev/null | wc -l}\n\nPhase 8: Summary\nGoal: Provide comprehensive results with paths and next steps\n\nActions:\n- Display feature count and spec locations by phase\n- Show project-specs.json location\n- List all phase directories and their contents:\n  !{bash for phase in specs/phase-*; do echo \"üìÅ $(basename $phase):\"; ls -1 \"$phase\" 2>/dev/null | sed 's/^/   /'; done}\n- Display summary:\n  - Total features analyzed\n  - Features by phase: Phase 0: [X], Phase 1: [Y], Phase 2: [Z]...\n  - Total specs created (spec.md, plan.md, tasks.md per feature)\n  - JSON consolidation location: .planning/project-specs.json\n  - Next steps: Review specs, run /planning:validate-specs"
              },
              {
                "name": "/init-website",
                "description": "Create all website page specs from comprehensive description - analyzes website requirements and generates W001-W0XX specs in parallel",
                "path": "plugins/planning/commands/init-website.md",
                "frontmatter": {
                  "description": "Create all website page specs from comprehensive description - analyzes website requirements and generates W001-W0XX specs in parallel",
                  "argument-hint": "<website-description> OR --doc=<path/to/document.md>",
                  "allowed-tools": "Read, Bash, Task, TodoWrite, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Analyze comprehensive website description and generate ALL website page specifications (W001-W0XX) in parallel. Creates complete planning documentation for marketing/content website.\n\nPhase 1: Parse Input\nGoal: Determine input mode and basic context\n\nActions:\n- Create todo: \"Initialize website specs from description\"\n- Parse $ARGUMENTS:\n  * If contains \"--doc=\": MODE = \"document\", extract DOC_PATH\n  * Otherwise: MODE = \"text\", DESCRIPTION = $ARGUMENTS\n- If MODE = \"document\":\n  * Validate file exists: !{bash test -f \"$DOC_PATH\" && echo \"exists\" || echo \"missing\"}\n  * If missing: Error and exit\n- Display: \"Mode: [MODE]\"\n\nPhase 2: Load Project Context\nGoal: Understand existing project structure\n\nActions:\n- Read configuration files:\n  * @roadmap/project.json (tech stack - should have Astro/website framework)\n  * @roadmap/website-design.json (existing website pages if any)\n  * @roadmap/features.json (to avoid overlap - features are separate from website)\n- Check if website directory structure exists:\n  * !{bash test -d specs/website && echo \"exists\" || echo \"create\"}\n- Display: \"Project context loaded\"\n\nPhase 3: Analyze Website Description\nGoal: Break down website into discrete pages\n\nActions:\n\nLaunch the feature-analyzer agent to analyze website description:\n\nTask(\n  description=\"Analyze website and break into pages\",\n  subagent_type=\"planning:feature-analyzer\",\n  prompt=\"Analyze this website description and break it into discrete PAGES (not features).\n\n  Input Mode: [MODE]\n  Description: $ARGUMENTS\n  Document Path: [DOC_PATH if applicable]\n\n  Read schema template:\n  - @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/schemas/website-design-schema.json\n\n  Analyze:\n  1. Read roadmap/project.json for tech stack (should have Astro or website framework)\n  2. Read roadmap/website-design.json for existing pages\n  3. Identify distinct website pages needed:\n     - Landing page, pricing, about, blog, docs, contact, etc.\n     - NOT application features (those go in features.json)\n     - Marketing/content pages only\n  4. For each page:\n     - Assign W00X ID (sequential: W001, W002, W003...)\n     - Determine route (/,  /pricing, /about, etc.)\n     - Identify page type (landing, content, interactive, blog)\n     - Extract content requirements\n     - Determine components needed\n  5. Check for duplicates with existing roadmap/website-design.json pages\n\n  Return JSON array of pages:\n  [\n    {\n      'page_id': 'W001',\n      'route': '/',\n      'title': 'Landing Page',\n      'type': 'landing',\n      'description': 'Main landing page with hero, features, CTA',\n      'components': ['Hero', 'Features', 'Testimonials', 'CTA'],\n      'content_needs': ['Hero copy', 'Feature list', 'Social proof']\n    },\n    {\n      'page_id': 'W002',\n      'route': '/pricing',\n      'title': 'Pricing',\n      'type': 'content',\n      'description': 'Pricing tiers and comparison',\n      'components': ['PricingTable', 'FAQ', 'CTA'],\n      'content_needs': ['Pricing tiers', 'FAQ content', 'Feature comparison']\n    }\n  ]\"\n)\n\n- Parse agent response (JSON array of pages)\n- Display: \"Identified X website pages\"\n\nPhase 4: Update roadmap/website-design.json\nGoal: Register all pages before generating specs\n\nActions:\n- Read roadmap/website-design.json (or create if missing)\n- For each page from Phase 3:\n  * Add entry with page_id, route, title, type, description, components\n  * Record created timestamp\n- Write updated roadmap/website-design.json\n- Display: \"‚úÖ Registered X pages in roadmap/website-design.json\"\n\nPhase 5: Generate Page Specs in Parallel\nGoal: Create all page specifications simultaneously\n\nActions:\n- For each page from Phase 3, spawn feature-spec-writer agent\n- Send ALL Task calls in ONE message (parallel execution)\n- Provide each agent with: page_id, route, title, type, description, components, content_needs\n- Each agent creates specs/website/[PAGE_ID]-[slug]/spec.md and tasks.md\n- Wait for all agents to complete\n- Display: \"‚úÖ Generated specs for X pages\"\n\nPhase 6: Summary\nGoal: Report results and next steps\n\nActions:\n- Mark todo complete\n- Display: \"‚úÖ Website initialized:\"\n  * Pages: X pages defined\n  * Specs: specs/website/W001-..., W002-..., etc.\n  * Config: roadmap/website-design.json updated\n- Next steps:\n  * Review specs: specs/website/\n  * Implement pages: /website-builder:* commands\n  * Generate content: /website-builder:generate-content\n  * Build site: /website-builder:deploy-marketing-site\n\n**Important Notes:**\n\n**Website vs Features:**\n- Website pages (W00X) = Marketing/content site (Astro, static)\n- Features (F00X) = Application functionality (Next.js, dynamic)\n- Keep these SEPARATE - don't mix them\n\n**Spec Location:**\nAll website page specs go in `specs/website/W00X-slug/`\n\n**Implementation:**\nUse /website-builder:* or /nextjs-frontend:* commands to build pages from specs"
              },
              {
                "name": "/notes",
                "description": "Capture technical notes and development journal",
                "path": "plugins/planning/commands/notes.md",
                "frontmatter": {
                  "description": "Capture technical notes and development journal",
                  "argument-hint": [
                    "note-topic"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Capture technical notes, decisions, learnings, and development journal entries\n\nCore Principles:\n- Quick capture - low friction for note-taking\n- Searchable - easy to find past notes\n- Dated - timestamped entries\n- Organized - categorized by topic\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Discovery\nGoal: Understand note request\n\nActions:\n- Parse $ARGUMENTS for note topic or search query\n- Check for notes directory\n- Example: !{bash test -d docs/notes && echo \"exists\" || mkdir -p docs/notes}\n- Determine action (create, search, list)\n\n## Phase 2: Validation\nGoal: Prepare for note operation\n\nActions:\n- For create: If topic not provided, ask user for note content\n- For search: Parse search terms\n- For list: Determine sorting (date, topic)\n\n## Phase 3: Execution\nGoal: Perform note operation\n\nActions:\n- For create:\n  - Create timestamped note file\n  - Example: docs/notes/YYYY-MM-DD-topic.md\n  - Add frontmatter with metadata\n  - Write note content\n\n- For search:\n  - Search note contents\n  - Example: !{bash grep -r \"$ARGUMENTS\" docs/notes/}\n\n- For list:\n  - List all notes with summaries\n  - Example: !{bash ls -lt docs/notes/*.md | head -20}\n\n## Phase 4: Summary\nGoal: Report note operation result\n\nActions:\n- For create: \"Note created: docs/notes/{filename}\"\n- For search: \"Found {count} notes matching query\"\n- For list: \"Showing {count} notes\"\n- Suggest: \"Use /planning:notes search <term> to find notes\""
              },
              {
                "name": "/roadmap",
                "description": null,
                "path": "plugins/planning/commands/roadmap.md",
                "frontmatter": null,
                "content": "---\ndescription: Create development roadmap and timeline\nargument-hint: [timeframe] [--refresh]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create project roadmap with milestones, phases, and timeline for development\n\nCore Principles:\n- Realistic - based on actual specs and tasks\n- Phased - organized into logical phases\n- Flexible - can be updated as project evolves\n- Visual - clear timeline representation\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Discovery\nGoal: Understand roadmap scope\n\nActions:\n- Parse $ARGUMENTS for:\n  - Timeframe (quarterly, annual, release-based)\n  - Flags: --refresh (regenerate from current specs)\n- Load all existing specs\n- Example: !{bash find specs -name \"README.md\" -type f}\n- Load architecture documentation\n- Example: @docs/architecture/README.md\n- Check for existing roadmap\n- Example: !{bash test -f docs/ROADMAP.md && echo \"exists\" || echo \"new\"}\n- If --refresh flag present:\n  - Display: \"üîÑ Refreshing roadmap from current specs and architecture\"\n  - Backup existing roadmap: !{bash cp docs/ROADMAP.md docs/ROADMAP.backup.md 2>/dev/null || true}\n\n## Phase 2: Analysis\nGoal: Analyze project scope\n\nActions:\n- Check for wizard requirements (if /planning:wizard was run):\n  - Load: @docs/requirements/*/01-initial-request.md\n  - Load: @docs/requirements/*/.wizard/extracted-requirements.json\n  - Load: @docs/requirements/*/02-wizard-qa.md\n  - These contain: features, constraints, timeline, priorities\n- Review all specs for estimation\n- Identify dependencies between specs\n- Determine phases and milestones\n- If unclear, use AskUserQuestion to ask:\n  - What's the target timeline?\n  - Any fixed milestones or deadlines?\n  - Priority order for features?\n\n## Phase 3: Planning\nGoal: Structure roadmap\n\nActions:\n- Organize into phases:\n  - Phase 1: Foundation\n  - Phase 2: Core Features\n  - Phase 3: Advanced Features\n  - Phase 4: Polish and Launch\n- Identify milestones\n- Estimate timelines based on task complexity\n\n## Phase 4: Implementation\nGoal: Create roadmap with agent\n\nActions:\n\nTask(description=\"Create project roadmap\", subagent_type=\"planning:roadmap-planner\", prompt=\"You are the roadmap-planner agent. Create project roadmap for $ARGUMENTS.\n\nContext:\n- Wizard requirements (if available): docs/requirements/*/01-initial-request.md, docs/requirements/*/.wizard/extracted-requirements.json, docs/requirements/*/02-wizard-qa.md\n- All specs: specs/*/\n- Architecture: docs/architecture/\n- Timeframe: $ARGUMENTS\n\nRequirements:\n  - Read wizard requirements first (if they exist) to understand features, priorities, constraints\n  - Create phased roadmap\n  - Define milestones\n  - Estimate timelines\n  - Show dependencies\n  - Include risk assessment\n  - Provide visual timeline (mermaid gantt chart)\n\nDeliverable: docs/ROADMAP.md with comprehensive project timeline\")\n\n## Phase 5: Review\nGoal: Verify roadmap\n\nActions:\n- Check roadmap created\n- Example: @docs/ROADMAP.md\n- Verify all specs included\n- Confirm timeline realistic\n\n## Phase 6: Summary\nGoal: Report roadmap creation\n\nActions:\n- If --refresh flag was used:\n  - Display: \"‚úÖ Roadmap refreshed: docs/ROADMAP.md\"\n  - Display: \"üìã Backup saved: docs/ROADMAP.backup.md\"\n  - Display: \"üîç Review changes to ensure timeline still accurate\"\n- Else:\n  - Display: \"‚úÖ Roadmap created: docs/ROADMAP.md\"\n- Show key milestones\n- Suggest: \"Review and adjust timeline as needed\"\n- Note: \"Use /iterate:tasks to break down each phase\"\n- Tip: \"Use --refresh flag to regenerate roadmap after spec changes\"\n"
              },
              {
                "name": "/spec",
                "description": "Create, list, and validate specifications in specs/ directory",
                "path": "plugins/planning/commands/spec.md",
                "frontmatter": {
                  "description": "Create, list, and validate specifications in specs/ directory",
                  "argument-hint": "<action> [spec-name]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Manage feature specifications in the specs/ directory - create new specs, list existing ones, and validate spec completeness\n\nCore Principles:\n- Framework-agnostic - works with any tech stack\n- Structured format - consistent spec template\n- Validate completeness - ensure all required sections present\n- Support iteration - specs guide task layering in iterate plugin\n\n## Available Skills\n\nThis commands has access to the following skills from the planning plugin:\n\n- **architecture-patterns**: Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.\n- **decision-tracking**: Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.\n- **doc-sync**: Documentation synchronization using Mem0 for tracking relationships between specs, architecture, ADRs, and roadmap. Use when syncing documentation, querying documentation relationships, finding impact of changes, validating doc consistency, or when user mentions doc sync, documentation tracking, spec dependencies, architecture references, or impact analysis.\n- **spec-management**: Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Discovery\n\nGoal: Understand the requested action and current spec state\n\nActions:\n- Parse $ARGUMENTS for action (create, list, validate, show)\n- Check if specs/ directory exists\n- Example: !{bash test -d specs && echo \"exists\" || echo \"missing\"}\n- If missing and creating, will create it\n- Load existing specs if listing or validating\n- Example: !{bash find specs -name \"*.md\" -type f 2>/dev/null | head -20}\n\n## Phase 2: Analysis\n\nGoal: Determine what needs to be done\n\nActions:\n- For 'create' action:\n  - If spec name not provided, use AskUserQuestion to ask:\n    - What feature are you specifying?\n    - Brief description?\n    - Any specific requirements?\n  - Determine next spec number (001, 002, etc.)\n  - Example: !{bash ls -d specs/[0-9][0-9][0-9] 2>/dev/null | tail -1}\n\n- For 'list' action:\n  - Read all spec directories\n  - Load spec metadata (name, status, date)\n\n- For 'validate' action:\n  - Load spec to validate\n  - Check for required sections\n\n- For 'show' action:\n  - Display specific spec content\n\n## Phase 3: Planning\n\nGoal: Prepare for spec operation\n\nActions:\n- For create: Outline spec structure sections\n- For validate: Define validation criteria\n- For list: Format output structure\n- Review spec-management skill templates\n- Confirm approach if significant\n\n## Phase 4: Implementation\n\nGoal: Execute spec operation with agent\n\nActions:\n\nTask(description=\"Handle spec operation\", subagent_type=\"planning:spec-writer\", prompt=\"You are the spec-writer agent. Handle specification operation for $ARGUMENTS.\n\nContext: Current specs/ directory state\nAction: $ARGUMENTS (create, list, validate, show)\n\nRequirements:\n  - For create: Generate complete specification with:\n    - Overview and goals\n    - Requirements (functional, non-functional)\n    - Technical approach\n    - Tasks breakdown\n    - Success criteria\n    - Dependencies\n  - For list: Show all specs with status\n  - For validate: Check completeness of spec sections\n  - For show: Display spec in readable format\n\nTemplate: Use spec-management skill templates\nDeliverable: Created/updated spec file or validation report\")\n\n## Phase 5: Review\n\nGoal: Verify spec operation results\n\nActions:\n- Check agent's output\n- Verify spec file created/updated (for create)\n- Validate spec structure (for validate)\n- Example: @specs/XXX/README.md (to verify content)\n- Ensure all required sections present\n\n## Phase 6: Documentation Sync\n\nGoal: Register spec in documentation system\n\nActions:\n- If action was 'create' or 'update':\n  - Sync spec to Mem0 documentation registry:\n    !{source /tmp/mem0-env/bin/activate && python plugins/planning/skills/doc-sync/scripts/sync-to-mem0.py --quiet 2>/dev/null && echo \"‚úÖ Spec registered in documentation system\" || echo \"‚ö†Ô∏è  Doc sync unavailable (mem0 not installed)\"}\n  - This registers:\n    - Architecture document references\n    - ADR implementations\n    - Spec dependencies\n    - Creation/modification timestamps\n- If action was 'list' or 'validate':\n  - Skip sync (no changes made)\n\n## Phase 7: Summary\n\nGoal: Report what was accomplished\n\nActions:\n- Display summary based on action:\n  - For create: \"Created specification: specs/{number}/{name}\"\n  - For list: \"{count} specifications found\"\n  - For validate: \"Validation result: {status}\"\n  - For show: \"Displaying spec: {name}\"\n- Show spec location and structure\n- Suggest next steps:\n  - After create: \"Run /iterate:tasks {spec-number} to create layered tasks\"\n  - After validate: \"Address missing sections if any\"\n  - General: \"Use /planning:architecture to design technical approach\""
              },
              {
                "name": "/sync-all",
                "description": null,
                "path": "plugins/planning/commands/sync-all.md",
                "frontmatter": null,
                "content": "---\ndescription: Sync features.json, specs/, project.json, and tasks.md - keeps all planning artifacts in sync\nargument-hint: [--auto-fix] [--report-only]\nallowed-tools: Read, Bash, Task, TodoWrite, Write, Edit, Glob, Grep\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON'T wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON'T treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n**Arguments**: $ARGUMENTS\n\nGoal: Ensure complete sync between features.json, specs/features/, specs/infrastructure/, project.json, and tasks.md files. Detects drift and fixes discrepancies.\n\nParse Arguments:\n- If contains \"--auto-fix\": MODE = \"auto\" (fix issues automatically)\n- If contains \"--report-only\": MODE = \"report\" (report issues only, no fixes)\n- Otherwise: MODE = \"interactive\" (ask before fixing)\n\nPhase 0: Initialize Sync Process\nGoal: Create tracking and prepare for sync analysis\n\nActions:\n- Create todo list:\n  * \"Scan all planning sources\"\n  * \"Detect drift and discrepancies\"\n  * \"Generate sync report\"\n  * \"Apply fixes (if mode allows)\"\n- Display: \"üîÑ Starting comprehensive sync check...\"\n- Display: \"Mode: [MODE]\"\n- Update todos\n\nPhase 1: Scan All Planning Sources\nGoal: Read and analyze all planning artifacts\n\nActions:\n- Read features.json: !{bash jq '.features | length' roadmap/features.json 2>/dev/null || echo \"0\"}\n  * Count total features\n  * Get status breakdown (completed, in-progress, planned)\n  * Store feature IDs: F001, F002, etc.\n- Scan specs/features/ directories: !{bash find specs/features -type d -name \"F0*\" | wc -l}\n  * Count spec directories\n  * Get list of feature IDs from directory names\n  * Store extra specs (specs without roadmap/features.json entry)\n  * Store missing specs (roadmap/features.json entries without spec dirs)\n- Read project.json infrastructure section:\n  * Count infrastructure.existing items\n  * Count infrastructure.needed items\n  * Store infrastructure IDs: I001, I002, etc.\n- Scan specs/infrastructure/ directories: !{bash find specs/infrastructure -type d -name \"I0*\" | wc -l 2>/dev/null || echo \"0\"}\n  * Count infrastructure spec directories\n  * Get list of infra IDs from directory names\n  * Store extra infra specs\n  * Store missing infra specs\n- Update todos\n\nPhase 2: Detect Drift and Discrepancies\nGoal: Identify all sync issues between sources\n\nActions:\n- Compare roadmap/features.json ‚Üî specs/features/:\n  * Missing specs: Features in roadmap/features.json without spec directories\n  * Extra specs: Spec directories without roadmap/features.json entries\n  * Status mismatches: Features marked \"completed\" but tasks.md shows incomplete tasks\n- Compare project.json ‚Üî specs/infrastructure/:\n  * Missing infra specs: Infrastructure items without spec directories\n  * Extra infra specs: Spec directories not in project.json\n  * Status mismatches: Infrastructure marked \"completed\" but no spec exists\n- Validate tasks.md completion:\n  * For each \"completed\" feature, check if tasks.md has all tasks marked [x]\n  * For each \"in-progress\" feature, check if tasks.md exists\n- Store all discrepancies in memory\n- Update todos\n\nPhase 3: Generate Sync Report\nGoal: Display comprehensive report of all issues found\n\nActions:\n- Write report to `.claude/sync-report.md`:\n\n```markdown\n# Planning Sync Report\nGenerated: [TIMESTAMP]\n\n## üìä Summary\n\n| Source | Count | Status |\n|--------|-------|--------|\n| roadmap/features.json | [X] features | ‚úÖ |\n| specs/features/ | [Y] specs | [‚ö†Ô∏è if X != Y] |\n| project.json (infra.existing) | [A] items | ‚úÖ |\n| project.json (infra.needed) | [B] items | ‚úÖ |\n| specs/infrastructure/ | [C] specs | [‚ö†Ô∏è if A+B != C] |\n\n## üîç Discrepancies Found\n\n### Features Issues ([N] total)\n\n#### Missing Specs ([N])\nFeatures in roadmap/features.json without spec directories:\n- F0XX: [Feature Name]\n- F0YY: [Feature Name]\n\n#### Extra Specs ([N])\nSpec directories without roadmap/features.json entries:\n- specs/features/phase-X/F0ZZ-feature-name/\n\n#### Status Mismatches ([N])\nFeatures marked completed but tasks incomplete:\n- F0AA: [Feature Name] - 3/10 tasks incomplete\n\n### Infrastructure Issues ([N] total)\n\n#### Missing Infrastructure Specs ([N])\nInfrastructure items in project.json without spec directories:\n- I0XX: [Infra Name]\n- I0YY: [Infra Name]\n\n#### Extra Infrastructure Specs ([N])\nSpec directories without project.json entries:\n- specs/infrastructure/I0ZZ-infra-name/\n\n## üîß Recommended Actions\n\n[If MODE = \"auto\"]:\n‚úÖ Auto-fix mode enabled - applying fixes automatically\n\n[If MODE = \"report\"]:\n‚ÑπÔ∏è  Report-only mode - no changes will be made\nRun with --auto-fix to apply fixes automatically\n\n[If MODE = \"interactive\"]:\n‚ö†Ô∏è  Interactive mode - will prompt before each fix\n```\n\n- Display report to user\n- Update todos\n\nPhase 4: Apply Fixes (Conditional)\nGoal: Fix sync issues based on mode\n\nActions:\n- If MODE = \"report\": Skip this phase, display report only and exit\n- If MODE = \"auto\" or MODE = \"interactive\":\n\nFor each missing spec (roadmap/features.json entry without spec dir):\n  - If MODE = \"interactive\": Ask user: \"Create spec for F0XX: [Feature Name]? (y/n)\"\n  - If yes or MODE = \"auto\":\n    * Create spec directory: specs/features/phase-[N]/F0XX-feature-slug/\n    * Generate spec.md using feature-spec-writer agent\n    * Generate tasks.md template\n    * Display: \"‚úÖ Created spec for F0XX\"\n\nFor each extra spec (spec dir without roadmap/features.json entry):\n  - If MODE = \"interactive\": Ask user: \"Add F0XX to roadmap/features.json? Extract from spec? (y/n)\"\n  - If yes or MODE = \"auto\":\n    * Read spec.md to extract feature metadata\n    * Add entry to roadmap/features.json with proper structure\n    * Display: \"‚úÖ Added F0XX to roadmap/features.json\"\n\nFor each missing infrastructure spec (project.json entry without spec dir):\n  - If MODE = \"interactive\": Ask user: \"Generate spec for I0XX: [Infra Name]? (y/n)\"\n  - If yes or MODE = \"auto\":\n    * Create spec directory: specs/infrastructure/I0XX-infra-slug/\n    * Generate spec.md from project.json metadata\n    * Generate setup.md template\n    * Display: \"‚úÖ Created infrastructure spec for I0XX\"\n\nFor each status mismatch (completed feature with incomplete tasks):\n  - Display warning: \"‚ö†Ô∏è  F0XX marked completed but tasks incomplete\"\n  - If MODE = \"interactive\": Ask user: \"Mark feature as in-progress? (y/n)\"\n  - If yes or MODE = \"auto\":\n    * Update roadmap/features.json status to \"in-progress\"\n    * Display: \"‚úÖ Updated F0XX status to in-progress\"\n\n- Update todos\n\nPhase 5: Finalize and Report\nGoal: Summary of changes made and next steps\n\nActions:\n- Count fixes applied\n- Display summary:\n  ```\n  üéâ Sync Complete!\n\n  Changes Made:\n  - ‚úÖ Created [N] missing specs\n  - ‚úÖ Added [N] features to roadmap/features.json\n  - ‚úÖ Generated [N] infrastructure specs\n  - ‚úÖ Fixed [N] status mismatches\n\n  Sync Report: .claude/sync-report.md\n  ```\n- If MODE = \"report\":\n  ```\n  ‚ÑπÔ∏è  Report generated (no changes made)\n\n  To apply fixes, run:\n  /planning:sync-all --auto-fix\n  ```\n- Mark all todos complete\n- Display: \"Sync check complete! ‚ú®\"\n\nTroubleshooting:\n- If roadmap/features.json doesn't exist: Error and suggest running /planning:extract-config first\n- If project.json doesn't exist: Error and suggest running /foundation:detect first\n- If specs/ directory doesn't exist: Error and suggest running /planning:init-project first\n\nNotes:\n- This command should be run periodically to catch drift\n- Can be added as a git pre-commit hook\n- Safe to run multiple times (idempotent)\n- Report-only mode is safe for CI/CD pipelines\n"
              },
              {
                "name": "/update-feature",
                "description": "Update existing feature across roadmap, specs, and architecture docs when requirements change",
                "path": "plugins/planning/commands/update-feature.md",
                "frontmatter": {
                  "description": "Update existing feature across roadmap, specs, and architecture docs when requirements change",
                  "argument-hint": "<spec-number> [changes] [--all]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Update an existing feature across all planning documentation when requirements, priorities, or architecture changes.\n\nCore Principles:\n- Identify scope of change: Requirements vs priority vs architecture\n- Cascade updates across all affected docs\n- Create new ADR if architecture decision changed\n- Maintain consistency across roadmap, specs, and architecture\n\n## Source of Truth: features.json\n\n**CRITICAL**: `roadmap/features.json` is the source of truth. Always read JSON first.\n\nPhase 1: Discovery\nGoal: Identify feature and understand what needs to change\n\nActions:\n- Create todo list tracking workflow phases using TodoWrite\n- Parse $ARGUMENTS for:\n  - Spec number or --all flag\n  - Change description\n  - Flags: --all (update all features with same change)\n- If --all flag present:\n  - Display: \"üîÑ Updating ALL features with change: [changes]\"\n  - Read features.json: `cat roadmap/features.json | jq '.features[].id'`\n  - Confirm with AskUserQuestion: \"Update all [count] features?\"\n- Else if spec number not provided, use AskUserQuestion to ask:\n  - Which feature needs updating? (spec number or name)\n\n**Read features.json FIRST (source of truth):**\n```bash\ncat roadmap/features.json | jq '.features[] | select(.id == \"F[NUMBER]\")'\n```\n- Extract: id, name, phase, status, priority, estimated_days, complexity, dependencies, infrastructure_dependencies\n- **VALIDATE**:\n  - phase is a STRING (MVP/Post-MVP/Beta) not a number\n  - status uses hyphens (in-progress not in_progress)\n  - priority is P0/P1/P2\n\n- Find spec directory:\n  !{bash find specs/features -type d -name \"F$SPEC_NUMBER-*\" 2>/dev/null | head -1}\n- If not found, display error and list available features\n- Load existing feature files (from discovered path):\n  - Read spec: [SPEC_DIR]/spec.md\n  - Read tasks: [SPEC_DIR]/tasks.md\n- Find feature in ROADMAP.md:\n  !{bash grep -n \"$SPEC_NUMBER\" docs/ROADMAP.md}\n\nPhase 2: Determine Change Scope\nGoal: Understand what changed and what needs updating\n\nActions:\n- Use AskUserQuestion to determine change type:\n  - What changed? (select all that apply)\n    - Requirements/scope changed\n    - Priority changed (P0 ‚Üî P1 ‚Üî P2)\n    - Timeline/phase changed\n    - Architecture/approach changed\n    - Dependencies changed\n  - Describe the changes:\n- For each change type, determine affected docs:\n  - Requirements ‚Üí spec.md, tasks.md\n  - Priority ‚Üí spec.md, ROADMAP.md\n  - Timeline ‚Üí ROADMAP.md, tasks.md\n  - Architecture ‚Üí spec.md, docs/architecture/*, new ADR\n  - Dependencies ‚Üí spec.md, ROADMAP.md\n- Ask: Does this change require a new architecture decision? (Yes/No)\n\nPhase 3: Update Spec\nGoal: Update feature specification\n\nActions:\n\nTask(description=\"Update spec\", subagent_type=\"planning:feature-spec-writer\", prompt=\"Update spec [NUMBER]-[NAME] with changes: [from Phase 2]. Read specs/features/[NUMBER]-*/spec.md and tasks.md. Apply changes (requirements/priority/timeline/architecture/dependencies). Maintain minimal format (100-150 lines). Preserve architecture references.\")\n\nUpdate todos\n\nPhase 4: Update Roadmap\nGoal: Update roadmap\n\nActions:\n- If Priority/Timeline/Dependencies changed:\n  Task(description=\"Update roadmap\", subagent_type=\"planning:roadmap-planner\", prompt=\"Update docs/ROADMAP.md for spec [NUMBER]: Priority [old‚Üínew], Timeline [old‚Üínew], Dependencies [old‚Üínew], Phase [old‚Üínew]. Read ROADMAP.md, find feature, apply changes, update gantt if needed, recalculate totals.\")\n- Update todos\n\nPhase 5: Create ADR (if needed)\nGoal: Document decision change\n\nActions:\n- If architecture decision changed:\n  Task(description=\"Create ADR\", subagent_type=\"planning:decision-documenter\", prompt=\"Create ADR for spec [NUMBER] decision change: [from Phase 2]. Document old‚Üínew approach, rationale, impact, consequences. Reference previous ADR if superseding. Create docs/adr/[NUMBER]-[slug].md.\")\n- Update todos\n\nPhase 6: Update Architecture (if needed)\nGoal: Update architecture docs\n\nActions:\n- If architecture approach changed:\n  Task(description=\"Update architecture\", subagent_type=\"planning:architecture-designer\", prompt=\"Update docs/architecture/ for spec [NUMBER] changes: [from Phase 2]. Read affected files, update sections, update diagrams, cross-reference spec and ADR.\")\n- Update todos\n\nPhase 7: Update Mem0\nGoal: Update stored relationships\n\nActions:\n- Run doc-sync: !{bash python plugins/planning/skills/doc-sync/scripts/update-relationships.py --spec [NUMBER]}\n- Update todos\n\nPhase 8: Summary\nGoal: Report results\n\nActions:\n- Mark all todos complete\n- Display:\n  - Feature: [NUMBER]-[NAME]\n  - Changes: Spec (updated), Roadmap (if changed), ADR (if created), Architecture (if updated)\n  - Before‚ÜíAfter: Priority [old‚Üínew], Timeline [old‚Üínew], Dependencies [old‚Üínew]\n- Show changes: !{bash git status --short specs/features/[NUMBER]-* docs/ROADMAP.md docs/adr/ docs/architecture/ 2>/dev/null}\n- Next steps: Review (git diff), sync code (/iterate:sync if needed), commit"
              },
              {
                "name": "/view-docs",
                "description": "Launch visual documentation registry viewer",
                "path": "plugins/planning/commands/view-docs.md",
                "frontmatter": {
                  "description": "Launch visual documentation registry viewer",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Launch the documentation registry web viewer to visualize all documentation relationships\n\n## Phase 1: Check Prerequisites\n\nGoal: Verify viewer components exist\n\nActions:\n- Check if API server exists:\n  !{bash test -f ~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/doc-sync/scripts/serve-viewer.py && echo \"‚úÖ API server found\" || echo \"‚ùå API server missing\"}\n- Check if HTML viewer exists:\n  !{bash test -f ~/.claude/doc-viewer.html && echo \"‚úÖ Viewer found\" || echo \"‚ùå Viewer missing\"}\n- Check if mem0 venv exists:\n  !{bash test -d /tmp/mem0-env && echo \"‚úÖ Mem0 environment ready\" || echo \"‚ùå Mem0 not installed\"}\n\n## Phase 2: Launch Viewer\n\nGoal: Start API server and open viewer in browser\n\nActions:\n- Launch viewer using script:\n  !{bash ~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/plugins/planning/skills/doc-sync/scripts/view-docs.sh}\n\nThis will:\n1. Start API server on http://localhost:8765\n2. Open viewer HTML in your browser\n3. Display all documentation relationships\n\nPress Ctrl+C to stop the server when done.\n\n## Phase 3: Usage Instructions\n\nGoal: Explain how to use the viewer\n\nActions:\n- Display viewer features:\n  - **Graph View**: Visual network of all documentation relationships\n    - Specs (green nodes)\n    - Architecture docs (blue nodes)\n    - ADRs (orange nodes)\n    - Click and drag to explore\n    - Hover over nodes for details\n\n  - **List View**: Organized list of all documentation\n    - Stats showing counts\n    - Expandable sections\n    - Full memory text visible\n\n  - **Project Selector**: Switch between different projects (dropdown at top)\n\n- Suggest workflow:\n  - Use Graph View to understand overall structure\n  - Use List View for detailed reading\n  - Keep viewer open while working on specs\n  - Refresh browser to see latest changes after running sync\n\n## Phase 4: Summary\n\nGoal: Confirm viewer launched\n\nActions:\n- Display:\n  - \"‚úÖ Documentation viewer launched\"\n  - \"üåê API: http://localhost:8765\"\n  - \"üìä Viewer: file://~/.claude/doc-viewer.html\"\n  - \"‚èπÔ∏è  Press Ctrl+C in terminal to stop server\""
              },
              {
                "name": "/wizard",
                "description": "Interactive multimodal wizard for comprehensive requirements gathering and spec generation",
                "path": "plugins/planning/commands/wizard.md",
                "frontmatter": {
                  "description": "Interactive multimodal wizard for comprehensive requirements gathering and spec generation",
                  "argument-hint": [
                    "--auto-continue"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Conduct interactive wizard to gather requirements, process multimodal inputs, generate architecture docs, and create feature specs ready for implementation.\n\nCore Principles:\n- Multimodal first: Accept files, images, URLs, and text\n- Progressive disclosure: Ask targeted questions based on inputs\n- Comprehensive planning: Generate architecture before specs\n- Automation: Script creates structure, agents fill content\n- Infrastructure via plugins: No infra specs (plugins handle that)\n\nPhase 1: Welcome and Initial Context\nGoal: Gather project description and multimodal inputs\n\nActions:\n- Display welcome message\n- Create todo list tracking workflow phases\n- Ask user for project description and file uploads:\n  - \"What would you like to build?\"\n  - \"Upload any files (wireframes, docs, code, URLs) to help me understand:\"\n    - Screenshots/wireframes (PNG, JPG)\n    - Requirements docs (PDF, Word, Markdown)\n    - Existing code (zip, GitHub URL)\n    - Competitor sites (URLs)\n- Store initial description in `.wizard/initial-request.md`\n- If user provides file paths or URLs, process them next phase\n\nPhase 2: Process Multimodal Inputs (if provided)\nGoal: Extract requirements from uploaded materials\n\nActions:\n- If user provided files/images/URLs, invoke requirements-processor:\n\nTask(description=\"Process uploaded inputs\", subagent_type=\"planning:requirements-processor\", prompt=\"You are the requirements-processor agent. Process all uploaded files, images, and URLs provided by the user.\n\nInputs to process: $USER_PROVIDED_FILES_AND_URLS\n\nExtract:\n- Features and capabilities\n- User stories and workflows\n- Technical constraints\n- Integration requirements\n- Data entities\n- UI components\n\nReturn structured JSON with:\n- extracted.features\n- extracted.user_stories\n- extracted.technical_constraints\n- extracted.integrations\n- extracted.data_entities\n- confidence scores\n\nDeliverable: Complete extraction report in JSON format\")\n\n- Save extracted data to `.wizard/extracted-requirements.json`\n- Update todos\n\nPhase 3: Structured Q&A Rounds\nGoal: Gather additional context through targeted questions\n\nActions:\n- Conduct 6-8 rounds using AskUserQuestion covering:\n  - Project type and users\n  - Core features (MVP vs. future)\n  - Technical stack and integrations\n  - Constraints (timeline, budget, team)\n  - Success metrics and KPIs\n- Save all Q&A to `docs/requirements/YYYY-MM-DD-project/02-wizard-qa.md`\n- Update todos\n\nPhase 4: Generate Architecture (BATCHED PARALLEL)\nGoal: Create architecture docs using batched parallel agents for speed and UI stability\n\n‚ö†Ô∏è CONSTRAINT: Maximum 10-12 agents per batch (UI breaks with >10)\n\nActions:\n**Batch 1 (8 agents)**: Launch architecture + ADRs + roadmap + page specs\n- Launch IN PARALLEL (one message, multiple Task calls):\n  Task 1: Generate README.md + backend.md (system overview, Claude Agent SDK, MCP)\n  Task 2: Generate data.md + ai.md (Supabase schema, AI architecture)\n  Task 3: Generate security.md + integrations.md (auth, API keys, external services)\n  Task 4: Generate infrastructure.md + frontend.md (deployment, component patterns)\n  Task 5: Generate application-pages.md (interactive app pages: dashboard, settings, chat, admin)\n  Task 6: Generate website-pages.md (static/marketing pages: landing, pricing, about, blog)\n  Task 7: Generate ADRs (decision-documenter agent)\n  Task 8: Generate ROADMAP.md (roadmap-planner agent)\n\n- Each agent receives same context:\n  - Wizard requirements: docs/requirements/\n  - Extracted data: .wizard/extracted-requirements.json\n  - Q&A: docs/requirements/*/02-wizard-qa.md\n\n- Agents work simultaneously (8 agents, UI-safe batch size)\n- Verify all 10 architecture files + ADRs + roadmap created\n- Update todos\n\nPhase 4.5: Validate Architecture (CTO Review)\nGoal: Multi-tier validation of generated architecture\n\nActions:\n- Launch validator agents IN PARALLEL:\n\n  Task 1: Technical validator (checks completeness, diagrams, security)\n  Task 2: Cost validator (verifies budget constraints, estimates costs)\n  Task 3: Timeline validator (confirms 1-3 month aggressive timeline feasible)\n\n- Each validator outputs: validation-report-[type].md\n- Launch CTO-level review agent:\n\n  Task: CTO reviewer reads all architecture + validation reports\n  - Identifies gaps, inconsistencies, risks\n  - Provides executive summary\n  - Outputs: docs/architecture/CTO-REVIEW.md\n\n- If critical issues found: Display and ask user to proceed or regenerate\n- Update todos\n\nPhase 5: Final Architecture Validation\nGoal: Validate architecture planning is complete and ready for spec generation\n\nActions:\n- Launch CTO reviewer for architecture approval:\n\n  Task: CTO reviewer reads architecture planning package\n  - All 10 architecture files (README.md, backend.md, frontend.md, data.md, ai.md, infrastructure.md, security.md, integrations.md, application-pages.md, website-pages.md)\n  - All ADRs and ROADMAP.md\n  - project.json (tech stack configuration)\n  - roadmap/features.json (feature breakdown)\n  - Previous validation reports (if any warnings)\n  - Wizard requirements and Q&A\n\n  CTO validates:\n  - Architecture is complete and coherent\n  - Tech stack choices are appropriate\n  - Features are well-defined and scoped\n  - Infrastructure components identified\n  - Security considerations documented\n  - Plan is ready for spec generation\n\n  Output: docs/FINAL-APPROVAL.md\n  - Status: APPROVED | APPROVED_WITH_CHANGES | REJECTED\n  - Executive summary of architecture plan\n  - Critical issues (blockers)\n  - Warnings (should fix before spec generation)\n  - Recommendations (optional)\n  - Go/no-go decision for proceeding to spec generation\n\n- If REJECTED: Display issues and ask user to regenerate architecture\n- If APPROVED_WITH_CHANGES: Display warnings and ask user to proceed or fix\n- If APPROVED: Continue to finalization\n- Update todos\n\nPhase 6: Finalization\nGoal: Complete wizard and prepare for spec generation\n\nActions:\n- Update ROADMAP.md with final approval status\n- Create summary report:\n  - Features defined: X\n  - Total estimated time: Y days\n  - Architecture docs created:\n    * docs/architecture/README.md\n    * docs/architecture/backend.md\n    * docs/architecture/frontend.md\n    * docs/architecture/data.md\n    * docs/architecture/ai.md\n    * docs/architecture/infrastructure.md\n    * docs/architecture/security.md\n    * docs/architecture/integrations.md\n  - ADRs: Z decisions documented\n  - Approval status: APPROVED (from Phase 5)\n  - Next steps: Run /planning:extract-config to create roadmap/project.json and roadmap/features.json\n\n- Save summary to `.wizard/completion-summary.md`\n- Mark all todos complete\n\nPhase 7: Summary\nGoal: Display results and next steps\n\nActions:\n- Display completion message:\n  ```\n  ‚úÖ Architecture Planning Complete!\n\n  Created Architecture Documentation:\n  - docs/requirements/ (wizard Q&A inputs)\n  - docs/architecture/README.md (system overview)\n  - docs/architecture/backend.md (backend architecture)\n  - docs/architecture/frontend.md (frontend architecture)\n  - docs/architecture/data.md (database schema)\n  - docs/architecture/ai.md (AI stack architecture)\n  - docs/architecture/infrastructure.md (infrastructure components)\n  - docs/architecture/security.md (security architecture)\n  - docs/architecture/integrations.md (external integrations)\n  - docs/adr/*.md (architectural decision records)\n  - docs/ROADMAP.md (project roadmap)\n\n  Next Steps:\n  1. Review docs/FINAL-APPROVAL.md for validation results\n  2. Run /planning:extract-config to create roadmap/project.json and roadmap/features.json\n  3. Run /planning:init-project to generate feature specs\n  4. Run /foundation:generate-infrastructure-specs for infrastructure specs\n  5. Begin implementation following the specs\n  ```\n\n- Update todos to completed"
              }
            ],
            "skills": [
              {
                "name": "Architecture Patterns",
                "description": "Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.",
                "path": "plugins/planning/_archive/skills/architecture-patterns/SKILL.md",
                "frontmatter": {
                  "name": "Architecture Patterns",
                  "description": "Architecture design templates, mermaid diagrams, documentation patterns, and validation tools. Use when designing system architecture, creating architecture documentation, generating mermaid diagrams, documenting component relationships, designing data flows, planning deployments, creating API architectures, or when user mentions architecture diagrams, system design, mermaid, architecture documentation, or component design.",
                  "allowed-tools": null
                },
                "content": "# Architecture Patterns Skill\n\n**CRITICAL: The description field above controls when Claude auto-loads this skill.**\n\n## Overview\n\nProvides comprehensive architecture design capabilities including mermaid diagram generation, architecture documentation templates, diagram validation, and pattern libraries for common architectural styles (microservices, RAG systems, full-stack applications).\n\n## Instructions\n\n### Create Architecture Documentation\n\n1. Use `bash scripts/create-architecture.sh <project-path> <architecture-type>` to scaffold architecture docs\n2. Architecture types: `nextjs`, `fastapi`, `fullstack`, `microservices`, `rag`, `generic`\n3. Generates complete architecture overview with mermaid diagrams\n4. Creates directory structure: `docs/architecture/` with overview, components, data-flow, deployment\n5. Includes table of contents and cross-references\n\n### Validate Mermaid Diagrams\n\n1. Use `bash scripts/validate-mermaid.sh <markdown-file>` to check mermaid syntax\n2. Validates diagram types: graph, flowchart, sequenceDiagram, classDiagram, erDiagram, stateDiagram\n3. Checks for syntax errors, invalid node definitions, broken connections\n4. Reports line numbers of errors\n5. Provides suggestions for common fixes\n\n### Generate Diagram Placeholders\n\n1. Use `bash scripts/generate-diagrams.sh <output-dir> <diagram-types>` to create diagram templates\n2. Diagram types: `component`, `data-flow`, `deployment`, `api`, `security`, `all`\n3. Creates markdown files with properly formatted mermaid code blocks\n4. Includes comments explaining diagram sections\n5. Provides example nodes and relationships\n\n### Update Existing Architecture\n\n1. Use `bash scripts/update-architecture.sh <architecture-file> <section>` to add new sections\n2. Sections: `component`, `api`, `security`, `deployment`, `data-flow`\n3. Inserts section with proper heading hierarchy\n4. Adds mermaid diagram placeholder\n5. Preserves existing content and formatting\n\n### Export Diagrams to Files\n\n1. Use `bash scripts/export-diagrams.sh <markdown-file> <output-dir>` to extract diagrams\n2. Extracts all mermaid code blocks from documentation\n3. Creates individual `.mmd` files for each diagram\n4. Names files based on diagram titles or section headings\n5. Generates index.md listing all exported diagrams\n\n## Available Scripts\n\n- **create-architecture.sh**: Scaffold complete architecture documentation with diagrams\n- **validate-mermaid.sh**: Validate mermaid diagram syntax and structure\n- **generate-diagrams.sh**: Create diagram template placeholders\n- **update-architecture.sh**: Add new sections to existing architecture docs\n- **export-diagrams.sh**: Extract mermaid diagrams to separate files\n\n## Templates\n\n- **architecture-overview.md**: Master architecture document template with TOC\n- **component-diagram.md**: Component architecture with relationships\n- **data-flow-diagram.md**: Data flow and processing pipelines\n- **deployment-diagram.md**: Infrastructure and deployment architecture\n- **api-architecture.md**: API design, endpoints, and authentication\n- **security-architecture.md**: Security patterns, auth flows, data protection\n\n## Examples\n\nSee `examples/` directory for detailed usage examples:\n- `example-nextjs-architecture.md` - Next.js 15 App Router architecture\n- `example-fastapi-architecture.md` - FastAPI backend with PostgreSQL\n- `example-fullstack-architecture.md` - Full stack Next.js + FastAPI\n- `example-ai-rag-architecture.md` - RAG system with vector database\n- `example-microservices-architecture.md` - Microservices pattern with API gateway\n\n## Architecture Patterns\n\n### Component Architecture Pattern\n- Define system components and boundaries\n- Show component relationships and dependencies\n- Identify shared services and libraries\n- Document component responsibilities\n\n### Data Flow Pattern\n- Map data movement through system\n- Show transformation and processing stages\n- Identify data sources and destinations\n- Document data formats and protocols\n\n### Deployment Pattern\n- Define infrastructure components\n- Show service deployment topology\n- Identify scaling and redundancy strategies\n- Document environment configurations\n\n### API Architecture Pattern\n- Design API structure and endpoints\n- Define authentication and authorization\n- Show request/response flows\n- Document rate limiting and caching\n\n### Security Architecture Pattern\n- Define security layers and boundaries\n- Show authentication and authorization flows\n- Identify threat vectors and mitigations\n- Document encryption and data protection\n\n## Mermaid Diagram Types\n\n### Graph/Flowchart Diagrams\n```mermaid\ngraph TD\n    A[Start] --> B{Decision}\n    B -->|Yes| C[Action 1]\n    B -->|No| D[Action 2]\n```\n\n### Sequence Diagrams\n```mermaid\nsequenceDiagram\n    Client->>API: Request\n    API->>Database: Query\n    Database-->>API: Result\n    API-->>Client: Response\n```\n\n### Class Diagrams\n```mermaid\nclassDiagram\n    class User {\n        +String name\n        +login()\n    }\n```\n\n### Entity Relationship Diagrams\n```mermaid\nerDiagram\n    USER ||--o{ ORDER : places\n    ORDER ||--|{ ITEM : contains\n```\n\n### State Diagrams\n```mermaid\nstateDiagram-v2\n    [*] --> Idle\n    Idle --> Processing\n    Processing --> Complete\n```\n\n## Output Standards\n\n- Use mermaid for all diagrams (ensures renderability)\n- Include diagram titles and descriptions\n- Add comments explaining key components\n- Follow consistent naming conventions\n- Use proper markdown heading hierarchy\n- Include cross-references between documents\n\n## Validation Checks\n\nScripts perform these validations:\n- Mermaid syntax correctness\n- Node and edge definitions\n- Diagram type compatibility\n- Character escaping in labels\n- Proper code block formatting\n- Complete relationship definitions\n\n## Integration\n\nThis skill is used by:\n- `planning:architecture` command - Generate architecture docs\n- `architecture-designer` agent - Create system designs\n- Documentation tools - Include architecture diagrams\n- Code generation - Architecture-aware scaffolding\n\n## Best Practices\n\n1. Start with high-level overview diagram\n2. Create separate diagrams for each architectural concern\n3. Use consistent component naming across diagrams\n4. Include legends for symbols and colors\n5. Keep diagrams focused and readable (max 15-20 nodes)\n6. Document assumptions and constraints\n7. Version architecture documents\n8. Update diagrams when system changes\n\n## Common Diagram Patterns\n\n### Layered Architecture\n```mermaid\ngraph TB\n    UI[Presentation Layer]\n    BL[Business Logic Layer]\n    DA[Data Access Layer]\n    DB[(Database)]\n    UI --> BL\n    BL --> DA\n    DA --> DB\n```\n\n### Event-Driven Architecture\n```mermaid\ngraph LR\n    P[Producer] -->|Event| Q[Event Queue]\n    Q -->|Event| C1[Consumer 1]\n    Q -->|Event| C2[Consumer 2]\n```\n\n### Microservices Architecture\n```mermaid\ngraph TB\n    AG[API Gateway]\n    AG --> MS1[Service 1]\n    AG --> MS2[Service 2]\n    AG --> MS3[Service 3]\n    MS1 --> DB1[(DB 1)]\n    MS2 --> DB2[(DB 2)]\n```\n\n---\n\n**Purpose**: Comprehensive architecture design and documentation\n**Used by**: Architecture designers, system planners, documentation tools"
              },
              {
                "name": "build-manifest",
                "description": "Templates and scripts for generating layered BUILD-GUIDE.json/md from Airtable plugin index - shows available commands organized by infrastructure layers",
                "path": "plugins/planning/_archive/skills/build-manifest/SKILL.md",
                "frontmatter": {
                  "name": "build-manifest",
                  "description": "Templates and scripts for generating layered BUILD-GUIDE.json/md from Airtable plugin index - shows available commands organized by infrastructure layers"
                },
                "content": "# Build Manifest Generation\n\nThis skill provides templates and scripts for generating **BUILD-GUIDE** files (both JSON and Markdown) that show available commands organized by infrastructure layers.\n\n## Purpose\n\nThe BUILD-GUIDE is a **layered execution blueprint** that:\n- Queries Airtable (as an index to marketplace plugins)\n- Identifies available commands for the project's tech stack\n- Organizes commands into sequential execution layers\n- Detects gaps where tech is mentioned but no plugin exists\n- Provides both machine-parseable (JSON) and human-readable (Markdown) formats\n\n## When to Use This Skill\n\nUse this skill when:\n- Initializing a new project after `/planning:wizard` completes\n- Architecture docs exist with tech stack identified\n- Need to generate a build execution plan\n- Want to see what commands are available for the detected stack\n- Need to identify missing plugins for technologies mentioned in specs\n\n## Layer Structure\n\nBUILD-GUIDE organizes commands into sequential layers:\n\n### Layer 1: Infrastructure Foundation\n- Core project setup (detection, specs, worktrees)\n- Same for most projects\n- Plugins: `foundation`, `planning`, `supervisor`\n\n### Layer 2: Tech Stack Initialization\n- Initialize detected frameworks\n- Dynamic based on architecture docs\n- Plugins: Project-specific (nextjs-frontend, fastapi-backend, supabase, etc.)\n\n### Layer 3: Feature Implementation\n- Build features from specs\n- Reference spec files\n- Use commands + agents from detected plugins\n\n### Layer 4: Quality & Deployment\n- Testing, validation, deployment\n- Plugins: `quality`, `deployment`, `versioning`\n\n## Templates\n\n### JSON Template\nLocation: `templates/BUILD-GUIDE.json.template`\n\nShows the complete JSON structure with:\n- Tech stack detection results\n- Layered command organization\n- Gap detection for missing plugins\n- Available/unavailable status per command\n\n### Markdown Template\nLocation: `templates/BUILD-GUIDE.md.template`\n\nHuman-readable version with:\n- Tech stack summary\n- Commands grouped by layer\n- Usage examples\n- Gap warnings\n\n## Scripts\n\n### generate-manifest.py\nLocation: `scripts/generate-manifest.py`\n\n**Purpose**: Query Airtable and generate BUILD-GUIDE files\n\n**Usage**:\n```bash\npython scripts/generate-manifest.py \\\n  --architecture docs/architecture/README.md \\\n  --output BUILD-GUIDE\n```\n\n**Process**:\n1. Read architecture docs to detect tech stack\n2. Query Airtable for plugins matching detected technologies\n3. Query Airtable for commands in those plugins\n4. Organize commands into layers\n5. Detect gaps (tech mentioned but no plugin exists)\n6. Generate both .json and .md files\n\n**Requirements**:\n- AIRTABLE_TOKEN environment variable\n- Architecture docs must exist\n- Airtable base ID: appHbSB7WhT1TxEQb\n\n## Examples\n\n### Example Output Structure\n\nSee `examples/BUILD-GUIDE.json` for complete Next.js + FastAPI + Supabase example\nSee `examples/BUILD-GUIDE.md` for markdown version\n\n### Example Gap Detection\n\n```json\n{\n  \"gaps\": [\n    {\n      \"technology\": \"Redis\",\n      \"mentioned_in\": \"docs/architecture/caching.md\",\n      \"reason\": \"No redis plugin found in any marketplace\",\n      \"suggestion\": \"Create redis plugin with /domain-plugin-builder:build-plugin redis\"\n    }\n  ]\n}\n```\n\n## Integration with build-manifest-generator Agent\n\nThe `build-manifest-generator` agent in this plugin uses this skill:\n\n```markdown\n!{skill planning:build-manifest}\n```\n\nThe agent:\n1. Invokes this skill to load templates\n2. Uses scripts to query Airtable\n3. Generates BUILD-GUIDE.json and BUILD-GUIDE.md\n4. Places files at project root\n\n## Output Files\n\n**BUILD-GUIDE.json**:\n- Machine-parseable\n- Complete structure with metadata\n- Used by Claude when user requests features\n- Location: Project root\n\n**BUILD-GUIDE.md**:\n- Human-readable\n- Summary format\n- Documentation reference\n- Location: Project root\n\n## Security\n\nAll templates use placeholder format:\n- `your_service_key_here` for API keys\n- Environment variable references in code examples\n- No hardcoded credentials\n\n## Validation\n\nThe skill validates:\n- Architecture docs exist and are readable\n- Airtable connection successful\n- All detected plugins have valid commands\n- Generated JSON is valid\n- Generated Markdown renders correctly\n\n## Usage Pattern\n\nTypical workflow:\n```bash\n# 1. Planning wizard creates architecture docs\n/planning:wizard\n\n# 2. Generate build manifest\n/planning:generate-build-guide  # Uses this skill\n\n# 3. Reference during development\n# Claude reads BUILD-GUIDE.json when user says \"add authentication\"\n# Claude sees: /supabase:add-auth available\n```\n\n## Maintenance\n\nWhen adding new plugins to marketplaces:\n1. Plugin metadata syncs to Airtable automatically (via sync scripts)\n2. Regenerate BUILD-GUIDE for existing projects: `/planning:generate-build-guide --refresh`\n3. New commands appear in Layer 2 (Tech Stack Initialization)"
              },
              {
                "name": "Decision Tracking",
                "description": "Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.",
                "path": "plugins/planning/_archive/skills/decision-tracking/SKILL.md",
                "frontmatter": {
                  "name": "Decision Tracking",
                  "description": "Architecture Decision Records (ADR) templates, sequential numbering, decision documentation patterns, and decision history management. Use when creating ADRs, documenting architectural decisions, tracking decision rationale, managing decision lifecycle, superseding decisions, searching decision history, or when user mentions ADR, architecture decision, decision record, decision tracking, or decision documentation.",
                  "allowed-tools": null
                },
                "content": "# Decision Tracking Skill\n\n**CRITICAL: The description field above controls when Claude auto-loads this skill.**\n\n## Overview\n\nProvides comprehensive Architecture Decision Record (ADR) management following Michael Nygard's ADR format. Includes automatic sequential numbering, decision lifecycle tracking, superseding workflows, and decision search capabilities.\n\n## Instructions\n\n### Creating New ADRs\n\n1. Use `scripts/create-adr.sh <title> [docs-path]` to create a new ADR with automatic numbering\n2. Script automatically determines next sequential number (0001, 0002, etc.)\n3. Creates ADR file with Michael Nygard format: `NNNN-title-in-kebab-case.md`\n4. Populates ADR with proper frontmatter (date, status, deciders)\n5. Updates the ADR index automatically\n\n### Listing ADRs\n\n1. Use `scripts/list-adrs.sh [docs-path] [--status=accepted|proposed|deprecated|superseded]` to view all ADRs\n2. Displays ADR number, title, status, date, and file path\n3. Supports filtering by status: accepted, proposed, deprecated, superseded\n4. Shows ADRs in chronological order by number\n5. Optionally displays quick summary of each ADR\n\n### Searching ADRs\n\n1. Use `scripts/search-adrs.sh <search-term> [docs-path]` to search ADR content\n2. Searches titles, context, decisions, and consequences sections\n3. Returns matching ADRs with relevant snippets\n4. Highlights search terms in results\n5. Supports regex patterns for advanced searches\n\n### Updating ADR Index\n\n1. Use `scripts/update-adr-index.sh [docs-path]` to regenerate ADR index\n2. Scans all ADR files and extracts metadata\n3. Generates comprehensive index with links to all ADRs\n4. Groups ADRs by status (accepted, proposed, deprecated, superseded)\n5. Updates `docs/adr/index.md` or specified path\n\n### Superseding ADRs\n\n1. Use `scripts/supersede-adr.sh <old-adr-number> <new-title> [docs-path]` to supersede an ADR\n2. Marks old ADR status as \"superseded\" with link to new ADR\n3. Creates new ADR with reference to superseded ADR\n4. Maintains decision history and rationale chain\n5. Updates ADR index automatically\n\n## Available Scripts\n\n- **create-adr.sh**: Create new ADR with auto-numbering and proper format\n- **list-adrs.sh**: List all ADRs with filtering and status display\n- **search-adrs.sh**: Search ADR content with regex support\n- **update-adr-index.sh**: Regenerate comprehensive ADR index\n- **supersede-adr.sh**: Mark ADR as superseded and create replacement\n\n## Templates\n\n- **adr-template.md**: Michael Nygard ADR format with all sections\n- **adr-frontmatter.yaml**: YAML frontmatter structure for ADR metadata\n- **adr-index-template.md**: ADR index format with status groupings\n- **decision-matrix.md**: Decision comparison matrix for evaluating options\n- **consequences-template.md**: Detailed consequences documentation format\n\n## Examples\n\nSee `examples/` directory for detailed usage examples:\n- `example-adr-technology.md` - Technology choice ADR (database selection)\n- `example-adr-architecture.md` - Architectural decision (microservices vs monolith)\n- `example-adr-security.md` - Security decision (authentication strategy)\n- `example-adr-superseded.md` - Superseded ADR with replacement links\n- `example-adr-index.md` - Complete ADR index with multiple entries\n\n## ADR Format (Michael Nygard)\n\n### Standard Sections\n\n1. **Title**: Short noun phrase describing the decision\n2. **Status**: proposed | accepted | deprecated | superseded\n3. **Context**: Forces at play, including technological, political, social, and project constraints\n4. **Decision**: Response to these forces, stated in full sentences with active voice\n5. **Consequences**: Context after applying the decision, including positive, negative, and neutral effects\n\n### Frontmatter Fields\n\n```yaml\n---\nnumber: 0001\ntitle: Use PostgreSQL for Primary Database\ndate: 2025-10-28\nstatus: accepted\ndeciders: [Tech Lead, Backend Team]\nconsulted: [DevOps, Security Team]\ninformed: [Frontend Team, Product]\n---\n```\n\n### Numbering Convention\n\n- Use 4-digit zero-padded sequential numbers: 0001, 0002, 0003, etc.\n- Filename format: `NNNN-title-in-kebab-case.md`\n- Examples: `0001-use-postgresql.md`, `0042-adopt-microservices.md`\n- Never reuse numbers even if ADR is deleted\n\n## Decision Lifecycle\n\n### Status Transitions\n\n1. **proposed** ‚Üí Initial state when ADR is created\n2. **accepted** ‚Üí Decision has been approved and implemented\n3. **deprecated** ‚Üí Decision is no longer recommended but still in use\n4. **superseded** ‚Üí Decision has been replaced by a newer ADR\n\n### Superseding Workflow\n\n1. Identify ADR to supersede (e.g., ADR-0005)\n2. Run `supersede-adr.sh 0005 \"New Decision Title\"`\n3. Old ADR updated: status ‚Üí \"superseded\", link added to new ADR\n4. New ADR created with reference to superseded ADR\n5. Index updated automatically\n\n## ADR Storage Structure\n\nRecommended directory structure:\n```\ndocs/\n  adr/\n    index.md              # Master index of all ADRs\n    0001-first-decision.md\n    0002-second-decision.md\n    0003-third-decision.md\n    templates/\n      adr-template.md     # Template for new ADRs\n```\n\n## Decision Matrix Usage\n\nWhen evaluating multiple options:\n1. Use `templates/decision-matrix.md` to structure comparison\n2. Define criteria (performance, cost, maintainability, etc.)\n3. Score each option against criteria\n4. Weight criteria by importance\n5. Calculate weighted scores to guide decision\n6. Include completed matrix in ADR context section\n\n## Search Capabilities\n\nThe search script supports:\n- **Full-text search**: Search all ADR content\n- **Regex patterns**: Use patterns like `\"auth.*strategy\"`\n- **Section-specific**: Search only in specific sections\n- **Status filtering**: Combine with status filter\n- **Date range**: Search ADRs within date range\n\n## Integration\n\nThis skill is used by:\n- `planning:adr-create` command - Create new ADRs interactively\n- `planning:adr-list` command - List and filter ADRs\n- `planning:adr-supersede` command - Supersede existing ADRs\n- All planning agents requiring decision documentation\n\n## Best Practices\n\n### Writing Effective ADRs\n\n1. **Be specific**: Clearly state what is being decided\n2. **Document context**: Explain why the decision is needed\n3. **List alternatives**: Show what options were considered\n4. **Describe consequences**: Include positive and negative impacts\n5. **Use active voice**: \"We will use PostgreSQL\" not \"PostgreSQL will be used\"\n\n### When to Create ADRs\n\n- Choosing between architectural patterns (monolith vs microservices)\n- Selecting core technologies (databases, frameworks, languages)\n- Defining system boundaries and interfaces\n- Establishing security or authentication strategies\n- Setting coding standards or development practices\n\n### When NOT to Create ADRs\n\n- Routine bug fixes or minor refactoring\n- Implementing already-decided features\n- Temporary workarounds or experiments\n- Decisions that can be easily reversed\n- Team process decisions (use meeting notes instead)\n\n## Output Format\n\nAll scripts output in consistent formats:\n- **list-adrs.sh**: Table format with columns: Number | Title | Status | Date\n- **search-adrs.sh**: List format with ADR number, title, and matching snippet\n- **create-adr.sh**: Outputs path to created ADR file\n- **supersede-adr.sh**: Outputs paths to both old and new ADR files\n\n## Requirements\n\n- ADRs must follow Michael Nygard format exactly\n- Sequential numbering must be maintained without gaps\n- Frontmatter must include all required fields\n- Status must be one of: proposed, accepted, deprecated, superseded\n- Superseded ADRs must link to replacement ADRs\n- Index must be updated after every ADR creation\n\n---\n\n**Purpose**: Comprehensive Architecture Decision Record management and documentation\n**Used by**: All planning agents and commands requiring decision tracking"
              },
              {
                "name": "doc-sync",
                "description": "Documentation synchronization using Mem0 for tracking relationships between specs, architecture, ADRs, and roadmap",
                "path": "plugins/planning/_archive/skills/doc-sync/SKILL.md",
                "frontmatter": {
                  "name": "doc-sync",
                  "description": "Documentation synchronization using Mem0 for tracking relationships between specs, architecture, ADRs, and roadmap",
                  "tags": [
                    "documentation",
                    "mem0",
                    "sync",
                    "relationships",
                    "tracking"
                  ]
                },
                "content": "# Documentation Sync Skill\n\n## Overview\n\nThis skill provides tools and scripts for intelligently synchronizing documentation across the dev-lifecycle-marketplace using Mem0 for relationship tracking.\n\n**Purpose:** Keep specs, architecture docs, ADRs, and roadmap interconnected and updated when dependencies change.\n\n## Key Concepts\n\n### Documentation Types\n- **Specs** (`specs/{number}-{name}/spec.md`) - Feature specifications derived from architecture\n- **Architecture** (`docs/architecture/*.md`) - System design and component specifications\n- **ADRs** (`docs/adr/*.md`) - Architecture Decision Records\n- **Roadmap** (`docs/ROADMAP.md`) - Project timeline and milestones\n\n### Relationship Tracking with Mem0\n\nUses Mem0 OSS (in-memory Qdrant) to store natural language relationships:\n\n```python\n# Example memory\n\"Specification 001 (user-authentication) is derived from\narchitecture/security.md sections #authentication and #jwt-tokens,\nand references ADR-0008 OAuth decision\"\n```\n\n### Benefits Over JSON/Database\n- ‚úÖ Natural language queries: \"What specs depend on security.md?\"\n- ‚úÖ No complex schemas or parsing\n- ‚úÖ Easy to understand and modify\n- ‚úÖ Conversational interface\n- ‚úÖ Local-first (no cloud dependencies)\n\n## Available Scripts\n\n### 1. `scripts/sync-to-mem0.py`\n\nScans documentation and populates Mem0 with relationships.\n\n**Usage:**\n```bash\npython scripts/sync-to-mem0.py\n```\n\n**What it does:**\n- Scans all `specs/*/spec.md` files\n- Parses architecture references: `@docs/architecture/file.md#section`\n- Parses dependencies: `dependencies: [001, 002]`\n- Creates Mem0 memories describing relationships\n- Uses user_id for project isolation\n\n### 2. `scripts/query-relationships.py`\n\nQuery Mem0 for documentation relationships.\n\n**Usage:**\n```bash\n# Find specs that depend on a doc\npython scripts/query-relationships.py \"What specs depend on architecture/security.md?\"\n\n# Find all references to an ADR\npython scripts/query-relationships.py \"Which specs reference ADR-0008?\"\n\n# Get spec dependencies\npython scripts/query-relationships.py \"What does spec 001 depend on?\"\n```\n\n### 3. `scripts/validate-docs.py`\n\nValidate documentation consistency using Mem0.\n\n**Usage:**\n```bash\npython scripts/validate-docs.py\n```\n\n**Checks:**\n- Broken architecture references\n- Missing dependency specs\n- Circular dependencies\n- Orphaned documents\n\n## Templates\n\n### Memory Templates\n\n**Spec Memory:**\n```\nSpecification {number} ({name}) is derived from architecture/{file}.md\nsections {sections}, references ADR-{numbers}, and depends on specs {deps}.\nStatus: {status}. Last updated: {date}\n```\n\n**Architecture Memory:**\n```\nArchitecture document {file}.md has sections: {sections}.\nSection {section} is referenced by specs {spec_numbers}\n```\n\n**Derivation Chain:**\n```\nWhen architecture/{file}.md #{section} changes, these specs need review:\n{spec_list}\n```\n\n## Examples\n\n### Example 1: Sync All Documentation\n\n```bash\n# Navigate to project root\ncd /path/to/dev-lifecycle-marketplace\n\n# Run sync to populate Mem0\npython plugins/planning/skills/doc-sync/scripts/sync-to-mem0.py\n\n# Output:\n# ‚úÖ Scanned 15 specs\n# ‚úÖ Found 42 architecture references\n# ‚úÖ Created 57 memories in Mem0\n# üìä Project: dev-lifecycle-marketplace\n```\n\n### Example 2: Query Impact of Changes\n\n```bash\n# Check what's affected by changing security.md\npython plugins/planning/skills/doc-sync/scripts/query-relationships.py \\\n  \"What specs are derived from architecture/security.md?\"\n\n# Output:\n# Specs affected by architecture/security.md:\n# - 001-user-authentication (sections: #authentication, #jwt-tokens)\n# - 005-admin-panel (sections: #rls-policies)\n# - 012-sso-integration (sections: #oauth)\n```\n\n### Example 3: Validate Before Deployment\n\n```bash\n# Check documentation consistency\npython plugins/planning/skills/doc-sync/scripts/validate-docs.py\n\n# Output:\n# ‚úÖ All architecture references valid\n# ‚ö†Ô∏è  Spec 003 references missing ADR-0015\n# ‚ö†Ô∏è  Circular dependency: 007 ‚Üí 008 ‚Üí 007\n# ‚ùå Broken reference: @docs/architecture/deleted.md\n```\n\n## Configuration\n\n### Mem0 Setup\n\nThe scripts use Mem0 OSS with in-memory Qdrant (no external dependencies):\n\n```python\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"documentation\",\n            \"host\": \"memory\",  # in-memory mode\n        }\n    }\n}\n```\n\n### Project Isolation\n\nUses `user_id` for multi-project support:\n\n```python\n# Add memory for specific project\nm.add(memory_text, user_id=\"dev-lifecycle-marketplace\")\n\n# Query specific project\nm.search(query, user_id=\"dev-lifecycle-marketplace\")\n```\n\n## Integration with Planning Commands\n\nThis skill powers these planning commands:\n\n- `/planning:doc-sync` - Populate Mem0 with current documentation\n- `/planning:impact-analysis <doc-path>` - Show what's affected by changes\n- `/planning:validate-docs` - Check documentation consistency\n- `/planning:update-docs` - Interactive sync with user approval\n\n## Best Practices\n\n1. **Run sync after major changes:**\n   ```bash\n   # After updating architecture or ADRs\n   python scripts/sync-to-mem0.py\n   ```\n\n2. **Check impact before modifying shared docs:**\n   ```bash\n   # Before editing architecture/security.md\n   python scripts/query-relationships.py \"specs depending on security.md\"\n   ```\n\n3. **Validate before commits:**\n   ```bash\n   # Add to pre-commit hook\n   python scripts/validate-docs.py || exit 1\n   ```\n\n4. **Use natural language queries:**\n   - \"What specs need updating if I change auth flow?\"\n   - \"Which ADRs does spec 001 reference?\"\n   - \"Show me all dependencies for the user module\"\n\n## Troubleshooting\n\n### Mem0 Not Installed\n\n```bash\n# Install in virtual environment\npython -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\npip install mem0ai\n```\n\n### Import Errors\n\nEnsure you're in the virtual environment where Mem0 is installed:\n\n```bash\nsource /tmp/mem0-env/bin/activate\npython scripts/sync-to-mem0.py\n```\n\n### No Memories Found\n\nRun the sync script first to populate Mem0:\n\n```bash\npython scripts/sync-to-mem0.py\n```\n\n## Future Enhancements\n\n- [ ] Auto-sync on file changes (git hooks)\n- [ ] Web UI for visualizing relationships\n- [ ] Slack/Discord notifications for affected docs\n- [ ] Integration with CI/CD for validation\n- [ ] Export to Mermaid diagrams\n- [ ] Graph memory for complex relationships\n\n## References\n\n- Mem0 OSS Documentation: https://docs.mem0.ai/open-source/overview\n- Planning Plugin: `plugins/planning/README.md`\n- Spec Management: `plugins/planning/skills/spec-management/`\n- Architecture Patterns: `plugins/planning/skills/architecture-patterns/`"
              },
              {
                "name": "Spec Management",
                "description": "Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.",
                "path": "plugins/planning/skills/spec-management/SKILL.md",
                "frontmatter": {
                  "name": "Spec Management",
                  "description": "Templates, scripts, and examples for managing feature specifications in specs/ directory. Use when creating feature specs, listing specifications, validating spec completeness, updating spec status, searching spec content, organizing project requirements, tracking feature development, managing technical documentation, or when user mentions spec management, feature specifications, requirements docs, spec validation, or specification organization.",
                  "allowed-tools": null
                },
                "content": "# Spec Management Skill\n\n**CRITICAL: The description field above controls when Claude auto-loads this skill.**\n\n## Overview\n\nProvides comprehensive specification management capabilities including spec creation, status tracking, validation, searching, and template-based documentation. Manages feature specifications in the `specs/` directory with consistent numbering, metadata, and status tracking.\n\n## Instructions\n\n### Creating New Specifications\n\n1. Use `scripts/create-spec.sh <spec-name> [description]` to create new numbered specs\n2. Automatically assigns next available spec number (e.g., 001-feature-name.md)\n3. Generates spec with complete frontmatter and all sections\n4. Initializes status as \"draft\" with creation timestamp\n5. Creates organized directory structure if needed\n\n### Listing Specifications\n\n1. Use `scripts/list-specs.sh [--status STATUS] [--format FORMAT]` to list all specs\n2. Displays specs with number, title, status, priority, and last modified date\n3. Filter by status: draft, in-progress, review, approved, implemented, rejected\n4. Output formats: table (default), json, markdown, csv\n5. Sorted by spec number with color-coded status indicators\n\n### Validating Specifications\n\n1. Use `scripts/validate-spec.sh <spec-file>` to check spec completeness\n2. Validates frontmatter: title, status, priority, owner, tags\n3. Checks required sections: Problem, Solution, Requirements, Tasks, Success Criteria\n4. Verifies task breakdown format and numbering\n5. Generates validation report with warnings and errors\n\n### Updating Spec Status\n\n1. Use `scripts/update-status.sh <spec-file> <new-status>` to change spec status\n2. Valid statuses: draft, in-progress, review, approved, implemented, rejected\n3. Updates status timestamp and maintains status history\n4. Optionally adds status change comment\n5. Validates status transition rules\n\n### Searching Specifications\n\n1. Use `scripts/search-specs.sh <query> [--section SECTION]` to search spec content\n2. Searches across all specs or within specific sections\n3. Supports regex patterns and multi-word queries\n4. Displays matches with context and spec location\n5. Filter by tags, status, or priority\n\n## Available Scripts\n\n- **create-spec.sh**: Create new numbered specification with template\n- **list-specs.sh**: List all specifications with filtering and formatting\n- **validate-spec.sh**: Validate spec completeness and format\n- **update-status.sh**: Update specification status with history tracking\n- **search-specs.sh**: Search specification content with context\n\n## Templates\n\n- **spec-template.md**: Complete specification template with all standard sections\n- **spec-metadata.yaml**: Frontmatter template with all metadata fields\n- **task-breakdown-template.md**: Task list format with subtasks and estimates\n- **requirements-template.md**: Requirements documentation format (functional, non-functional, constraints)\n- **success-criteria-template.md**: Success metrics and acceptance criteria format\n\n## Examples\n\nSee `examples/` directory for detailed usage examples:\n- `example-spec-simple.md` - Simple feature specification with basic sections\n- `example-spec-complex.md` - Complex feature with detailed technical design\n- `example-spec-ai-feature.md` - AI/ML feature specification with model details\n- `example-validation-report.md` - Example validation output with errors and warnings\n- `example-spec-list.md` - Example list command output in different formats\n\n## Specification Structure\n\n### Required Frontmatter\n```yaml\n---\nspec-id: 001\ntitle: Feature Name\nstatus: draft\npriority: medium\nowner: team-name\ncreated: 2025-01-15\nupdated: 2025-01-15\ntags: [category, feature-type]\n---\n```\n\n### Required Sections\n1. **Problem Statement** - What problem are we solving?\n2. **Proposed Solution** - How will we solve it?\n3. **Requirements** - Functional, non-functional, constraints\n4. **Technical Design** - Architecture, components, data models\n5. **Task Breakdown** - Numbered tasks with estimates\n6. **Success Criteria** - Measurable outcomes and acceptance criteria\n7. **Dependencies** - External dependencies and blockers\n8. **Timeline** - Estimated schedule and milestones\n9. **Risks** - Potential risks and mitigation strategies\n\n### Status Workflow\n```\ndraft ‚Üí in-progress ‚Üí review ‚Üí approved ‚Üí implemented\n                               ‚Üì\n                            rejected\n```\n\n## Validation Rules\n\n### Frontmatter Validation\n- Spec ID must be numeric and unique\n- Status must be valid enum value\n- Priority must be: low, medium, high, critical\n- Owner must be specified\n- Created and updated dates must be valid ISO dates\n- Tags must be non-empty array\n\n### Content Validation\n- All required sections must be present\n- Each section must have content (not empty)\n- Task breakdown must have numbered tasks\n- Requirements must be categorized\n- Success criteria must be measurable\n\n### Warnings\n- Long spec (>1000 lines) may need splitting\n- Missing optional sections (e.g., Alternatives Considered)\n- Outdated spec (not updated in >30 days)\n- Tasks without estimates\n- Vague success criteria\n\n## Directory Structure\n\n### Phase-Nested Structure (Recommended)\n\nSpecs are organized in phase directories based on dependencies:\n\n```\nspecs/\n‚îú‚îÄ‚îÄ phase-0/                    # Features with no dependencies\n‚îÇ   ‚îú‚îÄ‚îÄ F001-core-data/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spec.md\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tasks.md\n‚îÇ   ‚îî‚îÄ‚îÄ F002-base-api/\n‚îú‚îÄ‚îÄ phase-1/                    # Features depending on Phase 0\n‚îÇ   ‚îú‚îÄ‚îÄ F003-user-auth/\n‚îÇ   ‚îî‚îÄ‚îÄ F004-chat-system/\n‚îú‚îÄ‚îÄ phase-2/                    # Features depending on Phase 1\n‚îÇ   ‚îî‚îÄ‚îÄ F005-analytics/\n‚îî‚îÄ‚îÄ infrastructure/             # Infrastructure specs (not phased)\n    ‚îî‚îÄ‚îÄ 001-database/\n```\n\n### Phase Calculation\n\nPhase is calculated automatically based on dependencies:\n- **Phase 0**: No dependencies (foundation features)\n- **Phase N**: max(dependency phases) + 1\n\nExample: F003 depends on F001 (phase 0) and F002 (phase 0) ‚Üí F003 is Phase 1\n\n### Naming Convention\n\n- **Format**: `F{XXX}-{feature-slug}/`\n- **Numbering**: Zero-padded 3-digit IDs (F001, F002, ..., F050, F100)\n- **Slug**: kebab-case, 2-4 words max\n- Numbers are never reused. Deleted specs leave gaps in numbering.\n\n### Legacy Flat Structure\n\nFor backward compatibility, the system also supports:\n```\nspecs/\n‚îú‚îÄ‚îÄ features/\n‚îÇ   ‚îú‚îÄ‚îÄ 001-feature-name/\n‚îÇ   ‚îî‚îÄ‚îÄ 002-another-feature/\n‚îî‚îÄ‚îÄ infrastructure/\n    ‚îî‚îÄ‚îÄ 001-component/\n```\n\nThe system checks phase-nested first, then falls back to legacy structure.\n\n## Integration\n\nThis skill is used by:\n- `planning:create-spec` command - Create new feature specifications\n- `planning:review-specs` command - Review and validate all specs\n- `planning:track-progress` command - Track feature implementation progress\n- All development agents - Reference specs for implementation guidance\n- Project management tools - Export spec data for tracking\n\n## Best Practices\n\n1. **Keep specs focused** - One feature per spec\n2. **Update status regularly** - Reflect current development state\n3. **Link related specs** - Reference dependencies between specs\n4. **Include examples** - Add code samples and mockups\n5. **Review before approval** - Validate with team before implementation\n6. **Archive old specs** - Move implemented specs to archive/\n7. **Use consistent tags** - Maintain tag taxonomy for filtering\n8. **Write measurable criteria** - Success criteria must be testable\n\n---\n\n**Purpose**: Comprehensive specification management for feature documentation\n**Used by**: Planning agents, development teams, project managers"
              }
            ]
          }
        ]
      }
    },
    {
      "full_name": "vanman2024/ai-dev-marketplace",
      "url": "https://github.com/vanman2024/ai-dev-marketplace",
      "description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-29T02:15:49Z",
        "created_at": "2025-10-25T05:00:07Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".archive",
          "type": "tree",
          "size": null
        },
        {
          "path": ".archive/CONSOLIDATION-PLAN.md",
          "type": "blob",
          "size": 12789
        },
        {
          "path": ".archive/SETTINGS-SYNC-GUIDE.md",
          "type": "blob",
          "size": 10076
        },
        {
          "path": ".archive/WORKFLOW-EXAMPLE.md",
          "type": "blob",
          "size": 14541
        },
        {
          "path": ".archive/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": ".archive/agents/agent-comprehensive.md.template",
          "type": "blob",
          "size": 2140
        },
        {
          "path": ".archive/agents/agent-example.md",
          "type": "blob",
          "size": 3894
        },
        {
          "path": ".archive/agents/command-with-agents.md.template",
          "type": "blob",
          "size": 2469
        },
        {
          "path": ".archive/agents/example-pattern1-simple.md",
          "type": "blob",
          "size": 879
        },
        {
          "path": ".archive/agents/example-pattern2-single-agent.md",
          "type": "blob",
          "size": 1107
        },
        {
          "path": ".archive/agents/example-pattern3-sequential-with-slashcommands.md",
          "type": "blob",
          "size": 1670
        },
        {
          "path": ".archive/agents/example-pattern4-parallel-agents.md",
          "type": "blob",
          "size": 1881
        },
        {
          "path": ".archive/ai-tech-stack-1",
          "type": "tree",
          "size": null
        },
        {
          "path": ".archive/ai-tech-stack-1/AI-TECH-STACK-ARCHITECTURE.md",
          "type": "blob",
          "size": 9577
        },
        {
          "path": ".archive/ai-tech-stack-1/FINAL-ARCHITECTURE.md",
          "type": "blob",
          "size": 6794
        },
        {
          "path": ".archive/ai-tech-stack-1/dependency-map.json",
          "type": "blob",
          "size": 4867
        },
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 17350
        },
        {
          "path": ".claude",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude/settings.local.json",
          "type": "blob",
          "size": 10261
        },
        {
          "path": ".git-hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": ".git-hooks/pre-commit",
          "type": "blob",
          "size": 658
        },
        {
          "path": ".github",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/ISSUE_TEMPLATE/bug_report.yml",
          "type": "blob",
          "size": 1919
        },
        {
          "path": ".github/ISSUE_TEMPLATE/config.yml",
          "type": "blob",
          "size": 379
        },
        {
          "path": ".github/ISSUE_TEMPLATE/feature_request.yml",
          "type": "blob",
          "size": 2399
        },
        {
          "path": ".github/ISSUE_TEMPLATE/hotfix.yml",
          "type": "blob",
          "size": 1484
        },
        {
          "path": ".github/ISSUE_TEMPLATE/task.yml",
          "type": "blob",
          "size": 716
        },
        {
          "path": ".github/copilot-instructions.md",
          "type": "blob",
          "size": 4536
        },
        {
          "path": ".github/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/prompts/speckit.analyze.prompt.md",
          "type": "blob",
          "size": 7175
        },
        {
          "path": ".github/prompts/speckit.checklist.prompt.md",
          "type": "blob",
          "size": 16809
        },
        {
          "path": ".github/prompts/speckit.clarify.prompt.md",
          "type": "blob",
          "size": 11206
        },
        {
          "path": ".github/prompts/speckit.constitution.prompt.md",
          "type": "blob",
          "size": 5075
        },
        {
          "path": ".github/prompts/speckit.implement.prompt.md",
          "type": "blob",
          "size": 7418
        },
        {
          "path": ".github/prompts/speckit.plan.prompt.md",
          "type": "blob",
          "size": 2892
        },
        {
          "path": ".github/prompts/speckit.specify.prompt.md",
          "type": "blob",
          "size": 12597
        },
        {
          "path": ".github/prompts/speckit.tasks.prompt.md",
          "type": "blob",
          "size": 6080
        },
        {
          "path": ".github/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".github/workflows/playwright-mcp-tests.yml",
          "type": "blob",
          "size": 2311
        },
        {
          "path": ".gitignore",
          "type": "blob",
          "size": 761
        },
        {
          "path": ".gitleaks.toml",
          "type": "blob",
          "size": 1690
        },
        {
          "path": ".specify",
          "type": "tree",
          "size": null
        },
        {
          "path": ".specify/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": ".specify/memory/constitution.md",
          "type": "blob",
          "size": 2346
        },
        {
          "path": ".specify/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".specify/scripts/bash",
          "type": "tree",
          "size": null
        },
        {
          "path": ".specify/scripts/bash/check-prerequisites.sh",
          "type": "blob",
          "size": 4975
        },
        {
          "path": ".specify/scripts/bash/common.sh",
          "type": "blob",
          "size": 4891
        },
        {
          "path": ".specify/scripts/bash/create-new-feature.sh",
          "type": "blob",
          "size": 9411
        },
        {
          "path": ".specify/scripts/bash/setup-plan.sh",
          "type": "blob",
          "size": 1617
        },
        {
          "path": ".specify/scripts/bash/update-agent-context.sh",
          "type": "blob",
          "size": 24782
        },
        {
          "path": ".specify/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": ".specify/templates/agent-file-template.md",
          "type": "blob",
          "size": 464
        },
        {
          "path": ".specify/templates/checklist-template.md",
          "type": "blob",
          "size": 1312
        },
        {
          "path": ".specify/templates/plan-template.md",
          "type": "blob",
          "size": 3671
        },
        {
          "path": ".specify/templates/spec-template.md",
          "type": "blob",
          "size": 3960
        },
        {
          "path": ".specify/templates/tasks-template.md",
          "type": "blob",
          "size": 9177
        },
        {
          "path": ".vscode",
          "type": "tree",
          "size": null
        },
        {
          "path": ".vscode/keybindings.json",
          "type": "blob",
          "size": 536
        },
        {
          "path": "CLAUDE.md",
          "type": "blob",
          "size": 18690
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 13849
        },
        {
          "path": "docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/CREDENTIAL-SAFETY.md",
          "type": "blob",
          "size": 3495
        },
        {
          "path": "docs/EXECUTION-ANALYSIS-RED-AI.md",
          "type": "blob",
          "size": 11316
        },
        {
          "path": "docs/README.md",
          "type": "blob",
          "size": 3144
        },
        {
          "path": "docs/README_AUDIT.md",
          "type": "blob",
          "size": 7965
        },
        {
          "path": "docs/REFACTOR-NEEDED.md",
          "type": "blob",
          "size": 11033
        },
        {
          "path": "docs/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/agents/AGENT-COLORS.md",
          "type": "blob",
          "size": 5930
        },
        {
          "path": "docs/agents/AGENT_AUDIT_FINDINGS.md",
          "type": "blob",
          "size": 14101
        },
        {
          "path": "docs/agents/AGENT_AUDIT_REPORT.md",
          "type": "blob",
          "size": 14029
        },
        {
          "path": "docs/agents/AUDIT_SUMMARY.md",
          "type": "blob",
          "size": 8318
        },
        {
          "path": "docs/development",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/development/CLI-TOOL-BUILDER-PLUGIN-DESIGN.md",
          "type": "blob",
          "size": 41525
        },
        {
          "path": "docs/development/MOBILE-TECHNOLOGY-RESEARCH.md",
          "type": "blob",
          "size": 12432
        },
        {
          "path": "docs/development/NEXT-JS-PLUGIN-DEVELOPMENT-GUIDE.md",
          "type": "blob",
          "size": 41623
        },
        {
          "path": "docs/development/redis",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/development/redis/REDIS-COMPREHENSIVE-DOCUMENTATION.md",
          "type": "blob",
          "size": 24657
        },
        {
          "path": "docs/development/redis/REDIS-IMPLEMENTATION-CHECKLIST.md",
          "type": "blob",
          "size": 32748
        },
        {
          "path": "docs/development/redis/REDIS-QUICK-REFERENCE.md",
          "type": "blob",
          "size": 17626
        },
        {
          "path": "docs/integrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/integrations/DOPPLER-INTEGRATION.md",
          "type": "blob",
          "size": 13479
        },
        {
          "path": "docs/integrations/DOPPLER-QUICK-START.md",
          "type": "blob",
          "size": 1614
        },
        {
          "path": "docs/integrations/DOPPLER-SETUP-SUMMARY.md",
          "type": "blob",
          "size": 4666
        },
        {
          "path": "docs/integrations/DOPPLER-WORKFLOW.md",
          "type": "blob",
          "size": 8299
        },
        {
          "path": "docs/integrations/MAKE-COM-COMPLETE-API-REFERENCE.md",
          "type": "blob",
          "size": 12070
        },
        {
          "path": "docs/planning",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/planning/AUTOMATED-PLANNING-DESIGN.md",
          "type": "blob",
          "size": 22552
        },
        {
          "path": "docs/planning/INTEGRATION-MARKETPLACE-PLAN.md",
          "type": "blob",
          "size": 23870
        },
        {
          "path": "docs/planning/WORKTREE-INTEGRATION-PLAN.md",
          "type": "blob",
          "size": 10160
        },
        {
          "path": "docs/security",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/security/SECURITY-RULES.md",
          "type": "blob",
          "size": 6366
        },
        {
          "path": "docs/stacks",
          "type": "tree",
          "size": null
        },
        {
          "path": "docs/stacks/FASTAPI-VERCEL-AI-MEM0-STACK.md",
          "type": "blob",
          "size": 49372
        },
        {
          "path": "docs/stacks/GOOGLE-GEMINI-API-AGENTIC-COMPLETE-GUIDE.md",
          "type": "blob",
          "size": 43033
        },
        {
          "path": "docs/stacks/GOOGLE-GEMINI-API-COMPLETE-GUIDE.md",
          "type": "blob",
          "size": 32570
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 586
        },
        {
          "path": "plugins/a2a-protocol/.gitignore",
          "type": "blob",
          "size": 259
        },
        {
          "path": "plugins/a2a-protocol/CHANGELOG.md",
          "type": "blob",
          "size": 314
        },
        {
          "path": "plugins/a2a-protocol/LICENSE",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/a2a-protocol/README.md",
          "type": "blob",
          "size": 378
        },
        {
          "path": "plugins/a2a-protocol/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-agent-builder.md",
          "type": "blob",
          "size": 7995
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-client-builder.md",
          "type": "blob",
          "size": 9564
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-discovery.md",
          "type": "blob",
          "size": 9518
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-production.md",
          "type": "blob",
          "size": 11803
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-setup.md",
          "type": "blob",
          "size": 8791
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-streaming.md",
          "type": "blob",
          "size": 10172
        },
        {
          "path": "plugins/a2a-protocol/agents/a2a-verifier.md",
          "type": "blob",
          "size": 9174
        },
        {
          "path": "plugins/a2a-protocol/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/commands/add-agent.md",
          "type": "blob",
          "size": 3394
        },
        {
          "path": "plugins/a2a-protocol/commands/add-client.md",
          "type": "blob",
          "size": 3302
        },
        {
          "path": "plugins/a2a-protocol/commands/add-discovery.md",
          "type": "blob",
          "size": 3885
        },
        {
          "path": "plugins/a2a-protocol/commands/add-production.md",
          "type": "blob",
          "size": 4626
        },
        {
          "path": "plugins/a2a-protocol/commands/add-streaming.md",
          "type": "blob",
          "size": 5059
        },
        {
          "path": "plugins/a2a-protocol/commands/build-full.md",
          "type": "blob",
          "size": 10375
        },
        {
          "path": "plugins/a2a-protocol/commands/init.md",
          "type": "blob",
          "size": 3461
        },
        {
          "path": "plugins/a2a-protocol/commands/test.md",
          "type": "blob",
          "size": 2579
        },
        {
          "path": "plugins/a2a-protocol/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/hooks/hooks.json",
          "type": "blob",
          "size": 18
        },
        {
          "path": "plugins/a2a-protocol/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/SKILL.md",
          "type": "blob",
          "size": 10472
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/examples/function-executor.ts",
          "type": "blob",
          "size": 5073
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/examples/llm-executor.ts",
          "type": "blob",
          "size": 3810
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/examples/validation-executor.ts",
          "type": "blob",
          "size": 8251
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/examples/workflow-executor.ts",
          "type": "blob",
          "size": 7109
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/scripts/test-executor.sh",
          "type": "blob",
          "size": 5658
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/scripts/validate-executor.sh",
          "type": "blob",
          "size": 3902
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/async-executor.py",
          "type": "blob",
          "size": 7814
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/async-executor.ts",
          "type": "blob",
          "size": 6132
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/basic-executor.py",
          "type": "blob",
          "size": 4221
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/basic-executor.ts",
          "type": "blob",
          "size": 3701
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/batch-executor.ts",
          "type": "blob",
          "size": 6150
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/streaming-executor.py",
          "type": "blob",
          "size": 6166
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/templates/streaming-executor.ts",
          "type": "blob",
          "size": 5119
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/SKILL.md",
          "type": "blob",
          "size": 12829
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/architecture-patterns.md",
          "type": "blob",
          "size": 14178
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/data-pipeline-integration.py",
          "type": "blob",
          "size": 11305
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/python-hybrid-agent.py",
          "type": "blob",
          "size": 5315
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/security-best-practices.md",
          "type": "blob",
          "size": 10860
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/troubleshooting-integration.md",
          "type": "blob",
          "size": 12743
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/examples/typescript-hybrid-agent.ts",
          "type": "blob",
          "size": 6539
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/install-python-integration.sh",
          "type": "blob",
          "size": 1293
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/install-typescript-integration.sh",
          "type": "blob",
          "size": 1958
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/setup-hybrid-agent.sh",
          "type": "blob",
          "size": 2913
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/validate-protocol-compatibility.sh",
          "type": "blob",
          "size": 3110
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/validate-python-integration.sh",
          "type": "blob",
          "size": 1818
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/scripts/validate-typescript-integration.sh",
          "type": "blob",
          "size": 2712
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/auth-hybrid-template.txt",
          "type": "blob",
          "size": 4344
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/coordinator-worker-pattern.py",
          "type": "blob",
          "size": 6217
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/env-integration-template.txt",
          "type": "blob",
          "size": 1986
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/layered-stack-pattern.ts",
          "type": "blob",
          "size": 9363
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/mesh-centralized-tools-pattern.py",
          "type": "blob",
          "size": 10329
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/templates/peer-tool-sharing-pattern.ts",
          "type": "blob",
          "size": 7745
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/README.md",
          "type": "blob",
          "size": 7367
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/SKILL.md",
          "type": "blob",
          "size": 7028
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/csharp-basic.cs",
          "type": "blob",
          "size": 2735
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/error-handling-java.md",
          "type": "blob",
          "size": 7236
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/error-handling-python.md",
          "type": "blob",
          "size": 4108
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/error-handling-typescript.md",
          "type": "blob",
          "size": 6803
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/go-basic.go",
          "type": "blob",
          "size": 1881
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/java-basic.java",
          "type": "blob",
          "size": 2655
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/python-async.py",
          "type": "blob",
          "size": 2180
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/python-basic.py",
          "type": "blob",
          "size": 1488
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/examples/typescript-basic.ts",
          "type": "blob",
          "size": 1982
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/install-csharp.sh",
          "type": "blob",
          "size": 1055
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/install-go.sh",
          "type": "blob",
          "size": 1040
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/install-java.sh",
          "type": "blob",
          "size": 1603
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/install-python.sh",
          "type": "blob",
          "size": 900
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/install-typescript.sh",
          "type": "blob",
          "size": 1275
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-csharp.sh",
          "type": "blob",
          "size": 1110
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-go.sh",
          "type": "blob",
          "size": 1145
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-java.sh",
          "type": "blob",
          "size": 1455
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-python.sh",
          "type": "blob",
          "size": 1042
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-skill.sh",
          "type": "blob",
          "size": 7130
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/scripts/validate-typescript.sh",
          "type": "blob",
          "size": 1192
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/auth-api-key-template.txt",
          "type": "blob",
          "size": 1407
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/auth-jwt-template.txt",
          "type": "blob",
          "size": 5108
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/auth-oauth-template.txt",
          "type": "blob",
          "size": 3656
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/csharp-config.csproj",
          "type": "blob",
          "size": 3037
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/env-template.txt",
          "type": "blob",
          "size": 892
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/go-mod.txt",
          "type": "blob",
          "size": 2799
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/java-config.xml",
          "type": "blob",
          "size": 2898
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/python-config.py",
          "type": "blob",
          "size": 1750
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/templates/typescript-config.ts",
          "type": "blob",
          "size": 2484
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/SKILL.md",
          "type": "blob",
          "size": 6368
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/examples/http-fastapi-example.md",
          "type": "blob",
          "size": 3782
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/examples/sse-streaming-example.md",
          "type": "blob",
          "size": 5560
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/examples/stdio-simple-example.md",
          "type": "blob",
          "size": 4157
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/examples/websocket-bidirectional-example.md",
          "type": "blob",
          "size": 7305
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/scripts/generate-server.sh",
          "type": "blob",
          "size": 1896
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/scripts/test-transport.sh",
          "type": "blob",
          "size": 2670
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/scripts/validate-config.sh",
          "type": "blob",
          "size": 2577
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/python-http-server.py",
          "type": "blob",
          "size": 2214
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/python-sse-server.py",
          "type": "blob",
          "size": 2748
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/python-stdio-server.py",
          "type": "blob",
          "size": 3182
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/python-websocket-server.py",
          "type": "blob",
          "size": 4376
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/typescript-http-server.ts",
          "type": "blob",
          "size": 1796
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/typescript-sse-server.ts",
          "type": "blob",
          "size": 3068
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/typescript-stdio-server.ts",
          "type": "blob",
          "size": 2750
        },
        {
          "path": "plugins/a2a-protocol/skills/a2a-server-config/templates/typescript-websocket-server.ts",
          "type": "blob",
          "size": 4196
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/SKILL.md",
          "type": "blob",
          "size": 8831
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/examples/calculator-agent.md",
          "type": "blob",
          "size": 6281
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/examples/data-analysis-agent.md",
          "type": "blob",
          "size": 17682
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/examples/translation-agent.md",
          "type": "blob",
          "size": 10978
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/scripts/generate-agent-card.sh",
          "type": "blob",
          "size": 6240
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/scripts/validate-agent-card.sh",
          "type": "blob",
          "size": 7526
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates/authenticated-agent-card.json",
          "type": "blob",
          "size": 4438
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates/basic-agent-card.json",
          "type": "blob",
          "size": 1284
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates/multi-capability-agent-card.json",
          "type": "blob",
          "size": 4582
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates/schema.json",
          "type": "blob",
          "size": 8786
        },
        {
          "path": "plugins/a2a-protocol/skills/agent-card-templates/templates/streaming-agent-card.json",
          "type": "blob",
          "size": 5975
        },
        {
          "path": "plugins/celery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 798
        },
        {
          "path": "plugins/celery/.gitignore",
          "type": "blob",
          "size": 497
        },
        {
          "path": "plugins/celery/CHANGELOG.md",
          "type": "blob",
          "size": 770
        },
        {
          "path": "plugins/celery/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/celery/README.md",
          "type": "blob",
          "size": 4866
        },
        {
          "path": "plugins/celery/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/agents/backend-specialist.md",
          "type": "blob",
          "size": 9739
        },
        {
          "path": "plugins/celery/agents/beat-scheduler-agent.md",
          "type": "blob",
          "size": 8678
        },
        {
          "path": "plugins/celery/agents/broker-specialist.md",
          "type": "blob",
          "size": 9746
        },
        {
          "path": "plugins/celery/agents/celery-setup-agent.md",
          "type": "blob",
          "size": 9946
        },
        {
          "path": "plugins/celery/agents/deployment-architect.md",
          "type": "blob",
          "size": 9551
        },
        {
          "path": "plugins/celery/agents/django-integrator.md",
          "type": "blob",
          "size": 9287
        },
        {
          "path": "plugins/celery/agents/fastapi-integrator.md",
          "type": "blob",
          "size": 8770
        },
        {
          "path": "plugins/celery/agents/flask-integrator.md",
          "type": "blob",
          "size": 8942
        },
        {
          "path": "plugins/celery/agents/monitoring-integrator.md",
          "type": "blob",
          "size": 11575
        },
        {
          "path": "plugins/celery/agents/task-generator-agent.md",
          "type": "blob",
          "size": 7878
        },
        {
          "path": "plugins/celery/agents/worker-architect.md",
          "type": "blob",
          "size": 9799
        },
        {
          "path": "plugins/celery/agents/workflow-specialist.md",
          "type": "blob",
          "size": 8666
        },
        {
          "path": "plugins/celery/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/commands/add-beat.md",
          "type": "blob",
          "size": 5008
        },
        {
          "path": "plugins/celery/commands/add-broker.md",
          "type": "blob",
          "size": 4283
        },
        {
          "path": "plugins/celery/commands/add-error-handling.md",
          "type": "blob",
          "size": 3912
        },
        {
          "path": "plugins/celery/commands/add-monitoring.md",
          "type": "blob",
          "size": 3581
        },
        {
          "path": "plugins/celery/commands/add-result-backend.md",
          "type": "blob",
          "size": 4705
        },
        {
          "path": "plugins/celery/commands/add-routing.md",
          "type": "blob",
          "size": 4704
        },
        {
          "path": "plugins/celery/commands/add-task.md",
          "type": "blob",
          "size": 4198
        },
        {
          "path": "plugins/celery/commands/add-workers.md",
          "type": "blob",
          "size": 4206
        },
        {
          "path": "plugins/celery/commands/add-workflow.md",
          "type": "blob",
          "size": 4889
        },
        {
          "path": "plugins/celery/commands/deploy.md",
          "type": "blob",
          "size": 4708
        },
        {
          "path": "plugins/celery/commands/init.md",
          "type": "blob",
          "size": 4597
        },
        {
          "path": "plugins/celery/commands/integrate-django.md",
          "type": "blob",
          "size": 5128
        },
        {
          "path": "plugins/celery/commands/integrate-fastapi.md",
          "type": "blob",
          "size": 5612
        },
        {
          "path": "plugins/celery/commands/integrate-flask.md",
          "type": "blob",
          "size": 5262
        },
        {
          "path": "plugins/celery/commands/test.md",
          "type": "blob",
          "size": 4436
        },
        {
          "path": "plugins/celery/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/hooks/hooks.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/celery/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/beat-scheduling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/SKILL.md",
          "type": "blob",
          "size": 9579
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/examples/crontab-examples.md",
          "type": "blob",
          "size": 11828
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/examples/django-celery-beat-setup.md",
          "type": "blob",
          "size": 19232
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/examples/interval-examples.md",
          "type": "blob",
          "size": 14632
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/scripts/test-beat.sh",
          "type": "blob",
          "size": 11798
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/scripts/validate-schedule.sh",
          "type": "blob",
          "size": 10123
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates/crontab-schedule.py",
          "type": "blob",
          "size": 5444
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates/django-celery-beat.py",
          "type": "blob",
          "size": 13275
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates/dynamic-schedules.py",
          "type": "blob",
          "size": 14470
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates/interval-schedule.py",
          "type": "blob",
          "size": 7397
        },
        {
          "path": "plugins/celery/skills/beat-scheduling/templates/solar-schedule.py",
          "type": "blob",
          "size": 9214
        },
        {
          "path": "plugins/celery/skills/broker-configurations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/broker-configurations/README.md",
          "type": "blob",
          "size": 2939
        },
        {
          "path": "plugins/celery/skills/broker-configurations/SKILL.md",
          "type": "blob",
          "size": 13373
        },
        {
          "path": "plugins/celery/skills/broker-configurations/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/broker-configurations/examples/rabbitmq-ha.md",
          "type": "blob",
          "size": 12207
        },
        {
          "path": "plugins/celery/skills/broker-configurations/examples/redis-sentinel.md",
          "type": "blob",
          "size": 9651
        },
        {
          "path": "plugins/celery/skills/broker-configurations/examples/sqs-setup.md",
          "type": "blob",
          "size": 13978
        },
        {
          "path": "plugins/celery/skills/broker-configurations/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/broker-configurations/scripts/setup-rabbitmq.sh",
          "type": "blob",
          "size": 10825
        },
        {
          "path": "plugins/celery/skills/broker-configurations/scripts/setup-redis.sh",
          "type": "blob",
          "size": 8867
        },
        {
          "path": "plugins/celery/skills/broker-configurations/scripts/test-broker-connection.sh",
          "type": "blob",
          "size": 8423
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates/connection-strings.env",
          "type": "blob",
          "size": 4670
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates/rabbitmq-config.py",
          "type": "blob",
          "size": 4512
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates/redis-config.py",
          "type": "blob",
          "size": 2936
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates/sqs-config.py",
          "type": "blob",
          "size": 5592
        },
        {
          "path": "plugins/celery/skills/broker-configurations/templates/ssl-config.py",
          "type": "blob",
          "size": 6443
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/SKILL.md",
          "type": "blob",
          "size": 14693
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/examples/django-setup.md",
          "type": "blob",
          "size": 12748
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/examples/fastapi-setup.md",
          "type": "blob",
          "size": 1185
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/examples/flask-setup.md",
          "type": "blob",
          "size": 1321
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/examples/standalone-setup.md",
          "type": "blob",
          "size": 784
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/scripts/detect-framework.sh",
          "type": "blob",
          "size": 10444
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/scripts/generate-config.sh",
          "type": "blob",
          "size": 9592
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/scripts/test-broker-connection.sh",
          "type": "blob",
          "size": 2321
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/scripts/validate-config.sh",
          "type": "blob",
          "size": 11534
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/beat-schedule.py",
          "type": "blob",
          "size": 12576
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/celery-app-django.py",
          "type": "blob",
          "size": 3391
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/celery-app-fastapi.py",
          "type": "blob",
          "size": 7918
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/celery-app-flask.py",
          "type": "blob",
          "size": 4812
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/celery-app-standalone.py",
          "type": "blob",
          "size": 2106
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/config-rabbitmq.py",
          "type": "blob",
          "size": 11687
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/config-redis.py",
          "type": "blob",
          "size": 8852
        },
        {
          "path": "plugins/celery/skills/celery-config-patterns/templates/tasks-example.py",
          "type": "blob",
          "size": 14557
        },
        {
          "path": "plugins/celery/skills/deployment-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/SKILL.md",
          "type": "blob",
          "size": 17553
        },
        {
          "path": "plugins/celery/skills/deployment-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/examples/docker-deployment.md",
          "type": "blob",
          "size": 7245
        },
        {
          "path": "plugins/celery/skills/deployment-configs/examples/kubernetes-deployment.md",
          "type": "blob",
          "size": 12327
        },
        {
          "path": "plugins/celery/skills/deployment-configs/examples/systemd-setup.md",
          "type": "blob",
          "size": 11967
        },
        {
          "path": "plugins/celery/skills/deployment-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/scripts/deploy.sh",
          "type": "blob",
          "size": 9386
        },
        {
          "path": "plugins/celery/skills/deployment-configs/scripts/health-check.sh",
          "type": "blob",
          "size": 3691
        },
        {
          "path": "plugins/celery/skills/deployment-configs/scripts/test-deployment.sh",
          "type": "blob",
          "size": 9259
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/Dockerfile.worker",
          "type": "blob",
          "size": 2024
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/docker-compose.yml",
          "type": "blob",
          "size": 4468
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/health-checks.py",
          "type": "blob",
          "size": 11810
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/kubernetes",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/kubernetes/celery-beat.yaml",
          "type": "blob",
          "size": 3574
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/kubernetes/celery-worker.yaml",
          "type": "blob",
          "size": 5106
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/systemd",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/systemd/celery-beat.service",
          "type": "blob",
          "size": 1082
        },
        {
          "path": "plugins/celery/skills/deployment-configs/templates/systemd/celery-worker.service",
          "type": "blob",
          "size": 1126
        },
        {
          "path": "plugins/celery/skills/framework-integrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/SKILL.md",
          "type": "blob",
          "size": 10055
        },
        {
          "path": "plugins/celery/skills/framework-integrations/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/examples/django-complete-setup.md",
          "type": "blob",
          "size": 5882
        },
        {
          "path": "plugins/celery/skills/framework-integrations/examples/fastapi-async.md",
          "type": "blob",
          "size": 10256
        },
        {
          "path": "plugins/celery/skills/framework-integrations/examples/flask-factory-pattern.md",
          "type": "blob",
          "size": 7305
        },
        {
          "path": "plugins/celery/skills/framework-integrations/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/scripts/setup-framework.sh",
          "type": "blob",
          "size": 7382
        },
        {
          "path": "plugins/celery/skills/framework-integrations/scripts/test-integration.sh",
          "type": "blob",
          "size": 6463
        },
        {
          "path": "plugins/celery/skills/framework-integrations/scripts/validate-framework.sh",
          "type": "blob",
          "size": 7487
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/django-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/django-integration/__init__.py",
          "type": "blob",
          "size": 269
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/django-integration/celery.py",
          "type": "blob",
          "size": 1064
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/django-integration/settings.py",
          "type": "blob",
          "size": 7916
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/django-integration/tasks.py",
          "type": "blob",
          "size": 10303
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/fastapi-background.py",
          "type": "blob",
          "size": 15490
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/fastapi-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/fastapi-integration/celery_app.py",
          "type": "blob",
          "size": 439
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/fastapi-integration/main.py",
          "type": "blob",
          "size": 1176
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/fastapi-integration/tasks.py",
          "type": "blob",
          "size": 670
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/flask-context.py",
          "type": "blob",
          "size": 11203
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/flask-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/flask-integration/celery_app.py",
          "type": "blob",
          "size": 1105
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/flask-integration/tasks.py",
          "type": "blob",
          "size": 749
        },
        {
          "path": "plugins/celery/skills/framework-integrations/templates/transaction-safe-django.py",
          "type": "blob",
          "size": 6494
        },
        {
          "path": "plugins/celery/skills/monitoring-flower",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/SKILL.md",
          "type": "blob",
          "size": 13154
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/examples/custom-dashboards.md",
          "type": "blob",
          "size": 15522
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/examples/flower-setup.md",
          "type": "blob",
          "size": 11853
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/examples/prometheus-integration.md",
          "type": "blob",
          "size": 13112
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/scripts/flower-systemd.service",
          "type": "blob",
          "size": 5949
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/scripts/start-flower.sh",
          "type": "blob",
          "size": 8678
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/scripts/test-flower.sh",
          "type": "blob",
          "size": 9499
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/templates/custom-dashboard.py",
          "type": "blob",
          "size": 17130
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/templates/flower-auth.py",
          "type": "blob",
          "size": 11801
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/templates/flower-config.py",
          "type": "blob",
          "size": 6027
        },
        {
          "path": "plugins/celery/skills/monitoring-flower/templates/prometheus-metrics.py",
          "type": "blob",
          "size": 14666
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/SKILL.md",
          "type": "blob",
          "size": 12107
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/examples/postgresql-backend.md",
          "type": "blob",
          "size": 14891
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/examples/redis-backend-setup.md",
          "type": "blob",
          "size": 11609
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/examples/result-expiration-policies.md",
          "type": "blob",
          "size": 17859
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/scripts/migrate-backend.sh",
          "type": "blob",
          "size": 11617
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/scripts/test-backend.sh",
          "type": "blob",
          "size": 9395
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates/custom-serializers.py",
          "type": "blob",
          "size": 11946
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates/db-backend.py",
          "type": "blob",
          "size": 6446
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates/redis-backend.py",
          "type": "blob",
          "size": 3511
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates/result-expiration.py",
          "type": "blob",
          "size": 10400
        },
        {
          "path": "plugins/celery/skills/result-backend-patterns/templates/rpc-backend.py",
          "type": "blob",
          "size": 5745
        },
        {
          "path": "plugins/celery/skills/routing-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/routing-strategies/SKILL.md",
          "type": "blob",
          "size": 13709
        },
        {
          "path": "plugins/celery/skills/routing-strategies/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/routing-strategies/examples/priority-queue-setup.md",
          "type": "blob",
          "size": 9238
        },
        {
          "path": "plugins/celery/skills/routing-strategies/examples/topic-routing.md",
          "type": "blob",
          "size": 12231
        },
        {
          "path": "plugins/celery/skills/routing-strategies/examples/worker-queue-assignment.md",
          "type": "blob",
          "size": 13861
        },
        {
          "path": "plugins/celery/skills/routing-strategies/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/routing-strategies/scripts/test-routing.sh",
          "type": "blob",
          "size": 8917
        },
        {
          "path": "plugins/celery/skills/routing-strategies/scripts/validate-queues.sh",
          "type": "blob",
          "size": 9439
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates/priority-queues.py",
          "type": "blob",
          "size": 9502
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates/queue-config.py",
          "type": "blob",
          "size": 4922
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates/routing-rules.py",
          "type": "blob",
          "size": 8619
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates/topic-exchange.py",
          "type": "blob",
          "size": 12250
        },
        {
          "path": "plugins/celery/skills/routing-strategies/templates/worker-routing.py",
          "type": "blob",
          "size": 12930
        },
        {
          "path": "plugins/celery/skills/task-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/task-patterns/SKILL.md",
          "type": "blob",
          "size": 13183
        },
        {
          "path": "plugins/celery/skills/task-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/task-patterns/examples/custom-task-classes.md",
          "type": "blob",
          "size": 16234
        },
        {
          "path": "plugins/celery/skills/task-patterns/examples/rate-limiting.md",
          "type": "blob",
          "size": 12746
        },
        {
          "path": "plugins/celery/skills/task-patterns/examples/task-with-retries.md",
          "type": "blob",
          "size": 8653
        },
        {
          "path": "plugins/celery/skills/task-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/task-patterns/scripts/generate-task.sh",
          "type": "blob",
          "size": 5968
        },
        {
          "path": "plugins/celery/skills/task-patterns/scripts/test-task.sh",
          "type": "blob",
          "size": 5128
        },
        {
          "path": "plugins/celery/skills/task-patterns/scripts/validate-task.sh",
          "type": "blob",
          "size": 6722
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/api-task.py",
          "type": "blob",
          "size": 12240
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/basic-task.py",
          "type": "blob",
          "size": 1337
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/custom-task-class.py",
          "type": "blob",
          "size": 9005
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/database-task.py",
          "type": "blob",
          "size": 12929
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/pydantic-validation.py",
          "type": "blob",
          "size": 11389
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/rate-limited-task.py",
          "type": "blob",
          "size": 5181
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/retry-task.py",
          "type": "blob",
          "size": 3779
        },
        {
          "path": "plugins/celery/skills/task-patterns/templates/time-limited-task.py",
          "type": "blob",
          "size": 6419
        },
        {
          "path": "plugins/celery/skills/workflow-canvas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/SKILL.md",
          "type": "blob",
          "size": 8549
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/examples/chain-example.md",
          "type": "blob",
          "size": 7599
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/examples/chord-example.md",
          "type": "blob",
          "size": 13587
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/examples/complex-workflows.md",
          "type": "blob",
          "size": 15695
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/examples/group-example.md",
          "type": "blob",
          "size": 9873
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/scripts/generate-workflow.sh",
          "type": "blob",
          "size": 7974
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/scripts/test-workflow.sh",
          "type": "blob",
          "size": 6796
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/scripts/validate-canvas.sh",
          "type": "blob",
          "size": 10365
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/chain-workflow.py",
          "type": "blob",
          "size": 7026
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/chord-pattern.py",
          "type": "blob",
          "size": 10052
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/complex-workflow.py",
          "type": "blob",
          "size": 12706
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/error-handling-workflow.py",
          "type": "blob",
          "size": 13641
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/group-parallel.py",
          "type": "blob",
          "size": 9370
        },
        {
          "path": "plugins/celery/skills/workflow-canvas/templates/nested-workflows.py",
          "type": "blob",
          "size": 12365
        },
        {
          "path": "plugins/claude-agent-sdk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 570
        },
        {
          "path": "plugins/claude-agent-sdk/.gitignore",
          "type": "blob",
          "size": 114
        },
        {
          "path": "plugins/claude-agent-sdk/AGENT_SDK_FIXES.md",
          "type": "blob",
          "size": 5018
        },
        {
          "path": "plugins/claude-agent-sdk/CHANGELOG.md",
          "type": "blob",
          "size": 1252
        },
        {
          "path": "plugins/claude-agent-sdk/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/claude-agent-sdk/README.md",
          "type": "blob",
          "size": 6916
        },
        {
          "path": "plugins/claude-agent-sdk/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/agents/claude-agent-features.md",
          "type": "blob",
          "size": 10767
        },
        {
          "path": "plugins/claude-agent-sdk/agents/claude-agent-setup.md",
          "type": "blob",
          "size": 12977
        },
        {
          "path": "plugins/claude-agent-sdk/agents/claude-agent-verifier-py.md",
          "type": "blob",
          "size": 8323
        },
        {
          "path": "plugins/claude-agent-sdk/agents/claude-agent-verifier-ts.md",
          "type": "blob",
          "size": 8039
        },
        {
          "path": "plugins/claude-agent-sdk/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-cost-tracking.md",
          "type": "blob",
          "size": 4022
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-custom-tools.md",
          "type": "blob",
          "size": 3975
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-hosting.md",
          "type": "blob",
          "size": 4112
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-mcp-code-execution.md",
          "type": "blob",
          "size": 6218
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-mcp.md",
          "type": "blob",
          "size": 6437
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-memory.md",
          "type": "blob",
          "size": 5698
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-permissions.md",
          "type": "blob",
          "size": 4006
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-plugins.md",
          "type": "blob",
          "size": 4005
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-sessions.md",
          "type": "blob",
          "size": 3931
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-skills.md",
          "type": "blob",
          "size": 3934
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-slash-commands.md",
          "type": "blob",
          "size": 4090
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-streaming.md",
          "type": "blob",
          "size": 4035
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-subagents.md",
          "type": "blob",
          "size": 4003
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-system-prompts.md",
          "type": "blob",
          "size": 4169
        },
        {
          "path": "plugins/claude-agent-sdk/commands/add-todo-tracking.md",
          "type": "blob",
          "size": 4043
        },
        {
          "path": "plugins/claude-agent-sdk/commands/new-app.md",
          "type": "blob",
          "size": 5867
        },
        {
          "path": "plugins/claude-agent-sdk/commands/test-skill-loading.md",
          "type": "blob",
          "size": 3285
        },
        {
          "path": "plugins/claude-agent-sdk/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/docs/claude-agent-sdk-plugin-design.md",
          "type": "blob",
          "size": 12411
        },
        {
          "path": "plugins/claude-agent-sdk/docs/claude-api-documentation.md",
          "type": "blob",
          "size": 10317
        },
        {
          "path": "plugins/claude-agent-sdk/docs/sdk-documentation.md",
          "type": "blob",
          "size": 6256
        },
        {
          "path": "plugins/claude-agent-sdk/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/examples/README.md",
          "type": "blob",
          "size": 3346
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python/.env.example",
          "type": "blob",
          "size": 254
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python/basic-query.py",
          "type": "blob",
          "size": 1322
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python/fastmcp-cloud-http.py",
          "type": "blob",
          "size": 3304
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python/requirements.txt",
          "type": "blob",
          "size": 99
        },
        {
          "path": "plugins/claude-agent-sdk/examples/python/secure-agent-template.py",
          "type": "blob",
          "size": 5848
        },
        {
          "path": "plugins/claude-agent-sdk/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/SKILL.md",
          "type": "blob",
          "size": 8323
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/examples/connection-status.py",
          "type": "blob",
          "size": 3008
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/examples/fastmcp-cloud-http.py",
          "type": "blob",
          "size": 3304
        },
        {
          "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/examples/multi-server.py",
          "type": "blob",
          "size": 2303
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/README.md",
          "type": "blob",
          "size": 5099
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/SKILL.md",
          "type": "blob",
          "size": 3097
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/examples/common-fixes.md",
          "type": "blob",
          "size": 3923
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/examples/validation-report-example.md",
          "type": "blob",
          "size": 1749
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/scripts/check-sdk-version.sh",
          "type": "blob",
          "size": 1868
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/scripts/validate-env-setup.sh",
          "type": "blob",
          "size": 2699
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/scripts/validate-python.sh",
          "type": "blob",
          "size": 4624
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/scripts/validate-typescript.sh",
          "type": "blob",
          "size": 2458
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/templates/.env.example.template",
          "type": "blob",
          "size": 913
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/templates/pyproject-sdk.toml",
          "type": "blob",
          "size": 1046
        },
        {
          "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/templates/tsconfig-sdk.json",
          "type": "blob",
          "size": 682
        },
        {
          "path": "plugins/clerk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 473
        },
        {
          "path": "plugins/clerk/.gitignore",
          "type": "blob",
          "size": 80
        },
        {
          "path": "plugins/clerk/CHANGELOG.md",
          "type": "blob",
          "size": 159
        },
        {
          "path": "plugins/clerk/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/clerk/README.md",
          "type": "blob",
          "size": 744
        },
        {
          "path": "plugins/clerk/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/agents/clerk-api-builder.md",
          "type": "blob",
          "size": 7991
        },
        {
          "path": "plugins/clerk/agents/clerk-auth-builder.md",
          "type": "blob",
          "size": 9278
        },
        {
          "path": "plugins/clerk/agents/clerk-billing-integrator.md",
          "type": "blob",
          "size": 8373
        },
        {
          "path": "plugins/clerk/agents/clerk-framework-detector.md",
          "type": "blob",
          "size": 9061
        },
        {
          "path": "plugins/clerk/agents/clerk-mfa-specialist.md",
          "type": "blob",
          "size": 9420
        },
        {
          "path": "plugins/clerk/agents/clerk-migration-agent.md",
          "type": "blob",
          "size": 10449
        },
        {
          "path": "plugins/clerk/agents/clerk-nextjs-app-router-agent.md",
          "type": "blob",
          "size": 8876
        },
        {
          "path": "plugins/clerk/agents/clerk-nextjs-pages-router-agent.md",
          "type": "blob",
          "size": 8179
        },
        {
          "path": "plugins/clerk/agents/clerk-oauth-specialist.md",
          "type": "blob",
          "size": 9071
        },
        {
          "path": "plugins/clerk/agents/clerk-organization-builder.md",
          "type": "blob",
          "size": 8620
        },
        {
          "path": "plugins/clerk/agents/clerk-setup-agent.md",
          "type": "blob",
          "size": 10085
        },
        {
          "path": "plugins/clerk/agents/clerk-supabase-integrator.md",
          "type": "blob",
          "size": 9343
        },
        {
          "path": "plugins/clerk/agents/clerk-ui-customizer.md",
          "type": "blob",
          "size": 7561
        },
        {
          "path": "plugins/clerk/agents/clerk-validator.md",
          "type": "blob",
          "size": 8498
        },
        {
          "path": "plugins/clerk/agents/clerk-vercel-ai-integrator.md",
          "type": "blob",
          "size": 8427
        },
        {
          "path": "plugins/clerk/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/commands/add-auth.md",
          "type": "blob",
          "size": 4375
        },
        {
          "path": "plugins/clerk/commands/add-billing.md",
          "type": "blob",
          "size": 3750
        },
        {
          "path": "plugins/clerk/commands/add-mfa.md",
          "type": "blob",
          "size": 4315
        },
        {
          "path": "plugins/clerk/commands/add-oauth.md",
          "type": "blob",
          "size": 4529
        },
        {
          "path": "plugins/clerk/commands/add-organizations.md",
          "type": "blob",
          "size": 5026
        },
        {
          "path": "plugins/clerk/commands/customize-ui.md",
          "type": "blob",
          "size": 4407
        },
        {
          "path": "plugins/clerk/commands/init.md",
          "type": "blob",
          "size": 5877
        },
        {
          "path": "plugins/clerk/commands/integrate-api.md",
          "type": "blob",
          "size": 4759
        },
        {
          "path": "plugins/clerk/commands/integrate-supabase.md",
          "type": "blob",
          "size": 5252
        },
        {
          "path": "plugins/clerk/commands/integrate-vercel-ai.md",
          "type": "blob",
          "size": 4757
        },
        {
          "path": "plugins/clerk/commands/start-guide.md",
          "type": "blob",
          "size": 6378
        },
        {
          "path": "plugins/clerk/commands/validate-setup.md",
          "type": "blob",
          "size": 3056
        },
        {
          "path": "plugins/clerk/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/docs/clerk-documentation-links.md",
          "type": "blob",
          "size": 15927
        },
        {
          "path": "plugins/clerk/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/hooks/hooks.json",
          "type": "blob",
          "size": 18
        },
        {
          "path": "plugins/clerk/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/api-authentication",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/api-authentication/SKILL.md",
          "type": "blob",
          "size": 8004
        },
        {
          "path": "plugins/clerk/skills/api-authentication/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/api-authentication/examples/graphql-api.md",
          "type": "blob",
          "size": 6181
        },
        {
          "path": "plugins/clerk/skills/api-authentication/examples/rest-api.md",
          "type": "blob",
          "size": 4030
        },
        {
          "path": "plugins/clerk/skills/api-authentication/examples/webhooks.md",
          "type": "blob",
          "size": 8031
        },
        {
          "path": "plugins/clerk/skills/api-authentication/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/api-authentication/scripts/generate-api-client.sh",
          "type": "blob",
          "size": 6539
        },
        {
          "path": "plugins/clerk/skills/api-authentication/scripts/setup-api-auth.sh",
          "type": "blob",
          "size": 5291
        },
        {
          "path": "plugins/clerk/skills/api-authentication/scripts/test-api-auth.sh",
          "type": "blob",
          "size": 5706
        },
        {
          "path": "plugins/clerk/skills/api-authentication/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/api-authentication/templates/api-middleware.ts",
          "type": "blob",
          "size": 5675
        },
        {
          "path": "plugins/clerk/skills/api-authentication/templates/api-routes.ts",
          "type": "blob",
          "size": 5589
        },
        {
          "path": "plugins/clerk/skills/api-authentication/templates/backend-sdk-setup.ts",
          "type": "blob",
          "size": 4614
        },
        {
          "path": "plugins/clerk/skills/api-authentication/templates/fastapi-middleware.py",
          "type": "blob",
          "size": 7264
        },
        {
          "path": "plugins/clerk/skills/auth-components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/auth-components/README.md",
          "type": "blob",
          "size": 9874
        },
        {
          "path": "plugins/clerk/skills/auth-components/SKILL.md",
          "type": "blob",
          "size": 9863
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/custom-sign-in-guide.md",
          "type": "blob",
          "size": 4108
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/custom-sign-in.tsx",
          "type": "blob",
          "size": 12416
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/social-authentication-guide.md",
          "type": "blob",
          "size": 6639
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/social-buttons.tsx",
          "type": "blob",
          "size": 10499
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/theme-config.tsx",
          "type": "blob",
          "size": 8342
        },
        {
          "path": "plugins/clerk/skills/auth-components/examples/theming-guide.md",
          "type": "blob",
          "size": 8389
        },
        {
          "path": "plugins/clerk/skills/auth-components/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/auth-components/scripts/customize-appearance.sh",
          "type": "blob",
          "size": 5286
        },
        {
          "path": "plugins/clerk/skills/auth-components/scripts/generate-auth-ui.sh",
          "type": "blob",
          "size": 3590
        },
        {
          "path": "plugins/clerk/skills/auth-components/scripts/validate-components.sh",
          "type": "blob",
          "size": 5003
        },
        {
          "path": "plugins/clerk/skills/auth-components/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/auth-components/templates/protected-wrapper.tsx",
          "type": "blob",
          "size": 2355
        },
        {
          "path": "plugins/clerk/skills/auth-components/templates/sign-in-page.tsx",
          "type": "blob",
          "size": 976
        },
        {
          "path": "plugins/clerk/skills/auth-components/templates/sign-up-page.tsx",
          "type": "blob",
          "size": 976
        },
        {
          "path": "plugins/clerk/skills/auth-components/templates/user-button-custom.tsx",
          "type": "blob",
          "size": 2249
        },
        {
          "path": "plugins/clerk/skills/billing-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/billing-integration/SKILL.md",
          "type": "blob",
          "size": 14677
        },
        {
          "path": "plugins/clerk/skills/billing-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/billing-integration/examples/feature-gate-pattern.tsx",
          "type": "blob",
          "size": 10412
        },
        {
          "path": "plugins/clerk/skills/billing-integration/examples/organization-billing.tsx",
          "type": "blob",
          "size": 12775
        },
        {
          "path": "plugins/clerk/skills/billing-integration/examples/saas-billing-flow.tsx",
          "type": "blob",
          "size": 12127
        },
        {
          "path": "plugins/clerk/skills/billing-integration/examples/usage-based-billing.tsx",
          "type": "blob",
          "size": 12878
        },
        {
          "path": "plugins/clerk/skills/billing-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/billing-integration/scripts/configure-plans.sh",
          "type": "blob",
          "size": 8013
        },
        {
          "path": "plugins/clerk/skills/billing-integration/scripts/setup-billing.sh",
          "type": "blob",
          "size": 5277
        },
        {
          "path": "plugins/clerk/skills/billing-integration/scripts/setup-webhooks.sh",
          "type": "blob",
          "size": 10474
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/checkout-flow.tsx",
          "type": "blob",
          "size": 10928
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/pricing-page.tsx",
          "type": "blob",
          "size": 10424
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/subscription-management.tsx",
          "type": "blob",
          "size": 11843
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/webhook-handlers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/webhook-handlers/payment-succeeded.ts",
          "type": "blob",
          "size": 3379
        },
        {
          "path": "plugins/clerk/skills/billing-integration/templates/webhook-handlers/subscription-created.ts",
          "type": "blob",
          "size": 2831
        },
        {
          "path": "plugins/clerk/skills/middleware-protection",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/SKILL.md",
          "type": "blob",
          "size": 7716
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/api-middleware.ts",
          "type": "blob",
          "size": 3581
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/basic-middleware.ts",
          "type": "blob",
          "size": 2321
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/conditional-routing.ts",
          "type": "blob",
          "size": 7515
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/organization-routes.ts",
          "type": "blob",
          "size": 5114
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/public-private-routes.ts",
          "type": "blob",
          "size": 4100
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/role-based-protection.ts",
          "type": "blob",
          "size": 6213
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/examples/validate_routes.py",
          "type": "blob",
          "size": 8880
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/scripts/configure-routes.sh",
          "type": "blob",
          "size": 6655
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/scripts/generate-middleware.sh",
          "type": "blob",
          "size": 5602
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/scripts/test-protection.sh",
          "type": "blob",
          "size": 7997
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/scripts/validate-middleware.sh",
          "type": "blob",
          "size": 9119
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/templates/middleware.ts",
          "type": "blob",
          "size": 1045
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/templates/organization-middleware.ts",
          "type": "blob",
          "size": 2004
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/templates/role-based-middleware.ts",
          "type": "blob",
          "size": 1988
        },
        {
          "path": "plugins/clerk/skills/middleware-protection/templates/route-matchers.ts",
          "type": "blob",
          "size": 3347
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/README.md",
          "type": "blob",
          "size": 4247
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/SKILL.md",
          "type": "blob",
          "size": 10908
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/VALIDATION_SUMMARY.md",
          "type": "blob",
          "size": 3851
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/examples/client-component-auth.tsx",
          "type": "blob",
          "size": 10145
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/examples/protected-route.tsx",
          "type": "blob",
          "size": 3755
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/examples/server-component-auth.tsx",
          "type": "blob",
          "size": 9611
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/scripts/configure-middleware.sh",
          "type": "blob",
          "size": 6999
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/scripts/install-clerk.sh",
          "type": "blob",
          "size": 3193
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/scripts/setup-app-router.sh",
          "type": "blob",
          "size": 4966
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/scripts/setup-pages-router.sh",
          "type": "blob",
          "size": 5427
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/app-router",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/app-router/layout.tsx",
          "type": "blob",
          "size": 826
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/app-router/middleware.ts",
          "type": "blob",
          "size": 1566
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/pages-router",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/pages-router/_app.tsx",
          "type": "blob",
          "size": 515
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/pages-router/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/nextjs-integration/templates/pages-router/api/auth.ts",
          "type": "blob",
          "size": 1026
        },
        {
          "path": "plugins/clerk/skills/oauth-providers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/SKILL.md",
          "type": "blob",
          "size": 13922
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/examples/enterprise-sso-setup.md",
          "type": "blob",
          "size": 12394
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/examples/gaming-platform-oauth.md",
          "type": "blob",
          "size": 13457
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/examples/multi-provider-setup.md",
          "type": "blob",
          "size": 7992
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/examples/oauth-button-component.tsx",
          "type": "blob",
          "size": 9991
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/scripts/generate-redirect-urls.sh",
          "type": "blob",
          "size": 5920
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/scripts/setup-provider.sh",
          "type": "blob",
          "size": 12330
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/scripts/test-oauth-flow.sh",
          "type": "blob",
          "size": 10639
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/apple",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/apple/clerk-config.ts",
          "type": "blob",
          "size": 1525
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/discord",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/discord/clerk-config.ts",
          "type": "blob",
          "size": 3989
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/facebook",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/facebook/clerk-config.ts",
          "type": "blob",
          "size": 1459
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/github",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/github/clerk-config.ts",
          "type": "blob",
          "size": 3337
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/google",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/google/clerk-config.ts",
          "type": "blob",
          "size": 2108
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/linkedin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/linkedin/clerk-config.ts",
          "type": "blob",
          "size": 1233
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/microsoft",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/microsoft/clerk-config.ts",
          "type": "blob",
          "size": 3398
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/oauth-shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/oauth-shared/AuthButtons.tsx",
          "type": "blob",
          "size": 6189
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/oauth-shared/clerk-dashboard-config.ts",
          "type": "blob",
          "size": 7540
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/twitter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/oauth-providers/templates/twitter/clerk-config.ts",
          "type": "blob",
          "size": 1294
        },
        {
          "path": "plugins/clerk/skills/organization-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/organization-management/README.md",
          "type": "blob",
          "size": 7686
        },
        {
          "path": "plugins/clerk/skills/organization-management/SKILL.md",
          "type": "blob",
          "size": 9303
        },
        {
          "path": "plugins/clerk/skills/organization-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/organization-management/examples/multi-tenant-app.tsx",
          "type": "blob",
          "size": 10857
        },
        {
          "path": "plugins/clerk/skills/organization-management/examples/org-admin-dashboard.tsx",
          "type": "blob",
          "size": 13955
        },
        {
          "path": "plugins/clerk/skills/organization-management/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/organization-management/scripts/configure-roles.sh",
          "type": "blob",
          "size": 8200
        },
        {
          "path": "plugins/clerk/skills/organization-management/scripts/setup-organizations.sh",
          "type": "blob",
          "size": 4665
        },
        {
          "path": "plugins/clerk/skills/organization-management/scripts/test-org-isolation.sh",
          "type": "blob",
          "size": 5371
        },
        {
          "path": "plugins/clerk/skills/organization-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/organization-management/templates/organization-schema.md",
          "type": "blob",
          "size": 10484
        },
        {
          "path": "plugins/clerk/skills/organization-management/templates/organization-switcher.tsx",
          "type": "blob",
          "size": 10734
        },
        {
          "path": "plugins/clerk/skills/organization-management/templates/rbac-policies.ts",
          "type": "blob",
          "size": 9199
        },
        {
          "path": "plugins/clerk/skills/session-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/session-management/README.md",
          "type": "blob",
          "size": 8861
        },
        {
          "path": "plugins/clerk/skills/session-management/SKILL.md",
          "type": "blob",
          "size": 10913
        },
        {
          "path": "plugins/clerk/skills/session-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/session-management/examples/multi-session.tsx",
          "type": "blob",
          "size": 8710
        },
        {
          "path": "plugins/clerk/skills/session-management/examples/session-debugging.ts",
          "type": "blob",
          "size": 10887
        },
        {
          "path": "plugins/clerk/skills/session-management/examples/session-refresh.ts",
          "type": "blob",
          "size": 10239
        },
        {
          "path": "plugins/clerk/skills/session-management/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/session-management/scripts/configure-sessions.sh",
          "type": "blob",
          "size": 6786
        },
        {
          "path": "plugins/clerk/skills/session-management/scripts/setup-jwt.sh",
          "type": "blob",
          "size": 7889
        },
        {
          "path": "plugins/clerk/skills/session-management/scripts/test-sessions.sh",
          "type": "blob",
          "size": 12441
        },
        {
          "path": "plugins/clerk/skills/session-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/session-management/templates/custom-claims.ts",
          "type": "blob",
          "size": 8298
        },
        {
          "path": "plugins/clerk/skills/session-management/templates/jwt-verification.ts",
          "type": "blob",
          "size": 7807
        },
        {
          "path": "plugins/clerk/skills/session-management/templates/session-config.ts",
          "type": "blob",
          "size": 5265
        },
        {
          "path": "plugins/clerk/skills/session-management/templates/session-types.ts",
          "type": "blob",
          "size": 7387
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/SKILL.md",
          "type": "blob",
          "size": 8703
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/examples/complete-integration.tsx",
          "type": "blob",
          "size": 14931
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/examples/organization-access.tsx",
          "type": "blob",
          "size": 8668
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/examples/protected-route.tsx",
          "type": "blob",
          "size": 5874
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/examples/webhook-handler.ts",
          "type": "blob",
          "size": 10651
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts/configure-rls.sh",
          "type": "blob",
          "size": 8644
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts/create-webhooks.sh",
          "type": "blob",
          "size": 10866
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts/setup-sync.sh",
          "type": "blob",
          "size": 6011
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts/sync-users.sh",
          "type": "blob",
          "size": 5043
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/scripts/test-jwt.sh",
          "type": "blob",
          "size": 5229
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/clerk-jwt-template.json",
          "type": "blob",
          "size": 1032
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/edge-function-webhook.ts",
          "type": "blob",
          "size": 5496
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/env.example",
          "type": "blob",
          "size": 1237
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/middleware-auth.ts",
          "type": "blob",
          "size": 3223
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/rls-policies-clerk.sql",
          "type": "blob",
          "size": 11842
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/supabase-client-clerk.ts",
          "type": "blob",
          "size": 5832
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/triggers.sql",
          "type": "blob",
          "size": 9581
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/user-schema.sql",
          "type": "blob",
          "size": 5737
        },
        {
          "path": "plugins/clerk/skills/supabase-clerk-sync/templates/webhook-sync.ts",
          "type": "blob",
          "size": 9224
        },
        {
          "path": "plugins/clerk/skills/testing-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/SKILL.md",
          "type": "blob",
          "size": 10477
        },
        {
          "path": "plugins/clerk/skills/testing-validation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/examples/auth-flow-tests.spec.ts",
          "type": "blob",
          "size": 12850
        },
        {
          "path": "plugins/clerk/skills/testing-validation/examples/clerk-unit-tests.test.tsx",
          "type": "blob",
          "size": 12376
        },
        {
          "path": "plugins/clerk/skills/testing-validation/examples/security-audit.ts",
          "type": "blob",
          "size": 15707
        },
        {
          "path": "plugins/clerk/skills/testing-validation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/scripts/check-security.sh",
          "type": "blob",
          "size": 9535
        },
        {
          "path": "plugins/clerk/skills/testing-validation/scripts/test-auth-flows.sh",
          "type": "blob",
          "size": 4795
        },
        {
          "path": "plugins/clerk/skills/testing-validation/scripts/validate-setup.sh",
          "type": "blob",
          "size": 7342
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/e2e-tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/e2e-tests/clerk-auth-flows.spec.ts",
          "type": "blob",
          "size": 11667
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/e2e-tests/clerk-protected-routes.spec.ts",
          "type": "blob",
          "size": 10567
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/test-suites",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/test-suites/clerk-api.test.ts",
          "type": "blob",
          "size": 9699
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/test-suites/clerk-nextjs.test.tsx",
          "type": "blob",
          "size": 8234
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/test-suites/clerk-react.test.tsx",
          "type": "blob",
          "size": 7691
        },
        {
          "path": "plugins/clerk/skills/testing-validation/templates/validation-checklist.md",
          "type": "blob",
          "size": 8542
        },
        {
          "path": "plugins/elevenlabs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 598
        },
        {
          "path": "plugins/elevenlabs/README.md",
          "type": "blob",
          "size": 7392
        },
        {
          "path": "plugins/elevenlabs/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-agents-builder.md",
          "type": "blob",
          "size": 6595
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-production-agent.md",
          "type": "blob",
          "size": 6423
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-setup.md",
          "type": "blob",
          "size": 9999
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-stt-integrator.md",
          "type": "blob",
          "size": 6456
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-tts-integrator.md",
          "type": "blob",
          "size": 6289
        },
        {
          "path": "plugins/elevenlabs/agents/elevenlabs-voice-manager.md",
          "type": "blob",
          "size": 6232
        },
        {
          "path": "plugins/elevenlabs/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/commands/add-advanced-features.md",
          "type": "blob",
          "size": 7302
        },
        {
          "path": "plugins/elevenlabs/commands/add-agents-platform.md",
          "type": "blob",
          "size": 7537
        },
        {
          "path": "plugins/elevenlabs/commands/add-production.md",
          "type": "blob",
          "size": 7885
        },
        {
          "path": "plugins/elevenlabs/commands/add-speech-to-text.md",
          "type": "blob",
          "size": 7536
        },
        {
          "path": "plugins/elevenlabs/commands/add-streaming.md",
          "type": "blob",
          "size": 7364
        },
        {
          "path": "plugins/elevenlabs/commands/add-text-to-speech.md",
          "type": "blob",
          "size": 7905
        },
        {
          "path": "plugins/elevenlabs/commands/add-vercel-ai-sdk.md",
          "type": "blob",
          "size": 6868
        },
        {
          "path": "plugins/elevenlabs/commands/add-voice-management.md",
          "type": "blob",
          "size": 6927
        },
        {
          "path": "plugins/elevenlabs/commands/init.md",
          "type": "blob",
          "size": 6989
        },
        {
          "path": "plugins/elevenlabs/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/docs/sdk-documentation.md",
          "type": "blob",
          "size": 6808
        },
        {
          "path": "plugins/elevenlabs/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/SKILL.md",
          "type": "blob",
          "size": 6608
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/api-key-rotation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/api-key-rotation/README.md",
          "type": "blob",
          "size": 11431
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/edge-runtime",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/edge-runtime/README.md",
          "type": "blob",
          "size": 8677
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/multi-environment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/multi-environment/README.md",
          "type": "blob",
          "size": 8982
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/nextjs-auth",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/nextjs-auth/README.md",
          "type": "blob",
          "size": 8500
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/python-auth",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/examples/python-auth/README.md",
          "type": "blob",
          "size": 8872
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts/generate-client.sh",
          "type": "blob",
          "size": 5435
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts/install-sdk.sh",
          "type": "blob",
          "size": 5166
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts/setup-auth.sh",
          "type": "blob",
          "size": 3492
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts/test-connection.sh",
          "type": "blob",
          "size": 5221
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/scripts/validate-env.sh",
          "type": "blob",
          "size": 3123
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/.env.template",
          "type": "blob",
          "size": 748
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client-async.py.template",
          "type": "blob",
          "size": 8823
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client-edge.ts.template",
          "type": "blob",
          "size": 4818
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client-fastapi.py.template",
          "type": "blob",
          "size": 8558
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client-nextjs.ts.template",
          "type": "blob",
          "size": 4037
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client.py.template",
          "type": "blob",
          "size": 5708
        },
        {
          "path": "plugins/elevenlabs/skills/api-authentication/templates/api-client.ts.template",
          "type": "blob",
          "size": 3689
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/README.md",
          "type": "blob",
          "size": 8158
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/custom-mcp-server",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/custom-mcp-server/README.md",
          "type": "blob",
          "size": 8999
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/security-controls",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/security-controls/README.md",
          "type": "blob",
          "size": 10474
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/zapier-mcp-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/zapier-mcp-agent/README.md",
          "type": "blob",
          "size": 6183
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/examples/zapier-mcp-agent/agent-config.json",
          "type": "blob",
          "size": 2575
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/configure-mcp.sh",
          "type": "blob",
          "size": 6963
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/monitor-mcp-health.sh",
          "type": "blob",
          "size": 5679
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/rotate-mcp-tokens.sh",
          "type": "blob",
          "size": 6909
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/setup-mcp-caching.sh",
          "type": "blob",
          "size": 14005
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/setup-zapier-mcp.sh",
          "type": "blob",
          "size": 8895
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/test-mcp-connection.sh",
          "type": "blob",
          "size": 6761
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/scripts/validate-tool-permissions.sh",
          "type": "blob",
          "size": 9570
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates/agent-mcp-config.json.template",
          "type": "blob",
          "size": 1651
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates/mcp-server-config.json.template",
          "type": "blob",
          "size": 920
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates/multi-mcp-config.json.template",
          "type": "blob",
          "size": 5010
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates/tool-approval-config.json.template",
          "type": "blob",
          "size": 4994
        },
        {
          "path": "plugins/elevenlabs/skills/mcp-integration/templates/zapier-mcp-config.json.template",
          "type": "blob",
          "size": 4458
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/SKILL.md",
          "type": "blob",
          "size": 13491
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/error-handling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/error-handling/README.md",
          "type": "blob",
          "size": 11215
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/monitoring",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/monitoring/README.md",
          "type": "blob",
          "size": 12251
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/rate-limiting",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/examples/rate-limiting/README.md",
          "type": "blob",
          "size": 8073
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts/deploy-production.sh",
          "type": "blob",
          "size": 11481
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts/rollback.sh",
          "type": "blob",
          "size": 2108
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts/setup-monitoring.sh",
          "type": "blob",
          "size": 22371
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts/test-rate-limiting.sh",
          "type": "blob",
          "size": 15567
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/scripts/validate-config.sh",
          "type": "blob",
          "size": 5738
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/error-handler.js.template",
          "type": "blob",
          "size": 9793
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/error-handler.py.template",
          "type": "blob",
          "size": 14321
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/health-check.js.template",
          "type": "blob",
          "size": 7094
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/monitoring-config.json.template",
          "type": "blob",
          "size": 7690
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/rate-limiter.js.template",
          "type": "blob",
          "size": 7489
        },
        {
          "path": "plugins/elevenlabs/skills/production-deployment/templates/rate-limiter.py.template",
          "type": "blob",
          "size": 11241
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/README.md",
          "type": "blob",
          "size": 10178
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/SKILL.md",
          "type": "blob",
          "size": 7561
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/basic-stt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/basic-stt/README.md",
          "type": "blob",
          "size": 4396
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/diarization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/diarization/README.md",
          "type": "blob",
          "size": 10830
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/multi-language",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/multi-language/README.md",
          "type": "blob",
          "size": 11202
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/vercel-ai-stt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/vercel-ai-stt/README.md",
          "type": "blob",
          "size": 9545
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/webhook-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/examples/webhook-integration/README.md",
          "type": "blob",
          "size": 13796
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts/batch-transcribe.sh",
          "type": "blob",
          "size": 7717
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts/setup-vercel-ai.sh",
          "type": "blob",
          "size": 11195
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts/test-stt.sh",
          "type": "blob",
          "size": 11393
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts/transcribe-audio.sh",
          "type": "blob",
          "size": 7719
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/scripts/validate-audio.sh",
          "type": "blob",
          "size": 10997
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/api-transcribe.py.template",
          "type": "blob",
          "size": 14809
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/api-transcribe.ts.template",
          "type": "blob",
          "size": 10896
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/diarization-config.json.template",
          "type": "blob",
          "size": 9582
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/stt-config.json.template",
          "type": "blob",
          "size": 4388
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/vercel-ai-transcribe.py.template",
          "type": "blob",
          "size": 15554
        },
        {
          "path": "plugins/elevenlabs/skills/stt-integration/templates/vercel-ai-transcribe.ts.template",
          "type": "blob",
          "size": 10575
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/README.md",
          "type": "blob",
          "size": 12310
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/basic-tts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/basic-tts/README.md",
          "type": "blob",
          "size": 7362
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/basic-tts/basic-example.sh",
          "type": "blob",
          "size": 5621
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/basic-tts/sample-texts.txt",
          "type": "blob",
          "size": 1244
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/multi-voice",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/multi-voice/README.md",
          "type": "blob",
          "size": 9201
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/multi-voice/compare-models.sh",
          "type": "blob",
          "size": 4866
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/multi-voice/dialogue-script.txt",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/multi-voice/multi-speaker-dialogue.sh",
          "type": "blob",
          "size": 4077
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/streaming-tts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/streaming-tts/README.md",
          "type": "blob",
          "size": 10468
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/streaming-tts/client-example.js",
          "type": "blob",
          "size": 11166
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/examples/streaming-tts/stream-example.sh",
          "type": "blob",
          "size": 910
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts/batch-generate.sh",
          "type": "blob",
          "size": 14327
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts/convert-audio.sh",
          "type": "blob",
          "size": 13144
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts/optimize-settings.sh",
          "type": "blob",
          "size": 14379
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts/select-model.sh",
          "type": "blob",
          "size": 12431
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/scripts/test-tts.sh",
          "type": "blob",
          "size": 12291
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/templates/batch-config.json.template",
          "type": "blob",
          "size": 4459
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/templates/streaming-config.json.template",
          "type": "blob",
          "size": 3175
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/templates/tts-config.json.template",
          "type": "blob",
          "size": 2991
        },
        {
          "path": "plugins/elevenlabs/skills/tts-integration/templates/voice-settings.json.template",
          "type": "blob",
          "size": 9144
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/README.md",
          "type": "blob",
          "size": 9872
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/multi-modal-chat",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/multi-modal-chat/README.md",
          "type": "blob",
          "size": 7725
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/nextjs-transcription",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/nextjs-transcription/README.md",
          "type": "blob",
          "size": 7740
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/speaker-detection",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/speaker-detection/README.md",
          "type": "blob",
          "size": 13745
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/streaming-example",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/streaming-example/README.md",
          "type": "blob",
          "size": 12324
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/voice-workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/examples/voice-workflow/README.md",
          "type": "blob",
          "size": 10782
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts/benchmark-transcription.sh",
          "type": "blob",
          "size": 8796
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts/create-api-route.sh",
          "type": "blob",
          "size": 12498
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts/setup-vercel-ai.sh",
          "type": "blob",
          "size": 5531
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts/test-transcription.sh",
          "type": "blob",
          "size": 8560
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/scripts/validate-integration.sh",
          "type": "blob",
          "size": 8531
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/api-route.ts.template",
          "type": "blob",
          "size": 4957
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/multi-modal-chat.tsx.template",
          "type": "blob",
          "size": 9268
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/speaker-diarization.ts.template",
          "type": "blob",
          "size": 9091
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/streaming-transcription.ts.template",
          "type": "blob",
          "size": 9072
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/transcribe-hook.ts.template",
          "type": "blob",
          "size": 5951
        },
        {
          "path": "plugins/elevenlabs/skills/vercel-ai-patterns/templates/voice-workflow.ts.template",
          "type": "blob",
          "size": 9723
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/SKILL.md",
          "type": "blob",
          "size": 10769
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/audio-processing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/audio-processing/README.md",
          "type": "blob",
          "size": 2227
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/audio-processing/batch-process.sh",
          "type": "blob",
          "size": 1172
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/audio-processing/convert-formats.sh",
          "type": "blob",
          "size": 753
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/audio-processing/prepare-for-cloning.sh",
          "type": "blob",
          "size": 616
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/instant-cloning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/instant-cloning/README.md",
          "type": "blob",
          "size": 6252
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/instant-cloning/clone-workflow.sh",
          "type": "blob",
          "size": 4132
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/instant-cloning/verify-clone.sh",
          "type": "blob",
          "size": 2574
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/professional-cloning",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/professional-cloning/README.md",
          "type": "blob",
          "size": 11094
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/professional-cloning/clone-workflow.sh",
          "type": "blob",
          "size": 6028
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/professional-cloning/training-guide.md",
          "type": "blob",
          "size": 8721
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-library",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-library/README.md",
          "type": "blob",
          "size": 3037
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-library/add-to-collection.sh",
          "type": "blob",
          "size": 456
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-library/search-voices.sh",
          "type": "blob",
          "size": 1123
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-library/share-voice.sh",
          "type": "blob",
          "size": 589
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-settings-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-settings-optimization/README.md",
          "type": "blob",
          "size": 2355
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-settings-optimization/optimize-similarity.sh",
          "type": "blob",
          "size": 1288
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-settings-optimization/optimize-stability.sh",
          "type": "blob",
          "size": 1400
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/examples/voice-settings-optimization/test-settings.sh",
          "type": "blob",
          "size": 1217
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts/clone-voice.sh",
          "type": "blob",
          "size": 7274
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts/configure-voice-settings.sh",
          "type": "blob",
          "size": 10214
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts/list-voices.sh",
          "type": "blob",
          "size": 6713
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts/manage-voice-library.sh",
          "type": "blob",
          "size": 10997
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/scripts/process-audio.py",
          "type": "blob",
          "size": 11605
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/audio-processing-config.json.template",
          "type": "blob",
          "size": 2824
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/batch-clone-config.json.template",
          "type": "blob",
          "size": 3591
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/voice-clone-config.json.template",
          "type": "blob",
          "size": 651
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/voice-library-entry.json.template",
          "type": "blob",
          "size": 1230
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/voice-settings.json.template",
          "type": "blob",
          "size": 2636
        },
        {
          "path": "plugins/elevenlabs/skills/voice-processing/templates/voice-verification.json.template",
          "type": "blob",
          "size": 2913
        },
        {
          "path": "plugins/fastapi-backend",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 436
        },
        {
          "path": "plugins/fastapi-backend/.gitignore",
          "type": "blob",
          "size": 1528
        },
        {
          "path": "plugins/fastapi-backend/CHANGELOG.md",
          "type": "blob",
          "size": 2686
        },
        {
          "path": "plugins/fastapi-backend/LICENSE",
          "type": "blob",
          "size": 1083
        },
        {
          "path": "plugins/fastapi-backend/README.md",
          "type": "blob",
          "size": 670
        },
        {
          "path": "plugins/fastapi-backend/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/agents/database-architect-agent.md",
          "type": "blob",
          "size": 13098
        },
        {
          "path": "plugins/fastapi-backend/agents/deployment-architect-agent.md",
          "type": "blob",
          "size": 13460
        },
        {
          "path": "plugins/fastapi-backend/agents/endpoint-generator-agent.md",
          "type": "blob",
          "size": 13473
        },
        {
          "path": "plugins/fastapi-backend/agents/fastapi-setup-agent.md",
          "type": "blob",
          "size": 13901
        },
        {
          "path": "plugins/fastapi-backend/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/commands/add-auth.md",
          "type": "blob",
          "size": 9350
        },
        {
          "path": "plugins/fastapi-backend/commands/add-endpoint.md",
          "type": "blob",
          "size": 8442
        },
        {
          "path": "plugins/fastapi-backend/commands/add-testing.md",
          "type": "blob",
          "size": 8909
        },
        {
          "path": "plugins/fastapi-backend/commands/init-ai-app.md",
          "type": "blob",
          "size": 9696
        },
        {
          "path": "plugins/fastapi-backend/commands/init.md",
          "type": "blob",
          "size": 9557
        },
        {
          "path": "plugins/fastapi-backend/commands/integrate-mem0.md",
          "type": "blob",
          "size": 9266
        },
        {
          "path": "plugins/fastapi-backend/commands/search-examples.md",
          "type": "blob",
          "size": 8882
        },
        {
          "path": "plugins/fastapi-backend/commands/setup-database.md",
          "type": "blob",
          "size": 8481
        },
        {
          "path": "plugins/fastapi-backend/commands/setup-deployment.md",
          "type": "blob",
          "size": 7281
        },
        {
          "path": "plugins/fastapi-backend/commands/validate-api.md",
          "type": "blob",
          "size": 8847
        },
        {
          "path": "plugins/fastapi-backend/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/hooks/hooks.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/fastapi-backend/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/README.md",
          "type": "blob",
          "size": 5134
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/SKILL.md",
          "type": "blob",
          "size": 13827
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/examples/async_context_examples.py",
          "type": "blob",
          "size": 12650
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/examples/query_patterns.py",
          "type": "blob",
          "size": 9887
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/examples/user_model.py",
          "type": "blob",
          "size": 8102
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/scripts/generate-migration.sh",
          "type": "blob",
          "size": 1480
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/scripts/optimize-queries.sh",
          "type": "blob",
          "size": 1825
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/scripts/setup-alembic.sh",
          "type": "blob",
          "size": 3437
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/scripts/test-connection.sh",
          "type": "blob",
          "size": 2135
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/templates/alembic.ini",
          "type": "blob",
          "size": 1451
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/templates/base_model.py",
          "type": "blob",
          "size": 4525
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/templates/env.py",
          "type": "blob",
          "size": 3314
        },
        {
          "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/templates/session_manager.py",
          "type": "blob",
          "size": 6656
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/README.md",
          "type": "blob",
          "size": 5387
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/SKILL.md",
          "type": "blob",
          "size": 13611
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/examples/chat_api.py",
          "type": "blob",
          "size": 13363
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/examples/memory_endpoints.py",
          "type": "blob",
          "size": 15705
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/examples/user_management.py",
          "type": "blob",
          "size": 14292
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/scripts/generate-openapi-docs.sh",
          "type": "blob",
          "size": 8655
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/scripts/validate-endpoints.sh",
          "type": "blob",
          "size": 6502
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/templates/crud_endpoint.py",
          "type": "blob",
          "size": 11129
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/templates/error_handling.py",
          "type": "blob",
          "size": 12601
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/templates/pagination.py",
          "type": "blob",
          "size": 11487
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/README.md",
          "type": "blob",
          "size": 10775
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/SKILL.md",
          "type": "blob",
          "size": 10410
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/examples/permission_system.py",
          "type": "blob",
          "size": 18389
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/examples/protected_routes.py",
          "type": "blob",
          "size": 14095
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/scripts/setup-jwt.sh",
          "type": "blob",
          "size": 5973
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/scripts/validate-auth.sh",
          "type": "blob",
          "size": 5576
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/templates/jwt_auth.py",
          "type": "blob",
          "size": 9580
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/templates/oauth2_flow.py",
          "type": "blob",
          "size": 10293
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/templates/supabase_auth.py",
          "type": "blob",
          "size": 12259
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/README.md",
          "type": "blob",
          "size": 11782
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/SKILL.md",
          "type": "blob",
          "size": 9295
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/examples/aws_setup.md",
          "type": "blob",
          "size": 14458
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/examples/digitalocean_setup.md",
          "type": "blob",
          "size": 13015
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/examples/railway_setup.md",
          "type": "blob",
          "size": 10684
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/scripts/build-docker.sh",
          "type": "blob",
          "size": 6399
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/scripts/health-check.sh",
          "type": "blob",
          "size": 8760
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/scripts/validate-deployment.sh",
          "type": "blob",
          "size": 12523
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/.env.example",
          "type": "blob",
          "size": 1638
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/.env.production.example",
          "type": "blob",
          "size": 3326
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/Dockerfile",
          "type": "blob",
          "size": 2044
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/digitalocean-app.yaml",
          "type": "blob",
          "size": 3443
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/docker-compose.yml",
          "type": "blob",
          "size": 2706
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/nginx.conf",
          "type": "blob",
          "size": 7500
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/templates/railway.json",
          "type": "blob",
          "size": 366
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/README.md",
          "type": "blob",
          "size": 6944
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/SKILL.md",
          "type": "blob",
          "size": 12937
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/examples/mcp-integrated-example.md",
          "type": "blob",
          "size": 7381
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/examples/minimal-api-example.md",
          "type": "blob",
          "size": 3542
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/examples/production-best-practices.md",
          "type": "blob",
          "size": 12973
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/scripts/setup-project.sh",
          "type": "blob",
          "size": 10460
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/scripts/validate-structure.sh",
          "type": "blob",
          "size": 7516
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/config-template.py",
          "type": "blob",
          "size": 1536
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/dockerfile-template",
          "type": "blob",
          "size": 1146
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/health-route-template.py",
          "type": "blob",
          "size": 1855
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/main-template.py",
          "type": "blob",
          "size": 2140
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/mcp-config-template.json",
          "type": "blob",
          "size": 234
        },
        {
          "path": "plugins/fastapi-backend/skills/fastapi-project-structure/templates/pyproject-template.toml",
          "type": "blob",
          "size": 1484
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/README.md",
          "type": "blob",
          "size": 5944
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/SKILL.md",
          "type": "blob",
          "size": 17529
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/examples/chat_with_memory.py",
          "type": "blob",
          "size": 9228
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/examples/user_preferences.py",
          "type": "blob",
          "size": 10990
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/scripts/setup-mem0.sh",
          "type": "blob",
          "size": 4116
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/scripts/test-memory.sh",
          "type": "blob",
          "size": 5492
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/templates/memory_client.py",
          "type": "blob",
          "size": 6584
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/templates/memory_middleware.py",
          "type": "blob",
          "size": 5422
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/templates/memory_routes.py",
          "type": "blob",
          "size": 12102
        },
        {
          "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/templates/memory_service.py",
          "type": "blob",
          "size": 11508
        },
        {
          "path": "plugins/google-adk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 484
        },
        {
          "path": "plugins/google-adk/.gitignore",
          "type": "blob",
          "size": 470
        },
        {
          "path": "plugins/google-adk/CHANGELOG.md",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/google-adk/LICENSE",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/google-adk/README.md",
          "type": "blob",
          "size": 409
        },
        {
          "path": "plugins/google-adk/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/agents/google-adk-a2a-specialist.md",
          "type": "blob",
          "size": 9380
        },
        {
          "path": "plugins/google-adk/agents/google-adk-agent-builder.md",
          "type": "blob",
          "size": 9547
        },
        {
          "path": "plugins/google-adk/agents/google-adk-deployment-specialist.md",
          "type": "blob",
          "size": 9391
        },
        {
          "path": "plugins/google-adk/agents/google-adk-evaluation-specialist.md",
          "type": "blob",
          "size": 9916
        },
        {
          "path": "plugins/google-adk/agents/google-adk-interactions-specialist.md",
          "type": "blob",
          "size": 20448
        },
        {
          "path": "plugins/google-adk/agents/google-adk-observability-integrator.md",
          "type": "blob",
          "size": 11872
        },
        {
          "path": "plugins/google-adk/agents/google-adk-setup-agent.md",
          "type": "blob",
          "size": 10457
        },
        {
          "path": "plugins/google-adk/agents/google-adk-streaming-specialist.md",
          "type": "blob",
          "size": 9715
        },
        {
          "path": "plugins/google-adk/agents/google-adk-tools-integrator.md",
          "type": "blob",
          "size": 8445
        },
        {
          "path": "plugins/google-adk/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/commands/add-a2a.md",
          "type": "blob",
          "size": 3854
        },
        {
          "path": "plugins/google-adk/commands/add-agent.md",
          "type": "blob",
          "size": 3630
        },
        {
          "path": "plugins/google-adk/commands/add-evaluation.md",
          "type": "blob",
          "size": 4068
        },
        {
          "path": "plugins/google-adk/commands/add-observability.md",
          "type": "blob",
          "size": 3932
        },
        {
          "path": "plugins/google-adk/commands/add-streaming.md",
          "type": "blob",
          "size": 4186
        },
        {
          "path": "plugins/google-adk/commands/add-tools.md",
          "type": "blob",
          "size": 3600
        },
        {
          "path": "plugins/google-adk/commands/build-full-stack.md",
          "type": "blob",
          "size": 4961
        },
        {
          "path": "plugins/google-adk/commands/deploy.md",
          "type": "blob",
          "size": 3491
        },
        {
          "path": "plugins/google-adk/commands/init.md",
          "type": "blob",
          "size": 3065
        },
        {
          "path": "plugins/google-adk/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/SKILL.md",
          "type": "blob",
          "size": 16734
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/examples/README.md",
          "type": "blob",
          "size": 7052
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/examples/ecommerce-assistant.py",
          "type": "blob",
          "size": 8601
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/examples/research-cluster.py",
          "type": "blob",
          "size": 8070
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/scripts/consume-agent.sh",
          "type": "blob",
          "size": 2605
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/scripts/expose-agent.sh",
          "type": "blob",
          "size": 1808
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/scripts/generate-agent-card.sh",
          "type": "blob",
          "size": 2629
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/scripts/validate-a2a.sh",
          "type": "blob",
          "size": 2840
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates/a2a-client.py",
          "type": "blob",
          "size": 5461
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates/a2a-server.py",
          "type": "blob",
          "size": 3309
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates/agent-card.json",
          "type": "blob",
          "size": 1266
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates/grpc-config.py",
          "type": "blob",
          "size": 5244
        },
        {
          "path": "plugins/google-adk/skills/a2a-patterns/templates/multi-agent-orchestration.py",
          "type": "blob",
          "size": 7950
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/SKILL.md",
          "type": "blob",
          "size": 23670
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/examples/bigquery-queries.md",
          "type": "blob",
          "size": 13319
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/examples/complete-observability.md",
          "type": "blob",
          "size": 9842
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/examples/multi-tool-integration.md",
          "type": "blob",
          "size": 12686
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/setup-agentops.sh",
          "type": "blob",
          "size": 1120
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/setup-bigquery-analytics.sh",
          "type": "blob",
          "size": 3477
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/setup-cloud-trace.sh",
          "type": "blob",
          "size": 1428
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/setup-phoenix.sh",
          "type": "blob",
          "size": 1658
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/setup-weave.sh",
          "type": "blob",
          "size": 3021
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/scripts/validate-observability.sh",
          "type": "blob",
          "size": 3768
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/agentops-config.py",
          "type": "blob",
          "size": 3854
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/bigquery-analytics-config.py",
          "type": "blob",
          "size": 5947
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/bigquery-schema.json",
          "type": "blob",
          "size": 1942
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/cloud-trace-config.py",
          "type": "blob",
          "size": 3093
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/phoenix-config.py",
          "type": "blob",
          "size": 4786
        },
        {
          "path": "plugins/google-adk/skills/observability-patterns/templates/weave-config.py",
          "type": "blob",
          "size": 7362
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/SKILL.md",
          "type": "blob",
          "size": 8186
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/examples/multi-agent-handoff.py",
          "type": "blob",
          "size": 11743
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/examples/streaming-tool-agent.py",
          "type": "blob",
          "size": 10010
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/examples/video-agent.py",
          "type": "blob",
          "size": 9525
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/examples/voice-agent.py",
          "type": "blob",
          "size": 7843
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/scripts/check-modality-support.py",
          "type": "blob",
          "size": 6296
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/scripts/test-liverequest-queue.py",
          "type": "blob",
          "size": 4669
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/scripts/validate-streaming-config.py",
          "type": "blob",
          "size": 7290
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/audio-config.py",
          "type": "blob",
          "size": 6992
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/bidi-streaming-config.py",
          "type": "blob",
          "size": 4362
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/event-handler.py",
          "type": "blob",
          "size": 14064
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/liverequest-queue.py",
          "type": "blob",
          "size": 8963
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/streaming-tool-template.py",
          "type": "blob",
          "size": 7307
        },
        {
          "path": "plugins/google-adk/skills/streaming-patterns/templates/video-config.py",
          "type": "blob",
          "size": 8863
        },
        {
          "path": "plugins/mem0",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 853
        },
        {
          "path": "plugins/mem0/README.md",
          "type": "blob",
          "size": 9985
        },
        {
          "path": "plugins/mem0/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/agents/mem0-integrator.md",
          "type": "blob",
          "size": 11838
        },
        {
          "path": "plugins/mem0/agents/mem0-memory-architect.md",
          "type": "blob",
          "size": 10918
        },
        {
          "path": "plugins/mem0/agents/mem0-verifier.md",
          "type": "blob",
          "size": 10797
        },
        {
          "path": "plugins/mem0/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/commands/add-conversation-memory.md",
          "type": "blob",
          "size": 5404
        },
        {
          "path": "plugins/mem0/commands/add-graph-memory.md",
          "type": "blob",
          "size": 5580
        },
        {
          "path": "plugins/mem0/commands/add-user-memory.md",
          "type": "blob",
          "size": 5415
        },
        {
          "path": "plugins/mem0/commands/configure.md",
          "type": "blob",
          "size": 5986
        },
        {
          "path": "plugins/mem0/commands/init-mcp.md",
          "type": "blob",
          "size": 6579
        },
        {
          "path": "plugins/mem0/commands/init-oss.md",
          "type": "blob",
          "size": 5841
        },
        {
          "path": "plugins/mem0/commands/init-platform.md",
          "type": "blob",
          "size": 5227
        },
        {
          "path": "plugins/mem0/commands/init.md",
          "type": "blob",
          "size": 7132
        },
        {
          "path": "plugins/mem0/commands/migrate-to-supabase.md",
          "type": "blob",
          "size": 6344
        },
        {
          "path": "plugins/mem0/commands/test.md",
          "type": "blob",
          "size": 5806
        },
        {
          "path": "plugins/mem0/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/docs/MEM0_ALL_DOCUMENTATION_LINKS.md",
          "type": "blob",
          "size": 25463
        },
        {
          "path": "plugins/mem0/docs/overview.md",
          "type": "blob",
          "size": 7251
        },
        {
          "path": "plugins/mem0/docs/platform-vs-oss.md",
          "type": "blob",
          "size": 7527
        },
        {
          "path": "plugins/mem0/docs/sdk-documentation.md",
          "type": "blob",
          "size": 21299
        },
        {
          "path": "plugins/mem0/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/README.md",
          "type": "blob",
          "size": 10047
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/SKILL.md",
          "type": "blob",
          "size": 13957
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/SKILL_SUMMARY.txt",
          "type": "blob",
          "size": 2969
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/examples/customer-support-memory-architecture.md",
          "type": "blob",
          "size": 16105
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/retention-policy-session.yaml",
          "type": "blob",
          "size": 1164
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/analyze-memory-costs.sh",
          "type": "blob",
          "size": 7051
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/analyze-memory-performance.sh",
          "type": "blob",
          "size": 5243
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/analyze-retention.sh",
          "type": "blob",
          "size": 5582
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/audit-memory-security.sh",
          "type": "blob",
          "size": 7592
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/deduplicate-memories.sh",
          "type": "blob",
          "size": 6152
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/generate-retention-policy.sh",
          "type": "blob",
          "size": 4584
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/suggest-memory-type.sh",
          "type": "blob",
          "size": 7109
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/scripts/suggest-storage-architecture.sh",
          "type": "blob",
          "size": 10891
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/templates/graph-memory-config.py",
          "type": "blob",
          "size": 11787
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/templates/multi-level-memory-pattern.py",
          "type": "blob",
          "size": 8809
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/templates/retention-policy.yaml",
          "type": "blob",
          "size": 6744
        },
        {
          "path": "plugins/mem0/skills/memory-design-patterns/templates/vector-only-config.py",
          "type": "blob",
          "size": 7538
        },
        {
          "path": "plugins/mem0/skills/memory-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/QUICK_START.md",
          "type": "blob",
          "size": 3157
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/README.md",
          "type": "blob",
          "size": 9672
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/SKILL.md",
          "type": "blob",
          "size": 21069
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/examples/before-after-benchmarks.md",
          "type": "blob",
          "size": 10682
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/examples/optimization-case-studies.md",
          "type": "blob",
          "size": 11339
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/analyze-costs.sh",
          "type": "blob",
          "size": 9602
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/analyze-performance.sh",
          "type": "blob",
          "size": 11629
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/deduplicate-memories.sh",
          "type": "blob",
          "size": 4844
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/diagnose-slow-queries.sh",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/generate-cache-config.sh",
          "type": "blob",
          "size": 7921
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/setup-monitoring.sh",
          "type": "blob",
          "size": 598
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/scripts/suggest-vector-db.sh",
          "type": "blob",
          "size": 683
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/cache-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/cache-strategies/redis-cache.py",
          "type": "blob",
          "size": 9430
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/embedding-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/embedding-configs/cost-optimized.py",
          "type": "blob",
          "size": 1629
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/embedding-configs/performance-optimized.py",
          "type": "blob",
          "size": 1703
        },
        {
          "path": "plugins/mem0/skills/memory-optimization/templates/optimized-memory-config.py",
          "type": "blob",
          "size": 5391
        },
        {
          "path": "plugins/mem0/skills/supabase-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/.skill-metadata.json",
          "type": "blob",
          "size": 3861
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/README.md",
          "type": "blob",
          "size": 7538
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/SKILL.md",
          "type": "blob",
          "size": 18227
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/examples/multi-tenant-pattern.md",
          "type": "blob",
          "size": 12896
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/examples/platform-to-oss-migration-guide.md",
          "type": "blob",
          "size": 15689
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/examples/user-isolation-pattern.md",
          "type": "blob",
          "size": 10742
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/apply-mem0-rls.sh",
          "type": "blob",
          "size": 6837
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/apply-mem0-schema.sh",
          "type": "blob",
          "size": 5516
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/create-mem0-indexes.sh",
          "type": "blob",
          "size": 5599
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/setup-mem0-pgvector.sh",
          "type": "blob",
          "size": 1709
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/validate-mem0-setup.sh",
          "type": "blob",
          "size": 9423
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/scripts/verify-supabase-setup.sh",
          "type": "blob",
          "size": 3143
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates/mem0-basic-config.py",
          "type": "blob",
          "size": 1443
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates/mem0-enterprise-config.py",
          "type": "blob",
          "size": 5735
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates/mem0-graph-config.py",
          "type": "blob",
          "size": 2390
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates/mem0-schema-graph.sql",
          "type": "blob",
          "size": 6432
        },
        {
          "path": "plugins/mem0/skills/supabase-integration/templates/mem0-schema.sql",
          "type": "blob",
          "size": 3278
        },
        {
          "path": "plugins/ml-training",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 714
        },
        {
          "path": "plugins/ml-training/.gitignore",
          "type": "blob",
          "size": 470
        },
        {
          "path": "plugins/ml-training/CHANGELOG.md",
          "type": "blob",
          "size": 751
        },
        {
          "path": "plugins/ml-training/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/ml-training/README.md",
          "type": "blob",
          "size": 3720
        },
        {
          "path": "plugins/ml-training/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/agents/cost-optimizer.md",
          "type": "blob",
          "size": 14651
        },
        {
          "path": "plugins/ml-training/agents/data-engineer.md",
          "type": "blob",
          "size": 12960
        },
        {
          "path": "plugins/ml-training/agents/data-specialist.md",
          "type": "blob",
          "size": 13474
        },
        {
          "path": "plugins/ml-training/agents/distributed-training-specialist.md",
          "type": "blob",
          "size": 16995
        },
        {
          "path": "plugins/ml-training/agents/google-bigquery-ml-specialist.md",
          "type": "blob",
          "size": 12225
        },
        {
          "path": "plugins/ml-training/agents/google-vertex-specialist.md",
          "type": "blob",
          "size": 12849
        },
        {
          "path": "plugins/ml-training/agents/inference-deployer.md",
          "type": "blob",
          "size": 14575
        },
        {
          "path": "plugins/ml-training/agents/integration-specialist.md",
          "type": "blob",
          "size": 15230
        },
        {
          "path": "plugins/ml-training/agents/lambda-specialist.md",
          "type": "blob",
          "size": 12700
        },
        {
          "path": "plugins/ml-training/agents/ml-architect.md",
          "type": "blob",
          "size": 16133
        },
        {
          "path": "plugins/ml-training/agents/ml-tester.md",
          "type": "blob",
          "size": 14180
        },
        {
          "path": "plugins/ml-training/agents/modal-specialist.md",
          "type": "blob",
          "size": 13165
        },
        {
          "path": "plugins/ml-training/agents/peft-specialist.md",
          "type": "blob",
          "size": 13518
        },
        {
          "path": "plugins/ml-training/agents/runpod-specialist.md",
          "type": "blob",
          "size": 13798
        },
        {
          "path": "plugins/ml-training/agents/training-architect.md",
          "type": "blob",
          "size": 13751
        },
        {
          "path": "plugins/ml-training/agents/training-monitor.md",
          "type": "blob",
          "size": 15521
        },
        {
          "path": "plugins/ml-training/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/commands/add-dataset.md",
          "type": "blob",
          "size": 7876
        },
        {
          "path": "plugins/ml-training/commands/add-fastapi-endpoint.md",
          "type": "blob",
          "size": 7306
        },
        {
          "path": "plugins/ml-training/commands/add-monitoring.md",
          "type": "blob",
          "size": 9133
        },
        {
          "path": "plugins/ml-training/commands/add-nextjs-ui.md",
          "type": "blob",
          "size": 9820
        },
        {
          "path": "plugins/ml-training/commands/add-peft.md",
          "type": "blob",
          "size": 9126
        },
        {
          "path": "plugins/ml-training/commands/add-platform.md",
          "type": "blob",
          "size": 8082
        },
        {
          "path": "plugins/ml-training/commands/add-preprocessing.md",
          "type": "blob",
          "size": 7744
        },
        {
          "path": "plugins/ml-training/commands/add-training-config.md",
          "type": "blob",
          "size": 8373
        },
        {
          "path": "plugins/ml-training/commands/deploy-inference.md",
          "type": "blob",
          "size": 8942
        },
        {
          "path": "plugins/ml-training/commands/deploy-training.md",
          "type": "blob",
          "size": 7582
        },
        {
          "path": "plugins/ml-training/commands/estimate-cost.md",
          "type": "blob",
          "size": 7893
        },
        {
          "path": "plugins/ml-training/commands/init.md",
          "type": "blob",
          "size": 7749
        },
        {
          "path": "plugins/ml-training/commands/integrate-supabase.md",
          "type": "blob",
          "size": 9243
        },
        {
          "path": "plugins/ml-training/commands/monitor-training.md",
          "type": "blob",
          "size": 7840
        },
        {
          "path": "plugins/ml-training/commands/optimize-training.md",
          "type": "blob",
          "size": 8280
        },
        {
          "path": "plugins/ml-training/commands/setup-framework.md",
          "type": "blob",
          "size": 7941
        },
        {
          "path": "plugins/ml-training/commands/test.md",
          "type": "blob",
          "size": 7979
        },
        {
          "path": "plugins/ml-training/commands/validate-data.md",
          "type": "blob",
          "size": 8093
        },
        {
          "path": "plugins/ml-training/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/docs/ML-TRAINING-AND-INFERENCE.md",
          "type": "blob",
          "size": 43426
        },
        {
          "path": "plugins/ml-training/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/hooks/hooks.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/ml-training/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/README.md",
          "type": "blob",
          "size": 8984
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/SKILL.md",
          "type": "blob",
          "size": 5183
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/examples/lambda-a10-setup.md",
          "type": "blob",
          "size": 10815
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/examples/modal-t4-setup.md",
          "type": "blob",
          "size": 7088
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/scripts/setup-lambda.sh",
          "type": "blob",
          "size": 6341
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/scripts/setup-modal.sh",
          "type": "blob",
          "size": 4809
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/scripts/setup-runpod.sh",
          "type": "blob",
          "size": 8934
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/templates/lambda_config.yaml",
          "type": "blob",
          "size": 5219
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/templates/modal_image.py",
          "type": "blob",
          "size": 7437
        },
        {
          "path": "plugins/ml-training/skills/cloud-gpu-configs/templates/runpod_config.json",
          "type": "blob",
          "size": 5940
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/README.md",
          "type": "blob",
          "size": 5901
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/SKILL.md",
          "type": "blob",
          "size": 14147
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/SKILL_SUMMARY.md",
          "type": "blob",
          "size": 4091
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/examples/inference-cost-estimate.md",
          "type": "blob",
          "size": 10235
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/examples/training-cost-estimate.md",
          "type": "blob",
          "size": 7146
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/scripts/calculate-gpu-hours.sh",
          "type": "blob",
          "size": 4531
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/scripts/compare-platforms.sh",
          "type": "blob",
          "size": 5356
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/scripts/estimate-inference-cost.sh",
          "type": "blob",
          "size": 6628
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/scripts/estimate-training-cost.sh",
          "type": "blob",
          "size": 6450
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/templates/cost-breakdown.json",
          "type": "blob",
          "size": 3693
        },
        {
          "path": "plugins/ml-training/skills/cost-calculator/templates/platform-pricing.yaml",
          "type": "blob",
          "size": 7922
        },
        {
          "path": "plugins/ml-training/skills/example-projects",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/README.md",
          "type": "blob",
          "size": 8516
        },
        {
          "path": "plugins/ml-training/skills/example-projects/SKILL.md",
          "type": "blob",
          "size": 8311
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/README.md",
          "type": "blob",
          "size": 10200
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/inference.py",
          "type": "blob",
          "size": 8390
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/modal_deploy.py",
          "type": "blob",
          "size": 6285
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/requirements.txt",
          "type": "blob",
          "size": 75
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/sample_data.csv",
          "type": "blob",
          "size": 2152
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/redai-trade-classifier/train.py",
          "type": "blob",
          "size": 8755
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification/README.md",
          "type": "blob",
          "size": 6021
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification/data.json",
          "type": "blob",
          "size": 3905
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification/inference.py",
          "type": "blob",
          "size": 4769
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification/requirements.txt",
          "type": "blob",
          "size": 117
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/sentiment-classification/train.py",
          "type": "blob",
          "size": 8418
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/README.md",
          "type": "blob",
          "size": 8801
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/config.yaml",
          "type": "blob",
          "size": 445
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/generate.py",
          "type": "blob",
          "size": 5957
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/requirements.txt",
          "type": "blob",
          "size": 115
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/train.py",
          "type": "blob",
          "size": 8281
        },
        {
          "path": "plugins/ml-training/skills/example-projects/examples/text-generation/training_data.txt",
          "type": "blob",
          "size": 4024
        },
        {
          "path": "plugins/ml-training/skills/example-projects/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/scripts/run-training.sh",
          "type": "blob",
          "size": 4995
        },
        {
          "path": "plugins/ml-training/skills/example-projects/scripts/setup-example.sh",
          "type": "blob",
          "size": 5261
        },
        {
          "path": "plugins/ml-training/skills/example-projects/scripts/test-inference.sh",
          "type": "blob",
          "size": 7327
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates/README_template.md",
          "type": "blob",
          "size": 2306
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates/inference_template.py",
          "type": "blob",
          "size": 5112
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates/modal_deploy_template.py",
          "type": "blob",
          "size": 4283
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates/requirements_template.txt",
          "type": "blob",
          "size": 655
        },
        {
          "path": "plugins/ml-training/skills/example-projects/templates/train_template.py",
          "type": "blob",
          "size": 8749
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/README.md",
          "type": "blob",
          "size": 7953
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/SKILL.md",
          "type": "blob",
          "size": 14927
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/examples/bigquery-regression-example.sql",
          "type": "blob",
          "size": 10719
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/examples/vertex-huggingface-finetuning.py",
          "type": "blob",
          "size": 11340
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/examples/vertex-pytorch-training.py",
          "type": "blob",
          "size": 14401
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/scripts/configure-auth.sh",
          "type": "blob",
          "size": 10574
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/scripts/estimate-gcp-cost.sh",
          "type": "blob",
          "size": 14099
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/scripts/setup-bigquery-ml.sh",
          "type": "blob",
          "size": 6798
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/scripts/setup-vertex-ai.sh",
          "type": "blob",
          "size": 9678
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates/bigquery_ml_training.sql",
          "type": "blob",
          "size": 15603
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates/gcp_auth.json",
          "type": "blob",
          "size": 6950
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates/vertex_gpu_config.yaml",
          "type": "blob",
          "size": 12933
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates/vertex_tpu_config.yaml",
          "type": "blob",
          "size": 12722
        },
        {
          "path": "plugins/ml-training/skills/google-cloud-configs/templates/vertex_training_job.py",
          "type": "blob",
          "size": 16772
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/README.md",
          "type": "blob",
          "size": 7496
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/SKILL.md",
          "type": "blob",
          "size": 16163
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/examples/fastapi-inference-endpoint.md",
          "type": "blob",
          "size": 7868
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/examples/nextjs-ml-dashboard.md",
          "type": "blob",
          "size": 13785
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/scripts/add-fastapi-endpoint.sh",
          "type": "blob",
          "size": 3793
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/scripts/add-nextjs-component.sh",
          "type": "blob",
          "size": 3829
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/scripts/create-supabase-schema.sh",
          "type": "blob",
          "size": 15262
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/templates/fastapi-router.py",
          "type": "blob",
          "size": 11030
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/templates/nextjs-prediction-form.tsx",
          "type": "blob",
          "size": 8052
        },
        {
          "path": "plugins/ml-training/skills/integration-helpers/templates/supabase-schema.sql",
          "type": "blob",
          "size": 4536
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/SKILL.md",
          "type": "blob",
          "size": 14067
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/examples/tensorboard-integration.md",
          "type": "blob",
          "size": 13701
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/examples/wandb-integration.md",
          "type": "blob",
          "size": 18362
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/scripts/launch-monitoring.sh",
          "type": "blob",
          "size": 6268
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/scripts/setup-tensorboard.sh",
          "type": "blob",
          "size": 5838
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/scripts/setup-wandb.sh",
          "type": "blob",
          "size": 8915
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/templates/logging-config.json",
          "type": "blob",
          "size": 9075
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/templates/tensorboard-config.yaml",
          "type": "blob",
          "size": 3694
        },
        {
          "path": "plugins/ml-training/skills/monitoring-dashboard/templates/wandb-config.py",
          "type": "blob",
          "size": 10873
        },
        {
          "path": "plugins/ml-training/skills/training-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/README.md",
          "type": "blob",
          "size": 2096
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/SKILL.md",
          "type": "blob",
          "size": 20374
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/examples/sentiment-classifier.md",
          "type": "blob",
          "size": 7112
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/examples/text-generator.md",
          "type": "blob",
          "size": 10779
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/scripts/setup-classification.sh",
          "type": "blob",
          "size": 14935
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/scripts/setup-fine-tuning.sh",
          "type": "blob",
          "size": 11809
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/scripts/setup-generation.sh",
          "type": "blob",
          "size": 13171
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/scripts/setup-peft.sh",
          "type": "blob",
          "size": 17495
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/templates/classification-config.yaml",
          "type": "blob",
          "size": 537
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/templates/generation-config.yaml",
          "type": "blob",
          "size": 684
        },
        {
          "path": "plugins/ml-training/skills/training-patterns/templates/peft-config.json",
          "type": "blob",
          "size": 290
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/SKILL.md",
          "type": "blob",
          "size": 17352
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/examples/data-validation-example.md",
          "type": "blob",
          "size": 8949
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/examples/pipeline-testing-example.md",
          "type": "blob",
          "size": 11280
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/scripts/check-dependencies.sh",
          "type": "blob",
          "size": 10088
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/scripts/test-pipeline.sh",
          "type": "blob",
          "size": 13601
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/scripts/validate-data.sh",
          "type": "blob",
          "size": 14010
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/scripts/validate-model.sh",
          "type": "blob",
          "size": 12273
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/templates/test-config.yaml",
          "type": "blob",
          "size": 4734
        },
        {
          "path": "plugins/ml-training/skills/validation-scripts/templates/validation-schema.json",
          "type": "blob",
          "size": 4216
        },
        {
          "path": "plugins/nextjs-frontend",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 533
        },
        {
          "path": "plugins/nextjs-frontend/.gitignore",
          "type": "blob",
          "size": 224
        },
        {
          "path": "plugins/nextjs-frontend/CHANGELOG.md",
          "type": "blob",
          "size": 694
        },
        {
          "path": "plugins/nextjs-frontend/LICENSE",
          "type": "blob",
          "size": 1083
        },
        {
          "path": "plugins/nextjs-frontend/README.md",
          "type": "blob",
          "size": 3654
        },
        {
          "path": "plugins/nextjs-frontend/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/agents/ai-sdk-integration-agent.md",
          "type": "blob",
          "size": 11665
        },
        {
          "path": "plugins/nextjs-frontend/agents/analytics-specialist-agent.md",
          "type": "blob",
          "size": 16331
        },
        {
          "path": "plugins/nextjs-frontend/agents/api-route-generator-agent.md",
          "type": "blob",
          "size": 12954
        },
        {
          "path": "plugins/nextjs-frontend/agents/app-scaffolding-agent.md",
          "type": "blob",
          "size": 8018
        },
        {
          "path": "plugins/nextjs-frontend/agents/component-builder-agent-old.md",
          "type": "blob",
          "size": 14159
        },
        {
          "path": "plugins/nextjs-frontend/agents/component-builder-agent.md",
          "type": "blob",
          "size": 11288
        },
        {
          "path": "plugins/nextjs-frontend/agents/content-optimizer-agent.md",
          "type": "blob",
          "size": 13213
        },
        {
          "path": "plugins/nextjs-frontend/agents/conversion-specialist-agent.md",
          "type": "blob",
          "size": 15889
        },
        {
          "path": "plugins/nextjs-frontend/agents/design-enforcer-agent.md",
          "type": "blob",
          "size": 10826
        },
        {
          "path": "plugins/nextjs-frontend/agents/design-patterns-agent.md",
          "type": "blob",
          "size": 9796
        },
        {
          "path": "plugins/nextjs-frontend/agents/engagement-specialist-agent.md",
          "type": "blob",
          "size": 12184
        },
        {
          "path": "plugins/nextjs-frontend/agents/nextjs-setup-agent.md",
          "type": "blob",
          "size": 13190
        },
        {
          "path": "plugins/nextjs-frontend/agents/page-generator-agent.md",
          "type": "blob",
          "size": 14676
        },
        {
          "path": "plugins/nextjs-frontend/agents/seo-specialist-agent.md",
          "type": "blob",
          "size": 14119
        },
        {
          "path": "plugins/nextjs-frontend/agents/supabase-integration-agent.md",
          "type": "blob",
          "size": 11575
        },
        {
          "path": "plugins/nextjs-frontend/agents/ui-search-agent.md",
          "type": "blob",
          "size": 11221
        },
        {
          "path": "plugins/nextjs-frontend/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/commands/add-component.md",
          "type": "blob",
          "size": 7919
        },
        {
          "path": "plugins/nextjs-frontend/commands/add-page.md",
          "type": "blob",
          "size": 7479
        },
        {
          "path": "plugins/nextjs-frontend/commands/analyze-design.md",
          "type": "blob",
          "size": 5243
        },
        {
          "path": "plugins/nextjs-frontend/commands/build-landing-page.md",
          "type": "blob",
          "size": 9835
        },
        {
          "path": "plugins/nextjs-frontend/commands/enforce-design-system.md",
          "type": "blob",
          "size": 7801
        },
        {
          "path": "plugins/nextjs-frontend/commands/init-design-system.md",
          "type": "blob",
          "size": 3546
        },
        {
          "path": "plugins/nextjs-frontend/commands/init.md",
          "type": "blob",
          "size": 5930
        },
        {
          "path": "plugins/nextjs-frontend/commands/integrate-ai-sdk.md",
          "type": "blob",
          "size": 7640
        },
        {
          "path": "plugins/nextjs-frontend/commands/integrate-supabase.md",
          "type": "blob",
          "size": 8662
        },
        {
          "path": "plugins/nextjs-frontend/commands/optimize-engagement.md",
          "type": "blob",
          "size": 6089
        },
        {
          "path": "plugins/nextjs-frontend/commands/optimize-seo.md",
          "type": "blob",
          "size": 5901
        },
        {
          "path": "plugins/nextjs-frontend/commands/scaffold-app.md",
          "type": "blob",
          "size": 4114
        },
        {
          "path": "plugins/nextjs-frontend/commands/search-components.md",
          "type": "blob",
          "size": 6318
        },
        {
          "path": "plugins/nextjs-frontend/hooks.json",
          "type": "blob",
          "size": 1606
        },
        {
          "path": "plugins/nextjs-frontend/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/hooks/hooks.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/nextjs-frontend/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/scripts/README.md",
          "type": "blob",
          "size": 626
        },
        {
          "path": "plugins/nextjs-frontend/scripts/post-component-add.sh",
          "type": "blob",
          "size": 1153
        },
        {
          "path": "plugins/nextjs-frontend/scripts/pre-commit.sh",
          "type": "blob",
          "size": 1208
        },
        {
          "path": "plugins/nextjs-frontend/scripts/setup-hooks.sh",
          "type": "blob",
          "size": 1044
        },
        {
          "path": "plugins/nextjs-frontend/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/conversion-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/conversion-patterns/README.md",
          "type": "blob",
          "size": 1463
        },
        {
          "path": "plugins/nextjs-frontend/skills/conversion-patterns/SKILL.md",
          "type": "blob",
          "size": 18802
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/README.md",
          "type": "blob",
          "size": 10447
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/SKILL.md",
          "type": "blob",
          "size": 9077
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/examples/deployment-patterns.md",
          "type": "blob",
          "size": 12640
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/scripts/optimize-build.sh",
          "type": "blob",
          "size": 6606
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/scripts/setup-env-vars.sh",
          "type": "blob",
          "size": 8225
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/scripts/test-edge-functions.sh",
          "type": "blob",
          "size": 7202
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/scripts/validate-deployment.sh",
          "type": "blob",
          "size": 6011
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates/basic.vercel.json",
          "type": "blob",
          "size": 114
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates/edge-functions.vercel.json",
          "type": "blob",
          "size": 719
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates/env-template.md",
          "type": "blob",
          "size": 5986
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates/monorepo.vercel.json",
          "type": "blob",
          "size": 240
        },
        {
          "path": "plugins/nextjs-frontend/skills/deployment-config/templates/optimized.vercel.json",
          "type": "blob",
          "size": 1188
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-inspiration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-inspiration/README.md",
          "type": "blob",
          "size": 1150
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-inspiration/SKILL.md",
          "type": "blob",
          "size": 14196
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/README.md",
          "type": "blob",
          "size": 1533
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/SKILL.md",
          "type": "blob",
          "size": 10655
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/examples/taskflow-ai-example.md",
          "type": "blob",
          "size": 5638
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/scripts/setup-design-system.sh",
          "type": "blob",
          "size": 5297
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/scripts/validate-design-system.sh",
          "type": "blob",
          "size": 5293
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/design-system-enforcement/templates/design-system-template.md",
          "type": "blob",
          "size": 14091
        },
        {
          "path": "plugins/nextjs-frontend/skills/seo-2025-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/seo-2025-patterns/README.md",
          "type": "blob",
          "size": 1696
        },
        {
          "path": "plugins/nextjs-frontend/skills/seo-2025-patterns/SKILL.md",
          "type": "blob",
          "size": 11595
        },
        {
          "path": "plugins/nextjs-frontend/skills/seo-2025-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/seo-2025-patterns/scripts/seo-audit.sh",
          "type": "blob",
          "size": 4248
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/README.md",
          "type": "blob",
          "size": 9551
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/SKILL.md",
          "type": "blob",
          "size": 9832
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/examples/root-layout.tsx",
          "type": "blob",
          "size": 770
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/examples/sample-form.tsx",
          "type": "blob",
          "size": 4390
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/examples/theme-showcase.tsx",
          "type": "blob",
          "size": 5966
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/examples/theme-toggle.tsx",
          "type": "blob",
          "size": 1237
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/scripts/init-shadcn.sh",
          "type": "blob",
          "size": 2136
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/scripts/install-tailwind.sh",
          "type": "blob",
          "size": 3595
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/scripts/setup-dark-mode.sh",
          "type": "blob",
          "size": 4266
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/scripts/setup-theme.sh",
          "type": "blob",
          "size": 3192
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates/components.json",
          "type": "blob",
          "size": 415
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates/globals.css",
          "type": "blob",
          "size": 4720
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates/postcss.config.mjs",
          "type": "blob",
          "size": 155
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates/tailwind.config.ts",
          "type": "blob",
          "size": 2333
        },
        {
          "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/templates/theme-provider.tsx",
          "type": "blob",
          "size": 327
        },
        {
          "path": "plugins/openrouter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 727
        },
        {
          "path": "plugins/openrouter/.gitignore",
          "type": "blob",
          "size": 344
        },
        {
          "path": "plugins/openrouter/CHANGELOG.md",
          "type": "blob",
          "size": 1658
        },
        {
          "path": "plugins/openrouter/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/openrouter/README.md",
          "type": "blob",
          "size": 4852
        },
        {
          "path": "plugins/openrouter/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/agents/openrouter-langchain-agent.md",
          "type": "blob",
          "size": 9276
        },
        {
          "path": "plugins/openrouter/agents/openrouter-routing-agent.md",
          "type": "blob",
          "size": 9533
        },
        {
          "path": "plugins/openrouter/agents/openrouter-setup-agent.md",
          "type": "blob",
          "size": 9131
        },
        {
          "path": "plugins/openrouter/agents/openrouter-vercel-integration-agent.md",
          "type": "blob",
          "size": 9946
        },
        {
          "path": "plugins/openrouter/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/commands/add-langchain.md",
          "type": "blob",
          "size": 8693
        },
        {
          "path": "plugins/openrouter/commands/add-model-routing.md",
          "type": "blob",
          "size": 7809
        },
        {
          "path": "plugins/openrouter/commands/add-vercel-ai-sdk.md",
          "type": "blob",
          "size": 7901
        },
        {
          "path": "plugins/openrouter/commands/configure.md",
          "type": "blob",
          "size": 6862
        },
        {
          "path": "plugins/openrouter/commands/init.md",
          "type": "blob",
          "size": 7863
        },
        {
          "path": "plugins/openrouter/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md",
          "type": "blob",
          "size": 37846
        },
        {
          "path": "plugins/openrouter/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/README.md",
          "type": "blob",
          "size": 9650
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/SKILL.md",
          "type": "blob",
          "size": 10376
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/examples/cost-routing-example.md",
          "type": "blob",
          "size": 7983
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/examples/dynamic-routing-example.md",
          "type": "blob",
          "size": 11272
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/examples/fallback-chain-example.md",
          "type": "blob",
          "size": 14906
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/examples/monitoring-example.md",
          "type": "blob",
          "size": 16076
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/scripts/analyze-cost-savings.sh",
          "type": "blob",
          "size": 8602
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/scripts/generate-routing-config.sh",
          "type": "blob",
          "size": 7031
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/scripts/test-fallback-chain.sh",
          "type": "blob",
          "size": 6990
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/scripts/validate-routing-config.sh",
          "type": "blob",
          "size": 5425
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/balanced-routing.json",
          "type": "blob",
          "size": 3915
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/cost-optimized-routing.json",
          "type": "blob",
          "size": 1995
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/custom-routing-template.json",
          "type": "blob",
          "size": 3058
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/quality-optimized-routing.json",
          "type": "blob",
          "size": 2485
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/routing-config.py",
          "type": "blob",
          "size": 11014
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/routing-config.ts",
          "type": "blob",
          "size": 8013
        },
        {
          "path": "plugins/openrouter/skills/model-routing-patterns/templates/speed-optimized-routing.json",
          "type": "blob",
          "size": 2175
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/SKILL.md",
          "type": "blob",
          "size": 6359
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/api-key-troubleshooting.md",
          "type": "blob",
          "size": 6859
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/cost-optimization.md",
          "type": "blob",
          "size": 12496
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/fallback-issues.md",
          "type": "blob",
          "size": 11864
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/model-not-found.md",
          "type": "blob",
          "size": 7106
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/provider-errors.md",
          "type": "blob",
          "size": 10050
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/examples/rate-limiting.md",
          "type": "blob",
          "size": 11752
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/analyze-usage.sh",
          "type": "blob",
          "size": 2791
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/check-model-availability.sh",
          "type": "blob",
          "size": 2466
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/check-provider-status.sh",
          "type": "blob",
          "size": 2727
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/test-fallback.sh",
          "type": "blob",
          "size": 2495
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/test-routing.sh",
          "type": "blob",
          "size": 2908
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/troubleshoot.sh",
          "type": "blob",
          "size": 5867
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/validate-api-key.sh",
          "type": "blob",
          "size": 1931
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/scripts/validate-env-config.sh",
          "type": "blob",
          "size": 3567
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates/.env.example",
          "type": "blob",
          "size": 514
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates/.env.template",
          "type": "blob",
          "size": 3508
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates/budget-alerts.json",
          "type": "blob",
          "size": 5020
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates/monitoring-config.json",
          "type": "blob",
          "size": 3684
        },
        {
          "path": "plugins/openrouter/skills/openrouter-config-validator/templates/provider-preferences.json",
          "type": "blob",
          "size": 5941
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/SKILL.md",
          "type": "blob",
          "size": 7259
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples/langchain-chain-example.md",
          "type": "blob",
          "size": 7608
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples/langchain-rag-example.md",
          "type": "blob",
          "size": 10947
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples/openai-sdk-example.md",
          "type": "blob",
          "size": 8965
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples/vercel-streaming-example.md",
          "type": "blob",
          "size": 5400
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/examples/vercel-tools-example.md",
          "type": "blob",
          "size": 9535
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts/check-compatibility.sh",
          "type": "blob",
          "size": 6114
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts/setup-langchain-integration.sh",
          "type": "blob",
          "size": 4905
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts/setup-vercel-integration.sh",
          "type": "blob",
          "size": 2391
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts/test-streaming.sh",
          "type": "blob",
          "size": 2313
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/scripts/validate-integration.sh",
          "type": "blob",
          "size": 5064
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/langchain-agent.py",
          "type": "blob",
          "size": 5714
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/langchain-chain.py",
          "type": "blob",
          "size": 4263
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/langchain-config.py",
          "type": "blob",
          "size": 2761
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/langchain-config.ts",
          "type": "blob",
          "size": 2775
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/langchain-rag.py",
          "type": "blob",
          "size": 7687
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/openai-functions.ts",
          "type": "blob",
          "size": 8437
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/openai-sdk-config.py",
          "type": "blob",
          "size": 4107
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/openai-sdk-config.ts",
          "type": "blob",
          "size": 2487
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/openai-streaming.ts",
          "type": "blob",
          "size": 5509
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/vercel-ai-sdk-config.ts",
          "type": "blob",
          "size": 1411
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/vercel-api-route.ts",
          "type": "blob",
          "size": 2086
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/vercel-chat-component.tsx",
          "type": "blob",
          "size": 3795
        },
        {
          "path": "plugins/openrouter/skills/provider-integration-templates/templates/vercel-tools-config.ts",
          "type": "blob",
          "size": 3848
        },
        {
          "path": "plugins/payments",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 636
        },
        {
          "path": "plugins/payments/.gitignore",
          "type": "blob",
          "size": 562
        },
        {
          "path": "plugins/payments/CHANGELOG.md",
          "type": "blob",
          "size": 453
        },
        {
          "path": "plugins/payments/LICENSE",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/payments/README.md",
          "type": "blob",
          "size": 3476
        },
        {
          "path": "plugins/payments/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/agents/payments-architect.md",
          "type": "blob",
          "size": 9926
        },
        {
          "path": "plugins/payments/agents/stripe-integration-agent.md",
          "type": "blob",
          "size": 9200
        },
        {
          "path": "plugins/payments/agents/subscription-manager-agent.md",
          "type": "blob",
          "size": 8458
        },
        {
          "path": "plugins/payments/agents/webhook-handler-agent.md",
          "type": "blob",
          "size": 9735
        },
        {
          "path": "plugins/payments/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/commands/add-checkout.md",
          "type": "blob",
          "size": 5443
        },
        {
          "path": "plugins/payments/commands/add-subscriptions.md",
          "type": "blob",
          "size": 7272
        },
        {
          "path": "plugins/payments/commands/add-webhooks.md",
          "type": "blob",
          "size": 7896
        },
        {
          "path": "plugins/payments/commands/init.md",
          "type": "blob",
          "size": 5687
        },
        {
          "path": "plugins/payments/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/hooks/hooks.json",
          "type": "blob",
          "size": 14
        },
        {
          "path": "plugins/payments/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/checkout-components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/checkout-components/SKILL.md",
          "type": "blob",
          "size": 12796
        },
        {
          "path": "plugins/payments/skills/checkout-components/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/checkout-components/examples/checkout-page-example.tsx",
          "type": "blob",
          "size": 7646
        },
        {
          "path": "plugins/payments/skills/checkout-components/examples/payment-form-integration-example.tsx",
          "type": "blob",
          "size": 14171
        },
        {
          "path": "plugins/payments/skills/checkout-components/examples/subscription-management-example.tsx",
          "type": "blob",
          "size": 10408
        },
        {
          "path": "plugins/payments/skills/checkout-components/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/checkout-components/scripts/generate-component.sh",
          "type": "blob",
          "size": 4443
        },
        {
          "path": "plugins/payments/skills/checkout-components/scripts/install-stripe-react.sh",
          "type": "blob",
          "size": 1392
        },
        {
          "path": "plugins/payments/skills/checkout-components/scripts/setup-stripe-provider.sh",
          "type": "blob",
          "size": 4251
        },
        {
          "path": "plugins/payments/skills/checkout-components/scripts/validate-components.sh",
          "type": "blob",
          "size": 5732
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/CheckoutForm.tsx",
          "type": "blob",
          "size": 8124
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/PaymentHistory.tsx",
          "type": "blob",
          "size": 8698
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/PaymentMethodForm.tsx",
          "type": "blob",
          "size": 7931
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/PricingTable.tsx",
          "type": "blob",
          "size": 7126
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/StripeProvider.tsx",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "plugins/payments/skills/checkout-components/templates/SubscriptionCard.tsx",
          "type": "blob",
          "size": 8198
        },
        {
          "path": "plugins/payments/skills/stripe-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/SKILL.md",
          "type": "blob",
          "size": 8313
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/examples/fastapi-checkout-example.py",
          "type": "blob",
          "size": 7943
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/examples/nextjs-payment-form-example.tsx",
          "type": "blob",
          "size": 8808
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/examples/subscription-flow-example.py",
          "type": "blob",
          "size": 14291
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/scripts/setup-payment-intents.sh",
          "type": "blob",
          "size": 6602
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/scripts/setup-stripe-checkout.sh",
          "type": "blob",
          "size": 8558
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/scripts/setup-subscriptions.sh",
          "type": "blob",
          "size": 14232
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/scripts/validate-stripe-config.sh",
          "type": "blob",
          "size": 7268
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates/checkout_page.tsx",
          "type": "blob",
          "size": 8542
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates/checkout_session.py",
          "type": "blob",
          "size": 6984
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates/payment_intent.py",
          "type": "blob",
          "size": 7273
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates/stripe_elements.tsx",
          "type": "blob",
          "size": 4387
        },
        {
          "path": "plugins/payments/skills/stripe-patterns/templates/subscription.py",
          "type": "blob",
          "size": 10320
        },
        {
          "path": "plugins/payments/skills/subscription-schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/SKILL.md",
          "type": "blob",
          "size": 10347
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/examples/complete-schema-migration.sql",
          "type": "blob",
          "size": 14273
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/examples/rls-testing-examples.sql",
          "type": "blob",
          "size": 11736
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/examples/sample-queries.sql",
          "type": "blob",
          "size": 8591
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/scripts/create-payment-tables.sh",
          "type": "blob",
          "size": 3872
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/scripts/migrate-schema.sh",
          "type": "blob",
          "size": 4057
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/scripts/setup-rls-policies.sh",
          "type": "blob",
          "size": 3360
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/scripts/validate-schema.sh",
          "type": "blob",
          "size": 6816
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/customers_table.sql",
          "type": "blob",
          "size": 2458
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/invoices_table.sql",
          "type": "blob",
          "size": 5064
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/payments_table.sql",
          "type": "blob",
          "size": 3876
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/rls_policies.sql",
          "type": "blob",
          "size": 12665
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/subscriptions_table.sql",
          "type": "blob",
          "size": 3940
        },
        {
          "path": "plugins/payments/skills/subscription-schemas/templates/webhook_events_table.sql",
          "type": "blob",
          "size": 4761
        },
        {
          "path": "plugins/payments/skills/webhook-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/webhook-security/README.md",
          "type": "blob",
          "size": 8933
        },
        {
          "path": "plugins/payments/skills/webhook-security/SKILL.md",
          "type": "blob",
          "size": 13310
        },
        {
          "path": "plugins/payments/skills/webhook-security/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/webhook-security/examples/complete-webhook-handler.py",
          "type": "blob",
          "size": 9884
        },
        {
          "path": "plugins/payments/skills/webhook-security/examples/event-processing-example.py",
          "type": "blob",
          "size": 12670
        },
        {
          "path": "plugins/payments/skills/webhook-security/examples/webhook-testing-example.sh",
          "type": "blob",
          "size": 5241
        },
        {
          "path": "plugins/payments/skills/webhook-security/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/webhook-security/scripts/generate-webhook-secret.sh",
          "type": "blob",
          "size": 3875
        },
        {
          "path": "plugins/payments/skills/webhook-security/scripts/setup-webhook-endpoint.sh",
          "type": "blob",
          "size": 4628
        },
        {
          "path": "plugins/payments/skills/webhook-security/scripts/test-webhook-locally.sh",
          "type": "blob",
          "size": 1707
        },
        {
          "path": "plugins/payments/skills/webhook-security/scripts/verify-signature.py",
          "type": "blob",
          "size": 7175
        },
        {
          "path": "plugins/payments/skills/webhook-security/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/payments/skills/webhook-security/templates/event_logger.py",
          "type": "blob",
          "size": 9553
        },
        {
          "path": "plugins/payments/skills/webhook-security/templates/retry_handler.py",
          "type": "blob",
          "size": 8764
        },
        {
          "path": "plugins/payments/skills/webhook-security/templates/webhook_handler.py",
          "type": "blob",
          "size": 9237
        },
        {
          "path": "plugins/payments/skills/webhook-security/templates/webhook_test.py",
          "type": "blob",
          "size": 10997
        },
        {
          "path": "plugins/plugin-docs-loader",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 638
        },
        {
          "path": "plugins/plugin-docs-loader/README.md",
          "type": "blob",
          "size": 2422
        },
        {
          "path": "plugins/plugin-docs-loader/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/agents/doc-loader-agent.md",
          "type": "blob",
          "size": 11860
        },
        {
          "path": "plugins/plugin-docs-loader/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/commands/load-docs.md",
          "type": "blob",
          "size": 8865
        },
        {
          "path": "plugins/plugin-docs-loader/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/skills/doc-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/skills/doc-templates/SKILL.md",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/plugin-docs-loader/skills/doc-templates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/plugin-docs-loader/skills/doc-templates/templates/template-doc-loader-command.md",
          "type": "blob",
          "size": 7020
        },
        {
          "path": "plugins/rag-pipeline",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 560
        },
        {
          "path": "plugins/rag-pipeline/.gitignore",
          "type": "blob",
          "size": 847
        },
        {
          "path": "plugins/rag-pipeline/AGENT_MAPPING.md",
          "type": "blob",
          "size": 954
        },
        {
          "path": "plugins/rag-pipeline/CHANGELOG.md",
          "type": "blob",
          "size": 2921
        },
        {
          "path": "plugins/rag-pipeline/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/rag-pipeline/README.md",
          "type": "blob",
          "size": 1406
        },
        {
          "path": "plugins/rag-pipeline/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/agents/document-processor.md",
          "type": "blob",
          "size": 13771
        },
        {
          "path": "plugins/rag-pipeline/agents/embedding-specialist.md",
          "type": "blob",
          "size": 12103
        },
        {
          "path": "plugins/rag-pipeline/agents/google-file-search-specialist.md",
          "type": "blob",
          "size": 13637
        },
        {
          "path": "plugins/rag-pipeline/agents/langchain-specialist.md",
          "type": "blob",
          "size": 13572
        },
        {
          "path": "plugins/rag-pipeline/agents/llamaindex-specialist.md",
          "type": "blob",
          "size": 13081
        },
        {
          "path": "plugins/rag-pipeline/agents/rag-architect.md",
          "type": "blob",
          "size": 15681
        },
        {
          "path": "plugins/rag-pipeline/agents/rag-deployment-agent.md",
          "type": "blob",
          "size": 12501
        },
        {
          "path": "plugins/rag-pipeline/agents/rag-tester.md",
          "type": "blob",
          "size": 13838
        },
        {
          "path": "plugins/rag-pipeline/agents/retrieval-optimizer.md",
          "type": "blob",
          "size": 14721
        },
        {
          "path": "plugins/rag-pipeline/agents/vector-db-engineer.md",
          "type": "blob",
          "size": 13326
        },
        {
          "path": "plugins/rag-pipeline/agents/web-scraper-agent.md",
          "type": "blob",
          "size": 12394
        },
        {
          "path": "plugins/rag-pipeline/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/commands/add-chunking.md",
          "type": "blob",
          "size": 8677
        },
        {
          "path": "plugins/rag-pipeline/commands/add-embeddings.md",
          "type": "blob",
          "size": 8467
        },
        {
          "path": "plugins/rag-pipeline/commands/add-google-file-search.md",
          "type": "blob",
          "size": 6525
        },
        {
          "path": "plugins/rag-pipeline/commands/add-hybrid-search.md",
          "type": "blob",
          "size": 10304
        },
        {
          "path": "plugins/rag-pipeline/commands/add-metadata.md",
          "type": "blob",
          "size": 9233
        },
        {
          "path": "plugins/rag-pipeline/commands/add-monitoring.md",
          "type": "blob",
          "size": 8504
        },
        {
          "path": "plugins/rag-pipeline/commands/add-parser.md",
          "type": "blob",
          "size": 8121
        },
        {
          "path": "plugins/rag-pipeline/commands/add-scraper.md",
          "type": "blob",
          "size": 8961
        },
        {
          "path": "plugins/rag-pipeline/commands/add-vector-db.md",
          "type": "blob",
          "size": 8656
        },
        {
          "path": "plugins/rag-pipeline/commands/build-generation.md",
          "type": "blob",
          "size": 9719
        },
        {
          "path": "plugins/rag-pipeline/commands/build-ingestion.md",
          "type": "blob",
          "size": 7643
        },
        {
          "path": "plugins/rag-pipeline/commands/build-retrieval.md",
          "type": "blob",
          "size": 9216
        },
        {
          "path": "plugins/rag-pipeline/commands/deploy.md",
          "type": "blob",
          "size": 9828
        },
        {
          "path": "plugins/rag-pipeline/commands/init.md",
          "type": "blob",
          "size": 8226
        },
        {
          "path": "plugins/rag-pipeline/commands/optimize.md",
          "type": "blob",
          "size": 11108
        },
        {
          "path": "plugins/rag-pipeline/commands/test.md",
          "type": "blob",
          "size": 11833
        },
        {
          "path": "plugins/rag-pipeline/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/docs/COMPREHENSIVE-RAG-RESOURCES.md",
          "type": "blob",
          "size": 21460
        },
        {
          "path": "plugins/rag-pipeline/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/hooks/hooks.json",
          "type": "blob",
          "size": 161
        },
        {
          "path": "plugins/rag-pipeline/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/README.md",
          "type": "blob",
          "size": 9985
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/SKILL.md",
          "type": "blob",
          "size": 12617
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/SKILL_SUMMARY.md",
          "type": "blob",
          "size": 6876
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/examples/chunk-code.py",
          "type": "blob",
          "size": 12131
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/examples/chunk-markdown.py",
          "type": "blob",
          "size": 11210
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/examples/chunk-pdf.py",
          "type": "blob",
          "size": 10604
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/scripts/benchmark-chunking.py",
          "type": "blob",
          "size": 12442
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/scripts/chunk-fixed-size.py",
          "type": "blob",
          "size": 7479
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/scripts/chunk-recursive.py",
          "type": "blob",
          "size": 8370
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/scripts/chunk-semantic.py",
          "type": "blob",
          "size": 8139
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/templates/chunking-config.yaml",
          "type": "blob",
          "size": 6064
        },
        {
          "path": "plugins/rag-pipeline/skills/chunking-strategies/templates/custom-splitter.py",
          "type": "blob",
          "size": 10001
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/SKILL.md",
          "type": "blob",
          "size": 13768
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/examples/parse-legal-document.py",
          "type": "blob",
          "size": 14422
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/examples/parse-research-paper.py",
          "type": "blob",
          "size": 13014
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts/parse-docx.py",
          "type": "blob",
          "size": 8124
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts/parse-html.py",
          "type": "blob",
          "size": 8535
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts/parse-pdf.py",
          "type": "blob",
          "size": 10627
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts/setup-llamaparse.sh",
          "type": "blob",
          "size": 3505
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/scripts/setup-unstructured.sh",
          "type": "blob",
          "size": 4422
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/templates/multi-format-parser.py",
          "type": "blob",
          "size": 13029
        },
        {
          "path": "plugins/rag-pipeline/skills/document-parsers/templates/table-extraction.py",
          "type": "blob",
          "size": 13243
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/README.md",
          "type": "blob",
          "size": 6455
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/SKILL.md",
          "type": "blob",
          "size": 5244
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/examples/batch-embedding-generation.py",
          "type": "blob",
          "size": 7582
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/examples/embedding-cache.py",
          "type": "blob",
          "size": 10720
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/scripts/calculate-embedding-costs.py",
          "type": "blob",
          "size": 9330
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/scripts/setup-cohere-embeddings.sh",
          "type": "blob",
          "size": 2595
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/scripts/setup-huggingface-embeddings.sh",
          "type": "blob",
          "size": 2936
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/scripts/setup-openai-embeddings.sh",
          "type": "blob",
          "size": 2361
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/templates/custom-embedding-model.py",
          "type": "blob",
          "size": 8122
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/templates/huggingface-embedding-config.py",
          "type": "blob",
          "size": 5830
        },
        {
          "path": "plugins/rag-pipeline/skills/embedding-models/templates/openai-embedding-config.py",
          "type": "blob",
          "size": 4562
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/SKILL.md",
          "type": "blob",
          "size": 12116
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples/advanced-chunking.md",
          "type": "blob",
          "size": 10884
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples/basic-setup.md",
          "type": "blob",
          "size": 9318
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples/grounding-citations.md",
          "type": "blob",
          "size": 13194
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples/metadata-filtering.md",
          "type": "blob",
          "size": 9219
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/examples/multi-store.md",
          "type": "blob",
          "size": 12349
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/configure_chunking.py",
          "type": "blob",
          "size": 4524
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/extract_citations.py",
          "type": "blob",
          "size": 6193
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/search_query.py",
          "type": "blob",
          "size": 4560
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/setup_file_search.py",
          "type": "blob",
          "size": 2514
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/upload_documents.py",
          "type": "blob",
          "size": 5818
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/scripts/validate_setup.py",
          "type": "blob",
          "size": 5921
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates/chunking-config.json",
          "type": "blob",
          "size": 1944
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates/env.example",
          "type": "blob",
          "size": 3764
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates/metadata-schema.json",
          "type": "blob",
          "size": 3298
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates/python-client.py",
          "type": "blob",
          "size": 9052
        },
        {
          "path": "plugins/rag-pipeline/skills/google-file-search/templates/store-config.json",
          "type": "blob",
          "size": 1202
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/SKILL.md",
          "type": "blob",
          "size": 12070
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/examples/conversational-retrieval.py",
          "type": "blob",
          "size": 9945
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/examples/multi-query-retrieval.py",
          "type": "blob",
          "size": 10471
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/examples/self-querying-retrieval.py",
          "type": "blob",
          "size": 11305
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/scripts/create-vectorstore.sh",
          "type": "blob",
          "size": 9395
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/scripts/setup-langchain.sh",
          "type": "blob",
          "size": 7398
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/scripts/test-langchain.sh",
          "type": "blob",
          "size": 12023
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/templates/langgraph-workflow.py",
          "type": "blob",
          "size": 11540
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/templates/langsmith-integration.py",
          "type": "blob",
          "size": 12427
        },
        {
          "path": "plugins/rag-pipeline/skills/langchain-patterns/templates/rag-chain.py",
          "type": "blob",
          "size": 10678
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/SKILL.md",
          "type": "blob",
          "size": 16342
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/examples/chatbot-with-memory.py",
          "type": "blob",
          "size": 11080
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/examples/multi-document-rag.py",
          "type": "blob",
          "size": 14428
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/examples/question-answering.py",
          "type": "blob",
          "size": 8893
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/scripts/create-index.sh",
          "type": "blob",
          "size": 3517
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/scripts/setup-llamaindex.sh",
          "type": "blob",
          "size": 4074
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/scripts/test-llamaindex.sh",
          "type": "blob",
          "size": 4638
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/templates/basic-rag-pipeline.py",
          "type": "blob",
          "size": 6965
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/templates/custom-retriever.py",
          "type": "blob",
          "size": 9539
        },
        {
          "path": "plugins/rag-pipeline/skills/llamaindex-patterns/templates/llamacloud-integration.py",
          "type": "blob",
          "size": 9822
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/SKILL.md",
          "type": "blob",
          "size": 13647
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/examples/conversational-retrieval.py",
          "type": "blob",
          "size": 14829
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/examples/metadata-filtering.py",
          "type": "blob",
          "size": 15587
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/scripts/benchmark-retrieval.py",
          "type": "blob",
          "size": 14357
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/scripts/evaluate-retrieval-quality.py",
          "type": "blob",
          "size": 16098
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/templates/hybrid-search.py",
          "type": "blob",
          "size": 12208
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/templates/multi-query-retrieval.py",
          "type": "blob",
          "size": 15120
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/templates/reranking.py",
          "type": "blob",
          "size": 15543
        },
        {
          "path": "plugins/rag-pipeline/skills/retrieval-patterns/templates/semantic-search.py",
          "type": "blob",
          "size": 10630
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/README.md",
          "type": "blob",
          "size": 9234
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/SKILL.md",
          "type": "blob",
          "size": 9498
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/examples/migration-guide.md",
          "type": "blob",
          "size": 14541
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/examples/performance-tuning.md",
          "type": "blob",
          "size": 16838
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-chroma.sh",
          "type": "blob",
          "size": 10446
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-faiss.sh",
          "type": "blob",
          "size": 13689
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-pgvector.sh",
          "type": "blob",
          "size": 11571
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-pinecone.sh",
          "type": "blob",
          "size": 9899
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-qdrant.sh",
          "type": "blob",
          "size": 13973
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/scripts/setup-weaviate.sh",
          "type": "blob",
          "size": 13281
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/chroma-config.py",
          "type": "blob",
          "size": 12457
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/faiss-config.py",
          "type": "blob",
          "size": 13441
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/pgvector-schema.sql",
          "type": "blob",
          "size": 9758
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/pinecone-config.py",
          "type": "blob",
          "size": 7466
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/qdrant-config.py",
          "type": "blob",
          "size": 12385
        },
        {
          "path": "plugins/rag-pipeline/skills/vector-database-configs/templates/weaviate-schema.py",
          "type": "blob",
          "size": 11944
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/SKILL.md",
          "type": "blob",
          "size": 10861
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/examples/scrape-blog-posts.py",
          "type": "blob",
          "size": 15963
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/examples/scrape-github-docs.py",
          "type": "blob",
          "size": 10303
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/scripts/scrape-articles.py",
          "type": "blob",
          "size": 14106
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/scripts/scrape-documentation.py",
          "type": "blob",
          "size": 10061
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/scripts/setup-playwright-scraper.sh",
          "type": "blob",
          "size": 5786
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/templates/playwright-scraper-template.py",
          "type": "blob",
          "size": 9620
        },
        {
          "path": "plugins/rag-pipeline/skills/web-scraping-tools/templates/rate-limited-scraper.py",
          "type": "blob",
          "size": 11115
        },
        {
          "path": "plugins/redis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 903
        },
        {
          "path": "plugins/redis/.gitignore",
          "type": "blob",
          "size": 470
        },
        {
          "path": "plugins/redis/CHANGELOG.md",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/LICENSE",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/redis/README.md",
          "type": "blob",
          "size": 3913
        },
        {
          "path": "plugins/redis/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/agents/cache-architect.md",
          "type": "blob",
          "size": 4917
        },
        {
          "path": "plugins/redis/agents/deployment-architect.md",
          "type": "blob",
          "size": 1766
        },
        {
          "path": "plugins/redis/agents/fastapi-integrator.md",
          "type": "blob",
          "size": 1664
        },
        {
          "path": "plugins/redis/agents/monitoring-integrator.md",
          "type": "blob",
          "size": 1528
        },
        {
          "path": "plugins/redis/agents/nextjs-integrator.md",
          "type": "blob",
          "size": 1621
        },
        {
          "path": "plugins/redis/agents/pub-sub-specialist.md",
          "type": "blob",
          "size": 1881
        },
        {
          "path": "plugins/redis/agents/rate-limiter-specialist.md",
          "type": "blob",
          "size": 1907
        },
        {
          "path": "plugins/redis/agents/redis-setup-agent.md",
          "type": "blob",
          "size": 8365
        },
        {
          "path": "plugins/redis/agents/semantic-cache-specialist.md",
          "type": "blob",
          "size": 1917
        },
        {
          "path": "plugins/redis/agents/sentinel-architect.md",
          "type": "blob",
          "size": 1626
        },
        {
          "path": "plugins/redis/agents/session-manager.md",
          "type": "blob",
          "size": 2262
        },
        {
          "path": "plugins/redis/agents/vector-cache-specialist.md",
          "type": "blob",
          "size": 1925
        },
        {
          "path": "plugins/redis/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/commands/add-cache.md",
          "type": "blob",
          "size": 672
        },
        {
          "path": "plugins/redis/commands/add-cluster.md",
          "type": "blob",
          "size": 552
        },
        {
          "path": "plugins/redis/commands/add-connection-pool.md",
          "type": "blob",
          "size": 562
        },
        {
          "path": "plugins/redis/commands/add-monitoring.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "plugins/redis/commands/add-pub-sub.md",
          "type": "blob",
          "size": 619
        },
        {
          "path": "plugins/redis/commands/add-queue.md",
          "type": "blob",
          "size": 568
        },
        {
          "path": "plugins/redis/commands/add-rag.md",
          "type": "blob",
          "size": 5447
        },
        {
          "path": "plugins/redis/commands/add-rate-limiting.md",
          "type": "blob",
          "size": 617
        },
        {
          "path": "plugins/redis/commands/add-redisvl.md",
          "type": "blob",
          "size": 3878
        },
        {
          "path": "plugins/redis/commands/add-semantic-cache.md",
          "type": "blob",
          "size": 627
        },
        {
          "path": "plugins/redis/commands/add-sentinel.md",
          "type": "blob",
          "size": 568
        },
        {
          "path": "plugins/redis/commands/add-session-store.md",
          "type": "blob",
          "size": 641
        },
        {
          "path": "plugins/redis/commands/add-vector-cache.md",
          "type": "blob",
          "size": 651
        },
        {
          "path": "plugins/redis/commands/add-vector-search.md",
          "type": "blob",
          "size": 5510
        },
        {
          "path": "plugins/redis/commands/create-vector-index.md",
          "type": "blob",
          "size": 4279
        },
        {
          "path": "plugins/redis/commands/deploy.md",
          "type": "blob",
          "size": 548
        },
        {
          "path": "plugins/redis/commands/init.md",
          "type": "blob",
          "size": 2529
        },
        {
          "path": "plugins/redis/commands/integrate-celery.md",
          "type": "blob",
          "size": 574
        },
        {
          "path": "plugins/redis/commands/integrate-express.md",
          "type": "blob",
          "size": 550
        },
        {
          "path": "plugins/redis/commands/integrate-fastapi.md",
          "type": "blob",
          "size": 550
        },
        {
          "path": "plugins/redis/commands/integrate-langchain.md",
          "type": "blob",
          "size": 4322
        },
        {
          "path": "plugins/redis/commands/integrate-llamaindex.md",
          "type": "blob",
          "size": 4222
        },
        {
          "path": "plugins/redis/commands/integrate-nextjs.md",
          "type": "blob",
          "size": 550
        },
        {
          "path": "plugins/redis/commands/test.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "plugins/redis/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/hooks/hooks.json",
          "type": "blob",
          "size": 18
        },
        {
          "path": "plugins/redis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/README.md",
          "type": "blob",
          "size": 244
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/SKILL.md",
          "type": "blob",
          "size": 1007
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/examples/basic-usage.md",
          "type": "blob",
          "size": 204
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/ai-cache-patterns/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/cache-strategies",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/cache-strategies/README.md",
          "type": "blob",
          "size": 245
        },
        {
          "path": "plugins/redis/skills/cache-strategies/SKILL.md",
          "type": "blob",
          "size": 1607
        },
        {
          "path": "plugins/redis/skills/cache-strategies/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/cache-strategies/examples/basic-usage.md",
          "type": "blob",
          "size": 205
        },
        {
          "path": "plugins/redis/skills/cache-strategies/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/cache-strategies/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/cache-strategies/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/cache-strategies/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/cache-strategies/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/cache-strategies/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/cache-strategies/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/cache-strategies/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/cache-strategies/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/cache-strategies/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/cache-strategies/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/connection-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/connection-management/README.md",
          "type": "blob",
          "size": 243
        },
        {
          "path": "plugins/redis/skills/connection-management/SKILL.md",
          "type": "blob",
          "size": 1625
        },
        {
          "path": "plugins/redis/skills/connection-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/connection-management/examples/basic-usage.md",
          "type": "blob",
          "size": 203
        },
        {
          "path": "plugins/redis/skills/connection-management/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/connection-management/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/connection-management/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/connection-management/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/connection-management/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/connection-management/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/connection-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/connection-management/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/connection-management/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/connection-management/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/connection-management/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/deployment-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/deployment-configs/README.md",
          "type": "blob",
          "size": 231
        },
        {
          "path": "plugins/redis/skills/deployment-configs/SKILL.md",
          "type": "blob",
          "size": 968
        },
        {
          "path": "plugins/redis/skills/deployment-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/deployment-configs/examples/basic-usage.md",
          "type": "blob",
          "size": 191
        },
        {
          "path": "plugins/redis/skills/deployment-configs/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/deployment-configs/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/deployment-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/deployment-configs/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/deployment-configs/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/deployment-configs/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/deployment-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/deployment-configs/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/deployment-configs/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/deployment-configs/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/deployment-configs/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/framework-integrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/framework-integrations/README.md",
          "type": "blob",
          "size": 244
        },
        {
          "path": "plugins/redis/skills/framework-integrations/SKILL.md",
          "type": "blob",
          "size": 1007
        },
        {
          "path": "plugins/redis/skills/framework-integrations/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/framework-integrations/examples/basic-usage.md",
          "type": "blob",
          "size": 204
        },
        {
          "path": "plugins/redis/skills/framework-integrations/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/framework-integrations/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/framework-integrations/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/framework-integrations/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/framework-integrations/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/framework-integrations/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/framework-integrations/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/framework-integrations/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/framework-integrations/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/framework-integrations/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/framework-integrations/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/README.md",
          "type": "blob",
          "size": 229
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/SKILL.md",
          "type": "blob",
          "size": 962
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/examples/basic-usage.md",
          "type": "blob",
          "size": 189
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/monitoring-patterns/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/README.md",
          "type": "blob",
          "size": 240
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/SKILL.md",
          "type": "blob",
          "size": 1560
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/examples/basic-usage.md",
          "type": "blob",
          "size": 200
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/pub-sub-patterns/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/README.md",
          "type": "blob",
          "size": 239
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/SKILL.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/examples/basic-usage.md",
          "type": "blob",
          "size": 199
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/rate-limiting-patterns/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/README.md",
          "type": "blob",
          "size": 240
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/SKILL.md",
          "type": "blob",
          "size": 995
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/examples/basic-usage.md",
          "type": "blob",
          "size": 200
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/sentinel-configurations/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/redis/skills/session-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/session-management/README.md",
          "type": "blob",
          "size": 243
        },
        {
          "path": "plugins/redis/skills/session-management/SKILL.md",
          "type": "blob",
          "size": 1004
        },
        {
          "path": "plugins/redis/skills/session-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/session-management/examples/basic-usage.md",
          "type": "blob",
          "size": 203
        },
        {
          "path": "plugins/redis/skills/session-management/examples/fastapi-example.md",
          "type": "blob",
          "size": 588
        },
        {
          "path": "plugins/redis/skills/session-management/examples/nextjs-example.md",
          "type": "blob",
          "size": 718
        },
        {
          "path": "plugins/redis/skills/session-management/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/session-management/scripts/setup-redis.sh",
          "type": "blob",
          "size": 346
        },
        {
          "path": "plugins/redis/skills/session-management/scripts/test-connection.sh",
          "type": "blob",
          "size": 121
        },
        {
          "path": "plugins/redis/skills/session-management/scripts/validate-config.sh",
          "type": "blob",
          "size": 375
        },
        {
          "path": "plugins/redis/skills/session-management/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/redis/skills/session-management/templates/basic-config.py.template",
          "type": "blob",
          "size": 454
        },
        {
          "path": "plugins/redis/skills/session-management/templates/basic-config.ts.template",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/redis/skills/session-management/templates/docker-compose.yml.template",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/redis/skills/session-management/templates/redis-config.env.template",
          "type": "blob",
          "size": 236
        },
        {
          "path": "plugins/resend",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 508
        },
        {
          "path": "plugins/resend/.gitignore",
          "type": "blob",
          "size": 274
        },
        {
          "path": "plugins/resend/CHANGELOG.md",
          "type": "blob",
          "size": 325
        },
        {
          "path": "plugins/resend/LICENSE",
          "type": "blob",
          "size": 1073
        },
        {
          "path": "plugins/resend/README.md",
          "type": "blob",
          "size": 1483
        },
        {
          "path": "plugins/resend/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/agents/resend-broadcasts-agent.md",
          "type": "blob",
          "size": 5585
        },
        {
          "path": "plugins/resend/agents/resend-contacts-agent.md",
          "type": "blob",
          "size": 8866
        },
        {
          "path": "plugins/resend/agents/resend-domains-webhooks-agent.md",
          "type": "blob",
          "size": 7347
        },
        {
          "path": "plugins/resend/agents/resend-email-agent.md",
          "type": "blob",
          "size": 9405
        },
        {
          "path": "plugins/resend/agents/resend-setup-agent.md",
          "type": "blob",
          "size": 6206
        },
        {
          "path": "plugins/resend/agents/resend-templates-agent.md",
          "type": "blob",
          "size": 7406
        },
        {
          "path": "plugins/resend/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/commands/add-broadcasts.md",
          "type": "blob",
          "size": 3191
        },
        {
          "path": "plugins/resend/commands/add-contacts.md",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/resend/commands/add-domains.md",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "plugins/resend/commands/add-react-email.md",
          "type": "blob",
          "size": 3046
        },
        {
          "path": "plugins/resend/commands/add-templates.md",
          "type": "blob",
          "size": 3338
        },
        {
          "path": "plugins/resend/commands/add-webhooks.md",
          "type": "blob",
          "size": 3499
        },
        {
          "path": "plugins/resend/commands/build-email-system.md",
          "type": "blob",
          "size": 15370
        },
        {
          "path": "plugins/resend/commands/init.md",
          "type": "blob",
          "size": 3270
        },
        {
          "path": "plugins/resend/commands/send-email.md",
          "type": "blob",
          "size": 4071
        },
        {
          "path": "plugins/resend/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns/SKILL.md",
          "type": "blob",
          "size": 24628
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/error-handling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/error-handling/README.md",
          "type": "blob",
          "size": 18549
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/python-client",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/python-client/README.md",
          "type": "blob",
          "size": 15916
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/typescript-client",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/api-patterns/examples/typescript-client/README.md",
          "type": "blob",
          "size": 11972
        },
        {
          "path": "plugins/resend/skills/email-delivery",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/SKILL.md",
          "type": "blob",
          "size": 9760
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/attachments",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/attachments/README.md",
          "type": "blob",
          "size": 15391
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/batch-emails",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/batch-emails/README.md",
          "type": "blob",
          "size": 15119
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/scheduled",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/scheduled/README.md",
          "type": "blob",
          "size": 15736
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/single-email",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/examples/single-email/README.md",
          "type": "blob",
          "size": 12338
        },
        {
          "path": "plugins/resend/skills/email-delivery/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/scripts/generate-html-template.sh",
          "type": "blob",
          "size": 12695
        },
        {
          "path": "plugins/resend/skills/email-delivery/scripts/validate-attachment.sh",
          "type": "blob",
          "size": 3427
        },
        {
          "path": "plugins/resend/skills/email-delivery/scripts/validate-email.sh",
          "type": "blob",
          "size": 2285
        },
        {
          "path": "plugins/resend/skills/email-delivery/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/email-delivery/templates/attachment-email-template.ts",
          "type": "blob",
          "size": 8864
        },
        {
          "path": "plugins/resend/skills/email-delivery/templates/batch-email-template.ts",
          "type": "blob",
          "size": 5995
        },
        {
          "path": "plugins/resend/skills/email-delivery/templates/scheduled-email-template.ts",
          "type": "blob",
          "size": 7907
        },
        {
          "path": "plugins/resend/skills/email-delivery/templates/single-email-template.ts",
          "type": "blob",
          "size": 4395
        },
        {
          "path": "plugins/resend/skills/react-email-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/SKILL.md",
          "type": "blob",
          "size": 24638
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/marketing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/marketing/README.md",
          "type": "blob",
          "size": 23441
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/transactional",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/transactional/README.md",
          "type": "blob",
          "size": 20133
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/welcome-email",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/examples/welcome-email/README.md",
          "type": "blob",
          "size": 13554
        },
        {
          "path": "plugins/resend/skills/react-email-templates/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/scripts/generate-component.sh",
          "type": "blob",
          "size": 12285
        },
        {
          "path": "plugins/resend/skills/react-email-templates/scripts/setup-preview-server.sh",
          "type": "blob",
          "size": 5446
        },
        {
          "path": "plugins/resend/skills/react-email-templates/scripts/validate-component.sh",
          "type": "blob",
          "size": 3223
        },
        {
          "path": "plugins/resend/skills/react-email-templates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/react-email-templates/templates/.env.example",
          "type": "blob",
          "size": 493
        },
        {
          "path": "plugins/resend/skills/react-email-templates/templates/component-test.template",
          "type": "blob",
          "size": 1599
        },
        {
          "path": "plugins/resend/skills/react-email-templates/templates/package.json.template",
          "type": "blob",
          "size": 1354
        },
        {
          "path": "plugins/resend/skills/react-email-templates/templates/resend-integration.template",
          "type": "blob",
          "size": 5060
        },
        {
          "path": "plugins/resend/skills/webhook-handlers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/SKILL.md",
          "type": "blob",
          "size": 18977
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/event-processing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/event-processing/README.md",
          "type": "blob",
          "size": 16849
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/fastapi-webhook",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/fastapi-webhook/README.md",
          "type": "blob",
          "size": 17064
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/nextjs-webhook",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/examples/nextjs-webhook/README.md",
          "type": "blob",
          "size": 12406
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/scripts/generate-test-payload.sh",
          "type": "blob",
          "size": 2880
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/scripts/test-webhook.sh",
          "type": "blob",
          "size": 3786
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/scripts/verify-signature.sh",
          "type": "blob",
          "size": 1266
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/templates/.env.example",
          "type": "blob",
          "size": 547
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/templates/fastapi-endpoint.py",
          "type": "blob",
          "size": 3506
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/templates/nextjs-route.ts",
          "type": "blob",
          "size": 2500
        },
        {
          "path": "plugins/resend/skills/webhook-handlers/templates/prisma-schema.prisma",
          "type": "blob",
          "size": 2318
        },
        {
          "path": "plugins/supabase",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 725
        },
        {
          "path": "plugins/supabase/.gitignore",
          "type": "blob",
          "size": 406
        },
        {
          "path": "plugins/supabase/CHANGELOG.md",
          "type": "blob",
          "size": 3412
        },
        {
          "path": "plugins/supabase/LICENSE",
          "type": "blob",
          "size": 1085
        },
        {
          "path": "plugins/supabase/README.md",
          "type": "blob",
          "size": 6552
        },
        {
          "path": "plugins/supabase/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/agents/supabase-ai-specialist.md",
          "type": "blob",
          "size": 18056
        },
        {
          "path": "plugins/supabase/agents/supabase-architect.md",
          "type": "blob",
          "size": 18749
        },
        {
          "path": "plugins/supabase/agents/supabase-code-reviewer.md",
          "type": "blob",
          "size": 9336
        },
        {
          "path": "plugins/supabase/agents/supabase-database-executor.md",
          "type": "blob",
          "size": 12780
        },
        {
          "path": "plugins/supabase/agents/supabase-migration-applier.md",
          "type": "blob",
          "size": 10750
        },
        {
          "path": "plugins/supabase/agents/supabase-performance-analyzer.md",
          "type": "blob",
          "size": 9493
        },
        {
          "path": "plugins/supabase/agents/supabase-project-manager.md",
          "type": "blob",
          "size": 9943
        },
        {
          "path": "plugins/supabase/agents/supabase-realtime-builder.md",
          "type": "blob",
          "size": 10038
        },
        {
          "path": "plugins/supabase/agents/supabase-schema-validator.md",
          "type": "blob",
          "size": 8843
        },
        {
          "path": "plugins/supabase/agents/supabase-security-auditor.md",
          "type": "blob",
          "size": 8856
        },
        {
          "path": "plugins/supabase/agents/supabase-security-specialist.md",
          "type": "blob",
          "size": 19854
        },
        {
          "path": "plugins/supabase/agents/supabase-tester.md",
          "type": "blob",
          "size": 9155
        },
        {
          "path": "plugins/supabase/agents/supabase-ui-generator.md",
          "type": "blob",
          "size": 9737
        },
        {
          "path": "plugins/supabase/agents/supabase-validator.md",
          "type": "blob",
          "size": 9323
        },
        {
          "path": "plugins/supabase/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/commands/add-auth.md",
          "type": "blob",
          "size": 7864
        },
        {
          "path": "plugins/supabase/commands/add-realtime.md",
          "type": "blob",
          "size": 4917
        },
        {
          "path": "plugins/supabase/commands/add-rls.md",
          "type": "blob",
          "size": 4900
        },
        {
          "path": "plugins/supabase/commands/add-storage.md",
          "type": "blob",
          "size": 4869
        },
        {
          "path": "plugins/supabase/commands/add-ui-components.md",
          "type": "blob",
          "size": 4809
        },
        {
          "path": "plugins/supabase/commands/create-schema.md",
          "type": "blob",
          "size": 4888
        },
        {
          "path": "plugins/supabase/commands/deploy-migration.md",
          "type": "blob",
          "size": 4841
        },
        {
          "path": "plugins/supabase/commands/generate-types.md",
          "type": "blob",
          "size": 4805
        },
        {
          "path": "plugins/supabase/commands/init-ai-app.md",
          "type": "blob",
          "size": 6183
        },
        {
          "path": "plugins/supabase/commands/init.md",
          "type": "blob",
          "size": 6248
        },
        {
          "path": "plugins/supabase/commands/setup-ai.md",
          "type": "blob",
          "size": 8691
        },
        {
          "path": "plugins/supabase/commands/setup-pgvector.md",
          "type": "blob",
          "size": 4909
        },
        {
          "path": "plugins/supabase/commands/test-e2e.md",
          "type": "blob",
          "size": 8796
        },
        {
          "path": "plugins/supabase/commands/test-rls.md",
          "type": "blob",
          "size": 4847
        },
        {
          "path": "plugins/supabase/commands/validate-schema.md",
          "type": "blob",
          "size": 4857
        },
        {
          "path": "plugins/supabase/commands/validate-setup.md",
          "type": "blob",
          "size": 9461
        },
        {
          "path": "plugins/supabase/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/docs/SUPABASE_ALL_DOCUMENTATION_LINKS.md",
          "type": "blob",
          "size": 32295
        },
        {
          "path": "plugins/supabase/docs/SUPABASE_DOCS_SUMMARY.md",
          "type": "blob",
          "size": 9650
        },
        {
          "path": "plugins/supabase/docs/SUPABASE_INTEGRATION_GUIDE.md",
          "type": "blob",
          "size": 19596
        },
        {
          "path": "plugins/supabase/docs/SUPABASE_QUICK_REFERENCE.md",
          "type": "blob",
          "size": 10987
        },
        {
          "path": "plugins/supabase/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/README.md",
          "type": "blob",
          "size": 12080
        },
        {
          "path": "plugins/supabase/skills/auth-configs/SKILL.md",
          "type": "blob",
          "size": 7686
        },
        {
          "path": "plugins/supabase/skills/auth-configs/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/examples/ai-app-patterns.md",
          "type": "blob",
          "size": 17495
        },
        {
          "path": "plugins/supabase/skills/auth-configs/examples/auth-flows.md",
          "type": "blob",
          "size": 17643
        },
        {
          "path": "plugins/supabase/skills/auth-configs/examples/oauth-setup-guide.md",
          "type": "blob",
          "size": 9787
        },
        {
          "path": "plugins/supabase/skills/auth-configs/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/scripts/configure-jwt.sh",
          "type": "blob",
          "size": 10552
        },
        {
          "path": "plugins/supabase/skills/auth-configs/scripts/setup-email-auth.sh",
          "type": "blob",
          "size": 12299
        },
        {
          "path": "plugins/supabase/skills/auth-configs/scripts/setup-oauth-provider.sh",
          "type": "blob",
          "size": 9098
        },
        {
          "path": "plugins/supabase/skills/auth-configs/scripts/test-auth-flow.sh",
          "type": "blob",
          "size": 11898
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/email-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/email-templates/confirmation.html",
          "type": "blob",
          "size": 3318
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/email-templates/invite.html",
          "type": "blob",
          "size": 3427
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/email-templates/magic-link.html",
          "type": "blob",
          "size": 3340
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/email-templates/password-reset.html",
          "type": "blob",
          "size": 3621
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/helpers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/helpers/auth-helpers.ts",
          "type": "blob",
          "size": 9984
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/middleware",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/middleware/auth-middleware.ts",
          "type": "blob",
          "size": 3627
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/oauth-providers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/oauth-providers/discord-oauth-config.json",
          "type": "blob",
          "size": 4504
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/oauth-providers/github-oauth-config.json",
          "type": "blob",
          "size": 3742
        },
        {
          "path": "plugins/supabase/skills/auth-configs/templates/oauth-providers/google-oauth-config.json",
          "type": "blob",
          "size": 3563
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/QUICK_START.md",
          "type": "blob",
          "size": 5239
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/README.md",
          "type": "blob",
          "size": 5327
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/SKILL.md",
          "type": "blob",
          "size": 14616
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/SKILL_SUMMARY.md",
          "type": "blob",
          "size": 9242
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/examples/ci-cd-integration.md",
          "type": "blob",
          "size": 12013
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/examples/complete-test-workflow.md",
          "type": "blob",
          "size": 10715
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/examples/test-data-strategies.md",
          "type": "blob",
          "size": 14068
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/cleanup-test-resources.sh",
          "type": "blob",
          "size": 6743
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/run-e2e-tests.sh",
          "type": "blob",
          "size": 8848
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/setup-test-env.sh",
          "type": "blob",
          "size": 8641
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/test-ai-features.sh",
          "type": "blob",
          "size": 8284
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/test-auth-workflow.sh",
          "type": "blob",
          "size": 8103
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/scripts/test-realtime-workflow.sh",
          "type": "blob",
          "size": 4939
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates/auth-tests.ts",
          "type": "blob",
          "size": 8657
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates/ci-config.yml",
          "type": "blob",
          "size": 7574
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates/realtime-tests.ts",
          "type": "blob",
          "size": 13115
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates/test-suite-template.ts",
          "type": "blob",
          "size": 5022
        },
        {
          "path": "plugins/supabase/skills/e2e-test-scenarios/templates/vector-search-tests.ts",
          "type": "blob",
          "size": 11917
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/QUICK_START.md",
          "type": "blob",
          "size": 2794
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/README.md",
          "type": "blob",
          "size": 7409
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/SKILL.md",
          "type": "blob",
          "size": 7348
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/examples/document-search-pattern.md",
          "type": "blob",
          "size": 20155
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/examples/embedding-strategies.md",
          "type": "blob",
          "size": 10359
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/examples/vector-search-examples.md",
          "type": "blob",
          "size": 14249
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/scripts/create-indexes.sh",
          "type": "blob",
          "size": 4667
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/scripts/setup-hybrid-search.sh",
          "type": "blob",
          "size": 5550
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/scripts/setup-pgvector.sh",
          "type": "blob",
          "size": 1796
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/scripts/test-vector-search.sh",
          "type": "blob",
          "size": 6486
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates/embedding-table-schema.sql",
          "type": "blob",
          "size": 3568
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates/hnsw-index-config.sql",
          "type": "blob",
          "size": 2921
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates/hybrid-search-function.sql",
          "type": "blob",
          "size": 7197
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates/ivfflat-index-config.sql",
          "type": "blob",
          "size": 4547
        },
        {
          "path": "plugins/supabase/skills/pgvector-setup/templates/match-function.sql",
          "type": "blob",
          "size": 7441
        },
        {
          "path": "plugins/supabase/skills/rls-templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-templates/README.md",
          "type": "blob",
          "size": 9087
        },
        {
          "path": "plugins/supabase/skills/rls-templates/SKILL.md",
          "type": "blob",
          "size": 5023
        },
        {
          "path": "plugins/supabase/skills/rls-templates/VALIDATION.md",
          "type": "blob",
          "size": 5061
        },
        {
          "path": "plugins/supabase/skills/rls-templates/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-templates/examples/common-patterns.md",
          "type": "blob",
          "size": 13863
        },
        {
          "path": "plugins/supabase/skills/rls-templates/examples/migration-guide.md",
          "type": "blob",
          "size": 14698
        },
        {
          "path": "plugins/supabase/skills/rls-templates/examples/testing-guide.md",
          "type": "blob",
          "size": 14809
        },
        {
          "path": "plugins/supabase/skills/rls-templates/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-templates/scripts/apply-rls-policies.sh",
          "type": "blob",
          "size": 4629
        },
        {
          "path": "plugins/supabase/skills/rls-templates/scripts/audit-rls.sh",
          "type": "blob",
          "size": 9084
        },
        {
          "path": "plugins/supabase/skills/rls-templates/scripts/generate-policy.sh",
          "type": "blob",
          "size": 8762
        },
        {
          "path": "plugins/supabase/skills/rls-templates/scripts/test-rls-policies.sh",
          "type": "blob",
          "size": 8769
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates/ai-chat-policies.sql",
          "type": "blob",
          "size": 6718
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates/embeddings-policies.sql",
          "type": "blob",
          "size": 7734
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates/multi-tenant.sql",
          "type": "blob",
          "size": 3751
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates/role-based-access.sql",
          "type": "blob",
          "size": 4574
        },
        {
          "path": "plugins/supabase/skills/rls-templates/templates/user-isolation.sql",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/README.md",
          "type": "blob",
          "size": 11849
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/SKILL.md",
          "type": "blob",
          "size": 9369
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/examples/ci-integration.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/examples/common-vulnerabilities.md",
          "type": "blob",
          "size": 15036
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/examples/testing-workflow.md",
          "type": "blob",
          "size": 11361
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/audit-rls-coverage.sh",
          "type": "blob",
          "size": 5840
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/run-all-rls-tests.sh",
          "type": "blob",
          "size": 6531
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/test-anonymous-access.sh",
          "type": "blob",
          "size": 4587
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/test-multi-tenant-isolation.sh",
          "type": "blob",
          "size": 14755
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/test-role-permissions.sh",
          "type": "blob",
          "size": 9761
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/scripts/test-user-isolation.sh",
          "type": "blob",
          "size": 11216
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/templates/rls-test-suite.ts",
          "type": "blob",
          "size": 8391
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/templates/security-checklist.md",
          "type": "blob",
          "size": 6956
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/templates/test-scenarios.json",
          "type": "blob",
          "size": 10585
        },
        {
          "path": "plugins/supabase/skills/rls-test-patterns/templates/user-isolation-tests.sql",
          "type": "blob",
          "size": 4455
        },
        {
          "path": "plugins/supabase/skills/schema-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/README.md",
          "type": "blob",
          "size": 5783
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/SKILL.md",
          "type": "blob",
          "size": 5931
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/examples/complete-ai-app-schema.md",
          "type": "blob",
          "size": 11263
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/examples/indexing-strategy.md",
          "type": "blob",
          "size": 13196
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/examples/migration-guide.md",
          "type": "blob",
          "size": 11725
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/scripts/apply-migration.sh",
          "type": "blob",
          "size": 2934
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/scripts/generate-schema.sh",
          "type": "blob",
          "size": 3551
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/scripts/seed-data.sh",
          "type": "blob",
          "size": 11240
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/scripts/validate-schema.sh",
          "type": "blob",
          "size": 5129
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/ai-usage-tracking-schema.sql",
          "type": "blob",
          "size": 15277
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/chat-schema.sql",
          "type": "blob",
          "size": 10376
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/migration-template.sql",
          "type": "blob",
          "size": 4031
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/multi-tenant-schema.sql",
          "type": "blob",
          "size": 15589
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/rag-schema.sql",
          "type": "blob",
          "size": 12957
        },
        {
          "path": "plugins/supabase/skills/schema-patterns/templates/user-management-schema.sql",
          "type": "blob",
          "size": 12416
        },
        {
          "path": "plugins/supabase/skills/schema-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-validation/QUICKSTART.md",
          "type": "blob",
          "size": 5718
        },
        {
          "path": "plugins/supabase/skills/schema-validation/README.md",
          "type": "blob",
          "size": 10937
        },
        {
          "path": "plugins/supabase/skills/schema-validation/SKILL.md",
          "type": "blob",
          "size": 4049
        },
        {
          "path": "plugins/supabase/skills/schema-validation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-validation/examples/ci-integration.md",
          "type": "blob",
          "size": 14526
        },
        {
          "path": "plugins/supabase/skills/schema-validation/examples/common-issues.md",
          "type": "blob",
          "size": 11752
        },
        {
          "path": "plugins/supabase/skills/schema-validation/examples/test-schema.sql",
          "type": "blob",
          "size": 4837
        },
        {
          "path": "plugins/supabase/skills/schema-validation/examples/validation-workflow.md",
          "type": "blob",
          "size": 10014
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/full-validation.sh",
          "type": "blob",
          "size": 5806
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/validate-constraints.sh",
          "type": "blob",
          "size": 9224
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/validate-indexes.sh",
          "type": "blob",
          "size": 8360
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/validate-naming.sh",
          "type": "blob",
          "size": 7713
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/validate-rls.sh",
          "type": "blob",
          "size": 10567
        },
        {
          "path": "plugins/supabase/skills/schema-validation/scripts/validate-sql-syntax.sh",
          "type": "blob",
          "size": 6554
        },
        {
          "path": "plugins/supabase/skills/schema-validation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/supabase/skills/schema-validation/templates/naming-conventions.json",
          "type": "blob",
          "size": 7300
        },
        {
          "path": "plugins/supabase/skills/schema-validation/templates/sql-best-practices.md",
          "type": "blob",
          "size": 6401
        },
        {
          "path": "plugins/supabase/skills/schema-validation/templates/validation-report-template.md",
          "type": "blob",
          "size": 2793
        },
        {
          "path": "plugins/supabase/skills/schema-validation/templates/validation-rules.json",
          "type": "blob",
          "size": 3734
        },
        {
          "path": "plugins/vercel-ai-sdk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 744
        },
        {
          "path": "plugins/vercel-ai-sdk/.gitignore",
          "type": "blob",
          "size": 258
        },
        {
          "path": "plugins/vercel-ai-sdk/CHANGELOG.md",
          "type": "blob",
          "size": 655
        },
        {
          "path": "plugins/vercel-ai-sdk/LICENSE",
          "type": "blob",
          "size": 1067
        },
        {
          "path": "plugins/vercel-ai-sdk/README.md",
          "type": "blob",
          "size": 12638
        },
        {
          "path": "plugins/vercel-ai-sdk/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-advanced-agent.md",
          "type": "blob",
          "size": 14752
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-data-agent.md",
          "type": "blob",
          "size": 13258
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-production-agent.md",
          "type": "blob",
          "size": 13687
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-ui-agent.md",
          "type": "blob",
          "size": 12623
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-verifier-js.md",
          "type": "blob",
          "size": 7189
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-verifier-py.md",
          "type": "blob",
          "size": 7168
        },
        {
          "path": "plugins/vercel-ai-sdk/agents/vercel-ai-verifier-ts.md",
          "type": "blob",
          "size": 8806
        },
        {
          "path": "plugins/vercel-ai-sdk/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-advanced.md",
          "type": "blob",
          "size": 6697
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-chat.md",
          "type": "blob",
          "size": 5599
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-data-features.md",
          "type": "blob",
          "size": 6464
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-production.md",
          "type": "blob",
          "size": 6618
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-provider.md",
          "type": "blob",
          "size": 5321
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-streaming.md",
          "type": "blob",
          "size": 5940
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-tools.md",
          "type": "blob",
          "size": 5692
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/add-ui-features.md",
          "type": "blob",
          "size": 6482
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/new-ai-app.md",
          "type": "blob",
          "size": 14494
        },
        {
          "path": "plugins/vercel-ai-sdk/commands/new-app.md",
          "type": "blob",
          "size": 7200
        },
        {
          "path": "plugins/vercel-ai-sdk/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/docs/vercel-ai-sdk-documentation-links.md",
          "type": "blob",
          "size": 8526
        },
        {
          "path": "plugins/vercel-ai-sdk/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/SKILLS-OVERVIEW.md",
          "type": "blob",
          "size": 9645
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/agent-workflow-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/agent-workflow-patterns/SKILL.md",
          "type": "blob",
          "size": 6029
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/agent-workflow-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/agent-workflow-patterns/templates/react-agent.ts",
          "type": "blob",
          "size": 9529
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/SKILL.md",
          "type": "blob",
          "size": 8597
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/examples/chart-generator.tsx",
          "type": "blob",
          "size": 8742
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/scripts/validate-rsc-setup.sh",
          "type": "blob",
          "size": 4701
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/templates/client-wrapper.tsx",
          "type": "blob",
          "size": 6389
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/templates/server-action-pattern.tsx",
          "type": "blob",
          "size": 5072
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/README.md",
          "type": "blob",
          "size": 8118
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/SKILL.md",
          "type": "blob",
          "size": 4818
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/examples/troubleshooting-guide.md",
          "type": "blob",
          "size": 7542
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/scripts/check-model-compatibility.sh",
          "type": "blob",
          "size": 4345
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/scripts/generate-fix.sh",
          "type": "blob",
          "size": 13589
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/scripts/test-provider-connection.sh",
          "type": "blob",
          "size": 6081
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/scripts/validate-provider.sh",
          "type": "blob",
          "size": 7509
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/templates/env-template.txt",
          "type": "blob",
          "size": 2364
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/templates/error-handler-template.ts",
          "type": "blob",
          "size": 6187
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/templates/gitignore-template.txt",
          "type": "blob",
          "size": 419
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation/SKILL.md",
          "type": "blob",
          "size": 11992
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation/scripts/validate-rag-setup.sh",
          "type": "blob",
          "size": 4569
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/rag-implementation/templates/rag-pipeline.ts",
          "type": "blob",
          "size": 11794
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/testing-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/testing-patterns/SKILL.md",
          "type": "blob",
          "size": 2947
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/testing-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/vercel-ai-sdk/skills/testing-patterns/templates/mock-provider.ts",
          "type": "blob",
          "size": 7296
        },
        {
          "path": "plugins/website-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 657
        },
        {
          "path": "plugins/website-builder/.env.example",
          "type": "blob",
          "size": 3219
        },
        {
          "path": "plugins/website-builder/.gitignore",
          "type": "blob",
          "size": 401
        },
        {
          "path": "plugins/website-builder/.mcp.json.example",
          "type": "blob",
          "size": 441
        },
        {
          "path": "plugins/website-builder/CHANGELOG.md",
          "type": "blob",
          "size": 3737
        },
        {
          "path": "plugins/website-builder/CONTEXTUAL-DOCS-PATTERN.md",
          "type": "blob",
          "size": 6030
        },
        {
          "path": "plugins/website-builder/LICENSE",
          "type": "blob",
          "size": 1075
        },
        {
          "path": "plugins/website-builder/README.md",
          "type": "blob",
          "size": 6786
        },
        {
          "path": "plugins/website-builder/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/agents/website-ai-generator.md",
          "type": "blob",
          "size": 13461
        },
        {
          "path": "plugins/website-builder/agents/website-architect.md",
          "type": "blob",
          "size": 11806
        },
        {
          "path": "plugins/website-builder/agents/website-content.md",
          "type": "blob",
          "size": 13224
        },
        {
          "path": "plugins/website-builder/agents/website-setup.md",
          "type": "blob",
          "size": 12649
        },
        {
          "path": "plugins/website-builder/agents/website-verifier.md",
          "type": "blob",
          "size": 10045
        },
        {
          "path": "plugins/website-builder/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/commands/add-blog.md",
          "type": "blob",
          "size": 5643
        },
        {
          "path": "plugins/website-builder/commands/add-component.md",
          "type": "blob",
          "size": 5718
        },
        {
          "path": "plugins/website-builder/commands/add-page.md",
          "type": "blob",
          "size": 5628
        },
        {
          "path": "plugins/website-builder/commands/deploy-marketing-site.md",
          "type": "blob",
          "size": 14433
        },
        {
          "path": "plugins/website-builder/commands/deploy.md",
          "type": "blob",
          "size": 6222
        },
        {
          "path": "plugins/website-builder/commands/generate-content.md",
          "type": "blob",
          "size": 6026
        },
        {
          "path": "plugins/website-builder/commands/generate-images.md",
          "type": "blob",
          "size": 5926
        },
        {
          "path": "plugins/website-builder/commands/init.md",
          "type": "blob",
          "size": 6990
        },
        {
          "path": "plugins/website-builder/commands/integrate-content-generation.md",
          "type": "blob",
          "size": 6068
        },
        {
          "path": "plugins/website-builder/commands/integrate-supabase-cms.md",
          "type": "blob",
          "size": 5667
        },
        {
          "path": "plugins/website-builder/commands/optimize-seo.md",
          "type": "blob",
          "size": 5964
        },
        {
          "path": "plugins/website-builder/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/docs/API-KEYS-MANAGEMENT.md",
          "type": "blob",
          "size": 6217
        },
        {
          "path": "plugins/website-builder/docs/frameworks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/docs/frameworks/tailwind",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/docs/frameworks/tailwind/TAILWIND-UI-ANALYSIS.md",
          "type": "blob",
          "size": 15726
        },
        {
          "path": "plugins/website-builder/docs/frameworks/tailwind/TAILWIND-UI-MCP-SPEC.md",
          "type": "blob",
          "size": 18805
        },
        {
          "path": "plugins/website-builder/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/scripts/setup-env.sh",
          "type": "blob",
          "size": 3413
        },
        {
          "path": "plugins/website-builder/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/SKILL.md",
          "type": "blob",
          "size": 8709
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/examples/batch-content-generation-example.md",
          "type": "blob",
          "size": 3292
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/examples/dynamic-content-personalization-example.md",
          "type": "blob",
          "size": 2266
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/examples/hero-image-generation-example.md",
          "type": "blob",
          "size": 2848
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/calculate-cost.sh",
          "type": "blob",
          "size": 7811
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/enhance-image-prompt.sh",
          "type": "blob",
          "size": 4632
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/optimize-batch.sh",
          "type": "blob",
          "size": 9964
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/setup-environment.sh",
          "type": "blob",
          "size": 4353
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/validate-mcp-setup.sh",
          "type": "blob",
          "size": 4490
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/scripts/validate-output.sh",
          "type": "blob",
          "size": 8784
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/prompts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/prompts/blog-post-prompt.md",
          "type": "blob",
          "size": 1404
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/prompts/hero-section-prompt.md",
          "type": "blob",
          "size": 1374
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/python/content_generator.py",
          "type": "blob",
          "size": 656
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/python/image_generator.py",
          "type": "blob",
          "size": 551
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/typescript",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/typescript/content-generator.ts",
          "type": "blob",
          "size": 655
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/typescript/image-generator.ts",
          "type": "blob",
          "size": 669
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/workflows/batch-generation.ts",
          "type": "blob",
          "size": 912
        },
        {
          "path": "plugins/website-builder/skills/ai-content-generation/templates/workflows/cost-estimation.ts",
          "type": "blob",
          "size": 621
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/SKILL.md",
          "type": "blob",
          "size": 15063
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/examples/blog-site-example.md",
          "type": "blob",
          "size": 2110
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/examples/dynamic-routing-example.md",
          "type": "blob",
          "size": 3509
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/examples/seo-performance-example.md",
          "type": "blob",
          "size": 4839
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/analyze-performance.sh",
          "type": "blob",
          "size": 3634
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/generate-page.sh",
          "type": "blob",
          "size": 2277
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/generate-route.sh",
          "type": "blob",
          "size": 2316
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/generate-sitemap.sh",
          "type": "blob",
          "size": 2375
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/optimize-build.sh",
          "type": "blob",
          "size": 2982
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/setup-components.sh",
          "type": "blob",
          "size": 5489
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/setup-layouts.sh",
          "type": "blob",
          "size": 6451
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/setup-routing.sh",
          "type": "blob",
          "size": 2078
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/scripts/validate-structure.sh",
          "type": "blob",
          "size": 3049
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/components",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/components/base-component.astro",
          "type": "blob",
          "size": 566
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/content",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/content/blog-post.md",
          "type": "blob",
          "size": 262
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/content/landing-page.astro",
          "type": "blob",
          "size": 383
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/layouts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/layouts/base-layout.astro",
          "type": "blob",
          "size": 1205
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/routing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-patterns/templates/routing/dynamic-route.astro",
          "type": "blob",
          "size": 857
        },
        {
          "path": "plugins/website-builder/skills/astro-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/DOCUMENTATION-STRATEGY.md",
          "type": "blob",
          "size": 14332
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/SKILL.md",
          "type": "blob",
          "size": 2844
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/scripts/check-prerequisites.sh",
          "type": "blob",
          "size": 2429
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/scripts/init-project.sh",
          "type": "blob",
          "size": 6544
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/templates/astro.config.mjs",
          "type": "blob",
          "size": 931
        },
        {
          "path": "plugins/website-builder/skills/astro-setup/templates/project-structure.md",
          "type": "blob",
          "size": 6614
        },
        {
          "path": "plugins/website-builder/skills/component-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/SKILL.md",
          "type": "blob",
          "size": 10781
        },
        {
          "path": "plugins/website-builder/skills/component-integration/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/examples/basic-integration.md",
          "type": "blob",
          "size": 9205
        },
        {
          "path": "plugins/website-builder/skills/component-integration/examples/component-library-integration.md",
          "type": "blob",
          "size": 16202
        },
        {
          "path": "plugins/website-builder/skills/component-integration/examples/mdx-blog-post.md",
          "type": "blob",
          "size": 14462
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/add-component-library.sh",
          "type": "blob",
          "size": 5324
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/generate-component.sh",
          "type": "blob",
          "size": 7762
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/optimize-components.sh",
          "type": "blob",
          "size": 5665
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/setup-mdx.sh",
          "type": "blob",
          "size": 4302
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/setup-react.sh",
          "type": "blob",
          "size": 2586
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/setup-tailwind.sh",
          "type": "blob",
          "size": 4879
        },
        {
          "path": "plugins/website-builder/skills/component-integration/scripts/validate-integration.sh",
          "type": "blob",
          "size": 4466
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/integration/astro-config-full.ts",
          "type": "blob",
          "size": 672
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/integration/tsconfig-components.json",
          "type": "blob",
          "size": 417
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/mdx",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/mdx/basic-mdx.mdx",
          "type": "blob",
          "size": 483
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/mdx/mdx-with-components.mdx",
          "type": "blob",
          "size": 834
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/basic-component.tsx",
          "type": "blob",
          "size": 1041
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/component-with-context.tsx",
          "type": "blob",
          "size": 6889
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/data-fetching-component.tsx",
          "type": "blob",
          "size": 6192
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/form-component.tsx",
          "type": "blob",
          "size": 6445
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/interactive-component.tsx",
          "type": "blob",
          "size": 2215
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/react/island-component.tsx",
          "type": "blob",
          "size": 3261
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/tailwind",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/tailwind/global-styles.css",
          "type": "blob",
          "size": 609
        },
        {
          "path": "plugins/website-builder/skills/component-integration/templates/tailwind/tailwind.config.ts",
          "type": "blob",
          "size": 714
        },
        {
          "path": "plugins/website-builder/skills/content-collections",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/SKILL.md",
          "type": "blob",
          "size": 8164
        },
        {
          "path": "plugins/website-builder/skills/content-collections/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/examples/blog-with-tags-example.md",
          "type": "blob",
          "size": 1573
        },
        {
          "path": "plugins/website-builder/skills/content-collections/examples/documentation-site-example.md",
          "type": "blob",
          "size": 2617
        },
        {
          "path": "plugins/website-builder/skills/content-collections/examples/portfolio-projects-example.md",
          "type": "blob",
          "size": 2141
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts/generate-types.sh",
          "type": "blob",
          "size": 2132
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts/query-builder.sh",
          "type": "blob",
          "size": 6112
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts/setup-content-collections.sh",
          "type": "blob",
          "size": 3683
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts/test-collections.sh",
          "type": "blob",
          "size": 5170
        },
        {
          "path": "plugins/website-builder/skills/content-collections/scripts/validate-frontmatter.sh",
          "type": "blob",
          "size": 3445
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/python/collection-schema.py",
          "type": "blob",
          "size": 8536
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/python/query-patterns.py",
          "type": "blob",
          "size": 9921
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/queries",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/queries/advanced-queries.ts",
          "type": "blob",
          "size": 6877
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/queries/basic-queries.ts",
          "type": "blob",
          "size": 2986
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/queries/filtered-queries.ts",
          "type": "blob",
          "size": 6555
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/schemas/author-collection-schema.ts",
          "type": "blob",
          "size": 1699
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/schemas/blog-collection-schema.ts",
          "type": "blob",
          "size": 2451
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/schemas/docs-collection-schema.ts",
          "type": "blob",
          "size": 3206
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/schemas/product-collection-schema.ts",
          "type": "blob",
          "size": 3028
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/typescript",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/typescript/collection-helpers.ts",
          "type": "blob",
          "size": 1108
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/typescript/content-types.ts",
          "type": "blob",
          "size": 600
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/validation/frontmatter-validator.js",
          "type": "blob",
          "size": 1273
        },
        {
          "path": "plugins/website-builder/skills/content-collections/templates/validation/schema-validator.ts",
          "type": "blob",
          "size": 1558
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/SKILL.md",
          "type": "blob",
          "size": 7179
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/examples/blog-with-cms-example.md",
          "type": "blob",
          "size": 1826
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/examples/media-management-example.md",
          "type": "blob",
          "size": 15319
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/examples/multi-tenant-cms-example.md",
          "type": "blob",
          "size": 9073
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/examples/realtime-content-updates-example.md",
          "type": "blob",
          "size": 12770
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts/apply-migration.sh",
          "type": "blob",
          "size": 1493
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts/apply-rls-policies.sh",
          "type": "blob",
          "size": 1410
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts/generate-supabase-types.sh",
          "type": "blob",
          "size": 1271
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts/setup-supabase-cms.sh",
          "type": "blob",
          "size": 2986
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/scripts/test-rls-policies.sh",
          "type": "blob",
          "size": 4537
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/client",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/client/supabase-client.ts",
          "type": "blob",
          "size": 1013
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/queries",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/queries/content-queries.ts",
          "type": "blob",
          "size": 1583
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/queries/paginated-queries.ts",
          "type": "blob",
          "size": 1277
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/realtime",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/realtime/content-subscription.ts",
          "type": "blob",
          "size": 1767
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/rls",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/rls/basic-policies.sql",
          "type": "blob",
          "size": 616
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/rls/draft-publish-policies.sql",
          "type": "blob",
          "size": 898
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/rls/multi-tenant-policies.sql",
          "type": "blob",
          "size": 866
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/schemas",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/schemas/blog-schema.sql",
          "type": "blob",
          "size": 1346
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/schemas/media-schema.sql",
          "type": "blob",
          "size": 697
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/schemas/pages-schema.sql",
          "type": "blob",
          "size": 727
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/website-builder/skills/supabase-cms/templates/workflows/draft-publish.ts",
          "type": "blob",
          "size": 1704
        },
        {
          "path": "scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "scripts/README.md",
          "type": "blob",
          "size": 5543
        },
        {
          "path": "scripts/add-architecture-docs.py",
          "type": "blob",
          "size": 7218
        },
        {
          "path": "scripts/add-architecture-docs.sh",
          "type": "blob",
          "size": 6552
        },
        {
          "path": "scripts/add-skill-instructions.sh",
          "type": "blob",
          "size": 6033
        },
        {
          "path": "scripts/add-skill-tool.sh",
          "type": "blob",
          "size": 3313
        },
        {
          "path": "scripts/fix-marketplace-sync.sh",
          "type": "blob",
          "size": 839
        },
        {
          "path": "scripts/generate-mcp-instructions.sh",
          "type": "blob",
          "size": 3294
        },
        {
          "path": "scripts/generate-mcp-permissions.sh",
          "type": "blob",
          "size": 1196
        },
        {
          "path": "scripts/marketplace-validator.py",
          "type": "blob",
          "size": 3530
        },
        {
          "path": "scripts/mcp-instructions-review.txt",
          "type": "blob",
          "size": 33320
        },
        {
          "path": "scripts/remove-tools-from-agents.sh",
          "type": "blob",
          "size": 1025
        },
        {
          "path": "scripts/sanitize-env-for-docs.sh",
          "type": "blob",
          "size": 2088
        },
        {
          "path": "scripts/update-agent-colors-standard.sh",
          "type": "blob",
          "size": 11432
        },
        {
          "path": "scripts/update-agent-colors.sh",
          "type": "blob",
          "size": 3961
        },
        {
          "path": "scripts/validate-marketplace-sync.sh",
          "type": "blob",
          "size": 976
        },
        {
          "path": "temp-migrations",
          "type": "tree",
          "size": null
        },
        {
          "path": "temp-migrations/20251103_000000_initial_schema.sql",
          "type": "blob",
          "size": 58314
        }
      ],
      "marketplace": {
        "name": "ai-dev-marketplace",
        "version": "1.0.0",
        "description": "AI Development Marketplace - Master repository for tech-specific plugins (SDKs, frameworks, platforms)",
        "owner_info": {
          "name": "AI Development Team",
          "email": "noreply@ai-dev-marketplace.dev"
        },
        "keywords": [],
        "plugins": [
          {
            "name": "claude-agent-sdk",
            "description": "Complete Claude Agent SDK plugin for building AI agents",
            "source": "./plugins/claude-agent-sdk",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "dev@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install claude-agent-sdk@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-cost-tracking",
                "description": "Add cost and usage tracking to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-cost-tracking.md",
                "frontmatter": {
                  "description": "Add cost and usage tracking to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add cost tracking and usage monitoring to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for cost tracking patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK cost tracking documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if cost tracking is already implemented\n- Identify query() function calls\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design cost tracking implementation\n\nActions:\n- Determine what metrics to track (tokens, costs, requests)\n- Plan storage strategy for usage data\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add cost tracking with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add cost tracking.\n\nThe agent should:\n- Fetch cost tracking documentation: https://docs.claude.com/en/api/agent-sdk/cost-tracking\n- Implement usage tracking in query() calls\n- Add cost calculation logic\n- Implement usage data storage\n- Add reporting and analytics functions\n\nProvide the agent with:\n- Context: Project language and structure\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with cost tracking\n\nPhase 5: Review\nGoal: Verify cost tracking works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that cost tracking patterns match SDK documentation\n- Verify usage data is captured properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize cost tracking capabilities added\n- Show example usage data access\n- Link to SDK cost tracking documentation\n- Suggest testing with usage monitoring"
              },
              {
                "name": "/add-custom-tools",
                "description": "Add custom tools to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-custom-tools.md",
                "frontmatter": {
                  "description": "Add custom tools to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add custom tool definitions to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for custom tool patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK custom tools documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user what custom tools to add\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if custom tools are already defined\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design custom tools\n\nActions:\n- Define tool schemas and parameters\n- Plan tool implementation functions\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add custom tools with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add custom tools.\n\nThe agent should:\n- Fetch custom tools documentation: https://docs.claude.com/en/api/agent-sdk/custom-tools\n- Create tool definitions with proper schemas\n- Implement tool() function calls\n- Add tool permissions configuration\n- Add error handling for tool execution\n\nProvide the agent with:\n- Context: Project language, structure, and desired tools\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with custom tool definitions\n\nPhase 5: Review\nGoal: Verify custom tools work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that tool schemas are valid\n- Verify tools can be invoked properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize custom tools added\n- Show example tool usage\n- Link to SDK custom tools documentation\n- Suggest testing with tool calls"
              },
              {
                "name": "/add-hosting",
                "description": "Add hosting and deployment setup to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-hosting.md",
                "frontmatter": {
                  "description": "Add hosting and deployment setup to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add hosting and deployment configuration to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for hosting patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK hosting documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user about hosting target (local, cloud, serverless)\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if hosting configuration already exists\n- Identify server setup or entry points\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design hosting implementation\n\nActions:\n- Determine hosting platform (Express, FastAPI, serverless, etc.)\n- Plan server configuration strategy\n- Identify files to modify or create\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add hosting with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add hosting.\n\nThe agent should:\n- Fetch hosting documentation: https://docs.claude.com/en/api/agent-sdk/hosting\n- Set up server framework if needed\n- Configure endpoint routing\n- Add environment variable handling\n- Implement proper error handling for hosting\n- Add CORS and security configurations\n\nProvide the agent with:\n- Context: Project language, structure, and hosting target\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with hosting setup\n\nPhase 5: Review\nGoal: Verify hosting works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that hosting patterns match SDK documentation\n- Verify server can start and handle requests\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize hosting capabilities added\n- Show example deployment commands\n- Link to SDK hosting documentation\n- Suggest testing with local server startup"
              },
              {
                "name": "/add-mcp-code-execution",
                "description": "Add code execution with MCP pattern for 98.7% token reduction - generates typed tool wrappers and enables efficient tool usage via code",
                "path": "plugins/claude-agent-sdk/commands/add-mcp-code-execution.md",
                "frontmatter": {
                  "description": "Add code execution with MCP pattern for 98.7% token reduction - generates typed tool wrappers and enables efficient tool usage via code",
                  "argument-hint": [
                    "mcp-server-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\n**Reference**: https://www.anthropic.com/engineering/code-execution-with-mcp\n\nGoal: Implement code execution with MCP pattern in Claude Agent SDK application - transforms MCP servers into code APIs for 98.7% token reduction through progressive disclosure, context-efficient filtering, and familiar programming patterns.\n\nCore Principles:\n- Present MCP servers as filesystem APIs (servers/ directory tree)\n- Generate typed wrappers for on-demand tool loading\n- Filter data in execution environment before returning to model\n- Use familiar programming patterns for control flow\n- Enable state persistence across executions\n\nPhase 1: Discovery\nGoal: Understand project structure and MCP configuration\n\nActions:\n- Check if this is a Claude Agent SDK project: !{bash test -f package.json -o -f main.py && echo \"FOUND\" || echo \"NOT_FOUND\"}\n- Parse $ARGUMENTS for MCP server name (optional - if not provided, will prompt)\n- Detect language (TypeScript or Python): !{bash test -f package.json && echo \"TypeScript\" || echo \"Python\"}\n- Check for existing MCP configuration: !{bash test -f .mcp.json && cat .mcp.json || echo \"No .mcp.json found\"}\n- If no server specified in $ARGUMENTS, use AskUserQuestion to gather:\n  - Which MCP server(s) to generate code wrappers for?\n  - Use filesystem pattern (recommended) or custom structure?\n  - Include automatic PII tokenization?\n\nPhase 2: Generate MCP Tool Wrappers\nGoal: Create typed code APIs for MCP server tools in servers/ directory\n\nActions:\n\nCreate servers/ directory structure:\n!{bash mkdir -p servers/$MCP_SERVER_NAME}\n\n**For TypeScript Projects:**\n\nGenerate typed wrapper files for each tool:\n- Create servers/$MCP_SERVER_NAME/toolName.ts with:\n  - Typed input/output interfaces\n  - Async function calling callMCPTool helper\n  - JSDoc comments from MCP tool schema\n- Create servers/$MCP_SERVER_NAME/index.ts that exports all tools\n\n**For Python Projects:**\n\nGenerate typed wrapper files for each tool:\n- Create servers/$MCP_SERVER_NAME/tool_name.py with:\n  - Type hints using TypedDict\n  - Async function calling call_mcp_tool helper\n  - Docstrings from MCP tool schema\n- Create servers/$MCP_SERVER_NAME/__init__.py that exports all tools\n\nPhase 3: Create Helper Functions\nGoal: Implement core utilities for MCP tool execution\n\nActions:\n\n**For TypeScript:**\nCreate lib/mcp-helpers.ts with:\n- callMCPTool<T> generic function\n- Handles MCP tool invocation via SDK\n- Serialization/deserialization\n- Error handling and logging\n\n**For Python:**\nCreate lib/mcp_helpers.py with:\n- call_mcp_tool async function\n- Handles MCP tool invocation via SDK\n- Type conversion\n- Error handling and logging\n\nOptional: Create search_tools helper for keyword-based tool discovery\n\nPhase 4: Update Agent Configuration\nGoal: Enable code execution environment and tool access\n\nActions:\n\nUpdate agent configuration to:\n- Provide filesystem access to servers/ directory for tool discovery\n- Enable code execution with proper sandboxing\n- Add resource limits (memory, CPU, timeout) for safety\n- Configure PII tokenization if requested\n- Add workspace/ directory for persistent state\n\n**TypeScript:**\n- Update package.json with execution dependencies\n- Configure TypeScript for module resolution\n- Add filesystem permissions\n\n**Python:**\n- Update requirements.txt with execution dependencies\n- Configure virtual environment\n- Add filesystem permissions\n\nPhase 5: Create Example Usage\nGoal: Demonstrate code execution pattern\n\nActions:\n\nCreate example file showing:\n- Progressive disclosure: List servers/, load tools on-demand\n- Context-efficient filtering: Process data in code before returning\n- Control flow: Loops, conditionals, error handling\n- State persistence: Save to workspace/ for reuse\n\nPhase 6: Validation & Documentation\nGoal: Verify implementation and document usage\n\nActions:\n\nTest generated code:\n- Import tool wrappers and verify functionality\n- Test tool discovery and filtering\n\nAdd README section:\n- Explain code execution pattern (98.7% token reduction)\n- Show tool discovery (list servers/)\n- Document filtering and control flow patterns\n- Link to https://www.anthropic.com/engineering/code-execution-with-mcp\n\nPhase 7: Summary\nGoal: Present results and usage instructions\n\nActions:\n\nDisplay comprehensive summary:\n\n**Code Execution with MCP Added**: $MCP_SERVER_NAME\n\n**Files Created**:\n- servers/$MCP_SERVER_NAME/*.ts|py - Typed tool wrappers\n- lib/mcp-helpers.ts|py - Core execution utilities\n- examples/code-execution-demo - Usage examples\n- workspace/ - Persistent state directory\n\n**Benefits**:\n- 98.7% reduction in token usage vs direct tool calls\n- On-demand tool loading via filesystem navigation\n- Context-efficient result filtering in code\n- Familiar programming patterns for control flow\n- State persistence across executions\n- Automatic PII tokenization (if enabled)\n\n**Usage**:\n\nDiscover tools:\nList servers/ directory: `ls servers/`\n\nCall tools via code:\nImport and call like regular async functions\n\nFilter results:\nProcess data in code before returning to model\n\n**Next Steps**:\n- Add more MCP servers to servers/ directory\n- Create reusable skills in workspace/\n- Implement complex workflows with control flow\n- Review: https://www.anthropic.com/engineering/code-execution-with-mcp"
              },
              {
                "name": "/add-mcp",
                "description": "Add MCP integration to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-mcp.md",
                "frontmatter": {
                  "description": "Add MCP integration to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add Model Context Protocol (MCP) integration to an existing Claude Agent SDK application\n\nSupports:\n- Local STDIO MCP servers (e.g., Google Workspace MCPs running locally)\n- Remote HTTP MCP servers (e.g., FastMCP Cloud hosted servers)\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for MCP patterns\n- Follow official SDK examples\n- Configure appropriate transport (STDIO for local, HTTP for remote)\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK MCP documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if MCP is already configured\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n- Ask user:\n  1. Which MCP servers to integrate (name/purpose)\n  2. Server type: Local STDIO or Remote HTTP/FastMCP Cloud\n  3. If local: Path to MCP server directory\n  4. If remote: FastMCP Cloud URL and whether API key is needed\n\nPhase 3: Planning\nGoal: Design MCP integration\n\nActions:\n- Determine which MCP servers to add\n- Plan MCP server configuration\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add MCP integration with agent\n\nActions:\n\nFOR LOCAL STDIO MCP SERVERS:\nConfigure with STDIO transport (e.g., Google Workspace MCPs):\n\n```typescript\n// TypeScript example\nmcp_servers: {\n  \"google-tasks\": {\n    type: \"stdio\",\n    command: \"node\",\n    args: [\"/path/to/google-tasks/build/index.js\"],\n    env: {\n      GOOGLE_APPLICATION_CREDENTIALS: process.env.GOOGLE_APPLICATION_CREDENTIALS\n    }\n  }\n}\n```\n\n```python\n# Python example\nmcp_servers={\n    \"google-tasks\": {\n        \"type\": \"stdio\",\n        \"command\": \"node\",\n        \"args\": [\"/path/to/google-tasks/build/index.js\"],\n        \"env\": {\n            \"GOOGLE_APPLICATION_CREDENTIALS\": os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n        }\n    }\n}\n```\n\nFOR REMOTE HTTP/FASTMCP CLOUD SERVERS:\nINVOKE the fastmcp-integration skill to load HTTP patterns:\n\n!{skill fastmcp-integration}\n\nThis loads:\n- Complete FastMCP Cloud HTTP configuration patterns\n- Environment variable setup\n- Error handling for connection failures\n- Real-world examples with status checking\n- Common pitfalls (SSE vs HTTP, missing API keys)\n\nConfigure with HTTP transport:\n\n```typescript\n// TypeScript example\nmcp_servers: {\n  \"your-server\": {\n    type: \"http\",\n    url: \"https://your-server.fastmcp.app/mcp\",\n    headers: {\n      Authorization: `Bearer ${process.env.FASTMCP_CLOUD_API_KEY}`\n    }\n  }\n}\n```\n\nThen invoke the claude-agent-features agent to add MCP.\n\nThe agent should:\n- For LOCAL: Configure STDIO transport with command/args/env\n- For REMOTE: Use patterns from fastmcp-integration skill with HTTP transport\n- Add MCP tool permissions (allowed_tools)\n- Implement createSdkMcpServer() if creating custom MCP servers\n- Add proper error handling for MCP connections\n- For STDIO: Add any required env vars to .env/.env.example\n- For HTTP: Add FASTMCP_CLOUD_API_KEY to .env/.env.example\n\nProvide the agent with:\n- Context: Project language, structure, MCP server type (STDIO/HTTP), and paths/URLs\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with appropriate MCP transport configuration\n\nPhase 5: Review\nGoal: Verify MCP works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that MCP patterns match SDK documentation\n- Verify MCP servers connect properly:\n  - For STDIO: Verify command paths exist and are executable\n  - For HTTP: Verify URL is accessible and API key is configured\n- Test MCP tool calls work (list available tools from server)\n- Check error handling for connection failures\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize MCP capabilities added:\n  - Server name and type (STDIO/HTTP)\n  - Available tools from the MCP server\n  - Configuration location in code\n- Show example usage for calling MCP tools\n- Provide configuration details:\n  - For STDIO: Command path, args, required env vars\n  - For HTTP: URL, API key setup, headers\n- Link to SDK MCP documentation\n- Suggest testing with MCP tool calls\n- Document any troubleshooting steps for connection issues"
              },
              {
                "name": "/add-memory",
                "description": "Add Claude Memory Tool integration to Claude Agent SDK application for persistent memory across sessions and query caching",
                "path": "plugins/claude-agent-sdk/commands/add-memory.md",
                "frontmatter": {
                  "description": "Add Claude Memory Tool integration to Claude Agent SDK application for persistent memory across sessions and query caching",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add Claude Memory Tool integration to enable persistent memory storage across agent sessions\n\nThe Memory Tool enables Claude to:\n- Store and retrieve information in `/memories` directory\n- Remember user preferences, patterns, and context\n- Build knowledge over time without keeping everything in context\n- Cache search queries and generated code patterns\n- Maintain session state across restarts\n\nCore Principles:\n- Memory operations are file-based (client-controlled)\n- Claude automatically checks memory before tasks\n- Memory survives context window resets\n- You control storage location and security\n\n## Phase 1: Discovery\n\nGoal: Gather context about the project\n\nActions:\n- Check if project path provided in $ARGUMENTS\n- Read package.json (TypeScript) or requirements.txt (Python) to confirm SDK is installed\n- Identify main application files\n- Check current model version (Memory Tool requires Claude Sonnet 4.5+)\n- Verify if `/memories` directory already exists\n\n## Phase 2: Analysis\n\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if Memory Tool is already configured\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n- Check if beta features are enabled\n- Ask user:\n  1. What type of memory to store (user preferences, search patterns, learned context)?\n  2. Memory directory location (default: ./memories)\n  3. What information should persist across sessions?\n  4. Any sensitive data to exclude from memory?\n\n## Phase 3: Planning\n\nGoal: Design Memory Tool integration\n\nActions:\n- Plan memory directory structure\n- Identify memory operations needed (view, create, update, delete)\n- Determine what types of information to store:\n  - User preferences\n  - Search query patterns\n  - Generated code caching\n  - Session context\n  - Learned patterns\n- Plan files to modify\n- Present plan to user for confirmation\n\n## Phase 4: Implementation\n\nGoal: Add Memory Tool with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add Memory Tool integration.\n\nProvide the agent with:\n- Project path from $ARGUMENTS\n- Language detected (TypeScript or Python)\n- Memory directory location\n- Types of information to persist\n- User answers from Phase 2\n\nThe agent should:\n- Enable Memory Tool beta feature (`context-management-2025-06-27`)\n- Configure memory directory (default: ./memories)\n- Implement memory handlers (view, create, update, delete)\n- Add system prompt explaining memory usage to Claude\n- Create initial memory structure:\n  - user_preferences.json\n  - search_patterns.json\n  - session_context.json\n- Add security (.gitignore for memories/, path validation)\n- Upgrade to Sonnet 4.5+ if needed\n- Configure appropriate betaMemoryTool (TypeScript) or BetaAbstractMemoryTool (Python)\n\n## Phase 5: Review\n\nGoal: Verify Memory Tool works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Verify beta header is enabled\n- Check model is Sonnet 4.5+\n- Test memory operations:\n  - Create a memory file\n  - Read memory file\n  - Update memory file\n  - Delete memory file\n- Verify memory persists across sessions (restart and check)\n- Check security (memories/ in .gitignore, path validation)\n- Test with query caching use case\n\n## Phase 6: Summary\n\nGoal: Document what was added\n\nActions:\n- Summarize Memory Tool capabilities:\n  - Persistent storage location (./memories)\n  - Memory operations available (view, create, update, delete)\n  - Default memory files created\n  - Security measures implemented\n- Show example: Claude automatically checks /memories/search_patterns.json before generating code, reuses cached patterns if available, updates usage stats\n- Explain memory structure:\n  - `user_preferences.json` - User settings\n  - `search_patterns.json` - Cached queries\n  - `session_context.json` - Session state\n  - `learned_patterns/` - Learning over time\n- Document benefits:\n  - Persistent memory across sessions\n  - Automatic query caching\n  - Context survives restarts\n  - Reduced token costs\n- Provide testing steps:\n  1. Run agent with a search query\n  2. Check memories/ directory for created files\n  3. Run same query again - verify cache hit\n  4. Restart application - verify memory persists\n- Link to Memory Tool documentation:\n  https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool"
              },
              {
                "name": "/add-permissions",
                "description": "Add permission handling to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-permissions.md",
                "frontmatter": {
                  "description": "Add permission handling to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add permission handling capabilities to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for permission patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK permissions documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if permissions are already configured\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design permission implementation\n\nActions:\n- Determine which tools need permission controls\n- Plan permission configuration strategy\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add permissions with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add permissions.\n\nThe agent should:\n- Fetch permissions documentation: https://docs.claude.com/en/api/agent-sdk/permissions\n- Configure tool permission levels\n- Add askBeforeToolUse configuration\n- Implement permission callbacks if needed\n- Add proper permission error handling\n\nProvide the agent with:\n- Context: Project language and structure\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with permission handling\n\nPhase 5: Review\nGoal: Verify permissions work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that permission patterns match SDK documentation\n- Verify permission controls are properly enforced\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize permission capabilities added\n- Show example usage\n- Link to SDK permissions documentation\n- Suggest testing with restricted tool access"
              },
              {
                "name": "/add-plugins",
                "description": "Add plugin system to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-plugins.md",
                "frontmatter": {
                  "description": "Add plugin system to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add plugin system and plugin definitions to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for plugin patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK plugins documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user what plugins to add\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if plugins are already configured\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design plugin system\n\nActions:\n- Define plugin structure and capabilities\n- Plan plugin loading mechanism\n- Determine plugin registration patterns\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add plugins with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add plugins.\n\nThe agent should:\n- Fetch plugins documentation: https://docs.claude.com/en/api/agent-sdk/plugins\n- Create plugin definitions\n- Implement plugin loading system\n- Add plugin registration in query() calls\n- Add proper error handling for plugins\n\nProvide the agent with:\n- Context: Project language, structure, and desired plugins\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with plugin system\n\nPhase 5: Review\nGoal: Verify plugins work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that plugin patterns match SDK documentation\n- Verify plugins can be loaded and used properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize plugin capabilities added\n- Show example plugin usage\n- Link to SDK plugins documentation\n- Suggest testing with plugin loading"
              },
              {
                "name": "/add-sessions",
                "description": "Add session management to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-sessions.md",
                "frontmatter": {
                  "description": "Add session management to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add session management capabilities to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for session patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK session documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if sessions are already implemented\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design session implementation\n\nActions:\n- Determine session storage approach\n- Plan state management strategy\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add session management with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add sessions.\n\nThe agent should:\n- Fetch session documentation: https://docs.claude.com/en/api/agent-sdk/sessions\n- Implement session state management\n- Add session persistence if needed\n- Configure session options in query() calls\n- Add session cleanup handling\n\nProvide the agent with:\n- Context: Project language and structure\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with session support\n\nPhase 5: Review\nGoal: Verify sessions work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that session patterns match SDK documentation\n- Verify state persistence works\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize session capabilities added\n- Show example usage\n- Link to SDK session documentation\n- Suggest testing with multi-turn conversations"
              },
              {
                "name": "/add-skills",
                "description": "Add skills to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-skills.md",
                "frontmatter": {
                  "description": "Add skills to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add skill definitions to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for skill patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK skills documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user what skills to add\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if skills are already defined\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design skills\n\nActions:\n- Define skill names and capabilities\n- Plan skill implementation logic\n- Determine skill invocation patterns\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add skills with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add skills.\n\nThe agent should:\n- Fetch skills documentation: https://docs.claude.com/en/api/agent-sdk/skills\n- Create skill definitions\n- Implement skill handler functions\n- Add skill registration in query() calls\n- Add proper error handling for skills\n\nProvide the agent with:\n- Context: Project language, structure, and desired skills\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with skill definitions\n\nPhase 5: Review\nGoal: Verify skills work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that skill patterns match SDK documentation\n- Verify skills can be invoked properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize skills added\n- Show example skill usage\n- Link to SDK skills documentation\n- Suggest testing with skill invocations"
              },
              {
                "name": "/add-slash-commands",
                "description": "Add slash commands to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-slash-commands.md",
                "frontmatter": {
                  "description": "Add slash commands to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add slash command definitions to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for slash command patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK slash commands documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user what slash commands to add\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if slash commands are already defined\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design slash commands\n\nActions:\n- Define slash command names and purposes\n- Plan command handlers and logic\n- Determine command arguments if needed\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add slash commands with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add slash commands.\n\nThe agent should:\n- Fetch slash commands documentation: https://docs.claude.com/en/api/agent-sdk/slash-commands\n- Create slash command definitions\n- Implement command handler functions\n- Add command registration in query() calls\n- Add proper error handling for commands\n\nProvide the agent with:\n- Context: Project language, structure, and desired commands\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with slash command definitions\n\nPhase 5: Review\nGoal: Verify slash commands work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that slash command patterns match SDK documentation\n- Verify commands can be invoked properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize slash commands added\n- Show example command usage\n- Link to SDK slash commands documentation\n- Suggest testing with command invocations"
              },
              {
                "name": "/add-streaming",
                "description": "Add streaming capabilities to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-streaming.md",
                "frontmatter": {
                  "description": "Add streaming capabilities to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add streaming vs single-mode capabilities to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for streaming patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK streaming documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- If not provided, use current directory\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if streaming is already implemented\n- Identify query() function calls\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design streaming implementation\n\nActions:\n- Determine which streaming mode to add\n- Plan code changes needed\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add streaming capability with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add streaming.\n\nThe agent should:\n- Fetch streaming documentation: https://docs.claude.com/en/api/agent-sdk/streaming-vs-single-mode\n- Update query() calls to support streaming\n- Add proper async iteration handling\n- Implement streaming response processing\n- Add error handling for stream interruptions\n\nProvide the agent with:\n- Context: Project language and structure\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with streaming support\n\nPhase 5: Review\nGoal: Verify streaming works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that streaming patterns match SDK documentation\n- Verify async iteration is properly implemented\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize streaming capabilities added\n- Show example usage\n- Link to SDK streaming documentation\n- Suggest testing with actual prompts"
              },
              {
                "name": "/add-subagents",
                "description": "Add subagents to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-subagents.md",
                "frontmatter": {
                  "description": "Add subagents to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add subagent definitions to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for subagent patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK subagents documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user what subagents to add\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if subagents are already defined\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design subagents\n\nActions:\n- Define subagent roles and responsibilities\n- Plan subagent system prompts\n- Determine subagent tool permissions\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add subagents with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add subagents.\n\nThe agent should:\n- Fetch subagents documentation: https://docs.claude.com/en/api/agent-sdk/subagents\n- Create subagent definitions with names and descriptions\n- Configure subagent system prompts\n- Set subagent tool permissions\n- Add subagent invocation handling\n\nProvide the agent with:\n- Context: Project language, structure, and desired subagents\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with subagent definitions\n\nPhase 5: Review\nGoal: Verify subagents work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that subagent definitions are valid\n- Verify subagents can be invoked properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize subagents added\n- Show example subagent usage\n- Link to SDK subagents documentation\n- Suggest testing with subagent delegation"
              },
              {
                "name": "/add-system-prompts",
                "description": "Add system prompts configuration to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-system-prompts.md",
                "frontmatter": {
                  "description": "Add system prompts configuration to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add system prompts and agent behavior configuration to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for system prompt patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK system prompts documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n- Ask user about agent behavior requirements\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if system prompts are already configured\n- Identify query() function calls\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design system prompts implementation\n\nActions:\n- Define agent personality and behavior\n- Plan system prompt content\n- Determine if multiple prompt templates needed\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add system prompts with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add system prompts.\n\nThe agent should:\n- Fetch system prompts documentation: https://docs.claude.com/en/api/agent-sdk/system-prompts\n- Configure system prompt in query() calls\n- Add dynamic prompt generation if needed\n- Implement prompt templating\n- Add context injection capabilities\n\nProvide the agent with:\n- Context: Project language, structure, and behavior requirements\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with system prompt configuration\n\nPhase 5: Review\nGoal: Verify system prompts work correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that system prompt patterns match SDK documentation\n- Verify prompts are properly applied\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize system prompt capabilities added\n- Show example prompt configurations\n- Link to SDK system prompts documentation\n- Suggest testing with different prompt variations"
              },
              {
                "name": "/add-todo-tracking",
                "description": "Add todo list tracking to Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/add-todo-tracking.md",
                "frontmatter": {
                  "description": "Add todo list tracking to Claude Agent SDK application",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add todo list tracking and task management to an existing Claude Agent SDK application\n\nCore Principles:\n- Understand existing code before modifying\n- Load SDK documentation for todo tracking patterns\n- Follow official SDK examples\n\nPhase 1: Discovery\nGoal: Gather context about the project\n\nActions:\n- Load SDK todo tracking documentation:\n  Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check if project path provided in $ARGUMENTS\n- Read package.json or requirements.txt to confirm SDK is installed\n- Identify main application files\n\nPhase 2: Analysis\nGoal: Understand current implementation\n\nActions:\n- Read main application files\n- Check if todo tracking is already implemented\n- Identify query() function configuration\n- Determine language (TypeScript or Python)\n\nPhase 3: Planning\nGoal: Design todo tracking implementation\n\nActions:\n- Determine todo list structure and schema\n- Plan task management functions\n- Plan persistence strategy\n- Identify files to modify\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add todo tracking with agent\n\nActions:\n\nInvoke the claude-agent-features agent to add todo tracking.\n\nThe agent should:\n- Fetch todo tracking documentation: https://docs.claude.com/en/api/agent-sdk/todo-tracking\n- Implement todo list management\n- Add task creation and update functions\n- Configure todo persistence\n- Add todo query and filtering capabilities\n\nProvide the agent with:\n- Context: Project language and structure\n- Target: $ARGUMENTS (project path)\n- Expected output: Updated files with todo tracking\n\nPhase 5: Review\nGoal: Verify todo tracking works correctly\n\nActions:\n- Invoke appropriate verifier agent:\n  - TypeScript: claude-agent-verifier-ts\n  - Python: claude-agent-verifier-py\n- Check that todo tracking patterns match SDK documentation\n- Verify todo operations work properly\n\nPhase 6: Summary\nGoal: Document what was added\n\nActions:\n- Summarize todo tracking capabilities added\n- Show example todo list usage\n- Link to SDK todo tracking documentation\n- Suggest testing with task management operations"
              },
              {
                "name": "/new-app",
                "description": "Create and setup a new Claude Agent SDK application",
                "path": "plugins/claude-agent-sdk/commands/new-app.md",
                "frontmatter": {
                  "description": "Create and setup a new Claude Agent SDK application",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n\n!{skill skill-name}\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create and setup a new Claude Agent SDK application with TypeScript or Python support\n\nCore Principles:\n- Ask before acting - gather language preference and requirements first\n- Use latest SDK versions - check npm/PyPI for current releases\n- Verify code works - run type checking before completing\n\nPhase 1: Discovery\nGoal: Gather project requirements and preferences\n\nActions:\n- Parse $ARGUMENTS for project name (if provided)\n- Use AskUserQuestion to gather:\n  - Language preference: TypeScript or Python?\n  - Project name (if not in $ARGUMENTS)\n  - Agent type/purpose\n  - Starting point preference (minimal, basic, or specific example)\n  - Package manager preference (npm/yarn/pnpm for TS, pip/poetry for Python)\n\nPhase 2: Analysis\nGoal: Load SDK documentation and determine setup approach\n\nActions:\n- Read SDK documentation: ~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/claude-agent-sdk/docs/sdk-documentation.md\n- Check for latest SDK versions\n- Identify required dependencies based on language choice\n- Determine project structure needs\n\nPhase 3: Planning\nGoal: Design the project setup approach\n\nActions:\n- Outline project structure:\n  - TypeScript: package.json, tsconfig.json, src/index.ts, .env.example\n  - Python: requirements.txt, main.py, .env.example\n- Plan SDK installation command\n- Identify starter code to generate\n- Confirm approach with user\n\nPhase 4: Implementation\nGoal: Execute project setup with specialized agent\n\nActions:\n\nTask(description=\"Setup Claude Agent SDK project\", subagent_type=\"claude-agent-sdk:claude-agent-setup\", prompt=\"You are the claude-agent-setup agent. Create a new Claude Agent SDK project for $ARGUMENTS.\n\nContext from user discovery:\n- Language: [TypeScript or Python from Phase 1]\n- Project name: $ARGUMENTS\n- Agent type: [Purpose from Phase 1]\n- Package manager: [Preference from Phase 1]\n\nCreate the complete project:\n- Project directory structure\n- Initialize package manager (npm/pip)\n- Install Claude Agent SDK with latest version\n- Generate starter code with proper SDK usage\n- Create .env.example with API key placeholder\n- Add .gitignore for security\n- Create README.md with setup instructions\n\nExpected output: Fully initialized project ready to run\")\n\nPhase 5: Verification\nGoal: Validate the setup is correct\n\nActions:\n\nBased on language choice from Phase 1:\n\n**If TypeScript:**\n\nTask(description=\"Verify TypeScript setup\", subagent_type=\"claude-agent-sdk:claude-agent-verifier-ts\", prompt=\"Verify the TypeScript Claude Agent SDK setup at $ARGUMENTS.\n\nCheck:\n- Package.json has correct SDK dependency\n- tsconfig.json is properly configured\n- Starter code follows SDK patterns\n- .env.example exists with placeholders\n- .gitignore protects secrets\n\nReport any issues found.\")\n\nRun type checking:\n!{bash cd $ARGUMENTS && npx tsc --noEmit}\n\n**If Python:**\n\nTask(description=\"Verify Python setup\", subagent_type=\"claude-agent-sdk:claude-agent-verifier-py\", prompt=\"Verify the Python Claude Agent SDK setup at $ARGUMENTS.\n\nCheck:\n- requirements.txt has correct SDK package (claude-agent-sdk)\n- Starter code follows SDK patterns\n- .env.example exists with placeholders\n- .gitignore protects secrets\n- Virtual environment is set up\n\nReport any issues found.\")\n\nAddress any issues found before proceeding\n\nPhase 6: Summary\nGoal: Provide next steps to user\n\nActions:\n- Summarize what was created:\n  - Project structure\n  - SDK version installed\n  - Files generated\n- Provide instructions:\n  - How to set API key in .env\n  - How to run the agent\n  - Links to SDK documentation\n- Suggest next steps:\n  - Customize system prompt\n  - Add custom tools via MCP\n  - Create subagents\n- Point to examples:\n  - Basic usage: `examples/python/basic-query.py`\n  - FastMCP Cloud: `examples/python/fastmcp-cloud-http.py`\n- Common pitfalls to avoid:\n  - ‚úÖ Use `claude-agent-sdk` NOT `anthropic-agent-sdk`\n  - ‚úÖ Use `\"type\": \"http\"` for FastMCP Cloud, NOT `\"sse\"`\n  - ‚úÖ Pass API keys via `env` parameter in `ClaudeAgentOptions`"
              },
              {
                "name": "/test-skill-loading",
                "description": "Test if skills are properly loaded and used by agents",
                "path": "plugins/claude-agent-sdk/commands/test-skill-loading.md",
                "frontmatter": {
                  "description": "Test if skills are properly loaded and used by agents"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Purpose**: Test that the claude-agent-setup agent can properly invoke and use the fastmcp-integration skill\n\n## Available Skills\n\nThis commands has access to the following skills from the claude-agent-sdk plugin:\n\n- **fastmcp-integration**: Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport\n- **sdk-config-validator**: Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Test Workflow\n\n1. **Invoke the claude-agent-setup agent** with MCP requirements\n2. **Verify** the agent invokes the fastmcp-integration skill\n3. **Check** that skill content influences the agent's output\n\n## Test Command\n\nRun this test by asking:\n\n```\nCreate a new Python Agent SDK project called \"test-mcp-project\"\nthat will connect to FastMCP Cloud servers.\n```\n\n## Expected Behavior\n\nThe agent should:\n1. ‚úÖ Detect \"FastMCP Cloud servers\" in the request\n2. ‚úÖ Invoke the fastmcp-integration skill: `!{skill fastmcp-integration}`\n3. ‚úÖ Use skill knowledge to:\n   - Generate code with `\"type\": \"http\"` not `\"sse\"`\n   - Include FASTMCP_CLOUD_API_KEY in .env.example\n   - Reference correct examples\n   - Warn about common mistakes\n\n## Verification\n\nAfter agent completes, check:\n- Did the agent invoke the skill? (Look for skill invocation in output)\n- Does generated code have `\"type\": \"http\"`?\n- Does .env.example include FASTMCP_CLOUD_API_KEY?\n- Did agent mention the skill's warnings?\n\n## If Skill Not Invoked\n\nThe agent may not be detecting MCP requirements. Try being more explicit:\n\n```\nCreate a Python Agent SDK project that uses FastMCP Cloud MCP servers.\nI need help with the MCP configuration.\n```\n\nOr manually invoke the skill:\n\n```\n!{skill fastmcp-integration}\n```\n\nThen ask the agent to reference the loaded skill content."
              }
            ],
            "skills": [
              {
                "name": "fastmcp-integration",
                "description": "Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport",
                "path": "plugins/claude-agent-sdk/skills/fastmcp-integration/SKILL.md",
                "frontmatter": {
                  "name": "fastmcp-integration",
                  "description": "Examples and patterns for integrating FastMCP Cloud servers with Claude Agent SDK using HTTP transport"
                },
                "content": "# FastMCP Cloud Integration Skill\n\nThis skill provides examples and troubleshooting for FastMCP Cloud integration.\n\n## Critical Pattern: Use HTTP Transport\n\n**FastMCP Cloud uses HTTP, NOT SSE!**\n\n### ‚úÖ Correct Configuration\n\n**Example: Basic FastMCP Cloud Integration**\n\n```python\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\nfrom claude_agent_sdk import query\nfrom claude_agent_sdk.types import ClaudeAgentOptions\n\nload_dotenv()\n\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\nFASTMCP_CLOUD_API_KEY = os.getenv(\"FASTMCP_CLOUD_API_KEY\")\n\nasync def main():\n    async for message in query(\n        prompt=\"List available tools from the MCP server\",\n        options=ClaudeAgentOptions(\n            model=\"claude-sonnet-4-20250514\",\n\n            # ‚úÖ CRITICAL: Use HTTP for FastMCP Cloud\n            mcp_servers={\n                \"your-server\": {\n                    \"type\": \"http\",  # ‚Üê Must be \"http\" not \"sse\"\n                    \"url\": \"https://your-server.fastmcp.app/mcp\",\n                    \"headers\": {\n                        \"Authorization\": f\"Bearer {FASTMCP_CLOUD_API_KEY}\"\n                    }\n                }\n            },\n\n            # Allow MCP tools\n            allowed_tools=[\"mcp__your-server__*\"],\n\n            # Pass API keys via env\n            env={\n                \"ANTHROPIC_API_KEY\": ANTHROPIC_API_KEY,\n                \"FASTMCP_CLOUD_API_KEY\": FASTMCP_CLOUD_API_KEY\n            }\n        )\n    ):\n        if hasattr(message, 'type') and message.type == 'text':\n            print(message.text)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n**Example: Multiple FastMCP Cloud Servers**\n\n```python\nmcp_servers={\n    \"cats\": {\n        \"type\": \"http\",\n        \"url\": \"https://catsmcp.fastmcp.app/mcp\",\n        \"headers\": {\"Authorization\": f\"Bearer {FASTMCP_CLOUD_API_KEY}\"}\n    },\n    \"github\": {\n        \"type\": \"http\",\n        \"url\": \"https://github-mcp.fastmcp.app/mcp\",\n        \"headers\": {\"Authorization\": f\"Bearer {FASTMCP_CLOUD_API_KEY}\"}\n    }\n}\n\nallowed_tools=[\n    \"mcp__cats__*\",      # All CATS tools\n    \"mcp__github__*\",    # All GitHub tools\n]\n```\n\n**Example: Checking MCP Connection Status**\n\n```python\nasync for message in query(...):\n    if hasattr(message, 'type') and message.type == 'system':\n        if hasattr(message, 'data') and 'mcp_servers' in message.data:\n            for server in message.data['mcp_servers']:\n                status = server.get('status', 'unknown')\n                name = server.get('name', 'unknown')\n                print(f\"‚úÖ MCP Server '{name}': {status}\")\n\n                if status == 'failed':\n                    print(\"‚ùå Connection failed!\")\n                    print(\"   Check: 1) Using 'type': 'http'\")\n                    print(\"   Check: 2) FASTMCP_CLOUD_API_KEY is valid\")\n                    print(\"   Check: 3) URL is correct\")\n```\n\n### ‚ùå Common Mistakes\n\n**Wrong transport type:**\n```python\n\"type\": \"sse\"  # ‚ùå Doesn't work with FastMCP Cloud\n```\n\n**Missing API key:**\n```python\n# ‚ùå Not passing FASTMCP_CLOUD_API_KEY\nenv={\"ANTHROPIC_API_KEY\": ANTHROPIC_API_KEY}\n```\n\n**Wrong package:**\n```python\nfrom anthropic_agent_sdk import query  # ‚ùå Wrong!\n# Should be:\nfrom claude_agent_sdk import query  # ‚úÖ Correct\n```\n\n## Troubleshooting\n\n### Symptom: `'mcp_servers': [{'name': 'cats', 'status': 'failed'}]`\n\n**Causes:**\n1. Using `\"type\": \"sse\"` instead of `\"type\": \"http\"`\n2. Missing or invalid `FASTMCP_CLOUD_API_KEY`\n3. Wrong URL format\n\n**Fix:**\n- Change to `\"type\": \"http\"`\n- Verify API key is correct and passed in `env` parameter\n- Ensure URL is `https://your-server.fastmcp.app/mcp` (with `/mcp` endpoint)\n\n### Symptom: `ImportError: No module named 'anthropic_agent_sdk'`\n\n**Cause:** Wrong package name\n\n**Fix:**\n```bash\npip uninstall anthropic-agent-sdk  # Remove wrong package\npip install claude-agent-sdk       # Install correct package\n```\n\n## Complete Example\n\nSee `examples/python/fastmcp-cloud-http.py` for a full working example.\n\n## Environment Variables\n\nRequired in `.env`:\n```env\nANTHROPIC_API_KEY=sk-ant-api03-...\nFASTMCP_CLOUD_API_KEY=fmcp_...\n```\n\nMust be passed via `env` parameter:\n```python\nenv={\n    \"ANTHROPIC_API_KEY\": os.getenv(\"ANTHROPIC_API_KEY\"),\n    \"FASTMCP_CLOUD_API_KEY\": os.getenv(\"FASTMCP_CLOUD_API_KEY\")\n}\n```\n\n## What The Output Actually Looks Like\n\n### Example: Successful Connection\n\nWhen you run your agent, you'll see system messages like this:\n\n```python\n# System message with connection status\nSystemMessage(\n    subtype='init',\n    data={\n        'type': 'system',\n        'session_id': 'c8feee3e-bb62-4dcc-92bc-042b507e614a',\n        'mcp_servers': [{'name': 'cats', 'status': 'connected'}],  # ‚úÖ Connected!\n        'tools': ['mcp__cats__search_candidates', 'mcp__cats__get_candidate', ...],\n        'model': 'claude-sonnet-4-20250514',\n        ...\n    }\n)\n```\n\n**What this means:**\n- `'status': 'connected'` ‚úÖ - Your HTTP configuration worked!\n- `'tools': [...]` - All 163 CATS tools are now available\n- Agent can now use `mcp__cats__search_candidates`, etc.\n\n### Example: Failed Connection\n\nIf you use wrong transport type, you'll see:\n\n```python\nSystemMessage(\n    data={\n        'mcp_servers': [{'name': 'cats', 'status': 'failed'}],  # ‚ùå Failed!\n        'tools': ['Task', 'Bash', 'Read', ...],  # Only built-in tools, no MCP tools\n        ...\n    }\n)\n```\n\n**What this means:**\n- `'status': 'failed'` ‚ùå - Connection didn't work\n- No `mcp__cats__*` tools available\n- Common cause: Using `\"type\": \"sse\"` instead of `\"type\": \"http\"`\n\n### Example: Tool Call\n\nWhen Claude uses an MCP tool:\n\n```python\n# Claude decides to call search_candidates\nAssistantMessage(\n    content=[\n        ToolUseBlock(\n            id='toolu_01HhvXi5wyvVa2DWtbP8KvJw',\n            name='mcp__cats__search_candidates',\n            input={'search_string': 'heavy duty mechanic'}\n        )\n    ]\n)\n\n# Tool result comes back\nUserMessage(\n    content=[\n        ToolResultBlock(\n            tool_use_id='toolu_01HhvXi5wyvVa2DWtbP8KvJw',\n            content='{\"count\":2,\"total\":3540,\"_embedded\":{\"candidates\":[...]}}'\n        )\n    ]\n)\n\n# Claude responds with analysis\nAssistantMessage(\n    content=[\n        TextBlock(\n            text=\"I found 3,540 heavy duty mechanic candidates. Here are the first 2...\"\n        )\n    ]\n)\n```\n\n### Real Output From Working Demo\n\n```\n================================================================================\nCATS Multi-Tool Agent Demo - Claude Agent SDK\n================================================================================\n\nüîå MCP Server Status:\n--------------------------------------------------------------------------------\n‚úÖ cats: CONNECTED\n\nüì¶ Available CATS Tools: 163\n   - search_candidates\n   - get_candidate\n   - list_candidate_custom_fields\n   - list_candidate_attachments\n   - parse_resume\n   ... and 158 more\n\nüí¨ Claude:\n--------------------------------------------------------------------------------\nI'll search for heavy duty mechanics using the CATS database...\n\nüí¨ Claude:\n--------------------------------------------------------------------------------\nI found 3,540 heavy duty mechanic candidates in the system. Here are the\nfirst 2 results with their Red Seal certification status:\n\n1. **Sahlan Samsuddin**\n   - Email: sahlansamsuddin11@gmail.com\n   - Location: Mimika, Papua\n   - Red Seal Status: Not found in \"Notes on Qualifications\" field\n   - Tags: None\n\n2. **[Next candidate]**\n   ...\n```\n\n## Additional Examples\n\nSee the `examples/` directory in this skill:\n- `examples/multi-server.py` - Connecting to multiple FastMCP Cloud servers\n- `examples/connection-status.py` - Testing and troubleshooting connections\n- `@plugins/claude-agent-sdk/examples/python/complete-example-with-output.py` - Full example with output\n\n## Related Resources\n\n- Basic example: `@plugins/claude-agent-sdk/examples/python/basic-query.py`\n- FastMCP Cloud example: `@plugins/claude-agent-sdk/examples/python/fastmcp-cloud-http.py`\n- Examples README: `@plugins/claude-agent-sdk/examples/README.md`\n- Agent SDK Docs: `@plugins/claude-agent-sdk/docs/sdk-documentation.md`\n- FastMCP Cloud: https://fastmcp.com\n- MCP Protocol: https://modelcontextprotocol.io"
              },
              {
                "name": "sdk-config-validator",
                "description": "Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure",
                "path": "plugins/claude-agent-sdk/skills/sdk-config-validator/SKILL.md",
                "frontmatter": {
                  "name": "sdk-config-validator",
                  "description": "Validates Claude Agent SDK configuration files, environment setup, dependencies, and project structure",
                  "allowed-tools": "Bash, Read, Grep, Glob",
                  "category": "validation",
                  "complexity": "simple"
                },
                "content": "# SDK Configuration Validator\n\nValidates Claude Agent SDK project configuration, environment setup, and dependencies.\n\n## Use when...\n- User mentions \"validate SDK setup\", \"check SDK configuration\", or \"SDK not working\"\n- User reports SDK initialization errors or import failures\n- User asks \"is my SDK project configured correctly?\"\n- User requests \"troubleshoot SDK issues\" or \"debug SDK setup\"\n- Before starting SDK development work in a new project\n- After installing SDK dependencies to verify correctness\n\n## Capabilities\n- Validates TypeScript/Python SDK configuration files\n- Checks SDK version compatibility and dependencies\n- Verifies environment variable setup (.env files)\n- Validates project structure and required files\n- Generates validation reports with actionable fixes\n- Provides configuration templates for common setups\n\n## Usage\n\n### Basic Validation\n```bash\n# Validate TypeScript SDK setup\nbash scripts/validate-typescript.sh /path/to/project\n\n# Validate Python SDK setup\nbash scripts/validate-python.sh /path/to/project\n\n# Check SDK version compatibility\nbash scripts/check-sdk-version.sh /path/to/project\n```\n\n### Environment Validation\n```bash\n# Validate .env file setup\nbash scripts/validate-env-setup.sh /path/to/project\n```\n\n### Generate Templates\n```bash\n# Copy environment template\ncp templates/.env.example.template /path/to/project/.env.example\n\n# Copy TypeScript config\ncp templates/tsconfig-sdk.json /path/to/project/tsconfig.json\n\n# Copy Python config\ncp templates/pyproject-sdk.toml /path/to/project/pyproject.toml\n```\n\n## Validation Workflow\n\n1. **Detect Project Type**: Check for package.json (TS) or pyproject.toml (Python)\n2. **Run Configuration Validation**: Execute appropriate validation script\n3. **Check Dependencies**: Verify SDK package is installed with correct version\n4. **Validate Environment**: Check .env files for required variables\n5. **Generate Report**: Create validation report with findings and fixes\n6. **Apply Fixes**: Offer to apply recommended configuration changes\n\n## Common Issues Detected\n- Wrong package name (`anthropic-agent-sdk` instead of `claude-agent-sdk`) ‚ö†Ô∏è\n- Missing SDK dependency in package.json/requirements.txt\n- Incorrect TypeScript compiler options for SDK\n- Missing required environment variables (ANTHROPIC_API_KEY)\n- Missing FastMCP Cloud API key when using MCP servers\n- Wrong MCP transport type (`\"sse\"` instead of `\"http\"` for FastMCP Cloud) ‚ö†Ô∏è\n- SDK version incompatibility with Node/Python version\n- Missing configuration files (tsconfig.json, .env)\n- Incorrect module resolution settings\n- Missing async/await pattern in Python code\n\n## Exit Codes\n- 0: All validations passed\n- 1: Configuration errors found (see report)\n- 2: Critical errors (missing SDK, invalid structure)\n\n## Examples\nSee examples/ directory for sample validation reports and common fix patterns."
              }
            ]
          },
          {
            "name": "elevenlabs",
            "description": "Comprehensive ElevenLabs AI audio integration for voice-enabled applications with TTS, STT, voice cloning, and Vercel AI SDK support",
            "source": "./plugins/elevenlabs",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "plugins@ai-dev-marketplace.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install elevenlabs@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-advanced-features",
                "description": "Add sound effects generation, voice changer, dubbing, and voice isolator capabilities",
                "path": "plugins/elevenlabs/commands/add-advanced-features.md",
                "frontmatter": {
                  "description": "Add sound effects generation, voice changer, dubbing, and voice isolator capabilities",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add advanced audio capabilities including AI sound effects generation, voice transformation, multi-language dubbing, and background noise removal.\n\nCore Principles:\n- Support sound effects generation from text descriptions\n- Enable voice-to-voice transformation\n- Implement multi-language dubbing workflows\n- Provide voice isolation (noise removal)\n- Detect framework and adapt implementation\n\nPhase 1: Discovery\nGoal: Understand which advanced features are needed\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check setup: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null || pip show elevenlabs 2>/dev/null}\n- Detect framework: @package.json or @pyproject.toml\n- Parse $ARGUMENTS for feature selection\n\nPhase 2: Requirements Gathering\nGoal: Clarify feature scope\n\nActions:\n- Use AskUserQuestion if needed:\n  - Which features? (Sound effects, Voice changer, Dubbing, Voice isolator)\n  - Sound effects: text descriptions ‚Üí audio (cinematic quality)\n  - Voice changer: transform voice characteristics\n  - Dubbing: translate and dub audio to other languages\n  - Voice isolator: remove background noise from recordings\n  - Use cases for each selected feature\n\nPhase 3: Planning\nGoal: Design advanced features implementation\n\nActions:\n- For each selected feature, plan:\n  - Sound Effects: text prompt ‚Üí audio generation, duration control\n  - Voice Changer: source audio ‚Üí transformed voice, settings\n  - Dubbing: source audio + target language ‚Üí dubbed audio\n  - Voice Isolator: noisy audio ‚Üí clean voice output\n- File upload/processing requirements\n- API endpoints and parameters\n- Present plan\n\nPhase 4: Implementation\nGoal: Build advanced features\n\nActions:\n\nLaunch the general-purpose agent to implement advanced audio features.\n\nProvide detailed requirements:\n- Context: Framework, SDK status, selected features\n- Target: $ARGUMENTS\n- Requirements:\n  - Sound Effects Generation:\n    * POST /v1/sound-effects endpoint\n    * Text description input (e.g., \"cinematic thunder\")\n    * Duration parameter (optional)\n    * Audio format selection\n    * Preview and download generated audio\n  - Voice Changer:\n    * POST /v1/voice-changer endpoint\n    * Upload source audio\n    * Select target voice characteristics\n    * Transform voice while preserving speech\n    * Output transformed audio\n  - Dubbing:\n    * POST /v1/dubbing endpoint\n    * Upload source audio/video\n    * Select target language (70+ supported)\n    * Voice mapping for speakers\n    * Dubbed audio output with timing preserved\n  - Voice Isolator:\n    * POST /v1/voice-isolator endpoint\n    * Upload audio with background noise\n    * Remove noise, enhance voice clarity\n    * Output clean voice track\n  - For each feature:\n    * File upload interface\n    * Parameter configuration UI\n    * Processing status/progress\n    * Result preview and download\n    * Error handling\n  - Use progressive docs: fetch relevant feature docs\n- Expected output:\n  - Interface for each selected feature\n  - File upload handling\n  - API integration code\n  - Processing workflows\n  - Example usage\n\nPhase 5: Verification\nGoal: Ensure features work correctly\n\nActions:\n- Verify files created\n- Check syntax: !{bash npx tsc --noEmit 2>/dev/null || python -m py_compile *.py 2>/dev/null}\n- Verify API endpoints referenced\n- Test file upload logic\n\nPhase 6: Summary\nGoal: Guide on using advanced features\n\nActions:\n- Display summary:\n  - Features implemented: [list]\n  - Sound effects: text ‚Üí cinematic audio\n  - Voice changer: transform voice characteristics\n  - Dubbing: 70+ languages\n  - Voice isolator: noise removal\n- Usage instructions for each feature\n- Show code examples\n- Use cases:\n  - Sound effects: game audio, podcasts, videos\n  - Voice changer: privacy, creative projects\n  - Dubbing: international content\n  - Voice isolator: podcast cleanup, interviews\n- Next steps:\n  - Integrate with TTS/STT: /elevenlabs:add-text-to-speech\n  - Production deploy: /elevenlabs:add-production\n  - Full stack: /elevenlabs:build-full-stack"
              },
              {
                "name": "/add-agents-platform",
                "description": "Add conversational AI agents with MCP integration, tool calling, and real-time voice conversations",
                "path": "plugins/elevenlabs/commands/add-agents-platform.md",
                "frontmatter": {
                  "description": "Add conversational AI agents with MCP integration, tool calling, and real-time voice conversations",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add ElevenLabs Agents Platform with full MCP integration, enabling conversational voice agents that can access external tools via Model Context Protocol.\n\nCore Principles:\n- Build conversational voice agents with natural dialogue\n- Integrate MCP servers (Zapier, custom servers)\n- Configure fine-grained security controls for tool access\n- Support real-time WebSocket conversations\n- Enable multi-turn voice dialogues\n\nPhase 1: Discovery\nGoal: Understand agents and MCP requirements\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Load MCP examples:\n  @docs/elevenlabs/docs/mcp-integration-examples.md\n- Check setup: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null || pip show elevenlabs 2>/dev/null}\n- Detect framework: @package.json or @pyproject.toml\n- Parse $ARGUMENTS for agent and MCP needs\n\nPhase 2: Requirements Gathering\nGoal: Clarify agent capabilities\n\nActions:\n- Use AskUserQuestion if needed:\n  - Agent purpose? (customer service, personal assistant, sales, etc.)\n  - MCP servers? (Zapier, custom, none)\n  - Tool approval mode? (always ask, fine-grained, auto-approve)\n  - Real-time conversations? (WebSocket streaming)\n  - Multi-agent system? (specialized agents for different tasks)\n\nPhase 3: Planning\nGoal: Design agents platform integration\n\nActions:\n- Plan implementation:\n  - Agent configuration: name, description, voice, model\n  - MCP server setup: URLs, auth tokens, approval modes\n  - Tool definitions: allowed, approval-required, disabled\n  - WebSocket connection for real-time conversations\n  - Conversation management: sessions, state, history\n  - Security: tool approval workflow, user confirmation\n- Present plan\n\nPhase 4: Implementation\nGoal: Build agents platform with MCP\n\nActions:\n\nLaunch the elevenlabs-agents-builder agent to implement conversational agents with MCP.\n\nProvide detailed requirements:\n- Context: Framework, SDK status, MCP needs\n- Target: $ARGUMENTS\n- Requirements:\n  - Agent Configuration:\n    * Create agent with voice_id and model selection\n    * Set agent personality and instructions\n    * Configure conversation parameters\n  - MCP Integration (MAJOR FEATURE):\n    * Configure MCP server URLs\n    * Set authentication tokens (from environment)\n    * Define tool approval modes:\n      - always_ask: request permission for each tool\n      - fine_grained: custom rules per tool\n      - auto_approve: allow without asking (read-only tools)\n      - disabled: block specific tools\n    * Implement approval request handling\n    * Examples: Zapier MCP (hundreds of tools), custom MCP servers\n  - Real-time Conversations:\n    * WebSocket connection setup\n    * Multi-turn dialogue handling\n    * Conversation state management\n    * Voice streaming for agent responses\n  - Tool Calling:\n    * Define available tools from MCP\n    * Handle tool execution requests\n    * Process tool responses\n    * Integrate results into conversation\n  - Security:\n    * Tool approval workflow UI\n    * User confirmation for sensitive operations\n    * Rate limiting for tool calls\n    * Audit logging\n  - Use progressive docs: fetch agents platform and MCP docs\n- Expected output:\n  - Agent configuration files/code\n  - MCP server integration\n  - WebSocket conversation client\n  - Tool approval interface\n  - Example conversations\n\nPhase 5: Verification\nGoal: Ensure agents work correctly\n\nActions:\n- Verify files created\n- Check syntax: !{bash npx tsc --noEmit 2>/dev/null || python -m py_compile *.py 2>/dev/null}\n- Verify MCP configuration\n- Test WebSocket connection setup\n\nPhase 6: Summary\nGoal: Guide on using agents platform\n\nActions:\n- Display summary:\n  - Agent created: [name and purpose]\n  - MCP servers configured: [list]\n  - Tools available: [count]\n  - Security mode: [approval configuration]\n- Usage instructions:\n  - Start conversation with agent\n  - Agent can use MCP tools (with approval)\n  - Multi-turn dialogue examples\n  - Tool approval workflow\n- Configuration:\n  - MCP dashboard: https://elevenlabs.io/app/agents/integrations\n  - Zapier MCP: https://zapier.com/mcp\n  - Custom MCP servers\n- Show code examples\n- Next steps:\n  - Add more MCP servers\n  - Customize tool permissions\n  - Build multi-agent systems\n  - Production deploy: /elevenlabs:add-production"
              },
              {
                "name": "/add-production",
                "description": "Add rate limiting, monitoring, error handling, security best practices, and cost optimization",
                "path": "plugins/elevenlabs/commands/add-production.md",
                "frontmatter": {
                  "description": "Add rate limiting, monitoring, error handling, security best practices, and cost optimization",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add production-ready features including rate limiting, monitoring/telemetry, comprehensive error handling, security best practices, and cost optimization for ElevenLabs integration.\n\nCore Principles:\n- Implement client-side rate limiting\n- Add monitoring and usage tracking\n- Comprehensive error handling for all scenarios\n- Secure API key management\n- Optimize costs through model selection and caching\n- Production deployment best practices\n\nPhase 1: Discovery\nGoal: Understand production requirements\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check current implementation:\n  !{bash find . -name \"*.ts\" -o -name \"*.py\" | grep -E \"(eleven|voice|audio)\" | head -10}\n- Detect framework: @package.json or @pyproject.toml\n- Parse $ARGUMENTS for specific needs\n\nPhase 2: Requirements Gathering\nGoal: Clarify production features\n\nActions:\n- Use AskUserQuestion if needed:\n  - Which features? (Rate limiting, Monitoring, Error handling, Security, Cost optimization)\n  - Monitoring platform? (Built-in logging, external service)\n  - Error strategy? (Retry logic, fallbacks, user notifications)\n  - Rate limits? (Based on tier: Free, Starter, Pro, etc.)\n  - Caching? (Audio caching for repeated text)\n\nPhase 3: Planning\nGoal: Design production architecture\n\nActions:\n- Plan implementation:\n  - Rate Limiting: concurrent request limits, queue management\n  - Monitoring: usage tracking, latency metrics, error rates\n  - Error Handling: retry with exponential backoff, fallbacks\n  - Security: API key rotation, environment variables, HTTPS\n  - Cost Optimization: model selection, caching, batch processing\n- Present plan\n\nPhase 4: Implementation\nGoal: Build production features\n\nActions:\n\nLaunch the elevenlabs-production-agent to implement production features.\n\nProvide detailed requirements:\n- Context: Framework, current implementation, deployment target\n- Target: $ARGUMENTS\n- Requirements:\n  - Rate Limiting & Concurrency:\n    * Concurrent request limits (based on pricing tier)\n    * Request queue with priority\n    * Backpressure handling\n    * Rate limit error handling (429 responses)\n    * Per-user/per-IP rate limiting\n  - Monitoring & Telemetry:\n    * Usage tracking: characters (TTS), hours (STT)\n    * Latency monitoring per model\n    * Error rate tracking\n    * Cost estimation dashboards\n    * Alert thresholds for quota limits\n  - Error Handling:\n    * Retry logic with exponential backoff\n    * Circuit breaker pattern\n    * Graceful degradation\n    * User-friendly error messages\n    * Error logging and reporting\n  - Security Best Practices:\n    * API key in environment variables (never hardcoded)\n    * HTTPS for all requests\n    * Input validation and sanitization\n    * File upload security (type/size validation)\n    * Rate limiting per IP\n    * CORS configuration (if web app)\n  - Cost Optimization:\n    * Model selection guide:\n      - Flash v2.5: lowest cost, real-time\n      - Turbo v2.5: balanced\n      - Multilingual v2: quality for long-form\n      - v3 Alpha: highest quality when needed\n    * Audio caching for repeated text\n    * Batch processing when possible\n    * Character/hour usage tracking\n    * Cost alerts and limits\n  - Production Configuration:\n    * Environment-based config (dev/staging/prod)\n    * Feature flags\n    * Health check endpoints\n    * Graceful shutdown handling\n  - Use progressive docs: fetch production best practices\n- Expected output:\n  - Rate limiting middleware/utilities\n  - Monitoring dashboard/logging\n  - Error handling framework\n  - Security configuration\n  - Cost optimization utilities\n  - Production deployment guide\n\nPhase 5: Verification\nGoal: Ensure production readiness\n\nActions:\n- Verify files created\n- Check syntax: !{bash npx tsc --noEmit 2>/dev/null || python -m py_compile *.py 2>/dev/null}\n- Verify environment variables referenced\n- Test error handling logic\n\nPhase 6: Summary\nGoal: Guide on production deployment\n\nActions:\n- Display summary:\n  - Rate limiting: [concurrent limits]\n  - Monitoring: usage tracking, latency, errors\n  - Error handling: retries, fallbacks, logging\n  - Security: API keys secured, HTTPS, validation\n  - Cost optimization: model selection, caching\n- Configuration checklist:\n  - [ ] ELEVENLABS_API_KEY in environment\n  - [ ] Rate limits configured for tier\n  - [ ] Monitoring/logging enabled\n  - [ ] Error handling tested\n  - [ ] HTTPS enforced\n  - [ ] Cost tracking active\n- Deployment steps\n- Show configuration examples\n- Next steps:\n  - Test with production data\n  - Monitor usage and costs\n  - Scale as needed\n  - Full app: /elevenlabs:build-full-stack"
              },
              {
                "name": "/add-speech-to-text",
                "description": "Add speech-to-text transcription with Scribe v1, 99 languages, speaker diarization, and Vercel AI SDK integration",
                "path": "plugins/elevenlabs/commands/add-speech-to-text.md",
                "frontmatter": {
                  "description": "Add speech-to-text transcription with Scribe v1, 99 languages, speaker diarization, and Vercel AI SDK integration",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive STT transcription capabilities with Scribe v1 model, supporting 99 languages, speaker diarization, word-level timestamps, and seamless Vercel AI SDK integration.\n\nCore Principles:\n- Detect framework and adapt (Next.js, React, Python, Node.js)\n- Support Vercel AI SDK experimental_transcribe for TypeScript projects\n- Include native ElevenLabs SDK for Python projects\n- Implement file upload handling and audio processing\n- Provide speaker diarization and timestamping options\n\nPhase 1: Discovery\nGoal: Understand project setup and STT requirements\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check existing setup:\n  - SDK installed: !{bash npm list @elevenlabs/elevenlabs-js @ai-sdk/elevenlabs 2>/dev/null || pip show elevenlabs 2>/dev/null}\n  - Framework: @package.json or @pyproject.toml\n  - Authentication: @.env or @.env.local\n- Parse $ARGUMENTS for preferences (language, diarization, timestamps)\n\nPhase 2: Requirements Gathering\nGoal: Clarify STT implementation details\n\nActions:\n- If preferences not specified, use AskUserQuestion to ask:\n  - Which approach? (Vercel AI SDK for TypeScript, Native SDK for Python)\n  - Do you need speaker diarization? (identify multiple speakers)\n  - Timestamp granularity? (word-level, segment-level, or none)\n  - Audio event detection? (laughter, applause, background sounds)\n  - Default language? (or auto-detect from 99 supported languages)\n  - File upload interface? (drag-drop, file picker, URL input)\n\nPhase 3: Planning\nGoal: Design the STT implementation\n\nActions:\n- Based on framework, plan:\n  - Vercel AI SDK: Use experimental_transcribe with @ai-sdk/elevenlabs provider\n  - Native SDK: Use client.speechToText.transcribe() method\n  - File upload: multipart/form-data handling\n  - Audio processing: format validation, size limits\n  - Output format: text, words array, speaker labels, timestamps\n- Present plan for confirmation\n\nPhase 4: Implementation\nGoal: Build STT integration with specialized agent\n\nActions:\n\nLaunch the elevenlabs-stt-integrator agent to implement speech-to-text capabilities.\n\nProvide the agent with detailed requirements:\n- Context: Detected framework, SDK status, project structure\n- Target: $ARGUMENTS (specific requirements)\n- Requirements:\n  - Implement Scribe v1 transcription (99 languages, ‚â§5% WER for major languages)\n  - Add file upload interface with audio validation\n  - Configure transcription options:\n    * Language code (auto-detect or specify)\n    * Speaker diarization (up to 32 speakers)\n    * Timestamps granularity (word or segment level)\n    * Audio event tagging (optional)\n  - For Vercel AI SDK projects:\n    * Use experimental_transcribe from 'ai' package\n    * Configure providerOptions.elevenlabs settings\n    * Return structured TranscriptionResult\n  - For Native SDK projects:\n    * Use client.speechToText.transcribe()\n    * Handle async audio processing\n    * Format response consistently\n  - Add error handling for unsupported formats, large files\n  - Include loading states and progress indicators\n  - Use progressive documentation: fetch STT docs as needed\n- Expected output:\n  - STT component/function with all features\n  - File upload interface\n  - Transcription result display\n  - Example usage code\n\nPhase 5: Verification\nGoal: Ensure STT works correctly\n\nActions:\n- Verify files created\n- Check TypeScript/Python syntax:\n  - TypeScript: !{bash npx tsc --noEmit 2>/dev/null || echo \"No check\"}\n  - Python: !{bash python -m py_compile *.py 2>/dev/null || echo \"No Python\"}\n- Verify imports and dependencies\n- Test audio file validation logic\n\nPhase 6: Summary\nGoal: Guide user on STT usage\n\nActions:\n- Display summary:\n  - Files created: [list]\n  - Languages supported: 99 (excellent accuracy for 12 major languages)\n  - Features: diarization, timestamps, audio events\n  - Integration: Vercel AI SDK or Native SDK\n- Usage instructions:\n  - Upload audio file (mp3, wav, m4a, webm, etc.)\n  - Transcription with diarization\n  - Access word-level timestamps\n  - Speaker identification\n- Show code example\n- Next steps:\n  - Combine with TTS: /elevenlabs:add-text-to-speech\n  - Build voice chat: /elevenlabs:add-vercel-ai-sdk\n  - Add streaming: /elevenlabs:add-streaming"
              },
              {
                "name": "/add-streaming",
                "description": "Add real-time WebSocket audio streaming for both TTS and STT with low latency optimization",
                "path": "plugins/elevenlabs/commands/add-streaming.md",
                "frontmatter": {
                  "description": "Add real-time WebSocket audio streaming for both TTS and STT with low latency optimization",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add real-time WebSocket audio streaming for ultra-low latency TTS and continuous STT transcription with proper buffer management and connection handling.\n\nCore Principles:\n- Implement WebSocket connections for real-time audio\n- Support TTS streaming (audio chunks as generated)\n- Support STT streaming (continuous transcription)\n- Optimize for low latency (use Flash v2.5 model ~75ms)\n- Handle connection lifecycle and reconnection\n\nPhase 1: Discovery\nGoal: Understand streaming requirements\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check setup: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null || pip show elevenlabs 2>/dev/null}\n- Detect framework: @package.json or @pyproject.toml\n- Check for WebSocket support\n- Parse $ARGUMENTS for streaming preferences\n\nPhase 2: Requirements Gathering\nGoal: Clarify streaming implementation\n\nActions:\n- Use AskUserQuestion if needed:\n  - Which streaming? (TTS only, STT only, or both)\n  - Use case? (real-time conversations, live transcription, voice chat)\n  - Latency priority? (ultra-low <100ms, balanced, quality-first)\n  - Buffer size? (smaller for low latency, larger for quality)\n  - Client platform? (web, mobile, server-to-server)\n\nPhase 3: Planning\nGoal: Design streaming architecture\n\nActions:\n- Plan implementation:\n  - WebSocket connection management\n  - TTS streaming: text chunks ‚Üí audio stream\n  - STT streaming: audio stream ‚Üí text updates\n  - Buffer management: audio queuing, playback control\n  - Latency optimization: Flash v2.5 for TTS, streaming STT\n  - Error handling: connection loss, reconnection\n  - Client-side audio: Web Audio API, MediaRecorder API\n- Present plan\n\nPhase 4: Implementation\nGoal: Build streaming capabilities\n\nActions:\n\nLaunch the general-purpose agent to implement WebSocket streaming.\n\nProvide detailed requirements:\n- Context: Framework, SDK status, use case\n- Target: $ARGUMENTS\n- Requirements:\n  - TTS Streaming:\n    * WebSocket connection to /v1/text-to-speech/stream\n    * Send text chunks for conversion\n    * Receive audio chunks in real-time\n    * Use eleven_flash_v2_5 for ultra-low latency (~75ms)\n    * Buffer management for smooth playback\n    * Web Audio API integration for browser\n  - STT Streaming:\n    * Continuous audio capture (MediaRecorder)\n    * Stream audio chunks to STT endpoint\n    * Receive partial transcription results\n    * Update UI with interim results\n    * Final transcription when speech ends\n  - WebSocket Lifecycle:\n    * Connection establishment with auth\n    * Heartbeat/ping-pong for keep-alive\n    * Graceful disconnection\n    * Automatic reconnection on failure\n    * Error handling and recovery\n  - Buffer Management:\n    * Audio queue for TTS playback\n    * Chunk size optimization\n    * Prevent buffer underrun/overrun\n    * Latency monitoring\n  - Client Components:\n    * Streaming audio player\n    * Live transcription display\n    * Connection status indicator\n    * Latency metrics (optional)\n  - Use progressive docs: fetch streaming docs\n- Expected output:\n  - WebSocket client implementation\n  - TTS streaming component\n  - STT streaming component\n  - Buffer management utilities\n  - Example usage\n\nPhase 5: Verification\nGoal: Ensure streaming works\n\nActions:\n- Verify files created\n- Check syntax: !{bash npx tsc --noEmit 2>/dev/null || python -m py_compile *.py 2>/dev/null}\n- Verify WebSocket setup\n- Test buffer management logic\n\nPhase 6: Summary\nGoal: Guide on streaming usage\n\nActions:\n- Display summary:\n  - Features: TTS streaming, STT streaming, or both\n  - Latency: ~75ms with Flash v2.5\n  - Connection: WebSocket with auto-reconnect\n- Usage instructions:\n  - Start streaming TTS\n  - Continuous STT transcription\n  - Monitor connection status\n  - Handle latency/quality tradeoffs\n- Performance tips:\n  - Use Flash v2.5 for lowest latency\n  - Optimize buffer size for use case\n  - Monitor network conditions\n- Show code examples\n- Next steps:\n  - Build voice chat: /elevenlabs:add-vercel-ai-sdk\n  - Voice agents: /elevenlabs:add-agents-platform\n  - Production: /elevenlabs:add-production"
              },
              {
                "name": "/add-text-to-speech",
                "description": "Add comprehensive text-to-speech capabilities with multiple voice models (v3, Flash, Turbo, Multilingual) and streaming support",
                "path": "plugins/elevenlabs/commands/add-text-to-speech.md",
                "frontmatter": {
                  "description": "Add comprehensive text-to-speech capabilities with multiple voice models (v3, Flash, Turbo, Multilingual) and streaming support",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive TTS capabilities to the project with support for multiple ElevenLabs voice models, streaming audio, voice selection, and audio playback controls.\n\nCore Principles:\n- Detect framework and adapt implementation (Next.js, React, Python, Node.js)\n- Support all 4 voice models (Eleven v3, Flash v2.5, Turbo v2.5, Multilingual v2)\n- Implement both standard and streaming TTS\n- Create reusable components/functions\n- Include voice selection interface\n\nPhase 1: Discovery\nGoal: Understand project structure and existing setup\n\nActions:\n- Check if ElevenLabs SDK is already installed:\n  - TypeScript: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null}\n  - Python: !{bash pip show elevenlabs 2>/dev/null}\n- Detect framework:\n  - Next.js: @package.json (check for \"next\")\n  - Python: @requirements.txt or @pyproject.toml\n  - React: @package.json (check for \"react\")\n- Check if authentication is configured (@.env or @.env.local)\n- Parse $ARGUMENTS for specific options (model preference, streaming, etc.)\n\nPhase 2: Requirements Gathering\nGoal: Clarify TTS implementation needs\n\nActions:\n- If $ARGUMENTS doesn't specify preferences, use AskUserQuestion to ask:\n  - Which voice model to prioritize? (v3 Alpha for quality, Flash v2.5 for speed, Turbo v2.5 for balance, Multilingual v2 for stability)\n  - Do you need streaming audio support? (real-time vs complete audio)\n  - Should we include voice selection UI? (dropdown/list of available voices)\n  - Where should TTS functionality be added? (new page, existing component, API route, etc.)\n\nPhase 3: Planning\nGoal: Design the TTS implementation approach\n\nActions:\n- Based on detected framework, plan:\n  - Component structure (React components, Python functions, API routes)\n  - File locations following project conventions\n  - Voice model configuration strategy\n  - Audio playback implementation\n  - Error handling approach\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Build TTS integration with specialized agent\n\nActions:\n\nLaunch the elevenlabs-tts-integrator agent to implement text-to-speech capabilities.\n\nProvide the agent with a detailed prompt including:\n- Context: Detected framework, existing project structure, SDK installation status\n- Target: $ARGUMENTS (any specific requirements)\n- Requirements:\n  - Create TTS function/component with support for all 4 models:\n    * Eleven v3 Alpha (eleven_multilingual_v3) - highest quality, 70+ languages\n    * Eleven Flash v2.5 (eleven_flash_v2_5) - ultra-low latency ~75ms, 32 languages\n    * Eleven Turbo v2.5 (eleven_turbo_v2_5) - balanced speed/quality ~250ms\n    * Eleven Multilingual v2 (eleven_multilingual_v2) - stable, 29 languages\n  - Implement standard TTS (complete audio generation)\n  - Implement streaming TTS (real-time audio streaming) if requested\n  - Add voice selection interface (fetch from /v1/voices API)\n  - Create audio playback controls\n  - Include error handling and loading states\n  - Follow framework-specific patterns (React hooks, FastAPI routes, etc.)\n  - Add proper TypeScript types or Python type hints\n  - Use progressive documentation loading (fetch ElevenLabs TTS docs as needed)\n- Expected output:\n  - TTS component/function created\n  - Voice selection UI (if requested)\n  - Audio playback implementation\n  - Example usage code\n  - Configuration for voice model selection\n\nPhase 5: Verification\nGoal: Ensure TTS implementation works correctly\n\nActions:\n- Verify files were created in correct locations\n- Check for TypeScript/Python errors:\n  - TypeScript: !{bash npx tsc --noEmit 2>/dev/null || echo \"No TypeScript check available\"}\n  - Python: !{bash python -m py_compile *.py 2>/dev/null || echo \"No Python files to check\"}\n- Verify imports and dependencies\n- Test that API key is properly referenced from environment\n\nPhase 6: Summary\nGoal: Guide user on using TTS features\n\nActions:\n- Display implementation summary:\n  - Files created: [list of new files]\n  - Voice models available: [list of 4 models with descriptions]\n  - Features implemented: [standard TTS, streaming, voice selection, etc.]\n- Provide usage instructions:\n  - How to convert text to speech\n  - How to select different voice models\n  - How to use streaming vs standard mode\n  - How to customize voice settings (stability, clarity, style)\n- Show code example for detected framework\n- Suggest next steps:\n  - Test with different voice models\n  - Explore voice cloning: /elevenlabs:add-voice-management\n  - Add Vercel AI SDK integration: /elevenlabs:add-vercel-ai-sdk\n  - Configure production features: /elevenlabs:add-production"
              },
              {
                "name": "/add-vercel-ai-sdk",
                "description": "Add Vercel AI SDK integration with @ai-sdk/elevenlabs provider for multi-modal AI workflows",
                "path": "plugins/elevenlabs/commands/add-vercel-ai-sdk.md",
                "frontmatter": {
                  "description": "Add Vercel AI SDK integration with @ai-sdk/elevenlabs provider for multi-modal AI workflows",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate @ai-sdk/elevenlabs provider to enable multi-modal AI workflows combining voice transcription with LLM processing for voice-enabled chat applications.\n\nCore Principles:\n- Install @ai-sdk/elevenlabs provider package\n- Configure experimental_transcribe for STT\n- Integrate with existing AI SDK workflows (streamText, generateText)\n- Build voice input ‚Üí LLM ‚Üí voice output pipelines\n- Support Next.js App Router and API routes\n\nPhase 1: Discovery\nGoal: Understand existing AI SDK setup\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check for Vercel AI SDK:\n  !{bash npm list ai @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/elevenlabs 2>/dev/null}\n- Detect framework: @package.json\n- Check existing AI routes: !{bash find app/api -name \"*.ts\" 2>/dev/null | head -5}\n- Parse $ARGUMENTS for specific integration needs\n\nPhase 2: Requirements Gathering\nGoal: Clarify integration approach\n\nActions:\n- Use AskUserQuestion if needed:\n  - Which LLM provider are you using? (OpenAI, Anthropic, Google, etc.)\n  - Do you want voice-to-voice chat? (STT ‚Üí LLM ‚Üí TTS)\n  - Should we create API routes? (Next.js /api/transcribe, /api/chat)\n  - Multi-modal chat UI? (text + voice in same interface)\n\nPhase 3: Planning\nGoal: Design Vercel AI SDK integration\n\nActions:\n- Plan integration:\n  - Install @ai-sdk/elevenlabs if not present\n  - Create API route for transcription using experimental_transcribe\n  - Optionally integrate with existing chat routes\n  - Build client-side voice recording component\n  - Connect transcription ‚Üí LLM ‚Üí response pipeline\n- Present plan\n\nPhase 4: Implementation\nGoal: Build Vercel AI SDK integration\n\nActions:\n\nLaunch the elevenlabs-stt-integrator agent to implement Vercel AI SDK integration.\n\nProvide detailed requirements:\n- Context: Detected AI SDK setup, framework structure\n- Target: $ARGUMENTS\n- Requirements:\n  - Install @ai-sdk/elevenlabs: `npm install @ai-sdk/elevenlabs`\n  - Create transcription API route using experimental_transcribe\n  - Import elevenlabs provider: `import { elevenlabs } from '@ai-sdk/elevenlabs'`\n  - Configure transcription model: elevenlabs.transcription('scribe_v1')\n  - Set providerOptions for language, diarization, timestamps\n  - If voice chat requested:\n    * Integrate with streamText or generateText\n    * Create voice ‚Üí text ‚Üí LLM ‚Üí response flow\n    * Optionally add TTS for voice responses\n  - Create client components for:\n    * Audio recording interface\n    * Voice input handling\n    * Multi-modal chat UI (if requested)\n  - Add proper error handling\n  - Include TypeScript types from AI SDK\n  - Use progressive docs: fetch Vercel AI SDK docs as needed\n- Expected output:\n  - API routes created\n  - Client components for voice input\n  - Integration with LLM workflows\n  - Example usage code\n\nPhase 5: Verification\nGoal: Ensure integration works\n\nActions:\n- Verify package installed:\n  !{bash npm list @ai-sdk/elevenlabs 2>/dev/null}\n- Check TypeScript: !{bash npx tsc --noEmit 2>/dev/null || echo \"No check\"}\n- Verify imports resolve\n- Test API routes exist\n\nPhase 6: Summary\nGoal: Guide on Vercel AI SDK usage\n\nActions:\n- Display summary:\n  - Files created: [list]\n  - Integration: @ai-sdk/elevenlabs provider\n  - Features: transcription, multi-modal chat, LLM workflows\n- Usage instructions:\n  - API endpoint: POST /api/transcribe\n  - Voice chat workflow\n  - Multi-modal interface\n- Show code example\n- Next steps:\n  - Build complete voice app: /elevenlabs:build-full-stack\n  - Add streaming: /elevenlabs:add-streaming\n  - Production ready: /elevenlabs:add-production"
              },
              {
                "name": "/add-voice-management",
                "description": "Add voice cloning, library access, voice design, and voice customization capabilities",
                "path": "plugins/elevenlabs/commands/add-voice-management.md",
                "frontmatter": {
                  "description": "Add voice cloning, library access, voice design, and voice customization capabilities",
                  "argument-hint": [
                    "options"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive voice management features including instant/professional voice cloning, voice library access, voice design from text descriptions, and voice customization.\n\nCore Principles:\n- Support instant cloning (1 min audio) and professional cloning (30+ min)\n- Provide voice library browsing and selection\n- Enable voice design (generate from text descriptions)\n- Include voice remixing and customization\n- Implement voice upload and file handling\n\nPhase 1: Discovery\nGoal: Understand voice management needs\n\nActions:\n- Load SDK documentation:\n  @elevenlabs-documentation.md\n- Check setup: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null || pip show elevenlabs 2>/dev/null}\n- Detect framework: @package.json or @pyproject.toml\n- Parse $ARGUMENTS for specific voice features\n\nPhase 2: Requirements Gathering\nGoal: Clarify voice management scope\n\nActions:\n- Use AskUserQuestion if needed:\n  - Which features? (Voice cloning, Library, Design, Remix)\n  - Cloning type? (Instant for quick, Professional for quality)\n  - Voice library UI? (Browse, search, preview voices)\n  - Voice design? (Generate from text descriptions)\n  - File upload? (Audio samples for cloning)\n\nPhase 3: Planning\nGoal: Design voice management system\n\nActions:\n- Plan implementation:\n  - Voice cloning: upload audio, process, test cloned voice\n  - Voice library: fetch available voices, preview, select\n  - Voice design: text input ‚Üí voice generation\n  - Voice settings: stability, similarity, style, boost\n  - Storage: manage custom voices\n- Present plan\n\nPhase 4: Implementation\nGoal: Build voice management features\n\nActions:\n\nLaunch the elevenlabs-voice-manager agent to implement voice management.\n\nProvide detailed requirements:\n- Context: Framework, SDK status, project structure\n- Target: $ARGUMENTS\n- Requirements:\n  - Instant Voice Cloning:\n    * File upload (1 min minimum audio)\n    * POST /v1/voices/add endpoint\n    * Voice name and description\n    * Test cloned voice with sample text\n  - Professional Voice Cloning:\n    * Upload 30+ minutes of audio\n    * Higher quality cloning\n    * Additional customization options\n  - Voice Library:\n    * GET /v1/voices endpoint\n    * Browse 70+ pre-made voices\n    * Voice preview functionality\n    * Voice selection interface\n  - Voice Design:\n    * Text description ‚Üí voice generation\n    * Gender, age, accent parameters\n    * Preview and iterate\n  - Voice Settings:\n    * Stability (0-1): consistency vs expressiveness\n    * Similarity boost (0-1): closeness to original\n    * Style exaggeration (0-1): emphasis\n    * Speaker boost (boolean): enhance clarity\n  - File handling for audio upload\n  - Error handling for invalid audio\n  - Use progressive docs: fetch voice cloning docs\n- Expected output:\n  - Voice cloning interface\n  - Voice library browser\n  - Voice design tool\n  - Settings customization\n  - Example usage\n\nPhase 5: Verification\nGoal: Ensure voice features work\n\nActions:\n- Verify files created\n- Check syntax: !{bash npx tsc --noEmit 2>/dev/null || python -m py_compile *.py 2>/dev/null}\n- Verify API endpoints referenced\n- Test file upload validation\n\nPhase 6: Summary\nGoal: Guide on voice management\n\nActions:\n- Display summary:\n  - Features: cloning, library, design, customization\n  - Voice library: 70+ pre-made voices\n  - Cloning types: instant (1 min) vs professional (30+ min)\n- Usage instructions:\n  - Clone voice from audio sample\n  - Browse and preview voices\n  - Generate voice from description\n  - Customize voice settings\n- Show examples\n- Next steps:\n  - Use cloned voices: /elevenlabs:add-text-to-speech\n  - Build voice agents: /elevenlabs:add-agents-platform"
              },
              {
                "name": "/init",
                "description": "Initialize ElevenLabs project with SDK installation, authentication setup, and framework detection",
                "path": "plugins/elevenlabs/commands/init.md",
                "frontmatter": {
                  "description": "Initialize ElevenLabs project with SDK installation, authentication setup, and framework detection",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the elevenlabs plugin:\n\n- **api-authentication**: API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.\n- **mcp-integration**\n- **production-deployment**: Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.\n- **stt-integration**: ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.\n- **tts-integration**\n- **vercel-ai-patterns**\n- **voice-processing**: Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup ElevenLabs integration with automatic framework detection, SDK installation, authentication configuration, and basic TTS/STT examples.\n\nCore Principles:\n- Detect framework (Next.js, React, Python, Node.js) - never assume\n- Install appropriate SDK (@elevenlabs/elevenlabs-js or elevenlabs Python)\n- Configure authentication securely (.env files)\n- Provide working examples adapted to detected framework\n- Include Vercel AI SDK setup if Next.js detected\n\nPhase 1: Discovery\nGoal: Understand project structure and gather requirements\n\nActions:\n- Parse $ARGUMENTS for optional project name\n- Detect project type by checking for framework indicators:\n  - Next.js: package.json with \"next\" dependency\n  - React: package.json with \"react\" without \"next\"\n  - Python: requirements.txt, pyproject.toml, or setup.py\n  - Node.js: package.json without framework\n- Load relevant configuration files for context\n- Example: !{bash ls package.json pyproject.toml requirements.txt 2>/dev/null}\n\nPhase 2: Requirements Gathering\nGoal: Clarify user needs\n\nActions:\n- If framework cannot be detected or multiple options exist, use AskUserQuestion to ask:\n  - Which framework are you using? (Next.js, React, Python/FastAPI, Node.js, Other)\n  - Do you want Vercel AI SDK integration? (for Next.js/React projects)\n  - Which features to include? (TTS, STT, Voice Cloning, Streaming)\n  - Do you have an ElevenLabs API key? (if no, provide instructions)\n\nPhase 3: Planning\nGoal: Design the setup approach\n\nActions:\n- Based on detected framework, determine:\n  - Which SDK to install (TypeScript or Python)\n  - Where to place configuration (.env, .env.local)\n  - Which example files to create\n  - Whether to include Vercel AI SDK (@ai-sdk/elevenlabs)\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Execute setup with specialized agent\n\nActions:\n\nLaunch the elevenlabs-setup agent to initialize the ElevenLabs integration.\n\nProvide the agent with a detailed prompt including:\n- Context: Detected framework and project structure\n- Target: $ARGUMENTS (project name if provided)\n- Requirements:\n  - Install appropriate SDK (TypeScript: @elevenlabs/elevenlabs-js OR Python: elevenlabs)\n  - Setup authentication (.env file with ELEVENLABS_API_KEY)\n  - Create basic TTS example\n  - Create basic STT example (if requested)\n  - Add Vercel AI SDK integration if Next.js detected and user requested\n  - Follow existing project conventions and structure\n  - Use progressive documentation loading (fetch docs as needed, not all upfront)\n- Expected output:\n  - SDK installed and configured\n  - .env file created with API key placeholder\n  - Example files created in appropriate locations\n  - README or setup instructions\n\nPhase 5: Verification\nGoal: Ensure setup is complete and functional\n\nActions:\n- Verify SDK installation:\n  - TypeScript: !{bash npm list @elevenlabs/elevenlabs-js 2>/dev/null || echo \"Not installed\"}\n  - Python: !{bash pip show elevenlabs 2>/dev/null || echo \"Not installed\"}\n- Check .env file exists and has placeholder\n- Verify example files were created\n- Test imports/syntax if possible\n\nPhase 6: Summary\nGoal: Guide user on next steps\n\nActions:\n- Display setup summary:\n  - Framework detected: [framework]\n  - SDK installed: [version]\n  - Files created: [list of files]\n  - Configuration: .env file location\n- Provide next steps:\n  1. Add your ElevenLabs API key to .env file (get from https://elevenlabs.io/app/settings/api-keys)\n  2. Run example: [command to run example]\n  3. Explore features: /elevenlabs:add-text-to-speech, /elevenlabs:add-speech-to-text\n  4. For Vercel AI SDK: /elevenlabs:add-vercel-ai-sdk\n- Show quick start code snippet for detected framework"
              }
            ],
            "skills": [
              {
                "name": "api-authentication",
                "description": "API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.",
                "path": "plugins/elevenlabs/skills/api-authentication/SKILL.md",
                "frontmatter": {
                  "name": "api-authentication",
                  "description": "API authentication patterns, SDK installation scripts, environment variable management, and connection testing for ElevenLabs. Use when setting up ElevenLabs authentication, installing ElevenLabs SDK, configuring API keys, testing ElevenLabs connection, or when user mentions ElevenLabs authentication, xi-api-key, ELEVENLABS_API_KEY, or ElevenLabs setup.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# ElevenLabs API Authentication\n\nComprehensive authentication setup for ElevenLabs voice AI platform including SDK installation, API key management, environment configuration, and connection testing.\n\n## Overview\n\nThis skill provides:\n- Automated SDK installation for TypeScript and Python\n- Secure API key configuration with environment variables\n- Connection testing and validation scripts\n- Production-ready client templates\n- Complete authentication examples for Next.js and Python projects\n\n## Authentication Method\n\nElevenLabs uses API key authentication via HTTP headers:\n```\nxi-api-key: YOUR_ELEVENLABS_API_KEY\n```\n\n**Security Requirements:**\n- API keys must be stored in environment variables (never hardcoded)\n- Keys should never be exposed in client-side code\n- Each key can have endpoint restrictions and credit quotas\n\n## Scripts\n\nAll scripts are fully functional and production-ready:\n\n### 1. setup-auth.sh\nConfigures ELEVENLABS_API_KEY in .env file with validation.\n```bash\nbash scripts/setup-auth.sh [api-key]\n```\n\n### 2. test-connection.sh\nTests API connectivity using curl and validates credentials.\n```bash\nbash scripts/test-connection.sh\n```\n\n### 3. install-sdk.sh\nInstalls @elevenlabs/elevenlabs-js (TypeScript) or elevenlabs (Python) SDK.\n```bash\nbash scripts/install-sdk.sh [typescript|python]\n```\n\n### 4. validate-env.sh\nValidates .env file has required ELEVENLABS_API_KEY.\n```bash\nbash scripts/validate-env.sh\n```\n\n### 5. generate-client.sh\nGenerates API client boilerplate from templates.\n```bash\nbash scripts/generate-client.sh [typescript|python] [output-path]\n```\n\n## Templates\n\nAll templates are production-ready and fully implemented:\n\n### Environment Configuration\n- `templates/.env.template` - Environment variable template with all required keys\n\n### TypeScript Templates\n- `templates/api-client.ts.template` - ElevenLabs client with error handling\n- `templates/api-client-nextjs.ts.template` - Next.js server-side client\n- `templates/api-client-edge.ts.template` - Edge runtime compatible client\n\n### Python Templates\n- `templates/api-client.py.template` - ElevenLabs client with error handling\n- `templates/api-client-async.py.template` - Async client with connection pooling\n- `templates/api-client-fastapi.py.template` - FastAPI integration client\n\n## Examples\n\nAll examples include complete README.md files with step-by-step instructions:\n\n### Basic Usage Examples\n- `examples/nextjs-auth/README.md` - Complete Next.js authentication example\n  - Environment setup\n  - Server action implementation\n  - API route handler\n  - Error handling patterns\n\n- `examples/python-auth/README.md` - Complete Python authentication example\n  - Environment configuration\n  - Client initialization\n  - Error handling\n  - Connection testing\n\n- `examples/edge-runtime/README.md` - Edge runtime authentication example\n  - Vercel Edge Functions setup\n  - Cloudflare Workers setup\n  - Deno Deploy setup\n  - Environment variable access\n\n### Advanced Examples\n- `examples/multi-environment/README.md` - Multi-environment configuration (dev, staging, prod)\n  - Environment-specific API keys\n  - Configuration management\n  - Platform-specific setup (Vercel, Railway, Fly.io)\n\n- `examples/api-key-rotation/README.md` - API key rotation patterns\n  - Zero-downtime rotation strategies\n  - Dual-key pattern implementation\n  - Automated rotation scripts\n\n## Usage Instructions\n\n### Initial Setup\n\n1. **Install SDK**:\n   ```bash\n   # For TypeScript projects\n   bash scripts/install-sdk.sh typescript\n\n   # For Python projects\n   bash scripts/install-sdk.sh python\n   ```\n\n2. **Configure API Key**:\n   ```bash\n   # Interactive setup\n   bash scripts/setup-auth.sh\n\n   # Or provide key directly\n   bash scripts/setup-auth.sh sk_your_api_key_here\n   ```\n\n3. **Test Connection**:\n   ```bash\n   bash scripts/test-connection.sh\n   ```\n\n4. **Generate Client**:\n   ```bash\n   # TypeScript\n   bash scripts/generate-client.sh typescript src/lib/elevenlabs.ts\n\n   # Python\n   bash scripts/generate-client.sh python src/elevenlabs_client.py\n   ```\n\n### Integration Workflow\n\nFor **Next.js projects**:\n1. Run `bash scripts/install-sdk.sh typescript`\n2. Run `bash scripts/setup-auth.sh`\n3. Read `examples/nextjs-auth/README.md` for integration guide\n4. Generate client: `bash scripts/generate-client.sh typescript src/lib/elevenlabs.ts`\n\nFor **Python projects**:\n1. Run `bash scripts/install-sdk.sh python`\n2. Run `bash scripts/setup-auth.sh`\n3. Read `examples/python-auth/README.md` for integration guide\n4. Generate client: `bash scripts/generate-client.sh python src/elevenlabs_client.py`\n\nFor **FastAPI projects**:\n1. Run `bash scripts/install-sdk.sh python`\n2. Run `bash scripts/setup-auth.sh`\n3. Use template: `templates/api-client-fastapi.py.template`\n\n## Validation\n\nValidate your setup:\n```bash\n# Check environment variables\nbash scripts/validate-env.sh\n\n# Test API connection\nbash scripts/test-connection.sh\n```\n\n## Security Best Practices\n\n1. **Never commit .env files** - Add to .gitignore\n2. **Use environment-specific keys** - Different keys for dev/staging/prod\n3. **Rotate keys regularly** - Follow key rotation patterns in examples\n4. **Set endpoint restrictions** - Configure in ElevenLabs dashboard\n5. **Monitor credit usage** - Set custom credit quotas per key\n\n## Troubleshooting\n\n**API Key Not Found**:\n- Run `bash scripts/validate-env.sh`\n- Ensure .env file exists in project root\n- Check environment variable is loaded (dotenv)\n\n**Connection Failed**:\n- Run `bash scripts/test-connection.sh` for detailed diagnostics\n- Verify API key is valid in ElevenLabs dashboard\n- Check network connectivity and firewall rules\n\n**SDK Installation Failed**:\n- Ensure Node.js/npm (for TypeScript) or Python/pip (for Python) is installed\n- Check package.json or requirements.txt exists\n- Run with verbose flag: `bash -x scripts/install-sdk.sh typescript`\n\n## References\n\n- [ElevenLabs Authentication Docs](https://elevenlabs.io/docs/api-reference/authentication)\n- [ElevenLabs Quickstart](https://elevenlabs.io/docs/quickstart)\n- [TypeScript SDK Docs](https://github.com/elevenlabs/elevenlabs-js)\n- [Python SDK Docs](https://github.com/elevenlabs/elevenlabs-python)"
              },
              {
                "name": "production-deployment",
                "description": "Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.",
                "path": "plugins/elevenlabs/skills/production-deployment/SKILL.md",
                "frontmatter": {
                  "name": "production-deployment",
                  "description": "Production deployment patterns for ElevenLabs API including rate limiting, error handling, monitoring, and testing. Use when deploying to production, implementing rate limiting, setting up monitoring, handling errors, testing concurrency, or when user mentions production deployment, rate limits, error handling, monitoring, ElevenLabs production.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Production Deployment\n\nComplete production deployment guide for ElevenLabs API integration including rate limiting patterns, comprehensive error handling strategies, monitoring setup, and testing frameworks.\n\n## Overview\n\nThis skill provides battle-tested patterns for deploying ElevenLabs API integration to production environments with:\n\n- **Rate Limiting**: Concurrency-aware rate limiting respecting plan limits\n- **Error Handling**: Comprehensive error recovery and retry strategies\n- **Monitoring**: Real-time metrics, logging, and alerting\n- **Testing**: Load testing, concurrency validation, and production readiness checks\n\n## Quick Start\n\n### 1. Setup Monitoring Infrastructure\n\n```bash\nbash scripts/setup-monitoring.sh --project-name \"my-elevenlabs-app\" \\\n  --log-level \"info\" \\\n  --metrics-port 9090\n```\n\nThis script:\n- Configures Winston logging with rotation\n- Sets up Prometheus metrics endpoints\n- Creates health check endpoints\n- Initializes error tracking\n\n### 2. Deploy Production Configuration\n\n```bash\nbash scripts/deploy-production.sh --environment \"production\" \\\n  --api-key \"$ELEVENLABS_API_KEY\" \\\n  --concurrency-limit 10 \\\n  --region \"us-east-1\"\n```\n\nThis script:\n- Validates environment variables\n- Applies rate limiting configuration\n- Configures error handling middleware\n- Sets up monitoring integrations\n- Performs smoke tests\n\n### 3. Test Rate Limiting\n\n```bash\nbash scripts/test-rate-limiting.sh --concurrency 20 \\\n  --duration 60 \\\n  --plan-tier \"pro\"\n```\n\nThis script:\n- Simulates concurrent requests\n- Validates queue behavior\n- Measures latency under load\n- Generates performance report\n\n## ElevenLabs Concurrency Limits\n\n### Limits by Plan Tier\n\n| Plan | Multilingual v2 | Turbo/Flash | STT | Music |\n|------|-----------------|-------------|-----|-------|\n| Free | 2 | 4 | 8 | N/A |\n| Starter | 3 | 6 | 12 | 2 |\n| Creator | 5 | 10 | 20 | 2 |\n| Pro | 10 | 20 | 40 | 2 |\n| Scale | 15 | 30 | 60 | 3 |\n| Business | 15 | 30 | 60 | 3 |\n| Enterprise | Elevated | Elevated | Elevated | Highest |\n\n### Queue Management\n\nWhen concurrency limits are exceeded:\n- Requests are queued alongside lower-priority requests\n- Typical latency increase: ~50ms\n- Response headers include: `current-concurrent-requests`, `maximum-concurrent-requests`\n\n### Real-World Capacity\n\nA concurrency limit of 5 can typically support ~100 simultaneous audio broadcasts depending on:\n- Audio generation speed\n- User behavior patterns\n- Request distribution\n\n## Rate Limiting Patterns\n\n### 1. Token Bucket Algorithm\n\nBest for: Variable rate limiting with burst capacity\n\n```javascript\n// See templates/rate-limiter.js.template for full implementation\nconst limiter = new TokenBucketRateLimiter({\n  capacity: 10,           // Max concurrent requests\n  refillRate: 2,          // Tokens per second\n  queueSize: 100          // Max queued requests\n});\n```\n\n### 2. Sliding Window with Priority Queue\n\nBest for: Enforcing strict concurrency limits with prioritization\n\n```python\n# See templates/error-handler.py.template for full implementation\nlimiter = SlidingWindowRateLimiter(\n    max_concurrent=10\n    window_size=60\n    priority_levels=3\n)\n```\n\n### 3. Adaptive Rate Limiting\n\nBest for: Self-adjusting to API response headers\n\nMonitors `current-concurrent-requests` and `maximum-concurrent-requests` headers to dynamically adjust rate limits.\n\n## Error Handling Strategies\n\n### Error Categories\n\n**1. Rate Limit Errors (429)**\n- Implement exponential backoff\n- Queue requests for retry\n- Monitor queue depth\n\n**2. Service Errors (500-599)**\n- Retry with exponential backoff\n- Circuit breaker pattern\n- Fallback to cached audio\n\n**3. Client Errors (400-499)**\n- Log for debugging\n- Do not retry\n- Return meaningful error to user\n\n**4. Network Errors**\n- Retry with linear backoff\n- Timeout after 30 seconds\n- Circuit breaker after 5 failures\n\n### Circuit Breaker Pattern\n\n```javascript\n// Automatically opens circuit after threshold failures\nconst circuitBreaker = new CircuitBreaker({\n  failureThreshold: 5\n  resetTimeout: 60000\n  monitorInterval: 5000\n});\n```\n\n## Monitoring Setup\n\n### Key Metrics to Track\n\n**Request Metrics:**\n- `elevenlabs_requests_total` - Total requests by status\n- `elevenlabs_requests_duration_seconds` - Request latency histogram\n- `elevenlabs_concurrent_requests` - Current concurrent requests\n- `elevenlabs_queue_depth` - Queued requests waiting\n\n**Error Metrics:**\n- `elevenlabs_errors_total` - Total errors by type\n- `elevenlabs_retries_total` - Total retry attempts\n- `elevenlabs_circuit_breaker_state` - Circuit breaker state\n\n**Business Metrics:**\n- `elevenlabs_characters_generated` - Total characters processed\n- `elevenlabs_audio_duration_seconds` - Total audio duration\n- `elevenlabs_quota_used_percentage` - Quota utilization\n\n### Logging Best Practices\n\n**Structure logs with:**\n- Request ID for tracing\n- User ID for analysis\n- Timestamp in ISO 8601\n- Error stack traces\n- Performance metrics\n\n**Log Levels:**\n- `error` - Failures requiring attention\n- `warn` - Degraded performance, retries\n- `info` - Request completion, key events\n- `debug` - Detailed execution flow\n\n### Alerting Rules\n\n**Critical Alerts:**\n- Error rate > 5% over 5 minutes\n- Circuit breaker open for > 1 minute\n- Queue depth > 500 requests\n\n**Warning Alerts:**\n- Latency p95 > 2 seconds\n- Quota usage > 90%\n- Retry rate > 20%\n\n## Testing Frameworks\n\n### Load Testing\n\nSimulate production traffic patterns:\n\n```bash\n# Gradual ramp-up test\nbash scripts/test-rate-limiting.sh \\\n  --pattern \"ramp-up\" \\\n  --start-rps 1 \\\n  --end-rps 10 \\\n  --duration 300\n```\n\n### Concurrency Validation\n\nVerify concurrency limits are enforced:\n\n```bash\n# Burst test\nbash scripts/test-rate-limiting.sh \\\n  --pattern \"burst\" \\\n  --concurrency 50 \\\n  --iterations 100\n```\n\n### Chaos Testing\n\nTest error handling under adverse conditions:\n\n```bash\n# Simulate API failures\nbash scripts/test-rate-limiting.sh \\\n  --pattern \"chaos\" \\\n  --failure-rate 0.1 \\\n  --duration 120\n```\n\n## Production Checklist\n\n### Pre-Deployment\n\n- [ ] Environment variables configured\n- [ ] Rate limiting configured for plan tier\n- [ ] Error handling middleware implemented\n- [ ] Monitoring and logging configured\n- [ ] Health check endpoints created\n- [ ] Load testing completed\n- [ ] Chaos testing completed\n\n### Post-Deployment\n\n- [ ] Smoke tests passed\n- [ ] Metrics dashboard configured\n- [ ] Alerts configured and tested\n- [ ] On-call rotation established\n- [ ] Runbooks documented\n- [ ] Backup/fallback strategy tested\n\n## Scripts\n\n### setup-monitoring.sh\n\nConfigures comprehensive monitoring infrastructure:\n- Winston logging with daily rotation\n- Prometheus metrics exporter\n- Health check endpoints\n- Error tracking integration\n- Custom metric collectors\n\n**Usage:**\n```bash\nbash scripts/setup-monitoring.sh \\\n  --project-name \"my-app\" \\\n  --log-level \"info\" \\\n  --metrics-port 9090 \\\n  --health-port 8080\n```\n\n### deploy-production.sh\n\nProduction deployment orchestration:\n- Environment validation\n- Dependency installation\n- Configuration deployment\n- Service health checks\n- Smoke test execution\n- Rollback on failure\n\n**Usage:**\n```bash\nbash scripts/deploy-production.sh \\\n  --environment \"production\" \\\n  --api-key \"$ELEVENLABS_API_KEY\" \\\n  --concurrency-limit 10 \\\n  --skip-tests false\n```\n\n### test-rate-limiting.sh\n\nComprehensive rate limiting test suite:\n- Concurrency limit validation\n- Queue behavior testing\n- Latency measurement\n- Error rate tracking\n- Performance reporting\n\n**Usage:**\n```bash\nbash scripts/test-rate-limiting.sh \\\n  --concurrency 20 \\\n  --duration 60 \\\n  --plan-tier \"pro\" \\\n  --pattern \"ramp-up\"\n```\n\n### validate-config.sh\n\nProduction configuration validator:\n- Environment variable checks\n- API key validation\n- Rate limit configuration\n- Monitoring setup verification\n- Security audit\n\n**Usage:**\n```bash\nbash scripts/validate-config.sh \\\n  --config-file \"config/production.json\" \\\n  --strict true\n```\n\n### rollback.sh\n\nAutomated rollback script:\n- Reverts to previous deployment\n- Restores configuration\n- Validates health checks\n- Notifies team\n\n**Usage:**\n```bash\nbash scripts/rollback.sh \\\n  --deployment-id \"deploy-123\" \\\n  --reason \"High error rate\"\n```\n\n## Templates\n\n### rate-limiter.js.template\n\nToken bucket rate limiter with priority queue:\n- Configurable capacity and refill rate\n- Priority-based request queuing\n- Automatic backpressure handling\n- Prometheus metrics integration\n\n### rate-limiter.py.template\n\nSliding window rate limiter with async support:\n- Strict concurrency enforcement\n- Redis-backed for distributed systems\n- Circuit breaker integration\n- Comprehensive error handling\n\n### error-handler.js.template\n\nProduction-grade error handler:\n- Error categorization and routing\n- Exponential backoff retry logic\n- Circuit breaker pattern\n- Structured error logging\n\n### error-handler.py.template\n\nAsync error handler with context:\n- Context-aware error handling\n- Retry with jitter\n- Error aggregation and reporting\n- Integration with monitoring\n\n### monitoring-config.json.template\n\nComplete monitoring configuration:\n- Prometheus scrape configs\n- Alert rules and thresholds\n- Log aggregation settings\n- Dashboard definitions\n\n### health-check.js.template\n\nComprehensive health check endpoint:\n- API connectivity verification\n- Rate limiter health\n- Queue depth monitoring\n- Dependency checks\n\n## Examples\n\n### Rate Limiting Example\n\nComplete implementation showing:\n- Token bucket rate limiter\n- Priority queue management\n- Backpressure handling\n- Metrics collection\n\n**Location:** `examples/rate-limiting/`\n\n### Error Handling Example\n\nProduction error handling patterns:\n- Retry with exponential backoff\n- Circuit breaker implementation\n- Fallback strategies\n- Error logging and alerting\n\n**Location:** `examples/error-handling/`\n\n### Monitoring Example\n\nFull monitoring stack setup:\n- Prometheus metrics\n- Grafana dashboards\n- Winston logging\n- Alert manager configuration\n\n**Location:** `examples/monitoring/`\n\n## Best Practices\n\n### Rate Limiting\n\n1. **Configure for your plan tier** - Don't exceed concurrency limits\n2. **Implement graceful degradation** - Queue requests, don't drop\n3. **Monitor queue depth** - Alert on excessive queueing\n4. **Use adaptive limiting** - Adjust based on response headers\n5. **Test under load** - Validate behavior before production\n\n### Error Handling\n\n1. **Categorize errors** - Different strategies for different error types\n2. **Implement retries carefully** - Exponential backoff with jitter\n3. **Use circuit breakers** - Prevent cascade failures\n4. **Log comprehensively** - Include context for debugging\n5. **Provide fallbacks** - Cached audio, degraded experience\n\n### Monitoring\n\n1. **Track key metrics** - Request rate, latency, errors, concurrency\n2. **Set meaningful alerts** - Actionable, not noisy\n3. **Use structured logging** - JSON format for easy parsing\n4. **Create dashboards** - Real-time visibility\n5. **Test alerts** - Verify notification channels work\n\n### Testing\n\n1. **Load test gradually** - Ramp up to avoid overwhelming API\n2. **Simulate realistic patterns** - User behavior, not raw requests\n3. **Test error scenarios** - Chaos engineering\n4. **Validate concurrency** - Ensure limits are enforced\n5. **Monitor during tests** - Use production monitoring stack\n\n## Troubleshooting\n\n### High Error Rate\n\n**Symptoms:** Error rate > 5%\n\n**Diagnosis:**\n1. Check Prometheus metrics: `rate(elevenlabs_errors_total[5m])`\n2. Review error logs for patterns\n3. Verify API key is valid\n4. Check quota remaining\n\n**Resolution:**\n1. If rate limiting: Reduce request rate or upgrade plan\n2. If service errors: Implement circuit breaker, contact support\n3. If client errors: Fix request validation\n\n### High Latency\n\n**Symptoms:** p95 latency > 2 seconds\n\n**Diagnosis:**\n1. Check concurrency: `elevenlabs_concurrent_requests`\n2. Check queue depth: `elevenlabs_queue_depth`\n3. Review response headers: `current-concurrent-requests`\n\n**Resolution:**\n1. Increase concurrency limit (upgrade plan if needed)\n2. Optimize request payload size\n3. Implement request coalescing\n4. Use Turbo/Flash models for lower latency\n\n### Circuit Breaker Open\n\n**Symptoms:** Requests failing immediately\n\n**Diagnosis:**\n1. Check circuit breaker state metric\n2. Review error logs for failure pattern\n3. Check ElevenLabs status page\n\n**Resolution:**\n1. Wait for automatic reset (default 60s)\n2. If persistent: Check API connectivity\n3. Manual reset if resolved: Restart service\n\n## Resources\n\n- [ElevenLabs API Docs](https://elevenlabs.io/docs)\n- [Concurrency Documentation](https://elevenlabs.io/docs/models#concurrency-and-priority)\n- [API Reference](https://elevenlabs.io/docs/api-reference)\n- [Status Page](https://status.elevenlabs.io)\n\n## Contributing\n\nWhen updating this skill:\n\n1. Test scripts thoroughly in production-like environment\n2. Update templates with latest best practices\n3. Add examples for new patterns\n4. Update troubleshooting guide\n5. Validate with `validate-skill.sh`\n\n---\n\n**Version:** 1.0.0\n**Last Updated:** 2025-10-29\n**Maintainer:** ElevenLabs Plugin Team"
              },
              {
                "name": "stt-integration",
                "description": "ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.",
                "path": "plugins/elevenlabs/skills/stt-integration/SKILL.md",
                "frontmatter": {
                  "name": "stt-integration",
                  "description": "ElevenLabs Speech-to-Text transcription workflows with Scribe v1 supporting 99 languages, speaker diarization, and Vercel AI SDK integration. Use when implementing audio transcription, building STT features, integrating speech-to-text, setting up Vercel AI SDK with ElevenLabs, or when user mentions transcription, STT, Scribe v1, audio-to-text, speaker diarization, or multi-language transcription.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# stt-integration\n\nThis skill provides comprehensive guidance for implementing ElevenLabs Speech-to-Text (STT) capabilities using the Scribe v1 model, which supports 99 languages with state-of-the-art accuracy, speaker diarization for up to 32 speakers, and seamless Vercel AI SDK integration.\n\n## Core Capabilities\n\n### Scribe v1 Model Features\n- **Multi-language support**: 99 languages with varying accuracy levels\n- **Speaker diarization**: Up to 32 speakers with identification\n- **Word-level timestamps**: Precise synchronization for video/audio alignment\n- **Audio event detection**: Identifies sounds like laughter and applause\n- **High accuracy**: Optimized for accuracy over real-time processing\n\n### Supported Formats\n- **Audio**: AAC, AIFF, OGG, MP3, Opus, WAV, WebM, FLAC, M4A\n- **Video**: MP4, AVI, Matroska, QuickTime, WMV, FLV, WebM, MPEG, 3GPP\n- **Limits**: Max 3 GB file size, 10 hours duration\n\n## Skill Structure\n\n### Scripts (scripts/)\n1. **transcribe-audio.sh** - Direct API transcription with curl\n2. **setup-vercel-ai.sh** - Install and configure @ai-sdk/elevenlabs\n3. **test-stt.sh** - Test STT with sample audio files\n4. **validate-audio.sh** - Validate audio file format and size\n5. **batch-transcribe.sh** - Process multiple audio files\n\n### Templates (templates/)\n1. **stt-config.json.template** - STT configuration template\n2. **vercel-ai-transcribe.ts.template** - Vercel AI SDK TypeScript template\n3. **vercel-ai-transcribe.py.template** - Vercel AI SDK Python template\n4. **api-transcribe.ts.template** - Direct API TypeScript template\n5. **api-transcribe.py.template** - Direct API Python template\n6. **diarization-config.json.template** - Speaker diarization configuration\n\n### Examples (examples/)\n1. **basic-stt/** - Basic STT with direct API\n2. **vercel-ai-stt/** - Vercel AI SDK integration\n3. **diarization/** - Speaker diarization examples\n4. **multi-language/** - Multi-language transcription\n5. **webhook-integration/** - Async transcription with webhooks\n\n## Usage Instructions\n\n### 1. Setup Vercel AI SDK Integration\n\n```bash\n# Install dependencies\nbash scripts/setup-vercel-ai.sh\n\n# Verify installation\nnpm list @ai-sdk/elevenlabs\n```\n\n### 2. Basic Transcription\n\n```bash\n# Transcribe a single audio file\nbash scripts/transcribe-audio.sh path/to/audio.mp3 en\n\n# Validate audio before transcription\nbash scripts/validate-audio.sh path/to/audio.mp3\n\n# Batch transcribe multiple files\nbash scripts/batch-transcribe.sh path/to/audio/directory en\n```\n\n### 3. Test STT Implementation\n\n```bash\n# Run comprehensive tests\nbash scripts/test-stt.sh\n```\n\n### 4. Use Templates\n\n```typescript\n// Read Vercel AI SDK template\nRead: templates/vercel-ai-transcribe.ts.template\n\n// Customize for your use case\n// - Set language code\n// - Configure diarization\n// - Enable audio event tagging\n// - Set timestamp granularity\n```\n\n### 5. Explore Examples\n\n```bash\n# Basic STT example\nRead: examples/basic-stt/README.md\n\n# Vercel AI SDK example\nRead: examples/vercel-ai-stt/README.md\n\n# Speaker diarization example\nRead: examples/diarization/README.md\n```\n\n## Language Support\n\n### Excellent Accuracy (‚â§5% WER)\n30 languages including: English, French, German, Spanish, Italian, Japanese, Portuguese, Dutch, Polish, Russian\n\n### High Accuracy (>5-10% WER)\n19 languages including: Bengali, Mandarin Chinese, Tamil, Telugu, Vietnamese, Turkish\n\n### Good Accuracy (>10-25% WER)\n30 languages including: Arabic, Korean, Thai, Indonesian, Hebrew, Czech\n\n### Moderate Accuracy (>25-50% WER)\n19 languages including: Amharic, Khmer, Lao, Burmese, Nepali\n\n## Configuration Options\n\n### Provider Options (Vercel AI SDK)\n- **languageCode**: ISO-639-1/3 code (e.g., 'en', 'es', 'ja')\n- **tagAudioEvents**: Enable sound detection (default: true)\n- **numSpeakers**: Max speakers 1-32 (default: auto-detect)\n- **diarize**: Enable speaker identification (default: true)\n- **timestampsGranularity**: 'none' | 'word' | 'character' (default: 'word')\n- **fileFormat**: 'pcm_s16le_16' | 'other' (default: 'other')\n\n### Best Practices\n1. **Specify language code** when known for better performance\n2. **Use pcm_s16le_16** format for lowest latency with uncompressed audio\n3. **Enable diarization** for multi-speaker content\n4. **Set numSpeakers** for better accuracy when speaker count is known\n5. **Use webhooks** for files >8 minutes for async processing\n\n## Common Patterns\n\n### Pattern 1: Simple Transcription\nUse direct API or Vercel AI SDK for single-language, single-speaker transcription.\n\n### Pattern 2: Multi-Speaker Transcription\nEnable diarization and set numSpeakers for interviews, meetings, podcasts.\n\n### Pattern 3: Multi-Language Support\nDetect language automatically or specify when known for content in 99 languages.\n\n### Pattern 4: Video Transcription\nExtract audio from video formats and transcribe with timestamps for subtitles.\n\n### Pattern 5: Webhook Integration\nProcess long files asynchronously using webhook callbacks for results.\n\n## Integration with Other ElevenLabs Skills\n\n- **tts-integration**: Combine STT ‚Üí processing ‚Üí TTS for voice translation workflows\n- **voice-cloning**: Transcribe existing voice samples before cloning\n- **dubbing**: Use STT as first step in dubbing pipeline\n\n## Troubleshooting\n\n### Audio Format Issues\n```bash\n# Validate audio format\nbash scripts/validate-audio.sh your-audio.mp3\n```\n\n### Language Detection Problems\n- Specify languageCode explicitly instead of auto-detection\n- Ensure audio quality is sufficient for chosen language\n\n### Diarization Not Working\n- Verify numSpeakers is set correctly (1-32)\n- Check that diarize: true is configured\n- Ensure audio has clear speaker separation\n\n### File Size/Duration Limits\n- Max 3 GB file size\n- Max 10 hours duration\n- Files >8 minutes are chunked automatically\n\n## Script Reference\n\nAll scripts are located in `skills/stt-integration/scripts/`:\n\n1. **transcribe-audio.sh** - Main transcription script with curl\n2. **setup-vercel-ai.sh** - Install @ai-sdk/elevenlabs package\n3. **test-stt.sh** - Comprehensive test suite\n4. **validate-audio.sh** - Audio format and size validation\n5. **batch-transcribe.sh** - Batch processing for multiple files\n\n## Template Reference\n\nAll templates are located in `skills/stt-integration/templates/`:\n\n1. **stt-config.json.template** - JSON configuration\n2. **vercel-ai-transcribe.ts.template** - TypeScript with Vercel AI SDK\n3. **vercel-ai-transcribe.py.template** - Python with Vercel AI SDK\n4. **api-transcribe.ts.template** - TypeScript with direct API\n5. **api-transcribe.py.template** - Python with direct API\n6. **diarization-config.json.template** - Diarization settings\n\n## Example Reference\n\nAll examples are located in `skills/stt-integration/examples/`:\n\n1. **basic-stt/** - Basic transcription workflow\n2. **vercel-ai-stt/** - Vercel AI SDK integration\n3. **diarization/** - Speaker identification\n4. **multi-language/** - Multi-language support\n5. **webhook-integration/** - Async processing\n\n---\n\n**Skill Location**: `plugins/elevenlabs/skills/stt-integration/`\n**Version**: 1.0.0\n**Last Updated**: 2025-10-29"
              },
              {
                "name": "voice-processing",
                "description": "Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.",
                "path": "plugins/elevenlabs/skills/voice-processing/SKILL.md",
                "frontmatter": {
                  "name": "voice-processing",
                  "description": "Voice cloning workflows, voice library management, audio format conversion, and voice settings. Use when cloning voices, managing voice libraries, processing audio for voice creation, configuring voice settings, or when user mentions voice cloning, instant cloning, professional cloning, voice library, audio processing, voice settings, or ElevenLabs voices.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Voice Processing Skill\n\nThis skill provides comprehensive voice processing capabilities for ElevenLabs including voice cloning (instant and professional), voice library management, audio format conversion, and voice settings configuration.\n\n## Voice Cloning Overview\n\nElevenLabs offers two voice cloning approaches:\n\n### Instant Voice Cloning (IVC)\n- Requires only brief audio samples (1-5 minutes)\n- Fast processing and quick turnaround\n- Suitable for rapid prototyping and testing\n- Cannot be shared in Voice Library\n- Requires Creator plan or higher\n\n### Professional Voice Cloning (PVC)\n- Requires extended training audio (30+ minutes recommended)\n- Hyper-realistic, high-quality output\n- Suitable for production applications\n- Can be shared in Voice Library for earnings\n- Requires Creator plan or higher\n\n## Available Scripts\n\n### 1. clone-voice.sh\nClone a voice using instant or professional method.\n\n**Usage:**\n```bash\nbash scripts/clone-voice.sh --name \"Voice Name\" --method instant --files \"audio1.mp3,audio2.mp3\"\nbash scripts/clone-voice.sh --name \"Voice Name\" --method professional --files \"audio1.mp3,audio2.mp3\" --description \"Voice description\"\n```\n\n**Features:**\n- Supports both instant and professional cloning\n- Multiple audio file inputs\n- Automatic file validation\n- Voice captcha verification\n- Returns voice ID for use in TTS\n\n### 2. list-voices.sh\nFetch and list available voices with filtering options.\n\n**Usage:**\n```bash\nbash scripts/list-voices.sh --category all\nbash scripts/list-voices.sh --category cloned\nbash scripts/list-voices.sh --category library --search \"Australian narration\"\n```\n\n**Features:**\n- List all voices (owned, cloned, library)\n- Filter by category (cloned, library, default, voice-design)\n- Search by tags and descriptions\n- Display voice details (name, ID, category, description)\n- JSON or table output formats\n\n### 3. process-audio.py\nProcess audio files for voice cloning preparation.\n\n**Usage:**\n```bash\npython scripts/process-audio.py --input audio.mp3 --output processed.mp3 --sample-rate 22050\npython scripts/process-audio.py --input audio.mp3 --output processed.mp3 --remove-noise --normalize\n```\n\n**Features:**\n- Audio format conversion (MP3, WAV, FLAC, OGG)\n- Sample rate conversion\n- Noise reduction\n- Audio normalization\n- Silence trimming\n- Quality validation\n\n### 4. manage-voice-library.sh\nManage voice library operations.\n\n**Usage:**\n```bash\nbash scripts/manage-voice-library.sh --action add --voice-id \"abc123\" --collection \"My Voices\"\nbash scripts/manage-voice-library.sh --action share --voice-id \"abc123\" --enable-rewards\nbash scripts/manage-voice-library.sh --action remove --voice-id \"abc123\"\n```\n\n**Features:**\n- Add voices to library\n- Share professional clones for rewards\n- Remove voices from library\n- Add descriptions and tags\n- Manage voice collections\n\n### 5. configure-voice-settings.sh\nConfigure and update voice settings.\n\n**Usage:**\n```bash\nbash scripts/configure-voice-settings.sh --voice-id \"abc123\" --stability 0.75 --similarity 0.85\nbash scripts/configure-voice-settings.sh --voice-id \"abc123\" --get-defaults\n```\n\n**Features:**\n- Get default voice settings\n- Get voice-specific settings\n- Update stability parameter (0.0-1.0)\n- Update similarity boost (0.0-1.0)\n- Update style parameter (0.0-1.0)\n- Validate settings before applying\n\n## Templates\n\n### 1. voice-clone-config.json.template\nConfiguration template for voice cloning operations.\n\n**Variables:**\n- `${VOICE_NAME}` - Name for the cloned voice\n- `${VOICE_DESCRIPTION}` - Description of the voice\n- `${CLONING_METHOD}` - \"instant\" or \"professional\"\n- `${AUDIO_FILES}` - Array of audio file paths\n- `${LABELS}` - Custom tags/labels for organization\n\n### 2. voice-settings.json.template\nVoice settings configuration template.\n\n**Variables:**\n- `${VOICE_ID}` - Voice identifier\n- `${STABILITY}` - Stability parameter (0.0-1.0)\n- `${SIMILARITY_BOOST}` - Similarity boost (0.0-1.0)\n- `${STYLE}` - Style parameter (0.0-1.0)\n- `${USE_SPEAKER_BOOST}` - Enable speaker boost (true/false)\n\n### 3. voice-library-entry.json.template\nVoice library entry configuration template.\n\n**Variables:**\n- `${VOICE_ID}` - Voice identifier\n- `${DISPLAY_NAME}` - Public display name\n- `${DESCRIPTION}` - Voice description\n- `${TAGS}` - Array of searchable tags\n- `${CATEGORY}` - Voice category\n- `${ENABLE_REWARDS}` - Enable cash rewards (true/false)\n\n### 4. audio-processing-config.json.template\nAudio processing pipeline configuration.\n\n**Variables:**\n- `${INPUT_FORMAT}` - Input audio format\n- `${OUTPUT_FORMAT}` - Output audio format\n- `${SAMPLE_RATE}` - Target sample rate (Hz)\n- `${BITRATE}` - Audio bitrate (kbps)\n- `${REMOVE_NOISE}` - Enable noise reduction (true/false)\n- `${NORMALIZE}` - Enable normalization (true/false)\n\n### 5. voice-verification.json.template\nVoice verification captcha configuration.\n\n**Variables:**\n- `${VOICE_ID}` - Voice to verify\n- `${CAPTCHA_TYPE}` - Verification type\n- `${VERIFICATION_TEXT}` - Text to speak for verification\n\n### 6. batch-clone-config.json.template\nBatch voice cloning configuration for multiple voices.\n\n**Variables:**\n- `${VOICES}` - Array of voice configurations\n- `${COMMON_SETTINGS}` - Shared settings across all clones\n- `${OUTPUT_DIRECTORY}` - Directory for voice IDs output\n\n## Examples\n\n### instant-cloning/\nComplete example of instant voice cloning workflow.\n\n**Contents:**\n- `README.md` - Step-by-step guide for instant cloning\n- `sample-audio/` - Example audio files\n- `clone-workflow.sh` - Complete workflow script\n- `verify-clone.sh` - Clone verification script\n\n### professional-cloning/\nComplete example of professional voice cloning workflow.\n\n**Contents:**\n- `README.md` - Step-by-step guide for professional cloning\n- `sample-audio/` - Example training audio files (30+ minutes)\n- `clone-workflow.sh` - Complete workflow script\n- `training-guide.md` - Best practices for training audio\n\n### voice-library/\nVoice library browsing and management examples.\n\n**Contents:**\n- `README.md` - Voice library guide\n- `search-voices.sh` - Search and filter examples\n- `add-to-collection.sh` - Collection management\n- `share-voice.sh` - Voice sharing workflow\n\n### audio-processing/\nAudio processing pipeline examples.\n\n**Contents:**\n- `README.md` - Audio processing guide\n- `convert-formats.sh` - Format conversion examples\n- `prepare-for-cloning.sh` - Audio preparation workflow\n- `batch-process.sh` - Batch processing script\n\n### voice-settings-optimization/\nVoice settings optimization examples.\n\n**Contents:**\n- `README.md` - Settings optimization guide\n- `optimize-stability.sh` - Stability tuning\n- `optimize-similarity.sh` - Similarity tuning\n- `test-settings.sh` - Settings testing workflow\n\n## Voice Cloning Best Practices\n\n### Audio Quality Requirements\n1. **Sample Rate**: 22,050 Hz or higher recommended\n2. **Format**: MP3, WAV, FLAC, or OGG\n3. **Quality**: Clear audio with minimal background noise\n4. **Duration**:\n   - Instant: 1-5 minutes minimum\n   - Professional: 30+ minutes recommended\n5. **Content**: Natural speech, avoid music/effects\n\n### Multiple Files\n- More files improve clone quality\n- Each file should be 30 seconds to 5 minutes\n- Vary intonation and emotion across files\n- Consistent recording environment\n\n### Voice Settings Parameters\n\n**Stability (0.0-1.0)**\n- Lower values: More expressive, variable\n- Higher values: More consistent, stable\n- Default: 0.75\n- Use case: Narration (high), Dialogue (medium), Emotional (low)\n\n**Similarity Boost (0.0-1.0)**\n- Lower values: More creative, varied\n- Higher values: Closer to original voice\n- Default: 0.75\n- Use case: Clone fidelity vs. versatility trade-off\n\n**Style (0.0-1.0)**\n- Lower values: More neutral\n- Higher values: More expressive\n- Default: 0.0\n- Use case: Character voices (high), Neutral narration (low)\n\n## Voice Library Features\n\n### Discovery\n- Browse 5,000+ community voices\n- Search by tags, descriptions, categories\n- Preview voices before adding\n- Filter by language, accent, characteristics\n\n### Sharing\n- Share Professional Voice Clones only\n- Earn cash rewards when others use your voice\n- Add descriptions and tags for discoverability\n- Control sharing permissions\n\n### Collections\n- Organize voices into custom collections\n- Add/remove voices from collections\n- Share collections with team members\n- Export collection metadata\n\n## API Integration\n\nAll scripts use the ElevenLabs API with the following authentication:\n\n```bash\nexport ELEVEN_API_KEY=\"your_api_key_here\"\n```\n\nAPI endpoints used:\n- `POST /v1/voices/add` - Clone voice (instant/professional)\n- `GET /v1/voices` - List all voices\n- `GET /v1/voices/{voice_id}` - Get voice details\n- `POST /v1/voices/{voice_id}/edit` - Update voice\n- `DELETE /v1/voices/{voice_id}` - Delete voice\n- `GET /v1/voices/{voice_id}/settings` - Get voice settings\n- `POST /v1/voices/{voice_id}/settings/edit` - Update settings\n\n## Troubleshooting\n\n### Common Issues\n\n**Voice clone quality is poor**\n- Increase audio sample duration\n- Ensure clear, noise-free recordings\n- Use multiple audio files\n- Consider upgrading to professional cloning\n\n**API authentication fails**\n- Verify ELEVEN_API_KEY is set correctly\n- Check API key has necessary permissions\n- Ensure account has Creator plan or higher\n\n**Audio processing errors**\n- Verify audio file format is supported\n- Check file is not corrupted\n- Ensure ffmpeg is installed for conversions\n- Validate sample rate and bitrate\n\n**Voice not appearing in library**\n- Only Professional Voice Clones can be shared\n- Verify voice verification (captcha) is complete\n- Check voice meets library quality standards\n- Allow time for processing and review\n\n## Dependencies\n\n### Required\n- `curl` - API requests\n- `jq` - JSON parsing\n- `python3` - Audio processing scripts\n- ElevenLabs API key (Creator plan or higher)\n\n### Optional\n- `ffmpeg` - Advanced audio processing\n- `pydub` - Python audio manipulation\n- `librosa` - Audio analysis\n- `numpy` - Audio array operations\n\n## References\n\n- [ElevenLabs Voice Capabilities](https://elevenlabs.io/docs/capabilities/voices)\n- [Instant Voice Cloning Guide](https://elevenlabs.io/docs/cookbooks/voices/instant-voice-cloning)\n- [Voice API Reference](https://elevenlabs.io/docs/api-reference/voices)\n- Voice Library: https://elevenlabs.io/voice-library\n\n---\n\n**Generated for**: ElevenLabs Plugin\n**Version**: 1.0.0"
              }
            ]
          },
          {
            "name": "fastapi-backend",
            "description": "Production-ready FastAPI backend with async/await, Mem0, and deployment",
            "source": "./plugins/fastapi-backend",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Development Marketplace",
              "email": "marketplace@ai-dev.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install fastapi-backend@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-auth",
                "description": "Integrate authentication (JWT, OAuth2, Supabase) into FastAPI project",
                "path": "plugins/fastapi-backend/commands/add-auth.md",
                "frontmatter": {
                  "description": "Integrate authentication (JWT, OAuth2, Supabase) into FastAPI project",
                  "argument-hint": "auth-type"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate authentication into a FastAPI project with support for JWT, OAuth2, or Supabase authentication providers.\n\nCore Principles:\n- Detect existing project structure before implementing\n- Ask for auth type if not specified in $ARGUMENTS\n- Follow FastAPI security best practices\n- Use WebFetch to load current FastAPI security documentation\n- Provide complete, production-ready authentication implementation\n\nPhase 1: Discovery\nGoal: Understand project structure and authentication requirements\n\nActions:\n- Parse $ARGUMENTS for authentication type (jwt, oauth2, supabase)\n- If auth type not specified, use AskUserQuestion to ask:\n  - Which authentication method? (JWT, OAuth2.0, Supabase)\n  - What user storage? (PostgreSQL, MongoDB, SQLite, Supabase)\n  - Need role-based access control (RBAC)?\n  - OAuth providers if applicable? (Google, GitHub, Microsoft)\n- Detect FastAPI project structure:\n  - !{bash find . -name \"main.py\" -o -name \"app.py\" 2>/dev/null | head -5}\n  - !{bash find . -name \"requirements.txt\" -o -name \"pyproject.toml\" 2>/dev/null}\n- Load main application file for context\n\nPhase 2: Analysis\nGoal: Understand existing code patterns and architecture\n\nActions:\n- Identify current project structure:\n  - Main app file location\n  - Route organization (routers vs single file)\n  - Database setup (SQLAlchemy, raw SQL, etc.)\n  - Configuration management (.env, settings.py)\n- Check for existing authentication:\n  - !{bash grep -r \"OAuth2PasswordBearer\\|HTTPBearer\\|Depends\" --include=\"*.py\" . 2>/dev/null | head -10}\n- Determine where to place auth module:\n  - Typical: app/auth/ or src/auth/ or auth/\n\nPhase 3: Planning\nGoal: Design authentication implementation approach\n\nActions:\n- Based on selected auth type, outline implementation:\n  - **JWT**: Token generation, verification, password hashing, user model\n  - **OAuth2**: Provider integration, callback handling, token exchange\n  - **Supabase**: Client setup, auth middleware, RLS integration\n- Identify files to create/modify:\n  - Auth module (auth.py or auth/ directory)\n  - User models (if needed)\n  - Dependencies (security dependencies)\n  - Routes (login, register, logout endpoints)\n  - Configuration (secrets, OAuth credentials)\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Implement authentication with specialized agent\n\nActions:\n\nTask(description=\"Implement FastAPI authentication\", subagent_type=\"auth-specialist\", prompt=\"You are the auth-specialist agent. Implement authentication for this FastAPI project.\n\n**Authentication Type**: $ARGUMENTS\n\n**Context from Discovery**:\n- Project structure identified\n- Main app location found\n- Database configuration detected\n- User preferences gathered\n\n**Implementation Requirements**:\n- Use WebFetch to load FastAPI security documentation:\n  - https://fastapi.tiangolo.com/tutorial/security/\n  - https://fastapi.tiangolo.com/tutorial/security/oauth2-jwt/\n  - Provider-specific docs if OAuth2\n- Follow FastAPI best practices for security\n- Implement complete authentication flow:\n  - Password hashing (bcrypt or passlib)\n  - Token generation and validation\n  - Protected route dependencies\n  - User registration endpoint\n  - Login endpoint\n  - Current user retrieval\n- Add necessary dependencies to requirements.txt or pyproject.toml\n- Create .env.example with required secrets\n- Use existing code patterns and structure\n- Include error handling and validation\n- Add type hints throughout\n\n**Deliverables**:\n1. Auth module with all security utilities\n2. User model (if not exists)\n3. Authentication routes\n4. Protected route example\n5. Updated dependencies file\n6. .env.example with required variables\n7. Brief documentation of usage\")\n\nPhase 5: Validation\nGoal: Verify authentication implementation works\n\nActions:\n- Check all files were created:\n  - !{bash find . -path \"*/auth/*\" -o -name \"*auth*.py\" 2>/dev/null | grep -v __pycache__}\n- Verify dependencies added:\n  - !{bash grep -E \"python-jose|passlib|bcrypt|python-multipart|supabase\" requirements.txt pyproject.toml 2>/dev/null}\n- Check for syntax errors:\n  - !{bash python -m py_compile $(find . -name \"*auth*.py\" -not -path \"*/.venv/*\" -not -path \"*/venv/*\" 2>/dev/null) 2>&1 || echo \"Note: Install dependencies to validate\"}\n- Verify .env.example exists with required secrets\n\nPhase 6: Summary\nGoal: Document what was accomplished and next steps\n\nActions:\n- Summarize implementation:\n  - Authentication type implemented\n  - Files created/modified\n  - Dependencies added\n  - Environment variables required\n- Provide usage example:\n  - How to protect routes\n  - How to get current user\n  - How to test authentication\n- Next steps:\n  - Install dependencies: pip install -r requirements.txt\n  - Copy .env.example to .env and fill in secrets\n  - Run migrations if database changes needed\n  - Test authentication endpoints\n  - Consider adding refresh tokens\n  - Consider adding password reset flow"
              },
              {
                "name": "/add-endpoint",
                "description": "Generate new API endpoint with validation and documentation",
                "path": "plugins/fastapi-backend/commands/add-endpoint.md",
                "frontmatter": {
                  "description": "Generate new API endpoint with validation and documentation",
                  "argument-hint": "endpoint-path"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a complete FastAPI endpoint with request/response models, validation, documentation, and tests following best practices.\n\nCore Principles:\n- Understand existing patterns before generating new code\n- Follow FastAPI best practices and conventions\n- Generate complete endpoints with proper validation\n- Include comprehensive documentation and tests\n\nPhase 1: Discovery\nGoal: Gather endpoint requirements and understand project structure\n\nActions:\n- Parse $ARGUMENTS for endpoint path (e.g., \"/api/v1/users\")\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - What is the endpoint path?\n  - What HTTP method(s)? (GET, POST, PUT, DELETE, PATCH)\n  - What does this endpoint do?\n  - Request/response data structure?\n  - Authentication required?\n- Detect FastAPI project structure\n- Example: !{bash find . -name \"main.py\" -o -name \"app.py\" | head -5}\n- Locate existing routers and models\n- Example: !{bash find . -type f -name \"*.py\" | grep -E \"(router|route|api)\" | head -10}\n\nPhase 2: Analysis\nGoal: Understand existing code patterns and architecture\n\nActions:\n- Load main application file to understand structure\n- Read existing router files to understand patterns\n- Read existing model files (Pydantic schemas)\n- Identify where new endpoint should be placed\n- Check for existing authentication/authorization patterns\n- Example: !{bash grep -r \"APIRouter\\|@router\\|@app\" --include=\"*.py\" | head -20}\n\nPhase 3: Planning\nGoal: Design the endpoint implementation approach\n\nActions:\n- Outline implementation plan:\n  - Router location (new or existing file)\n  - Request/response models structure\n  - Validation requirements\n  - Error handling approach\n  - Documentation strategy\n- Identify dependencies needed\n- Present plan to user for confirmation\n\nPhase 4: Reference Documentation\nGoal: Load FastAPI best practices and patterns\n\nActions:\n- Load FastAPI documentation for reference:\n- WebFetch: https://fastapi.tiangolo.com/tutorial/path-params/\n- WebFetch: https://fastapi.tiangolo.com/tutorial/body/\n- WebFetch: https://fastapi.tiangolo.com/tutorial/response-model/\n\nPhase 5: Implementation\nGoal: Generate complete endpoint with agent\n\nActions:\n\nTask(description=\"Generate FastAPI endpoint\", subagent_type=\"endpoint-generator\", prompt=\"You are the endpoint-generator agent. Generate a complete FastAPI endpoint for $ARGUMENTS.\n\nContext:\n- Endpoint path: [from $ARGUMENTS]\n- HTTP method(s): [from requirements]\n- Purpose: [from requirements]\n- Project structure: [identified structure]\n\nRequirements:\n- Create/update router file in appropriate location\n- Generate Pydantic request model with validation\n- Generate Pydantic response model\n- Include comprehensive docstrings\n- Add proper error handling (HTTPException)\n- Include example values in schema\n- Add OpenAPI tags and metadata\n- Follow existing code patterns and conventions\n- Use proper typing annotations\n- Include input validation (constraints, regex, etc.)\n\nAuthentication:\n- [Apply auth requirements if specified]\n\nExpected output:\n- Router file with endpoint implementation\n- Model files with request/response schemas\n- Proper imports and dependencies\n- Clear inline documentation\")\n\nPhase 6: Verification\nGoal: Validate the generated endpoint\n\nActions:\n- Check generated files exist\n- Example: !{bash find . -name \"*.py\" -newer /tmp -type f}\n- Verify syntax is valid\n- Example: !{bash python -m py_compile [generated-file]}\n- Check if FastAPI can import the module\n- Run linting if configured\n- Example: !{bash which ruff && ruff check [generated-file] || echo \"Linting skipped\"}\n\nPhase 7: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize changes:\n  - Files created/modified\n  - Endpoint path and methods\n  - Request/response models\n  - Validation rules applied\n  - Authentication requirements\n- Show example usage:\n  - cURL command example\n  - Expected request/response format\n- Suggest next steps:\n  - Add unit tests\n  - Add integration tests\n  - Update API documentation\n  - Test with Swagger UI at /docs"
              },
              {
                "name": "/add-testing",
                "description": "Generate pytest test suite with fixtures for FastAPI endpoints",
                "path": "plugins/fastapi-backend/commands/add-testing.md",
                "frontmatter": {
                  "description": "Generate pytest test suite with fixtures for FastAPI endpoints",
                  "argument-hint": "endpoint-or-module-path"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate comprehensive pytest test suite with proper fixtures, mocking, and async support for FastAPI endpoints\n\nCore Principles:\n- Detect existing test patterns and follow them\n- Use pytest fixtures for dependencies\n- Generate async tests for async endpoints\n- Include both success and error test cases\n- Follow FastAPI testing best practices\n\nPhase 1: Discovery\nGoal: Understand the target endpoint and existing test structure\n\nActions:\n- Parse $ARGUMENTS to identify target (endpoint file, router, or module)\n- Detect project structure and locate test directory\n- Example: !{bash find . -type d -name \"tests\" -o -name \"test\" 2>/dev/null | head -5}\n- Check if pytest is configured: !{bash test -f pytest.ini || test -f pyproject.toml && echo \"Found\" || echo \"Not found\"}\n- Load existing test files to understand patterns\n- Identify the target endpoint file to test\n\nPhase 2: Analysis\nGoal: Analyze endpoint structure and dependencies\n\nActions:\n- Read the target endpoint file to understand:\n  - Route definitions and HTTP methods\n  - Request/response models (Pydantic)\n  - Dependencies (Depends())\n  - Database interactions\n  - Authentication/authorization requirements\n- Search for existing test utilities and fixtures\n- Example: !{bash find tests -name \"conftest.py\" -o -name \"fixtures.py\" 2>/dev/null}\n- Check for TestClient usage patterns in existing tests\n\nPhase 3: Clarification\nGoal: Gather missing requirements\n\nActions:\n- If $ARGUMENTS is unclear or target not found, use AskUserQuestion to gather:\n  - Which endpoint or module should be tested?\n  - Should tests include database fixtures?\n  - Are there authentication flows to test?\n  - Any specific edge cases to cover?\n- Confirm test file location and naming convention\n- Verify which dependencies need mocking\n\nPhase 4: Planning\nGoal: Design the test suite structure\n\nActions:\n- Plan test file structure:\n  - Test class organization\n  - Fixture requirements (TestClient, database, auth)\n  - Mock objects needed\n  - Test cases to cover (success, validation errors, auth failures)\n- Identify which FastAPI testing utilities to use\n- Outline fixture dependencies and scope\n\nPhase 5: Implementation\nGoal: Generate comprehensive test suite\n\nActions:\n\nTask(description=\"Generate pytest test suite\", subagent_type=\"test-generator\", prompt=\"You are the test-generator agent. Generate a comprehensive pytest test suite for $ARGUMENTS.\n\nWebFetch: https://fastapi.tiangolo.com/tutorial/testing/\nWebFetch: https://docs.pytest.org/en/stable/how-to/fixtures.html\n\nContext:\n- FastAPI project with pytest\n- Target: $ARGUMENTS\n- Follow async/await patterns for async endpoints\n- Use FastAPI TestClient for HTTP testing\n\nRequirements:\n- Create test file with descriptive name (test_*.py)\n- Import pytest and FastAPI TestClient\n- Define fixtures for:\n  - TestClient instance\n  - Database session (if needed)\n  - Authentication tokens (if needed)\n  - Mock dependencies\n- Write test functions covering:\n  - Happy path (successful requests)\n  - Validation errors (invalid input)\n  - Authentication/authorization failures (if applicable)\n  - Edge cases and error conditions\n- Use pytest parametrize for multiple test cases\n- Include docstrings explaining what each test validates\n- Follow naming convention: test_<method>_<endpoint>_<scenario>\n- Use proper async test syntax (@pytest.mark.asyncio) if needed\n\nExpected output:\n- Complete test file with fixtures\n- Clear test coverage for all endpoints\n- Proper mocking of dependencies\n- Follows existing project test patterns\")\n\nPhase 6: Verification\nGoal: Validate the generated tests\n\nActions:\n- Check that test file was created in correct location\n- Verify syntax with Python parser: !{bash python -m py_compile tests/test_*.py 2>&1 | head -20}\n- Run pytest collection to ensure tests are discovered: !{bash pytest --collect-only tests/ 2>&1 | tail -20}\n- Check for common issues:\n  - Missing imports\n  - Incorrect fixture usage\n  - Async/sync mismatches\n\nPhase 7: Summary\nGoal: Report test suite generation results\n\nActions:\n- Display summary:\n  - Test file location and name\n  - Number of test cases generated\n  - Fixtures created\n  - Coverage areas (success cases, errors, edge cases)\n- Provide command to run tests: `pytest tests/test_<name>.py -v`\n- Suggest next steps:\n  - Review generated tests and customize as needed\n  - Add more edge cases if required\n  - Update fixtures in conftest.py for reuse\n  - Run tests with coverage: `pytest --cov=app tests/`"
              },
              {
                "name": "/init-ai-app",
                "description": "Initialize complete AI backend with Mem0, PostgreSQL, and async SQLAlchemy",
                "path": "plugins/fastapi-backend/commands/init-ai-app.md",
                "frontmatter": {
                  "description": "Initialize complete AI backend with Mem0, PostgreSQL, and async SQLAlchemy",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create production-ready FastAPI backend with Mem0 memory, PostgreSQL, and async SQLAlchemy\n\nCore Principles:\n- Fetch latest documentation before building\n- Ask clarifying questions early\n- Validate environment before setup\n- Track progress with todos\n\nPhase 1: Architecture Detection\nGoal: Check if architecture docs exist and load backend requirements\n\nActions:\n- Create todo list using TodoWrite\n- Parse project name from $ARGUMENTS\n- Check for architecture docs: !{bash test -f docs/architecture/backend.md && echo \"spec-driven\" || echo \"interactive\"}\n\n- If spec-driven (architecture docs exist):\n  - Load backend architecture: @docs/architecture/backend.md\n  - Load data architecture: @docs/architecture/data.md\n  - Load AI architecture: @docs/architecture/ai.md\n  - Extract from architecture:\n    - API endpoints and routes (from backend.md)\n    - Database models and schema (from data.md)\n    - AI provider requirements (from ai.md)\n    - Authentication requirements (from backend.md)\n  - Display: \"üìã Building from docs/architecture/*.md\"\n  - Store architecture context for agent\n\n- If interactive (no architecture docs):\n  - Ask: \"AI provider? (OpenAI/Anthropic/Google/Multiple)\"\n  - Ask: \"Deployment target? (Vercel/Railway/Render/Docker/Local)\"\n  - Use defaults for structure\n\n- Validate Python: !{bash python3 --version}\n- Check directory: !{bash test -d \"$ARGUMENTS\" && echo \"exists\" || echo \"new\"}\n\nPhase 2: Documentation\nGoal: Fetch latest setup guides\n\nActions:\nUse WebFetch to load documentation in parallel:\n- https://docs.mem0.ai/getting-started\n- https://docs.mem0.ai/api-reference\n- https://fastapi.tiangolo.com/tutorial/sql-databases/\n- https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html\n- https://docs.pydantic.dev/latest/concepts/pydantic_settings/\n\nWait for all to complete. Update todos.\n\nPhase 3: Implementation\nGoal: Build complete FastAPI AI backend\n\nActions:\n\nTask(description=\"Build FastAPI AI backend\", subagent_type=\"fastapi-backend-builder\", prompt=\"You are the fastapi-backend-builder agent. Create a complete FastAPI backend with Mem0, PostgreSQL, and async SQLAlchemy for $ARGUMENTS.\n\nARCHITECTURE CONTEXT:\n- If docs/architecture/backend.md exists: Read and implement API endpoints, services, and routes from architecture\n- If docs/architecture/data.md exists: Read and implement database models and schema from architecture\n- If docs/architecture/ai.md exists: Read and implement AI integrations from architecture\n\nBased on architecture documentation (if available) and fetched API documentation, implement:\n\n1. Project Structure:\n   - Create $ARGUMENTS directory with src/app layout\n   - requirements.txt with: fastapi[all], uvicorn[standard], sqlalchemy[asyncio], asyncpg, pydantic-settings, mem0ai, python-dotenv, alembic, pytest\n\n2. Database (src/app/db/):\n   - database.py: async engine, AsyncSession factory\n   - models.py: Base, example User/Conversation models\n   - deps.py: get_db dependency\n\n3. Mem0 Integration (src/app/memory/):\n   - mem0_client.py: Mem0 initialization\n   - memory_manager.py: add/get/search/delete operations\n   - schemas.py: Memory Pydantic models\n\n4. API Structure (src/app/):\n   - main.py: FastAPI app, CORS, lifecycle events, health endpoint\n   - api/v1/router.py: aggregate routes\n   - api/v1/endpoints/memory.py: POST/GET/DELETE memory endpoints\n   - api/v1/endpoints/chat.py: POST /chat with memory context\n   - config.py: Pydantic Settings for env vars\n\n5. Configuration:\n   - .env.example: DATABASE_URL, MEM0_API_KEY, AI provider keys, server settings\n   - docker-compose.yml: PostgreSQL service\n   - alembic.ini: async migration config\n   - pytest.ini: test configuration\n\n6. Development Tools:\n   - Makefile: dev, migrate, test, format commands\n   - tests/conftest.py: fixtures\n   - .gitignore: Python/FastAPI standard\n\n7. Documentation:\n   - README.md: setup, quickstart, API docs, deployment\n\nFollow async/await patterns from SQLAlchemy docs. Use Pydantic v2 for all models. Include error handling and type hints.\n\nDeliverable: Complete working FastAPI backend with all files created and documented.\")\n\nPhase 4: Validation\nGoal: Verify setup is complete and functional\n\nActions:\n- Verify structure: !{bash ls -la $ARGUMENTS/src/app}\n- Check dependencies: !{bash test -f $ARGUMENTS/requirements.txt && wc -l $ARGUMENTS/requirements.txt}\n- Validate Python syntax: !{bash python3 -m py_compile $ARGUMENTS/src/app/main.py}\n- Test imports: !{bash cd $ARGUMENTS && python3 -c \"from src.app.main import app; print('OK')\"}\n- Update todos marking validation complete\n\nPhase 5: Summary\nGoal: Present setup information and next steps\n\nActions:\nDisplay:\n- Project created at $ARGUMENTS/\n- FastAPI + Mem0 + PostgreSQL + SQLAlchemy configured\n- Key files: main.py, db/, memory/, api/v1/\n\nNext Steps:\n1. cd $ARGUMENTS\n2. pip install -r requirements.txt\n3. Configure .env from .env.example\n4. Get Mem0 API key: https://mem0.ai\n5. docker-compose up -d (PostgreSQL)\n6. alembic upgrade head (migrations)\n7. uvicorn src.app.main:app --reload\n8. Visit http://localhost:8000/docs\n\nMark all todos complete.\n\nResources:\n- Mem0: https://docs.mem0.ai\n- FastAPI: https://fastapi.tiangolo.com\n- SQLAlchemy Async: https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html"
              },
              {
                "name": "/init",
                "description": "Initialize FastAPI project with modern async/await setup, dependencies, and configuration",
                "path": "plugins/fastapi-backend/commands/init.md",
                "frontmatter": {
                  "description": "Initialize FastAPI project with modern async/await setup, dependencies, and configuration",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Bootstrap a production-ready FastAPI project with async/await patterns, dependency injection, configuration management, and essential integrations.\n\nCore Principles:\n- Detect existing structure before creating new files\n- Use modern FastAPI patterns (async/await, dependency injection)\n- Follow official FastAPI documentation conventions\n- Ask user for preferences when multiple approaches are valid\n- Create comprehensive configuration and project structure\n\nPhase 1: Discovery\nGoal: Understand the target directory and gather project requirements\n\nActions:\n- Parse $ARGUMENTS for project name (default to \"fastapi-app\" if not provided)\n- Check current directory structure: !{bash pwd && ls -la}\n- Detect if FastAPI project already exists: !{bash test -f main.py -o -f app/main.py && echo \"EXISTS\" || echo \"NEW\"}\n- If unclear about scope, use AskUserQuestion to gather:\n  - What features should be included? (Auth, Database, Memory/Mem0, AI integration)\n  - What deployment target? (Uvicorn, Docker, Serverless)\n  - Which AI providers? (OpenAI, Anthropic, both)\n  - Memory layer needed? (Mem0 platform, self-hosted, none)\n\nPhase 2: Template Loading\nGoal: Load FastAPI documentation and best practices\n\nActions:\n\nLoad project documentation:\n@plugins/fastapi-backend/docs/FASTAPI-VERCEL-AI-MEM0-STACK.md\n\nLoad FastAPI references from documentation:\n- Installation guide: https://fastapi.tiangolo.com/#installation\n- First steps: https://fastapi.tiangolo.com/tutorial/first-steps/\n- Async & Concurrency: https://fastapi.tiangolo.com/async/\n- Dependencies: https://fastapi.tiangolo.com/tutorial/dependencies/\n- Settings & Environment: https://fastapi.tiangolo.com/advanced/settings/\n- Deployment: https://fastapi.tiangolo.com/deployment/\n\nPhase 3: Planning\nGoal: Design the project structure based on requirements\n\nActions:\n- Determine project structure based on user input\n- Identify required dependencies (fastapi, uvicorn, pydantic, etc.)\n- Plan directory layout (app/, config/, services/, models/, api/routes/)\n- Confirm approach with user if significant\n- Present clear implementation plan\n\nPhase 4: Implementation\nGoal: Create FastAPI project with modern setup\n\nActions:\n\nTask(description=\"Initialize FastAPI project\", subagent_type=\"fastapi-setup-agent\", prompt=\"You are the fastapi-setup-agent. Initialize a production-ready FastAPI project for $ARGUMENTS.\n\nProject Requirements:\n- Modern async/await patterns throughout\n- Pydantic Settings for configuration management\n- Dependency injection for services\n- Proper CORS configuration\n- Health check endpoint\n- Structured directory layout (app/, config/, services/, models/, api/routes/)\n- Environment variable management (.env, .env.example)\n- Requirements.txt with pinned versions\n- README.md with setup instructions\n\nReference Documentation:\n- FastAPI main docs: https://fastapi.tiangolo.com\n- Async patterns: https://fastapi.tiangolo.com/async/\n- Settings: https://fastapi.tiangolo.com/advanced/settings/\n- Dependencies: https://fastapi.tiangolo.com/tutorial/dependencies/\n- Deployment: https://fastapi.tiangolo.com/deployment/\n\nCreate the following structure:\n1. Project root configuration (requirements.txt, .env.example, README.md)\n2. app/main.py - FastAPI application with lifespan, CORS, health check\n3. app/config/settings.py - Pydantic Settings configuration\n4. app/api/deps.py - Dependency injection setup\n5. app/api/routes/ - API route modules\n6. app/services/ - Service layer implementations\n7. app/models/ - Pydantic models\n\nFollow patterns from the loaded documentation. Use modern FastAPI conventions.\n\nExpected output: Complete project structure with all files created and configured.\")\n\nPhase 5: Dependency Installation\nGoal: Set up Python environment and install dependencies\n\nActions:\n- Check if virtual environment exists: !{bash test -d venv && echo \"EXISTS\" || echo \"NONE\"}\n- If no venv, create one: !{bash python -m venv venv}\n- Install dependencies: !{bash source venv/bin/activate && pip install -r requirements.txt}\n- Verify installation: !{bash source venv/bin/activate && python -c \"import fastapi; print(f'FastAPI {fastapi.__version__} installed')\"}\n\nPhase 6: Validation\nGoal: Verify the project is properly configured\n\nActions:\n- Check all required files exist\n- Validate Python syntax: !{bash source venv/bin/activate && python -m py_compile app/main.py}\n- Test FastAPI imports: !{bash source venv/bin/activate && python -c \"from app.main import app; print('FastAPI app loads successfully')\"}\n- Verify .env.example has all required variables\n- Check README.md has setup instructions\n\nPhase 7: Summary\nGoal: Present results and next steps\n\nActions:\n- Display project structure created\n- Show installed dependencies and versions\n- Highlight configuration files to customize (.env)\n- Provide development server command: `uvicorn app.main:app --reload`\n- Suggest next steps:\n  - Copy .env.example to .env and configure\n  - Add authentication if needed\n  - Integrate AI providers (OpenAI, Anthropic)\n  - Add Mem0 memory layer\n  - Set up database connections\n  - Review FastAPI docs for advanced features"
              },
              {
                "name": "/integrate-mem0",
                "description": "Add Mem0 memory layer to FastAPI endpoints with user context and conversation history",
                "path": "plugins/fastapi-backend/commands/integrate-mem0.md",
                "frontmatter": {
                  "description": "Add Mem0 memory layer to FastAPI endpoints with user context and conversation history",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Mem0 memory layer into FastAPI backend to enable AI memory capabilities for user context, conversation history, and personalized responses.\n\nCore Principles:\n- Detect existing FastAPI project structure\n- Ask user for memory configuration preferences\n- Follow FastAPI dependency injection patterns\n- Configure Mem0 with vector store and LLM providers\n- Provide clear examples and documentation\n\nPhase 1: Discovery\n\nGoal: Understand the FastAPI project structure and existing setup\n\nActions:\n\nCheck if we're in a FastAPI project:\n!{bash test -f requirements.txt && echo \"Found requirements.txt\" || echo \"No requirements.txt\"}\n\nLoad requirements.txt to check existing dependencies:\n@requirements.txt\n\nCheck for main FastAPI application file:\n!{bash find . -name \"main.py\" -o -name \"app.py\" | head -1}\n\nDetect project structure (app/, routes/, services/):\n!{bash ls -d app 2>/dev/null || ls -d src 2>/dev/null || echo \"Root structure\"}\n\nCheck for existing Mem0 or memory-related dependencies:\n!{bash grep -E \"(mem0|memory|vector)\" requirements.txt 2>/dev/null || echo \"No memory dependencies found\"}\n\nPhase 2: Requirements Gathering\n\nGoal: Ask user about Mem0 configuration preferences\n\nActions:\n\nUse AskUserQuestion to gather:\n- Which Mem0 deployment? (Options: Hosted Platform with API key, Self-hosted with vector store)\n- Vector store preference for self-hosted? (Qdrant, Pinecone, Weaviate, ChromaDB - default Qdrant)\n- LLM provider for Mem0? (OpenAI, Anthropic - default OpenAI)\n- Do you want memory API endpoints created? (Yes/No - default Yes)\n- Do you want example chat route with memory? (Yes/No - default Yes)\n\nPhase 3: Implementation\n\nGoal: Invoke mem0-integration-agent to add memory capabilities\n\nActions:\n\nTask(description=\"Integrate Mem0 memory layer\", subagent_type=\"fastapi-backend:mem0-integration-agent\", prompt=\"You are the mem0-integration-agent. Integrate Mem0 memory capabilities into this FastAPI backend for $ARGUMENTS.\n\nProject context from Phase 1-2. Reference: docs/FASTAPI-VERCEL-AI-MEM0-STACK.md sections 646-992\n\nCore Tasks:\n1. Install Mem0 dependencies (mem0ai, vector store client if self-hosted)\n2. Create app/services/memory_service.py with MemoryClient/AsyncMemory\n3. Implement: add_conversation(), search_memories(), get_user_summary(), add_user_preference()\n4. Create app/api/routes/memory.py with endpoints: /conversation, /search, /summary, /preference\n5. Add memory dependency injection to FastAPI\n6. Create .env.example with required variables (MEM0_API_KEY or QDRANT_HOST/OPENAI_API_KEY)\n7. Add example chat integration with memory context and background tasks\n8. Include error handling and logging\n\nConfiguration:\n- Hosted: Use MemoryClient with MEM0_API_KEY\n- Self-hosted: Configure vector store (Qdrant/Pinecone), LLM provider, embedder\n\nWebFetch documentation:\n- https://docs.mem0.ai/platform/quickstart\n- https://docs.mem0.ai/open-source/overview\n- https://docs.mem0.ai/integrations\n\nDeliverable: Complete Mem0 integration with service layer, API routes, and working examples\")\n\nWait for agent to complete.\n\nPhase 4: Validation\n\nGoal: Verify Mem0 integration is correct\n\nActions:\n\nCheck that mem0ai package was added:\n!{bash grep -i mem0 requirements.txt}\n\nVerify memory service exists:\n!{bash find . -name \"memory_service.py\" | head -1}\n\nCheck memory API routes created:\n!{bash find . -path \"*/routes/memory.py\" -o -path \"*/api/memory.py\" | head -1}\n\nVerify .env.example has Mem0 variables:\n!{bash grep -E \"(MEM0|QDRANT|VECTOR)\" .env.example 2>/dev/null || echo \"Check .env configuration\"}\n\nRun Python import check:\n!{bash python -c \"import mem0; print('‚úÖ Mem0 installed')\" 2>/dev/null || echo \"‚ö†Ô∏è Run: pip install -r requirements.txt\"}\n\nPhase 5: Summary and Next Steps\n\nGoal: Guide user through setup and usage\n\nActions:\n\nDisplay what was added:\n- Memory service layer (app/services/memory_service.py)\n- Memory API routes (app/api/routes/memory.py)\n- Environment configuration (.env.example)\n- Updated requirements.txt with Mem0 dependencies\n\nAPI Endpoints created:\n- POST /api/v1/memory/conversation - Add conversation\n- POST /api/v1/memory/search - Search memories\n- GET /api/v1/memory/summary - Get user summary\n- POST /api/v1/memory/preference - Add preference\n- DELETE /api/v1/memory/user/{user_id} - Delete memories\n\nSetup instructions:\n1. Copy .env.example to .env and add API keys\n2. Install: pip install -r requirements.txt\n3. For self-hosted: docker run -p 6333:6333 qdrant/qdrant\n4. Test at /api/v1/docs\n\nEnvironment variables (based on deployment):\n- Hosted: MEM0_API_KEY, MEM0_HOST (optional)\n- Self-hosted: QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY, OPENAI_API_KEY\n\nYou can now:\n- Track conversation history per user\n- Search memories with semantic search\n- Store and retrieve user preferences\n- Build memory-enhanced AI experiences"
              },
              {
                "name": "/search-examples",
                "description": "Search and add FastAPI examples/patterns to your project",
                "path": "plugins/fastapi-backend/commands/search-examples.md",
                "frontmatter": {
                  "description": "Search and add FastAPI examples/patterns to your project",
                  "argument-hint": "<topic>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Search FastAPI documentation and examples for specific patterns/topics and provide code examples that can be added to the project.\n\nCore Principles:\n- Fetch examples from official FastAPI documentation\n- Provide production-ready code patterns\n- Include multiple implementation options when available\n- Show best practices and common use cases\n\nPhase 1: Parse Request\nGoal: Understand what examples are being requested\n\nActions:\n- Parse $ARGUMENTS for the topic/pattern to search\n- Identify search category (routing, database, auth, middleware, testing, etc.)\n- Set search scope based on topic keywords\n\nExample topics:\n- \"authentication\" ‚Üí OAuth2, JWT, API keys\n- \"database\" ‚Üí SQLAlchemy, async ORM, migrations\n- \"validation\" ‚Üí Pydantic models, request validation\n- \"middleware\" ‚Üí CORS, authentication, logging\n- \"websockets\" ‚Üí WebSocket endpoints, broadcasting\n- \"testing\" ‚Üí pytest, async testing, fixtures\n- \"deployment\" ‚Üí Docker, uvicorn, production config\n- \"background\" ‚Üí background tasks, Celery\n- \"file upload\" ‚Üí file handling, multipart forms\n- \"dependencies\" ‚Üí dependency injection patterns\n\nPhase 2: Load Documentation\nGoal: Fetch relevant FastAPI documentation and examples\n\nActions:\n- Based on topic, load appropriate documentation sections\n- Fetch official FastAPI examples from GitHub\n- Load best practice guides\n\n**Authentication & Security Examples:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/security/\nWebFetch: https://fastapi.tiangolo.com/tutorial/security/oauth2-jwt/\n\n**Database Integration Examples:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/sql-databases/\nWebFetch: https://fastapi.tiangolo.com/advanced/async-sql-databases/\n\n**Request Validation & Pydantic:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/body/\nWebFetch: https://fastapi.tiangolo.com/tutorial/body-multiple-params/\n\n**Middleware & CORS:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/cors/\nWebFetch: https://fastapi.tiangolo.com/advanced/middleware/\n\n**WebSocket Examples:**\nWebFetch: https://fastapi.tiangolo.com/advanced/websockets/\n\n**Background Tasks:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/background-tasks/\n\n**Testing Examples:**\nWebFetch: https://fastapi.tiangolo.com/tutorial/testing/\n\n**Deployment Guides:**\nWebFetch: https://fastapi.tiangolo.com/deployment/docker/\nWebFetch: https://fastapi.tiangolo.com/deployment/server-workers/\n\nPhase 3: Find Matching Examples\nGoal: Identify and extract relevant code examples\n\nActions:\n- Review fetched documentation for topic match\n- Extract complete, runnable code examples\n- Identify dependencies and imports needed\n- Note any configuration requirements\n\nPhase 4: Analyze Project Context\nGoal: Understand how to integrate examples into current project\n\nActions:\n- Check if project exists in current directory:\n  !{bash test -f main.py && echo \"FastAPI project found\" || echo \"No main.py found\"}\n- Look for existing patterns:\n  !{bash test -d app && ls app/*.py 2>/dev/null || echo \"No app directory\"}\n- Check for requirements.txt or pyproject.toml:\n  !{bash ls requirements.txt pyproject.toml 2>/dev/null}\n- Identify project structure (flat vs app directory)\n\nPhase 5: Present Examples\nGoal: Display examples with integration guidance\n\nActions:\n- Show 2-3 most relevant code examples for the topic\n- Include complete code with imports\n- Explain each example's use case\n- List required dependencies\n- Provide integration steps:\n  1. Required pip packages\n  2. Where to add the code (file structure)\n  3. Configuration needed\n  4. How to test the implementation\n\n**Output Format for Each Topic:**\n\nTopic: $ARGUMENTS\n\nExample 1: [Pattern Name]\n- Use case: [When to use this pattern]\n- Dependencies: [pip packages needed]\n- Complete code example with imports\n- Integration: Add to app/[module].py\n- Install: pip install [packages]\n- Configure: [any settings needed]\n\nExample 2: [Alternative Pattern]\n- Repeat format above\n\nBest Practices:\n- [Key consideration 1]\n- [Key consideration 2]\n- [Key consideration 3]\n\nNext Steps:\n- Review examples and choose appropriate pattern\n- Install dependencies: pip install [packages]\n- Implement in your project structure\n- Run tests to verify\n\nPhase 6: Summary\nGoal: Provide clear next steps\n\nActions:\n- Summarize examples provided\n- List all dependencies needed\n- Suggest which example to start with based on project context\n- Offer to implement the chosen example if user requests\n- Reference official docs for deeper dive"
              },
              {
                "name": "/setup-database",
                "description": "Configure async SQLAlchemy with PostgreSQL/Supabase",
                "path": "plugins/fastapi-backend/commands/setup-database.md",
                "frontmatter": {
                  "description": "Configure async SQLAlchemy with PostgreSQL/Supabase",
                  "argument-hint": "<database-type>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up async SQLAlchemy database configuration with PostgreSQL or Supabase, including models, sessions, and migrations.\n\nCore Principles:\n- Detect existing project structure before creating files\n- Use async/await patterns for all database operations\n- Follow SQLAlchemy 2.0+ best practices\n- Configure proper connection pooling and session management\n\nPhase 1: Discovery\nGoal: Understand project structure and database requirements\n\nActions:\n- Parse $ARGUMENTS to determine database type (postgresql/supabase)\n- If unclear, use AskUserQuestion to gather:\n  - What database are you using? (PostgreSQL, Supabase)\n  - Do you need authentication tables?\n  - Any specific models to create?\n- Detect existing FastAPI project structure\n- Check for existing database configuration\n- Example: !{bash ls -la src/ app/ 2>/dev/null | head -20}\n\nPhase 2: Context Loading\nGoal: Load relevant FastAPI files and understand current setup\n\nActions:\n- Find main application file\n- Example: !{bash find . -name \"main.py\" -o -name \"app.py\" 2>/dev/null | head -5}\n- Check for existing requirements/dependencies\n- Example: @requirements.txt or @pyproject.toml\n- Identify where database code should live\n\nPhase 3: Requirements Validation\nGoal: Confirm database setup approach\n\nActions:\n- Present proposed structure to user:\n  - Database URL configuration (.env)\n  - Models directory structure\n  - Session management (dependency injection)\n  - Alembic migrations setup\n- Get user confirmation before proceeding\n\nPhase 4: Implementation\nGoal: Create complete async database setup\n\nActions:\n\nTask(description=\"Setup async SQLAlchemy database\", subagent_type=\"database-architect-agent\", prompt=\"You are the database-architect-agent. Configure async SQLAlchemy with $ARGUMENTS database.\n\nContext:\n- FastAPI project structure detected\n- Database type: $ARGUMENTS\n- Need async/await patterns throughout\n- SQLAlchemy 2.0+ syntax required\n\nRequirements:\n- Create database configuration module with async engine and session\n- Set up proper connection pooling (pool_pre_ping, pool_size, max_overflow)\n- Create base model class with common fields (id, created_at, updated_at)\n- Configure session dependency for FastAPI dependency injection\n- Set up Alembic for migrations\n- Create .env.example with database URL template\n- Add required dependencies to requirements.txt or pyproject.toml\n- Include example model demonstrating relationships and async queries\n\nFor Supabase:\n- Configure PostgREST compatibility\n- Include Row Level Security (RLS) considerations\n- Document Supabase-specific setup steps\n\nFor PostgreSQL:\n- Standard asyncpg driver configuration\n- Connection string format\n\nReference FastAPI SQL database documentation:\n- WebFetch: https://fastapi.tiangolo.com/tutorial/sql-databases/\n- WebFetch: https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html\n\nExpected output:\n- src/database.py or app/database.py (engine, session, dependency)\n- src/models/base.py (base model class)\n- src/models/__init__.py (model exports)\n- src/models/example.py (example model)\n- alembic.ini and alembic/ directory (migrations)\n- .env.example (database URL template)\n- Updated dependencies file\n- Brief setup instructions\")\n\nPhase 5: Verification\nGoal: Validate the database setup\n\nActions:\n- Check all required files were created\n- Verify imports are correct\n- Test database connection (optional)\n- Example: !{bash python -m pip list | grep -i sqlalchemy}\n- Review generated code for async/await consistency\n\nPhase 6: Summary\nGoal: Provide setup instructions and next steps\n\nActions:\n- List all files created\n- Provide database setup instructions:\n  - Copy .env.example to .env\n  - Update DATABASE_URL with credentials\n  - Run migrations: alembic upgrade head\n  - Create first migration: alembic revision --autogenerate -m \"Initial\"\n- Highlight key patterns:\n  - How to create new models\n  - How to use database session in routes\n  - How to run migrations\n- Suggest next steps:\n  - Create specific models for your domain\n  - Set up database seeding\n  - Add database testing utilities"
              },
              {
                "name": "/setup-deployment",
                "description": "Configure deployment for FastAPI (Docker, Railway, DigitalOcean)",
                "path": "plugins/fastapi-backend/commands/setup-deployment.md",
                "frontmatter": {
                  "description": "Configure deployment for FastAPI (Docker, Railway, DigitalOcean)",
                  "argument-hint": "<platform> (docker|railway|digitalocean|all)"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure production deployment for FastAPI backend with Docker and platform-specific configs.\n\nCore Principles:\n- Detect FastAPI structure before configuring\n- Ask about deployment requirements\n- Use official FastAPI deployment best practices\n- Support multiple platforms with optimized configurations\n\nPhase 1: Discovery\n\nActions:\n\nDetect FastAPI structure:\n!{bash if [ -f \"main.py\" ] || [ -f \"app/main.py\" ]; then echo \"FastAPI detected\"; else echo \"No main.py\"; fi}\n\nCheck existing config:\n!{bash ls Dockerfile docker-compose.yml requirements.txt 2>/dev/null || echo \"No config\"}\n\nLoad project files:\n@requirements.txt\n\nGet Python version:\n!{bash python3 --version 2>/dev/null || python --version}\n\nParse $ARGUMENTS for platform. If empty, ask user.\n\nPhase 2: Requirements\n\nActions:\n\nAskUserQuestion to gather:\n- Platform? (docker, railway, digitalocean, all)\n- Database? (postgresql, mysql, none)\n- Redis needed? (yes/no)\n- CORS origins? (domains or *)\n- Health checks? (yes/no)\n\nPhase 3: Configuration\n\nActions:\n\nTask(description=\"Configure FastAPI deployment\", subagent_type=\"fastapi-backend:deployment-architect-agent\", prompt=\"You are the deployment-architect-agent. Configure production deployment for this FastAPI backend.\n\nProject: [from Phase 1]\nPlatform: $ARGUMENTS or [from AskUserQuestion]\nDatabase: [from user]\nRedis: [from user]\nCORS: [from user]\nHealth checks: [from user]\n\nTasks:\n1. Dockerfile (multi-stage, slim base, non-root, Gunicorn+Uvicorn)\n2. docker-compose.yml (FastAPI, DB, Redis if needed)\n3. .dockerignore (exclude __pycache__, .venv, .git, tests)\n4. railway.json (if Railway)\n5. .env.example (all variables with comments)\n6. config/production.py (HTTPS, CORS, security headers)\n7. /health endpoint (if requested)\n8. docs/deployment.md (platform instructions)\n\nReference docs:\n- https://fastapi.tiangolo.com/deployment/\n- https://docs.docker.com/develop/dev-best-practices/\n- https://docs.gunicorn.org/en/stable/\n- https://docs.railway.app/\n- https://docs.digitalocean.com/products/app-platform/\n\nDeliverable: Complete deployment files\")\n\nWait for agent to complete.\n\nPhase 4: Validation\n\nActions:\n\nCheck files created:\n!{bash ls Dockerfile .dockerignore 2>/dev/null | wc -l}\n\nVerify env template:\n!{bash test -f .env.example && echo \"Created\" || echo \"Missing\"}\n\nCheck docs:\n!{bash test -f docs/deployment.md && echo \"Created\" || echo \"Missing\"}\n\nPhase 5: Summary\n\nActions:\n\nDisplay summary:\n- Platform: [platform]\n- Files: Dockerfile, docker-compose.yml, .env.example, docs/deployment.md\n- Next steps:\n  1. Copy .env.example to .env\n  2. Set DATABASE_URL and SECRET_KEY\n  3. Configure CORS origins\n\nDocker: docker-compose up --build\nRailway: railway login && railway up\nDigitalOcean: doctl apps create --spec\n\nSecurity: Strong SECRET_KEY, HTTPS, rate limiting, monitoring"
              },
              {
                "name": "/validate-api",
                "description": "Validate API schema, endpoints, and security",
                "path": "plugins/fastapi-backend/commands/validate-api.md",
                "frontmatter": {
                  "description": "Validate API schema, endpoints, and security",
                  "argument-hint": [
                    "api-directory"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Validate FastAPI schema, endpoints, and security using parallel validation agents.\n\nCore Principles:\n- Detect API structure and configuration\n- Fetch FastAPI OpenAPI/security docs\n- Run parallel validation: schema, endpoints, security\n- Provide actionable validation report\n\nPhase 1: Discovery\nGoal: Understand API structure\n\nActions:\n- Parse $ARGUMENTS for API directory (default: current directory)\n- Verify directory exists: !{bash test -d \"$ARGUMENTS\" && echo \"Found\" || echo \"Not found\"}\n- If not found: Use current directory !{bash pwd}\n- Detect FastAPI files: !{bash find \"$ARGUMENTS\" -name \"main.py\" -o -name \"app.py\" -type f 2>/dev/null | head -5}\n- Load key files: @$ARGUMENTS/main.py or @$ARGUMENTS/app.py\n- Check for requirements.txt or pyproject.toml\n- Create validation todo list\n\nPhase 2: Fetch Documentation\nGoal: Get FastAPI validation docs\n\nActions:\nFetch these docs in parallel:\n\n1. WebFetch: https://fastapi.tiangolo.com/reference/openapi/\n2. WebFetch: https://fastapi.tiangolo.com/advanced/security/\n3. WebFetch: https://fastapi.tiangolo.com/tutorial/metadata/\n\nPhase 3: Parallel Validation\nGoal: Run independent validation checks simultaneously\n\nActions:\n\nLaunch three validation agents in parallel:\n\nTask(description=\"Validate API schema\", subagent_type=\"general-purpose\", prompt=\"You are a schema validation specialist. Validate the FastAPI schema for $ARGUMENTS.\n\nCheck:\n- OpenAPI schema generation (app.openapi())\n- Response models use Pydantic properly\n- Request validation schemas defined\n- Field validators and constraints\n- Enum usage for fixed values\n- Optional vs Required fields correct\n\nUse fetched OpenAPI documentation.\n\nDeliverable: Schema validation report with issues found.\")\n\nTask(description=\"Validate API endpoints\", subagent_type=\"general-purpose\", prompt=\"You are an endpoint validation specialist. Validate FastAPI endpoints for $ARGUMENTS.\n\nCheck:\n- Route naming conventions (REST best practices)\n- HTTP methods match operations (GET/POST/PUT/DELETE/PATCH)\n- Path parameters properly typed\n- Query parameters with defaults\n- Status codes appropriate for operations\n- Error responses defined\n- Dependency injection usage\n- CORS configuration if needed\n\nDeliverable: Endpoint validation report with issues found.\")\n\nTask(description=\"Validate API security\", subagent_type=\"general-purpose\", prompt=\"You are a security validation specialist. Validate FastAPI security for $ARGUMENTS.\n\nCheck:\n- Authentication schemes defined (OAuth2, API key, JWT)\n- Security dependencies applied to protected routes\n- HTTPS/SSL configuration recommendations\n- CORS properly restricted\n- Rate limiting considerations\n- SQL injection protection (ORM usage)\n- Input sanitization\n- Secret management (no hardcoded keys)\n- Environment variables for sensitive data\n\nUse fetched security documentation.\n\nDeliverable: Security validation report with issues and recommendations.\")\n\nWait for all three agents to complete before proceeding.\n\nPhase 4: Verification\nGoal: Run automated checks\n\nActions:\n- Python syntax check: !{bash cd \"$ARGUMENTS\" && python -m py_compile *.py 2>&1}\n- Generate OpenAPI schema: !{bash cd \"$ARGUMENTS\" && python -c \"from main import app; import json; print(json.dumps(app.openapi(), indent=2))\" 2>&1 | head -50}\n- Check dependencies installed: !{bash test -f \"$ARGUMENTS/requirements.txt\" && pip freeze | grep -f \"$ARGUMENTS/requirements.txt\" || echo \"No requirements.txt\"}\n- Mark verification complete\n\nPhase 5: Consolidated Report\nGoal: Combine all validation results\n\nActions:\n- Aggregate findings from all three agents\n- Write VALIDATION-REPORT.md with sections:\n  * Schema Validation Results\n  * Endpoint Validation Results\n  * Security Validation Results\n  * Automated Checks\n  * Priority Issues (High/Medium/Low)\n  * Recommendations\n  * Next Steps\n\n- Display: @VALIDATION-REPORT.md\n\n- Status summary:\n  * All passed: \"‚úÖ API Validation PASSED\"\n  * Minor issues: \"‚ö†Ô∏è Validation passed with warnings\"\n  * Critical issues: \"‚ùå Validation FAILED - Fix critical issues\"\n\n- Mark all todos complete\n\nImportant Notes:\n- Three agents run in parallel for speed\n- Adapts to main.py or app.py entry points\n- Uses FastAPI official docs for validation rules\n- Produces actionable report with prioritized fixes\n- Checks both code quality and security\n\n## Available Skills\n\nThis commands has access to the following skills from the fastapi-backend plugin:\n\n- **async-sqlalchemy-patterns**: Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.\n- **fastapi-api-patterns**: REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.\n- **fastapi-auth-patterns**: Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.\n- **fastapi-deployment-config**: Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.\n- **fastapi-project-structure**: Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.\n- **mem0-fastapi-integration**: Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Usage\n\n/fastapi-backend:validate-api\n/fastapi-backend:validate-api ./backend\n/fastapi-backend:validate-api /path/to/api"
              }
            ],
            "skills": [
              {
                "name": "async-sqlalchemy-patterns",
                "description": "Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.",
                "path": "plugins/fastapi-backend/skills/async-sqlalchemy-patterns/SKILL.md",
                "frontmatter": {
                  "name": "async-sqlalchemy-patterns",
                  "description": "Async SQLAlchemy 2.0+ database patterns for FastAPI including session management, connection pooling, Alembic migrations, relationship loading strategies, and query optimization. Use when implementing database models, configuring async sessions, setting up migrations, optimizing queries, managing relationships, or when user mentions SQLAlchemy, async database, ORM, Alembic, database performance, or connection pooling.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Async SQLAlchemy Patterns\n\n**Purpose:** Implement production-ready async SQLAlchemy 2.0+ patterns in FastAPI with proper session management, migrations, and performance optimization.\n\n**Activation Triggers:**\n- Database model implementation\n- Async session configuration\n- Alembic migration setup\n- Query performance optimization\n- Relationship loading issues\n- Connection pool configuration\n- Transaction management\n- Database schema migrations\n\n**Key Resources:**\n- `scripts/setup-alembic.sh` - Initialize Alembic with async support\n- `scripts/generate-migration.sh` - Create migrations from model changes\n- `templates/base_model.py` - Base model with common patterns\n- `templates/session_manager.py` - Async session factory and dependency\n- `templates/alembic.ini` - Alembic configuration for async\n- `examples/user_model.py` - Complete model with relationships\n- `examples/async_context_examples.py` - Session usage patterns\n\n## Core Patterns\n\n### 1. Async Engine and Session Setup\n\n**Database Configuration:**\n\n```python\n# app/core/database.py\nfrom sqlalchemy.ext.asyncio import (\n    AsyncSession,\n    create_async_engine,\n    async_sessionmaker\n)\nfrom sqlalchemy.orm import declarative_base\nfrom app.core.config import settings\n\n# Create async engine\nengine = create_async_engine(\n    settings.DATABASE_URL,\n    echo=settings.DEBUG,\n    pool_pre_ping=True,  # Verify connections before using\n    pool_size=5,         # Base number of connections\n    max_overflow=10,     # Additional connections when pool is full\n    pool_recycle=3600,   # Recycle connections after 1 hour\n    pool_timeout=30,     # Wait 30s for available connection\n)\n\n# Create async session factory\nAsyncSessionLocal = async_sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False,  # Keep objects usable after commit\n    autoflush=False,         # Manual flush control\n    autocommit=False,        # Explicit commits only\n)\n\nBase = declarative_base()\n```\n\n**Dependency Injection Pattern:**\n\n```python\n# app/core/deps.py\nfrom typing import AsyncGenerator\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.core.database import AsyncSessionLocal\n\nasync def get_db() -> AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    FastAPI dependency for database sessions.\n    Automatically handles cleanup.\n    \"\"\"\n    async with AsyncSessionLocal() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n```\n\n### 2. Base Model Pattern\n\nUse template from `templates/base_model.py` with:\n- UUID primary keys by default\n- Automatic created_at/updated_at timestamps\n- Soft delete support\n- Common query methods\n\n**Essential Mixins:**\n\n```python\nfrom datetime import datetime\nfrom sqlalchemy import DateTime, Boolean, func\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass TimestampMixin:\n    \"\"\"Auto-managed timestamps\"\"\"\n    created_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now(),\n        nullable=False\n    )\n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime(timezone=True),\n        server_default=func.now(),\n        onupdate=func.now(),\n        nullable=False\n    )\n\nclass SoftDeleteMixin:\n    \"\"\"Soft delete support\"\"\"\n    is_deleted: Mapped[bool] = mapped_column(\n        Boolean,\n        default=False,\n        nullable=False\n    )\n    deleted_at: Mapped[datetime | None] = mapped_column(\n        DateTime(timezone=True),\n        nullable=True\n    )\n```\n\n### 3. Relationship Loading Strategies\n\n**Lazy Loading (Default - N+1 Problem Risk):**\n```python\n# Bad: Causes N+1 queries\nusers = await session.execute(select(User))\nfor user in users.scalars():\n    print(user.posts)  # Separate query per user!\n```\n\n**Eager Loading (Recommended):**\n\n```python\nfrom sqlalchemy.orm import selectinload, joinedload\n\n# selectinload: Separate query, good for one-to-many\nstmt = select(User).options(selectinload(User.posts))\nresult = await session.execute(stmt)\nusers = result.scalars().all()\n\n# joinedload: SQL JOIN, good for many-to-one\nstmt = select(Post).options(joinedload(Post.author))\nresult = await session.execute(stmt)\nposts = result.scalars().unique().all()  # unique() required with joins!\n\n# subqueryload: Subquery loading\nstmt = select(User).options(subqueryload(User.posts))\n```\n\n**Relationship Configuration:**\n\n```python\nfrom sqlalchemy.orm import relationship\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    # One-to-many with cascade\n    posts: Mapped[list[\"Post\"]] = relationship(\n        back_populates=\"author\",\n        cascade=\"all, delete-orphan\",\n        lazy=\"selectin\"  # Default eager loading\n    )\n\n    # Many-to-many\n    roles: Mapped[list[\"Role\"]] = relationship(\n        secondary=\"user_roles\",\n        back_populates=\"users\",\n        lazy=\"selectin\"\n    )\n```\n\n### 4. Query Patterns\n\n**Basic CRUD:**\n\n```python\nfrom sqlalchemy import select, update, delete\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n# Create\nasync def create_user(session: AsyncSession, user_data: dict):\n    user = User(**user_data)\n    session.add(user)\n    await session.commit()\n    await session.refresh(user)\n    return user\n\n# Read with filters\nasync def get_users(session: AsyncSession, skip: int = 0, limit: int = 100):\n    stmt = (\n        select(User)\n        .where(User.is_deleted == False)\n        .offset(skip)\n        .limit(limit)\n        .order_by(User.created_at.desc())\n    )\n    result = await session.execute(stmt)\n    return result.scalars().all()\n\n# Update\nasync def update_user(session: AsyncSession, user_id: int, updates: dict):\n    stmt = (\n        update(User)\n        .where(User.id == user_id)\n        .values(**updates)\n        .returning(User)\n    )\n    result = await session.execute(stmt)\n    await session.commit()\n    return result.scalar_one()\n\n# Delete\nasync def delete_user(session: AsyncSession, user_id: int):\n    stmt = delete(User).where(User.id == user_id)\n    await session.execute(stmt)\n    await session.commit()\n```\n\n**Complex Queries:**\n\n```python\nfrom sqlalchemy import func, and_, or_\n\n# Aggregations\nstmt = (\n    select(User.id, func.count(Post.id))\n    .join(Post)\n    .group_by(User.id)\n    .having(func.count(Post.id) > 5)\n)\n\n# Subqueries\nsubq = (\n    select(func.avg(Post.views))\n    .where(Post.user_id == User.id)\n    .scalar_subquery()\n)\nstmt = select(User).where(User.id.in_(\n    select(Post.user_id).where(Post.views > subq)\n))\n\n# Window functions\nfrom sqlalchemy import over\nstmt = select(\n    Post,\n    func.row_number().over(\n        partition_by=Post.user_id,\n        order_by=Post.created_at.desc()\n    ).label('row_num')\n).where(over.row_num <= 10)\n```\n\n### 5. Transaction Management\n\n**Nested Transactions:**\n\n```python\nasync def transfer_funds(\n    session: AsyncSession,\n    from_account: int,\n    to_account: int,\n    amount: float\n):\n    async with session.begin_nested():  # Savepoint\n        # Debit\n        stmt = (\n            update(Account)\n            .where(Account.id == from_account)\n            .where(Account.balance >= amount)\n            .values(balance=Account.balance - amount)\n        )\n        result = await session.execute(stmt)\n        if result.rowcount == 0:\n            raise ValueError(\"Insufficient funds\")\n\n        # Credit\n        stmt = (\n            update(Account)\n            .where(Account.id == to_account)\n            .values(balance=Account.balance + amount)\n        )\n        await session.execute(stmt)\n\n    await session.commit()\n```\n\n**Manual Transaction Control:**\n\n```python\nasync def batch_operation(session: AsyncSession, items: list):\n    try:\n        for item in items:\n            session.add(item)\n            if len(session.new) >= 100:\n                await session.flush()  # Flush but don't commit\n\n        await session.commit()\n    except Exception as e:\n        await session.rollback()\n        raise\n```\n\n### 6. Alembic Migration Setup\n\n```bash\n# Initialize Alembic with async template\n./scripts/setup-alembic.sh\n\n# Creates:\n# - alembic/ directory\n# - alembic.ini configured for async\n# - env.py with async support\n```\n\n**Generate Migrations:**\n\n```bash\n# Auto-generate from model changes\n./scripts/generate-migration.sh \"add user table\"\n\n# Review generated migration in alembic/versions/\n# Always review auto-generated migrations!\n```\n\n**Migration Best Practices:**\n\n1. **Always review auto-generated migrations**\n2. **Use batch operations for large tables**\n3. **Add indexes in separate migrations**\n4. **Include both upgrade and downgrade**\n5. **Test migrations on staging first**\n\n**Manual Migration Template:**\n\n```python\n# alembic/versions/xxx_add_index.py\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade() -> None:\n    # Use batch for SQLite compatibility\n    with op.batch_alter_table('users') as batch_op:\n        batch_op.create_index(\n            'ix_users_email',\n            ['email'],\n            unique=True\n        )\n\ndef downgrade() -> None:\n    with op.batch_alter_table('users') as batch_op:\n        batch_op.drop_index('ix_users_email')\n```\n\n### 7. Connection Pool Configuration\n\n**Production Settings:**\n\n```python\n# For web applications\nengine = create_async_engine(\n    DATABASE_URL,\n    pool_size=20,           # Concurrent requests\n    max_overflow=40,        # Burst capacity\n    pool_recycle=3600,      # 1 hour\n    pool_pre_ping=True,     # Verify connections\n    pool_timeout=30,        # Wait time\n    echo_pool=False,        # Pool debug logging\n)\n\n# For background tasks\nengine = create_async_engine(\n    DATABASE_URL,\n    pool_size=5,            # Fewer connections\n    max_overflow=10,\n    pool_recycle=7200,      # 2 hours\n)\n```\n\n**Connection Health Check:**\n\n```python\nfrom sqlalchemy import text\n\nasync def check_database_health(session: AsyncSession) -> bool:\n    try:\n        await session.execute(text(\"SELECT 1\"))\n        return True\n    except Exception:\n        return False\n```\n\n### 8. Performance Optimization\n\n**Bulk Operations:**\n\n```python\n# Bulk insert\nasync def bulk_create_users(session: AsyncSession, users: list[dict]):\n    stmt = insert(User).values(users)\n    await session.execute(stmt)\n    await session.commit()\n\n# Bulk update\nasync def bulk_update_status(session: AsyncSession, user_ids: list[int]):\n    stmt = (\n        update(User)\n        .where(User.id.in_(user_ids))\n        .values(status=\"active\")\n    )\n    await session.execute(stmt)\n    await session.commit()\n```\n\n**Query Result Streaming:**\n\n```python\nasync def stream_large_dataset(session: AsyncSession):\n    stmt = select(User).execution_options(yield_per=100)\n    result = await session.stream(stmt)\n\n    async for partition in result.partitions(100):\n        users = partition.scalars().all()\n        # Process batch\n        yield users\n```\n\n**Index Optimization:**\n\n```python\nfrom sqlalchemy import Index\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    email: Mapped[str] = mapped_column(unique=True, index=True)\n\n    __table_args__ = (\n        Index('ix_user_status_created', 'status', 'created_at'),\n        Index('ix_user_email_active', 'email', postgresql_where=~is_deleted),\n    )\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**\"Object is not bound to session\":**\n```python\n# Bad: Object expires after commit\nuser = await create_user(session, data)\nprint(user.email)  # Error if expire_on_commit=True\n\n# Fix: Refresh or configure session\nawait session.refresh(user)\n# Or: AsyncSessionLocal(expire_on_commit=False)\n```\n\n**\"N+1 Query Problem\":**\n```python\n# Enable SQL logging to detect\nengine = create_async_engine(url, echo=True)\n\n# Fix with eager loading\nstmt = select(User).options(selectinload(User.posts))\n```\n\n**\"DetachedInstanceError\":**\n```python\n# Bad: Accessing relationship outside session\nasync with AsyncSessionLocal() as session:\n    user = await session.get(User, user_id)\nprint(user.posts)  # Error!\n\n# Fix: Load within session or use eager loading\nstmt = select(User).options(selectinload(User.posts))\n```\n\n**\"Connection Pool Exhausted\":**\n```python\n# Increase pool size or add timeout\nengine = create_async_engine(\n    url,\n    pool_size=20,\n    max_overflow=40,\n    pool_timeout=30\n)\n```\n\n## Resources\n\n**Scripts:** `scripts/` directory contains:\n- `setup-alembic.sh` - Initialize Alembic with async configuration\n- `generate-migration.sh` - Create migrations from model changes\n- `test-connection.sh` - Test database connectivity\n- `optimize-queries.sh` - Analyze and suggest query optimizations\n\n**Templates:** `templates/` directory includes:\n- `base_model.py` - Base model with UUID, timestamps, soft delete\n- `session_manager.py` - Complete session factory and dependencies\n- `alembic.ini` - Production-ready Alembic configuration\n- `env.py` - Alembic async environment template\n\n**Examples:** `examples/` directory provides:\n- `user_model.py` - Full model with relationships and indexes\n- `async_context_examples.py` - Session patterns and best practices\n- `query_patterns.py` - Common query optimization examples\n- `migration_examples.py` - Manual migration patterns\n\n---\n\n**SQLAlchemy Version:** 2.0+\n**Database Support:** PostgreSQL, MySQL, SQLite (async drivers)\n**Async Drivers:** asyncpg (PostgreSQL), aiomysql (MySQL), aiosqlite (SQLite)\n**Version:** 1.0.0"
              },
              {
                "name": "fastapi-api-patterns",
                "description": "REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.",
                "path": "plugins/fastapi-backend/skills/fastapi-api-patterns/SKILL.md",
                "frontmatter": {
                  "name": "fastapi-api-patterns",
                  "description": "REST API design and implementation patterns for FastAPI endpoints including CRUD operations, pagination, filtering, error handling, and request/response models. Use when building FastAPI endpoints, creating REST APIs, implementing CRUD operations, adding pagination, designing API routes, handling API errors, or when user mentions FastAPI patterns, REST API design, endpoint structure, API best practices, or HTTP endpoints.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# fastapi-api-patterns\n\n## Instructions\n\nThis skill provides comprehensive REST API design patterns and implementation templates for FastAPI applications. It covers CRUD operations, pagination, filtering, request/response models, error handling, and API organization following modern best practices.\n\n### 1. CRUD Endpoint Patterns\n\nCreate, Read, Update, Delete endpoints using FastAPI routers:\n\n```bash\n# Use CRUD template to generate complete endpoint set\ncp ./skills/fastapi-api-patterns/templates/crud_endpoint.py app/routers/items.py\n\n# Customize for your model\n# - Replace Item model with your Pydantic model\n# - Update database operations\n# - Add authentication dependencies\n```\n\n**What This Provides:**\n- `POST /items/` - Create new item\n- `GET /items/{item_id}` - Read single item by ID\n- `GET /items/` - Read multiple items with pagination\n- `PUT /items/{item_id}` - Update entire item\n- `PATCH /items/{item_id}` - Partial update\n- `DELETE /items/{item_id}` - Delete item\n\n**Router Structure:**\n```python\nfrom fastapi import APIRouter, HTTPException, Depends, status\nfrom typing import List\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    responses={404: {\"description\": \"Not found\"}},\n)\n```\n\n### 2. Pagination and Filtering\n\nImplement pagination with query parameters:\n\n```bash\n# Use pagination template\ncp ./skills/fastapi-api-patterns/templates/pagination.py app/utils/pagination.py\n```\n\n**Pagination Strategies:**\n\n**1. Offset-Based Pagination (Simple):**\n```python\n@router.get(\"/items/\")\nasync def list_items(skip: int = 0, limit: int = 10):\n    return items[skip : skip + limit]\n```\n\n**2. Cursor-Based Pagination (Performance):**\n```python\n@router.get(\"/items/\")\nasync def list_items(cursor: str | None = None, limit: int = 10):\n    # Use last item ID as cursor for next page\n    # Better for large datasets\n```\n\n**3. Page-Based Pagination (User-Friendly):**\n```python\n@router.get(\"/items/\")\nasync def list_items(page: int = 1, page_size: int = 10):\n    skip = (page - 1) * page_size\n    return items[skip : skip + page_size]\n```\n\n**Filtering Patterns:**\n```python\n@router.get(\"/items/\")\nasync def list_items(\n    skip: int = 0,\n    limit: int = 10,\n    category: str | None = None,\n    min_price: float | None = None,\n    max_price: float | None = None,\n    search: str | None = None,\n):\n    # Apply filters before pagination\n    filtered_items = apply_filters(items, category, min_price, max_price, search)\n    return filtered_items[skip : skip + limit]\n```\n\n**Sorting:**\n```python\nfrom enum import Enum\n\nclass SortBy(str, Enum):\n    name = \"name\"\n    price = \"price\"\n    created_at = \"created_at\"\n\n@router.get(\"/items/\")\nasync def list_items(\n    sort_by: SortBy = SortBy.created_at,\n    order: Literal[\"asc\", \"desc\"] = \"desc\",\n):\n    # Sort before returning\n```\n\n### 3. Request and Response Models\n\nDefine clear Pydantic models for type safety and validation:\n\n**Base Models:**\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom datetime import datetime\n\nclass ItemBase(BaseModel):\n    \"\"\"Shared properties\"\"\"\n    name: str = Field(..., min_length=1, max_length=100)\n    description: str | None = Field(None, max_length=500)\n    price: float = Field(..., gt=0)\n    category: str\n\nclass ItemCreate(ItemBase):\n    \"\"\"Properties required for creation\"\"\"\n    pass\n\nclass ItemUpdate(BaseModel):\n    \"\"\"Properties that can be updated\"\"\"\n    name: str | None = None\n    description: str | None = None\n    price: float | None = Field(None, gt=0)\n    category: str | None = None\n\nclass ItemInDB(ItemBase):\n    \"\"\"Properties stored in database\"\"\"\n    id: int\n    created_at: datetime\n    updated_at: datetime\n\nclass Item(ItemInDB):\n    \"\"\"Properties returned to client\"\"\"\n    class Config:\n        from_attributes = True\n```\n\n**Response Models with Metadata:**\n```python\nfrom typing import Generic, TypeVar, List\nfrom pydantic import BaseModel\n\nT = TypeVar('T')\n\nclass PaginatedResponse(BaseModel, Generic[T]):\n    items: List[T]\n    total: int\n    page: int\n    page_size: int\n    pages: int\n\n@router.get(\"/items/\", response_model=PaginatedResponse[Item])\nasync def list_items(page: int = 1, page_size: int = 10):\n    total = len(items)\n    pages = (total + page_size - 1) // page_size\n    skip = (page - 1) * page_size\n\n    return PaginatedResponse(\n        items=items[skip : skip + page_size],\n        total=total,\n        page=page,\n        page_size=page_size,\n        pages=pages,\n    )\n```\n\n### 4. Error Handling Strategies\n\nImplement consistent error handling:\n\n```bash\n# Use error handling template\ncp ./skills/fastapi-api-patterns/templates/error_handling.py app/utils/errors.py\n```\n\n**HTTP Exception Patterns:**\n```python\nfrom fastapi import HTTPException, status\n\n# 404 Not Found\nif item is None:\n    raise HTTPException(\n        status_code=status.HTTP_404_NOT_FOUND,\n        detail=f\"Item with id {item_id} not found\"\n    )\n\n# 400 Bad Request\nif price < 0:\n    raise HTTPException(\n        status_code=status.HTTP_400_BAD_REQUEST,\n        detail=\"Price must be positive\"\n    )\n\n# 409 Conflict\nif item_exists:\n    raise HTTPException(\n        status_code=status.HTTP_409_CONFLICT,\n        detail=\"Item with this name already exists\"\n    )\n\n# 403 Forbidden\nif not is_owner:\n    raise HTTPException(\n        status_code=status.HTTP_403_FORBIDDEN,\n        detail=\"Not authorized to modify this item\"\n    )\n```\n\n**Custom Exception Handlers:**\n```python\nfrom fastapi import Request\nfrom fastapi.responses import JSONResponse\n\nclass ItemNotFoundError(Exception):\n    def __init__(self, item_id: int):\n        self.item_id = item_id\n\n@app.exception_handler(ItemNotFoundError)\nasync def item_not_found_handler(request: Request, exc: ItemNotFoundError):\n    return JSONResponse(\n        status_code=404,\n        content={\n            \"error\": \"not_found\",\n            \"message\": f\"Item {exc.item_id} not found\",\n            \"item_id\": exc.item_id\n        }\n    )\n```\n\n**Validation Error Customization:**\n```python\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"error\": \"validation_error\",\n            \"message\": \"Invalid request data\",\n            \"details\": exc.errors()\n        }\n    )\n```\n\n### 5. Dependency Injection for Common Logic\n\nUse dependencies for authentication, database sessions, and validation:\n\n```python\nfrom fastapi import Depends, Header, HTTPException\n\n# Authentication dependency\nasync def verify_token(x_token: str = Header(...)):\n    if x_token != \"secret-token\":\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n    return x_token\n\n# Database session dependency\nasync def get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n# Pagination dependency\nasync def pagination_params(\n    skip: int = 0,\n    limit: int = 10,\n    max_limit: int = 100\n):\n    if limit > max_limit:\n        limit = max_limit\n    return {\"skip\": skip, \"limit\": limit}\n\n# Use in endpoints\n@router.get(\"/items/\")\nasync def list_items(\n    token: str = Depends(verify_token),\n    db: Session = Depends(get_db),\n    pagination: dict = Depends(pagination_params),\n):\n    return db.query(Item).offset(pagination[\"skip\"]).limit(pagination[\"limit\"]).all()\n```\n\n### 6. API Router Organization\n\nStructure APIs with APIRouter for modularity:\n\n```python\n# app/routers/items.py\nfrom fastapi import APIRouter\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    dependencies=[Depends(verify_token)],\n    responses={404: {\"description\": \"Not found\"}},\n)\n\n# app/main.py\nfrom fastapi import FastAPI\nfrom app.routers import items, users\n\napp = FastAPI()\n\napp.include_router(items.router)\napp.include_router(users.router, prefix=\"/api/v1\")\n```\n\n### 7. OpenAPI Documentation Enhancement\n\nGenerate better API documentation:\n\n```bash\n# Generate enhanced OpenAPI docs\nbash ./skills/fastapi-api-patterns/scripts/generate-openapi-docs.sh\n```\n\n**Endpoint Documentation:**\n```python\n@router.post(\n    \"/items/\",\n    response_model=Item,\n    status_code=status.HTTP_201_CREATED,\n    summary=\"Create a new item\",\n    description=\"Create a new item with the provided data\",\n    response_description=\"The created item\",\n    responses={\n        201: {\"description\": \"Item created successfully\"},\n        400: {\"description\": \"Invalid input data\"},\n        409: {\"description\": \"Item already exists\"},\n    }\n)\nasync def create_item(item: ItemCreate):\n    \"\"\"\n    Create a new item with all the information:\n\n    - **name**: Item name (required, 1-100 characters)\n    - **description**: Item description (optional, max 500 characters)\n    - **price**: Item price (required, must be positive)\n    - **category**: Item category (required)\n    \"\"\"\n    pass\n```\n\n## Examples\n\n### Example 1: Complete CRUD API for Chat Messages\n\n```bash\n# Copy chat API example\ncp ./skills/fastapi-api-patterns/examples/chat_api.py app/routers/chat.py\n```\n\n**Features:**\n- Create chat messages\n- List messages with pagination and filtering\n- Get single message by ID\n- Update message content\n- Delete messages\n- Search messages by content\n- Filter by user, channel, date range\n\n**Result:** Production-ready chat message API with full CRUD operations\n\n### Example 2: User Management API\n\n```bash\n# Copy user management example\ncp ./skills/fastapi-api-patterns/examples/user_management.py app/routers/users.py\n```\n\n**Features:**\n- User registration with validation\n- User authentication (simulated)\n- Profile retrieval and updates\n- Password change endpoint\n- List users with role filtering\n- User deactivation (soft delete)\n\n**Result:** Complete user management system with security best practices\n\n### Example 3: Memory/Context Endpoints for AI Applications\n\n```bash\n# Copy memory endpoints example\ncp ./skills/fastapi-api-patterns/examples/memory_endpoints.py app/routers/memory.py\n```\n\n**Features:**\n- Store conversation context\n- Retrieve context by session ID\n- Update context with new messages\n- Clear old contexts\n- Search contexts by keywords\n- Pagination for large context histories\n\n**Result:** API for managing AI conversation memory and context\n\n## Requirements\n\n**Dependencies:**\n- FastAPI 0.100+\n- Pydantic 2.0+\n- Python 3.10+\n\n**Optional Dependencies:**\n- SQLAlchemy (for database operations)\n- python-jose (for JWT authentication)\n- passlib (for password hashing)\n- python-multipart (for file uploads)\n\n**Project Structure:**\n```\napp/\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ routers/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ items.py\n‚îÇ   ‚îú‚îÄ‚îÄ users.py\n‚îÇ   ‚îî‚îÄ‚îÄ chat.py\n‚îú‚îÄ‚îÄ models/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ schemas.py\n‚îú‚îÄ‚îÄ utils/\n‚îÇ   ‚îú‚îÄ‚îÄ pagination.py\n‚îÇ   ‚îî‚îÄ‚îÄ errors.py\n‚îî‚îÄ‚îÄ dependencies.py\n```\n\n## Best Practices\n\n**1. Use Response Models:**\n- Always specify `response_model` to control what's returned\n- Use separate models for create, update, and read operations\n- Never expose sensitive data (passwords, tokens)\n\n**2. Consistent Error Responses:**\n- Use standard HTTP status codes\n- Return structured error objects with `error`, `message`, and `details`\n- Include request ID for debugging\n\n**3. Pagination Everywhere:**\n- Never return unbounded lists\n- Default to reasonable page sizes (10-50 items)\n- Include total count and page metadata\n\n**4. Validation and Documentation:**\n- Use Pydantic Field validators for complex validation\n- Document all endpoints with descriptions and examples\n- Use response examples in OpenAPI schema\n\n**5. Dependencies for Reusability:**\n- Extract common logic into dependencies\n- Use dependency injection for auth, DB, pagination\n- Keep endpoints thin, move logic to services\n\n**6. Versioning:**\n- Use prefix-based versioning (`/api/v1/items`)\n- Keep old versions running during migration\n- Document breaking changes clearly\n\n## Validation Script\n\nValidate endpoint structure and best practices:\n\n```bash\n# Validate all endpoints in a router file\nbash ./skills/fastapi-api-patterns/scripts/validate-endpoints.sh app/routers/items.py\n\n# What it checks:\n# - Response models defined\n# - Status codes specified\n# - Error handling present\n# - Documentation strings\n# - Proper HTTP methods\n# - Path parameter validation\n```\n\n## Performance Considerations\n\n**Database Queries:**\n- Use pagination to limit query size\n- Add indexes on frequently filtered fields\n- Use database-level filtering, not Python filtering\n- Implement query result caching for expensive operations\n\n**Response Size:**\n- Exclude unnecessary fields from responses\n- Support field selection via query params\n- Compress responses with gzip middleware\n- Use streaming for large responses\n\n**Request Validation:**\n- Set reasonable limits on request sizes\n- Validate early and fail fast\n- Use background tasks for heavy processing\n- Implement rate limiting on expensive endpoints\n\n---\n\n**Plugin:** fastapi-backend\n**Version:** 1.0.0\n**Category:** API Development\n**Skill Type:** REST API Patterns"
              },
              {
                "name": "fastapi-auth-patterns",
                "description": "Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.",
                "path": "plugins/fastapi-backend/skills/fastapi-auth-patterns/SKILL.md",
                "frontmatter": {
                  "name": "fastapi-auth-patterns",
                  "description": "Implement and validate FastAPI authentication strategies including JWT tokens, OAuth2 password flows, OAuth2 scopes for permissions, and Supabase integration. Use when implementing authentication, securing endpoints, handling user login/signup, managing permissions, integrating OAuth providers, or when user mentions JWT, OAuth2, Supabase auth, protected routes, access control, role-based permissions, or authentication errors.",
                  "allowed-tools": "Read, Grep, Glob, Bash, Write, Edit"
                },
                "content": "# FastAPI Authentication Patterns\n\n**Purpose:** Autonomously implement, validate, and debug FastAPI authentication systems with multiple strategies.\n\n**Activation Triggers:**\n- Implementing user authentication\n- Securing API endpoints\n- JWT token generation/validation issues\n- OAuth2 flow configuration\n- Permission and role-based access control\n- Supabase authentication integration\n- Authentication errors (401, 403)\n- Password hashing and security\n\n**Key Resources:**\n- `scripts/setup-jwt.sh` - Initialize JWT authentication system\n- `scripts/validate-auth.sh` - Validate authentication configuration\n- `templates/jwt_auth.py` - Complete JWT authentication implementation\n- `templates/oauth2_flow.py` - OAuth2 password flow with scopes\n- `templates/supabase_auth.py` - Supabase integration for FastAPI\n- `examples/protected_routes.py` - Protected endpoint patterns\n- `examples/permission_system.py` - Role and permission-based access\n\n## Authentication Strategies\n\n### 1. JWT Token Authentication\n\n**Use When:**\n- Need stateless authentication\n- Building API for mobile/web clients\n- Require token expiration control\n- Implementing refresh token patterns\n\n**Setup:**\n```bash\n./scripts/setup-jwt.sh\n```\n\n**Core Components:**\n- Password hashing with Argon2 (pwdlib)\n- JWT token generation with expiration\n- Token validation and user extraction\n- Secure secret key management\n\n**Implementation Pattern:**\n```python\n# Hash passwords (never store plaintext)\npassword_hash = PasswordHash.recommended()\nhashed = password_hash.hash(plain_password)\n\n# Generate JWT token\ndef create_access_token(data: dict, expires_delta: timedelta):\n    to_encode = data.copy()\n    expire = datetime.now(timezone.utc) + expires_delta\n    to_encode.update({\"exp\": expire})\n    return jwt.encode(to_encode, SECRET_KEY, algorithm=\"HS256\")\n\n# Validate token and extract user\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n    username = payload.get(\"sub\")\n    return get_user(username)\n```\n\n**Key Points:**\n- Use environment variables for SECRET_KEY\n- Default token expiration: 30 minutes\n- Store username in \"sub\" claim\n- Validate token signature and expiration\n\n### 2. OAuth2 Password Flow\n\n**Use When:**\n- Building first-party applications\n- Need username/password authentication\n- Following OAuth2 standards\n- Integrating with OpenAPI documentation\n\n**Template:** `templates/oauth2_flow.py`\n\n**Flow:**\n1. User submits credentials via `OAuth2PasswordRequestForm`\n2. Server verifies password hash\n3. Server returns signed JWT access token\n4. Client includes token in Authorization header\n\n**Security Scheme:**\n```python\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n@app.post(\"/token\")\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    access_token = create_access_token(data={\"sub\": user.username})\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n```\n\n### 3. OAuth2 Scopes (Permissions)\n\n**Use When:**\n- Need fine-grained permission control\n- Implementing role-based access\n- Building multi-tenant systems\n- Following least-privilege principle\n\n**Template:** See `templates/oauth2_flow.py` (includes scopes)\n\n**Define Scopes:**\n```python\noauth2_scheme = OAuth2PasswordBearer(\n    tokenUrl=\"token\",\n    scopes={\n        \"me\": \"Read information about current user\",\n        \"items\": \"Read items\",\n        \"items:write\": \"Create and modify items\"\n    }\n)\n```\n\n**Encode Scopes in Token:**\n```python\n# During login, add user's scopes to token\ntoken_data = {\"sub\": username, \"scopes\": user.scopes}\naccess_token = create_access_token(token_data)\n```\n\n**Protect Endpoints with Scopes:**\n```python\nasync def get_current_user(\n    security_scopes: SecurityScopes,\n    token: str = Depends(oauth2_scheme)\n):\n    # Validate token has required scopes\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(401, \"Not enough permissions\")\n\n@app.get(\"/users/me/items/\")\nasync def read_items(\n    current_user: User = Security(get_current_active_user, scopes=[\"items\"])\n):\n    return current_user.items\n```\n\n### 4. Supabase Authentication\n\n**Use When:**\n- Using Supabase as backend\n- Need managed authentication service\n- Want OAuth providers (Google, GitHub, etc.)\n- Require user management dashboard\n\n**Template:** `templates/supabase_auth.py`\n\n**Setup:**\n```python\nfrom supabase import create_client, Client\n\nsupabase: Client = create_client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_KEY\")\n)\n```\n\n**Sign Up:**\n```python\n# Email/password signup\nresponse = supabase.auth.sign_up({\n    \"email\": \"user@example.com\",\n    \"password\": \"secure_password\",\n    \"options\": {\"data\": {\"full_name\": \"John Doe\"}}\n})\n```\n\n**Sign In:**\n```python\nresponse = supabase.auth.sign_in_with_password({\n    \"email\": \"user@example.com\",\n    \"password\": \"secure_password\"\n})\naccess_token = response.session.access_token\n```\n\n**Validate Token in FastAPI:**\n```python\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    # Validate Supabase JWT token\n    user = supabase.auth.get_user(token)\n    return user\n```\n\n**Integration Pattern:**\n- Store Supabase session in HTTP-only cookies (server-side)\n- Use PKCE flow for OAuth providers\n- Implement token refresh logic\n- Leverage Supabase RLS policies for data access\n\n## Validation Workflow\n\n### 1. Run Authentication Validator\n\n```bash\n./scripts/validate-auth.sh\n```\n\n**Checks Performed:**\n- ‚úÖ Required packages installed (fastapi, python-jose[cryptography], passlib[argon2], pwdlib)\n- ‚úÖ Environment variables set (SECRET_KEY, ALGORITHM, ACCESS_TOKEN_EXPIRE_MINUTES)\n- ‚úÖ Security scheme configured correctly\n- ‚úÖ Password hashing implemented\n- ‚úÖ Token generation and validation functions present\n- ‚úÖ Protected endpoints use proper dependencies\n\n### 2. Common Issues & Fixes\n\n**Missing SECRET_KEY:**\n```bash\n# Generate secure random key\nopenssl rand -hex 32\n\n# Add to .env\necho 'SECRET_KEY=your_generated_key' >> .env\n```\n\n**Token Expired (401):**\n- Increase ACCESS_TOKEN_EXPIRE_MINUTES\n- Implement refresh token pattern\n- Check server/client time sync\n\n**Invalid Credentials:**\n- Verify password hashing algorithm matches\n- Check user exists in database\n- Validate password comparison logic\n\n**Missing Permissions (403):**\n- Verify user has required scopes\n- Check scope encoding in token\n- Validate SecurityScopes configuration\n\n**Supabase Connection Failed:**\n- Verify SUPABASE_URL and SUPABASE_KEY\n- Check project settings in Supabase dashboard\n- Validate network connectivity\n\n## Protected Routes Pattern\n\n**Example:** `examples/protected_routes.py`\n\n```python\n# Public endpoint (no auth)\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Public endpoint\"}\n\n# Protected endpoint (requires authentication)\n@app.get(\"/users/me\")\nasync def read_users_me(current_user: User = Depends(get_current_user)):\n    return current_user\n\n# Protected with specific permission\n@app.post(\"/items/\")\nasync def create_item(\n    item: Item,\n    current_user: User = Security(get_current_active_user, scopes=[\"items:write\"])\n):\n    return create_item_for_user(current_user, item)\n\n# Admin-only endpoint\n@app.delete(\"/users/{user_id}\")\nasync def delete_user(\n    user_id: int,\n    current_user: User = Depends(get_current_admin_user)\n):\n    return delete_user_by_id(user_id)\n```\n\n## Permission System Pattern\n\n**Example:** `examples/permission_system.py`\n\n**Role-Based Access Control (RBAC):**\n```python\nclass Role(str, Enum):\n    ADMIN = \"admin\"\n    USER = \"user\"\n    GUEST = \"guest\"\n\nclass User(BaseModel):\n    username: str\n    role: Role\n    permissions: List[str]\n\ndef has_permission(user: User, required_permission: str) -> bool:\n    # Admins have all permissions\n    if user.role == Role.ADMIN:\n        return True\n    return required_permission in user.permissions\n\nasync def require_permission(permission: str):\n    async def permission_checker(current_user: User = Depends(get_current_user)):\n        if not has_permission(current_user, permission):\n            raise HTTPException(403, f\"Permission '{permission}' required\")\n        return current_user\n    return permission_checker\n\n# Usage\n@app.delete(\"/items/{item_id}\")\nasync def delete_item(\n    item_id: int,\n    current_user: User = Depends(require_permission(\"items:delete\"))\n):\n    return delete_item_by_id(item_id)\n```\n\n## Best Practices\n\n**Security:**\n- Never store passwords in plaintext\n- Use Argon2 for password hashing (recommended over bcrypt)\n- Store SECRET_KEY in environment variables (never commit)\n- Use HTTPS in production\n- Implement rate limiting on login endpoints\n- Add token refresh mechanism for long sessions\n\n**Token Management:**\n- Short access token expiration (15-30 minutes)\n- Long refresh token expiration (7-30 days)\n- Rotate refresh tokens on use\n- Implement token revocation list for logout\n\n**Scope Design:**\n- Use hierarchical scopes (e.g., items, items:read, items:write)\n- Follow least-privilege principle\n- Document all scopes in OpenAPI\n- Validate scopes on every request\n\n**Error Handling:**\n- Return 401 for authentication failures\n- Return 403 for authorization failures\n- Include WWW-Authenticate header with 401\n- Log authentication attempts for security monitoring\n\n## Dependencies\n\n**Required Packages:**\n```bash\npip install fastapi\npip install python-jose[cryptography]\npip install pwdlib[argon2]\npip install supabase  # If using Supabase\n```\n\n**Environment Variables:**\n```\nSECRET_KEY=your_secret_key_here\nALGORITHM=HS256\nACCESS_TOKEN_EXPIRE_MINUTES=30\n\n# For Supabase\nSUPABASE_URL=https://your-project.supabase.co\nSUPABASE_KEY=your_anon_key\n```\n\n---\n\n**Supported Auth Strategies:** JWT, OAuth2 Password Flow, OAuth2 Scopes, Supabase\n\n**Version:** 1.0.0\n**FastAPI Compatibility:** 0.100+"
              },
              {
                "name": "fastapi-deployment-config",
                "description": "Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.",
                "path": "plugins/fastapi-backend/skills/fastapi-deployment-config/SKILL.md",
                "frontmatter": {
                  "name": "fastapi-deployment-config",
                  "description": "Configure multi-platform deployment for FastAPI applications including Docker containerization, Railway, DigitalOcean App Platform, and AWS deployment. Use when deploying FastAPI apps, setting up production environments, containerizing applications, configuring cloud platforms, implementing health checks, managing environment variables, setting up reverse proxies, or when user mentions Docker, Railway, DigitalOcean, AWS, deployment configuration, production setup, or container orchestration.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# FastAPI Deployment Configuration\n\n**Purpose:** Autonomously configure and deploy FastAPI applications across multiple platforms with production-ready configurations.\n\n**Activation Triggers:**\n- Deployment setup requests\n- Docker containerization needs\n- Platform-specific configuration (Railway, DigitalOcean, AWS)\n- Health check implementation\n- Environment variable management\n- Reverse proxy setup (Nginx)\n- Production optimization\n- Multi-stage build configurations\n\n**Key Resources:**\n- `scripts/build-docker.sh` - Multi-stage Docker build automation\n- `scripts/validate-deployment.sh` - Pre-deployment validation checks\n- `scripts/health-check.sh` - Application health verification\n- `templates/` - Production-ready Dockerfile, docker-compose.yml, platform configs\n- `examples/` - Platform-specific deployment guides (Railway, DigitalOcean, AWS)\n\n## Deployment Workflow\n\n### 1. Pre-Deployment Validation\n\nRun comprehensive checks before deployment:\n\n```bash\n./scripts/validate-deployment.sh\n\n# Validates:\n# - Python dependencies (requirements.txt)\n# - Environment variables (.env.example)\n# - FastAPI application structure\n# - Database migrations (if using Alembic)\n# - Static file configuration\n# - CORS settings\n# - Security configurations\n```\n\n**Common issues detected:**\n- Missing required dependencies\n- Unset environment variables\n- Database connection configuration\n- Missing CORS origins\n- Insecure secret key defaults\n\n### 2. Docker Containerization\n\nBuild optimized Docker image using multi-stage builds:\n\n```bash\n./scripts/build-docker.sh [--platform=linux/amd64] [--tag=myapp:latest]\n\n# Features:\n# - Multi-stage build (builder + runtime)\n# - Layer caching optimization\n# - Non-root user for security\n# - Health check integration\n# - Minimal production image size\n```\n\n**Dockerfile template** (`templates/Dockerfile`):\n- Python 3.11+ slim base image\n- Virtual environment isolation\n- Production dependency separation\n- Gunicorn/Uvicorn workers\n- Security best practices\n\n### 3. Platform-Specific Configuration\n\n#### Railway Deployment\n\n```bash\n# Railway uses railway.json for configuration\n# See: examples/railway_setup.md\n\n# Key features:\n# - Automatic HTTPS\n# - Environment variable management\n# - Auto-deploy from Git\n# - Database provisioning\n# - Custom domains\n```\n\n**Configuration:** `templates/railway.json`\n- Build command: `pip install -r requirements.txt`\n- Start command: `uvicorn main:app --host 0.0.0.0 --port $PORT`\n- Health check endpoint: `/health`\n\n#### DigitalOcean App Platform\n\n```bash\n# DigitalOcean uses app.yaml for configuration\n# See: examples/digitalocean_setup.md\n\n# Key features:\n# - Container registry integration\n# - Managed databases\n# - Auto-scaling\n# - CDN integration\n# - Monitoring & alerts\n```\n\n**Configuration:** `templates/digitalocean-app.yaml`\n- Dockerfile-based deployment\n- Health check configuration\n- Environment variable secrets\n- Database component linking\n\n#### AWS Deployment Options\n\n**ECS (Elastic Container Service):**\n```bash\n# See: examples/aws_setup.md#ecs-deployment\n\n# Features:\n# - Fargate serverless containers\n# - Load balancer integration\n# - Auto-scaling policies\n# - CloudWatch logging\n# - VPC networking\n```\n\n**App Runner:**\n```bash\n# Simplified container deployment\n# Automatic scaling and load balancing\n# See: examples/aws_setup.md#app-runner\n```\n\n### 4. Health Check Implementation\n\nImplement comprehensive health checks:\n\n```bash\n./scripts/health-check.sh <endpoint-url>\n\n# Checks:\n# - HTTP endpoint availability (GET /health)\n# - Database connectivity\n# - Redis/cache availability\n# - External API dependencies\n# - Response time monitoring\n```\n\n**Health endpoint template:**\n```python\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"version\": \"1.0.0\",\n        \"database\": check_db(),\n        \"cache\": check_redis()\n    }\n```\n\n### 5. Environment Variable Management\n\n**Required environment variables:**\n- `DATABASE_URL` - Database connection string\n- `SECRET_KEY` - Application secret key\n- `CORS_ORIGINS` - Allowed CORS origins\n- `ENVIRONMENT` - prod/staging/dev\n- `LOG_LEVEL` - Logging verbosity\n\n**Templates provided:**\n- `.env.example` - Development template\n- `.env.production.example` - Production template\n\n### 6. Reverse Proxy Configuration (Nginx)\n\nFor self-hosted deployments:\n\n```bash\n# Nginx configuration: templates/nginx.conf\n\n# Features:\n# - SSL/TLS termination\n# - Rate limiting\n# - Request buffering\n# - Gzip compression\n# - Static file serving\n# - WebSocket support\n# - Security headers\n```\n\n**Configuration highlights:**\n- Proxy to Uvicorn on port 8000\n- Client max body size: 10M\n- Connection timeout: 60s\n- Rate limiting: 100 req/min per IP\n\n## Docker Compose Orchestration\n\nFor local development and testing:\n\n```bash\ndocker-compose up -d\n\n# Services configured:\n# - FastAPI application\n# - PostgreSQL database\n# - Redis cache\n# - Nginx reverse proxy\n```\n\n**Template:** `templates/docker-compose.yml`\n- Volume mounts for development\n- Network isolation\n- Health check dependencies\n- Environment variable injection\n\n## Production Optimization\n\n### Multi-Stage Docker Build\n\n**Stage 1: Builder**\n- Install all dependencies\n- Compile Python packages\n- Create virtual environment\n\n**Stage 2: Runtime**\n- Copy only runtime dependencies\n- Non-root user execution\n- Minimal attack surface\n- Optimized layer caching\n\n### Worker Configuration\n\n**Gunicorn + Uvicorn:**\n```bash\n# Recommended workers: (2 x CPU cores) + 1\ngunicorn main:app \\\n  --workers 4 \\\n  --worker-class uvicorn.workers.UvicornWorker \\\n  --bind 0.0.0.0:8000 \\\n  --access-logfile - \\\n  --error-logfile - \\\n  --log-level info\n```\n\n### Database Connection Pooling\n\n```python\n# SQLAlchemy configuration\nengine = create_engine(\n    DATABASE_URL,\n    pool_size=10,\n    max_overflow=20,\n    pool_pre_ping=True\n)\n```\n\n## Security Configurations\n\n**Implemented in templates:**\n- ‚úÖ Non-root Docker user\n- ‚úÖ Read-only root filesystem (where possible)\n- ‚úÖ Security headers (HSTS, X-Frame-Options, CSP)\n- ‚úÖ CORS configuration\n- ‚úÖ Rate limiting\n- ‚úÖ Secret management via environment variables\n- ‚úÖ SQL injection prevention (SQLAlchemy ORM)\n- ‚úÖ Input validation (Pydantic models)\n\n## Platform Comparison\n\n| Feature | Railway | DigitalOcean | AWS ECS | AWS App Runner |\n|---------|---------|--------------|---------|----------------|\n| **Ease of Setup** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n| **Cost (Low Traffic)** | $5-10/mo | $12/mo | $20-30/mo | $15-25/mo |\n| **Auto-Scaling** | Limited | Yes | Yes | Yes |\n| **Database Included** | Yes (add-on) | Yes (managed) | Separate (RDS) | Separate (RDS) |\n| **Custom Domains** | Yes | Yes | Yes | Yes |\n| **CI/CD** | Git-based | Container registry | CodePipeline | Git/ECR |\n\n## Common Deployment Scenarios\n\n### Scenario 1: Simple API (No Database)\n**Recommended:** Railway or AWS App Runner\n- Minimal configuration\n- Fast deployment\n- Auto-scaling included\n\n### Scenario 2: API + PostgreSQL\n**Recommended:** Railway or DigitalOcean\n- Integrated database provisioning\n- Automatic backups\n- Connection pooling\n\n### Scenario 3: Microservices Architecture\n**Recommended:** AWS ECS or DigitalOcean App Platform\n- Service mesh capabilities\n- Container orchestration\n- Advanced networking\n\n### Scenario 4: High-Traffic Production\n**Recommended:** AWS ECS with RDS\n- Advanced monitoring\n- Multi-AZ deployment\n- Enterprise support\n\n## Troubleshooting\n\n### Build Failures\n```bash\n# Check Docker build logs\n./scripts/build-docker.sh --verbose\n\n# Common fixes:\n# - Update requirements.txt versions\n# - Check Python version compatibility\n# - Verify base image availability\n```\n\n### Health Check Failures\n```bash\n# Debug health endpoint\n./scripts/health-check.sh http://localhost:8000/health --debug\n\n# Common issues:\n# - Database connection timeout\n# - Missing environment variables\n# - Port binding conflicts\n```\n\n### Performance Issues\n```bash\n# Monitor worker utilization\n# Increase Gunicorn workers\n# Enable connection pooling\n# Implement caching (Redis)\n# Optimize database queries\n```\n\n## Resources\n\n**Scripts:** All scripts are executable and include help documentation (`--help`)\n\n**Templates:** Production-ready configurations in `templates/` directory\n\n**Examples:** Detailed platform-specific guides in `examples/` directory\n- `railway_setup.md` - Complete Railway deployment walkthrough\n- `digitalocean_setup.md` - DigitalOcean App Platform setup\n- `aws_setup.md` - AWS ECS and App Runner deployment\n\n---\n\n**Supported Platforms:** Railway, DigitalOcean App Platform, AWS ECS, AWS App Runner, Self-hosted (Docker + Nginx)\n\n**FastAPI Version:** 0.104.0+\n**Python Version:** 3.11+\n**Docker Version:** 20.10+"
              },
              {
                "name": "fastapi-project-structure",
                "description": "Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.",
                "path": "plugins/fastapi-backend/skills/fastapi-project-structure/SKILL.md",
                "frontmatter": {
                  "name": "fastapi-project-structure",
                  "description": "Production-ready FastAPI project scaffolding templates including directory structure, configuration files, settings management, dependency injection, MCP server integration, and development/production setup patterns. Use when creating FastAPI projects, setting up project structure, configuring FastAPI applications, implementing settings management, adding MCP integration, or when user mentions FastAPI setup, project scaffold, app configuration, environment management, or backend structure.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# FastAPI Project Structure Skill\n\nProduction-ready FastAPI project scaffolding templates and best practices for building scalable, maintainable backend applications with MCP integration support.\n\n## Instructions\n\n### 1. Choose Project Template\n\nSelect the appropriate project template based on your use case:\n\n- **minimal**: Basic FastAPI app structure (single file, quick prototypes)\n- **standard**: Standard production structure (API routes, models, services)\n- **mcp-server**: FastAPI app with MCP server integration\n- **full-stack**: Complete backend with auth, database, background tasks\n- **microservice**: Microservice-ready structure with health checks, metrics\n\n### 2. Generate Project Structure\n\nUse the setup script to scaffold a new FastAPI project:\n\n```bash\ncd /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/fastapi-backend/skills/fastapi-project-structure\n./scripts/setup-project.sh <project-name> <template-type>\n```\n\n**Template types:** `minimal`, `standard`, `mcp-server`, `full-stack`, `microservice`\n\n**Example:**\n```bash\n./scripts/setup-project.sh my-api-service standard\n```\n\n**What This Creates:**\n- Complete directory structure\n- Configuration files (pyproject.toml, .env.example)\n- Main application entry point\n- Settings management system\n- Docker configuration (for production templates)\n- README with setup instructions\n\n### 3. Configure Application Settings\n\nThe skill uses Pydantic Settings for configuration management:\n\n**Settings Structure:**\n```python\n# app/core/config.py\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    # App Configuration\n    PROJECT_NAME: str = \"FastAPI App\"\n    VERSION: str = \"1.0.0\"\n    DEBUG: bool = False\n\n    # Server Configuration\n    HOST: str = \"0.0.0.0\"\n    PORT: int = 8000\n\n    # Database Configuration (if needed)\n    DATABASE_URL: str\n\n    # Security\n    SECRET_KEY: str\n    ALLOWED_ORIGINS: list[str] = [\"*\"]\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = True\n```\n\n**Environment Variables:**\nCopy `.env.example` to `.env` and customize:\n```bash\ncp .env.example .env\n# Edit .env with your configuration\n```\n\n### 4. Project Structure Patterns\n\n#### Standard Structure\n\n```\nmy-api-service/\n‚îú‚îÄ‚îÄ app/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Application entry point\n‚îÇ   ‚îú‚îÄ‚îÄ core/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Settings management\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dependencies.py  # Dependency injection\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/          # API route handlers\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.py\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ users.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deps.py          # Route dependencies\n‚îÇ   ‚îú‚îÄ‚îÄ models/              # Pydantic models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user.py\n‚îÇ   ‚îú‚îÄ‚îÄ schemas/             # Request/Response schemas\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user.py\n‚îÇ   ‚îî‚îÄ‚îÄ services/            # Business logic\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îî‚îÄ‚îÄ user_service.py\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ conftest.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_api/\n‚îú‚îÄ‚îÄ .env.example\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ Dockerfile (optional)\n```\n\n#### MCP Server Integration Structure\n\n```\nmy-mcp-api/\n‚îú‚îÄ‚îÄ app/\n‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI + MCP server\n‚îÇ   ‚îú‚îÄ‚îÄ core/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mcp_config.py    # MCP-specific settings\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/\n‚îÇ   ‚îú‚îÄ‚îÄ mcp/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py        # MCP server instance\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools/           # MCP tools\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resources/       # MCP resources\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts/         # MCP prompts\n‚îÇ   ‚îî‚îÄ‚îÄ services/\n‚îú‚îÄ‚îÄ .mcp.json                # MCP configuration\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ README.md\n```\n\n### 5. Validate Project Structure\n\nRun validation to ensure proper structure and dependencies:\n\n```bash\n./scripts/validate-structure.sh <project-directory>\n```\n\n**Validation Checks:**\n- Directory structure compliance\n- Required files present (main.py, config.py, pyproject.toml)\n- Python syntax validation\n- Dependency declarations\n- Environment variable configuration\n- Import structure validity\n- Type hints presence\n\n### 6. Development Setup\n\nInitialize the development environment:\n\n```bash\n# Navigate to project\ncd <project-name>\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -e \".[dev]\"\n\n# Run development server\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n```\n\n### 7. MCP Server Integration (Optional)\n\nFor projects with MCP server support:\n\n```bash\n# Configure MCP settings\ncp templates/mcp-config-template.json .mcp.json\n\n# Edit MCP configuration\n# Add tools, resources, and prompts\n\n# Run as MCP server (STDIO mode)\npython -m app.main --mcp\n\n# Run as HTTP server\nuvicorn app.main:app --host 0.0.0.0 --port 8000\n```\n\n## Available Templates\n\n### Core Templates\n\n- **pyproject.toml**: Modern Python project configuration with dependencies\n- **main.py**: Application entry point with FastAPI initialization\n- **config.py**: Pydantic Settings-based configuration management\n- **dependencies.py**: Dependency injection patterns\n- **health.py**: Health check endpoints with database/service checks\n- **docker-template**: Multi-stage Docker build for production\n- **nginx-template**: Nginx reverse proxy configuration\n\n### MCP Integration Templates\n\n- **mcp-server.py**: MCP server initialization with FastAPI\n- **mcp-tool-template.py**: MCP tool implementation pattern\n- **mcp-resource-template.py**: MCP resource pattern\n- **mcp-config.json**: MCP server configuration\n\n### Settings & Configuration\n\n- **.env.example**: Environment variables template\n- **settings-dev.py**: Development-specific settings\n- **settings-prod.py**: Production-specific settings\n- **logging-config.yaml**: Structured logging configuration\n\n## Key Features\n\n### Settings Management\n- Pydantic-based type-safe configuration\n- Environment-specific settings (dev, staging, prod)\n- Automatic validation and type conversion\n- Secret management with environment variables\n- Nested configuration support\n\n### Dependency Injection\n- FastAPI's built-in DI system\n- Reusable dependencies for auth, database, services\n- Request-scoped and application-scoped dependencies\n- Easy testing with dependency overrides\n\n### Project Organization\n- Clear separation of concerns (routes, models, services)\n- Scalable directory structure\n- Consistent naming conventions\n- Module-based organization for large projects\n\n### MCP Integration\n- Dual-mode operation (HTTP + MCP STDIO)\n- MCP tools, resources, and prompts\n- Configuration management via .mcp.json\n- FastMCP framework compatibility\n\n### Production-Ready\n- Docker multi-stage builds\n- Health check endpoints\n- Structured logging\n- Error handling middleware\n- CORS configuration\n- Security headers\n\n## Examples\n\nSee the examples directory for:\n- `minimal-api/`: Simple FastAPI application\n- `crud-api/`: Complete CRUD API with database\n- `mcp-integrated-api/`: FastAPI + MCP server\n- `microservice-template/`: Production microservice\n- `auth-api/`: API with JWT authentication\n\n## Configuration Files Generated\n\n### pyproject.toml\n```toml\n[project]\nname = \"my-api-service\"\nversion = \"1.0.0\"\ndescription = \"FastAPI application\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fastapi>=0.115.0\",\n    \"uvicorn[standard]>=0.32.0\",\n    \"pydantic>=2.0.0\",\n    \"pydantic-settings>=2.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=8.0.0\",\n    \"pytest-asyncio>=0.24.0\",\n    \"httpx>=0.27.0\",\n    \"ruff>=0.6.0\",\n    \"mypy>=1.11.0\",\n]\nmcp = [\n    \"mcp>=1.0.0\",\n]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.mypy]\npython_version = \"3.11\"\nstrict = true\n```\n\n### main.py (Standard Template)\n```python\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.core.config import settings\nfrom app.api.routes import health, users\n\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n    version=settings.VERSION,\n    debug=settings.DEBUG,\n)\n\n# CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\napp.include_router(health.router, prefix=\"/health\", tags=[\"health\"])\napp.include_router(users.router, prefix=\"/api/v1/users\", tags=[\"users\"])\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": f\"Welcome to {settings.PROJECT_NAME}\"}\n```\n\n### Dockerfile (Production Template)\n```dockerfile\nFROM python:3.11-slim as builder\nWORKDIR /app\nCOPY pyproject.toml .\nRUN pip install --no-cache-dir build && \\\n    python -m build --wheel\n\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /app/dist/*.whl .\nRUN pip install --no-cache-dir *.whl && rm *.whl\nCOPY app/ ./app/\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n## Best Practices\n\n**Project Structure:**\n1. Keep business logic in services, not routes\n2. Use schemas for request/response validation\n3. Separate models (DB) from schemas (API)\n4. Implement dependency injection for services\n5. Use async/await for I/O-bound operations\n\n**Configuration:**\n1. Never commit secrets (.env in .gitignore)\n2. Use Pydantic Settings for type safety\n3. Separate dev/staging/prod configurations\n4. Validate all environment variables at startup\n5. Document required environment variables in .env.example\n\n**Code Organization:**\n1. One router per resource (users.py, posts.py)\n2. Group related endpoints in routers\n3. Keep route handlers thin (delegate to services)\n4. Use consistent naming conventions\n5. Type-hint all function parameters and returns\n\n**Testing:**\n1. Use TestClient for API testing\n2. Override dependencies for mocking\n3. Separate unit and integration tests\n4. Use fixtures for common test data\n5. Test both success and error cases\n\n**Security:**\n1. Validate all inputs with Pydantic\n2. Implement proper CORS configuration\n3. Use environment variables for secrets\n4. Enable security headers middleware\n5. Implement rate limiting for public APIs\n\n**Performance:**\n1. Use async route handlers for I/O\n2. Implement connection pooling for databases\n3. Add response caching where appropriate\n4. Use background tasks for non-critical operations\n5. Monitor with health check endpoints\n\n## Common Workflows\n\n### Creating a New API Endpoint\n\n```bash\n# 1. Generate project structure\n./scripts/setup-project.sh my-api standard\n\n# 2. Add new route file\n# Create app/api/routes/items.py\n\n# 3. Add schemas\n# Create app/schemas/item.py\n\n# 4. Add service logic\n# Create app/services/item_service.py\n\n# 5. Register router in main.py\n# app.include_router(items.router, prefix=\"/api/v1/items\")\n```\n\n### Setting Up MCP Integration\n\n```bash\n# 1. Generate MCP-enabled project\n./scripts/setup-project.sh my-mcp-api mcp-server\n\n# 2. Configure .mcp.json\ncp templates/mcp-config-template.json my-mcp-api/.mcp.json\n\n# 3. Add MCP tools\n# Copy from templates/mcp-tool-template.py to app/mcp/tools/\n\n# 4. Run in MCP mode\ncd my-mcp-api\npython -m app.main --mcp\n```\n\n### Production Deployment\n\n```bash\n# 1. Build Docker image\ndocker build -t my-api:latest .\n\n# 2. Run container\ndocker run -d -p 8000:8000 \\\n  --env-file .env.prod \\\n  --name my-api \\\n  my-api:latest\n\n# 3. Health check\ncurl http://localhost:8000/health\n```\n\n## Troubleshooting\n\n**Import errors**: Ensure virtual environment is activated and dependencies installed\n\n**Port already in use**: Change PORT in .env or use different port with `--port` flag\n\n**Environment variables not loading**: Check .env file location and syntax, ensure pydantic-settings installed\n\n**MCP server not starting**: Verify .mcp.json configuration and mcp package installed\n\n**Type checking errors**: Run `mypy app/` to see detailed type errors, ensure all dependencies have type stubs\n\n---\n\n**Plugin:** fastapi-backend\n**Version:** 1.0.0\n**Category:** Project Structure & Scaffolding\n**Skill Type:** Templates & Automation"
              },
              {
                "name": "mem0-fastapi-integration",
                "description": "Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.",
                "path": "plugins/fastapi-backend/skills/mem0-fastapi-integration/SKILL.md",
                "frontmatter": {
                  "name": "mem0-fastapi-integration",
                  "description": "Memory layer integration patterns for FastAPI with Mem0 including client setup, memory service patterns, user tracking, conversation persistence, and background task integration. Use when implementing AI memory, adding Mem0 to FastAPI, building chat with memory, or when user mentions Mem0, conversation history, user context, or memory layer.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Mem0 FastAPI Integration Patterns\n\n**Purpose:** Provide complete Mem0 integration templates, memory service patterns, user tracking implementations, and conversation persistence strategies for building FastAPI applications with intelligent AI memory.\n\n**Activation Triggers:**\n- Integrating Mem0 memory layer into FastAPI\n- Building chat applications with conversation history\n- Implementing user context and personalization\n- Adding memory to AI agents\n- Creating stateful AI interactions\n- User preference management\n\n**Key Resources:**\n- `templates/memory_service.py` - Complete Mem0 service implementation\n- `templates/memory_middleware.py` - Request-scoped memory middleware\n- `templates/memory_client.py` - Mem0 client configuration\n- `templates/memory_routes.py` - API routes for memory operations\n- `scripts/setup-mem0.sh` - Mem0 installation and configuration\n- `scripts/test-memory.sh` - Memory service testing utility\n- `examples/chat_with_memory.py` - Complete chat implementation\n- `examples/user_preferences.py` - User preference management\n\n## Core Mem0 Integration\n\n### 1. Client Configuration\n\n**Template:** `templates/memory_client.py`\n\n**Workflow:**\n```python\nfrom mem0 import Memory, AsyncMemory, MemoryClient\nfrom mem0.configs.base import MemoryConfig\n\n# Hosted Mem0 Platform\nclient = MemoryClient(api_key=settings.MEM0_API_KEY)\n\n# Self-Hosted Configuration\nconfig = MemoryConfig(\n    vector_store={\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"host\": settings.QDRANT_HOST,\n            \"port\": settings.QDRANT_PORT,\n            \"api_key\": settings.QDRANT_API_KEY\n        }\n    },\n    llm={\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4\",\n            \"api_key\": settings.OPENAI_API_KEY\n        }\n    },\n    embedder={\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"text-embedding-3-small\",\n            \"api_key\": settings.OPENAI_API_KEY\n        }\n    }\n)\nmemory = AsyncMemory(config)\n```\n\n### 2. Memory Service Pattern\n\n**Template:** `templates/memory_service.py`\n\n**Key Operations:**\n```python\nclass MemoryService:\n    async def add_conversation(\n        user_id: str,\n        messages: List[Dict[str, str]],\n        metadata: Optional[Dict] = None\n    ) -> Dict\n\n    async def search_memories(\n        query: str,\n        user_id: str,\n        limit: int = 5\n    ) -> List[Dict]\n\n    async def get_user_summary(user_id: str) -> Dict\n\n    async def add_user_preference(\n        user_id: str,\n        preference: str,\n        category: str = \"general\"\n    ) -> bool\n```\n\n**Initialization:**\n```python\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    memory_service = MemoryService()\n    app.state.memory_service = memory_service\n    yield\n    # Shutdown\n```\n\n## Memory Patterns\n\n### 1. Conversation Persistence\n\n**When to use:** Chat applications, conversational AI\n\n**Template:** `templates/memory_service.py#add_conversation`\n\n```python\nasync def add_conversation(\n    self,\n    user_id: str,\n    messages: List[Dict[str, str]],\n    metadata: Optional[Dict[str, Any]] = None\n) -> Optional[Dict]:\n    enhanced_metadata = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"conversation_type\": \"chat\",\n        **(metadata or {})\n    }\n\n    if self.client:\n        result = self.client.add(\n            messages=messages,\n            user_id=user_id,\n            metadata=enhanced_metadata\n        )\n    elif self.memory:\n        result = await self.memory.add(\n            messages=messages,\n            user_id=user_id,\n            metadata=enhanced_metadata\n        )\n\n    return result\n```\n\n**Best for:** Chat history, conversation context, multi-turn interactions\n\n### 2. Semantic Memory Search\n\n**When to use:** Context retrieval, relevant history lookup\n\n**Template:** `templates/memory_service.py#search_memories`\n\n```python\nasync def search_memories(\n    self,\n    query: str,\n    user_id: str,\n    limit: int = 5,\n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    if self.client:\n        result = self.client.search(\n            query=query,\n            user_id=user_id,\n            limit=limit\n        )\n    elif self.memory:\n        result = await self.memory.search(\n            query=query,\n            user_id=user_id,\n            limit=limit\n        )\n\n    memories = result.get('results', [])\n    return memories\n```\n\n**Best for:** Finding relevant past conversations, context-aware responses\n\n### 3. User Preference Storage\n\n**When to use:** Personalization, user settings, behavioral tracking\n\n**Template:** `templates/memory_service.py#add_user_preference`\n\n```python\nasync def add_user_preference(\n    self,\n    user_id: str,\n    preference: str,\n    category: str = \"general\"\n) -> bool:\n    preference_message = {\n        \"role\": \"system\",\n        \"content\": f\"User preference ({category}): {preference}\"\n    }\n\n    metadata = {\n        \"type\": \"preference\",\n        \"category\": category,\n        \"timestamp\": datetime.now().isoformat()\n    }\n\n    if self.client:\n        self.client.add(\n            messages=[preference_message],\n            user_id=user_id,\n            metadata=metadata\n        )\n    elif self.memory:\n        await self.memory.add(\n            messages=[preference_message],\n            user_id=user_id,\n            metadata=metadata\n        )\n\n    return True\n```\n\n**Best for:** User customization, learning user behavior, preference management\n\n## API Routes Integration\n\n### 1. Memory Management Endpoints\n\n**Template:** `templates/memory_routes.py`\n\n```python\nfrom fastapi import APIRouter, Depends, BackgroundTasks\nfrom app.api.deps import get_current_user, get_memory_service\nfrom app.services.memory_service import MemoryService\n\nrouter = APIRouter()\n\n@router.post(\"/conversation\")\nasync def add_conversation(\n    request: ConversationRequest,\n    background_tasks: BackgroundTasks,\n    user_id: str = Depends(get_current_user),\n    memory_service: MemoryService = Depends(get_memory_service)\n):\n    # Add to memory in background\n    background_tasks.add_task(\n        memory_service.add_conversation,\n        user_id,\n        request.messages,\n        request.metadata\n    )\n\n    return {\n        \"status\": \"success\",\n        \"message\": \"Conversation added to memory\"\n    }\n\n@router.post(\"/search\")\nasync def search_memories(\n    request: SearchRequest,\n    user_id: str = Depends(get_current_user),\n    memory_service: MemoryService = Depends(get_memory_service)\n):\n    memories = await memory_service.search_memories(\n        query=request.query,\n        user_id=user_id,\n        limit=request.limit\n    )\n\n    return {\n        \"query\": request.query,\n        \"results\": memories,\n        \"count\": len(memories)\n    }\n\n@router.get(\"/summary\")\nasync def get_memory_summary(\n    user_id: str = Depends(get_current_user),\n    memory_service: MemoryService = Depends(get_memory_service)\n):\n    summary = await memory_service.get_user_summary(user_id)\n    return {\"user_id\": user_id, \"summary\": summary}\n```\n\n### 2. Request Models\n\n**Template:** `templates/memory_routes.py#models`\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\n\nclass ConversationRequest(BaseModel):\n    messages: List[Dict[str, str]] = Field(..., description=\"Conversation messages\")\n    session_id: Optional[str] = Field(None, description=\"Session identifier\")\n    metadata: Optional[Dict[str, Any]] = Field(None, description=\"Additional metadata\")\n\nclass SearchRequest(BaseModel):\n    query: str = Field(..., description=\"Search query\")\n    limit: int = Field(5, ge=1, le=20, description=\"Number of results\")\n    filters: Optional[Dict[str, Any]] = Field(None, description=\"Search filters\")\n\nclass PreferenceRequest(BaseModel):\n    preference: str = Field(..., description=\"User preference\")\n    category: str = Field(\"general\", description=\"Preference category\")\n```\n\n## Background Task Integration\n\n### 1. Async Memory Storage\n\n**Pattern:**\n```python\nfrom fastapi import BackgroundTasks\n\n@router.post(\"/chat\")\nasync def chat(\n    request: ChatRequest,\n    background_tasks: BackgroundTasks,\n    memory_service: MemoryService = Depends(get_memory_service)\n):\n    # Generate AI response\n    response = await ai_service.generate(request.message)\n\n    # Store conversation in background\n    background_tasks.add_task(\n        memory_service.add_conversation,\n        user_id=request.user_id,\n        messages=[\n            {\"role\": \"user\", \"content\": request.message},\n            {\"role\": \"assistant\", \"content\": response}\n        ]\n    )\n\n    return {\"response\": response}\n```\n\n**Benefits:**\n- Non-blocking memory operations\n- Faster response times\n- Graceful failure handling\n\n### 2. Batch Memory Updates\n\n**Pattern:**\n```python\nasync def batch_update_memories(\n    conversations: List[Conversation],\n    memory_service: MemoryService\n):\n    tasks = [\n        memory_service.add_conversation(\n            user_id=conv.user_id,\n            messages=conv.messages\n        )\n        for conv in conversations\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n```\n\n## Dependency Injection Pattern\n\n### 1. Memory Service Dependency\n\n**Template:** `templates/memory_middleware.py#deps`\n\n```python\nfrom fastapi import Request\n\ndef get_memory_service(request: Request) -> MemoryService:\n    \"\"\"Get memory service from app state\"\"\"\n    return request.app.state.memory_service\n\n# Usage in routes\n@router.get(\"/\")\nasync def endpoint(\n    memory_service: MemoryService = Depends(get_memory_service)\n):\n    memories = await memory_service.search_memories(...)\n    return memories\n```\n\n### 2. User Context Injection\n\n**Pattern:**\n```python\nasync def get_user_context(\n    user_id: str = Depends(get_current_user),\n    memory_service: MemoryService = Depends(get_memory_service)\n) -> Dict[str, Any]:\n    \"\"\"Get enriched user context from memory\"\"\"\n    summary = await memory_service.get_user_summary(user_id)\n    return {\n        \"user_id\": user_id,\n        \"preferences\": summary.get(\"user_preferences\", []),\n        \"total_conversations\": summary.get(\"total_memories\", 0)\n    }\n\n# Usage\n@router.post(\"/personalized-response\")\nasync def personalized(\n    request: QueryRequest,\n    context: Dict = Depends(get_user_context)\n):\n    # Use context for personalized AI response\n    pass\n```\n\n## Implementation Workflow\n\n### Step 1: Setup Mem0\n\n```bash\n# Install and configure Mem0\n./scripts/setup-mem0.sh\n```\n\n**Creates:**\n- Mem0 configuration file\n- Environment variable template\n- Vector database setup (if self-hosted)\n\n### Step 2: Configure Memory Service\n\n**Decision tree:**\n- **Hosted Mem0**: Use MemoryClient with API key (simple, managed)\n- **Self-Hosted**: Use AsyncMemory with config (full control, cost-effective)\n- **Hybrid**: Support both with fallback logic\n\n### Step 3: Integrate with FastAPI Lifespan\n\n**Pattern:**\n```python\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    memory_service = MemoryService()\n    app.state.memory_service = memory_service\n    logger.info(\"Memory service initialized\")\n\n    yield\n\n    # Shutdown\n    logger.info(\"Shutting down memory service\")\n```\n\n### Step 4: Add Memory Routes\n\n**Use template:** `templates/memory_routes.py`\n\n**Register:**\n```python\napp.include_router(\n    memory.router,\n    prefix=f\"{settings.API_V1_STR}/memory\",\n    tags=[\"memory\"]\n)\n```\n\n### Step 5: Test Memory Service\n\n```bash\n# Run memory service tests\n./scripts/test-memory.sh\n```\n\n**Tests:**\n- Add conversation\n- Search memories\n- Get user summary\n- Add preferences\n- Error handling\n\n## Optimization Strategies\n\n### 1. Memory Caching\n\n**Pattern:**\n```python\nfrom functools import lru_cache\nfrom datetime import datetime, timedelta\n\nclass CachedMemoryService(MemoryService):\n    def __init__(self):\n        super().__init__()\n        self._cache = {}\n        self._cache_ttl = timedelta(minutes=5)\n\n    async def search_memories(self, query: str, user_id: str, limit: int = 5):\n        cache_key = f\"{user_id}:{query}:{limit}\"\n\n        if cache_key in self._cache:\n            cached_time, cached_result = self._cache[cache_key]\n            if datetime.now() - cached_time < self._cache_ttl:\n                return cached_result\n\n        result = await super().search_memories(query, user_id, limit)\n        self._cache[cache_key] = (datetime.now(), result)\n        return result\n```\n\n### 2. Batch Operations\n\n**Pattern:**\n```python\nasync def batch_add_conversations(\n    conversations: List[Tuple[str, List[Dict]]],\n    memory_service: MemoryService\n):\n    \"\"\"Add multiple conversations efficiently\"\"\"\n    tasks = [\n        memory_service.add_conversation(user_id, messages)\n        for user_id, messages in conversations\n    ]\n    return await asyncio.gather(*tasks)\n```\n\n### 3. Error Handling with Fallback\n\n**Pattern:**\n```python\nasync def resilient_memory_add(\n    user_id: str,\n    messages: List[Dict],\n    memory_service: MemoryService\n):\n    try:\n        return await memory_service.add_conversation(user_id, messages)\n    except Exception as e:\n        logger.error(f\"Memory add failed: {e}\")\n        # Fallback: Queue for retry or log to backup storage\n        await backup_storage.save(user_id, messages)\n        return None\n```\n\n## Production Best Practices\n\n### 1. Environment Configuration\n\n**Template:** `.env` variables\n```bash\n# Hosted Mem0\nMEM0_API_KEY=your_mem0_api_key\n\n# Self-Hosted Configuration\nQDRANT_HOST=localhost\nQDRANT_PORT=6333\nQDRANT_API_KEY=your_qdrant_key\nOPENAI_API_KEY=your_openai_key\n\n# Memory Settings\nMEMORY_CACHE_TTL_SECONDS=300\nMEMORY_SEARCH_LIMIT_DEFAULT=5\nMEMORY_SEARCH_LIMIT_MAX=20\n```\n\n### 2. Monitoring and Logging\n\n**Pattern:**\n```python\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass MonitoredMemoryService(MemoryService):\n    async def add_conversation(self, user_id: str, messages: List[Dict], metadata=None):\n        start_time = datetime.now()\n        try:\n            result = await super().add_conversation(user_id, messages, metadata)\n            latency = (datetime.now() - start_time).total_seconds()\n            logger.info(f\"Memory add success: user={user_id}, latency={latency}s\")\n            return result\n        except Exception as e:\n            logger.error(f\"Memory add failed: user={user_id}, error={e}\")\n            raise\n```\n\n### 3. Rate Limiting\n\n**Pattern:**\n```python\nfrom fastapi import HTTPException\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\n\n@router.post(\"/search\")\n@limiter.limit(\"10/minute\")\nasync def search_memories(request: Request, ...):\n    # Rate-limited memory search\n    pass\n```\n\n## Common Memory Patterns\n\n### 1. Chat with Memory\n\n**Example:** `examples/chat_with_memory.py`\n\nComplete chat implementation with conversation history and context retrieval\n\n**Features:**\n- Automatic conversation storage\n- Context-aware responses\n- Session management\n- Memory search integration\n\n### 2. User Preference Management\n\n**Example:** `examples/user_preferences.py`\n\nUser preference tracking and personalization\n\n**Features:**\n- Preference categorization\n- Preference retrieval\n- Behavioral learning\n- Personalized AI responses\n\n### 3. Multi-User Memory Isolation\n\n**Pattern:**\n```python\nasync def get_user_memories_isolated(\n    user_id: str,\n    memory_service: MemoryService\n):\n    \"\"\"Ensure memory isolation between users\"\"\"\n    # All memory operations scoped to user_id\n    memories = await memory_service.search_memories(\n        query=\"...\",\n        user_id=user_id  # Always scoped\n    )\n    return memories\n```\n\n## Vector Database Support\n\n### Supported Vector Databases\n\n**1. Qdrant (Recommended for Self-Hosted)**\n```python\nvector_store={\n    \"provider\": \"qdrant\",\n    \"config\": {\n        \"host\": \"localhost\",\n        \"port\": 6333,\n        \"api_key\": settings.QDRANT_API_KEY\n    }\n}\n```\n\n**2. Pinecone (Fully Managed)**\n```python\nvector_store={\n    \"provider\": \"pinecone\",\n    \"config\": {\n        \"api_key\": settings.PINECONE_API_KEY,\n        \"environment\": \"us-west1-gcp\"\n    }\n}\n```\n\n**3. Chroma (Local Development)**\n```python\nvector_store={\n    \"provider\": \"chroma\",\n    \"config\": {\n        \"host\": \"localhost\",\n        \"port\": 8000\n    }\n}\n```\n\n## Resources\n\n**Scripts:**\n- `setup-mem0.sh` - Install and configure Mem0\n- `test-memory.sh` - Test memory service functionality\n\n**Templates:**\n- `memory_service.py` - Complete MemoryService implementation\n- `memory_client.py` - Mem0 client configuration\n- `memory_middleware.py` - Middleware and dependencies\n- `memory_routes.py` - FastAPI routes for memory operations\n\n**Examples:**\n- `chat_with_memory.py` - Complete chat with memory\n- `user_preferences.py` - User preference management\n- `memory_analytics.py` - Memory usage analytics\n\n---\n\n**Supported Configurations:** Hosted Mem0, Self-Hosted (Qdrant, Pinecone, Chroma)\n**FastAPI Version:** 0.100+\n**Mem0 Version:** 0.1.0+\n**Python Version:** 3.9+\n\n**Best Practice:** Start with hosted Mem0 for simplicity, migrate to self-hosted for cost optimization"
              }
            ]
          },
          {
            "name": "mem0",
            "description": "AI memory management with Platform (hosted), Open Source (self-hosted with Supabase), and MCP (OpenMemory) support. Persistent conversation memory, user preferences, and graph relationships for AI applications.",
            "source": "./plugins/mem0",
            "category": "development",
            "version": "1.1.0",
            "author": {
              "name": "vanman2024",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install mem0@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-conversation-memory",
                "description": "Add conversation memory tracking to existing chat/AI application",
                "path": "plugins/mem0/commands/add-conversation-memory.md",
                "frontmatter": {
                  "description": "Add conversation memory tracking to existing chat/AI application",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate conversation memory tracking into existing application, automatically storing and retrieving conversation context.\n\nCore Principles:\n- Detect existing AI framework (Vercel AI SDK, LangChain, etc.)\n- Add memory middleware/wrapper\n- Minimal code changes required\n- Automatic context retrieval\n\nPhase 1: Framework Detection\nGoal: Identify existing AI framework\n\nActions:\n- Search for AI framework imports and usage:\n  - Vercel AI SDK (streamText, generateText, useChat)\n  - LangChain (ConversationalChain, ChatOpenAI)\n  - CrewAI (Crew, Agent)\n  - OpenAI Agents SDK (Agent, query)\n- Locate AI route handlers or chat functions\n- Check Mem0 is already initialized\n\nPhase 2: Integration Planning\nGoal: Determine where to add memory hooks\n\nActions:\n- Identify chat entry points\n- Find message handling logic\n- Determine user/session identification method\n- Plan memory retrieval and storage points\n\nPhase 3: Implementation\nGoal: Add conversation memory integration\n\nActions:\n\nLaunch the mem0-integrator agent to add conversation memory.\n\nProvide the agent with:\n- Framework: [Detected from Phase 1]\n- Integration points: [Identified in Phase 2]\n- Requirements:\n  - Add memory retrieval before AI generation\n  - Add memory storage after AI response\n  - Include user_id and session_id tracking\n  - Handle conversation context automatically\n  - Add error handling for memory operations\n  - Generate framework-specific integration code\n- Expected output: Complete integration with conversation memory\n\nPhase 4: Verification\nGoal: Test conversation memory works\n\nActions:\n- Test memory is stored after conversations\n- Test memory is retrieved in next conversation\n- Verify context improves AI responses\n- Check user isolation works correctly\n\nPhase 5: Summary\nGoal: Show what was integrated\n\nActions:\n- Display integration results:\n  - Framework: [Name]\n  - Files modified: [List]\n  - Memory hooks added: [List]\n- Show usage:\n  - How conversation memory is stored\n  - How context is retrieved automatically\n  - How to customize memory behavior\n- Provide next steps:\n  - Test with multi-turn conversations\n  - Customize memory filtering if needed\n  - Use /mem0:add-user-memory for preferences\n  - Use /mem0:configure for advanced settings"
              },
              {
                "name": "/add-graph-memory",
                "description": "Enable graph memory for tracking relationships between memories and entities",
                "path": "plugins/mem0/commands/add-graph-memory.md",
                "frontmatter": {
                  "description": "Enable graph memory for tracking relationships between memories and entities",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Enable graph memory to track relationships between memories, entities, and concepts.\n\nCore Principles:\n- Relationship tracking between memories\n- Entity recognition and linking\n- Knowledge graph construction\n- Advanced queries for connected data\n\nPhase 1: Capability Check\nGoal: Verify graph memory is available\n\nActions:\n- Check deployment mode (Platform or OSS)\n- If Platform: Graph memory available\n- If OSS: Check if memory_relationships table exists\n- Verify current Mem0 configuration\n\nPhase 2: Graph Schema Planning\nGoal: Design relationship structure\n\nActions:\n- Determine what relationships to track:\n  - Entity relationships (person ‚Üí works at ‚Üí company)\n  - Memory connections (topic A ‚Üí related to ‚Üí topic B)\n  - Temporal relationships (event A ‚Üí happened before ‚Üí event B)\n- Plan relationship types and properties\n- Design query patterns for graph traversal\n\nPhase 3: Implementation\nGoal: Enable graph memory features\n\nActions:\n\nLaunch the mem0-integrator agent to enable graph memory.\n\nProvide the agent with:\n- Deployment mode: [Platform or OSS]\n- Relationship schema: [Designed in Phase 2]\n- Requirements:\n  - Enable graph memory in configuration\n  - If OSS: Create/verify memory_relationships table\n  - Add entity extraction from conversations\n  - Store relationships automatically\n  - Create helper functions for graph queries\n  - Add examples for common relationship queries\n- Expected output: Complete graph memory system\n\nPhase 4: Verification\nGoal: Test graph memory works\n\nActions:\n- Test relationships are extracted and stored\n- Test graph queries return connected memories\n- Verify relationship types are correct\n- Check performance of graph traversal\n\nPhase 5: Summary\nGoal: Show what was enabled\n\nActions:\n- Display graph memory setup:\n  - Graph memory: Enabled\n  - Relationship types: [List]\n  - Query helpers: [List]\n  - Files modified: [List]\n- Show usage examples:\n  - How to query related memories\n  - How to traverse the knowledge graph\n  - How to visualize relationships\n  - How to manually add relationships\n- Provide next steps:\n  - Test with entity-rich conversations\n  - Build knowledge graph queries\n  - Use /mem0:configure for graph thresholds\n  - Use /mem0:test to validate graph operations\n- Provide documentation:\n  - Platform: https://docs.mem0.ai/platform/features/graph-memory\n  - OSS: https://docs.mem0.ai/open-source/features/graph-memory"
              },
              {
                "name": "/add-user-memory",
                "description": "Add user preference and profile memory tracking across conversations",
                "path": "plugins/mem0/commands/add-user-memory.md",
                "frontmatter": {
                  "description": "Add user preference and profile memory tracking across conversations",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add user-level memory to track preferences, profile data, and learned facts across all conversations.\n\nCore Principles:\n- Persistent user context\n- Profile data extraction\n- Preference learning\n- Cross-conversation continuity\n\nPhase 1: User Model Analysis\nGoal: Understand current user data structure\n\nActions:\n- Find user model/schema in codebase\n- Check existing user properties\n- Identify user identification method (user_id, email, etc.)\n- Locate user profile or settings areas\n\nPhase 2: Memory Schema Design\nGoal: Plan user memory structure\n\nActions:\n- Determine what user data should be remembered:\n  - Preferences (language, tone, interests)\n  - Profile facts (occupation, location, etc.)\n  - Learned information (habits, patterns)\n  - Long-term context\n- Design memory categories and tags\n- Plan memory retrieval queries\n\nPhase 3: Implementation\nGoal: Add user memory tracking\n\nActions:\n\nLaunch the mem0-integrator agent to add user memory.\n\nProvide the agent with:\n- User model: [Found in Phase 1]\n- Memory schema: [Designed in Phase 2]\n- Requirements:\n  - Extract user preferences from conversations\n  - Store user-level memories with user_id\n  - Retrieve user context at conversation start\n  - Update user memories as new info is learned\n  - Add helper functions for user memory management\n  - Generate UI for viewing/editing user memories (optional)\n- Expected output: Complete user memory system\n\nPhase 4: Verification\nGoal: Test user memory works\n\nActions:\n- Test preferences are learned from conversations\n- Test user memories persist across sessions\n- Verify user isolation (one user can't see another's memories)\n- Check memory retrieval is fast and relevant\n\nPhase 5: Summary\nGoal: Show what was added\n\nActions:\n- Display implementation:\n  - User memory tracking: Enabled\n  - Memory categories: [List]\n  - Helper functions: [List]\n  - Files modified: [List]\n- Show usage examples:\n  - How to manually add user memories\n  - How to query user preferences\n  - How to update user memories\n  - How to delete user data (GDPR compliance)\n- Provide next steps:\n  - Test with user-specific preferences\n  - Add UI for user memory management\n  - Use /mem0:configure for retention policies\n  - Use /mem0:test to validate user isolation"
              },
              {
                "name": "/configure",
                "description": "Configure Mem0 settings (memory types, retention, embeddings, rerankers, webhooks)",
                "path": "plugins/mem0/commands/configure.md",
                "frontmatter": {
                  "description": "Configure Mem0 settings (memory types, retention, embeddings, rerankers, webhooks)",
                  "argument-hint": [
                    "setting-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure Mem0 settings for memory types, retention policies, embedding models, rerankers, webhooks, and custom categories.\n\nCore Principles:\n- Interactive configuration\n- Explain options and tradeoffs\n- Validate settings before applying\n- Document configuration choices\n\nPhase 1: Configuration Discovery\nGoal: Determine what to configure\n\nActions:\n- If $ARGUMENTS specifies setting, focus on that\n- Otherwise, show configuration menu with options:\n  - Memory types (user/agent/session)\n  - Retention policies (expiration, archival)\n  - Embedding models (OpenAI, HuggingFace, etc.)\n  - Rerankers (for better retrieval)\n  - Webhooks (memory events)\n  - Custom categories\n  - Graph memory settings\n- Load current configuration\n- Check deployment mode (Platform or OSS)\n\nPhase 2: Configuration Planning\nGoal: Gather configuration preferences\n\nActions:\n- Use AskUserQuestion to ask about configuration:\n  - \"Which setting do you want to configure?\" (if not specified)\n  - Based on selection, ask specific questions:\n    - Retention: \"How long should memories persist?\"\n    - Embeddings: \"Which embedding model do you want?\"\n    - Rerankers: \"Enable reranker for better search?\"\n    - Webhooks: \"Configure webhooks for memory events?\"\n- Explain tradeoffs for each option\n- Recommend defaults for common use cases\n\nPhase 3: Implementation\nGoal: Apply configuration changes\n\nActions:\n\nLaunch the mem0-integrator agent to apply configuration.\n\nProvide the agent with:\n- Configuration target: [Selected setting]\n- New values: [From Phase 2]\n- Deployment mode: [Platform or OSS]\n- Requirements:\n  - Update configuration files\n  - Modify environment variables if needed\n  - Add necessary imports/dependencies\n  - Update memory client initialization\n  - Add validation for new settings\n  - Document configuration in comments\n- Expected output: Updated configuration with settings applied\n\nPhase 4: Verification\nGoal: Validate configuration works\n\nActions:\n- Test new configuration loads correctly\n- If embeddings changed: Test embedding generation\n- If reranker enabled: Test search quality improvement\n- If webhooks: Test webhook delivery\n- Verify no breaking changes\n\nPhase 5: Summary\nGoal: Document what was configured\n\nActions:\n- Display configuration changes:\n  - Setting: [Name]\n  - Old value: [Previous]\n  - New value: [Current]\n  - Files modified: [List]\n- Show impact of changes\n- Provide recommendations:\n  - When to adjust settings\n  - How to monitor performance\n  - Related configuration options\n- Provide next steps:\n  - Run /mem0:test to validate\n  - Monitor memory operations\n  - Adjust based on usage patterns\n- Provide documentation:\n  - Platform features: https://docs.mem0.ai/platform/features/platform-overview\n  - OSS configuration: https://docs.mem0.ai/open-source/configuration"
              },
              {
                "name": "/init-mcp",
                "description": "Setup Mem0 with OpenMemory MCP server for local-first AI memory",
                "path": "plugins/mem0/commands/init-mcp.md",
                "frontmatter": {
                  "description": "Setup Mem0 with OpenMemory MCP server for local-first AI memory",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Initialize Mem0 using the official OpenMemory MCP server for private, local-first memory management.\n\nCore Principles:\n- Local-first approach (no cloud sync)\n- MCP protocol for cross-tool compatibility\n- Load API keys from environment (~/.bashrc)\n- Validate MCP server connectivity\n- Test memory operations\n\nPhase 1: Prerequisites Check\nGoal: Verify requirements for OpenMemory MCP\n\nActions:\n- Check if MEM0_API_KEY exists in ~/.bashrc\n- Check if OPENAI_API_KEY exists in environment\n- Verify Docker is installed (required for OpenMemory)\n- Check if port 8765 is available\n- Display prerequisites status\n\nIf missing keys:\n- Show instructions to add to ~/.bashrc:\n  ```bash\n  export MEM0_API_KEY=\"your-key-here\"\n  export OPENAI_API_KEY=\"your-openai-key\"\n  ```\n\nPhase 2: OpenMemory Installation\nGoal: Install and start OpenMemory MCP server\n\nActions:\n- Use AskUserQuestion to confirm installation:\n  - \"Install OpenMemory MCP server? This will use Docker to run locally.\"\n  - Options: \"Install now\", \"Already installed\", \"Skip\"\n\nIf \"Install now\":\n- Run: `curl -sL https://raw.githubusercontent.com/mem0ai/mem0/main/openmemory/run.sh | bash`\n- Wait for Docker containers to start\n- Verify server is running on http://localhost:8765\n\nIf \"Already installed\":\n- Check if server is running\n- If not running, provide start command\n\nPhase 3: MCP Configuration\nGoal: Configure project to use OpenMemory MCP\n\nActions:\n\nLaunch the mem0-integrator agent to configure MCP integration.\n\nProvide the agent with:\n- Deployment mode: MCP (OpenMemory)\n- MCP endpoint: http://localhost:8765/mcp/claude-code/sse/default\n- API key source: ~/.bashrc (MEM0_API_KEY)\n- Requirements:\n  - Configure MCP client connection\n  - Test MCP server connectivity\n  - Verify memory operations work\n  - Setup error handling\n  - Configure user ID for memory isolation\n  - Document MCP endpoints\n- Expected output: Working MCP integration with tested memory operations\n\nPhase 4: Verification\nGoal: Validate OpenMemory MCP is working\n\nActions:\n- Test MCP server is responding at http://localhost:8765\n- Check MCP API docs at http://localhost:8765/docs\n- Test memory add operation via MCP\n- Test memory search operation via MCP\n- Verify UI is accessible at http://localhost:8765\n- Show memory operation examples\n\nPhase 5: Summary\nGoal: Provide MCP setup documentation\n\nActions:\n- Display setup summary:\n  - OpenMemory MCP status: [Running/Not running]\n  - MCP endpoint: http://localhost:8765\n  - UI dashboard: http://localhost:8765\n  - API docs: http://localhost:8765/docs\n  - Configuration: [Files modified]\n- Show MCP features:\n  - Local-first (no cloud sync)\n  - Cross-tool memory sharing\n  - Private and secure\n  - Built-in UI for memory management\n- Provide usage instructions:\n  - How to start/stop OpenMemory\n  - How to view memories in UI\n  - How to use MCP tools in code\n  - How to configure user isolation\n- Provide next steps:\n  - Add conversation memory: /mem0:add-conversation-memory\n  - Add user memory: /mem0:add-user-memory\n  - Enable graph memory: /mem0:add-graph-memory\n  - Test setup: /mem0:test\n- Show comparison with other modes:\n  - MCP: Local-first, private, cross-tool\n  - Platform: Managed, enterprise, cloud\n  - OSS: Self-hosted, full control, Supabase\n- Provide documentation:\n  - OpenMemory docs: https://docs.mem0.ai/openmemory\n  - MCP protocol: https://modelcontextprotocol.io\n  - GitHub: https://github.com/mem0ai/mem0-mcp"
              },
              {
                "name": "/init-oss",
                "description": "Setup self-hosted Mem0 OSS with Supabase backend and pgvector",
                "path": "plugins/mem0/commands/init-oss.md",
                "frontmatter": {
                  "description": "Setup self-hosted Mem0 OSS with Supabase backend and pgvector",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup Mem0 Open Source (self-hosted) with Supabase PostgreSQL + pgvector backend.\n\nCore Principles:\n- Full control over infrastructure\n- Supabase provides database, auth, storage\n- pgvector for embeddings storage\n- Cost-effective for production\n\nPhase 1: Supabase Validation\nGoal: Ensure Supabase is initialized\n\nActions:\n- Check if Supabase is already initialized:\n  - Look for .mcp.json with supabase server\n  - Check for SUPABASE_* environment variables\n- If not initialized:\n  - Run /supabase:init to setup Supabase\n  - Wait for completion\n  - Verify Supabase MCP connectivity\n\nPhase 2: Package Installation\nGoal: Install Mem0 OSS with all dependencies\n\nActions:\n- Detect language (Python or JavaScript/TypeScript)\n- Install correct package with full dependencies:\n  - Python: pip install \"mem0ai[all]\"\n  - JavaScript/TypeScript: npm install mem0ai pg @supabase/supabase-js\n- Verify installation successful\n\nPhase 3: Database Setup\nGoal: Create memory tables in Supabase\n\nActions:\n\nLaunch the mem0-integrator agent to setup OSS database.\n\nProvide the agent with:\n- Mode: Open Source (self-hosted)\n- Backend: Supabase PostgreSQL + pgvector\n- Requirements:\n  - Enable pgvector extension\n  - Create memories table with embedding vector column\n  - Create memory_relationships table (for graph memory)\n  - Setup indexes for performance\n  - Create RLS policies for security\n  - Configure connection pooling\n- Expected output: Complete database schema with tables created\n\nPhase 4: Client Configuration\nGoal: Configure Mem0 to use Supabase\n\nActions:\n- Generate memory client configuration\n- Configure vector store to use PostgreSQL\n- Setup embedding model (OpenAI or custom)\n- Add environment variables for Supabase connection\n- Create client initialization code with Supabase config\n\nPhase 5: Verification\nGoal: Test OSS setup with Supabase\n\nActions:\n- Test database connection\n- Verify pgvector extension is enabled\n- Test memory operations (add, search)\n- Check RLS policies are active\n- Validate embeddings storage\n\nPhase 6: Summary\nGoal: Show setup results and next steps\n\nActions:\n- Display what was configured:\n  - Supabase: Initialized with pgvector\n  - Database: Memory tables created\n  - Package: mem0ai[all]@version\n  - Client: Configured for PostgreSQL backend\n- Show next steps:\n  1. Configure embedding model in .env\n  2. Test memory operations\n  3. Use /mem0:add-conversation-memory for chat integration\n  4. Use /mem0:add-graph-memory for relationships\n  5. Run /mem0:test for complete validation\n- Provide documentation links:\n  - OSS: https://docs.mem0.ai/open-source/overview\n  - Supabase integration: https://docs.mem0.ai/open-source/configuration"
              },
              {
                "name": "/init-platform",
                "description": "Setup hosted Mem0 Platform with API keys and quick configuration",
                "path": "plugins/mem0/commands/init-platform.md",
                "frontmatter": {
                  "description": "Setup hosted Mem0 Platform with API keys and quick configuration",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup Mem0 Platform (hosted mode) with API key configuration and quick start code.\n\nCore Principles:\n- Quick setup with minimal configuration\n- Platform handles infrastructure\n- Enterprise features ready to use\n- Provide usage examples\n\nPhase 1: Package Installation\nGoal: Install Mem0 Platform client\n\nActions:\n- Detect language (Python or JavaScript/TypeScript)\n- Install correct package:\n  - Python: pip install mem0ai\n  - JavaScript/TypeScript: npm install mem0ai\n- Verify installation successful\n- Check package versions\n\nPhase 2: API Key Configuration\nGoal: Setup environment variables\n\nActions:\n- Create or update .env file with MEM0_API_KEY placeholder\n- Add .env to .gitignore if not already present\n- Create .env.example for documentation\n- Explain how to get API key from https://app.mem0.ai\n\nPhase 3: Client Code Generation\nGoal: Create memory client initialization code\n\nActions:\n\nLaunch the mem0-integrator agent to generate Platform client code.\n\nProvide the agent with:\n- Mode: Platform (hosted)\n- Language: [Detected from Phase 1]\n- Requirements:\n  - Generate MemoryClient initialization code\n  - Add example memory operations (add, search, get, update, delete)\n  - Include error handling\n  - Add TypeScript types (if applicable)\n- Expected output: Working memory client code with examples\n\nPhase 4: Verification\nGoal: Test Platform connection\n\nActions:\n- Test client initialization (will fail without API key, but validates code)\n- Show sample usage code\n- Verify environment setup is correct\n\nPhase 5: Summary\nGoal: Show setup results and instructions\n\nActions:\n- Display what was configured:\n  - Package installed: mem0ai@version\n  - Client code created: [file path]\n  - Environment template: .env.example\n- Show next steps:\n  1. Get API key from https://app.mem0.ai\n  2. Add key to .env file: MEM0_API_KEY=your-key-here\n  3. Test with sample code\n  4. Use /mem0:add-conversation-memory to integrate with chat\n  5. Use /mem0:configure for advanced settings\n- Provide documentation link: https://docs.mem0.ai/platform/quickstart"
              },
              {
                "name": "/init",
                "description": "Initialize Mem0 (Platform, OSS, or MCP) - intelligent router that asks deployment mode and routes to appropriate init command",
                "path": "plugins/mem0/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Mem0 (Platform, OSS, or MCP) - intelligent router that asks deployment mode and routes to appropriate init command",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Initialize Mem0 in the current project by detecting existing setup and asking user for Platform (hosted), Open Source (self-hosted with Supabase), or MCP (local OpenMemory) deployment mode.\n\nCore Principles:\n- Ask user for deployment preference (Platform vs OSS vs MCP)\n- Detect existing frameworks and adapt integration\n- Verify setup works before completing\n- Provide clear next steps\n\nPhase 1: Discovery\nGoal: Understand project context and requirements\n\nActions:\n- Detect project type and language:\n  - Check for package.json (Node.js/TypeScript)\n  - Check for requirements.txt or pyproject.toml (Python)\n  - Check for existing AI frameworks (Vercel AI SDK, LangChain, etc.)\n- Check if Mem0 is already installed:\n  - Look for mem0ai in dependencies\n  - Check for existing memory configuration\n- Check if Supabase is initialized:\n  - Look for .mcp.json with supabase server\n  - Check for SUPABASE_* environment variables\n- Check if OpenMemory MCP is running:\n  - Test connection to http://localhost:8765\n  - Check MEM0_API_KEY in ~/.bashrc\n\nPhase 2: Deployment Mode Selection\nGoal: Ask user how they want to deploy Mem0\n\nActions:\n- Use AskUserQuestion to ask:\n  - \"Which deployment mode do you want?\"\n    - MCP (Local): Private, local-first, cross-tool memory (OpenMemory)\n    - Platform (Hosted): Managed by Mem0, quick setup, enterprise features\n    - Open Source (Self-hosted): Full control, Supabase backend, unlimited usage\n- If user selects MCP:\n  - Check if MEM0_API_KEY exists in ~/.bashrc\n  - Check if OpenMemory is running\n  - Route to /mem0:init-mcp\n- If user selects Platform:\n  - Proceed to Phase 3 with Platform mode\n  - Route to /mem0:init-platform\n- If user selects OSS:\n  - Check if Supabase is initialized\n  - If not, warn that Supabase is required for OSS mode\n  - Suggest running /supabase:init first\n  - Proceed to Phase 3 with OSS mode\n  - Route to /mem0:init-oss\n\nPhase 3: Integration Planning\nGoal: Determine what needs to be integrated\n\nActions:\n- Based on detected frameworks, plan integration approach\n- Identify where memory operations should be added\n- Check for existing memory patterns in codebase\n- Determine if graph memory is needed (ask user if unclear)\n\nPhase 4: Implementation\nGoal: Setup Mem0 with chosen deployment mode\n\nActions:\n\nLaunch the mem0-integrator agent to initialize Mem0.\n\nProvide the agent with:\n- Deployment mode: [Platform or OSS from Phase 2]\n- Project type: [Detected language and frameworks]\n- Supabase status: [Initialized or not]\n- Integration targets: [Frameworks that need memory integration]\n- Requirements:\n  - Install correct packages (mem0ai or mem0ai[all])\n  - Configure environment variables\n  - Create memory client configuration\n  - Generate integration code for detected frameworks\n  - If OSS mode: Setup Supabase tables and pgvector\n  - Test memory operations work correctly\n- Expected output: Complete Mem0 setup with working memory operations\n\nPhase 5: Verification\nGoal: Ensure setup is correct\n\nActions:\n- Test Mem0 client initialization\n- Run a simple memory operation (add and search)\n- Verify environment variables are set\n- Check Supabase connection (if OSS mode)\n- Confirm integration code is correct\n\nPhase 6: Summary\nGoal: Show what was accomplished and next steps\n\nActions:\n- Display setup results:\n  - Deployment mode: [Platform or OSS]\n  - Installed packages: [List]\n  - Configuration files created: [List]\n  - Environment variables needed: [List]\n  - Integration code added: [List]\n- Show next steps:\n  - Set API keys in .env file (Platform mode)\n  - Run /mem0:add-conversation-memory to integrate with chat\n  - Run /mem0:add-user-memory to track user preferences\n  - Run /mem0:configure to customize memory settings\n  - Run /mem0:test to validate complete setup\n- Provide links to documentation:\n  - Platform: https://docs.mem0.ai/platform/quickstart\n  - OSS: https://docs.mem0.ai/open-source/overview"
              },
              {
                "name": "/migrate-to-supabase",
                "description": "Migrate from Mem0 Platform to Open Source with Supabase backend",
                "path": "plugins/mem0/commands/migrate-to-supabase.md",
                "frontmatter": {
                  "description": "Migrate from Mem0 Platform to Open Source with Supabase backend",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Migrate from Mem0 Platform (hosted) to Mem0 OSS (self-hosted with Supabase), preserving all memories and relationships.\n\nCore Principles:\n- Export data safely from Platform\n- Setup OSS infrastructure\n- Import data to Supabase\n- Verify data integrity\n- Update application code\n\nPhase 1: Pre-Migration Validation\nGoal: Ensure migration is feasible\n\nActions:\n- Verify currently using Mem0 Platform\n- Check memory count and size\n- Estimate migration time\n- Warn about potential downtime\n- Use AskUserQuestion to confirm:\n  - \"Ready to migrate from Platform to OSS? This will require downtime.\"\n  - \"Do you have Supabase account ready?\"\n\nPhase 2: Data Export\nGoal: Export all memories from Platform\n\nActions:\n- Use Mem0 Platform export API\n- Export memories with metadata\n- Export relationships (if graph memory enabled)\n- Export user and agent data\n- Save exports to local files\n- Verify export completeness\n- Create backup of exports\n\nPhase 3: OSS Setup\nGoal: Initialize Mem0 OSS with Supabase\n\nActions:\n- Run /supabase:init if not already setup\n- Run /mem0:init-oss to setup OSS mode\n- Wait for setup to complete\n- Verify Supabase tables are created\n- Verify pgvector extension is enabled\n\nPhase 4: Data Import\nGoal: Import memories to Supabase\n\nActions:\n\nLaunch the mem0-integrator agent to import data.\n\nProvide the agent with:\n- Export files: [From Phase 2]\n- Target: Supabase OSS setup\n- Requirements:\n  - Import all memories to Supabase tables\n  - Preserve memory IDs and metadata\n  - Import relationships (if graph memory)\n  - Maintain user/agent associations\n  - Verify vector embeddings\n  - Handle import errors gracefully\n  - Provide progress updates\n- Expected output: Complete data migration to Supabase\n\nPhase 5: Application Updates\nGoal: Update code to use OSS instead of Platform\n\nActions:\n- Update memory client from Platform to OSS configuration\n- Change from MemoryClient to Memory with Supabase config\n- Update environment variables\n- Remove Platform API key\n- Add Supabase connection variables\n- Test memory operations work with new setup\n\nPhase 6: Verification\nGoal: Validate migration was successful\n\nActions:\n- Run /mem0:test to validate OSS setup\n- Compare memory counts (Platform export vs Supabase import)\n- Test sample memory queries return correct results\n- Verify relationships preserved (if graph memory)\n- Check user isolation still works\n- Test application functionality end-to-end\n\nPhase 7: Summary\nGoal: Document migration results\n\nActions:\n- Display migration summary:\n  - Memories exported from Platform: [Count]\n  - Memories imported to Supabase: [Count]\n  - Relationships migrated: [Count]\n  - Data integrity: [Verified/Issues]\n  - Application updated: [Files modified]\n- Show post-migration tasks:\n  - Cancel Platform subscription (if desired)\n  - Monitor OSS performance\n  - Setup backups for Supabase\n  - Configure retention policies\n- Provide next steps:\n  - Optimize OSS performance\n  - Setup monitoring\n  - Configure auto-backups\n  - Use /mem0:configure for OSS tuning\n- Provide documentation:\n  - OSS configuration: https://docs.mem0.ai/open-source/configuration\n  - Supabase best practices: https://supabase.com/docs/guides/database"
              },
              {
                "name": "/test",
                "description": "Test Mem0 functionality end-to-end (setup, operations, performance, security)",
                "path": "plugins/mem0/commands/test.md",
                "frontmatter": {
                  "description": "Test Mem0 functionality end-to-end (setup, operations, performance, security)",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the mem0 plugin:\n\n- **memory-design-patterns**: Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.\n- **memory-optimization**: Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.\n- **supabase-integration**: Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Comprehensively test Mem0 installation, operations, performance, and security.\n\nCore Principles:\n- Test all memory operations\n- Validate performance benchmarks\n- Check security configuration\n- Provide actionable recommendations\n\nPhase 1: Setup Validation\nGoal: Verify Mem0 is properly initialized\n\nActions:\n- Check Mem0 packages are installed\n- Verify configuration files exist\n- Check environment variables are set (MEM0_API_KEY in ~/.bashrc)\n- Validate deployment mode (Platform, OSS, or MCP)\n- If MCP: Verify OpenMemory server is running at http://localhost:8765\n- If Platform: Verify MEM0_API_KEY is loaded from ~/.bashrc\n- If OSS: Verify Supabase connection and pgvector extension\n\nPhase 2: Operations Testing\nGoal: Test all memory operations work\n\nActions:\n\nLaunch the mem0-verifier agent to test Mem0 operations.\n\nProvide the agent with:\n- Test scope: Comprehensive\n- Requirements:\n  - Test add memory (single and batch)\n  - Test search memory (various queries)\n  - Test update memory\n  - Test delete memory\n  - Test get memory by ID\n  - If graph memory: Test relationships\n  - If webhooks: Test delivery\n  - Verify error handling\n  - Check response formats\n- Expected output: Test report with pass/fail status\n\nPhase 3: Performance Benchmarking\nGoal: Measure memory operation performance\n\nActions:\n- Measure latency for each operation:\n  - Add memory (target: < 500ms)\n  - Search memory (target: < 200ms)\n  - Update memory (target: < 300ms)\n- Test concurrent operations\n- Check database performance (if OSS with Supabase)\n- Identify bottlenecks\n\nPhase 4: Security Audit\nGoal: Validate security configuration\n\nActions:\n- Check for exposed API keys in code\n- Verify environment variables not hardcoded\n- If OSS: Test RLS policies (user isolation)\n- Check data encryption\n- Validate access controls\n- Test GDPR compliance features (delete user data)\n\nPhase 5: Summary\nGoal: Present comprehensive test report\n\nActions:\n- Display test results:\n  - Setup validation: ‚úÖ Pass / ‚ùå Fail\n  - Memory operations: [X/Y passed]\n  - Performance benchmarks: [Latency results]\n  - Security audit: [Findings]\n- Show issues found:\n  - Critical issues (fix immediately)\n  - Warnings (should address)\n  - Recommendations (nice to have)\n- Provide fixes for failures:\n  - Exact steps to resolve\n  - Code changes needed\n  - Configuration adjustments\n- Provide next steps:\n  - Fix critical issues\n  - Optimize performance bottlenecks\n  - Address security findings\n  - Re-run test after fixes\n- Provide documentation:\n  - Platform FAQs: https://docs.mem0.ai/platform/faqs\n  - OSS troubleshooting: https://docs.mem0.ai/open-source/overview"
              }
            ],
            "skills": [
              {
                "name": "memory-design-patterns",
                "description": "Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.",
                "path": "plugins/mem0/skills/memory-design-patterns/SKILL.md",
                "frontmatter": {
                  "name": "memory-design-patterns",
                  "description": "Best practices for memory architecture design including user vs agent vs session memory patterns, vector vs graph memory tradeoffs, retention strategies, and performance optimization. Use when designing memory systems, architecting AI memory layers, choosing memory types, planning retention strategies, or when user mentions memory architecture, user memory, agent memory, session memory, memory patterns, vector storage, graph memory, or Mem0 architecture.",
                  "allowed-tools": "Read, Write, Bash, Edit"
                },
                "content": "# Memory Design Patterns\n\nProduction-ready memory architecture patterns for AI applications using Mem0. This skill provides comprehensive guidance on designing scalable, performant memory systems with proper isolation, retention strategies, and optimization techniques.\n\n## Instructions\n\n### Phase 1: Understand Memory Types\n\nMem0 provides three distinct memory scopes, each serving different purposes:\n\n#### 1. User Memory (Persistent Preferences & Profile)\n**Purpose**: Long-term personal preferences, profile data, and user characteristics that persist across all interactions.\n\n**Use Cases**:\n- User preferences (dietary restrictions, communication style, language preferences)\n- Personal information (location, occupation, family details)\n- Long-term goals and interests\n- Historical context that should persist indefinitely\n\n**Implementation**:\n```python\n# Add user-level memory\nmemory.add(\n    \"User prefers concise responses without technical jargon\"\n    user_id=\"customer_bob\"\n)\n\n# Search user memories\nuser_context = memory.search(\n    \"communication style\"\n    user_id=\"customer_bob\"\n)\n```\n\n**Key Characteristics**:\n- Persists indefinitely (or until explicitly deleted)\n- Shared across all agents interacting with this user\n- Should contain stable, long-term information\n- Typically 10-50 memories per user\n\n#### 2. Agent Memory (Agent-Specific Context)\n**Purpose**: Agent-specific knowledge, behaviors, and learned patterns that apply across all users interacting with this agent.\n\n**Use Cases**:\n- Agent capabilities and limitations\n- Domain-specific knowledge\n- Learned behaviors and patterns\n- Agent-specific instructions and protocols\n\n**Implementation**:\n```python\n# Add agent-level memory\nmemory.add(\n    \"When handling refund requests, always check order date first\"\n    agent_id=\"support_agent_v2\"\n)\n\n# Search agent memories\nagent_context = memory.search(\n    \"refund process\"\n    agent_id=\"support_agent_v2\"\n)\n```\n\n**Key Characteristics**:\n- Shared across all users interacting with this agent\n- Contains agent-specific procedures and knowledge\n- Moderate retention (days to months)\n- Typically 50-200 memories per agent\n\n#### 3. Session/Run Memory (Temporary Conversation Context)\n**Purpose**: Ephemeral context specific to a single conversation or task session.\n\n**Use Cases**:\n- Current conversation topic\n- Temporary task context\n- Session-specific state\n- Short-term working memory\n\n**Implementation**:\n```python\n# Add session-level memory\nmemory.add(\n    \"Current issue: payment failed with error code 402\"\n    run_id=\"session_12345_20250115\"\n)\n\n# Search session memories\nsession_context = memory.search(\n    \"current issue\"\n    run_id=\"session_12345_20250115\"\n)\n```\n\n**Key Characteristics**:\n- Short-lived (minutes to hours)\n- Isolated to specific conversation or task\n- Should be cleaned up after session ends\n- Typically 5-20 memories per session\n\n### Phase 2: Choose Storage Backend (Vector vs Graph)\n\n#### Vector Memory (Default)\n**How It Works**: Embeddings stored in vector database, semantic similarity search using cosine distance.\n\n**Strengths**:\n- Fast semantic search\n- Excellent for unstructured data\n- Low setup complexity\n- Works out-of-the-box with Mem0\n\n**Weaknesses**:\n- Cannot query relationships\n- No explicit entity connections\n- Limited reasoning about connections\n\n**Best For**:\n- Simple preference storage\n- Document/chunk retrieval\n- Semantic search use cases\n- Quick prototyping\n\n**Configuration**:\n```python\nfrom mem0 import Memory\n\n# Default vector-only configuration\nmemory = Memory()\n```\n\n#### Graph Memory (Advanced)\n**How It Works**: Entities and relationships stored in graph database (Neo4j/Memgraph), enables relationship traversal and complex queries.\n\n**Strengths**:\n- Explicit entity relationships\n- Complex query capabilities\n- Relationship reasoning\n- Multi-hop traversal\n\n**Weaknesses**:\n- Requires graph database setup\n- Higher infrastructure complexity\n- Slower for pure semantic search\n- More storage overhead\n\n**Best For**:\n- Multi-entity systems\n- Relationship-heavy domains\n- Complex reasoning requirements\n- Enterprise knowledge graphs\n\n**Configuration**:\n```python\nfrom mem0 import Memory\nfrom mem0.configs.base import MemoryConfig\n\nconfig = MemoryConfig(\n    graph_store={\n        \"provider\": \"neo4j\"\n        \"config\": {\n            \"url\": \"bolt://localhost:7687\"\n            \"username\": \"neo4j\"\n            \"password\": \"password\"\n        }\n    }\n)\nmemory = Memory(config)\n```\n\n**Decision Matrix**:\n| Use Case | Vector | Graph |\n|----------|--------|-------|\n| User preferences | ‚úÖ Best | ‚ö†Ô∏è Overkill |\n| Product recommendations | ‚úÖ Best | ‚ö†Ô∏è Overkill |\n| Customer support | ‚úÖ Good | ‚úÖ Better |\n| Knowledge management | ‚ö†Ô∏è Limited | ‚úÖ Best |\n| Multi-tenant systems | ‚úÖ Good | ‚úÖ Best |\n| Team collaboration | ‚ö†Ô∏è Limited | ‚úÖ Best |\n\n### Phase 3: Design Retention Strategy\n\nUse the retention strategy template:\n```bash\nbash scripts/generate-retention-policy.sh <memory-type> <retention-days>\n```\n\n#### Retention Guidelines\n\n**User Memory**:\n- Retention: Indefinite (with user control)\n- Cleanup: User-initiated deletion only\n- Archival: After 1 year of inactivity\n- GDPR: Must support right to deletion\n\n**Agent Memory**:\n- Retention: 90-180 days typical\n- Cleanup: Automatic based on relevance score\n- Versioning: Keep agent version history\n- Deprecation: Clear old agent memories on major updates\n\n**Session Memory**:\n- Retention: 1-24 hours\n- Cleanup: Automatic after session end\n- Conversion: Promote important memories to user/agent level\n- Storage: Consider in-memory for very short sessions\n\n#### Retention Implementation\n\nRun the retention analyzer:\n```bash\nbash scripts/analyze-retention.sh <user_id_or_agent_id>\n```\n\nThis script:\n1. Analyzes memory age and access patterns\n2. Identifies stale memories\n3. Suggests cleanup actions\n4. Generates retention reports\n\n### Phase 4: Implement Multi-Level Memory Pattern\n\n**Pattern**: Combine all three memory types for comprehensive context.\n\n**Template**: Use `templates/multi-level-memory-pattern.py`\n\n**Architecture**:\n```\nQuery Processing Flow:\n1. Retrieve session context (immediate)\n2. Retrieve user context (preferences)\n3. Retrieve agent context (capabilities)\n4. Merge contexts with priority weighting\n5. Generate response with full context\n```\n\n**Priority Weighting**:\n- Session: 40% weight (most relevant to current task)\n- User: 35% weight (personalizes response)\n- Agent: 25% weight (ensures consistent behavior)\n\n**Implementation**:\n```python\n# Retrieve all context levels\nsession_memories = memory.search(query, run_id=run_id)\nuser_memories = memory.search(query, user_id=user_id)\nagent_memories = memory.search(query, agent_id=agent_id)\n\n# Weighted merge\ncontext = merge_contexts(\n    session=session_memories\n    user=user_memories\n    agent=agent_memories\n    weights={\"session\": 0.4, \"user\": 0.35, \"agent\": 0.25}\n)\n```\n\n### Phase 5: Optimize Performance\n\n#### Vector Search Optimization\n\nRun the performance analyzer:\n```bash\nbash scripts/analyze-memory-performance.sh <project_name>\n```\n\n**Optimization Techniques**:\n\n1. **Limit Search Results**:\n   ```python\n   memories = memory.search(query, user_id=user_id, limit=5)\n   ```\n   - Default: 10 results\n   - Recommended: 3-5 for chat, 10-20 for RAG\n\n2. **Use Filters to Reduce Search Space**:\n   ```python\n   memories = memory.search(\n       query\n       filters={\n           \"AND\": [\n               {\"user_id\": \"alex\"}\n               {\"agent_id\": \"support_agent\"}\n           ]\n       }\n   )\n   ```\n\n3. **Cache Frequently Accessed Memories**:\n   - Cache user preferences (rarely change)\n   - Refresh cache every 5-10 minutes\n   - Invalidate on explicit memory updates\n\n4. **Batch Operations**:\n   ```python\n   # Add multiple memories in one call\n   memory.add(messages, user_id=user_id)\n   ```\n\n#### Graph Query Optimization\n\nFor graph memory:\n1. **Limit Traversal Depth**: Max 2-3 hops\n2. **Index Key Properties**: user_id, agent_id, timestamps\n3. **Use Relationship Filters**: Reduce unnecessary traversals\n4. **Monitor Query Performance**: Track slow queries > 100ms\n\n### Phase 6: Implement Cost Optimization\n\nRun the cost analyzer:\n```bash\nbash scripts/analyze-memory-costs.sh <user_id> <date_range>\n```\n\n**Cost Optimization Strategies**:\n\n1. **Deduplication**: Remove similar/redundant memories\n   ```bash\n   bash scripts/deduplicate-memories.sh <user_id>\n   ```\n\n2. **Archival**: Move old memories to cold storage\n   - Active: Last 30 days (vector DB)\n   - Archive: 30-180 days (compressed JSON)\n   - Long-term: > 180 days (S3/cold storage)\n\n3. **Compression**: Use shorter embeddings for less critical memories\n   - Critical: 1536 dimensions (OpenAI large)\n   - Standard: 768 dimensions (OpenAI small)\n   - Archival: 384 dimensions (lightweight model)\n\n4. **Smart Pruning**: Remove low-value memories\n   - Score-based: Keep only high relevance scores\n   - Access-based: Remove never-accessed memories\n   - Importance-based: User/agent priority tagging\n\n### Phase 7: Security and Isolation\n\n#### Multi-Tenant Isolation\n\n**Pattern**: Ensure complete data isolation between users/organizations.\n\n**Implementation**:\n```python\n# Always scope by user_id or org_id\nmemories = memory.search(\n    query\n    filters={\"user_id\": current_user_id}\n)\n\n# Validate access before retrieval\nif not user_has_access(user_id, requested_user_id):\n    raise PermissionError(\"Access denied\")\n```\n\n**Security Checklist**:\n- ‚úÖ Never allow cross-user memory access\n- ‚úÖ Validate all user_id parameters\n- ‚úÖ Implement org-level isolation for multi-tenant apps\n- ‚úÖ Audit memory access logs\n- ‚úÖ Encrypt sensitive memory content\n- ‚úÖ Support GDPR right to deletion\n\nRun the security audit:\n```bash\nbash scripts/audit-memory-security.sh\n```\n\n## Decision Trees\n\n### When to Use Each Memory Type\n\nUse the decision helper:\n```bash\nbash scripts/suggest-memory-type.sh \"<use_case_description>\"\n```\n\n**Quick Reference**:\n- User dietary preferences ‚Üí User Memory\n- Agent's SOP for task X ‚Üí Agent Memory\n- Current conversation topic ‚Üí Session Memory\n- Customer support ticket details ‚Üí Session Memory (promote to User if resolved)\n- System capabilities ‚Üí Agent Memory\n- User's birthday ‚Üí User Memory\n\n### Vector vs Graph Decision\n\nUse the architecture advisor:\n```bash\nbash scripts/suggest-storage-architecture.sh \"<project_description>\"\n```\n\n**Decision Criteria**:\n1. Need relationship traversal? ‚Üí Graph\n2. Pure semantic search? ‚Üí Vector\n3. < 10,000 memories total? ‚Üí Vector\n4. Complex entity relationships? ‚Üí Graph\n5. Team/org hierarchies? ‚Üí Graph\n6. Simple preference storage? ‚Üí Vector\n\n## Key Files\n\n**Scripts** (all functional, not placeholders):\n- `scripts/generate-retention-policy.sh` - Create retention policy configs\n- `scripts/analyze-retention.sh` - Analyze memory age and access patterns\n- `scripts/analyze-memory-performance.sh` - Performance profiling\n- `scripts/analyze-memory-costs.sh` - Cost analysis and optimization suggestions\n- `scripts/deduplicate-memories.sh` - Find and remove duplicate memories\n- `scripts/audit-memory-security.sh` - Security compliance checking\n- `scripts/suggest-memory-type.sh` - Interactive memory type advisor\n- `scripts/suggest-storage-architecture.sh` - Architecture recommendation tool\n\n**Templates**:\n- `templates/multi-level-memory-pattern.py` - Complete implementation\n- `templates/retention-policy.yaml` - Retention configuration\n- `templates/vector-only-config.py` - Vector memory setup\n- `templates/graph-memory-config.py` - Graph memory setup\n- `templates/hybrid-architecture.py` - Vector + Graph combined\n- `templates/cost-optimization-config.yaml` - Cost optimization settings\n\n**Examples**:\n- `examples/customer-support-memory-architecture.md` - Full implementation guide\n- `examples/multi-agent-collaboration.md` - Shared memory patterns\n- `examples/e-commerce-personalization.md` - Product recommendation memory\n- `examples/healthcare-assistant.md` - HIPAA-compliant memory architecture\n\n## Best Practices\n\n1. **Start Simple**: Use vector-only with user + session memories\n2. **Add Complexity as Needed**: Only introduce graph when relationships matter\n3. **Monitor Performance**: Track memory retrieval times and costs\n4. **Implement Retention Early**: Don't let memory grow unbounded\n5. **Test Isolation**: Verify cross-user memory access is impossible\n6. **Document Memory Schema**: Track what memories mean and when they're used\n7. **Version Agent Memories**: Clear separation between agent versions\n8. **Promote Important Memories**: Session ‚Üí User when patterns emerge\n9. **Use Metadata**: Tag memories with categories for better filtering\n10. **Regular Audits**: Monthly review of memory growth and costs\n\n## Troubleshooting\n\n**Slow Memory Retrieval**:\n- Reduce search limit\n- Add more specific filters\n- Check vector index performance\n- Consider caching\n\n**High Costs**:\n- Run cost analyzer script\n- Implement deduplication\n- Review retention policy\n- Archive old memories\n\n**Poor Search Results**:\n- Check embedding model quality\n- Verify memory content is descriptive\n- Use hybrid search (keyword + semantic)\n- Add metadata for filtering\n\n**Memory Leakage Between Users**:\n- Audit security script immediately\n- Review all memory queries for user_id filtering\n- Check RLS policies if using custom backends\n- Implement access logging\n\n---\n\n**Plugin**: mem0\n**Version**: 1.0.0\n**Last Updated**: 2025-10-27"
              },
              {
                "name": "memory-optimization",
                "description": "Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.",
                "path": "plugins/mem0/skills/memory-optimization/SKILL.md",
                "frontmatter": {
                  "name": "memory-optimization",
                  "description": "Performance optimization patterns for Mem0 memory operations including query optimization, caching strategies, embedding efficiency, database tuning, batch operations, and cost reduction for both Platform and OSS deployments. Use when optimizing memory performance, reducing costs, improving query speed, implementing caching, tuning database performance, analyzing bottlenecks, or when user mentions memory optimization, performance tuning, cost reduction, slow queries, caching, or Mem0 optimization.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Memory Optimization\n\nPerformance optimization patterns and tools for Mem0 memory systems. This skill provides comprehensive optimization techniques for query performance, cost reduction, caching strategies, and infrastructure tuning for both Platform and OSS deployments.\n\n## Instructions\n\n### Phase 1: Performance Assessment\n\nStart by analyzing your current memory system performance:\n\n```bash\nbash scripts/analyze-performance.sh [project_name]\n```\n\nThis generates a comprehensive performance report including:\n- Query latency metrics (average, P95, P99)\n- Operation throughput (searches, adds, updates, deletes)\n- Cache performance statistics\n- Resource utilization (memory, storage, CPU)\n- Slow query identification\n- Cost analysis\n\n**Review the output to identify optimization priorities:**\n- Query latency > 200ms ‚Üí Focus on query optimization\n- High costs ‚Üí Focus on cost optimization\n- Low cache hit rate < 60% ‚Üí Focus on caching\n- High resource usage ‚Üí Focus on infrastructure tuning\n\n### Phase 2: Query Optimization\n\nOptimize memory search operations for speed and efficiency.\n\n#### 2.1 Limit Search Results\n\n**Problem**: Retrieving too many results increases latency and costs.\n\n**Solution**: Use appropriate limit values based on use case.\n\n```python\n# ‚ùå BAD: Using default or excessive limits\nmemories = memory.search(query, user_id=user_id)  # Default: 10\n\n# ‚úÖ GOOD: Optimized limits\nmemories = memory.search(query, user_id=user_id, limit=5)  # Chat apps\nmemories = memory.search(query, user_id=user_id, limit=3)  # Quick context\nmemories = memory.search(query, user_id=user_id, limit=10) # RAG systems\n```\n\n**Impact**: 30-40% reduction in query time\n\n**Guidelines**:\n- Chat applications: 3-5 results\n- RAG context retrieval: 8-12 results\n- Recommendation systems: 10-20 results\n- Semantic search: 20-50 results\n\n#### 2.2 Use Filters to Reduce Search Space\n\n**Problem**: Searching entire index is slow and expensive.\n\n**Solution**: Apply filters to narrow search scope.\n\n```python\n# ‚ùå BAD: Full index scan\nmemories = memory.search(query)\n\n# ‚úÖ GOOD: Filtered search\nmemories = memory.search(\n    query\n    filters={\n        \"user_id\": user_id\n        \"categories\": [\"preferences\", \"profile\"]\n    }\n    limit=5\n)\n\n# ‚úÖ BETTER: Multiple filter conditions\nmemories = memory.search(\n    query\n    filters={\n        \"AND\": [\n            {\"user_id\": user_id}\n            {\"agent_id\": \"support_v2\"}\n            {\"created_after\": \"2025-01-01\"}\n        ]\n    }\n    limit=5\n)\n```\n\n**Impact**: 40-60% reduction in query time\n\n**Available Filters**:\n- `user_id`: Scope to specific user\n- `agent_id`: Scope to specific agent\n- `run_id`: Scope to session/run\n- `categories`: Filter by memory categories\n- `metadata`: Custom metadata filters\n- Date ranges: `created_after`, `created_before`\n\n#### 2.3 Optimize Reranking\n\n**Problem**: Default reranking may be overkill for simple queries.\n\n**Solution**: Configure reranker based on accuracy requirements.\n\n```python\n# Platform Mode (Mem0 Cloud)\nfrom mem0 import MemoryClient\n\n# Disable reranking for fast, simple queries\nmemory = MemoryClient(api_key=api_key)\nmemories = memory.search(\n    query\n    user_id=user_id\n    rerank=False  # 2x faster, slightly lower accuracy\n)\n\n# OSS Mode\nfrom mem0 import Memory\nfrom mem0.configs.base import MemoryConfig\n\n# Use lightweight reranker\nconfig = MemoryConfig(\n    reranker={\n        \"provider\": \"cohere\"\n        \"config\": {\n            \"model\": \"rerank-english-v3.0\",  # Fast model\n            \"top_n\": 5  # Rerank only top results\n        }\n    }\n)\nmemory = Memory(config)\n```\n\n**Reranker Options**:\n- **No reranking**: Fastest, 90-95% accuracy\n- **Lightweight (Cohere rerank-english-v3.0)**: 2x faster than full\n- **Full reranking (Cohere rerank-english-v3.5)**: Highest accuracy\n\n**Decision Guide**:\n- Simple preference retrieval ‚Üí No reranking\n- Chat context ‚Üí Lightweight reranking\n- Critical RAG applications ‚Üí Full reranking\n\n#### 2.4 Use Async Operations\n\n**Problem**: Blocking operations limit throughput.\n\n**Solution**: Use async for high-concurrency scenarios.\n\n```python\nimport asyncio\nfrom mem0 import AsyncMemory\n\nasync def get_user_context(user_id: str, queries: list[str]):\n    memory = AsyncMemory()\n\n    # Run multiple searches concurrently\n    results = await asyncio.gather(*[\n        memory.search(q, user_id=user_id, limit=3)\n        for q in queries\n    ])\n\n    return results\n\n# Usage\ncontexts = await get_user_context(\n    \"user_123\"\n    [\"preferences\", \"recent activity\", \"goals\"]\n)\n```\n\n**Impact**: 3-5x throughput improvement under load\n\n### Phase 3: Caching Strategies\n\nImplement multi-layer caching to reduce API calls and improve response times.\n\n#### 3.1 In-Memory Caching (Python)\n\n**Use for**: Frequently accessed, rarely changing data (user preferences).\n\n```python\nfrom functools import lru_cache\nimport hashlib\n\n@lru_cache(maxsize=1000)\ndef get_user_preferences(user_id: str) -> list:\n    \"\"\"Cache user preferences for 5 minutes\"\"\"\n    return memory.search(\n        \"user preferences\"\n        user_id=user_id\n        limit=5\n    )\n\n# Clear cache when preferences update\nget_user_preferences.cache_clear()\n```\n\n**Impact**: Near-instant response for cached queries\n\n**Configuration**:\n- `maxsize=1000`: Cache 1000 users' preferences\n- Clear cache on memory updates\n- TTL: Implement with time-based wrapper\n\n#### 3.2 Redis Caching (Production)\n\n**Use for**: Shared caching across services, TTL control.\n\n```python\nimport redis\nimport json\n\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\n\ndef get_user_context_cached(user_id: str, query: str) -> list:\n    # Generate cache key\n    cache_key = f\"mem0:search:{user_id}:{hashlib.md5(query.encode()).hexdigest()}\"\n\n    # Check cache\n    cached = redis_client.get(cache_key)\n    if cached:\n        return json.loads(cached)\n\n    # Cache miss - query Mem0\n    result = memory.search(query, user_id=user_id, limit=5)\n\n    # Cache result (5 minute TTL)\n    redis_client.setex(\n        cache_key\n        300,  # 5 minutes\n        json.dumps(result)\n    )\n\n    return result\n\n# Invalidate cache on update\ndef update_memory(user_id: str, message: str):\n    memory.add(message, user_id=user_id)\n\n    # Clear user's cache\n    pattern = f\"mem0:search:{user_id}:*\"\n    for key in redis_client.scan_iter(match=pattern):\n        redis_client.delete(key)\n```\n\n**Impact**: 50-70% reduction in API calls\n\n**TTL Guidelines**:\n- User preferences: 5-15 minutes\n- Agent knowledge: 30-60 minutes\n- Session context: 1-2 minutes\n- Static content: 1 hour\n\nUse the caching template generator:\n```bash\nbash scripts/generate-cache-config.sh redis [ttl_seconds]\n```\n\n#### 3.3 Edge Caching (Advanced)\n\n**Use for**: Global applications, very high traffic.\n\nSee template: `templates/edge-cache-config.yaml`\n\n### Phase 4: Embedding Optimization\n\nOptimize embedding generation and storage costs.\n\n#### 4.1 Choose Appropriate Embedding Model\n\n**Problem**: Oversized embeddings increase cost and latency.\n\n**Solution**: Match model to use case.\n\n```python\nfrom mem0 import Memory\nfrom mem0.configs.base import MemoryConfig\n\n# ‚ùå EXPENSIVE: Large model for simple data\nconfig = MemoryConfig(\n    embedder={\n        \"provider\": \"openai\"\n        \"config\": {\n            \"model\": \"text-embedding-3-large\",  # 3072 dims, $0.13/1M tokens\n        }\n    }\n)\n\n# ‚úÖ OPTIMIZED: Appropriate model\nconfig = MemoryConfig(\n    embedder={\n        \"provider\": \"openai\"\n        \"config\": {\n            \"model\": \"text-embedding-3-small\",  # 1536 dims, $0.02/1M tokens\n        }\n    }\n)\n```\n\n**Model Selection Guide**:\n\n| Use Case | Recommended Model | Dimensions | Cost |\n|----------|------------------|------------|------|\n| User preferences | text-embedding-3-small | 1536 | $0.02/1M |\n| Simple chat context | text-embedding-3-small | 1536 | $0.02/1M |\n| Advanced RAG | text-embedding-3-large | 3072 | $0.13/1M |\n| Multilingual | text-embedding-3-large | 3072 | $0.13/1M |\n| Budget-conscious | text-embedding-ada-002 | 1536 | $0.0001/1M |\n\n**Impact**: 70-85% cost reduction with appropriate model selection\n\n#### 4.2 Batch Embedding Generation\n\n**Problem**: Individual embedding calls have overhead.\n\n**Solution**: Batch multiple texts for embedding.\n\n```python\n# ‚ùå BAD: Individual embedding calls\nfor message in messages:\n    memory.add(message, user_id=user_id)  # Separate API call each\n\n# ‚úÖ GOOD: Batched operation\nmemory.add(messages, user_id=user_id)  # Single batched call\n```\n\n**Impact**: 40-60% reduction in embedding costs\n\n**Batch Size Guidelines**:\n- Platform Mode: Up to 100 messages per batch\n- OSS Mode: Limited by embedding provider (OpenAI: 2048 texts)\n\n#### 4.3 Embedding Caching\n\n**Problem**: Re-embedding same text wastes costs.\n\n**Solution**: Cache embeddings for frequent queries.\n\n```python\nimport hashlib\n\nembedding_cache = {}\n\ndef get_or_create_embedding(text: str) -> list[float]:\n    # Generate hash of text\n    text_hash = hashlib.sha256(text.encode()).hexdigest()\n\n    # Check cache\n    if text_hash in embedding_cache:\n        return embedding_cache[text_hash]\n\n    # Generate embedding\n    embedding = generate_embedding(text)\n    embedding_cache[text_hash] = embedding\n\n    return embedding\n```\n\n**Use Cases**:\n- Canned responses\n- Template messages\n- System prompts\n- Frequently asked questions\n\n### Phase 5: Database Optimization (OSS Mode)\n\nOptimize vector database performance for self-hosted deployments.\n\n#### 5.1 Choose Optimal Vector Database\n\n**Decision Matrix**:\n\n```bash\nbash scripts/suggest-vector-db.sh\n```\n\n| Database | Best For | Performance | Setup Complexity |\n|----------|----------|-------------|------------------|\n| **Qdrant** | Production, high scale | Excellent | Medium |\n| **Chroma** | Development, prototyping | Good | Low |\n| **pgvector** | Existing PostgreSQL | Good | Low |\n| **Milvus** | Enterprise, billions of vectors | Excellent | High |\n\n**Recommendation**:\n- Start: Chroma (easy setup)\n- Production: Qdrant (best performance)\n- Existing Postgres: pgvector (no new infra)\n- Enterprise: Milvus (handles massive scale)\n\n#### 5.2 Configure Optimal Indexes\n\n**For Qdrant**:\n```python\nconfig = MemoryConfig(\n    vector_store={\n        \"provider\": \"qdrant\"\n        \"config\": {\n            \"collection_name\": \"memories\"\n            \"host\": \"localhost\"\n            \"port\": 6333\n            \"on_disk\": True,  # Reduce memory usage\n            \"hnsw_config\": {\n                \"m\": 16,  # Balance between speed and accuracy\n                \"ef_construct\": 200,  # Higher = better quality\n            }\n            \"quantization_config\": {\n                \"scalar\": {\n                    \"type\": \"int8\",  # Reduce storage by 4x\n                    \"quantile\": 0.99\n                }\n            }\n        }\n    }\n)\n```\n\n**For pgvector** (Supabase):\n```bash\n# Use specialized Supabase integration skill\nbash ../supabase-integration/scripts/optimize-pgvector.sh\n```\n\nSee: `templates/vector-db-optimization/` for database-specific configs.\n\n#### 5.3 Connection Pooling\n\n**Problem**: Creating new connections on each request is slow.\n\n**Solution**: Use connection pooling.\n\n```python\nfrom mem0 import Memory\nfrom mem0.configs.base import MemoryConfig\n\nconfig = MemoryConfig(\n    vector_store={\n        \"provider\": \"qdrant\"\n        \"config\": {\n            \"host\": \"localhost\"\n            \"port\": 6333\n            \"grpc_port\": 6334\n            \"prefer_grpc\": True,  # Faster protocol\n            \"timeout\": 5\n            \"connection_pool_size\": 50,  # Reuse connections\n        }\n    }\n)\n```\n\n**Impact**: 30-50% reduction in connection overhead\n\n**Pool Size Guidelines**:\n- Low traffic: 10-20 connections\n- Medium traffic: 30-50 connections\n- High traffic: 50-100 connections\n\n### Phase 6: Batch Operations\n\nOptimize bulk operations for efficiency.\n\n#### 6.1 Batch Memory Addition\n\n```python\n# ‚ùå BAD: Individual operations\nfor msg in conversation_history:\n    memory.add(msg, user_id=user_id)\n\n# ‚úÖ GOOD: Batched operation\nmemory.add(conversation_history, user_id=user_id)\n```\n\n**Impact**: 60% faster, 40% lower cost\n\n#### 6.2 Batch Search Operations\n\n```python\nimport asyncio\n\n# ‚ùå BAD: Sequential searches\nresults = []\nfor query in queries:\n    results.append(memory.search(query, user_id=user_id))\n\n# ‚úÖ GOOD: Parallel searches\nasync def batch_search(queries, user_id):\n    memory = AsyncMemory()\n    return await asyncio.gather(*[\n        memory.search(q, user_id=user_id, limit=5)\n        for q in queries\n    ])\n\nresults = await batch_search(queries, user_id)\n```\n\n**Impact**: 4-5x faster for multiple searches\n\n### Phase 7: Cost Optimization\n\nReduce operational costs for memory systems.\n\n#### 7.1 Run Cost Analysis\n\n```bash\nbash scripts/analyze-costs.sh [user_id] [date_range]\n```\n\nThis generates:\n- Daily/monthly cost breakdown\n- Cost per operation type\n- Cost per user analysis\n- Optimization recommendations\n- Projected savings\n\n#### 7.2 Implement Cost-Saving Strategies\n\n**Strategy 1: Memory Deduplication**\n\n```bash\nbash scripts/deduplicate-memories.sh [user_id]\n```\n\nRemoves similar/duplicate memories to reduce storage and query costs.\n\n**Impact**: 20-40% storage reduction\n\n**Strategy 2: Archival and Tiered Storage**\n\n```bash\nbash scripts/setup-memory-archival.sh [retention_days]\n```\n\nMove old memories to cheaper storage:\n- Active (0-30 days): Fast vector DB\n- Archive (30-180 days): Compressed JSON in S3\n- Cold storage (>180 days): Glacier\n\n**Impact**: 50-70% storage cost reduction\n\n**Strategy 3: Smaller Embeddings for Archives**\n\n```python\n# Use cheaper embeddings for archived memories\narchived_config = MemoryConfig(\n    embedder={\n        \"provider\": \"openai\"\n        \"config\": {\n            \"model\": \"text-embedding-ada-002\",  # Cheaper\n        }\n    }\n)\n```\n\n**Strategy 4: Smart Pruning**\n\n```bash\nbash scripts/prune-low-value-memories.sh [user_id] [score_threshold]\n```\n\nRemove memories that:\n- Have never been retrieved\n- Have low relevance scores\n- Are redundant with other memories\n- Haven't been accessed in 90+ days\n\n**Impact**: 30-50% cost reduction\n\n### Phase 8: Monitoring and Alerts\n\nSet up performance monitoring and alerts.\n\n#### 8.1 Configure Monitoring\n\n```bash\nbash scripts/setup-monitoring.sh [project_name]\n```\n\nTracks:\n- Query latency (average, P95, P99)\n- Cache hit rate\n- Error rate\n- Cost per day/week/month\n- Memory growth rate\n- Top slow queries\n\n#### 8.2 Set Up Alerts\n\nUse the alert configuration template:\n\n```bash\nbash scripts/generate-alert-config.sh\n```\n\n**Recommended Alerts**:\n- üö® P99 latency > 500ms\n- üö® Error rate > 5%\n- üö® Cache hit rate < 50%\n- üö® Daily cost exceeds budget\n- üö® Storage growth > 20%/week\n- üö® Slow query percentage > 10%\n\n### Phase 9: Performance Testing\n\nBenchmark and validate optimizations.\n\n#### 9.1 Run Performance Benchmarks\n\n```bash\nbash scripts/benchmark-performance.sh [config_name]\n```\n\nMeasures:\n- Query latency under load\n- Throughput (queries/second)\n- Cache effectiveness\n- Cost per 1000 operations\n\n#### 9.2 Load Testing\n\n```bash\nbash scripts/load-test.sh [concurrent_users] [duration_seconds]\n```\n\nSimulates real-world load to identify bottlenecks.\n\n#### 9.3 Compare Configurations\n\n```bash\nbash scripts/compare-configs.sh [config1] [config2]\n```\n\nA/B test different optimization strategies.\n\n## Optimization Decision Trees\n\n### Query Performance Issues\n\n```bash\nbash scripts/diagnose-slow-queries.sh\n```\n\n**Diagnostic Flow**:\n1. Average latency > 200ms? ‚Üí Reduce limit, add filters\n2. P99 latency > 500ms? ‚Üí Add caching, optimize indexes\n3. High variance? ‚Üí Check for slow queries, optimize those specifically\n4. All queries slow? ‚Üí Database infrastructure issue\n\n### High Cost Issues\n\n```bash\nbash scripts/diagnose-high-costs.sh\n```\n\n**Diagnostic Flow**:\n1. Embedding costs high? ‚Üí Smaller model, batch operations, caching\n2. Storage costs high? ‚Üí Deduplication, archival, pruning\n3. Query costs high? ‚Üí Reduce search frequency, implement caching\n4. Overall high? ‚Üí Review all strategies above\n\n### Low Cache Hit Rate\n\n```bash\nbash scripts/optimize-cache.sh\n```\n\n**Diagnostic Flow**:\n1. < 30% hit rate? ‚Üí Cache wrong queries, review cache keys\n2. 30-60% hit rate? ‚Üí Increase TTL, cache more query patterns\n3. 60-80% hit rate? ‚Üí Good, minor tuning possible\n4. > 80% hit rate? ‚Üí Excellent, no action needed\n\n## Key Files Reference\n\n**Scripts** (all functional):\n- `scripts/analyze-performance.sh` - Comprehensive performance analysis\n- `scripts/analyze-costs.sh` - Cost breakdown and optimization\n- `scripts/benchmark-performance.sh` - Performance benchmarking\n- `scripts/load-test.sh` - Load testing and stress testing\n- `scripts/compare-configs.sh` - A/B test configurations\n- `scripts/diagnose-slow-queries.sh` - Query performance diagnostics\n- `scripts/diagnose-high-costs.sh` - Cost diagnostics\n- `scripts/optimize-cache.sh` - Cache tuning recommendations\n- `scripts/deduplicate-memories.sh` - Remove duplicate memories\n- `scripts/prune-low-value-memories.sh` - Remove unused memories\n- `scripts/setup-memory-archival.sh` - Configure archival system\n- `scripts/setup-monitoring.sh` - Configure performance monitoring\n- `scripts/generate-alert-config.sh` - Create alert rules\n- `scripts/generate-cache-config.sh` - Generate cache configurations\n- `scripts/suggest-vector-db.sh` - Vector database recommendations\n\n**Templates**:\n- `templates/optimized-memory-config.py` - Production-ready configuration\n- `templates/cache-strategies/` - Caching implementation patterns\n  - `in-memory-cache.py` - Python LRU cache\n  - `redis-cache.py` - Redis caching layer\n  - `edge-cache-config.yaml` - CDN/edge caching\n- `templates/vector-db-optimization/` - Database-specific tuning\n  - `qdrant-config.py` - Optimized Qdrant setup\n  - `pgvector-config.py` - Optimized pgvector setup\n  - `milvus-config.py` - Optimized Milvus setup\n- `templates/embedding-configs/` - Embedding optimization\n  - `cost-optimized.py` - Minimal cost configuration\n  - `performance-optimized.py` - Maximum performance\n  - `balanced.py` - Cost/performance balance\n- `templates/monitoring/` - Monitoring configurations\n  - `prometheus-metrics.yaml` - Metrics collection\n  - `grafana-dashboard.json` - Performance dashboard\n  - `alert-rules.yaml` - Alert configurations\n\n**Examples**:\n- `examples/optimization-case-studies.md` - Real-world optimization examples\n- `examples/before-after-benchmarks.md` - Performance improvement results\n- `examples/cost-reduction-strategies.md` - Cost optimization success stories\n- `examples/caching-patterns.md` - Effective caching implementations\n- `examples/oss-vs-platform-optimization.md` - Platform-specific strategies\n\n## Best Practices\n\n1. **Measure First**: Run performance analysis before optimizing\n2. **Prioritize**: Address biggest bottlenecks first (80/20 rule)\n3. **Incremental**: Implement one optimization at a time, measure impact\n4. **Cache Wisely**: Cache frequently accessed, rarely changing data\n5. **Right-Size Models**: Don't use large embeddings for simple use cases\n6. **Batch Operations**: Always batch when possible\n7. **Monitor Continuously**: Set up alerts, review metrics weekly\n8. **Test Changes**: Benchmark before/after every optimization\n9. **Document Impact**: Track cost/performance improvements\n10. **Review Regularly**: Monthly optimization reviews\n\n## Performance Targets\n\n**Query Latency**:\n- Average: < 100ms\n- P95: < 200ms\n- P99: < 500ms\n\n**Cache Performance**:\n- Hit rate: > 70%\n- Miss penalty: < 2x uncached time\n\n**Cost Efficiency**:\n- Cost per 1000 queries: < $0.10 (Platform), < $0.02 (OSS)\n- Storage growth: < 10% per month\n- Embedding costs: < 40% of total\n\n**Resource Usage** (OSS):\n- CPU: < 60% average\n- Memory: < 70% average\n- Storage: Plan for 6 months growth\n\n## Troubleshooting\n\n**Slow Queries Despite Optimization**:\n- Check database indexes exist\n- Verify connection pooling active\n- Review filter effectiveness\n- Check for database resource constraints\n\n**Cache Not Improving Performance**:\n- Verify cache keys are consistent\n- Check TTL isn't too short\n- Ensure cache size is adequate\n- Monitor cache eviction rate\n\n**High Costs After Optimization**:\n- Review actual usage patterns\n- Check for memory leaks (unbounded growth)\n- Verify deduplication running\n- Review archival policies\n\n**Optimization Caused Accuracy Issues**:\n- Reranking disabled may reduce quality\n- Smaller embeddings reduce semantic understanding\n- Lower limits may miss relevant memories\n- Balance performance vs accuracy needs\n\n---\n\n**Plugin**: mem0\n**Version**: 1.0.0\n**Last Updated**: 2025-10-27"
              },
              {
                "name": "supabase-integration",
                "description": "Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.",
                "path": "plugins/mem0/skills/supabase-integration/SKILL.md",
                "frontmatter": {
                  "name": "supabase-integration",
                  "description": "Complete Supabase setup for Mem0 OSS including PostgreSQL schema with pgvector for embeddings, memory_relationships tables for graph memory, RLS policies for user/tenant isolation, performance indexes, connection pooling, and backup/migration strategies. Use when setting up Mem0 with Supabase, configuring OSS memory backend, implementing memory persistence, migrating from Platform to OSS, or when user mentions Mem0 Supabase, memory database, pgvector for Mem0, memory isolation, or Mem0 backup.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Supabase Integration for Mem0 OSS\n\nComplete guide for setting up Supabase as the backend for Mem0 Open Source (self-hosted) mode, including PostgreSQL schema with pgvector, RLS policies for security, and production-ready configurations.\n\n## Instructions\n\n### Phase 1: Supabase Project Setup\n\n**Prerequisites Check:**\n1. Verify Supabase is initialized:\n   ```bash\n   bash scripts/verify-supabase-setup.sh\n   ```\n\n2. If not initialized, set up Supabase first:\n   - Run `/supabase:init` command\n   - Note down project ID and connection details\n   - Obtain connection string from Supabase dashboard\n\n**Environment Configuration:**\n```bash\n# Required environment variables\nSUPABASE_URL=https://your-project.supabase.co\nSUPABASE_ANON_KEY=your-anon-key\nSUPABASE_SERVICE_KEY=your-service-key\nSUPABASE_DB_URL=postgresql://postgres:[password]@db.[project].supabase.co:5432/postgres\n\n# Optional: Mem0-specific configs\nMEM0_EMBEDDING_MODEL=text-embedding-3-small\nMEM0_VECTOR_DIMENSION=1536\n```\n\n### Phase 2: Enable pgvector Extension\n\n**Enable Extension:**\n```bash\nbash scripts/setup-mem0-pgvector.sh\n```\n\nThis script:\n1. Connects to Supabase database\n2. Enables pgvector extension\n3. Verifies extension is active\n4. Checks PostgreSQL version compatibility (>= 12)\n\n**Manual Verification:**\n```sql\n-- Check pgvector is enabled\nSELECT * FROM pg_extension WHERE extname = 'vector';\n\n-- Test vector operations\nSELECT '[1,2,3]'::vector;\n```\n\n### Phase 3: Create Memory Tables Schema\n\n**Apply Memory Schema:**\n```bash\nbash scripts/apply-mem0-schema.sh\n```\n\nThis creates three core tables:\n\n**1. memories table** (vector storage):\n- `id` (uuid, primary key)\n- `user_id` (text, indexed) - User identifier for isolation\n- `agent_id` (text, indexed, nullable) - Agent identifier\n- `run_id` (text, indexed, nullable) - Session/conversation identifier\n- `memory` (text) - Memory content\n- `hash` (text) - Content hash for deduplication\n- `metadata` (jsonb) - Flexible metadata storage\n- `categories` (text[]) - Memory categorization\n- `embedding` (vector(1536)) - Semantic embedding\n- `created_at` (timestamptz)\n- `updated_at` (timestamptz)\n\n**2. memory_relationships table** (graph memory):\n- `id` (uuid, primary key)\n- `source_memory_id` (uuid, foreign key)\n- `target_memory_id` (uuid, foreign key)\n- `relationship_type` (text) - e.g., \"references\", \"caused_by\", \"related_to\"\n- `strength` (numeric) - Relationship strength (0.0-1.0)\n- `metadata` (jsonb)\n- `user_id` (text, indexed) - For RLS isolation\n- `created_at` (timestamptz)\n\n**3. memory_history table** (audit trail):\n- `id` (uuid, primary key)\n- `memory_id` (uuid)\n- `operation` (text) - \"create\", \"update\", \"delete\"\n- `old_value` (jsonb)\n- `new_value` (jsonb)\n- `user_id` (text)\n- `timestamp` (timestamptz)\n\n**Use Template for Custom Schema:**\n```bash\n# Generate schema with custom dimensions\nbash scripts/generate-mem0-schema.sh \\\n  --dimensions 1536 \\\n  --include-graph true \\\n  --include-history true \\\n  > custom-mem0-schema.sql\n\n# Apply custom schema\npsql $SUPABASE_DB_URL < custom-mem0-schema.sql\n```\n\n### Phase 4: Create Performance Indexes\n\n**Apply Optimized Indexes:**\n```bash\nbash scripts/create-mem0-indexes.sh\n```\n\n**Index Strategy:**\n\n1. **Vector Search Indexes (HNSW)**:\n   ```sql\n   -- Main embedding index (cosine distance)\n   CREATE INDEX idx_memories_embedding ON memories\n   USING hnsw (embedding vector_cosine_ops)\n   WITH (m = 16, ef_construction = 64);\n   ```\n\n2. **User/Agent Isolation Indexes**:\n   ```sql\n   CREATE INDEX idx_memories_user_id ON memories(user_id);\n   CREATE INDEX idx_memories_agent_id ON memories(agent_id);\n   CREATE INDEX idx_memories_run_id ON memories(run_id);\n   ```\n\n3. **Composite Indexes for Common Queries**:\n   ```sql\n   -- User + timestamp for chronological retrieval\n   CREATE INDEX idx_memories_user_created ON memories(user_id, created_at DESC);\n\n   -- User + agent for agent-specific memories\n   CREATE INDEX idx_memories_user_agent ON memories(user_id, agent_id);\n   ```\n\n4. **Graph Relationship Indexes**:\n   ```sql\n   CREATE INDEX idx_relationships_source ON memory_relationships(source_memory_id);\n   CREATE INDEX idx_relationships_target ON memory_relationships(target_memory_id);\n   CREATE INDEX idx_relationships_type ON memory_relationships(relationship_type);\n   ```\n\n5. **Full-Text Search Index (optional)**:\n   ```sql\n   CREATE INDEX idx_memories_content_fts ON memories\n   USING gin(to_tsvector('english', memory));\n   ```\n\n**Index Selection Guide:**\n- Small dataset (< 100K memories): Start with basic indexes\n- Medium dataset (100K-1M): Add HNSW with m=16\n- Large dataset (> 1M): Use HNSW with m=32, consider IVFFlat\n- Write-heavy workload: Consider IVFFlat over HNSW\n\n### Phase 5: Implement Row Level Security (RLS)\n\n**Apply RLS Policies:**\n```bash\nbash scripts/apply-mem0-rls.sh\n```\n\n**Security Patterns:**\n\n**1. User Isolation (Default)**:\n```sql\n-- Users can only access their own memories\nCREATE POLICY \"Users access own memories\"\nON memories FOR ALL\nUSING (auth.uid()::text = user_id);\n\n-- Users can only see their own relationships\nCREATE POLICY \"Users access own relationships\"\nON memory_relationships FOR ALL\nUSING (auth.uid()::text = user_id);\n```\n\n**2. Multi-Tenant Isolation (Enterprise)**:\n```sql\n-- Check organization membership\nCREATE POLICY \"Organization members access memories\"\nON memories FOR ALL\nUSING (\n  EXISTS (\n    SELECT 1 FROM org_members\n    WHERE org_members.user_id = auth.uid()::text\n    AND org_members.org_id = memories.metadata->>'org_id'\n  )\n);\n```\n\n**3. Agent-Specific Policies**:\n```sql\n-- Public agent memories (shared across users)\nCREATE POLICY \"Agent memories readable by all\"\nON memories FOR SELECT\nUSING (agent_id IS NOT NULL AND user_id IS NULL);\n\n-- Agent can write to their own memory space\nCREATE POLICY \"Agent writes own memories\"\nON memories FOR INSERT\nWITH CHECK (agent_id = current_setting('app.agent_id', true));\n```\n\n**Test RLS Enforcement:**\n```bash\nbash scripts/test-mem0-rls.sh --user-id \"test-user-123\"\n```\n\n### Phase 6: Configure Connection Pooling\n\n**Setup PgBouncer (Recommended for Production):**\n```bash\nbash scripts/configure-connection-pool.sh\n```\n\n**Connection Pooling Strategy:**\n\n**For Mem0 OSS:**\n- Transaction mode (default): Each memory operation gets fresh connection\n- Session mode: Use for graph traversals requiring multiple queries\n- Pool size: Start with 20 connections, scale based on load\n\n**Configuration:**\n```python\n# Mem0 with connection pooling\nfrom mem0 import Memory\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"postgres\"\n        \"config\": {\n            \"url\": \"postgresql://user:pass@pooler.project.supabase.co:6543/postgres\"\n            \"pool_size\": 20\n            \"max_overflow\": 10\n            \"pool_timeout\": 30\n            \"pool_recycle\": 3600\n        }\n    }\n}\n\nmemory = Memory.from_config(config)\n```\n\n**Supabase Pooler URLs:**\n- Transaction mode: `pooler.project.supabase.co:6543`\n- Session mode: `pooler.project.supabase.co:5432`\n\n### Phase 7: Implement Backup Strategy\n\n**Setup Automated Backups:**\n```bash\nbash scripts/setup-mem0-backup.sh --schedule daily --retention 30\n```\n\n**Backup Strategies:**\n\n**1. Point-in-Time Recovery (Supabase Built-in)**:\n- Automatic backups (Pro plan and above)\n- Restore to any point in last 7-30 days\n- No manual configuration needed\n\n**2. Manual SQL Dumps**:\n```bash\n# Full database backup\nbash scripts/backup-mem0-memories.sh\n\n# Incremental backup (changes since last backup)\nbash scripts/backup-mem0-memories.sh --incremental --since \"2025-10-01\"\n\n# Backup to S3\nbash scripts/backup-mem0-memories.sh --destination s3://my-bucket/mem0-backups/\n```\n\n**3. Selective Backups**:\n```bash\n# Backup specific user's memories\nbash scripts/backup-user-memories.sh --user-id \"customer-123\"\n\n# Backup by date range\nbash scripts/backup-mem0-memories.sh --from \"2025-01-01\" --to \"2025-10-27\"\n```\n\n**Restore Procedures:**\n```bash\n# Restore from backup file\nbash scripts/restore-mem0-backup.sh backup-2025-10-27.sql\n\n# Restore specific user\nbash scripts/restore-user-memories.sh backup-user-123.sql --user-id \"customer-123\"\n```\n\n### Phase 8: Migration from Platform to OSS\n\n**Export from Mem0 Platform:**\n```bash\nbash scripts/export-from-platform.sh \\\n  --api-key \"your-platform-api-key\" \\\n  --output platform-export.json\n```\n\n**Transform and Import to Supabase:**\n```bash\nbash scripts/migrate-platform-to-oss.sh \\\n  --input platform-export.json \\\n  --supabase-url $SUPABASE_URL \\\n  --dry-run  # Test first\n\n# After validation, run actual migration\nbash scripts/migrate-platform-to-oss.sh \\\n  --input platform-export.json \\\n  --supabase-url $SUPABASE_URL\n```\n\n**Migration Steps:**\n1. Export memories from Platform API\n2. Transform format (Platform JSON ‚Üí Postgres schema)\n3. Generate embeddings if missing\n4. Validate data integrity\n5. Batch insert to Supabase\n6. Verify counts and sample queries\n7. Update application configs to use OSS\n\n**Rollback Plan:**\n```bash\n# Create migration checkpoint before starting\nbash scripts/create-migration-checkpoint.sh\n\n# Rollback if issues occur\nbash scripts/rollback-migration.sh --checkpoint checkpoint-2025-10-27\n```\n\n### Phase 9: Validation and Testing\n\n**Run Complete Validation Suite:**\n```bash\nbash scripts/validate-mem0-setup.sh\n```\n\n**Validation Checks:**\n- ‚úÖ pgvector extension enabled\n- ‚úÖ All tables created with correct schema\n- ‚úÖ Indexes created and being used\n- ‚úÖ RLS policies active and enforcing\n- ‚úÖ Connection pooling configured\n- ‚úÖ Backup system operational\n- ‚úÖ Sample memory CRUD operations working\n- ‚úÖ Vector search returning results\n- ‚úÖ Graph relationships functional (if enabled)\n\n**Performance Benchmarks:**\n```bash\nbash scripts/benchmark-mem0-performance.sh\n```\n\n**Expected Performance (1K-10K memories):**\n- Memory insertion: < 50ms\n- Vector search (top 10): < 100ms\n- Memory retrieval by ID: < 10ms\n- Graph traversal (1-2 hops): < 150ms\n\n## Configuration Templates\n\n### Template 1: Basic Mem0 OSS + Supabase\n\nUse template: `templates/mem0-basic-config.py`\n\n```python\nfrom mem0 import Memory\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"postgres\"\n        \"config\": {\n            \"url\": \"postgresql://postgres:[password]@db.[project].supabase.co:5432/postgres\"\n            \"table_name\": \"memories\"\n            \"embedding_dimension\": 1536\n        }\n    }\n}\n\nmemory = Memory.from_config(config)\n```\n\n### Template 2: Full-Featured (Vector + Graph)\n\nUse template: `templates/mem0-graph-config.py`\n\n```python\nfrom mem0 import Memory\n\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"postgres\"\n        \"config\": {\n            \"url\": os.getenv(\"SUPABASE_DB_URL\")\n            \"table_name\": \"memories\"\n            \"embedding_dimension\": 1536\n        }\n    }\n    \"graph_store\": {\n        \"provider\": \"postgres\",  # Using same DB for graph\n        \"config\": {\n            \"url\": os.getenv(\"SUPABASE_DB_URL\")\n            \"relationship_table\": \"memory_relationships\"\n        }\n    }\n    \"version\": \"v1.1\"\n}\n\nmemory = Memory.from_config(config)\n```\n\n### Template 3: Enterprise Multi-Tenant\n\nUse template: `templates/mem0-enterprise-config.py`\n\nIncludes:\n- Organization-level isolation\n- Role-based access control\n- Audit logging\n- Advanced RLS policies\n- Cost tracking per tenant\n\n## Common Patterns\n\n### Pattern 1: User Memory Isolation\n\n**Scenario**: SaaS app with user-specific memories\n\n**Implementation**: See `examples/user-isolation-pattern.md`\n\n**Key Points**:\n- Always filter by user_id\n- RLS policies prevent cross-user access\n- Use composite indexes (user_id + created_at)\n\n### Pattern 2: Multi-Tenant Organization\n\n**Scenario**: Teams/organizations share memories within org\n\n**Implementation**: See `examples/multi-tenant-pattern.md`\n\n**Key Points**:\n- Add org_id to metadata\n- RLS checks org membership\n- Hierarchical access (org admins see all)\n\n### Pattern 3: Agent Knowledge Base\n\n**Scenario**: Shared agent memories across all users\n\n**Implementation**: See `examples/agent-knowledge-pattern.md`\n\n**Key Points**:\n- agent_id not null, user_id null for shared memories\n- Separate RLS policies for public agent knowledge\n- Versioning for agent memory updates\n\n### Pattern 4: Session-Based Memory\n\n**Scenario**: Temporary conversation context\n\n**Implementation**: See `examples/session-memory-pattern.md`\n\n**Key Points**:\n- Use run_id for session identification\n- Auto-cleanup after session expiry\n- Promote important memories to user level\n\n## Troubleshooting\n\n### pgvector Extension Issues\n\n**Problem**: Extension not available\n```bash\nERROR: extension \"vector\" is not available\n```\n\n**Solution**:\n1. Verify PostgreSQL version >= 12\n2. Enable in Supabase dashboard: Database ‚Üí Extensions ‚Üí vector\n3. Wait 1-2 minutes for activation\n4. Verify: `SELECT * FROM pg_extension WHERE extname = 'vector';`\n\n### Slow Vector Search\n\n**Problem**: Queries taking > 500ms\n\n**Solutions**:\n1. Check index exists:\n   ```sql\n   SELECT indexname FROM pg_indexes\n   WHERE tablename = 'memories' AND indexname LIKE '%embedding%';\n   ```\n\n2. Reduce search space with filters:\n   ```python\n   memory.search(query, user_id=\"specific-user\", limit=5)\n   ```\n\n3. Increase HNSW parameters (rebuild index):\n   ```sql\n   DROP INDEX idx_memories_embedding;\n   CREATE INDEX idx_memories_embedding ON memories\n   USING hnsw (embedding vector_cosine_ops)\n   WITH (m = 32, ef_construction = 128);\n   ```\n\n4. Switch to IVFFlat for large datasets (> 1M):\n   ```bash\n   bash scripts/migrate-to-ivfflat.sh\n   ```\n\n### RLS Blocking Queries\n\n**Problem**: Queries return empty results despite data existing\n\n**Solution**:\n1. Check RLS is enabled:\n   ```sql\n   SELECT tablename, rowsecurity FROM pg_tables\n   WHERE tablename = 'memories';\n   ```\n\n2. Verify auth context is set:\n   ```python\n   # Ensure user_id in JWT\n   supabase.auth.get_user()\n   ```\n\n3. Test with service key (bypasses RLS):\n   ```bash\n   bash scripts/test-mem0-rls.sh --bypass-rls\n   ```\n\n4. Review policy logs:\n   ```bash\n   bash scripts/debug-rls-policies.sh\n   ```\n\n### Connection Pool Exhaustion\n\n**Problem**: \"too many connections\" errors\n\n**Solutions**:\n1. Use transaction pooler (port 6543)\n2. Increase pool size in config\n3. Implement connection retry logic\n4. Monitor connection usage:\n   ```bash\n   bash scripts/monitor-connections.sh\n   ```\n\n### Migration Failures\n\n**Problem**: Migration script fails partway through\n\n**Recovery**:\n```bash\n# Check migration status\nbash scripts/check-migration-status.sh\n\n# Rollback to checkpoint\nbash scripts/rollback-migration.sh --checkpoint last\n\n# Resume from last successful batch\nbash scripts/resume-migration.sh\n```\n\n## Security Best Practices\n\n### Checklist\n\n- ‚úÖ RLS enabled on all Mem0 tables\n- ‚úÖ Service key never exposed to client\n- ‚úÖ All queries filtered by user_id/org_id\n- ‚úÖ Connection strings use environment variables\n- ‚úÖ SSL/TLS enforced for database connections\n- ‚úÖ Regular security audits run\n- ‚úÖ Sensitive memory content encrypted at rest\n- ‚úÖ Backup files encrypted\n- ‚úÖ Access logs monitored for anomalies\n- ‚úÖ GDPR/data deletion policies implemented\n\n### Audit Script\n\nRun regular security audits:\n```bash\nbash scripts/audit-mem0-security.sh --report security-audit.md\n```\n\nChecks:\n- RLS policy coverage\n- Unprotected tables\n- Missing indexes on security columns\n- Suspicious access patterns\n- Cross-user query attempts\n- Service key usage in logs\n\n## Files Reference\n\n**Scripts** (all executable, production-ready):\n- `scripts/verify-supabase-setup.sh` - Check Supabase initialization\n- `scripts/setup-mem0-pgvector.sh` - Enable pgvector extension\n- `scripts/apply-mem0-schema.sh` - Create memory tables\n- `scripts/generate-mem0-schema.sh` - Generate custom schema\n- `scripts/create-mem0-indexes.sh` - Create optimized indexes\n- `scripts/apply-mem0-rls.sh` - Apply RLS policies\n- `scripts/test-mem0-rls.sh` - Test security policies\n- `scripts/configure-connection-pool.sh` - Setup pooling\n- `scripts/setup-mem0-backup.sh` - Configure backups\n- `scripts/backup-mem0-memories.sh` - Manual backup\n- `scripts/restore-mem0-backup.sh` - Restore from backup\n- `scripts/backup-user-memories.sh` - User-specific backup\n- `scripts/export-from-platform.sh` - Export Platform memories\n- `scripts/migrate-platform-to-oss.sh` - Platform ‚Üí OSS migration\n- `scripts/validate-mem0-setup.sh` - Complete validation\n- `scripts/benchmark-mem0-performance.sh` - Performance testing\n- `scripts/migrate-to-ivfflat.sh` - Switch to IVFFlat index\n- `scripts/debug-rls-policies.sh` - Debug RLS issues\n- `scripts/monitor-connections.sh` - Connection monitoring\n- `scripts/audit-mem0-security.sh` - Security audit\n\n**Templates**:\n- `templates/mem0-schema.sql` - Base schema with pgvector\n- `templates/mem0-schema-graph.sql` - Schema with graph support\n- `templates/mem0-indexes.sql` - Performance indexes\n- `templates/mem0-rls-policies.sql` - Security policies\n- `templates/mem0-basic-config.py` - Basic Python config\n- `templates/mem0-graph-config.py` - Full-featured config\n- `templates/mem0-enterprise-config.py` - Multi-tenant config\n- `templates/backup-policy.yaml` - Backup configuration\n- `templates/connection-pool-config.ini` - PgBouncer config\n\n**Examples**:\n- `examples/user-isolation-pattern.md` - User-specific memories\n- `examples/multi-tenant-pattern.md` - Organization isolation\n- `examples/agent-knowledge-pattern.md` - Shared agent memories\n- `examples/session-memory-pattern.md` - Temporary session context\n- `examples/platform-to-oss-migration-guide.md` - Complete migration walkthrough\n- `examples/backup-restore-procedures.md` - Disaster recovery guide\n- `examples/performance-tuning-guide.md` - Optimization strategies\n\n---\n\n**Plugin**: mem0\n**Version**: 1.0.0\n**Last Updated**: 2025-10-27"
              }
            ]
          },
          {
            "name": "nextjs-frontend",
            "description": "Next.js 15 App Router with AI SDK, Supabase, shadcn/ui integration for building modern AI-powered applications",
            "source": "./plugins/nextjs-frontend",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Development Marketplace",
              "email": "marketplace@ai-dev.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install nextjs-frontend@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-component",
                "description": "Add component with shadcn/ui integration and TypeScript",
                "path": "plugins/nextjs-frontend/commands/add-component.md",
                "frontmatter": {
                  "description": "Add component with shadcn/ui integration and TypeScript",
                  "argument-hint": "<component-name>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a new React component with shadcn/ui integration, TypeScript types, and proper file structure\n\nCore Principles:\n- Understand component requirements before building\n- Follow Next.js and shadcn/ui conventions\n- Use TypeScript for type safety\n- Follow existing project patterns\n\nPhase 1: Parse Arguments\nGoal: Extract component name from user input\n\nActions:\n- Parse $ARGUMENTS for component name\n- Validate component name is in kebab-case or PascalCase\n- Example: !{bash echo \"$ARGUMENTS\" | grep -E '^[a-z-]+$|^[A-Z][a-zA-Z]+$'}\n\nPhase 2: Discovery\nGoal: Gather component requirements and understand project structure\n\nActions:\n- Use AskUserQuestion to determine:\n  - Component type: UI component, feature component, or layout component?\n  - Does it need shadcn/ui primitives (Button, Input, Card, etc.)?\n  - Should it be client-side or server component?\n  - Any specific props or functionality needed?\n- Load Next.js config to understand project setup\n- Example: @next.config.js\n- Check existing component structure\n- Example: !{bash find . -type f -name \"*.tsx\" -path \"*/components/*\" | head -5}\n\nPhase 3: Pattern Analysis\nGoal: Understand existing component conventions\n\nActions:\n- Read 2-3 existing components to identify patterns\n- Check for component directory structure (flat vs nested)\n- Identify naming conventions (PascalCase files, index exports)\n- Note import patterns and common utilities\n- Example: !{bash ls -la app/components/ src/components/ components/ 2>/dev/null | head -20}\n\nPhase 4: Load Design System (MANDATORY)\nGoal: Ensure design system exists and load it into context\n\nActions:\n- Check if design system exists: !{bash test -f design-system.md && echo \"‚úÖ Design system found\" || echo \"‚ùå ERROR: Design system missing - run /nextjs-frontend:init-design-system first\"}\n- If missing, STOP and tell user to run /nextjs-frontend:init-design-system first\n- Read design system rules: @design-system.md\n\n**Design System Rules Now Loaded from design-system.md:**\n(Agent now has design system in context and will follow these rules automatically)\n\nPhase 5: Implementation\nGoal: Create the component using component-builder-agent\n\nActions:\n\nTask(description=\"Build React component\", subagent_type=\"component-builder-agent\", prompt=\"You are the component-builder-agent. Create a new React component for $ARGUMENTS.\n\nContext from Discovery:\n- Component name: $ARGUMENTS\n- Project type: Next.js with TypeScript\n- Available: shadcn/ui primitives, Tailwind CSS\n- Component conventions: Follow existing patterns identified\n\n**MANDATORY: Design System Rules (MUST FOLLOW):**\n- Typography: 4 font sizes ONLY (text-sm=14px, text-base=16px, text-2xl=24px, text-3xl=32px), 2 weights (font-normal, font-semibold)\n- FORBIDDEN: text-xs, text-lg, text-xl, text-4xl, font-bold, font-light, font-medium\n- Spacing: 8pt grid ONLY - values divisible by 4 (p-1, p-2, p-4, p-6, p-8)\n- FORBIDDEN spacing: p-5, m-7, gap-3, space-y-1.5, px-2.5, py-0.5\n- Colors: 60/30/10 rule (60% bg-background, 30% text-foreground, 10% bg-primary)\n- Use OKLCH colors from theme (bg-background, text-foreground, bg-primary, etc.)\n- WCAG AA accessibility (4.5:1 contrast for text)\n- Buttons: Use variant and size props, never custom h-X or px-X classes\n\nRequirements:\n- Create component file with proper TypeScript types\n- Use functional component with React.FC or direct typing\n- Include Props interface if component accepts props\n- Add shadcn/ui components if specified by user\n- Follow Next.js 13+ conventions (use client/server directives as needed)\n- Include proper imports and exports\n- Add JSDoc comments for props\n- Follow existing project structure\n- **APPLY DESIGN SYSTEM RULES ABOVE**\n\nDeliverable:\n- Component file at appropriate location\n- Proper TypeScript types\n- shadcn/ui integration if needed\n- Export statement for easy imports\n- **Design system compliant styling**\")\n\nPhase 5: Verification & Design System Enforcement\nGoal: Verify component was created correctly and follows design system\n\n**IMPORTANT: Load design system enforcement skill:**\n\n!{skill design-system-enforcement}\n\nThis loads design system rules, validation scripts, and auto-fix patterns.\n\nActions:\n- Check that component file exists\n- Example: !{bash find . -name \"*$ARGUMENTS*\" -type f 2>/dev/null}\n- Run TypeScript type checking if available\n- Example: !{bash npm run typecheck 2>/dev/null || npx tsc --noEmit 2>/dev/null || echo \"Type check not available\"}\n- Verify imports resolve correctly\n- **Validate against design system using loaded skill patterns:**\n  - Check font sizes (must be 4 max)\n  - Check font weights (must be 2 max)\n  - Validate 8pt grid spacing\n  - Check color usage (60/30/10 rule)\n  - Validate OKLCH color format\n  - Auto-fix violations if detected\n- List component location for user reference\n\nPhase 6: Usage Instructions\nGoal: Show user how to use the new component\n\nActions:\n- Display component file path\n- Show import statement example\n- Provide basic usage example\n- Suggest next steps:\n  - Add to Storybook if using\n  - Create tests\n  - Add to component library documentation\n  - Integrate into pages/layouts"
              },
              {
                "name": "/add-page",
                "description": "Add new page to Next.js application with App Router conventions",
                "path": "plugins/nextjs-frontend/commands/add-page.md",
                "frontmatter": {
                  "description": "Add new page to Next.js application with App Router conventions",
                  "argument-hint": "<page-name>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a new Next.js page following App Router conventions with proper routing, metadata, and component structure.\n\nCore Principles:\n- Follow Next.js App Router conventions and best practices\n- Ask user about page requirements before generating\n- Detect existing patterns and match codebase style\n- Validate page creation and provide clear usage instructions\n\nPhase 1: Parse Arguments\nGoal: Extract page name from arguments\n\nActions:\n- Parse $ARGUMENTS to extract page name\n- Validate page name format (should be kebab-case for routes)\n- Example: \"about\" -> /app/about/page.tsx\n- Example: \"blog/[slug]\" -> /app/blog/[slug]/page.tsx\n\nPhase 2: Gather Requirements\nGoal: Understand page type and requirements\n\nActions:\n- Use AskUserQuestion to gather:\n  - Page type: Static, Dynamic (with params), Protected (requires auth), or API route?\n  - Layout needs: Should it use root layout or custom layout?\n  - Data fetching: Server-side data fetching needed?\n  - UI components: Any specific shadcn/ui components to include?\n  - Metadata: Page title, description, OpenGraph tags?\n- Store responses for agent context\n\nPhase 3: Analyze Existing Structure\nGoal: Understand current app structure and patterns\n\nActions:\n- Check if app directory exists: !{bash test -d app && echo \"App Router\" || echo \"Pages Router\"}\n- Load existing page examples to match patterns\n- Find similar pages: !{bash find app -name \"page.tsx\" -type f 2>/dev/null | head -5}\n- Read one example page to understand structure\n- Identify layout files: !{bash find app -name \"layout.tsx\" -type f 2>/dev/null}\n\nPhase 3.5: Load Design System (MANDATORY)\nGoal: Ensure design system exists and load it into context\n\nActions:\n- Check if design system exists: !{bash test -f design-system.md && echo \"‚úÖ Design system found\" || echo \"‚ùå ERROR: Design system missing - run /nextjs-frontend:init-design-system first\"}\n- If missing, STOP and tell user to run /nextjs-frontend:init-design-system first\n- Read design system rules: @design-system.md\n\nPhase 4: Page Generation\nGoal: Generate page with proper structure\n\nActions:\n\nTask(description=\"Generate Next.js page\", subagent_type=\"page-generator-agent\", prompt=\"You are the page-generator-agent. Create a new Next.js App Router page for: $ARGUMENTS\n\nUser Requirements:\n- Page name: [from parsed arguments]\n- Page type: [from user responses]\n- Layout needs: [from user responses]\n- Data fetching: [from user responses]\n- UI components: [from user responses]\n- Metadata: [from user responses]\n\nContext from codebase:\n- Router type: [App Router or Pages Router]\n- Existing patterns: [from analyzed pages]\n- Layout structure: [from analyzed layouts]\n\n**MANDATORY: Design System Rules (MUST FOLLOW):**\n- Typography: 4 font sizes ONLY (text-sm=14px, text-base=16px, text-2xl=24px, text-3xl=32px), 2 weights (font-normal, font-semibold)\n- FORBIDDEN: text-xs, text-lg, text-xl, text-4xl, font-bold, font-light, font-medium\n- Spacing: 8pt grid ONLY - values divisible by 4 (p-1, p-2, p-4, p-6, p-8)\n- FORBIDDEN spacing: p-5, m-7, gap-3, space-y-1.5, px-2.5, py-0.5\n- Colors: 60/30/10 rule (60% bg-background, 30% text-foreground, 10% bg-primary)\n- Use OKLCH colors from theme\n- WCAG AA accessibility standards\n- Buttons: Use variant and size props from design-system.md\n\nTasks:\n1. Create page.tsx at correct location (app/[page-name]/page.tsx)\n2. Add proper TypeScript types for params/searchParams if dynamic\n3. Include metadata export with provided title/description\n4. Add server component or client component directive as needed\n5. Implement data fetching if required\n6. Include shadcn/ui components if specified\n7. Add loading.tsx if async data fetching\n8. Add error.tsx for error boundaries\n9. Match existing code style and patterns\n\nDeliverable: Complete page implementation with all files created and proper Next.js conventions followed.\")\n\nPhase 5: Validation\nGoal: Verify page was created correctly\n\nActions:\n- Check page file exists: !{bash test -f app/$ARGUMENTS/page.tsx && echo \"Created\" || echo \"Missing\"}\n- Verify TypeScript compiles: !{bash npx tsc --noEmit 2>&1 | head -20}\n- Check for common issues (missing imports, type errors)\n- List created files: !{bash find app/$ARGUMENTS -type f 2>/dev/null}\n\nPhase 6: Summary\nGoal: Provide usage instructions\n\nActions:\n- Display page route: http://localhost:3000/[page-name]\n- Show created files with absolute paths\n- Explain how to:\n  - Navigate to the page (Link component or direct URL)\n  - Add to navigation if applicable\n  - Customize metadata\n  - Add data fetching\n- Suggest next steps:\n  - Add to main navigation\n  - Create related API routes if needed\n  - Add tests for the page\n  - Update sitemap.xml"
              },
              {
                "name": "/analyze-design",
                "description": "Analyze website design patterns using Playwright browser automation",
                "path": "plugins/nextjs-frontend/commands/analyze-design.md",
                "frontmatter": {
                  "description": "Analyze website design patterns using Playwright browser automation",
                  "argument-hint": "<url> [--mobile] [--compare <url2>]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite, Task)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n---\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n- Never hardcode API keys or secrets\n- Use placeholders where needed\n\n**Arguments**: $ARGUMENTS\n\nGoal: Analyze one or more websites to extract design patterns including layout, typography, colors, spacing, and components. Uses Playwright browser automation to navigate and capture screenshots.\n\n## Quick Reference Sites\n\nIf user doesn't provide a URL, suggest these top-tier design examples:\n\n**SaaS/Product:**\n- `https://linear.app` - Clean, minimal, dark mode\n- `https://vercel.com` - Developer-focused, modern\n- `https://stripe.com` - Enterprise, trust signals\n- `https://notion.so` - Friendly, approachable\n- `https://figma.com` - Creative, colorful\n\n**Marketing:**\n- `https://apple.com` - Premium, minimal\n- `https://airbnb.com` - Photography, warm\n- `https://spotify.com` - Bold, colorful\n\nPhase 1: Parse Arguments\nGoal: Extract URL(s) and options from arguments\n\nActions:\n- Parse $ARGUMENTS for:\n  - Primary URL to analyze\n  - `--mobile` flag for mobile-first analysis\n  - `--compare <url2>` for comparison analysis\n- If no URL provided, ask user to choose from curated list\n- Validate URLs are accessible\n\nPhase 2: Launch Browser & Navigate\nGoal: Open browser and navigate to target site\n\nActions:\n- Invoke design-patterns-agent for analysis:\n\nTask(description=\"Analyze website design patterns\", subagent_type=\"nextjs-frontend:design-patterns-agent\", prompt=\"You are the design-patterns-agent with Playwright MCP access. Analyze the design patterns of a website.\n\n**Target URL:** $ARGUMENTS\n\n**Your Tasks:**\n\n1. **Navigate to the site:**\n   Use mcp__playwright__playwright_navigate with url: '[extracted URL]'\n\n2. **Capture desktop screenshot:**\n   Use mcp__playwright__playwright_screenshot with:\n   - name: 'desktop-full'\n   - fullPage: true\n   - savePng: true\n\n3. **Capture mobile screenshot:**\n   Use mcp__playwright__playwright_resize with device: 'iPhone 13'\n   Use mcp__playwright__playwright_screenshot with:\n   - name: 'mobile-full'\n   - fullPage: true\n\n4. **Get page HTML for analysis:**\n   Use mcp__playwright__playwright_get_visible_html\n\n5. **Analyze and document:**\n   - Layout patterns (header, hero, sections, footer)\n   - Typography (fonts, sizes, weights, line-heights)\n   - Colors (primary, secondary, neutrals, accents)\n   - Spacing (section padding, gaps, max-widths)\n   - Components (buttons, cards, forms)\n\n6. **Generate implementation:**\n   - Tailwind config with extracted values\n   - Example components based on patterns\n\n7. **Close browser:**\n   Use mcp__playwright__playwright_close\n\n**Deliverable:** Complete design analysis report with:\n- Screenshots (desktop + mobile)\n- Pattern documentation\n- Tailwind config snippet\n- Example component code\")\n\nPhase 3: Compare Sites (If --compare flag)\nGoal: Analyze second site and compare patterns\n\nActions:\n- If --compare flag present in $ARGUMENTS:\n  - Run same analysis on second URL\n  - Compare patterns between sites\n  - Identify common patterns\n  - Note unique differentiators\n\nPhase 4: Generate Report\nGoal: Create comprehensive design report\n\nActions:\n- Compile findings into markdown report:\n\n```markdown\n# Design Pattern Analysis\n\n## Site: [URL]\n**Analyzed:** [Date]\n\n## Screenshots\n- Desktop: [screenshot path]\n- Mobile: [screenshot path]\n\n## Layout Patterns\n[Extracted layout information]\n\n## Typography\n[Font families, sizes, weights]\n\n## Color Palette\n[Primary, secondary, neutrals, accents]\n\n## Spacing System\n[Base unit, section padding, gaps]\n\n## Component Patterns\n[Buttons, cards, navigation]\n\n## Tailwind Configuration\n\\`\\`\\`typescript\n// tailwind.config.ts additions\n{\n  theme: {\n    extend: {\n      // Extracted config\n    }\n  }\n}\n\\`\\`\\`\n\n## Example Components\n[Generated component code]\n\n## Recommendations\n[How to apply these patterns]\n```\n\nPhase 5: Save & Present\nGoal: Save report and present to user\n\nActions:\n- Save report to `docs/design-analysis-[domain].md`\n- Display summary of key findings\n- Show screenshots inline if possible\n- Provide actionable next steps\n\nDisplay completion:\n```\nDesign Analysis Complete! üé®\n============================\n\nSite: [URL]\nScreenshots: Saved to downloads\n\nKey Findings:\n- Layout: [summary]\n- Typography: [font stack]\n- Colors: [primary color]\n- Spacing: [base unit]\n\nReport saved to: docs/design-analysis-[domain].md\n\nNext Steps:\n1. Review the full report\n2. Apply Tailwind config to your project\n3. Use example components as starting points\n4. Run /build-landing-page to apply patterns\n```"
              },
              {
                "name": "/build-landing-page",
                "description": "Build a high-converting landing page with SEO, engagement, and conversion optimization",
                "path": "plugins/nextjs-frontend/commands/build-landing-page.md",
                "frontmatter": {
                  "description": "Build a high-converting landing page with SEO, engagement, and conversion optimization",
                  "argument-hint": "<page-name> [--product|--saas|--newsletter|--webinar]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite, Task)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n---\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n- Never hardcode API keys or secrets\n- Use placeholders: `your_api_key_here`\n- Protect `.env` files with `.gitignore`\n\n**Arguments**: $ARGUMENTS\n\nGoal: Build a complete, high-converting landing page optimized for SEO, user engagement, and conversion. This command orchestrates multiple specialist agents to create a landing page that ranks, engages, and converts.\n\nCore Principles:\n- Mobile-first responsive design\n- SEO-optimized from the start (2025 best practices)\n- Conversion-focused layout and copy\n- Engaging micro-interactions\n- Analytics-ready with tracking\n\nLanding Page Types:\n- **--product**: Product launch with features, pricing, testimonials\n- **--saas**: SaaS homepage with trial signup, feature comparison\n- **--newsletter**: Email capture with value proposition\n- **--webinar**: Event registration with urgency elements\n\nPhase 1: Discovery\nGoal: Understand landing page requirements\n\nActions:\n- Parse $ARGUMENTS for page name and type flag\n- Default to --saas if no type specified\n- Check project setup:\n  ```bash\n  test -d app && echo \"Next.js App Router: Found\" || echo \"App Router: Not found\"\n  grep -q \"shadcn\" package.json && echo \"shadcn/ui: Installed\" || echo \"shadcn/ui: Not installed\"\n  ```\n- If shadcn/ui not installed, run `/nextjs-frontend:init` first\n\nPhase 2: Gather Requirements\nGoal: Collect landing page specifications\n\nActions:\n- Ask user for key information:\n\n1. **Value Proposition**: What problem does the product solve?\n2. **Target Audience**: Who is the ideal customer?\n3. **Key Features**: 3-5 main features/benefits\n4. **Social Proof**: Any testimonials, user counts, logos?\n5. **CTA Goal**: Trial signup, demo request, newsletter, purchase?\n6. **Urgency Elements**: Limited time offer, scarcity?\n\n- Store responses for use by specialist agents\n\nPhase 3: Content Strategy\nGoal: Generate optimized copy using content-optimizer-agent\n\nActions:\n- Invoke content optimizer for headline and copy:\n\nTask(description=\"Generate landing page copy\", subagent_type=\"nextjs-frontend:content-optimizer-agent\", prompt=\"You are the content optimizer agent. Generate high-converting copy for a landing page.\n\nPage Type: $ARGUMENTS\nValue Proposition: [from Phase 2]\nTarget Audience: [from Phase 2]\nFeatures: [from Phase 2]\n\nGenerate:\n1. **Hero Section**\n   - Main headline (benefit-focused, use proven formula)\n   - Subheadline (how it works, 1-2 sentences)\n   - Primary CTA text\n   - Supporting text (no credit card, free trial, etc.)\n\n2. **Social Proof**\n   - Logo cloud caption\n   - Stats section (3-4 metrics)\n   - Testimonial quotes (2-3)\n\n3. **Features Section**\n   - Section headline\n   - 3-5 feature cards with title + description\n\n4. **Pricing Section** (if applicable)\n   - Section headline\n   - Plan descriptions\n   - CTA text for each plan\n\n5. **FAQ Section**\n   - 5-7 common objections as questions\n   - Answers that overcome objections\n\n6. **Final CTA**\n   - Headline\n   - CTA button text\n   - Supporting text\n\n7. **Meta Content**\n   - Page title (50-60 chars)\n   - Meta description (150-160 chars)\n   - OG title and description\n\nDeliverable: Complete copy document ready for implementation\")\n\nPhase 4: Build Page Structure\nGoal: Create landing page with shadcn/ui components\n\nActions:\n- Create page file:\n  ```bash\n  mkdir -p app/$PAGE_NAME\n  ```\n- Use page-generator-agent for structure:\n\nTask(description=\"Build landing page structure\", subagent_type=\"nextjs-frontend:page-generator-agent\", prompt=\"You are the page generator agent. Create a landing page with these sections:\n\nPage: app/$PAGE_NAME/page.tsx\nType: $ARGUMENTS\n\nSections (in order):\n1. **Hero** - Full-width, centered content, gradient background\n2. **LogoCloud** - Trusted by logos\n3. **Features** - 3-column grid with icons\n4. **HowItWorks** - 3-step process with illustrations\n5. **Testimonials** - 3-column cards with ratings\n6. **Pricing** - 3 tier comparison table\n7. **FAQ** - Accordion component\n8. **FinalCTA** - Full-width, contrasting background\n\nRequirements:\n- Use shadcn/ui components (Button, Card, Accordion)\n- Mobile-first responsive design\n- Follow design system (4 font sizes, 8pt grid)\n- Include placeholder content initially\n- Export metadata for SEO\n\nDeliverable: Complete landing page file with all sections\")\n\nPhase 5: SEO Optimization\nGoal: Apply SEO best practices using seo-specialist-agent\n\nActions:\n- Invoke SEO specialist:\n\nTask(description=\"Optimize landing page SEO\", subagent_type=\"nextjs-frontend:seo-specialist-agent\", prompt=\"You are the SEO specialist agent. Optimize the landing page for 2025 SEO.\n\nPage: app/$PAGE_NAME/page.tsx\n\nImplement:\n1. **Metadata** - generateMetadata function with title, description, OG, Twitter\n2. **Schema** - Organization, Product/Service, FAQ JSON-LD\n3. **Semantic HTML** - Proper heading hierarchy (h1 ‚Üí h2 ‚Üí h3)\n4. **Image optimization** - next/image with alt text\n5. **Core Web Vitals** - Priority loading for hero image\n\nDeliverable: SEO-optimized page with all technical requirements\")\n\nPhase 6: Conversion Optimization\nGoal: Apply CRO best practices using conversion-specialist-agent\n\nActions:\n- Invoke conversion specialist:\n\nTask(description=\"Optimize for conversions\", subagent_type=\"nextjs-frontend:conversion-specialist-agent\", prompt=\"You are the conversion specialist agent. Optimize the landing page for maximum conversions.\n\nPage: app/$PAGE_NAME/page.tsx\n\nImplement:\n1. **CTA Optimization**\n   - Primary CTA above fold\n   - Sticky CTA on mobile\n   - Supporting text below buttons\n   - Action-oriented button copy\n\n2. **Trust Signals**\n   - Logo cloud with hover effects\n   - Testimonials with photos and titles\n   - Security badges near forms\n   - Money-back guarantee badge\n\n3. **Form Optimization**\n   - Minimal fields (email only for newsletter)\n   - Inline validation\n   - Progress indicator for multi-step\n\n4. **Urgency Elements** (if applicable)\n   - Countdown timer component\n   - Limited spots indicator\n   - 'Most popular' badge\n\nDeliverable: Conversion-optimized landing page components\")\n\nPhase 7: Engagement Enhancement\nGoal: Add micro-interactions using engagement-specialist-agent\n\nActions:\n- Invoke engagement specialist:\n\nTask(description=\"Add engagement features\", subagent_type=\"nextjs-frontend:engagement-specialist-agent\", prompt=\"You are the engagement specialist agent. Add engaging interactions to the landing page.\n\nPage: app/$PAGE_NAME/page.tsx\n\nImplement:\n1. **Scroll Animations**\n   - Fade-up for sections\n   - Staggered animation for feature cards\n   - Parallax for hero background (subtle)\n\n2. **Micro-Interactions**\n   - Button hover/click effects\n   - Card lift on hover\n   - Form focus animations\n\n3. **Progress Indicators**\n   - Scroll progress bar (if long page)\n   - Step indicators for process section\n\n4. **Social Proof Animations**\n   - Testimonial carousel/slider\n   - Stats count-up animation\n   - Logo cloud subtle movement\n\nDeliverable: Engaging landing page with Framer Motion animations\")\n\nPhase 8: Analytics Setup\nGoal: Implement tracking using analytics-specialist-agent\n\nActions:\n- Invoke analytics specialist:\n\nTask(description=\"Setup landing page analytics\", subagent_type=\"nextjs-frontend:analytics-specialist-agent\", prompt=\"You are the analytics specialist agent. Set up comprehensive tracking for the landing page.\n\nPage: app/$PAGE_NAME/page.tsx\n\nImplement:\n1. **Pageview Tracking** - GA4 integration\n2. **Event Tracking**\n   - CTA clicks (with location: hero, pricing, footer)\n   - Form submissions\n   - Scroll depth (25%, 50%, 75%, 100%)\n   - Video plays (if applicable)\n3. **Conversion Tracking**\n   - Signup/registration events\n   - Lead generation events\n4. **A/B Test Setup**\n   - Variant assignment hook\n   - Conversion tracking by variant\n\nDeliverable: Analytics-ready landing page with comprehensive tracking\")\n\nPhase 9: Final Integration\nGoal: Integrate all specialist outputs into cohesive page\n\nActions:\n- Combine copy from content optimizer\n- Apply SEO from SEO specialist\n- Add conversion elements from CRO specialist\n- Integrate animations from engagement specialist\n- Connect analytics from analytics specialist\n- Run build to verify:\n  ```bash\n  npm run build\n  ```\n\nPhase 10: Summary\nGoal: Present completed landing page\n\nActions:\n- Display completion summary:\n```\nLanding Page Built Successfully! üöÄ\n===================================\n\nPage: app/$PAGE_NAME/page.tsx\nType: [--saas|--product|--newsletter|--webinar]\n\n‚úÖ Content: Headlines, copy, CTAs optimized\n‚úÖ SEO: Metadata, schema, semantic HTML\n‚úÖ Conversion: CTAs, trust signals, forms\n‚úÖ Engagement: Animations, micro-interactions\n‚úÖ Analytics: GA4, events, A/B ready\n\nSections Created:\n1. Hero with CTA\n2. Logo Cloud\n3. Features Grid\n4. How It Works\n5. Testimonials\n6. Pricing Table\n7. FAQ Accordion\n8. Final CTA\n\nNext Steps:\n1. Replace placeholder content with real copy\n2. Add actual logos and testimonials\n3. Connect forms to backend\n4. Set up GA4 property\n5. Launch and monitor conversions\n\nPreview:\nnpm run dev\n‚Üí Open http://localhost:3000/$PAGE_NAME\n```"
              },
              {
                "name": "/enforce-design-system",
                "description": null,
                "path": "plugins/nextjs-frontend/commands/enforce-design-system.md",
                "frontmatter": null,
                "content": "---\ndescription: Enforce design system consistency across Next.js components\nargument-hint: [component-path] [--fix]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Validate and optionally fix design system compliance across Next.js components using shadcn/ui with Tailwind v4 standards.\n\nCore Principles:\n- Validate against mandatory design system constraints (4 font sizes, 2 weights, 8pt grid, 60/30/10 color rule)\n- Provide clear violation reports with actionable feedback\n- Auto-fix violations when --fix flag is provided\n- Ensure all components follow OKLCH color format and accessibility standards\n\nPhase 1: Discovery\nGoal: Understand target scope and check for design system configuration\n\nActions:\n- Parse $ARGUMENTS to extract component path and --fix flag\n- Check if design system is configured: !{bash test -f design-system.md && echo \"Found\" || echo \"Not configured\"}\n- If not configured, warn user to run initialization first\n- Determine validation scope (single component or all components)\n- Example: !{bash echo \"$ARGUMENTS\" | grep -q \"\\-\\-fix\" && echo \"Fix mode enabled\" || echo \"Validation only\"}\n\nPhase 2: Load Design System\nGoal: Read current design system configuration\n\nActions:\n- Load design system file: @design-system.md\n  - This file contains ALL component standards: button variants/sizes, card patterns, form layouts, typography, spacing, colors\n- Load validation script for reference\n- Identify target components to validate\n- If $ARGUMENTS contains specific path, validate that component only\n- Otherwise, find all components: !{bash find app/components -name \"*.tsx\" -o -name \"*.jsx\" 2>/dev/null || find src/components -name \"*.tsx\" -o -name \"*.jsx\" 2>/dev/null}\n\nPhase 3: Validation\nGoal: Run comprehensive design system validation\n\nActions:\n- Execute validation script on target components\n- Example: !{bash bash plugins/nextjs-frontend/skills/design-system-enforcement/scripts/validate-design-system.sh}\n- Capture validation results:\n  - Typography violations (>4 font sizes, wrong weights)\n  - Spacing violations (not divisible by 8/4)\n  - Color distribution violations (>10% accent usage)\n  - Custom CSS usage (should use Tailwind)\n  - Accessibility issues (missing ARIA labels)\n  - Non-shadcn/ui components\n\nPhase 4: Report Generation\nGoal: Present clear validation results\n\nActions:\n- Display validation summary with counts:\n  - Total components scanned\n  - Compliant components\n  - Components with violations\n  - Breakdown by violation type\n- For each violation, show:\n  - File path and line number\n  - Specific constraint violated\n  - Current value vs. expected value\n  - Suggested fix\n\nPhase 5: Auto-Fix (Conditional)\nGoal: Automatically repair violations if --fix flag provided\n\nActions:\n- Check if --fix flag present in $ARGUMENTS\n- If NOT present:\n  - Display \"Run with --fix flag to auto-repair violations\"\n  - Show manual fix instructions\n  - Exit with validation report\n- If --fix flag IS present:\n  - Invoke component-builder-agent to fix violations\n\nTask(description=\"Fix design system violations\", subagent_type=\"component-builder-agent\", prompt=\"You are the component-builder-agent. Fix design system violations in components identified by validation.\n\nDesign System Configuration:\nRead design-system.md from project root\n\nViolations to Fix:\n[Based on validation results from Phase 3]\n\nFix Requirements:\n- Consolidate to 4 font sizes maximum (from design-system.md)\n- Use only Semibold and Regular font weights\n- Ensure all spacing divisible by 8 or 4 (use Tailwind classes: p-2, p-4, p-6, p-8, m-2, m-4, gap-2, gap-4, etc.)\n- Enforce 60/30/10 color distribution (60% bg-background, 30% text-foreground, 10% bg-primary)\n- Replace custom CSS with Tailwind utilities\n- Use only shadcn/ui components from @/components/ui/\n- Add missing ARIA labels for accessibility\n- Convert colors to OKLCH format\n\nProcess:\n1. Read each violating component\n2. Apply fixes following design system constraints\n3. Preserve functionality while updating styles\n4. Self-validate after each fix\n5. Report all changes made\n\nDeliverable: Fixed components passing all design system validation checks\")\n\nPhase 6: Re-Validation (If Fixed)\nGoal: Verify all violations were resolved\n\nActions:\n- If fixes were applied, re-run validation\n- Example: !{bash bash plugins/nextjs-frontend/skills/design-system-enforcement/scripts/validate-design-system.sh}\n- Compare before/after results\n- Confirm all violations resolved\n- If violations remain, report what still needs manual attention\n\nPhase 7: Summary\nGoal: Present comprehensive enforcement results\n\nActions:\n- Display final validation status:\n  - \"All components compliant\" or \"X violations remaining\"\n  - Components fixed (if --fix was used)\n  - Auto-fixable vs. manual fixes needed\n- Show key metrics:\n  - Typography compliance: X/Y components\n  - Spacing compliance: X/Y components\n  - Color distribution compliance: X/Y components\n  - Accessibility compliance: X/Y components\n- Suggest next steps:\n  - If violations remain: \"Review manual fixes needed above\"\n  - If all compliant: \"Design system enforcement complete\"\n  - Recommend running validation in CI/CD pipeline\n"
              },
              {
                "name": "/init-design-system",
                "description": "Initialize design system interactively with colors, typography, and generate design-system.md",
                "path": "plugins/nextjs-frontend/commands/init-design-system.md",
                "frontmatter": {
                  "description": "Initialize design system interactively with colors, typography, and generate design-system.md",
                  "argument-hint": [
                    "project-name"
                  ],
                  "allowed-tools": "Read, Write, Edit, Bash, AskUserQuestion, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Interactively configure and generate a complete design system with brand colors, typography scale, and OKLCH color variables for shadcn/ui with Tailwind v4.\n\nCore Principles:\n- Use AskUserQuestion for interactive design decisions\n- Generate design-system.md with user preferences\n- Create/update globals.css with OKLCH color variables\n- Support both light and dark modes\n- Follow 60/30/10 color distribution rule\n\nPhase 0: Check Existing Configuration\nGoal: Determine if design system already exists\n\nActions:\n- Create todo list using TodoWrite\n- Check for existing design system: !{bash test -f design-system.md && echo \"EXISTS\" || echo \"NOT_FOUND\"}\n- If exists, warn user and ask to overwrite or exit\n- Parse $ARGUMENTS for project name (optional)\n\nPhase 1: Gather Project Information\nGoal: Collect basic project details\n\nActions:\n- If project name not in $ARGUMENTS, check package.json: !{bash jq -r '.name // \"my-app\"' package.json 2>/dev/null || echo \"my-app\"}\n- Ask user to confirm or provide project name\n\nPhase 2: Interactive Color Selection\nGoal: Gather brand color preferences from user\n\nActions:\n- Use AskUserQuestion with these questions:\n  1. \"What is your primary brand color?\" - Options: Blue (#3B82F6), Purple (#8B5CF6), Green (#10B981), Orange (#F97316)\n  2. \"What color scheme do you prefer?\" - Options: Neutral, Warm, Cool\n  3. \"Do you need dark mode support?\" - Options: Yes, No\n\nStore selections for Phase 4.\n\nPhase 3: Typography Scale Selection\nGoal: Let user choose typography scale\n\nActions:\n- Use AskUserQuestion: \"What typography scale do you prefer?\"\n  - Compact: 12px, 14px, 18px, 24px (Dense UI)\n  - Standard: 14px, 16px, 24px, 32px (Balanced)\n  - Spacious: 16px, 18px, 30px, 48px (Reading-focused)\n\nPhase 4: Generate Color Variables\nGoal: Convert selections to OKLCH color values\n\nActions:\n- Map brand color to OKLCH (Blue‚Üíoklch(0.623 0.214 259.815), Purple‚Üíoklch(0.558 0.228 293.071), Green‚Üíoklch(0.696 0.17 162.48), Orange‚Üíoklch(0.705 0.191 47.604))\n- Map color scheme to background OKLCH (Neutral‚Üíoklch(1 0 0), Warm‚Üíoklch(0.995 0.005 85), Cool‚Üíoklch(0.995 0.005 240))\n- Generate dark mode variants if selected\n\nPhase 5: Generate design-system.md\nGoal: Create the design system configuration file\n\nActions:\n- Load template: @~/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/nextjs-frontend/skills/design-system-enforcement/templates/design-system-template.md\n- Replace all placeholders with user selections\n- Write file: !{Write design-system.md}\n\nPhase 6: Update globals.css\nGoal: Add or update CSS color variables\n\nActions:\n- Check if globals.css exists: !{bash find . -name \"globals.css\" -type f | head -1}\n- If exists, update :root and .dark sections with new OKLCH values\n- If not exists, create app/globals.css with complete color variable setup\n- Include @theme block for Tailwind v4 compatibility\n\nPhase 7: Summary\nGoal: Report what was created\n\nActions:\n- Mark all todos complete\n- Display configuration summary (project, brand color, scheme, typography, dark mode)\n- List files created/updated\n- Show next steps: Review design-system.md, Build components with /nextjs-frontend:add-component, Validate with /nextjs-frontend:enforce-design-system\n- Remind user of enforced constraints: 4 font sizes, 2 weights, 8pt grid, 60/30/10 colors"
              },
              {
                "name": "/init",
                "description": "Initialize Next.js 15 App Router project with AI SDK, Supabase, and shadcn/ui",
                "path": "plugins/nextjs-frontend/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Next.js 15 App Router project with AI SDK, Supabase, and shadcn/ui",
                  "argument-hint": "<project-name>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a production-ready Next.js 15 application with App Router, TypeScript, Tailwind CSS, shadcn/ui, Vercel AI SDK, and Supabase integration.\n\nCore Principles:\n- Detect project requirements before creating\n- Ask clarifying questions for configuration\n- Use official templates and best practices\n- Validate setup after creation\n\nPhase 1: Requirements Gathering\n\nGoal: Understand project requirements and configuration preferences\n\nActions:\n\nParse $ARGUMENTS for project name. If not provided, ask user.\n\nUse AskUserQuestion to gather:\n- AI providers to integrate (Anthropic, OpenAI, Google, or all)\n- Authentication needed (yes/no)\n- Database features (basic CRUD, real-time, vector search)\n- UI component preferences (minimal, standard, comprehensive)\n\nPhase 2: Project Setup\n\nGoal: Invoke nextjs-setup-agent to create project structure\n\nActions:\n\nTask(description=\"Setup Next.js 15 project\", subagent_type=\"nextjs-frontend:nextjs-setup-agent\", prompt=\"You are the nextjs-setup-agent. Initialize a Next.js 15 App Router project for $ARGUMENTS.\n\nRequirements:\n- Project name: $PROJECT_NAME\n- Framework: Next.js 15 with App Router\n- Language: TypeScript with strict mode\n- Styling: Tailwind CSS\n- UI Components: shadcn/ui\n- AI Providers: $AI_PROVIDERS\n- Database: Supabase\n- Authentication: $AUTH_ENABLED\n\nTasks:\n1. Create Next.js 15 project with create-next-app\n2. Configure TypeScript with strict settings\n3. Setup Tailwind CSS with custom configuration\n4. Initialize shadcn/ui component library\n5. Install Vercel AI SDK and configure providers\n6. Setup Supabase client and environment variables\n7. Create basic project structure (app/, components/, lib/)\n8. Generate example API route with AI streaming\n9. Create example page with chat interface\n\nDocumentation sources to fetch:\n- Next.js 15 App Router setup\n- Vercel AI SDK integration\n- Supabase client configuration\n- shadcn/ui installation\n\nDeliverable: Complete working Next.js project ready for development\")\n\nWait for agent to complete.\n\nPhase 3: Validation\n\nGoal: Verify the project was created successfully\n\nActions:\n\nCheck project directory exists:\n!{bash test -d \"$PROJECT_NAME\" && echo \"‚úÖ Project created\" || echo \"‚ùå Project not found\"}\n\nVerify key files:\n!{bash ls $PROJECT_NAME/package.json $PROJECT_NAME/tsconfig.json $PROJECT_NAME/tailwind.config.ts $PROJECT_NAME/.env.local 2>/dev/null | wc -l}\n\nRun type checking:\n!{bash cd $PROJECT_NAME && npm run build}\n\nPhase 4: Summary\n\nGoal: Provide next steps and usage instructions\n\nActions:\n\nDisplay summary:\n- Project created at: ./$PROJECT_NAME\n- Framework: Next.js 15 App Router\n- Features enabled: TypeScript, Tailwind, shadcn/ui, AI SDK, Supabase\n- Environment variables configured in .env.local\n\nNext steps:\n1. cd $PROJECT_NAME\n2. Update .env.local with your API keys\n3. npm run dev\n4. Visit http://localhost:3000\n\nAdditional commands:\n- /nextjs-frontend:add-page <page-name> - Add new pages\n- /nextjs-frontend:add-component <name> - Add components\n- /nextjs-frontend:integrate-ai-sdk - Add more AI features\n- /nextjs-frontend:search-components - Browse shadcn/ui components"
              },
              {
                "name": "/integrate-ai-sdk",
                "description": "Integrate Vercel AI SDK for streaming AI responses",
                "path": "plugins/nextjs-frontend/commands/integrate-ai-sdk.md",
                "frontmatter": {
                  "description": "Integrate Vercel AI SDK for streaming AI responses",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Vercel AI SDK into Next.js project with streaming support and API routes for selected AI providers\n\nCore Principles:\n- Detect existing Next.js setup and configuration\n- Ask user for provider preferences before installing\n- Follow Next.js App Router conventions for API routes\n- Configure environment variables properly\n- Provide clear examples for immediate use\n\nPhase 1: Discovery\nGoal: Understand the Next.js project structure and existing setup\n\nActions:\n- Check if we're in a Next.js project:\n  !{bash test -f package.json && echo \"Found package.json\" || echo \"No package.json\"}\n- Load package.json to check Next.js version and existing dependencies:\n  @package.json\n- Detect if using App Router or Pages Router:\n  !{bash test -d app && echo \"App Router\" || test -d pages && echo \"Pages Router\" || echo \"Unknown\"}\n- Check for existing AI SDK installations:\n  !{bash grep -E \"(ai|@ai-sdk|openai|anthropic)\" package.json 2>/dev/null || echo \"No AI dependencies found\"}\n\nPhase 2: Requirements Gathering\nGoal: Ask user about AI provider preferences\n\nActions:\n- Use AskUserQuestion to gather provider preferences:\n  - Which AI providers would you like to integrate? (Options: Anthropic/Claude, OpenAI, Google/Gemini, Multiple)\n  - Do you need streaming support? (Yes/No - default Yes)\n  - Do you want example API routes created? (Yes/No - default Yes)\n  - Do you want a client-side example component? (Yes/No - default Yes)\n\nPhase 3: Planning\nGoal: Determine what needs to be installed and configured\n\nActions:\n- Based on user responses, identify required packages:\n  - Core: ai, @ai-sdk/react (for streaming hooks)\n  - Anthropic: @ai-sdk/anthropic\n  - OpenAI: @ai-sdk/openai\n  - Google: @ai-sdk/google\n- Identify App Router vs Pages Router structure\n- Plan API route locations:\n  - App Router: app/api/chat/route.ts\n  - Pages Router: pages/api/chat.ts\n- Determine .env.local variables needed based on providers\n\nPhase 4: Implementation\nGoal: Install packages and integrate AI SDK\n\nActions:\n\nTask(description=\"Integrate Vercel AI SDK\", subagent_type=\"ai-sdk-integration-agent\", prompt=\"You are the ai-sdk-integration-agent. Integrate Vercel AI SDK into this Next.js project for $ARGUMENTS.\n\nContext:\n- Next.js project detected with structure analyzed\n- User selected providers and preferences from Phase 2\n- Package.json loaded for dependency management\n\nRequirements:\n- Install ai and @ai-sdk/react packages\n- Install provider packages based on user selection (Anthropic, OpenAI, Google)\n- Create API route for chat endpoint (streaming support)\n- Follow App Router or Pages Router conventions detected\n- Create .env.local template with required API keys\n- Add proper TypeScript types if project uses TypeScript\n- Create example client component showing streaming usage\n- Follow Next.js best practices for API routes and server components\n\nProvider Configuration:\n- Anthropic: Use @ai-sdk/anthropic with ANTHROPIC_API_KEY\n- OpenAI: Use @ai-sdk/openai with OPENAI_API_KEY\n- Google: Use @ai-sdk/google with GOOGLE_API_KEY\n\nExpected Output:\n- Installed packages listed\n- API route created at correct location\n- .env.local template created\n- Example client component (if requested)\n- Clear instructions for adding API keys\")\n\nPhase 5: Validation\nGoal: Verify the integration is correct\n\nActions:\n- Check that packages were installed:\n  !{bash grep -E \"(ai|@ai-sdk)\" package.json}\n- Verify API route exists:\n  !{bash test -f app/api/chat/route.ts && echo \"App Router API found\" || test -f pages/api/chat.ts && echo \"Pages Router API found\" || echo \"API route not found\"}\n- Check .env.local template exists:\n  !{bash test -f .env.local.example && echo \"Found .env.local.example\" || test -f .env.local && echo \"Found .env.local\" || echo \"No env template\"}\n- Run TypeScript check if applicable:\n  !{bash test -f tsconfig.json && npm run type-check 2>/dev/null || echo \"TypeScript not configured\"}\n\nPhase 6: Examples and Next Steps\nGoal: Show the user how to use the integration\n\nActions:\n- Display API route location and basic usage\n- Show example of streaming chat completion:\n  - Client component with useChat() hook\n  - API route with streamText() function\n- List environment variables that need to be set:\n  - Based on selected providers\n  - Show where to get API keys\n- Provide next steps:\n  - Add API keys to .env.local\n  - Start development server: npm run dev\n  - Test the chat endpoint\n  - Customize the system prompt and model parameters\n\nPhase 7: Summary\nGoal: Recap what was accomplished\n\nActions:\n- Summarize integration:\n  - Packages installed (ai, provider SDKs)\n  - API routes created\n  - Environment variables configured\n  - Examples provided\n- Highlight key files:\n  - API route location\n  - Example component location\n  - .env configuration\n- Remind about API key setup\n- Suggest testing the integration with a simple prompt"
              },
              {
                "name": "/integrate-supabase",
                "description": "Integrate Supabase client, auth, and database into Next.js project",
                "path": "plugins/nextjs-frontend/commands/integrate-supabase.md",
                "frontmatter": {
                  "description": "Integrate Supabase client, auth, and database into Next.js project",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Supabase authentication, database, and storage capabilities into an existing Next.js project with proper client configuration, middleware, and type safety.\n\nCore Principles:\n- Detect existing Next.js structure before making changes\n- Ask user about required Supabase features to avoid unnecessary complexity\n- Follow Next.js App Router or Pages Router conventions based on detection\n- Ensure type safety with Supabase TypeScript types\n\nPhase 1: Discovery\nGoal: Understand the Next.js project structure and user requirements\n\nActions:\n- Detect Next.js version and routing pattern:\n  !{bash if [ -d \"app\" ]; then echo \"App Router\"; elif [ -d \"pages\" ]; then echo \"Pages Router\"; else echo \"Unknown\"; fi}\n- Check if Supabase is already configured:\n  !{bash ls .env.local supabase package.json 2>/dev/null | grep -E \"(supabase|SUPABASE)\" || echo \"No existing Supabase\"}\n- Load existing package.json to understand dependencies:\n  @package.json\n- Check for existing TypeScript configuration:\n  !{bash test -f tsconfig.json && echo \"TypeScript\" || echo \"JavaScript\"}\n\nPhase 2: Requirements Gathering\nGoal: Understand which Supabase features the user needs\n\nActions:\n- Use AskUserQuestion to gather Supabase integration requirements:\n  - Which Supabase features do you need? (auth, database, storage, realtime)\n  - Do you have an existing Supabase project URL and anon key?\n  - Which authentication providers? (email/password, OAuth, magic link)\n  - Do you need Row Level Security (RLS) policies?\n  - Should we generate TypeScript types from your database schema?\n- Confirm detected Next.js routing pattern (App Router vs Pages Router)\n- Ask about preferred authentication flow (server-side, client-side, or hybrid)\n\nPhase 3: Planning\nGoal: Design the integration approach based on findings\n\nActions:\n- Outline integration steps based on:\n  - Detected routing pattern (App Router vs Pages Router)\n  - Requested features (auth, database, storage, realtime)\n  - Authentication flow preference\n- Identify files to create/modify:\n  - .env.local (Supabase credentials)\n  - lib/supabase/* (client configuration)\n  - middleware.ts (auth middleware for App Router)\n  - _app.tsx or layout.tsx (session provider)\n  - types/supabase.ts (generated types)\n- Present plan to user for confirmation\n\nPhase 4: Integration\nGoal: Execute Supabase integration with the supabase-integration-agent\n\nActions:\n\nTask(description=\"Integrate Supabase into Next.js project\", subagent_type=\"supabase-integration-agent\", prompt=\"You are the supabase-integration-agent. Integrate Supabase into this Next.js project based on user requirements.\n\nProject Context:\n- Next.js routing: [App Router or Pages Router from detection]\n- TypeScript: [Yes/No from detection]\n- Existing dependencies: [from package.json]\n\nUser Requirements:\n- Features: $ARGUMENTS or [features from AskUserQuestion]\n- Auth providers: [from user response]\n- Auth flow: [server-side/client-side/hybrid from user response]\n- Type generation: [Yes/No from user response]\n\nIntegration Tasks:\n1. Install Supabase dependencies (@supabase/supabase-js, @supabase/ssr for App Router)\n2. Create environment configuration (.env.local with NEXT_PUBLIC_SUPABASE_URL and NEXT_PUBLIC_SUPABASE_ANON_KEY)\n3. Set up Supabase client utilities in lib/supabase/:\n   - Client-side client (for client components)\n   - Server-side client (for App Router server components/actions or API routes)\n   - Middleware configuration (for auth protection)\n4. Configure authentication:\n   - Auth middleware (middleware.ts for App Router)\n   - Session provider wrapper\n   - Login/signup helpers\n5. If database integration requested:\n   - Generate TypeScript types from schema (if credentials provided)\n   - Create database query examples\n6. If storage integration requested:\n   - Create storage upload/download utilities\n7. If realtime integration requested:\n   - Set up realtime subscription helpers\n8. Follow existing code style and conventions\n9. Add proper TypeScript types throughout\n\nExpected Output:\n- Installed dependencies listed\n- Created/modified files with full paths\n- Configuration instructions for .env.local\n- Example usage code for each requested feature\n- Next steps for database schema setup if needed\")\n\nPhase 5: Validation\nGoal: Verify the integration is complete and functional\n\nActions:\n- Check that all required dependencies were installed:\n  !{bash grep -E \"@supabase/(supabase-js|ssr|auth-helpers)\" package.json || echo \"Missing dependencies\"}\n- Verify Supabase client files exist:\n  !{bash find lib -name \"*supabase*\" -type f 2>/dev/null || find src/lib -name \"*supabase*\" -type f 2>/dev/null || echo \"Client files not found\"}\n- Check environment template was created:\n  !{bash test -f .env.local && echo \"Environment configured\" || test -f .env.example && echo \"Template created\" || echo \"No env file\"}\n- Verify TypeScript types if applicable:\n  !{bash test -f types/supabase.ts && echo \"Types generated\" || echo \"No types file\"}\n- Run type check if TypeScript project:\n  !{bash if [ -f tsconfig.json ]; then npx tsc --noEmit 2>&1 | head -20; fi}\n\nPhase 6: Summary\nGoal: Provide clear next steps and configuration instructions\n\nActions:\n- Summarize what was integrated:\n  - Dependencies installed\n  - Files created/modified\n  - Features configured (auth, database, storage, realtime)\n- Provide configuration instructions:\n  - How to add Supabase URL and anon key to .env.local\n  - How to generate TypeScript types: npx supabase gen types typescript\n  - How to set up database migrations if needed\n- Show example usage for each integrated feature\n- Suggest next steps:\n  - Set up Supabase project at https://supabase.com if not done\n  - Configure authentication providers in Supabase dashboard\n  - Create database tables and RLS policies\n  - Test authentication flow\n  - Generate and update TypeScript types from schema"
              },
              {
                "name": "/optimize-engagement",
                "description": null,
                "path": "plugins/nextjs-frontend/commands/optimize-engagement.md",
                "frontmatter": null,
                "content": "---\ndescription: Optimize user engagement with micro-interactions, scroll animations, and UX improvements\nargument-hint: [component-path] [--add-animations]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite, Task)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n---\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n- Never hardcode API keys or secrets\n- Use placeholders where needed\n- Protect `.env` files with `.gitignore`\n\n**Arguments**: $ARGUMENTS\n\nGoal: Enhance user engagement through micro-interactions, scroll animations, loading states, and UX improvements that keep users interacting with the application.\n\nCore Principles:\n- Add subtle animations that delight without distracting\n- Provide immediate feedback for user actions\n- Implement scroll-triggered content reveals\n- Optimize mobile touch interactions\n- Maintain performance while adding animations\n\nPhase 1: Discovery\nGoal: Analyze current engagement patterns and animation setup\n\nActions:\n- Parse $ARGUMENTS for component path and flags\n- Check for animation library:\n  ```bash\n  grep -q \"framer-motion\" package.json && echo \"Framer Motion: Installed\" || echo \"Framer Motion: Not installed\"\n  ```\n- Check for existing animations:\n  ```bash\n  grep -r \"motion\\|animate\" --include=\"*.tsx\" app components 2>/dev/null | wc -l\n  ```\n- Identify engagement gaps:\n  - Static buttons without hover/click feedback\n  - Pages without scroll animations\n  - Missing loading states\n  - No progress indicators\n\nPhase 2: Install Dependencies\nGoal: Ensure animation libraries are available\n\nActions:\n- If framer-motion not installed:\n  ```bash\n  npm install framer-motion --save\n  ```\n- If sonner (toasts) not installed:\n  ```bash\n  npm install sonner --save\n  ```\n- Verify installation:\n  ```bash\n  grep \"framer-motion\\|sonner\" package.json\n  ```\n\nPhase 3: Engagement Audit\nGoal: Run comprehensive engagement analysis using specialist agent\n\nActions:\n- Invoke engagement specialist for detailed audit:\n\nTask(description=\"Engagement audit for Next.js\", subagent_type=\"nextjs-frontend:engagement-specialist-agent\", prompt=\"You are the engagement specialist agent. Analyze this Next.js application for user engagement optimization opportunities.\n\nTarget: $ARGUMENTS\n\nAudit Checklist:\n\n1. **Button Interactions**\n   - Do buttons have hover states?\n   - Is there click feedback (scale, color change)?\n   - Are loading states shown during async operations?\n\n2. **Form Interactions**\n   - Focus states visible?\n   - Error states clear and helpful?\n   - Success feedback provided?\n\n3. **Scroll Engagement**\n   - Are there scroll progress indicators?\n   - Do elements animate in on scroll?\n   - Is lazy loading implemented?\n\n4. **Navigation**\n   - Page transitions smooth?\n   - Active states visible?\n   - Mobile menu has smooth open/close?\n\n5. **Loading States**\n   - Skeleton loaders for content?\n   - Spinners for async operations?\n   - Progress bars for uploads?\n\n6. **Social Proof**\n   - Testimonials animated?\n   - Stats have count-up animations?\n   - Trust badges visible?\n\nDeliverable: Prioritized list of engagement improvements with implementation difficulty (Easy/Medium/Hard)\")\n\nPhase 4: Implement Micro-Interactions\nGoal: Add button, card, and form interactions\n\nActions:\n- Create engaging button component if needed:\n  - Hover scale effect (1.02)\n  - Click scale effect (0.98)\n  - Loading state with spinner\n  - Success state with checkmark\n\n- Create card hover effects:\n  - Subtle lift on hover (y: -4)\n  - Shadow increase\n  - Optional image zoom\n\n- Enhance form interactions:\n  - Focus ring animations\n  - Error shake animation\n  - Success checkmark animation\n\nPhase 5: Add Scroll Animations\nGoal: Implement scroll-triggered reveals\n\nActions:\n- Create ScrollReveal component:\n  - Fade up on viewport entry\n  - Configurable delay and duration\n  - useInView hook integration\n\n- Add scroll progress indicator (for long pages):\n  - Fixed top bar\n  - Shows reading progress\n  - Smooth animation\n\n- Implement lazy loading:\n  - Images fade in on load\n  - Content sections reveal on scroll\n\nPhase 6: Optimize Loading States\nGoal: Ensure all async operations have feedback\n\nActions:\n- Check for async operations without loading states:\n  ```bash\n  grep -r \"async\\|await\\|fetch\\|useSWR\\|useQuery\" --include=\"*.tsx\" app components | head -20\n  ```\n- Add skeleton loaders for data-dependent components\n- Implement progress indicators for multi-step processes\n- Add toast notifications for background operations\n\nPhase 7: Mobile Touch Optimization\nGoal: Enhance mobile interaction experience\n\nActions:\n- Verify touch targets are 44x44px minimum\n- Add touch feedback (active states)\n- Implement swipe gestures where appropriate\n- Test pull-to-refresh if applicable\n\nPhase 8: Generate Engagement Report\nGoal: Document all improvements\n\nActions:\n- Generate markdown report with:\n  - Engagement score improvement\n  - Components enhanced\n  - Animations added\n  - Performance impact notes\n\nDisplay final summary:\n```\nEngagement Optimization Complete\n================================\n‚úÖ Framer Motion installed and configured\n‚úÖ Button interactions enhanced\n‚úÖ Scroll animations implemented\n‚úÖ Loading states added\n‚úÖ Mobile touch optimized\n\nComponents Enhanced:\n- Button: hover/click/loading states\n- Card: lift and shadow animations\n- Forms: focus/error/success feedback\n- Sections: scroll reveal animations\n\nPerformance Notes:\n- Animations use GPU acceleration\n- Reduced motion respected\n- Lazy loading prevents layout shift\n\nNext Steps:\n1. Test on various devices\n2. Monitor engagement metrics\n3. A/B test animation intensity\n```\n"
              },
              {
                "name": "/optimize-seo",
                "description": null,
                "path": "plugins/nextjs-frontend/commands/optimize-seo.md",
                "frontmatter": null,
                "content": "---\ndescription: Optimize Next.js application for 2025 SEO (Core Web Vitals, E-E-A-T, Schema markup)\nargument-hint: [page-path] [--full-audit]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite, Task)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\n---\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n- Never hardcode API keys or secrets\n- Use placeholders: `your_ga_id_here`, `your_site_url_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n\n**Arguments**: $ARGUMENTS\n\nGoal: Optimize a Next.js application for 2025 SEO best practices including Core Web Vitals, E-E-A-T signals, Schema markup, meta optimization, and technical SEO.\n\nCore Principles:\n- Focus on 2025 ranking factors (INP replaces FID, AI content guidelines)\n- Implement E-E-A-T signals (Experience, Expertise, Authoritativeness, Trust)\n- Add comprehensive Schema.org structured data\n- Optimize Core Web Vitals (LCP < 2.5s, INP < 200ms, CLS < 0.1)\n- Create dynamic sitemaps and robots.txt\n\nPhase 1: Discovery\nGoal: Understand current SEO state and scope\n\nActions:\n- Parse $ARGUMENTS to extract page path and --full-audit flag\n- Check for existing SEO setup:\n  ```bash\n  test -f app/sitemap.ts && echo \"Sitemap: Found\" || echo \"Sitemap: Missing\"\n  test -f app/robots.ts && echo \"Robots: Found\" || echo \"Robots: Missing\"\n  grep -r \"Metadata\" app/layout.tsx 2>/dev/null && echo \"Metadata: Found\" || echo \"Metadata: Check needed\"\n  ```\n- Identify target scope:\n  - If specific page path provided: optimize that page\n  - If --full-audit: optimize entire site\n  - Default: optimize key pages (home, about, pricing)\n\nPhase 2: SEO Audit\nGoal: Run comprehensive SEO analysis using seo-specialist-agent\n\nActions:\n- Invoke SEO specialist agent for detailed audit:\n\nTask(description=\"SEO audit for Next.js app\", subagent_type=\"nextjs-frontend:seo-specialist-agent\", prompt=\"You are the SEO specialist agent. Perform a comprehensive 2025 SEO audit for this Next.js application.\n\nTarget: $ARGUMENTS\n\nAudit Checklist:\n1. **Technical SEO**\n   - Check app/layout.tsx for Metadata API usage\n   - Verify sitemap.ts exists and generates all pages\n   - Verify robots.ts with proper rules\n   - Check for canonical URLs\n   - Verify mobile responsiveness\n\n2. **Meta Optimization**\n   - Check all pages for unique title tags\n   - Verify meta descriptions (150-160 chars)\n   - Check Open Graph tags\n   - Verify Twitter cards\n   - Check for duplicate content\n\n3. **Core Web Vitals Preparation**\n   - Check for next/image usage\n   - Verify font optimization (next/font)\n   - Check for lazy loading implementation\n   - Identify render-blocking resources\n\n4. **E-E-A-T Signals**\n   - Check for author information\n   - Verify About page exists\n   - Check for contact information\n   - Look for testimonials/reviews\n\n5. **Schema Markup**\n   - Check for existing JSON-LD\n   - Identify required schema types\n\nDeliverable: Detailed audit report with specific recommendations and priority levels (High/Medium/Low)\")\n\nPhase 3: Implement Meta Configuration\nGoal: Set up comprehensive metadata configuration\n\nActions:\n- If metadata not properly configured, create/update app/layout.tsx:\n  ```typescript\n  // Ensure metadata export exists with:\n  // - metadataBase\n  // - title template\n  // - description\n  // - openGraph configuration\n  // - twitter configuration\n  // - robots configuration\n  ```\n- Create metadata utility file if complex configuration needed\n- Add page-specific generateMetadata functions\n\nPhase 4: Sitemap & Robots\nGoal: Implement dynamic sitemap and robots.txt\n\nActions:\n- Create or update app/sitemap.ts:\n  ```bash\n  test -f app/sitemap.ts || echo \"Creating sitemap.ts\"\n  ```\n- Create or update app/robots.ts:\n  ```bash\n  test -f app/robots.ts || echo \"Creating robots.ts\"\n  ```\n- Ensure sitemap includes all indexable pages\n- Configure robots to allow crawling, block /api/ and /_next/\n\nPhase 5: Schema Markup Implementation\nGoal: Add structured data for rich snippets\n\nActions:\n- Create JSON-LD components for:\n  - Organization schema (all pages)\n  - WebSite schema (home page)\n  - Article schema (blog posts)\n  - FAQ schema (FAQ sections)\n  - BreadcrumbList (navigation)\n- Add schema components to appropriate pages\n\nPhase 6: Core Web Vitals Optimization\nGoal: Optimize for LCP, INP, and CLS\n\nActions:\n- Check image optimization:\n  ```bash\n  grep -r \"next/image\" app --include=\"*.tsx\" | wc -l\n  ```\n- If images not using next/image, flag for update\n- Check font configuration:\n  ```bash\n  grep -r \"next/font\" app --include=\"*.tsx\" | wc -l\n  ```\n- Verify priority attribute on above-fold images\n- Check for layout shift prevention (explicit dimensions)\n\nPhase 7: Generate SEO Report\nGoal: Create comprehensive SEO status report\n\nActions:\n- Generate markdown report with:\n  - Current SEO score\n  - Implemented optimizations\n  - Remaining recommendations\n  - Core Web Vitals checklist\n  - Testing instructions\n\nDisplay final summary:\n```\nSEO Optimization Complete\n========================\n‚úÖ Metadata configured in app/layout.tsx\n‚úÖ Dynamic sitemap at app/sitemap.ts\n‚úÖ Robots.txt at app/robots.ts\n‚úÖ Schema markup implemented\n‚úÖ Core Web Vitals optimized\n\nNext Steps:\n1. Test with Google Search Console\n2. Run PageSpeed Insights\n3. Submit sitemap to Google\n4. Monitor Core Web Vitals in production\n```\n"
              },
              {
                "name": "/scaffold-app",
                "description": "Scaffold complete Next.js application with sidebar, header, footer, and navigation from architecture docs using shadcn application blocks",
                "path": "plugins/nextjs-frontend/commands/scaffold-app.md",
                "frontmatter": {
                  "description": "Scaffold complete Next.js application with sidebar, header, footer, and navigation from architecture docs using shadcn application blocks",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Arguments**: $ARGUMENTS\n\nGoal: Build complete application scaffold with navigation, layout components, and route structure from architecture documentation using shadcn/ui application blocks\n\nCore Principles:\n- Discover architecture docs dynamically (don't assume paths!)\n- Use shadcn application blocks (examples) not individual components\n- Build complete navigation structure from architecture specs\n- Follow Next.js 15 App Router conventions\n\nPhase 1: Discovery & Setup\nGoal: Find architecture documentation and understand application structure\n\nActions:\n- Create TodoWrite list with phases\n- Discover architecture docs using Glob: !{glob docs/architecture/**/frontend.md}\n- If found: Read architecture and extract layout requirements, navigation routes, page structure\n- If not found: Ask user what layout type needed (Dashboard, Marketing, Admin Panel, Custom)\n- Update TodoWrite\n\nPhase 2: Search shadcn Application Blocks\nGoal: Find complete application scaffolds in shadcn registry\n\nActions:\n- Update TodoWrite status\n- Search shadcn MCP for APPLICATION BLOCKS using get_item_examples_from_registries\n- Query for: example-dashboard, example-sidebar, example-header, example-admin-panel\n- Review implementation code, note dependencies, understand layout structure\n- Update TodoWrite\n\nPhase 3: Generate Layout Components\nGoal: Build sidebar, header, footer, and navigation using component-builder-agent\n\nActions:\n- Update TodoWrite status\n- Launch component-builder-agent THREE times in parallel using Task tool:\n  1. Sidebar: Build components/layout/sidebar.tsx with navigation routes from architecture, shadcn components, design system compliance\n  2. Header: Build components/layout/header.tsx with user menu, theme toggle, branding, search if needed\n  3. Footer: Build components/layout/footer.tsx with copyright, links, responsive design\n- Wait for all agents to complete\n- Update TodoWrite\n\nPhase 4: Wire Navigation & Layout\nGoal: Create dashboard layout with all components integrated\n\nActions:\n- Update TodoWrite status\n- Launch page-generator-agent using Task tool to create app/(dashboard)/layout.tsx\n- Layout should import Sidebar, Header, Footer components and arrange with flexbox\n- Follow design system for spacing, colors, typography\n- Wait for completion\n- Update TodoWrite\n\nPhase 5: Validation\nGoal: Verify everything works correctly\n\nActions:\n- Update TodoWrite status\n- Check files created: !{bash ls -la components/layout/*.tsx app/\\(dashboard\\)/layout.tsx 2>/dev/null | wc -l}\n- Run type check: !{bash npm run type-check || npx tsc --noEmit}\n- Verify design system: /nextjs-frontend:enforce-design-system components/layout/\n- Update TodoWrite: Mark all complete\n\nPhase 6: Summary\nGoal: Report what was built and next steps\n\nActions:\n- Display summary of created components\n- List navigation routes wired from architecture\n- Confirm design system compliance, dark mode, responsive design\n- Show next steps: npm run dev, add pages with /nextjs-frontend:add-page\n- Reference architecture file source and route count"
              },
              {
                "name": "/search-components",
                "description": "Search and add shadcn/ui components from component library",
                "path": "plugins/nextjs-frontend/commands/search-components.md",
                "frontmatter": {
                  "description": "Search and add shadcn/ui components from component library",
                  "argument-hint": "<search-query>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the nextjs-frontend plugin:\n\n- **deployment-config**: Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.\n- **design-system-enforcement**: Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.\n- **tailwind-shadcn-setup**: Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Search shadcn/ui component library, display matching components, and add selected components to the project with configuration and usage examples.\n\nCore Principles:\n\n- Search before adding components\n- Display component details and variants\n- Ask user to confirm selection\n- Provide usage examples after installation\n\nPhase 1: Discovery\n\nGoal: Parse search query and validate project environment\n\nActions:\n\nParse $ARGUMENTS for search query. If empty, ask user what component they're looking for.\n\nVerify this is a Next.js project with shadcn/ui:\n!{bash test -f components.json && echo \"shadcn/ui detected\" || echo \"shadcn/ui not initialized\"}\n\nIf components.json not found, suggest running initialization first.\n\nPhase 2: Component Search\n\nGoal: Search shadcn/ui component library and display results\n\nActions:\n\nTask(description=\"Search shadcn/ui components\", subagent_type=\"nextjs-frontend:ui-search-agent\", prompt=\"You are the ui-search-agent. Search for shadcn/ui components matching '$ARGUMENTS'.\n\nSearch scope:\n\n- Official shadcn/ui component registry\n- Component categories: forms, layout, data-display, feedback, navigation\n- Search by: name, description, use-case, keywords\n\nFor each matching component, provide:\n\n1. Component name\n2. Description (1-2 sentences)\n3. Category\n4. Dependencies (other components required)\n5. Variants available\n6. Common use cases\n7. Installation command\n\nDocumentation to fetch:\n\n- shadcn/ui component registry\n- Component documentation pages\n- Installation guides\n\nDeliverable: Formatted list of matching components with details\")\n\nWait for agent to complete.\n\nPhase 3: Component Selection\n\nGoal: Let user choose which component to add\n\nActions:\n\nDisplay search results in organized format.\n\nUse AskUserQuestion to ask:\n\n- Which component would you like to add? (provide component name)\n- Which variants/features do you want? (if applicable)\n- Any customization preferences? (color scheme, size defaults)\n\nParse user selection and validate it exists in search results.\n\nPhase 4: Installation\n\nGoal: Add selected component to the project\n\nActions:\n\nInstall component using shadcn/ui CLI:\n!{bash npx shadcn@latest add $COMPONENT_NAME -y}\n\nVerify installation:\n!{bash test -f components/ui/$COMPONENT_NAME.tsx && echo \"Component installed\" || echo \"Installation failed\"}\n\nRead installed component to understand structure:\n@components/ui/$COMPONENT_NAME.tsx\n\nPhase 5: Usage Examples\n\nGoal: Provide code examples showing how to use the component\n\nActions:\n\nDisplay usage information:\n\n- Import statement\n- Basic usage example\n- Props reference\n- Common patterns\n- Integration tips\n\nShow where component is installed:\n\n- File path: components/ui/$COMPONENT_NAME.tsx\n- Dependencies installed (if any)\n\nSuggest next steps:\n\n- Create example page using the component\n- Customize component styling\n- Explore component variants\n\nPhase 6: Summary\n\nGoal: Confirm successful installation and provide guidance\n\nActions:\n\nSummary:\n\n- Component added: $COMPONENT_NAME\n- Location: components/ui/$COMPONENT_NAME.tsx\n- Dependencies: [list any additional components installed]\n- Ready to import and use in your app\n\nQuick start example:\n\n- Import: from \"@/components/ui/component-name\"\n- Usage: Add component to your page/component JSX\n- Props: Check component file for available props\n\nAdditional resources:\n\n- Official docs: https://ui.shadcn.com/docs/components/$COMPONENT_NAME\n- Browse more: /nextjs-frontend:search-components <query>\n- Add pages: /nextjs-frontend:add-page <name>"
              }
            ],
            "skills": [
              {
                "name": "conversion-patterns",
                "description": "Conversion rate optimization (CRO) patterns for Next.js applications including CTA design, landing page layouts, trust signals, form optimization, pricing tables, and A/B testing. Use when building landing pages, optimizing CTAs, adding social proof, or improving conversion funnels.",
                "path": "plugins/nextjs-frontend/skills/conversion-patterns/SKILL.md",
                "frontmatter": {
                  "name": "conversion-patterns",
                  "description": "Conversion rate optimization (CRO) patterns for Next.js applications including CTA design, landing page layouts, trust signals, form optimization, pricing tables, and A/B testing. Use when building landing pages, optimizing CTAs, adding social proof, or improving conversion funnels.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Conversion Patterns\n\n**Purpose:** Implement proven conversion rate optimization patterns for Next.js applications to maximize signups, sales, and engagement.\n\n**Activation Triggers:**\n- Building landing pages\n- Optimizing call-to-action buttons\n- Adding social proof elements\n- Creating pricing tables\n- Implementing lead capture forms\n- A/B testing setup\n- Conversion tracking implementation\n\n**Key Resources:**\n- `templates/hero-section.tsx` - High-converting hero patterns\n- `templates/cta-components.tsx` - CTA button variants\n- `templates/pricing-table.tsx` - Pricing comparison table\n- `templates/testimonials.tsx` - Social proof components\n- `examples/landing-page-structure.md` - Complete landing page example\n\n## Conversion Psychology\n\n### Core Principles\n\n1. **Clarity Over Cleverness** - Clear value proposition in 5 seconds\n2. **Reduce Anxiety** - Trust signals, guarantees, social proof\n3. **Create Urgency** - Time-limited offers, scarcity (ethical)\n4. **Remove Friction** - Minimal form fields, clear CTAs\n\n### Conversion Hierarchy\n\n```\nAttention ‚Üí Interest ‚Üí Desire ‚Üí Action (AIDA)\n     ‚Üì          ‚Üì         ‚Üì        ‚Üì\n   Hero    Features   Proof     CTA\n```\n\n## Hero Section Patterns\n\n### Pattern 1: Benefit-Focused Hero\n\n```typescript\n// components/marketing/HeroSection.tsx\nimport { Button } from '@/components/ui/button'\nimport { Badge } from '@/components/ui/badge'\nimport { ArrowRight, Star } from 'lucide-react'\n\nexport function HeroSection() {\n  return (\n    <section className=\"relative py-20 lg:py-32 overflow-hidden\">\n      {/* Background gradient */}\n      <div className=\"absolute inset-0 bg-gradient-to-br from-primary/5 via-background to-background\" />\n\n      <div className=\"container mx-auto px-4 relative\">\n        <div className=\"max-w-4xl mx-auto text-center\">\n          {/* Social Proof Badge */}\n          <Badge variant=\"secondary\" className=\"mb-6\">\n            <Star className=\"h-3 w-3 mr-1 fill-yellow-500 text-yellow-500\" />\n            Rated #1 by 50,000+ users\n          </Badge>\n\n          {/* Main Headline - Benefit Focused */}\n          <h1 className=\"text-4xl lg:text-6xl font-bold tracking-tight mb-6\">\n            Build Websites That{' '}\n            <span className=\"text-primary\">Convert</span>\n          </h1>\n\n          {/* Subheadline - How It Works */}\n          <p className=\"text-xl text-muted-foreground mb-8 max-w-2xl mx-auto\">\n            AI-powered tools that help you create high-converting landing pages\n            in minutes, not days. No coding required.\n          </p>\n\n          {/* CTA Buttons */}\n          <div className=\"flex flex-col sm:flex-row gap-4 justify-center\">\n            <Button size=\"lg\" className=\"text-lg px-8\">\n              Start Free Trial\n              <ArrowRight className=\"ml-2 h-5 w-5\" />\n            </Button>\n            <Button size=\"lg\" variant=\"outline\" className=\"text-lg px-8\">\n              Watch Demo\n            </Button>\n          </div>\n\n          {/* Trust Signals */}\n          <p className=\"mt-6 text-sm text-muted-foreground\">\n            ‚úì No credit card required ‚úì 14-day free trial ‚úì Cancel anytime\n          </p>\n        </div>\n      </div>\n    </section>\n  )\n}\n```\n\n### Pattern 2: Problem-Agitation Hero\n\n```typescript\nexport function ProblemAgitationHero() {\n  return (\n    <section className=\"py-20\">\n      <div className=\"container mx-auto px-4 text-center\">\n        {/* Problem Statement */}\n        <p className=\"text-lg text-muted-foreground mb-4\">\n          Tired of landing pages that don't convert?\n        </p>\n\n        {/* Agitation */}\n        <h1 className=\"text-4xl lg:text-6xl font-bold mb-6\">\n          Stop Losing Customers to{' '}\n          <span className=\"text-destructive line-through\">Bad Design</span>\n        </h1>\n\n        {/* Solution */}\n        <p className=\"text-xl mb-8 max-w-2xl mx-auto\">\n          Our AI analyzes 100,000+ high-converting pages to build\n          landing pages that actually work.\n        </p>\n\n        <Button size=\"lg\">Get Started Free</Button>\n      </div>\n    </section>\n  )\n}\n```\n\n## CTA Component Patterns\n\n### Primary CTA with Social Proof\n\n```typescript\n// components/marketing/CTAWithProof.tsx\ninterface CTAProps {\n  text: string\n  userCount: number\n  avatars: string[]\n}\n\nexport function CTAWithProof({ text, userCount, avatars }: CTAProps) {\n  return (\n    <div className=\"flex flex-col items-center gap-4\">\n      <Button size=\"lg\" className=\"text-lg px-8 py-6\">\n        {text}\n        <ArrowRight className=\"ml-2 h-5 w-5\" />\n      </Button>\n\n      {/* Social Proof */}\n      <div className=\"flex items-center gap-3\">\n        <div className=\"flex -space-x-2\">\n          {avatars.slice(0, 4).map((src, i) => (\n            <Image\n              key={i}\n              src={src}\n              alt=\"\"\n              width={32}\n              height={32}\n              className=\"rounded-full border-2 border-background\"\n            />\n          ))}\n        </div>\n        <span className=\"text-sm text-muted-foreground\">\n          Join {userCount.toLocaleString()}+ users\n        </span>\n      </div>\n    </div>\n  )\n}\n```\n\n### Sticky Mobile CTA\n\n```typescript\n// components/marketing/StickyCTA.tsx\n'use client'\n\nimport { useState, useEffect } from 'react'\nimport { Button } from '@/components/ui/button'\n\nexport function StickyCTA({ text = 'Get Started' }) {\n  const [show, setShow] = useState(false)\n\n  useEffect(() => {\n    const handleScroll = () => {\n      setShow(window.scrollY > 500)\n    }\n    window.addEventListener('scroll', handleScroll)\n    return () => window.removeEventListener('scroll', handleScroll)\n  }, [])\n\n  if (!show) return null\n\n  return (\n    <div className=\"fixed bottom-0 left-0 right-0 p-4 bg-background/80 backdrop-blur-sm border-t md:hidden z-50\">\n      <Button className=\"w-full\" size=\"lg\">\n        {text}\n      </Button>\n    </div>\n  )\n}\n```\n\n## Pricing Table Patterns\n\n### Three-Tier Pricing\n\n```typescript\n// components/marketing/PricingTable.tsx\nconst plans = [\n  {\n    name: 'Starter',\n    price: { monthly: 29, yearly: 24 },\n    description: 'Perfect for individuals',\n    features: ['5 projects', '10GB storage', 'Email support'],\n    cta: 'Start Free Trial',\n    popular: false,\n  },\n  {\n    name: 'Pro',\n    price: { monthly: 79, yearly: 66 },\n    description: 'For growing teams',\n    features: [\n      'Unlimited projects',\n      '100GB storage',\n      'Priority support',\n      'Advanced analytics',\n      'Custom domains',\n    ],\n    cta: 'Start Free Trial',\n    popular: true,\n  },\n  {\n    name: 'Enterprise',\n    price: { monthly: 199, yearly: 166 },\n    description: 'For large organizations',\n    features: [\n      'Everything in Pro',\n      'Unlimited storage',\n      'Dedicated support',\n      'Custom integrations',\n      'SLA guarantee',\n      'SSO & SAML',\n    ],\n    cta: 'Contact Sales',\n    popular: false,\n  },\n]\n\nexport function PricingTable() {\n  const [yearly, setYearly] = useState(false)\n\n  return (\n    <section className=\"py-20\">\n      <div className=\"container mx-auto px-4\">\n        {/* Header */}\n        <div className=\"text-center mb-12\">\n          <h2 className=\"text-3xl font-bold mb-4\">\n            Simple, Transparent Pricing\n          </h2>\n          <p className=\"text-muted-foreground mb-6\">\n            Start free. Upgrade when you need more.\n          </p>\n\n          {/* Billing Toggle */}\n          <div className=\"flex items-center justify-center gap-3\">\n            <span className={cn(!yearly && 'font-medium')}>Monthly</span>\n            <Switch checked={yearly} onCheckedChange={setYearly} />\n            <span className={cn(yearly && 'font-medium')}>\n              Yearly\n              <Badge variant=\"secondary\" className=\"ml-2\">Save 20%</Badge>\n            </span>\n          </div>\n        </div>\n\n        {/* Pricing Cards */}\n        <div className=\"grid md:grid-cols-3 gap-8 max-w-5xl mx-auto\">\n          {plans.map((plan) => (\n            <Card\n              key={plan.name}\n              className={cn(\n                'relative',\n                plan.popular && 'border-primary shadow-lg scale-105'\n              )}\n            >\n              {plan.popular && (\n                <Badge className=\"absolute -top-3 left-1/2 -translate-x-1/2\">\n                  Most Popular\n                </Badge>\n              )}\n\n              <CardHeader>\n                <CardTitle>{plan.name}</CardTitle>\n                <CardDescription>{plan.description}</CardDescription>\n              </CardHeader>\n\n              <CardContent>\n                <div className=\"mb-6\">\n                  <span className=\"text-4xl font-bold\">\n                    ${yearly ? plan.price.yearly : plan.price.monthly}\n                  </span>\n                  <span className=\"text-muted-foreground\">/month</span>\n                  {yearly && (\n                    <p className=\"text-sm text-muted-foreground\">\n                      billed annually\n                    </p>\n                  )}\n                </div>\n\n                <ul className=\"space-y-3\">\n                  {plan.features.map((feature) => (\n                    <li key={feature} className=\"flex items-center gap-2\">\n                      <Check className=\"h-4 w-4 text-primary\" />\n                      <span className=\"text-sm\">{feature}</span>\n                    </li>\n                  ))}\n                </ul>\n              </CardContent>\n\n              <CardFooter>\n                <Button\n                  className=\"w-full\"\n                  variant={plan.popular ? 'default' : 'outline'}\n                >\n                  {plan.cta}\n                </Button>\n              </CardFooter>\n            </Card>\n          ))}\n        </div>\n\n        {/* Guarantee */}\n        <p className=\"text-center mt-8 text-muted-foreground\">\n          <Shield className=\"inline h-4 w-4 mr-1\" />\n          30-day money-back guarantee. No questions asked.\n        </p>\n      </div>\n    </section>\n  )\n}\n```\n\n## Social Proof Patterns\n\n### Logo Cloud\n\n```typescript\n// components/marketing/LogoCloud.tsx\nexport function LogoCloud() {\n  const logos = [\n    { name: 'Google', src: '/logos/google.svg' },\n    { name: 'Microsoft', src: '/logos/microsoft.svg' },\n    { name: 'Amazon', src: '/logos/amazon.svg' },\n    { name: 'Meta', src: '/logos/meta.svg' },\n    { name: 'Netflix', src: '/logos/netflix.svg' },\n  ]\n\n  return (\n    <section className=\"py-12 border-y\">\n      <div className=\"container mx-auto px-4\">\n        <p className=\"text-center text-sm text-muted-foreground mb-8\">\n          Trusted by teams at the world's best companies\n        </p>\n        <div className=\"flex flex-wrap justify-center items-center gap-x-12 gap-y-6\">\n          {logos.map((logo) => (\n            <Image\n              key={logo.name}\n              src={logo.src}\n              alt={logo.name}\n              width={120}\n              height={40}\n              className=\"h-8 w-auto opacity-60 hover:opacity-100 transition-opacity\"\n            />\n          ))}\n        </div>\n      </div>\n    </section>\n  )\n}\n```\n\n### Testimonial Cards\n\n```typescript\n// components/marketing/Testimonials.tsx\ninterface Testimonial {\n  quote: string\n  author: string\n  title: string\n  company: string\n  avatar: string\n  rating: number\n}\n\nexport function TestimonialCard({ testimonial }: { testimonial: Testimonial }) {\n  return (\n    <Card>\n      <CardContent className=\"pt-6\">\n        {/* Rating */}\n        <div className=\"flex gap-1 mb-4\">\n          {[...Array(5)].map((_, i) => (\n            <Star\n              key={i}\n              className={cn(\n                'h-4 w-4',\n                i < testimonial.rating\n                  ? 'fill-yellow-400 text-yellow-400'\n                  : 'text-muted'\n              )}\n            />\n          ))}\n        </div>\n\n        {/* Quote */}\n        <p className=\"text-muted-foreground mb-4\">\n          \"{testimonial.quote}\"\n        </p>\n\n        {/* Author */}\n        <div className=\"flex items-center gap-3\">\n          <Image\n            src={testimonial.avatar}\n            alt={testimonial.author}\n            width={40}\n            height={40}\n            className=\"rounded-full\"\n          />\n          <div>\n            <p className=\"font-medium\">{testimonial.author}</p>\n            <p className=\"text-sm text-muted-foreground\">\n              {testimonial.title}, {testimonial.company}\n            </p>\n          </div>\n        </div>\n      </CardContent>\n    </Card>\n  )\n}\n```\n\n## Form Optimization\n\n### Multi-Step Lead Form\n\n```typescript\n// components/marketing/MultiStepForm.tsx\n'use client'\n\nimport { useState } from 'react'\n\nexport function MultiStepForm() {\n  const [step, setStep] = useState(1)\n  const totalSteps = 3\n\n  return (\n    <Card className=\"max-w-md mx-auto\">\n      <CardHeader>\n        <CardTitle>Get Started</CardTitle>\n\n        {/* Progress Bar */}\n        <div className=\"flex gap-2 mt-4\">\n          {[...Array(totalSteps)].map((_, i) => (\n            <div\n              key={i}\n              className={cn(\n                'h-1 flex-1 rounded',\n                i < step ? 'bg-primary' : 'bg-muted'\n              )}\n            />\n          ))}\n        </div>\n        <p className=\"text-sm text-muted-foreground\">\n          Step {step} of {totalSteps}\n        </p>\n      </CardHeader>\n\n      <CardContent>\n        {step === 1 && (\n          <div className=\"space-y-4\">\n            <div>\n              <Label htmlFor=\"email\">Work Email</Label>\n              <Input\n                id=\"email\"\n                type=\"email\"\n                placeholder=\"you@company.com\"\n                autoFocus\n              />\n            </div>\n            <Button onClick={() => setStep(2)} className=\"w-full\">\n              Continue\n            </Button>\n          </div>\n        )}\n\n        {step === 2 && (\n          <div className=\"space-y-4\">\n            <div>\n              <Label htmlFor=\"name\">Full Name</Label>\n              <Input id=\"name\" placeholder=\"John Doe\" />\n            </div>\n            <div>\n              <Label htmlFor=\"company\">Company</Label>\n              <Input id=\"company\" placeholder=\"Acme Inc\" />\n            </div>\n            <Button onClick={() => setStep(3)} className=\"w-full\">\n              Continue\n            </Button>\n          </div>\n        )}\n\n        {step === 3 && (\n          <div className=\"space-y-4\">\n            <div>\n              <Label htmlFor=\"size\">Team Size</Label>\n              <Select>\n                <SelectTrigger>\n                  <SelectValue placeholder=\"Select team size\" />\n                </SelectTrigger>\n                <SelectContent>\n                  <SelectItem value=\"1-10\">1-10</SelectItem>\n                  <SelectItem value=\"11-50\">11-50</SelectItem>\n                  <SelectItem value=\"51-200\">51-200</SelectItem>\n                  <SelectItem value=\"200+\">200+</SelectItem>\n                </SelectContent>\n              </Select>\n            </div>\n            <Button className=\"w-full\">\n              Create Account\n            </Button>\n          </div>\n        )}\n      </CardContent>\n\n      <CardFooter className=\"flex-col gap-2\">\n        <p className=\"text-xs text-muted-foreground text-center\">\n          By continuing, you agree to our Terms and Privacy Policy\n        </p>\n        <div className=\"flex items-center gap-2 text-sm text-muted-foreground\">\n          <Lock className=\"h-3 w-3\" />\n          <span>Your data is secure</span>\n        </div>\n      </CardFooter>\n    </Card>\n  )\n}\n```\n\n## Urgency Patterns\n\n### Countdown Timer\n\n```typescript\n// components/marketing/CountdownTimer.tsx\n'use client'\n\nimport { useState, useEffect } from 'react'\n\ninterface CountdownProps {\n  endDate: Date\n  label?: string\n}\n\nexport function CountdownTimer({ endDate, label = 'Offer ends in' }: CountdownProps) {\n  const [timeLeft, setTimeLeft] = useState(calculateTimeLeft())\n\n  function calculateTimeLeft() {\n    const difference = +endDate - +new Date()\n    if (difference <= 0) return null\n\n    return {\n      days: Math.floor(difference / (1000 * 60 * 60 * 24)),\n      hours: Math.floor((difference / (1000 * 60 * 60)) % 24),\n      minutes: Math.floor((difference / 1000 / 60) % 60),\n      seconds: Math.floor((difference / 1000) % 60),\n    }\n  }\n\n  useEffect(() => {\n    const timer = setInterval(() => {\n      setTimeLeft(calculateTimeLeft())\n    }, 1000)\n    return () => clearInterval(timer)\n  }, [endDate])\n\n  if (!timeLeft) return null\n\n  return (\n    <div className=\"text-center\">\n      <p className=\"text-sm text-muted-foreground mb-2\">{label}</p>\n      <div className=\"flex justify-center gap-4\">\n        {Object.entries(timeLeft).map(([unit, value]) => (\n          <div key={unit} className=\"text-center\">\n            <div className=\"text-2xl font-bold tabular-nums\">\n              {String(value).padStart(2, '0')}\n            </div>\n            <div className=\"text-xs text-muted-foreground uppercase\">\n              {unit}\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  )\n}\n```\n\n## A/B Testing\n\n### Simple A/B Hook\n\n```typescript\n// hooks/useABTest.ts\n'use client'\n\nimport { useEffect, useState } from 'react'\n\nexport function useABTest(testName: string): 'A' | 'B' {\n  const [variant, setVariant] = useState<'A' | 'B'>('A')\n\n  useEffect(() => {\n    // Check localStorage for existing assignment\n    const stored = localStorage.getItem(`ab_${testName}`)\n    if (stored === 'A' || stored === 'B') {\n      setVariant(stored)\n      return\n    }\n\n    // Assign randomly\n    const assigned = Math.random() < 0.5 ? 'A' : 'B'\n    localStorage.setItem(`ab_${testName}`, assigned)\n    setVariant(assigned)\n\n    // Track assignment (GA4, etc.)\n    if (typeof window.gtag !== 'undefined') {\n      window.gtag('event', 'experiment_assignment', {\n        experiment_name: testName,\n        variant: assigned,\n      })\n    }\n  }, [testName])\n\n  return variant\n}\n\n// Usage\nfunction CTASection() {\n  const variant = useABTest('cta_text_2024')\n\n  return (\n    <Button>\n      {variant === 'A' ? 'Start Free Trial' : 'Get Started Now'}\n    </Button>\n  )\n}\n```\n\n## Conversion Checklist\n\n```bash\n# Run conversion audit\n./scripts/conversion-audit.sh\n\n# Checks:\n# ‚úì Primary CTA above fold\n# ‚úì CTA text is action-oriented\n# ‚úì Trust signals present (logos, testimonials)\n# ‚úì Social proof with numbers\n# ‚úì Form has minimal fields\n# ‚úì Mobile CTA visible\n# ‚úì Pricing has \"Most Popular\"\n# ‚úì Money-back guarantee visible\n# ‚úì FAQ addresses objections\n```"
              },
              {
                "name": "deployment-config",
                "description": "Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.",
                "path": "plugins/nextjs-frontend/skills/deployment-config/SKILL.md",
                "frontmatter": {
                  "name": "deployment-config",
                  "description": "Vercel deployment configuration and optimization for Next.js applications including vercel.json setup, environment variables, build optimization, edge functions, and deployment troubleshooting. Use when deploying to Vercel, configuring deployment settings, optimizing build performance, setting up environment variables, configuring edge functions, or when user mentions Vercel deployment, production setup, build errors, or deployment optimization.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Deployment Configuration\n\n**Purpose:** Configure and optimize Next.js deployments on Vercel with automated validation and best practices.\n\n**Activation Triggers:**\n- Deployment failures or errors\n- Build performance optimization needed\n- Environment variable configuration\n- Edge function setup\n- vercel.json configuration\n- Production deployment preparation\n- Custom domain configuration\n\n**Key Resources:**\n- `scripts/validate-deployment.sh` - Validate deployment configuration\n- `scripts/optimize-build.sh` - Analyze and optimize build performance\n- `scripts/setup-env-vars.sh` - Interactive environment variable setup\n- `scripts/test-edge-functions.sh` - Test edge function configuration\n- `templates/vercel.json` - Production-ready vercel.json templates\n- `templates/env-template.md` - Environment variable documentation\n- `examples/deployment-patterns.md` - Common deployment scenarios\n\n## Deployment Workflow\n\n### 1. Pre-Deployment Validation\n\nValidate configuration before deploying:\n\n```bash\n# Full deployment validation\n./scripts/validate-deployment.sh\n\n# Checks performed:\n# - vercel.json syntax and schema\n# - Environment variables documented\n# - Build command configuration\n# - Output directory exists\n# - Framework detection correct\n# - No hardcoded secrets in code\n```\n\n### 2. Configure vercel.json\n\nChoose appropriate template based on needs:\n\n```bash\n# Generate vercel.json from template\ncp templates/vercel.json vercel.json\n\n# Templates available:\n# - basic.vercel.json         ‚Üí Minimal configuration\n# - optimized.vercel.json     ‚Üí Performance optimized\n# - edge-functions.vercel.json ‚Üí With edge/middleware\n# - monorepo.vercel.json      ‚Üí Monorepo setup\n```\n\n**Key Configuration Options:**\n\n**Build Settings:**\n- `buildCommand` - Override build command (default: `next build`)\n- `installCommand` - Custom install command or skip with `\"\"`\n- `outputDirectory` - Build output location (default: `.next`)\n- `framework` - Force framework detection (usually auto-detected)\n\n**Routing:**\n- `cleanUrls` - Remove .html extensions (true/false)\n- `trailingSlash` - Enforce trailing slash behavior\n- `redirects` - 301/302/307/308 redirect rules\n- `rewrites` - Internal URL rewrites\n- `headers` - Custom response headers\n\n**Functions:**\n- `functions` - Configure memory, duration, runtime per function\n- `regions` - Deployment regions (default: iad1)\n- Edge runtime for specific routes\n\n### 3. Environment Variables\n\n```bash\n# Interactive setup\n./scripts/setup-env-vars.sh\n\n# Guided process:\n# 1. Identifies required variables from code\n# 2. Categorizes by environment (dev/preview/prod)\n# 3. Generates .env.local and .env.example\n# 4. Provides Vercel CLI commands for upload\n```\n\n**Best Practices:**\n- Never commit `.env.local` or `.env.production`\n- Always commit `.env.example` (no values)\n- Use different values per environment\n- Prefix public variables with `NEXT_PUBLIC_`\n- Document all variables in `templates/env-template.md`\n\n**Upload to Vercel:**\n```bash\n# Production\nvercel env add VARIABLE_NAME production\n\n# Preview\nvercel env add VARIABLE_NAME preview\n\n# Development\nvercel env add VARIABLE_NAME development\n\n# Pull to local\nvercel env pull .env.local\n```\n\n### 4. Build Optimization\n\n```bash\n# Analyze build performance\n./scripts/optimize-build.sh\n\n# Provides:\n# - Bundle size analysis\n# - Build time breakdown\n# - Optimization recommendations\n# - Tree-shaking opportunities\n# - Code-splitting suggestions\n```\n\n**Common Optimizations:**\n\n**Image Optimization:**\n```json\n{\n  \"images\": {\n    \"domains\": [\"cdn.example.com\"],\n    \"formats\": [\"image/avif\", \"image/webp\"],\n    \"minimumCacheTTL\": 60\n  }\n}\n```\n\n**Function Configuration:**\n```json\n{\n  \"functions\": {\n    \"app/api/**/*.ts\": {\n      \"memory\": 1024,\n      \"maxDuration\": 10\n    }\n  }\n}\n```\n\n**Build Cache:**\n- Ensure `node_modules/.cache` is in .gitignore\n- Use Vercel's automatic caching\n- Enable SWC for faster builds (default in Next.js 12+)\n\n### 5. Edge Functions Setup\n\n```bash\n# Test edge function configuration\n./scripts/test-edge-functions.sh\n\n# Validates:\n# - Edge runtime compatibility\n# - No Node.js-specific APIs\n# - Size limits (1MB compressed)\n# - Cold start performance\n```\n\n**Edge Configuration in next.config.js:**\n```javascript\nexport const runtime = 'edge'\nexport const preferredRegion = 'iad1'\n```\n\n**Common Edge Use Cases:**\n- A/B testing and feature flags\n- Authentication/authorization\n- Geo-based content\n- Rate limiting\n- Request/response transformation\n\n## Deployment Methods\n\n### Git Integration (Recommended)\n\n```bash\n# Connect Git repository via Vercel Dashboard\n# Automatic deployments on:\n# - main/master ‚Üí Production\n# - other branches ‚Üí Preview\n\n# Manual trigger via commit\ngit push origin main\n```\n\n### Vercel CLI\n\n```bash\n# Install CLI\nnpm i -g vercel\n\n# Preview deployment\nvercel\n\n# Production deployment\nvercel --prod\n\n# With environment\nvercel --prod --env NEXT_PUBLIC_API_URL=https://api.example.com\n```\n\n### Deploy Hooks\n\nCustom deployment triggers without Git push:\n\n```bash\n# Create hook in Vercel Dashboard ‚Üí Settings ‚Üí Git ‚Üí Deploy Hooks\n# Then trigger via HTTP:\ncurl -X POST https://api.vercel.com/v1/integrations/deploy/[hook-id]\n```\n\n## Troubleshooting\n\n### Build Failures\n\n**Module not found:**\n```bash\n# Verify all dependencies in package.json\nnpm install --frozen-lockfile\n\n# Check for case-sensitive import issues\n# Vercel is case-sensitive, local dev may not be\n```\n\n**Out of memory:**\n```json\n{\n  \"builds\": [{\n    \"src\": \"package.json\",\n    \"use\": \"@vercel/next\",\n    \"config\": {\n      \"maxLambdaSize\": \"50mb\"\n    }\n  }]\n}\n```\n\n**Build timeout:**\n- Default: 15 minutes (Hobby), 45 minutes (Pro)\n- Optimize build with code-splitting\n- Use incremental static regeneration\n- Remove unnecessary dependencies\n\n### Environment Variable Issues\n\n```bash\n# Verify variables are set\nvercel env ls\n\n# Check variable is available\nvercel env pull .env.local\ncat .env.local\n\n# Common mistakes:\n# - Forgot NEXT_PUBLIC_ prefix for client-side\n# - Set in wrong environment (dev/preview/prod)\n# - Contains quotes or spaces incorrectly\n```\n\n### Function Errors\n\n**Function size exceeded:**\n- Individual function limit: 50MB\n- Check dependencies with `vercel build`\n- Use dynamic imports to reduce bundle size\n- Split large functions into smaller ones\n\n**Function timeout:**\n- Hobby: 10s, Pro: 60s, Enterprise: 900s\n- Optimize database queries\n- Use edge functions for faster response\n- Implement proper caching\n\n### Deployment URL Issues\n\n**Custom domain not working:**\n```bash\n# Verify DNS settings\ndig yourdomain.com\n\n# Should show:\n# A record ‚Üí 76.76.21.21\n# or CNAME ‚Üí cname.vercel-dns.com\n```\n\n**Preview URLs:**\n- Format: `project-git-branch-team.vercel.app`\n- Always HTTPS\n- Unique per commit\n- Expires based on plan limits\n\n## Configuration Examples\n\n### Basic Production Setup\n\n```json\n{\n  \"$schema\": \"https://openapi.vercel.sh/vercel.json\",\n  \"buildCommand\": \"next build\",\n  \"outputDirectory\": \".next\",\n  \"framework\": \"nextjs\",\n  \"headers\": [\n    {\n      \"source\": \"/(.*)\",\n      \"headers\": [\n        {\n          \"key\": \"X-Content-Type-Options\",\n          \"value\": \"nosniff\"\n        },\n        {\n          \"key\": \"X-Frame-Options\",\n          \"value\": \"DENY\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Monorepo Configuration\n\n```json\n{\n  \"buildCommand\": \"cd ../.. && npm run build --filter=web\",\n  \"installCommand\": \"npm install --prefix=../.. --frozen-lockfile\",\n  \"outputDirectory\": \".next\"\n}\n```\n\n### Advanced Rewrites\n\n```json\n{\n  \"rewrites\": [\n    {\n      \"source\": \"/api/:path*\",\n      \"destination\": \"https://api.example.com/:path*\"\n    },\n    {\n      \"source\": \"/blog/:slug\",\n      \"destination\": \"/news/:slug\"\n    }\n  ]\n}\n```\n\n## Resources\n\n**Scripts:** All scripts in `scripts/` are executable and include:\n- `validate-deployment.sh` - Pre-deployment validation\n- `optimize-build.sh` - Build performance analysis\n- `setup-env-vars.sh` - Environment variable configuration\n- `test-edge-functions.sh` - Edge function testing\n- `check-bundle-size.sh` - Bundle analysis\n\n**Templates:** `templates/` contains:\n- Multiple vercel.json presets\n- Environment variable documentation template\n- Security headers configuration\n- CORS configuration examples\n\n**Examples:** `examples/deployment-patterns.md` includes:\n- Multi-environment setup\n- Monorepo deployment\n- Custom domain configuration\n- Edge function patterns\n- Deployment automation workflows\n\n---\n\n**Platform:** Vercel\n**Framework:** Next.js 13+ (App Router and Pages Router)\n**CLI Version:** Latest Vercel CLI\n**Version:** 1.0.0"
              },
              {
                "name": "design-inspiration",
                "description": "Curated collection of top-tier website designs organized by category, style, and pattern. Includes SaaS, marketing, e-commerce, and portfolio sites with analysis of what makes them effective. Use when looking for design inspiration or examples of specific patterns.",
                "path": "plugins/nextjs-frontend/skills/design-inspiration/SKILL.md",
                "frontmatter": {
                  "name": "design-inspiration",
                  "description": "Curated collection of top-tier website designs organized by category, style, and pattern. Includes SaaS, marketing, e-commerce, and portfolio sites with analysis of what makes them effective. Use when looking for design inspiration or examples of specific patterns.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, mcp__playwright"
                },
                "content": "# Design Inspiration\n\n**Purpose:** Provide curated design inspiration and patterns from top-tier websites to inform design decisions.\n\n**Activation Triggers:**\n- Looking for design inspiration\n- Researching competitors\n- Finding examples of specific patterns\n- Building landing pages\n- Designing SaaS dashboards\n- Creating marketing sites\n\n## Curated Site Collections\n\n### üöÄ SaaS & Product Sites\n\n**Minimal & Clean**\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [linear.app](https://linear.app) | Dark mode, smooth animations, developer-focused | Sticky nav, product screenshots, keyboard shortcuts showcase |\n| [raycast.com](https://raycast.com) | macOS-native feel, premium aesthetic | Command palette hero, extension gallery, dark theme |\n| [arc.net](https://arc.net) | Bold typography, playful illustrations | Split hero, waitlist CTA, feature cards |\n| [height.app](https://height.app) | Gradient mesh backgrounds, clean UI | Animated hero, timeline feature display |\n\n**Enterprise & Trust**\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [stripe.com](https://stripe.com) | Premium feel, gradient accents, developer trust | API code samples, globe animation, pricing table |\n| [vercel.com](https://vercel.com) | Speed-focused messaging, dark mode | Edge network visualization, framework logos, deploy button |\n| [planetscale.com](https://planetscale.com) | Database-specific messaging, technical credibility | Schema visualization, branching explanation |\n| [supabase.com](https://supabase.com) | Open source trust, feature comparison | GitHub stats, pricing comparison, real-time demo |\n\n**Friendly & Approachable**\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [notion.so](https://notion.so) | Warm, template-focused, community | Template gallery, use case tabs, team testimonials |\n| [slack.com](https://slack.com) | Playful illustrations, channel demos | Product demo GIFs, enterprise logos, integration grid |\n| [airtable.com](https://airtable.com) | Colorful, template gallery | Universe showcase, formula builder, view switcher |\n| [figma.com](https://figma.com) | Creative, collaborative focus | Real-time cursor demo, plugin showcase, community |\n\n### üíº Marketing & Agency Sites\n\n**Bold & Creative**\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [apple.com](https://apple.com) | Minimal, product-focused, premium | Large product photography, scroll animations, comparison tables |\n| [spotify.com](https://spotify.com) | Duotone colors, music-focused | Artist features, playlist cards, year-wrapped style |\n| [airbnb.com](https://airbnb.com) | Photography-driven, human warmth | Search hero, experience cards, host stories |\n| [mailchimp.com](https://mailchimp.com) | Quirky illustrations, yellow brand | Freddie mascot, feature illustrations, pricing clarity |\n\n**Professional & Corporate**\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [hubspot.com](https://hubspot.com) | Orange accent, clear value prop | Product suite tabs, ROI calculator, case studies |\n| [salesforce.com](https://salesforce.com) | Enterprise trust, cloud imagery | Trailhead learning, industry solutions, character mascots |\n| [zendesk.com](https://zendesk.com) | Green accent, support-focused | Product demos, customer logos, pricing tiers |\n\n### üõí E-commerce & Marketplace\n\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [shopify.com](https://shopify.com) | Merchant success focus, green brand | Store examples, pricing clarity, 14-day trial |\n| [gumroad.com](https://gumroad.com) | Creator-focused, simple pricing | Fee transparency, creator testimonials, product cards |\n| [lemonsqueezy.com](https://lemonsqueezy.com) | Modern e-commerce, yellow brand | Dashboard preview, global payments, tax handling |\n\n### üé® Design Tools & Resources\n\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [ui.shadcn.com](https://ui.shadcn.com) | Component documentation, copy-paste | Code previews, theme customizer, installation steps |\n| [tailwindui.com](https://tailwindui.com) | Premium components, preview grid | Category organization, responsive previews, code tabs |\n| [framer.com](https://framer.com) | Motion-focused, template gallery | Site builder demo, animation examples, pricing |\n| [webflow.com](https://webflow.com) | Visual builder, template showcase | Interactions demo, CMS explanation, university |\n\n### üìä Analytics & Data\n\n| Site | Why It's Great | Key Patterns |\n|------|----------------|--------------|\n| [plausible.io](https://plausible.io) | Privacy-focused, simple dashboard | Live demo, GDPR compliance, open source |\n| [posthog.com](https://posthog.com) | All-in-one analytics, hedgehog brand | Product suite, self-host option, pricing calculator |\n| [mixpanel.com](https://mixpanel.com) | Event analytics, funnel visualization | Dashboard demos, integration logos, case studies |\n\n## Design Patterns Library\n\n### Hero Section Patterns\n\n**1. Split Hero (Text + Image)**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ Badge        ‚îÇ  ‚îÇ                  ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ Headline     ‚îÇ  ‚îÇ   Product Image  ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ Subheadline  ‚îÇ  ‚îÇ   or Screenshot  ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ [CTA] [CTA]  ‚îÇ  ‚îÇ                  ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ Trust text   ‚îÇ  ‚îÇ                  ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n**Used by:** Notion, Slack, Airtable\n\n**2. Centered Hero**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                 Badge                   ‚îÇ\n‚îÇ              Headline                   ‚îÇ\n‚îÇ            Subheadline                  ‚îÇ\n‚îÇ           [CTA]  [CTA]                  ‚îÇ\n‚îÇ            Trust logos                  ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n‚îÇ         ‚îÇ Product Preview ‚îÇ             ‚îÇ\n‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n**Used by:** Linear, Vercel, Stripe\n\n**3. Full-Width Hero with Video**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n‚îÇ ‚ïë      Background Video/Animation      ‚ïë\n‚îÇ ‚ïë                                       ‚ïë\n‚îÇ ‚ïë            Headline                   ‚ïë\n‚îÇ ‚ïë          Subheadline                  ‚ïë\n‚îÇ ‚ïë            [CTA]                      ‚ïë\n‚îÇ ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n**Used by:** Apple, Spotify, Tesla\n\n### Navigation Patterns\n\n**1. Minimal Nav**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Logo          Links            [CTA]    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**2. Mega Menu Nav**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Logo    Products ‚ñº  Resources ‚ñº  [CTA]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ ‚îÇ Feature 1‚îÇ ‚îÇ Feature 2‚îÇ ‚îÇ Feature 3‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Desc     ‚îÇ ‚îÇ Desc     ‚îÇ ‚îÇ Desc     ‚îÇ  ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Pricing Patterns\n\n**1. Three-Tier Pricing**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Starter  ‚îÇ ‚îÇ    Pro     ‚îÇ ‚îÇ Enterprise ‚îÇ\n‚îÇ            ‚îÇ ‚îÇ  POPULAR   ‚îÇ ‚îÇ            ‚îÇ\n‚îÇ   $29/mo   ‚îÇ ‚îÇ   $79/mo   ‚îÇ ‚îÇ  Contact   ‚îÇ\n‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n‚îÇ ‚úì Feature  ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ\n‚îÇ ‚úì Feature  ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ\n‚îÇ            ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ\n‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ ‚úì Feature  ‚îÇ\n‚îÇ  [Start]   ‚îÇ ‚îÇ  [Start]   ‚îÇ ‚îÇ [Contact]  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**2. Usage-Based Pricing**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          Monthly API Calls              ‚îÇ\n‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÇ\n‚îÇ         10,000 calls                    ‚îÇ\n‚îÇ                                         ‚îÇ\n‚îÇ         Estimated: $49/month            ‚îÇ\n‚îÇ              [Get Started]              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Social Proof Patterns\n\n**1. Logo Cloud**\n```\nTrusted by teams at:\n[Logo] [Logo] [Logo] [Logo] [Logo]\n```\n\n**2. Stats Bar**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  50K+    ‚îÇ   1M+    ‚îÇ   99.9%  ‚îÇ   4.9/5  ‚îÇ\n‚îÇ  Users   ‚îÇ  Pages   ‚îÇ  Uptime  ‚îÇ  Rating  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**3. Testimonial Grid**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ      ‚îÇ ‚îÇ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ      ‚îÇ ‚îÇ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ      ‚îÇ\n‚îÇ \"Quote...\" ‚îÇ ‚îÇ \"Quote...\" ‚îÇ ‚îÇ \"Quote...\" ‚îÇ\n‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n‚îÇ üë§ Name    ‚îÇ ‚îÇ üë§ Name    ‚îÇ ‚îÇ üë§ Name    ‚îÇ\n‚îÇ Title, Co  ‚îÇ ‚îÇ Title, Co  ‚îÇ ‚îÇ Title, Co  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Color Palettes by Style\n\n### Dark Mode (Developer)\n```css\n--background: hsl(0 0% 4%);      /* #0a0a0a */\n--foreground: hsl(0 0% 98%);     /* #fafafa */\n--primary: hsl(217 91% 60%);     /* #3b82f6 */\n--muted: hsl(0 0% 15%);          /* #262626 */\n--border: hsl(0 0% 20%);         /* #333333 */\n```\n\n### Light Mode (SaaS)\n```css\n--background: hsl(0 0% 100%);    /* #ffffff */\n--foreground: hsl(222 47% 11%);  /* #0f172a */\n--primary: hsl(221 83% 53%);     /* #2563eb */\n--muted: hsl(210 40% 96%);       /* #f1f5f9 */\n--border: hsl(214 32% 91%);      /* #e2e8f0 */\n```\n\n### Warm & Friendly\n```css\n--background: hsl(30 50% 98%);   /* #fefcfa */\n--foreground: hsl(20 14% 20%);   /* #3d3531 */\n--primary: hsl(25 95% 53%);      /* #f97316 */\n--muted: hsl(30 25% 93%);        /* #f5efe8 */\n--accent: hsl(47 96% 53%);       /* #facc15 */\n```\n\n## Typography Scales\n\n### Modular Scale 1.25 (Minor Third)\n```css\n--text-xs: 0.64rem;    /* 10.24px */\n--text-sm: 0.8rem;     /* 12.8px */\n--text-base: 1rem;     /* 16px */\n--text-lg: 1.25rem;    /* 20px */\n--text-xl: 1.563rem;   /* 25px */\n--text-2xl: 1.953rem;  /* 31.25px */\n--text-3xl: 2.441rem;  /* 39px */\n--text-4xl: 3.052rem;  /* 48.8px */\n```\n\n### Modular Scale 1.333 (Perfect Fourth)\n```css\n--text-sm: 0.75rem;    /* 12px */\n--text-base: 1rem;     /* 16px */\n--text-lg: 1.333rem;   /* 21.3px */\n--text-xl: 1.777rem;   /* 28.4px */\n--text-2xl: 2.369rem;  /* 37.9px */\n--text-3xl: 3.157rem;  /* 50.5px */\n--text-4xl: 4.209rem;  /* 67.3px */\n```\n\n## Spacing Systems\n\n### 8pt Grid\n```css\n--space-1: 0.25rem;   /* 4px */\n--space-2: 0.5rem;    /* 8px */\n--space-3: 0.75rem;   /* 12px */\n--space-4: 1rem;      /* 16px */\n--space-6: 1.5rem;    /* 24px */\n--space-8: 2rem;      /* 32px */\n--space-12: 3rem;     /* 48px */\n--space-16: 4rem;     /* 64px */\n--space-24: 6rem;     /* 96px */\n--space-32: 8rem;     /* 128px */\n```\n\n### Section Spacing\n```css\n/* Desktop */\n--section-padding: 7.5rem;  /* 120px */\n\n/* Tablet */\n--section-padding: 5rem;    /* 80px */\n\n/* Mobile */\n--section-padding: 4rem;    /* 64px */\n```\n\n## Using This Skill\n\n```bash\n# Load the skill\n!{skill design-inspiration}\n\n# Analyze a specific site\n/analyze-design https://linear.app\n\n# Compare multiple sites\n/analyze-design https://vercel.com --compare https://stripe.com\n\n# Apply patterns to your project\n/build-landing-page homepage --saas\n```"
              },
              {
                "name": "design-system-enforcement",
                "description": "Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.",
                "path": "plugins/nextjs-frontend/skills/design-system-enforcement/SKILL.md",
                "frontmatter": {
                  "name": "design-system-enforcement",
                  "description": "Mandatory design system guidelines for shadcn/ui with Tailwind v4. Enforces 4 font sizes, 2 weights, 8pt grid spacing, 60/30/10 color rule, OKLCH colors, and accessibility standards. Use when creating components, pages, or any UI elements. ALL agents MUST read and validate against design system before generating code.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Design System Enforcement\n\n**Purpose:** Enforce consistent, accessible, and beautiful UI across all Next.js projects using shadcn/ui with Tailwind v4.\n\n**Activation Triggers:**\n\n- Creating new components or pages\n- Generating UI elements\n- Styling React components\n- Setting up project design system\n- Before ANY UI code generation\n- Component library initialization\n- Design system validation needed\n\n**Key Resources:**\n\n- `scripts/setup-design-system.sh` - Interactive design system configuration\n- `scripts/validate-design-system.sh` - Validate code against design system\n- `templates/design-system-template.md` - Template with placeholders\n- `examples/taskflow-ai-example.md` - Complete example configuration\n\n## Core Design Principles (MANDATORY)\n\n### 1. Typography: 4 Sizes, 2 Weights ONLY\n\n**STRICTLY ENFORCED:**\n\n- ‚úÖ Size 1: Large headings\n- ‚úÖ Size 2: Subheadings\n- ‚úÖ Size 3: Body text\n- ‚úÖ Size 4: Small text/labels\n\n- ‚úÖ Semibold: Headings and emphasis\n- ‚úÖ Regular: Body text and UI\n\n**‚ùå FORBIDDEN:**\n\n- More than 4 font sizes\n- Additional font weights (bold, light, etc.)\n- Inconsistent size application\n\n### 2. 8pt Grid System\n\n**STRICTLY ENFORCED:**\n\n- ALL spacing MUST be divisible by 8 or 4\n- ‚úÖ Allowed: 8, 16, 24, 32, 40, 48, 56, 64px\n- ‚ùå Forbidden: 25, 11, 7, 13, 15, 19px\n\n**Tailwind Classes:**\n\n```\np-2 (8px)   | m-2 (8px)   | gap-2 (8px)\np-4 (16px)  | m-4 (16px)  | gap-4 (16px)\np-6 (24px)  | m-6 (24px)  | gap-6 (24px)\np-8 (32px)  | m-8 (32px)  | gap-8 (32px)\n```\n\n### 3. 60/30/10 Color Rule\n\n**STRICTLY ENFORCED:**\n\n- 60% Neutral (`bg-background`) - White/dark backgrounds\n- 30% Complementary (`text-foreground`) - Text and icons\n- 10% Accent (`bg-primary`) - CTAs and highlights only\n\n**‚ùå FORBIDDEN:**\n\n- Overusing accent colors (>10%)\n- Multiple competing accent colors\n- Insufficient contrast ratios\n\n### 4. Clean Visual Structure\n\n**REQUIRED:**\n\n- Logical grouping of related elements\n- Deliberate spacing following 8pt grid\n- Proper alignment within containers\n- Simplicity over flashiness\n\n## Setup Workflow\n\n### 1. Initialize Design System\n\nRun setup script during project initialization:\n\n```bash\n# Interactive setup\n./scripts/setup-design-system.sh\n\n# Guided configuration:\n# 1. Project name and brand color\n# 2. Typography scale (4 sizes)\n# 3. Color configuration (OKLCH format)\n# 4. Dark mode colors\n# 5. Figma design system URL\n\n# Generates: design-system.md in project root\n```\n\n**What Gets Configured:**\n\n- Project-specific brand colors\n- Font size scale (must be 4 sizes)\n- OKLCH color values\n- Dark mode palette\n- globals.css color variables\n- Design system metadata\n\n### 2. Validate Existing Code\n\nCheck if existing code follows design system:\n\n```bash\n# Validate all components\n./scripts/validate-design-system.sh\n\n# Checks performed:\n# - Font size count (must be ‚â§ 4)\n# - Font weight usage (must be 2)\n# - Spacing divisibility (by 8 or 4)\n# - Color distribution (60/30/10)\n# - Custom CSS usage (should use Tailwind)\n# - shadcn/ui component usage\n# - Accessibility compliance\n```\n\n**Validation Output:**\n\n```\n‚úÖ Typography: 4 sizes, 2 weights\n‚úÖ Spacing: All divisible by 8/4\n‚ùå Colors: Accent usage at 15% (exceeds 10%)\n‚ùå Custom CSS: Found 3 instances, use Tailwind utilities\n‚ö†Ô∏è  Accessibility: Missing ARIA labels on 2 components\n```\n\n### 3. Before Creating UI\n\n**MANDATORY AGENT WORKFLOW:**\n\n```bash\n# 1. Read design system (REQUIRED)\ncat design-system.md\n\n# 2. Understand constraints\n# - Only 4 font sizes from config\n# - Only 2 font weights\n# - All spacing divisible by 8/4\n# - Color distribution 60/30/10\n# - OKLCH colors only\n# - shadcn/ui components only\n\n# 3. Generate code following design system\n\n# 4. Self-validate before completion\n./scripts/validate-design-system.sh app/components/MyNewComponent.tsx\n```\n\n## Design System Configuration\n\n### Typography Configuration\n\n**From Template:**\n\n```markdown\nSize 1: {{FONT_SIZE_1}} - Large headings\nSize 2: {{FONT_SIZE_2}} - Subheadings\nSize 3: {{FONT_SIZE_3}} - Body text\nSize 4: {{FONT_SIZE_4}} - Small text\n```\n\n**After Setup (Example):**\n\n```markdown\nSize 1: text-2xl (24px) - Large headings\nSize 2: text-lg (18px) - Subheadings\nSize 3: text-base (16px) - Body text\nSize 4: text-sm (14px) - Small text\n```\n\n### Color Configuration\n\n**Template (OKLCH format):**\n\n```css\n:root {\n  --background: {{BACKGROUND_OKLCH}};\n  --foreground: {{FOREGROUND_OKLCH}};\n  --primary: {{PRIMARY_OKLCH}};\n  --primary-foreground: {{PRIMARY_FOREGROUND_OKLCH}};\n}\n```\n\n**After Setup:**\n\n```css\n:root {\n  --background: oklch(1 0 0);\n  --foreground: oklch(0.145 0 0);\n  --primary: oklch(0.549 0.175 252.417);\n  --primary-foreground: oklch(0.985 0 0);\n}\n\n@theme {\n  --color-background: var(--background);\n  --color-foreground: var(--foreground);\n  --color-primary: var(--primary);\n  --color-primary-foreground: var(--primary-foreground);\n}\n```\n\n## Agent Enforcement Rules\n\n### Before Generating ANY UI Code\n\n**MANDATORY CHECKLIST:**\n\n1. [ ] Read `design-system.md` file\n2. [ ] Understand font size constraints (4 only)\n3. [ ] Understand font weight constraints (2 only)\n4. [ ] Understand spacing constraints (divisible by 8/4)\n5. [ ] Understand color distribution (60/30/10)\n6. [ ] Know OKLCH color variables\n7. [ ] Use only shadcn/ui components\n\n### During Code Generation\n\n**ENFORCE:**\n\n- Use only configured font sizes\n- Use only Semibold or Regular weights\n- All spacing values divisible by 8 or 4\n- 60% `bg-background`, 30% `text-foreground`, 10% `bg-primary`\n- OKLCH colors from globals.css\n- shadcn/ui components from `@/components/ui/`\n- Proper accessibility (ARIA labels, keyboard nav)\n\n### After Code Generation\n\n**VALIDATE:**\n\n```bash\n# Self-validation\n./scripts/validate-design-system.sh path/to/component.tsx\n\n# Must pass all checks before completion:\n# ‚úÖ Typography constraints\n# ‚úÖ Spacing constraints\n# ‚úÖ Color distribution\n# ‚úÖ No custom CSS\n# ‚úÖ Accessibility\n```\n\n**‚ùå AUTOMATIC REJECTION:**\n\n- More than 4 font sizes\n- Font weights other than Semibold/Regular\n- Spacing not divisible by 4 or 8\n- Accent color usage > 10%\n- Custom CSS instead of Tailwind\n- Non-shadcn/ui components\n\n## Example Component (Compliant)\n\n```tsx\nimport { Button } from '@/components/ui/button';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\n\nexport function ExampleComponent() {\n  return (\n    <Card className=\"p-6 bg-background\">\n      {' '}\n      {/* 24px padding - ‚úÖ divisible by 8 */}\n      <CardHeader className=\"mb-4\">\n        {' '}\n        {/* 16px margin - ‚úÖ divisible by 8 */}\n        <CardTitle className=\"text-2xl font-semibold\">\n          {' '}\n          {/* ‚úÖ Size 1, Semibold */}\n          Welcome to TaskFlow\n        </CardTitle>\n      </CardHeader>\n      <CardContent className=\"space-y-4\">\n        {' '}\n        {/* 16px gap - ‚úÖ divisible by 8 */}\n        <p className=\"text-base font-normal text-foreground\">\n          {' '}\n          {/* ‚úÖ Size 3, Regular, 60% */}\n          Manage your tasks efficiently with AI-powered workflows.\n        </p>\n        <div className=\"flex gap-4\">\n          {' '}\n          {/* 16px gap - ‚úÖ divisible by 8 */}\n          <Button className=\"bg-primary text-primary-foreground\">\n            {' '}\n            {/* ‚úÖ 10% accent */}\n            Get Started\n          </Button>\n          <Button variant=\"outline\" className=\"text-foreground\">\n            {' '}\n            {/* ‚úÖ 30% complementary */}\n            Learn More\n          </Button>\n        </div>\n      </CardContent>\n    </Card>\n  );\n}\n```\n\n**Validation:**\n\n- ‚úÖ Typography: 2 sizes (text-2xl, text-base), 2 weights (semibold, normal)\n- ‚úÖ Spacing: All divisible by 8 (p-6=24px, mb-4=16px, space-y-4=16px, gap-4=16px)\n- ‚úÖ Colors: 60% bg-background, 30% text-foreground, 10% bg-primary\n- ‚úÖ Components: shadcn/ui Button and Card\n- ‚úÖ No custom CSS\n- ‚úÖ Accessible: Proper semantic HTML\n\n## Integration with Commands\n\n### add-page.md Integration\n\n```markdown\nPhase 1: Parse Arguments\nActions:\n\n- **FIRST**: Read design system: !{bash cat design-system.md}\n- Parse page name from $ARGUMENTS\n\nPhase 4: Page Generation\n\n- Agent MUST validate against design system before completion\n```\n\n### add-component.md Integration\n\n```markdown\nPhase 1: Parse Arguments\nActions:\n\n- **FIRST**: Read design system: !{bash cat design-system.md}\n- Parse component name from $ARGUMENTS\n\nPhase 4: Implementation\n\n- Agent MUST enforce all design system constraints\n- Self-validate before returning component\n```\n\n## Troubleshooting\n\n### Violation: Too Many Font Sizes\n\n**Error:**\n\n```\n‚ùå Found 6 font sizes: text-xs, text-sm, text-base, text-lg, text-xl, text-2xl\nExpected: 4 font sizes only\n```\n\n**Fix:**\n\n1. Review configured sizes in `design-system.md`\n2. Consolidate to 4 sizes (typically: 2xl, lg, base, sm)\n3. Update components to use only allowed sizes\n\n### Violation: Invalid Spacing\n\n**Error:**\n\n```\n‚ùå Found spacing not divisible by 8/4:\n  - padding: 15px (line 42)\n  - margin: 25px (line 58)\n```\n\n**Fix:**\n\n```tsx\n// ‚ùå Before\n<div className=\"p-[15px] mb-[25px]\">\n\n// ‚úÖ After\n<div className=\"p-4 mb-6\"> {/* 16px and 24px */}\n```\n\n### Violation: Color Distribution\n\n**Error:**\n\n```\n‚ùå Accent color usage: 18% (exceeds 10% limit)\nFound 12 instances of bg-primary\n```\n\n**Fix:**\n\n1. Review component layout\n2. Reduce accent color to CTAs and highlights only\n3. Use `bg-background` and `text-foreground` for majority\n\n## Resources\n\n**Scripts:** `scripts/` directory:\n\n- `setup-design-system.sh` - Interactive configuration\n- `validate-design-system.sh` - Code validation\n- `update-colors.sh` - Batch color updates\n- `check-typography.sh` - Typography audit\n\n**Templates:** `templates/` directory:\n\n- `design-system-template.md` - Base template with placeholders\n- `globals-css-template.css` - Color variable template\n- `components-json-template.json` - shadcn/ui config\n\n**Examples:** `examples/` directory:\n\n- `taskflow-ai-example.md` - Complete configured example\n- `e-commerce-example.md` - E-commerce design system\n- `dashboard-example.md` - Admin dashboard design system\n\n---\n\n**Framework:** Next.js 13+ with App Router\n**UI Library:** shadcn/ui (Radix UI + Tailwind CSS v4)\n**Color Format:** OKLCH\n**Enforcement:** Mandatory for all agents\n**Version:** 1.0.0"
              },
              {
                "name": "seo-2025-patterns",
                "description": "2025 SEO best practices for Next.js including Core Web Vitals (INP replaces FID), E-E-A-T signals, Schema markup, AI content guidelines, and technical SEO. Use when optimizing pages for search engines, implementing metadata, adding structured data, or improving page speed.",
                "path": "plugins/nextjs-frontend/skills/seo-2025-patterns/SKILL.md",
                "frontmatter": {
                  "name": "seo-2025-patterns",
                  "description": "2025 SEO best practices for Next.js including Core Web Vitals (INP replaces FID), E-E-A-T signals, Schema markup, AI content guidelines, and technical SEO. Use when optimizing pages for search engines, implementing metadata, adding structured data, or improving page speed.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# SEO 2025 Patterns\n\n**Purpose:** Implement 2025 SEO best practices for Next.js applications to maximize search visibility and organic traffic.\n\n**Activation Triggers:**\n- Optimizing pages for search engines\n- Implementing metadata and Open Graph\n- Adding Schema.org structured data\n- Improving Core Web Vitals\n- E-E-A-T signal implementation\n- Sitemap and robots.txt setup\n- Meta description optimization\n\n**Key Resources:**\n- `scripts/seo-audit.sh` - Comprehensive SEO audit script\n- `scripts/validate-schema.sh` - Validate JSON-LD structured data\n- `templates/metadata-patterns.tsx` - Next.js Metadata API patterns\n- `templates/schema-components.tsx` - JSON-LD schema components\n- `examples/complete-seo-setup.md` - Full SEO implementation example\n\n## 2025 SEO Landscape\n\n### Key Changes from 2024\n\n1. **INP Replaces FID** - Interaction to Next Paint is now a Core Web Vital\n2. **AI Content Guidelines** - Google rewards helpful AI content with human oversight\n3. **E-E-A-T Enhanced** - \"Experience\" added as first factor\n4. **Passage Ranking** - Google indexes specific passages\n5. **Video SEO** - Video snippets dominate SERPs\n6. **Zero-Click Optimization** - Featured snippets and AI Overviews\n\n### Core Web Vitals Targets (2025)\n\n| Metric | Target | What It Measures |\n|--------|--------|------------------|\n| LCP | < 2.5s | Largest Contentful Paint |\n| INP | < 200ms | Interaction to Next Paint |\n| CLS | < 0.1 | Cumulative Layout Shift |\n\n## Next.js Metadata API\n\n### Basic Metadata Configuration\n\n```typescript\n// app/layout.tsx\nimport type { Metadata } from 'next'\n\nexport const metadata: Metadata = {\n  metadataBase: new URL('https://example.com'),\n  title: {\n    default: 'Site Name - Main Tagline',\n    template: '%s | Site Name',\n  },\n  description: 'Your site description for search engines (150-160 characters)',\n  keywords: ['keyword1', 'keyword2', 'keyword3'],\n  authors: [{ name: 'Author Name', url: 'https://author.com' }],\n  creator: 'Creator Name',\n  publisher: 'Publisher Name',\n  formatDetection: {\n    email: false,\n    address: false,\n    telephone: false,\n  },\n}\n```\n\n### Open Graph Configuration\n\n```typescript\nexport const metadata: Metadata = {\n  openGraph: {\n    type: 'website',\n    locale: 'en_US',\n    url: 'https://example.com',\n    siteName: 'Site Name',\n    title: 'Page Title for Social Sharing',\n    description: 'Description for social media (150-200 chars)',\n    images: [\n      {\n        url: '/og-image.png',\n        width: 1200,\n        height: 630,\n        alt: 'OG Image Alt Text',\n      },\n    ],\n  },\n}\n```\n\n### Twitter Card Configuration\n\n```typescript\nexport const metadata: Metadata = {\n  twitter: {\n    card: 'summary_large_image',\n    site: '@sitehandle',\n    creator: '@creatorhandle',\n    title: 'Title for Twitter',\n    description: 'Description for Twitter (150-200 chars)',\n    images: ['/twitter-image.png'],\n  },\n}\n```\n\n### Robots Configuration\n\n```typescript\nexport const metadata: Metadata = {\n  robots: {\n    index: true,\n    follow: true,\n    nocache: false,\n    googleBot: {\n      index: true,\n      follow: true,\n      'max-video-preview': -1,\n      'max-image-preview': 'large',\n      'max-snippet': -1,\n    },\n  },\n}\n```\n\n### Dynamic Metadata\n\n```typescript\n// app/blog/[slug]/page.tsx\nimport type { Metadata, ResolvingMetadata } from 'next'\n\ntype Props = {\n  params: { slug: string }\n}\n\nexport async function generateMetadata(\n  { params }: Props,\n  parent: ResolvingMetadata\n): Promise<Metadata> {\n  const post = await getPost(params.slug)\n\n  return {\n    title: post.title,\n    description: post.excerpt,\n    openGraph: {\n      title: post.title,\n      description: post.excerpt,\n      type: 'article',\n      publishedTime: post.publishedAt,\n      modifiedTime: post.updatedAt,\n      authors: [post.author.name],\n      images: [\n        {\n          url: post.coverImage,\n          width: 1200,\n          height: 630,\n          alt: post.title,\n        },\n      ],\n    },\n  }\n}\n```\n\n## Dynamic Sitemap\n\n```typescript\n// app/sitemap.ts\nimport { MetadataRoute } from 'next'\n\nexport default async function sitemap(): Promise<MetadataRoute.Sitemap> {\n  const baseUrl = 'https://example.com'\n\n  // Static pages\n  const staticPages: MetadataRoute.Sitemap = [\n    {\n      url: baseUrl,\n      lastModified: new Date(),\n      changeFrequency: 'weekly',\n      priority: 1,\n    },\n    {\n      url: `${baseUrl}/about`,\n      lastModified: new Date(),\n      changeFrequency: 'monthly',\n      priority: 0.8,\n    },\n    {\n      url: `${baseUrl}/pricing`,\n      lastModified: new Date(),\n      changeFrequency: 'monthly',\n      priority: 0.9,\n    },\n  ]\n\n  // Dynamic pages from database\n  const posts = await getAllPosts()\n  const postPages: MetadataRoute.Sitemap = posts.map((post) => ({\n    url: `${baseUrl}/blog/${post.slug}`,\n    lastModified: new Date(post.updatedAt),\n    changeFrequency: 'weekly' as const,\n    priority: 0.6,\n  }))\n\n  return [...staticPages, ...postPages]\n}\n```\n\n## Robots.txt\n\n```typescript\n// app/robots.ts\nimport { MetadataRoute } from 'next'\n\nexport default function robots(): MetadataRoute.Robots {\n  const baseUrl = 'https://example.com'\n\n  return {\n    rules: [\n      {\n        userAgent: '*',\n        allow: '/',\n        disallow: ['/api/', '/admin/', '/_next/', '/private/'],\n      },\n      {\n        userAgent: 'GPTBot',\n        disallow: '/', // Block AI training crawlers if desired\n      },\n    ],\n    sitemap: `${baseUrl}/sitemap.xml`,\n  }\n}\n```\n\n## Schema.org Structured Data\n\n### Organization Schema\n\n```typescript\n// components/seo/OrganizationSchema.tsx\nexport function OrganizationSchema() {\n  const schema = {\n    '@context': 'https://schema.org',\n    '@type': 'Organization',\n    name: 'Company Name',\n    url: 'https://example.com',\n    logo: 'https://example.com/logo.png',\n    sameAs: [\n      'https://twitter.com/company',\n      'https://linkedin.com/company/company',\n      'https://github.com/company',\n    ],\n    contactPoint: {\n      '@type': 'ContactPoint',\n      telephone: '+1-555-555-5555',\n      contactType: 'customer service',\n      availableLanguage: ['English'],\n    },\n  }\n\n  return (\n    <script\n      type=\"application/ld+json\"\n      dangerouslySetInnerHTML={{ __html: JSON.stringify(schema) }}\n    />\n  )\n}\n```\n\n### Article Schema\n\n```typescript\n// components/seo/ArticleSchema.tsx\ninterface ArticleSchemaProps {\n  title: string\n  description: string\n  image: string\n  author: { name: string; url: string }\n  publishedAt: string\n  updatedAt: string\n  url: string\n}\n\nexport function ArticleSchema(props: ArticleSchemaProps) {\n  const schema = {\n    '@context': 'https://schema.org',\n    '@type': 'Article',\n    headline: props.title,\n    description: props.description,\n    image: props.image,\n    author: {\n      '@type': 'Person',\n      name: props.author.name,\n      url: props.author.url,\n    },\n    publisher: {\n      '@type': 'Organization',\n      name: 'Company Name',\n      logo: {\n        '@type': 'ImageObject',\n        url: 'https://example.com/logo.png',\n      },\n    },\n    datePublished: props.publishedAt,\n    dateModified: props.updatedAt,\n    mainEntityOfPage: {\n      '@type': 'WebPage',\n      '@id': props.url,\n    },\n  }\n\n  return (\n    <script\n      type=\"application/ld+json\"\n      dangerouslySetInnerHTML={{ __html: JSON.stringify(schema) }}\n    />\n  )\n}\n```\n\n### FAQ Schema\n\n```typescript\n// components/seo/FAQSchema.tsx\ninterface FAQItem {\n  question: string\n  answer: string\n}\n\nexport function FAQSchema({ items }: { items: FAQItem[] }) {\n  const schema = {\n    '@context': 'https://schema.org',\n    '@type': 'FAQPage',\n    mainEntity: items.map((item) => ({\n      '@type': 'Question',\n      name: item.question,\n      acceptedAnswer: {\n        '@type': 'Answer',\n        text: item.answer,\n      },\n    })),\n  }\n\n  return (\n    <script\n      type=\"application/ld+json\"\n      dangerouslySetInnerHTML={{ __html: JSON.stringify(schema) }}\n    />\n  )\n}\n```\n\n### Breadcrumb Schema\n\n```typescript\n// components/seo/BreadcrumbSchema.tsx\ninterface BreadcrumbItem {\n  name: string\n  url: string\n}\n\nexport function BreadcrumbSchema({ items }: { items: BreadcrumbItem[] }) {\n  const schema = {\n    '@context': 'https://schema.org',\n    '@type': 'BreadcrumbList',\n    itemListElement: items.map((item, index) => ({\n      '@type': 'ListItem',\n      position: index + 1,\n      name: item.name,\n      item: item.url,\n    })),\n  }\n\n  return (\n    <script\n      type=\"application/ld+json\"\n      dangerouslySetInnerHTML={{ __html: JSON.stringify(schema) }}\n    />\n  )\n}\n```\n\n## Core Web Vitals Optimization\n\n### LCP Optimization\n\n```typescript\n// Prioritize above-fold images\nimport Image from 'next/image'\n\n// Hero image with priority\n<Image\n  src=\"/hero.jpg\"\n  alt=\"Hero description\"\n  width={1200}\n  height={600}\n  priority  // Preloads image for LCP\n  sizes=\"(max-width: 768px) 100vw, 1200px\"\n/>\n```\n\n### Font Optimization\n\n```typescript\n// app/layout.tsx\nimport { Inter } from 'next/font/google'\n\nconst inter = Inter({\n  subsets: ['latin'],\n  display: 'swap',  // Prevents CLS\n  preload: true,\n  variable: '--font-inter',\n})\n\nexport default function RootLayout({ children }) {\n  return (\n    <html lang=\"en\" className={inter.variable}>\n      <body>{children}</body>\n    </html>\n  )\n}\n```\n\n### Script Optimization\n\n```typescript\nimport Script from 'next/script'\n\n// Defer non-critical scripts\n<Script\n  src=\"https://analytics.example.com/script.js\"\n  strategy=\"lazyOnload\"  // Load after page is interactive\n/>\n\n// Load immediately for critical scripts\n<Script\n  src=\"https://critical.example.com/script.js\"\n  strategy=\"afterInteractive\"\n/>\n```\n\n## E-E-A-T Signals\n\n### Author Information\n\n```typescript\n// components/AuthorBio.tsx\nexport function AuthorBio({ author }) {\n  return (\n    <div className=\"flex items-center gap-4 p-4 bg-muted rounded-lg\">\n      <Image\n        src={author.avatar}\n        alt={author.name}\n        width={64}\n        height={64}\n        className=\"rounded-full\"\n      />\n      <div>\n        <h3 className=\"font-semibold\">{author.name}</h3>\n        <p className=\"text-sm text-muted-foreground\">{author.title}</p>\n        <p className=\"text-sm\">{author.bio}</p>\n        <div className=\"flex gap-2 mt-2\">\n          <a href={author.twitter}>Twitter</a>\n          <a href={author.linkedin}>LinkedIn</a>\n        </div>\n      </div>\n    </div>\n  )\n}\n```\n\n### Trust Signals\n\n- Clear contact information\n- Privacy policy and terms of service\n- About page with company information\n- Author pages with credentials\n- Published and updated dates\n- HTTPS everywhere\n\n## SEO Audit Checklist\n\n```bash\n# Run comprehensive SEO audit\n./scripts/seo-audit.sh\n\n# Checks:\n# ‚úì All pages have unique titles\n# ‚úì All pages have meta descriptions\n# ‚úì Open Graph tags present\n# ‚úì Twitter cards configured\n# ‚úì Sitemap.xml exists and valid\n# ‚úì Robots.txt configured\n# ‚úì Schema markup valid\n# ‚úì Images have alt text\n# ‚úì Heading hierarchy correct\n# ‚úì Internal linking structure\n```\n\n## Testing Tools\n\n- **Google Search Console** - Monitor indexing and search performance\n- **PageSpeed Insights** - Core Web Vitals measurement\n- **Rich Results Test** - Validate structured data\n- **Mobile-Friendly Test** - Mobile compatibility\n- **Schema.org Validator** - Validate JSON-LD"
              },
              {
                "name": "tailwind-shadcn-setup",
                "description": "Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.",
                "path": "plugins/nextjs-frontend/skills/tailwind-shadcn-setup/SKILL.md",
                "frontmatter": {
                  "name": "tailwind-shadcn-setup",
                  "description": "Setup Tailwind CSS and shadcn/ui component library for Next.js projects. Use when configuring Tailwind CSS, installing shadcn/ui, setting up design tokens, configuring dark mode, initializing component library, or when user mentions Tailwind setup, shadcn/ui installation, component system, design system, or theming.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# tailwind-shadcn-setup\n\n## Instructions\n\nThis skill provides complete setup and configuration for Tailwind CSS and shadcn/ui in Next.js projects. It covers installation, configuration, theming, dark mode, and component integration following modern best practices.\n\n### 1. Tailwind CSS Installation\n\nInstall and configure Tailwind CSS for Next.js:\n\n```bash\n# Run automated installation script\nbash ./skills/tailwind-shadcn-setup/scripts/install-tailwind.sh\n\n# Or manually install dependencies\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n```\n\n**What This Does:**\n- Installs Tailwind CSS, PostCSS, and Autoprefixer\n- Creates `tailwind.config.ts` and `postcss.config.mjs`\n- Configures content paths for Next.js\n- Sets up CSS imports in global styles\n\n### 2. shadcn/ui Initialization\n\nInitialize shadcn/ui component library:\n\n```bash\n# Run automated shadcn/ui setup\nbash ./skills/tailwind-shadcn-setup/scripts/init-shadcn.sh\n\n# Or use shadcn CLI directly\nnpx shadcn@latest init\n```\n\n**Configuration Prompts:**\n- TypeScript: Yes (recommended)\n- Style: Default or New York\n- Base color: Slate, Zinc, Neutral, Stone, or Gray\n- CSS variables: Yes (recommended for theming)\n- Tailwind config: tailwind.config.ts\n- Components location: @/components\n- Utils location: @/lib/utils\n\n**What Gets Created:**\n- `components.json` - shadcn/ui configuration\n- `lib/utils.ts` - Utility functions (cn helper)\n- `app/globals.css` - CSS variables and theme definitions\n- Base component structure\n\n### 3. Design Tokens & Theme Configuration\n\nConfigure design tokens using CSS variables:\n\n```bash\n# Apply comprehensive theme configuration\nbash ./skills/tailwind-shadcn-setup/scripts/setup-theme.sh\n```\n\n**Theme Configuration Includes:**\n- Primary, secondary, accent colors\n- Background and foreground pairs\n- Border, input, ring colors\n- Muted, destructive, card, popover colors\n- Chart colors (chart-1 through chart-5)\n- Sidebar colors for navigation components\n\n**Using CSS Variables Template:**\n```typescript\n// Copy and customize base theme\ncp ./skills/tailwind-shadcn-setup/templates/globals.css app/globals.css\n```\n\n**Color System:**\n- Uses OKLCH color space for better perceptual uniformity\n- Separate light and dark mode definitions\n- Foreground colors automatically calculated for accessibility\n\n### 4. Dark Mode Configuration\n\nSet up dark mode with class-based or system-based detection:\n\n```bash\n# Configure dark mode\nbash ./skills/tailwind-shadcn-setup/scripts/setup-dark-mode.sh\n```\n\n**Dark Mode Strategies:**\n1. **Class-based** (Recommended): Uses `.dark` class for manual toggle\n2. **Media-based**: Respects system preference automatically\n3. **Hybrid**: Manual toggle with system default\n\n**Provider Setup:**\n```typescript\n// Copy theme provider template\ncp ./skills/tailwind-shadcn-setup/templates/theme-provider.tsx components/theme-provider.tsx\n```\n\n**Theme Toggle Component:**\n```bash\n# Add theme toggle button\nnpx shadcn@latest add dropdown-menu\ncp ./skills/tailwind-shadcn-setup/examples/theme-toggle.tsx components/theme-toggle.tsx\n```\n\n### 5. Adding Components\n\nInstall shadcn/ui components as needed:\n\n```bash\n# Add individual components\nnpx shadcn@latest add button\nnpx shadcn@latest add card\nnpx shadcn@latest add input\nnpx shadcn@latest add form\n\n# Add multiple components at once\nnpx shadcn@latest add button card input form dialog sheet\n```\n\n**Common Component Sets:**\n- **Forms**: button, input, label, select, textarea, checkbox, radio-group, form\n- **Layout**: card, separator, aspect-ratio, scroll-area\n- **Navigation**: navigation-menu, menubar, dropdown-menu, tabs\n- **Feedback**: toast, alert, alert-dialog, dialog\n- **Data**: table, data-table, pagination\n\n### 6. Custom Component Configuration\n\nCreate custom components using shadcn/ui primitives:\n\n```typescript\n// Example: Custom button variant\nimport { Button } from \"@/components/ui/button\"\n\n<Button variant=\"default\">Default</Button>\n<Button variant=\"destructive\">Destructive</Button>\n<Button variant=\"outline\">Outline</Button>\n<Button variant=\"secondary\">Secondary</Button>\n<Button variant=\"ghost\">Ghost</Button>\n<Button variant=\"link\">Link</Button>\n```\n\n**Extending Components:**\n```typescript\n// Add custom variants in tailwind.config.ts\n// Components automatically use CSS variables for theming\n```\n\n## Examples\n\n### Example 1: Complete Setup for New Next.js Project\n\n```bash\n# 1. Install Tailwind CSS\nbash ./skills/tailwind-shadcn-setup/scripts/install-tailwind.sh\n\n# 2. Initialize shadcn/ui\nbash ./skills/tailwind-shadcn-setup/scripts/init-shadcn.sh\n\n# 3. Setup theme and dark mode\nbash ./skills/tailwind-shadcn-setup/scripts/setup-dark-mode.sh\n\n# 4. Add common components\nnpx shadcn@latest add button card input form dialog toast\n\n# 5. Copy theme toggle component\ncp ./skills/tailwind-shadcn-setup/examples/theme-toggle.tsx components/theme-toggle.tsx\n```\n\n**Result:** Fully configured Next.js project with Tailwind, shadcn/ui, dark mode, and essential components\n\n### Example 2: Custom Theme with Brand Colors\n\n```bash\n# 1. Run theme setup\nbash ./skills/tailwind-shadcn-setup/scripts/setup-theme.sh\n\n# 2. Edit CSS variables for brand colors\n# Modify app/globals.css to use your brand colors\n# Example: Primary color = oklch(0.5 0.2 250) for brand blue\n\n# 3. Test theme with sample components\ncp ./skills/tailwind-shadcn-setup/examples/theme-showcase.tsx app/page.tsx\n```\n\n**Result:** Custom-branded design system using your colors while maintaining shadcn/ui components\n\n### Example 3: Form-Heavy Application Setup\n\n```bash\n# 1. Complete base setup\nbash ./skills/tailwind-shadcn-setup/scripts/install-tailwind.sh\nbash ./skills/tailwind-shadcn-setup/scripts/init-shadcn.sh\n\n# 2. Add all form-related components\nnpx shadcn@latest add form input label select textarea checkbox radio-group switch slider\n\n# 3. Install react-hook-form and zod (form dependencies)\nnpm install react-hook-form zod @hookform/resolvers\n\n# 4. Copy form example template\ncp ./skills/tailwind-shadcn-setup/examples/sample-form.tsx components/forms/sample-form.tsx\n```\n\n**Result:** Complete form setup with validation, accessibility, and consistent styling\n\n## Requirements\n\n**Dependencies:**\n- Next.js 13.4+ (App Router or Pages Router)\n- React 18+\n- Node.js 18.17+ or 20+\n- TypeScript (recommended)\n\n**Package Manager:**\n- npm, pnpm, yarn, or bun (any modern package manager)\n\n**Project Structure:**\n- Next.js project initialized with `create-next-app`\n- `app/` directory (App Router) or `pages/` directory (Pages Router)\n- `components/` directory for UI components\n- `lib/` directory for utilities\n\n**For Dark Mode:**\n- `next-themes` package (automatically installed by shadcn CLI)\n- Client-side provider component\n\n## Configuration Files Created\n\n**tailwind.config.ts:**\n```typescript\nimport type { Config } from \"tailwindcss\"\n\nconst config: Config = {\n  darkMode: [\"class\"],\n  content: [\n    \"./pages/**/*.{ts,tsx}\",\n    \"./components/**/*.{ts,tsx}\",\n    \"./app/**/*.{ts,tsx}\",\n    \"./src/**/*.{ts,tsx}\",\n  ],\n  theme: {\n    extend: {\n      // CSS variable-based theming\n    },\n  },\n  plugins: [require(\"tailwindcss-animate\")],\n}\nexport default config\n```\n\n**components.json:**\n```json\n{\n  \"$schema\": \"https://ui.shadcn.com/schema.json\",\n  \"style\": \"default\",\n  \"rsc\": true,\n  \"tsx\": true,\n  \"tailwind\": {\n    \"config\": \"tailwind.config.ts\",\n    \"css\": \"app/globals.css\",\n    \"baseColor\": \"slate\",\n    \"cssVariables\": true\n  },\n  \"aliases\": {\n    \"components\": \"@/components\",\n    \"utils\": \"@/lib/utils\"\n  }\n}\n```\n\n**app/globals.css:**\n- `:root` variables for light mode\n- `.dark` variables for dark mode\n- Base layer imports\n- Tailwind directives\n\n## Best Practices\n\n**CSS Variables Over Utility Classes:**\n- Use `cssVariables: true` in components.json\n- Allows runtime theme switching\n- Better for multi-theme applications\n- Easier to customize without recompiling\n\n**Component Organization:**\n```\ncomponents/\n‚îú‚îÄ‚îÄ ui/              # shadcn/ui components (auto-generated)\n‚îÇ   ‚îú‚îÄ‚îÄ button.tsx\n‚îÇ   ‚îú‚îÄ‚îÄ card.tsx\n‚îÇ   ‚îî‚îÄ‚îÄ input.tsx\n‚îú‚îÄ‚îÄ theme-provider.tsx\n‚îú‚îÄ‚îÄ theme-toggle.tsx\n‚îî‚îÄ‚îÄ [custom-components]/\n```\n\n**Theming Strategy:**\n1. Start with a base color (Slate, Zinc, etc.)\n2. Customize CSS variables for brand colors\n3. Use OKLCH color space for consistency\n4. Test in both light and dark modes\n5. Ensure foreground colors meet WCAG contrast ratios\n\n**Performance:**\n- Only install components you actually use\n- shadcn/ui copies components to your codebase (not a dependency)\n- Tree-shaking automatically removes unused Tailwind classes\n- PostCSS processes CSS at build time (zero runtime cost)\n\n**Dark Mode UX:**\n- Persist user preference in localStorage\n- Respect system preference as default\n- Provide manual toggle for user control\n- Avoid flash of unstyled content (FOUC) with theme script\n\n## Integration with Next.js Features\n\n**Server Components:**\n- shadcn/ui components work with Server Components\n- Theme provider must be Client Component (`'use client'`)\n- Form components require Client Components for interactivity\n\n**Route Handlers:**\n- Use consistent styling across API-driven UIs\n- Theme variables accessible in CSS for generated content\n\n**Metadata API:**\n- Configure theme-color meta tag based on dark mode\n- Integrate with app manifest for PWA support\n\n---\n\n**Plugin:** nextjs-frontend\n**Version:** 1.0.0\n**Category:** UI & Styling\n**Skill Type:** Setup & Configuration"
              }
            ]
          },
          {
            "name": "openrouter",
            "description": "OpenRouter SDK plugin - unified interface for 500+ LLM models with intelligent routing, cost optimization, and framework integrations (Vercel AI SDK, LangChain, OpenAI SDK, PydanticAI)",
            "source": "./plugins/openrouter",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install openrouter@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-langchain",
                "description": "Add LangChain integration with OpenRouter for chains, agents, and RAG",
                "path": "plugins/openrouter/commands/add-langchain.md",
                "frontmatter": {
                  "description": "Add LangChain integration with OpenRouter for chains, agents, and RAG",
                  "argument-hint": [
                    "feature"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the openrouter plugin:\n\n- **model-routing-patterns**: Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.\n- **openrouter-config-validator**: Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.\n- **provider-integration-templates**: OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate LangChain with OpenRouter for building chains, agents, and RAG applications with access to 500+ models.\n\nCore Principles:\n- Detect project language (Python/TypeScript/JavaScript)\n- Install appropriate LangChain packages for OpenRouter\n- Create working examples for chains, agents, and RAG\n- Provide model routing capabilities\n\nPhase 1: Discovery\nGoal: Understand project setup and requirements\n\nActions:\n- Load OpenRouter documentation:\n  @plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md\n- Detect project language:\n  !{bash test -f package.json && echo \"Node.js\" || test -f requirements.txt -o -f pyproject.toml && echo \"Python\" || echo \"Unknown\"}\n- Check for existing LangChain installation:\n  !{bash npm list langchain 2>/dev/null || pip list | grep langchain 2>/dev/null || echo \"Not installed\"}\n- Parse $ARGUMENTS for specific feature (chains, agents, rag, or all)\n\nPhase 2: Analysis\nGoal: Understand existing code patterns\n\nActions:\n- Check if OpenRouter client already exists:\n  !{bash find . -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" | xargs grep -l \"openrouter\" 2>/dev/null || echo \"No existing config\"}\n- Identify project structure:\n  !{bash ls -d src lib app components 2>/dev/null || echo \"Root level\"}\n- Check environment configuration:\n  !{bash test -f .env -o -f .env.local && echo \"Env file exists\" || echo \"No env file\"}\n\nPhase 3: Implementation\nGoal: Add LangChain integration with OpenRouter\n\nActions:\n\nTask(description=\"Integrate LangChain with OpenRouter\", subagent_type=\"openrouter-langchain-agent\", prompt=\"You are the openrouter-langchain-agent. Add LangChain integration with OpenRouter for $ARGUMENTS.\n\nContext from Discovery:\n- Project language detected\n- Existing LangChain status\n- Feature request: $ARGUMENTS (chains/agents/rag/all)\n\nTasks:\n1. Install dependencies based on language:\n\n   **Python:**\n   - pip install langchain langchain-openai openai\n   - Verify installation successful\n\n   **TypeScript/JavaScript:**\n   - npm install langchain @langchain/openai openai\n   - Verify installation successful\n\n2. Create OpenRouter client configuration:\n\n   **Python:**\n   - File: src/langchain_openrouter.py or lib/langchain_openrouter.py\n   - Import ChatOpenAI from langchain_openai\n   - Configure with OpenRouter base URL and API key\n   - Export configured client\n\n   **TypeScript:**\n   - File: src/lib/langchain-openrouter.ts or lib/langchain-openrouter.ts\n   - Import ChatOpenAI from @langchain/openai\n   - Configure with OpenRouter configuration\n   - Export typed client\n\n3. Create feature-specific implementations based on $ARGUMENTS:\n\n   If chains requested:\n   - Create chain example with LLMChain or LCEL\n   - Show prompt template usage\n   - Demonstrate sequential chain operations\n   - Include model switching example\n\n   If agents requested:\n   - Create agent with tools (calculator, search, etc.)\n   - Set up agent executor\n   - Show tool calling patterns\n   - Include memory/conversation history\n\n   If RAG requested:\n   - Set up vector store (Chroma, FAISS, or in-memory)\n   - Create embeddings configuration\n   - Build retrieval chain\n   - Show document loading and querying\n\n4. Update environment configuration:\n   - Add OPENROUTER_API_KEY to .env or .env.local\n   - Add OPENROUTER_BASE_URL=https://openrouter.ai/api/v1\n   - Add to .env.example for reference\n   - Ensure .gitignore includes .env files\n\n5. Create usage examples:\n   - File: examples/langchain-openrouter.py or examples/langchain-openrouter.ts\n   - Show complete working example for requested features\n   - Include multiple model usage (Claude, GPT-4, Gemini)\n   - Document available patterns and best practices\n\nWebFetch latest documentation:\n- https://openrouter.ai/docs/frameworks/langchain\n- https://python.langchain.com/docs/integrations/chat/openai\n- https://js.langchain.com/docs/integrations/chat/openai\n- https://python.langchain.com/docs/use_cases/question_answering/\n- https://python.langchain.com/docs/modules/agents/\n\nDeliverable: Working LangChain integration with OpenRouter and examples\")\n\nPhase 4: Verification\nGoal: Ensure integration works\n\nActions:\n- Verify dependencies installed:\n  !{bash pip list | grep -E \"langchain|openai\" 2>/dev/null || npm list langchain 2>/dev/null || echo \"Check installation\"}\n- Check configuration file created:\n  !{bash find . -name \"*langchain*openrouter*\" -o -name \"*openrouter*langchain*\" | head -3}\n- Verify environment setup:\n  !{bash grep -q \"OPENROUTER_API_KEY\" .env 2>/dev/null || grep -q \"OPENROUTER_API_KEY\" .env.local 2>/dev/null && echo \"‚úÖ Env configured\" || echo \"‚ö†Ô∏è Add API key to .env\"}\n\nPhase 5: Summary\nGoal: Provide usage instructions\n\nActions:\n- Display integration summary:\n  - ‚úÖ LangChain and OpenRouter packages installed\n  - ‚úÖ OpenRouter client configured for LangChain\n  - ‚úÖ Example implementations created\n  - ‚úÖ Environment variables ready\n\n- Next steps:\n  1. Add your OpenRouter API key to .env or .env.local\n  2. Test the integration with provided examples\n  3. Explore model routing: https://openrouter.ai/models\n  4. Build chains, agents, or RAG applications\n  5. Customize for your use case\n\n- Available models via OpenRouter:\n  - anthropic/claude-3.5-sonnet (recommended for chains)\n  - openai/gpt-4-turbo (great for agents)\n  - google/gemini-pro-1.5 (cost-effective for RAG)\n  - meta-llama/llama-3.1-70b-instruct (open source)\n  - 500+ more at https://openrouter.ai/models\n\n- Features enabled:\n  - Chain composition with LCEL\n  - Agent execution with tools\n  - RAG with vector stores\n  - Model routing and fallback\n  - Cost optimization\n  - Multi-provider access\n\n- Example usage patterns:\n  - Simple chain: prompt ‚Üí model ‚Üí output\n  - Agent: tools + reasoning loop\n  - RAG: documents ‚Üí embeddings ‚Üí retrieval ‚Üí generation\n  - Multi-model: route different tasks to optimal models"
              },
              {
                "name": "/add-model-routing",
                "description": "Configure intelligent model routing and cost optimization with fallback strategies",
                "path": "plugins/openrouter/commands/add-model-routing.md",
                "frontmatter": {
                  "description": "Configure intelligent model routing and cost optimization with fallback strategies",
                  "argument-hint": [
                    "routing-strategy"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the openrouter plugin:\n\n- **model-routing-patterns**: Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.\n- **openrouter-config-validator**: Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.\n- **provider-integration-templates**: OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure intelligent model routing in OpenRouter to optimize cost, performance, and reliability with automatic fallback strategies.\n\nCore Principles:\n- Detect project language and existing OpenRouter setup\n- Configure provider preferences and fallback chains\n- Set up cost-aware routing strategies\n- Enable monitoring and analytics\n\nPhase 1: Discovery\nGoal: Understand project setup and routing requirements\n\nActions:\n- Load OpenRouter documentation:\n  @plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md\n- Detect project language:\n  !{bash test -f package.json && echo \"Node.js\" || test -f requirements.txt -o -f pyproject.toml && echo \"Python\" || echo \"Unknown\"}\n- Check for existing OpenRouter configuration:\n  !{bash find . -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" | xargs grep -l \"openrouter\" 2>/dev/null | head -5}\n- Parse $ARGUMENTS for routing strategy (cost, speed, quality, balanced, custom)\n\nPhase 2: Analysis\nGoal: Understand current configuration and requirements\n\nActions:\n- Check existing API configuration:\n  !{bash grep -r \"OPENROUTER\" .env .env.local 2>/dev/null || echo \"No config found\"}\n- Identify use cases from codebase:\n  !{bash find . -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" | xargs grep -l \"ChatOpenAI\\|openai\\|anthropic\" 2>/dev/null | wc -l}\n- Check if monitoring is enabled:\n  !{bash grep -r \"X-Title\\|HTTP-Referer\" . 2>/dev/null || echo \"No monitoring\"}\n\nPhase 3: Implementation\nGoal: Configure model routing and optimization\n\nActions:\n\nTask(description=\"Configure model routing\", subagent_type=\"openrouter-routing-agent\", prompt=\"You are the openrouter-routing-agent. Configure intelligent model routing for $ARGUMENTS.\n\nContext from Discovery:\n- Project language detected\n- Existing OpenRouter setup status\n- Routing strategy: $ARGUMENTS (cost/speed/quality/balanced/custom)\n\nTasks:\n1. Create routing configuration file:\n\n   **Python:**\n   - File: src/openrouter_config.py or config/openrouter.py\n   - Define model routing strategies\n   - Set up fallback chains\n   - Configure provider preferences\n\n   **TypeScript:**\n   - File: src/config/openrouter.ts or lib/openrouter-config.ts\n   - Define typed routing configurations\n   - Set up fallback chains\n   - Configure provider preferences\n\n2. Implement routing strategies based on $ARGUMENTS:\n   - Cost: Free/low-cost models with premium fallback\n   - Speed: Fastest models with streaming\n   - Quality: Premium models with high-quality fallbacks\n   - Balanced: Task-based routing with cost/quality tradeoffs\n   - Custom: User-defined preferences and rules\n\n3. Set up fallback strategies:\n   - Primary model configuration\n   - Secondary fallback options\n   - Tertiary emergency fallbacks\n   - Error handling and retries\n   - Circuit breaker patterns\n\n4. Configure monitoring and analytics:\n   - Add X-Title header for request tracking\n   - Add HTTP-Referer for attribution\n   - Set up cost tracking\n   - Enable usage analytics\n   - Configure alerts for cost thresholds\n\n5. Update environment configuration:\n   - OPENROUTER_API_KEY (if not present)\n   - OPENROUTER_APP_NAME for tracking\n   - OPENROUTER_SITE_URL for attribution\n   - Cost limits and budgets\n   - Preferred providers list\n\n6. Create helper utilities:\n   - Model selector function\n   - Cost calculator\n   - Performance tracker\n   - Fallback handler\n   - Provider availability checker\n\n7. Generate usage examples:\n   - Basic routing usage\n   - Dynamic model selection\n   - Cost-aware prompting\n   - Fallback handling\n   - Monitoring integration\n\nWebFetch latest documentation:\n- https://openrouter.ai/docs/quick-start\n- https://openrouter.ai/docs/provider-routing\n- https://openrouter.ai/docs/models\n- https://openrouter.ai/docs/requests\n\nDeliverable: Complete routing configuration with examples and monitoring\")\n\nPhase 4: Verification\nGoal: Ensure routing configuration works\n\nActions:\n- Verify configuration file created:\n  !{bash find . -name \"*routing*\" -o -name \"*openrouter*config*\" | grep -v node_modules | head -3}\n- Check environment variables:\n  !{bash grep -E \"OPENROUTER_(API_KEY|APP_NAME|SITE_URL)\" .env .env.local 2>/dev/null || echo \"‚ö†Ô∏è Configure environment\"}\n- Test routing logic exists:\n  !{bash find . -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" | xargs grep -l \"fallback\\|routing\" 2>/dev/null | wc -l}\n\nPhase 5: Summary\nGoal: Provide configuration overview\n\nActions:\n- Display configuration summary:\n  - ‚úÖ Model routing strategy configured\n  - ‚úÖ Fallback chains established\n  - ‚úÖ Monitoring and analytics enabled\n  - ‚úÖ Cost optimization rules set\n\n- Next steps:\n  1. Add OpenRouter credentials to environment\n  2. Test routing with sample requests\n  3. Monitor costs at https://openrouter.ai/activity\n  4. Adjust routing rules based on usage\n\n- Features enabled:\n  - Provider preferences and fallback chains\n  - Auto-routing by task complexity\n  - Cost controls and budget alerts\n  - Performance tracking and analytics"
              },
              {
                "name": "/add-vercel-ai-sdk",
                "description": "Add Vercel AI SDK integration with OpenRouter provider for streaming, chat, and tool calling",
                "path": "plugins/openrouter/commands/add-vercel-ai-sdk.md",
                "frontmatter": {
                  "description": "Add Vercel AI SDK integration with OpenRouter provider for streaming, chat, and tool calling",
                  "argument-hint": [
                    "feature"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the openrouter plugin:\n\n- **model-routing-patterns**: Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.\n- **openrouter-config-validator**: Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.\n- **provider-integration-templates**: OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Vercel AI SDK with OpenRouter provider for Next.js applications, enabling streaming responses, chat interfaces, and tool calling with 500+ models.\n\nCore Principles:\n- Detect Next.js project structure and version\n- Install @openrouter/ai-sdk-provider package\n- Configure OpenRouter provider with proper types\n- Create working examples for streaming, chat, and tools\n\nPhase 1: Discovery\nGoal: Understand project setup and requirements\n\nActions:\n- Load OpenRouter documentation:\n  @plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md\n- Detect Next.js project:\n  !{bash test -f next.config.js -o -f next.config.ts && echo \"Next.js found\" || echo \"No Next.js config\"}\n- Check package.json for existing AI SDK:\n  @package.json\n- Parse $ARGUMENTS for specific feature (streaming, chat, tools, or all)\n\nPhase 2: Analysis\nGoal: Understand existing code patterns\n\nActions:\n- Check if Vercel AI SDK already installed:\n  !{bash npm list ai 2>/dev/null | grep \"ai@\" || echo \"Not installed\"}\n- Check for existing OpenRouter config:\n  !{bash find . -name \"*.ts\" -o -name \"*.tsx\" | xargs grep -l \"openrouter\" 2>/dev/null || echo \"No existing config\"}\n- Identify app structure (pages vs app router):\n  !{bash test -d app && echo \"App Router\" || test -d pages && echo \"Pages Router\" || echo \"Unknown\"}\n\nPhase 3: Implementation\nGoal: Add Vercel AI SDK with OpenRouter integration\n\nActions:\n\nTask(description=\"Integrate Vercel AI SDK with OpenRouter\", subagent_type=\"openrouter-vercel-integration-agent\", prompt=\"You are the openrouter-vercel-integration-agent. Add Vercel AI SDK integration with OpenRouter provider for $ARGUMENTS.\n\nContext from Discovery:\n- Next.js project structure detected\n- Existing AI SDK status\n- App Router vs Pages Router\n- Feature request: $ARGUMENTS (streaming/chat/tools/all)\n\nTasks:\n1. Install dependencies:\n   - npm install ai @openrouter/ai-sdk-provider\n   - Verify installation successful\n\n2. Create OpenRouter provider configuration:\n   - File: src/lib/openrouter.ts or lib/openrouter.ts\n   - Import openrouter from '@openrouter/ai-sdk-provider'\n   - Configure with API key from environment\n   - Export typed client for app usage\n\n3. Create feature-specific implementations based on $ARGUMENTS:\n\n   If streaming requested:\n   - Create API route: app/api/chat/route.ts (App Router) or pages/api/chat.ts (Pages Router)\n   - Use streamText() from 'ai' package\n   - Configure OpenRouter model (e.g., 'anthropic/claude-3.5-sonnet')\n   - Return proper streaming response\n\n   If chat requested:\n   - Create chat component: components/Chat.tsx\n   - Use useChat() hook from 'ai/react'\n   - Connect to API route\n   - Add message display and input\n\n   If tools requested:\n   - Create tool definition with Zod schema\n   - Example: weather tool or calculator\n   - Configure in streamText() call\n   - Handle tool results in response\n\n4. Update .env.local:\n   - Add OPENROUTER_API_KEY if not present\n   - Add to .env.example for reference\n   - Ensure .gitignore includes .env.local\n\n5. Create usage example:\n   - File: examples/openrouter-chat.tsx or README section\n   - Show complete working example\n   - Include model switching example\n   - Document available features\n\nWebFetch latest documentation:\n- https://openrouter.ai/docs/community/vercel-ai-sdk\n- https://sdk.vercel.ai/docs/introduction\n- https://sdk.vercel.ai/docs/ai-sdk-core/streaming\n- https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot\n\nDeliverable: Working Vercel AI SDK integration with OpenRouter provider and examples\")\n\nPhase 4: Verification\nGoal: Ensure integration works\n\nActions:\n- Verify dependencies installed:\n  !{bash npm list @openrouter/ai-sdk-provider 2>/dev/null && echo \"‚úÖ Provider installed\" || echo \"‚ùå Installation failed\"}\n- Check configuration file created:\n  !{bash test -f src/lib/openrouter.ts -o -f lib/openrouter.ts && echo \"‚úÖ Config created\" || echo \"‚ùå Config missing\"}\n- Verify API route exists:\n  !{bash find . -path \"*/api/chat/route.ts\" -o -path \"*/api/chat.ts\" | head -1}\n- Check environment setup:\n  !{bash grep -q \"OPENROUTER_API_KEY\" .env.local 2>/dev/null && echo \"‚úÖ Env configured\" || echo \"‚ö†Ô∏è Add API key to .env.local\"}\n\nPhase 5: Summary\nGoal: Provide usage instructions\n\nActions:\n- Display integration summary:\n  - ‚úÖ @openrouter/ai-sdk-provider installed\n  - ‚úÖ OpenRouter provider configured\n  - ‚úÖ API routes created\n  - ‚úÖ Example components ready\n\n- Next steps:\n  1. Add your OpenRouter API key to .env.local\n  2. Start dev server: npm run dev\n  3. Test chat interface at /chat (if created)\n  4. Explore model routing: https://openrouter.ai/models\n  5. Customize for your use case\n\n- Available models via OpenRouter:\n  - anthropic/claude-3.5-sonnet (recommended)\n  - openai/gpt-4-turbo\n  - google/gemini-pro-1.5\n  - meta-llama/llama-3.1-70b-instruct\n  - 500+ more at https://openrouter.ai/models\n\n- Features enabled:\n  - Streaming text responses\n  - Chat UI with useChat hook\n  - Tool calling (if requested)\n  - Model switching\n  - Cost optimization via routing"
              },
              {
                "name": "/configure",
                "description": null,
                "path": "plugins/openrouter/commands/configure.md",
                "frontmatter": null,
                "content": "---\ndescription: Configure OpenRouter settings, API keys, and preferences\nargument-hint: [setting-name] [value]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the openrouter plugin:\n\n- **model-routing-patterns**: Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.\n- **openrouter-config-validator**: Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.\n- **provider-integration-templates**: OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Manage OpenRouter configuration including API keys, preferences, monitoring settings, and environment variables.\n\nCore Principles:\n- Detect existing configuration files\n- Validate settings before applying\n- Secure API key storage\n- Provide clear feedback\n\nPhase 1: Discovery\nGoal: Understand current configuration state\n\nActions:\n- Load OpenRouter documentation:\n  @plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md\n- Parse $ARGUMENTS for configuration action (set, get, list, reset)\n- Detect environment files:\n  !{bash ls -la .env .env.local .env.example 2>/dev/null || echo \"No env files\"}\n- Check existing OpenRouter configuration:\n  !{bash grep -r \"OPENROUTER\" .env .env.local 2>/dev/null || echo \"No config\"}\n- Identify project type:\n  !{bash test -f package.json && echo \"Node.js\" || test -f requirements.txt && echo \"Python\" || echo \"Unknown\"}\n\nPhase 2: Configuration Action\nGoal: Execute requested configuration change\n\nActions:\n\n**If no arguments provided (interactive mode):**\n- Use AskUserQuestion to gather:\n  - What to configure? (API key, app name, site URL, preferences)\n  - Current values if updating existing settings\n  - Confirmation before making changes\n\n**Setting API Key:**\n- Add or update OPENROUTER_API_KEY in .env or .env.local\n- Verify key format (sk-or-v1-...)\n- Test key validity with API call if requested\n- Ensure .env is in .gitignore\n\n**Setting App Name:**\n- Add or update OPENROUTER_APP_NAME for request tracking\n- Used in X-Title header for monitoring\n- Helps identify requests in OpenRouter dashboard\n\n**Setting Site URL:**\n- Add or update OPENROUTER_SITE_URL for attribution\n- Used in HTTP-Referer header\n- Important for analytics and credits\n\n**Setting Base URL (advanced):**\n- Default: https://openrouter.ai/api/v1\n- Custom endpoints for enterprise/proxy setups\n\n**Getting Configuration:**\n- Display current settings (mask API key)\n- Show which env file is being used\n- List all OpenRouter-related variables\n\n**Listing Available Settings:**\n- OPENROUTER_API_KEY (required)\n- OPENROUTER_APP_NAME (recommended for tracking)\n- OPENROUTER_SITE_URL (recommended for attribution)\n- OPENROUTER_BASE_URL (optional, default provided)\n- Model preferences and routing config\n\n**Resetting Configuration:**\n- Remove OpenRouter variables from env files\n- Optionally backup current configuration\n- Confirm before deletion\n\nPhase 3: Validation\nGoal: Verify configuration is correct\n\nActions:\n- Check environment file syntax:\n  !{bash grep \"^OPENROUTER\" .env .env.local 2>/dev/null | grep -v \"^#\"}\n- Validate API key format (if set):\n  !{bash grep \"OPENROUTER_API_KEY\" .env .env.local 2>/dev/null | grep -o \"sk-or-v1-\"}\n- Ensure .env is in .gitignore:\n  !{bash grep -q \"^\\.env$\" .gitignore 2>/dev/null && echo \"‚úÖ Protected\" || echo \"‚ö†Ô∏è Add .env to .gitignore\"}\n- Create .env.example if missing:\n  !{bash test -f .env.example || echo \"OPENROUTER_API_KEY=your-api-key-here\" > .env.example}\n\nPhase 4: Summary\nGoal: Report configuration status\n\nActions:\n- Display configuration summary:\n  - ‚úÖ API key configured (or ‚ö†Ô∏è Missing)\n  - ‚úÖ App name set (or ‚ö†Ô∏è Recommended)\n  - ‚úÖ Site URL set (or ‚ö†Ô∏è Recommended)\n  - ‚úÖ Environment secured (.gitignore)\n\n- Configuration file location:\n  - Using: .env or .env.local\n  - Example: .env.example created\n\n- Next steps:\n  1. Get API key at: https://openrouter.ai/keys\n  2. Test configuration with sample request\n  3. Set up monitoring with app name and site URL\n  4. Configure model routing preferences\n  5. Review usage at: https://openrouter.ai/activity\n\n- Security reminders:\n  - Never commit .env files to git\n  - Keep API keys secure and rotate regularly\n  - Use read-only keys for client-side apps\n  - Monitor usage for unexpected activity\n\n- Available commands:\n  - /openrouter:configure set api-key <key>\n  - /openrouter:configure set app-name <name>\n  - /openrouter:configure set site-url <url>\n  - /openrouter:configure get\n  - /openrouter:configure list\n  - /openrouter:configure reset\n"
              },
              {
                "name": "/init",
                "description": "Initialize OpenRouter SDK with API key configuration, model selection, and framework integration setup",
                "path": "plugins/openrouter/commands/init.md",
                "frontmatter": {
                  "description": "Initialize OpenRouter SDK with API key configuration, model selection, and framework integration setup",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the openrouter plugin:\n\n- **model-routing-patterns**: Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.\n- **openrouter-config-validator**: Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.\n- **provider-integration-templates**: OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up OpenRouter SDK in a new or existing project with intelligent framework detection, API key configuration, and provider/model selection.\n\nCore Principles:\n- Detect existing frameworks (Next.js, Python, TypeScript) - never assume\n- Ask user for preferences when multiple options exist\n- Create proper environment configuration with secure API key storage\n- Provide framework-specific integration examples\n\nPhase 1: Discovery\nGoal: Understand project structure and user requirements\n\nActions:\n- Load OpenRouter documentation for reference:\n  @plugins/openrouter/docs/OpenRouter_Documentation_Analysis.md\n- Parse $ARGUMENTS for project path (default to current directory if not provided)\n- Detect project type and framework:\n  !{bash test -f package.json && echo \"Node.js project\" || echo \"No package.json\"}\n  !{bash test -f requirements.txt && echo \"Python project\" || echo \"No requirements.txt\"}\n  !{bash test -f pyproject.toml && echo \"Python project (Poetry)\" || echo \"No pyproject.toml\"}\n- Check if OpenRouter is already configured:\n  !{bash grep -q \"OPENROUTER_API_KEY\" .env 2>/dev/null && echo \"Found existing config\" || echo \"No config found\"}\n\nPhase 2: Gather Requirements\nGoal: Ask user for configuration preferences\n\nActions:\n- Use AskUserQuestion to gather:\n  1. Which language/framework integration?\n     - TypeScript (Vercel AI SDK, OpenAI SDK)\n     - Python (OpenAI SDK, LangChain, PydanticAI)\n     - JavaScript (OpenAI SDK)\n  2. Primary use case?\n     - Chat/streaming applications\n     - Model routing and cost optimization\n     - Framework integration (which one?)\n     - MCP server development\n  3. Do you have an OpenRouter API key?\n     - Yes (I'll provide it)\n     - No (guide me to get one)\n  4. Which models do you plan to use?\n     - OpenAI models (GPT-4, GPT-3.5)\n     - Anthropic models (Claude 3.5 Sonnet, etc.)\n     - Google models (Gemini)\n     - All/Multiple providers (intelligent routing)\n\nPhase 3: Implementation\nGoal: Configure OpenRouter SDK with proper setup\n\nActions:\n\nTask(description=\"Setup OpenRouter SDK\", subagent_type=\"openrouter-setup-agent\", prompt=\"You are the openrouter-setup-agent. Initialize OpenRouter SDK for $ARGUMENTS.\n\nContext from Phase 1:\n- Project type detected (Node.js/Python/etc.)\n- Existing configuration status\n- OpenRouter documentation loaded\n\nUser Requirements from Phase 2:\n- Language/framework choice\n- Primary use case\n- API key availability\n- Model preferences\n\nTasks:\n1. Install appropriate dependencies:\n   - TypeScript: npm install openai @openrouter/ai-sdk-provider (if using Vercel AI SDK)\n   - Python: pip install openai (for OpenAI SDK compatibility)\n   - Additional: langchain, pydantic-ai if selected\n\n2. Create/update .env file with:\n   - OPENROUTER_API_KEY=your-api-key-here\n   - OPENROUTER_BASE_URL=https://openrouter.ai/api/v1\n   - Add to .gitignore if not already there\n\n3. Create framework-specific setup file:\n   - TypeScript: src/lib/openrouter.ts with client configuration\n   - Python: src/openrouter_client.py with client setup\n   - Include examples for chosen use case\n\n4. Generate example usage file:\n   - Chat streaming example if selected\n   - Model routing example if selected\n   - Framework integration example (Vercel AI SDK, LangChain, etc.)\n\n5. Create README section with:\n   - Getting API key instructions (https://openrouter.ai/keys)\n   - Configuration guide\n   - Example usage\n   - Links to OpenRouter docs\n\nWebFetch URLs for latest documentation:\n- https://openrouter.ai/docs/quickstart\n- https://openrouter.ai/docs/api-reference/overview\n- Framework-specific docs based on user selection\n\nDeliverable: Fully configured OpenRouter SDK with working examples and documentation\")\n\nPhase 4: Verification\nGoal: Ensure setup is working\n\nActions:\n- Check that .env file was created:\n  !{bash test -f .env && echo \"‚úÖ .env created\" || echo \"‚ùå .env missing\"}\n- Verify dependencies installed:\n  !{bash test -f package.json && npm list openai 2>/dev/null || echo \"TypeScript check skipped\"}\n  !{bash test -f requirements.txt && pip list | grep openai || echo \"Python check skipped\"}\n- Check if .gitignore includes .env:\n  !{bash grep -q \"^\\.env$\" .gitignore 2>/dev/null && echo \"‚úÖ .env in .gitignore\" || echo \"‚ö†Ô∏è Add .env to .gitignore\"}\n\nPhase 5: Summary\nGoal: Provide user with next steps\n\nActions:\n- Display setup summary:\n  - ‚úÖ Dependencies installed\n  - ‚úÖ Environment configured\n  - ‚úÖ Example files created\n  - ‚úÖ Documentation added\n\n- Next steps:\n  1. Add your OpenRouter API key to .env file\n  2. Get API key at: https://openrouter.ai/keys\n  3. Test the setup with provided examples\n  4. Explore model routing and cost optimization features\n  5. Run example: [provide specific command based on framework]\n\n- Useful resources:\n  - OpenRouter Documentation: https://openrouter.ai/docs\n  - Model Browser: https://openrouter.ai/models\n  - Request Builder: https://openrouter.ai/request-builder\n  - Pricing: https://openrouter.ai/pricing"
              }
            ],
            "skills": [
              {
                "name": "model-routing-patterns",
                "description": "Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.",
                "path": "plugins/openrouter/skills/model-routing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "model-routing-patterns",
                  "description": "Model routing configuration templates and strategies for cost optimization, speed optimization, quality optimization, and intelligent fallback chains. Use when building AI applications with OpenRouter, implementing model routing strategies, optimizing API costs, setting up fallback chains, implementing quality-based routing, or when user mentions model routing, cost optimization, fallback strategies, model selection, intelligent routing, or dynamic model switching.",
                  "allowed-tools": "Read, Write, Bash, Edit, Glob"
                },
                "content": "# Model Routing Patterns\n\nProduction-ready model routing configurations and strategies for OpenRouter that optimize for cost, speed, quality, or balanced performance with intelligent fallback chains.\n\n## Purpose\n\nThis skill provides comprehensive templates, scripts, and strategies for implementing sophisticated model routing in OpenRouter-powered applications. It helps you:\n- Reduce API costs by routing to cheaper models when appropriate\n- Optimize for speed with fast models and streaming\n- Maintain quality with premium model fallbacks\n- Implement intelligent task-based routing\n- Build reliable multi-tier fallback chains\n\n## Activation Triggers\n\nUse this skill when:\n- Designing model routing strategies\n- Implementing cost optimization\n- Setting up fallback chains for reliability\n- Building task complexity-based routing\n- Configuring dynamic model selection\n- Optimizing API performance vs cost tradeoffs\n- Implementing A/B testing for models\n- Setting up monitoring and analytics\n\n## Available Routing Strategies\n\n### 1. Cost-Optimized Routing\n**Goal:** Minimize API costs while maintaining acceptable quality\n\n**Strategy:**\n- Use free models (google/gemma-2-9b-it:free, meta-llama/llama-3.2-3b-instruct:free)\n- Fallback to budget models (anthropic/claude-4.5-sonnet, openai/gpt-4o-mini)\n- Premium models only for complex tasks requiring highest quality\n\n**Template:** `templates/cost-optimized-routing.json`\n\n**Best for:**\n- High-volume applications\n- Simple tasks (classification, extraction, formatting)\n- Development/testing environments\n- Budget-constrained projects\n\n### 2. Speed-Optimized Routing\n**Goal:** Minimize latency and response time\n\n**Strategy:**\n- Prioritize fastest models regardless of cost\n- Enable streaming for immediate feedback\n- Use smaller models with quick inference\n- Geographic routing to nearest endpoints\n\n**Template:** `templates/speed-optimized-routing.json`\n\n**Best for:**\n- Real-time chat applications\n- Interactive user experiences\n- Low-latency requirements\n- Streaming responses\n\n### 3. Quality-Optimized Routing\n**Goal:** Maximize output quality with premium models\n\n**Strategy:**\n- Use top-tier models (gpt-4o, claude-4.5-sonnet, gemini-pro)\n- Fallback to other premium models for availability\n- Multi-model voting for critical tasks\n- Quality verification layers\n\n**Template:** `templates/quality-optimized-routing.json`\n\n**Best for:**\n- Critical business decisions\n- Content creation\n- Complex reasoning tasks\n- Customer-facing applications\n\n### 4. Balanced Routing\n**Goal:** Dynamically route based on task complexity\n\n**Strategy:**\n- Analyze request complexity\n- Route simple tasks to cheap models\n- Route complex tasks to premium models\n- Adaptive based on success metrics\n\n**Template:** `templates/balanced-routing.json`\n\n**Best for:**\n- Mixed workloads\n- Production applications\n- General-purpose AI services\n- Optimizing cost/quality tradeoff\n\n### 5. Custom Routing\n**Goal:** Implement domain-specific routing logic\n\n**Template:** `templates/custom-routing-template.json`\n\n**Customizable factors:**\n- User tier/subscription level\n- Geographic location\n- Time of day pricing\n- Model availability\n- Rate limit status\n- Historical success rates\n\n## Key Resources\n\n### Scripts\n\n**validate-routing-config.sh**\n- Validates routing configuration syntax\n- Checks model availability on OpenRouter\n- Verifies fallback chain logic\n- Ensures no circular dependencies\n- Validates model IDs and parameters\n\n**test-fallback-chain.sh**\n- Tests fallback chain execution\n- Simulates model failures\n- Verifies graceful degradation\n- Measures latency through chain\n- Validates error handling\n\n**generate-routing-config.sh**\n- Generates routing config from strategy type\n- Interactive configuration builder\n- Validates and optimizes settings\n- Exports to JSON/TypeScript/Python formats\n\n**analyze-cost-savings.sh**\n- Analyzes potential cost savings from routing\n- Compares routing strategies\n- Projects monthly costs\n- Generates cost reports\n- Identifies optimization opportunities\n\n### Templates\n\n**Configuration Templates (JSON):**\n- `cost-optimized-routing.json` - Free/cheap models with premium fallback\n- `speed-optimized-routing.json` - Fastest models with streaming\n- `quality-optimized-routing.json` - Premium models with fallbacks\n- `balanced-routing.json` - Task-based dynamic routing\n- `custom-routing-template.json` - Template for custom strategies\n\n**Code Templates:**\n- `routing-config.ts` - TypeScript routing configuration\n- `routing-config.py` - Python routing configuration\n\n### Examples\n\n- `cost-routing-example.md` - Complete cost-optimized routing setup\n- `dynamic-routing-example.md` - Task complexity-based routing\n- `fallback-chain-example.md` - 3-tier fallback strategy\n- `monitoring-example.md` - Cost tracking and analytics setup\n\n## Workflow\n\n### 1. Identify Requirements\n\nDetermine your optimization goals:\n```bash\n# Interactive strategy selector\n./scripts/generate-routing-config.sh\n```\n\nAnswer questions about:\n- Primary goal (cost/speed/quality/balanced)\n- Budget constraints\n- Latency requirements\n- Quality thresholds\n- Supported model providers\n\n### 2. Generate Configuration\n\n```bash\n# Generate from strategy type\n./scripts/generate-routing-config.sh cost-optimized > config.json\n\n# Or copy template\ncp templates/cost-optimized-routing.json config.json\n```\n\n### 3. Validate Configuration\n\n```bash\n# Validate syntax and model availability\n./scripts/validate-routing-config.sh config.json\n```\n\nChecks:\n- JSON syntax\n- Model IDs exist on OpenRouter\n- Fallback chain is valid\n- No circular references\n- Required fields present\n\n### 4. Test Fallback Chain\n\n```bash\n# Test fallback behavior\n./scripts/test-fallback-chain.sh config.json\n```\n\nSimulates failures to ensure graceful degradation.\n\n### 5. Analyze Cost Impact\n\n```bash\n# Compare routing strategies\n./scripts/analyze-cost-savings.sh config.json baseline-config.json\n```\n\nShows projected savings and performance tradeoffs.\n\n### 6. Deploy and Monitor\n\n- Deploy configuration to production\n- Monitor using examples/monitoring-example.md\n- Track metrics: cost, latency, success rate, quality\n- Iterate based on real-world performance\n\n## Common Routing Patterns\n\n### Pattern 1: Simple Fallback Chain\n```json\n{\n  \"primary\": \"meta-llama/llama-3.2-3b-instruct:free\",\n  \"fallback\": [\n    \"anthropic/claude-4.5-sonnet\",\n    \"openai/gpt-4o-mini\"\n  ]\n}\n```\n\n### Pattern 2: Task Complexity Routing\n```json\n{\n  \"simple_tasks\": {\n    \"models\": [\"google/gemma-2-9b-it:free\"]\n  },\n  \"medium_tasks\": {\n    \"models\": [\"anthropic/claude-4.5-sonnet\"]\n  },\n  \"complex_tasks\": {\n    \"models\": [\"openai/gpt-4o\"]\n  }\n}\n```\n\n### Pattern 3: Time-Based Routing\n```json\n{\n  \"peak_hours\": {\n    \"models\": [\"openai/gpt-4o-mini\"],\n    \"max_latency_ms\": 1000\n  },\n  \"off_peak\": {\n    \"models\": [\"google/gemini-pro\"],\n    \"max_latency_ms\": 3000\n  }\n}\n```\n\n### Pattern 4: User Tier Routing\n```json\n{\n  \"free_tier\": {\n    \"models\": [\"meta-llama/llama-3.2-3b-instruct:free\"],\n    \"rate_limit\": 10\n  },\n  \"premium_tier\": {\n    \"models\": [\"anthropic/claude-4.5-sonnet\"],\n    \"rate_limit\": 1000\n  }\n}\n```\n\n## Model Categories for Routing\n\n### Free Models (Cost: $0)\n- `google/gemma-2-9b-it:free`\n- `meta-llama/llama-3.2-3b-instruct:free`\n- `meta-llama/llama-3.2-1b-instruct:free`\n- `microsoft/phi-3-mini-128k-instruct:free`\n\n**Use for:** High-volume, simple tasks, development\n\n### Budget Models (Cost: $0.10-0.50/1M tokens)\n- `openai/gpt-4o-mini`\n- `google/gemini-flash-1.5`\n\n**Use for:** Production workloads, balanced cost/quality\n\n### Premium Models (Cost: $3-15/1M tokens)\n- `anthropic/claude-4.5-sonnet`\n- `openai/gpt-4o`\n- `google/gemini-pro-1.5`\n\n**Use for:** Complex reasoning, critical tasks, high quality\n\n### Specialized Models\n- **Vision:** `openai/gpt-4-vision-preview`\n- **Code:** `anthropic/claude-4.5-sonnet` (code-specific)\n- **Long Context:** `google/gemini-pro-1.5` (1M+ tokens)\n\n## Best Practices\n\n1. **Always implement fallback chains** - Single points of failure cause downtime\n2. **Monitor actual costs** - Theoretical savings may differ from real usage\n3. **Test quality degradation** - Ensure cheaper models meet quality thresholds\n4. **Set timeout limits** - Prevent slow models from blocking requests\n5. **Track model availability** - Some models have rate limits or downtime\n6. **Use task classification** - Route based on complexity, not one-size-fits-all\n7. **Implement retry logic** - Handle transient failures gracefully\n8. **Cache responses** - Reduce API calls for repeated queries\n9. **A/B test routing strategies** - Validate improvements with data\n10. **Budget alerting** - Get notified before exceeding cost limits\n\n## Troubleshooting\n\n**All models in fallback chain failing:**\n- Check OpenRouter status page\n- Verify API key has credits\n- Test with a known-working model\n- Review rate limit status\n\n**Higher costs than expected:**\n- Analyze actual request distribution\n- Check if complex tasks are routed to premium models\n- Verify caching is working\n- Review retry/fallback patterns\n\n**Quality degradation:**\n- Review which model is actually being used\n- Test free models against quality benchmarks\n- Consider upgrading routing strategy\n- Implement quality verification layer\n\n**High latency:**\n- Check if using slow models\n- Enable streaming for faster perceived response\n- Use geographic routing\n- Implement parallel model requests with first-response-wins\n\n## Integration Examples\n\nSee examples directory for complete implementations:\n- Cost-optimized chat application\n- Dynamic routing based on conversation context\n- Multi-tier fallback with monitoring\n- Real-time cost tracking dashboard\n\n---\n\n**Skill Location:** `plugins/openrouter/skills/model-routing-patterns/`\n**Version:** 1.0.0\n**Supported Frameworks:** Node.js, Python, TypeScript, any OpenRouter-compatible client"
              },
              {
                "name": "openrouter-config-validator",
                "description": "Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.",
                "path": "plugins/openrouter/skills/openrouter-config-validator/SKILL.md",
                "frontmatter": {
                  "name": "openrouter-config-validator",
                  "description": "Configuration validation and testing utilities for OpenRouter API. Use when validating API keys, testing model availability, checking routing configuration, troubleshooting connection issues, analyzing usage costs, or when user mentions OpenRouter validation, config testing, API troubleshooting, model availability, or cost analysis.",
                  "allowed-tools": "Bash, Read, Write, Grep, Glob"
                },
                "content": "# OpenRouter Config Validator\n\nComprehensive validation and testing utilities for OpenRouter API configuration, model availability, routing setup, and cost monitoring.\n\n## What This Skill Provides\n\n1. **API Key Validation**: Format checking and connectivity testing\n2. **Model Availability Checking**: Verify requested models are accessible\n3. **Routing Configuration Testing**: Validate model routing and fallback chains\n4. **Environment Validation**: Check .env file completeness and correctness\n5. **Fallback Testing**: Test fallback chain execution and behavior\n6. **Provider Status Checking**: Monitor provider availability and health\n7. **Usage Analysis**: Track API usage patterns and cost optimization\n8. **Troubleshooting**: Comprehensive diagnostic tools for common issues\n\n## Instructions\n\n### Phase 1: Identify Validation Need\n\nDetermine what needs validation:\n- API key format and connectivity\n- Model availability and access\n- Routing configuration correctness\n- Environment variable completeness\n- Fallback chain behavior\n- Provider health status\n- Usage patterns and costs\n- General troubleshooting\n\n### Phase 2: Run Appropriate Validation Script\n\nExecute the relevant validation script from `scripts/` directory:\n\n**API Key Validation:**\n```bash\nbash scripts/validate-api-key.sh <api-key>\n```\n\n**Model Availability Check:**\n```bash\nbash scripts/check-model-availability.sh <model-id>\n```\n\n**Routing Configuration Test:**\n```bash\nbash scripts/test-routing.sh <config-file>\n```\n\n**Environment Validation:**\n```bash\nbash scripts/validate-env-config.sh <env-file>\n```\n\n**Fallback Chain Testing:**\n```bash\nbash scripts/test-fallback.sh <fallback-config>\n```\n\n**Provider Status Check:**\n```bash\nbash scripts/check-provider-status.sh <provider-name>\n```\n\n**Usage Analysis:**\n```bash\nbash scripts/analyze-usage.sh <date-range>\n```\n\n**Comprehensive Troubleshooting:**\n```bash\nbash scripts/troubleshoot.sh\n```\n\n### Phase 3: Load Appropriate Template\n\nUse templates from `templates/` directory for configuration:\n\n**Environment Setup:**\n- `templates/.env.template` - Complete configuration template\n- `templates/.env.example` - Minimal configuration example\n\n**Monitoring Configuration:**\n- `templates/monitoring-config.json` - X-Title and HTTP-Referer setup\n- `templates/budget-alerts.json` - Cost alert configuration\n\n**Provider Preferences:**\n- `templates/provider-preferences.json` - Provider routing preferences\n\n### Phase 4: Reference Troubleshooting Examples\n\nCheck `examples/` directory for common issue resolutions:\n\n- `examples/api-key-troubleshooting.md` - API key issues\n- `examples/model-not-found.md` - Model availability problems\n- `examples/rate-limiting.md` - Rate limit handling\n- `examples/fallback-issues.md` - Fallback chain debugging\n- `examples/cost-optimization.md` - Cost reduction strategies\n- `examples/provider-errors.md` - Provider-specific errors\n\n### Phase 5: Report Results\n\nSummarize validation results:\n- What was tested\n- Issues found (if any)\n- Recommended fixes\n- Next steps\n\n## When Agents Should Use This Skill\n\n**Automatic triggers:**\n- Setting up new OpenRouter integration\n- Debugging API connection failures\n- Validating configuration before deployment\n- Troubleshooting model access issues\n- Analyzing unexpected API costs\n- Testing fallback chain behavior\n- Monitoring provider health\n\n**User-requested scenarios:**\n- \"Validate my OpenRouter configuration\"\n- \"Test if model X is available\"\n- \"Why is my fallback chain not working?\"\n- \"Check my API usage costs\"\n- \"Troubleshoot OpenRouter connection\"\n- \"Verify my routing configuration\"\n\n## Validation Capabilities\n\n### API Key Validation\n- Format checking (sk-or-v1-* pattern)\n- Connectivity testing via /auth/key endpoint\n- Permission verification\n- Credit balance checking\n\n### Model Availability\n- Model ID existence verification\n- Provider availability checking\n- Access permission validation\n- Alternative model suggestions\n\n### Configuration Testing\n- .env file completeness\n- Required variable presence\n- Format validation\n- Value sanity checks\n\n### Routing Validation\n- Model routing configuration\n- Fallback chain structure\n- Provider preference validation\n- Cost optimization checks\n\n### Usage Analysis\n- Request volume tracking\n- Cost breakdowns by model\n- Provider distribution\n- Rate limit monitoring\n- Budget alert configuration\n\n## Script Reference\n\nAll scripts located in `skills/openrouter-config-validator/scripts/`:\n\n1. `validate-api-key.sh` - API key format and connectivity\n2. `check-model-availability.sh` - Model access verification\n3. `test-routing.sh` - Routing configuration testing\n4. `validate-env-config.sh` - Environment validation\n5. `test-fallback.sh` - Fallback chain testing\n6. `check-provider-status.sh` - Provider health monitoring\n7. `analyze-usage.sh` - Usage pattern analysis\n8. `troubleshoot.sh` - Comprehensive diagnostics\n\n## Template Reference\n\nAll templates located in `skills/openrouter-config-validator/templates/`:\n\n1. `.env.template` - Complete environment configuration\n2. `.env.example` - Minimal configuration example\n3. `monitoring-config.json` - Request monitoring setup\n4. `budget-alerts.json` - Cost alert configuration\n5. `provider-preferences.json` - Provider routing preferences\n\n## Example Reference\n\nAll examples located in `skills/openrouter-config-validator/examples/`:\n\n1. `api-key-troubleshooting.md` - Common API key issues\n2. `model-not-found.md` - Model availability solutions\n3. `rate-limiting.md` - Rate limit handling strategies\n4. `fallback-issues.md` - Fallback debugging guide\n5. `cost-optimization.md` - Cost reduction techniques\n6. `provider-errors.md` - Provider-specific error handling\n\n## Requirements\n\n- bash 4.0+\n- curl (for API testing)\n- jq (for JSON parsing)\n- OpenRouter API key (for live testing)\n- Internet connectivity (for API calls)\n\n---\n\n**Skill Location**: plugins/openrouter/skills/openrouter-config-validator/\n**Version**: 1.0.0"
              },
              {
                "name": "provider-integration-templates",
                "description": "OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.",
                "path": "plugins/openrouter/skills/provider-integration-templates/SKILL.md",
                "frontmatter": {
                  "name": "provider-integration-templates",
                  "description": "OpenRouter framework integration templates for Vercel AI SDK, LangChain, and OpenAI SDK. Use when integrating OpenRouter with frameworks, setting up AI providers, building chat applications, implementing streaming responses, or when user mentions Vercel AI SDK, LangChain, OpenAI SDK, framework integration, or provider setup.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep"
                },
                "content": "# Provider Integration Templates\n\nThis skill provides complete integration templates, setup scripts, and working examples for integrating OpenRouter with popular AI frameworks: Vercel AI SDK, LangChain, and OpenAI SDK.\n\n## What This Skill Provides\n\n1. **Setup Scripts**: Automated installation and configuration for each framework\n2. **Integration Templates**: Drop-in code templates for provider configuration\n3. **Working Examples**: Complete implementation examples with best practices\n4. **Validation Tools**: Scripts to verify integrations are working correctly\n\n## Supported Frameworks\n\n### Vercel AI SDK (TypeScript)\n- Provider configuration with `createOpenAI()`\n- API route templates with `streamText()` and `generateText()`\n- Chat UI components with `useChat()` hook\n- Tool calling with Zod schemas\n- Streaming responses\n\n### LangChain (Python & TypeScript)\n- ChatOpenAI configuration for OpenRouter\n- LCEL chain templates\n- Agent templates with tool support\n- RAG (Retrieval Augmented Generation) implementations\n- Memory and context management\n\n### OpenAI SDK (Python & TypeScript)\n- Drop-in replacement configuration\n- Chat completions with streaming\n- Function calling support\n- Embeddings integration\n\n## Available Templates\n\n### Vercel AI SDK Templates\n- `templates/vercel-ai-sdk-config.ts` - OpenRouter provider setup\n- `templates/vercel-api-route.ts` - API route with streaming\n- `templates/vercel-chat-component.tsx` - Chat UI component\n- `templates/vercel-tools-config.ts` - Tool calling setup\n\n### LangChain Templates\n- `templates/langchain-config.py` - Python ChatOpenAI setup\n- `templates/langchain-config.ts` - TypeScript ChatOpenAI setup\n- `templates/langchain-chain.py` - LCEL chain template\n- `templates/langchain-agent.py` - Agent with tools\n- `templates/langchain-rag.py` - RAG implementation\n\n### OpenAI SDK Templates\n- `templates/openai-sdk-config.ts` - TypeScript configuration\n- `templates/openai-sdk-config.py` - Python configuration\n- `templates/openai-streaming.ts` - Streaming example\n- `templates/openai-functions.ts` - Function calling\n\n## Setup Scripts\n\n### Installation Scripts\n```bash\n# Vercel AI SDK setup\nbash scripts/setup-vercel-integration.sh\n\n# LangChain setup (Python)\nbash scripts/setup-langchain-integration.sh --python\n\n# LangChain setup (TypeScript)\nbash scripts/setup-langchain-integration.sh --typescript\n```\n\n### Validation Scripts\n```bash\n# Validate integration is working\nbash scripts/validate-integration.sh --framework vercel\n\n# Test streaming functionality\nbash scripts/test-streaming.sh --provider openrouter\n\n# Check version compatibility\nbash scripts/check-compatibility.sh\n```\n\n## How to Use This Skill\n\n### 1. Setup Framework Integration\n\n**Read the setup script** for your target framework:\n```\nRead: skills/provider-integration-templates/scripts/setup-vercel-integration.sh\n```\n\n**Execute the setup script** to install dependencies:\n```bash\nbash skills/provider-integration-templates/scripts/setup-vercel-integration.sh\n```\n\n### 2. Use Integration Templates\n\n**Read the template** you need:\n```\nRead: skills/provider-integration-templates/templates/vercel-ai-sdk-config.ts\n```\n\n**Copy template to project**:\n```bash\ncp skills/provider-integration-templates/templates/vercel-ai-sdk-config.ts src/lib/ai.ts\n```\n\n**Customize with project-specific values**:\n- Replace `YOUR_OPENROUTER_API_KEY` with actual key or env var\n- Update model selection\n- Configure streaming options\n\n### 3. Review Working Examples\n\n**Read complete examples**:\n```\nRead: skills/provider-integration-templates/examples/vercel-streaming-example.md\n```\n\nExamples show:\n- Complete file structure\n- Environment variable setup\n- API route implementation\n- Frontend component integration\n- Error handling patterns\n\n### 4. Validate Integration\n\n**Run validation script**:\n```bash\nbash skills/provider-integration-templates/scripts/validate-integration.sh --framework vercel\n```\n\n**Test streaming**:\n```bash\nbash scripts/test-streaming.sh --provider openrouter --model anthropic/claude-4.5-sonnet\n```\n\n## Integration Patterns\n\n### Pattern 1: Vercel AI SDK Chat Application\n\n1. Read Vercel AI SDK config template\n2. Copy to `src/lib/ai.ts`\n3. Read API route template\n4. Copy to `app/api/chat/route.ts`\n5. Read chat component template\n6. Copy to `components/chat.tsx`\n7. Run validation script\n\n### Pattern 2: LangChain LCEL Chain\n\n1. Read LangChain config template (Python or TS)\n2. Copy to `src/config/langchain.py`\n3. Read LCEL chain template\n4. Copy to `src/chains/chat_chain.py`\n5. Customize prompts and models\n6. Test with validation script\n\n### Pattern 3: OpenAI SDK Drop-in Replacement\n\n1. Read OpenAI SDK config template\n2. Replace base URL with OpenRouter endpoint\n3. Add `HTTP-Referer` and `X-Title` headers\n4. Update API key to use OpenRouter key\n5. Test existing OpenAI code (should work unchanged)\n\n## Environment Variables\n\nAll templates use these standard environment variables:\n\n```bash\nOPENROUTER_API_KEY=sk-or-v1-...\nOPENROUTER_MODEL=anthropic/claude-4.5-sonnet\nOPENROUTER_SITE_URL=https://yourapp.com  # Optional: for rankings\nOPENROUTER_SITE_NAME=YourApp  # Optional: for rankings\n```\n\n## Model Selection\n\nTemplates use configurable model selection. Common models:\n\n- `anthropic/claude-4.5-sonnet` - Best reasoning, long context\n- `anthropic/claude-4.5-sonnet` - Most capable, highest cost\n- `meta-llama/llama-3.1-70b-instruct` - Fast, cost-effective\n- `openai/gpt-4-turbo` - Strong general purpose\n- `google/gemini-pro-1.5` - Long context, multimodal\n\nUpdate model selection in templates based on use case.\n\n## Best Practices\n\n1. **Use Environment Variables**: Never hardcode API keys\n2. **Enable Streaming**: Better UX for chat applications\n3. **Add Error Handling**: Handle rate limits and API errors\n4. **Set HTTP Headers**: Include site URL and name for rankings\n5. **Test Before Deployment**: Use validation scripts\n6. **Monitor Usage**: Track costs with OpenRouter dashboard\n\n## Troubleshooting\n\n**Issue**: API key not working\n- Check key format: `sk-or-v1-...`\n- Verify key in OpenRouter dashboard\n- Check environment variable is loaded\n\n**Issue**: Streaming not working\n- Ensure `stream: true` in request\n- Check framework version compatibility\n- Verify response handler supports streaming\n\n**Issue**: Model not found\n- Check model ID format: `provider/model-name`\n- Verify model is available on OpenRouter\n- Check for typos in model name\n\n## Progressive Disclosure\n\nFor detailed implementation guides, load these files as needed:\n\n- `examples/vercel-streaming-example.md` - Complete Vercel AI SDK setup\n- `examples/langchain-rag-example.md` - RAG implementation guide\n- `examples/openai-sdk-example.md` - OpenAI SDK migration guide\n\n---\n\n**Template Version**: 1.0.0\n**Framework Support**: Vercel AI SDK 4.x, LangChain 0.3.x, OpenAI SDK 1.x\n**Last Updated**: 2025-10-31"
              }
            ]
          },
          {
            "name": "rag-pipeline",
            "description": "Complete RAG pipeline toolkit with LlamaIndex, LangChain, and vector database support",
            "source": "./plugins/rag-pipeline",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install rag-pipeline@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-chunking",
                "description": "Implement document chunking strategies (fixed, semantic, recursive, hybrid)",
                "path": "plugins/rag-pipeline/commands/add-chunking.md",
                "frontmatter": {
                  "description": "Implement document chunking strategies (fixed, semantic, recursive, hybrid)",
                  "argument-hint": [
                    "strategy-type"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Implement document chunking strategies for RAG pipeline with configurable parameters and validation\n\nCore Principles:\n- Ask user for chunking strategy preferences\n- Fetch minimal documentation (LlamaIndex + LangChain)\n- Generate chunking implementation with chosen strategy\n- Test with sample documents\n- Provide chunk statistics and recommendations\n\nPhase 1: Gather Requirements\nGoal: Understand chunking strategy needs\n\nActions:\n- Parse $ARGUMENTS to check if strategy specified\n- If strategy in $ARGUMENTS, use it; otherwise ask user\n- AskUserQuestion: \"Which chunking strategy would you like to implement?\n  1. Fixed-size chunking (simple, predictable chunks)\n  2. Semantic chunking (context-aware boundaries)\n  3. Recursive chunking (hierarchical splitting)\n  4. Hybrid chunking (combined approaches)\n\n  Enter number (1-4) or strategy name:\"\n- AskUserQuestion: \"Chunk size in characters/tokens? (default: 512)\"\n- AskUserQuestion: \"Chunk overlap in characters/tokens? (default: 50)\"\n- Detect project structure: Check for existing Python environment\n\nPhase 2: Fetch Documentation\nGoal: Load chunking strategy documentation\n\nActions:\nFetch these docs in parallel (2 URLs):\n\n1. WebFetch: https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/\n2. WebFetch: https://python.langchain.com/docs/modules/data_connection/document_transformers/\n\nPhase 3: Implementation\nGoal: Generate chunking script with selected strategy\n\nActions:\n\nTask(description=\"Generate chunking implementation\", subagent_type=\"rag-pipeline:document-processor\", prompt=\"You are the document-processor agent. Implement document chunking for a RAG pipeline.\n\nStrategy selected: [user's choice from Phase 1]\nChunk size: [user's chunk size]\nChunk overlap: [user's overlap]\n\nUsing the documentation fetched in Phase 2, create a Python script that:\n\n1. Implements the chosen chunking strategy\n2. Supports multiple document formats (txt, pdf, markdown)\n3. Includes configuration for chunk size and overlap\n4. Provides chunk metadata (position, source, length)\n5. Handles edge cases (empty documents, very small documents)\n6. Uses appropriate library (LlamaIndex or LangChain based on strategy):\n   - Fixed-size: Use LangChain CharacterTextSplitter or LlamaIndex SentenceSplitter\n   - Semantic: Use LangChain SemanticChunker or LlamaIndex SemanticSplitterNodeParser\n   - Recursive: Use LangChain RecursiveCharacterTextSplitter\n   - Hybrid: Combine multiple approaches\n\nCreate these files:\n- chunking/chunker.py - Main chunking implementation\n- chunking/config.py - Configuration for chunk parameters\n- chunking/test_chunker.py - Test with sample documents\n- chunking/requirements.txt - Dependencies\n\nInclude comprehensive docstrings and type hints.\nDeliverable: Working chunking implementation ready for testing\")\n\nWait for Task to complete.\n\nPhase 4: Test Chunking\nGoal: Validate chunking with sample documents\n\nActions:\n- Create sample test document if not exists: !{bash mkdir -p chunking/samples}\n- Generate sample text: Write simple test document to chunking/samples/test.txt\n- Run chunking test: !{bash cd chunking && python test_chunker.py}\n- Capture chunk statistics: Number of chunks, avg size, overlap effectiveness\n- Verify chunk boundaries are appropriate\n\nPhase 5: Statistics and Recommendations\nGoal: Provide chunk analysis and next steps\n\nActions:\nDisplay summary:\n- Strategy implemented: [chosen strategy]\n- Configuration: Chunk size [size], overlap [overlap]\n- Test results: [number] chunks generated from sample\n- Average chunk size: [avg] characters\n- Files created:\n  * chunking/chunker.py\n  * chunking/config.py\n  * chunking/test_chunker.py\n  * chunking/requirements.txt\n  * chunking/samples/test.txt\n\nRecommendations:\n- For semantic search: Consider chunk size 256-512 tokens\n- For question answering: Consider chunk size 512-1024 tokens\n- For summarization: Consider larger chunks 1024-2048 tokens\n- Overlap should be 10-20% of chunk size\n- Test with your actual documents and adjust parameters\n\nNext steps:\n- Install dependencies: pip install -r chunking/requirements.txt\n- Test with your documents: python chunking/chunker.py your_document.pdf\n- Integrate with vector database: /rag-pipeline:add-vector-db\n- Add embeddings: /rag-pipeline:add-embeddings\n\nImportant Notes:\n- Adapts to user's chunking strategy preference\n- Fetches minimal docs (2 URLs)\n- Generates production-ready chunking code\n- Tests implementation with samples\n- Provides tuning recommendations"
              },
              {
                "name": "/add-embeddings",
                "description": "Configure embedding models (OpenAI, HuggingFace, Cohere, Voyage)",
                "path": "plugins/rag-pipeline/commands/add-embeddings.md",
                "frontmatter": {
                  "description": "Configure embedding models (OpenAI, HuggingFace, Cohere, Voyage)",
                  "argument-hint": [
                    "model-provider"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure embedding models for RAG pipeline with FREE HuggingFace or paid providers (OpenAI, Cohere, Voyage).\n\nCore Principles:\n- Highlight FREE HuggingFace models first\n- Test embeddings before completing\n- Calculate cost estimates for paid providers\n- Provide clear next steps\n\nPhase 1: Provider Selection\nGoal: Determine which embedding provider to configure\n\nActions:\n- Parse $ARGUMENTS to check if provider specified\n- If $ARGUMENTS empty, use AskUserQuestion:\n  \"Which embedding provider?\n   1. HuggingFace (FREE local - recommended for testing)\n   2. OpenAI (Paid - high quality embeddings)\n   3. Cohere (Paid - multilingual support)\n   4. Voyage (Paid - specialized for retrieval)\n\n   Recommendation: Start with FREE HuggingFace, upgrade later if needed.\"\n- Display provider info (pricing, models, requirements)\n\nPhase 2: Environment Detection\nGoal: Understand project setup\n\nActions:\n- Check config files: !{bash ls -la requirements.txt pyproject.toml package.json 2>/dev/null}\n- Detect Python/Node.js environment\n- Check .env exists: !{bash test -f .env && echo \"exists\" || echo \"not found\"}\n- Identify package manager (pip, poetry, npm)\n\nPhase 3: Installation & Configuration\nGoal: Install dependencies and configure provider\n\nActions:\n\nTask(description=\"Install and configure embeddings\", subagent_type=\"rag-pipeline:embedding-specialist\", prompt=\"You are the embedding-specialist agent. Install and configure $ARGUMENTS embedding provider.\n\nProvider Details:\n\nHuggingFace (FREE): Install sentence-transformers, torch. Popular models: all-MiniLM-L6-v2 (384d, fast), all-mpnet-base-v2 (768d, quality). Docs: https://huggingface.co/models?pipeline_tag=sentence-similarity. No API key needed.\n\nOpenAI: Install openai package. Models: text-embedding-ada-002, text-embedding-3-small/large. Docs: https://platform.openai.com/docs/guides/embeddings. Needs OPENAI_API_KEY.\n\nCohere: Install cohere package. Models: embed-english-v3.0, embed-multilingual-v3.0. Docs: https://docs.cohere.com/docs/embeddings. Needs COHERE_API_KEY.\n\nVoyage: Install voyageai package. Models: voyage-2, voyage-code-2. Docs: https://docs.voyageai.com/. Needs VOYAGE_API_KEY.\n\nTasks:\n1. Install appropriate packages for detected environment\n2. For paid providers: Ask user for API key using AskUserQuestion, add to .env file\n3. For HuggingFace: Skip API key (local processing)\n4. Create embeddings_config.yaml with: provider, model, dimensions, pricing (if paid), device settings\n5. Verify installation with test import\n6. Report installed packages and versions\n\nExpected output: Configuration file path, installed packages, API key status\")\n\nPhase 4: Testing & Validation\nGoal: Verify embeddings work correctly\n\nActions:\n\nTask(description=\"Test embedding generation\", subagent_type=\"rag-pipeline:embedding-specialist\", prompt=\"You are the embedding-specialist agent. Test embedding generation for $ARGUMENTS provider.\n\nCreate test_embeddings script that:\n1. Loads config from Phase 3\n2. Generates embeddings for sample text: 'RAG pipeline test document'\n3. Validates output: correct dimensions, numeric values, proper shape\n4. Measures: generation time, tokens processed, cost estimate (if paid)\n\nFor HuggingFace: Test model loading, check GPU/CPU usage, verify cache location\nFor Paid APIs: Validate API key, test connection, verify response format\n\nExpected output: Test script path, results (pass/fail), sample vector (first 5 values), performance metrics\")\n\nPhase 5: Cost Analysis\nGoal: Calculate and display cost estimates\n\nActions:\n- Read test results from Phase 4\n- If HuggingFace: Display \"Cost: FREE (local)\", show compute requirements\n- If paid: Calculate costs for 1K, 10K, 100K documents (~500 words each)\n- Display pricing breakdown and monthly estimates\n- Compare to FREE HuggingFace option\n- Provide cost optimization tips\n\nPhase 6: Summary\nGoal: Document setup and next steps\n\nActions:\n- Summarize: Provider, model, dimensions, cost, location (local/cloud)\n- List created files: config file, test script, .env status\n- Explain trade-offs: cost vs quality vs speed\n- Next steps:\n  * Run test: python test_embeddings.py\n  * Add vector database: /rag-pipeline:add-vectorstore\n  * Batch process existing documents\n- Provide documentation links and troubleshooting resources"
              },
              {
                "name": "/add-google-file-search",
                "description": "Integrate Google File Search API for managed RAG with Gemini - handles store creation, file uploads, chunking, and citations",
                "path": "plugins/rag-pipeline/commands/add-google-file-search.md",
                "frontmatter": {
                  "description": "Integrate Google File Search API for managed RAG with Gemini - handles store creation, file uploads, chunking, and citations",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Google File Search API into projects for fully managed RAG with automatic chunking, semantic search, and grounding citations - eliminating the need for separate vector database infrastructure.\n\nCore Principles:\n- Understand project structure before making changes\n- Ask clarifying questions about document types and volume\n- Follow official Google File Search API patterns\n- Secure API key management (no hardcoded credentials)\n- Provide clear setup instructions\n\nPhase 1: Discovery\nGoal: Understand project structure and requirements\n\nActions:\n- Create todo list with TodoWrite for tracking progress\n- Parse $ARGUMENTS for project path (default: current directory)\n- Detect project type (Python vs Node.js/TypeScript)\n- Check for existing Google AI SDK installation\n- Load relevant files for context:\n  - @package.json (Node.js projects)\n  - @requirements.txt or @pyproject.toml (Python projects)\n- Identify if RAG components already exist\n\nPhase 2: Requirements Gathering\nGoal: Clarify implementation details\n\nActions:\n\nUse AskUserQuestion to gather critical information:\n\nQuestions to ask:\n- \"What types of documents will you be indexing?\" (PDF, Office docs, code files, etc.)\n- \"What's your expected document volume?\" (helps determine storage tier)\n- \"Do you need metadata filtering capabilities?\" (for multi-tenant or categorized search)\n- \"Do you want custom chunking configuration?\" (or use default white space chunking)\n\nThis information will guide the configuration strategy.\n\nPhase 3: Analysis\nGoal: Review existing codebase patterns\n\nActions:\n- Search for existing RAG implementation:\n  - !{bash find . -type f \\( -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" \\) -exec grep -l \"embedding\\|vector\\|rag\\|retrieval\" {} \\; 2>/dev/null | head -10}\n- Read existing vector database setup (if any)\n- Understand current document processing workflows\n- Identify integration points for File Search\n\nPhase 4: Planning\nGoal: Design File Search integration approach\n\nActions:\n- Determine store strategy (single store vs multiple stores)\n- Plan chunking configuration based on document types\n- Design metadata schema if filtering needed\n- Map out document upload workflow\n- Identify where to extract and display grounding citations\n- Present plan to user and confirm approach\n\nPhase 5: Implementation\nGoal: Integrate Google File Search API\n\nActions:\n\nTask(description=\"Implement Google File Search\", subagent_type=\"rag-pipeline:google-file-search-specialist\", prompt=\"You are the google-file-search-specialist agent. Integrate Google File Search API for $ARGUMENTS.\n\nProject Context:\n- Project type: [Python/Node.js detected from Phase 1]\n- Existing setup: [RAG components found in Phase 3]\n- Document types: [From Phase 2 questions]\n- Volume: [From Phase 2 questions]\n- Metadata filtering: [From Phase 2 questions]\n- Chunking: [From Phase 2 questions]\n\nRequirements:\n- Install Google Generative AI SDK (google-generativeai for Python, @google/generative-ai for Node.js)\n- Create File Search store with appropriate configuration\n- Implement document upload workflow (direct or separate method)\n- Configure chunking strategy based on document types\n- Add metadata schema if filtering required\n- Integrate with Gemini generation calls\n- Extract and display grounding citations\n- Set up secure API key management (environment variables)\n- Create .env.example with placeholders\n- Add .gitignore protection for .env files\n- Document API key acquisition in README or setup guide\n\nExpected Deliverables:\n- Store creation and configuration code\n- Document upload utilities\n- Search and retrieval functions\n- Grounding citation extraction\n- Environment configuration files (.env.example)\n- Setup documentation with clear instructions\n- Example usage code\")\n\nPhase 6: Verification\nGoal: Ensure implementation works correctly\n\nActions:\n- Check that SDK dependencies were installed\n- Verify .env.example exists with placeholder API keys\n- Confirm .gitignore protects .env files\n- Review generated code for security best practices\n- Test store creation code (if API key available)\n- Validate chunking configuration\n- Run type checking if applicable:\n  - TypeScript: !{bash npx tsc --noEmit 2>&1 || echo \"No TypeScript\"}\n  - Python: !{bash python -m mypy . 2>&1 || echo \"No mypy\"}\n\nPhase 7: Summary\nGoal: Document what was accomplished\n\nActions:\n- Mark all todos complete with TodoWrite\n- Display comprehensive summary:\n\n**Google File Search Integration Complete!**\n\n**What was implemented:**\n- File Search store creation and management\n- Document upload workflow\n- Semantic search with embeddings\n- Grounding citations extraction\n- Secure API key configuration\n\n**Files created/modified:**\n- [List all files created by agent]\n\n**Key Features:**\n- Fully Managed RAG - No vector database setup needed\n- Automatic Chunking - Google handles document segmentation\n- Built-in Citations - Grounding metadata with source attribution\n- Persistent Storage - Documents don't expire\n- Cost Effective - Free storage, pay only for indexing\n\n**Setup Instructions:**\n1. Get Google AI API key from https://ai.google.dev/\n2. Copy .env.example to .env and add your API key\n3. Run the setup/example code to create your first store\n\n**Next Steps:**\n- Upload documents and test semantic search\n- Review grounding citations in responses\n- Configure custom chunking or metadata filtering as needed\n\n**Documentation:**\n- https://ai.google.dev/gemini-api/docs/file-search\n- https://ai.google.dev/pricing"
              },
              {
                "name": "/add-hybrid-search",
                "description": "Implement hybrid search (vector + keyword with RRF)",
                "path": "plugins/rag-pipeline/commands/add-hybrid-search.md",
                "frontmatter": {
                  "description": "Implement hybrid search (vector + keyword with RRF)",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Implement hybrid search combining vector similarity search and keyword-based full-text search using Reciprocal Rank Fusion (RRF) for optimal retrieval performance.\n\nCore Principles:\n- Build on existing vector search infrastructure\n- Add full-text search capability (BM25 or database FTS)\n- Implement RRF algorithm for result fusion\n- Provide configurable weight tuning\n- Compare hybrid vs pure vector search performance\n\nPhase 1: Verify Vector Search Foundation\nGoal: Ensure vector search infrastructure exists\n\nActions:\n- Check for vector database configuration: !{bash find . -name \"*vector*\" -o -name \"*embed*\" -o -name \"*index*\" 2>/dev/null | grep -E \"\\.(py|ts|js|json|yml|yaml)$\" | head -10}\n- Look for embedding models: !{bash grep -r \"embedding\" --include=\"*.py\" --include=\"*.ts\" --include=\"*.js\" . 2>/dev/null | head -5}\n- Detect vector store (Pinecone, Chroma, pgvector, etc.): !{bash grep -r -E \"(pinecone|chroma|pgvector|qdrant|weaviate|faiss)\" --include=\"*.py\" --include=\"*.ts\" . 2>/dev/null | head -5}\n- Verify dependencies: !{bash test -f requirements.txt && cat requirements.txt | grep -E \"(llama-index|langchain|pinecone|chromadb|pgvector)\" || echo \"No requirements.txt found\"}\n\nPhase 2: Analyze Project Structure\nGoal: Understand codebase organization\n\nActions:\n- Identify main framework (LlamaIndex vs LangChain): !{bash grep -r \"from llama_index\\|from langchain\" --include=\"*.py\" . 2>/dev/null | head -5}\n- Find existing retrieval code: !{bash find . -name \"*retriev*\" -o -name \"*query*\" -o -name \"*search*\" 2>/dev/null | grep -E \"\\.(py|ts|js)$\" | head -10}\n- Locate configuration files: !{bash find . -name \"config.*\" -o -name \"settings.*\" -o -name \".env*\" 2>/dev/null | head -10}\n- Check for existing documentation: !{bash find . -name \"README*\" -o -name \"DOCS*\" -o -name \"*.md\" 2>/dev/null | grep -v node_modules | head -10}\n\nPhase 3: Implement Hybrid Search\nGoal: Add hybrid search capability with RRF\n\nActions:\n\nTask(description=\"Implement hybrid search with RRF\", subagent_type=\"rag-pipeline:retrieval-optimizer\", prompt=\"You are the retrieval-optimizer agent. Implement hybrid search (vector + keyword with RRF) for this RAG pipeline.\n\nContext from analysis:\n- Vector store detected: [from Phase 1 detection]\n- Main framework: [LlamaIndex or LangChain from Phase 2]\n- Existing retrieval code: [from Phase 2 findings]\n- Project structure: [from Phase 2 analysis]\n\nReference Documentation:\n- Hybrid Search Concepts: https://developers.llamaindex.ai/python/framework/understanding/\n- RRF Algorithm: Reciprocal Rank Fusion combines rankings from multiple retrieval methods\n\nImplementation Requirements:\n\n1. **Add Full-Text Search Component**:\n   - If using pgvector: Add PostgreSQL full-text search (tsvector/tsquery)\n   - If using Pinecone/Chroma: Implement BM25 keyword search\n   - If using LlamaIndex: Use BM25Retriever or custom keyword retriever\n   - If using LangChain: Use BM25Retriever from langchain-community\n\n2. **Implement RRF Algorithm**:\n   - Create RRF fusion function: score = sum(1 / (k + rank_i)) for each retrieval method\n   - Default k=60 (standard RRF parameter)\n   - Combine results from vector search and keyword search\n   - Re-rank merged results by RRF score\n\n3. **Create Hybrid Retriever Class**:\n   - Constructor accepts: vector_retriever, keyword_retriever, weights (optional)\n   - retrieve() method that:\n     * Runs both retrievers in parallel\n     * Applies RRF fusion\n     * Returns top-k merged results\n   - Support configurable weight tuning (alpha parameter: 0=pure keyword, 1=pure vector)\n\n4. **Add Configuration Options**:\n   - config.py or settings file with:\n     * HYBRID_SEARCH_ENABLED (bool)\n     * RRF_K_PARAM (int, default 60)\n     * VECTOR_WEIGHT (float, default 0.5)\n     * KEYWORD_WEIGHT (float, default 0.5)\n     * TOP_K_RESULTS (int, default 10)\n\n5. **Create Comparison Utilities**:\n   - compare_search_methods() function that runs same query with:\n     * Pure vector search\n     * Pure keyword search\n     * Hybrid search (RRF)\n   - Display metrics: retrieval time, result count, overlap analysis\n   - Example usage script or notebook\n\n6. **Add Tests**:\n   - Unit tests for RRF algorithm\n   - Integration tests for hybrid retriever\n   - Performance benchmarks\n   - Example queries demonstrating hybrid superiority\n\n7. **Update Documentation**:\n   - README section on hybrid search\n   - API documentation for hybrid retriever\n   - Configuration guide\n   - Performance tuning tips\n\nFiles to Create/Modify:\n- hybrid_search.py or hybrid_retriever.py (main implementation)\n- config.py or settings.py (configuration)\n- tests/test_hybrid_search.py (tests)\n- examples/hybrid_search_demo.py (usage example)\n- README.md (documentation updates)\n\nDeliverable: Complete hybrid search implementation with RRF, tests, configuration, comparison utilities, and documentation.\")\n\nPhase 4: Validation and Testing\nGoal: Verify hybrid search implementation works correctly\n\nActions:\n- Check created files exist: !{bash ls -la hybrid_search.py hybrid_retriever.py 2>/dev/null || echo \"Check for created hybrid search files\"}\n- Verify RRF implementation: !{bash grep -n \"rrf\\|reciprocal\\|rank.*fusion\" --include=\"*.py\" . -r 2>/dev/null | head -10}\n- Look for configuration options: !{bash grep -n \"HYBRID\\|RRF_K\\|VECTOR_WEIGHT\" --include=\"*.py\" . -r 2>/dev/null | head -10}\n- Check if tests were created: !{bash find . -path \"*/test*\" -name \"*hybrid*\" 2>/dev/null}\n- Run tests if they exist: !{bash if [ -f tests/test_hybrid_search.py ]; then python -m pytest tests/test_hybrid_search.py -v 2>&1 | head -30; else echo \"No tests found\"; fi}\n\nPhase 5: Summary and Usage Guide\nGoal: Provide clear usage instructions\n\nActions:\n- Display created files with absolute paths\n- Show example usage: HybridRetriever(vector_retriever, keyword_retriever, rrf_k=60, vector_weight=0.5)\n- Explain RRF parameters: k=60 (rank fusion), vector_weight 0.0-1.0 (balance vector/keyword)\n- Suggest next steps: Tune weights, run benchmarks, integrate with RAG pipeline, monitor quality\n- Performance tips: Start with 0.5/0.5 weights, increase vector weight for semantic queries, keyword for exact matches"
              },
              {
                "name": "/add-metadata",
                "description": "Add metadata filtering and multi-tenant support",
                "path": "plugins/rag-pipeline/commands/add-metadata.md",
                "frontmatter": {
                  "description": "Add metadata filtering and multi-tenant support",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add metadata filtering and multi-tenant isolation to enable filtered retrieval by tags, categories, dates, and tenant boundaries.\n\nCore Principles:\n- Define flexible metadata schema\n- Update ingestion to extract and store metadata\n- Implement metadata filtering in retrieval\n- Add multi-tenant isolation with secure boundaries\n\nPhase 1: Analyze Current Infrastructure\nGoal: Understand existing RAG pipeline structure\n\nActions:\n- Detect vector database: !{bash grep -r -E \"(pinecone|chroma|pgvector|qdrant|weaviate|faiss)\" --include=\"*.py\" --include=\"*.ts\" . 2>/dev/null | head -5}\n- Check existing metadata: !{bash grep -r \"metadata\" --include=\"*.py\" --include=\"*.ts\" . 2>/dev/null | head -10}\n- Find ingestion pipeline: !{bash find . -name \"*ingest*\" -o -name \"*pipeline*\" 2>/dev/null | grep -E \"\\.(py|ts|js)$\" | head -5}\n- Locate retrieval code: !{bash find . -name \"*retriev*\" -o -name \"*query*\" 2>/dev/null | grep -E \"\\.(py|ts|js)$\" | head -5}\n- Check framework: !{bash grep -r \"from llama_index\\|from langchain\" --include=\"*.py\" . 2>/dev/null | head -5}\n\nPhase 2: Gather Requirements\nGoal: Determine metadata schema and tenant needs\n\nActions:\n- AskUserQuestion: \"Metadata fields needed? (Examples: tags, categories, departments, date ranges, authors). Default: tags, categories, tenant_id, created_at, author, document_type\"\n- AskUserQuestion: \"Enable multi-tenant isolation? (yes/no, default: yes)\"\n- AskUserQuestion: \"Tenant identifier field? (Examples: tenant_id, organization_id, workspace_id). Default: tenant_id\"\n\nPhase 3: Implement Metadata Support\nGoal: Add metadata extraction, storage, and filtering\n\nActions:\n\nTask(description=\"Implement metadata filtering and multi-tenant support\", subagent_type=\"rag-pipeline:retrieval-optimizer\", prompt=\"You are the retrieval-optimizer agent. Add metadata filtering and multi-tenant support to this RAG pipeline.\n\nContext: Vector database [Phase 1], Framework [Phase 1], Schema fields [Phase 2], Multi-tenant [Phase 2], Tenant field [Phase 2]\n\nRequirements:\n\n1. **Metadata Schema** (metadata_schema.py):\n   - MetadataSchema class with fields: tenant_id, tags (array), category, created_at, updated_at, author, document_type, custom_fields\n   - Validation functions for each field type\n   - Support required/optional fields\n\n2. **Ingestion Updates**:\n   - Extract file metadata (filename, extension, dates)\n   - Extract content metadata (type, author)\n   - Accept custom metadata via parameters\n   - Validate and store metadata with vectors\n\n3. **Metadata Filtering** (metadata_filter.py):\n   - MetadataFilter class with operations: ==, !=, contains, in, range, AND, OR, NOT\n   - Integrate with vector DB API (LlamaIndex MetadataFilters, LangChain filter param, Pinecone/Chroma/pgvector syntax)\n   - FilterBuilder with fluent API: .where(field, op, value).build()\n\n4. **Multi-Tenant Isolation** (tenant_context.py):\n   - TenantContext class for current tenant\n   - TenantRetriever wrapper auto-injecting tenant_id filter\n   - Tenant validation middleware\n   - Security: never trust client tenant_id, audit cross-tenant access\n\n5. **Configuration** (config.py/.env):\n   - MULTI_TENANT_ENABLED, TENANT_FIELD_NAME, METADATA_FIELDS, REQUIRE_TENANT_FILTER, DEFAULT_METADATA\n\n6. **Examples**:\n   - examples/metadata_ingestion.py (ingest with metadata)\n   - examples/metadata_retrieval.py (filtered queries)\n   - examples/multi_tenant_demo.py (tenant isolation)\n\n7. **Tests** (tests/test_metadata.py):\n   - Validation, filtered retrieval, tenant isolation (no leakage), performance, edge cases\n\n8. **Documentation**:\n   - README: metadata filtering, multi-tenant architecture, API reference, security practices, migration guide\n\nFiles: metadata_schema.py, metadata_filter.py, tenant_context.py, config.py, tests/test_metadata.py, examples/metadata_*.py, README.md updates\n\nDeliverable: Complete metadata filtering and multi-tenant support with schema, ingestion updates, filtered retrieval, tenant isolation, tests, examples, docs.\")\n\nPhase 4: Validation\nGoal: Verify implementation\n\nActions:\n- Check files: !{bash ls -la metadata_schema.* metadata_filter.* tenant_context.* 2>/dev/null}\n- Verify filters: !{bash grep -n \"MetadataFilter\\|tenant\" --include=\"*.py\" --include=\"*.ts\" . -r 2>/dev/null | head -10}\n- Find tests: !{bash find . -path \"*/test*\" -name \"*metadata*\" -o -path \"*/test*\" -name \"*tenant*\" 2>/dev/null}\n\nPhase 5: Summary\nGoal: Usage instructions\n\nActions:\nSummary:\n- Files: metadata_schema.py, metadata_filter.py, tenant_context.py, updated ingestion/retrieval, tests, examples\n- Ingest: ingest_document(content, metadata={'tenant_id': 'org-123', 'tags': ['api'], 'category': 'docs'})\n- Query: filter = FilterBuilder().where('tenant_id', '==', 'org-123').where('tags', 'contains', 'api').build()\n- Tenant-scoped: TenantRetriever(base_retriever, tenant_id='org-123').retrieve(query)\n- Security: Validate tenant_id from auth context, enable REQUIRE_TENANT_FILTER, audit cross-tenant access\n- Next: Update existing documents, test filtering, add tenant auth, monitor performance, add metadata indexes"
              },
              {
                "name": "/add-monitoring",
                "description": "Add observability (LangSmith/LlamaCloud integration)",
                "path": "plugins/rag-pipeline/commands/add-monitoring.md",
                "frontmatter": {
                  "description": "Add observability (LangSmith/LlamaCloud integration)",
                  "argument-hint": [
                    "platform"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive observability and monitoring to RAG pipeline with cost tracking, latency monitoring, and quality metrics\n\nCore Principles:\n- Highlight free and open-source options (Custom/OSS solutions)\n- Fetch latest vendor documentation for chosen platform\n- Implement cost tracking, latency monitoring, quality metrics\n- Test monitoring with sample queries\n\nPhase 1: Platform Selection\nGoal: Determine which monitoring platform to configure\n\nActions:\n- Check if $ARGUMENTS specifies a monitoring platform\n- If not provided, use AskUserQuestion:\n\n  \"Which observability platform would you like to configure?\n\n  **Managed Platforms:**\n  - LangSmith (LangChain native, 5K traces/month free tier)\n  - LlamaCloud (LlamaIndex native, free tier available)\n\n  **Open Source/Custom:**\n  - Custom (Python logging + metrics, completely free)\n\n  Enter platform name (langsmith, llamacloud, or custom):\"\n\n- Store selection for subsequent phases\n\nPhase 2: Fetch Documentation\nGoal: Load platform-specific setup documentation\n\nActions:\nFetch docs based on selection using WebFetch:\n\nLangSmith:\n- WebFetch: https://docs.langchain.com/langsmith/home\n- WebFetch: https://docs.smith.langchain.com/tracing\n\nLlamaCloud:\n- WebFetch: https://docs.cloud.llamaindex.ai/\n- WebFetch: https://docs.llamaindex.ai/en/stable/module_guides/observability/\n\nCustom:\n- WebFetch: https://docs.python.org/3/howto/logging.html\n- WebFetch: https://prometheus.io/docs/introduction/overview/\n\nPhase 3: Project Discovery\nGoal: Understand existing RAG pipeline structure\n\nActions:\n- Detect project type: !{bash test -f requirements.txt && echo \"python\" || test -f package.json && echo \"node\"}\n- Find RAG components: !{bash find . -name \"*retrieval*\" -o -name \"*generation*\" -o -name \"*query*\" 2>/dev/null | head -10}\n- Identify framework: Check for LangChain or LlamaIndex imports\n- Check existing monitoring: !{bash grep -r \"LANGCHAIN_TRACING\\|LlamaCloud\\|prometheus\" . 2>/dev/null | head -5}\n\nPhase 4: Implementation\nGoal: Install dependencies and configure monitoring platform\n\nActions:\n\nTask(description=\"Setup RAG pipeline monitoring\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Configure $ARGUMENTS monitoring for RAG pipeline based on fetched documentation.\n\nPlatform: $ARGUMENTS (or from user question)\nProject type: [from Phase 3]\nFramework: [from Phase 3]\n\nUsing the documentation fetched in Phase 2, implement:\n\n1. Core Monitoring Components:\n   - Tracer initialization and context propagation\n   - Metrics collection (latency, cost, quality)\n   - Platform-specific callbacks/handlers\n   - Structured logging\n\n2. Instrumentation:\n   - Wrap retrieval calls with tracing\n   - Wrap LLM calls with cost tracking\n   - End-to-end pipeline monitoring\n\n3. Configuration:\n   - Install required packages\n   - Setup API keys in .env (ask user if needed)\n   - Configure platform-specific settings\n   - Set latency/cost thresholds\n\n4. Testing:\n   - Create test file to verify monitoring\n   - Run sample query with full instrumentation\n   - Verify traces appear in platform dashboard\n\n5. Documentation:\n   - Add setup instructions to README\n   - Document metrics and how to interpret them\n\nDeliverable: Working monitoring setup with test results\")\n\nPhase 5: Validation\nGoal: Verify monitoring is working correctly\n\nActions:\n- Run test query: !{bash python -m tests.test_monitoring 2>&1 || echo \"manual-test-needed\"}\n- Check for traces/metrics in platform dashboard\n- Verify cost tracking is accurate\n- Test latency measurement\n\nPhase 6: Summary\nGoal: Display setup summary and next steps\n\nActions:\nDisplay summary:\n- Platform configured: [platform name]\n- Monitoring enabled for: [components]\n- Metrics tracked: latency, cost, quality, errors\n- Dashboard URL: [platform-specific link]\n- Test results: [pass/fail status]\n\nNext steps:\n- Review dashboard: [platform URL]\n- Configure alerts for: errors, latency spikes, cost overruns\n- Set up evaluation datasets: /rag-pipeline:test\n- Monitor production queries and iterate on retrieval quality\n\nImportant Notes:\n- Adapts to user's platform choice (LangSmith, LlamaCloud, or Custom)\n- Fetches vendor docs for latest API changes\n- Tests monitoring with sample queries before completing\n- Provides clear next steps for dashboard setup\n- Highlights free tiers and open-source options"
              },
              {
                "name": "/add-parser",
                "description": "Add document parsers (LlamaParse, Unstructured, PyPDF, PDFPlumber)",
                "path": "plugins/rag-pipeline/commands/add-parser.md",
                "frontmatter": {
                  "description": "Add document parsers (LlamaParse, Unstructured, PyPDF, PDFPlumber)",
                  "argument-hint": [
                    "parser-type"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add and configure document parsers for RAG pipeline with multi-format support and advanced features.\n\nCore Principles:\n- Highlight FREE parsers (Unstructured, PyPDF2, PDFPlumber)\n- Understand existing structure before adding dependencies\n- Generate unified parsing interface\n- Test parsing quality\n\nPhase 1: Requirements & Discovery\nGoal: Determine parser type and analyze project structure\n\nActions:\n- Parse $ARGUMENTS for parser type (llamaparse, unstructured, pypdf, pdfplumber)\n- If unclear, use AskUserQuestion to gather:\n  - Which parser? (LlamaParse PAID | Unstructured/PyPDF2/PDFPlumber FREE)\n  - Document types? (PDF, DOCX, HTML, Markdown)\n  - Need table/image extraction? OCR?\n  - Local processing or cloud API?\n- Detect Python environment: !{bash python --version 2>&1}\n- Check requirements: !{bash find . -name \"requirements.txt\" -o -name \"pyproject.toml\" | head -5}\n- Find existing parsers: !{bash find . -type f -name \"*.py\" | xargs grep -l \"parser\\|parse\" 2>/dev/null | head -10}\n- Check dependencies: !{bash cat requirements.txt 2>/dev/null | grep -E \"pdf|parse|unstructured\" || echo \"None\"}\n\nPhase 2: Load Documentation\nGoal: Reference parser documentation based on selection\n\nActions:\n- For LlamaParse: WebFetch: https://docs.cloud.llamaindex.ai/llamaparse/getting_started\n- For Unstructured: WebFetch: https://unstructured-io.github.io/unstructured/introduction.html\n- For PyPDF2: WebFetch: https://pypdf2.readthedocs.io/en/latest/user/extract-text.html\n- For PDFPlumber: WebFetch: https://github.com/jsvine/pdfplumber#table-extraction\n\nPhase 3: Implementation\nGoal: Install parser and generate code\n\nActions:\n\nTask(description=\"Configure document parser\", subagent_type=\"rag-pipeline:document-processor\", prompt=\"You are the document-processor agent. Add document parser for $ARGUMENTS.\n\nParser: [selected parser]\nFeatures: Table extraction [yes/no], Image extraction [yes/no], OCR [yes/no]\nProject: [structure]\n\nRequirements:\n1. Dependencies:\n   - Update requirements.txt with parser package\n   - LlamaParse: llama-parse\n   - Unstructured: unstructured[all-docs]\n   - PyPDF2: pypdf2\n   - PDFPlumber: pdfplumber\n\n2. Parser Implementation:\n   - Create parser module with unified interface\n   - Support PDF, DOCX, HTML, Markdown\n   - Configure table/image handling\n   - Add error handling and logging\n   - Support batch processing\n\n3. Configuration:\n   - LlamaParse: Add LLAMA_CLOUD_API_KEY to .env\n   - Create config file with parsing settings\n   - Document all options\n\n4. Utility Script:\n   - Create parsing script supporting input dir/file\n   - Auto-detect document type\n   - Output structured text/JSON\n   - Include progress tracking\n\n5. Quality:\n   - Type hints and docstrings\n   - Usage examples in comments\n   - Follow project style\n\nDeliverables:\n- Updated requirements.txt\n- Parser module (parsers/[name].py)\n- Utility script (scripts/parse_documents.py)\n- Config file/env updates\n- README documentation\")\n\nPhase 4: Validation\nGoal: Test parser installation and quality\n\nActions:\n- Verify install: !{bash pip list | grep -E \"llama-parse|unstructured|pypdf|pdfplumber\"}\n- Check files: !{bash find . -type f -name \"*parser*.py\" -o -name \"parse*.py\" | head -10}\n- Validate syntax: !{bash python -m py_compile [generated-files]}\n- Test import: !{bash python -c \"from parsers import [parser_class]\" 2>&1}\n\nPhase 5: Summary\nGoal: Document accomplishments and next steps\n\nActions:\n- Summarize:\n  - Parser: [name and version]\n  - Files: [created/modified]\n  - Features: [enabled capabilities]\n  - Format support: [PDF, DOCX, etc.]\n- Usage examples:\n  - Code snippet\n  - CLI command\n  - Configuration\n- API keys (if LlamaParse):\n  - Add LLAMA_CLOUD_API_KEY to .env\n  - Get key from llamaindex.ai\n- Cost notes:\n  - FREE: Unstructured, PyPDF2, PDFPlumber\n  - PAID: LlamaParse\n- Next steps:\n  - Test with your documents\n  - Tune parameters\n  - Integrate with embedding pipeline\n  - Monitor quality metrics"
              },
              {
                "name": "/add-scraper",
                "description": "Add web scraping capability (Playwright, Selenium, BeautifulSoup, Scrapy)",
                "path": "plugins/rag-pipeline/commands/add-scraper.md",
                "frontmatter": {
                  "description": "Add web scraping capability (Playwright, Selenium, BeautifulSoup, Scrapy)",
                  "argument-hint": [
                    "scraper-type"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add web scraping capability to RAG pipeline with polite scraping practices (rate limiting, robots.txt)\n\nCore Principles:\n- Recommend Playwright for dynamic content, BeautifulSoup for static HTML\n- Always implement rate limiting and respect robots.txt\n- Generate production-ready scraping scripts with error handling\n- Test with sample URL before deployment\n\nPhase 1: Discovery & Requirements\nGoal: Determine scraper type and gather requirements\n\nActions:\n- Parse $ARGUMENTS to check if scraper type specified\n- If not specified, use AskUserQuestion to ask:\n  - What type of content are you scraping? (dynamic JS-heavy sites, static HTML, API endpoints)\n  - Do you need JavaScript execution? (Yes = Playwright/Selenium, No = BeautifulSoup/Scrapy)\n  - What's the scale? (Few pages = BeautifulSoup, Large scale = Scrapy, Browser automation = Playwright)\n  - Sample URL to test?\n- Detect project structure:\n  - !{bash ls -la | grep -E \"requirements.txt|pyproject.toml|package.json\"}\n  - @requirements.txt (if exists)\n  - @package.json (if exists)\n\nRecommendations:\n- Playwright: Modern, reliable, great for JS-heavy sites, built-in anti-detection\n- BeautifulSoup: Simple, lightweight, perfect for static HTML\n- Scrapy: Industrial-scale crawling, async, powerful middleware\n- Selenium: Mature, wide browser support, but slower than Playwright\n\nPhase 2: Validation\nGoal: Verify environment and prerequisites\n\nActions:\n- Check Python version: !{bash python3 --version}\n- Check if virtual environment exists: !{bash ls -la venv .venv 2>/dev/null}\n- Identify existing dependencies\n- Validate sample URL if provided\n\nPhase 3: Implementation\nGoal: Install scraper and generate script with polite scraping practices\n\nActions:\n\nTask(description=\"Install scraper and generate script\", subagent_type=\"rag-pipeline:web-scraper-agent\", prompt=\"You are the web-scraper-agent. Add web scraping capability for $ARGUMENTS.\n\nScraper Selection: Based on Phase 1 discovery, install the recommended scraper.\n\nInstallation Tasks:\n1. Install scraper package:\n   - Playwright: pip install playwright && playwright install\n   - BeautifulSoup: pip install beautifulsoup4 requests lxml\n   - Scrapy: pip install scrapy\n   - Selenium: pip install selenium webdriver-manager\n\n2. Create scraping script at scripts/scraper.py with:\n   - Polite scraping practices (rate limiting, delays)\n   - Robots.txt checking\n   - User-Agent headers\n   - Error handling and retries\n   - Progress logging\n   - Data extraction logic\n\n3. For Playwright specifically:\n   - Configure browser settings (headless, viewport)\n   - Add stealth plugins if needed\n   - Set up screenshots for debugging\n   - Use mcp__playwright tool if available\n\n4. Include configuration file (scraper_config.yaml) with:\n   - Rate limits (requests per second)\n   - Retry settings (max retries, backoff)\n   - User-Agent string\n   - Robots.txt compliance toggle\n   - Output format (JSON, CSV, etc.)\n\nDocumentation References:\n- Playwright: https://playwright.dev/python/\n- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n- Scrapy: https://docs.scrapy.org/\n- Selenium: https://selenium-python.readthedocs.io/\n\nBest Practices:\n- Always check robots.txt before scraping\n- Implement exponential backoff on errors\n- Use appropriate delays between requests (1-2 seconds minimum)\n- Set descriptive User-Agent with contact info\n- Handle pagination gracefully\n- Save intermediate results (checkpoint system)\n\nDeliverable: Production-ready scraping script with polite scraping configuration\")\n\nPhase 4: Configuration Review\nGoal: Ensure polite scraping settings are appropriate\n\nActions:\n- Review generated configuration file\n- Check rate limiting settings (should be conservative)\n- Verify robots.txt compliance is enabled by default\n- Confirm User-Agent includes contact information\n- Display configuration to user for approval\n\nPhase 5: Testing\nGoal: Validate scraper works with sample URL\n\nActions:\n- If sample URL provided, run test:\n  - !{bash python3 scripts/scraper.py --url \"SAMPLE_URL\" --limit 1}\n- Check for errors or warnings\n- Verify output format\n- Display sample scraped data\n\nPhase 6: Summary\nGoal: Document what was added and next steps\n\nActions:\n- Summarize installation:\n  - Scraper type installed\n  - Dependencies added\n  - Script location\n  - Configuration file location\n- Provide usage examples:\n  - How to run scraper\n  - How to adjust rate limits\n  - How to modify selectors\n- Highlight polite scraping features:\n  - Rate limiting: X requests per second\n  - Robots.txt: Enabled\n  - User-Agent: Configured\n  - Retry logic: Exponential backoff\n- Suggest next steps:\n  - Integrate with document loader\n  - Add to data ingestion pipeline\n  - Set up scheduling (cron/celery)\n  - Monitor scraping metrics"
              },
              {
                "name": "/add-vector-db",
                "description": "Configure vector database (pgvector, Chroma, Pinecone, Weaviate, Qdrant, FAISS)",
                "path": "plugins/rag-pipeline/commands/add-vector-db.md",
                "frontmatter": {
                  "description": "Configure vector database (pgvector, Chroma, Pinecone, Weaviate, Qdrant, FAISS)",
                  "argument-hint": [
                    "database-type"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure and setup a vector database for RAG pipeline with cost-aware recommendations\n\nCore Principles:\n- Highlight free and open-source options (pgvector, Chroma, FAISS)\n- Fetch vendor-specific documentation\n- Generate working configuration\n- Test connectivity before completion\n- Provide cost estimates and free tier information\n\nPhase 1: Database Selection\nGoal: Determine which vector database to configure\n\nActions:\n- Check if $ARGUMENTS specifies a database type\n- If not provided, ask user which vector database:\n\n  \"Which vector database would you like to configure?\n\n  **Free & Open Source:**\n  - pgvector (Postgres extension, free, production-ready)\n  - Chroma (embedded or server, free, developer-friendly)\n  - FAISS (Facebook AI, in-memory, free, high-performance)\n\n  **Managed Services (paid with free tiers):**\n  - Pinecone (serverless, free tier: 100K vectors)\n  - Weaviate (cloud or self-hosted, free tier available)\n  - Qdrant (cloud or self-hosted, free tier: 1GB cluster)\n\n  Enter database name (pgvector, chroma, pinecone, weaviate, qdrant, or faiss):\"\n\n- Store selection for use in subsequent phases\n\nPhase 2: Fetch Documentation\nGoal: Load vendor-specific setup documentation\n\nActions:\nFetch docs based on selection (WebFetch in parallel):\n- pgvector: github.com/pgvector/pgvector, supabase.com/docs/guides/ai/vector-columns\n- Chroma: docs.trychroma.com, docs.trychroma.com/getting-started\n- Pinecone: docs.pinecone.io, docs.pinecone.io/guides/get-started/quickstart\n- Weaviate: weaviate.io/developers/weaviate, weaviate.io/developers/weaviate/quickstart\n- Qdrant: qdrant.tech/documentation, qdrant.tech/documentation/quickstart\n- FAISS: faiss.ai, github.com/facebookresearch/faiss/wiki/Getting-started\n\nPhase 3: Project Discovery\nGoal: Understand existing project structure\n\nActions:\n- Detect project type: Check for package.json, requirements.txt, pyproject.toml\n- Load existing configuration if present\n- Identify framework (Next.js, FastAPI, Express, etc.)\n- Check if database client libraries already installed\n- Locate or create config directory for database settings\n\nPhase 4: Implementation\nGoal: Install dependencies and generate configuration\n\nActions:\n\nTask(description=\"Setup vector database configuration\", subagent_type=\"rag-pipeline:vector-db-engineer\", prompt=\"You are the vector-db-engineer agent. Configure $ARGUMENTS for RAG pipeline.\n\nInstall dependencies based on detected language (Python/Node.js):\n- pgvector: psycopg2-binary+pgvector or pg+pgvector\n- Chroma/Pinecone/Weaviate/Qdrant: respective client libraries\n- FAISS: faiss-cpu or faiss-gpu (Python only)\n\nCreate config/vector_db.py or config/vector-db.ts with connection params, dimensions (default 1536), distance metric.\n\nCreate schema/collection setup script for chosen database.\n\nAdd environment variables to .env.example (DATABASE_URL, API keys, etc).\n\nCreate scripts/test_vector_db script to verify connectivity and vector operations.\n\nUse fetched docs for latest patterns.\")\n\nPhase 5: Connectivity Test\nGoal: Verify the database configuration works\n\nActions:\n- Prompt user to configure environment variables if needed\n- Run the test script created in Phase 4\n- For pgvector with Supabase: Optionally use mcp__supabase tool to verify connection\n- Display test results (success/failure)\n- If failures occur, provide troubleshooting guidance\n- Verify vector operations work (insert test vector, query similarity)\n\nPhase 6: Cost & Usage Summary\nGoal: Inform user about pricing and free tier limits\n\nActions:\nDisplay summary for chosen database:\n\nFree Options:\n- pgvector: Free (uses Postgres), Supabase: 500MB free, production-ready\n- Chroma: Free/OSS, embedded or server, good for dev/medium datasets\n- FAISS: Free/OSS, in-memory, high-performance, custom persistence needed\n\nManaged Services:\n- Pinecone: Free tier 100K vectors, paid pricing starts at low cost per GB/month, fully managed\n- Weaviate: 14-day sandbox free, paid clusters from USD 25/month, or self-host OSS\n- Qdrant: 1GB free cluster (1M vectors), paid from USD 25/month for 2GB, or self-host OSS\n\nNext Steps:\n- Insert embeddings, perform similarity search, update/delete vectors\n- Monitor performance and optimize\n- Link to relevant documentation\n\nNotes:\n- Check existing config before overwriting\n- Use proper error handling and connection pooling\n- Recommend pgvector for cost-conscious, Chroma for dev, managed for production\n- Test vector operations before complete"
              },
              {
                "name": "/build-generation",
                "description": "Build RAG generation pipeline with streaming support",
                "path": "plugins/rag-pipeline/commands/build-generation.md",
                "frontmatter": {
                  "description": "Build RAG generation pipeline with streaming support",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Build complete RAG generation pipeline with retrieval, LLM generation, and streaming support\n\nCore Principles:\n- Verify retrieval pipeline exists before proceeding\n- Fetch latest documentation for chosen LLM provider\n- Highlight FREE Groq option for cost-effective development\n- Implement streaming for real-time responses\n- Manage context windows intelligently\n\nPhase 1: Environment Verification\nGoal: Ensure retrieval pipeline is ready and gather requirements\n\nActions:\n- Create todo list using TodoWrite\n- Check for existing retrieval components: !{bash find . -name \"*retrieval*\" -o -name \"*embedding*\" -o -name \"*index*\" 2>/dev/null | head -10}\n- Verify vector store exists: !{bash test -f *.index -o -d chroma_db -o -d faiss_index && echo \"found\" || echo \"missing\"}\n- If missing, warn: \"Retrieval pipeline not found. Run build-retrieval first or /rag-pipeline:init\"\n- AskUserQuestion: \"LLM provider? (OpenAI/Anthropic/Groq-FREE/Ollama/Multiple)\"\n- Record provider choice for documentation fetch\n\nPhase 2: Documentation Loading\nGoal: Fetch latest RAG and LLM provider documentation\n\nActions:\nUse WebFetch to load documentation in parallel:\n- https://docs.llamaindex.ai/en/stable/examples/query_engine/\n- https://python.langchain.com/docs/tutorials/rag/\n- https://platform.openai.com/docs/guides/text-generation\n- https://console.groq.com/docs/quickstart\n- https://docs.anthropic.com/claude/reference/streaming\n\nWait for all to complete. Update todos.\n\nPhase 3: RAG Pipeline Generation\nGoal: Build complete generation pipeline with retrieval integration\n\nActions:\n\nTask(description=\"Build RAG generation pipeline\", subagent_type=\"rag-pipeline:llamaindex-specialist\", prompt=\"You are the llamaindex-specialist agent. Build a complete RAG generation pipeline with streaming support based on fetched documentation.\n\nProvider: $ARGUMENTS (or from user question)\n\nImplementation:\n\n1. Core Pipeline (src/generation/):\n   - query_engine.py: load_retriever(), retrieve_context(), format_prompt(), generate_response(), generate_stream()\n   - llm_client.py: OpenAI/Anthropic/Groq/Ollama clients with streaming\n   - context_manager.py: count_tokens(), truncate_context(), prioritize_chunks(), sliding_window()\n\n2. Utilities (src/generation/utils/):\n   - prompt_templates.py: rag_system_prompt, citation_template, conversation_template\n   - response_parser.py: extract_citations(), format_markdown(), validate_response()\n\n3. Streaming (src/generation/):\n   - async_generator.py: stream_response(), buffer_tokens(), handle_errors()\n   - sse_formatter.py: format_sse(), heartbeat()\n\n4. API (api/endpoints/rag.py):\n   - POST /rag/query, POST /rag/stream, GET /rag/sources\n\n5. Config (config/generation.py + .env):\n   - LLM_PROVIDER, MODEL_NAME, MAX_TOKENS, TEMPERATURE, CONTEXT_WINDOW, TOP_K_CHUNKS\n   - OPENAI_API_KEY, ANTHROPIC_API_KEY, GROQ_API_KEY (FREE!), OLLAMA_BASE_URL\n\n6. Testing:\n   - tests/test_generation.py: Unit tests for all components\n   - tests/test_e2e_rag.py: Full pipeline, citations, context overflow\n\n7. Examples: simple_rag.py, streaming_rag.py, multi_turn_rag.py\n\n8. Docs: generation.md, streaming.md, providers.md, README.md update\n\nBest Practices: async/await, retry logic, caching, token logging, rate limits, citation validation, edge case testing\n\nGroq FREE: 30 req/min, fastest inference, OpenAI-compatible, llama-3.1-70b/mixtral-8x7b\n\nDeliverable: Complete RAG generation pipeline with streaming, context management, and provider flexibility.\")\n\nPhase 4: Validation\nGoal: Verify generation pipeline works end-to-end\n\nActions:\n- Verify structure: !{bash ls -la src/generation/}\n- Check LLM client: !{bash test -f src/generation/llm_client.py && echo \"exists\"}\n- Check streaming: !{bash test -f src/generation/async_generator.py && echo \"exists\"}\n- Validate Python syntax: !{bash python3 -m py_compile src/generation/*.py 2>&1 | head -20}\n- Test imports: !{bash python3 -c \"from src.generation.query_engine import generate_response; print('OK')\" 2>&1}\n- Check examples: !{bash ls examples/*rag*.py 2>/dev/null}\n- Update todos marking validation complete\n\nPhase 5: End-to-End Test\nGoal: Run complete RAG query to verify integration\n\nActions:\n- Run simple RAG example: !{bash cd . && python3 examples/simple_rag.py 2>&1 | head -50}\n- If successful, show sample output\n- If errors, display for debugging\n- Test streaming if available: !{bash python3 examples/streaming_rag.py 2>&1 | head -30}\n- Update todos\n\nPhase 6: Summary\nGoal: Present setup information and next steps\n\nActions:\nDisplay:\n- RAG Generation Pipeline created\n- Provider: [from user selection]\n- Streaming: Enabled\n- Context management: Configured\n- Key files: query_engine.py, llm_client.py, context_manager.py\n\nNext Steps:\n1. Configure API key in .env:\n   - Groq (FREE): Get key at https://console.groq.com\n   - OpenAI: https://platform.openai.com/api-keys\n   - Anthropic: https://console.anthropic.com\n2. pip install -r requirements.txt\n3. Test basic RAG: python examples/simple_rag.py\n4. Test streaming: python examples/streaming_rag.py\n5. Integrate with API: Add to FastAPI endpoints\n6. Monitor token usage and costs\n\nPerformance Tips:\n- Use Groq for fast free inference\n- Cache frequent queries\n- Limit context chunks (top_k=3-5)\n- Stream for better UX\n- Monitor context window usage\n\nMark all todos complete.\n\nResources:\n- LlamaIndex Query: https://docs.llamaindex.ai/en/stable/examples/query_engine/\n- LangChain RAG: https://python.langchain.com/docs/tutorials/rag/\n- Groq Console: https://console.groq.com\n- OpenAI API: https://platform.openai.com/docs"
              },
              {
                "name": "/build-ingestion",
                "description": "Build document ingestion pipeline (load, parse, chunk, embed, store)",
                "path": "plugins/rag-pipeline/commands/build-ingestion.md",
                "frontmatter": {
                  "description": "Build document ingestion pipeline (load, parse, chunk, embed, store)",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Build complete document ingestion pipeline with loading, parsing, chunking, embedding, and vector storage\n\nCore Principles:\n- Detect existing configurations\n- Fetch minimal documentation\n- Generate unified ingestion script with batch processing, error handling, and progress tracking\n\nPhase 1: Discovery and Documentation\nGoal: Understand RAG infrastructure and fetch ingestion docs\n\nActions:\n- Detect project: Check for package.json, requirements.txt, pyproject.toml\n- Load configs: @config.yaml, @.env for chunking, embedding, vector DB settings\n- Example: !{bash grep -r \"chunk_size\\|embedding\\|vector\" . --include=\"*.py\" --include=\"*.json\" --include=\"*.yaml\" 2>/dev/null | head -10}\n\nFetch docs in parallel:\n1. WebFetch: https://docs.llamaindex.ai/en/stable/understanding/loading/loading/\n2. WebFetch: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/\n3. WebFetch: https://python.langchain.com/docs/modules/data_connection/document_loaders/\n4. WebFetch: https://python.langchain.com/docs/modules/data_connection/document_transformers/\n\nPhase 2: Implementation\nGoal: Generate unified ingestion pipeline with all stages\n\nActions:\n\nTask(description=\"Build ingestion pipeline\", subagent_type=\"rag-pipeline:document-processor\", prompt=\"You are the document-processor agent. Build complete document ingestion pipeline for $ARGUMENTS.\n\nContext: Detected project type, configs (chunking, embedding, vector DB) from Phase 1\nDocumentation: LlamaIndex ingestion, LangChain loaders/transformers fetched\n\nRequirements:\nCreate ingestion.py with 6 stages:\n1. Load documents (PDF, DOCX, TXT, MD, HTML support)\n2. Parse and extract text + metadata\n3. Chunk/split with configurable size and overlap\n4. Generate embeddings in batches\n5. Store vectors in database with metadata\n6. Verify ingestion success\n\nFeatures:\n- Batch processing for large document sets\n- Retry logic with exponential backoff\n- Progress tracking (tqdm/logging)\n- Error logging with failed document tracking\n- Resume capability for interrupted runs\n- Metadata preservation (source, page, timestamps)\n- CLI interface (argparse/typer)\n- Type hints and docstrings\n- Config loading from .env/config file\n\nDeliverables:\n- ingestion.py or ingestion_pipeline.py\n- config.yaml/.env template\n- test_ingestion.py with validation\n- Usage documentation\")\n\nPhase 3: Testing and Verification\nGoal: Set up testing infrastructure and verify pipeline\n\nActions:\n- Create test_data/ directory with sample document\n- Example: !{bash mkdir -p test_data && echo \"Sample test document\" > test_data/sample.txt}\n- Verify ingestion script exists: !{bash ls -la ingestion*.py 2>/dev/null}\n- Check imports compile: !{bash python -m py_compile ingestion*.py 2>/dev/null || echo \"Check imports manually\"}\n- List all created files (ingestion.py, config template, test script)\n\nPhase 4: Summary\nGoal: Display usage instructions\n\nActions:\nSummary:\n- Files: ingestion.py, config.yaml/.env, test_ingestion.py, test_data/\n- Capabilities: Multi-format support, batch processing, error handling, progress tracking, resume capability\n- Usage:\n  1. Configure API keys and vector DB credentials\n  2. Run: python ingestion.py --source ./documents\n  3. Test: python test_ingestion.py\n- Next steps: Add documents, configure credentials, run test ingestion, consider /rag-pipeline:build-retrieval\n\nImportant Notes:\n- Adapts to LlamaIndex or LangChain\n- Production-ready with error handling and batch processing"
              },
              {
                "name": "/build-retrieval",
                "description": "Build retrieval pipeline (simple, hybrid, rerank)",
                "path": "plugins/rag-pipeline/commands/build-retrieval.md",
                "frontmatter": {
                  "description": "Build retrieval pipeline (simple, hybrid, rerank)",
                  "argument-hint": [
                    "retrieval-type"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Build a production-ready retrieval pipeline with support for simple semantic search, hybrid search, and reranking capabilities.\n\nCore Principles:\n- Ask clarifying questions to understand requirements\n- Fetch minimal documentation (3-4 URLs)\n- Implement retrieval strategy based on user needs\n- Add metadata filtering and query optimization\n- Validate with sample queries\n\nPhase 1: Discovery\nGoal: Understand retrieval requirements and existing setup\n\nActions:\n- Parse $ARGUMENTS to check if retrieval type is specified\n- Detect existing RAG setup: Check for vector database config, existing embeddings\n- Load existing configuration: @config files, @embedding setup\n- Identify available vector store: Pinecone, Chroma, PGVector, etc.\n\nIf retrieval type is not clear from $ARGUMENTS, use AskUserQuestion to gather:\n- What retrieval strategy do you need?\n  * Simple semantic search (vector similarity only)\n  * Hybrid search (vector + keyword/BM25)\n  * With reranking (cross-encoder or LLM-based)\n- Do you need metadata filtering? (filter by date, category, source, etc.)\n- What's your expected query complexity? (simple questions vs complex analytical queries)\n- Which framework are you using? (LlamaIndex, LangChain, or both)\n\nPhase 2: Load Retrieval Documentation\nGoal: Fetch framework-specific retrieval docs\n\nActions:\nFetch these docs in parallel (4 URLs max):\n\n1. WebFetch: https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/\n2. WebFetch: https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_vs_recursive_retriever/\n3. WebFetch: https://python.langchain.com/docs/modules/data_connection/retrievers/\n4. WebFetch: https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble/ (if hybrid search)\n\nPhase 3: Generate Retrieval Configuration\nGoal: Create retrieval pipeline configuration\n\nActions:\n\nInvoke the **general-purpose** agent to implement retrieval configuration:\n\nThe agent should:\n- Create retrieval configuration based on selected strategy\n- For simple semantic: Configure VectorStoreRetriever with similarity settings\n- For hybrid: Set up ensemble retriever combining vector + BM25/keyword search\n- For reranking: Add reranker model (Cohere, cross-encoder, or LLM-based)\n- Configure retrieval parameters: top_k, similarity_threshold, alpha (for hybrid)\n- Add error handling and fallback strategies\n\nProvide the agent with:\n- Context: Detected vector store and framework\n- Strategy: Simple, hybrid, or reranking based on user input\n- Framework preference: LlamaIndex, LangChain, or both\n- Expected output: Retrieval configuration file with proper imports\n\nPhase 4: Implement Query Optimization\nGoal: Add query preprocessing and optimization\n\nActions:\n\nContinue with the **general-purpose** agent:\n\nThe agent should:\n- Add query transformation layer: Expand, decompose, or rephrase queries\n- Implement query routing: Route to appropriate retrieval strategy\n- Add hypothetical document embeddings (HyDE) if beneficial\n- Configure query caching to avoid redundant retrievals\n- Add logging and telemetry for query analysis\n- Include examples of query optimization patterns\n\nPhase 5: Add Metadata Filtering Support\nGoal: Enable filtering by metadata fields\n\nActions:\n\nContinue with the **general-purpose** agent:\n\nThe agent should:\n- Define metadata schema: date ranges, categories, sources, custom fields\n- Implement filter builders for common patterns\n- Add pre-filter and post-filter strategies\n- Support complex filter logic: AND, OR, NOT operations\n- Create filter validation utilities\n- Add examples showing metadata filtering usage\n\nPhase 6: Test Retrieval Quality\nGoal: Validate retrieval with sample queries\n\nActions:\n\nCreate test file with sample queries:\n- Simple factual questions\n- Complex analytical queries\n- Edge cases (no results, ambiguous queries)\n- Queries requiring metadata filtering\n\nInvoke the **general-purpose** agent to create test suite:\n\nThe agent should:\n- Create retrieval testing script\n- Run sample queries through the pipeline\n- Measure retrieval metrics: precision@k, recall@k, MRR\n- Test metadata filtering works correctly\n- Verify reranking improves results (if enabled)\n- Generate retrieval quality report\n\nPhase 7: Summary\nGoal: Document the retrieval pipeline\n\nActions:\nProvide comprehensive summary:\n- Retrieval strategy implemented (simple/hybrid/reranking)\n- Configuration files created\n- Query optimization features added\n- Metadata filtering capabilities\n- Test results and quality metrics\n- Files modified/created\n- Example usage code showing:\n  * Basic retrieval\n  * Retrieval with metadata filters\n  * Query optimization usage\n  * Reranking (if enabled)\n- Performance tuning recommendations\n- Next steps: Consider adding /rag-pipeline:optimize-chunks for better chunking\n\nImportant Notes:\n- Adapts to existing vector store setup\n- Supports both LlamaIndex and LangChain\n- Fetches minimal docs (4 URLs)\n- Uses general-purpose agent for all implementation\n- Includes quality testing and validation\n- Focused on retrieval pipeline only"
              },
              {
                "name": "/deploy",
                "description": "Deploy RAG application to production platforms",
                "path": "plugins/rag-pipeline/commands/deploy.md",
                "frontmatter": {
                  "description": "Deploy RAG application to production platforms",
                  "argument-hint": [
                    "platform"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Deploy RAG application to production platforms with platform-specific configuration and health validation.\n\nCore Principles:\n- Verify RAG pipeline readiness before deployment\n- Fetch latest platform documentation\n- Generate platform-specific deployment files\n- Validate deployment with automated health checks\n\nPhase 1: Pre-Deployment Verification\nGoal: Ensure RAG application is complete and ready\n\nActions:\n- Create deployment tracking: TodoWrite\n- Check project structure: !{bash ls -la src/ config/ requirements.txt 2>/dev/null || echo \"missing\"}\n- Verify RAG pipeline: !{bash test -f src/rag_pipeline.py -o -f src/generation/query_engine.py && echo \"ready\" || echo \"incomplete\"}\n- Check API exists: !{bash find . -name \"*api*\" -o -name \"app.py\" 2>/dev/null | head -3}\n- If incomplete, warn: \"Run /rag-pipeline:init or build commands first\"\n- Parse $ARGUMENTS for platform override\n- AskUserQuestion: \"Platform? (1) DigitalOcean, (2) Vercel, (3) HuggingFace Spaces, (4) Google Colab\"\n- Record selection for documentation fetch\n\nPhase 2: Documentation Loading\nGoal: Fetch platform deployment guides\n\nActions:\nUse WebFetch to load documentation:\n- https://docs.digitalocean.com/products/app-platform/\n- https://vercel.com/docs\n- https://huggingface.co/docs/hub/spaces\n- https://fastapi.tiangolo.com/deployment/\n\nWait for completion. Update todos.\n\nPhase 3: Deployment Configuration\nGoal: Generate platform-specific configs and API endpoints\n\nActions:\n\nTask(description=\"Generate deployment files\", subagent_type=\"rag-pipeline:rag-deployment-agent\", prompt=\"You are the rag-deployment-agent. Generate deployment configuration for RAG application.\n\nPlatform: $ARGUMENTS (from user selection)\n\nFiles to Create:\n\n**DigitalOcean:**\n- .do/app.yaml: Service spec with health checks, environment, routes\n- Dockerfile: Multi-stage build (python:3.11-slim base)\n- .dockerignore: venv/, .git/, __pycache__/, *.pyc, data/raw/\n- scripts/deploy_do.sh: doctl app create/update commands\n\n**Vercel:**\n- vercel.json: Routes, builds, environment variables\n- api/index.py: Serverless function wrapper for RAG endpoint\n- .vercelignore: Exclude venv/, data/\n\n**HuggingFace:**\n- app.py: Gradio interface with RAG query box and response display\n- requirements.txt: Ensure HF-compatible (no heavy deps)\n- README.md: Space card with usage instructions\n\n**Colab:**\n- notebooks/deploy_demo.ipynb: Complete RAG demo with setup cells\n- README_COLAB.md: Badge and instructions\n\n**Universal (all platforms):**\n- api/main.py (if not exists): FastAPI with POST /api/query, GET /health, GET /docs\n- .env.production.example: OPENAI_API_KEY, GROQ_API_KEY, VECTOR_DB_PATH, CORS_ORIGINS\n- health/check.py: check_vector_db(), check_llm(), check_embeddings()\n- docs/deployment.md: Platform instructions, domain setup, monitoring, troubleshooting\n\nBest Practices: Environment variables for secrets, rate limiting, CORS configuration, async endpoints, error handling, deployment validation.\n\nGroq Recommendation: FREE API with 30 req/min, fastest inference, perfect for RAG deployments.\n\nDeliverable: Complete platform-ready deployment configuration.\")\n\nPhase 4: Environment Setup\nGoal: Configure production environment variables\n\nActions:\n- Check .env.production: !{bash test -f .env.production && echo \"exists\" || echo \"needed\"}\n- List required vars: !{bash grep -E \"^[A-Z_]+=\" .env.example 2>/dev/null | cut -d= -f1}\n- Display platform-specific instructions:\n  - DigitalOcean: App Platform > Settings > Environment Variables\n  - Vercel: Dashboard > Settings > Environment Variables or vercel env add\n  - HuggingFace: Space > Settings > Repository Secrets\n  - Colab: Use Colab Secrets or getpass()\n- Update todos\n\nPhase 5: Deploy\nGoal: Execute platform deployment\n\nActions:\nExecute based on platform:\n\n**DigitalOcean:**\n!{bash command -v doctl && echo \"ready\" || echo \"install: brew install doctl\"}\n!{bash bash scripts/deploy_do.sh 2>&1 | tee deploy.log}\n\n**Vercel:**\n!{bash command -v vercel && echo \"ready\" || echo \"install: npm i -g vercel\"}\n!{bash vercel --prod 2>&1 | tee deploy.log}\n\n**HuggingFace:**\n!{bash git remote -v | grep -q huggingface && echo \"configured\" || echo \"add remote\"}\n!{bash git push huggingface main 2>&1 | tee deploy.log}\n\n**Colab:**\nDisplay notebook path and Colab open link (no deployment)\n\nExtract deployment URL from logs. Update todos.\n\nPhase 6: Health Validation\nGoal: Verify deployment is functional\n\nActions:\n- Wait for startup: !{bash sleep 10}\n- Test health: !{bash curl -f [deployment-url]/health 2>&1}\n- Test RAG query: !{bash curl -X POST [deployment-url]/api/query -H \"Content-Type: application/json\" -d '{\"query\":\"test\"}' 2>&1}\n- Verify response format and status codes\n- Update todos with results\n\nPhase 7: Summary\nGoal: Display deployment info and next steps\n\nActions:\nDisplay:\n- Status: SUCCESS/FAILED\n- Platform: [selected]\n- Endpoint: https://[url]\n- Health: /health\n- API Docs: /docs\n- Logs: deploy.log\n\nAPI Usage:\n```bash\ncurl -X POST https://[url]/api/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"Your question\"}'\n```\n\nNext Steps:\n1. Test at https://[url]/docs (Swagger UI)\n2. Configure custom domain (platform settings)\n3. Setup monitoring and alerts\n4. Add authentication/API keys\n5. Implement rate limiting\n6. Monitor costs and usage\n\nTroubleshooting:\n- Check logs: [platform command]\n- Verify environment variables\n- Test vector DB connection\n- Validate API keys\n\nResources:\n- DigitalOcean: https://cloud.digitalocean.com/apps\n- Vercel: https://vercel.com/dashboard\n- HuggingFace: https://huggingface.co/spaces\n- Groq (FREE): https://console.groq.com\n\nMark todos complete.\n\nDeployment live at: https://[endpoint-url]"
              },
              {
                "name": "/init",
                "description": "Initialize RAG project with framework selection (LlamaIndex/LangChain)",
                "path": "plugins/rag-pipeline/commands/init.md",
                "frontmatter": {
                  "description": "Initialize RAG project with framework selection (LlamaIndex/LangChain)",
                  "argument-hint": [
                    "project-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Initialize complete RAG project with user-selected framework, directory structure, dependencies, and starter code.\n\nCore Principles:\n- Ask user for framework preference early\n- Create clean project structure\n- Install dependencies correctly\n- Provide documentation links and next steps\n\nPhase 1: Discovery and Framework Selection\nGoal: Determine project path and gather user preferences\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory if empty)\n- Check if path exists: !{bash test -d \"$ARGUMENTS\" && echo \"exists\" || echo \"not-found\"}\n- Create directory if needed: !{bash mkdir -p \"$ARGUMENTS\"}\n- Check existing setup: !{bash ls \"$ARGUMENTS\"/requirements.txt \"$ARGUMENTS\"/venv 2>/dev/null || echo \"new-project\"}\n- Use AskUserQuestion: \"Which RAG framework? (1) LlamaIndex, (2) LangChain, (3) Both\"\n- If Both selected, ask: \"Vector database? (Chroma/Pinecone/FAISS)\"\n\nPhase 2: Project Setup\nGoal: Create directory structure and install dependencies\n\nActions:\n- Create directories: !{bash mkdir -p \"$ARGUMENTS\"/{data/{raw,processed},scripts,config,notebooks,tests,src}}\n- Create venv: !{bash cd \"$ARGUMENTS\" && python3 -m venv venv}\n- Install based on selection:\n  - LlamaIndex: !{bash cd \"$ARGUMENTS\" && source venv/bin/activate && pip install llama-index python-dotenv}\n  - LangChain: !{bash cd \"$ARGUMENTS\" && source venv/bin/activate && pip install langchain langchain-community python-dotenv}\n  - Both: !{bash cd \"$ARGUMENTS\" && source venv/bin/activate && pip install llama-index langchain langchain-community python-dotenv}\n- Install vector DB if selected: !{bash cd \"$ARGUMENTS\" && source venv/bin/activate && pip install chromadb}\n- Generate requirements: !{bash cd \"$ARGUMENTS\" && source venv/bin/activate && pip freeze > requirements.txt}\n\nPhase 3: Code Generation\nGoal: Generate framework-specific starter code and documentation\n\nActions:\n\nTask(description=\"Generate RAG starter code\", subagent_type=\"rag-pipeline:rag-architect\", prompt=\"You are the rag-architect agent. Generate starter code for RAG project at $ARGUMENTS.\n\nFramework: [User's selection from Phase 1]\nVector DB: [User's selection from Phase 1]\n\nCreate these files:\n\n1. config/.env.example - API key placeholders (OPENAI_API_KEY, etc), vector DB config\n2. src/rag_pipeline.py - Main RAG pipeline with document loading, chunking, embeddings, vector store, query functions\n3. scripts/index_documents.py - Index documents from data/raw/\n4. scripts/query_example.py - Example query script\n5. notebooks/rag_demo.ipynb - Step-by-step RAG demo notebook\n6. README.md with:\n   - Setup instructions\n   - Documentation links:\n     * LlamaIndex: https://developers.llamaindex.ai/python/framework/\n     * LlamaIndex Getting Started: https://developers.llamaindex.ai/python/framework/getting_started/reading\n     * LangChain: https://python.langchain.com/docs/\n     * LangChain RAG Tutorial: https://python.langchain.com/docs/tutorials/rag/\n   - Project structure explanation\n   - Next steps guide\n7. .gitignore - Exclude venv/, .env, __pycache__/, *.pyc, .ipynb_checkpoints/, data/\n\nUse framework best practices, type hints, comprehensive comments, error handling.\n\nExpected output: All files created with working code.\")\n\nPhase 4: Git Initialization and Summary\nGoal: Initialize git repository and display summary\n\nActions:\n- Check git status: !{bash cd \"$ARGUMENTS\" && git status 2>&1 | grep -q \"not a git repository\" && echo \"no-git\" || echo \"exists\"}\n- If no git, initialize: !{bash cd \"$ARGUMENTS\" && git init}\n- Stage files: !{bash cd \"$ARGUMENTS\" && git add .}\n- Create commit: !{bash cd \"$ARGUMENTS\" && git commit -m \"Initial RAG project setup\n\nFramework: [selected framework]\nVector DB: [selected vector DB]\n\nGenerated with Claude Code\nCo-Authored-By: Claude <noreply@anthropic.com>\"}\n- Display summary:\n  - Project location: $ARGUMENTS\n  - Framework and vector DB selections\n  - Files created\n  - Next steps: cd $ARGUMENTS, source venv/bin/activate, configure .env, review README.md\n  - Documentation links for reference"
              },
              {
                "name": "/optimize",
                "description": "Optimize RAG performance and reduce costs",
                "path": "plugins/rag-pipeline/commands/optimize.md",
                "frontmatter": {
                  "description": "Optimize RAG performance and reduce costs",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Analyze and optimize RAG pipeline performance, reduce latency, improve accuracy, and minimize costs through intelligent caching, batching, and parameter tuning.\n\nCore Principles:\n- Analyze current performance metrics and identify bottlenecks\n- Optimize chunk sizes and overlap based on content type\n- Tune vector index parameters (HNSW, IVF) for speed/accuracy tradeoff\n- Implement caching strategies to reduce redundant operations\n- Apply cost reduction techniques (batch processing, cheaper embeddings)\n\nPhase 1: Performance Analysis\nGoal: Understand current RAG pipeline performance and identify optimization opportunities\n\nActions:\n- Parse $ARGUMENTS for specific optimization focus areas\n- Detect RAG setup: !{bash find . -name \"*config*\" -o -name \"*vector*\" -o -name \"*retrieval*\" 2>/dev/null | head -10}\n- Check for metrics: !{bash grep -r \"latency\\|cost\\|performance\" . --include=\"*.py\" --include=\"*.ts\" 2>/dev/null | head -10}\n- Identify bottlenecks: chunking strategy, embedding model, vector index config, query latency, cache hit rates, API costs\n\nIf optimization goals are unclear from $ARGUMENTS, use AskUserQuestion:\n- \"What are your main concerns? (latency/accuracy/cost/throughput)\"\n- \"Current average query latency?\"\n- \"Monthly API/embedding cost?\"\n- \"Documents in your index?\"\n- \"Framework? (LlamaIndex/LangChain/both)\"\n\nPhase 2: Load Optimization Documentation\nGoal: Fetch framework-specific optimization guides\n\nActions:\nFetch these docs in parallel:\n\n1. WebFetch: https://docs.llamaindex.ai/en/stable/optimizing/production_rag/\n2. WebFetch: https://python.langchain.com/docs/guides/productionization/\n3. WebFetch: https://www.pinecone.io/learn/hnsw/ (if using HNSW index)\n4. WebFetch: https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/\n\nPhase 3: Parallel Optimization\nGoal: Run independent optimization tasks in parallel for maximum efficiency\n\nActions:\n\nTask(description=\"Optimize chunk sizes\", subagent_type=\"rag-pipeline:document-processor\", prompt=\"You are the document-processor agent. Optimize the chunking strategy for a RAG pipeline.\n\nUsing the fetched optimization docs:\n- Analyze current chunk_size and chunk_overlap\n- Test different sizes based on content type (code: 512-1024, docs: 256-512, articles: 512-1024)\n- Consider semantic vs fixed-size chunking\n- Implement adaptive chunking based on content structure\n- Generate chunk size recommendation report\n\nDeliverable: Optimized chunking configuration with analysis report\")\n\nTask(description=\"Optimize vector index\", subagent_type=\"rag-pipeline:vector-db-engineer\", prompt=\"You are the vector-db-engineer agent. Optimize the vector index configuration for a RAG pipeline.\n\nUsing the fetched documentation, tune index parameters based on detected database:\n\nHNSW indexes (Pinecone, Chroma, Qdrant):\n- Adjust ef_construction (default 200, increase for accuracy)\n- Tune M parameter (connections per node, 16-64)\n- Optimize ef_search at query time\n\nIVF indexes (FAISS):\n- Optimize nlist (number of clusters)\n- Tune nprobe at query time\n- Consider PQ (product quantization)\n\nPGVector:\n- Optimize lists parameter for IVF\n- Configure probes for query time\n- Add indexes on metadata columns\n\nDeliverable: Optimized index configuration with before/after benchmarks\")\n\nTask(description=\"Optimize query processing\", subagent_type=\"rag-pipeline:retrieval-optimizer\", prompt=\"You are the retrieval-optimizer agent. Optimize query processing and retrieval for a RAG pipeline.\n\nUsing fetched docs, implement these optimizations:\n\nQuery Caching:\n- Semantic cache for similar queries (0.95+ similarity)\n- Result caching with TTL\n- Redis or in-memory cache\n- Track cache hit rates\n\nQuery Optimization:\n- Query preprocessing: normalization, expansion, decomposition\n- Query routing to appropriate retrieval strategies\n- Optimize top_k parameter (fetch fewer, rerank if needed)\n- Add early stopping for high-confidence results\n\nBatch Processing:\n- Batch embedding requests to reduce API calls\n- Async processing for parallel queries\n- Request deduplication\n- Embedding model caching\n\nDeliverable: Query optimization with caching layer and performance metrics\")\n\nTask(description=\"Reduce RAG costs\", subagent_type=\"rag-pipeline:retrieval-optimizer\", prompt=\"You are the retrieval-optimizer agent. Implement cost reduction strategies for a RAG pipeline.\n\nUsing fetched docs, apply these cost optimizations:\n\nEmbedding Cost Reduction:\n- Use cheaper models where appropriate (text-embedding-3-small)\n- Implement embedding caching (never re-embed same content)\n- Batch embed requests\n- Consider local models (sentence-transformers) for dev\n\nLLM Cost Reduction:\n- Reduce context window (fetch only top-k relevant chunks)\n- Implement context compression (extract key sentences)\n- Use cheaper models for simple queries (GPT-3.5-turbo vs GPT-4)\n- Enable streaming to reduce timeout costs\n\nInfrastructure:\n- Use free tiers: Chroma (free local), HuggingFace (free embeddings), Groq (free LLM tier)\n- Cache frequently accessed documents\n- Implement request deduplication\n- Use batch processing for bulk operations\n\nDeliverable: Cost reduction implementation with savings analysis\")\n\nWait for all parallel tasks to complete before proceeding to Phase 4.\n\nPhase 4: Results Aggregation\nGoal: Collect optimization results and generate comprehensive report\n\nActions:\n- Collect results from all optimization tasks\n- Aggregate performance improvements: !{bash cat *_optimization_results.json 2>/dev/null}\n- Calculate total improvements: latency reduction %, cost savings %, accuracy improvements\n- Identify remaining bottlenecks\n\nPhase 5: Optimization Report\nGoal: Create comprehensive optimization report with results and recommendations\n\nActions:\nDisplay summary:\n- **Chunking Optimization**: New chunk size [size], overlap [overlap], improvement [%]\n- **Vector Index Optimization**: Index params tuned, query latency reduced by [%]\n- **Query Optimization**: Cache hit rate [%], API calls reduced by [%]\n- **Cost Reduction**: Monthly cost reduced from $[before] to $[after] ([%] savings)\n- **Overall**: Latency [before]ms ‚Üí [after]ms, Cost [before] ‚Üí [after], Accuracy [before] ‚Üí [after]\n\nRecommendations:\n- Monitor cache hit rates and adjust cache size\n- Run A/B tests on chunk sizes with production queries\n- Set up alerts for latency spikes and cost overruns\n- Continue iterating on retrieval quality\n- Consider additional optimizations: hybrid search, reranking, query decomposition\n\nNext steps:\n- Test optimizations with production traffic: /rag-pipeline:test\n- Monitor performance metrics: /rag-pipeline:add-monitoring\n- Deploy optimized configuration: /rag-pipeline:deploy\n- Set up continuous optimization: schedule monthly reviews\n\nImportant Notes:\n- Runs 4 optimization tasks in parallel for efficiency\n- Uses fetched docs for latest best practices\n- Generates before/after comparisons\n- Provides actionable recommendations\n- Focuses on quick wins with high ROI"
              },
              {
                "name": "/test",
                "description": "Run comprehensive RAG pipeline tests",
                "path": "plugins/rag-pipeline/commands/test.md",
                "frontmatter": {
                  "description": "Run comprehensive RAG pipeline tests",
                  "argument-hint": [
                    "--coverage"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the rag-pipeline plugin:\n\n- **chunking-strategies**: Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.\n- **document-parsers**: Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.\n- **embedding-models**: Embedding model configurations and cost calculators\n- **langchain-patterns**: LangChain implementation patterns with templates, scripts, and examples for RAG pipelines\n- **llamaindex-patterns**: LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.\n- **retrieval-patterns**: Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.\n- **vector-database-configs**: Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers\n- **web-scraping-tools**: Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Execute comprehensive test suite for RAG pipeline including ingestion, embeddings, retrieval quality, end-to-end queries, performance benchmarking, cost analysis, and generate detailed test report.\n\nCore Principles:\n- Run independent test suites in parallel for speed\n- Measure quality metrics (precision, recall, latency)\n- Track costs (API calls, embeddings, storage)\n- Generate actionable test reports\n\nPhase 1: Discovery and Setup\nGoal: Detect test infrastructure and prepare test environment\n\nActions:\n- Check test files: !{bash find . -name \"test_*.py\" -o -name \"*_test.py\" 2>/dev/null | head -20}\n- Detect framework: !{bash grep -l \"llama_index\\|langchain\" requirements.txt setup.py pyproject.toml 2>/dev/null}\n- Parse coverage flag: !{bash echo \"$ARGUMENTS\" | grep -q \"\\-\\-coverage\" && echo \"coverage-enabled\" || echo \"coverage-disabled\"}\n- Load test configs: @tests/config.yaml, @.env.test, @pytest.ini\n- Check vector DB: !{bash python -c \"import os; print(os.getenv('VECTOR_DB_URL', 'not-configured'))\" 2>/dev/null}\n\nPhase 2: Parallel Test Execution\nGoal: Run independent test suites in parallel for maximum speed\n\nActions:\n\nTask(description=\"Test ingestion pipeline\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run ingestion pipeline tests for $ARGUMENTS.\n\nTest scope: Document loading (PDF, DOCX, TXT, MD, HTML), parsing accuracy, metadata extraction, error handling for corrupted files, batch processing, resume capability\n\nTest approach: Create fixtures with sample documents, run ingestion on test_data/, verify document count and metadata preservation, test error scenarios, measure throughput (docs/sec), check logging\n\nDeliverable: ingestion_test_results.json with pass/fail status, throughput metrics, error logs\")\n\nTask(description=\"Test embedding generation\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run embedding generation tests for $ARGUMENTS.\n\nTest scope: Embedding model initialization, batch generation, vector dimension validation, consistency (same input = same output), API rate limiting and retry logic, embedding quality (semantic similarity)\n\nTest approach: Test with sample chunks, verify vector dimensions, test batch sizes (1, 10, 100 documents), measure speed (chunks/sec), test semantic similarity with cosine similarity, test rate limit handling\n\nDeliverable: embeddings_test_results.json with quality metrics, speed benchmarks, API call counts\")\n\nTask(description=\"Test retrieval quality\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run retrieval quality tests for $ARGUMENTS.\n\nTest scope: Precision (relevance of retrieved documents), recall (coverage of relevant documents), ranking quality, hybrid search (if enabled), filtering and metadata-based retrieval, query latency\n\nTest approach: Create golden dataset with queries plus expected documents, run retrieval for each query, calculate precision at k, recall at k, MRR, NDCG, test semantic vs keyword vs hybrid search, measure query latency (p50, p95, p99), test edge cases (empty queries, very long queries)\n\nDeliverable: retrieval_test_results.json with precision, recall, MRR, NDCG scores, latency percentiles\")\n\nTask(description=\"Test end-to-end RAG\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run end-to-end RAG query tests for $ARGUMENTS.\n\nTest scope: Complete RAG pipeline (retrieve plus generate), answer quality and factual accuracy, citation/source attribution, hallucination detection, response formatting, error handling (no results, API failures)\n\nTest approach: Define test questions with expected answers, run complete RAG pipeline, validate answer correctness (exact match, semantic similarity, LLM-as-judge), check source citations are included and accurate, test hallucination by verifying claims are grounded in retrieved docs, measure end-to-end latency, test error scenarios\n\nDeliverable: e2e_test_results.json with answer quality scores, citation accuracy, hallucination rate, latency\")\n\nTask(description=\"Performance benchmarking\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run performance benchmarks for $ARGUMENTS.\n\nTest scope: Ingestion throughput (docs/sec, MB/sec), embedding generation speed (chunks/sec), query latency (p50, p95, p99), concurrent query handling, vector DB performance, memory usage, scalability (100, 1K, 10K, 100K documents)\n\nTest approach: Run load tests with increasing document counts, measure throughput at each scale, test concurrent queries (1, 10, 50, 100 concurrent), profile memory usage during ingestion and querying, identify bottlenecks (embedding API, vector DB, LLM), generate performance charts\n\nDeliverable: performance_results.json with throughput, latency, memory metrics, scalability analysis\")\n\nTask(description=\"Cost analysis\", subagent_type=\"rag-pipeline:rag-tester\", prompt=\"You are the rag-tester agent. Run cost analysis for $ARGUMENTS.\n\nTest scope: Embedding API costs (per document, per query), LLM generation costs (per query), vector DB storage costs, total cost projections, cost optimization opportunities\n\nTest approach: Track API calls during test runs, calculate costs based on pricing (OpenAI embeddings, GPT-4, GPT-3.5), estimate monthly costs for different usage levels, identify cost reduction strategies (caching, batch processing, model selection), compare cost vs quality tradeoffs\n\nDeliverable: cost_analysis.json with per-operation costs, projections, optimization recommendations\")\n\nWait for all parallel tasks to complete before proceeding to Phase 3.\n\nPhase 3: Results Aggregation\nGoal: Collect all test results and generate comprehensive report\n\nActions:\n- Collect results: !{bash cat *_test_results.json cost_analysis.json 2>/dev/null}\n- Check failures: !{bash grep -l '\"status\": \"failed\"' *_test_results.json 2>/dev/null || echo \"all-passed\"}\n- Count tests: !{bash grep -oh '\"tests_run\": [0-9]*' *_test_results.json | awk '{sum+=$NF} END {print sum}' 2>/dev/null}\n- Pass rate: !{bash grep -oh '\"tests_passed\": [0-9]*\\|\"tests_run\": [0-9]*' *_test_results.json | awk 'NR%2{p=$NF;next} {print (p/$NF)*100\"%\"}' 2>/dev/null}\n- Coverage: !{bash if echo \"$ARGUMENTS\" | grep -q \"\\-\\-coverage\"; then pytest --cov=. --cov-report=html tests/ 2>&1 && echo \"Coverage report: htmlcov/index.html\"; fi}\n\nPhase 4: Test Report Generation\nGoal: Create comprehensive test report with results, metrics, and recommendations\n\nActions:\n- Create test_report.md with:\n  * Executive Summary (pass rate, total tests, execution time)\n  * Ingestion Results (throughput, error rate, supported formats)\n  * Embedding Results (speed, quality, API usage)\n  * Retrieval Quality (precision at 5, recall at 10, MRR, NDCG)\n  * E2E RAG Results (answer quality, hallucination rate, citation accuracy)\n  * Performance Benchmarks (latency percentiles, throughput at scale)\n  * Cost Analysis (per-query cost, monthly projections, optimization opportunities)\n  * Failed Tests (if any with error details)\n  * Recommendations (performance optimizations, cost reductions, quality improvements)\n  * Coverage Report Link (if coverage flag used)\n\nDisplay summary:\n- Total tests run and pass rate\n- Key metrics: retrieval precision, query latency, cost per query\n- Critical failures (if any)\n- Report location: test_report.md\n- Coverage report: htmlcov/index.html (if enabled)\n\nImportant Notes:\n- Tests run in parallel for speed\n- Quality metrics use golden datasets (create if missing)\n- Cost tracking uses actual pricing from providers\n- Performance tests scale based on available test data\n- Report includes actionable recommendations"
              }
            ],
            "skills": [
              {
                "name": "chunking-strategies",
                "description": "Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.",
                "path": "plugins/rag-pipeline/skills/chunking-strategies/SKILL.md",
                "frontmatter": {
                  "name": "chunking-strategies",
                  "description": "Document chunking implementations and benchmarking tools for RAG pipelines including fixed-size, semantic, recursive, and sentence-based strategies. Use when implementing document processing, optimizing chunk sizes, comparing chunking approaches, benchmarking retrieval performance, or when user mentions chunking, text splitting, document segmentation, RAG optimization, or chunk evaluation.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep, Edit"
                },
                "content": "# Chunking Strategies\n\n**Purpose:** Provide production-ready document chunking implementations, benchmarking tools, and strategy selection guidance for RAG pipelines.\n\n**Activation Triggers:**\n- Implementing document chunking for RAG\n- Optimizing chunk size and overlap\n- Comparing different chunking strategies\n- Benchmarking chunking performance\n- Processing different document types (markdown, code, PDFs)\n- Evaluating retrieval quality with different chunk strategies\n\n**Key Resources:**\n- `scripts/chunk-fixed-size.py` - Fixed-size chunking implementation\n- `scripts/chunk-semantic.py` - Semantic chunking with paragraph preservation\n- `scripts/chunk-recursive.py` - Recursive chunking for hierarchical documents\n- `scripts/benchmark-chunking.py` - Benchmark and compare chunking strategies\n- `templates/chunking-config.yaml` - Chunking configuration template\n- `templates/custom-splitter.py` - Template for custom chunking logic\n- `examples/chunk-markdown.py` - Markdown-specific chunking\n- `examples/chunk-code.py` - Source code chunking\n- `examples/chunk-pdf.py` - PDF document chunking\n\n## Chunking Strategy Overview\n\n### Strategy Selection Guide\n\n**Fixed-Size Chunking:**\n- Best for: Uniform documents, simple content, consistent structure\n- Pros: Fast, predictable, simple implementation\n- Cons: May split semantic units, no context awareness\n- Use when: Speed matters more than semantic coherence\n\n**Semantic Chunking:**\n- Best for: Natural language documents, articles, books\n- Pros: Preserves semantic boundaries, better context\n- Cons: Slower, variable chunk sizes\n- Use when: Content has clear paragraph/section structure\n\n**Recursive Chunking:**\n- Best for: Hierarchical documents, technical docs, code\n- Pros: Preserves structure, handles nested content\n- Cons: Most complex, requires structure detection\n- Use when: Documents have clear hierarchical organization\n\n**Sentence-Based Chunking:**\n- Best for: Q&A pairs, chatbots, precise retrieval\n- Pros: Natural boundaries, good for citations\n- Cons: Small chunks may lack context\n- Use when: Need precise attribution and citations\n\n## Implementation Scripts\n\n### 1. Fixed-Size Chunking\n\n**Script:** `scripts/chunk-fixed-size.py`\n\n**Usage:**\n```bash\npython scripts/chunk-fixed-size.py \\\n  --input document.txt \\\n  --chunk-size 1000 \\\n  --overlap 200 \\\n  --output chunks.json\n```\n\n**Parameters:**\n- `chunk-size`: Number of characters per chunk (default: 1000)\n- `overlap`: Character overlap between chunks (default: 200)\n- `split-on`: Split on sentences, words, or characters (default: sentences)\n\n**Best Practices:**\n- Use 500-1000 character chunks for most RAG applications\n- Set overlap to 10-20% of chunk size\n- Split on sentences for better coherence\n\n### 2. Semantic Chunking\n\n**Script:** `scripts/chunk-semantic.py`\n\n**Usage:**\n```bash\npython scripts/chunk-semantic.py \\\n  --input document.txt \\\n  --max-chunk-size 1500 \\\n  --output chunks.json\n```\n\n**How it works:**\n1. Detects natural boundaries (paragraphs, headings, line breaks)\n2. Groups content while respecting max chunk size\n3. Preserves semantic units (paragraphs stay together)\n4. Adds context headers for nested sections\n\n**Best for:** Articles, blog posts, documentation, books\n\n### 3. Recursive Chunking\n\n**Script:** `scripts/chunk-recursive.py`\n\n**Usage:**\n```bash\npython scripts/chunk-recursive.py \\\n  --input document.md \\\n  --chunk-size 1000 \\\n  --separators '[\"\\\\n## \", \"\\\\n### \", \"\\\\n\\\\n\", \"\\\\n\", \" \"]' \\\n  --output chunks.json\n```\n\n**How it works:**\n1. Tries to split on first separator (e.g., ## headings)\n2. If chunks still too large, recursively splits on next separator\n3. Continues until all chunks are within size limit\n4. Preserves hierarchical context\n\n**Separator hierarchy examples:**\n- **Markdown:** `[\"\\\\n## \", \"\\\\n### \", \"\\\\n\\\\n\", \"\\\\n\", \" \"]`\n- **Python:** `[\"\\\\nclass \", \"\\\\ndef \", \"\\\\n\\\\n\", \"\\\\n\", \" \"]`\n- **General:** `[\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \"]`\n\n**Best for:** Structured documents, source code, technical manuals\n\n### 4. Benchmark Chunking Strategies\n\n**Script:** `scripts/benchmark-chunking.py`\n\n**Usage:**\n```bash\npython scripts/benchmark-chunking.py \\\n  --input document.txt \\\n  --strategies fixed,semantic,recursive \\\n  --chunk-sizes 500,1000,1500 \\\n  --output benchmark-results.json\n```\n\n**Metrics Evaluated:**\n- **Processing time:** Speed of chunking\n- **Chunk count:** Total chunks generated\n- **Chunk size variance:** Consistency of chunk sizes\n- **Context preservation:** Semantic unit integrity (scored)\n- **Retrieval quality:** Simulated query performance\n\n**Output:**\n```json\n{\n  \"fixed-1000\": {\n    \"time_ms\": 45,\n    \"chunk_count\": 127,\n    \"avg_size\": 982,\n    \"size_variance\": 12.3,\n    \"context_score\": 0.72\n  },\n  \"semantic-1000\": {\n    \"time_ms\": 156,\n    \"chunk_count\": 114,\n    \"avg_size\": 1087,\n    \"size_variance\": 234.5,\n    \"context_score\": 0.91\n  }\n}\n```\n\n## Configuration Template\n\n**Template:** `templates/chunking-config.yaml`\n\n**Complete configuration:**\n```yaml\nchunking:\n  # Global defaults\n  default_strategy: semantic\n  default_chunk_size: 1000\n  default_overlap: 200\n\n  # Strategy-specific configs\n  strategies:\n    fixed_size:\n      chunk_size: 1000\n      overlap: 200\n      split_on: sentence  # sentence, word, character\n\n    semantic:\n      max_chunk_size: 1500\n      min_chunk_size: 200\n      preserve_paragraphs: true\n      add_headers: true  # Include section headers\n\n    recursive:\n      chunk_size: 1000\n      overlap: 100\n      separators:\n        markdown: [\"\\\\n## \", \"\\\\n### \", \"\\\\n\\\\n\", \"\\\\n\", \" \"]\n        code: [\"\\\\nclass \", \"\\\\ndef \", \"\\\\n\\\\n\", \"\\\\n\", \" \"]\n        text: [\"\\\\n\\\\n\", \". \", \" \"]\n\n  # Document type mappings\n  document_types:\n    \".md\": semantic\n    \".py\": recursive\n    \".txt\": fixed_size\n    \".pdf\": semantic\n```\n\n## Custom Splitter Template\n\n**Template:** `templates/custom-splitter.py`\n\n**Create your own chunking logic:**\n```python\nfrom typing import List, Dict\nimport re\n\nclass CustomChunker:\n    def __init__(self, chunk_size: int = 1000, overlap: int = 200):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n\n    def chunk(self, text: str, metadata: Dict = None) -> List[Dict]:\n        \"\"\"\n        Implement custom chunking logic here.\n\n        Returns:\n            List of chunks with metadata:\n            [\n                {\n                    \"text\": \"chunk content\",\n                    \"metadata\": {\n                        \"chunk_id\": 0,\n                        \"source\": \"document.txt\",\n                        \"start_char\": 0,\n                        \"end_char\": 1000\n                    }\n                }\n            ]\n        \"\"\"\n        chunks = []\n\n        # Your custom chunking logic here\n        # Example: Split on custom pattern\n        sections = self._split_sections(text)\n\n        for i, section in enumerate(sections):\n            chunks.append({\n                \"text\": section,\n                \"metadata\": {\n                    \"chunk_id\": i,\n                    \"source\": metadata.get(\"source\", \"unknown\"),\n                    \"chunk_size\": len(section)\n                }\n            })\n\n        return chunks\n\n    def _split_sections(self, text: str) -> List[str]:\n        # Implement your splitting logic\n        pass\n```\n\n## Document-Specific Examples\n\n### Markdown Chunking\n\n**Example:** `examples/chunk-markdown.py`\n\n**Features:**\n- Preserves heading hierarchy\n- Keeps code blocks together\n- Maintains list structure\n- Adds parent section context to chunks\n\n**Usage:**\n```bash\npython examples/chunk-markdown.py README.md --output readme-chunks.json\n```\n\n### Code Chunking\n\n**Example:** `examples/chunk-code.py`\n\n**Features:**\n- Splits on class and function boundaries\n- Preserves complete functions\n- Includes docstrings with implementations\n- Language-aware separator selection\n\n**Supported languages:** Python, JavaScript, TypeScript, Java, Go\n\n**Usage:**\n```bash\npython examples/chunk-code.py src/main.py --language python --output code-chunks.json\n```\n\n### PDF Chunking\n\n**Example:** `examples/chunk-pdf.py`\n\n**Features:**\n- Extracts text from PDF\n- Preserves page boundaries\n- Maintains formatting clues\n- Handles multi-column layouts\n\n**Dependencies:** `pypdf`, `pdfminer.six`\n\n**Usage:**\n```bash\npython examples/chunk-pdf.py research-paper.pdf --strategy semantic --output pdf-chunks.json\n```\n\n## Optimization Guidelines\n\n### Chunk Size Selection\n\n**General recommendations:**\n| Content Type | Chunk Size | Overlap | Strategy |\n|--------------|------------|---------|----------|\n| Q&A / FAQs | 200-400 | 50 | Sentence |\n| Articles | 500-1000 | 100-200 | Semantic |\n| Documentation | 1000-1500 | 200-300 | Recursive |\n| Books | 1000-2000 | 300-400 | Semantic |\n| Source code | 500-1000 | 100 | Recursive |\n\n**Test with your data:** Use `benchmark-chunking.py` to find optimal settings\n\n### Overlap Strategies\n\n**Why overlap matters:**\n- Prevents information loss at boundaries\n- Improves retrieval of cross-boundary information\n- Balances redundancy vs. coverage\n\n**Overlap guidelines:**\n- **10-15%**: Minimal overlap for speed\n- **15-20%**: Standard overlap for most use cases\n- **20-30%**: High overlap for critical applications\n\n### Performance Optimization\n\n**Fast chunking (large documents):**\n```bash\n# Use fixed-size for speed\npython scripts/chunk-fixed-size.py --input large-doc.txt --chunk-size 1000\n```\n\n**Quality chunking (smaller documents):**\n```bash\n# Use semantic for better context\npython scripts/chunk-semantic.py --input article.txt --max-chunk-size 1500\n```\n\n**Batch processing:**\n```bash\n# Process multiple files\nfor file in documents/*.txt; do\n  python scripts/chunk-semantic.py --input \"$file\" --output \"chunks/$(basename $file .txt).json\"\ndone\n```\n\n## Evaluation Workflow\n\n### Step 1: Benchmark Strategies\n\n```bash\npython scripts/benchmark-chunking.py \\\n  --input sample-document.txt \\\n  --strategies fixed,semantic,recursive \\\n  --chunk-sizes 500,1000,1500\n```\n\n### Step 2: Analyze Results\n\n**Review metrics:**\n- Processing time (prefer < 100ms per document)\n- Context preservation score (target > 0.85)\n- Chunk size variance (lower is more predictable)\n\n### Step 3: A/B Test Retrieval\n\n**Compare retrieval quality:**\n1. Chunk same corpus with different strategies\n2. Run identical test queries against each\n3. Measure precision@k and recall@k\n4. Select strategy with best retrieval metrics\n\n### Step 4: Production Deployment\n\n**Use configuration file:**\n```python\nimport yaml\nfrom chunking_strategies import get_chunker\n\nconfig = yaml.safe_load(open('chunking-config.yaml'))\nchunker = get_chunker(config['chunking']['default_strategy'], config)\n\nchunks = chunker.chunk(document_text)\n```\n\n## Common Issues & Solutions\n\n**Issue: Chunks too small/large**\n- Adjust `chunk_size` parameter\n- Check document structure (may need different strategy)\n- Verify separator selection for recursive chunking\n\n**Issue: Lost context at boundaries**\n- Increase overlap\n- Switch to semantic chunking\n- Add parent context to metadata\n\n**Issue: Slow processing**\n- Use fixed-size chunking for large batches\n- Reduce overlap\n- Process files in parallel\n\n**Issue: Poor retrieval quality**\n- Benchmark different strategies\n- Increase chunk size\n- Try hybrid approach (semantic + fixed fallback)\n\n## Dependencies\n\n**Core libraries:**\n```bash\npip install tiktoken  # Token counting\npip install nltk      # Sentence splitting\npip install spacy     # Advanced NLP (optional)\n```\n\n**For PDF support:**\n```bash\npip install pypdf pdfminer.six\n```\n\n**For benchmarking:**\n```bash\npip install pandas numpy scikit-learn\n```\n\n## Best Practices Summary\n\n1. Start with semantic chunking for most documents\n2. Use recursive chunking for structured/hierarchical content\n3. Benchmark on your actual data before production\n4. Set overlap to 15-20% of chunk size\n5. Include metadata (source, page, section) in chunks\n6. Test retrieval quality, not just chunking speed\n7. Use appropriate chunk size for your embedding model token limit\n8. Document your chunking strategy choice and parameters\n\n---\n\n**Supported Strategies:** Fixed-Size, Semantic, Recursive, Sentence-Based, Custom\n**Output Format:** JSON with text and metadata\n**Version:** 1.0.0"
              },
              {
                "name": "document-parsers",
                "description": "Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.",
                "path": "plugins/rag-pipeline/skills/document-parsers/SKILL.md",
                "frontmatter": {
                  "name": "document-parsers",
                  "description": "Multi-format document parsing tools for PDF, DOCX, HTML, and Markdown with support for LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, and python-docx. Use when parsing documents, extracting text from PDFs, processing Word documents, converting HTML to text, extracting tables from documents, building RAG pipelines, chunking documents, or when user mentions document parsing, PDF extraction, DOCX processing, table extraction, OCR, LlamaParse, Unstructured.io, or document ingestion.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob, Edit"
                },
                "content": "# Document Parsers\n\n**Purpose:** Autonomously parse and extract content from multiple document formats (PDF, DOCX, HTML, Markdown) using industry-standard libraries and AI-powered parsing tools.\n\n**Activation Triggers:**\n- Building RAG (Retrieval-Augmented Generation) pipelines\n- Extracting text, tables, or metadata from documents\n- Processing large document collections\n- Converting documents to structured formats\n- Handling complex PDFs with tables and layouts\n- OCR for scanned documents\n- Chunking documents for vector embeddings\n- Building document search systems\n\n**Key Resources:**\n- `scripts/setup-llamaparse.sh` - Install and configure LlamaParse (AI-powered parsing)\n- `scripts/setup-unstructured.sh` - Install Unstructured.io library\n- `scripts/parse-pdf.py` - Functional PDF parser with multiple backend options\n- `scripts/parse-docx.py` - DOCX document parser\n- `scripts/parse-html.py` - HTML to structured text parser\n- `templates/multi-format-parser.py` - Universal document parser template\n- `templates/table-extraction.py` - Specialized table extraction template\n- `examples/parse-research-paper.py` - Research paper parsing with citations\n- `examples/parse-legal-document.py` - Legal document parsing with sections\n\n## Parser Comparison & Selection Guide\n\n### 1. LlamaParse (AI-Powered Premium)\n\n**Best For:**\n- Complex PDFs with tables, charts, and mixed layouts\n- Scanned documents requiring OCR\n- Documents where accuracy is critical\n- Multi-column layouts and scientific papers\n- Financial reports and invoices\n\n**Pros:**\n- AI-powered layout understanding\n- Excellent table extraction accuracy\n- Built-in OCR support\n- Handles complex formatting\n- Structured output (Markdown/JSON)\n\n**Cons:**\n- Requires API key (paid service)\n- API rate limits\n- Network dependency\n- Slower than local parsers\n\n**Documentation:** https://docs.cloud.llamaindex.ai/llamaparse\n\n**Setup:**\n```bash\n./scripts/setup-llamaparse.sh\n```\n\n**Usage Pattern:**\n```python\nfrom llama_parse import LlamaParse\n\nparser = LlamaParse(\n    api_key=\"llx-...\",\n    result_type=\"markdown\",  # or \"text\"\n    language=\"en\",\n    verbose=True\n)\n\ndocuments = parser.load_data(\"document.pdf\")\nfor doc in documents:\n    print(doc.text)\n```\n\n### 2. Unstructured.io (Local Processing)\n\n**Best For:**\n- Batch processing many documents\n- Multiple format support (PDF, DOCX, HTML, PPTX, Images)\n- Local processing without API dependencies\n- Structured element extraction\n- Production RAG pipelines\n\n**Pros:**\n- Open-source and free\n- Multi-format support\n- Runs locally (no API keys)\n- Good table detection\n- Element-based chunking\n\n**Cons:**\n- Requires system dependencies (poppler, tesseract)\n- Complex installation\n- Less accurate than LlamaParse for complex layouts\n\n**Documentation:** https://unstructured-io.github.io/unstructured/\n\n**Setup:**\n```bash\n./scripts/setup-unstructured.sh\n```\n\n**Usage Pattern:**\n```python\nfrom unstructured.partition.auto import partition\n\nelements = partition(\"document.pdf\")\nfor element in elements:\n    print(f\"{element.category}: {element.text}\")\n```\n\n### 3. PyPDF2 (Simple PDF Text Extraction)\n\n**Best For:**\n- Simple text-based PDFs\n- Quick prototyping\n- Metadata extraction\n- PDF manipulation (merge, split)\n\n**Pros:**\n- Pure Python (no dependencies)\n- Fast and lightweight\n- Good for simple PDFs\n- Active maintenance\n\n**Cons:**\n- Poor table extraction\n- Struggles with complex layouts\n- No OCR support\n- Limited formatting preservation\n\n**Documentation:** https://github.com/py-pdf/pypdf2\n\n**Setup:**\n```bash\npip install pypdf2\n```\n\n**Usage Pattern:**\n```python\nfrom PyPDF2 import PdfReader\n\nreader = PdfReader(\"document.pdf\")\nfor page in reader.pages:\n    print(page.extract_text())\n```\n\n### 4. PDFPlumber (Advanced PDF Analysis)\n\n**Best For:**\n- Table extraction from PDFs\n- PDF with tabular data\n- Financial statements and reports\n- Coordinate-based extraction\n\n**Pros:**\n- Excellent table extraction\n- Visual debugging tools\n- Coordinate-level control\n- Metadata and layout info\n\n**Cons:**\n- Slower than PyPDF2\n- Requires pdfminer.six dependency\n- No OCR support\n\n**Documentation:** https://github.com/jsvine/pdfplumber\n\n**Setup:**\n```bash\npip install pdfplumber\n```\n\n**Usage Pattern:**\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        text = page.extract_text()\n```\n\n### 5. python-docx (Word Documents)\n\n**Best For:**\n- Microsoft Word (.docx) documents\n- Extracting paragraphs, tables, headers\n- Document metadata\n- Template-based document generation\n\n**Pros:**\n- Native DOCX support\n- Preserves structure (paragraphs, tables, sections)\n- Access to styles and formatting\n- Can also write/modify DOCX\n\n**Cons:**\n- Only works with .docx (not .doc)\n- Limited image extraction\n\n**Documentation:** https://github.com/python-openxml/python-docx\n\n**Setup:**\n```bash\npip install python-docx\n```\n\n**Usage Pattern:**\n```python\nfrom docx import Document\n\ndoc = Document(\"document.docx\")\nfor para in doc.paragraphs:\n    print(para.text)\nfor table in doc.tables:\n    for row in table.rows:\n        print([cell.text for cell in row.cells])\n```\n\n## Decision Matrix\n\n| Use Case | Recommended Parser | Alternative |\n|----------|-------------------|-------------|\n| Simple PDF text extraction | PyPDF2 | Unstructured |\n| Complex PDFs with tables | LlamaParse | PDFPlumber |\n| Scanned documents (OCR) | LlamaParse | Unstructured + Tesseract |\n| Word documents (.docx) | python-docx | Unstructured |\n| HTML to text | parse-html.py | Unstructured |\n| Multi-format batch processing | Unstructured | Multi-format-parser |\n| Table extraction | PDFPlumber | LlamaParse |\n| Research papers | LlamaParse | Unstructured |\n| Legal documents | LlamaParse | PDFPlumber |\n| Production RAG pipeline | Unstructured | LlamaParse |\n\n## Functional Scripts\n\n### 1. Parse PDF (`scripts/parse-pdf.py`)\n\nCommand-line PDF parser supporting multiple backends:\n\n```bash\n# Using PyPDF2 (default)\npython scripts/parse-pdf.py document.pdf\n\n# Using PDFPlumber (better for tables)\npython scripts/parse-pdf.py document.pdf --backend pdfplumber\n\n# Using LlamaParse (AI-powered)\npython scripts/parse-pdf.py document.pdf --backend llamaparse --api-key llx-...\n\n# Output to file\npython scripts/parse-pdf.py document.pdf --output output.txt\n\n# Extract tables as JSON\npython scripts/parse-pdf.py document.pdf --backend pdfplumber --tables-only --output tables.json\n```\n\n**Features:**\n- Multiple backend support (PyPDF2, PDFPlumber, LlamaParse)\n- Table extraction\n- Metadata extraction\n- Page range selection\n- JSON/Text output formats\n\n### 2. Parse DOCX (`scripts/parse-docx.py`)\n\nWord document parser with structure preservation:\n\n```bash\n# Basic extraction\npython scripts/parse-docx.py document.docx\n\n# Extract with structure\npython scripts/parse-docx.py document.docx --preserve-structure\n\n# Extract tables only\npython scripts/parse-docx.py document.docx --tables-only\n\n# Output as JSON\npython scripts/parse-docx.py document.docx --output output.json --format json\n```\n\n**Features:**\n- Paragraph extraction with styles\n- Table extraction\n- Header/footer extraction\n- Metadata (author, created date, etc.)\n- Structured JSON output\n\n### 3. Parse HTML (`scripts/parse-html.py`)\n\nHTML to clean text converter:\n\n```bash\n# Basic HTML parsing\npython scripts/parse-html.py document.html\n\n# From URL\npython scripts/parse-html.py https://example.com/article\n\n# Preserve links\npython scripts/parse-html.py document.html --preserve-links\n\n# Extract specific selector\npython scripts/parse-html.py document.html --selector \"article.content\"\n```\n\n**Features:**\n- Clean text extraction (removes scripts, styles)\n- Link preservation\n- CSS selector support\n- URL fetching\n- Markdown output option\n\n## Templates\n\n### Multi-Format Parser (`templates/multi-format-parser.py`)\n\nUniversal parser handling multiple formats with automatic format detection:\n\n```python\nfrom multi_format_parser import MultiFormatParser\n\nparser = MultiFormatParser(\n    llamaparse_api_key=\"llx-...\",  # Optional\n    use_ocr=True,\n    chunk_size=1000\n)\n\n# Automatic format detection\nresult = parser.parse_file(\"document.pdf\")\nprint(result.text)\nprint(result.metadata)\nprint(result.tables)\n\n# Batch processing\nresults = parser.parse_directory(\"./documents/\")\nfor filename, result in results.items():\n    print(f\"{filename}: {len(result.text)} characters\")\n```\n\n**Supports:**\n- PDF, DOCX, HTML, Markdown, TXT\n- Automatic chunking for RAG\n- Metadata extraction\n- Table extraction across all formats\n- Error handling and fallbacks\n\n### Table Extraction (`templates/table-extraction.py`)\n\nSpecialized table extraction with multiple strategies:\n\n```python\nfrom table_extraction import TableExtractor\n\nextractor = TableExtractor(\n    prefer_llamaparse=True,\n    fallback_to_pdfplumber=True\n)\n\n# Extract all tables from document\ntables = extractor.extract_tables(\"financial_report.pdf\")\n\nfor i, table in enumerate(tables):\n    print(f\"Table {i + 1}:\")\n    print(table.to_markdown())  # or .to_csv(), .to_json()\n    print(f\"Confidence: {table.confidence}\")\n```\n\n**Features:**\n- Multiple extraction strategies\n- Automatic fallback\n- Table validation\n- Format conversion (CSV, JSON, Markdown, DataFrame)\n- Confidence scoring\n\n## Examples\n\n### Research Paper Parsing (`examples/parse-research-paper.py`)\n\nComplete example for parsing academic papers:\n\n```python\n# Extracts title, abstract, sections, citations, tables, figures\npython examples/parse-research-paper.py paper.pdf --output paper.json\n```\n\n**Extracts:**\n- Title and authors\n- Abstract\n- Section structure (Introduction, Methods, Results, etc.)\n- Citations and references\n- Tables and figures with captions\n- Metadata (DOI, publication date, journal)\n\n### Legal Document Parsing (`examples/parse-legal-document.py`)\n\nSpecialized parser for legal documents:\n\n```python\n# Extracts clauses, sections, definitions, parties\npython examples/parse-legal-document.py contract.pdf --output contract.json\n```\n\n**Extracts:**\n- Document type (contract, agreement, etc.)\n- Parties involved\n- Definitions section\n- Numbered clauses and sections\n- Signature blocks\n- Dates and deadlines\n\n## RAG Pipeline Integration\n\n### Document Chunking for Embeddings\n\n```python\nfrom multi_format_parser import MultiFormatParser\n\nparser = MultiFormatParser(chunk_size=512, chunk_overlap=50)\nresult = parser.parse_file(\"document.pdf\")\n\n# Chunks ready for embedding\nfor chunk in result.chunks:\n    print(f\"Chunk {chunk.id}: {chunk.text[:100]}...\")\n    print(f\"Metadata: {chunk.metadata}\")\n    # Send to embedding model\n```\n\n### Batch Processing Pipeline\n\n```python\nimport glob\nfrom multi_format_parser import MultiFormatParser\n\nparser = MultiFormatParser()\n\n# Process all documents in directory\nfor filepath in glob.glob(\"./documents/**/*\", recursive=True):\n    try:\n        result = parser.parse_file(filepath)\n        # Store in vector database\n        store_embeddings(result.chunks)\n        print(f\"‚úì Processed {filepath}\")\n    except Exception as e:\n        print(f\"‚úó Failed {filepath}: {e}\")\n```\n\n## Best Practices\n\n**Parser Selection:**\n- Start with PyPDF2 for simple PDFs, upgrade if needed\n- Use LlamaParse for complex layouts (budget permitting)\n- Use Unstructured for multi-format production systems\n- Use PDFPlumber specifically for table extraction\n\n**Performance:**\n- Cache parsed results to avoid re-processing\n- Use batch processing for multiple documents\n- Consider async processing for large collections\n- Monitor API rate limits for LlamaParse\n\n**Accuracy:**\n- Validate table extraction results\n- Implement fallback strategies\n- Log parsing errors for debugging\n- Use confidence scores when available\n\n**RAG Optimization:**\n- Chunk size: 512-1024 tokens for embeddings\n- Overlap: 10-20% for context preservation\n- Preserve metadata (page numbers, sections) for retrieval\n- Clean extracted text (remove headers/footers)\n\n## Troubleshooting\n\n**PyPDF2 returns garbled text:**\n- Try PDFPlumber or LlamaParse\n- PDF may have non-standard encoding\n- Check if PDF is scanned (needs OCR)\n\n**Unstructured installation fails:**\n- Install system dependencies: `sudo apt-get install poppler-utils tesseract-ocr`\n- On macOS: `brew install poppler tesseract`\n\n**LlamaParse API errors:**\n- Verify API key is correct\n- Check rate limits in dashboard\n- Ensure document size is within limits\n\n**Table extraction misses columns:**\n- Try different parser (PDFPlumber vs LlamaParse)\n- Adjust table detection settings\n- Validate table structure manually\n\n**DOCX parsing fails:**\n- Ensure file is .docx not .doc\n- Check file is not corrupted\n- Try converting to .docx with LibreOffice\n\n## Dependencies\n\n**Core:**\n```bash\npip install pypdf2 pdfplumber python-docx beautifulsoup4 lxml markdown\n```\n\n**Optional (Unstructured):**\n```bash\npip install unstructured[local-inference]\nsudo apt-get install poppler-utils tesseract-ocr  # Linux\nbrew install poppler tesseract  # macOS\n```\n\n**Optional (LlamaParse):**\n```bash\npip install llama-parse\n# Requires API key from https://cloud.llamaindex.ai\n```\n\n---\n\n**Supported Formats:** PDF, DOCX, HTML, Markdown, TXT\n**Parsers:** LlamaParse, Unstructured.io, PyPDF2, PDFPlumber, python-docx\n**Version:** 1.0.0"
              },
              {
                "name": "embedding-models",
                "description": "Embedding model configurations and cost calculators",
                "path": "plugins/rag-pipeline/skills/embedding-models/SKILL.md",
                "frontmatter": {
                  "name": "embedding-models",
                  "description": "Embedding model configurations and cost calculators",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# Embedding Models Skill\n\nEmbedding model selection, configuration, and cost optimization for RAG pipelines.\n\n## Use When\n\n- Selecting embedding models for vector search\n- Configuring OpenAI, Cohere, or HuggingFace embeddings\n- Calculating embedding generation costs\n- Optimizing embedding performance vs cost tradeoffs\n- Setting up local vs cloud embedding models\n- Implementing embedding caching strategies\n- User mentions: \"embeddings\", \"vector models\", \"embedding costs\", \"semantic search models\"\n\n## Model Selection Guide\n\n### Commercial Models\n\n**OpenAI Embeddings:**\n- `text-embedding-3-small` - 1536 dims, $0.02/1M tokens, balanced performance\n- `text-embedding-3-large` - 3072 dims, $0.13/1M tokens, highest quality\n- `text-embedding-ada-002` - 1536 dims, $0.10/1M tokens, legacy model\n\n**Cohere Embeddings:**\n- `embed-english-v3.0` - 1024 dims, multilingual support\n- `embed-english-light-v3.0` - 384 dims, faster/cheaper\n- `embed-multilingual-v3.0` - 1024 dims, 100+ languages\n\n### Open Source Models (HuggingFace)\n\n**Sentence Transformers:**\n- `all-MiniLM-L6-v2` - 384 dims, 80MB, fast and efficient\n- `all-mpnet-base-v2` - 768 dims, 420MB, high quality\n- `multi-qa-mpnet-base-dot-v1` - 768 dims, optimized for Q&A\n- `paraphrase-multilingual-mpnet-base-v2` - 768 dims, 50+ languages\n\n**Specialized Models:**\n- `BAAI/bge-small-en-v1.5` - 384 dims, SOTA small model\n- `BAAI/bge-base-en-v1.5` - 768 dims, excellent retrieval\n- `BAAI/bge-large-en-v1.5` - 1024 dims, top performance\n- `intfloat/e5-base-v2` - 768 dims, strong general purpose\n\n## Cost Calculator\n\nUse the cost calculator script to estimate embedding costs:\n\n```bash\n# Calculate costs for different models and volumes\npython scripts/calculate-embedding-costs.py \\\n  --documents 100000 \\\n  --avg-tokens 500 \\\n  --model text-embedding-3-small\n\n# Compare multiple models\npython scripts/calculate-embedding-costs.py \\\n  --documents 100000 \\\n  --avg-tokens 500 \\\n  --compare\n```\n\n## Setup Scripts\n\n### OpenAI Embeddings\n```bash\nbash scripts/setup-openai-embeddings.sh\n```\n\nConfigures OpenAI embedding client with API key management and retry logic.\n\n### HuggingFace Embeddings\n```bash\nbash scripts/setup-huggingface-embeddings.sh\n```\n\nDownloads and configures sentence-transformers models locally.\n\n### Cohere Embeddings\n```bash\nbash scripts/setup-cohere-embeddings.sh\n```\n\nSets up Cohere embedding client with API credentials.\n\n## Configuration Templates\n\n### OpenAI Configuration\n```python\n# templates/openai-embedding-config.py\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-key\")\n\nembeddings = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\"Your text here\"]\n)\n```\n\n### HuggingFace Configuration\n```python\n# templates/huggingface-embedding-config.py\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode([\"Your text here\"])\n```\n\n### Custom Model Template\n```python\n# templates/custom-embedding-model.py\n# Wrapper for any embedding model with consistent interface\n```\n\n## Optimization Strategies\n\n**Cost Optimization:**\n1. Use smaller models for high-volume applications\n2. Implement embedding caching (see examples/embedding-cache.py)\n3. Batch embedding generation (see examples/batch-embedding-generation.py)\n4. Consider local models for sensitive data\n\n**Performance Optimization:**\n1. Use GPU acceleration for local models\n2. Batch processing for throughput\n3. Dimension reduction for storage/speed\n4. Model distillation for faster inference\n\n## Model Comparison Matrix\n\n| Model | Dimensions | Size | Speed | Quality | Cost |\n|-------|-----------|------|-------|---------|------|\n| text-embedding-3-small | 1536 | API | Fast | Good | $0.02/1M |\n| text-embedding-3-large | 3072 | API | Medium | Excellent | $0.13/1M |\n| all-MiniLM-L6-v2 | 384 | 80MB | Very Fast | Good | Free |\n| all-mpnet-base-v2 | 768 | 420MB | Fast | Excellent | Free |\n| bge-base-en-v1.5 | 768 | 420MB | Fast | Excellent | Free |\n| embed-english-v3.0 | 1024 | API | Fast | Excellent | $0.10/1M |\n\n## Examples\n\n**Batch Embedding Generation:**\n```python\n# examples/batch-embedding-generation.py\n# Process large document collections efficiently\n```\n\n**Embedding Cache:**\n```python\n# examples/embedding-cache.py\n# Cache embeddings to avoid redundant API calls\n```\n\n## Decision Framework\n\n**Use OpenAI when:**\n- Need highest quality embeddings\n- Low to medium volume (<10M tokens/month)\n- Prefer managed service over self-hosting\n- Working with latest models\n\n**Use Cohere when:**\n- Need multilingual support\n- Require production SLA\n- Want embedding customization\n- Need both embedding and reranking\n\n**Use HuggingFace/Local when:**\n- High volume (>10M tokens/month)\n- Data privacy requirements\n- Have GPU infrastructure\n- Cost optimization priority\n- Offline/air-gapped environments\n\n## References\n\n- Sentence Transformers: https://www.sbert.net/\n- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings\n- Cohere Embeddings: https://docs.cohere.com/docs/embeddings\n- MTEB Leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
              },
              {
                "name": "google-file-search",
                "description": "Google File Search API templates, configuration patterns, and usage examples for managed RAG with Gemini. Use when building File Search integrations, implementing RAG with Google AI, chunking documents, configuring grounding citations, or when user mentions Google File Search, Gemini RAG, document indexing, or semantic search.",
                "path": "plugins/rag-pipeline/skills/google-file-search/SKILL.md",
                "frontmatter": {
                  "name": "google-file-search",
                  "description": "Google File Search API templates, configuration patterns, and usage examples for managed RAG with Gemini. Use when building File Search integrations, implementing RAG with Google AI, chunking documents, configuring grounding citations, or when user mentions Google File Search, Gemini RAG, document indexing, or semantic search.",
                  "allowed-tools": "Bash, Read, Write, Edit, Grep, Glob, WebFetch"
                },
                "content": "# Google File Search\n\nComprehensive skill for implementing Google File Search API with Gemini models for Retrieval-Augmented Generation (RAG).\n\n## Overview\n\nGoogle File Search provides managed RAG capabilities through:\n- Automatic document chunking and embedding generation\n- Semantic search across multiple document types\n- Metadata-based filtering for targeted retrieval\n- Grounding citations showing source documents\n- Persistent storage with file search stores\n- Integration with Gemini 2.5 models\n\nThis skill provides templates, scripts, and examples for implementing File Search in Python applications using the `google-generativeai` package.\n\n## Use When\n\nThis skill is automatically invoked when:\n- Building RAG systems with Google Gemini\n- Implementing document search and retrieval\n- Configuring chunking strategies\n- Setting up grounding citations\n- Managing file search stores\n- Uploading and indexing documents\n- Filtering search results by metadata\n- Testing semantic search capabilities\n\n## Key Capabilities\n\n### 1. Store Management\n- Create persistent file search stores\n- List and retrieve existing stores\n- Delete stores with force option\n- Monitor storage quotas (1GB-1TB by tier)\n\n### 2. Document Upload & Indexing\n- Direct upload and indexing in single operation\n- Separate upload via Files API then import\n- Batch file processing\n- Support for 100+ file types (PDF, DOCX, code, etc.)\n- Maximum file size: 100 MB per document\n\n### 3. Chunking Configuration\n- White space-based chunking strategies\n- Configurable tokens per chunk\n- Overlap token settings for context preservation\n- Custom chunking for domain-specific needs\n\n### 4. Metadata & Filtering\n- Custom key-value metadata during import\n- String and numeric metadata values\n- AIP-160 compliant filter syntax\n- Multi-condition metadata queries\n\n### 5. Grounding & Citations\n- Access to source document references\n- Citation extraction from responses\n- Fact-checking and verification support\n- Transparent sourcing for AI responses\n\n## Security: API Key Handling\n\n**CRITICAL:** All templates and examples use placeholder values:\n\n‚ùå NEVER hardcode actual API keys\n‚úÖ ALWAYS use: `GOOGLE_API_KEY=your_google_api_key_here`\n‚úÖ ALWAYS use: `GOOGLE_GENAI_API_KEY=your_google_genai_api_key_here`\n‚úÖ ALWAYS read from environment variables in code\n‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n\nObtain API keys from: https://aistudio.google.com/apikey\n\n## Usage Instructions\n\n### Phase 1: Load Required Documentation\n\nBefore implementing File Search, fetch the latest documentation:\n\n```markdown\nWebFetch: https://ai.google.dev/gemini-api/docs/file-search\nWebFetch: https://ai.google.dev/gemini-api/docs/embeddings\n```\n\n### Phase 2: Initialize File Search Store\n\nUse the Python setup script to create a new store:\n\n```bash\npython scripts/setup_file_search.py --name \"My RAG Store\"\n```\n\nThis Python script:\n- Creates a new file search store\n- Saves store ID to environment\n- Validates creation\n- Returns store details\n\n### Phase 3: Configure Chunking Strategy\n\nCustomize chunking for your document domain:\n\n```bash\npython scripts/configure_chunking.py --max-tokens 200 --overlap 20\n```\n\nGenerates configuration file with:\n- Maximum tokens per chunk\n- Overlap tokens for context\n- White space chunking strategy\n\n### Phase 4: Upload and Index Documents\n\nUpload files to the store:\n\n```bash\npython scripts/upload_documents.py --path /path/to/documents\n```\n\nThis script:\n- Validates file types and sizes\n- Uploads and indexes simultaneously\n- Applies chunking configuration\n- Adds optional metadata\n- Tracks upload progress\n\n### Phase 5: Test Semantic Search\n\nVerify search functionality:\n\n```bash\npython scripts/search_query.py --query \"your search query\"\n```\n\nTests:\n- Semantic search capabilities\n- Citation extraction\n- Metadata filtering\n- Response grounding\n\n### Phase 6: Validate Setup\n\nRun comprehensive validation:\n\n```bash\npython scripts/validate_setup.py\n```\n\nChecks:\n- Store existence and accessibility\n- Indexed document count\n- Chunking configuration\n- API key configuration\n- Storage quota usage\n\n## Available Scripts\n\n### `scripts/setup_file_search.py`\nInitialize a new file search store with display name.\n\n**Usage:**\n```bash\npython scripts/setup_file_search.py --name \"Store Name\"\n```\n\n### `scripts/upload_documents.py`\nUpload and index documents to a file search store.\n\n**Usage:**\n```bash\npython scripts/upload_documents.py --path /path/to/documents\npython scripts/upload_documents.py --file /path/to/file.pdf --metadata author=\"John Doe\"\n```\n\n### `scripts/configure_chunking.py`\nGenerate chunking configuration file.\n\n**Usage:**\n```bash\npython scripts/configure_chunking.py --max-tokens 200 --overlap 20\npython scripts/configure_chunking.py --preset small  # 100 tokens, 10 overlap\npython scripts/configure_chunking.py --preset large  # 500 tokens, 50 overlap\n```\n\n### `scripts/search_query.py`\nTest semantic search with sample queries.\n\n**Usage:**\n```bash\npython scripts/search_query.py --query \"explain quantum computing\"\npython scripts/search_query.py --query \"author=Einstein\" --metadata-filter\n```\n\n### `scripts/validate_setup.py`\nComprehensive validation of File Search configuration.\n\n**Usage:**\n```bash\npython scripts/validate_setup.py\npython scripts/validate_setup.py --verbose\n```\n\n## Available Templates\n\n### Configuration Templates\n\n**`templates/store-config.json`**\n- File search store creation configuration\n- Display name and description\n- Storage tier settings\n\n**`templates/chunking-config.json`**\n- White space chunking configuration\n- Token limits and overlap settings\n- Strategy presets\n\n**`templates/metadata-schema.json`**\n- Metadata field definitions\n- String and numeric value types\n- Filtering examples\n\n**`templates/env.example`**\n- Environment variable template\n- API key placeholders\n- Store ID configuration\n\n### Code Templates\n\n**`templates/python-setup.py`**\nComplete Python implementation template:\n- Store creation and management\n- Document upload with chunking\n- Search with metadata filtering\n- Citation extraction\n- Error handling\n\n**`templates/typescript-setup.ts`**\nComplete TypeScript implementation template:\n- Store initialization\n- File upload and indexing\n- Semantic search queries\n- Grounding metadata parsing\n- Type-safe interfaces\n\n## Available Examples\n\n### `examples/basic-setup.md`\nSimple File Search implementation for getting started:\n- Create first store\n- Upload single document\n- Perform basic search\n- Extract citations\n\n### `examples/advanced-chunking.md`\nCustom chunking strategies for different document types:\n- Technical documentation (larger chunks)\n- Legal documents (precise boundaries)\n- Code repositories (function-level chunks)\n- Scientific papers (section-based chunks)\n\n### `examples/metadata-filtering.md`\nUsing metadata for targeted search:\n- Add custom metadata during upload\n- Filter by author, date, category\n- Multi-condition metadata queries\n- Combining metadata with semantic search\n\n### `examples/grounding-citations.md`\nExtract and display source citations:\n- Parse grounding metadata\n- Extract document references\n- Display citation information\n- Build source attribution UI\n\n### `examples/multi-store.md`\nManage multiple file search stores:\n- Separate stores by domain\n- Cross-store search patterns\n- Store migration strategies\n- Quota management across stores\n\n## Supported Models\n\n- **gemini-2.5-pro**: Production model for complex reasoning\n- **gemini-2.5-flash**: Fast model for quick responses\n\n## Supported File Types\n\n**Documents:** PDF, DOCX, ODT, PPTX, XLSX, CSV, TXT, MD\n**Code:** Python, JavaScript, Java, TypeScript, Go, Rust, SQL\n**Data:** JSON, XML, YAML, HTML\n**Archives:** ZIP (automatically extracted)\n\nOver 100 MIME types supported.\n\n## Storage Limits\n\n**Per-Document:**\n- Maximum file size: 100 MB\n- Recommended store size: Under 20 GB\n\n**Total Storage by Tier:**\n- Free: 1 GB\n- Tier 1: 10 GB\n- Tier 2: 100 GB\n- Tier 3: 1 TB\n\nStorage calculation: Input size √ó ~3 (includes embeddings)\n\n## Pricing Considerations\n\n- **Indexing:** $0.15 per 1M tokens (one-time per document)\n- **Storage:** Free\n- **Query embeddings:** Free\n- **Retrieved tokens:** Standard context pricing\n\n**Optimization tip:** Index documents once, query multiple times for cost efficiency.\n\n## Best Practices\n\n1. **Chunk Size Optimization**\n   - Technical docs: 300-500 tokens\n   - General content: 200-300 tokens\n   - Code: 100-200 tokens\n   - Use overlap for context preservation\n\n2. **Metadata Strategy**\n   - Add author, date, category during upload\n   - Use consistent naming conventions\n   - Plan filtering needs upfront\n   - Leverage numeric values for date ranges\n\n3. **Store Organization**\n   - Separate stores by domain/project\n   - Keep stores under 20 GB for optimal retrieval\n   - Name stores descriptively\n   - Monitor quota usage\n\n4. **Citation Handling**\n   - Always extract grounding metadata\n   - Display sources to users\n   - Enable fact-checking workflows\n   - Track citation coverage\n\n5. **Error Handling**\n   - Validate file types before upload\n   - Check file size limits\n   - Handle quota exceeded errors\n   - Retry failed uploads with backoff\n\n## Integration Patterns\n\n### With FastAPI Backend\n```python\nfrom google import genai\nfrom fastapi import FastAPI, HTTPException\n\napp = FastAPI()\nclient = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n@app.post(\"/search\")\nasync def search(query: str, store_id: str):\n    response = client.models.generate_content(\n        model=\"gemini-2.5-flash\",\n        contents=query,\n        config={\n            \"tools\": [{\"file_search\": {\"store_id\": store_id}}]\n        }\n    )\n    return {\n        \"answer\": response.text,\n        \"citations\": response.candidates[0].grounding_metadata\n    }\n```\n\n### With Next.js Frontend\n```typescript\n// app/api/search/route.ts\nimport { GoogleGenAI } from '@google/generative-ai';\n\nexport async function POST(request: Request) {\n  const { query, storeId } = await request.json();\n\n  const genai = new GoogleGenAI(process.env.GOOGLE_API_KEY!);\n  const model = genai.getGenerativeModel({ model: 'gemini-2.5-flash' });\n\n  const result = await model.generateContent({\n    contents: [{ role: 'user', parts: [{ text: query }] }],\n    tools: [{ fileSearch: { storeId } }]\n  });\n\n  return Response.json({\n    answer: result.response.text(),\n    citations: result.response.candidates[0].groundingMetadata\n  });\n}\n```\n\n## Troubleshooting\n\n**Issue: Files not uploading**\n- Check file size (max 100 MB)\n- Verify file type is supported\n- Ensure API key has correct permissions\n- Check storage quota availability\n\n**Issue: Poor search results**\n- Adjust chunking configuration\n- Add relevant metadata for filtering\n- Try different chunk sizes\n- Verify documents indexed successfully\n\n**Issue: Missing citations**\n- Enable grounding in API request\n- Check response for grounding_metadata\n- Ensure store has indexed documents\n- Verify model supports grounding\n\n**Issue: Quota exceeded**\n- Check current storage usage\n- Delete unused stores\n- Upgrade to higher tier\n- Archive old documents\n\n## Related Skills\n\n- **embedding-specialist**: For custom embedding strategies\n- **vector-db-engineer**: For alternative vector storage\n- **langchain-specialist**: For LangChain integration\n- **llamaindex-specialist**: For LlamaIndex integration\n\n## References\n\n- **Official Docs**: https://ai.google.dev/gemini-api/docs/file-search\n- **Embeddings Guide**: https://ai.google.dev/gemini-api/docs/embeddings\n- **API Keys**: https://aistudio.google.com/apikey\n- **Filter Syntax**: https://google.aip.dev/160\n\n## Version\n\n**Skill Version:** 1.0.0\n**Last Updated:** 2025-11-11\n**Compatible With:** Gemini 2.5 Pro/Flash, Google GenAI SDK"
              },
              {
                "name": "langchain-patterns",
                "description": "LangChain implementation patterns with templates, scripts, and examples for RAG pipelines",
                "path": "plugins/rag-pipeline/skills/langchain-patterns/SKILL.md",
                "frontmatter": {
                  "name": "langchain-patterns",
                  "description": "LangChain implementation patterns with templates, scripts, and examples for RAG pipelines",
                  "allowed-tools": "Bash, Read, Write, Edit, Glob, Grep"
                },
                "content": "# LangChain Patterns Skill\n\nComprehensive LangChain implementation patterns for building production-ready RAG (Retrieval-Augmented Generation) pipelines. This skill provides working scripts, templates, and examples for document loading, vector stores, retrieval chains, LangGraph workflows, and LangSmith observability.\n\n## Use When\n\n- Building RAG applications with LangChain\n- Setting up vector stores and embeddings\n- Implementing conversational retrieval systems\n- Creating multi-step agent workflows with LangGraph\n- Integrating LangSmith for tracing and evaluation\n- Generating LangChain boilerplate code\n- Validating LangChain implementations\n- Need working examples of advanced retrieval patterns\n\n## Core Capabilities\n\n### 1. Environment Setup\n- Install LangChain and dependencies\n- Configure vector store backends (FAISS, Chroma, Pinecone)\n- Set up API keys and environment variables\n- Validate installation and connectivity\n\n### 2. RAG Chain Patterns\n- Document loaders (PDF, CSV, JSON, Web)\n- Text splitting strategies\n- Embedding models (OpenAI, HuggingFace, Cohere)\n- Vector store integration\n- Retrieval chains (basic, conversational, multi-query)\n- Self-querying retrievers\n\n### 3. LangGraph Workflows\n- Stateful multi-actor applications\n- Graph-based agent orchestration\n- Conditional routing and loops\n- State management patterns\n- Human-in-the-loop workflows\n\n### 4. LangSmith Integration\n- Tracing and debugging\n- Evaluation datasets\n- Performance monitoring\n- Production observability\n\n## Directory Structure\n\n```\nlangchain-patterns/\n‚îú‚îÄ‚îÄ SKILL.md                          # This file\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ setup-langchain.sh            # Install dependencies\n‚îÇ   ‚îú‚îÄ‚îÄ create-vectorstore.sh         # Vector store setup\n‚îÇ   ‚îî‚îÄ‚îÄ test-langchain.sh             # Validation tests\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ rag-chain.py                  # Basic RAG chain template\n‚îÇ   ‚îú‚îÄ‚îÄ langgraph-workflow.py         # LangGraph agent workflow\n‚îÇ   ‚îî‚îÄ‚îÄ langsmith-integration.py      # Observability template\n‚îî‚îÄ‚îÄ examples/\n    ‚îú‚îÄ‚îÄ conversational-retrieval.py   # Conversational RAG\n    ‚îú‚îÄ‚îÄ multi-query-retrieval.py      # Multi-query retrieval\n    ‚îî‚îÄ‚îÄ self-querying-retrieval.py    # Self-querying retriever\n```\n\n## Scripts\n\n### setup-langchain.sh\n\nInstalls LangChain and common dependencies for RAG applications.\n\n**Usage:**\n```bash\nbash scripts/setup-langchain.sh [--all|--minimal|--vectorstore]\n```\n\n**Options:**\n- `--all`: Install full suite (LangChain, LangGraph, LangSmith, all vector stores)\n- `--minimal`: Core LangChain only\n- `--vectorstore <name>`: Install specific vector store (faiss, chroma, pinecone, qdrant)\n\n**Environment Variables:**\n- `OPENAI_API_KEY`: OpenAI API key\n- `ANTHROPIC_API_KEY`: Anthropic API key\n- `LANGSMITH_API_KEY`: LangSmith API key (optional)\n\n### create-vectorstore.sh\n\nCreates and populates a vector store from documents.\n\n**Usage:**\n```bash\nbash scripts/create-vectorstore.sh <store_type> <documents_path> <output_path>\n```\n\n**Parameters:**\n- `store_type`: Vector store type (faiss, chroma, pinecone, qdrant)\n- `documents_path`: Path to documents directory or file\n- `output_path`: Where to save the vector store\n\n**Environment Variables:**\n- `EMBEDDING_MODEL`: Embedding model to use (default: text-embedding-3-small)\n- `CHUNK_SIZE`: Text chunk size (default: 1000)\n- `CHUNK_OVERLAP`: Chunk overlap (default: 200)\n\n### test-langchain.sh\n\nValidates LangChain installation and configuration.\n\n**Usage:**\n```bash\nbash scripts/test-langchain.sh [--verbose]\n```\n\n**Tests:**\n- ‚úì LangChain installation\n- ‚úì API key configuration\n- ‚úì Vector store connectivity\n- ‚úì Embedding model access\n- ‚úì LLM connectivity\n- ‚úì LangSmith connection (if configured)\n\n## Templates\n\n### rag-chain.py\n\nBasic RAG chain template with document loading, vector store, and retrieval.\n\n**Features:**\n- Document loading from multiple formats\n- Text splitting with configurable chunks\n- Vector store creation and persistence\n- Basic retrieval chain\n- Conversation memory (optional)\n\n**Usage:**\n```python\nfrom templates.rag_chain import RAGChain\n\n# Initialize RAG chain\nrag = RAGChain(\n    documents_path=\"./docs\",\n    vectorstore_path=\"./vectorstore\",\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Load and index documents\nrag.load_documents()\n\n# Query\nresult = rag.query(\"What are the main features?\")\nprint(result)\n```\n\n### langgraph-workflow.py\n\nLangGraph workflow template for multi-step agent orchestration.\n\n**Features:**\n- State management\n- Conditional routing\n- Tool integration\n- Error handling and retries\n- Human-in-the-loop approval\n\n**Usage:**\n```python\nfrom templates.langgraph_workflow import create_workflow\n\n# Create workflow\nworkflow = create_workflow(\n    llm=llm,\n    tools=tools,\n    checkpointer=MemorySaver()\n)\n\n# Execute\nresult = workflow.invoke({\n    \"messages\": [HumanMessage(content=\"Analyze this document\")]\n})\n```\n\n### langsmith-integration.py\n\nLangSmith integration template for tracing and evaluation.\n\n**Features:**\n- Automatic tracing\n- Custom run names and tags\n- Evaluation datasets\n- Feedback collection\n- Performance metrics\n\n**Usage:**\n```python\nfrom templates.langsmith_integration import LangSmithTracer\n\n# Initialize tracer\ntracer = LangSmithTracer(\n    project_name=\"rag-pipeline\",\n    tags=[\"production\", \"v1\"]\n)\n\n# Trace chain execution\nwith tracer.trace(\"rag-query\"):\n    result = chain.invoke({\"query\": \"...\"})\n```\n\n## Examples\n\n### conversational-retrieval.py\n\nComplete conversational retrieval system with memory.\n\n**Features:**\n- Conversation history management\n- Context-aware retrieval\n- Follow-up question handling\n- Source citation\n\n**Run:**\n```bash\npython examples/conversational-retrieval.py --docs ./docs --query \"Tell me about RAG\"\n```\n\n### multi-query-retrieval.py\n\nMulti-query retrieval for better coverage.\n\n**Features:**\n- Query expansion from single question\n- Parallel retrieval across queries\n- Result deduplication\n- Ranked fusion\n\n**Run:**\n```bash\npython examples/multi-query-retrieval.py --docs ./docs --query \"What is LangChain?\"\n```\n\n### self-querying-retrieval.py\n\nSelf-querying retriever with metadata filtering.\n\n**Features:**\n- Natural language to structured query\n- Metadata filter generation\n- Semantic + metadata search\n- Filter validation\n\n**Run:**\n```bash\npython examples/self-querying-retrieval.py --docs ./docs --query \"Recent papers about transformers\"\n```\n\n## Common Patterns\n\n### Pattern 1: Basic RAG Pipeline\n\n```python\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n\n# Load documents\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.pdf\")\ndocuments = loader.load()\n\n# Split\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(documents)\n\n# Embed and store\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\nvectorstore = FAISS.from_documents(chunks, embeddings)\n\n# Create chain\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\nchain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=vectorstore.as_retriever()\n)\n\n# Query\nresult = chain.invoke({\"query\": \"What is RAG?\"})\n```\n\n### Pattern 2: Conversational RAG\n\n```python\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\nchain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=vectorstore.as_retriever(),\n    memory=memory\n)\n\n# Chat\nresult1 = chain({\"question\": \"What is LangChain?\"})\nresult2 = chain({\"question\": \"How do I use it?\"})  # Uses context\n```\n\n### Pattern 3: LangGraph Agent\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, \"messages\"]\n    documents: list\n\ndef retrieve(state):\n    docs = vectorstore.similarity_search(state[\"messages\"][-1])\n    return {\"documents\": docs}\n\ndef generate(state):\n    response = llm.invoke(state[\"messages\"] + state[\"documents\"])\n    return {\"messages\": state[\"messages\"] + [response]}\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"retrieve\", retrieve)\nworkflow.add_node(\"generate\", generate)\nworkflow.set_entry_point(\"retrieve\")\nworkflow.add_edge(\"retrieve\", \"generate\")\nworkflow.add_edge(\"generate\", END)\n\napp = workflow.compile()\n```\n\n## Vector Store Comparison\n\n| Store | Local | Cloud | Best For |\n|-------|-------|-------|----------|\n| FAISS | ‚úì | ‚úó | Development, single-machine |\n| Chroma | ‚úì | ‚úì | Local development, small datasets |\n| Pinecone | ‚úó | ‚úì | Production, large scale |\n| Qdrant | ‚úì | ‚úì | Hybrid, metadata filtering |\n\n## Troubleshooting\n\n### Import Errors\n\n```bash\n# Use langchain_community for loaders and stores\nfrom langchain_community.document_loaders import PDFLoader\nfrom langchain_community.vectorstores import FAISS\n\n# Use langchain_openai for OpenAI integrations\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n```\n\n### API Key Issues\n\n```bash\nexport OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport LANGSMITH_API_KEY=\"lsv2_pt_...\"  # Optional\n```\n\n### Vector Store Persistence\n\n```python\n# Save\nvectorstore.save_local(\"./my_vectorstore\")\n\n# Load\nvectorstore = FAISS.load_local(\n    \"./my_vectorstore\",\n    embeddings,\n    allow_dangerous_deserialization=True  # Only if you trust the source\n)\n```\n\n## Best Practices\n\n1. **Chunk Size**: Start with 1000 tokens, adjust based on domain\n2. **Overlap**: 10-20% of chunk size for context continuity\n3. **Embeddings**: Use OpenAI text-embedding-3-small for cost/performance balance\n4. **Retrieval**: Start with k=4 documents, increase if needed\n5. **Memory**: Use ConversationBufferWindowMemory for long conversations\n6. **Tracing**: Always enable LangSmith in production\n7. **Error Handling**: Wrap chains in try/except, log failures\n8. **Caching**: Cache embeddings and vector stores to reduce API calls\n\n## Resources\n\n- **LangChain Docs**: https://docs.langchain.com/oss/python/langchain/overview\n- **LangGraph Docs**: https://docs.langchain.com/oss/python/langgraph/\n- **LangSmith Docs**: https://docs.langchain.com/langsmith/home\n- **Cookbooks**: https://github.com/langchain-ai/langchain/tree/master/cookbook\n- **Templates**: https://github.com/langchain-ai/langchain/tree/master/templates\n\n## Implementation Workflow\n\nWhen using this skill:\n\n1. **Setup**: Run `setup-langchain.sh` to install dependencies\n2. **Configure**: Set API keys in environment\n3. **Choose Pattern**: Select template based on use case\n4. **Load Documents**: Use appropriate document loaders\n5. **Create Vector Store**: Run `create-vectorstore.sh` or use template\n6. **Build Chain**: Implement retrieval logic\n7. **Test**: Run `test-langchain.sh` to validate\n8. **Iterate**: Adjust chunk size, retrieval params based on results\n9. **Add Observability**: Integrate LangSmith for production\n10. **Deploy**: Monitor with LangSmith, optimize based on metrics\n\n## Advanced Features\n\n### Multi-Modal RAG\n- Image embeddings with CLIP\n- Audio transcription with Whisper\n- Video frame analysis\n\n### Hybrid Search\n- Combine semantic + keyword search\n- BM25 + vector similarity\n- Metadata filtering\n\n### Agent Tools\n- Web search integration\n- API calls within chains\n- Code execution tools\n\n### Production Optimization\n- Async operations for speed\n- Batch processing for scale\n- Caching strategies\n- Rate limiting\n\nThis skill provides everything needed to build production-ready RAG applications with LangChain, from basic retrieval to advanced agent workflows with full observability."
              },
              {
                "name": "llamaindex-patterns",
                "description": "LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.",
                "path": "plugins/rag-pipeline/skills/llamaindex-patterns/SKILL.md",
                "frontmatter": {
                  "name": "llamaindex-patterns",
                  "description": "LlamaIndex implementation patterns with templates, scripts, and examples for building RAG applications. Use when implementing LlamaIndex, building RAG pipelines, creating vector indices, setting up query engines, implementing chat engines, integrating LlamaCloud, or when user mentions LlamaIndex, RAG, VectorStoreIndex, document indexing, semantic search, or question answering systems.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# LlamaIndex Patterns\n\nComprehensive implementation patterns, templates, and examples for building production-ready RAG (Retrieval-Augmented Generation) applications with LlamaIndex.\n\n## Overview\n\nThis skill provides complete, functional implementations for:\n- **RAG Pipeline Setup**: End-to-end document indexing and querying\n- **Custom Retrievers**: Advanced retrieval strategies with filtering and reranking\n- **LlamaCloud Integration**: Managed parsing and hosted indices\n- **Chat Engines**: Conversational AI with memory management\n- **Multi-Document RAG**: Cross-document reasoning and source attribution\n\nAll scripts, templates, and examples are production-ready and fully functional.\n\n## Scripts\n\n### 1. setup-llamaindex.sh\nAutomated LlamaIndex installation with dependency management and environment setup.\n\n```bash\nbash scripts/setup-llamaindex.sh\n```\n\n**Features:**\n- Detects Python version and virtual environment\n- Installs LlamaIndex core and common integrations\n- Sets up vector stores (Chroma, Pinecone, Qdrant)\n- Configures embedding models (OpenAI, HuggingFace)\n- Creates .env template with all required API keys\n- Generates requirements.txt for reproducibility\n- Creates data and storage directories\n- Validates installation\n\n**Output:**\n- `.env` file with API key templates\n- `requirements.txt` with pinned versions\n- `data/` directory for documents\n- `storage/` directory for persisted indices\n- Installation verification report\n\n### 2. create-index.sh\nCreate a VectorStoreIndex from documents with progress tracking.\n\n```bash\nbash scripts/create-index.sh [data_dir] [storage_dir] [index_name]\n```\n\n**Arguments:**\n- `data_dir`: Directory containing documents (default: `./data`)\n- `storage_dir`: Where to persist the index (default: `./storage`)\n- `index_name`: Name for the index (default: `default_index`)\n\n**Features:**\n- Loads documents from multiple formats (.txt, .pdf, .md, .csv, .json, .html)\n- Configures optimal embedding model\n- Shows progress during indexing\n- Displays document statistics\n- Persists index to disk\n- Tests index with sample query\n- Provides usage instructions\n\n**Example:**\n```bash\nbash scripts/create-index.sh ./documents ./indices my_knowledge_base\n```\n\n### 3. test-llamaindex.sh\nComprehensive validation tests for LlamaIndex installation and configuration.\n\n```bash\nbash scripts/test-llamaindex.sh\n```\n\n**Tests:**\n- Python 3 installation and version\n- LlamaIndex core package and version\n- Core imports (VectorStoreIndex, Settings, etc.)\n- Environment file existence and configuration\n- API key setup (OpenAI, Anthropic, etc.)\n- Vector store integrations (Chroma, Pinecone, Qdrant)\n- Embedding models (OpenAI, HuggingFace)\n- LLM integrations (OpenAI, Anthropic)\n- Basic functionality (index creation)\n- Data and storage directories\n\n**Output:**\n- ‚úì for passing tests\n- ‚úó for critical failures\n- ‚ö† for warnings\n- Exit code 0 for success, 1 for failures\n\n## Templates\n\n### 1. basic-rag-pipeline.py\nComplete RAG pipeline implementation with best practices.\n\n**Features:**\n- Document loading from directory\n- Index creation and persistence\n- Query with source attribution\n- Interactive chat interface\n- Configurable LLM and embedding models\n- Error handling and validation\n\n**Key Components:**\n```python\nclass BasicRAGPipeline:\n    def load_or_create_index()  # Smart index loading/creation\n    def query()                  # Simple question answering\n    def query_with_sources()     # Answers with citations\n    def chat()                   # Interactive chat mode\n```\n\n**Usage:**\n```python\nfrom basic_rag_pipeline import BasicRAGPipeline\n\npipeline = BasicRAGPipeline(\n    data_dir=\"./data\",\n    storage_dir=\"./storage\",\n    model=\"gpt-4o-mini\"\n)\n\npipeline.load_or_create_index()\nresponse = pipeline.query(\"What is LlamaIndex?\")\n```\n\n**Use Cases:**\n- Document Q&A systems\n- Knowledge base queries\n- Research assistants\n- Documentation search\n\n### 2. custom-retriever.py\nAdvanced retrieval strategies with filtering, reranking, and hybrid search.\n\n**Retrievers Included:**\n\n**MetadataFilteredRetriever:**\n- Filter results by metadata (category, author, date, etc.)\n- Multi-tenant applications\n- Document versioning\n- Access control\n\n**HybridRetriever:**\n- Combines semantic search with keyword matching\n- Configurable weights for vector vs keyword scores\n- Better results for specific terminology\n- Handles exact phrase matching\n\n**RerankedRetriever:**\n- Two-stage retrieval (broad then narrow)\n- Custom scoring with multiple factors\n- Recency weighting\n- Document quality scoring\n\n**Example:**\n```python\nfrom custom_retriever import MetadataFilteredRetriever\n\nretriever = MetadataFilteredRetriever(\n    index=index,\n    similarity_top_k=10,\n    metadata_filters={\"category\": \"technical\", \"year\": 2024}\n)\n\nnodes = retriever.retrieve(\"How to deploy?\")\n```\n\n**Use Cases:**\n- Filtered search (by category, date, author)\n- Improved accuracy with hybrid search\n- Production systems requiring precise results\n- Applications with diverse document types\n\n### 3. llamacloud-integration.py\nLlamaCloud managed services integration template.\n\n**Features:**\n- LlamaParse for complex document parsing\n- Managed index hosting\n- Production deployment patterns\n- Enterprise-ready architecture\n\n**Components:**\n\n**LlamaParse Integration:**\n- Parse complex PDFs with tables/charts\n- Multi-column layout handling\n- OCR for scanned documents\n- Academic paper processing\n\n**Managed Indices:**\n- Automatic scaling\n- High availability\n- Built-in monitoring\n- Version control\n\n**Example:**\n```python\nfrom llamacloud_integration import LlamaCloudRAG\n\nrag = LlamaCloudRAG(api_key=\"your_key\")\ndocuments = rag.parse_with_llamaparse(\"complex.pdf\")\nrag.create_managed_index(documents, \"prod-index\")\n```\n\n**Use Cases:**\n- Complex document parsing (PDFs with tables)\n- Production RAG applications\n- Enterprise deployments\n- Scalable knowledge bases\n\n**Note:** Requires LlamaCloud account and `llama-parse` package for full functionality. Template includes fallbacks for development.\n\n## Examples\n\n### 1. question-answering.py\nComplete Q&A system with citations and interactive mode.\n\n**Run:**\n```bash\npython examples/question-answering.py\n```\n\n**Features:**\n- Automatic sample document creation\n- Pre-configured example queries\n- Source attribution with relevance scores\n- Interactive chat mode\n- Streaming responses (advanced)\n- Metadata-aware queries\n\n**Demonstrates:**\n- Document loading and indexing\n- Query engine configuration\n- Source node extraction\n- Response synthesis modes\n- Interactive user interfaces\n\n### 2. chatbot-with-memory.py\nConversational AI with memory management and context awareness.\n\n**Run:**\n```bash\npython examples/chatbot-with-memory.py\n```\n\n**Features:**\n- Conversation history tracking\n- Context-aware multi-turn dialogues\n- Memory summarization\n- Session management\n- Custom system prompts\n- Sample knowledge base creation\n\n**Components:**\n```python\nclass ConversationalChatbot:\n    def load_knowledge_base()      # Setup knowledge\n    def initialize_chat_engine()   # Configure memory\n    def chat()                     # Send/receive messages\n    def reset_conversation()       # Clear memory\n    def get_conversation_summary() # History summary\n    def interactive_mode()         # CLI interface\n```\n\n**Commands:**\n- `/help` - Show available commands\n- `/reset` - Reset conversation memory\n- `/summary` - View conversation history\n- `/exit` - Exit chatbot\n\n**Use Cases:**\n- Customer service chatbots\n- Technical support assistants\n- Interactive documentation\n- Educational tutoring systems\n\n### 3. multi-document-rag.py\nAdvanced RAG with cross-document reasoning and filtering.\n\n**Run:**\n```bash\npython examples/multi-document-rag.py\n```\n\n**Features:**\n- Multiple document handling\n- Metadata-based filtering\n- Cross-document queries\n- Source attribution by document\n- Document comparison\n- Category-based search\n\n**Components:**\n```python\nclass MultiDocumentRAG:\n    def build_index()            # Index multiple docs\n    def query_by_category()      # Filtered queries\n    def cross_document_query()   # Search all docs\n    def compare_documents()      # Compare specific docs\n```\n\n**Demonstrates:**\n- Document metadata enrichment\n- Category-based filtering\n- Cross-document reasoning\n- Source tracking by document\n- Comparative analysis\n- Interactive multi-doc search\n\n**Use Cases:**\n- Multi-source research\n- Document comparison\n- Categorized knowledge bases\n- Enterprise document search\n\n## Usage Instructions\n\n### Initial Setup\n\n**Step 1: Install Dependencies**\n```bash\ncd plugins/rag-pipeline/skills/llamaindex-patterns\nbash scripts/setup-llamaindex.sh\n```\n\n**Step 2: Configure API Keys**\nEdit `.env` file:\n```bash\nOPENAI_API_KEY=sk-your-actual-key-here\nANTHROPIC_API_KEY=sk-ant-your-key-here  # Optional\n```\n\n**Step 3: Validate Installation**\n```bash\nbash scripts/test-llamaindex.sh\n```\n\n### Building Your First RAG Application\n\n**Option 1: Using Scripts**\n```bash\n# 1. Add your documents to ./data directory\nmkdir -p data\ncp /path/to/your/docs/* data/\n\n# 2. Create index\nbash scripts/create-index.sh data storage my_index\n\n# 3. Use the index in your code\npython -c \"\nfrom llama_index.core import StorageContext, load_index_from_storage\nstorage_context = StorageContext.from_defaults(persist_dir='storage/my_index')\nindex = load_index_from_storage(storage_context)\nresponse = index.as_query_engine().query('Your question?')\nprint(response)\n\"\n```\n\n**Option 2: Using Templates**\n```bash\n# Copy template to your project\ncp templates/basic-rag-pipeline.py my_rag_app.py\n\n# Customize and run\npython my_rag_app.py\n```\n\n**Option 3: Using Examples**\n```bash\n# Run examples directly\npython examples/question-answering.py\npython examples/chatbot-with-memory.py\npython examples/multi-document-rag.py\n```\n\n### Integration Patterns\n\n**For Next.js Applications:**\n```python\n# Use in API route: app/api/chat/route.ts\n# Create Python backend with FastAPI:\n\nfrom fastapi import FastAPI\nfrom basic_rag_pipeline import BasicRAGPipeline\n\napp = FastAPI()\npipeline = BasicRAGPipeline()\npipeline.load_or_create_index()\n\n@app.post(\"/query\")\nasync def query(question: str):\n    response = pipeline.query(question)\n    return {\"answer\": response}\n```\n\n**For FastAPI Projects:**\n```python\n# Integrate into existing FastAPI app\nfrom contextlib import asynccontextmanager\nfrom basic_rag_pipeline import BasicRAGPipeline\n\nrag_pipeline = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    global rag_pipeline\n    rag_pipeline = BasicRAGPipeline()\n    rag_pipeline.load_or_create_index()\n    yield\n\napp = FastAPI(lifespan=lifespan)\n```\n\n**For Standalone Python Applications:**\n```python\n# Use directly in your Python code\nfrom basic_rag_pipeline import BasicRAGPipeline\n\ndef main():\n    pipeline = BasicRAGPipeline(\n        data_dir=\"./knowledge_base\",\n        storage_dir=\"./indices\"\n    )\n    pipeline.load_or_create_index()\n\n    while True:\n        question = input(\"Ask: \")\n        answer = pipeline.query(question)\n        print(f\"Answer: {answer}\")\n```\n\n## Advanced Patterns\n\n### Custom Node Parsing\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\n\nnode_parser = SentenceSplitter(\n    chunk_size=512,      # Smaller chunks for precise retrieval\n    chunk_overlap=50,    # Overlap for context continuity\n)\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    node_parser=node_parser\n)\n```\n\n### Multi-Index Routing\n```python\n# Use custom retriever template for routing between indices\ntech_index = VectorStoreIndex.from_documents(tech_docs)\nbusiness_index = VectorStoreIndex.from_documents(business_docs)\n\n# Route queries based on content\nif \"technical\" in query.lower():\n    response = tech_index.as_query_engine().query(query)\nelse:\n    response = business_index.as_query_engine().query(query)\n```\n\n### Streaming Responses\n```python\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Your question\")\n\nfor text in streaming_response.response_gen:\n    print(text, end=\"\", flush=True)\n```\n\n### Persisting and Loading Indices\n```python\n# Persist\nindex.storage_context.persist(persist_dir=\"./storage\")\n\n# Load\nfrom llama_index.core import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n```\n\n## Production Deployment\n\n### Environment-Specific Configuration\n```bash\n# Development\nexport OPENAI_API_KEY=sk-dev-key\nexport ENVIRONMENT=development\n\n# Production\nexport OPENAI_API_KEY=sk-prod-key\nexport ENVIRONMENT=production\nexport REDIS_URL=redis://prod-cache:6379  # For caching\n```\n\n### Monitoring and Observability\n```python\n# Enable logging\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Track usage\nfrom llama_index.core import set_global_handler\nset_global_handler(\"simple\")\n```\n\n### Error Handling\n```python\ntry:\n    response = pipeline.query(question)\nexcept Exception as e:\n    logger.error(f\"Query failed: {e}\")\n    response = \"I encountered an error. Please try again.\"\n```\n\n### Rate Limiting\n```python\nfrom ratelimit import limits, sleep_and_retry\n\n@sleep_and_retry\n@limits(calls=10, period=60)  # 10 calls per minute\ndef query_with_rate_limit(question: str):\n    return pipeline.query(question)\n```\n\n## Performance Optimization\n\n### Caching\n```python\n# Enable response caching\nfrom llama_index.core.storage.cache import SimpleCache\n\nSettings.cache = SimpleCache()\n```\n\n### Batch Processing\n```python\n# Process multiple queries efficiently\nquestions = [\"Q1\", \"Q2\", \"Q3\"]\nresponses = [pipeline.query(q) for q in questions]\n```\n\n### Index Optimization\n```python\n# Use appropriate similarity_top_k\nquery_engine = index.as_query_engine(\n    similarity_top_k=3  # Lower for speed, higher for accuracy\n)\n```\n\n## Troubleshooting\n\n### API Key Issues\n```bash\n# Validate environment\nbash scripts/test-llamaindex.sh\n\n# Check .env file\ncat .env | grep OPENAI_API_KEY\n```\n\n### Import Errors\n```bash\n# Reinstall dependencies\nbash scripts/setup-llamaindex.sh\n\n# Verify installation\npython -c \"import llama_index; print(llama_index.__version__)\"\n```\n\n### Index Not Loading\n```python\n# Check storage directory exists\nimport os\nassert os.path.exists(\"./storage\"), \"Storage directory not found\"\n\n# Verify index files\nassert os.path.exists(\"./storage/docstore.json\"), \"Index not persisted\"\n```\n\n### Out of Memory\n```python\n# Reduce chunk size\nnode_parser = SentenceSplitter(chunk_size=256)  # Smaller chunks\n\n# Process documents in batches\nfor batch in document_batches:\n    batch_index = VectorStoreIndex.from_documents(batch)\n    # Merge indices\n```\n\n## References\n\n### Official Documentation\n- LlamaIndex Framework: https://developers.llamaindex.ai/python/framework/\n- VectorStoreIndex: https://developers.llamaindex.ai/python/framework/understanding/\n- Query Engines: https://developers.llamaindex.ai/python/framework/use_cases/q_and_a\n- LlamaCloud: https://docs.cloud.llamaindex.ai/\n\n### GitHub Resources\n- Examples: https://github.com/run-llama/llama_index/tree/main/docs/examples\n- Cookbooks: https://github.com/run-llama/llama_index/tree/main/docs/cookbooks\n\n### Community\n- Discord: https://discord.gg/dGcwcsnxhU\n- Twitter: @llama_index\n- Blog: https://blog.llamaindex.ai/\n\n## Best Practices\n\n1. **Always persist indices** - Avoid rebuilding on every run\n2. **Use appropriate chunk sizes** - Balance between context and precision\n3. **Add metadata** - Enables filtering and better organization\n4. **Monitor token usage** - Track costs in production\n5. **Implement error handling** - Graceful degradation for API failures\n6. **Cache responses** - Reduce API calls for common queries\n7. **Version your indices** - Track changes over time\n8. **Test with real data** - Validate with actual use cases\n9. **Configure embeddings wisely** - Match model to use case\n10. **Document your setup** - Record configurations and decisions"
              },
              {
                "name": "retrieval-patterns",
                "description": "Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.",
                "path": "plugins/rag-pipeline/skills/retrieval-patterns/SKILL.md",
                "frontmatter": {
                  "name": "retrieval-patterns",
                  "description": "Search and retrieval strategies including semantic, hybrid, and reranking for RAG systems. Use when implementing retrieval mechanisms, optimizing search performance, comparing retrieval approaches, or when user mentions semantic search, hybrid search, reranking, BM25, or retrieval optimization.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Retrieval Patterns\n\n**Purpose:** Provide comprehensive retrieval strategies, benchmarking tools, and implementation templates for building high-performance RAG retrieval systems using LlamaIndex and LangChain.\n\n**Activation Triggers:**\n- Implementing semantic search for RAG systems\n- Optimizing retrieval performance and relevance\n- Comparing different retrieval strategies\n- Setting up hybrid search (vector + keyword)\n- Implementing reranking mechanisms\n- Benchmarking retrieval quality\n- Multi-query retrieval patterns\n- Conversational retrieval with context\n\n**Key Resources:**\n- `scripts/benchmark-retrieval.py` - Performance testing for different retrieval methods\n- `scripts/evaluate-retrieval-quality.py` - Quality metrics (precision, recall, MRR, NDCG)\n- `templates/semantic-search.py` - Pure vector similarity search\n- `templates/hybrid-search.py` - Combined vector + BM25 search\n- `templates/reranking.py` - Cross-encoder and LLM-based reranking\n- `templates/multi-query-retrieval.py` - Query expansion and fusion\n- `examples/conversational-retrieval.py` - Context-aware retrieval\n- `examples/metadata-filtering.py` - Filtered retrieval with metadata\n\n## Retrieval Strategy Comparison\n\n### 1. Semantic Search (Vector Only)\n\n**How it works:** Embed query and documents, compute cosine similarity, return top-k matches\n\n**Strengths:**\n- Captures semantic meaning and context\n- Works well with paraphrasing and synonyms\n- Language-agnostic similarity\n\n**Weaknesses:**\n- May miss exact keyword matches\n- Sensitive to embedding model quality\n- Can struggle with rare terms or proper nouns\n\n**When to use:**\n- Natural language queries\n- Conceptual similarity needed\n- Multi-lingual scenarios\n\n**Performance:** ~50-100ms per query (depends on index size)\n\n**Template:** `templates/semantic-search.py`\n\n### 2. Hybrid Search (Vector + BM25)\n\n**How it works:** Combine semantic vector search with keyword-based BM25, merge results using Reciprocal Rank Fusion (RRF)\n\n**Strengths:**\n- Best of both worlds (semantic + lexical)\n- Better recall than either method alone\n- Robust to different query types\n\n**Weaknesses:**\n- More complex implementation\n- Slightly higher latency\n- Requires tuning fusion weights\n\n**When to use:**\n- Production RAG systems (recommended default)\n- Mixed query types (conceptual + factual)\n- When you need high recall\n\n**Performance:** ~100-200ms per query\n\n**Template:** `templates/hybrid-search.py`\n\n**Reciprocal Rank Fusion (RRF) formula:**\n```\nRRF_score(d) = sum(1 / (k + rank_i(d)))\nwhere k = 60 (constant), rank_i(d) = rank of document d in retriever i\n```\n\n### 3. Reranking\n\n**How it works:** Initial retrieval (semantic or hybrid) returns top-k candidates (e.g., 20), then reranker scores all pairs (query, doc) and returns top-n (e.g., 5)\n\n**Strengths:**\n- Significantly improves relevance\n- Captures query-document interactions\n- Can use more powerful models\n\n**Weaknesses:**\n- Adds latency (100-500ms depending on reranker)\n- Requires additional API calls or compute\n- Only improves results within initial candidate set\n\n**When to use:**\n- When quality is critical\n- Production systems with quality requirements\n- After initial hybrid search\n\n**Performance:** +100-500ms additional latency\n\n**Reranker options:**\n- **Cross-encoder models** (most accurate): Cohere rerank, sentence-transformers\n- **LLM-based** (flexible): GPT-4, Claude with ranking prompts\n- **Custom models** (optimized): Fine-tuned for your domain\n\n**Template:** `templates/reranking.py`\n\n### 4. Multi-Query Retrieval\n\n**How it works:** Generate multiple query variations, retrieve for each, deduplicate and fuse results\n\n**Strengths:**\n- Increases recall by covering query ambiguity\n- Handles different phrasings\n- Can improve diversity of results\n\n**Weaknesses:**\n- 3-5x more retrieval calls\n- Higher latency and cost\n- May retrieve less focused results\n\n**When to use:**\n- Complex or ambiguous queries\n- When single query formulation may miss relevant docs\n- Exploration vs. precision use cases\n\n**Performance:** 3-5x base retrieval time\n\n**Template:** `templates/multi-query-retrieval.py`\n\n## Strategy Selection Decision Tree\n\n```\nStart\n‚îÇ\n‚îú‚îÄ Need highest quality? ‚Üí Use Hybrid + Reranking\n‚îÇ\n‚îú‚îÄ Budget/latency constrained?\n‚îÇ  ‚îú‚îÄ Yes ‚Üí Semantic Search (simplest, fastest)\n‚îÇ  ‚îî‚îÄ No ‚Üí Hybrid Search (best default)\n‚îÇ\n‚îú‚îÄ Queries are ambiguous/complex? ‚Üí Multi-Query Retrieval\n‚îÇ\n‚îú‚îÄ Need exact keyword matches? ‚Üí Hybrid Search\n‚îÇ\n‚îî‚îÄ Multilingual or conceptual similarity? ‚Üí Semantic Search\n```\n\n## Benchmarking Framework\n\n### Performance Metrics\n\n**Script:** `scripts/benchmark-retrieval.py`\n\n**Measures:**\n- **Latency** (p50, p95, p99)\n- **Throughput** (queries per second)\n- **Cost** (API calls, compute)\n\n**Usage:**\n```bash\npython scripts/benchmark-retrieval.py \\\n  --strategies semantic,hybrid,reranking \\\n  --queries queries.jsonl \\\n  --num-runs 100 \\\n  --output benchmark-results.json\n```\n\n**Output:**\n```json\n{\n  \"semantic\": {\n    \"latency_p50\": 75.2,\n    \"latency_p95\": 120.5,\n    \"latency_p99\": 180.3,\n    \"throughput\": 13.3,\n    \"cost_per_query\": 0.0001\n  },\n  \"hybrid\": {\n    \"latency_p50\": 145.8,\n    \"latency_p95\": 220.1,\n    \"latency_p99\": 300.5,\n    \"throughput\": 6.9,\n    \"cost_per_query\": 0.0002\n  }\n}\n```\n\n### Quality Metrics\n\n**Script:** `scripts/evaluate-retrieval-quality.py`\n\n**Measures:**\n- **Precision@k**: Percentage of retrieved docs that are relevant\n- **Recall@k**: Percentage of relevant docs that were retrieved\n- **MRR (Mean Reciprocal Rank)**: 1/rank of first relevant result\n- **NDCG (Normalized Discounted Cumulative Gain)**: Graded relevance scoring\n- **Hit Rate**: Percentage of queries with at least 1 relevant result\n\n**Usage:**\n```bash\npython scripts/evaluate-retrieval-quality.py \\\n  --strategy hybrid \\\n  --test-set labeled-queries.jsonl \\\n  --k-values 1,3,5,10 \\\n  --output quality-metrics.json\n```\n\n**Output:**\n```json\n{\n  \"precision@5\": 0.78,\n  \"recall@5\": 0.65,\n  \"mrr\": 0.72,\n  \"ndcg@5\": 0.81,\n  \"hit_rate@5\": 0.92\n}\n```\n\n## Implementation Patterns\n\n### Pattern 1: Simple Semantic Search\n\n**Use case:** Prototype, low latency requirements, conceptual queries\n\n**Template:** `templates/semantic-search.py`\n\n**Implementation:**\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# Initialize\nembed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n\n# Retrieve\nretriever = index.as_retriever(similarity_top_k=5)\nresults = retriever.retrieve(\"query\")\n```\n\n### Pattern 2: Hybrid Search with Ensemble\n\n**Use case:** Production systems, balanced performance/quality\n\n**Template:** `templates/hybrid-search.py`\n\n**Implementation:**\n```python\nfrom langchain.retrievers import EnsembleRetriever, BM25Retriever\nfrom langchain_community.vectorstores import FAISS\n\n# Vector retriever\nvector_retriever = FAISS.from_documents(docs, embeddings).as_retriever(\n    search_kwargs={\"k\": 10}\n)\n\n# BM25 retriever\nbm25_retriever = BM25Retriever.from_documents(docs)\nbm25_retriever.k = 10\n\n# Ensemble with RRF\nensemble = EnsembleRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    weights=[0.5, 0.5]\n)\n\nresults = ensemble.get_relevant_documents(\"query\")\n```\n\n### Pattern 3: Reranking Pipeline\n\n**Use case:** Quality-critical applications\n\n**Template:** `templates/reranking.py`\n\n**Implementation:**\n```python\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\n\n# Initial retrieval (hybrid recommended)\ninitial_results = hybrid_retriever.retrieve(\"query\", top_k=20)\n\n# Rerank\nreranker = CohereRerank(api_key=api_key, top_n=5)\nreranked = reranker.postprocess_nodes(\n    initial_results,\n    query_str=\"query\"\n)\n\n# Use top 5 reranked results\nfinal_results = reranked[:5]\n```\n\n### Pattern 4: Multi-Query Fusion\n\n**Use case:** Complex queries, exploration scenarios\n\n**Template:** `templates/multi-query-retrieval.py`\n\n**Implementation:**\n```python\nfrom langchain.retrievers import MultiQueryRetriever\nfrom langchain.llms import OpenAI\n\n# Generate query variations\nretriever = MultiQueryRetriever.from_llm(\n    retriever=base_retriever,\n    llm=OpenAI(temperature=0.7)\n)\n\n# Automatically generates variations and fuses results\nresults = retriever.get_relevant_documents(\"query\")\n```\n\n## Advanced Patterns\n\n### Conversational Retrieval\n\n**Use case:** Chatbots, multi-turn interactions\n\n**Example:** `examples/conversational-retrieval.py`\n\n**Key features:**\n- Chat history compression\n- Query rewriting with context\n- Follow-up question handling\n\n**Implementation highlights:**\n```python\n# Rewrite query with conversation context\nstandalone_query = rewrite_with_history(current_query, chat_history)\n\n# Retrieve with rewritten query\nresults = retriever.retrieve(standalone_query)\n```\n\n### Metadata Filtering\n\n**Use case:** Filtered search, access control, temporal queries\n\n**Example:** `examples/metadata-filtering.py`\n\n**Key features:**\n- Pre-filtering before vector search\n- Hybrid metadata + semantic filtering\n- Dynamic filter construction\n\n**Implementation highlights:**\n```python\n# Filter by metadata before retrieval\nretriever = index.as_retriever(\n    similarity_top_k=5,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key=\"source\", value=\"documentation\"),\n            DateFilter(key=\"timestamp\", after=\"2024-01-01\")\n        ]\n    )\n)\n```\n\n## Optimization Guidelines\n\n### 1. Embedding Model Selection\n\n**Fast & Cheap:**\n- OpenAI `text-embedding-3-small` (1536 dim, $0.02/1M tokens)\n- Suitable for most use cases\n\n**High Quality:**\n- OpenAI `text-embedding-3-large` (3072 dim, $0.13/1M tokens)\n- Cohere `embed-english-v3.0` (1024 dim, $0.10/1M tokens)\n\n**Domain-Specific:**\n- Fine-tune open-source models (e.g., sentence-transformers)\n\n### 2. Top-K Tuning\n\n**Initial Retrieval:**\n- Semantic only: k=5-10\n- Hybrid: k=10-20 (for reranking)\n- Multi-query: k=3-5 per query\n\n**Final Results:**\n- RAG generation: 3-5 chunks typical\n- Search results: 10-20 results\n\n### 3. Latency Optimization\n\n**Techniques:**\n- Cache embeddings for common queries\n- Use approximate nearest neighbor (ANN) indexes\n- Async/parallel retrieval for multi-query\n- Batch queries when possible\n\n**Target latencies:**\n- Semantic: <100ms\n- Hybrid: <200ms\n- Hybrid + Reranking: <500ms\n\n### 4. Cost Optimization\n\n**Strategies:**\n- Use smaller embedding models for development\n- Implement query caching\n- Batch embedding generation\n- Use open-source rerankers when possible\n\n## Testing & Validation\n\n### Benchmarking Workflow\n\n```bash\n# 1. Collect test queries\n# Create queries.jsonl with representative queries\n\n# 2. Run performance benchmark\npython scripts/benchmark-retrieval.py \\\n  --strategies semantic,hybrid \\\n  --queries queries.jsonl \\\n  --num-runs 100\n\n# 3. Run quality evaluation (requires labeled data)\npython scripts/evaluate-retrieval-quality.py \\\n  --strategy hybrid \\\n  --test-set labeled-queries.jsonl \\\n  --k-values 3,5,10\n\n# 4. Compare results\n# Analyze benchmark-results.json and quality-metrics.json\n```\n\n### A/B Testing\n\n**Pattern:**\n```python\nimport random\n\ndef retrieve_with_strategy(query, user_id):\n    # A/B test: 50% hybrid, 50% semantic\n    strategy = \"hybrid\" if hash(user_id) % 2 == 0 else \"semantic\"\n\n    if strategy == \"hybrid\":\n        return hybrid_retriever.retrieve(query)\n    else:\n        return semantic_retriever.retrieve(query)\n```\n\n## Common Pitfalls\n\n### 1. Using Only Semantic Search\n**Problem:** Misses exact keyword matches\n**Solution:** Default to hybrid search for production\n\n### 2. Not Reranking\n**Problem:** Initial retrieval may have poor ordering\n**Solution:** Add reranking for quality-critical applications\n\n### 3. Wrong Top-K Values\n**Problem:** Too few = miss relevant docs, too many = noise\n**Solution:** Benchmark with your data (typically 5-10 for final results)\n\n### 4. Ignoring Latency\n**Problem:** Multi-query without caching can be slow\n**Solution:** Profile each component, optimize critical paths\n\n### 5. No Quality Metrics\n**Problem:** Can't measure improvement\n**Solution:** Implement evaluation with labeled test set\n\n## Resources\n\n**Scripts:**\n- `benchmark-retrieval.py` - Measure latency, throughput, cost\n- `evaluate-retrieval-quality.py` - Precision, recall, MRR, NDCG\n\n**Templates:**\n- `semantic-search.py` - Vector-only retrieval\n- `hybrid-search.py` - Vector + BM25 with RRF\n- `reranking.py` - Cross-encoder and LLM reranking\n- `multi-query-retrieval.py` - Query expansion and fusion\n\n**Examples:**\n- `conversational-retrieval.py` - Chat context handling\n- `metadata-filtering.py` - Filtered retrieval\n\n**Documentation:**\n- LlamaIndex Retrievers: https://developers.llamaindex.ai/python/framework/understanding/\n- LangChain Retrievers: https://python.langchain.com/docs/modules/data_connection/retrievers/\n- Hybrid Search Guide: https://python.langchain.com/docs/how_to/hybrid/\n- Ensemble Retrievers: https://python.langchain.com/docs/how_to/ensemble_retriever/\n\n---\n\n**Supported Frameworks:** LlamaIndex, LangChain\n**Vector Stores:** FAISS, Chroma, Pinecone, Weaviate, Qdrant\n**Rerankers:** Cohere, cross-encoders, LLM-based\n\n**Best Practice:** Start with hybrid search, add reranking if quality is critical, benchmark with your data"
              },
              {
                "name": "vector-database-configs",
                "description": "Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers",
                "path": "plugins/rag-pipeline/skills/vector-database-configs/SKILL.md",
                "frontmatter": {
                  "name": "vector-database-configs",
                  "description": "Vector database configuration and setup for pgvector, Chroma, Pinecone, Weaviate, Qdrant, and FAISS with comparison guide and migration helpers",
                  "allowed-tools": "Bash, Read, Write, Edit, Grep, Glob"
                },
                "content": "# Vector Database Configurations\n\nProvides comprehensive configuration, setup scripts, and templates for six popular vector databases used in RAG pipelines. Includes database selection guide, functional setup scripts, configuration templates, and migration examples.\n\n## Use When\n\n- Setting up vector storage for RAG pipeline\n- Comparing vector database options for a project\n- Migrating between vector databases\n- Configuring vector indexes and schema\n- Optimizing vector search performance\n- Setting up development or production vector databases\n- Generating database-specific configuration files\n\n## Vector Database Comparison\n\n### Quick Selection Guide\n\n**pgvector (PostgreSQL extension)**\n- Best for: Existing PostgreSQL users, transactional consistency, complex filtering\n- Hosting: Self-hosted, managed Postgres (Supabase, RDS, etc.)\n- Scale: Up to millions of vectors with proper indexing\n- Cost: Free (open source), hosting costs only\n- When to use: You already use PostgreSQL, need ACID guarantees, complex metadata queries\n\n**Chroma**\n- Best for: Development, prototyping, local-first applications\n- Hosting: In-process, client-server, or cloud\n- Scale: Thousands to millions of vectors\n- Cost: Free (open source), cloud pricing for hosted\n- When to use: Fast prototyping, embedded applications, simple deployments\n\n**Pinecone**\n- Best for: Production deployments, managed service, high performance\n- Hosting: Fully managed cloud only\n- Scale: Billions of vectors\n- Cost: Paid service with free tier\n- When to use: Production systems, need managed infrastructure, high scale\n\n**Weaviate**\n- Best for: GraphQL APIs, semantic search, hybrid search\n- Hosting: Self-hosted or cloud\n- Scale: Billions of vectors\n- Cost: Free (open source), cloud pricing for managed\n- When to use: Need GraphQL interface, hybrid keyword+vector search, modules ecosystem\n\n**Qdrant**\n- Best for: High performance, filtering, payload indexing\n- Hosting: Self-hosted, Docker, or cloud\n- Scale: Billions of vectors\n- Cost: Free (open source), cloud pricing for managed\n- When to use: Need advanced filtering, high-performance search, Rust performance\n\n**FAISS**\n- Best for: Research, maximum performance, custom algorithms\n- Hosting: Self-managed in application\n- Scale: Billions of vectors (with proper hardware)\n- Cost: Free (open source)\n- When to use: Maximum performance, custom algorithms, research projects, batch processing\n\n### Feature Comparison\n\n| Feature | pgvector | Chroma | Pinecone | Weaviate | Qdrant | FAISS |\n|---------|----------|--------|----------|----------|--------|-------|\n| Setup Complexity | Medium | Low | Low | Medium | Medium | High |\n| Distance Metrics | L2, Cosine, Inner Product | L2, Cosine, IP | Cosine, Euclidean, Dot | Multiple | Multiple | Multiple |\n| Metadata Filtering | Excellent (SQL) | Good | Good | Excellent (GraphQL) | Excellent | Manual |\n| ACID Transactions | Yes | No | No | No | No | No |\n| Horizontal Scaling | Limited | Yes | Yes | Yes | Yes | Manual |\n| Cloud Managed | Via providers | Yes | Yes (only) | Yes | Yes | No |\n| Open Source | Yes | Yes | No | Yes | Yes | Yes |\n\n## Available Scripts\n\nAll scripts are located in `scripts/` directory and are fully functional:\n\n**setup-pgvector.sh**\n- Installs pgvector extension\n- Creates database and enables extension\n- Sets up vector schema with indexes\n- Configures optimal settings\n\n**setup-chroma.sh**\n- Installs Chroma via pip\n- Initializes persistent client\n- Creates collections\n- Configures embedding functions\n\n**setup-pinecone.sh**\n- Validates API key\n- Creates indexes with specified dimensions\n- Configures metadata indexes\n- Sets up serverless or pod-based deployment\n\n**setup-weaviate.sh**\n- Starts Weaviate with Docker\n- Creates schema classes\n- Configures vectorizer modules\n- Sets up GraphQL endpoint\n\n**setup-qdrant.sh**\n- Starts Qdrant with Docker or installs client\n- Creates collections\n- Configures vector and payload indexes\n- Sets up API endpoints\n\n**setup-faiss.sh**\n- Installs FAISS via conda or pip\n- Creates index with specified algorithm\n- Configures search parameters\n- Sets up serialization\n\n## Available Templates\n\nAll templates are located in `templates/` directory:\n\n**pgvector-schema.sql**\n- Vector column definitions\n- Index creation (IVFFlat, HNSW)\n- Distance function examples\n- Optimized search queries\n\n**chroma-config.py**\n- Client initialization\n- Collection setup\n- Embedding configuration\n- Query patterns\n\n**pinecone-config.py**\n- API authentication\n- Index creation\n- Upsert and query examples\n- Metadata filtering\n\n**weaviate-schema.py**\n- Schema class definition\n- Vector configuration\n- Module setup\n- GraphQL queries\n\n**qdrant-config.py**\n- Collection creation\n- Vector and payload config\n- Search API examples\n- Filter queries\n\n**faiss-config.py**\n- Index factory patterns\n- Training and adding vectors\n- Search configuration\n- Serialization helpers\n\n## Available Examples\n\nLocated in `examples/` directory:\n\n**migration-guide.md**\n- Migration paths between databases\n- Data export/import scripts\n- Schema mapping examples\n- Testing migration accuracy\n\n**performance-tuning.md**\n- Index optimization for each database\n- Query performance tips\n- Batch operation patterns\n- Monitoring and metrics\n\n## Usage Examples\n\n### Compare Databases for Project\n\nRead SKILL.md comparison table and selection guide to choose the right database based on:\n- Existing infrastructure (PostgreSQL ‚Üí pgvector)\n- Scale requirements (Billions ‚Üí Pinecone, Qdrant, Weaviate)\n- Hosting preference (Managed ‚Üí Pinecone, Self-hosted ‚Üí others)\n- Budget constraints (Free tier needed ‚Üí Chroma, pgvector, FAISS)\n- Feature requirements (GraphQL ‚Üí Weaviate, Advanced filtering ‚Üí Qdrant)\n\n### Setup pgvector on Existing PostgreSQL\n\n```bash\n# Run setup script\nbash scripts/setup-pgvector.sh --database mydb --user postgres --dimensions 1536\n\n# Apply schema template\npsql -U postgres -d mydb -f templates/pgvector-schema.sql\n```\n\n### Setup Chroma for Development\n\n```bash\n# Install and configure\nbash scripts/setup-chroma.sh --persist-dir ./chroma_data --collection documents\n\n# Use Python configuration template\ncp templates/chroma-config.py ./config/vector_db.py\n```\n\n### Setup Pinecone for Production\n\n```bash\n# Configure with API key\nexport PINECONE_API_KEY=\"your-key\"\nbash scripts/setup-pinecone.sh --index production-docs --dimensions 1536 --metric cosine\n\n# Use configuration template\ncp templates/pinecone-config.py ./config/vector_db.py\n```\n\n### Migrate from Chroma to Qdrant\n\n```bash\n# Export from Chroma\npython examples/migration-scripts/export-chroma.py --output vectors.json\n\n# Setup Qdrant\nbash scripts/setup-qdrant.sh --collection documents --dimensions 1536\n\n# Import to Qdrant\npython examples/migration-scripts/import-qdrant.py --input vectors.json\n```\n\n## Integration with RAG Pipeline\n\nThis skill integrates with other rag-pipeline components:\n\n**document-chunking skill** ‚Üí Generates chunks to be embedded\n**embedding-models skill** ‚Üí Generates vectors to be stored\n**vector-database-configs** ‚Üí Stores and retrieves vectors\n**retrieval-strategies skill** ‚Üí Queries vector database\n\n## Distance Metrics Guide\n\n**Cosine Similarity**\n- Range: -1 to 1 (higher is more similar)\n- Best for: Text embeddings (OpenAI, Cohere, etc.)\n- Normalized: Yes (magnitude independent)\n\n**Euclidean (L2)**\n- Range: 0 to infinity (lower is more similar)\n- Best for: Image embeddings, spatial data\n- Normalized: No (affected by magnitude)\n\n**Inner Product (Dot Product)**\n- Range: -infinity to infinity (higher is more similar)\n- Best for: When embeddings are normalized\n- Normalized: Depends on vectors\n\n## Performance Optimization Tips\n\n**For All Databases:**\n- Use appropriate index type for scale (HNSW for large datasets)\n- Batch insert operations\n- Monitor query latency and adjust index parameters\n- Pre-filter on metadata before vector search when possible\n\n**pgvector Specific:**\n- Use `ivfflat` for 100K-1M vectors, `hnsw` for 1M+\n- Increase `maintenance_work_mem` for index building\n- Use partial indexes for filtered queries\n- Consider table partitioning for very large datasets\n\n**FAISS Specific:**\n- Choose index based on dataset size (Flat < 10K, IVF < 100M, HNSW for most)\n- Use GPU indices for maximum performance\n- Pre-train IVF indices with representative data\n- Adjust nprobe parameter for accuracy/speed tradeoff\n\n## Error Handling\n\nAll scripts include:\n- Dependency checking before execution\n- Connection validation\n- Clear error messages with resolution steps\n- Rollback on failure where applicable\n- Environment variable validation\n\n## Security Considerations\n\n- Store API keys in environment variables (never hardcode)\n- Use connection pooling for database connections\n- Enable TLS/SSL for network connections\n- Implement authentication for self-hosted deployments\n- Use read-only credentials for query-only applications\n- Sanitize user inputs before using in filters\n\n## References\n\n- pgvector: https://github.com/pgvector/pgvector\n- Supabase pgvector: https://supabase.com/docs/guides/ai/vector-columns\n- Chroma: https://docs.trychroma.com/\n- Pinecone: https://docs.pinecone.io/\n- Weaviate: https://weaviate.io/developers/weaviate\n- Qdrant: https://qdrant.tech/documentation/\n- FAISS: https://faiss.ai/"
              },
              {
                "name": "web-scraping-tools",
                "description": "Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.",
                "path": "plugins/rag-pipeline/skills/web-scraping-tools/SKILL.md",
                "frontmatter": {
                  "name": "web-scraping-tools",
                  "description": "Web scraping templates, scripts, and patterns for documentation and content collection using Playwright, BeautifulSoup, and Scrapy. Includes rate limiting, error handling, and extraction patterns. Use when scraping documentation, collecting web content, extracting structured data, building RAG knowledge bases, harvesting articles, crawling websites, or when user mentions web scraping, documentation collection, content extraction, Playwright scraping, BeautifulSoup parsing, or Scrapy spiders.",
                  "allowed-tools": "Bash, Read, Write, Edit, mcp__playwright"
                },
                "content": "# web-scraping-tools\n\n## Instructions\n\nThis skill provides production-ready web scraping tools, templates, and patterns for documentation collection and content extraction. It includes functional scripts with rate limiting, error handling, and multiple scraping frameworks (Playwright, BeautifulSoup, Scrapy).\n\n### 1. Choose the Right Scraping Tool\n\n**Decision Matrix:**\n\n| Use Case | Tool | Reason |\n|----------|------|--------|\n| JavaScript-heavy sites | Playwright | Full browser rendering |\n| Static HTML parsing | BeautifulSoup | Lightweight, fast parsing |\n| Large-scale crawling | Scrapy | Built-in concurrency, pipelines |\n| Authentication required | Playwright | Cookie/session handling |\n| Simple data extraction | BeautifulSoup | Minimal dependencies |\n| Complex crawling rules | Scrapy | Spider middleware, rules |\n\n### 2. Setup Scraping Environment\n\nInstall required dependencies:\n\n```bash\n# Setup Playwright scraper (includes browser installation)\nbash ./skills/web-scraping-tools/scripts/setup-playwright-scraper.sh\n\n# Install for BeautifulSoup\npip install beautifulsoup4 requests lxml\n\n# Install for Scrapy\npip install scrapy\n```\n\n**What the setup script does:**\n- Installs Playwright with Python bindings\n- Installs Chromium, Firefox, WebKit browsers\n- Configures browser contexts\n- Sets up rate limiting utilities\n- Creates output directories\n\n### 3. Use Functional Scraper Scripts\n\n**Scrape Documentation Sites:**\n```bash\n# Scrape technical documentation with automatic rate limiting\npython ./skills/web-scraping-tools/scripts/scrape-documentation.py \\\n  --url \"https://docs.example.com\" \\\n  --output-dir \"./scraped-docs\" \\\n  --max-depth 3 \\\n  --rate-limit 2\n\n# Parameters:\n# --url: Starting URL to scrape\n# --output-dir: Where to save scraped content\n# --max-depth: How many levels deep to crawl\n# --rate-limit: Seconds to wait between requests\n```\n\n**Features:**\n- Automatic rate limiting (respectful scraping)\n- Error handling and retries\n- Content deduplication\n- Markdown conversion\n- Metadata extraction (title, description, keywords)\n- Progress tracking\n\n**Scrape Articles and Blog Posts:**\n```bash\n# Extract article content with readability parsing\npython ./skills/web-scraping-tools/scripts/scrape-articles.py \\\n  --urls urls.txt \\\n  --output-dir \"./articles\" \\\n  --format markdown\n\n# Input file format (urls.txt):\n# https://blog.example.com/post-1\n# https://blog.example.com/post-2\n```\n\n**Features:**\n- Article content extraction (removes nav, ads, footers)\n- Author and date detection\n- Tag/category extraction\n- Image downloading (optional)\n- Multiple output formats (markdown, json, html)\n\n### 4. Customizable Templates\n\n**Playwright Scraper Template:**\n```bash\n# Copy template for customization\ncp ./skills/web-scraping-tools/templates/playwright-scraper-template.py my-scraper.py\n\n# Edit for your needs:\n# - Update selectors for target site\n# - Customize extraction logic\n# - Add specific data transformations\n# - Configure authentication if needed\n```\n\n**Template Features:**\n- Browser context setup\n- Page navigation with retries\n- Element waiting strategies\n- Screenshot capture\n- Cookie/localStorage handling\n- Concurrent page processing\n\n**Rate-Limited Scraper Template:**\n```bash\n# Copy rate-limited scraper for respectful crawling\ncp ./skills/web-scraping-tools/templates/rate-limited-scraper.py my-crawler.py\n```\n\n**Template Features:**\n- Configurable request delays\n- Exponential backoff on errors\n- Request queue management\n- robots.txt respect\n- User-agent rotation\n- Request logging\n\n### 5. Real-World Examples\n\n**Example 1: Scrape GitHub Documentation**\n```bash\n# Run GitHub docs scraper\npython ./skills/web-scraping-tools/examples/scrape-github-docs.py \\\n  --repo \"microsoft/playwright\" \\\n  --output \"./github-docs\"\n```\n\n**What it does:**\n- Scrapes README, Wiki, Docs folder\n- Extracts code examples\n- Downloads images\n- Creates structured markdown\n- Builds navigation index\n\n**Example 2: Scrape Blog Posts**\n```bash\n# Run blog post scraper\npython ./skills/web-scraping-tools/examples/scrape-blog-posts.py \\\n  --blog-url \"https://blog.example.com\" \\\n  --category \"tutorials\" \\\n  --max-posts 50\n```\n\n**What it does:**\n- Finds all blog posts in category\n- Extracts article content\n- Captures metadata (author, date, tags)\n- Downloads featured images\n- Generates index JSON\n\n### 6. Best Practices for Ethical Scraping\n\n**Always Implement:**\n- Rate limiting (1-2 seconds between requests minimum)\n- User-agent identification\n- robots.txt compliance\n- Error handling and retries\n- Request caching to avoid redundant requests\n\n**Never Do:**\n- Scrape faster than a human would browse\n- Ignore robots.txt\n- Overwhelm small sites with requests\n- Scrape copyrighted content without permission\n- Bypass authentication to access private data\n\n**Respect Website Resources:**\n```python\n# Good: Rate limited with retries\nawait asyncio.sleep(2)  # Wait between requests\nif response.status == 429:  # Too Many Requests\n    await asyncio.sleep(60)  # Back off\n    retry()\n\n# Bad: Rapid fire requests\nfor url in urls:  # No delays\n    scrape(url)  # Hammers server\n```\n\n### 7. Data Extraction Patterns\n\n**CSS Selectors (BeautifulSoup/Playwright):**\n```python\n# Article content\ncontent = soup.select_one('article.main-content')\n\n# All links in navigation\nnav_links = soup.select('nav a[href]')\n\n# Metadata\ntitle = soup.select_one('meta[property=\"og:title\"]')['content']\ndescription = soup.select_one('meta[name=\"description\"]')['content']\n```\n\n**XPath Selectors (Scrapy/Playwright):**\n```python\n# Article with specific class\narticle = page.locator('xpath=//article[@class=\"post\"]')\n\n# All headings\nheadings = response.xpath('//h1 | //h2 | //h3/text()').getall()\n```\n\n**Pagination Handling:**\n```python\n# Follow \"Next\" button\nwhile next_button := page.locator('a.next'):\n    await next_button.click()\n    await page.wait_for_load_state('networkidle')\n    extract_data(page)\n```\n\n### 8. Output Formats\n\n**Markdown (for RAG/Documentation):**\n```python\noutput = {\n    'title': title,\n    'url': url,\n    'content': markdown_content,\n    'metadata': {\n        'author': author,\n        'date': date,\n        'tags': tags\n    }\n}\n```\n\n**JSON (for structured data):**\n```python\n{\n    \"url\": \"https://example.com/page\",\n    \"title\": \"Page Title\",\n    \"content\": \"Full text content...\",\n    \"links\": [\"url1\", \"url2\"],\n    \"images\": [\"img1.jpg\", \"img2.png\"],\n    \"scraped_at\": \"2025-10-31T21:43:00Z\"\n}\n```\n\n**SQLite (for large datasets):**\n```python\n# Store in database for efficient querying\nconn = sqlite3.connect('scraped_data.db')\ncursor.execute('''\n    INSERT INTO pages (url, title, content, scraped_at)\n    VALUES (?, ?, ?, ?)\n''', (url, title, content, datetime.now()))\n```\n\n## Requirements\n\n**Python Dependencies:**\n```\nplaywright>=1.40.0\nbeautifulsoup4>=4.12.0\nrequests>=2.31.0\nlxml>=4.9.0\nscrapy>=2.11.0\naiohttp>=3.9.0\nmarkdownify>=0.11.0\n```\n\n**System Requirements:**\n- Python 3.10+\n- 500MB disk space for browsers (Playwright)\n- Internet connection for scraping\n\n**Installation:**\n```bash\npip install playwright beautifulsoup4 requests lxml scrapy aiohttp markdownify\nplaywright install chromium\n```\n\n## Examples\n\n### Example 1: Scrape Technical Documentation\n\n**Use Case:** Build a RAG knowledge base from Python documentation\n\n```bash\npython ./skills/web-scraping-tools/examples/scrape-github-docs.py \\\n  --repo \"python/cpython\" \\\n  --docs-path \"Doc\" \\\n  --output \"./python-docs\" \\\n  --format markdown\n```\n\n**Result:**\n- 500+ documentation pages as markdown\n- Properly formatted code examples\n- Navigation structure preserved\n- Images downloaded locally\n- Metadata JSON index\n\n### Example 2: Scrape Tutorial Articles\n\n**Use Case:** Collect programming tutorials for training data\n\n```bash\npython ./skills/web-scraping-tools/examples/scrape-blog-posts.py \\\n  --blog-url \"https://realpython.com\" \\\n  --category \"tutorials\" \\\n  --max-posts 100 \\\n  --output \"./tutorials\"\n```\n\n**Result:**\n- 100 tutorial articles in markdown\n- Author and publication date\n- Code examples extracted\n- Tags and categories\n- Index of all articles\n\n### Example 3: Monitor Competitor Content\n\n**Use Case:** Track new posts on competitor blogs\n\n```bash\n# Setup scheduled scraper\npython ./skills/web-scraping-tools/scripts/scrape-articles.py \\\n  --urls competitor-urls.txt \\\n  --output \"./competitor-content\" \\\n  --since-date \"2025-10-01\" \\\n  --notify-new\n```\n\n**Result:**\n- Only new content since last run\n- Email notification on new posts\n- Diff from previous scrape\n- Change tracking\n\n## Troubleshooting\n\n**Playwright browser installation fails:**\n```bash\n# Manual browser installation\nplaywright install chromium\n\n# Or use system browser\nPLAYWRIGHT_BROWSERS_PATH=/usr/bin playwright install\n```\n\n**Rate limiting / 429 errors:**\n- Increase delay between requests (`--rate-limit 5`)\n- Reduce concurrency (`--max-concurrent 1`)\n- Add random jitter to delays\n- Check robots.txt for crawl-delay directive\n\n**JavaScript content not loading:**\n- Use Playwright instead of BeautifulSoup\n- Add explicit waits for elements\n- Wait for network idle: `page.wait_for_load_state('networkidle')`\n\n**Memory issues with large scrapes:**\n- Process pages in batches\n- Clear browser cache between pages\n- Use Scrapy for better memory management\n- Write to disk incrementally, don't hold in memory\n\n**Blocked by anti-bot protection:**\n- Rotate user agents\n- Add realistic delays\n- Use residential proxies (if legal and permitted)\n- Respect robots.txt and Terms of Service\n\n## Performance Optimization\n\n**Concurrent Scraping:**\n```python\n# Use asyncio for parallel requests\nimport asyncio\n\nasync def scrape_all(urls):\n    tasks = [scrape_page(url) for url in urls]\n    return await asyncio.gather(*tasks)\n```\n\n**Caching:**\n```python\n# Cache responses to avoid re-scraping\nimport diskcache\n\ncache = diskcache.Cache('./scrape-cache')\n\nif url in cache:\n    return cache[url]\nelse:\n    content = scrape(url)\n    cache[url] = content\n    return content\n```\n\n**Incremental Updates:**\n```python\n# Track what's already scraped\nscraped_urls = load_scraped_urls()\nnew_urls = [url for url in all_urls if url not in scraped_urls]\nscrape_batch(new_urls)\n```\n\n---\n\n**Plugin:** rag-pipeline\n**Version:** 1.0.0\n**Category:** Data Collection\n**Skill Type:** Web Scraping & Content Extraction"
              }
            ]
          },
          {
            "name": "supabase",
            "description": "Comprehensive Supabase integration for AI applications with database, auth, storage, realtime, and vector search capabilities",
            "source": "./plugins/supabase",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "vanman2024",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install supabase@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-auth",
                "description": null,
                "path": "plugins/supabase/commands/add-auth.md",
                "frontmatter": null,
                "content": "---\ndescription: Add authentication - OAuth providers, email auth, RLS policies with parallel validation\nargument-hint: [provider1,provider2,...] (e.g., google,github,discord)\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Phase 1: Requirements Analysis\nGoal: Parse authentication requirements\n\nActions:\n\nParse provider list from arguments (comma-separated):\n- OAuth providers: google, github, discord, facebook, apple, twitter\n- Email auth: email (includes magic links)\n- SMS auth: sms (requires Twilio)\n\nDefault if no arguments: email,google\n\nVerify environment variables:\n- $SUPABASE_PROJECT_REF\n- $SUPABASE_ACCESS_TOKEN\n- $SUPABASE_DB_URL\n\nCheck for OAuth credentials:\n- Google: $GOOGLE_CLIENT_ID, $GOOGLE_CLIENT_SECRET\n- GitHub: $GITHUB_CLIENT_ID, $GITHUB_CLIENT_SECRET\n- Discord: $DISCORD_CLIENT_ID, $DISCORD_CLIENT_SECRET\n\nIf critical credentials missing, display setup guide and exit.\n\n# Phase 2: Authentication Setup\nGoal: Configure providers and RLS\n\nActions:\n\nInvoke the supabase-security-specialist agent to configure authentication.\n\nPass context:\n- Provider list: [parsed providers]\n- Application type: [detect from schema or ask]\n- Multi-tenant: [yes/no, detect from schema]\n- Tables requiring RLS: [detect from schema]\n\nAgent will:\n- Configure OAuth providers via auth-configs skill\n- Set up email authentication\n- Generate RLS policies via rls-templates skill\n- Apply policies to database\n- Configure auth middleware\n\n# Phase 3: Parallel Validation\nGoal: Validate authentication setup\n\nActions:\n\nLaunch the following validation agents IN PARALLEL (all at once):\n\n**Agent 1 - Schema Validation:**\nInvoke the supabase-schema-validator agent to validate auth-related schema.\nFocus on: RLS policies applied, auth integration, policy naming, no tables without RLS\nDeliverable: Schema validation report\n\n**Agent 2 - Security Audit:**\nInvoke the supabase-security-auditor agent to audit authentication security.\nFocus on: User isolation, multi-tenant isolation, role-based access, anonymous restrictions, RLS coverage, vulnerabilities\nDeliverable: Security audit report with vulnerability scan\n\n**Agent 3 - Auth Workflow Testing:**\nInvoke the supabase-tester agent to test authentication workflows.\nFocus on: OAuth flows, email auth, magic links, password reset, session management, token refresh\nDeliverable: E2E auth test results\n\nWait for ALL validation agents to complete before proceeding.\n\n# Phase 4: Results Summary\nGoal: Present authentication setup report\n\nActions:\n\nAggregate results from all agents:\n\n**Authentication Configuration:**\n- OAuth providers configured: [list with status]\n- Email auth: [enabled/disabled]\n- Magic links: [enabled/disabled]\n- RLS policies applied: [count]\n- Tables protected: [list]\n\n**Validation Results:**\n- Schema validation: [PASS/FAIL with issues]\n- Security audit: [PASS/FAIL with vulnerabilities]\n- Auth workflow tests: [X passed, Y failed]\n\n**OAuth Provider Setup:**\nFor each configured provider:\n- Provider: [name]\n- Status: [configured/failed]\n- Redirect URL: [URL for provider settings]\n- Required credentials: [status]\n\n**Next Steps:**\n1. Configure OAuth credentials in provider dashboards\n2. Test authentication flows in development\n3. Review RLS policies for business logic\n4. Configure auth UI components (optional)\n5. Set up auth middleware\n6. Deploy to production\n\nDisplay integration code examples for TypeScript and Python.\n\nIf any validation failed, display remediation steps.\n"
              },
              {
                "name": "/add-realtime",
                "description": null,
                "path": "plugins/supabase/commands/add-realtime.md",
                "frontmatter": null,
                "content": "---\ndescription: Setup Supabase Realtime - enables realtime on tables, configures subscriptions, presence, broadcast\nargument-hint: [table1,table2,...] [--features=subscriptions,presence,broadcast]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure realtime features using supabase-realtime-builder agent\n\nActions:\n- Invoke supabase-realtime-builder with table list and feature flags\n- Agent will enable realtime and configure features\n- Display realtime configuration and usage examples\n"
              },
              {
                "name": "/add-rls",
                "description": "Add Row Level Security policies - generates and applies RLS policies for tables",
                "path": "plugins/supabase/commands/add-rls.md",
                "frontmatter": {
                  "description": "Add Row Level Security policies - generates and applies RLS policies for tables",
                  "argument-hint": "<table1,table2,...> [--pattern=user-isolation|multi-tenant|role-based]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Apply RLS policies using security-specialist agent\n\nActions:\n- Invoke supabase-security-specialist with table list and pattern\n- Agent uses rls-templates skill to generate policies\n- Apply policies via database-executor\n- Display applied policies"
              },
              {
                "name": "/add-storage",
                "description": "Configure Supabase Storage - creates buckets, sets up RLS policies for file access",
                "path": "plugins/supabase/commands/add-storage.md",
                "frontmatter": {
                  "description": "Configure Supabase Storage - creates buckets, sets up RLS policies for file access",
                  "argument-hint": "<bucket-name> [public|private]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up storage buckets with proper security policies\n\nActions:\n- Parse bucket name and visibility from arguments\n- Use database-executor to create storage bucket via MCP\n- Configure RLS policies for bucket access\n- Display bucket URL and usage examples"
              },
              {
                "name": "/add-ui-components",
                "description": "Install Supabase UI components - adds auth, realtime, file upload React components",
                "path": "plugins/supabase/commands/add-ui-components.md",
                "frontmatter": {
                  "description": "Install Supabase UI components - adds auth, realtime, file upload React components",
                  "argument-hint": [
                    "component1",
                    "component2",
                    "..."
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Supabase UI components using ui-generator agent\n\nActions:\n- Invoke supabase-ui-generator with component list\n- Agent installs and configures UI components\n- Display integration examples"
              },
              {
                "name": "/create-schema",
                "description": "Generate database schema for AI applications - creates tables, relationships, indexes based on app type",
                "path": "plugins/supabase/commands/create-schema.md",
                "frontmatter": {
                  "description": "Generate database schema for AI applications - creates tables, relationships, indexes based on app type",
                  "argument-hint": "<schema-type> [chat|rag|multi-tenant|complete]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Design and create database schema using supabase-architect agent\n\nActions:\n- Invoke supabase-architect with schema type\n- Agent uses schema-patterns skill to generate optimal schema\n- Display schema SQL and apply via database-executor"
              },
              {
                "name": "/deploy-migration",
                "description": "Deploy database migration - applies migration files safely with rollback capability",
                "path": "plugins/supabase/commands/deploy-migration.md",
                "frontmatter": {
                  "description": "Deploy database migration - applies migration files safely with rollback capability",
                  "argument-hint": "<migration-file>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Deploy migration safely using migration-applier agent\n\nActions:\n- Invoke code-reviewer to validate migration first\n- If validation passes, invoke migration-applier to deploy\n- Display migration results and rollback instructions if needed"
              },
              {
                "name": "/generate-types",
                "description": "Generate TypeScript types from database schema",
                "path": "plugins/supabase/commands/generate-types.md",
                "frontmatter": {
                  "description": "Generate TypeScript types from database schema",
                  "argument-hint": [
                    "--output=types/supabase.ts"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate TypeScript types from current schema\n\nActions:\n- Use supabase-database-executor to fetch schema via MCP\n- Generate TypeScript types from schema\n- Write types to specified output file\n- Display generated types location"
              },
              {
                "name": "/init-ai-app",
                "description": "Complete AI application setup - chains schema creation, pgvector setup, auth, realtime, and type generation for a full-stack AI app",
                "path": "plugins/supabase/commands/init-ai-app.md",
                "frontmatter": {
                  "description": "Complete AI application setup - chains schema creation, pgvector setup, auth, realtime, and type generation for a full-stack AI app",
                  "argument-hint": "<app-type> [chat|rag|agents|multi-tenant]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up complete AI application infrastructure by chaining multiple Supabase configuration commands.\n\nPhase 1: Determine App Type\n\nActions:\n- Parse app type from arguments: chat, rag, agents, or multi-tenant\n- If no app type specified, ask user to choose\n- Load appropriate schema pattern based on type\n\nPhase 2: Chain Schema Setup\n\nActions:\n- Invoke /supabase:create-schema $APP_TYPE\n- Wait for schema creation to complete\n- Verify schema created successfully\n\nPhase 3: Chain AI Features\n\nActions:\n- Invoke /supabase:setup-ai\n- Configure pgvector for embeddings\n- Set up AI edge functions\n- Verify AI features configured\n\nPhase 4: Chain Authentication\n\nActions:\n- Invoke /supabase:add-auth\n- Configure OAuth providers\n- Set up RLS policies\n- Verify auth working\n\nPhase 5: Chain Realtime (if applicable)\n\nActions:\n- If app type is chat or multi-tenant:\n  - Invoke /supabase:add-realtime\n  - Configure realtime subscriptions\n  - Verify realtime features\n\nPhase 6: Generate Types\n\nActions:\n- Invoke /supabase:generate-types\n- Generate TypeScript types from schema\n- Verify types generated correctly\n\nPhase 7: Validation\n\nActions:\n- Invoke /supabase:validate-setup\n- Run comprehensive validation\n- Report any issues\n\nPhase 8: Summary\n\nActions:\n- Display complete setup summary:\n  - ‚úÖ Database schema created\n  - ‚úÖ pgvector configured for AI\n  - ‚úÖ Authentication set up\n  - ‚úÖ Realtime enabled (if applicable)\n  - ‚úÖ TypeScript types generated\n  - ‚úÖ Validation passed\n- Show next steps for development"
              },
              {
                "name": "/init",
                "description": "Initialize Supabase in your project - sets up MCP configuration, creates .env, and prepares project for Supabase integration",
                "path": "plugins/supabase/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Supabase in your project - sets up MCP configuration, creates .env, and prepares project for Supabase integration",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Phase 1: Verify Project Context\nGoal: Check existing project setup\n\nActions:\n\nCheck project files:\n- package.json exists (determine project type)\n- .mcp.json exists\n- .env file exists\n\nAsk user for Supabase project details if needed:\n- Project reference (from Supabase dashboard)\n- Access token (from Supabase dashboard)\n\n# Phase 2: Configure MCP Server\nGoal: Add Supabase MCP server to project\n\nActions:\n\nIf .mcp.json doesn't exist, create it with Supabase server configuration.\n\nIf .mcp.json exists, merge Supabase server configuration:\n- Server type: http\n- URL: https://mcp.supabase.com/mcp?project_ref=${SUPABASE_PROJECT_REF}\n- Headers: Authorization with Bearer ${SUPABASE_ACCESS_TOKEN}\n\n# Phase 3: Setup Environment Variables\nGoal: Create or update .env file\n\nActions:\n\nCreate or update .env file with Supabase credentials:\n- SUPABASE_PROJECT_REF\n- SUPABASE_ACCESS_TOKEN\n- SUPABASE_URL\n- SUPABASE_ANON_KEY\n- SUPABASE_SERVICE_ROLE_KEY\n\nEnsure .env is in .gitignore.\n\n# Phase 4: Verify Configuration\nGoal: Validate setup using agent\n\nActions:\n\nInvoke the supabase-project-manager agent to:\n- Verify MCP connectivity\n- Validate project access\n- Confirm configuration is correct\n\n# Phase 5: Summary\nGoal: Display initialization results\n\nActions:\n\nDisplay initialization results:\n- MCP server configured: [status]\n- Environment variables set: [count]\n- Project connection verified: [status]\n\nShow next steps:\n- Use /supabase:create-schema to design your database\n- Use /supabase:add-auth to set up authentication\n- Use /supabase:setup-ai for AI features\n- Use /supabase:validate-setup to check configuration"
              },
              {
                "name": "/setup-ai",
                "description": "Complete AI setup - pgvector, embeddings, schemas, RLS, validation (parallel multi-agent)",
                "path": "plugins/supabase/commands/setup-ai.md",
                "frontmatter": {
                  "description": "Complete AI setup - pgvector, embeddings, schemas, RLS, validation (parallel multi-agent)",
                  "argument-hint": [
                    "--app-type=chat|rag|multi-tenant"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Phase 1: Requirements Gathering\nGoal: Understand AI application requirements\n\nActions:\n\nParse application type from arguments (default: rag):\n- `chat` - Chat application with conversation history\n- `rag` - RAG system with document embeddings\n- `multi-tenant` - Multi-tenant AI platform\n\nDetect required environment variables:\n- $SUPABASE_PROJECT_REF\n- $SUPABASE_ACCESS_TOKEN\n- $SUPABASE_DB_URL\n\nIf any missing, display setup instructions and exit.\n\n# Phase 2: Parallel AI Setup\nGoal: Configure AI features, schema, and security simultaneously\n\nActions:\n\nLaunch the following agents IN PARALLEL (all at once):\n\n**Agent 1 - AI Features Configuration:**\nInvoke the supabase-ai-specialist agent to configure all AI features.\nFocus on: pgvector setup, embedding tables, HNSW/IVFFlat indexes, hybrid search, AI Edge Functions\nTarget: $SUPABASE_DB_URL, $SUPABASE_PROJECT_REF\nDeliverable: Fully configured pgvector with indexes and search capabilities\n\n**Agent 2 - Schema Design:**\nInvoke the supabase-architect agent to design AI-optimized database schema.\nFocus on: App-specific schema (chat/RAG/multi-tenant), relationships, foreign keys, index strategy, auth integration\nTarget: Application type from $ARGUMENTS\nDeliverable: Complete schema migration file ready for deployment\n\n**Agent 3 - Security Configuration:**\nInvoke the supabase-security-specialist agent to implement RLS policies.\nFocus on: User isolation, multi-tenant isolation, role-based access, auth integration, anonymous restrictions\nTarget: All schema tables\nDeliverable: Comprehensive RLS policies for all tables\n\nWait for ALL agents to complete before proceeding.\n\nCollect outputs:\n- AI specialist: pgvector configuration, index details\n- Architect: Schema file path, table summary\n- Security specialist: RLS policy count, coverage report\n\n# Phase 3: Schema Deployment\nGoal: Apply generated schema and policies\n\nActions:\n\nDisplay schema summary from architect output.\n\nAsk user for confirmation to deploy:\n- Table count, index count, RLS policies\n- Migration file path\n\nIf confirmed:\n- Execute migration using schema deployment\n- Verify tables, indexes, and RLS policies created\n\nIf deployment fails, display error and rollback instructions.\n\n# Phase 4: Parallel Validation\nGoal: Validate all configurations\n\nActions:\n\nLaunch the following validation agents IN PARALLEL (all at once):\n\n**Agent 1 - Schema Validation:**\nInvoke the supabase-schema-validator agent to validate deployed schema.\nFocus on: SQL syntax, naming conventions, constraints, index coverage, RLS presence\nDeliverable: Schema validation report\n\n**Agent 2 - Security Audit:**\nInvoke the supabase-security-auditor agent to audit security configuration.\nFocus on: RLS testing, user isolation, multi-tenant isolation, role permissions, vulnerabilities, coverage\nDeliverable: Security audit report\n\n**Agent 3 - E2E Testing:**\nInvoke the supabase-tester agent to run AI feature tests.\nFocus on: Vector search, embedding insertion/retrieval, hybrid search, AI Edge Functions, E2E AI workflows\nDeliverable: E2E test results\n\nWait for ALL validation agents to complete before proceeding.\n\n# Phase 5: Results Summary\nGoal: Present comprehensive setup report\n\nActions:\n\nAggregate results from all agents:\n\n**AI Features:**\n- pgvector extension: [status]\n- Embedding tables: [names]\n- Vector indexes: [HNSW/IVFFlat details]\n- Hybrid search: [status]\n- Edge Functions: [names]\n\n**Database Schema:**\n- Tables created: [count and names]\n- Indexes created: [count and types]\n- Foreign keys: [count]\n- Migration file: [path]\n\n**Security:**\n- RLS policies: [count]\n- Policy coverage: [percentage]\n- Auth integration: [status]\n\n**Validation Results:**\n- Schema validation: [PASS/FAIL]\n- Security audit: [PASS/FAIL]\n- E2E tests: [X passed, Y failed]\n\n**Next Steps:**\n1. Review validation issues if any\n2. Test AI features with sample data\n3. Configure Edge Functions for production\n4. Set up monitoring and logging\n5. Deploy to production\n\nDisplay integration code examples for TypeScript and Python.\n\nIf any validation failed, display remediation steps."
              },
              {
                "name": "/setup-pgvector",
                "description": null,
                "path": "plugins/supabase/commands/setup-pgvector.md",
                "frontmatter": null,
                "content": "---\ndescription: Configure pgvector for vector search - enables extension, creates embedding tables, sets up HNSW/IVFFlat indexes\nargument-hint: [--dimensions=1536] [--index=hnsw|ivfflat]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up pgvector for AI embeddings using ai-specialist agent\n\nActions:\n- Invoke supabase-ai-specialist agent with dimension and index preferences\n- Agent uses pgvector-setup skill to configure extension\n- Display vector search setup and usage examples\n"
              },
              {
                "name": "/test-e2e",
                "description": null,
                "path": "plugins/supabase/commands/test-e2e.md",
                "frontmatter": null,
                "content": "---\ndescription: Run end-to-end tests - parallel test execution across database, auth, realtime, AI features\nargument-hint: [--suite=all|auth|database|realtime|ai|storage] [--parallel]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Phase 1: Test Configuration\nGoal: Parse test requirements and prepare environment\n\nActions:\n\nParse test suite from arguments (default: all):\n- `all` - All test suites in parallel\n- `auth` - Authentication tests only\n- `database` - Database CRUD tests only\n- `realtime` - Realtime features only\n- `ai` - AI/pgvector features only\n- `storage` - Storage operations only\n\nVerify environment variables:\n- $SUPABASE_PROJECT_REF\n- $SUPABASE_ACCESS_TOKEN\n- $SUPABASE_DB_URL\n- $SUPABASE_ANON_KEY\n- $SUPABASE_SERVICE_KEY\n\nDetect configured features from project to scope tests appropriately.\n\n# Phase 2: Test Environment Setup\nGoal: Prepare clean test environment\n\nActions:\n\nInvoke the supabase-tester agent for environment setup:\n- Clean previous test data\n- Seed test data\n- Verify test isolation\n\n# Phase 3: Parallel Test Execution\nGoal: Run comprehensive test suites in parallel\n\nActions:\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Suite: all (default - maximum parallelism)\n\nLaunch the following test agents IN PARALLEL (all at once):\n\n**Agent 1 - Database Tests:**\nInvoke the supabase-tester agent to run database test suite.\nFocus on: CRUD operations, queries, transactions, constraints, triggers, functions\nDeliverable: Database test results with performance metrics\n\n**Agent 2 - Auth Tests:**\nInvoke the supabase-tester agent to run authentication test suite.\nFocus on: OAuth flows, email auth, magic links, password reset, sessions, MFA\nDeliverable: Auth test results with flow success rates\n\n**Agent 3 - Realtime Tests (if configured):**\nInvoke the supabase-tester agent to run realtime test suite.\nFocus on: Subscriptions, presence, broadcast, connection stability\nDeliverable: Realtime test results with latency metrics\n\n**Agent 4 - AI Features Tests (if pgvector configured):**\nInvoke the supabase-tester agent to run AI features test suite.\nFocus on: Vector search, embeddings, hybrid search, Edge Functions\nDeliverable: AI test results with search accuracy and performance\n\n**Agent 5 - Storage Tests (if buckets exist):**\nInvoke the supabase-tester agent to run storage test suite.\nFocus on: Upload/download, bucket policies, CDN, metadata\nDeliverable: Storage test results with upload/download speeds\n\n**Agent 6 - Schema Validation:**\nInvoke the supabase-schema-validator agent to validate schema integrity.\nFocus on: Foreign keys, constraints, indexes, data consistency\nDeliverable: Schema integrity report\n\n**Agent 7 - Security Validation:**\nInvoke the supabase-security-auditor agent to validate RLS during tests.\nFocus on: RLS enforcement, user isolation, no data leaks\nDeliverable: Security validation report\n\nWait for ALL test agents to complete before proceeding.\n\n## Suite-Specific Execution\n\nFor suite-specific tests (auth, database, etc.), launch only relevant agents in parallel based on the selected suite.\n\n# Phase 4: Results Aggregation\nGoal: Collect and analyze test results\n\nActions:\n\nAggregate test results from all agents:\n\nDisplay overall test summary:\n- Total tests run, passed, failed, skipped\n- Test results by suite (database, auth, realtime, AI, storage)\n- Performance metrics (query times, latency, speeds)\n- Schema integrity status\n- Security validation status\n\n# Phase 5: Failure Analysis\nGoal: Categorize and analyze failures\n\nActions:\n\nFor each failed test:\n- Test name and suite\n- Failure type and error message\n- Root cause analysis\n- Impact assessment (critical/high/medium/low)\n- Fix suggestion\n\nGroup failures by category:\n- Configuration issues\n- Code bugs\n- Performance issues\n- Security issues\n- Infrastructure issues\n\n# Phase 6: Recommendations\nGoal: Provide actionable next steps\n\nActions:\n\n**If all tests passed:**\nDisplay success message with production readiness confirmation.\n\n**If tests failed:**\nPrioritize fixes by severity (CRITICAL > HIGH > MEDIUM > LOW).\n\nDisplay next steps:\n1. Fix all CRITICAL failures immediately\n2. Address HIGH priority issues\n3. Re-run failed suites: `/supabase:test-e2e --suite=X`\n4. Run full suite before deployment\n5. Set up CI/CD for automated testing\n\nSave test results to:\n- `supabase-test-results-{timestamp}.json`\n- `supabase-test-results-{timestamp}.md`\n- `supabase-performance-{timestamp}.json`\n"
              },
              {
                "name": "/test-rls",
                "description": "Test RLS policy enforcement - validates Row Level Security policies work correctly",
                "path": "plugins/supabase/commands/test-rls.md",
                "frontmatter": {
                  "description": "Test RLS policy enforcement - validates Row Level Security policies work correctly",
                  "argument-hint": "<table1,table2,...>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Test RLS policies using security-auditor agent\n\nActions:\n- Invoke supabase-security-auditor with table list\n- Agent uses rls-test-patterns skill to test policies\n- Display test results showing user isolation, policy coverage, vulnerabilities"
              },
              {
                "name": "/validate-schema",
                "description": "Validate database schema integrity - checks constraints, indexes, naming conventions",
                "path": "plugins/supabase/commands/validate-schema.md",
                "frontmatter": {
                  "description": "Validate database schema integrity - checks constraints, indexes, naming conventions",
                  "argument-hint": [
                    "schema-file.sql|--live"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Validate schema using schema-validator agent\n\nActions:\n- Invoke supabase-schema-validator with schema file or live database flag\n- Agent uses schema-validation skill to check integrity\n- Display validation report with issues and recommendations"
              },
              {
                "name": "/validate-setup",
                "description": null,
                "path": "plugins/supabase/commands/validate-setup.md",
                "frontmatter": null,
                "content": "---\ndescription: Validate Supabase setup - MCP connectivity, configuration, security, schema (parallel validation)\nargument-hint: [--production] [--suite=all|mcp|schema|security|performance]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Phase 1: Requirements Analysis\nGoal: Parse validation requirements\n\nActions:\n\nParse arguments:\n- `--production` - Enable production-level checks (stricter requirements)\n- `--suite` - Validation suite selection:\n  - `all` (default) - All validation checks\n  - `mcp` - MCP connectivity and configuration only\n  - `schema` - Database schema validation only\n  - `security` - Security and RLS validation only\n  - `performance` - Performance checks only\n\nVerify environment variables:\n- $SUPABASE_PROJECT_REF\n- $SUPABASE_ACCESS_TOKEN\n- $SUPABASE_DB_URL\n\nSet validation level:\n- Development: Warnings allowed, some checks optional\n- Production: Zero warnings, all checks mandatory\n\n# Phase 2: Parallel Validation Execution\nGoal: Run comprehensive validation checks in parallel\n\nActions:\n\n## Available Skills\n\nThis commands has access to the following skills from the supabase plugin:\n\n- **auth-configs**: Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.\n- **e2e-test-scenarios**: End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.\n- **pgvector-setup**: Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.\n- **rls-templates**: Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.\n- **rls-test-patterns**: RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.\n- **schema-patterns**: Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.\n- **schema-validation**: Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Suite: all (default)\n\nLaunch the following validation agents IN PARALLEL (all at once):\n\n**Agent 1 - MCP & Configuration Validation:**\nInvoke the supabase-validator agent to validate MCP and project configuration.\nFocus on: MCP connectivity, environment variables, API keys, database URL, Supabase CLI\nValidation level: $PRODUCTION_FLAG\nDeliverable: MCP connectivity report\n\n**Agent 2 - Schema Validation:**\nInvoke the supabase-schema-validator agent to validate database schema.\nFocus on: SQL syntax, naming conventions, constraints, indexes, foreign keys, primary keys\nValidation level: $PRODUCTION_FLAG\nDeliverable: Schema validation report\n\n**Agent 3 - Security Validation:**\nInvoke the supabase-security-auditor agent to audit security configuration.\nFocus on: RLS coverage, user isolation, multi-tenant isolation, role-based access, vulnerabilities\nValidation level: $PRODUCTION_FLAG\nDeliverable: Security audit report\n\n**Agent 4 - E2E Workflow Validation:**\nInvoke the supabase-tester agent to test critical workflows.\nFocus on: Database ops, auth flows, realtime, vector search, storage, Edge Functions\nValidation level: $PRODUCTION_FLAG\nDeliverable: E2E workflow test results\n\nWait for ALL validation agents to complete before proceeding.\n\n## Suite-Specific Validation\n\nFor suite-specific validation (mcp, schema, security, performance), launch only relevant agents based on the selected suite.\n\n# Phase 3: Results Aggregation\nGoal: Collect and analyze validation results\n\nActions:\n\nAggregate results from all validation agents:\n\nDisplay overall status: PASS | FAIL | WARNING\n\nShow validation results:\n- MCP & Configuration: connectivity status, environment variables, issues found\n- Schema Validation: tables validated, syntax errors, naming violations, constraint issues\n- Security Validation: RLS coverage, tables without RLS, vulnerabilities, critical issues\n- Workflow Validation: database ops, auth workflows, realtime, vector search, storage\n\n# Phase 4: Issue Categorization\nGoal: Categorize issues by severity\n\nActions:\n\nCategorize all issues:\n\n**CRITICAL** (must fix for production):\n- MCP connectivity failures\n- RLS policies missing on user data tables\n- SQL syntax errors\n- Missing primary keys\n- Auth configuration failures\n\n**HIGH** (should fix before production):\n- Missing indexes on foreign keys\n- Naming convention violations\n- Incomplete RLS coverage\n- Performance bottlenecks\n\n**MEDIUM** (recommended fixes):\n- Optimization opportunities\n- Documentation gaps\n- Test coverage improvements\n\n**LOW** (nice to have):\n- Code style suggestions\n- Minor optimizations\n\n# Phase 5: Production Readiness Assessment\nGoal: Determine production readiness\n\nActions:\n\nIf --production flag is set:\n\nCalculate production readiness score:\n- MCP & Configuration: score/100\n- Schema Quality: score/100\n- Security: score/100\n- Workflow Reliability: score/100\n- Overall Score: average/100\n\nProduction Readiness: READY | NOT READY\n\nRequirements for production:\n- Zero critical issues\n- Zero high-priority security issues\n- 100% RLS coverage on user tables\n- All auth workflows passing\n- All migrations validated\n- Performance benchmarks met\n- MCP connectivity stable\n\nIf NOT READY, display blocking issues and remediation steps.\n\n# Phase 6: Remediation Guidance\nGoal: Provide actionable fix instructions\n\nActions:\n\nFor each issue category, provide:\n- Issue description\n- Location (file:line or table/policy name)\n- Impact explanation\n- Step-by-step fix instructions\n- Verification method\n\nDisplay next steps:\n1. Fix all CRITICAL issues immediately\n2. Address HIGH priority issues before production\n3. Review and implement MEDIUM priority fixes\n4. Consider LOW priority optimizations\n5. Re-run validation: `/supabase:validate-setup --production`\n6. Review production readiness checklist\n7. Deploy when all checks pass\n\nSave validation reports to:\n- `supabase-validation-report-{timestamp}.json`\n- `supabase-validation-report-{timestamp}.md`\n\nIf production ready, display congratulations and deployment instructions.\n"
              }
            ],
            "skills": [
              {
                "name": "auth-configs",
                "description": "Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.",
                "path": "plugins/supabase/skills/auth-configs/SKILL.md",
                "frontmatter": {
                  "name": "auth-configs",
                  "description": "Configure Supabase authentication providers (OAuth, JWT, email). Use when setting up authentication, configuring OAuth providers (Google/GitHub/Discord), implementing auth flows, configuring JWT settings, or when user mentions Supabase auth, social login, authentication setup, or auth configuration.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# auth-configs\n\n## Instructions\n\nThis skill provides complete authentication configuration for Supabase-powered AI applications. It covers OAuth provider setup, JWT configuration, email authentication with PKCE flow, and auth middleware templates.\n\n### 1. OAuth Provider Setup\n\nConfigure social login providers for your Supabase project:\n\n**Supported Providers:**\n- Google - Best for consumer apps, Google Workspace integration\n- GitHub - Ideal for developer tools, technical audiences\n- Discord - Perfect for community-driven AI applications\n- Facebook, Apple, Microsoft Azure, Twitter, LinkedIn, Slack, and 20+ more\n\n**Setup Process:**\n```bash\n# Configure OAuth provider (creates config, provides setup instructions)\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/setup-oauth-provider.sh google\n\n# Or use template directly\ncat /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/templates/oauth-providers/google-oauth-config.json\n```\n\n**Provider Setup Steps:**\n1. Create OAuth application in provider console (Google Cloud, GitHub Settings, etc)\n2. Configure authorized redirect URIs (template provides exact URLs)\n3. Copy Client ID and Client Secret\n4. Update Supabase project auth settings\n5. Test authentication flow\n\n### 2. JWT Configuration\n\nConfigure JSON Web Token settings for secure session management:\n\n```bash\n# Set up JWT signing secrets and configuration\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/configure-jwt.sh\n```\n\n**JWT Settings:**\n- Signing algorithm (HS256 recommended for most apps)\n- Token expiration times (access and refresh tokens)\n- JWT secret rotation\n- Custom claims for role-based access\n\n### 3. Email Authentication with PKCE Flow\n\nConfigure secure email authentication for server-side rendering:\n\n```bash\n# Set up email auth with PKCE flow for SSR applications\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/setup-email-auth.sh\n```\n\n**Email Auth Features:**\n- Password-based authentication\n- Magic link (passwordless) login\n- Email verification templates\n- Password reset flow\n- PKCE flow for SSR security\n\n### 4. Auth Middleware & Helpers\n\nUse pre-built middleware templates for Next.js and other frameworks:\n\n**Next.js Middleware:**\n```typescript\n// Copy template and customize\ncp /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/templates/middleware/auth-middleware.ts ./middleware.ts\n```\n\n**Auth Helper Functions:**\n```typescript\n// Reusable auth utilities\ncp /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/templates/helpers/auth-helpers.ts ./lib/auth.ts\n```\n\n### 5. Testing Authentication Flows\n\nValidate your authentication setup end-to-end:\n\n```bash\n# Test all configured auth flows\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/test-auth-flow.sh\n```\n\n**Tests Include:**\n- OAuth provider redirect flows\n- Email/password authentication\n- Session persistence\n- Token refresh handling\n- Protected route access\n\n## Examples\n\n### Example 1: Setting Up Google OAuth for AI Chat Application\n\n```bash\n# 1. Run OAuth setup script\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/setup-oauth-provider.sh google\n\n# 2. Follow prompts to configure:\n#    - Google Cloud Console OAuth app\n#    - Authorized redirect URIs\n#    - Client credentials in Supabase\n\n# 3. Add middleware to Next.js app\ncp /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/templates/middleware/auth-middleware.ts ./middleware.ts\n\n# 4. Test the flow\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/test-auth-flow.sh\n```\n\n**Result:** Fully configured Google OAuth with protected routes and session management\n\n### Example 2: Multi-Provider Setup for RAG Application\n\nConfigure multiple OAuth providers for user choice:\n\n```bash\n# Set up Google, GitHub, and Discord\nfor provider in google github discord; do\n  bash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/setup-oauth-provider.sh $provider\ndone\n\n# Configure email auth as fallback\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/setup-email-auth.sh\n\n# Test all providers\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/test-auth-flow.sh --all\n```\n\n**Result:** Users can sign in with Google, GitHub, Discord, or email\n\n### Example 3: AI Platform with Role-Based Access\n\nConfigure JWT claims for AI model access control:\n\n```bash\n# 1. Set up JWT with custom claims\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/auth-configs/scripts/configure-jwt.sh\n\n# 2. Add role-based middleware\n# Edit middleware.ts to check JWT claims for AI model permissions\n\n# 3. Configure RLS policies in Supabase\n# Link JWT claims to database row-level security\n```\n\n**Result:** Different user tiers (free, pro, enterprise) with model access control\n\n## Requirements\n\n**Environment Variables:**\n- `SUPABASE_URL` - Your Supabase project URL\n- `SUPABASE_ANON_KEY` - Public anonymous key\n- `SUPABASE_SERVICE_ROLE_KEY` - Service role key (for admin operations)\n- Provider-specific credentials (Google, GitHub, etc Client IDs and Secrets)\n\n**Dependencies:**\n- `@supabase/supabase-js` - Supabase JavaScript client\n- `@supabase/ssr` - Server-side rendering support (replaces deprecated auth-helpers)\n- Node.js 18+ or compatible runtime\n- jq (for JSON processing in scripts)\n\n**Supabase Project Setup:**\n- Active Supabase project (free tier works)\n- Email authentication enabled in project settings\n- Custom SMTP configured (optional, for branded emails)\n\n**For OAuth Providers:**\n- Developer accounts on each platform (Google Cloud, GitHub, Discord Developer Portal)\n- Ability to create OAuth applications\n- Access to configure redirect URIs\n\n## AI Application Patterns\n\n**Multi-User AI Chat:**\n- OAuth for quick onboarding\n- Session-based conversation history\n- User-specific API usage tracking\n\n**RAG Systems:**\n- Email auth for document ownership\n- JWT claims for data access control\n- Secure document storage per user\n\n**AI API Platforms:**\n- OAuth for developer authentication\n- JWT tokens for API key management\n- Rate limiting per user tier\n\n## Security Best Practices\n\n**Never Hardcode Secrets:**\n```bash\n# ‚úÖ CORRECT - Use environment variables\nexport GOOGLE_CLIENT_SECRET=\"your-secret-here\"\n\n# ‚ùå WRONG - Never commit secrets\nconst secret = \"GOCSPX-abc123...\" // DON'T DO THIS\n```\n\n**Use PKCE Flow for SSR:**\n- Required for Next.js, SvelteKit, Remix\n- Prevents authorization code interception\n- Mandatory for production applications\n\n**Validate Redirect URLs:**\n- Whitelist exact redirect URIs in provider console\n- Use HTTPS in production (HTTP only for localhost development)\n- Never allow wildcard redirects\n\n**Rotate JWT Secrets:**\n- Change JWT signing secret periodically\n- Use strong random strings (min 32 characters)\n- Store in environment variables, never in code\n\n---\n\n**Plugin:** supabase\n**Version:** 1.0.0\n**Category:** Authentication\n**Skill Type:** Configuration"
              },
              {
                "name": "e2e-test-scenarios",
                "description": "End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.",
                "path": "plugins/supabase/skills/e2e-test-scenarios/SKILL.md",
                "frontmatter": {
                  "name": "e2e-test-scenarios",
                  "description": "End-to-end testing scenarios for Supabase - complete workflow tests from project creation to AI features, validation scripts, and comprehensive test suites. Use when testing Supabase integrations, validating AI workflows, running E2E tests, verifying production readiness, or when user mentions Supabase testing, E2E tests, integration testing, pgvector testing, auth testing, or test automation.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# e2e-test-scenarios\n\n## Instructions\n\nThis skill provides comprehensive end-to-end testing capabilities for Supabase applications, covering database operations, authentication flows, AI features (pgvector), realtime subscriptions, and production readiness validation.\n\n### Phase 1: Setup Test Environment\n\n1. Initialize test environment:\n   ```bash\n   bash scripts/setup-test-env.sh\n   ```\n   This creates:\n   - Test database configuration\n   - Environment variables for testing\n   - Test data fixtures\n   - CI/CD configuration templates\n\n2. Configure test database:\n   - Use dedicated test project or local Supabase instance\n   - Never run tests against production\n   - Set `SUPABASE_TEST_URL` and `SUPABASE_TEST_ANON_KEY`\n   - Enable pgTAP extension for database tests\n\n3. Install dependencies:\n   ```bash\n   npm install --save-dev @supabase/supabase-js jest @types/jest\n   # or\n   pnpm add -D @supabase/supabase-js vitest\n   ```\n\n### Phase 2: Database Workflow Testing\n\nTest complete database operations from schema to queries:\n\n1. Run database E2E tests:\n   ```bash\n   bash scripts/test-database-workflow.sh\n   ```\n\n   This validates:\n   - Schema creation and migrations\n   - Table relationships and foreign keys\n   - RLS policies enforcement\n   - Triggers and functions\n   - Data integrity constraints\n\n2. Use pgTAP for SQL-level tests:\n   ```bash\n   # Run all database tests\n   supabase test db\n\n   # Run specific test file\n   supabase test db --file tests/database/users.test.sql\n   ```\n\n3. Test schema migrations:\n   ```bash\n   # Test migration up/down\n   supabase db reset --linked\n   supabase db push\n\n   # Verify schema state\n   bash scripts/validate-schema.sh\n   ```\n\n### Phase 3: Authentication Flow Testing\n\nTest complete auth workflows end-to-end:\n\n1. Run authentication E2E tests:\n   ```bash\n   bash scripts/test-auth-workflow.sh\n   ```\n\n   This validates:\n   - Email/password signup and login\n   - Magic link authentication\n   - OAuth provider flows (Google, GitHub, etc.)\n   - Session management and refresh\n   - Password reset flows\n   - Email verification\n   - Multi-factor authentication (MFA)\n\n2. Test RLS with authenticated users:\n   ```typescript\n   // See templates/auth-tests.ts for complete examples\n   test('authenticated users see only their data', async () => {\n     const { data, error } = await supabase\n       .from('private_notes')\n       .select('*');\n\n     expect(data).toHaveLength(userNoteCount);\n     expect(data.every(note => note.user_id === userId)).toBe(true);\n   });\n   ```\n\n3. Test session persistence:\n   ```bash\n   # Run session validation tests\n   npm test -- auth-session.test.ts\n   ```\n\n### Phase 4: AI Features Testing (pgvector)\n\nTest vector search and semantic operations:\n\n1. Run AI features E2E tests:\n   ```bash\n   bash scripts/test-ai-features.sh\n   ```\n\n   This validates:\n   - pgvector extension is enabled\n   - Embedding storage and retrieval\n   - Vector similarity search accuracy\n   - HNSW/IVFFlat index performance\n   - Hybrid search (keyword + semantic)\n   - Embedding dimension consistency\n\n2. Test vector search accuracy:\n   ```typescript\n   // See templates/vector-search-tests.ts\n   test('semantic search returns relevant results', async () => {\n     const queryEmbedding = await generateEmbedding('machine learning');\n\n     const { data } = await supabase.rpc('match_documents', {\n       query_embedding: queryEmbedding\n       match_threshold: 0.7\n       match_count: 5\n     });\n\n     expect(data.length).toBeGreaterThan(0);\n     expect(data[0].similarity).toBeGreaterThan(0.7);\n   });\n   ```\n\n3. Test embedding operations:\n   ```bash\n   # Validate embedding pipeline\n   npm test -- embedding-workflow.test.ts\n   ```\n\n4. Performance benchmarking:\n   ```bash\n   # Run performance tests\n   bash scripts/benchmark-vector-search.sh [TABLE_NAME] [VECTOR_DIM]\n   ```\n\n### Phase 5: Realtime Features Testing\n\nTest realtime subscriptions and presence:\n\n1. Run realtime E2E tests:\n   ```bash\n   bash scripts/test-realtime-workflow.sh\n   ```\n\n   This validates:\n   - Database change subscriptions\n   - Broadcast messaging\n   - Presence tracking\n   - Connection handling\n   - Reconnection logic\n   - Subscription cleanup\n\n2. Test database change subscriptions:\n   ```typescript\n   // See templates/realtime-tests.ts\n   test('receives real-time updates on insert', async () => {\n     const updates = [];\n\n     const subscription = supabase\n       .channel('test-channel')\n       .on('postgres_changes'\n         { event: 'INSERT', schema: 'public', table: 'messages' }\n         (payload) => updates.push(payload)\n       )\n       .subscribe();\n\n     await supabase.from('messages').insert({ content: 'test' });\n\n     await waitFor(() => expect(updates).toHaveLength(1));\n   });\n   ```\n\n3. Test presence and broadcast:\n   ```bash\n   # Run presence tests\n   npm test -- presence.test.ts\n   ```\n\n### Phase 6: Complete Integration Tests\n\nRun full workflow tests simulating real user scenarios:\n\n1. Execute complete E2E test suite:\n   ```bash\n   bash scripts/run-e2e-tests.sh\n   ```\n\n   This runs:\n   - User signup ‚Üí profile creation ‚Üí data CRUD ‚Üí logout\n   - Document upload ‚Üí embedding generation ‚Üí semantic search\n   - Chat message ‚Üí realtime delivery ‚Üí read receipts\n   - Multi-user collaboration scenarios\n   - Error handling and recovery\n\n2. Run test scenarios in parallel:\n   ```bash\n   # Run all test suites\n   npm test -- --maxWorkers=4\n\n   # Run specific workflow\n   npm test -- workflows/document-rag.test.ts\n   ```\n\n3. Generate test reports:\n   ```bash\n   # Generate coverage report\n   npm test -- --coverage\n\n   # Generate HTML report\n   npm test -- --coverage --coverageReporters=html\n   ```\n\n### Phase 7: CI/CD Integration\n\nSetup automated testing in CI/CD pipelines:\n\n1. Use GitHub Actions template:\n   ```bash\n   # Copy CI config to your repo\n   cp templates/ci-config.yml .github/workflows/supabase-tests.yml\n   ```\n\n2. Configure test secrets:\n   - Add `SUPABASE_TEST_URL` to GitHub secrets\n   - Add `SUPABASE_TEST_ANON_KEY` to GitHub secrets\n   - Add `SUPABASE_TEST_SERVICE_ROLE_KEY` for admin tests\n\n3. Run tests on pull requests:\n   - Automatic test execution on PR creation\n   - Blocking PRs if tests fail\n   - Test result reporting in PR comments\n\n### Phase 8: Cleanup and Teardown\n\nClean up test resources after testing:\n\n1. Run cleanup script:\n   ```bash\n   bash scripts/cleanup-test-resources.sh\n   ```\n\n   This removes:\n   - Test users and sessions\n   - Test data from tables\n   - Temporary test databases\n   - Test file uploads\n\n2. Reset test database:\n   ```bash\n   # Reset to clean state\n   supabase db reset --linked\n\n   # Or use migration-based reset\n   bash scripts/reset-test-db.sh\n   ```\n\n## Test Coverage Areas\n\n### Database Testing\n- **Schema Validation**: Table structure, columns, constraints\n- **Migration Testing**: Up/down migrations, rollback safety\n- **RLS Policies**: Access control, policy enforcement\n- **Functions & Triggers**: Database logic, event handlers\n- **Performance**: Query optimization, index usage\n\n### Authentication Testing\n- **User Flows**: Signup, login, logout, password reset\n- **Provider Integration**: OAuth, magic links, phone auth\n- **Session Management**: Token refresh, expiration, persistence\n- **MFA**: Setup, verification, recovery codes\n- **Security**: Rate limiting, brute force protection\n\n### AI Features Testing\n- **Vector Operations**: Insert, update, delete embeddings\n- **Similarity Search**: Accuracy, relevance, threshold tuning\n- **Index Performance**: HNSW vs IVFFlat, query speed\n- **Hybrid Search**: Combined keyword and semantic search\n- **Dimension Validation**: Embedding size consistency\n\n### Realtime Testing\n- **Subscriptions**: Database changes, broadcasts, presence\n- **Connection Management**: Connect, disconnect, reconnect\n- **Message Delivery**: Ordering, reliability, deduplication\n- **Performance**: Latency, throughput, concurrent users\n- **Cleanup**: Subscription disposal, memory leaks\n\n### Integration Testing\n- **Multi-Component**: Auth + Database + Storage\n- **User Journeys**: Complete workflows end-to-end\n- **Error Scenarios**: Network failures, invalid data, rate limits\n- **Edge Cases**: Concurrent updates, race conditions, timeouts\n\n## Test Data Strategies\n\n### Fixture Management\n```typescript\n// Load test fixtures\nconst testData = await loadFixtures('users', 'posts', 'comments');\n\n// Seed database\nawait seedDatabase(testData);\n\n// Cleanup after tests\nafterAll(async () => {\n  await cleanupFixtures();\n});\n```\n\n### Factory Patterns\n```typescript\n// Create test users with factories\nconst user = await createTestUser({\n  email: 'test@example.com'\n  metadata: { role: 'admin' }\n});\n\n// Create related data\nconst posts = await createTestPosts(user.id, 5);\n```\n\n### Isolation Strategies\n- **Test Database**: Dedicated test project\n- **Transaction Rollback**: Rollback after each test\n- **Namespace Prefixes**: `test_` prefix for test data\n- **Time-based Cleanup**: Delete data older than X hours\n\n## Performance Benchmarks\n\n### Query Performance Targets\n- Simple queries: < 50ms\n- Vector similarity search: < 100ms\n- Hybrid search: < 200ms\n- Complex joins: < 300ms\n\n### Realtime Latency Targets\n- Message delivery: < 100ms\n- Presence updates: < 200ms\n- Database changes: < 500ms\n\n### Throughput Targets\n- Authenticated requests: 100+ req/s\n- Vector searches: 50+ req/s\n- Realtime messages: 1000+ msg/s\n\n## Common Test Patterns\n\n### Pattern 1: Auth-Protected Resource Test\n```typescript\ntest('authenticated user CRUD operations', async () => {\n  // 1. Sign up user\n  const { user } = await supabase.auth.signUp({\n    email: 'test@example.com'\n    password: 'test123'\n  });\n\n  // 2. Create resource\n  const { data } = await supabase\n    .from('notes')\n    .insert({ content: 'test note' })\n    .select()\n    .single();\n\n  // 3. Verify ownership\n  expect(data.user_id).toBe(user.id);\n\n  // 4. Update resource\n  await supabase\n    .from('notes')\n    .update({ content: 'updated' })\n    .eq('id', data.id);\n\n  // 5. Delete resource\n  await supabase.from('notes').delete().eq('id', data.id);\n});\n```\n\n### Pattern 2: Vector Search Workflow Test\n```typescript\ntest('document RAG workflow', async () => {\n  // 1. Upload document\n  const doc = await uploadDocument('test.pdf');\n\n  // 2. Generate embeddings\n  const chunks = await chunkDocument(doc);\n  const embeddings = await generateEmbeddings(chunks);\n\n  // 3. Store in database\n  await supabase.from('document_chunks').insert(\n    chunks.map((chunk, i) => ({\n      content: chunk\n      embedding: embeddings[i]\n      document_id: doc.id\n    }))\n  );\n\n  // 4. Perform semantic search\n  const query = 'What is the main topic?';\n  const queryEmbedding = await generateEmbedding(query);\n\n  const { data } = await supabase.rpc('match_documents', {\n    query_embedding: queryEmbedding\n    match_count: 5\n  });\n\n  // 5. Verify results\n  expect(data.length).toBeGreaterThan(0);\n  expect(data[0].similarity).toBeGreaterThan(0.7);\n});\n```\n\n### Pattern 3: Realtime Collaboration Test\n```typescript\ntest('multi-user realtime chat', async () => {\n  // 1. Create two users\n  const user1 = await createTestUser();\n  const user2 = await createTestUser();\n\n  // 2. Subscribe to messages\n  const user1Messages = [];\n  const user2Messages = [];\n\n  const sub1 = await subscribeToMessages(user1, user1Messages);\n  const sub2 = await subscribeToMessages(user2, user2Messages);\n\n  // 3. User 1 sends message\n  await sendMessage(user1, 'Hello from user 1');\n\n  // 4. Verify both users receive it\n  await waitFor(() => {\n    expect(user1Messages).toHaveLength(1);\n    expect(user2Messages).toHaveLength(1);\n  });\n\n  // 5. Cleanup subscriptions\n  await sub1.unsubscribe();\n  await sub2.unsubscribe();\n});\n```\n\n## Troubleshooting\n\n### Tests Failing Intermittently\n- **Race Conditions**: Add proper wait conditions\n- **Async Issues**: Ensure all promises are awaited\n- **Cleanup**: Verify test isolation and cleanup\n- **Timeouts**: Increase timeout for slow operations\n\n### Tests Passing Locally but Failing in CI\n- **Environment**: Verify env variables in CI\n- **Timing**: CI may be slower, increase timeouts\n- **Parallelization**: Check for shared state issues\n- **Dependencies**: Ensure all deps are installed\n\n### Slow Test Execution\n- **Parallel Tests**: Use `--maxWorkers` flag\n- **Test Selection**: Run only changed tests\n- **Database Reset**: Use faster cleanup methods\n- **Mocking**: Mock external services\n\n### Database Connection Issues\n- **Connection Pooling**: Limit concurrent connections\n- **Cleanup**: Properly close connections after tests\n- **Retries**: Implement connection retry logic\n- **Timeouts**: Set appropriate connection timeouts\n\n## Files Reference\n\n**Scripts:**\n- `scripts/setup-test-env.sh` - Initialize test environment\n- `scripts/run-e2e-tests.sh` - Execute complete test suite\n- `scripts/test-database-workflow.sh` - Database E2E tests\n- `scripts/test-auth-workflow.sh` - Authentication E2E tests\n- `scripts/test-ai-features.sh` - Vector search E2E tests\n- `scripts/test-realtime-workflow.sh` - Realtime E2E tests\n- `scripts/cleanup-test-resources.sh` - Clean up test data\n- `scripts/validate-schema.sh` - Validate database schema\n- `scripts/benchmark-vector-search.sh` - Performance benchmarks\n- `scripts/reset-test-db.sh` - Reset test database\n\n**Templates:**\n- `templates/test-suite-template.ts` - Jest/Vitest boilerplate\n- `templates/database-tests.ts` - Database operation tests\n- `templates/auth-tests.ts` - Authentication flow tests\n- `templates/vector-search-tests.ts` - pgvector tests\n- `templates/realtime-tests.ts` - Realtime subscription tests\n- `templates/ci-config.yml` - GitHub Actions CI/CD\n- `templates/jest.config.js` - Jest configuration\n- `templates/vitest.config.ts` - Vitest configuration\n\n**Examples:**\n- `examples/complete-test-workflow.md` - Full E2E testing guide\n- `examples/ci-cd-integration.md` - CI/CD setup guide\n- `examples/test-data-strategies.md` - Test data management\n- `examples/performance-benchmarks.md` - Performance testing\n- `examples/mocking-strategies.md` - Mocking external services\n\n---\n\n**Plugin**: supabase\n**Version**: 1.0.0\n**Last Updated**: 2025-10-26"
              },
              {
                "name": "pgvector-setup",
                "description": "Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.",
                "path": "plugins/supabase/skills/pgvector-setup/SKILL.md",
                "frontmatter": {
                  "name": "pgvector-setup",
                  "description": "Configure pgvector extension for vector search in Supabase - includes embedding storage, HNSW/IVFFlat indexes, hybrid search setup, and AI-optimized query patterns. Use when setting up vector search, building RAG systems, configuring semantic search, creating embedding storage, or when user mentions pgvector, vector database, embeddings, semantic search, or hybrid search.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# pgvector-setup\n\n## Instructions\n\nThis skill provides complete pgvector setup for Supabase databases, enabling vector search capabilities for AI applications, RAG systems, and semantic search.\n\n### Phase 1: Enable pgvector Extension\n\n1. Run the setup script to enable pgvector:\n   ```bash\n   bash scripts/setup-pgvector.sh [SUPABASE_DB_URL]\n   ```\n   This creates the pgvector extension and sets up basic embedding tables.\n\n2. Choose your embedding dimensions based on your model:\n   - OpenAI text-embedding-3-small: 1536 dimensions\n   - OpenAI text-embedding-3-large: 3072 dimensions\n   - Cohere embed-english-v3.0: 1024 dimensions\n   - Custom models: Check model documentation\n\n### Phase 2: Create Embedding Tables\n\n1. Use the embedding table template:\n   ```bash\n   # Copy template and customize for your use case\n   cat templates/embedding-table-schema.sql\n   ```\n\n2. Customize the schema:\n   - Adjust vector dimensions to match your model\n   - Add metadata columns (tags, timestamps, user_id, etc.)\n   - Configure RLS policies for security\n\n3. Apply the schema:\n   ```bash\n   psql $SUPABASE_DB_URL < templates/embedding-table-schema.sql\n   ```\n\n### Phase 3: Create Vector Indexes\n\n**Choose index type based on your data size:**\n\n**HNSW (Recommended for most cases):**\n- Best for: < 1M vectors, high recall requirements\n- Pros: Fast queries, good recall, works well with small-medium datasets\n- Cons: Slower inserts, higher memory usage\n- Run: `bash scripts/create-indexes.sh hnsw [TABLE_NAME] [DIMENSION]`\n\n**IVFFlat:**\n- Best for: > 1M vectors, write-heavy workloads\n- Pros: Faster inserts, lower memory\n- Cons: Requires training, lower recall\n- Run: `bash scripts/create-indexes.sh ivfflat [TABLE_NAME] [DIMENSION]`\n\n**Performance Tuning:**\n- HNSW m parameter (default 16): Higher = better recall, more memory\n- HNSW ef_construction (default 64): Higher = better quality, slower builds\n- IVFFlat lists (default sqrt(rows)): More lists = faster queries, lower recall\n\n### Phase 4: Implement Semantic Search\n\n1. Create the match function:\n   ```sql\n   -- See templates/match-function.sql for complete example\n   create or replace function match_documents(\n     query_embedding vector(1536)\n     match_threshold float\n     match_count int\n   ) returns setof documents ...\n   ```\n\n2. Query from application:\n   ```javascript\n   const { data } = await supabase.rpc('match_documents', {\n     query_embedding: embedding\n     match_threshold: 0.78\n     match_count: 10\n   });\n   ```\n\n### Phase 5: Setup Hybrid Search (Optional)\n\nFor combining keyword and semantic search:\n\n1. Run hybrid search setup:\n   ```bash\n   bash scripts/setup-hybrid-search.sh [TABLE_NAME]\n   ```\n\n2. This configures:\n   - Full-text search with tsvector and GIN indexes\n   - Vector search with HNSW indexes\n   - RRF (Reciprocal Rank Fusion) for combining results\n   - Weighted scoring for tuning keyword vs semantic importance\n\n3. Use the hybrid search function:\n   ```sql\n   select * from hybrid_search(\n     'search query text'\n     query_embedding\n     match_count := 10\n     full_text_weight := 1.0\n     semantic_weight := 1.0\n   );\n   ```\n\n### Phase 6: Test and Validate\n\n1. Run validation tests:\n   ```bash\n   bash scripts/test-vector-search.sh [TABLE_NAME]\n   ```\n\n2. This verifies:\n   - pgvector extension is enabled\n   - Tables have correct vector dimensions\n   - Indexes are created and being used\n   - Query performance is acceptable\n   - Similarity functions return correct results\n\n## Key Decisions\n\n**Distance Metric Selection:**\n- Cosine distance (`<=>`): Safe default, handles varying vector magnitudes\n- Inner product (`<#>`): Faster for normalized vectors (OpenAI embeddings)\n- Euclidean distance (`<->`): Use when absolute distances matter\n\n**Index Choice:**\n- Start with HNSW for most applications\n- Switch to IVFFlat only if:\n  - You have > 1M vectors\n  - Insert performance is critical\n  - You can tolerate lower recall\n\n**Dimension Size:**\n- Higher dimensions = better semantic understanding\n- Lower dimensions = faster queries, less storage\n- Match your embedding model exactly (never truncate)\n\n## Common Patterns\n\n**Pattern 1: Document Search**\n- Store document chunks with metadata\n- Use HNSW index for semantic search\n- Add full-text for hybrid search\n- See: examples/document-search-pattern.md\n\n**Pattern 2: User Preference Matching**\n- Store user profile embeddings\n- Use cosine similarity for matching\n- Update embeddings as preferences change\n- See: examples/preference-matching-pattern.md\n\n**Pattern 3: Product Recommendations**\n- Store product feature embeddings\n- Use hybrid search (keywords + semantic)\n- Weight by popularity or ratings\n- See: examples/product-recommendations-pattern.md\n\n## Troubleshooting\n\n**Slow queries (> 100ms):**\n- Check if index is being used: `EXPLAIN ANALYZE`\n- Increase HNSW ef_search parameter\n- Consider reducing result limit\n- Add WHERE clauses to reduce search space\n\n**Poor recall (missing relevant results):**\n- Increase match_count\n- Lower match_threshold\n- For HNSW: increase m and ef_construction\n- For IVFFlat: increase lists parameter\n\n**High memory usage:**\n- HNSW uses ~10KB per vector\n- Reduce m parameter (quality tradeoff)\n- Consider IVFFlat for large datasets\n- Use partial indexes if possible\n\n**Insert performance issues:**\n- HNSW is slow for bulk inserts\n- Disable index during bulk load, rebuild after\n- Use IVFFlat for write-heavy workloads\n- Batch inserts when possible\n\n## Security Considerations\n\n**Row Level Security (RLS):**\n- Enable RLS on all embedding tables\n- Filter by user_id or organization_id\n- Prevent embedding leakage between users\n- See templates for RLS policy examples\n\n**API Key Protection:**\n- Never expose embedding API keys\n- Use Supabase Edge Functions for embedding generation\n- Store keys in Supabase secrets\n- Rate limit embedding requests\n\n## Files Reference\n\n**Scripts:**\n- `scripts/setup-pgvector.sh` - Enable extension and create base tables\n- `scripts/create-indexes.sh` - Create HNSW or IVFFlat indexes\n- `scripts/setup-hybrid-search.sh` - Configure hybrid search\n- `scripts/test-vector-search.sh` - Validate setup\n\n**Templates:**\n- `templates/embedding-table-schema.sql` - Table structure with metadata\n- `templates/hnsw-index-config.sql` - HNSW index with tuning\n- `templates/ivfflat-index-config.sql` - IVFFlat index configuration\n- `templates/hybrid-search-function.sql` - Hybrid search with RRF\n- `templates/match-function.sql` - Basic semantic search function\n\n**Examples:**\n- `examples/embedding-strategies.md` - Index selection guide\n- `examples/vector-search-examples.md` - Common search patterns\n- `examples/document-search-pattern.md` - Full document search implementation\n- `examples/preference-matching-pattern.md` - User matching system\n- `examples/product-recommendations-pattern.md` - Recommendation engine\n\n---\n\n**Plugin**: supabase\n**Version**: 1.0.0\n**Last Updated**: 2025-10-26"
              },
              {
                "name": "rls-templates",
                "description": "Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.",
                "path": "plugins/supabase/skills/rls-templates/SKILL.md",
                "frontmatter": {
                  "name": "rls-templates",
                  "description": "Row Level Security policy templates for Supabase - multi-tenant patterns, user isolation, role-based access, and secure-by-default configurations. Use when securing Supabase tables, implementing RLS policies, building multi-tenant AI apps, protecting user data, creating chat/RAG systems, or when user mentions row level security, RLS, Supabase security, tenant isolation, or data access policies.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# RLS Templates\n\nProduction-ready Row Level Security policy templates for Supabase applications, with focus on AI application patterns (multi-tenant chat, RAG systems, user-specific embeddings).\n\n## Instructions\n\n### 1. Applying RLS Policies\n\n**Apply policies to tables:**\n```bash\n# Apply user isolation policies\nbash scripts/apply-rls-policies.sh user-isolation conversations messages\n\n# Apply multi-tenant policies\nbash scripts/apply-rls-policies.sh multi-tenant organizations org_members documents\n\n# Apply AI-specific policies\nbash scripts/apply-rls-policies.sh ai-chat conversations messages message_embeddings\n```\n\n**Generate custom policy:**\n```bash\n# Generate policy from template\nbash scripts/generate-policy.sh user-isolation my_table user_id\n\n# Generate with custom column\nbash scripts/generate-policy.sh multi-tenant projects organization_id\n```\n\n### 2. Testing RLS Enforcement\n\n**Test policies work correctly:**\n```bash\n# Test all policies on a table\nbash scripts/test-rls-policies.sh conversations\n\n# Test specific user context\nbash scripts/test-rls-policies.sh messages --user-id \"user-uuid-here\"\n\n# Test multi-tenant isolation\nbash scripts/test-rls-policies.sh documents --org-id \"org-uuid-here\"\n```\n\n### 3. Auditing Security\n\n**Audit tables for missing RLS:**\n```bash\n# Audit all tables in public schema\nbash scripts/audit-rls.sh\n\n# Audit specific tables\nbash scripts/audit-rls.sh conversations messages embeddings\n\n# Generate audit report\nbash scripts/audit-rls.sh --report audit-report.md\n```\n\n### 4. Policy Pattern Selection\n\n**Choose the right pattern:**\n\n- **user-isolation.sql**: User owns row directly (`user_id` column)\n  - Use for: User profiles, settings, personal documents\n  - Pattern: `auth.uid() = user_id`\n\n- **multi-tenant.sql**: Organization/team-based isolation\n  - Use for: SaaS apps, team workspaces, shared resources\n  - Pattern: Check organization membership via join\n\n- **role-based-access.sql**: Different permissions per role\n  - Use for: Admin panels, hierarchical access, permission levels\n  - Pattern: Check role from `auth.jwt()` claims\n\n- **ai-chat-policies.sql**: Chat/conversation data\n  - Use for: AI chat apps, message history, conversation threads\n  - Pattern: User owns conversation + participants table\n\n- **embeddings-policies.sql**: Vector/embedding data\n  - Use for: RAG systems, semantic search, vector databases\n  - Pattern: User owns source document that owns embeddings\n\n## Examples\n\n**Example 1: Secure Chat Application**\n```sql\n-- Apply chat policies to tables\n\\i templates/ai-chat-policies.sql\n\n-- Tables: conversations, messages, participants\n-- Result: Users only see conversations they participate in\n```\n\n**Example 2: Multi-Tenant RAG System**\n```sql\n-- Apply organization isolation\n\\i templates/multi-tenant.sql\n\n-- Apply embedding security\n\\i templates/embeddings-policies.sql\n\n-- Tables: organizations, documents, document_embeddings\n-- Result: Each org only sees their own documents and embeddings\n```\n\n**Example 3: Role-Based Admin Panel**\n```sql\n-- Apply role-based policies\n\\i templates/role-based-access.sql\n\n-- Roles: admin (full access), editor (read/write), viewer (read-only)\n-- Result: Different permissions based on user role\n```\n\n## Requirements\n\n### Prerequisites\n- Supabase project with database access\n- PostgreSQL client (`psql`) installed\n- Environment variables set:\n  - `SUPABASE_DB_URL`: PostgreSQL connection string\n  - `SUPABASE_ANON_KEY`: For testing anon access\n  - `SUPABASE_SERVICE_KEY`: For admin operations\n\n### Security Checklist\n- [ ] RLS enabled on all tables in public schema\n- [ ] Policies test both authenticated and anonymous access\n- [ ] Indexes created on columns used in policies (user_id, org_id, etc.)\n- [ ] Service key never exposed to client applications\n- [ ] Policies use `(SELECT auth.uid())` for performance\n- [ ] WITH CHECK clause included on INSERT/UPDATE policies\n- [ ] Testing validates isolation between users/tenants\n\n### Performance Optimization\n- Create indexes: `CREATE INDEX idx_table_user_id ON table(user_id);`\n- Wrap auth functions: `(SELECT auth.uid())` instead of `auth.uid()`\n- Always filter queries: `.eq('user_id', userId)` in client code\n- Use security definer functions for complex authorization logic\n- Specify roles in policies: `TO authenticated` to skip anon checks\n\n---\n\n**Best Practices:**\n1. Enable RLS before adding any data\n2. Test with multiple user contexts\n3. Use audit script regularly in CI/CD\n4. Document policy decisions in migration files\n5. Monitor query performance after adding policies"
              },
              {
                "name": "rls-test-patterns",
                "description": "RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.",
                "path": "plugins/supabase/skills/rls-test-patterns/SKILL.md",
                "frontmatter": {
                  "name": "rls-test-patterns",
                  "description": "RLS policy testing patterns for Supabase - automated test cases for Row Level Security enforcement, user isolation verification, multi-tenant security, and comprehensive security audit scripts. Use when testing RLS policies, validating user isolation, auditing Supabase security, verifying tenant isolation, testing row level security, running security tests, or when user mentions RLS testing, security validation, policy testing, or data leak prevention.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# RLS Test Patterns\n\nComprehensive testing framework for Row Level Security (RLS) policies in Supabase. Catch security vulnerabilities before production with automated tests for user isolation, multi-tenant security, role-based access, and anonymous user restrictions.\n\n## Instructions\n\n### 1. Test User Isolation\n\n**Verify users can only access their own data:**\n```bash\n# Test user isolation on specific tables\nbash scripts/test-user-isolation.sh conversations messages profiles\n\n# Test with specific user IDs\nbash scripts/test-user-isolation.sh documents --user1 \"uuid1\" --user2 \"uuid2\"\n\n# Generate detailed report\nbash scripts/test-user-isolation.sh --all --report isolation-report.md\n```\n\n**What it tests:**\n- User A cannot read User B's data\n- User A cannot modify/delete User B's data\n- User A can only insert data owned by themselves\n- Null user_id values are properly rejected\n\n### 2. Test Multi-Tenant Isolation\n\n**Verify organization/team data separation:**\n```bash\n# Test organization isolation\nbash scripts/test-multi-tenant-isolation.sh organizations projects documents\n\n# Test with specific org IDs\nbash scripts/test-multi-tenant-isolation.sh --org1 \"org-uuid-1\" --org2 \"org-uuid-2\"\n\n# Test member access patterns\nbash scripts/test-multi-tenant-isolation.sh --test-members\n```\n\n**What it tests:**\n- Org A members cannot access Org B's data\n- Users not in an org cannot access org data\n- Removing user from org revokes access immediately\n- Shared resources respect org boundaries\n\n### 3. Test Role-Based Permissions\n\n**Verify role-based access control:**\n```bash\n# Test RBAC policies\nbash scripts/test-role-permissions.sh admin_panel sensitive_data\n\n# Test specific role hierarchy\nbash scripts/test-role-permissions.sh --roles \"admin,editor,viewer\"\n\n# Test permission escalation prevention\nbash scripts/test-role-permissions.sh --test-escalation\n```\n\n**What it tests:**\n- Admin role has full access\n- Editor role can read/write but not delete\n- Viewer role has read-only access\n- Users cannot escalate their own permissions\n- Role changes take effect immediately\n\n### 4. Test Anonymous Access Restrictions\n\n**Verify anonymous users are properly restricted:**\n```bash\n# Test anonymous access on all public tables\nbash scripts/test-anonymous-access.sh\n\n# Test specific tables\nbash scripts/test-anonymous-access.sh public_posts comments\n\n# Test auth.uid() null handling\nbash scripts/test-anonymous-access.sh --test-null-uid\n```\n\n**What it tests:**\n- Anonymous users cannot access protected data\n- Anonymous users can only access designated public data\n- auth.uid() returns null correctly for anon users\n- Policies handle null uid safely\n\n### 5. Audit RLS Coverage\n\n**Check all tables have proper RLS policies:**\n```bash\n# Audit entire database\nbash scripts/audit-rls-coverage.sh\n\n# Audit specific schema\nbash scripts/audit-rls-coverage.sh --schema public\n\n# Generate compliance report\nbash scripts/audit-rls-coverage.sh --report compliance-report.md --format markdown\n```\n\n**What it checks:**\n- All public schema tables have RLS enabled\n- Each table has policies for all DML operations (SELECT, INSERT, UPDATE, DELETE)\n- Policies target appropriate roles (authenticated, anon)\n- No tables are accidentally exposed without policies\n- Policy naming follows best practices\n\n### 6. Run Complete Test Suite\n\n**Execute all RLS tests:**\n```bash\n# Run all tests with default settings\nbash scripts/run-all-rls-tests.sh\n\n# Run with custom database URL\nbash scripts/run-all-rls-tests.sh --db-url \"postgresql://...\"\n\n# Run and generate comprehensive report\nbash scripts/run-all-rls-tests.sh --report rls-test-results.json --verbose\n\n# Run in CI/CD mode (exit 1 on any failure)\nbash scripts/run-all-rls-tests.sh --ci --fail-fast\n```\n\n**Test sequence:**\n1. Audit RLS coverage\n2. Test user isolation\n3. Test multi-tenant isolation\n4. Test role permissions\n5. Test anonymous access\n6. Generate summary report\n\n## Examples\n\n**Example 1: Testing Chat Application Security**\n```bash\n# Test conversation isolation\nbash scripts/test-user-isolation.sh conversations messages participants\n\n# Output:\n# ‚úì User cannot read other user's conversations\n# ‚úì User cannot send messages to other user's conversations\n# ‚úì User cannot add participants to other user's conversations\n# ‚úì All isolation tests passed (12/12)\n```\n\n**Example 2: Multi-Tenant SaaS Security Audit**\n```bash\n# Full audit of multi-tenant application\nbash scripts/test-multi-tenant-isolation.sh organizations projects documents embeddings\n\n# Output:\n# ‚úì Org A users cannot access Org B projects\n# ‚úì Removed users lose access immediately\n# ‚úì Cross-org document access blocked\n# ‚úì Embeddings respect org boundaries\n# ‚úì All multi-tenant tests passed (24/24)\n```\n\n**Example 3: CI/CD Integration**\n```bash\n# In .github/workflows/security-tests.yml\n- name: Run RLS Tests\n  run: |\n    bash scripts/run-all-rls-tests.sh \\\n      --ci \\\n      --fail-fast \\\n      --report rls-results.json\n\n- name: Upload Test Report\n  uses: actions/upload-artifact@v3\n  with:\n    name: rls-test-results\n    path: rls-results.json\n```\n\n**Example 4: Pre-Production Security Check**\n```bash\n# Complete security validation before deploy\nbash scripts/audit-rls-coverage.sh --report audit.md\nbash scripts/run-all-rls-tests.sh --verbose --report tests.json\n\n# Review both reports before deploying\ncat audit.md\ncat tests.json\n```\n\n## Requirements\n\n### Prerequisites\n- Supabase project with database access\n- PostgreSQL client (`psql`) installed\n- Node.js 18+ (for TypeScript test suite)\n- Supabase CLI v1.11.4+ (for pgTAP tests)\n\n### Environment Variables\nRequired in `.env` file:\n```bash\n# Database connection\nSUPABASE_DB_URL=\"postgresql://postgres:[password]@[host]:5432/postgres\"\n\n# API keys for client testing\nSUPABASE_URL=\"https://[project-ref].supabase.co\"\nSUPABASE_ANON_KEY=\"eyJ...\"\nSUPABASE_SERVICE_KEY=\"eyJ...\"\n\n# Test user credentials (optional, for client tests)\nTEST_USER_1_EMAIL=\"test1@example.com\"\nTEST_USER_1_PASSWORD=\"testpass123\"\nTEST_USER_2_EMAIL=\"test2@example.com\"\nTEST_USER_2_PASSWORD=\"testpass456\"\n```\n\n### Test Data Setup\nTests create and clean up their own data, but you can provide:\n```bash\n# Optional: Use existing test users\nTEST_USER_1_ID=\"uuid-for-test-user-1\"\nTEST_USER_2_ID=\"uuid-for-test-user-2\"\n\n# Optional: Use existing test orgs\nTEST_ORG_1_ID=\"uuid-for-test-org-1\"\nTEST_ORG_2_ID=\"uuid-for-test-org-2\"\n```\n\n### Security Best Practices Tested\n- ‚úì RLS enabled on all public schema tables\n- ‚úì User isolation prevents cross-user data access\n- ‚úì Multi-tenant isolation prevents cross-org access\n- ‚úì Anonymous users properly restricted\n- ‚úì Role hierarchy enforced correctly\n- ‚úì auth.uid() null values handled safely\n- ‚úì Policies use indexed columns for performance\n- ‚úì WITH CHECK clauses prevent policy bypass\n- ‚úì Service key never exposed to clients\n\n### Common Vulnerabilities Detected\n- Missing RLS on public tables (data leak)\n- Missing WITH CHECK clauses (INSERT/UPDATE bypass)\n- Improper null handling (anonymous access leak)\n- Missing role checks (privilege escalation)\n- Unindexed policy columns (DoS via slow queries)\n- Cross-tenant joins (org data leak)\n- Missing policies for specific operations\n- Overly permissive anonymous access\n\n## Test Output Format\n\n### Success Output\n```\nüîí RLS Test Suite v1.0.0\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\nüìä Test Summary\n  Tables tested: 5\n  Total tests: 48\n  Passed: 48 ‚úì\n  Failed: 0\n  Duration: 12.3s\n\n‚úÖ All RLS policies working correctly!\n```\n\n### Failure Output\n```\nüîí RLS Test Suite v1.0.0\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n‚ùå SECURITY ISSUE DETECTED\n\nTable: conversations\nTest: User isolation - SELECT\nIssue: User B could read User A's conversations\nExpected: 0 rows\nActual: 5 rows\n\nRecommendation: Add USING clause to SELECT policy:\n  USING (auth.uid() = user_id)\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüìä Test Summary\n  Total tests: 48\n  Passed: 43 ‚úì\n  Failed: 5 ‚ùå\n\n‚ùå Critical security issues found. Do not deploy.\n```\n\n---\n\n**Integration Points:**\n- Used by: `/supabase:test-rls` command\n- Used by: `supabase-security-auditor` agent\n- Complements: `rls-templates` skill (creates policies, this tests them)\n- CI/CD: Run in GitHub Actions before deployment\n\n**Best Practices:**\n1. Run tests after every RLS policy change\n2. Include in pre-commit hooks for critical tables\n3. Run full suite weekly in CI/CD\n4. Test with real user data patterns (anonymized)\n5. Keep test users separate from production users\n6. Document expected behavior for each policy\n7. Update tests when adding new tables/policies"
              },
              {
                "name": "schema-patterns",
                "description": "Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.",
                "path": "plugins/supabase/skills/schema-patterns/SKILL.md",
                "frontmatter": {
                  "name": "schema-patterns",
                  "description": "Production-ready database schema patterns for AI applications including chat/conversation schemas, RAG document storage with pgvector, multi-tenant organization models, user management, and AI usage tracking. Use when building AI applications, creating database schemas, setting up chat systems, implementing RAG, designing multi-tenant databases, or when user mentions supabase schemas, chat database, RAG storage, pgvector, embeddings, conversation history, or AI application database.",
                  "allowed-tools": "Read, Write, Bash, Edit"
                },
                "content": "# Database Schema Patterns for AI Applications\n\nProduction-ready PostgreSQL/Supabase database schemas optimized for AI applications including chat systems, RAG (Retrieval-Augmented Generation), multi-tenancy, and usage tracking.\n\n## Instructions\n\n### 1. Identify Required Pattern Type\n\nAsk the user which schema pattern they need:\n- **chat**: Conversation and messaging systems\n- **rag**: Document storage with vector embeddings (pgvector)\n- **multi-tenant**: Organization-based multi-tenancy\n- **user-management**: Extended user profiles and metadata\n- **ai-usage**: Token tracking, costs, and rate limiting\n- **complete**: All patterns combined\n\n### 2. Generate Schema\n\nUse the generation script:\n```bash\ncd /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/schema-patterns\n./scripts/generate-schema.sh <pattern-type> <output-file>\n```\n\nPattern types: `chat`, `rag`, `multi-tenant`, `user-management`, `ai-usage`, `complete`\n\n### 3. Validate Schema\n\nBefore applying, validate the generated schema:\n```bash\n./scripts/validate-schema.sh <schema-file>\n```\n\nThis checks for:\n- Proper table naming conventions (lowercase, underscores)\n- Primary keys on all tables\n- Foreign key relationships\n- Index optimization\n- pgvector extension usage (for RAG patterns)\n- RLS policy structure\n- Migration version format\n\n### 4. Apply Migration\n\nApply the schema to your Supabase project:\n```bash\n./scripts/apply-migration.sh <schema-file> <migration-name>\n```\n\nThis creates a timestamped migration file and validates before applying.\n\n### 5. Seed Test Data (Optional)\n\nFor development, generate realistic test data:\n```bash\n./scripts/seed-data.sh <pattern-type>\n```\n\n## Available Templates\n\n### Core Schemas\n- `chat-schema.sql`: Complete chat/conversation system with users, conversations, messages, participants\n- `rag-schema.sql`: RAG document storage with chunks, embeddings (pgvector), and similarity search\n- `multi-tenant-schema.sql`: Organization-based multi-tenancy with orgs, teams, members, roles\n- `user-management-schema.sql`: Extended user profiles, metadata, preferences\n- `ai-usage-tracking-schema.sql`: Token usage, API costs, rate limiting, usage analytics\n\n### Supporting Templates\n- `migration-template.sql`: Boilerplate migration structure with version tracking\n- `indexes-template.sql`: Performance optimization index patterns\n- `rls-policies-template.sql`: Row Level Security policy patterns\n\n## Key Features\n\n### pgvector Integration (RAG Schemas)\nAll RAG schemas include:\n- Vector column setup with proper dimensions\n- HNSW indexing for similarity search\n- Cosine distance operators\n- Automatic embedding column generation\n- Metadata storage alongside embeddings\n\n### Multi-Tenancy Support\nOrganization-based isolation:\n- Tenant identification (org_id on all tables)\n- Team-based access control\n- Member role management\n- RLS policies for data isolation\n\n### Chat System Optimization\nOptimized for real-time messaging:\n- Conversation participants tracking\n- Message ordering and pagination indexes\n- Read/unread status tracking\n- Typing indicators support\n- Message search with full-text indexes\n\n### Performance Patterns\n- Composite indexes for common queries\n- Partial indexes for filtered queries\n- Generated columns for computed fields\n- Proper foreign key cascades\n- Optimized join patterns\n\n## Examples\n\nSee the examples directory for:\n- `complete-ai-app-schema.md`: Full schema combining all patterns\n- `migration-guide.md`: Schema evolution and versioning\n- `indexing-strategy.md`: Performance optimization guide\n\n## Best Practices\n\n1. **Always use lowercase with underscores** for table/column names\n2. **Enable pgvector extension** before creating vector columns\n3. **Add indexes on foreign keys** for join performance\n4. **Use generated columns** for computed fields (created_at, updated_at)\n5. **Implement RLS policies** for security and multi-tenancy\n6. **Version all migrations** with timestamps\n7. **Use halfvec for embeddings** to save storage (16-bit vs 32-bit)\n8. **Add metadata JSONB columns** for flexibility\n9. **Plan for soft deletes** (deleted_at timestamp)\n10. **Include audit trails** (created_by, updated_by)\n\n## Common Workflows\n\n### Setting Up a Chat Application\n```bash\n./scripts/generate-schema.sh chat schema.sql\n./scripts/validate-schema.sh schema.sql\n./scripts/apply-migration.sh schema.sql \"initial-chat-schema\"\n./scripts/seed-data.sh chat\n```\n\n### Building a RAG System\n```bash\n./scripts/generate-schema.sh rag schema.sql\n./scripts/validate-schema.sh schema.sql\n./scripts/apply-migration.sh schema.sql \"add-rag-storage\"\n./scripts/seed-data.sh rag\n```\n\n### Complete AI Platform\n```bash\n./scripts/generate-schema.sh complete schema.sql\n./scripts/validate-schema.sh schema.sql\n./scripts/apply-migration.sh schema.sql \"complete-ai-platform\"\n```\n\n## Troubleshooting\n\n**pgvector not found**: Enable the vector extension in Supabase dashboard (Database > Extensions)\n\n**RLS blocks queries**: Check RLS policies or temporarily disable for testing (not recommended for production)\n\n**Slow similarity search**: Ensure HNSW index is created on vector columns with proper operator class\n\n**Migration conflicts**: Check migration version ordering and resolve conflicts manually\n\n---\n\n**Skill Location**: /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/schema-patterns/\n**Version**: 1.0.0"
              },
              {
                "name": "schema-validation",
                "description": "Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.",
                "path": "plugins/supabase/skills/schema-validation/SKILL.md",
                "frontmatter": {
                  "name": "schema-validation",
                  "description": "Database schema validation tools - SQL syntax checking, constraint validation, naming convention enforcement, and schema integrity verification. Use when validating database schemas, checking migrations, enforcing naming conventions, verifying constraints, or when user mentions schema validation, migration checks, database best practices, or PostgreSQL validation.",
                  "allowed-tools": "Bash, Read, Write, Grep, Glob"
                },
                "content": "# Schema Validation\n\nComprehensive database schema validation for Supabase/PostgreSQL projects. Validates SQL syntax, naming conventions, constraints, indexes, and RLS policies before deployment.\n\n## Instructions\n\n### 1. Run Full Validation\n```bash\ncd /path/to/project\nbash /home/vanman2025/Projects/ai-dev-marketplace/plugins/supabase/skills/schema-validation/scripts/full-validation.sh <schema-file-or-directory>\n```\n\n### 2. Run Individual Validations\n\n**SQL Syntax Validation:**\n```bash\nbash scripts/validate-sql-syntax.sh <sql-file>\n```\n\n**Naming Convention Validation:**\n```bash\nbash scripts/validate-naming.sh <sql-file>\n```\n\n**Constraint Validation:**\n```bash\nbash scripts/validate-constraints.sh <sql-file>\n```\n\n**Index Validation:**\n```bash\nbash scripts/validate-indexes.sh <sql-file>\n```\n\n**RLS Policy Validation:**\n```bash\nbash scripts/validate-rls.sh <sql-file>\n```\n\n### 3. Generate Validation Report\nThe full validation script generates a detailed markdown report showing:\n- Validation results for each check\n- Issues found with severity levels (ERROR, WARNING, INFO)\n- Recommendations for fixes\n- Summary statistics\n\n### 4. Configure Validation Rules\nCustomize validation rules by editing:\n```bash\ntemplates/validation-rules.json\ntemplates/naming-conventions.json\n```\n\n## Examples\n\n### Example 1: Validate Migration Before Deployment\n```bash\n# Validate a new migration file\nbash scripts/full-validation.sh supabase/migrations/20250126_add_users_table.sql\n\n# Review the generated report\ncat validation-report.md\n```\n\n### Example 2: Validate Entire Schema Directory\n```bash\n# Validate all migration files\nbash scripts/full-validation.sh supabase/migrations/\n\n# Check for common issues\ngrep \"ERROR\" validation-report.md\n```\n\n### Example 3: CI/CD Integration\n```bash\n# Add to .github/workflows/validate-schema.yml\n- name: Validate Database Schema\n  run: |\n    bash plugins/supabase/skills/schema-validation/scripts/full-validation.sh supabase/migrations/\n    if grep -q \"ERROR\" validation-report.md; then\n      exit 1\n    fi\n```\n\n## Validation Checks\n\n### SQL Syntax (validate-sql-syntax.sh)\n- PostgreSQL syntax compliance\n- Reserved keyword usage\n- Statement termination\n- Data type validity\n\n### Naming Conventions (validate-naming.sh)\n- Lowercase with underscores (snake_case)\n- Plural table names\n- Singular column names\n- Constraint naming patterns (fk_, uq_, ck_, pk_)\n- Index naming patterns (idx_, uidx_)\n\n### Constraints (validate-constraints.sh)\n- Primary key existence\n- Foreign key references\n- Unique constraint usage\n- Check constraint validity\n- NOT NULL enforcement\n\n### Indexes (validate-indexes.sh)\n- Index coverage for foreign keys\n- Index coverage for RLS policy columns\n- Duplicate index detection\n- Index naming conventions\n\n### RLS Policies (validate-rls.sh)\n- RLS enabled on public tables\n- Policy existence for CRUD operations\n- Role specification (authenticated, anon)\n- Performance optimization (proper indexing)\n\n## Requirements\n\n- PostgreSQL client tools (psql) for syntax validation\n- Bash 4.0+ for script execution\n- Read access to schema files\n- Write access for generating reports\n\n## Best Practices\n\n1. **Run validation before every migration**\n2. **Fix ERRORs immediately** - these will cause deployment failures\n3. **Address WARNINGs** - these indicate potential issues\n4. **Review INFO items** - these are suggestions for improvement\n5. **Keep validation rules updated** as your schema evolves\n6. **Integrate into CI/CD** to prevent invalid schemas from being deployed\n\n---\n\n**Plugin**: supabase\n**Version**: 1.0.0\n**Last Updated**: 2025-01-26"
              }
            ]
          },
          {
            "name": "vercel-ai-sdk",
            "description": "Modular Vercel AI SDK development plugin with feature bundles and specialized agents. Build AI applications incrementally or all-at-once. Supports TypeScript, JavaScript, and Python.",
            "source": "./plugins/vercel-ai-sdk",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "vanman2024",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install vercel-ai-sdk@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-advanced",
                "description": "Add advanced features to Vercel AI SDK app including AI agents with workflows, MCP tools, image generation, transcription, and speech synthesis",
                "path": "plugins/vercel-ai-sdk/commands/add-advanced.md",
                "frontmatter": {
                  "description": "Add advanced features to Vercel AI SDK app including AI agents with workflows, MCP tools, image generation, transcription, and speech synthesis",
                  "argument-hint": [
                    "feature-requests"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add cutting-edge AI capabilities to a Vercel AI SDK application including autonomous agents with workflows, MCP tools integration, image generation, audio transcription, speech synthesis, and multi-step reasoning.\n\nCore Principles:\n- Understand use case complexity before designing agent workflows\n- Ask about tool requirements and MCP server availability\n- Follow Vercel AI SDK documentation patterns\n- Implement safeguards for autonomous agents (loop control, cost limits)\n\nPhase 1: Discovery\nGoal: Understand what advanced features are needed\n\nActions:\n- Parse $ARGUMENTS to identify requested features\n- If unclear or no arguments provided, use AskUserQuestion to gather:\n  - Which advanced features do you want? (AI agents, MCP tools, image generation, transcription, speech)\n  - What problem should the agent solve?\n  - Do you have MCP servers configured?\n  - What's your expected volume for image/audio generation?\n- Load package.json to understand current setup\n- Example: @package.json\n\nPhase 2: Analysis\nGoal: Understand current project state\n\nActions:\n- Check for existing AI SDK installation and tools\n- Identify infrastructure needs (storage for images/audio)\n- Review MCP server configuration if applicable\n- Assess security requirements for agent autonomy\n- Example: !{bash ls .mcp.json 2>/dev/null && echo \"MCP configured\" || echo \"No MCP config\"}\n\nPhase 3: Implementation\nGoal: Add advanced features using specialized agent\n\nActions:\n\nInvoke the vercel-ai-advanced-agent to implement the requested advanced features.\n\nThe agent should:\n- Fetch relevant Vercel AI SDK documentation for the requested features\n- Design agent architecture and workflow\n- Install required packages (MCP clients, image providers, audio libraries)\n- Implement requested features following SDK best practices:\n  - AI agents with multi-step reasoning and tool calling\n  - Workflow orchestration with loop control\n  - MCP tools integration (if applicable)\n  - Image generation using OpenAI DALL-E or Fal AI\n  - Audio transcription using Whisper\n  - Text-to-speech synthesis\n- Add proper TypeScript types\n- Implement loop controls and cost safeguards\n- Set up storage for generated content\n\nProvide the agent with:\n- Context: Current project structure and infrastructure\n- Target: $ARGUMENTS (requested advanced features)\n- Expected output: Production-ready advanced AI features with safeguards\n\nPhase 4: Verification\nGoal: Ensure features work correctly\n\nActions:\n- Run TypeScript compilation check\n- Example: !{bash npx tsc --noEmit}\n- Test agent workflows with various scenarios\n- Verify tool calling and MCP integration (if applicable)\n- Test image/audio generation\n- Check loop controls and cost limits\n\nPhase 5: Summary\nGoal: Document advanced features\n\nActions:\n- List all advanced features implemented\n- Show agent workflows and tool definitions\n- Note API keys and environment variables needed\n- Provide cost estimates for generation operations\n- Suggest next steps (workflow optimization, monitoring, scaling)"
              },
              {
                "name": "/add-chat",
                "description": "Add chat UI with message persistence to existing Vercel AI SDK project",
                "path": "plugins/vercel-ai-sdk/commands/add-chat.md",
                "frontmatter": {
                  "description": "Add chat UI with message persistence to existing Vercel AI SDK project",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add chat interface with message persistence to existing Vercel AI SDK project\n\nCore Principles:\n- Detect existing framework (React/Next.js/Vue/Svelte)\n- Fetch chat-specific docs (2-3 URLs)\n- Implement UI components and persistence\n- Verify functionality\n\nPhase 1: Discovery\nGoal: Understand project and framework\n\nActions:\n- Detect framework: @package.json\n- Check for UI library (React, Vue, Svelte)\n- Identify where to add chat components\n- Check if streaming already exists\n\nPhase 2: Fetch Chat Documentation\nGoal: Get chat UI docs only\n\nActions:\nFetch these docs in parallel (3 URLs max):\n\n1. WebFetch: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\n2. WebFetch: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\n3. WebFetch: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\n\nPhase 3: Implementation\nGoal: Add chat UI and persistence\n\nActions:\n\nInvoke the **general-purpose** agent to implement chat:\n\nThe agent should:\n- Create chat UI component (useChat hook or equivalent)\n- Add message display component\n- Implement input handling\n- Add message persistence (localStorage or database)\n- Create API route/endpoint for chat\n- Add loading states and error handling\n- Style with existing CSS framework or Tailwind\n\nProvide the agent with:\n- Context: Framework and existing code\n- Target: Working chat interface with persistence\n- Expected output: Chat component, API route, styling\n\nPhase 4: Verification\nGoal: Ensure chat works\n\nActions:\n- For TypeScript: Run npx tsc --noEmit\n- Verify chat component renders\n- Check API route exists\n- Test message persistence\n- Verify styling applied\n\nPhase 5: Summary\nGoal: Show what was added\n\nActions:\nProvide summary:\n- Chat components created\n- API routes added\n- Persistence mechanism used\n- How to test chat\n- Customization options\n- Next steps: Add tools with /vercel-ai-sdk:add-tools\n\nImportant Notes:\n- Framework-specific implementation\n- Fetches minimal docs (3 URLs)\n- Includes persistence out of the box\n- Adapts styling to project"
              },
              {
                "name": "/add-data-features",
                "description": "Add data features to Vercel AI SDK app including embeddings generation, RAG with vector databases, and structured data generation",
                "path": "plugins/vercel-ai-sdk/commands/add-data-features.md",
                "frontmatter": {
                  "description": "Add data features to Vercel AI SDK app including embeddings generation, RAG with vector databases, and structured data generation",
                  "argument-hint": [
                    "feature-requests"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add AI-powered data processing capabilities to a Vercel AI SDK application including embeddings generation, RAG (Retrieval Augmented Generation) with vector databases, and structured data generation.\n\nCore Principles:\n- Understand data sources and volume before designing solutions\n- Ask about vector database preferences\n- Follow Vercel AI SDK documentation patterns\n- Optimize for cost and performance\n\nPhase 1: Discovery\nGoal: Understand what data features are needed\n\nActions:\n- Parse $ARGUMENTS to identify requested features\n- If unclear or no arguments provided, use AskUserQuestion to gather:\n  - Which data features do you want? (Embeddings, RAG, structured data generation)\n  - What's the size of your dataset?\n  - Do you have a vector database? (Pinecone, Weaviate, Chroma, pgvector, etc.)\n  - What kind of data needs to be processed?\n- Load package.json to understand current setup\n- Example: @package.json\n\nPhase 2: Analysis\nGoal: Understand current project state\n\nActions:\n- Check for existing AI SDK installation\n- Identify data sources (files, APIs, databases)\n- Verify database infrastructure availability\n- Assess data volume and processing requirements\n- Example: !{bash ls *.txt *.pdf *.md 2>/dev/null | wc -l}\n\nPhase 3: Implementation\nGoal: Add requested data features using specialized agent\n\nActions:\n\nInvoke the vercel-ai-data-agent to implement the requested data features.\n\nThe agent should:\n- Fetch relevant Vercel AI SDK documentation for the requested features\n- Design optimal architecture for the data volume\n- Install required packages (vector DB clients, zod, etc.)\n- Implement requested features following SDK best practices:\n  - Embeddings generation using embed() and embedMany()\n  - Vector database integration and schema design\n  - RAG pipeline with document chunking and retrieval\n  - Structured data generation using generateObject/streamObject\n- Add proper TypeScript types and Zod schemas\n- Implement error handling and retry logic\n- Optimize for cost and performance\n\nProvide the agent with:\n- Context: Current project structure and data sources\n- Target: $ARGUMENTS (requested data features)\n- Expected output: Production-ready data processing pipeline\n\nPhase 4: Verification\nGoal: Ensure features work correctly\n\nActions:\n- Run TypeScript compilation check\n- Example: !{bash npx tsc --noEmit}\n- Test embeddings generation with sample data\n- Verify vector database operations (if applicable)\n- Check cost implications and optimization\n\nPhase 5: Summary\nGoal: Document what was added\n\nActions:\n- List all data features that were implemented\n- Show database schema and indexes created\n- Note any API keys or environment variables needed\n- Provide cost estimates and optimization suggestions\n- Suggest next steps (data ingestion, query optimization)"
              },
              {
                "name": "/add-production",
                "description": "Add production features to Vercel AI SDK app including telemetry, rate limiting, error handling, testing, and middleware",
                "path": "plugins/vercel-ai-sdk/commands/add-production.md",
                "frontmatter": {
                  "description": "Add production features to Vercel AI SDK app including telemetry, rate limiting, error handling, testing, and middleware",
                  "argument-hint": [
                    "feature-requests"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Prepare a Vercel AI SDK application for production deployment with telemetry/observability, rate limiting, comprehensive error handling, testing infrastructure, and middleware.\n\nCore Principles:\n- Security and reliability first\n- Ask about monitoring platform preferences\n- Follow Vercel AI SDK documentation patterns\n- Implement comprehensive error handling and testing\n\nPhase 1: Discovery\nGoal: Understand what production features are needed\n\nActions:\n- Parse $ARGUMENTS to identify requested features\n- If unclear or no arguments provided, use AskUserQuestion to gather:\n  - Which production features do you want? (Telemetry, rate limiting, error handling, testing, middleware)\n  - What observability platform do you use? (Datadog, New Relic, Vercel Analytics, etc.)\n  - Do you have Redis/Upstash for rate limiting?\n  - What's your target error rate and latency?\n- Load package.json to understand current setup\n- Example: @package.json\n\nPhase 2: Analysis\nGoal: Understand current project state\n\nActions:\n- Check for existing monitoring/logging setup\n- Identify production environment (Vercel, AWS, self-hosted)\n- Review current error handling patterns\n- Assess existing test coverage\n- Example: !{bash ls *.test.ts *.spec.ts 2>/dev/null | wc -l}\n\nPhase 3: Implementation\nGoal: Add production features using specialized agent\n\nActions:\n\nInvoke the vercel-ai-production-agent to implement the requested production features.\n\nThe agent should:\n- Fetch relevant Vercel AI SDK documentation for the requested features\n- Design production-ready architecture\n- Install required packages (OpenTelemetry, testing libraries, rate limiting, etc.)\n- Implement requested features following SDK best practices:\n  - Telemetry with OpenTelemetry or custom providers\n  - Rate limiting with Redis/Upstash or edge solutions\n  - Comprehensive error handling with retry and circuit breaker patterns\n  - Test suites with mocks and snapshots (>80% coverage)\n  - Middleware for authentication, validation, logging\n- Add proper TypeScript types\n- Implement monitoring dashboards and alerts\n- Follow security best practices\n\nProvide the agent with:\n- Context: Current project structure and deployment platform\n- Target: $ARGUMENTS (requested production features)\n- Expected output: Production-ready application with monitoring and reliability features\n\nPhase 4: Verification\nGoal: Ensure production readiness\n\nActions:\n- Run test suites and check coverage\n- Example: !{bash npm test}\n- Verify telemetry data flows to monitoring platform\n- Test rate limiting under load\n- Validate error handling with failure scenarios\n- Run TypeScript compilation check\n- Example: !{bash npx tsc --noEmit}\n\nPhase 5: Summary\nGoal: Document production setup\n\nActions:\n- List all production features implemented\n- Show monitoring dashboard configuration\n- Note environment variables and secrets needed\n- Provide deployment checklist\n- Suggest next steps (load testing, security audit, deployment)"
              },
              {
                "name": "/add-provider",
                "description": "Add another AI provider to existing Vercel AI SDK project",
                "path": "plugins/vercel-ai-sdk/commands/add-provider.md",
                "frontmatter": {
                  "description": "Add another AI provider to existing Vercel AI SDK project",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add additional AI provider (OpenAI, Anthropic, Google, etc.) to project\n\nCore Principles:\n- Ask which provider to add\n- Fetch provider-specific docs (1-2 URLs)\n- Install and configure provider\n- Simple configuration, no complex logic needed\n\nPhase 1: Discovery\nGoal: Determine which provider to add\n\nActions:\n- Check existing providers: @package.json or @requirements.txt\n- Ask user: \"Which AI provider would you like to add? OpenAI, Anthropic, Google, xAI, or other?\"\n- Wait for response\n\nPhase 2: Fetch Provider Documentation\nGoal: Get provider setup docs\n\nActions:\nBased on user selection, fetch (2 URLs max):\n\n1. WebFetch: https://ai-sdk.dev/providers/ai-sdk-providers/[provider]\n2. WebFetch: https://ai-sdk.dev/docs/foundations/providers-and-models\n\nReplace [provider] with: openai, anthropic, google-generative-ai, xai, etc.\n\nPhase 3: Installation\nGoal: Install provider package\n\nActions:\n- Check latest version: WebSearch for package on npm/PyPI\n- Install provider package:\n  - TypeScript/JavaScript: npm install @ai-sdk/[provider]\n  - Python: pip install [provider-package]\n- Verify installation\n\nPhase 4: Configuration\nGoal: Update config and env\n\nActions:\n- Add API key to .env.example: [PROVIDER]_API_KEY=your_key_here\n- Update code to import new provider\n- Add example showing how to use provider\n- Document how to get API key\n- Show provider-specific model names\n\nPhase 5: Summary\nGoal: Show configuration steps\n\nActions:\nProvide:\n- Provider installed\n- Environment variable needed\n- Where to get API key\n- Example usage with new provider\n- How to switch between providers\n\nImportant Notes:\n- Simple configuration task (Pattern 1-style)\n- Minimal doc fetching (2 URLs)\n- Focused on setup only\n- No complex implementation"
              },
              {
                "name": "/add-streaming",
                "description": "Add text streaming capability to existing Vercel AI SDK project",
                "path": "plugins/vercel-ai-sdk/commands/add-streaming.md",
                "frontmatter": {
                  "description": "Add text streaming capability to existing Vercel AI SDK project",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add text streaming to existing Vercel AI SDK project with minimal docs and focused implementation\n\nCore Principles:\n- Detect existing project structure\n- Fetch only streaming-specific docs (2-3 URLs)\n- Implement streamText() or useChat() based on framework\n- Verify functionality\n\nPhase 1: Discovery\nGoal: Understand existing project setup\n\nActions:\n- Detect project type: Check for package.json, requirements.txt, framework configs\n- Load existing configuration: @package.json or @requirements.txt\n- Identify framework: Next.js, React, Node.js, Python, etc.\n- Find entry points: Look for existing AI SDK usage\n\nPhase 2: Fetch Streaming Documentation\nGoal: Get streaming-specific docs only\n\nActions:\nFetch these docs in parallel (3 URLs max):\n\n1. WebFetch: https://ai-sdk.dev/docs/foundations/streaming\n2. WebFetch: https://ai-sdk.dev/docs/ai-sdk-core/generating-text\n3. WebFetch: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot (if React/Next.js/frontend)\n\nPhase 3: Implementation\nGoal: Add streaming capability using appropriate agent\n\nActions:\n\nInvoke the **general-purpose** agent to implement streaming:\n\nThe agent should:\n- Analyze existing code structure\n- Add streamText() function for backend/Node.js projects\n- Add useChat() hook for React/Next.js/frontend projects\n- Create example endpoint/component showing streaming\n- Add proper error handling\n- Include helpful comments explaining streaming\n\nProvide the agent with:\n- Context: Existing project files and structure\n- Target: Add streaming based on framework detected\n- Expected output: Working streaming implementation with example\n\nPhase 4: Verification\nGoal: Ensure streaming works\n\nActions:\n- For TypeScript: Run npx tsc --noEmit to check types\n- For JavaScript: Verify syntax\n- For Python: Check imports\n- Test that streaming endpoint/component exists\n- Verify proper SDK usage patterns\n\nPhase 5: Summary\nGoal: Show what was added\n\nActions:\nProvide summary:\n- Files modified/created\n- Streaming implementation approach (streamText vs useChat)\n- How to test streaming\n- Example usage code\n- Next steps: Consider adding /vercel-ai-sdk:add-tools for function calling\n\nImportant Notes:\n- Adapts to existing framework (Next.js, React, Node.js, Python)\n- Fetches minimal docs (3 URLs)\n- Uses general-purpose agent for implementation\n- Verifies code compiles/runs\n- Focused on streaming only"
              },
              {
                "name": "/add-tools",
                "description": "Add tool/function calling capability to existing Vercel AI SDK project",
                "path": "plugins/vercel-ai-sdk/commands/add-tools.md",
                "frontmatter": {
                  "description": "Add tool/function calling capability to existing Vercel AI SDK project",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add tool/function calling to existing Vercel AI SDK project with focused implementation\n\nCore Principles:\n- Detect existing project structure\n- Fetch only tools-specific docs (2-3 URLs)\n- Implement tool definitions with proper schemas\n- Verify functionality\n\nPhase 1: Discovery\nGoal: Understand existing project setup\n\nActions:\n- Detect project type: Check for package.json, requirements.txt\n- Load existing configuration: @package.json or @requirements.txt\n- Identify framework and AI SDK usage\n- Find where tools should be integrated\n\nPhase 2: Fetch Tools Documentation\nGoal: Get tools-specific docs only\n\nActions:\nFetch these docs in parallel (3 URLs max):\n\n1. WebFetch: https://ai-sdk.dev/docs/foundations/tools\n2. WebFetch: https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\n3. WebFetch: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage (if frontend)\n\nPhase 3: Implementation\nGoal: Add tool calling capability\n\nActions:\n\nInvoke the **general-purpose** agent to implement tools: The agent should:\n- Define tool schemas with zod or similar validation\n- Create tool handler functions\n- Integrate tools with existing AI calls\n- Add example tools (e.g., getWeather, calculator)\n- Include proper error handling and validation\n- Add comments explaining tool structure\n\nProvide the agent with:\n- Context: Existing project files\n- Target: Add 1-2 example tools with proper schemas\n- Expected output: Working tool implementation\n\nPhase 4: Verification\nGoal: Ensure tools work\n\nActions:\n- For TypeScript: Run npx tsc --noEmit\n- For JavaScript: Verify syntax\n- For Python: Check imports and schemas\n- Verify tool schemas are valid\n- Check tool handlers exist\n\nPhase 5: Summary\nGoal: Show what was added\n\nActions:\nProvide summary:\n- Tool definitions created\n- Handler functions implemented\n- How to test tool calling\n- Example tool usage\n- How to add more custom tools\n- Next steps: Consider /vercel-ai-sdk:add-chat for UI\n\nImportant Notes:\n- Adapts to existing framework\n- Fetches minimal docs (3 URLs)\n- Creates 1-2 example tools\n- Uses proper schema validation\n- Focused on tools only"
              },
              {
                "name": "/add-ui-features",
                "description": "Add advanced UI features to Vercel AI SDK app including generative UI, useObject, useCompletion, message persistence, and attachments",
                "path": "plugins/vercel-ai-sdk/commands/add-ui-features.md",
                "frontmatter": {
                  "description": "Add advanced UI features to Vercel AI SDK app including generative UI, useObject, useCompletion, message persistence, and attachments",
                  "argument-hint": [
                    "feature-requests"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add advanced UI capabilities to a Vercel AI SDK application including generative user interfaces, structured object generation, text completion, message persistence, and file attachments.\n\nCore Principles:\n- Understand existing project structure before adding features\n- Ask for clarification when feature requirements are unclear\n- Follow Vercel AI SDK documentation patterns\n- Maintain framework-agnostic approach (Next.js, React, Node.js, etc.)\n\nPhase 1: Discovery\nGoal: Understand what UI features are needed\n\nActions:\n- Parse $ARGUMENTS to identify requested features\n- If unclear or no arguments provided, use AskUserQuestion to gather:\n  - Which UI features do you want? (Generative UI, useObject, useCompletion, persistence, attachments)\n  - Do you have a database set up for persistence?\n  - What framework are you using? (or should we detect it?)\n- Load package.json to understand current setup\n- Example: @package.json\n\nPhase 2: Analysis\nGoal: Understand current project state\n\nActions:\n- Check for existing AI SDK installation\n- Identify framework (Next.js App Router, Pages Router, plain React, etc.)\n- Locate existing AI-related code\n- Verify TypeScript/JavaScript setup\n- Example: !{bash ls tsconfig.json package.json 2>/dev/null}\n\nPhase 3: Implementation\nGoal: Add requested UI features using specialized agent\n\nActions:\n\nInvoke the vercel-ai-ui-agent to implement the requested UI features.\n\nThe agent should:\n- Fetch relevant Vercel AI SDK documentation for the requested features\n- Detect the framework and adapt implementation accordingly\n- Install required packages (@ai-sdk/ui-utils, zod, database clients if needed)\n- Implement requested features following SDK best practices:\n  - Generative UI using AI SDK RSC (if Next.js App Router)\n  - useObject hook for structured outputs\n  - useCompletion hook for text completion\n  - Message persistence with database integration\n  - File attachment handling for multi-modal chat\n- Add proper TypeScript types\n- Implement error handling and loading states\n- Follow existing project patterns and conventions\n\nProvide the agent with:\n- Context: Current project structure and framework\n- Target: $ARGUMENTS (requested UI features)\n- Expected output: Production-ready UI components with proper error handling\n\nPhase 4: Verification\nGoal: Ensure features work correctly\n\nActions:\n- Run TypeScript compilation check\n- Example: !{bash npx tsc --noEmit}\n- Verify new features are properly integrated\n- Check that dependencies are in package.json\n- Confirm environment variables are documented\n\nPhase 5: Summary\nGoal: Document what was added\n\nActions:\n- List all UI features that were implemented\n- Show file locations for new components/routes\n- Note any environment variables that need to be set\n- Suggest next steps (testing, styling, deployment)"
              },
              {
                "name": "/new-ai-app",
                "description": "Create and setup a new Vercel AI SDK application",
                "path": "plugins/vercel-ai-sdk/commands/new-ai-app.md",
                "frontmatter": {
                  "description": "Create and setup a new Vercel AI SDK application",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\nYou are tasked with helping the user create a new Vercel AI SDK application. Follow these steps in order:\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Step 1: Fetch Latest Documentation (DO THIS FIRST)\n\nUse WebFetch to read the official documentation NOW before asking any questions. Fetch documentation progressively throughout the setup process to get the most relevant, up-to-date information.\n\n**Initial Documentation (fetch in parallel):**\n\n1. Use WebFetch to read: https://sdk.vercel.ai/docs/introduction\n2. Use WebFetch to read: https://sdk.vercel.ai/docs/getting-started\n3. Use WebFetch to read: https://sdk.vercel.ai/docs/ai-sdk-core/overview\n4. Use WebFetch to read: https://sdk.vercel.ai/docs/ai-sdk-ui/overview\n\n**CRITICAL**: Do NOT skip these WebFetch calls. Fetch them in parallel. The documentation may have changed since your training data. Only after fetching these docs should you proceed to Step 2.\n\n## Step 2: Gather Requirements\n\nIMPORTANT: Ask these questions one at a time. Wait for the user's response before asking the next question. This makes it easier for the user to respond.\n\nAsk the questions in this order (skip any that the user has already provided via arguments):\n\n1. **Language** (ask first): \"Would you like to use TypeScript, JavaScript, or Python?\"\n\n   - Wait for response before continuing\n\n2. **Project name** (ask second): \"What would you like to name your project?\"\n\n   - If $ARGUMENTS is provided, use that as the project name and skip this question\n   - Wait for response before continuing\n\n3. **Framework choice** (ask third): \"Which framework would you like to use?\n\n   - Next.js (React framework with App Router support)\n   - React (standalone with Vite)\n   - Node.js (backend/API only)\n   - Python (backend/API with FastAPI or Flask)\n   - Svelte (with SvelteKit)\n   - Vue (with Nuxt or Vite)\"\n   - Wait for response before continuing\n\n4. **AI Provider** (ask fourth): \"Which AI provider would you like to use?\n\n   - OpenAI (GPT-4, GPT-3.5)\n   - Anthropic (Claude)\n   - Google (Gemini)\n   - Multiple providers (configure several)\"\n   - Wait for response before continuing\n\n5. **Features** (ask fifth): \"What features do you need? (Select all that apply)\n\n   - Text streaming (real-time AI responses)\n   - Tool/Function calling (AI can call your functions)\n   - Multi-modal (text, images, files)\n   - Chat history management\n   - Rate limiting and caching\"\n   - Wait for response before continuing\n\n6. **Tooling choice** (ask sixth): Let the user know what tools you'll use, and confirm with them that these are the tools they want to use (for example, they may prefer pnpm or bun over npm). Respect the user's preferences when executing on the requirements.\n\nAfter all questions are answered, proceed to fetch additional documentation based on their choices and create the setup plan.\n\n## Step 3: Fetch Feature-Specific Documentation\n\nBased on the user's feature selections, fetch relevant documentation:\n\n**If Text Streaming selected:**\n- WebFetch: https://sdk.vercel.ai/docs/ai-sdk-core/generating-text\n- WebFetch: https://sdk.vercel.ai/docs/ai-sdk-ui/streaming\n\n**If Tool Calling selected:**\n- WebFetch: https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling\n- WebFetch: https://sdk.vercel.ai/docs/foundations/tools\n\n**If Multi-modal selected:**\n- WebFetch: https://sdk.vercel.ai/docs/ai-sdk-core/multimodal\n\n**If Chat History selected:**\n- WebFetch: https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot\n\n**For provider-specific setup, fetch:**\n- OpenAI: https://sdk.vercel.ai/providers/ai-sdk-providers/openai\n- Anthropic: https://sdk.vercel.ai/providers/ai-sdk-providers/anthropic\n- Google: https://sdk.vercel.ai/providers/ai-sdk-providers/google-generative-ai\n\n## Setup Plan\n\nBased on the user's answers, create a plan that includes:\n\n1. **Project initialization**:\n\n   - Create project directory (if it doesn't exist)\n   - Initialize framework and package manager:\n     - Next.js: `npx create-next-app@latest` or manual setup with TypeScript\n     - React: `npm create vite@latest` with React + TypeScript template\n     - Node.js: `npm init -y` and setup `package.json` with type: \"module\" and scripts\n     - Python: Create `requirements.txt` or use `poetry init`\n     - Svelte: `npm create svelte@latest`\n     - Vue: `npm create vue@latest`\n   - Add necessary configuration files based on framework\n\n2. **Check for Latest Versions**:\n\n   - BEFORE installing, use WebSearch or check npm/PyPI to find the latest version\n   - For TypeScript/JavaScript: Check https://www.npmjs.com/package/ai\n   - For Python: Check https://pypi.org/project/ai/ or appropriate Python SDK\n   - Inform the user which version you're installing\n\n3. **SDK Installation**:\n\n   - TypeScript/JavaScript: `npm install ai@latest` (or specify latest version)\n   - Install provider SDKs based on selections:\n     - OpenAI: `npm install @ai-sdk/openai`\n     - Anthropic: `npm install @ai-sdk/anthropic`\n     - Google: `npm install @ai-sdk/google`\n   - Python: `pip install ai` or appropriate Python packages\n   - After installation, verify the installed versions\n\n4. **Create starter files**:\n\n   - Create appropriate entry points based on framework:\n     - Next.js: Create API route in `app/api/chat/route.ts` and UI component\n     - React: Create components in `src/` with example chat interface\n     - Node.js: Create `index.ts` or `src/index.ts` with API endpoints\n     - Python: Create `main.py` with FastAPI or Flask endpoints\n   - Include proper imports and error handling\n   - Use modern, up-to-date syntax and patterns from the latest SDK version\n   - Implement selected features (streaming, tool calling, etc.)\n\n5. **Environment setup**:\n\n   - Create a `.env.example` file with required API keys:\n     - `OPENAI_API_KEY=your_openai_key_here` (if using OpenAI)\n     - `ANTHROPIC_API_KEY=your_anthropic_key_here` (if using Anthropic)\n     - `GOOGLE_GENERATIVE_AI_API_KEY=your_google_key_here` (if using Google)\n   - Create `.env.local` (for Next.js) or `.env` with placeholder values\n   - Add `.env.local` and `.env` to `.gitignore`\n   - Explain how to get API keys from respective providers\n\n6. **Feature Implementation**:\n\n   - **If Text Streaming**: Implement streaming with `streamText()` or `useChat()` hook\n   - **If Tool Calling**: Set up tools with proper schemas and handlers\n   - **If Multi-modal**: Configure file/image handling\n   - **If Chat History**: Implement message storage and retrieval\n   - **If Rate Limiting**: Add rate limiting middleware or configuration\n\n7. **UI Components** (if applicable):\n   - Create chat interface components\n   - Add loading states and error handling\n   - Implement streaming UI updates\n   - Style with Tailwind CSS (if available) or basic CSS\n\n## Implementation\n\nAfter gathering requirements and getting user confirmation on the plan:\n\n1. Check for latest package versions using WebSearch or WebFetch\n2. Execute the setup steps\n3. Create all necessary files\n4. Install dependencies (always use latest stable versions)\n5. Verify installed versions and inform the user\n6. Create a working example based on their selections\n7. Add helpful comments in the code explaining what each part does\n8. **VERIFY THE CODE WORKS BEFORE FINISHING**:\n   - For TypeScript:\n     - Run `npx tsc --noEmit` to check for type errors\n     - Fix ALL type errors until types pass completely\n     - Ensure imports and types are correct\n     - Only proceed when type checking passes with no errors\n   - For JavaScript:\n     - Verify imports are correct\n     - Check for basic syntax errors\n   - For Python:\n     - Verify imports are correct\n     - Run basic linting if available\n   - **DO NOT consider the setup complete until the code verifies successfully**\n\n## Verification\n\nAfter all files are created and dependencies are installed, invoke the appropriate verifier agent to validate that the Vercel AI SDK application is properly configured and ready for use:\n\n1. **For TypeScript projects**: Invoke the **vercel-ai-verifier-ts** agent to validate the setup\n2. **For JavaScript projects**: Invoke the **vercel-ai-verifier-js** agent to validate the setup\n3. **For Python projects**: Invoke the **vercel-ai-verifier-py** agent to validate the setup\n4. The agent will check SDK usage, configuration, functionality, and adherence to official documentation\n5. Review the verification report and address any issues\n\n## Getting Started Guide\n\nOnce setup is complete and verified, provide the user with:\n\n1. **Next steps**:\n\n   - How to set their API key(s)\n   - How to run their application:\n     - Next.js: `npm run dev` (opens on http://localhost:3000)\n     - React: `npm run dev` (Vite dev server)\n     - Node.js: `npm start` or `node --loader ts-node/esm index.ts`\n     - Python: `python main.py` or `uvicorn main:app --reload`\n\n2. **Useful resources**:\n\n   - Link to Vercel AI SDK docs: https://sdk.vercel.ai/docs\n   - Link to provider-specific docs\n   - Link to examples: https://sdk.vercel.ai/examples\n   - Explain key concepts: streaming, tool calling, providers\n\n3. **Common next steps**:\n   - How to customize prompts and model parameters\n   - How to add custom tools/functions\n   - How to implement authentication\n   - How to deploy to Vercel or other platforms\n   - How to add chat history persistence\n   - How to implement rate limiting\n\n4. **Testing the application**:\n   - Provide example prompts to test\n   - Show how to test tool calling (if enabled)\n   - Demonstrate streaming behavior\n\n## Important Notes\n\n- **ALWAYS USE LATEST VERSIONS**: Before installing any packages, check for the latest versions using WebSearch or by checking npm/PyPI directly\n- **FETCH DOCS PROGRESSIVELY**: Don't fetch all docs at once. Fetch relevant documentation based on user's choices throughout the process\n- **VERIFY CODE RUNS CORRECTLY**:\n  - For TypeScript: Run `npx tsc --noEmit` and fix ALL type errors before finishing\n  - For JavaScript: Verify syntax and imports are correct\n  - For Python: Verify syntax and imports are correct\n  - Do NOT consider the task complete until the code passes verification\n- Verify the installed versions after installation and inform the user\n- Check the official documentation for any version-specific requirements (Node.js version, Python version, etc.)\n- Always check if directories/files already exist before creating them\n- Use the user's preferred package manager (npm, yarn, pnpm, bun for TypeScript/JavaScript; pip, poetry for Python)\n- Ensure all code examples are functional and include proper error handling\n- Use modern syntax and patterns that are compatible with the latest SDK version\n- Make the experience interactive and educational\n- **ASK QUESTIONS ONE AT A TIME** - Do not ask multiple questions in a single response\n- **PROGRESSIVE DOCUMENTATION**: Fetch docs as needed based on user selections, not all at once\n\nBegin by fetching the initial documentation, then ask the FIRST requirement question only. Wait for the user's answer before proceeding to the next question."
              },
              {
                "name": "/new-app",
                "description": "Create initial Vercel AI SDK project scaffold with basic setup",
                "path": "plugins/vercel-ai-sdk/commands/new-app.md",
                "frontmatter": {
                  "description": "Create initial Vercel AI SDK project scaffold with basic setup",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the vercel-ai-sdk plugin:\n\n- **SKILLS-OVERVIEW.md**\n- **agent-workflow-patterns**: AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.\n- **generative-ui-patterns**: Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.\n- **provider-config-validator**: Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.\n- **rag-implementation**: RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.\n- **testing-patterns**: Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create minimal Vercel AI SDK project scaffold with basic configuration. Features like streaming, tools, and chat are added separately using other commands.\n\nCore Principles:\n- Keep it simple - scaffold only, no features\n- Fetch minimal docs (3-5 URLs)\n- Get user choices early\n- Verify setup works\n\nPhase 1: Discovery\nGoal: Understand project requirements\n\nActions:\n- Parse $ARGUMENTS for project name (if provided)\n- Ask ONE question at a time, wait for response:\n  1. **Project name**: Use $ARGUMENTS or ask \"What should we name your project?\"\n  2. **Language**: \"TypeScript, JavaScript, or Python?\"\n  3. **Framework**: \"Next.js, React (Vite), Node.js, Python (FastAPI), Svelte, or Vue?\"\n  4. **AI Provider**: \"OpenAI, Anthropic, Google, or xAI?\"\n\nPhase 2: Fetch Minimal Documentation\nGoal: Get essential setup docs only\n\nActions:\nFetch these docs in parallel (4 URLs max):\n\n1. WebFetch: https://ai-sdk.dev/docs/introduction\n2. WebFetch: https://ai-sdk.dev/docs/foundations/overview\n3. WebFetch: https://ai-sdk.dev/docs/getting-started/[framework-specific]\n4. WebFetch: https://ai-sdk.dev/providers/ai-sdk-providers/[provider]\n\nReplace [framework-specific] and [provider] with user selections.\n\nPhase 3: Project Setup\nGoal: Create basic project structure\n\nActions:\n\n1. **Check latest version**:\n   - Use WebSearch to find latest `ai` package version\n   - For TypeScript/JavaScript: https://www.npmjs.com/package/ai\n   - For Python: https://pypi.org/project/ai/\n\n2. **Create project directory**:\n   - Create directory with project name\n   - Navigate into it\n\n3. **Initialize project**:\n   - TypeScript/JavaScript: Run npm init -y, setup package.json with type: module\n   - Python: Create requirements.txt or pyproject.toml\n\n4. **Install SDK and provider**:\n   - TypeScript/JavaScript: npm install ai@latest @ai-sdk/[provider]\n   - Python: pip install ai [provider-package]\n   - Verify installed versions\n\n5. **Create basic files**:\n   - Create .env.example with [PROVIDER]_API_KEY placeholder\n   - Add .env to .gitignore\n   - Create minimal entry point (index.ts/js or main.py) with simple example\n   - Add helpful comments explaining next steps\n\n6. **Environment notes**:\n   - Explain how to get API key from provider\n   - Show how to set .env file\n\nPhase 4: Verification\nGoal: Ensure setup is correct\n\nActions:\n\nInvoke the appropriate verifier agent based on language:\n\n**For TypeScript**: Invoke the **vercel-ai-verifier-ts** agent to validate the setup\n**For JavaScript**: Invoke the **vercel-ai-verifier-js** agent to validate the setup\n**For Python**: Invoke the **vercel-ai-verifier-py** agent to validate the setup\n\nAgent should check:\n- Package installation\n- Configuration files\n- Basic imports work\n- Environment setup\n\nPhase 5: Next Steps\nGoal: Guide user on what to do next\n\nActions:\nProvide clear next steps:\n\n1. **Set API key**: How to add key to .env file\n2. **Run the app**: Command to execute (npm start, python main.py, etc.)\n3. **Add features**: Point to other commands:\n   - /vercel-ai-sdk:add-streaming - Add text streaming\n   - /vercel-ai-sdk:add-tools - Add function/tool calling\n   - /vercel-ai-sdk:add-chat - Add chat UI\n   - /vercel-ai-sdk:add-provider - Add another AI provider\n\n4. **Resources**:\n   - SDK docs: https://ai-sdk.dev/docs\n   - Examples: https://ai-sdk.dev/examples\n   - Provider docs link\n\nImportant Notes:\n- This creates SCAFFOLD ONLY - no features implemented yet\n- Use other commands to add streaming, tools, chat, etc.\n- Always check latest package versions before installing\n- Verify code compiles/runs before finishing\n- Keep it minimal and focused"
              }
            ],
            "skills": [
              {
                "name": "agent-workflow-patterns",
                "description": "AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.",
                "path": "plugins/vercel-ai-sdk/skills/agent-workflow-patterns/SKILL.md",
                "frontmatter": {
                  "name": "agent-workflow-patterns",
                  "description": "AI agent workflow patterns including ReAct agents, multi-agent systems, loop control, tool orchestration, and autonomous agent architectures. Use when building AI agents, implementing workflows, creating autonomous systems, or when user mentions agents, workflows, ReAct, multi-step reasoning, loop control, agent orchestration, or autonomous AI.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Agent Workflow Patterns\n\n**Purpose:** Provide production-ready agent architectures, workflow patterns, and loop control strategies for building autonomous AI systems with Vercel AI SDK.\n\n**Activation Triggers:**\n- Building autonomous AI agents\n- Implementing multi-step reasoning\n- Creating agent workflows\n- Tool orchestration and coordination\n- Loop control and iteration management\n- Multi-agent system architectures\n- ReAct (Reasoning + Acting) patterns\n\n**Key Resources:**\n- `templates/react-agent.ts` - ReAct agent pattern\n- `templates/multi-agent-system.ts` - Multiple specialized agents\n- `templates/workflow-orchestrator.ts` - Workflow coordination\n- `templates/loop-control.ts` - Iteration and safeguards\n- `templates/tool-coordinator.ts` - Tool orchestration\n- `scripts/validate-agent.sh` - Validate agent configuration\n- `examples/` - Production agent implementations (RAG agent, SQL agent, etc.)\n\n## Core Agent Patterns\n\n### 1. ReAct Agent (Reasoning + Acting)\n\n**When to use:** Complex problem-solving requiring iterative thought and action\n\n**Template:** `templates/react-agent.ts`\n\n**Pattern:**\n```typescript\nasync function reactAgent(task: string, maxIterations: number = 5) {\n  const tools = { /* tool definitions */ }\n  let iteration = 0\n\n  while (iteration < maxIterations) {\n    // Reasoning step\n    const thought = await generateText({\n      model: openai('gpt-4o')\n      messages: [\n        { role: 'system', content: 'Think step-by-step...' }\n        { role: 'user', content: task }\n      ]\n    })\n\n    // Acting step (tool calls)\n    const action = await generateText({\n      model: openai('gpt-4o')\n      tools\n      toolChoice: 'auto'\n      messages: [/* ... */]\n    })\n\n    // Check if task complete\n    if (isComplete(action)) break\n    iteration++\n  }\n\n  return result\n}\n```\n\n**Best for:** Research, analysis, complex planning\n\n### 2. Multi-Agent System\n\n**When to use:** Complex domains requiring specialized expertise\n\n**Template:** `templates/multi-agent-system.ts`\n\n**Pattern:**\n- Coordinator agent routes tasks\n- Specialist agents handle specific domains\n- Result aggregation and synthesis\n\n**Best for:** Multi-domain problems, parallel task execution\n\n### 3. Workflow Orchestration\n\n**When to use:** Pre-defined sequences of steps\n\n**Template:** `templates/workflow-orchestrator.ts`\n\n**Pattern:**\n- Define workflow steps\n- Execute sequentially with error handling\n- State management between steps\n- Conditional branching\n\n**Best for:** Structured processes, pipelines\n\n## Loop Control Strategies\n\n### 1. Iteration Limits\n\n```typescript\nconst config = {\n  maxIterations: 10\n  onMaxIterations: 'return-last' | 'throw-error'\n}\n```\n\n**Prevents:** Infinite loops\n\n### 2. Cost Limits\n\n```typescript\nconst config = {\n  maxTokens: 10000\n  onMaxTokens: 'graceful-stop'\n}\n```\n\n**Prevents:** Runaway costs\n\n### 3. Time Limits\n\n```typescript\nconst config = {\n  maxDuration: 30000, // 30 seconds\n  onTimeout: 'return-partial'\n}\n```\n\n**Prevents:** Long-running operations\n\n### 4. Quality Gates\n\n```typescript\nconst config = {\n  stopCondition: (result) => result.confidence > 0.9\n}\n```\n\n**Ensures:** Quality outputs\n\n## Tool Orchestration\n\n### Sequential Tool Execution\n\n```typescript\nconst tools = {\n  search: tool({ /* ... */ })\n  analyze: tool({ /* ... */ })\n  summarize: tool({ /* ... */ })\n}\n\n// AI decides order and usage\nconst result = await generateText({\n  model: openai('gpt-4o')\n  tools\n  maxToolRoundtrips: 5\n})\n```\n\n### Parallel Tool Execution\n\n```typescript\nconst results = await Promise.all([\n  callTool('search', { query: 'topic1' })\n  callTool('search', { query: 'topic2' })\n  callTool('search', { query: 'topic3' })\n])\n```\n\n## Agent State Management\n\n```typescript\ninterface AgentState {\n  conversation: Message[]\n  context: Record<string, any>\n  toolResults: ToolResult[]\n  iteration: number\n}\n\nclass StatefulAgent {\n  private state: AgentState\n\n  async execute(task: string) {\n    while (!this.isComplete()) {\n      await this.step()\n      this.updateState()\n    }\n    return this.state\n  }\n}\n```\n\n## Production Best Practices\n\n### 1. Error Recovery\n\n```typescript\ntry {\n  result = await agent.execute(task)\n} catch (error) {\n  if (error.code === 'MAX_ITERATIONS') {\n    return agent.getBestSoFar()\n  }\n  throw error\n}\n```\n\n### 2. Monitoring\n\n```typescript\nagent.on('iteration', ({ count, result }) => {\n  metrics.record('agent.iteration', { count })\n})\n```\n\n### 3. Safeguards\n\n- Rate limiting\n- Input validation\n- Output sanitization\n- Cost tracking\n\n## Common Agent Architectures\n\n### 1. RAG Agent\n\n**Example:** `examples/rag-agent.ts`\n\nRetrieves information and answers questions\n\n### 2. SQL Agent\n\n**Example:** `examples/sql-agent.ts`\n\nQueries databases using natural language\n\n### 3. Research Agent\n\n**Example:** `examples/research-agent.ts`\n\nGathers and synthesizes information\n\n### 4. Code Agent\n\n**Example:** `examples/code-agent.ts`\n\nWrites and debugs code\n\n## Resources\n\n**Templates:**\n- `react-agent.ts` - ReAct pattern implementation\n- `multi-agent-system.ts` - Multi-agent coordination\n- `workflow-orchestrator.ts` - Workflow execution\n- `loop-control.ts` - Iteration safeguards\n- `tool-coordinator.ts` - Tool orchestration\n\n**Scripts:**\n- `validate-agent.sh` - Agent config validation\n\n**Examples:**\n- `rag-agent.ts` - Complete RAG agent\n- `sql-agent.ts` - Natural language SQL\n- `research-agent.ts` - Information gathering\n- `code-agent.ts` - Code generation\n\n---\n\n**SDK Version:** Vercel AI SDK 5+\n**Agent Frameworks:** Built-in tools, MCP integration\n\n**Best Practice:** Start simple (single tool), add complexity as needed"
              },
              {
                "name": "generative-ui-patterns",
                "description": "Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.",
                "path": "plugins/vercel-ai-sdk/skills/generative-ui-patterns/SKILL.md",
                "frontmatter": {
                  "name": "generative-ui-patterns",
                  "description": "Generative UI implementation patterns for AI SDK RSC including server-side streaming components, dynamic UI generation, and client-server coordination. Use when implementing generative UI, building AI SDK RSC, creating streaming components, or when user mentions generative UI, React Server Components, dynamic UI, AI-generated interfaces, or server-side streaming.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Generative UI Patterns\n\n**Purpose:** Provide reusable templates, patterns, and validation scripts for implementing Generative UI with Vercel AI SDK RSC (React Server Components).\n\n**Activation Triggers:**\n- Building generative UI interfaces\n- Implementing AI SDK RSC patterns\n- Creating server-side streaming components\n- Dynamic UI generation based on AI responses\n- Client-server component coordination\n- Next.js App Router RSC integration\n\n**Key Resources:**\n- `templates/server-action-pattern.tsx` - Server action template for AI RSC\n- `templates/streaming-component.tsx` - Streaming component pattern\n- `templates/client-wrapper.tsx` - Client component wrapper pattern\n- `templates/route-handler.ts` - API route handler for streaming UI\n- `scripts/validate-rsc-setup.sh` - Validate Next.js RSC configuration\n- `scripts/generate-ui-component.sh` - Generate UI component from schema\n- `examples/` - Real-world generative UI implementations\n\n## Core Patterns\n\n### 1. Server Action Pattern (AI SDK RSC)\n\n**When to use:** Next.js App Router with React Server Components\n\n**Template:** `templates/server-action-pattern.tsx`\n\n**Pattern:**\n```typescript\n'use server'\nimport { streamUI } from 'ai/rsc'\nimport { openai } from '@ai-sdk/openai'\n\nexport async function generateUI(prompt: string) {\n  const result = await streamUI({\n    model: openai('gpt-4')\n    prompt\n    text: ({ content }) => <p>{content}</p>\n    tools: {\n      // Tool definitions for dynamic UI generation\n    }\n  })\n\n  return result.value\n}\n```\n\n**Key features:**\n- Server-side only execution (security)\n- Streaming UI components to client\n- Tool-based dynamic component selection\n- Type-safe component generation\n\n### 2. Streaming Component Pattern\n\n**When to use:** Need real-time UI updates during AI generation\n\n**Template:** `templates/streaming-component.tsx`\n\n**Pattern:**\n- Server component streams UI chunks\n- Client component receives and renders\n- Suspense boundaries for loading states\n- Error boundaries for failure handling\n\n### 3. Client-Server Coordination\n\n**When to use:** Complex interactions between client and server components\n\n**Template:** `templates/client-wrapper.tsx`\n\n**Pattern:**\n- Client components handle interactivity\n- Server components handle AI calls\n- Proper hydration boundaries\n- State management across boundary\n\n## Implementation Workflow\n\n### Step 1: Validate Next.js Setup\n\n```bash\n# Check Next.js version and App Router setup\n./scripts/validate-rsc-setup.sh\n```\n\n**Checks:**\n- Next.js 13.4+ (App Router required)\n- React 18+ (Server Components support)\n- `app/` directory exists\n- TypeScript configuration for RSC\n\n### Step 2: Choose Component Pattern\n\n**Decision tree:**\n- Simple text streaming ‚Üí Use basic streamUI with text callback\n- Dynamic UI (charts, cards, forms) ‚Üí Use tools with component mapping\n- Complex multi-step ‚Üí Use workflow with multiple streamUI calls\n- Interactive elements ‚Üí Use client wrapper pattern\n\n### Step 3: Generate Component Template\n\n```bash\n# Generate component from pattern\n./scripts/generate-ui-component.sh <pattern-type> <component-name>\n\n# Examples:\n./scripts/generate-ui-component.sh stream-text MessageCard\n./scripts/generate-ui-component.sh dynamic-tool ChartGenerator\n./scripts/generate-ui-component.sh workflow MultiStepForm\n```\n\n### Step 4: Implement Server Action\n\n**Use template:** `templates/server-action-pattern.tsx`\n\n**Customize:**\n1. Define tools for dynamic component selection\n2. Map tool outputs to React components\n3. Add error handling and fallbacks\n4. Configure streaming options\n\n### Step 5: Add Client Wrapper (if needed)\n\n**Use template:** `templates/client-wrapper.tsx`\n\n**For:**\n- User interactions (buttons, forms)\n- Client-side state management\n- Browser APIs (localStorage, etc.)\n- Animations and transitions\n\n## Component Mapping Strategy\n\n### Tool-Based Dynamic UI\n\n**Pattern:**\n```typescript\nconst tools = {\n  showChart: tool({\n    description: 'Display data as chart'\n    parameters: z.object({\n      data: z.array(z.number())\n      type: z.enum(['bar', 'line', 'pie'])\n    })\n    generate: async ({ data, type }) => {\n      return <ChartComponent data={data} type={type} />\n    }\n  })\n  showTable: tool({\n    description: 'Display data as table'\n    parameters: z.object({\n      rows: z.array(z.record(z.string()))\n    })\n    generate: async ({ rows }) => {\n      return <TableComponent rows={rows} />\n    }\n  })\n}\n```\n\n**Key principle:** Let AI choose appropriate UI component based on data\n\n## Error Handling & Fallbacks\n\n### Pattern: Graceful Degradation\n\n```typescript\nconst result = await streamUI({\n  // ... config\n  onError: (error) => {\n    return <ErrorBoundary error={error} />\n  }\n  fallback: <LoadingSpinner />\n})\n```\n\n**Best practices:**\n- Always provide fallback component\n- Handle streaming interruptions\n- Validate tool parameters\n- Sanitize AI-generated content\n\n## Performance Optimization\n\n### 1. Component Code Splitting\n\nUse dynamic imports for heavy components:\n```typescript\nconst HeavyChart = dynamic(() => import('./HeavyChart'))\n```\n\n### 2. Streaming Chunks\n\nControl chunk size for optimal UX:\n```typescript\nconst result = await streamUI({\n  // ... config\n  experimental_streamChunking: true\n})\n```\n\n### 3. Caching Strategy\n\nCache static UI components:\n```typescript\nexport const revalidate = 3600 // 1 hour\n```\n\n## Security Considerations\n\n### Server-Only Code\n\n**Critical:** Never expose server actions to client\n\n```typescript\n// ‚úÖ Good: server action\n'use server'\nexport async function generateUI() { /* ... */ }\n\n// ‚ùå Bad: client-accessible\nexport async function generateUI() { /* ... */ }\n```\n\n### Content Sanitization\n\nAlways sanitize AI-generated content:\n```typescript\nimport DOMPurify from 'isomorphic-dompurify'\n\nconst sanitized = DOMPurify.sanitize(aiContent)\n```\n\n## Framework Integration\n\n### Next.js App Router\n\n**File structure:**\n```\napp/\n  actions/\n    generate-ui.ts       # Server actions\n  components/\n    ui/\n      generated/         # Generated UI components\n    client-wrapper.tsx   # Client components\n  api/\n    stream-ui/\n      route.ts           # Alternative API route pattern\n```\n\n### TypeScript Configuration\n\n**Required:** Proper RSC types\n\n```typescript\n// tsconfig.json\n{\n  \"compilerOptions\": {\n    \"jsx\": \"preserve\"\n    \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"]\n    \"module\": \"esnext\"\n    \"moduleResolution\": \"bundler\"\n  }\n}\n```\n\n## Testing Strategy\n\n### Unit Testing Server Actions\n\n```typescript\nimport { generateUI } from './actions'\n\ntest('generates UI from prompt', async () => {\n  const ui = await generateUI('Show weather chart')\n  expect(ui).toMatchSnapshot()\n})\n```\n\n### Integration Testing Streaming\n\nUse testing-patterns skill for comprehensive streaming tests\n\n## Common Patterns\n\n### 1. Multi-Step Form Generation\n\n**Use case:** AI generates form fields dynamically\n\n**Template:** `examples/multi-step-form.tsx`\n\n### 2. Data Visualization\n\n**Use case:** AI selects appropriate chart type\n\n**Template:** `examples/chart-generator.tsx`\n\n### 3. Dashboard Generation\n\n**Use case:** AI creates dashboard widgets\n\n**Template:** `examples/dashboard-generator.tsx`\n\n### 4. Content Cards\n\n**Use case:** AI generates content in card layouts\n\n**Template:** `examples/content-cards.tsx`\n\n## Resources\n\n**Scripts:**\n- `validate-rsc-setup.sh` - Verify Next.js and RSC configuration\n- `generate-ui-component.sh` - Scaffold component from pattern\n\n**Templates:**\n- `server-action-pattern.tsx` - Complete server action template\n- `streaming-component.tsx` - Streaming component pattern\n- `client-wrapper.tsx` - Client component wrapper\n- `route-handler.ts` - API route alternative pattern\n\n**Examples:**\n- `multi-step-form.tsx` - Complete multi-step form implementation\n- `chart-generator.tsx` - Dynamic chart generation\n- `dashboard-generator.tsx` - Full dashboard example\n- `content-cards.tsx` - Card-based content layout\n\n---\n\n**Supported Frameworks:** Next.js 13.4+ (App Router only)\n**SDK Version:** Vercel AI SDK 5+ with RSC support\n**React Version:** React 18+ (Server Components)\n\n**Best Practice:** Always start with `validate-rsc-setup.sh` to ensure environment compatibility"
              },
              {
                "name": "provider-config-validator",
                "description": "Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.",
                "path": "plugins/vercel-ai-sdk/skills/provider-config-validator/SKILL.md",
                "frontmatter": {
                  "name": "provider-config-validator",
                  "description": "Validate and debug Vercel AI SDK provider configurations including API keys, environment setup, model compatibility, and rate limiting. Use when encountering provider errors, authentication failures, API key issues, missing environment variables, model compatibility problems, rate limiting errors, or when user mentions provider setup, configuration debugging, or SDK connection issues.",
                  "allowed-tools": "Read, Grep, Glob, Bash"
                },
                "content": "# Provider Configuration Validator\n\n**Purpose:** Autonomously validate, diagnose, and fix Vercel AI SDK provider configurations.\n\n**Activation Triggers:**\n- API errors (401, 403, 429, 404)\n- Missing/invalid API keys\n- Package installation issues\n- Model compatibility errors\n- Environment variable problems\n- Import/connection failures\n\n**Key Resources:**\n- `scripts/validate-provider.sh` - Full validation (packages, keys, env)\n- `scripts/check-model-compatibility.sh` - Validate model names\n- `scripts/generate-fix.sh` - Generate fixes for common issues\n- `scripts/test-provider-connection.sh` - Test real API connection\n- `examples/troubleshooting-guide.md` - Comprehensive troubleshooting (10+ scenarios)\n- `templates/` - .env, .gitignore, error handler code templates\n\n## Diagnostic Workflow\n\n### 1. Detect Issue Type\n\nIdentify error from symptoms:\n- **401/403**: Invalid API key ‚Üí Check key format and env var\n- **429**: Rate limiting ‚Üí Add retry logic\n- **404/model_not_found**: Invalid model ‚Üí Validate model name\n- **Cannot find module**: Missing package ‚Üí Install provider SDK\n- **Missing env var**: No .env ‚Üí Create .env file\n- **Import error**: Wrong syntax ‚Üí Fix imports for framework\n\n### 2. Run Validation Script\n\n```bash\n# Main validation - checks everything\n./scripts/validate-provider.sh <provider>\n\n# Examples:\n./scripts/validate-provider.sh openai\n./scripts/validate-provider.sh anthropic\n```\n\n**Checks performed:**\n- ‚úÖ Provider package installed (correct version)\n- ‚úÖ Core SDK (`ai`) installed\n- ‚úÖ .env file exists\n- ‚úÖ API key set with correct format\n- ‚úÖ .env in .gitignore (security)\n\n### 3. Validate Model Name (if applicable)\n\n```bash\n./scripts/check-model-compatibility.sh <provider> <model>\n\n# Examples:\n./scripts/check-model-compatibility.sh openai gpt-4o\n./scripts/check-model-compatibility.sh anthropic claude-sonnet-4-5-20250929\n```\n\nShows valid models if name is wrong, suggests closest matches.\n\n### 4. Generate Fixes\n\n```bash\n./scripts/generate-fix.sh <issue-type> <provider>\n\n# Issue types:\n# - missing-api-key      ‚Üí Creates .env with correct format\n# - wrong-format         ‚Üí Shows valid key format\n# - missing-package      ‚Üí Installs provider package\n# - model-compatibility  ‚Üí Lists valid models\n# - rate-limiting        ‚Üí Adds retry helper with exponential backoff\n# - import-error         ‚Üí Fixes import statements\n```\n\n### 5. Test Connection\n\n```bash\n# Verify API credentials work\n./scripts/test-provider-connection.sh <provider>\n```\n\nMakes real API call with minimal tokens to verify setup.\n\n## Provider-Specific Info\n\n**OpenAI:**\n- Package: `@ai-sdk/openai`\n- Env: `OPENAI_API_KEY`\n- Format: `sk-proj-...` or `sk-...`\n- Models: gpt-4o, gpt-4o-mini, gpt-4, gpt-3.5-turbo\n\n**Anthropic:**\n- Package: `@ai-sdk/anthropic`\n- Env: `ANTHROPIC_API_KEY`\n- Format: `sk-ant-api03-...`\n- Models: claude-sonnet-4-5-20250929, claude-opus-4-20250514\n\n**Google:**\n- Package: `@ai-sdk/google`\n- Env: `GOOGLE_GENERATIVE_AI_API_KEY`\n- Format: `AIza...`\n- Models: gemini-1.5-pro, gemini-1.5-flash\n\n**xAI:**\n- Package: `@ai-sdk/xai`\n- Env: `XAI_API_KEY`\n- Format: `xai-...`\n- Models: grok-beta, grok-vision-beta\n\n## Common Fixes\n\n### Missing API Key\n```bash\n# Create .env with correct structure\n./scripts/generate-fix.sh missing-api-key openai\n\n# Then add actual key from provider dashboard\n```\n\n### Wrong Model Name\n```bash\n# Check valid models\n./scripts/check-model-compatibility.sh anthropic claude-v3-opus\n# Shows: Did you mean? ‚Üí claude-opus-4-20250514\n```\n\n### Rate Limiting\n```bash\n# Generate retry helper\n./scripts/generate-fix.sh rate-limiting openai\n# Creates retryHelper.ts with exponential backoff\n```\n\n### Package Not Installed\n```bash\n# Install provider package\n./scripts/generate-fix.sh missing-package anthropic\n# Runs: npm install ai @ai-sdk/anthropic\n```\n\n## Resources\n\n**Scripts:** All scripts in `scripts/` directory are executable and documented in README.md\n\n**Templates:** `templates/` contains .env, .gitignore, and error-handler-template.ts\n\n**Examples:** `examples/troubleshooting-guide.md` has detailed solutions for 10+ scenarios including CORS, streaming, TypeScript, and provider-specific issues\n\n---\n\n**Supported Providers:** OpenAI, Anthropic, Google, xAI, Groq, Mistral, Cohere, DeepSeek\n\n**Version:** 1.0.0\n**SDK Compatibility:** Vercel AI SDK 5+"
              },
              {
                "name": "rag-implementation",
                "description": "RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.",
                "path": "plugins/vercel-ai-sdk/skills/rag-implementation/SKILL.md",
                "frontmatter": {
                  "name": "rag-implementation",
                  "description": "RAG (Retrieval Augmented Generation) implementation patterns including document chunking, embedding generation, vector database integration, semantic search, and RAG pipelines. Use when building RAG systems, implementing semantic search, creating knowledge bases, or when user mentions RAG, embeddings, vector database, retrieval, document chunking, or knowledge retrieval.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# RAG Implementation Patterns\n\n**Purpose:** Provide complete RAG pipeline templates, chunking strategies, vector database schemas, and retrieval patterns for building production-ready RAG systems with Vercel AI SDK.\n\n**Activation Triggers:**\n- Building RAG (Retrieval Augmented Generation) systems\n- Implementing semantic search functionality\n- Creating AI-powered knowledge bases\n- Document ingestion and embedding generation\n- Vector database integration\n- Hybrid search (vector + keyword) implementation\n\n**Key Resources:**\n- `templates/rag-pipeline.ts` - Complete RAG pipeline template\n- `templates/vector-db-schemas/` - Database schemas for Pinecone, Chroma, pgvector, Weaviate\n- `templates/chunking-strategies.ts` - Document chunking implementations\n- `templates/retrieval-patterns.ts` - Semantic search and hybrid search patterns\n- `scripts/chunk-documents.sh` - Document chunking utility\n- `scripts/generate-embeddings.sh` - Batch embedding generation\n- `scripts/validate-rag-setup.sh` - Validate RAG configuration\n- `examples/` - Complete RAG implementations (chatbot, Q&A, search)\n\n## Core RAG Pipeline\n\n### 1. Document Ingestion ‚Üí Chunking ‚Üí Embedding ‚Üí Storage ‚Üí Retrieval ‚Üí Generation\n\n**Template:** `templates/rag-pipeline.ts`\n\n**Workflow:**\n```typescript\n// 1. Ingest documents\nconst documents = await loadDocuments()\n\n// 2. Chunk documents\nconst chunks = await chunkDocuments(documents, {\n  chunkSize: 1000\n  overlap: 200\n  strategy: 'semantic'\n})\n\n// 3. Generate embeddings\nconst embeddings = await embedMany({\n  model: openai.embedding('text-embedding-3-small')\n  values: chunks.map(c => c.text)\n})\n\n// 4. Store in vector DB\nawait vectorDB.upsert(chunks.map((chunk, i) => ({\n  id: chunk.id\n  embedding: embeddings.embeddings[i]\n  metadata: chunk.metadata\n})))\n\n// 5. Retrieve relevant chunks\nconst query = await embed({\n  model: openai.embedding('text-embedding-3-small')\n  value: userQuestion\n})\n\nconst results = await vectorDB.query({\n  vector: query.embedding\n  topK: 5\n})\n\n// 6. Generate response with context\nconst response = await generateText({\n  model: openai('gpt-4o')\n  messages: [\n    {\n      role: 'system'\n      content: `Answer based on this context:\\n\\n${results.map(r => r.text).join('\\n\\n')}`\n    }\n    { role: 'user', content: userQuestion }\n  ]\n})\n```\n\n## Chunking Strategies\n\n### 1. Fixed-Size Chunking\n\n**When to use:** Simple documents, consistent structure\n\n**Template:** `templates/chunking-strategies.ts#fixedSize`\n\n```typescript\nfunction chunkByFixedSize(text: string, chunkSize: number, overlap: number) {\n  const chunks = []\n  for (let i = 0; i < text.length; i += chunkSize - overlap) {\n    chunks.push(text.slice(i, i + chunkSize))\n  }\n  return chunks\n}\n```\n\n**Best for:** Articles, blog posts, documentation\n\n### 2. Semantic Chunking\n\n**When to use:** Preserve meaning and context\n\n**Template:** `templates/chunking-strategies.ts#semantic`\n\n```typescript\nfunction chunkBySemantic(text: string) {\n  // Split on paragraphs, headings, or natural breaks\n  const sections = text.split(/\\n\\n+/)\n  const chunks = []\n\n  let currentChunk = ''\n  for (const section of sections) {\n    if ((currentChunk + section).length > 1000) {\n      if (currentChunk) chunks.push(currentChunk.trim())\n      currentChunk = section\n    } else {\n      currentChunk += '\\n\\n' + section\n    }\n  }\n  if (currentChunk) chunks.push(currentChunk.trim())\n\n  return chunks\n}\n```\n\n**Best for:** Books, research papers, structured content\n\n### 3. Recursive Chunking\n\n**When to use:** Hierarchical documents with sections/subsections\n\n**Template:** `templates/chunking-strategies.ts#recursive`\n\n**Best for:** Technical docs, manuals, legal documents\n\n## Vector Database Integration\n\n### Supported Databases\n\n**1. Pinecone (Fully Managed)**\n\n**Template:** `templates/vector-db-schemas/pinecone-schema.ts`\n\n```typescript\nimport { Pinecone } from '@pinecone-database/pinecone'\n\nconst pinecone = new Pinecone({\n  apiKey: process.env.PINECONE_API_KEY!\n})\n\nconst index = pinecone.index('knowledge-base')\n\n// Upsert embeddings\nawait index.upsert([\n  {\n    id: 'doc-1-chunk-1'\n    values: embedding\n    metadata: {\n      text: chunk.text\n      source: chunk.source\n      timestamp: Date.now()\n    }\n  }\n])\n\n// Query\nconst results = await index.query({\n  vector: queryEmbedding\n  topK: 5\n  includeMetadata: true\n})\n```\n\n**2. Chroma (Open Source)**\n\n**Template:** `templates/vector-db-schemas/chroma-schema.ts`\n\n**Best for:** Local development, prototyping\n\n**3. pgvector (Postgres Extension)**\n\n**Template:** `templates/vector-db-schemas/pgvector-schema.sql`\n\n**Best for:** Existing Postgres infrastructure, cost-effective\n\n**4. Weaviate (Open Source/Cloud)**\n\n**Template:** `templates/vector-db-schemas/weaviate-schema.ts`\n\n**Best for:** Advanced filtering, hybrid search\n\n## Retrieval Patterns\n\n### 1. Simple Semantic Search\n\n**Template:** `templates/retrieval-patterns.ts#simpleSearch`\n\n```typescript\nasync function semanticSearch(query: string, topK: number = 5) {\n  // Embed query\n  const { embedding } = await embed({\n    model: openai.embedding('text-embedding-3-small')\n    value: query\n  })\n\n  // Search vector DB\n  const results = await vectorDB.query({\n    vector: embedding\n    topK\n  })\n\n  return results\n}\n```\n\n### 2. Hybrid Search (Vector + Keyword)\n\n**Template:** `templates/retrieval-patterns.ts#hybridSearch`\n\n```typescript\nasync function hybridSearch(query: string, topK: number = 10) {\n  // Vector search\n  const vectorResults = await semanticSearch(query, topK)\n\n  // Keyword search (BM25 or full-text)\n  const keywordResults = await fullTextSearch(query, topK)\n\n  // Combine and re-rank\n  const combined = rerank(vectorResults, keywordResults)\n\n  return combined.slice(0, topK)\n}\n```\n\n**Best practice:** Use hybrid search for better recall\n\n### 3. Re-Ranking\n\n**Template:** `templates/retrieval-patterns.ts#reranking`\n\n```typescript\nasync function rerankResults(query: string, results: any[]) {\n  // Use cross-encoder or LLM for re-ranking\n  const reranked = await generateObject({\n    model: openai('gpt-4o')\n    schema: z.object({\n      rankedIds: z.array(z.string())\n    })\n    messages: [\n      {\n        role: 'system'\n        content: 'Rank these documents by relevance to the query.'\n      }\n      {\n        role: 'user'\n        content: `Query: ${query}\\n\\nDocuments: ${JSON.stringify(results)}`\n      }\n    ]\n  })\n\n  return reranked.object.rankedIds.map(id =>\n    results.find(r => r.id === id)\n  )\n}\n```\n\n## Implementation Workflow\n\n### Step 1: Validate RAG Setup\n\n```bash\n# Check dependencies and configuration\n./scripts/validate-rag-setup.sh\n```\n\n**Checks:**\n- AI SDK installation\n- Vector database client installed\n- Environment variables configured\n- Embedding model accessible\n\n### Step 2: Choose Chunking Strategy\n\n**Decision tree:**\n- Uniform documents ‚Üí Fixed-size chunking\n- Natural sections ‚Üí Semantic chunking\n- Hierarchical structure ‚Üí Recursive chunking\n- Mixed content ‚Üí Hybrid approach\n\n### Step 3: Select Vector Database\n\n**Considerations:**\n- **Pinecone**: Best for production, fully managed, higher cost\n- **Chroma**: Best for prototypes, local development, free\n- **pgvector**: Best if using Postgres, cost-effective\n- **Weaviate**: Best for complex filtering, hybrid search\n\n### Step 4: Implement Embedding Generation\n\n```bash\n# Batch generate embeddings\n./scripts/generate-embeddings.sh ./documents/ openai\n```\n\n**Optimization:**\n- Use `embedMany` for batch processing\n- Implement rate limiting for API quotas\n- Cache embeddings to avoid re-generation\n- Use cheaper models for prototyping\n\n### Step 5: Build Retrieval Pipeline\n\n**Use template:** `templates/retrieval-patterns.ts`\n\n**Customize:**\n1. Set topK (typically 3-10 chunks)\n2. Add metadata filtering if needed\n3. Implement re-ranking for better results\n4. Add hybrid search for improved recall\n\n### Step 6: Integrate with Generation\n\n**Pattern:**\n```typescript\nconst context = retrievedChunks.map(chunk => chunk.text).join('\\n\\n')\n\nconst response = await generateText({\n  model: openai('gpt-4o')\n  messages: [\n    {\n      role: 'system'\n      content: `Answer based on this context. If the answer is not in the context, say so.\\n\\nContext:\\n${context}`\n    }\n    { role: 'user', content: query }\n  ]\n})\n```\n\n## Optimization Strategies\n\n### 1. Chunk Size Optimization\n\n**Guideline:**\n- Small chunks (200-500 tokens): Better precision, more API calls\n- Medium chunks (500-1000 tokens): Balanced\n- Large chunks (1000-2000 tokens): Better context, less precision\n\n**Test with your data:** Use `scripts/chunk-documents.sh` with different sizes\n\n### 2. Embedding Model Selection\n\n**OpenAI text-embedding-3-small:**\n- Dimensions: 1536\n- Cost: $0.02 per 1M tokens\n- Best for: Most use cases\n\n**OpenAI text-embedding-3-large:**\n- Dimensions: 3072\n- Cost: $0.13 per 1M tokens\n- Best for: Higher accuracy needs\n\n**Cohere embed-english-v3.0:**\n- Dimensions: 1024 (configurable)\n- Cost: $0.10 per 1M tokens\n- Best for: Semantic search, compression support\n\n### 3. Query Optimization\n\n**Multi-query retrieval:**\n```typescript\n// Generate multiple query variations\nconst variations = await generateText({\n  model: openai('gpt-4o')\n  messages: [{\n    role: 'user'\n    content: `Generate 3 variations of this query: \"${query}\"`\n  }]\n})\n\n// Search with all variations and combine results\nconst allResults = await Promise.all(\n  variations.map(v => semanticSearch(v))\n)\n\nconst combined = deduplicateAndRank(allResults.flat())\n```\n\n## Production Best Practices\n\n### 1. Error Handling\n\n```typescript\ntry {\n  const results = await ragPipeline(query)\n  return results\n} catch (error) {\n  if (error.code === 'RATE_LIMIT') {\n    // Implement exponential backoff\n  } else if (error.code === 'VECTOR_DB_ERROR') {\n    // Fallback to keyword search\n  }\n  throw error\n}\n```\n\n### 2. Caching\n\n```typescript\n// Cache embeddings\nconst cache = new Map<string, number[]>()\n\nasync function getEmbedding(text: string) {\n  if (cache.has(text)) {\n    return cache.get(text)!\n  }\n\n  const { embedding } = await embed({ model, value: text })\n  cache.set(text, embedding)\n  return embedding\n}\n```\n\n### 3. Monitoring\n\n```typescript\n// Track RAG metrics\nmetrics.record({\n  operation: 'rag_query'\n  latency: Date.now() - startTime\n  chunksRetrieved: results.length\n  vectorDBCalls: 1\n  embeddingCost: calculateCost(query.length)\n})\n```\n\n## Common RAG Patterns\n\n### 1. Conversational RAG\n\n**Example:** `examples/conversational-rag.ts`\n\nMaintains conversation context while retrieving relevant information\n\n### 2. Multi-Document RAG\n\n**Example:** `examples/multi-document-rag.ts`\n\nRetrieves from multiple knowledge bases\n\n### 3. Agentic RAG\n\n**Example:** `examples/agentic-rag.ts`\n\nUses tools to decide when and what to retrieve\n\n## Resources\n\n**Scripts:**\n- `chunk-documents.sh` - Chunk documents with different strategies\n- `generate-embeddings.sh` - Batch embedding generation\n- `validate-rag-setup.sh` - Validate configuration\n\n**Templates:**\n- `rag-pipeline.ts` - Complete RAG implementation\n- `chunking-strategies.ts` - All chunking approaches\n- `retrieval-patterns.ts` - Search and re-ranking patterns\n- `vector-db-schemas/` - Database-specific schemas\n\n**Examples:**\n- `conversational-rag.ts` - Chat with memory\n- `multi-document-rag.ts` - Multiple sources\n- `agentic-rag.ts` - Tool-based retrieval\n\n---\n\n**Supported Vector DBs:** Pinecone, Chroma, pgvector, Weaviate, Qdrant\n**SDK Version:** Vercel AI SDK 5+\n**Embedding Models:** OpenAI, Cohere, Custom\n\n**Best Practice:** Start with simple semantic search, add complexity as needed"
              },
              {
                "name": "testing-patterns",
                "description": "Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.",
                "path": "plugins/vercel-ai-sdk/skills/testing-patterns/SKILL.md",
                "frontmatter": {
                  "name": "testing-patterns",
                  "description": "Testing patterns for Vercel AI SDK including mock providers, streaming tests, tool calling tests, snapshot testing, and test coverage strategies. Use when implementing tests, creating test suites, mocking AI providers, or when user mentions testing, mocks, test coverage, AI testing, streaming tests, or tool testing.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Testing Patterns for Vercel AI SDK\n\n**Purpose:** Provide comprehensive testing templates, mock providers, and testing strategies for Vercel AI SDK applications.\n\n**Activation Triggers:**\n- Writing tests for AI features\n- Creating mock AI providers\n- Testing streaming responses\n- Testing tool calling functionality\n- Implementing test coverage\n- Snapshot testing for AI outputs\n\n**Key Resources:**\n- `templates/mock-provider.ts` - Mock language model implementation\n- `templates/streaming-test.ts` - Test streaming responses\n- `templates/tool-calling-test.ts` - Test tool execution\n- `templates/snapshot-test.ts` - Snapshot testing for outputs\n- `scripts/generate-test-suite.sh` - Generate test scaffolds\n- `scripts/run-coverage.sh` - Run tests with coverage\n- `examples/` - Complete test suites for different features\n\n## Mock Provider Pattern\n\n**Template:** `templates/mock-provider.ts`\n\n```typescript\nimport { createMockLanguageModelV1 } from 'ai/test'\n\nconst mockProvider = createMockLanguageModelV1({\n  doGenerate: async ({ prompt, mode }) => ({\n    text: 'Mocked response'\n    finishReason: 'stop'\n    usage: { promptTokens: 10, completionTokens: 20 }\n  })\n  doStream: async function* ({ prompt, mode }) {\n    yield { type: 'text-delta', textDelta: 'Mocked ' }\n    yield { type: 'text-delta', textDelta: 'streaming ' }\n    yield { type: 'text-delta', textDelta: 'response' }\n    yield {\n      type: 'finish'\n      finishReason: 'stop'\n      usage: { promptTokens: 10, completionTokens: 20 }\n    }\n  }\n})\n```\n\n## Testing Strategies\n\n### 1. Unit Tests (Mock Providers)\n\nTest AI functions without real API calls\n\n### 2. Integration Tests (Real Providers)\n\nTest with real providers in CI (with rate limits)\n\n### 3. Snapshot Tests\n\nEnsure consistent outputs over time\n\n### 4. E2E Tests\n\nTest complete user flows with mocks\n\n## Test Coverage Goals\n\n- Core Functions: >90%\n- Error Handling: >80%\n- Tool Calling: 100%\n- Streaming: >85%\n\n## Resources\n\n**Templates:**\n- `mock-provider.ts` - Complete mock implementation\n- `streaming-test.ts` - Streaming test patterns\n- `tool-calling-test.ts` - Tool execution tests\n- `snapshot-test.ts` - Snapshot testing setup\n\n**Scripts:**\n- `generate-test-suite.sh` - Scaffold tests\n- `run-coverage.sh` - Run with coverage\n\n**Examples:**\n- `complete-test-suite.test.ts` - Full test suite example\n\n---\n\n**Testing Frameworks:** Vitest, Jest, Node Test Runner\n**SDK Version:** Vercel AI SDK 5+\n**Coverage Tool:** c8, nyc, or built-in coverage\n\n**Best Practice:** Use mock providers for fast, reliable tests"
              }
            ]
          },
          {
            "name": "website-builder",
            "description": "AI-powered website builder with Astro, MDX, content-image-generation MCP, Supabase CMS, and component integration for marketing sites, blogs, and documentation",
            "source": "./plugins/website-builder",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "dev@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install website-builder@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-blog",
                "description": "Add blog functionality to Astro website with content collections, MDX posts, and RSS feed",
                "path": "plugins/website-builder/commands/add-blog.md",
                "frontmatter": {
                  "description": "Add blog functionality to Astro website with content collections, MDX posts, and RSS feed",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add complete blog functionality to Astro website including content collections, blog post templates, list pages, RSS feed, and pagination\n\nCore Principles:\n- Use Astro content collections for blog posts\n- Support MDX for rich blog content\n- Generate RSS feed automatically\n- Implement pagination for post lists\n\nPhase 1: Discovery & Requirements\nGoal: Understand blog configuration\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Use AskUserQuestion to gather:\n  - Blog URL path? (e.g., /blog, /articles)\n  - Posts per page for pagination? (default: 10)\n  - Include RSS feed? (recommended: yes)\n  - Include tags/categories?\n  - Generate sample blog posts with AI?\n- Load Astro content collections documentation via Context7\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check existing project structure\n\nActions:\n- Check for src/content directory\n- Check astro.config.mjs for content collections\n- Identify existing blog setup (if any)\n- Plan blog directory structure\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Add blog functionality\n\nActions:\n\nLaunch the website-content agent to add blog functionality.\n\nProvide the agent with:\n- Blog configuration from Phase 1\n- Project structure from Phase 2\n- Content collections schema\n- Expected output: Complete blog with posts, list pages, RSS, pagination\n\nPhase 4: Validation\nGoal: Verify blog was added correctly\n\nActions:\n- Check content collections configured\n- Verify blog post schema defined\n- Check blog pages created (list, individual post, pagination)\n- Validate RSS feed generation\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document what was created\n\nActions:\n- Mark all todos complete\n- Display blog structure\n- Show how to add new blog posts\n- Provide example blog post frontmatter\n- Show next steps (write posts, generate content with AI)"
              },
              {
                "name": "/add-component",
                "description": "Add React component from shadcn or Tailwind UI to Astro website with proper integration",
                "path": "plugins/website-builder/commands/add-component.md",
                "frontmatter": {
                  "description": "Add React component from shadcn or Tailwind UI to Astro website with proper integration",
                  "argument-hint": "component-name"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add React component (shadcn or Tailwind UI) to Astro website with proper React integration and styling\n\nCore Principles:\n- Use Astro's React integration (@astrojs/react)\n- Support shadcn/ui components\n- Support Tailwind UI components\n- Maintain Tailwind CSS configuration\n\nPhase 1: Discovery & Requirements\nGoal: Determine which component to add\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Parse $ARGUMENTS for component name\n- If details missing, use AskUserQuestion to gather:\n  - Component source? (shadcn/ui, Tailwind UI, custom)\n  - Component name? (e.g., button, card, hero)\n  - Where to use? (page path or layout)\n  - Client-side interactive? (Astro island with client:load)\n- Use mcp__shadcn__search_items_in_registries to find component if shadcn\n- Load Astro React integration docs via Context7\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check React integration setup\n\nActions:\n- Check if @astrojs/react installed\n- Check if Tailwind CSS configured\n- Check for src/components directory\n- Identify component dependencies\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Add the React component\n\nActions:\n\nLaunch the website-content agent to add the component.\n\nProvide the agent with:\n- Component name and source from Phase 1\n- Integration requirements\n- Component code (from shadcn MCP if applicable)\n- Expected output: Component added to src/components with proper integration\n\nPhase 4: Validation\nGoal: Verify component was added correctly\n\nActions:\n- Check component file created\n- Verify React integration working\n- Check Tailwind classes applied\n- Validate component can be imported\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document what was created\n\nActions:\n- Mark all todos complete\n- Display component location\n- Show import example\n- Provide usage examples (static vs interactive island)\n- Show next steps (use in pages, customize styling)"
              },
              {
                "name": "/add-page",
                "description": "Add new static page to Astro website with MDX support and optional AI content generation",
                "path": "plugins/website-builder/commands/add-page.md",
                "frontmatter": {
                  "description": "Add new static page to Astro website with MDX support and optional AI content generation",
                  "argument-hint": "page-name"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add a new static page to existing Astro website with MDX support, layout integration, and optional AI-generated content\n\nCore Principles:\n- Follow Astro file-based routing conventions\n- Use MDX for content with frontmatter\n- Support AI content generation via MCP\n- Integrate with existing layouts and components\n\nPhase 1: Discovery & Requirements\nGoal: Understand the page to create\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Parse $ARGUMENTS for page name/path\n- If details missing, use AskUserQuestion to gather:\n  - What's the page name? (e.g., about, pricing, contact)\n  - Page type? (static content, form, hero+features)\n  - Generate content with AI? (use content-image-generation MCP)\n  - Use existing layout or create new?\n- Load Astro routing documentation via Context7\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Analyze existing Astro project\n\nActions:\n- Check for src/pages directory\n- Identify existing layouts in src/layouts\n- Check content collections configuration\n- Determine routing path for new page\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Create the new page\n\nActions:\n\nLaunch the website-content agent to create the page.\n\nProvide the agent with:\n- Page name and path from Phase 1\n- Layout to use\n- Content structure\n- AI generation preferences\n- Expected output: New MDX page with frontmatter, layout, and content\n\nPhase 4: Validation\nGoal: Verify page was created correctly\n\nActions:\n- Check page file created in correct location\n- Verify frontmatter is valid\n- Check layout reference is correct\n- Validate MDX syntax\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document what was created\n\nActions:\n- Mark all todos complete\n- Display page path and URL\n- Show page preview command\n- Provide next steps (add content, generate images, optimize SEO)"
              },
              {
                "name": "/deploy-marketing-site",
                "description": "Deploy complete Astro website with all integrations automatically - analyzes project, orchestrates all setup commands",
                "path": "plugins/website-builder/commands/deploy-marketing-site.md",
                "frontmatter": {
                  "description": "Deploy complete Astro website with all integrations automatically - analyzes project, orchestrates all setup commands",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\n# Marketing Site Deployment Orchestrator\n\nGoal: Analyze project context, then systematically deploy complete Astro marketing website with all integrations, running commands in optimal order without manual intervention.\n\nNOTE: This is for STATIC MARKETING SITES (blogs, landing pages, documentation). For full-stack AI APPLICATIONS, use /ai-tech-stack-1:build-full-stack instead.\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Core Principles\n\n1. **One Command Deployment**: User runs `/website-builder:deploy-marketing-site` and everything happens automatically\n2. **Context-Aware**: Detect existing project vs new project\n3. **Parallel When Possible**: Run independent steps concurrently\n4. **Sequential When Required**: Respect dependencies (init before integrate)\n5. **Comprehensive**: Setup, content, CMS, AI, SEO, deployment - everything\n\n## Deployment Flow\n\n### Phase 1: Discovery & Analysis (5 minutes)\n\n**Understand what we're deploying to:**\n\nCreate comprehensive todo list with TodoWrite:\n- Analyze project context\n- Determine deployment strategy\n- Initialize project (if needed)\n- Integrate Supabase CMS (if needed)\n- Setup AI content generation (if needed)\n- Create initial content structure\n- Optimize for SEO\n- Configure deployment\n- Validate everything\n\n**Detect project state:**\n- Read package.json (if exists)\n- Read astro.config.mjs (if exists)\n- Check if Astro project exists\n- Check if Supabase is configured\n- Check if content-image-generation MCP is available\n- Identify what's already setup vs what needs setup\n\n**Determine requirements through intelligent questioning:**\n\nIf project doesn't exist, ask:\n- Use AskUserQuestion:\n  - Question: \"What type of Astro website do you want to build?\"\n    - Header: \"Site Type\"\n    - Options:\n      - \"Marketing Site\" - Static pages optimized for conversion\n      - \"Blog\" - Content-focused with posts and RSS\n      - \"Documentation\" - Technical docs with search\n      - \"Landing Page\" - Single high-converting page\n  - Question: \"What integrations do you need?\"\n    - Header: \"Integrations\"\n    - MultiSelect: true\n    - Options:\n      - \"Supabase CMS\" - Database-backed content management\n      - \"AI Content\" - Generate content with Claude/Gemini\n      - \"AI Images\" - Generate images with Imagen\n      - \"React Components\" - Interactive UI components\n  - Question: \"What's your deployment target?\"\n    - Header: \"Deployment\"\n    - Options:\n      - \"Vercel\" - Deploy to Vercel\n      - \"Netlify\" - Deploy to Netlify\n      - \"Cloudflare\" - Deploy to Cloudflare Pages\n      - \"Not yet\" - Just setup locally for now\n\n**Parse project name from $ARGUMENTS** or use default \"my-astro-site\"\n\n**Document deployment plan:**\n- Write deployment-plan.md with:\n  - Project type and requirements\n  - What will be installed\n  - What commands will run\n  - Estimated timeline\n  - What user needs to provide (API keys, etc.)\n\n### Phase 2: Project Initialization (10 minutes)\n\n**If project doesn't exist, initialize it:**\n\nSlashCommand: `/website-builder:init $PROJECT_NAME`\n\nThis runs the init command which:\n- Executes init-project.sh script\n- Creates Astro project with TypeScript\n- Installs integrations (React, MDX, Tailwind, Sitemap)\n- Installs dependencies (Supabase, Zod, date-fns)\n- Creates directory structure\n- Sets up environment files\n\n**Wait for init to complete** before proceeding.\n\nMark todo as completed, move to next phase.\n\n### Phase 3: Parallel Integrations (15 minutes)\n\n**Run integrations in parallel when possible:**\n\nThese can run simultaneously as they're independent:\n\n1. **Supabase CMS Integration** (if requested)\n   - SlashCommand: `/website-builder:integrate-supabase-cms`\n   - Sets up database schema\n   - Configures RLS policies\n   - Creates CMS tables\n\n2. **Content Generation Setup** (if requested)\n   - SlashCommand: `/website-builder:integrate-content-generation`\n   - Configures content-image-generation MCP\n   - Tests AI connectivity\n   - Sets up generation workflows\n\n**Run both commands in parallel:**\n```\nUse Task tool with multiple commands in single message to run concurrently\n```\n\n**Wait for both to complete** before proceeding.\n\nMark todos as completed, move to next phase.\n\n### Phase 4: Content Structure Creation (20 minutes)\n\n**Create initial content based on site type:**\n\nBased on site type selected in Phase 1:\n\n**If Blog:**\n- SlashCommand: `/website-builder:add-blog`\n  - Creates blog content collection\n  - Adds RSS feed\n  - Creates blog listing page\n  - Generates sample posts\n\n**If Marketing Site or Landing Page:**\n- SlashCommand: `/website-builder:add-page home`\n- SlashCommand: `/website-builder:add-page about`\n- SlashCommand: `/website-builder:add-page contact`\n\n**If Documentation:**\n- SlashCommand: `/website-builder:add-page getting-started`\n- Create docs content collection\n- Setup search functionality\n\n**Run page creation commands sequentially** (they modify same files).\n\nMark todos as completed, move to next phase.\n\n### Phase 5: AI Content Generation (10 minutes, optional)\n\n**If AI content was requested, generate initial content:**\n\nSlashCommand: `/website-builder:generate-content`\n- Generates hero copy\n- Creates initial blog posts (if blog)\n- Writes page content\n- Optimizes for SEO\n\nSlashCommand: `/website-builder:generate-images`\n- Generates hero images\n- Creates blog headers (if blog)\n- Generates OG images\n\n**Run both commands in parallel** (independent operations).\n\nMark todos as completed, move to next phase.\n\n### Phase 6: SEO & Performance Optimization (5 minutes)\n\n**Optimize for search engines and performance:**\n\nSlashCommand: `/website-builder:optimize-seo`\n- Generates sitemap\n- Optimizes meta tags\n- Creates robots.txt\n- Adds structured data (JSON-LD)\n- Optimizes images\n- Configures prefetching\n\nMark todo as completed, move to next phase.\n\n### Phase 7: Deployment Configuration (10 minutes)\n\n**Setup deployment target:**\n\nSlashCommand: `/website-builder:deploy`\n\nThis will:\n- Configure deployment for selected platform (Vercel/Netlify/Cloudflare)\n- Create deployment configuration files\n- Test build process\n- Provide deployment instructions\n\nMark todo as completed, move to next phase.\n\n### Phase 8: Final Validation (5 minutes)\n\n**Verify everything is working:**\n\nRun comprehensive checks:\n1. Bash: `npm run check` (Astro validation)\n2. Bash: `npm run build` (build test)\n3. Check all required files exist\n4. Verify environment variables documented\n5. Test dev server starts\n\n**If any validation fails:**\n- Document issues in validation-report.md\n- Provide fix instructions\n- DO NOT mark as complete\n\n### Phase 9: Completion Summary\n\n**Mark all todos as completed.**\n\n**Generate deployment summary:**\n\nCreate DEPLOYMENT-SUMMARY.md with:\n```markdown\n# Astro Website Deployment Complete\n\n## Project Details\n- Name: $PROJECT_NAME\n- Type: [blog/marketing/docs/landing]\n- Integrations: [list all installed]\n\n## What Was Created\n- ‚úÖ Astro project with TypeScript\n- ‚úÖ React + MDX + Tailwind integrations\n- ‚úÖ [Supabase CMS if enabled]\n- ‚úÖ [AI content generation if enabled]\n- ‚úÖ Content structure ([blog/pages])\n- ‚úÖ SEO optimization\n- ‚úÖ Deployment configuration\n\n## Environment Variables Needed\nReview .env.example and configure:\n- PUBLIC_SUPABASE_URL\n- PUBLIC_SUPABASE_ANON_KEY\n- GOOGLE_API_KEY (for AI content)\n- ANTHROPIC_API_KEY (for AI content)\n\n## Next Steps\n1. Update .env with your API keys\n2. Run `npm run dev` to start development\n3. Run `npm run build` to build for production\n4. Deploy using: [deployment instructions]\n\n## Project Structure\n[Show directory tree]\n\n## Commands Available\n- npm run dev - Start dev server\n- npm run build - Build for production\n- npm run preview - Preview production build\n- /website-builder:add-page - Add new page\n- /website-builder:generate-content - Generate AI content\n- /website-builder:generate-images - Generate AI images\n\n## Support\n- Documentation: /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/website-builder/\n- Astro docs: https://docs.astro.build/\n```\n\n**Display summary to user.**\n\n## Error Handling\n\n**If any phase fails:**\n1. Mark current todo as failed with error details\n2. Stop execution (don't proceed to next phase)\n3. Provide troubleshooting guidance\n4. Offer to retry the failed phase\n5. Create error-report.md with full details\n\n**Common failure scenarios:**\n- Node.js version too old ‚Üí Guide to upgrade\n- npm install fails ‚Üí Check network, clear cache\n- MCP server not configured ‚Üí Provide MCP setup guide\n- API keys missing ‚Üí Remind to configure .env\n- Build fails ‚Üí Run troubleshooting agent\n\n## Timing Estimates\n\n**Total deployment time: ~80 minutes**\n- Phase 1 (Discovery): 5 min\n- Phase 2 (Init): 10 min\n- Phase 3 (Integrations): 15 min\n- Phase 4 (Content): 20 min\n- Phase 5 (AI Generation): 10 min (optional)\n- Phase 6 (SEO): 5 min\n- Phase 7 (Deployment): 10 min\n- Phase 8 (Validation): 5 min\n\n**User can walk away** - everything runs automatically!\n\n## Parallel Execution Strategy\n\n**Commands that CAN run in parallel:**\n- Supabase integration + Content generation setup\n- Content generation + Image generation\n- Multiple page creation (different pages)\n\n**Commands that MUST run sequentially:**\n- Init BEFORE any integrations\n- Integrations BEFORE content creation\n- Content creation BEFORE AI generation\n- Everything BEFORE validation\n\n**Use Task tool for parallelization:**\n```\nWhen running parallel commands, use single message with multiple Task invocations\n```\n\n## Output Standards\n\n- All phases logged to deployment.log\n- Progress updates via TodoWrite\n- Clear error messages if anything fails\n- Comprehensive summary at the end\n- User can resume if interrupted\n\n## Self-Verification Checklist\n\nBefore marking deployment as complete:\n- ‚úÖ Project initialized successfully\n- ‚úÖ All requested integrations configured\n- ‚úÖ Content structure created\n- ‚úÖ AI content generated (if requested)\n- ‚úÖ SEO optimized\n- ‚úÖ Deployment configured\n- ‚úÖ Build passes validation\n- ‚úÖ Dev server starts\n- ‚úÖ .env.example documented\n- ‚úÖ DEPLOYMENT-SUMMARY.md created\n\n## Usage Examples\n\n**Simple blog deployment:**\n```\n/website-builder:deploy-full-stack my-blog\n```\n\n**Marketing site with AI content:**\n```\n/website-builder:deploy-full-stack company-website\n# Answer questions:\n# - Type: Marketing Site\n# - Integrations: AI Content, AI Images\n# - Deployment: Vercel\n```\n\n**Existing project enhancement:**\n```\ncd existing-astro-project\n/website-builder:deploy-full-stack\n# Detects existing project, offers to add integrations\n```\n\nThis command provides the **one-click deployment** you envisioned - analyze project, run all commands in optimal order, complete automation!"
              },
              {
                "name": "/deploy",
                "description": "Deploy Astro website to Vercel with optimized build configuration and environment variables",
                "path": "plugins/website-builder/commands/deploy.md",
                "frontmatter": {
                  "description": "Deploy Astro website to Vercel with optimized build configuration and environment variables",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Deploy Astro website to Vercel with optimized build settings, environment variables, and production configuration\n\nCore Principles:\n- Use Vercel CLI for deployment\n- Optimize build configuration for Astro\n- Configure environment variables securely\n- Validate before deploying\n\nPhase 1: Discovery & Requirements\nGoal: Understand deployment configuration\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Use AskUserQuestion to gather:\n  - Deploy to production or preview?\n  - Vercel project already exists?\n  - Environment variables needed?\n  - Custom domain configured?\n  - Need to configure build settings?\n- Load Vercel deployment docs via Context7\n- Summarize requirements\n\nPhase 2: Pre-Deployment Validation\nGoal: Verify project is ready for deployment\n\nActions:\n\nLaunch the website-verifier agent to validate deployment readiness.\n\nProvide the agent with:\n- Project structure and configuration\n- Validation checks:\n  - Build succeeds locally\n  - No build errors or warnings\n  - Environment variables documented\n  - SEO meta tags configured\n  - Sitemap generated\n  - All pages render correctly\n- Expected output: Deployment readiness report with any blockers\n\nPhase 3: Deployment Configuration\nGoal: Configure Vercel deployment\n\nActions:\n\nLaunch the website-architect agent to configure deployment.\n\nProvide the agent with:\n- Deployment requirements from Phase 1\n- Validation results from Phase 2\n- Configuration to add:\n  - vercel.json with build settings\n  - .vercelignore for excluded files\n  - Environment variable documentation\n  - Build output directory configuration\n  - Node.js version specification\n- Expected output: Complete Vercel configuration files\n\nPhase 4: Deploy\nGoal: Execute deployment to Vercel\n\nActions:\n- Check if Vercel CLI installed\n- Install if needed: !{bash npm install -g vercel}\n- Run deployment: !{bash vercel --prod} or !{bash vercel} for preview\n- Capture deployment URL\n- Update TodoWrite\n\nPhase 5: Post-Deployment Validation\nGoal: Verify deployment succeeded\n\nActions:\n- Check deployment status\n- Verify site is accessible\n- Test key pages load correctly\n- Verify environment variables applied\n- Update TodoWrite\n\nPhase 6: Summary\nGoal: Document deployment\n\nActions:\n- Mark all todos complete\n- Display deployment URL\n- Show environment variables configured\n- Provide monitoring recommendations\n- Show next steps (configure custom domain, setup analytics)"
              },
              {
                "name": "/generate-content",
                "description": "Generate AI-powered content for pages, blogs, and marketing copy using content-image-generation MCP",
                "path": "plugins/website-builder/commands/generate-content.md",
                "frontmatter": {
                  "description": "Generate AI-powered content for pages, blogs, and marketing copy using content-image-generation MCP",
                  "argument-hint": "content-type"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate AI-powered marketing content, blog posts, and page copy using Claude Sonnet 4 or Gemini 2.0 via content-image-generation MCP\n\nCore Principles:\n- Use content-image-generation MCP for AI content generation\n- Support multiple content types (hero copy, blog posts, product descriptions)\n- Allow customization of tone, style, and length\n- Save generated content to project\n\nPhase 1: Discovery & Requirements\nGoal: Understand what content to generate\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Parse $ARGUMENTS for content type (hero, blog, product, custom)\n- Use AskUserQuestion to gather:\n  - What content type? (hero section, blog post, product description, landing page)\n  - What's the topic/subject?\n  - Tone and style? (professional, casual, technical, marketing)\n  - Target audience?\n  - Length preference? (short, medium, long)\n  - Which AI model? (Claude Sonnet 4, Gemini 2.0 Pro)\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check project structure\n\nActions:\n- Check if content-image-generation MCP configured\n- Identify where to save generated content\n- Check existing content patterns\n- Update TodoWrite\n\nPhase 3: Content Generation\nGoal: Generate AI content\n\nActions:\n\nLaunch the website-ai-generator agent to generate content.\n\nProvide the agent with:\n- Content requirements from Phase 1\n- Project structure from Phase 2\n- MCP tool to use: generate_marketing_content\n- Content parameters:\n  - Topic and subject matter\n  - Tone and style preferences\n  - Target audience\n  - Desired length\n  - AI model selection\n- Expected output: Generated content saved to appropriate location\n\nPhase 4: Content Review\nGoal: Review and refine generated content\n\nActions:\n\nLaunch the website-content agent to review and integrate content.\n\nProvide the agent with:\n- Generated content from Phase 3\n- Integration requirements\n- Format requirements (MDX, HTML, plain text)\n- Expected output: Polished content ready for use\n\nPhase 5: Summary\nGoal: Document generated content\n\nActions:\n- Mark all todos complete\n- Display generated content preview\n- Show file location\n- Provide cost estimate for generation\n- Show next steps (edit content, generate images, publish)"
              },
              {
                "name": "/generate-images",
                "description": "Generate AI-powered images for hero sections, blog posts, and marketing using Google Imagen",
                "path": "plugins/website-builder/commands/generate-images.md",
                "frontmatter": {
                  "description": "Generate AI-powered images for hero sections, blog posts, and marketing using Google Imagen",
                  "argument-hint": "image-type"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate AI-powered images using Google Imagen 3/4 for hero sections, blog posts, and marketing materials\n\nCore Principles:\n- Use content-image-generation MCP for image generation\n- Support multiple image types (hero, blog header, product, icon)\n- Allow aspect ratio and quality customization\n- Save images to project assets\n\nPhase 1: Discovery & Requirements\nGoal: Understand what images to generate\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Parse $ARGUMENTS for image type (hero, blog, product, icon)\n- Use AskUserQuestion to gather:\n  - What image type? (hero section, blog header, product image, icon, custom)\n  - Image description/prompt?\n  - Aspect ratio? (1:1, 16:9, 4:3, 9:16)\n  - Quality? (SD or HD)\n  - How many images? (single or batch)\n  - Use Imagen 3 or Imagen 4?\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check project structure\n\nActions:\n- Check if content-image-generation MCP configured\n- Identify assets directory for saving images\n- Check existing image patterns\n- Update TodoWrite\n\nPhase 3: Image Generation\nGoal: Generate AI images\n\nActions:\n\nLaunch the website-ai-generator agent to generate images.\n\nProvide the agent with:\n- Image requirements from Phase 1\n- Project structure from Phase 2\n- MCP tools to use:\n  - generate_image_imagen3 (for single image)\n  - batch_generate_images (for multiple images)\n- Image parameters:\n  - Prompt (enhanced with image_prompt_enhancer if needed)\n  - Negative prompt (optional)\n  - Aspect ratio selection\n  - Quality (SD or HD)\n  - Output format (PNG, JPEG, WebP)\n  - Seed (for reproducibility)\n- Expected output: Images saved to assets directory\n\nPhase 4: Validation\nGoal: Verify images generated successfully\n\nActions:\n- Check images saved to correct location\n- Verify image quality and dimensions\n- Display image paths and preview\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document generated images\n\nActions:\n- Mark all todos complete\n- Display image locations\n- Show cost breakdown for generation\n- Provide usage examples (how to reference in pages)\n- Show next steps (optimize images, add to content)"
              },
              {
                "name": "/init",
                "description": "Initialize new Astro website project with AI content generation, MDX, and optional integrations",
                "path": "plugins/website-builder/commands/init.md",
                "frontmatter": {
                  "description": "Initialize new Astro website project with AI content generation, MDX, and optional integrations",
                  "argument-hint": "project-name"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create a production-ready Astro website with AI content generation, MDX support, component integration, and optional Supabase CMS\n\nCore Principles:\n- Use Astro best practices from official documentation\n- Integrate content-image-generation MCP for AI capabilities\n- Support React components (shadcn + Tailwind UI)\n- Create complete, runnable project\n\nPhase 1: Discovery & Requirements\nGoal: Understand what website to build\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Parse $ARGUMENTS for project name\n- If no project name provided, use AskUserQuestion to gather:\n  - What's the project name?\n  - Website type? (marketing site, blog, documentation, landing page)\n  - Include AI content generation? (content-image-generation MCP)\n  - Include Supabase CMS backend?\n  - Include React components? (shadcn + Tailwind UI)\n- Load Astro documentation using Context7:\n  - Use mcp__context7__resolve-library-id to find Astro\n  - Use mcp__context7__get-library-docs for Astro project structure\n- Summarize requirements\n\nPhase 2: Project Structure Planning\nGoal: Design the Astro project structure\n\nActions:\n- Determine directory structure based on website type\n- Plan content collections for MDX\n- Plan component integration strategy\n- Identify dependencies (Astro, React, Tailwind, etc.)\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Create the Astro website project\n\nActions:\n\nStep 1: Run automated initialization script\n- Execute the astro-setup skill's init-project.sh script\n- Script handles: prerequisites check, npm create astro, integrations, dependencies, directory structure, env files\n- Command: bash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/website-builder/skills/astro-setup/scripts/init-project.sh <project-name> --template=<website-type>\n- The script will:\n  - Check Node.js 18.14.1+ requirement\n  - Create Astro project with TypeScript strictest mode\n  - Install integrations: React, MDX, Tailwind, Sitemap\n  - Install dependencies: @supabase/supabase-js, zod, date-fns\n  - Create directory structure (src/lib, src/content/blog, public/images)\n  - Generate utility files (utils.ts, supabase.ts)\n  - Create SEO component\n  - Setup .env.example and .env files\n  - Update astro.config.mjs with all integrations\n\nStep 2: Additional customizations based on requirements\nIf user requested specific integrations from Phase 1:\n\nLaunch the website-setup agent for additional customizations:\n- Project name: $ARGUMENTS\n- Website type from Phase 1\n- Requirements from Phase 1\n- Structure plan from Phase 2\n- Astro documentation context\n- Expected output: Customized features beyond base installation\n\nPhase 4: Validation\nGoal: Verify the project was created correctly\n\nActions:\n- Check all required files exist (astro.config.mjs, package.json, tsconfig.json)\n- Verify dependencies installed\n- Check content collections configured\n- Validate MCP configuration if included\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document what was created\n\nActions:\n- Mark all todos complete\n- Display project structure created\n- Show installation and setup instructions\n- Provide next steps (add pages, generate content, deploy)"
              },
              {
                "name": "/integrate-content-generation",
                "description": "Setup content-image-generation MCP server for AI-powered content and image generation",
                "path": "plugins/website-builder/commands/integrate-content-generation.md",
                "frontmatter": {
                  "description": "Setup content-image-generation MCP server for AI-powered content and image generation",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup and configure content-image-generation MCP server for AI-powered content generation (Claude/Gemini) and image generation (Google Imagen 3/4)\n\nCore Principles:\n- Configure content-image-generation MCP server\n- Setup Google Cloud credentials for Imagen\n- Configure Anthropic/Google AI API keys\n- Integrate with Astro project\n\nPhase 1: Discovery & Requirements\nGoal: Understand MCP integration needs\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Use AskUserQuestion to gather:\n  - Have Google Cloud project with Vertex AI enabled?\n  - Have Anthropic API key for content generation?\n  - Have Google AI API key (optional alternative)?\n  - Where to store MCP configuration? (.mcp.json in project root)\n  - Need to generate sample content now?\n- Load content-image-generation MCP documentation\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check existing setup\n\nActions:\n- Check if .mcp.json already exists\n- Check for existing MCP servers configured\n- Verify Astro project structure\n- Check for environment variable files\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Configure MCP server\n\nActions:\n\nLaunch the website-setup agent to configure content-image-generation MCP.\n\nProvide the agent with:\n- MCP server location: mcp-servers/content-image-generation-mcp\n- Configuration requirements from Phase 1\n- Project structure from Phase 2\n- Environment variables needed:\n  - GOOGLE_CLOUD_PROJECT\n  - ANTHROPIC_API_KEY\n  - GOOGLE_AI_API_KEY (optional)\n- Expected output: .mcp.json configuration, .env.example template, integration guide\n\nPhase 4: Validation\nGoal: Verify MCP integration works\n\nActions:\n- Check .mcp.json created and valid\n- Verify MCP server can be loaded\n- Test basic MCP connection\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document integration\n\nActions:\n- Mark all todos complete\n- Display MCP configuration\n- Show available MCP tools: - generate_image_imagen3 (Google Imagen 3/4 image generation)\n  - batch_generate_images (batch image generation)\n  - generate_video_veo3 (Google Veo 2/3 video generation)\n  - generate_marketing_content (Claude/Gemini content generation)\n  - calculate_cost_estimate (campaign cost estimation)\n- Provide usage examples\n- Show next steps (generate content, generate images)"
              },
              {
                "name": "/integrate-supabase-cms",
                "description": "Integrate Supabase as CMS backend for Astro website with content management and draft/publish workflows",
                "path": "plugins/website-builder/commands/integrate-supabase-cms.md",
                "frontmatter": {
                  "description": "Integrate Supabase as CMS backend for Astro website with content management and draft/publish workflows",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Supabase as a headless CMS backend for Astro website with content tables, draft/publish workflows, and real-time updates\n\nCore Principles:\n- Use Supabase PostgreSQL for content storage\n- Implement draft/publish workflow with RLS policies\n- Support content types (pages, posts, media)\n- Enable real-time content updates\n\nPhase 1: Discovery & Requirements\nGoal: Understand CMS integration needs\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Use AskUserQuestion to gather:\n  - What content types? (pages, posts, media, custom)\n  - Need draft/publish workflow? (recommended: yes)\n  - Need multi-author support?\n  - Need content versioning?\n  - Existing Supabase project or create new?\n- Load Supabase CMS patterns via Context7\n- Load Astro content integration docs\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check existing setup\n\nActions:\n- Check if Supabase already configured\n- Check Astro project structure\n- Identify content integration points\n- Plan database schema\n- Update TodoWrite\n\nPhase 3: Implementation\nGoal: Setup Supabase CMS integration\n\nActions:\n\nLaunch the website-architect agent to integrate Supabase CMS.\n\nProvide the agent with:\n- CMS requirements from Phase 1\n- Project structure from Phase 2\n- Database schema design\n- Content types and workflows\n- Expected output: Complete Supabase integration with content tables, RLS policies, and Astro client\n\nPhase 4: Validation\nGoal: Verify CMS integration works\n\nActions:\n- Check database schema created\n- Verify RLS policies applied\n- Check Supabase client configured\n- Test content fetch from Astro\n- Update TodoWrite\n\nPhase 5: Summary\nGoal: Document integration\n\nActions:\n- Mark all todos complete\n- Display database schema\n- Show how to create/update content\n- Provide example queries\n- Show next steps (add content, configure auth)"
              },
              {
                "name": "/optimize-seo",
                "description": "Optimize Astro website for SEO with meta tags, sitemap, robots.txt, and structured data",
                "path": "plugins/website-builder/commands/optimize-seo.md",
                "frontmatter": {
                  "description": "Optimize Astro website for SEO with meta tags, sitemap, robots.txt, and structured data",
                  "argument-hint": "none"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the website-builder plugin:\n\n- **ai-content-generation**: AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.\n- **astro-patterns**: Astro best practices, routing patterns, component architecture, and static site generation techniques. Use when building Astro websites, setting up routing, designing component architecture, configuring static site generation, optimizing build performance, implementing content strategies, or when user mentions Astro patterns, routing, component design, SSG, static sites, or Astro best practices.\n- **astro-setup**: Provides installation, prerequisite checking, and project initialization for Astro websites with AI Tech Stack 1 integration\n- **component-integration**: React, MDX, and Tailwind CSS integration patterns for Astro websites. Use when adding React components, configuring MDX content, setting up Tailwind styling, integrating component libraries, building interactive UI elements, or when user mentions React integration, MDX setup, Tailwind configuration, component patterns, or UI frameworks.\n- **content-collections**: Astro content collections setup, type-safe schemas, query patterns, and frontmatter validation. Use when building Astro sites, setting up content collections, creating collection schemas, querying content, validating frontmatter, or when user mentions Astro collections, content management, MDX content, type-safe content, or collection queries.\n- **supabase-cms**: Supabase CMS integration patterns, schema design, RLS policies, and content management for Astro websites. Use when building CMS systems, setting up Supabase backends, creating content schemas, implementing RLS security, or when user mentions Supabase CMS, headless CMS, content management, database schemas, or Row Level Security.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Optimize Astro website for search engines with comprehensive SEO configuration including meta tags, sitemap, robots.txt, structured data, and Open Graph tags\n\nCore Principles:\n- Use Astro SEO best practices\n- Generate sitemap automatically\n- Add structured data (JSON-LD)\n- Optimize meta tags and social sharing\n\nPhase 1: Discovery & Requirements\nGoal: Understand SEO optimization needs\n\nActions:\n- Create todo list with all phases using TodoWrite\n- Use AskUserQuestion to gather:\n  - Site name and description?\n  - Primary domain URL?\n  - Default social sharing image?\n  - Generate sitemap? (recommended: yes)\n  - Add structured data? (recommended: yes)\n  - Support multiple languages?\n- Load Astro SEO documentation via Context7\n- Summarize requirements\n\nPhase 2: Project Analysis\nGoal: Check existing SEO setup\n\nActions:\n- Check existing meta tags in layouts\n- Check for sitemap configuration\n- Check robots.txt\n- Identify pages needing SEO optimization\n- Update TodoWrite\n\nPhase 3: SEO Implementation\nGoal: Implement SEO optimizations\n\nActions:\n\nLaunch the website-architect agent to implement SEO.\n\nProvide the agent with:\n- SEO requirements from Phase 1\n- Project structure from Phase 2\n- Components to add:\n  - SEO component with meta tags\n  - Open Graph tags\n  - Twitter Card tags\n  - JSON-LD structured data\n  - Sitemap generation (@astrojs/sitemap)\n  - robots.txt configuration\n- Expected output: Complete SEO setup with all components\n\nPhase 4: Validation\nGoal: Verify SEO implementation\n\nActions:\n\nLaunch the website-verifier agent to validate SEO.\n\nProvide the agent with:\n- SEO implementation from Phase 3\n- Validation checklist:\n  - Meta tags present on all pages\n  - Sitemap generated correctly\n  - robots.txt configured\n  - Structured data valid\n  - Social sharing tags complete\n- Expected output: SEO validation report with any issues\n\nPhase 5: Summary\nGoal: Document SEO setup\n\nActions:\n- Mark all todos complete\n- Display SEO configuration\n- Show sitemap URL\n- Provide testing recommendations:\n  - Google Rich Results Test\n  - Facebook Sharing Debugger\n  - Twitter Card Validator\n- Show next steps (submit sitemap, monitor rankings)"
              }
            ],
            "skills": [
              {
                "name": "ai-content-generation",
                "description": "AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.",
                "path": "plugins/website-builder/skills/ai-content-generation/SKILL.md",
                "frontmatter": {
                  "name": "ai-content-generation",
                  "description": "AI-powered content and image generation using content-image-generation MCP with Google Imagen 3/4, Veo 2/3, Claude Sonnet, and Gemini 2.0. Use when generating marketing content, creating hero images, building blog posts, generating product descriptions, creating videos, optimizing AI prompts, estimating generation costs, or when user mentions Imagen, Veo, AI content, AI images, content generation, image generation, video generation, marketing copy, or Google AI.",
                  "allowed-tools": "Read(*), Write(*), Bash(*), Glob(*), Grep(*), mcp__content-image-generation(*)"
                },
                "content": "# AI Content Generation\n\nAI-powered content and image generation skill using the content-image-generation MCP server. This skill provides capabilities for generating marketing content with Claude Sonnet 4 or Gemini 2.0 Pro, creating images with Google Imagen 3/4, and generating videos with Google Veo 2/3.\n\n## Core Capabilities\n\n### Image Generation\n- Generate images with Google Imagen 3 or Imagen 4\n- Support multiple aspect ratios (1:1, 16:9, 4:3, 9:16, custom)\n- Quality control (SD or HD)\n- Batch generation for multiple images\n- Prompt enhancement for better results\n- Custom negative prompts for quality control\n- Seed-based reproducibility\n\n### Content Generation\n- Marketing content with Claude Sonnet 4 or Gemini 2.0 Pro\n- Hero section copy and landing page content\n- Blog posts and articles\n- Product descriptions and feature lists\n- Email campaigns and ad copy\n- SEO-optimized content\n- Tone and style customization\n\n### Video Generation\n- Short-form videos with Google Veo 2 or Veo 3\n- Multiple aspect ratios and durations\n- Prompt-based video creation\n- Quality and format options\n\n### Cost Optimization\n- Pre-generation cost estimation\n- Batch optimization recommendations\n- Quality vs cost tradeoffs\n- Token usage tracking\n\n## Instructions\n\n### Setup and Validation\n\nBefore generating content, verify MCP integration:\n\n```bash\n# Validate MCP server configuration\nbash scripts/validate-mcp-setup.sh\n\n# Setup environment variables\nbash scripts/setup-environment.sh\n```\n\n### Image Generation Workflow\n\n1. **Enhance Prompt**: Use prompt enhancement for better results\n   ```bash\n   bash scripts/enhance-image-prompt.sh \"your basic prompt\"\n   ```\n\n2. **Estimate Cost**: Calculate generation costs\n   ```bash\n   bash scripts/calculate-cost.sh --type image --quality HD --count 5\n   ```\n\n3. **Generate Image**: Use MCP tool with enhanced parameters\n   - Read template: `templates/typescript/image-generation.ts` or `templates/python/image-generation.py`\n   - Call MCP tool: `generate_image_imagen3` or `batch_generate_images`\n   - Save to assets directory\n\n4. **Validate Output**: Check image quality and dimensions\n   ```bash\n   bash scripts/validate-output.sh --type image --path /path/to/image.png\n   ```\n\n### Content Generation Workflow\n\n1. **Define Requirements**: Gather content parameters\n   - Content type (hero, blog, product, email)\n   - Topic and keywords\n   - Tone and style (professional, casual, technical)\n   - Target audience\n   - Desired length\n\n2. **Estimate Cost**: Calculate generation costs\n   ```bash\n   bash scripts/calculate-cost.sh --type content --model claude-sonnet-4 --length 1000\n   ```\n\n3. **Generate Content**: Use MCP tool\n   - Read template: `templates/typescript/content-generation.ts` or `templates/python/content-generation.py`\n   - Call MCP tool: `generate_marketing_content`\n   - Save to content directory\n\n4. **Review and Refine**: Validate content quality\n   ```bash\n   bash scripts/validate-output.sh --type content --path /path/to/content.md\n   ```\n\n### Video Generation Workflow\n\n1. **Prepare Prompt**: Create detailed video description\n   ```bash\n   bash scripts/enhance-video-prompt.sh \"your video description\"\n   ```\n\n2. **Estimate Cost**: Calculate video generation costs\n   ```bash\n   bash scripts/calculate-cost.sh --type video --duration 5 --quality HD\n   ```\n\n3. **Generate Video**: Use MCP tool\n   - Read template: `templates/typescript/video-generation.ts` or `templates/python/video-generation.py`\n   - Call MCP tool: `generate_video_veo3`\n   - Save to assets directory\n\n### Batch Operations\n\nFor multiple assets, use batch generation:\n\n```bash\n# Optimize batch parameters\nbash scripts/optimize-batch.sh --type image --count 10 --budget 50\n\n# Generate batch with optimized settings\n# Use batch_generate_images MCP tool\n```\n\n## MCP Tools Reference\n\n### Image Generation\n- `generate_image_imagen3`: Generate single image with Imagen 3/4\n  - Parameters: prompt, negative_prompt, aspect_ratio, quality, model, seed\n  - Returns: Base64 image data, metadata, cost\n\n- `batch_generate_images`: Generate multiple images efficiently\n  - Parameters: prompts array, shared settings, batch_size\n  - Returns: Array of images with metadata\n\n### Content Generation\n- `generate_marketing_content`: Generate marketing copy\n  - Parameters: topic, content_type, tone, style, length, model, keywords\n  - Returns: Generated content, metadata, cost\n\n### Video Generation\n- `generate_video_veo3`: Generate video with Veo 2/3\n  - Parameters: prompt, duration, aspect_ratio, quality, model\n  - Returns: Video data, metadata, cost\n\n### Utilities\n- `calculate_cost_estimate`: Estimate generation costs\n  - Parameters: operation_type, parameters, quantity\n  - Returns: Cost breakdown, recommendations\n\n- `image_prompt_enhancer`: Enhance image prompts\n  - Parameters: basic_prompt, style, quality_level\n  - Returns: Enhanced prompt, suggestions\n\n## Scripts Reference\n\nAll scripts are located in `skills/ai-content-generation/scripts/`:\n\n- `setup-environment.sh`: Configure environment variables and credentials\n- `validate-mcp-setup.sh`: Verify MCP server connection and tools\n- `enhance-image-prompt.sh`: Improve image generation prompts\n- `calculate-cost.sh`: Estimate generation costs before execution\n- `validate-output.sh`: Check quality of generated assets\n- `optimize-batch.sh`: Optimize batch generation parameters\n- `test-generation.sh`: Run test generation to verify setup\n\n## Templates Reference\n\n### TypeScript Templates (`templates/typescript/`)\n- `image-generation.ts`: Complete image generation implementation\n- `content-generation.ts`: Marketing content generation\n- `video-generation.ts`: Video generation workflow\n\n### Python Templates (`templates/python/`)\n- `image-generation.py`: Complete image generation implementation\n- `content-generation.py`: Marketing content generation\n- `video-generation.py`: Video generation workflow\n\n## Examples\n\nSee `examples/` directory for comprehensive usage examples:\n\n- `basic-usage.md`: Simple image and content generation\n- `advanced-usage.md`: Batch operations, cost optimization, custom parameters\n- `common-patterns.md`: Hero images, blog headers, product galleries\n- `error-handling.md`: Retry logic, fallbacks, validation\n- `integration.md`: Astro integration, asset management, workflow automation\n\n## Best Practices\n\n### Image Generation\n- Always enhance prompts for better quality\n- Use HD quality for hero sections and key visuals\n- Use SD quality for thumbnails and secondary images\n- Specify negative prompts to avoid unwanted elements\n- Use consistent seeds for reproducible results\n- Batch similar images to optimize costs\n\n### Content Generation\n- Provide clear topic and keywords\n- Specify target audience for better relevance\n- Choose appropriate tone and style\n- Review and customize generated content\n- Use Claude for technical/detailed content\n- Use Gemini for creative/marketing content\n\n### Cost Optimization\n- Estimate costs before generation\n- Use batch operations for multiple assets\n- Choose SD quality when HD is not required\n- Optimize prompt length for content generation\n- Cache and reuse similar assets\n- Monitor token usage and costs\n\n### Error Handling\n- Validate MCP setup before operations\n- Check API quotas and limits\n- Implement retry logic for transient failures\n- Validate output quality after generation\n- Log costs and metadata for tracking\n\n## Requirements\n\n### Environment Variables\n- `GOOGLE_CLOUD_PROJECT`: Google Cloud project ID for Vertex AI\n- `ANTHROPIC_API_KEY`: API key for Claude Sonnet content generation\n- `GOOGLE_AI_API_KEY`: (Optional) API key for Gemini content generation\n\n### MCP Configuration\n- content-image-generation MCP server must be configured in `.mcp.json`\n- Google Cloud credentials must be set up for Vertex AI\n- Appropriate APIs must be enabled (Vertex AI, Imagen, Veo)\n\n### Project Structure\n- Assets directory for storing generated images/videos\n- Content directory for storing generated text\n- Environment file for API credentials\n\n---\n\n**Skill Version**: 1.0.0\n**Plugin**: website-builder\n**MCP Server**: content-image-generation"
              }
            ]
          },
          {
            "name": "ml-training",
            "description": "Machine learning training and inference pipeline using cloud GPUs (Modal, Lambda Labs, RunPod) with HuggingFace ecosystem - no local GPU required",
            "source": "./plugins/ml-training",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "builder@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install ml-training@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-dataset",
                "description": "Add training dataset from Supabase/local/HuggingFace",
                "path": "plugins/ml-training/commands/add-dataset.md",
                "frontmatter": {
                  "description": "Add training dataset from Supabase/local/HuggingFace",
                  "argument-hint": "<source-type> [path]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Load and validate training datasets from multiple sources (Supabase, local files, or HuggingFace) for ML training workflows\n\nCore Principles:\n- Detect source type from arguments or ask user\n- Validate data format before accepting\n- Support multiple data sources seamlessly\n- Provide clear feedback on dataset statistics\n\nPhase 1: Discovery\nGoal: Understand dataset source and requirements\n\nActions:\n- Parse $ARGUMENTS to extract source type and path\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - What is the data source? (supabase/local/huggingface)\n  - What is the dataset path or identifier?\n  - What is the expected data format? (csv/json/parquet/custom)\n  - Any preprocessing requirements?\n- Validate that source type is supported\n- Check if project has required dependencies installed\n- Example: !{bash pip list | grep -E \"datasets|supabase|torch\"}\n\nPhase 2: Environment Check\nGoal: Verify environment and dependencies\n\nActions:\n- Check for datasets library installation\n- If source is Supabase, verify mcp__supabase is available\n- Check for existing dataset configuration\n- Look for data directory: !{bash ls -la data/ 2>/dev/null || echo \"No data directory\"}\n- Load any existing dataset configs: @data/config.json (if exists)\n\nPhase 3: Source Validation\nGoal: Verify dataset source accessibility\n\nActions:\n- Based on source type, validate access:\n  - **Supabase**: Test connection and query table structure\n  - **Local**: Verify file/directory exists\n  - **HuggingFace**: Verify dataset identifier is valid\n- Example local check: !{bash test -f \"$path\" && echo \"Found\" || echo \"Not found\"}\n- Present dataset source details to user for confirmation\n\nPhase 4: Dataset Loading\nGoal: Load and validate dataset using specialized agent\n\nActions:\n\nTask(description=\"Load and validate dataset\", subagent_type=\"data-engineer\", prompt=\"You are the data-engineer agent. Load the dataset from $ARGUMENTS and validate its format.\n\nSource Information:\n- Source type: [Extracted from arguments]\n- Path/Identifier: [Extracted from arguments]\n- Expected format: [From user input or detected]\n\nRequirements:\n- Connect to data source (use mcp__supabase for Supabase sources)\n- Load dataset using appropriate method (datasets library for HF, pandas for local, SQL for Supabase)\n- Validate data schema and format\n- Check for missing values and data quality issues\n- Create data loader configuration\n- Generate dataset statistics (rows, columns, dtypes, missing %)\n\nExpected output:\n- Dataset successfully loaded confirmation\n- Schema and statistics summary\n- Data loader configuration saved to data/loader_config.json\n- Sample of first few rows for verification\")\n\nPhase 5: Verification\nGoal: Verify dataset was loaded correctly\n\nActions:\n- Check that data loader configuration was created\n- Read configuration: @data/loader_config.json\n- Verify dataset statistics make sense\n- Check for any validation warnings from agent\n- Run quick sanity check: !{bash python -c \"import json; config = json.load(open('data/loader_config.json')); print(f'Loaded {config.get(\\\"num_rows\\\", 0)} rows')\" 2>/dev/null || echo \"Manual verification needed\"}\n\nPhase 6: Summary\nGoal: Report dataset loading results\n\nActions:\n- Display dataset summary:\n  - Source type and location\n  - Number of rows and columns\n  - Data types and schema\n  - Any quality issues detected\n  - Configuration file location\n- Suggest next steps:\n  - Review data/loader_config.json\n  - Run /ml-training:preprocess if preprocessing needed\n  - Use /ml-training:train to start training"
              },
              {
                "name": "/add-fastapi-endpoint",
                "description": "Add ML inference endpoint to FastAPI backend",
                "path": "plugins/ml-training/commands/add-fastapi-endpoint.md",
                "frontmatter": {
                  "description": "Add ML inference endpoint to FastAPI backend",
                  "argument-hint": [
                    "model-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Create FastAPI endpoint for ML model inference with proper request/response handling and error handling\n\nCore Principles:\n- Detect project structure before modifying\n- Follow existing FastAPI patterns\n- Validate model exists before creating endpoint\n- Generate type-safe Pydantic models\n\nPhase 1: Discovery\nGoal: Understand project structure and model configuration\n\nActions:\n- Parse $ARGUMENTS for model name\n- Detect FastAPI backend location\n- Example: !{bash find . -name \"main.py\" -o -name \"app.py\" | grep -E \"(api|backend|server)\" | head -5}\n- Locate model files and configuration\n- Example: !{bash find . -name \"*.pt\" -o -name \"*.pth\" -o -name \"*.safetensors\" | head -10}\n- Check for existing routers and endpoint patterns\n- Example: @app/routers/ @api/routes/\n\nPhase 2: Validation\nGoal: Verify prerequisites and model availability\n\nActions:\n- Validate model name provided in $ARGUMENTS\n- Check if FastAPI is installed\n- Example: !{bash python -c \"import fastapi; print(fastapi.__version__)\" 2>/dev/null || echo \"Not installed\"}\n- Verify model file exists\n- Confirm model can be loaded\n- Check for existing endpoint with same name\n\nPhase 3: Planning\nGoal: Design endpoint structure and response models\n\nActions:\n- Determine model input/output shapes\n- Plan Pydantic request/response models\n- Identify router file location\n- Design error handling strategy\n- Outline endpoint route and HTTP method\n\nPhase 4: Implementation\nGoal: Create FastAPI endpoint with agent\n\nActions:\n\nTask(description=\"Create FastAPI inference endpoint\", subagent_type=\"integration-specialist\", prompt=\"You are the integration-specialist agent. Create a FastAPI inference endpoint for $ARGUMENTS.\n\nContext: Model name from $ARGUMENTS, project structure from Phase 1\n\nRequirements:\n- Create Pydantic models for request/response validation\n- Implement POST endpoint at /api/inference/{model-name}\n- Add proper error handling (404, 422, 500)\n- Load model efficiently (singleton pattern if needed)\n- Add input preprocessing and output postprocessing\n- Include logging for requests and errors\n- Add endpoint documentation with examples\n- Follow existing FastAPI patterns in codebase\n\nExpected output: Router file with endpoint, Pydantic models, error handlers\")\n\nPhase 5: Testing\nGoal: Verify endpoint functionality\n\nActions:\n- Check endpoint is registered in FastAPI app\n- Test endpoint with sample request\n- Example: !{bash curl -X POST http://localhost:8000/api/inference/model -H \"Content-Type: application/json\" -d '{\"input\": \"test\"}'}\n- Validate error handling with malformed requests\n- Check logs for proper request tracking\n\nPhase 6: Summary\nGoal: Document endpoint creation and usage\n\nActions:\n- Display endpoint URL and route\n- Show example request/response\n- List files created/modified:\n  - Router file path\n  - Pydantic models location\n  - Main app registration\n- Suggest next steps:\n  - Add authentication if needed\n  - Configure rate limiting\n  - Set up monitoring\n  - Add batch inference support"
              },
              {
                "name": "/add-monitoring",
                "description": null,
                "path": "plugins/ml-training/commands/add-monitoring.md",
                "frontmatter": null,
                "content": "---\ndescription: Add training monitoring and logging (TensorBoard/WandB)\nargument-hint: [monitoring-tool] [metrics]\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up comprehensive training monitoring and logging infrastructure using TensorBoard or Weights & Biases (WandB) for ML training workflows\n\nCore Principles:\n- Support both TensorBoard and WandB monitoring tools\n- Integrate seamlessly with existing training scripts\n- Provide real-time metrics visualization\n- Enable experiment tracking and comparison\n\nPhase 1: Discovery\nGoal: Understand monitoring requirements and current setup\n\nActions:\n- Parse $ARGUMENTS to extract monitoring tool preference and metrics\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - Which monitoring tool? (tensorboard/wandb/both)\n  - What metrics to track? (loss/accuracy/lr/custom)\n  - Existing training script location?\n  - Remote logging needed? (yes/no)\n- Check current project structure: !{bash ls -la 2>/dev/null}\n- Look for existing training scripts: !{bash find . -name \"train*.py\" -o -name \"*training*.py\" 2>/dev/null | head -5}\n\nPhase 2: Environment Check\nGoal: Verify dependencies and training framework\n\nActions:\n- Detect ML framework in use\n- Check for PyTorch: !{bash pip list | grep -i torch || echo \"Not found\"}\n- Check for TensorFlow: !{bash pip list | grep -i tensorflow || echo \"Not found\"}\n- Check existing monitoring tools: !{bash pip list | grep -E \"tensorboard|wandb\" || echo \"None installed\"}\n- Load existing training script if found: @train.py (if exists)\n- Check for existing monitoring configuration\n\nPhase 3: Configuration Preparation\nGoal: Prepare monitoring tool configuration\n\nActions:\n- Based on selected tool, prepare configuration:\n  - **TensorBoard**: Set up log directory structure\n  - **WandB**: Prepare API key prompt and project settings\n- Check for existing logs: !{bash ls -la logs/ runs/ wandb/ 2>/dev/null || echo \"No log directories\"}\n- Verify write permissions: !{bash test -w . && echo \"Writable\" || echo \"Permission issue\"}\n- Prepare metrics configuration based on user requirements\n\nPhase 4: Monitoring Integration\nGoal: Integrate monitoring using specialized agent\n\nActions:\n\nTask(description=\"Set up training monitoring\", subagent_type=\"training-monitor\", prompt=\"You are the training-monitor agent. Set up monitoring and logging infrastructure for ML training with $ARGUMENTS.\n\nMonitoring Configuration:\n- Tool: [Extracted from arguments or user input]\n- Metrics: [Extracted from arguments or user input]\n- Training script: [Detected or provided path]\n- Framework: [Detected ML framework]\n\nRequirements:\n- Install required monitoring libraries (tensorboard and/or wandb)\n- Create monitoring configuration file (monitoring_config.json)\n- Integrate logging callbacks into training script\n- Set up log directory structure (logs/ or wandb/)\n- Add metric tracking code for specified metrics\n- Configure visualization dashboard settings\n- For WandB: Add API key configuration instructions\n- For TensorBoard: Create tensorboard launch script\n\nIntegration Tasks:\n- Add monitoring imports to training script\n- Initialize monitoring client (TensorBoard SummaryWriter or wandb.init)\n- Add logging callbacks for metrics tracking\n- Configure periodic checkpoint logging\n- Add visualization for loss curves, accuracy, learning rate\n- Set up custom metric tracking if specified\n- Test monitoring integration with dummy data\n\nExpected output:\n- monitoring_config.json with tool settings\n- Updated training script with monitoring integrated\n- Launch script (start_tensorboard.sh or wandb_setup.md)\n- Quick start guide for using the monitoring tools\n- Example commands to view logs and dashboards\")\n\nPhase 5: Verification\nGoal: Verify monitoring setup is complete\n\nActions:\n- Check monitoring configuration was created: @monitoring_config.json\n- Verify training script was updated\n- Check for launch scripts: !{bash ls -la start_tensorboard.sh wandb_setup.md 2>/dev/null || echo \"Check monitoring files\"}\n- Test monitoring imports: !{bash python -c \"import tensorboard; print('TensorBoard OK')\" 2>/dev/null || python -c \"import wandb; print('WandB OK')\" 2>/dev/null || echo \"Verify installation\"}\n- Verify log directories exist: !{bash ls -la logs/ wandb/ 2>/dev/null || echo \"Log directories not yet created\"}\n\nPhase 6: Summary\nGoal: Report monitoring setup results\n\nActions:\n- Display setup summary:\n  - Monitoring tool(s) installed\n  - Metrics being tracked\n  - Log directory locations\n  - Dashboard access instructions\n  - Configuration file location\n- Provide usage instructions:\n  - How to launch monitoring dashboard\n  - How to view real-time metrics\n  - How to compare experiments\n  - How to access remote dashboards (if WandB)\n- Suggest next steps:\n  - Run training to test monitoring: python train.py\n  - View TensorBoard: tensorboard --logdir=logs\n  - View WandB: wandb login && check dashboard URL\n  - Review monitoring_config.json for customization\n"
              },
              {
                "name": "/add-nextjs-ui",
                "description": "Add ML UI components to Next.js frontend",
                "path": "plugins/ml-training/commands/add-nextjs-ui.md",
                "frontmatter": {
                  "description": "Add ML UI components to Next.js frontend",
                  "argument-hint": [
                    "feature"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add production-ready ML UI components to Next.js frontend for model predictions, results visualization, and user interaction\n\nCore Principles:\n- Detect Next.js project structure before creating components\n- Create reusable React components with TypeScript support\n- Integrate with ML backend APIs (Modal, FastAPI, or serverless endpoints)\n- Handle loading states, errors, and real-time updates\n- Follow Next.js App Router patterns\n\nPhase 1: Discovery\nGoal: Understand Next.js project structure and feature requirements\n\nActions:\n- Parse $ARGUMENTS for feature type (default: \"prediction-form\")\n- Validate Next.js project: !{bash test -f next.config.js -o -f next.config.mjs -o -f next.config.ts && echo \"Next.js detected\" || echo \"Error: No Next.js project found\"}\n- Detect structure and TypeScript: !{bash test -d app && echo \"App Router\" || test -d pages && echo \"Pages Router\"} | !{bash test -f tsconfig.json && echo \"TypeScript\" || echo \"JavaScript\"}\n- List existing components: !{bash find . -path ./node_modules -prune -o -path ./components -type f -name \"*.tsx\" -o -name \"*.jsx\" 2>/dev/null | grep -v node_modules | head -10}\n- Check UI framework: !{bash grep -E \"(tailwindcss|shadcn|chakra-ui|mui)\" package.json 2>/dev/null}\n\nPhase 2: Backend Detection\nGoal: Identify ML inference endpoints to integrate with\n\nActions:\n- Look for API configuration:\n  - !{bash grep -r \"API_URL\\|INFERENCE_URL\\|MODEL_ENDPOINT\" .env .env.local 2>/dev/null | head -5}\n- Check for API client files:\n  - !{bash find . -path ./node_modules -prune -o -name \"*api*.ts\" -o -name \"*client*.ts\" 2>/dev/null | grep -v node_modules | head -5}\n- Detect inference platform:\n  - !{bash find . -name \"inference_*.py\" -o -name \"*.modal.py\" -o -name \"*fastapi*.py\" 2>/dev/null | head -3}\n- Report detected backend or prompt for endpoint URL\n\nPhase 3: Component Generation\nGoal: Create ML UI components with integration-specialist agent\n\nActions:\n\nTask(description=\"Generate Next.js ML UI components\", subagent_type=\"integration-specialist\", prompt=\"You are the integration-specialist agent. Generate Next.js ML UI components for $ARGUMENTS.\n\nFeature Type: $ARGUMENTS\n\nComponent Requirements:\n1. Create ML prediction form component:\n   - Input fields for model parameters\n   - File upload support (images, text files)\n   - Real-time validation\n   - Loading states with progress indicators\n   - Error handling and user feedback\n\n2. Create results display component:\n   - Visualize model predictions\n   - Show confidence scores\n   - Display generated content (text, images)\n   - Support multiple result formats (JSON, tables, charts)\n   - Copy/download functionality\n\n3. Create API integration:\n   - Client-side API calls using fetch/axios\n   - Server actions for App Router (if detected)\n   - Request/response type definitions\n   - Error handling and retries\n   - Loading and error states management\n\n4. Add UI enhancements:\n   - Responsive design (mobile, tablet, desktop)\n   - Dark mode support\n   - Accessibility (ARIA labels, keyboard navigation)\n   - Animation and transitions\n   - Toast notifications for feedback\n\n5. Integration points:\n   - Connect to ML inference endpoints\n   - Handle authentication if required\n   - Support real-time updates (WebSocket/SSE if applicable)\n   - Cache results with React Query or SWR\n   - Rate limiting and quota display\n\nComponent Structure:\n- components/ml/PredictionForm.tsx\n- components/ml/ResultsDisplay.tsx\n- components/ml/ModelSelector.tsx (if multiple models)\n- lib/api/ml-client.ts (API integration)\n- types/ml.ts (TypeScript types)\n- hooks/usePrediction.ts (custom hook)\n\nDeliverable:\n- All component files created in proper directories\n- API client with typed endpoints\n- Example usage in app/ml/page.tsx or pages/ml.tsx\n- README section with component documentation\")\n\nPhase 4: Dependencies\nGoal: Install required npm packages for ML UI\n\nActions:\n- Check package manager: !{bash test -f pnpm-lock.yaml && echo \"pnpm\" || test -f yarn.lock && echo \"yarn\" || echo \"npm\"}\n- Install dependencies: !{bash npm install react-query axios recharts lucide-react --save 2>&1 | tail -5}\n- Install dev dependencies: !{bash npm install @types/react-query --save-dev 2>&1 | tail -3}\n\nPhase 5: Configuration\nGoal: Set up environment variables and configuration\n\nActions:\n- Create or update .env.local with NEXT_PUBLIC_ML_API_URL, ML_API_KEY, and feature flags\n- Update next.config with image domains, API rewrite rules, and environment allowlist\n- Create .env.example for documentation\n\nPhase 6: Summary\nGoal: Display component creation results and usage instructions\n\nActions:\n- List created files:\n  - !{bash find components/ml lib/api types hooks -type f 2>/dev/null | sort}\n- Show file sizes:\n  - !{bash du -sh components/ml lib/api 2>/dev/null}\n- Display summary:\n  - Feature: $ARGUMENTS ML UI components\n  - Components created: PredictionForm, ResultsDisplay, API client\n  - Framework: Next.js [App Router/Pages Router]\n  - TypeScript: [Yes/No]\n  - Dependencies installed: react-query, axios, recharts\n  - Environment: .env.local configured\n- Show usage example:\n  - \"Import: import { PredictionForm } from '@/components/ml/PredictionForm'\"\n  - \"Use in page: <PredictionForm modelEndpoint={ML_API_URL} />\"\n  - \"Access page: http://localhost:3000/ml\"\n- Next steps:\n  - \"Configure ML_API_URL in .env.local\"\n  - \"Test components: npm run dev\"\n  - \"Customize styling to match your design system\"\n  - \"Add authentication if ML endpoints require it\"\n  - \"Monitor API usage and add rate limiting UI\""
              },
              {
                "name": "/add-peft",
                "description": "Add parameter-efficient fine-tuning (LoRA/QLoRA/prefix-tuning)",
                "path": "plugins/ml-training/commands/add-peft.md",
                "frontmatter": {
                  "description": "Add parameter-efficient fine-tuning (LoRA/QLoRA/prefix-tuning)",
                  "argument-hint": "<lora|qlora|prefix-tuning>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure and add parameter-efficient fine-tuning (PEFT) methods to reduce memory usage and training costs while maintaining model performance\n\nCore Principles:\n- Support multiple PEFT methods (LoRA, QLoRA, prefix-tuning)\n- Optimize memory efficiency for cloud GPU usage\n- Maintain compatibility with HuggingFace PEFT library\n- Provide clear memory savings estimates\n\nPhase 1: Discovery\nGoal: Understand PEFT method and project requirements\n\nActions:\n- Parse $ARGUMENTS to extract PEFT method (lora/qlora/prefix-tuning)\n- If $ARGUMENTS is unclear or missing, use AskUserQuestion to gather:\n  - Which PEFT method? (lora/qlora/prefix-tuning)\n  - Target model architecture? (llama/mistral/gpt/t5)\n  - Training task type? (text-generation/classification/seq2seq)\n  - Memory constraints or GPU size?\n- Validate that PEFT method is supported\n- Check current project setup: !{bash ls -la train*.py config/ 2>/dev/null || echo \"No training files found\"}\n\nPhase 2: Environment Check\nGoal: Verify dependencies and existing configuration\n\nActions:\n- Check for PEFT library installation: !{bash pip list | grep peft || echo \"PEFT not installed\"}\n- Verify transformers version compatibility: !{bash pip list | grep transformers}\n- Check for existing training configuration: @config/training_config.json (if exists)\n- Locate training script: !{bash find . -name \"train*.py\" -o -name \"*training*.py\" | head -5}\n- Check for existing PEFT config: !{bash test -f config/peft_config.json && echo \"Found\" || echo \"Not found\"}\n\nPhase 3: Configuration Analysis\nGoal: Analyze current training setup\n\nActions:\n- Search for model initialization code: !{bash grep -r \"AutoModel\\|from_pretrained\" *.py config/ 2>/dev/null | head -10}\n- Check training arguments: !{bash grep -r \"TrainingArguments\\|Trainer\" *.py 2>/dev/null | head -10}\n- Identify model size and architecture from existing code\n- Determine current memory requirements\n- Present findings to user for confirmation\n\nPhase 4: PEFT Configuration\nGoal: Create PEFT configuration using specialized agent\n\nActions:\n\nTask(description=\"Configure PEFT method\", subagent_type=\"peft-specialist\", prompt=\"You are the peft-specialist agent. Configure parameter-efficient fine-tuning for $ARGUMENTS.\n\nPEFT Method Information:\n- Method: [Extracted from arguments - lora/qlora/prefix-tuning]\n- Model architecture: [From user input or detected]\n- Task type: [From user input or detected]\n- Memory constraints: [From user input]\n\nRequirements:\n- Install PEFT library if not present: pip install peft\n- Create appropriate PEFT configuration based on method:\n  - LoRA: Set rank (r), alpha, dropout, target modules\n  - QLoRA: Add 4-bit quantization config, NF4 type, compute dtype\n  - Prefix-tuning: Set num_virtual_tokens, task type\n- Update training script to wrap model with get_peft_model()\n- Configure training arguments for PEFT (gradient checkpointing, bf16/fp16)\n- Add memory-efficient optimizer (paged_adamw_8bit for QLoRA)\n- Calculate and report memory savings estimate\n- Create config/peft_config.json with all settings\n\nExpected output:\n- PEFT configuration created and saved\n- Training script updated with PEFT integration\n- Memory savings estimate (e.g., '75% reduction for QLoRA')\n- Trainable parameters comparison (before/after)\n- Configuration saved to config/peft_config.json\")\n\nPhase 5: Verification\nGoal: Verify PEFT configuration was applied correctly\n\nActions:\n- Check that PEFT configuration was created: !{bash test -f config/peft_config.json && echo \"Created\" || echo \"Missing\"}\n- Read configuration: @config/peft_config.json\n- Verify training script was updated: !{bash grep -n \"get_peft_model\\|LoraConfig\\|PeftConfig\" train*.py 2>/dev/null | head -5}\n- Validate PEFT library is installed: !{bash python -c \"import peft; print(f'PEFT version: {peft.__version__}')\" 2>/dev/null || echo \"Import failed\"}\n- Run dry-run to check trainable parameters: !{bash python -c \"import json; config = json.load(open('config/peft_config.json')); print(f'Method: {config.get(\\\"peft_type\\\", \\\"unknown\\\")}')\" 2>/dev/null || echo \"Manual check needed\"}\n\nPhase 6: Summary\nGoal: Report PEFT configuration results\n\nActions:\n- Display configuration summary:\n  - PEFT method configured\n  - Target modules for parameter-efficient training\n  - Estimated memory savings\n  - Trainable vs. frozen parameters\n  - Configuration file location\n  - Updated training script path\n- Provide memory efficiency metrics:\n  - Original model parameters\n  - Trainable parameters with PEFT\n  - Percentage reduction\n  - Estimated GPU memory requirement\n- Suggest next steps:\n  - Review config/peft_config.json for fine-tuning\n  - Test with small batch: python train.py --dry_run\n  - Monitor memory usage during training\n  - Use /ml-training:add-platform to deploy on cloud GPU"
              },
              {
                "name": "/add-platform",
                "description": "Add cloud GPU platform integration (Modal/Lambda/RunPod)",
                "path": "plugins/ml-training/commands/add-platform.md",
                "frontmatter": {
                  "description": "Add cloud GPU platform integration (Modal/Lambda/RunPod)",
                  "argument-hint": "<modal|lambda|runpod>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add cloud GPU platform integration for ML training with authentication setup and test configuration\n\nCore Principles:\n- Parse platform choice from arguments\n- Install platform-specific SDK\n- Configure authentication\n- Create GPU image definition\n- Verify setup with test script\n\nPhase 1: Parse Platform Choice\nGoal: Determine which platform to integrate\n\nActions:\n- Parse $ARGUMENTS to extract platform name (modal, lambda, or runpod)\n- If no argument or invalid platform provided:\n  - Use AskUserQuestion to ask: \"Which cloud GPU platform? (modal/lambda/runpod)\"\n- Validate platform choice is one of: modal, lambda, runpod\n- Convert to lowercase for consistency\n\nPhase 2: Check Existing Setup\nGoal: Understand current project structure\n\nActions:\n- Check for existing requirements.txt: @requirements.txt\n- Check for existing platform configs:\n  - Modal: Look for modal_*.py files\n  - Lambda: Look for lambda_config.json\n  - RunPod: Look for runpod_config.json\n- Detect Python version from runtime files\n- Check if platform SDK already installed\n\nPhase 3: Install Platform SDK\nGoal: Install correct SDK for chosen platform\n\nActions:\nInstall SDK based on platform:\n- Modal: !{bash pip install modal}\n- Lambda: !{bash pip install lambda-cloud}\n- RunPod: !{bash pip install runpod}\n\nUpdate requirements.txt with SDK entry\n\nPhase 4: Platform Setup and Authentication\nGoal: Configure platform authentication\n\nActions:\nTask(description=\"Setup platform authentication\", subagent_type=\"ml-architect\", prompt=\"You are the ml-architect agent configuring $ARGUMENTS cloud GPU platform.\n\nBased on platform:\n\n**Modal:**\n- Run: python3 -m modal setup (interactive auth)\n- Guide user through account creation/login\n- Saves token to ~/.modal.toml\n\n**Lambda Labs:**\n- Create lambda_config.json with api_key, region, instance_type fields\n- Instruct: Get API key from https://cloud.lambdalabs.com/api-keys\n\n**RunPod:**\n- Create runpod_config.json with api_key, gpu_type, container_disk_size_gb fields\n- Instruct: Get API key from https://www.runpod.io/console/user/settings\n\nComplete authentication setup for $ARGUMENTS platform.\")\n\nPhase 5: Create Platform Files\nGoal: Create GPU image definition and test script\n\nActions:\nTask(description=\"Create platform integration files\", subagent_type=\"ml-architect\", prompt=\"You are the ml-architect agent. Create GPU platform files for $ARGUMENTS:\n\n**Modal:** Create modal_image.py with:\n- modal.Image.debian_slim() with torch, transformers, accelerate\n- @stub.function(gpu='A100') training function\n- test_platform.py to verify GPU access\n\n**Lambda Labs:** Create lambda_image.py with:\n- launch_instance() function for GPU instances\n- test_platform.py to verify API connection\n\n**RunPod:** Create runpod_image.py with:\n- create_pod() function for GPU pods\n- test_platform.py to verify API connection\n\nInclude error handling and GPU verification.\")\n\nPhase 6: Summary and Next Steps\nGoal: Provide user with setup confirmation and usage guide\n\nActions:\nDisplay summary:\n- Platform integrated: [platform name]\n- SDK installed: [version]\n- Configuration files created\n- Image definition ready\n- Authentication status: [configured/needs API key]\n\nNext steps:\n1. **For Modal**: Run `python test_platform.py` to verify GPU access\n2. **For Lambda/RunPod**: Add API key to config file, then test\n3. Use platform for training: See docs/[platform]-training.md\n4. Estimate costs: Check platform pricing calculator\n5. Set up W&B logging: Run `/ml-training:add-tracking wandb`\n\nImportant Notes:\n- Modal has free tier with GPU credits\n- Lambda Labs requires payment for GPU hours\n- RunPod bills by the minute for GPU usage\n- Always terminate instances after training to avoid charges\n- Consider using spot instances for cost savings"
              },
              {
                "name": "/add-preprocessing",
                "description": "Add data preprocessing pipelines (tokenization/transforms)",
                "path": "plugins/ml-training/commands/add-preprocessing.md",
                "frontmatter": {
                  "description": "Add data preprocessing pipelines (tokenization/transforms)",
                  "argument-hint": [
                    "tokenizer|transforms"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add data preprocessing pipelines with tokenization for text data or transforms for image data, including data caching\n\nCore Principles:\n- Detect preprocessing type from arguments and existing data\n- Use HuggingFace tokenizers for text, torchvision transforms for images\n- Implement efficient data caching\n- Test pipeline before finalizing\n\nPhase 1: Discovery\nGoal: Understand preprocessing requirements and existing setup\n\nActions:\n- Parse $ARGUMENTS to identify preprocessing type (tokenizer vs transforms)\n- Check for existing data loading code and training scripts\n- Example: !{bash find . -name \"*.py\" -type f | grep -E \"(train|data|preprocess)\" | head -10}\n- Detect data type from existing files (text datasets vs image datasets)\n- Load any existing preprocessing configuration\n\nPhase 2: Analysis\nGoal: Understand current data pipeline and determine preprocessing needs\n\nActions:\n- Read existing training scripts to understand data format\n- Identify model type (language model vs vision model)\n- Check for existing tokenizer or transform configurations\n- Determine if dataset is local or HuggingFace Hub-based\n- Example: !{bash grep -r \"from datasets import\" . 2>/dev/null | head -5}\n\nPhase 3: Planning\nGoal: Design preprocessing pipeline approach\n\nActions:\n- Based on analysis, determine:\n  - For text: Which tokenizer (model-specific or custom)\n  - For images: Which transforms (resize, normalize, augmentation)\n  - Caching strategy (disk vs memory)\n  - Batch processing configuration\n- Outline integration points with existing data loading\n- Present plan to user\n\nPhase 4: Implementation\nGoal: Build preprocessing pipeline with agent\n\nActions:\n\nTask(description=\"Create preprocessing pipeline\", subagent_type=\"data-specialist\", prompt=\"You are the data-specialist agent. Create a data preprocessing pipeline for $ARGUMENTS.\n\nContext: ML training project using HuggingFace ecosystem and cloud GPUs (Modal/Lambda/RunPod)\n\nRequirements:\n- If tokenizer: Use HuggingFace AutoTokenizer with proper padding and truncation\n- If transforms: Use torchvision transforms with normalization and augmentation\n- Implement data caching to disk for faster iterations\n- Support batch processing\n- Handle common edge cases (variable length sequences, different image sizes)\n- Add preprocessing test function\n- Follow HuggingFace datasets .map() pattern for efficiency\n\nExpected output:\n- Preprocessing module with tokenizer or transform configuration\n- Data caching setup (using datasets.save_to_disk() or torch cache)\n- Test function to validate preprocessing\n- Integration code for existing training pipeline\")\n\nPhase 5: Verification\nGoal: Test preprocessing pipeline\n\nActions:\n- Run preprocessing test function to verify correctness\n- Example: !{bash python -c \"from preprocessing import test_preprocessing; test_preprocessing()\"}\n- Check cache is created successfully\n- Verify processed data format matches model requirements\n- Test with small batch to ensure no errors\n\nPhase 6: Summary\nGoal: Document preprocessing setup\n\nActions:\n- Summarize preprocessing pipeline created:\n  - Type (tokenizer or transforms)\n  - Configuration details\n  - Caching location and strategy\n  - Files created/modified\n- Provide usage instructions:\n  - How to use in training script\n  - How to adjust preprocessing parameters\n  - How to clear/rebuild cache\n- Suggest next steps (integrate into training, tune hyperparameters)"
              },
              {
                "name": "/add-training-config",
                "description": "Create training configuration for classification/generation/fine-tuning",
                "path": "plugins/ml-training/commands/add-training-config.md",
                "frontmatter": {
                  "description": "Create training configuration for classification/generation/fine-tuning",
                  "argument-hint": "<classification|generation|fine-tuning>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate production-ready training configuration including TrainingArguments, hyperparameters, and train.py script for the specified training type.\n\nCore Principles:\n- Detect existing project structure before generating configs\n- Use appropriate defaults based on training type\n- Generate framework-agnostic configurations when possible\n- Validate compatibility with detected ML frameworks\n\nPhase 1: Discovery\nGoal: Understand project context and training requirements\n\nActions:\n- Parse $ARGUMENTS to extract training type (classification/generation/fine-tuning)\n- Detect ML framework in use (PyTorch, TensorFlow, JAX)\n- Check for existing training scripts or configurations\n- Example: !{bash ls train.py training_config.yaml config/ 2>/dev/null}\n- Identify project structure and data locations\n\nPhase 2: Validation\nGoal: Verify training type and environment compatibility\n\nActions:\n- Validate training type is one of: classification, generation, fine-tuning\n- Check if required dependencies are available\n- Example: !{bash python -c \"import transformers; import torch; import datasets\" 2>&1}\n- Load existing configs if present to understand patterns\n- Identify GPU/CPU availability for hardware-specific settings\n\nPhase 3: Configuration Design\nGoal: Architect training configuration with optimal hyperparameters\n\nActions:\n\nTask(description=\"Design training configuration\", subagent_type=\"training-architect\", prompt=\"You are the training-architect agent. Create a comprehensive training configuration for $ARGUMENTS.\n\nContext:\n- Training type: Extract from $ARGUMENTS (classification/generation/fine-tuning)\n- Detected framework: Based on discovery phase findings\n- Project structure: Based on codebase analysis\n\nRequirements:\n- Generate TrainingArguments configuration with appropriate hyperparameters\n- Set learning rate, batch size, epochs based on training type\n- Configure evaluation strategy and checkpointing\n- Include mixed precision training settings (fp16/bf16)\n- Set up gradient accumulation if needed\n- Configure warmup steps and scheduler\n- Add logging and early stopping parameters\n\nTraining Type Specific Settings:\n- Classification: CrossEntropyLoss, accuracy metrics, class weights\n- Generation: Language modeling loss, perplexity metrics, generation parameters\n- Fine-tuning: LoRA/QLoRA configs, adapter settings, freeze layers\n\nDeliverables:\n1. training_config.yaml - Complete TrainingArguments configuration\n2. train.py - Training script with data loading, model setup, trainer initialization\n3. hyperparameters.json - Searchable hyperparameter ranges for tuning\n4. README-TRAINING.md - Documentation on running training and tuning parameters\n\nFollow best practices for reproducibility, mixed precision, and gradient checkpointing.\")\n\nPhase 4: File Generation\nGoal: Write configuration files to project\n\nActions:\n- Write training_config.yaml to project root or config/ directory\n- Write train.py script with proper imports and setup\n- Write hyperparameters.json for reference\n- Create README-TRAINING.md with usage instructions\n- Ensure all files follow project conventions\n\nPhase 5: Verification\nGoal: Validate generated configurations\n\nActions:\n- Check that all required files were created\n- Validate YAML/JSON syntax\n- Example: !{bash python -c \"import yaml; yaml.safe_load(open('training_config.yaml'))\"}\n- Verify train.py has no syntax errors\n- Example: !{bash python -m py_compile train.py}\n- Check that hyperparameters are in valid ranges\n\nPhase 6: Summary\nGoal: Report generated configuration and next steps\n\nActions:\n- Summarize created files and their locations\n- Display key hyperparameters set for the training type\n- Provide command to start training\n- Suggest next steps:\n  - Review and adjust hyperparameters for your dataset\n  - Prepare dataset using /ml-training:prepare-dataset command\n  - Run training with: python train.py --config training_config.yaml\n  - Monitor training with TensorBoard or wandb\n  - Experiment with hyperparameter tuning ranges provided"
              },
              {
                "name": "/deploy-inference",
                "description": "Deploy trained model for serverless inference",
                "path": "plugins/ml-training/commands/deploy-inference.md",
                "frontmatter": {
                  "description": "Deploy trained model for serverless inference",
                  "argument-hint": "<model-path>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Deploy trained model to serverless endpoint with auto-scaling and return accessible URL\n\nCore Principles:\n- Validate model exists before deployment\n- Select optimal serverless platform based on model requirements\n- Configure auto-scaling for cost efficiency\n- Provide clear endpoint access information\n\nPhase 1: Validation\nGoal: Parse arguments and validate model availability\n\nActions:\n- Parse $ARGUMENTS for model path\n- Validate model path is provided:\n  - !{bash echo \"$ARGUMENTS\" | grep -q . && echo \"valid\" || echo \"Usage: /ml-training:deploy-inference <model-path>\"}\n- Check if model file/directory exists:\n  - !{bash test -e \"$ARGUMENTS\" && echo \"Model found\" || echo \"Error: Model not found at $ARGUMENTS\"}\n- Determine model type and size:\n  - !{bash du -sh \"$ARGUMENTS\" 2>/dev/null}\n  - !{bash file \"$ARGUMENTS\" 2>/dev/null || ls -lh \"$ARGUMENTS\" 2>/dev/null}\n\nPhase 2: Discovery\nGoal: Understand deployment context and requirements\n\nActions:\n- Check for existing deployment configurations:\n  - !{bash find . -name \"inference_*.py\" -o -name \"deploy_*.py\" -o -name \"*.modal.py\" 2>/dev/null | head -5}\n- Load model metadata if available:\n  - @config.json (if exists in model path)\n  - @model_card.md (if exists)\n- Check for requirements files:\n  - !{bash ls requirements.txt inference_requirements.txt 2>/dev/null}\n\nPhase 3: Platform Selection\nGoal: Determine optimal serverless platform\n\nActions:\n- Analyze model characteristics from Phase 1\n- Select platform based on:\n  - Model size: Large models (>5GB) -> RunPod or Lambda Labs\n  - HuggingFace models -> Modal with GPU support\n  - Custom models -> Modal or RunPod with custom container\n- Display selected platform and reasoning\n\nPhase 4: Deployment\nGoal: Deploy model to serverless endpoint with inference-deployer agent\n\nActions:\n\nTask(description=\"Deploy model to serverless inference\", subagent_type=\"inference-deployer\", prompt=\"You are the inference-deployer agent. Deploy the trained model at $ARGUMENTS to a serverless inference endpoint.\n\nModel Path: $ARGUMENTS\n\nDeployment Requirements:\n1. Create serverless inference endpoint:\n   - Use Modal, RunPod, or Lambda Labs based on model requirements\n   - Configure GPU acceleration if model requires it\n   - Set up model loading and inference handler\n   - Implement health check endpoint\n\n2. Configure auto-scaling:\n   - Set min replicas to 0 (scale to zero when idle)\n   - Set max replicas based on expected load (default: 10)\n   - Configure scale-up threshold (requests per second)\n   - Configure scale-down timeout (idle seconds before shutdown)\n\n3. Set up API endpoint:\n   - Create /predict or /generate endpoint\n   - Accept JSON input with model-specific parameters\n   - Return predictions in JSON format\n   - Add error handling and validation\n\n4. Configure environment:\n   - Load model weights from path\n   - Set up dependencies (transformers, torch, etc.)\n   - Configure memory limits\n   - Set timeout thresholds\n\n5. Create deployment files:\n   - inference_endpoint.py (main handler)\n   - deploy_inference.sh (deployment script)\n   - Update requirements with inference dependencies\n\nDeliverable:\n- Deployment files created\n- Endpoint URL (or instructions to get URL after deployment)\n- Example curl command to test endpoint\n- Auto-scaling configuration summary\")\n\nPhase 5: Verification\nGoal: Verify deployment configuration and test endpoint\n\nActions:\n- List deployment files created:\n  - !{bash ls -lh inference_*.py deploy_*.sh 2>/dev/null}\n- Display deployment configuration:\n  - Show auto-scaling settings\n  - Show resource limits (GPU, memory)\n  - Show timeout configuration\n- Test deployment locally if possible:\n  - !{bash python -m py_compile inference_*.py 2>/dev/null && echo \"Syntax valid\" || echo \"Check for syntax errors\"}\n\nPhase 6: Summary\nGoal: Provide deployment results and access information\n\nActions:\n- Display deployment summary:\n  - Model deployed: [model-path]\n  - Platform: [selected-platform]\n  - Endpoint URL or deployment instructions\n  - Auto-scaling config: min=0, max=10, idle-timeout=60s\n- Show testing command:\n  - Example: curl -X POST [endpoint-url]/predict -H \"Content-Type: application/json\" -d '{\"input\": \"test\"}'\n- Show next steps:\n  - \"Deploy to cloud: bash deploy_inference.sh\"\n  - \"Monitor endpoint: [platform-specific monitoring URL]\"\n  - \"Scale configuration: Edit auto-scaling in inference_endpoint.py\"\n  - \"Update model: Re-run with new model path\"\n- Display cost optimization tips:\n  - \"Endpoint scales to zero when idle (no cost)\"\n  - \"Adjust max replicas based on expected load\"\n  - \"Monitor cold start times and adjust keep-warm settings if needed\""
              },
              {
                "name": "/deploy-training",
                "description": "Deploy training job to cloud GPU platform",
                "path": "plugins/ml-training/commands/deploy-training.md",
                "frontmatter": {
                  "description": "Deploy training job to cloud GPU platform",
                  "argument-hint": [
                    "config-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Deploy ML training job to cloud GPU platform with cost estimation, progress monitoring, and job status reporting\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Parse and Validate Configuration\n\nGoal: Load config and verify environment setup\n\nActions:\n- Parse $ARGUMENTS for config path or check defaults: !{bash test -f training_config.yaml && echo \"training_config.yaml\" || test -f config/training.yaml && echo \"config/training.yaml\" || echo \"none\"}\n- If no config found, ask user: AskUserQuestion(\"Provide path to training config file:\")\n- Load config file: @{config-path}\n- Extract platform (modal/lambda/runpod), GPU type (A100/A10G/H100), script path, duration estimate\n- Verify platform SDK: !{bash python -c \"import modal\" 2>&1 || python -c \"import lambda_cloud\" 2>&1 || python -c \"import runpod\" 2>&1 || echo \"missing\"}\n- Check auth: !{bash test -f ~/.modal.toml || test -f lambda_config.json || test -f runpod_config.json && echo \"configured\" || echo \"missing\"}\n- If SDK/auth missing, report: \"Install SDK with: /ml-training:add-platform [platform]\"\n\n## Phase 2: Cost Estimation\n\nGoal: Calculate and confirm deployment costs\n\nActions:\n- Calculate cost from config GPU type and duration (Modal A100: $4.00/hr, A10G: $1.10/hr | Lambda A100: $1.29/hr, A10G: $0.60/hr | RunPod A100: $1.89/hr, A10G: $0.44/hr)\n- Display: GPU [type], Duration [hours], Cost $[min]-$[max], Platform [platform]\n- Ask confirmation: AskUserQuestion(\"Proceed with deployment? (yes/no)\")\n- If no, exit: \"Deployment cancelled\"\n\n## Phase 3: Deploy Job\n\nGoal: Submit training job to cloud platform\n\nActions:\nTask(description=\"Deploy training job to cloud platform\", subagent_type=\"training-architect\", prompt=\"You are the training-architect agent. Deploy ML training job to cloud platform from config: {config-path}\n\n**Platform Deployment (based on config platform):**\n\nModal: Run `modal run {script-path} --config={config-path}`, capture app ID\nLambda: Load lambda_config.json, create instance with GPU from config, SSH start training, capture instance ID\nRunPod: Load runpod_config.json, create pod with GPU from config, execute training, capture pod ID\n\n**Error Handling:**\n- Deployment fails: Report error details\n- Auth fails: Provide setup instructions\n- Quota exceeded: Suggest alternatives\n\n**Capture:** Job/instance ID, GPU type, start time, estimated completion, monitoring URL\")\n\n## Phase 4: Summary\n\nGoal: Display deployment status and monitoring info\n\nActions:\n- Check W&B: !{bash test -f .env && grep -q \"WANDB_API_KEY\" .env && echo \"enabled\" || echo \"disabled\"}\n- Display summary:\n  - Status: Job deployed successfully\n  - Platform: [platform], GPU: [type], Job ID: [id]\n  - Cost estimate: $[amount], ETA: [time]\n  - Config: [config-path]\n- Monitoring:\n  - Modal: modal app logs [app-id] | https://modal.com/apps/[workspace]/[app-id]\n  - Lambda: Instance console | https://cloud.lambdalabs.com/instances/[instance-id]\n  - RunPod: runpod logs [pod-id] | https://www.runpod.io/console/pods/[pod-id]\n  - W&B dashboard (if enabled)\n- Next steps:\n  1. Monitor with platform commands above\n  2. Checkpoints auto-save to configured location\n  3. Terminate if needed: [platform-specific command]\n  4. Download model: /ml-training:download-model [job-id]\n- Warning: Job incurs charges until completion/termination"
              },
              {
                "name": "/estimate-cost",
                "description": "Estimate training and inference costs",
                "path": "plugins/ml-training/commands/estimate-cost.md",
                "frontmatter": {
                  "description": "Estimate training and inference costs",
                  "argument-hint": [
                    "config-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Calculate estimated costs for ML training and inference across different cloud GPU platforms (Modal, Lambda Labs, RunPod) based on training configuration.\n\nCore Principles:\n- Parse training configuration to extract compute requirements\n- Calculate GPU hours needed based on dataset size and model\n- Compare pricing across multiple platforms\n- Provide cost breakdown and optimization recommendations\n\nPhase 1: Configuration Discovery\nGoal: Parse configuration and extract training parameters\n\nActions:\n- Extract config path from $ARGUMENTS (default: config/training_config.yaml)\n- Verify config file exists: !{bash test -f \"$ARGUMENTS\" && echo \"Found\" || echo \"Not found\"}\n- Read configuration to extract: model size, dataset size, batch size, epochs, GPU preference\n- Example: !{bash echo \"Dataset samples: $(grep -i 'num_samples\\|dataset_size' \"$ARGUMENTS\" | head -1)\"}\n\nPhase 2: Compute Requirements\nGoal: Calculate GPU hours needed for training\n\nActions:\n- Estimate training time based on model and dataset:\n  - Small (<1B params): ~0.5-2h per 10k samples | Medium (1B-7B): ~2-8h | Large (7B-70B): ~8-40h\n- Calculate total GPU hours: (samples / batch_size) * epochs * time_per_batch\n- Factor in: gradient accumulation, evaluation frequency, checkpoint overhead, multi-GPU efficiency\n\nPhase 3: Platform Pricing\nGoal: Apply GPU pricing rates\n\nActions:\n- GPU pricing per hour (2025): Modal (A10G: $1.10, A100-40GB: $3.50, A100-80GB: $5.00, H100: $8.00)\n- Lambda Labs (A10: $0.60, A100-40GB: $1.10, A100-80GB: $1.30, H100: $2.00)\n- RunPod (A10: $0.79, A100-40GB: $1.59, A100-80GB: $2.09, H100: $4.89)\n- GPU selection: <7B params use A10 | 7B-13B use A100-40GB | 13B-70B use A100-80GB | >70B use H100\n\nPhase 4: Cost Calculation\nGoal: Calculate total training cost per platform\n\nActions:\n- Calculate training cost: GPU_hours * price_per_hour\n- Example: !{bash echo \"Estimated GPU hours: 24\" && echo \"Modal A100-40GB: \\$84.00 (24h * \\$3.50)\"}\n- Add inference ($0.0001-0.001/request serverless, or same hourly for dedicated)\n- Add storage (~$0.02/GB/month for checkpoints and datasets)\n- Total cost = training + inference + storage\n\nPhase 5: Cost Breakdown\nGoal: Present detailed cost analysis\n\nActions:\n- Display cost comparison table with columns: Platform | GPU Type | Training | Inference/month | Total\n- Example: Lambda Labs A100-40GB: $26.40 training, $20 inference, $46.40 total\n- Show percentage breakdown: GPU compute, storage, inference, network/egress\n- Identify cheapest option and note availability/reliability trade-offs\n\nPhase 6: Optimization Recommendations\nGoal: Suggest cost-saving strategies\n\nActions:\n- Recommend optimizations:\n  - LoRA/QLoRA: 4-8x savings | Reduce batch size: smaller GPU tier | Gradient checkpointing: trade speed for memory\n  - Spot instances: 30-70% savings | Cache preprocessed data | Mixed precision training | Dataset subsampling for experiments\n- Provide cost-optimized config suggestions and estimate savings per optimization\n\nPhase 7: Summary\nGoal: Present final cost estimate with actionable insights\n\nActions:\n- Display summary: Config path, model size/name, dataset samples, estimated GPU hours, recommended GPU type\n- Show best value: [platform] at $[cost] monthly (including inference)\n- List optimization opportunities: potential savings $XX/month (XX%), top 3 recommendations\n- Next steps: 1) Review estimate and budget 2) Apply optimizations 3) Set up monitoring 4) Deploy with /ml-training:deploy-training\n- Save estimate: !{bash mkdir -p estimates && echo \"Cost estimate saved to estimates/cost-estimate-$(date +%Y%m%d).txt\"}"
              },
              {
                "name": "/init",
                "description": "Initialize ML training project with cloud GPU setup",
                "path": "plugins/ml-training/commands/init.md",
                "frontmatter": {
                  "description": "Initialize ML training project with cloud GPU setup",
                  "argument-hint": [
                    "project-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up complete ML training project with Python environment, cloud GPU configuration, and proper directory structure optimized for Modal/cloud execution\n\nCore Principles:\n- Verify foundation tools first (Python, pip) using /foundation commands\n- Lightweight local dependencies (Modal CLI, datasets, supabase)\n- Heavy ML dependencies (torch, transformers) run on cloud GPU only\n- Clear separation between local and cloud environments\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Foundation Verification\n\nGoal: Ensure Python and required tools are available\n\nActions:\n- Parse $ARGUMENTS for project name (default: current directory name)\n- Run /foundation:detect to identify Python in project\n- Run /foundation:env-check to validate Python 3.9+ installed\n- If Python missing, report installation instructions and exit\n- Detect OS for environment activation: !{bash uname -s}\n\n## Phase 2: Environment Setup\n\nGoal: Create .env file and virtual environment\n\nActions:\n- Run /foundation:env-vars generate to create .env with:\n  - MODAL_TOKEN_ID, MODAL_TOKEN_SECRET\n  - ANTHROPIC_API_KEY, SUPABASE_URL, SUPABASE_ANON_KEY\n  - HUGGINGFACE_TOKEN, WANDB_API_KEY\n- Check if venv exists: !{bash test -d venv && echo \"exists\" || echo \"none\"}\n- If exists, ask user: Use existing, Delete and recreate, or Skip\n- Create venv: !{bash python3 -m venv venv}\n- Upgrade pip: !{bash ./venv/bin/pip install --upgrade pip}\n- Report activation command: Windows vs Linux/Mac\n\n## Phase 3: Dependencies\n\nGoal: Install lightweight local tools (NO heavy ML libraries)\n\nActions:\n- Create requirements-local.txt with Modal CLI, datasets, supabase, wandb, python-dotenv, tqdm\n- Install: !{bash ./venv/bin/pip install -r requirements-local.txt}\n- Create requirements-gpu.txt with torch, transformers, accelerate, bitsandbytes, peft, trl (cloud only)\n- Report: \"Local tools installed. GPU libraries install on Modal cloud.\"\n\n## Phase 4: Project Structure\n\nGoal: Create ML-specific directories and configuration\n\nActions:\n- Create directories:\n  - !{bash mkdir -p data/raw data/processed data/cache}\n  - !{bash mkdir -p models/checkpoints models/final}\n  - !{bash mkdir -p logs/training logs/evaluation}\n  - !{bash mkdir -p scripts notebooks}\n- Create .gitignore with patterns for: venv/, .env, data/*, models/*, logs/*, __pycache__/, .ipynb_checkpoints/, .modal_cache/, wandb/\n- Create README.md with:\n  - Setup instructions (activate venv, configure .env, authenticate Modal)\n  - Directory structure explanation\n  - Running training: modal run scripts/train.py\n  - Local vs Cloud dependencies distinction\n\n## Phase 5: Summary\n\nGoal: Display setup completion and next steps\n\nActions:\n- Show summary:\n  - Project: $ARGUMENTS initialized for ML training\n  - Python: {version} detected\n  - Virtual environment: Created at ./venv\n  - Local dependencies: Modal CLI, datasets, supabase installed\n  - GPU dependencies: Ready for cloud execution\n  - Directories: data/, models/, logs/, scripts/, notebooks/\n  - Environment: .env generated (fill in API keys)\n- Next steps:\n  1. Activate venv: source venv/bin/activate (Linux/Mac) or venv\\Scripts\\activate (Windows)\n  2. Fill in .env file with API keys\n  3. Authenticate Modal: modal token new\n  4. Test setup: modal app list\n  5. Create training script in scripts/train.py\n  6. Run on cloud GPU: modal run scripts/train.py\n- Warnings:\n  - Do NOT install torch/transformers locally\n  - Add large data files to .gitignore\n  - Never commit .env file"
              },
              {
                "name": "/integrate-supabase",
                "description": "Connect ML pipeline to Supabase storage",
                "path": "plugins/ml-training/commands/integrate-supabase.md",
                "frontmatter": {
                  "description": "Connect ML pipeline to Supabase storage",
                  "argument-hint": [
                    "table-name"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate ML training pipeline with Supabase for storing datasets, model metadata, training metrics, and predictions\n\nCore Principles:\n- Validate Supabase connection before creating schemas\n- Support flexible table naming for different ML workflows\n- Ensure data persistence for datasets, metadata, and predictions\n- Follow ML-specific schema patterns\n\nPhase 1: Discovery\nGoal: Understand integration requirements and current setup\n\nActions:\n- Parse $ARGUMENTS to extract table name\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - What is the primary table name for ML data? (default: ml_training)\n  - What data needs to be stored? (datasets/metadata/predictions/all)\n  - Are there existing Supabase tables to integrate with?\n  - What ML framework is being used? (tensorflow/pytorch/scikit-learn)\n- Check for existing Supabase configuration\n- Load project config if exists: @supabase/config.toml (if available)\n- Verify mcp__supabase is available: !{bash echo \"Checking Supabase MCP availability\"}\n\nPhase 2: Environment Check\nGoal: Verify Supabase connectivity and project structure\n\nActions:\n- Test Supabase connection using mcp__supabase\n- Check for existing ML training configuration\n- Look for data directory: !{bash ls -la data/ ml_data/ 2>/dev/null || echo \"No ML data directory found\"}\n- Check for Python dependencies: !{bash pip list | grep -E \"supabase|psycopg2|sqlalchemy\" || echo \"May need Supabase client\"}\n- Verify project has proper .env or config for Supabase credentials\n\nPhase 3: Schema Planning\nGoal: Design Supabase schema for ML pipeline\n\nActions:\n- Review ML workflow requirements\n- Identify required tables:\n  - Datasets table (data sources, versions, statistics)\n  - Training runs table (hyperparameters, metrics, timestamps)\n  - Models metadata table (versions, performance, deployment status)\n  - Predictions table (inference results, model used, timestamps)\n- Consider RLS policies for data security\n- Plan indexes for query performance\n\nPhase 4: Integration\nGoal: Create Supabase schemas and connect ML pipeline\n\nActions:\n\nTask(description=\"Create Supabase integration\", subagent_type=\"integration-specialist\", prompt=\"You are the integration-specialist agent. Set up Supabase storage for ML training pipeline using table name: $ARGUMENTS.\n\nIntegration Requirements:\n- Create comprehensive schema for ML data storage\n- Tables needed:\n  1. Datasets: id, name, source, format, rows, columns, created_at, metadata_json\n  2. Training_runs: id, dataset_id, model_type, hyperparameters_json, metrics_json, status, started_at, completed_at\n  3. Models: id, training_run_id, version, performance_metrics, artifact_path, deployed, created_at\n  4. Predictions: id, model_id, input_data_json, prediction_result_json, confidence, created_at\n\nUse mcp__supabase to:\n- Create tables with proper types and constraints\n- Set up foreign key relationships\n- Create indexes on frequently queried columns (dataset_id, model_id, created_at)\n- Configure RLS policies for secure access\n- Create views for common queries (latest models, active training runs)\n\nPython Integration:\n- Generate Python helper functions for CRUD operations\n- Create connection pooling configuration\n- Add data validation before inserts\n- Include batch insert utilities for predictions\n- Save to: ml_pipeline/supabase_client.py\n\nConfiguration:\n- Create or update .env.example with required variables\n- Document connection setup in README section\n- Save schema DDL to: supabase/migrations/ml_schema.sql\n\nExpected output:\n- Supabase tables created successfully\n- Python client code generated\n- Migration files saved\n- Integration guide with usage examples\")\n\nPhase 5: Verification\nGoal: Verify integration works correctly\n\nActions:\n- Check that migration file was created: @supabase/migrations/ml_schema.sql\n- Verify Python client exists: @ml_pipeline/supabase_client.py\n- Test connection with simple query via mcp__supabase\n- Run sanity check: !{bash python -c \"from ml_pipeline.supabase_client import test_connection; test_connection()\" 2>/dev/null || echo \"Manual verification recommended\"}\n- Confirm all tables were created with proper structure\n\nPhase 6: Summary\nGoal: Report integration setup results\n\nActions:\n- Display integration summary:\n  - Tables created and their purposes\n  - Python client location and key functions\n  - Migration files location\n  - Example usage for storing datasets\n  - Example usage for logging training runs\n  - Example usage for storing predictions\n- Provide next steps:\n  - Review supabase/migrations/ml_schema.sql\n  - Configure .env with Supabase credentials\n  - Test integration with: python ml_pipeline/supabase_client.py\n  - Use /ml-training:add-dataset to start loading data\n  - Set up RLS policies via Supabase dashboard if needed\n- Highlight security considerations:\n  - Keep Supabase credentials secure\n  - Configure RLS policies before production\n  - Use service role key only server-side"
              },
              {
                "name": "/monitor-training",
                "description": "Monitor active training jobs and display metrics",
                "path": "plugins/ml-training/commands/monitor-training.md",
                "frontmatter": {
                  "description": "Monitor active training jobs and display metrics",
                  "argument-hint": [
                    "job-id"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Monitor active ML training jobs on cloud platforms (Modal, AWS, GCP) and display real-time metrics including job status, GPU utilization, training progress, estimated completion time, and cost\n\nCore Principles:\n- Detect cloud platform from job ID format or configuration\n- Query platform API for real-time job status\n- Display metrics in clear, actionable format\n- Calculate cost estimates based on GPU hours\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Phase 1: Job Discovery\n\nGoal: Parse job ID and identify cloud platform\n\nActions:\n- Parse $ARGUMENTS for job ID\n- If no job ID provided, list all active jobs across platforms\n- Detect platform from job ID format:\n  - Modal: starts with \"ap-\" or use modal CLI\n  - AWS SageMaker: job-* format\n  - GCP Vertex AI: projects/* format\n- Check if Modal CLI available: !{bash which modal}\n- Load .env for API credentials if needed: !{bash test -f .env && echo \"found\" || echo \"missing\"}\n\n## Phase 2: Query Job Status\n\nGoal: Retrieve job information from cloud platform\n\nActions:\n- **Modal Platform**:\n  - List running apps: !{bash modal app list --json}\n  - Get specific job status: !{bash modal app logs $ARGUMENTS --raw}\n  - Parse JSON output for job metadata\n- **AWS SageMaker** (if detected):\n  - Query job: !{bash aws sagemaker describe-training-job --training-job-name $ARGUMENTS}\n  - Get CloudWatch metrics for GPU utilization\n- **GCP Vertex AI** (if detected):\n  - Query job: !{bash gcloud ai custom-jobs describe $ARGUMENTS}\n  - Get resource metrics\n- If platform not detected, report: \"Unable to detect platform from job ID: $ARGUMENTS\"\n\n## Phase 3: Metrics Display\n\nGoal: Format and display training metrics\n\nActions:\n- Extract key metrics from platform response:\n  - Job status (Running, Completed, Failed, Stopped)\n  - Start time and elapsed time\n  - GPU type and count (e.g., \"A100-40GB x2\")\n  - GPU utilization percentage\n  - Memory usage (GPU and system)\n  - Current training step/epoch\n  - Training loss (if logged)\n  - Estimated time remaining (if available)\n- Display metrics in formatted table:\n  ```\n  Job ID: {job-id}\n  Platform: {Modal|AWS|GCP}\n  Status: {status}\n  Duration: {hours}h {minutes}m\n  GPU: {type} x{count}\n  GPU Util: {percent}%\n  Memory: {used}GB / {total}GB\n  Progress: Step {current}/{total}\n  Loss: {value}\n  ETA: {hours}h {minutes}m\n  ```\n\n## Phase 4: Cost Calculation\n\nGoal: Estimate training costs based on GPU usage\n\nActions:\n- Calculate billable time from job start to current time\n- Apply platform pricing rates:\n  - Modal A100-40GB: $1.10/hour\n  - Modal A100-80GB: $2.21/hour\n  - AWS p4d.24xlarge (A100): $32.77/hour\n  - GCP a2-highgpu-1g (A100): $3.67/hour\n- Formula: cost = (GPU count) x (hours elapsed) x (rate per GPU hour)\n- Display cost estimate:\n  ```\n  Cost Estimate:\n  GPU Hours: {hours}\n  Rate: ${rate}/hour x {count} GPUs\n  Current Cost: ${amount}\n  Projected Total: ${projected} (if ETA available)\n  ```\n\n## Phase 5: Summary\n\nGoal: Display actionable information and next steps\n\nActions:\n- Show summary status:\n  - Job {job-id} is {Running|Completed|Failed}\n  - {elapsed} elapsed, {eta} remaining\n  - Current cost: ${amount}\n- If job is running:\n  - \"Use 'modal app logs {job-id}' to stream full logs\"\n  - \"Use 'modal app stop {job-id}' to terminate job\"\n- If job completed:\n  - \"Check models/checkpoints/ for saved model\"\n  - \"Review logs/training/ for metrics history\"\n- If job failed:\n  - \"Review error logs: modal app logs {job-id} --tail 50\"\n  - \"Check GPU memory errors or timeout issues\"\n- Refresh suggestion: \"Re-run this command to update metrics\""
              },
              {
                "name": "/optimize-training",
                "description": "Optimize training settings for cost and speed",
                "path": "plugins/ml-training/commands/optimize-training.md",
                "frontmatter": {
                  "description": "Optimize training settings for cost and speed",
                  "argument-hint": [
                    "config-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Optimize ML training configuration for cost efficiency and training speed by tuning batch size, mixed precision, gradient accumulation, and other performance settings.\n\nCore Principles:\n- Analyze before optimizing: Understand current configuration\n- Balance cost and performance: Find optimal trade-offs\n- Preserve model quality: Don't sacrifice accuracy for speed\n- Validate changes: Ensure configurations are valid\n\nPhase 1: Discovery\nGoal: Locate and validate training configuration\n\nActions:\n- Parse $ARGUMENTS for config file path\n- If no path provided, search for common config files:\n  !{bash find . -maxdepth 3 -type f \\( -name \"train*.py\" -o -name \"train*.yaml\" -o -name \"train*.json\" -o -name \"config*.yaml\" \\) 2>/dev/null | head -10}\n- Verify config file exists\n- Load current configuration: @$ARGUMENTS\n\nPhase 2: Analysis\nGoal: Understand current training setup and identify optimization opportunities\n\nActions:\n- Extract key training parameters from config:\n  - Batch size\n  - Learning rate\n  - Precision settings (fp32, fp16, bf16)\n  - Gradient accumulation steps\n  - Number of GPUs/devices\n  - Model architecture details\n- Identify GPU/accelerator type if specified\n- Calculate current estimated training time and cost\n\nPhase 3: Optimization Planning\nGoal: Determine optimal settings based on hardware and model\n\nActions:\n\nTask(description=\"Optimize training configuration\", subagent_type=\"cost-optimizer\", prompt=\"You are the cost-optimizer agent. Analyze and optimize the training configuration at: $ARGUMENTS\n\nCurrent Configuration Context:\n- Review the existing training configuration\n- Identify batch size, precision, gradient accumulation settings\n- Understand hardware constraints and model size\n\nOptimization Tasks:\n1. **Batch Size Optimization**:\n   - Calculate optimal batch size for available GPU memory\n   - Consider gradient accumulation for larger effective batch sizes\n   - Balance throughput vs memory usage\n\n2. **Mixed Precision Optimization**:\n   - Recommend fp16, bf16, or fp32 based on model and hardware\n   - Enable automatic mixed precision (AMP) if beneficial\n   - Consider gradient scaling for numerical stability\n\n3. **Gradient Accumulation**:\n   - Calculate optimal accumulation steps\n   - Balance effective batch size with training speed\n   - Recommend micro-batch strategies\n\n4. **Additional Optimizations**:\n   - Enable gradient checkpointing if memory-constrained\n   - Optimize data loading (num_workers, pin_memory, prefetch)\n   - Suggest compile optimizations (torch.compile, CUDA graphs)\n   - Recommend distributed training strategies if multi-GPU\n\n5. **Cost-Speed Trade-offs**:\n   - Estimate training time with new settings\n   - Calculate cost savings vs baseline\n   - Provide multiple configuration tiers (balanced, fast, economical)\n\nExpected Output:\n- Updated configuration file with optimized settings\n- Detailed optimization report showing:\n  - Changes made to each parameter\n  - Expected speedup and cost savings\n  - Trade-offs and recommendations\n  - Validation steps to verify improvements\")\n\nPhase 4: Validation\nGoal: Verify optimized configuration is valid and ready to use\n\nActions:\n- Check that updated config file exists\n- Verify syntax is correct (YAML/JSON/Python parsing)\n- Confirm all required parameters are present\n- Run basic validation if training script has a --validate flag:\n  !{bash python $ARGUMENTS --validate 2>&1 || echo \"No validation mode available\"}\n\nPhase 5: Summary\nGoal: Report optimization results and next steps\n\nActions:\n- Display optimization summary:\n  - Configuration file location\n  - Key changes made (batch size, precision, etc.)\n  - Expected performance improvements\n  - Estimated cost savings\n- Recommend next steps:\n  - Test with small dataset to validate settings\n  - Monitor GPU memory usage during initial runs\n  - Adjust if needed based on actual performance\n  - Consider A/B testing configurations\n- Provide command to start training with optimized config"
              },
              {
                "name": "/setup-framework",
                "description": "Configure training framework (HuggingFace/PyTorch Lightning/Ray)",
                "path": "plugins/ml-training/commands/setup-framework.md",
                "frontmatter": {
                  "description": "Configure training framework (HuggingFace/PyTorch Lightning/Ray)",
                  "argument-hint": "<huggingface|pytorch-lightning|ray>"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure ML training framework with dependencies, cloud image definitions, and config files\n\nCore Principles:\n- Validate framework choice before proceeding\n- Add dependencies to both local and cloud environments\n- Create framework-specific configuration files\n- Verify installation and imports work\n\nPhase 1: Validation\nGoal: Parse and validate framework argument\n\nActions:\n- Parse $ARGUMENTS for framework choice (huggingface, pytorch-lightning, or ray)\n- Validate framework is one of the supported options\n- If invalid or missing, display usage: \"Usage: /ml-training:setup-framework <huggingface|pytorch-lightning|ray>\"\n- Display which framework is being configured\n\nPhase 2: Discovery\nGoal: Understand current project setup\n\nActions:\n- Check for existing requirements files:\n  - !{bash ls requirements.txt pyproject.toml setup.py 2>/dev/null || echo \"none\"}\n- Check for existing cloud configuration:\n  - !{bash find . -name \"modal_*.py\" -o -name \"runpod_*.py\" -o -name \"lambda_*.py\" 2>/dev/null | head -5}\n- Load existing requirements if found:\n  - @requirements.txt (if exists)\n  - @pyproject.toml (if exists)\n\nPhase 3: Framework Configuration\nGoal: Configure framework dependencies and files\n\nActions:\n\nTask(description=\"Configure ML framework\", subagent_type=\"ml-architect\", prompt=\"You are the ml-architect agent. Configure $ARGUMENTS framework for ML training.\n\nFramework: $ARGUMENTS\n\nConfiguration Tasks:\n1. Add framework dependencies to requirements file:\n   - HuggingFace: transformers, datasets, accelerate, peft, bitsandbytes\n   - PyTorch Lightning: pytorch-lightning, torchmetrics, lightning-bolts\n   - Ray: ray[train], ray[tune], ray[data]\n\n2. Update cloud image definitions to include framework:\n   - Add pip install commands for cloud platforms (Modal/RunPod/Lambda)\n   - Ensure GPU-compatible versions specified\n   - Pin versions for reproducibility\n\n3. Create framework configuration file:\n   - HuggingFace: Create training_config.yaml with model, dataset, training args\n   - PyTorch Lightning: Create lightning_config.yaml with trainer config\n   - Ray: Create ray_config.yaml with scaling config and compute resources\n\n4. Add example training script stub if none exists:\n   - Framework-specific training loop template\n   - Cloud deployment wrapper\n\nDeliverable: All files updated/created with framework dependencies and configs\")\n\nPhase 4: Verification\nGoal: Verify framework installation works\n\nActions:\n- Attempt to import framework to verify installation:\n  - HuggingFace: !{bash python -c \"import transformers; print(f'transformers version: {transformers.__version__}')\" 2>&1 || echo \"Install with: pip install -r requirements.txt\"}\n  - PyTorch Lightning: !{bash python -c \"import pytorch_lightning; print(f'pytorch-lightning version: {pytorch_lightning.__version__}')\" 2>&1 || echo \"Install with: pip install -r requirements.txt\"}\n  - Ray: !{bash python -c \"import ray; print(f'ray version: {ray.__version__}')\" 2>&1 || echo \"Install with: pip install -r requirements.txt\"}\n- List files created/modified:\n  - !{bash ls -lh requirements.txt *_config.yaml train_*.py 2>/dev/null}\n\nPhase 5: Summary\nGoal: Report configuration results\n\nActions:\n- Display what was configured:\n  - Framework name and version\n  - Files created or updated\n  - Dependencies added\n  - Configuration files created\n- Show next steps:\n  - \"Install dependencies: pip install -r requirements.txt\"\n  - \"Review config file: cat <framework>_config.yaml\"\n  - \"Test cloud deployment: /ml-training:deploy-cloud <modal|runpod|lambda>\"\n- Note any warnings or issues from verification phase"
              },
              {
                "name": "/test",
                "description": "Test ML components (data/training/inference)",
                "path": "plugins/ml-training/commands/test.md",
                "frontmatter": {
                  "description": "Test ML components (data/training/inference)",
                  "argument-hint": [
                    "component"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Run comprehensive tests on ML components (data pipelines, training jobs, inference endpoints)\n\nCore Principles:\n- Test data quality, training correctness, and inference accuracy\n- Run independent tests in parallel for speed\n- Provide detailed test reports with pass/fail status\n- Track test progress and results\n\nPhase 1: Discovery\nGoal: Identify what components to test\n\nActions:\n- Create test progress tracker using TodoWrite\n- Parse $ARGUMENTS to determine test scope (all, data, training, inference, or specific component)\n- Detect project structure and ML framework\n- Example: !{bash ls -la data/ models/ 2>/dev/null}\n- Load ML configuration if exists\n- Example: @ml-config.yaml or @config/ml-training.json\n- Identify test targets and scope\n- Update todos\n\nPhase 2: Test Environment Check\nGoal: Verify test prerequisites\n\nActions:\n- Check if test data exists: !{bash test -d data/test && echo \"Found\" || echo \"Missing\"}\n- Verify model checkpoints available if needed\n- Check inference endpoints if testing inference\n- Load test configurations and expected outputs\n- Validate environment variables and dependencies\n- Update todos\n\nPhase 3: Parallel Test Execution\nGoal: Run multiple test suites simultaneously\n\nActions:\n\nRun the following test agents IN PARALLEL (all at once):\n\nTask(description=\"Data Quality Tests\", subagent_type=\"ml-tester\", prompt=\"You are the ml-tester agent focused on data testing. Run data quality tests for $ARGUMENTS.\n\nFocus on:\n- Data schema validation\n- Data integrity checks\n- Preprocessing pipeline correctness\n- Train/val/test split verification\n- Data augmentation validation\n\nDeliverable: Data test report with pass/fail status, coverage metrics, and any issues found\")\n\nTask(description=\"Training Tests\", subagent_type=\"ml-tester\", prompt=\"You are the ml-tester agent focused on training validation. Run training tests for $ARGUMENTS.\n\nFocus on:\n- Model architecture validation\n- Training loop correctness\n- Loss calculation accuracy\n- Gradient flow verification\n- Checkpoint saving/loading\n- Hyperparameter configuration\n\nDeliverable: Training test report with pass/fail status, model metrics, and any issues found\")\n\nTask(description=\"Inference Tests\", subagent_type=\"ml-tester\", prompt=\"You are the ml-tester agent focused on inference validation. Run inference tests for $ARGUMENTS.\n\nFocus on:\n- Model loading correctness\n- Prediction accuracy\n- Input/output format validation\n- Performance benchmarks\n- Edge case handling\n- API endpoint health (if applicable)\n\nDeliverable: Inference test report with pass/fail status, accuracy metrics, latency, and any issues found\")\n\nWait for ALL test agents to complete before proceeding.\n\nUpdate todos as each test suite completes.\n\nPhase 4: Results Consolidation\nGoal: Analyze and combine test results\n\nActions:\n- Review all test agent outputs\n- Identify critical failures (blocking issues)\n- Categorize warnings and recommendations\n- Calculate overall test coverage and pass rate\n- Cross-reference findings for validation\n- Update todos\n\nPhase 5: Summary\nGoal: Present comprehensive test results\n\nActions:\n- Mark all todos complete\n- Present consolidated test report:\n\n  **Data Tests**:\n  - Pass/Fail status\n  - Coverage metrics\n  - Issues found\n\n  **Training Tests**:\n  - Pass/Fail status\n  - Model metrics\n  - Issues found\n\n  **Inference Tests**:\n  - Pass/Fail status\n  - Accuracy/latency metrics\n  - Issues found\n\n  **Overall Status**:\n  - Total tests run\n  - Pass rate\n  - Critical issues (high priority fixes needed)\n  - Warnings and recommendations\n  - Suggested next steps\n\n- If failures detected, provide debugging guidance\n- Recommend specific fixes or improvements"
              },
              {
                "name": "/validate-data",
                "description": "Validate training data quality and format",
                "path": "plugins/ml-training/commands/validate-data.md",
                "frontmatter": {
                  "description": "Validate training data quality and format",
                  "argument-hint": [
                    "dataset-path"
                  ]
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n## Available Skills\n\nThis commands has access to the following skills from the ml-training plugin:\n\n- **cloud-gpu-configs**: Platform-specific configuration templates for Modal, Lambda Labs, and RunPod with GPU selection guides\n- **cost-calculator**: Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.\n- **example-projects**: Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.\n- **integration-helpers**: Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.\n- **monitoring-dashboard**: Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.\n- **training-patterns**: Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.\n- **validation-scripts**: Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n**Arguments**: $ARGUMENTS\n\nGoal: Validate machine learning training data for quality, format consistency, and structural integrity, providing comprehensive statistics and issue reporting.\n\nCore Principles:\n- Detect data characteristics, don't assume format\n- Validate comprehensively across multiple dimensions\n- Provide actionable feedback with clear statistics\n- Support common ML data formats (CSV, JSON, Parquet, NPY)\n\nPhase 1: Discovery\nGoal: Parse arguments and detect dataset format\n\nActions:\n- Extract dataset path from $ARGUMENTS\n- Verify dataset exists and is accessible\n- Detect file format based on extension\n- Example: !{bash test -e \"$ARGUMENTS\" && echo \"Found: $ARGUMENTS\" || echo \"Error: Dataset not found\"}\n- List dataset files if directory provided\n- Example: !{bash if [ -d \"$ARGUMENTS\" ]; then ls -lh \"$ARGUMENTS\"; fi}\n\nPhase 2: Format Detection\nGoal: Identify dataset type and structure\n\nActions:\n- Determine file type (CSV, JSON, Parquet, NPY, etc.)\n- Check if single file or directory of files\n- Example: !{bash file \"$ARGUMENTS\"}\n- For CSV: Detect delimiter, quote character, header presence\n- For JSON: Check if JSONL, nested, or array format\n- Report detected format to user\n\nPhase 3: Data Validation\nGoal: Run comprehensive validation checks\n\nActions:\n\n**For CSV files:**\n- Check header row exists and is valid\n- Count rows and columns\n- Example: !{bash head -1 \"$ARGUMENTS\" && wc -l \"$ARGUMENTS\"}\n- Detect column data types\n- Check for missing values per column\n- Identify duplicate rows\n- Validate delimiter consistency\n- Example: !{bash awk -F',' '{print NF}' \"$ARGUMENTS\" | sort -u}\n\n**For JSON files:**\n- Validate JSON syntax\n- Example: !{bash python3 -m json.tool \"$ARGUMENTS\" > /dev/null && echo \"Valid JSON\" || echo \"Invalid JSON\"}\n- Check schema consistency across records\n- Count total records\n- Identify missing or null fields\n\n**For Parquet files:**\n- Use Python to read and validate\n- Example: !{bash python3 -c \"import pandas as pd; df=pd.read_parquet('$ARGUMENTS'); print(f'Rows: {len(df)}, Cols: {len(df.columns)}')\"}\n- Check schema and data types\n- Report compression and size\n\n**For NumPy files:**\n- Validate array shape and dtype\n- Example: !{bash python3 -c \"import numpy as np; arr=np.load('$ARGUMENTS'); print(f'Shape: {arr.shape}, Dtype: {arr.dtype}')\"}\n- Check for NaN or inf values\n\nPhase 4: Quality Checks\nGoal: Assess data quality metrics\n\nActions:\n- Calculate basic statistics (min, max, mean for numeric columns)\n- Identify outliers or anomalous values\n- Check class distribution for classification datasets\n- Detect data imbalance issues\n- Validate value ranges are reasonable\n- Check file size and memory requirements\n- Example: !{bash du -h \"$ARGUMENTS\"}\n\nPhase 5: Issue Reporting\nGoal: Summarize validation results and flag problems\n\nActions:\n- Report dataset statistics:\n  - Total records/rows\n  - Number of features/columns\n  - Data types per column\n  - Missing value counts\n  - File size\n- Flag critical issues:\n  - Format errors\n  - Missing values exceeding threshold\n  - Duplicate records\n  - Class imbalance\n  - Invalid data types\n- Provide warnings for potential problems:\n  - Small dataset size\n  - High missing value percentage\n  - Inconsistent formats\n- Suggest fixes for identified issues\n\nPhase 6: Summary\nGoal: Present comprehensive validation report\n\nActions:\n- Display validation summary:\n  - Dataset: [path]\n  - Format: [detected format]\n  - Status: PASS/FAIL\n  - Records: [count]\n  - Features: [count]\n  - Issues Found: [count]\n- List all issues with severity (CRITICAL/WARNING/INFO)\n- Recommend next steps:\n  - If PASS: Ready for training\n  - If FAIL: List required fixes\n  - Suggest data cleaning commands if applicable\n- Save validation report to file if requested\n- Example: !{bash echo \"Validation complete for $ARGUMENTS\"}"
              }
            ],
            "skills": [
              {
                "name": "cost-calculator",
                "description": "Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.",
                "path": "plugins/ml-training/skills/cost-calculator/SKILL.md",
                "frontmatter": {
                  "name": "cost-calculator",
                  "description": "Cost estimation scripts and tools for calculating GPU hours, training costs, and inference pricing across Modal, Lambda Labs, and RunPod platforms. Use when estimating ML training costs, comparing platform pricing, calculating GPU hours, budgeting for ML projects, or when user mentions cost estimation, pricing comparison, GPU budgeting, training cost analysis, or inference cost optimization.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep, Edit"
                },
                "content": "# ML Training Cost Calculator\n\n**Purpose:** Provide production-ready cost estimation tools for ML training and inference across cloud GPU platforms (Modal, Lambda Labs, RunPod).\n\n**Activation Triggers:**\n- Estimating training costs for ML models\n- Comparing GPU platform pricing\n- Calculating GPU hours for training jobs\n- Budgeting for ML projects\n- Optimizing inference costs\n- Evaluating cost-effectiveness of different GPU types\n- Planning resource allocation\n\n**Key Resources:**\n- `scripts/estimate-training-cost.sh` - Calculate training costs based on model size, data, GPU type\n- `scripts/estimate-inference-cost.sh` - Estimate inference costs for production workloads\n- `scripts/calculate-gpu-hours.sh` - Convert training parameters to GPU hours\n- `scripts/compare-platforms.sh` - Compare costs across Modal, Lambda, RunPod\n- `templates/cost-breakdown.json` - Structured cost breakdown template\n- `templates/platform-pricing.yaml` - Up-to-date platform pricing data\n- `examples/training-cost-estimate.md` - Example training cost calculation\n- `examples/inference-cost-estimate.md` - Example inference cost analysis\n\n## Platform Pricing Overview\n\n### Modal (Serverless - Pay Per Second)\n\n**GPU Options:**\n- **T4**: $0.000164/sec ($0.59/hr) - Development, small models\n- **L4**: $0.000222/sec ($0.80/hr) - Cost-effective training\n- **A10**: $0.000306/sec ($1.10/hr) - Mid-range training\n- **A100 40GB**: $0.000583/sec ($2.10/hr) - Large model training\n- **A100 80GB**: $0.000694/sec ($2.50/hr) - Very large models\n- **H100**: $0.001097/sec ($3.95/hr) - Cutting-edge training\n- **H200**: $0.001261/sec ($4.54/hr) - Latest generation\n- **B200**: $0.001736/sec ($6.25/hr) - Maximum performance\n\n**Free Credits:**\n- Starter: $30/month free\n- Startup credits: Up to $50,000 FREE\n\n### Lambda Labs (On-Demand Hourly)\n\n**Single GPU:**\n- **1x A10**: $0.31/hr - Cheapest single GPU option\n- **1x V100 16GB**: $0.55/hr - Most affordable multi-GPU base\n\n**8x GPU Clusters:**\n- **8x V100**: $4.40/hr ($0.55/GPU) - Most affordable multi-GPU\n- **8x A100 40GB**: $10.32/hr ($1.29/GPU)\n- **8x A100 80GB**: $14.32/hr ($1.79/GPU)\n- **8x H100**: $23.92/hr ($2.99/GPU)\n\n### RunPod (Serverless - Pay Per Minute)\n\n**Key Features:**\n- Pay-per-minute billing\n- FlashBoot <200ms cold-starts\n- Zero egress fees on storage\n- 30+ GPU SKUs available\n\n## Cost Estimation Scripts\n\n### 1. Estimate Training Cost\n\n**Script:** `scripts/estimate-training-cost.sh`\n\n**Usage:**\n```bash\nbash scripts/estimate-training-cost.sh \\\n  --model-size 7B \\\n  --dataset-size 10000 \\\n  --epochs 3 \\\n  --gpu t4 \\\n  --platform modal\n```\n\n**Parameters:**\n- `--model-size`: Model size (125M, 350M, 1B, 3B, 7B, 13B, 70B)\n- `--dataset-size`: Number of training samples\n- `--epochs`: Number of training epochs\n- `--batch-size`: Training batch size (default: auto-calculated)\n- `--gpu`: GPU type (t4, a10, a100-40gb, a100-80gb, h100)\n- `--platform`: Cloud platform (modal, lambda, runpod)\n- `--peft`: Use PEFT/LoRA (yes/no, default: no)\n- `--mixed-precision`: Use FP16/BF16 (yes/no, default: yes)\n\n**Output:**\n```json\n{\n  \"model\": \"7B\",\n  \"dataset_size\": 10000,\n  \"epochs\": 3,\n  \"gpu\": \"T4\",\n  \"platform\": \"Modal\",\n  \"estimated_hours\": 4.2,\n  \"cost_breakdown\": {\n    \"compute_cost\": 2.48,\n    \"storage_cost\": 0.05,\n    \"total_cost\": 2.53\n  },\n  \"cost_optimizations\": {\n    \"with_peft\": 1.26,\n    \"savings_percentage\": 50\n  },\n  \"alternative_platforms\": {\n    \"lambda_a10\": 1.30,\n    \"runpod_t4\": 2.40\n  }\n}\n```\n\n**Calculation Methodology:**\n- Estimates tokens per sample (avg 500 tokens)\n- Calculates total training tokens\n- Applies throughput rates per GPU type\n- Accounts for PEFT (90% memory reduction)\n- Accounts for mixed precision (2x speedup)\n\n### 2. Estimate Inference Cost\n\n**Script:** `scripts/estimate-inference-cost.sh`\n\n**Usage:**\n```bash\nbash scripts/estimate-inference-cost.sh \\\n  --requests-per-day 1000 \\\n  --avg-latency 2 \\\n  --gpu t4 \\\n  --platform modal \\\n  --deployment serverless\n```\n\n**Parameters:**\n- `--requests-per-day`: Expected daily requests\n- `--avg-latency`: Average inference time (seconds)\n- `--gpu`: GPU type\n- `--platform`: Cloud platform\n- `--deployment`: Deployment type (serverless, dedicated)\n- `--batch-inference`: Batch requests (yes/no, default: no)\n\n**Output:**\n```json\n{\n  \"requests_per_day\": 1000,\n  \"requests_per_month\": 30000,\n  \"avg_latency_sec\": 2,\n  \"gpu\": \"T4\",\n  \"platform\": \"Modal Serverless\",\n  \"cost_breakdown\": {\n    \"daily_compute_seconds\": 2000,\n    \"daily_cost\": 0.33,\n    \"monthly_cost\": 9.90,\n    \"cost_per_request\": 0.00033\n  },\n  \"scaling_analysis\": {\n    \"requests_10k_day\": 99.00,\n    \"requests_100k_day\": 990.00\n  },\n  \"dedicated_alternative\": {\n    \"monthly_cost\": 442.50,\n    \"break_even_requests_day\": 4500\n  }\n}\n```\n\n### 3. Calculate GPU Hours\n\n**Script:** `scripts/calculate-gpu-hours.sh`\n\n**Usage:**\n```bash\nbash scripts/calculate-gpu-hours.sh \\\n  --model-params 7B \\\n  --tokens-total 30M \\\n  --gpu a100-40gb\n```\n\n**Parameters:**\n- `--model-params`: Model parameters (125M, 350M, 1B, 3B, 7B, 13B, 70B)\n- `--tokens-total`: Total training tokens\n- `--gpu`: GPU type\n- `--peft`: Use PEFT (yes/no)\n- `--multi-gpu`: Number of GPUs (default: 1)\n\n**GPU Throughput Benchmarks:**\n```\nT4 (16GB):\n  - 7B full fine-tune: 150 tokens/sec\n  - 7B with PEFT: 600 tokens/sec\n\nA100 40GB:\n  - 7B full fine-tune: 800 tokens/sec\n  - 7B with PEFT: 3200 tokens/sec\n  - 13B with PEFT: 1600 tokens/sec\n\nA100 80GB:\n  - 13B full fine-tune: 600 tokens/sec\n  - 70B with PEFT: 400 tokens/sec\n\nH100:\n  - 70B with PEFT: 1200 tokens/sec\n```\n\n### 4. Compare Platforms\n\n**Script:** `scripts/compare-platforms.sh`\n\n**Usage:**\n```bash\nbash scripts/compare-platforms.sh \\\n  --training-hours 4 \\\n  --gpu-type a100-40gb\n```\n\n**Output:**\n```markdown\n# Platform Cost Comparison\n\n## Training Job: 4 hours on A100 40GB\n\n| Platform | GPU Cost | Egress Fees | Total | Notes |\n|----------|----------|-------------|-------|-------|\n| Modal | $8.40 | $0.00 | $8.40 | Serverless, pay-per-second |\n| Lambda | $5.16 | $0.00 | $5.16 | Cheapest for dedicated |\n| RunPod | $8.00 | $0.00 | $8.00 | Pay-per-minute |\n\n## Winner: Lambda Labs ($5.16)\n\nSavings: $3.24 (38.6% vs Modal)\n\nRecommendation: Use Lambda for long-running dedicated training, Modal for\nserverless/bursty workloads.\n```\n\n## Cost Templates\n\n### Cost Breakdown Template\n\n**Template:** `templates/cost-breakdown.json`\n\n```json\n{\n  \"project_name\": \"ML Training Project\",\n  \"cost_estimate\": {\n    \"training\": {\n      \"model_size\": \"7B\",\n      \"training_runs\": 4,\n      \"hours_per_run\": 4.2,\n      \"gpu_type\": \"T4\",\n      \"platform\": \"Modal\",\n      \"cost_per_run\": 2.48,\n      \"total_training_cost\": 9.92\n    },\n    \"inference\": {\n      \"deployment_type\": \"serverless\",\n      \"expected_requests_month\": 30000,\n      \"gpu_type\": \"T4\",\n      \"platform\": \"Modal\",\n      \"monthly_cost\": 9.90\n    },\n    \"storage\": {\n      \"model_artifacts_gb\": 14,\n      \"dataset_storage_gb\": 5,\n      \"monthly_storage_cost\": 0.50\n    },\n    \"total_monthly_cost\": 20.32,\n    \"breakdown_percentage\": {\n      \"training\": 48.8,\n      \"inference\": 48.7,\n      \"storage\": 2.5\n    }\n  },\n  \"cost_optimizations_applied\": {\n    \"peft_lora\": \"50% training cost reduction\",\n    \"mixed_precision\": \"2x faster training\",\n    \"serverless_inference\": \"Pay only for actual usage\",\n    \"batch_inference\": \"Up to 10x reduction in inference cost\"\n  },\n  \"potential_savings\": {\n    \"without_optimizations\": 45.00,\n    \"with_optimizations\": 20.32,\n    \"total_savings\": 24.68,\n    \"savings_percentage\": 54.8\n  }\n}\n```\n\n### Platform Pricing Data\n\n**Template:** `templates/platform-pricing.yaml`\n\n```yaml\nplatforms:\n  modal:\n    billing: per-second\n    free_credits: 30  # USD per month\n    startup_credits: 50000  # USD for eligible startups\n    gpus:\n      t4:\n        price_per_sec: 0.000164\n        price_per_hour: 0.59\n        vram_gb: 16\n      a100_40gb:\n        price_per_sec: 0.000583\n        price_per_hour: 2.10\n        vram_gb: 40\n      h100:\n        price_per_sec: 0.001097\n        price_per_hour: 3.95\n        vram_gb: 80\n\n  lambda:\n    billing: per-hour\n    free_credits: 0\n    minimum_billing: 1-hour\n    gpus:\n      a10_1x:\n        price_per_hour: 0.31\n        vram_gb: 24\n      a100_40gb_1x:\n        price_per_hour: 1.29\n        vram_gb: 40\n      a100_40gb_8x:\n        price_per_hour: 10.32\n        total_vram_gb: 320\n\n  runpod:\n    billing: per-minute\n    free_credits: 0\n    features:\n      - zero_egress_fees\n      - flashboot_200ms\n    gpus:\n      t4:\n        price_per_hour: 0.60  # Approximate\n        vram_gb: 16\n```\n\n## Cost Estimation Examples\n\n### Example 1: Training 7B Model\n\n**File:** `examples/training-cost-estimate.md`\n\n**Scenario:**\n- Model: Llama 2 7B fine-tuning\n- Dataset: 10,000 samples (5M tokens)\n- Epochs: 3\n- Total tokens: 15M\n- Method: LoRA/PEFT\n\n**Cost Calculation:**\n\n```bash\nbash scripts/estimate-training-cost.sh \\\n  --model-size 7B \\\n  --dataset-size 10000 \\\n  --epochs 3 \\\n  --gpu t4 \\\n  --platform modal \\\n  --peft yes\n```\n\n**Results:**\n```\nTraining Time: 4.2 hours\nModal T4 Cost: $2.48\nAlternative (Lambda A10): $1.30 (47% cheaper)\n\nOptimization Impact:\n- Without PEFT: $12.40 (5x more expensive)\n- With PEFT: $2.48\n- Savings: $9.92 (80%)\n```\n\n**Recommendation:** Use Lambda A10 for cheapest option, or Modal T4 for\nserverless convenience.\n\n### Example 2: Production Inference\n\n**File:** `examples/inference-cost-estimate.md`\n\n**Scenario:**\n- Model: Custom 7B classifier\n- Expected traffic: 1,000 requests/day\n- Avg latency: 2 seconds per request\n- Growth: 10x in 6 months\n\n**Cost Calculation:**\n\n```bash\nbash scripts/estimate-inference-cost.sh \\\n  --requests-per-day 1000 \\\n  --avg-latency 2 \\\n  --gpu t4 \\\n  --platform modal \\\n  --deployment serverless\n```\n\n**Current (1K requests/day):**\n```\nServerless Modal T4:\n- Daily cost: $0.33\n- Monthly cost: $9.90\n- Cost per request: $0.00033\n\nDedicated Lambda A10:\n- Monthly cost: $223 (24/7 instance)\n- Break-even: 2,250 requests/day\n- Not recommended for current traffic\n```\n\n**After Growth (10K requests/day):**\n```\nServerless Modal T4:\n- Monthly cost: $99.00\n- Still cost-effective\n\nDedicated Lambda A10:\n- Monthly cost: $223\n- Break-even reached at 2,250 requests/day\n- Recommendation: Stay serverless until 10K+ daily\n```\n\n## Cost Optimization Strategies\n\n### 1. Use PEFT/LoRA\n\n**Savings:** 50-90% training cost reduction\n\n```bash\n# Calculate savings\nbash scripts/estimate-training-cost.sh --model-size 7B --peft no\n# Cost: $12.40\n\nbash scripts/estimate-training-cost.sh --model-size 7B --peft yes\n# Cost: $2.48\n# Savings: $9.92 (80%)\n```\n\n### 2. Mixed Precision Training\n\n**Savings:** 2x faster training, 50% cost reduction\n\nAutomatically enabled in cost estimations with `--mixed-precision yes`\n\n### 3. Platform Selection\n\n**Use Case Guidelines:**\n\n```bash\n# Short jobs (<1 hour): Modal serverless\nbash scripts/compare-platforms.sh --training-hours 0.5 --gpu-type t4\n# Winner: Modal ($0.30 vs Lambda $0.31 minimum)\n\n# Long jobs (4+ hours): Lambda dedicated\nbash scripts/compare-platforms.sh --training-hours 4 --gpu-type a100-40gb\n# Winner: Lambda ($5.16 vs Modal $8.40)\n\n# Variable workloads: Modal serverless\n# Pay only for actual usage, no idle cost\n```\n\n### 4. Batch Inference\n\n**Savings:** Up to 10x reduction in inference cost\n\n```bash\n# Single inference\nbash scripts/estimate-inference-cost.sh \\\n  --requests-per-day 1000 \\\n  --avg-latency 2 \\\n  --batch-inference no\n# Cost: $9.90/month\n\n# Batch inference (10 requests per batch)\nbash scripts/estimate-inference-cost.sh \\\n  --requests-per-day 1000 \\\n  --avg-latency 0.3 \\\n  --batch-inference yes\n# Cost: $1.49/month\n# Savings: $8.41 (85%)\n```\n\n## Quick Reference: Cost Per Use Case\n\n### Small Model Training (< 1B params)\n- **Best GPU:** T4\n- **Best Platform:** Modal (serverless)\n- **Typical Cost:** $0.50-$2.00 per run\n- **Time:** 30 min - 2 hours\n\n### Medium Model Training (1B-7B params)\n- **Best GPU:** T4 (with PEFT) or A100 40GB\n- **Best Platform:** Lambda A10 (cheapest) or Modal T4 (convenience)\n- **Typical Cost:** $1.00-$8.00 per run\n- **Time:** 2-8 hours\n\n### Large Model Training (7B-70B params)\n- **Best GPU:** A100 80GB or H100 (with PEFT)\n- **Best Platform:** Lambda (dedicated) or Modal (serverless)\n- **Typical Cost:** $10-$100 per run\n- **Time:** 8-48 hours\n\n### Low-Traffic Inference (<1K requests/day)\n- **Best Deployment:** Modal serverless\n- **Best GPU:** T4\n- **Typical Cost:** $5-$15/month\n\n### High-Traffic Inference (>10K requests/day)\n- **Best Deployment:** Dedicated or batch serverless\n- **Best GPU:** A10 or A100\n- **Typical Cost:** $100-$500/month\n\n## Dependencies\n\n**Required for scripts:**\n```bash\n# Bash 4.0+ (for associative arrays)\nbash --version\n\n# jq (for JSON processing)\nsudo apt-get install jq\n\n# bc (for floating-point calculations)\nsudo apt-get install bc\n\n# yq (for YAML processing)\npip install yq\n```\n\n## Best Practices Summary\n\n1. **Always estimate before training** - Use cost scripts to avoid surprises\n2. **Use PEFT for large models** - 50-90% cost savings\n3. **Enable mixed precision** - 2x speedup with no quality loss\n4. **Choose platform based on workload:**\n   - Modal: Serverless, short jobs, variable workloads\n   - Lambda: Long-running, dedicated, multi-GPU\n   - RunPod: Per-minute billing flexibility\n5. **Batch inference when possible** - Up to 10x cost reduction\n6. **Apply for startup credits** - Modal offers $50K free\n7. **Monitor actual costs** - Compare estimates to actuals, optimize\n8. **Use smallest viable GPU** - T4 often sufficient with PEFT\n\n---\n\n**Supported Platforms:** Modal, Lambda Labs, RunPod\n**GPU Types:** T4, L4, A10, A100 (40GB/80GB), H100, H200, B200\n**Output Format:** JSON cost breakdowns and markdown reports\n**Version:** 1.0.0"
              },
              {
                "name": "example-projects",
                "description": "Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.",
                "path": "plugins/ml-training/skills/example-projects/SKILL.md",
                "frontmatter": {
                  "name": "example-projects",
                  "description": "Provides three production-ready ML training examples (sentiment classification, text generation, RedAI trade classifier) with complete training scripts, deployment configs, and datasets. Use when user needs example projects, reference implementations, starter templates, or wants to see working code for sentiment analysis, text generation, or financial trade classification.",
                  "allowed-tools": "Read, Bash, Write, Edit, Grep, Glob"
                },
                "content": "# ML Training Example Projects\n\n**Purpose:** Provide complete, runnable example projects demonstrating ML training workflows from data preparation through deployment.\n\n**Activation Triggers:**\n- User requests example projects or starter templates\n- User wants to see working sentiment classification code\n- User needs text generation training examples\n- User mentions RedAI trade classifier\n- User wants reference implementations\n- User needs to understand complete training workflows\n\n**Key Resources:**\n- `scripts/setup-example.sh` - Initialize and setup any example project\n- `scripts/run-training.sh` - Execute training for any example\n- `scripts/test-inference.sh` - Test trained models\n- `examples/sentiment-classification/` - Binary sentiment classification (IMDB-style)\n- `examples/text-generation/` - GPT-style text generation with LoRA\n- `examples/redai-trade-classifier/` - Financial trade classification with Modal deployment\n- `templates/` - Scaffolding for new projects\n\n## Available Example Projects\n\n### 1. Sentiment Classification\n\n**Use Case:** Binary sentiment analysis (positive/negative reviews)\n\n**Features:**\n- DistilBERT fine-tuning for text classification\n- Custom dataset loading from JSON\n- Training with validation metrics\n- Model saving and inference\n- Production-ready inference API\n\n**Files:**\n- `train.py` - Complete training script\n- `data.json` - Sample training data (50 examples)\n- `inference.py` - Inference server\n- `README.md` - Setup and usage guide\n\n**Dataset Format:**\n```json\n{\"text\": \"This movie was amazing!\", \"label\": 1}\n{\"text\": \"Terrible waste of time\", \"label\": 0}\n```\n\n### 2. Text Generation\n\n**Use Case:** Fine-tune GPT-2 for custom text generation\n\n**Features:**\n- GPT-2 small model fine-tuning\n- LoRA (Low-Rank Adaptation) for efficient training\n- Custom tokenization\n- Generation with temperature/top-p sampling\n- Modal deployment configuration\n\n**Files:**\n- `train.py` - LoRA training script\n- `config.yaml` - Hyperparameters and model config\n- `generate.py` - Text generation script\n- `modal_deploy.py` - Modal deployment\n- `README.md` - Complete guide\n\n**Config Structure:**\n```yaml\nmodel:\n  name: gpt2\n  max_length: 512\ntraining:\n  epochs: 3\n  batch_size: 4\n  learning_rate: 2e-4\nlora:\n  r: 8\n  alpha: 16\n  dropout: 0.1\n```\n\n### 3. RedAI Trade Classifier\n\n**Use Case:** Financial trade classification (buy/sell/hold)\n\n**Features:**\n- Multi-class classification for trading signals\n- Feature engineering from market data\n- Class imbalance handling\n- Modal deployment for production inference\n- Real-time prediction API\n\n**Files:**\n- `train.py` - Training with class weighting\n- `modal_deploy.py` - Complete Modal deployment\n- `data_preprocessing.py` - Feature engineering\n- `README.md` - Trading strategy guide\n\n**Model Input:**\n- Price features (open, high, low, close)\n- Volume indicators\n- Technical indicators (RSI, MACD, moving averages)\n- Sentiment scores\n\n## Quick Start\n\n### Setup Any Example\n\n```bash\n# Initialize example project\n./scripts/setup-example.sh <project-name>\n\n# Options: sentiment-classification, text-generation, redai-trade-classifier\n./scripts/setup-example.sh sentiment-classification\n```\n\n**What it does:**\n- Creates project directory\n- Copies example files\n- Installs dependencies\n- Downloads/prepares sample data\n- Validates environment\n\n### Run Training\n\n```bash\n# Train model for any example\n./scripts/run-training.sh <project-name>\n\n# Examples:\n./scripts/run-training.sh sentiment-classification\n./scripts/run-training.sh text-generation\n./scripts/run-training.sh redai-trade-classifier\n```\n\n**Monitors:**\n- Training progress\n- Loss curves\n- Validation metrics\n- GPU utilization\n- Checkpoint saving\n\n### Test Inference\n\n```bash\n# Test trained model\n./scripts/test-inference.sh <project-name> <input>\n\n# Examples:\n./scripts/test-inference.sh sentiment-classification \"This product is great!\"\n./scripts/test-inference.sh text-generation \"Once upon a time\"\n./scripts/test-inference.sh redai-trade-classifier market_data.json\n```\n\n## Common Workflows\n\n### Start From Example Template\n\n1. **Choose example** based on use case:\n   - Classification ‚Üí sentiment-classification\n   - Generation ‚Üí text-generation\n   - Financial ML ‚Üí redai-trade-classifier\n\n2. **Setup project:**\n   ```bash\n   ./scripts/setup-example.sh <example-name>\n   ```\n\n3. **Customize for your data:**\n   - Update data loading in `train.py`\n   - Modify model architecture if needed\n   - Adjust hyperparameters in config\n\n4. **Run training:**\n   ```bash\n   ./scripts/run-training.sh <example-name>\n   ```\n\n5. **Deploy:**\n   - Local: Use `inference.py`\n   - Production: Use `modal_deploy.py`\n\n### Extend Example with Custom Data\n\n1. **Prepare data** in example format\n2. **Replace data files** (data.json, config.yaml)\n3. **Update preprocessing** if needed\n4. **Train with same script**\n5. **Test inference** with new data\n\n### Deploy Example to Production\n\nAll examples include Modal deployment:\n\n```bash\n# Deploy to Modal\ncd examples/<project-name>\nmodal deploy modal_deploy.py\n\n# Get endpoint URL\nmodal app show <app-name>\n```\n\n## Example Comparison\n\n| Feature | Sentiment | Text Gen | Trade Classifier |\n|---------|-----------|----------|------------------|\n| Task Type | Binary Classification | Generation | Multi-class |\n| Model | DistilBERT | GPT-2 + LoRA | Custom Transformer |\n| Training Time | 5-10 min | 15-30 min | 10-20 min |\n| GPU Required | Optional | Recommended | Required |\n| Modal Deploy | ‚úÖ | ‚úÖ | ‚úÖ |\n| Custom Data | Easy | Moderate | Advanced |\n\n## Customization Guide\n\n### Sentiment Classification\n\n**Change dataset:**\n```python\n# In train.py, update load_data()\ndef load_data(path):\n    # Your custom loading logic\n    return texts, labels\n```\n\n**Change model:**\n```python\n# Replace DistilBERT with other models\nmodel_name = \"bert-base-uncased\"  # or roberta-base, etc.\n```\n\n### Text Generation\n\n**Change generation style:**\n```yaml\n# In config.yaml\ngeneration:\n  temperature: 0.8    # Higher = more creative\n  top_p: 0.9          # Nucleus sampling\n  max_length: 200     # Output length\n```\n\n**Add custom prompts:**\n```python\n# In generate.py\nprompts = [\n    \"Your custom prompt here\",\n    \"Another prompt\"\n]\n```\n\n### Trade Classifier\n\n**Add features:**\n```python\n# In data_preprocessing.py\ndef engineer_features(df):\n    df['rsi'] = calculate_rsi(df['close'])\n    df['macd'] = calculate_macd(df['close'])\n    # Add your custom indicators\n    return df\n```\n\n**Change strategy:**\n```python\n# Update labels in train.py\n# 0 = sell, 1 = hold, 2 = buy\nlabels = your_strategy(prices, indicators)\n```\n\n## Dependencies\n\nEach example includes its own `requirements.txt`:\n\n**Sentiment Classification:**\n- transformers\n- torch\n- datasets\n- scikit-learn\n\n**Text Generation:**\n- transformers\n- peft (LoRA)\n- torch\n- modal (deployment)\n\n**Trade Classifier:**\n- transformers\n- pandas\n- numpy\n- modal\n- ta (technical analysis)\n\n## Troubleshooting\n\n### Training Fails\n\n**Issue:** Out of memory\n**Fix:** Reduce batch size in config\n\n**Issue:** CUDA not available\n**Fix:** Use CPU or install CUDA toolkit\n\n### Inference Errors\n\n**Issue:** Model not found\n**Fix:** Check checkpoint path in inference script\n\n**Issue:** Wrong input format\n**Fix:** Validate input matches training data format\n\n### Deployment Issues\n\n**Issue:** Modal authentication\n**Fix:** Run `modal token new` to authenticate\n\n**Issue:** Dependency conflicts\n**Fix:** Use exact versions from requirements.txt\n\n## Resources\n\n**Scripts:** All scripts are in `scripts/` with execution permissions\n\n**Examples:** Complete projects in `examples/` directory\n\n**Templates:** Scaffolding in `templates/` for creating new projects\n\n**Documentation:** Each example has detailed README.md\n\n---\n\n**Supported Frameworks:** PyTorch, Transformers, PEFT\n**Deployment Platforms:** Modal, Local, FastAPI\n**Version:** 1.0.0"
              },
              {
                "name": "google-cloud-configs",
                "description": "Google Cloud Platform configuration templates for BigQuery ML and Vertex AI training with authentication setup, GPU/TPU configs, and cost estimation tools. Use when setting up GCP ML training, configuring BigQuery ML models, deploying Vertex AI training jobs, estimating GCP costs, configuring cloud authentication, selecting GPUs/TPUs for training, or when user mentions BigQuery ML, Vertex AI, GCP training, cloud ML setup, TPU training, or Google Cloud costs.",
                "path": "plugins/ml-training/skills/google-cloud-configs/SKILL.md",
                "frontmatter": {
                  "name": "google-cloud-configs",
                  "description": "Google Cloud Platform configuration templates for BigQuery ML and Vertex AI training with authentication setup, GPU/TPU configs, and cost estimation tools. Use when setting up GCP ML training, configuring BigQuery ML models, deploying Vertex AI training jobs, estimating GCP costs, configuring cloud authentication, selecting GPUs/TPUs for training, or when user mentions BigQuery ML, Vertex AI, GCP training, cloud ML setup, TPU training, or Google Cloud costs.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "Use when:\n- Setting up BigQuery ML for SQL-based machine learning\n- Configuring Vertex AI custom training jobs\n- Setting up GCP authentication for ML workflows\n- Selecting appropriate GPU/TPU configurations\n- Estimating costs for GCP ML training\n- Deploying models to Vertex AI endpoints\n- Configuring distributed training on GCP\n- Optimizing cost vs performance for cloud ML\n\n## Platform Overview\n\n### BigQuery ML\n\n**What it is**: SQL-based machine learning directly in BigQuery\n**Best for**:\n- Quick ML prototypes using existing data warehouse data\n- Classification, regression, forecasting on structured data\n- Users familiar with SQL but not Python/ML frameworks\n- Large-scale batch predictions\n\n**Available Models**:\n- Linear/Logistic Regression\n- XGBoost (BOOSTED_TREE)\n- Deep Neural Networks (DNN)\n- AutoML Tables\n- TensorFlow/PyTorch imported models\n\n**Pricing**:\n- Based on data processed (same as BigQuery queries)\n- $5 per TB processed for analysis\n- AutoML: $19.32/hour for training\n\n### Vertex AI Training\n\n**What it is**: Fully managed ML training platform\n**Best for**:\n- Custom PyTorch/TensorFlow training\n- Large-scale distributed training\n- GPU/TPU-accelerated workloads\n- Production ML pipelines\n\n**Available Compute**:\n- **CPUs**: n1-standard, n1-highmem, n1-highcpu\n- **GPUs**: NVIDIA T4, P4, V100, P100, A100, L4\n- **TPUs**: v2, v3, v4, v5e (8 cores to 512 cores)\n\n**Pricing**:\n- CPU: $0.05-0.30/hour depending on machine type\n- GPU T4: $0.35/hour\n- GPU A100: $3.67/hour (40GB) or $4.95/hour (80GB)\n- TPU v3: $8.00/hour (8 cores)\n- TPU v4: $11.00/hour (8 cores)\n\n## GPU/TPU Selection Guide\n\n### GPU Selection (Vertex AI)\n\n**T4 (16GB VRAM)**:\n- Use case: Inference, light training, small models\n- Cost: $0.35/hour\n- Good for: BERT-base, small CNNs, inference serving\n\n**V100 (16GB VRAM)**:\n- Use case: Mid-size training, mixed precision training\n- Cost: $2.48/hour\n- Good for: ResNet training, medium transformers\n\n**A100 (40GB/80GB VRAM)**:\n- Use case: Large model training, distributed training\n- Cost: $3.67/hour (40GB), $4.95/hour (80GB)\n- Good for: GPT-style models, large vision models, multi-GPU training\n\n**L4 (24GB VRAM)**:\n- Use case: Modern alternative to T4, better performance\n- Cost: $0.66/hour\n- Good for: Mid-size models, efficient inference\n\n### TPU Selection (Vertex AI)\n\n**TPU v2 (8 cores)**:\n- Use case: TensorFlow/JAX training, matrix operations\n- Cost: $4.50/hour\n- Memory: 8GB per core (64GB total)\n- Good for: Legacy TensorFlow models\n\n**TPU v3 (8 cores)**:\n- Use case: Standard TPU training\n- Cost: $8.00/hour\n- Memory: 16GB per core (128GB total)\n- Good for: BERT, T5, image classification\n\n**TPU v4 (8 cores)**:\n- Use case: Latest generation, best performance\n- Cost: $11.00/hour\n- Memory: 32GB per core (256GB total)\n- Good for: Large language models, cutting-edge research\n\n**TPU v5e (8 cores)**:\n- Use case: Cost-optimized TPU\n- Cost: $2.50/hour\n- Good for: Development, training at scale on budget\n\n**Multi-node TPU Pods**:\n- v3-32: 32 cores, $32/hour\n- v3-128: 128 cores, $128/hour\n- v4-128: 128 cores, $176/hour\n- Use for: Massive distributed training (GPT-3 scale)\n\n## Usage\n\n### Setup BigQuery ML Environment\n\n```bash\nbash scripts/setup-bigquery-ml.sh\n```\n\n**Prompts for**:\n- GCP Project ID\n- BigQuery dataset name\n- Service account credentials\n- Default model type preference\n\n**Creates**:\n- `bigquery_config.json` - Project configuration\n- `.bigqueryrc` - CLI configuration\n- Example training SQL in examples/\n\n### Setup Vertex AI Training Environment\n\n```bash\nbash scripts/setup-vertex-ai.sh\n```\n\n**Prompts for**:\n- GCP Project ID\n- Region (us-central1, europe-west4, etc.)\n- Service account credentials\n- Default machine type\n- GPU/TPU preference\n\n**Creates**:\n- `vertex_config.yaml` - Training job configuration\n- `vertex_requirements.txt` - Python dependencies\n- Training script template\n\n### Configure GCP Authentication\n\n```bash\nbash scripts/configure-auth.sh\n```\n\n**Prompts for**:\n- Authentication method (service account, user account, workload identity)\n- Service account key path (if applicable)\n- IAM roles needed\n\n**Creates**:\n- `.gcp_auth_config` - Authentication configuration\n- Sets GOOGLE_APPLICATION_CREDENTIALS environment variable\n- Validates permissions\n\n**Required IAM Roles**:\n- BigQuery ML: `roles/bigquery.dataEditor`, `roles/bigquery.jobUser`\n- Vertex AI: `roles/aiplatform.user`, `roles/storage.objectAdmin`\n- Both: `roles/serviceusage.serviceUsageConsumer`\n\n### Estimate GCP Training Costs\n\n```bash\nbash scripts/estimate-gcp-cost.sh\n```\n\n**Interactive prompts**:\n- Platform: BigQuery ML or Vertex AI\n- If BigQuery ML: Data size to process\n- If Vertex AI:\n  - Machine type (CPU/GPU/TPU)\n  - Number of machines\n  - Training duration estimate\n  - Storage requirements\n\n**Output**:\n- Estimated compute cost\n- Storage cost\n- Data transfer cost (if applicable)\n- Total estimated cost\n- Cost comparison with other GCP options\n\n## Templates\n\n### BigQuery ML Training Template (`templates/bigquery_ml_training.sql`)\n\nSQL template for creating and training models:\n- Model creation syntax\n- Feature engineering examples\n- Training options (L1/L2 reg, learning rate, etc.)\n- Evaluation queries\n- Prediction queries\n\n**Supported model types**:\n- LINEAR_REG, LOGISTIC_REG\n- BOOSTED_TREE_CLASSIFIER, BOOSTED_TREE_REGRESSOR\n- DNN_CLASSIFIER, DNN_REGRESSOR\n- AUTOML_CLASSIFIER, AUTOML_REGRESSOR\n\n### Vertex AI Training Job Template (`templates/vertex_training_job.py`)\n\nPython template for custom training:\n- Training loop structure\n- Distributed training setup (PyTorch DDP)\n- Checkpointing and model saving\n- Metrics logging to Vertex AI\n- Hyperparameter tuning integration\n\n**Includes**:\n- Single GPU training\n- Multi-GPU training (DataParallel, DistributedDataParallel)\n- TPU training with PyTorch/XLA\n- Cloud Storage integration\n\n### GPU Configuration Template (`templates/vertex_gpu_config.yaml`)\n\nYAML configuration for GPU training jobs:\n- Machine type selection\n- GPU type and count\n- Disk configuration\n- Network configuration\n- Environment variables\n\n**Presets included**:\n- Single T4 (budget)\n- Single A100 (standard)\n- 4x A100 (distributed)\n- 8x A100 (large-scale)\n\n### TPU Configuration Template (`templates/vertex_tpu_config.yaml`)\n\nYAML configuration for TPU training jobs:\n- TPU type and topology\n- TPU version selection\n- JAX/TensorFlow runtime\n- XLA compilation flags\n\n**Presets included**:\n- v3-8 (single TPU)\n- v4-32 (TPU pod slice)\n- v5e-8 (cost-optimized)\n\n### GCP Authentication Template (`templates/gcp_auth.json`)\n\nService account configuration template:\n- Project ID\n- Service account email\n- Key file path\n- Required scopes\n- IAM role assignments\n\n**Security notes**:\n- Uses placeholders only (never real keys)\n- Documents how to create service accounts\n- Includes `.gitignore` protection\n\n## Examples\n\n### BigQuery ML Regression Example (`examples/bigquery-regression-example.sql`)\n\nComplete example:\n- Dataset: NYC taxi trip data\n- Task: Predict trip duration\n- Model: BOOSTED_TREE_REGRESSOR\n- Includes feature engineering, training, evaluation\n\n**Demonstrates**:\n- CREATE MODEL syntax\n- TRANSFORM clause for feature engineering\n- MODEL evaluation\n- Batch predictions\n\n### Vertex AI PyTorch Training Example (`examples/vertex-pytorch-training.py`)\n\nComplete training script:\n- Dataset: IMDB sentiment analysis\n- Model: DistilBERT fine-tuning\n- Training: Single GPU\n- Logging: Vertex AI experiments\n\n**Demonstrates**:\n- Loading data from GCS\n- Training loop with mixed precision\n- Checkpointing to GCS\n- Metrics logging\n- Model export to Vertex AI\n\n### Vertex AI Distributed Training Example (`examples/vertex-distributed-training.py`)\n\nMulti-GPU training example:\n- Dataset: ImageNet subset\n- Model: ResNet-50\n- Training: 4x A100 with DDP\n- Scaling: Linear scaling rule\n\n**Demonstrates**:\n- PyTorch DistributedDataParallel\n- Gradient accumulation\n- Learning rate scaling\n- Synchronized batch norm\n- Multi-node coordination\n\n### Hugging Face Fine-tuning on Vertex AI (`examples/vertex-huggingface-finetuning.py`)\n\nProduction fine-tuning template:\n- Dataset: Custom text classification\n- Model: BERT/RoBERTa/DeBERTa\n- Training: Hugging Face Trainer API\n- Deployment: Vertex AI endpoint\n\n**Demonstrates**:\n- Hugging Face Trainer integration\n- Hyperparameter tuning with Vertex AI\n- Model versioning\n- Endpoint deployment\n- Online predictions\n\n## Cost Optimization Tips\n\n### BigQuery ML\n\n**Reduce data processed**:\n- Use partitioned tables\n- Filter data in WHERE clause before training\n- Use table sampling for experimentation\n- Cache intermediate results\n\n**Use appropriate model types**:\n- Start with LINEAR_REG/LOGISTIC_REG (cheapest)\n- Use BOOSTED_TREE for better accuracy at moderate cost\n- Reserve AutoML for when simpler models fail\n\n**Optimize queries**:\n- Avoid SELECT * (specify columns)\n- Use clustering on filter columns\n- Materialize views for repeated training\n\n### Vertex AI\n\n**Machine type selection**:\n- Start with CPU for prototyping\n- Use T4 for small models (cheapest GPU)\n- Use A100 only for large models that need it\n- Consider TPU v5e for TensorFlow/JAX (very cost-effective)\n\n**Training optimization**:\n- Use preemptible instances (60-70% cheaper, can be interrupted)\n- Enable automatic checkpoint/resume for preemptible\n- Use mixed precision training (FP16/BF16) for faster training\n- Profile to eliminate CPU bottlenecks\n\n**Storage optimization**:\n- Store datasets in Cloud Storage (cheaper than persistent disk)\n- Use Filestore only if needed for POSIX filesystem\n- Clean up old model artifacts\n- Use lifecycle policies to archive old data\n\n**Multi-GPU efficiency**:\n- Ensure near-linear scaling before adding more GPUs\n- Profile inter-GPU communication\n- Use gradient accumulation instead of larger batch sizes\n- Consider 2x GPUs instead of 1x larger GPU (often same cost, better availability)\n\n## Integration with ML Training Plugin\n\nThis skill integrates with other ml-training components:\n\n- **training-patterns**: Provides GCP configs for generated training scripts\n- **cost-calculator**: Uses GCP pricing data for budget planning\n- **monitoring-dashboard**: Integrates with Vertex AI TensorBoard\n- **validation-scripts**: Validates GCP credentials and permissions\n- **integration-helpers**: Deploys trained models to Vertex AI endpoints\n\n## Common Workflows\n\n### Workflow 1: Quick BigQuery ML Prototype\n\n1. Run `bash scripts/setup-bigquery-ml.sh`\n2. Copy `templates/bigquery_ml_training.sql` to your project\n3. Modify SQL for your dataset and features\n4. Run training query in BigQuery console\n5. Evaluate with built-in ML.EVALUATE()\n6. Export predictions with ML.PREDICT()\n\n**Time**: 30 minutes setup + training time\n**Cost**: $5 per TB of data processed\n\n### Workflow 2: Custom PyTorch Training on Vertex AI\n\n1. Run `bash scripts/configure-auth.sh`\n2. Run `bash scripts/setup-vertex-ai.sh`\n3. Copy `templates/vertex_training_job.py`\n4. Customize training loop for your model\n5. Copy `templates/vertex_gpu_config.yaml`\n6. Submit job: `gcloud ai custom-jobs create ...`\n7. Monitor in Vertex AI console\n\n**Time**: 1 hour setup + training time\n**Cost**: Depends on GPU/TPU selection\n\n### Workflow 3: Large-Scale Distributed Training\n\n1. Setup Vertex AI (workflow 2)\n2. Copy `examples/vertex-distributed-training.py`\n3. Adapt for your model architecture\n4. Test locally with 1 GPU\n5. Test with 2 GPUs to verify scaling\n6. Scale to 4-8 GPUs for full training\n7. Use preemptible instances with checkpointing\n\n**Time**: 2-4 hours setup + training time\n**Cost**: $15-60/hour depending on GPU count\n\n## Troubleshooting\n\n### BigQuery ML Issues\n\n**\"Insufficient permissions\"**:\n- Verify `roles/bigquery.dataEditor` and `roles/bigquery.jobUser`\n- Check dataset-level permissions\n- Ensure billing is enabled\n\n**\"Model training failed\"**:\n- Check for NULL values in features\n- Verify data types match model expectations\n- Review feature engineering TRANSFORM clause\n- Check for sufficient training data\n\n### Vertex AI Issues\n\n**\"Service account lacks permissions\"**:\n- Verify `roles/aiplatform.user`\n- Add `roles/storage.objectAdmin` for GCS access\n- Check project-level IAM policies\n\n**\"GPU/TPU quota exceeded\"**:\n- Request quota increase in GCP console\n- Use different region with availability\n- Start with smaller GPU/TPU configuration\n- Use preemptible instances (separate quota)\n\n**\"Training job crashes\"**:\n- Check for CUDA OOM (reduce batch size)\n- Verify dependencies in requirements.txt\n- Review logs in Cloud Logging\n- Test locally before submitting to Vertex\n\n## Security Best Practices\n\n### Credentials Management\n\n**DO**:\n- ‚úÖ Use service accounts with minimal permissions\n- ‚úÖ Store credentials in Secret Manager\n- ‚úÖ Use Workload Identity for GKE deployments\n- ‚úÖ Rotate service account keys regularly\n- ‚úÖ Add `.gitignore` for `*.json` key files\n\n**DON'T**:\n- ‚ùå Hardcode credentials in code\n- ‚ùå Commit service account keys to git\n- ‚ùå Use overly permissive roles (e.g., Owner)\n- ‚ùå Share service account keys across projects\n- ‚ùå Use personal credentials for production\n\n### IAM Best Practices\n\n- Use separate service accounts for training vs serving\n- Grant roles at resource level, not project level when possible\n- Use Workload Identity Federation instead of keys when possible\n- Enable Cloud Audit Logs for ML API usage\n- Review IAM permissions quarterly\n\n## Performance Benchmarks\n\n### BigQuery ML vs Vertex AI\n\n**BigQuery ML**:\n- Best for: Structured data, SQL users, quick prototypes\n- Training time: Minutes to hours (depends on data size)\n- Scalability: Automatic (serverless)\n- Cost: $5/TB processed\n\n**Vertex AI Custom Training**:\n- Best for: Deep learning, custom architectures, GPU/TPU workloads\n- Training time: Hours to days (configurable hardware)\n- Scalability: Manual (choose machine type)\n- Cost: $0.35-20/hour depending on hardware\n\n**Rule of thumb**:\n- Use BigQuery ML for tabular data with < 100M rows\n- Use Vertex AI for images, text, audio, or custom models\n- Use Vertex AI for models requiring GPU/TPU acceleration\n\n## Additional Resources\n\n- **GCP ML Documentation**: https://cloud.google.com/vertex-ai/docs\n- **BigQuery ML Reference**: https://cloud.google.com/bigquery-ml/docs\n- **Pricing Calculator**: https://cloud.google.com/products/calculator\n- **TPU Best Practices**: https://cloud.google.com/tpu/docs/best-practices\n- **Vertex AI Samples**: https://github.com/GoogleCloudPlatform/vertex-ai-samples"
              },
              {
                "name": "integration-helpers",
                "description": "Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.",
                "path": "plugins/ml-training/skills/integration-helpers/SKILL.md",
                "frontmatter": {
                  "name": "integration-helpers",
                  "description": "Integration templates for FastAPI endpoints, Next.js UI components, and Supabase schemas for ML model deployment. Use when deploying ML models, creating inference APIs, building ML prediction UIs, designing ML database schemas, integrating trained models with applications, or when user mentions FastAPI ML endpoints, prediction forms, model serving, ML API deployment, inference integration, or production ML deployment.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# integration-helpers\n\n## Instructions\n\nThis skill provides production-ready integration templates for deploying machine learning models into full-stack applications. It covers FastAPI inference endpoints, Next.js prediction interfaces, and Supabase schemas for ML metadata storage.\n\n### 1. FastAPI Inference Endpoints\n\nCreate production-ready ML inference APIs with proper error handling and validation:\n\n```bash\n# Generate FastAPI ML router\nbash ./skills/integration-helpers/scripts/add-fastapi-endpoint.sh <model-type> <endpoint-name>\n\n# Model types: classification, regression, text-generation, image-classification, embeddings\n```\n\n**What This Creates:**\n- Pydantic models for request/response validation\n- Inference endpoint with proper error handling\n- Model loading and caching logic\n- Health check endpoint\n- Batch prediction support\n- Async request handling\n\n**Router Structure:**\n```python\nfrom fastapi import APIRouter, HTTPException, UploadFile\nfrom pydantic import BaseModel, Field\nimport numpy as np\n\nrouter = APIRouter(\n    prefix=\"/ml\",\n    tags=[\"machine-learning\"],\n    responses={500: {\"description\": \"Model inference error\"}},\n)\n```\n\n**Example Usage:**\n```bash\n# Create text classification endpoint\nbash ./skills/integration-helpers/scripts/add-fastapi-endpoint.sh classification sentiment-analysis\n\n# Creates: app/routers/ml_sentiment_analysis.py\n```\n\n### 2. Request/Response Models\n\nDefine type-safe ML inference contracts:\n\n**Classification Model:**\n```python\nclass ClassificationRequest(BaseModel):\n    text: str = Field(..., min_length=1, max_length=10000)\n    model_version: str | None = None\n    return_probabilities: bool = False\n\nclass ClassificationResponse(BaseModel):\n    prediction: str\n    confidence: float = Field(..., ge=0.0, le=1.0)\n    probabilities: dict[str, float] | None = None\n    model_version: str\n    inference_time_ms: float\n```\n\n**Regression Model:**\n```python\nclass RegressionRequest(BaseModel):\n    features: list[float] = Field(..., min_items=1)\n    feature_names: list[str] | None = None\n\nclass RegressionResponse(BaseModel):\n    prediction: float\n    feature_importance: dict[str, float] | None = None\n    model_version: str\n```\n\n**Image Classification:**\n```python\nclass ImageClassificationResponse(BaseModel):\n    predictions: list[dict[str, Any]]\n    top_prediction: str\n    confidence: float\n    processing_time_ms: float\n```\n\n### 3. Model Loading and Caching\n\nImplement efficient model loading with caching:\n\n```python\nfrom functools import lru_cache\nimport joblib\nimport torch\n\n# Singleton model loader\nclass ModelLoader:\n    _instance = None\n    _model = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def load_model(self, model_path: str):\n        if self._model is None:\n            # Load based on framework\n            if model_path.endswith('.pkl'):\n                self._model = joblib.load(model_path)\n            elif model_path.endswith('.pt'):\n                self._model = torch.load(model_path)\n            # Add TensorFlow, ONNX, etc.\n        return self._model\n\n# Dependency for endpoints\nasync def get_model():\n    loader = ModelLoader()\n    return loader.load_model(\"models/latest.pkl\")\n```\n\n### 4. Inference Endpoints with Error Handling\n\nImplement robust inference with proper error handling:\n\n```python\n@router.post(\"/predict\", response_model=ClassificationResponse)\nasync def predict(\n    request: ClassificationRequest,\n    model = Depends(get_model)\n):\n    try:\n        start_time = time.time()\n\n        # Preprocess input\n        processed_input = preprocess_text(request.text)\n\n        # Run inference\n        prediction = model.predict([processed_input])[0]\n        probabilities = None\n\n        if request.return_probabilities:\n            probs = model.predict_proba([processed_input])[0]\n            probabilities = {\n                label: float(prob)\n                for label, prob in zip(model.classes_, probs)\n            }\n\n        inference_time = (time.time() - start_time) * 1000\n\n        return ClassificationResponse(\n            prediction=str(prediction),\n            confidence=float(max(probs)) if probabilities else 0.0,\n            probabilities=probabilities,\n            model_version=MODEL_VERSION,\n            inference_time_ms=inference_time\n        )\n\n    except ValueError as e:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid input: {str(e)}\"\n        )\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Model inference failed: {str(e)}\"\n        )\n```\n\n### 5. Batch Prediction Support\n\nEnable efficient batch inference:\n\n```python\nclass BatchClassificationRequest(BaseModel):\n    texts: list[str] = Field(..., min_items=1, max_items=100)\n    model_version: str | None = None\n\nclass BatchClassificationResponse(BaseModel):\n    predictions: list[ClassificationResponse]\n    total_inference_time_ms: float\n\n@router.post(\"/predict/batch\", response_model=BatchClassificationResponse)\nasync def predict_batch(\n    request: BatchClassificationRequest,\n    model = Depends(get_model)\n):\n    start_time = time.time()\n    predictions = []\n\n    # Process in batches for efficiency\n    for text in request.texts:\n        pred = await predict(\n            ClassificationRequest(text=text),\n            model=model\n        )\n        predictions.append(pred)\n\n    total_time = (time.time() - start_time) * 1000\n\n    return BatchClassificationResponse(\n        predictions=predictions,\n        total_inference_time_ms=total_time\n    )\n```\n\n### 6. Next.js Prediction Forms\n\nCreate React components for ML model interaction:\n\n```bash\n# Generate Next.js prediction form\nbash ./skills/integration-helpers/scripts/add-nextjs-component.sh <component-type> <component-name>\n\n# Component types: classification-form, regression-form, image-upload, chat-interface\n```\n\n**What This Creates:**\n- TypeScript React component with shadcn/ui\n- Form validation with react-hook-form and zod\n- Loading states and error handling\n- Result visualization components\n- API integration with fetch/axios\n\n**Example Component:**\n```typescript\n// components/ml/sentiment-form.tsx\n'use client'\n\nimport { useState } from 'react'\nimport { useForm } from 'react-hook-form'\nimport { zodResolver } from '@hookform/resolvers/zod'\nimport * as z from 'zod'\nimport { Button } from '@/components/ui/button'\nimport { Textarea } from '@/components/ui/textarea'\nimport { Card } from '@/components/ui/card'\n\nconst formSchema = z.object({\n  text: z.string().min(1).max(10000),\n})\n\nexport function SentimentForm() {\n  const [result, setResult] = useState<any>(null)\n  const [loading, setLoading] = useState(false)\n\n  const form = useForm<z.infer<typeof formSchema>>({\n    resolver: zodResolver(formSchema),\n  })\n\n  async function onSubmit(values: z.infer<typeof formSchema>) {\n    setLoading(true)\n    try {\n      const response = await fetch('/api/ml/predict', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(values),\n      })\n      const data = await response.json()\n      setResult(data)\n    } catch (error) {\n      console.error(error)\n    } finally {\n      setLoading(false)\n    }\n  }\n\n  return (\n    <Card className=\"p-6\">\n      <form onSubmit={form.handleSubmit(onSubmit)}>\n        <Textarea {...form.register('text')} />\n        <Button type=\"submit\" disabled={loading}>\n          {loading ? 'Analyzing...' : 'Analyze Sentiment'}\n        </Button>\n      </form>\n      {result && <ResultDisplay result={result} />}\n    </Card>\n  )\n}\n```\n\n### 7. Result Visualization Components\n\nDisplay ML predictions with visual feedback:\n\n```typescript\n// Classification result with confidence\nfunction ClassificationResult({ prediction, confidence, probabilities }) {\n  return (\n    <div className=\"space-y-4\">\n      <div className=\"text-2xl font-bold\">{prediction}</div>\n      <div className=\"text-muted-foreground\">\n        Confidence: {(confidence * 100).toFixed(1)}%\n      </div>\n      {probabilities && (\n        <div className=\"space-y-2\">\n          {Object.entries(probabilities).map(([label, prob]) => (\n            <div key={label} className=\"flex items-center gap-2\">\n              <span className=\"w-24\">{label}</span>\n              <div className=\"flex-1 bg-secondary rounded-full h-2\">\n                <div\n                  className=\"bg-primary h-2 rounded-full\"\n                  style={{ width: `${prob * 100}%` }}\n                />\n              </div>\n              <span className=\"w-12 text-right\">{(prob * 100).toFixed(1)}%</span>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  )\n}\n```\n\n### 8. Supabase ML Metadata Schemas\n\nCreate database schemas for ML model tracking and results:\n\n```bash\n# Generate Supabase schema for ML metadata\nbash ./skills/integration-helpers/scripts/create-supabase-schema.sh <schema-type>\n\n# Schema types: ml-models, predictions, training-runs, model-versions\n```\n\n**ML Models Table:**\n```sql\ncreate table ml_models (\n  id uuid default gen_random_uuid() primary key,\n  name text not null,\n  model_type text not null, -- classification, regression, etc.\n  framework text not null, -- scikit-learn, pytorch, tensorflow\n  version text not null,\n  artifact_url text, -- Cloud storage URL\n  metrics jsonb, -- Accuracy, F1, RMSE, etc.\n  hyperparameters jsonb,\n  feature_names text[],\n  target_classes text[],\n  is_active boolean default true,\n  created_at timestamptz default now(),\n  updated_at timestamptz default now(),\n  created_by uuid references auth.users(id)\n);\n\ncreate index idx_ml_models_active on ml_models(is_active, created_at desc);\ncreate index idx_ml_models_type on ml_models(model_type);\n```\n\n**Predictions Log Table:**\n```sql\ncreate table predictions (\n  id uuid default gen_random_uuid() primary key,\n  model_id uuid references ml_models(id),\n  model_version text not null,\n  input_data jsonb not null,\n  prediction jsonb not null,\n  confidence float,\n  inference_time_ms float,\n  user_id uuid references auth.users(id),\n  session_id text,\n  created_at timestamptz default now()\n);\n\ncreate index idx_predictions_model on predictions(model_id, created_at desc);\ncreate index idx_predictions_user on predictions(user_id, created_at desc);\ncreate index idx_predictions_session on predictions(session_id);\n```\n\n**Training Runs Table:**\n```sql\ncreate table training_runs (\n  id uuid default gen_random_uuid() primary key,\n  model_id uuid references ml_models(id),\n  dataset_name text not null,\n  dataset_size integer,\n  train_test_split jsonb, -- {train: 0.8, test: 0.2}\n  hyperparameters jsonb,\n  training_metrics jsonb, -- Loss curves, accuracy per epoch\n  validation_metrics jsonb,\n  test_metrics jsonb,\n  training_duration_seconds integer,\n  status text default 'running', -- running, completed, failed\n  error_message text,\n  artifact_url text,\n  created_at timestamptz default now(),\n  completed_at timestamptz,\n  created_by uuid references auth.users(id)\n);\n\ncreate index idx_training_runs_model on training_runs(model_id, created_at desc);\ncreate index idx_training_runs_status on training_runs(status);\n```\n\n**Model Versions Table:**\n```sql\ncreate table model_versions (\n  id uuid default gen_random_uuid() primary key,\n  model_id uuid references ml_models(id),\n  version text not null,\n  changelog text,\n  metrics_comparison jsonb, -- Compare with previous version\n  is_deployed boolean default false,\n  deployment_url text,\n  created_at timestamptz default now(),\n  deployed_at timestamptz,\n  created_by uuid references auth.users(id),\n  unique(model_id, version)\n);\n\ncreate index idx_model_versions_deployed on model_versions(model_id, is_deployed);\n```\n\n### 9. API Route Handlers for Next.js\n\nCreate Next.js API routes that call FastAPI backend:\n\n```typescript\n// app/api/ml/predict/route.ts\nimport { NextRequest, NextResponse } from 'next/server'\n\nconst FASTAPI_URL = process.env.FASTAPI_URL || 'http://localhost:8000'\n\nexport async function POST(request: NextRequest) {\n  try {\n    const body = await request.json()\n\n    const response = await fetch(`${FASTAPI_URL}/ml/predict`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify(body),\n    })\n\n    if (!response.ok) {\n      const error = await response.json()\n      return NextResponse.json(\n        { error: error.detail },\n        { status: response.status }\n      )\n    }\n\n    const data = await response.json()\n    return NextResponse.json(data)\n\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to connect to ML service' },\n      { status: 500 }\n    )\n  }\n}\n```\n\n## Examples\n\n### Example 1: Complete Sentiment Analysis Integration\n\n```bash\n# 1. Create FastAPI endpoint\ncd /path/to/fastapi-backend\nbash plugins/ml-training/skills/integration-helpers/scripts/add-fastapi-endpoint.sh classification sentiment-analysis\n\n# 2. Create Next.js form component\ncd /path/to/nextjs-frontend\nbash plugins/ml-training/skills/integration-helpers/scripts/add-nextjs-component.sh classification-form sentiment-form\n\n# 3. Create Supabase schema for logging\nbash plugins/ml-training/skills/integration-helpers/scripts/create-supabase-schema.sh ml-models\nbash plugins/ml-training/skills/integration-helpers/scripts/create-supabase-schema.sh predictions\n```\n\n**Result:** End-to-end sentiment analysis system with API, UI, and data logging\n\n### Example 2: Image Classification Service\n\n```bash\n# 1. Create image classification endpoint\nbash plugins/ml-training/skills/integration-helpers/scripts/add-fastapi-endpoint.sh image-classification image-classifier\n\n# 2. Create image upload component\nbash plugins/ml-training/skills/integration-helpers/scripts/add-nextjs-component.sh image-upload image-classifier-form\n\n# 3. Setup model versioning schema\nbash plugins/ml-training/skills/integration-helpers/scripts/create-supabase-schema.sh model-versions\n```\n\n**Result:** Complete image classification service with upload UI and version tracking\n\n## Requirements\n\n**FastAPI Dependencies:**\n- FastAPI 0.100+\n- Pydantic 2.0+\n- scikit-learn, PyTorch, or TensorFlow (based on your model)\n- python-multipart (for file uploads)\n- joblib or pickle (for model serialization)\n\n**Next.js Dependencies:**\n- Next.js 14+\n- React 18+\n- shadcn/ui components\n- react-hook-form\n- zod\n- TypeScript\n\n**Supabase:**\n- PostgreSQL 15+\n- Row Level Security enabled\n- UUID extension enabled\n\n## Best Practices\n\n**API Design:**\n- Use proper HTTP status codes (200, 400, 500)\n- Implement request validation with Pydantic\n- Add rate limiting for production\n- Log all predictions for monitoring\n- Version your models in URLs or headers\n\n**Performance:**\n- Cache models in memory (singleton pattern)\n- Use async endpoints for I/O operations\n- Implement batch prediction for efficiency\n- Consider model quantization for speed\n- Use background tasks for heavy operations\n\n**Security:**\n- Validate all inputs strictly\n- Implement authentication for API endpoints\n- Use CORS properly in production\n- Don't expose model internals in errors\n- Rate limit inference endpoints\n\n**Monitoring:**\n- Log inference times\n- Track prediction distributions\n- Monitor error rates\n- Store predictions for retraining\n- Set up alerts for degraded performance\n\n**Error Handling:**\n- Return structured error responses\n- Differentiate input errors from model errors\n- Provide helpful error messages\n- Log errors for debugging\n- Implement graceful degradation\n\n---\n\n**Plugin:** ml-training\n**Version:** 1.0.0\n**Category:** ML Integration\n**Skill Type:** Integration Templates"
              },
              {
                "name": "monitoring-dashboard",
                "description": "Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.",
                "path": "plugins/ml-training/skills/monitoring-dashboard/SKILL.md",
                "frontmatter": {
                  "name": "monitoring-dashboard",
                  "description": "Training monitoring dashboard setup with TensorBoard and Weights & Biases (WandB) including real-time metrics tracking, experiment comparison, hyperparameter visualization, and integration patterns. Use when setting up training monitoring, tracking experiments, visualizing metrics, comparing model runs, or when user mentions TensorBoard, WandB, training metrics, experiment tracking, or monitoring dashboard.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Monitoring Dashboard\n\n**Purpose:** Provide complete monitoring dashboard templates and setup scripts for ML training with TensorBoard and Weights & Biases (WandB).\n\n**Activation Triggers:**\n- Setting up training monitoring dashboards\n- Tracking experiments and metrics in real-time\n- Comparing multiple training runs\n- Visualizing hyperparameters and results\n- Integrating monitoring into existing training pipelines\n- Logging custom metrics, images, and model artifacts\n\n**Key Resources:**\n- `scripts/setup-tensorboard.sh` - Install and configure TensorBoard\n- `scripts/setup-wandb.sh` - Install and configure Weights & Biases\n- `scripts/launch-monitoring.sh` - Launch monitoring dashboards\n- `templates/tensorboard-config.yaml` - TensorBoard configuration template\n- `templates/wandb-config.py` - WandB integration template\n- `templates/logging-config.json` - Unified logging configuration\n- `examples/tensorboard-integration.md` - Complete TensorBoard integration guide\n- `examples/wandb-integration.md` - Complete WandB integration guide\n\n## Quick Start\n\n### 1. Choose Monitoring Solution\n\n**TensorBoard (Local/Open Source):**\n- Free, runs locally\n- Best for: Single-user development, offline work\n- Features: Metrics, histograms, graphs, images, embeddings\n- Storage: Local filesystem\n\n**Weights & Biases (Cloud/Collaboration):**\n- Free tier available, cloud-hosted\n- Best for: Team collaboration, experiment comparison, production\n- Features: All TensorBoard features + collaboration, alerts, reports\n- Storage: Cloud with unlimited history\n\n**Both (Recommended for Production):**\n- Use TensorBoard for local development\n- Use WandB for team collaboration and production tracking\n\n### 2. Setup TensorBoard\n\n```bash\n# Install and configure TensorBoard\n./scripts/setup-tensorboard.sh\n\n# Launch TensorBoard\n./scripts/launch-monitoring.sh tensorboard --logdir ./runs\n```\n\n**Access:** Open browser to http://localhost:6006\n\n### 3. Setup Weights & Biases\n\n```bash\n# Install and configure WandB\n./scripts/setup-wandb.sh\n\n# Login with API key\nwandb login\n\n# Launch monitoring\n./scripts/launch-monitoring.sh wandb\n```\n\n**Access:** Dashboard at https://wandb.ai/your-username/your-project\n\n## TensorBoard Integration\n\n### Basic Setup\n\n**Template:** `templates/tensorboard-config.yaml`\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\n\n# Create TensorBoard writer\nlog_dir = f\"runs/experiment_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\nwriter = SummaryWriter(log_dir=log_dir)\n\n# Log scalar metrics\nwriter.add_scalar('Loss/train', train_loss, epoch)\nwriter.add_scalar('Loss/validation', val_loss, epoch)\nwriter.add_scalar('Accuracy/train', train_acc, epoch)\nwriter.add_scalar('Accuracy/validation', val_acc, epoch)\n\n# Log learning rate\nwriter.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n\n# Close writer when done\nwriter.close()\n```\n\n### Advanced Logging\n\n**Histograms (Weight Distributions):**\n```python\n# Log model weights\nfor name, param in model.named_parameters():\n    writer.add_histogram(f'weights/{name}', param, epoch)\n    writer.add_histogram(f'gradients/{name}', param.grad, epoch)\n```\n\n**Images:**\n```python\n# Log sample predictions\nwriter.add_image('predictions', image_grid, epoch)\nwriter.add_images('batch_samples', image_batch, epoch)\n```\n\n**Text:**\n```python\n# Log hyperparameters as text\nconfig_text = '\\n'.join([f'{k}: {v}' for k, v in config.items()])\nwriter.add_text('hyperparameters', config_text, 0)\n```\n\n**Model Graph:**\n```python\n# Log model architecture\nwriter.add_graph(model, input_tensor)\n```\n\n**Embeddings (t-SNE, PCA):**\n```python\n# Visualize embeddings\nwriter.add_embedding(embeddings, metadata=labels, label_img=images)\n```\n\n### Launch TensorBoard\n\n```bash\n# Basic launch\ntensorboard --logdir runs\n\n# Specify port\ntensorboard --logdir runs --port 6007\n\n# Load faster (sample data)\ntensorboard --logdir runs --samples_per_plugin scalars=1000\n\n# Enable reload\ntensorboard --logdir runs --reload_interval 5\n```\n\n## Weights & Biases Integration\n\n### Basic Setup\n\n**Template:** `templates/wandb-config.py`\n\n```python\nimport wandb\n\n# Initialize WandB run\nwandb.init(\n    project=\"my-ml-project\",\n    name=f\"experiment-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n    config={\n        \"learning_rate\": 0.001,\n        \"epochs\": 100,\n        \"batch_size\": 32,\n        \"model\": \"resnet50\",\n        \"dataset\": \"imagenet\"\n    }\n)\n\n# Log metrics\nwandb.log({\n    \"train_loss\": train_loss,\n    \"val_loss\": val_loss,\n    \"train_acc\": train_acc,\n    \"val_acc\": val_acc,\n    \"epoch\": epoch\n})\n\n# Finish run\nwandb.finish()\n```\n\n### Advanced Features\n\n**Log Media:**\n```python\n# Log images\nwandb.log({\"predictions\": [wandb.Image(img, caption=f\"Pred: {pred}\")]})\n\n# Log tables\ntable = wandb.Table(columns=[\"epoch\", \"loss\", \"accuracy\"], data=data)\nwandb.log({\"results_table\": table})\n\n# Log audio\nwandb.log({\"audio\": wandb.Audio(audio_array, sample_rate=16000)})\n\n# Log videos\nwandb.log({\"video\": wandb.Video(video_path, fps=30)})\n```\n\n**Track Model Artifacts:**\n```python\n# Save model checkpoint\nartifact = wandb.Artifact('model-checkpoint', type='model')\nartifact.add_file('model.pth')\nwandb.log_artifact(artifact)\n\n# Load model from artifact\nartifact = wandb.use_artifact('model-checkpoint:latest')\nmodel_path = artifact.download()\n```\n\n**Hyperparameter Sweeps:**\n```python\n# Define sweep configuration\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_loss', 'goal': 'minimize'},\n    'parameters': {\n        'learning_rate': {'min': 0.0001, 'max': 0.1},\n        'batch_size': {'values': [16, 32, 64]},\n        'optimizer': {'values': ['adam', 'sgd', 'adamw']}\n    }\n}\n\n# Initialize sweep\nsweep_id = wandb.sweep(sweep_config, project=\"my-project\")\n\n# Run sweep agent\nwandb.agent(sweep_id, function=train_model, count=10)\n```\n\n**Custom Charts:**\n```python\n# Create custom plot\ndata = [[x, y] for (x, y) in zip(x_values, y_values)]\ntable = wandb.Table(data=data, columns=[\"x\", \"y\"])\nwandb.log({\n    \"custom_plot\": wandb.plot.line(table, \"x\", \"y\", title=\"Custom Plot\")\n})\n```\n\n**Alerts:**\n```python\n# Alert on metric threshold\nif val_loss < 0.1:\n    wandb.alert(\n        title=\"Low Validation Loss\",\n        text=f\"Validation loss dropped to {val_loss:.4f}\",\n        level=wandb.AlertLevel.INFO\n    )\n```\n\n## Unified Logging Configuration\n\n**Template:** `templates/logging-config.json`\n\nUse this configuration to log to both TensorBoard and WandB simultaneously:\n\n```python\nimport wandb\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass UnifiedLogger:\n    def __init__(self, project_name, experiment_name, config):\n        # TensorBoard\n        self.tb_writer = SummaryWriter(\n            log_dir=f\"runs/{experiment_name}\"\n        )\n\n        # WandB\n        wandb.init(\n            project=project_name,\n            name=experiment_name,\n            config=config\n        )\n\n    def log_metrics(self, metrics_dict, step):\n        \"\"\"Log to both TensorBoard and WandB\"\"\"\n        # TensorBoard\n        for key, value in metrics_dict.items():\n            self.tb_writer.add_scalar(key, value, step)\n\n        # WandB\n        wandb.log(metrics_dict, step=step)\n\n    def log_images(self, images_dict, step):\n        \"\"\"Log images to both platforms\"\"\"\n        for key, image in images_dict.items():\n            # TensorBoard\n            self.tb_writer.add_image(key, image, step)\n\n            # WandB\n            wandb.log({key: wandb.Image(image)}, step=step)\n\n    def log_model(self, model, input_sample):\n        \"\"\"Log model architecture\"\"\"\n        # TensorBoard graph\n        self.tb_writer.add_graph(model, input_sample)\n\n        # WandB watches gradients\n        wandb.watch(model, log=\"all\", log_freq=100)\n\n    def close(self):\n        \"\"\"Close both loggers\"\"\"\n        self.tb_writer.close()\n        wandb.finish()\n\n# Usage\nlogger = UnifiedLogger(\n    project_name=\"my-project\",\n    experiment_name=\"exp-001\",\n    config={\"lr\": 0.001, \"batch_size\": 32}\n)\n\nlogger.log_metrics({\n    \"train_loss\": 0.5,\n    \"val_loss\": 0.6\n}, step=epoch)\n\nlogger.close()\n```\n\n## Common Monitoring Patterns\n\n### 1. Training Loop Integration\n\n```python\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        loss = train_step(model, data, target, optimizer)\n        train_loss += loss.item()\n\n        # Log batch-level metrics\n        global_step = epoch * len(train_loader) + batch_idx\n        logger.log_metrics({\n            \"batch_loss\": loss.item(),\n            \"learning_rate\": optimizer.param_groups[0]['lr']\n        }, step=global_step)\n\n    # Validation phase\n    model.eval()\n    val_loss, val_acc = validate(model, val_loader)\n\n    # Log epoch-level metrics\n    logger.log_metrics({\n        \"epoch\": epoch,\n        \"train_loss\": train_loss / len(train_loader),\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc\n    }, step=epoch)\n\n    # Log model weights distribution\n    for name, param in model.named_parameters():\n        logger.tb_writer.add_histogram(f'weights/{name}', param, epoch)\n```\n\n### 2. Experiment Comparison\n\n**TensorBoard:**\n```bash\n# Compare multiple runs\ntensorboard --logdir_spec \\\n  exp1:runs/experiment_1,\\\n  exp2:runs/experiment_2,\\\n  exp3:runs/experiment_3\n```\n\n**WandB:**\n```python\n# Automatically compares all runs in project dashboard\n# Filter and group runs by tags, config values, or custom fields\n```\n\n### 3. Real-Time Monitoring\n\n**TensorBoard:**\n```bash\n# Auto-reload new data\ntensorboard --logdir runs --reload_interval 5\n```\n\n**WandB:**\n```python\n# Real-time by default\n# Enable email/slack alerts for key metrics\nwandb.alert(\n    title=\"Training Alert\",\n    text=f\"Accuracy reached {acc:.2%}\",\n    level=wandb.AlertLevel.INFO\n)\n```\n\n## Best Practices\n\n### 1. Metric Naming Conventions\n\n**Organize by category:**\n```python\n# Good: Hierarchical naming\n\"Loss/train\"\n\"Loss/validation\"\n\"Accuracy/train\"\n\"Accuracy/validation\"\n\"Metrics/precision\"\n\"Metrics/recall\"\n\n# Bad: Flat naming\n\"train_loss\"\n\"validation_loss\"\n\"train_accuracy\"\n```\n\n### 2. Logging Frequency\n\n**Guidelines:**\n- Scalars: Every batch or every N batches\n- Histograms: Every epoch\n- Images: Every epoch or every N epochs\n- Model graph: Once at start\n- Embeddings: Once per major checkpoint\n\n```python\n# Log batch metrics every 10 batches\nif batch_idx % 10 == 0:\n    logger.log_metrics({\"batch_loss\": loss}, step)\n\n# Log epoch metrics\nif batch_idx == len(train_loader) - 1:\n    logger.log_metrics({\"epoch_loss\": epoch_loss}, epoch)\n\n# Log images every 5 epochs\nif epoch % 5 == 0:\n    logger.log_images({\"samples\": sample_images}, epoch)\n```\n\n### 3. Disk Space Management\n\n**TensorBoard:**\n```bash\n# Limit log retention\nfind runs/ -type d -mtime +30 -exec rm -rf {} +\n\n# Compress old logs\ntar -czf archive_$(date +%Y%m%d).tar.gz runs/old_experiments/\nrm -rf runs/old_experiments/\n```\n\n**WandB:**\n```python\n# Cloud storage handles retention\n# Configure retention in project settings\n# Download important runs for local backup\nwandb.restore('model.pth', run_path=\"user/project/run_id\")\n```\n\n### 4. Security & Privacy\n\n**TensorBoard:**\n```bash\n# Restrict access to localhost only\ntensorboard --logdir runs --host 127.0.0.1\n\n# Or use SSH tunnel for remote access\nssh -L 6006:localhost:6006 user@remote-server\n```\n\n**WandB:**\n```python\n# Use private projects\nwandb.init(project=\"my-project\", entity=\"private-team\")\n\n# Disable cloud sync for sensitive data\nwandb.init(mode=\"offline\")  # Logs locally only\n```\n\n## Troubleshooting\n\n### TensorBoard Issues\n\n**Problem: Dashboard not updating**\n```bash\n# Force reload\ntensorboard --logdir runs --reload_interval 1\n\n# Clear cache\nrm -rf /tmp/.tensorboard-info/\n```\n\n**Problem: Port already in use**\n```bash\n# Use different port\ntensorboard --logdir runs --port 6007\n\n# Or kill existing process\npkill -f tensorboard\n```\n\n### WandB Issues\n\n**Problem: Login fails**\n```bash\n# Re-login with API key\nwandb login --relogin\n\n# Or set via environment\nexport WANDB_API_KEY=your_api_key\n```\n\n**Problem: Slow logging**\n```python\n# Reduce logging frequency\nwandb.init(settings=wandb.Settings(\n    _disable_stats=True,  # Disable system metrics\n    _disable_meta=True    # Disable metadata\n))\n```\n\n## Scripts Usage\n\n### Setup TensorBoard\n\n```bash\n./scripts/setup-tensorboard.sh\n\n# Verifies:\n# - Python environment\n# - TensorBoard installation\n# - Creates default log directory structure\n```\n\n### Setup WandB\n\n```bash\n./scripts/setup-wandb.sh\n\n# Verifies:\n# - WandB installation\n# - API key configuration\n# - Creates wandb config file\n```\n\n### Launch Monitoring\n\n```bash\n# TensorBoard\n./scripts/launch-monitoring.sh tensorboard --logdir ./runs --port 6006\n\n# WandB (opens browser to dashboard)\n./scripts/launch-monitoring.sh wandb --project my-project\n\n# Both\n./scripts/launch-monitoring.sh both --logdir ./runs --project my-project\n```\n\n## Resources\n\n**Scripts:**\n- `setup-tensorboard.sh` - Install and configure TensorBoard\n- `setup-wandb.sh` - Install and configure WandB\n- `launch-monitoring.sh` - Launch monitoring dashboards\n\n**Templates:**\n- `tensorboard-config.yaml` - TensorBoard setup configuration\n- `wandb-config.py` - WandB integration template\n- `logging-config.json` - Unified logging configuration\n\n**Examples:**\n- `tensorboard-integration.md` - Complete TensorBoard integration\n- `wandb-integration.md` - Complete WandB integration with sweeps\n\n---\n\n**Supported Frameworks:** PyTorch, TensorFlow, JAX, Hugging Face Transformers\n**Python Version:** 3.8+\n**Best Practice:** Use both TensorBoard (local dev) and WandB (team collaboration)"
              },
              {
                "name": "training-patterns",
                "description": "Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.",
                "path": "plugins/ml-training/skills/training-patterns/SKILL.md",
                "frontmatter": {
                  "name": "training-patterns",
                  "description": "Templates and patterns for common ML training scenarios including text classification, text generation, fine-tuning, and PEFT/LoRA. Provides ready-to-use training configurations, dataset preparation scripts, and complete training pipelines. Use when building ML training pipelines, fine-tuning models, implementing classification or generation tasks, setting up PEFT/LoRA training, or when user mentions model training, fine-tuning, classification, generation, or parameter-efficient tuning.",
                  "allowed-tools": "Bash, Read, Write, Edit, Glob, Grep"
                },
                "content": "# ML Training Patterns\n\n**Purpose:** Provide production-ready training templates, configuration files, and automation scripts for common ML training scenarios including classification, generation, fine-tuning, and PEFT/LoRA approaches.\n\n**Activation Triggers:**\n- Building text classification models (sentiment, intent, NER, etc.)\n- Training text generation models (summarization, Q&A, chatbots)\n- Fine-tuning pre-trained models for specific tasks\n- Implementing PEFT (Parameter-Efficient Fine-Tuning) with LoRA\n- Setting up training pipelines with HuggingFace Transformers\n- Configuring training hyperparameters and optimization\n- Preparing datasets for model training\n\n**Key Resources:**\n- `scripts/setup-classification.sh` - Classification training setup automation\n- `scripts/setup-generation.sh` - Generation training setup automation\n- `scripts/setup-fine-tuning.sh` - Full fine-tuning setup automation\n- `scripts/setup-peft.sh` - PEFT/LoRA training setup automation\n- `templates/classification-config.yaml` - Classification training configuration\n- `templates/generation-config.yaml` - Generation training configuration\n- `templates/peft-config.json` - PEFT/LoRA configuration\n- `examples/sentiment-classifier.md` - Complete sentiment classification example\n- `examples/text-generator.md` - Complete text generation example\n\n## Training Scenarios Overview\n\n### 1. Text Classification\n\n**Use cases:** Sentiment analysis, intent classification, topic categorization, spam detection, named entity recognition (NER)\n\n**Key characteristics:**\n- Input: Text ‚Üí Output: Class label(s)\n- Typically uses encoder models (BERT, RoBERTa, DistilBERT)\n- Fast inference, suitable for production\n- Requires labeled training data\n\n**Setup command:**\n```bash\n./scripts/setup-classification.sh <project-name> <model-name> <num-classes>\n```\n\n**Example:**\n```bash\n./scripts/setup-classification.sh sentiment-model distilbert-base-uncased 3\n```\n\n### 2. Text Generation\n\n**Use cases:** Summarization, question answering, chatbots, text completion, translation, code generation\n\n**Key characteristics:**\n- Input: Text (prompt) ‚Üí Output: Generated text\n- Uses decoder or encoder-decoder models (GPT-2, T5, BART)\n- More computationally intensive\n- Can be trained with or without labeled data\n\n**Setup command:**\n```bash\n./scripts/setup-generation.sh <project-name> <model-name> <generation-type>\n```\n\n**Example:**\n```bash\n./scripts/setup-generation.sh qa-bot t5-small question-answering\n```\n\n### 3. Full Fine-Tuning\n\n**Use cases:** When you have sufficient data and compute to retrain all model parameters\n\n**Key characteristics:**\n- Updates all model weights\n- Requires significant compute (GPU with 16GB+ VRAM)\n- Best for substantial domain adaptation\n- Training time: hours to days\n\n**Setup command:**\n```bash\n./scripts/setup-fine-tuning.sh <project-name> <model-name> <task-type>\n```\n\n**Example:**\n```bash\n./scripts/setup-fine-tuning.sh medical-classifier bert-base-uncased classification\n```\n\n### 4. PEFT (Parameter-Efficient Fine-Tuning)\n\n**Use cases:** Limited compute resources, quick experimentation, domain adaptation with small datasets\n\n**Key characteristics:**\n- Only trains a small subset of parameters (LoRA adapters)\n- 10-100x less memory than full fine-tuning\n- Fast training (minutes to hours)\n- Can fine-tune large models (7B+) on consumer GPUs\n- Uses techniques like LoRA, QLoRA, Prefix Tuning, Adapter Layers\n\n**Setup command:**\n```bash\n./scripts/setup-peft.sh <project-name> <model-name> <peft-method>\n```\n\n**Example:**\n```bash\n./scripts/setup-peft.sh efficient-classifier roberta-base lora\n```\n\n## Classification Training Pattern\n\n### Configuration Template\n\n**File:** `templates/classification-config.yaml`\n\n**Key parameters:**\n```yaml\nmodel:\n  name: distilbert-base-uncased\n  num_labels: 3\n  task_type: classification\n\ndataset:\n  train_file: data/train.csv\n  validation_file: data/val.csv\n  test_file: data/test.csv\n  text_column: text\n  label_column: label\n\ntraining:\n  output_dir: ./outputs\n  num_epochs: 3\n  batch_size: 16\n  learning_rate: 2e-5\n  warmup_steps: 500\n  weight_decay: 0.01\n  evaluation_strategy: epoch\n  save_strategy: epoch\n  logging_steps: 100\n  fp16: true  # Mixed precision training\n  gradient_accumulation_steps: 1\n\noptimizer:\n  name: adamw\n  betas: [0.9, 0.999]\n  epsilon: 1e-8\n```\n\n### Training Pipeline\n\n**1. Dataset Preparation:**\n```python\nfrom datasets import load_dataset\n\n# Load from CSV\ndataset = load_dataset('csv', data_files={\n    'train': 'data/train.csv',\n    'validation': 'data/val.csv',\n    'test': 'data/test.csv'\n})\n\n# Preprocess\ndef preprocess(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        padding='max_length',\n        max_length=512\n    )\n\ndataset = dataset.map(preprocess, batched=True)\n```\n\n**2. Model Initialization:**\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_classes,\n    id2label={0: 'negative', 1: 'neutral', 2: 'positive'},\n    label2id={'negative': 0, 'neutral': 1, 'positive': 2}\n)\n```\n\n**3. Training:**\n```python\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir='./outputs',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=2e-5,\n    warmup_steps=500,\n    weight_decay=0.01,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    fp16=True,  # Enable mixed precision\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```\n\n**4. Evaluation:**\n```python\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n\n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n# Evaluate on test set\nresults = trainer.evaluate(dataset['test'])\nprint(results)\n```\n\n## Generation Training Pattern\n\n### Configuration Template\n\n**File:** `templates/generation-config.yaml`\n\n**Key parameters:**\n```yaml\nmodel:\n  name: t5-small\n  task_type: generation\n  generation_type: question-answering  # or summarization, translation, etc.\n\ndataset:\n  train_file: data/train.json\n  validation_file: data/val.json\n  input_column: question\n  target_column: answer\n  max_input_length: 512\n  max_target_length: 128\n\ntraining:\n  output_dir: ./outputs\n  num_epochs: 5\n  batch_size: 8\n  learning_rate: 3e-4\n  warmup_steps: 1000\n  weight_decay: 0.01\n  evaluation_strategy: steps\n  eval_steps: 500\n  save_steps: 500\n  logging_steps: 100\n  fp16: true\n  gradient_accumulation_steps: 2\n  predict_with_generate: true\n\ngeneration:\n  max_length: 128\n  min_length: 10\n  num_beams: 4\n  length_penalty: 2.0\n  early_stopping: true\n  no_repeat_ngram_size: 3\n```\n\n### Training Pipeline\n\n**1. Dataset Preparation:**\n```python\nfrom datasets import load_dataset\n\n# Load from JSON (question-answer pairs)\ndataset = load_dataset('json', data_files={\n    'train': 'data/train.json',\n    'validation': 'data/val.json'\n})\n\n# Preprocess for seq2seq\ndef preprocess(examples):\n    inputs = tokenizer(\n        examples['question'],\n        max_length=512,\n        truncation=True,\n        padding='max_length'\n    )\n\n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        targets = tokenizer(\n            examples['answer'],\n            max_length=128,\n            truncation=True,\n            padding='max_length'\n        )\n\n    inputs['labels'] = targets['input_ids']\n    return inputs\n\ndataset = dataset.map(preprocess, batched=True)\n```\n\n**2. Model & Training:**\n```python\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./outputs',\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=3e-4,\n    predict_with_generate=True,\n    generation_max_length=128,\n    generation_num_beams=4,\n    fp16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n)\n\ntrainer.train()\n```\n\n**3. Generation & Evaluation:**\n```python\n# Generate predictions\ndef generate_answer(question):\n    inputs = tokenizer(question, return_tensors='pt', max_length=512, truncation=True)\n    outputs = model.generate(\n        **inputs,\n        max_length=128,\n        num_beams=4,\n        length_penalty=2.0,\n        early_stopping=True\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test\nquestion = \"What is machine learning?\"\nanswer = generate_answer(question)\nprint(f\"Q: {question}\\nA: {answer}\")\n```\n\n## PEFT/LoRA Training Pattern\n\n### Why PEFT/LoRA?\n\n**Traditional fine-tuning challenges:**\n- Requires updating all model parameters (millions to billions)\n- High GPU memory requirements (often 40GB+ for 7B models)\n- Slow training (hours to days)\n- Risk of catastrophic forgetting\n\n**PEFT/LoRA benefits:**\n- Only trains ~0.1-1% of parameters (LoRA adapters)\n- 10-100x less memory usage\n- 3-10x faster training\n- Can fine-tune 7B+ models on consumer GPUs (RTX 3090, 4090)\n- Multiple task adapters for same base model\n\n### Configuration Template\n\n**File:** `templates/peft-config.json`\n\n```json\n{\n  \"peft_type\": \"LORA\",\n  \"task_type\": \"SEQ_CLS\",\n  \"inference_mode\": false,\n  \"r\": 8,\n  \"lora_alpha\": 16,\n  \"lora_dropout\": 0.1,\n  \"target_modules\": [\n    \"query\",\n    \"key\",\n    \"value\",\n    \"dense\"\n  ],\n  \"bias\": \"none\",\n  \"modules_to_save\": [\"classifier\"]\n}\n```\n\n**Key parameters:**\n- `r`: LoRA rank (lower = fewer parameters, typically 4-64)\n- `lora_alpha`: Scaling factor (typically 2x rank)\n- `lora_dropout`: Dropout for LoRA layers (0.05-0.1)\n- `target_modules`: Which layers to apply LoRA (query, key, value, dense)\n\n### Training Pipeline\n\n**1. Install PEFT:**\n```bash\npip install peft\n```\n\n**2. Setup PEFT Model:**\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Load base model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=3\n)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=['query', 'key', 'value', 'dense']\n)\n\n# Apply PEFT\nmodel = get_peft_model(base_model, peft_config)\nmodel.print_trainable_parameters()\n# Output: trainable params: 296,448 || all params: 124,940,546 || trainable%: 0.237%\n```\n\n**3. Training:**\n```python\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir='./peft_outputs',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,  # Can use larger batch size!\n    learning_rate=1e-3,  # Higher learning rate for PEFT\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n)\n\ntrainer.train()\n```\n\n**4. Save & Load Adapters:**\n```python\n# Save only LoRA adapters (tiny file, ~1-10MB)\nmodel.save_pretrained('./lora_adapters')\n\n# Load adapters later\nfrom peft import PeftModel\nbase_model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\nmodel = PeftModel.from_pretrained(base_model, './lora_adapters')\n```\n\n### QLoRA (Quantized LoRA)\n\nFor even more memory efficiency with large models:\n\n```python\nfrom transformers import BitsAndBytesConfig\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model in 4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    'meta-llama/Llama-2-7b-hf',\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\n# Apply LoRA on top of quantized model\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Now can fine-tune 7B model on 16GB GPU!\n```\n\n## Setup Scripts Usage\n\n### Classification Setup\n\n```bash\ncd /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/ml-training/skills/training-patterns\n./scripts/setup-classification.sh my-classifier distilbert-base-uncased 3\n```\n\n**Creates:**\n- Project directory structure\n- Training script with Trainer API\n- Configuration file (classification-config.yaml)\n- Dataset preparation script\n- Requirements.txt\n- README with instructions\n\n**Arguments:**\n- `project-name`: Name of training project\n- `model-name`: HuggingFace model identifier\n- `num-classes`: Number of classification labels\n\n### Generation Setup\n\n```bash\n./scripts/setup-generation.sh my-generator t5-small summarization\n```\n\n**Creates:**\n- Seq2Seq training pipeline\n- Generation configuration\n- Dataset processing for input-target pairs\n- Evaluation with ROUGE/BLEU metrics\n- Inference script\n\n**Arguments:**\n- `project-name`: Name of training project\n- `model-name`: HuggingFace model identifier\n- `generation-type`: summarization, question-answering, translation, etc.\n\n### Fine-Tuning Setup\n\n```bash\n./scripts/setup-fine-tuning.sh domain-model bert-base-uncased classification\n```\n\n**Creates:**\n- Full fine-tuning pipeline\n- GPU memory optimization configs\n- Gradient checkpointing setup\n- Mixed precision training\n- Model checkpointing strategy\n\n**Arguments:**\n- `project-name`: Name of training project\n- `model-name`: HuggingFace model identifier\n- `task-type`: classification or generation\n\n### PEFT Setup\n\n```bash\n./scripts/setup-peft.sh efficient-trainer roberta-base lora\n```\n\n**Creates:**\n- PEFT training pipeline with LoRA\n- Adapter configuration\n- Memory-efficient training setup\n- Adapter save/load utilities\n- Multi-adapter management\n\n**Arguments:**\n- `project-name`: Name of training project\n- `model-name`: HuggingFace model identifier\n- `peft-method`: lora, qlora, prefix-tuning, or adapter\n\n## Dataset Formats\n\n### Classification Dataset (CSV)\n\n```csv\ntext,label\n\"This product is amazing!\",positive\n\"Terrible experience\",negative\n\"It's okay, nothing special\",neutral\n```\n\n### Generation Dataset (JSON)\n\n```json\n[\n  {\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"The capital of France is Paris.\"\n  },\n  {\n    \"question\": \"How does photosynthesis work?\",\n    \"answer\": \"Photosynthesis is the process where plants convert light energy into chemical energy...\"\n  }\n]\n```\n\n### HuggingFace Datasets Integration\n\n```python\nfrom datasets import load_dataset\n\n# Load from HuggingFace Hub\ndataset = load_dataset('glue', 'sst2')  # Sentiment classification\ndataset = load_dataset('squad')  # Question answering\ndataset = load_dataset('cnn_dailymail', '3.0.0')  # Summarization\n\n# Load local files\ndataset = load_dataset('csv', data_files='data.csv')\ndataset = load_dataset('json', data_files='data.json')\n```\n\n## Training Best Practices\n\n### 1. Hyperparameter Selection\n\n**Learning Rate:**\n- Full fine-tuning: 1e-5 to 5e-5\n- PEFT/LoRA: 1e-4 to 1e-3 (can be higher)\n- Rule of thumb: Start with 2e-5 for full, 3e-4 for PEFT\n\n**Batch Size:**\n- As large as GPU memory allows\n- Use gradient accumulation if batch size limited\n- Effective batch size = batch_size √ó gradient_accumulation_steps\n\n**Epochs:**\n- Classification: 3-5 epochs\n- Generation: 5-10 epochs\n- Watch for overfitting with validation metrics\n\n**Warmup Steps:**\n- Typically 10% of total training steps\n- Helps stabilize training initially\n\n### 2. GPU Memory Optimization\n\n**Techniques:**\n- Mixed precision (fp16/bf16): 2x memory reduction\n- Gradient checkpointing: 30-50% memory reduction (slower training)\n- Gradient accumulation: Simulate larger batch sizes\n- PEFT/LoRA: 10-100x memory reduction\n- 8-bit/4-bit quantization: 2-4x memory reduction\n\n**Example:**\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    fp16=True,  # Mixed precision\n    gradient_checkpointing=True,  # Memory optimization\n    gradient_accumulation_steps=4,  # Effective batch size √ó 4\n    per_device_train_batch_size=4,  # Small batch per GPU\n)\n```\n\n### 3. Monitoring Training\n\n**Track these metrics:**\n- Training loss (should decrease steadily)\n- Validation loss (should decrease, not increase)\n- Validation accuracy/F1/ROUGE (should increase)\n- Learning rate schedule\n- GPU memory usage\n\n**Use Weights & Biases:**\n```python\ntraining_args = TrainingArguments(\n    report_to='wandb',\n    run_name='my-training-run',\n)\n```\n\n### 4. Early Stopping\n\n```python\nfrom transformers import EarlyStoppingCallback\n\ntrainer = Trainer(\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n```\n\n### 5. Model Checkpointing\n\n```python\ntraining_args = TrainingArguments(\n    save_strategy='epoch',  # Save after each epoch\n    save_total_limit=3,  # Keep only best 3 checkpoints\n    load_best_model_at_end=True,  # Load best after training\n    metric_for_best_model='f1',  # Choose best by F1 score\n)\n```\n\n## Common Training Patterns\n\n### Pattern 1: Quick Experimentation (PEFT)\n\n**When:** Testing ideas, limited compute, small datasets\n**Approach:** LoRA with small rank (r=4-8)\n**Time:** Minutes to 1 hour\n**Memory:** Can fine-tune 7B models on 16GB GPU\n\n### Pattern 2: Production Classification (Full Fine-Tuning)\n\n**When:** Production deployment, sufficient labeled data\n**Approach:** Full fine-tuning with early stopping\n**Time:** 1-6 hours\n**Memory:** 16GB GPU for base models (110M-340M params)\n\n### Pattern 3: Domain Adaptation (PEFT + Full)\n\n**When:** Adapting to specific domain, then task-specific fine-tuning\n**Approach:**\n1. PEFT on domain data (unlabeled or weakly labeled)\n2. Full fine-tuning on task data (labeled)\n**Time:** 2-12 hours total\n**Memory:** 16-40GB GPU\n\n### Pattern 4: Multi-Task Learning (Multiple LoRA Adapters)\n\n**When:** One model for multiple tasks\n**Approach:** Train separate LoRA adapters per task, swap at inference\n**Time:** 1-3 hours per task\n**Memory:** 16GB GPU, adapters are tiny (1-10MB each)\n\n## Troubleshooting\n\n**Out of Memory (OOM) Errors:**\n- Reduce batch size\n- Enable gradient checkpointing\n- Use gradient accumulation\n- Switch to PEFT/LoRA\n- Use 8-bit quantization\n\n**Training Not Converging:**\n- Lower learning rate\n- Increase warmup steps\n- Check data quality and preprocessing\n- Verify labels are correct\n- Try different optimizer (AdamW vs SGD)\n\n**Overfitting:**\n- Add dropout\n- Use weight decay\n- Get more training data\n- Use data augmentation\n- Early stopping\n\n**Slow Training:**\n- Enable fp16 mixed precision\n- Increase batch size\n- Use gradient accumulation less\n- Remove gradient checkpointing\n- Use faster model variant (distilbert vs bert)\n\n**Poor Evaluation Metrics:**\n- Check data distribution (train vs val vs test)\n- Verify preprocessing is consistent\n- Try different model architecture\n- Increase model size or training time\n- Check for label imbalance\n\n---\n\n**Supported Models:**\n- Classification: BERT, RoBERTa, DistilBERT, ALBERT, DeBERTa\n- Generation: T5, BART, GPT-2, Llama-2, Mistral, Phi\n- PEFT: Compatible with all transformer models\n\n**Requirements:**\n- Python 3.11+\n- PyTorch 2.0+\n- Transformers 4.30+\n- PEFT 0.7+ (for LoRA)\n- Datasets 2.14+\n\n**Best Practice:** Start with PEFT/LoRA for quick iteration, switch to full fine-tuning only when necessary"
              },
              {
                "name": "validation-scripts",
                "description": "Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.",
                "path": "plugins/ml-training/skills/validation-scripts/SKILL.md",
                "frontmatter": {
                  "name": "validation-scripts",
                  "description": "Data validation and pipeline testing utilities for ML training projects. Validates datasets, model checkpoints, training pipelines, and dependencies. Use when validating training data, checking model outputs, testing ML pipelines, verifying dependencies, debugging training failures, or ensuring data quality before training.",
                  "allowed-tools": "Bash, Read, Write, Grep, Glob, Edit"
                },
                "content": "# ML Training Validation Scripts\n\n**Purpose:** Production-ready validation and testing utilities for ML training workflows. Ensures data quality, model integrity, pipeline correctness, and dependency availability before and during training.\n\n**Activation Triggers:**\n- Validating training datasets before fine-tuning\n- Checking model checkpoints and outputs\n- Testing end-to-end training pipelines\n- Verifying system dependencies and GPU availability\n- Debugging training failures or data issues\n- Ensuring data format compliance\n- Validating model configurations\n- Testing inference pipelines\n\n**Key Resources:**\n- `scripts/validate-data.sh` - Comprehensive dataset validation\n- `scripts/validate-model.sh` - Model checkpoint and config validation\n- `scripts/test-pipeline.sh` - End-to-end pipeline testing\n- `scripts/check-dependencies.sh` - System and package dependency checking\n- `templates/test-config.yaml` - Pipeline test configuration template\n- `templates/validation-schema.json` - Data validation schema template\n- `examples/data-validation-example.md` - Dataset validation workflow\n- `examples/pipeline-testing-example.md` - Complete pipeline testing guide\n\n## Quick Start\n\n### Validate Training Data\n```bash\nbash scripts/validate-data.sh \\\n  --data-path ./data/train.jsonl \\\n  --format jsonl \\\n  --schema templates/validation-schema.json\n```\n\n### Validate Model Checkpoint\n```bash\nbash scripts/validate-model.sh \\\n  --model-path ./checkpoints/epoch-3 \\\n  --framework pytorch \\\n  --check-weights\n```\n\n### Test Complete Pipeline\n```bash\nbash scripts/test-pipeline.sh \\\n  --config templates/test-config.yaml \\\n  --data ./data/sample.jsonl \\\n  --verbose\n```\n\n### Check System Dependencies\n```bash\nbash scripts/check-dependencies.sh \\\n  --framework pytorch \\\n  --gpu-required \\\n  --min-vram 16\n```\n\n## Validation Scripts\n\n### 1. Data Validation (validate-data.sh)\n\n**Purpose:** Validate training datasets for format compliance, data quality, and schema conformance.\n\n**Usage:**\n```bash\nbash scripts/validate-data.sh [OPTIONS]\n```\n\n**Options:**\n- `--data-path PATH` - Path to dataset file or directory (required)\n- `--format FORMAT` - Data format: jsonl, csv, parquet, arrow (default: jsonl)\n- `--schema PATH` - Path to validation schema JSON file\n- `--sample-size N` - Number of samples to validate (default: all)\n- `--check-duplicates` - Check for duplicate entries\n- `--check-null` - Check for null/missing values\n- `--check-length` - Validate text length constraints\n- `--check-tokens` - Validate tokenization compatibility\n- `--tokenizer MODEL` - Tokenizer to use for token validation (default: gpt2)\n- `--max-length N` - Maximum sequence length (default: 2048)\n- `--output REPORT` - Output validation report path (default: validation-report.json)\n\n**Validation Checks:**\n- **Format Compliance**: Verify file format and structure\n- **Schema Validation**: Check against JSON schema if provided\n- **Data Types**: Validate field types (string, int, float, etc.)\n- **Required Fields**: Ensure all required fields are present\n- **Value Ranges**: Check numeric values within expected ranges\n- **Text Quality**: Detect empty strings, excessive whitespace, encoding issues\n- **Duplicates**: Identify duplicate entries (exact or near-duplicate)\n- **Token Counts**: Verify sequences fit within model context length\n- **Label Distribution**: Check class balance for classification tasks\n- **Missing Values**: Detect and report null/missing values\n\n**Example Output:**\n```json\n{\n  \"status\": \"PASS\",\n  \"total_samples\": 10000,\n  \"valid_samples\": 9987,\n  \"invalid_samples\": 13,\n  \"validation_errors\": [\n    {\n      \"sample_id\": 42,\n      \"field\": \"text\",\n      \"error\": \"Exceeds max token length: 2150 > 2048\"\n    },\n    {\n      \"sample_id\": 156,\n      \"field\": \"label\",\n      \"error\": \"Invalid label value: 'unknwn' (typo)\"\n    }\n  ],\n  \"statistics\": {\n    \"avg_text_length\": 487,\n    \"avg_token_count\": 128,\n    \"label_distribution\": {\n      \"positive\": 4892,\n      \"negative\": 4895,\n      \"neutral\": 213\n    }\n  },\n  \"recommendations\": [\n    \"Remove or fix 13 invalid samples before training\",\n    \"Label distribution is imbalanced - consider class weighting\"\n  ]\n}\n```\n\n**Exit Codes:**\n- `0` - All validations passed\n- `1` - Validation errors found\n- `2` - Script error (invalid arguments, file not found)\n\n### 2. Model Validation (validate-model.sh)\n\n**Purpose:** Validate model checkpoints, configurations, and inference readiness.\n\n**Usage:**\n```bash\nbash scripts/validate-model.sh [OPTIONS]\n```\n\n**Options:**\n- `--model-path PATH` - Path to model checkpoint directory (required)\n- `--framework FRAMEWORK` - Framework: pytorch, tensorflow, jax (default: pytorch)\n- `--check-weights` - Verify model weights are loadable\n- `--check-config` - Validate model configuration file\n- `--check-tokenizer` - Verify tokenizer files are present\n- `--check-inference` - Test inference with sample input\n- `--sample-input TEXT` - Sample text for inference test\n- `--expected-output TEXT` - Expected output for verification\n- `--output REPORT` - Output validation report path\n\n**Validation Checks:**\n- **File Structure**: Verify required files are present\n- **Config Validation**: Check model_config.json for correctness\n- **Weight Integrity**: Load and verify model weights\n- **Tokenizer Files**: Ensure tokenizer files exist and are loadable\n- **Model Architecture**: Validate architecture matches config\n- **Memory Requirements**: Estimate GPU/CPU memory needed\n- **Inference Test**: Run sample inference if requested\n- **Quantization**: Verify quantized models load correctly\n- **LoRA/PEFT**: Validate adapter weights and configuration\n\n**Example Output:**\n```json\n{\n  \"status\": \"PASS\",\n  \"model_path\": \"./checkpoints/llama-7b-finetuned\",\n  \"framework\": \"pytorch\",\n  \"checks\": {\n    \"file_structure\": \"PASS\",\n    \"config\": \"PASS\",\n    \"weights\": \"PASS\",\n    \"tokenizer\": \"PASS\",\n    \"inference\": \"PASS\"\n  },\n  \"model_info\": {\n    \"architecture\": \"LlamaForCausalLM\",\n    \"parameters\": \"7.2B\",\n    \"precision\": \"float16\",\n    \"lora_enabled\": true,\n    \"lora_rank\": 16\n  },\n  \"memory_estimate\": {\n    \"model_size_gb\": 13.5,\n    \"inference_vram_gb\": 16.2,\n    \"training_vram_gb\": 24.8\n  },\n  \"inference_test\": {\n    \"input\": \"Hello, world!\",\n    \"output\": \"Hello, world! How can I help you today?\",\n    \"latency_ms\": 142\n  }\n}\n```\n\n### 3. Pipeline Testing (test-pipeline.sh)\n\n**Purpose:** Test end-to-end ML training and inference pipelines with sample data.\n\n**Usage:**\n```bash\nbash scripts/test-pipeline.sh [OPTIONS]\n```\n\n**Options:**\n- `--config PATH` - Pipeline test configuration file (required)\n- `--data PATH` - Sample data for testing\n- `--steps STEPS` - Comma-separated steps to test (default: all)\n- `--verbose` - Enable detailed output\n- `--output REPORT` - Output test report path\n- `--fail-fast` - Stop on first failure\n- `--cleanup` - Clean up temporary files after test\n\n**Pipeline Steps:**\n- **data_loading**: Test data loading and preprocessing\n- **tokenization**: Verify tokenization pipeline\n- **model_loading**: Load model and verify initialization\n- **training_step**: Run single training step\n- **validation_step**: Run single validation step\n- **checkpoint_save**: Test checkpoint saving\n- **checkpoint_load**: Test checkpoint loading\n- **inference**: Test inference pipeline\n- **metrics**: Verify metrics calculation\n\n**Test Configuration (test-config.yaml):**\n```yaml\npipeline:\n  name: llama-7b-finetuning\n  framework: pytorch\n\ndata:\n  train_path: ./data/sample-train.jsonl\n  val_path: ./data/sample-val.jsonl\n  format: jsonl\n\nmodel:\n  base_model: meta-llama/Llama-2-7b-hf\n  checkpoint_path: ./checkpoints/test\n  load_in_8bit: false\n  lora:\n    enabled: true\n    r: 16\n    alpha: 32\n\ntraining:\n  batch_size: 1\n  gradient_accumulation: 1\n  learning_rate: 2e-4\n  max_steps: 5\n\ntesting:\n  sample_size: 10\n  timeout_seconds: 300\n  expected_loss_range: [0.5, 3.0]\n```\n\n**Example Output:**\n```\nPipeline Test Report\n====================\n\nPipeline: llama-7b-finetuning\nStarted: 2025-11-01 12:34:56\nDuration: 127 seconds\n\nTest Results:\n‚úì data_loading (2.3s) - Loaded 10 samples successfully\n‚úì tokenization (1.1s) - Tokenized all samples, avg length: 128 tokens\n‚úì model_loading (8.7s) - Model loaded, 7.2B parameters\n‚úì training_step (15.4s) - Training step completed, loss: 1.847\n‚úì validation_step (12.1s) - Validation step completed, loss: 1.923\n‚úì checkpoint_save (3.2s) - Checkpoint saved to ./checkpoints/test\n‚úì checkpoint_load (6.8s) - Checkpoint loaded successfully\n‚úì inference (2.9s) - Inference completed, latency: 142ms\n‚úì metrics (0.4s) - Metrics calculated correctly\n\nOverall: PASS (8/8 tests passed)\n\nPerformance Metrics:\n- Total time: 127s\n- GPU memory used: 15.2 GB\n- CPU memory used: 8.4 GB\n\nRecommendations:\n- Pipeline is ready for full training\n- Consider increasing batch_size to improve throughput\n```\n\n### 4. Dependency Checking (check-dependencies.sh)\n\n**Purpose:** Verify all required system dependencies, packages, and GPU availability.\n\n**Usage:**\n```bash\nbash scripts/check-dependencies.sh [OPTIONS]\n```\n\n**Options:**\n- `--framework FRAMEWORK` - ML framework: pytorch, tensorflow, jax (default: pytorch)\n- `--gpu-required` - Require GPU availability\n- `--min-vram GB` - Minimum GPU VRAM required (GB)\n- `--cuda-version VERSION` - Required CUDA version\n- `--packages FILE` - Path to requirements.txt or packages list\n- `--platform PLATFORM` - Platform: modal, lambda, runpod, local (default: local)\n- `--fix` - Attempt to install missing packages\n- `--output REPORT` - Output dependency report path\n\n**Dependency Checks:**\n- **Python Version**: Verify compatible Python version\n- **ML Framework**: Check PyTorch/TensorFlow/JAX installation\n- **CUDA/cuDNN**: Verify CUDA toolkit and cuDNN\n- **GPU Availability**: Detect and validate GPUs\n- **VRAM**: Check GPU memory capacity\n- **System Packages**: Verify system-level dependencies\n- **Python Packages**: Check required pip packages\n- **Platform Tools**: Validate platform-specific tools (Modal, Lambda CLI)\n- **Storage**: Check available disk space\n- **Network**: Test internet connectivity for model downloads\n\n**Example Output:**\n```json\n{\n  \"status\": \"PASS\",\n  \"platform\": \"modal\",\n  \"checks\": {\n    \"python\": {\n      \"status\": \"PASS\",\n      \"version\": \"3.10.12\",\n      \"required\": \">=3.9\"\n    },\n    \"pytorch\": {\n      \"status\": \"PASS\",\n      \"version\": \"2.1.0\",\n      \"cuda_available\": true,\n      \"cuda_version\": \"12.1\"\n    },\n    \"gpu\": {\n      \"status\": \"PASS\",\n      \"count\": 1,\n      \"type\": \"NVIDIA A100\",\n      \"vram_gb\": 40,\n      \"driver_version\": \"535.129.03\"\n    },\n    \"packages\": {\n      \"status\": \"PASS\",\n      \"installed\": 42,\n      \"missing\": 0,\n      \"outdated\": 3\n    },\n    \"storage\": {\n      \"status\": \"PASS\",\n      \"available_gb\": 128,\n      \"required_gb\": 50\n    }\n  },\n  \"recommendations\": [\n    \"Update transformers to latest version (4.36.0 -> 4.37.2)\",\n    \"Consider upgrading to PyTorch 2.2.0 for better performance\"\n  ]\n}\n```\n\n## Templates\n\n### Validation Schema Template (templates/validation-schema.json)\n\nDefines expected data structure and validation rules for training datasets.\n\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"required\": [\"text\", \"label\"],\n  \"properties\": {\n    \"text\": {\n      \"type\": \"string\",\n      \"minLength\": 10,\n      \"maxLength\": 5000,\n      \"description\": \"Input text for training\"\n    },\n    \"label\": {\n      \"type\": \"string\",\n      \"enum\": [\"positive\", \"negative\", \"neutral\"],\n      \"description\": \"Classification label\"\n    },\n    \"metadata\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"source\": {\n          \"type\": \"string\"\n        },\n        \"timestamp\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        }\n      }\n    }\n  },\n  \"validation_rules\": {\n    \"max_token_length\": 2048,\n    \"tokenizer\": \"meta-llama/Llama-2-7b-hf\",\n    \"check_duplicates\": true,\n    \"min_label_count\": 100,\n    \"max_label_imbalance_ratio\": 10.0\n  }\n}\n```\n\n**Customization:**\n- Modify `required` fields for your use case\n- Add custom properties and validation rules\n- Set appropriate length constraints\n- Define label enums for classification\n- Configure tokenizer and max length\n\n### Test Configuration Template (templates/test-config.yaml)\n\nDefines pipeline testing parameters and expected behaviors.\n\n```yaml\npipeline:\n  name: test-pipeline\n  framework: pytorch\n  platform: modal  # modal, lambda, runpod, local\n\ndata:\n  train_path: ./data/sample-train.jsonl\n  val_path: ./data/sample-val.jsonl\n  format: jsonl  # jsonl, csv, parquet\n  sample_size: 10  # Number of samples to use for testing\n\nmodel:\n  base_model: meta-llama/Llama-2-7b-hf\n  checkpoint_path: ./checkpoints/test\n  quantization: null  # null, 8bit, 4bit\n  lora:\n    enabled: true\n    r: 16\n    alpha: 32\n    dropout: 0.05\n    target_modules: [\"q_proj\", \"v_proj\"]\n\ntraining:\n  batch_size: 1\n  gradient_accumulation: 1\n  learning_rate: 2e-4\n  max_steps: 5\n  warmup_steps: 0\n  eval_steps: 2\n\ntesting:\n  sample_size: 10\n  timeout_seconds: 300\n  expected_loss_range: [0.5, 3.0]\n  fail_fast: true\n  cleanup: true\n\nvalidation:\n  metrics: [\"loss\", \"perplexity\"]\n  check_gradients: true\n  check_memory_leak: true\n\ngpu:\n  required: true\n  min_vram_gb: 16\n  allow_cpu_fallback: false\n```\n\n## Integration with ML Training Workflow\n\n### Pre-Training Validation\n```bash\n# 1. Check system dependencies\nbash scripts/check-dependencies.sh \\\n  --framework pytorch \\\n  --gpu-required \\\n  --min-vram 16\n\n# 2. Validate training data\nbash scripts/validate-data.sh \\\n  --data-path ./data/train.jsonl \\\n  --schema templates/validation-schema.json \\\n  --check-duplicates \\\n  --check-tokens\n\n# 3. Test pipeline with sample data\nbash scripts/test-pipeline.sh \\\n  --config templates/test-config.yaml \\\n  --verbose\n```\n\n### Post-Training Validation\n```bash\n# 1. Validate model checkpoint\nbash scripts/validate-model.sh \\\n  --model-path ./checkpoints/final \\\n  --framework pytorch \\\n  --check-weights \\\n  --check-inference\n\n# 2. Test inference pipeline\nbash scripts/test-pipeline.sh \\\n  --config templates/test-config.yaml \\\n  --steps inference,metrics\n```\n\n### CI/CD Integration\n```bash\n# .github/workflows/validate-training.yml\n- name: Validate Training Data\n  run: |\n    bash plugins/ml-training/skills/validation-scripts/scripts/validate-data.sh \\\n      --data-path ./data/train.jsonl \\\n      --schema ./validation-schema.json\n\n- name: Test Training Pipeline\n  run: |\n    bash plugins/ml-training/skills/validation-scripts/scripts/test-pipeline.sh \\\n      --config ./test-config.yaml \\\n      --fail-fast\n```\n\n## Error Handling and Debugging\n\n### Common Validation Failures\n\n**Data Validation Failures:**\n- Token length exceeded ‚Üí Truncate or filter long sequences\n- Missing required fields ‚Üí Fix data preprocessing\n- Duplicate entries ‚Üí Deduplicate dataset\n- Invalid labels ‚Üí Clean label data\n\n**Model Validation Failures:**\n- Missing config files ‚Üí Ensure complete checkpoint\n- Weight loading errors ‚Üí Check framework compatibility\n- Tokenizer mismatch ‚Üí Verify tokenizer version\n\n**Pipeline Test Failures:**\n- Out of memory ‚Üí Reduce batch size, enable gradient checkpointing\n- CUDA errors ‚Üí Check GPU drivers, CUDA version\n- Slow performance ‚Üí Profile and optimize data loading\n\n### Debug Mode\n\nAll scripts support verbose debugging:\n```bash\nbash scripts/validate-data.sh --data-path ./data/train.jsonl --verbose --debug\n```\n\nOutputs:\n- Detailed progress logs\n- Stack traces for errors\n- Performance metrics\n- Memory usage statistics\n\n## Best Practices\n\n1. **Always validate before training** - Catch data issues early\n2. **Use schema validation** - Enforce data quality standards\n3. **Test with sample data first** - Verify pipeline before full training\n4. **Check dependencies on new platforms** - Avoid runtime failures\n5. **Validate checkpoints** - Ensure model integrity before deployment\n6. **Monitor validation metrics** - Track data quality over time\n7. **Automate validation in CI/CD** - Prevent bad data from entering pipeline\n8. **Keep validation schemas updated** - Reflect current data requirements\n\n## Performance Tips\n\n- Use `--sample-size` for quick validation of large datasets\n- Run dependency checks once per environment, cache results\n- Parallelize validation with GNU parallel for multi-file datasets\n- Use `--fail-fast` in CI/CD to save time\n- Cache tokenizers to avoid re-downloading\n\n## Troubleshooting\n\n**Script Not Found:**\n```bash\n# Ensure you're in the correct directory\ncd /path/to/ml-training/skills/validation-scripts\nbash scripts/validate-data.sh --help\n```\n\n**Permission Denied:**\n```bash\n# Make scripts executable\nchmod +x scripts/*.sh\n```\n\n**Missing Dependencies:**\n```bash\n# Install required tools\npip install jsonschema pandas pyarrow\nsudo apt-get install jq bc\n```\n\n---\n\n**Plugin**: ml-training\n**Version**: 1.0.0\n**Supported Frameworks**: PyTorch, TensorFlow, JAX\n**Platforms**: Modal, Lambda Labs, RunPod, Local"
              }
            ]
          },
          {
            "name": "plugin-docs-loader",
            "description": "Universal documentation loading system with intelligent link extraction and parallel WebFetch for all plugins",
            "source": "./plugins/plugin-docs-loader",
            "category": "infrastructure",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install plugin-docs-loader@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/load-docs",
                "description": "Load plugin documentation on-demand with intelligent link extraction and parallel WebFetch for any plugin in the marketplace",
                "path": "plugins/plugin-docs-loader/commands/load-docs.md",
                "frontmatter": {
                  "description": "Load plugin documentation on-demand with intelligent link extraction and parallel WebFetch for any plugin in the marketplace",
                  "argument-hint": "<plugin-name> [scope]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode API keys or secrets\n- Use placeholders: `your_service_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n- Document key acquisition for users\n\n# Load Plugin Documentation\n\n**Purpose:** Load fresh documentation for any plugin by extracting external links from local docs and fetching them intelligently. Automatically uses agent for large link counts (‚â•10) to handle parallel batching efficiently.\n\n## Available Skills\n\nThis commands has access to the following skills from the plugin-docs-loader plugin:\n\n- **doc-templates**: Provides reusable templates for generating documentation loading commands across all plugins\n\n**To use a skill:**\n```\n!{skill skill-name}\n```\n\nUse skills when you need:\n- Domain-specific templates and examples\n- Validation scripts and automation\n- Best practices and patterns\n- Configuration generators\n\nSkills provide pre-built resources to accelerate your work.\n\n---\n\n\n## Command Arguments\n\n**Syntax:** `/plugin-docs-loader:load-docs <plugin-name> [scope]`\n\n**Required:**\n- `<plugin-name>` - Name of the plugin (e.g., \"claude-agent-sdk\", \"vercel-ai-sdk\", \"mem0\")\n\n**Optional:**\n- `[scope]` - What to load:\n  - **Empty/core** - Load essential P0 documentation (~5-10K tokens)\n  - **all** - Load comprehensive P0+P1+P2 documentation (~20-30K tokens)\n  - **{feature-name}** - Load core + specific feature documentation (~10-15K tokens)\n\n**Examples:**\n```bash\n# Load core documentation for Claude Agent SDK\n/plugin-docs-loader:load-docs claude-agent-sdk\n\n# Load all documentation for Vercel AI SDK\n/plugin-docs-loader:load-docs vercel-ai-sdk all\n\n# Load streaming feature documentation for FastAPI Backend\n/plugin-docs-loader:load-docs fastapi-backend streaming\n\n# Load tool calling documentation for Vercel AI SDK\n/plugin-docs-loader:load-docs vercel-ai-sdk tools\n```\n\n## Workflow\n\n### Phase 1: Parse Arguments\n\n**Extract plugin name and scope from $ARGUMENTS:**\n\n!{bash\n# Parse arguments\nPLUGIN_NAME=$(echo \"$ARGUMENTS\" | awk '{print $1}')\nSCOPE=$(echo \"$ARGUMENTS\" | awk '{print $2}')\n\n# Validate plugin name\nif [ -z \"$PLUGIN_NAME\" ]; then\n  echo \"‚ùå Error: Plugin name is required\"\n  echo \"Usage: /plugin-docs-loader:load-docs <plugin-name> [scope]\"\n  exit 1\nfi\n\n# Default scope to \"core\" if empty\nif [ -z \"$SCOPE\" ]; then\n  SCOPE=\"core\"\nfi\n\n# Verify plugin exists\nPLUGIN_PATH=\"plugins/$PLUGIN_NAME\"\nif [ ! -d \"$PLUGIN_PATH\" ]; then\n  echo \"‚ùå Error: Plugin '$PLUGIN_NAME' not found at $PLUGIN_PATH\"\n  echo \"\"\n  echo \"Available plugins:\"\n  ls -1 plugins/ | grep -v \"plugin-docs-loader\"\n  exit 1\nfi\n\n# Verify docs directory exists\nDOCS_PATH=\"$PLUGIN_PATH/docs\"\nif [ ! -d \"$DOCS_PATH\" ]; then\n  echo \"‚ùå Error: No docs directory found at $DOCS_PATH\"\n  exit 1\nfi\n\necho \"üìö Loading documentation for: $PLUGIN_NAME\"\necho \"üìÇ Documentation path: $DOCS_PATH\"\necho \"üéØ Scope: $SCOPE\"\necho \"\"\n\n# Count total links in documentation\nLINK_COUNT=$(grep -rh \"https\\?://\" \"$DOCS_PATH\" 2>/dev/null | grep -o \"https\\?://[^\\s\\)\\\"\\']+\" | sort -u | wc -l)\n\necho \"üîó Found $LINK_COUNT unique external links\"\necho \"\"\n\n# Decision: Use agent if many links, handle directly if few\nif [ \"$LINK_COUNT\" -ge 10 ]; then\n  echo \"üì¶ Large link count detected - invoking agent for parallel batch processing\"\n  USE_AGENT=\"true\"\nelse\n  echo \"‚úÖ Small link count - handling directly without agent overhead\"\n  USE_AGENT=\"false\"\nfi\n}\n\n### Phase 3: Load Documentation (Agent or Direct)\n\n**Strategy Decision:**\n- **< 10 links** ‚Üí Handle directly with WebFetch (fast, no agent overhead)\n- **‚â• 10 links** ‚Üí Invoke agent for parallel batch processing (handles 60-100+ links efficiently)\n\n**If using agent:**\n\nTask(\n  description=\"Load $PLUGIN_NAME documentation with scope $SCOPE\",\n  subagent_type=\"doc-loader-agent\",\n  prompt=\"Load documentation for $PLUGIN_NAME plugin with the following parameters:\n\n  **Plugin Information:**\n  - Plugin Name: $PLUGIN_NAME\n  - Documentation Path: $DOCS_PATH\n  - Loading Scope: $SCOPE\n\n  **Instructions:**\n  1. Find all markdown documentation files in $DOCS_PATH\n  2. Extract ALL external links (URLs) from these files - there could be 60-100+ links\n  3. Categorize links by priority:\n     - P0 (Essential): overview, introduction, quickstart, getting-started\n     - P1 (Features): feature-specific documentation matching scope\n     - P2 (Advanced): advanced topics, reference, migration\n  4. Fetch documentation in parallel batches based on scope:\n     - If scope is 'core': Load P0 only (4-6 URLs)\n     - If scope is 'all': Load P0 + P1 + P2 (up to 20-30 URLs in batches)\n     - If scope is feature name: Load P0 + P1 URLs matching feature (10-15 URLs)\n  5. Execute WebFetch in parallel (4-6 URLs per batch)\n  6. Handle errors gracefully - if WebFetch fails, note it but continue\n  7. Return formatted documentation summary with all loaded content\n\n  **Deliverable:**\n  Return comprehensive documentation summary including:\n  - List of URLs fetched by priority\n  - Documentation content organized by topic\n  - Total URLs processed\n  - Any errors encountered\n  - Estimated token usage\n\n  **Important:** You MUST process all links found in the documentation. This is critical because other agents need access to ALL the documentation, not just a subset. Use parallel batching to handle large numbers of links efficiently.\"\n)\n\n**If handling directly (< 10 links):**\n\n!{bash\n# Extract unique URLs\nURLS=$(grep -rh \"https\\?://\" \"$DOCS_PATH\" 2>/dev/null | grep -o \"https\\?://[^\\s\\)\\\"\\']+\" | sort -u)\n\necho \"üì• Fetching $LINK_COUNT documentation URLs directly...\"\necho \"\"\n\n# Note: Claude will see these URLs and can WebFetch them directly\n# No need to invoke an agent for small numbers of links\necho \"URLs to fetch:\"\necho \"$URLS\"\n}\n\nWebFetch each URL found above with appropriate prompts to extract documentation content.\n\n### Phase 4: Display Results\n\n**Present loaded documentation to user:**\n\nThe agent will return:\n- ‚úÖ Documentation content organized by priority\n- ‚úÖ List of all URLs fetched\n- ‚úÖ Total link count and token usage\n- ‚úÖ Any errors or warnings\n\n**Next Steps:**\n- Review the loaded documentation\n- Use it to inform your work with other agents\n- Re-run with different scope if needed\n\n## Scope Recommendations\n\n**Use `core` when:**\n- Quick reference needed\n- Getting started with a plugin\n- Understanding basic concepts\n- Working on simple tasks\n\n**Use `{feature-name}` when:**\n- Implementing specific feature\n- Need detailed feature documentation\n- Working on feature-specific code\n- Example: `streaming`, `tools`, `auth`, etc.\n\n**Use `all` when:**\n- Comprehensive understanding needed\n- Complex implementation work\n- Need access to ALL documentation\n- Working with multiple features\n- Another agent needs full context (‚ö†Ô∏è uses 20-30K tokens)\n\n## How It Works\n\n**Automatic Strategy Selection:**\n\n1. **Command counts links** in plugin's docs/ directory\n2. **If < 10 links** ‚Üí Handles directly with WebFetch (fast, no overhead)\n3. **If ‚â• 10 links** ‚Üí Invokes agent for parallel batch processing\n\n**Why This Matters:**\n\n- **Small plugins** (< 10 links) ‚Üí Fast, direct fetching\n- **Large plugins** (10-100+ links) ‚Üí Agent handles parallel batching efficiently\n- **Prevents context overflow** ‚Üí Agent manages large numbers of WebFetch calls\n- **Optimized for other agents** ‚Üí Documentation ready to pass to other agents\n\n**Threshold:**\n- Default threshold: 10 links\n- Adjustable based on context window considerations\n- Agent can handle 60-100+ links in parallel batches\n\n## Notes\n\n- Fresh documentation is always fetched from official sources\n- Context usage scales with scope selection\n- Documentation is formatted for easy consumption by other agents\n- Agent uses parallel batching to handle large link counts without context overflow"
              }
            ],
            "skills": []
          },
          {
            "name": "payments",
            "description": "Stripe integration for payments and subscriptions with FastAPI backend, Next.js frontend, and Supabase database",
            "source": "./plugins/payments",
            "category": "SDK Integration",
            "version": "1.0.0",
            "author": {
              "name": "Plugin Builder",
              "email": "builder@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install payments@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-checkout",
                "description": "Add Stripe Checkout flow with payment intents and success/cancel pages",
                "path": "plugins/payments/commands/add-checkout.md",
                "frontmatter": {
                  "description": "Add Stripe Checkout flow with payment intents and success/cancel pages",
                  "argument-hint": "[checkout-type]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Implement complete Stripe Checkout flow with payment intents, session creation, success/cancel pages, and Supabase payment records\n\nCore Principles:\n- Never hardcode Stripe API keys - use placeholders only\n- Follow security best practices for payment handling\n- Implement proper error handling for payment failures\n- Store payment records securely in Supabase\n- Follow existing codebase patterns\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n‚ùå NEVER hardcode actual API keys or secrets\n‚ùå NEVER include real Stripe keys in examples\n‚ùå NEVER commit sensitive values to git\n\n‚úÖ ALWAYS use placeholders: `your_stripe_key_here`\n‚úÖ ALWAYS create `.env.example` with placeholders only\n‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n‚úÖ ALWAYS read from environment variables in code\n‚úÖ ALWAYS document where to obtain Stripe keys\n\n**Placeholder format:** `stripe_{env}_your_key_here`\n\nPhase 1: Discovery\nGoal: Understand project structure and existing payment setup\n\nActions:\n- Create todo list with TodoWrite for tracking progress\n- Parse $ARGUMENTS for checkout type (one-time, subscription, custom)\n- Detect project structure: !{bash ls -la | grep -E \"(package.json|requirements.txt)\"}\n- Find payment files: !{bash find . -name \"*payment*\" 2>/dev/null | head -10}\n- Load configuration: @.env.example\n- Search payment schemas with Glob\n- Update todos\n\nPhase 2: Backend Implementation\nGoal: Create Stripe Checkout session endpoint in FastAPI\n\nActions:\n\nTask(description=\"Implement Stripe Checkout backend\", subagent_type=\"payments:stripe-integration-agent\", prompt=\"You are the stripe-integration-agent. Implement Stripe Checkout backend for $ARGUMENTS.\n\nSECURITY CRITICAL: Use placeholder keys only, read from environment, never hardcode.\n\nRequirements:\n- FastAPI endpoint POST /api/checkout/create-session\n- Payment intent creation with error handling\n- Webhook endpoint POST /api/checkout/webhook\n- Supabase payment table migration\n- Session metadata storage in database\n- Success/cancel redirect URLs\n- CORS configuration\n- Pydantic request validation\n\nEnvironment: Create .env.example with placeholders, update .gitignore, document key setup.\n\nDeliverables: checkout.py, webhooks.py, payment.py model, migrations, .env.example, updated requirements.txt\")\n\nWait for agent to complete.\nUpdate todos.\n\nPhase 3: Frontend Implementation\nGoal: Build Next.js checkout page with Stripe Elements\n\nActions:\n\nTask(description=\"Implement Stripe Checkout frontend\", subagent_type=\"payments:stripe-integration-agent\", prompt=\"You are the stripe-integration-agent. Build Next.js checkout page for $ARGUMENTS.\n\nSECURITY CRITICAL: Use placeholder publishable key, read from environment, never hardcode.\n\nRequirements:\n- Checkout page app/checkout/page.tsx\n- Integrate @stripe/stripe-js and @stripe/react-stripe-js\n- Payment form with CardElement or Payment Element\n- Client-side validation and error handling\n- Loading states\n- Success page app/checkout/success/page.tsx\n- Cancel page app/checkout/cancel/page.tsx\n- TypeScript types for Stripe\n- Follow shadcn/ui design system\n- Form accessibility\n\nEnvironment: Add NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY to .env.example, document setup.\n\nDeliverables: page.tsx files, lib/stripe.ts, types/stripe.ts, updated .env.example and package.json\")\n\nWait for agent to complete.\nUpdate todos.\n\nPhase 4: Integration & Testing\nGoal: Wire up backend and frontend, verify error handling\n\nActions:\n- Verify backend endpoints accessible from frontend\n- Check CORS configuration\n- Test checkout flow: !{bash curl -X POST localhost:8000/api/checkout/create-session || echo \"Test\"}\n- Verify Supabase connection and payment table\n- Review error handling frontend and backend\n- Confirm webhook signature verification\n- Update todos\n\nPhase 5: Documentation & Security Review\nGoal: Document setup and verify security compliance\n\nActions:\n- Create setup guide: Stripe key acquisition, webhook config, testing\n- Security checklist: No hardcoded keys, .gitignore protection, placeholder .env.example, environment reads only, webhook verification, HTTPS, server validation, safe errors\n- Generate README.md setup section\n- Update todos\n\nPhase 6: Summary\nGoal: Report completion and next steps\n\nActions:\n- Mark all todos complete\n- Display summary: Files created, Checkout flow, environment variables, setup steps, testing\n- Next steps: Stripe account setup, webhook dashboard config, test cards, production deployment\n- Confirm security compliance"
              },
              {
                "name": "/add-subscriptions",
                "description": "Add subscription billing with plans, upgrades, downgrades, and lifecycle management",
                "path": "plugins/payments/commands/add-subscriptions.md",
                "frontmatter": {
                  "description": "Add subscription billing with plans, upgrades, downgrades, and lifecycle management",
                  "argument-hint": "[subscription-type]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Key requirements:**\n- Never hardcode Stripe API keys or secrets\n- Use placeholders: your_stripe_key_here, your_stripe_webhook_secret_here\n- Protect .env files with .gitignore\n- Create .env.example with placeholders only\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add complete subscription billing system with Stripe integration, plan management, and customer portal\n\nPhase 1: Discovery and Planning\n\nActions:\n- Parse $ARGUMENTS to determine subscription type (SaaS tiers, usage-based, hybrid)\n- Detect backend framework: @package.json or @requirements.txt\n- Detect frontend framework: Check for Next.js, React, Vue in package.json\n- Identify database: Supabase, PostgreSQL, MySQL\n- Check for existing Stripe integration\n- Create TodoWrite list for tracking subscription implementation phases\n\nPhase 2: Stripe Product Setup\n\nActions:\nTask(description=\"Configure Stripe products and pricing\", subagent_type=\"payments:subscription-manager-agent\", prompt=\"You are the subscription-manager-agent. Set up Stripe subscription products and pricing for $ARGUMENTS subscription type.\n\nSECURITY CRITICAL: Use placeholders for ALL Stripe keys. Never include actual API keys. Create .env.example with placeholder keys only.\n\nCreate config/stripe-products.json with 3 tiers (Starter, Professional, Enterprise) including name, description, monthly price, trial days, and feature list.\n\nCreate scripts/setup-stripe-products script (js or py based on detected framework) to read config and create products in Stripe using API.\n\nAdd to .env.example: STRIPE_SECRET_KEY, STRIPE_PUBLISHABLE_KEY, STRIPE_WEBHOOK_SECRET with placeholder values.\n\nUpdate .gitignore to protect .env files.\")\n\nPhase 3: Database Schema\n\nActions:\nTask(description=\"Create subscription database schema\", subagent_type=\"payments:payments-architect\", prompt=\"You are the payments-architect agent. Create database schema for subscription management.\n\nTables needed:\n1. subscriptions: id, user_id, stripe_customer_id, stripe_subscription_id, stripe_price_id, status (active/canceled/past_due/trialing/incomplete), current_period_start, current_period_end, cancel_at_period_end, trial_end, created_at, updated_at\n\n2. subscription_items: id, subscription_id, stripe_subscription_item_id, stripe_price_id, quantity, created_at\n\n3. invoices: id, user_id, subscription_id, stripe_invoice_id, amount_paid, currency, status (paid/open/void/uncollectible), invoice_pdf, created_at\n\nCreate migration file based on detected database (Supabase SQL, Alembic for Python, Prisma for Node.js).\n\nAdd indexes on user_id, stripe_customer_id, stripe_subscription_id, and status.\")\n\nPhase 4: Backend API\n\nActions:\nTask(description=\"Build subscription API endpoints\", subagent_type=\"payments:subscription-manager-agent\", prompt=\"You are the subscription-manager-agent. Build backend API endpoints for subscription management.\n\nSECURITY: Read Stripe keys from environment variables only. Validate webhook signatures. Never expose secret keys to frontend.\n\nCreate endpoints:\n- POST /api/subscriptions/create: Create subscription with trial\n- POST /api/subscriptions/upgrade: Upgrade with proration\n- POST /api/subscriptions/downgrade: Downgrade at period end\n- POST /api/subscriptions/cancel: Cancel subscription\n- POST /api/subscriptions/reactivate: Reactivate canceled subscription\n- GET /api/subscriptions/current: Get user's subscription\n- POST /api/webhooks/stripe: Handle webhooks (subscription.created/updated/deleted, invoice.paid/payment_failed, trial_will_end)\n\nImplement based on detected framework (FastAPI with Pydantic, Express with middleware, or Next.js API routes).\n\nInclude error handling and logging.\")\n\nPhase 5: Frontend Components\n\nActions:\nTask(description=\"Build subscription UI components\", subagent_type=\"payments:subscription-manager-agent\", prompt=\"You are the subscription-manager-agent. Build frontend components for subscription management.\n\nSECURITY: Use Stripe publishable key only (safe for frontend). Load from environment variable.\n\nCreate components:\n- PricingTable: Display plans with features, trial info, CTA buttons\n- SubscriptionManager: Show current plan, status, billing cycle, cancel/reactivate buttons, portal link\n- UpgradeFlow: Show proration, confirm upgrade\n- DowngradeFlow: Explain period end change, confirm\n- BillingHistory: List invoices with PDF downloads\n- SubscriptionStatus: Status badge (active/trialing/past_due/canceled)\n\nUse Stripe Elements for payment. Handle loading and error states. Responsive with Tailwind CSS.\n\nFramework-specific: Next.js App Router with server components, React hooks, or Vue Composition API.\")\n\nPhase 6: Customer Portal\n\nActions:\nTask(description=\"Integrate Stripe Customer Portal\", subagent_type=\"payments:subscription-manager-agent\", prompt=\"You are the subscription-manager-agent. Integrate Stripe Customer Portal for self-service management.\n\nSECURITY: Create portal sessions server-side only. Validate user authentication. Use secret key server-side only.\n\nCreate POST /api/subscriptions/portal endpoint that creates Stripe billing portal session and returns URL.\n\nAdd Manage Billing button in account settings that calls portal endpoint and redirects user.\n\nCreate docs/stripe-portal-setup.md with instructions for configuring portal in Stripe Dashboard (enable payment methods, invoices, subscription cancellation, set branding).\")\n\nPhase 7: Testing\n\nActions:\n- Run database migrations: !{bash npm run migrate} or !{bash alembic upgrade head}\n- Test Stripe product creation script\n- Verify webhook endpoint: !{bash stripe listen --forward-to localhost:3000/api/webhooks/stripe}\n- Check environment variables loaded correctly\n- Verify components render\n- Test subscription flows: create, upgrade, downgrade, cancel, reactivate\n- Verify webhook handling updates database\n- Test customer portal access\n\nPhase 8: Documentation\n\nActions:\n- Create docs/subscriptions-setup.md with Stripe configuration steps, webhook setup, database migration commands, product creation script usage, test card numbers, production checklist, and subscription lifecycle explanation\n- Display summary: Files created, database tables, API endpoints, frontend components, setup instructions, next steps for testing\n\nUpdate TodoWrite: Mark all phases complete"
              },
              {
                "name": "/add-webhooks",
                "description": "Setup secure webhook handlers for payment events and subscription updates",
                "path": "plugins/payments/commands/add-webhooks.md",
                "frontmatter": {
                  "description": "Setup secure webhook handlers for payment events and subscription updates",
                  "argument-hint": "[webhook-events]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup secure Stripe webhook infrastructure with signature verification, event handlers, and debugging capabilities\n\n**SECURITY CRITICAL:**\n- NEVER hardcode webhook secrets\n- Use placeholder: STRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n- Always implement signature verification\n- Read secrets from environment variables ONLY\n\nCore Principles:\n- Security first: Always verify webhook signatures\n- Never trust unverified webhook data\n- Store webhook events for debugging and replay\n- Implement idempotency for webhook processing\n\nPhase 1: Project Discovery\n\nActions:\n- Parse $ARGUMENTS to determine which events to handle\n- Verify FastAPI backend: !{bash test -d backend && echo \"Found\" || echo \"Not found\"}\n- Check Supabase integration: !{bash test -f backend/.env.example && grep -q \"SUPABASE\" backend/.env.example && echo \"Configured\" || echo \"Missing\"}\n- Load existing payment routes: @backend/app/api/routes/payments.py\n\nPhase 2: Environment Configuration\n\nActions:\n- Update backend/.env.example with webhook placeholders\n- Add STRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n- Add STRIPE_WEBHOOK_TOLERANCE=300\n- Verify .gitignore protects: .env, .env.local, .env.development, .env.production\n- Add webhook secret to .gitignore if missing\n\nPhase 3: Create Webhook Infrastructure\n\nTask(description=\"Create webhook infrastructure\", subagent_type=\"payments:webhook-handler-agent\", prompt=\"You are the webhook-handler-agent. Create secure Stripe webhook infrastructure.\n\n**SECURITY:** Use placeholder STRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n\nCreate backend/app/api/routes/webhooks.py with:\n- POST /api/webhooks/stripe endpoint\n- Stripe signature verification using stripe.Webhook.construct_event()\n- Raw request body handling for signature verification\n- Event type routing for: payment_intent.succeeded, payment_intent.payment_failed, customer.subscription.created, customer.subscription.updated, customer.subscription.deleted, invoice.payment_succeeded, invoice.payment_failed\n- Return 400 for invalid signatures\n- Store ALL events in webhook_events table\n- Implement idempotency: check event_id, skip duplicates\n- Comprehensive error handling\n\nDeliverable: backend/app/api/routes/webhooks.py with signature verification and event routing\")\n\nPhase 4: Database Schema\n\nTask(description=\"Create webhook events schema\", subagent_type=\"payments:payments-architect\", prompt=\"You are the payments-architect agent. Create Supabase schema for webhook event storage.\n\nCreate supabase/migrations/YYYYMMDDHHMMSS_create_webhook_events.sql with:\n- webhook_events table: id uuid, event_id text unique, event_type text, payload jsonb, status text, error_message text, processed_at timestamp, created_at timestamp, updated_at timestamp\n- Indexes: event_id unique, event_type, status, created_at\n- RLS policies: service role full access, anon/authenticated no access\n- Helper functions: get_unprocessed_events(), mark_event_processed(event_id), get_events_by_type(event_type)\n\nDeliverable: Migration file with complete schema and indexes\")\n\nPhase 5: Event Handlers\n\nTask(description=\"Implement event handlers\", subagent_type=\"payments:webhook-handler-agent\", prompt=\"You are the webhook-handler-agent. Create event handler functions for Stripe webhooks.\n\nCreate backend/app/services/webhook_handlers.py with handlers for:\n- handle_payment_intent_succeeded: Update payment status, send confirmation, update subscription\n- handle_payment_intent_failed: Update payment status, send notification, trigger retry\n- handle_subscription_created: Create subscription record, update user status, send welcome email\n- handle_subscription_updated: Update subscription, handle plan changes, notify user\n- handle_subscription_deleted: Mark cancelled, set cancellation date, send confirmation\n- handle_invoice_payment_succeeded: Record payment, update billing date, send receipt\n- handle_invoice_payment_failed: Record failure, update retry schedule, send notification\n\nEach handler must:\n- Accept full Stripe event object\n- Extract data safely\n- Update database atomically\n- Handle errors gracefully\n- Return success/failure status\n- Log all actions\n\nDeliverable: webhook_handlers.py with all event handlers and error handling\")\n\nPhase 6: Testing Setup\n\nTask(description=\"Setup webhook testing\", subagent_type=\"payments:stripe-integration-agent\", prompt=\"You are the stripe-integration-agent. Create local webhook testing infrastructure.\n\nCreate:\n1. docs/webhooks/LOCAL_TESTING.md with Stripe CLI installation, webhook forwarding, testing workflow\n2. backend/scripts/test-webhooks.sh to trigger all webhook events and verify processing\n3. Testing checklist: signature verification, invalid signatures rejected, all event types handled, database records created, idempotency works\n\nInclude Stripe CLI commands for local testing and event triggering.\n\nDeliverable: LOCAL_TESTING.md, test-webhooks.sh, testing checklist\")\n\nPhase 7: Production Documentation\n\nActions:\n- Create docs/webhooks/PRODUCTION_SETUP.md documenting:\n  - Webhook endpoint registration in Stripe Dashboard\n  - Production webhook URL format\n  - Required webhook events to enable\n  - Webhook secret retrieval from Stripe\n  - Environment variable configuration\n  - Webhook endpoint testing in production\n  - Monitoring and alerting setup\n- Include security reminders: HTTPS only, never log secrets, verify all signatures, monitor patterns, alert on failures\n\nPhase 8: Error Handling\n\nActions:\n- Add retry logic for transient failures with exponential backoff\n- Implement dead letter queue for failed webhooks\n- Create admin endpoint to replay failed webhooks\n- Set up alerts for repeated failures\n- Document error scenarios and responses\n\nPhase 9: Validation\n\nActions:\n- Run webhook tests: !{bash cd backend && python -m pytest tests/test_webhooks.py -v}\n- Verify signature validation works\n- Test all event handlers\n- Check database records created correctly\n- Verify idempotency handles duplicates\n- Test error scenarios: invalid signatures, malformed payloads\n\nPhase 10: Summary\n\nActions:\n- Display created files: webhooks.py, webhook_handlers.py, migration file, .env.example, LOCAL_TESTING.md, PRODUCTION_SETUP.md, test-webhooks.sh\n- Show configuration checklist: endpoint created, signature verification implemented, event handlers created, database schema, env vars configured, testing documented, production setup documented, error handling implemented\n- Provide next steps:\n  1. Copy backend/.env.example to backend/.env\n  2. Get webhook secret: stripe listen --print-secret\n  3. Add to backend/.env: STRIPE_WEBHOOK_SECRET=whsec_...\n  4. Start backend: uvicorn app.main:app --reload\n  5. Forward webhooks: stripe listen --forward-to localhost:8000/api/webhooks/stripe\n  6. Test events: stripe trigger payment_intent.succeeded\n  7. Verify in Supabase webhook_events table\n- Production deployment: register endpoint in Stripe Dashboard, add production secret, deploy backend, test webhook, monitor events"
              },
              {
                "name": "/init",
                "description": "Initialize payment infrastructure with Stripe SDK, environment variables, database schemas",
                "path": "plugins/payments/commands/init.md",
                "frontmatter": {
                  "description": "Initialize payment infrastructure with Stripe SDK, environment variables, database schemas",
                  "argument-hint": "[project-path]"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up complete payment infrastructure with Stripe SDK, secure environment configuration, database schemas, and local webhook testing\n\nCore Principles:\n- NEVER hardcode API keys or secrets - use placeholders only\n- Detect project structure before making changes\n- Create secure environment templates with clear documentation\n- Validate setup without requiring actual API keys\n\nPhase 1: Discovery\nGoal: Understand project structure and existing configuration\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory)\n- Detect backend framework (FastAPI, Express, etc.)\n- Detect frontend framework (Next.js, React, Vue, etc.)\n- Check for existing payment infrastructure\n- Locate or identify where environment files should live\n- Example: !{bash ls package.json pyproject.toml requirements.txt 2>/dev/null}\n\nPhase 2: Environment Configuration\nGoal: Create secure environment templates with placeholder values\n\n**SECURITY CRITICAL**: All API keys MUST use placeholders\n\nActions:\n- Create .env.example with Stripe placeholders (sk_test_your_key_here, pk_test_your_key_here, whsec_your_secret_here)\n- Update .gitignore to protect .env files (exclude .env.example)\n- Document how to obtain Stripe API keys from dashboard\n- NEVER include actual API key values\n\nPhase 3: Backend SDK Setup\nGoal: Install and configure Stripe SDK for backend\n\nActions:\n- Detect backend language (Python/Node.js)\n- Python: Install stripe package, create payment service reading from os.getenv()\n- Node.js: Install stripe package, create payment service reading from process.env\n- Create payment endpoint stubs (create-payment-intent, webhooks)\n- All code MUST read from environment variables, never hardcode keys\n\nPhase 4: Frontend SDK Setup\nGoal: Install and configure Stripe Elements for frontend\n\nActions:\n- Detect frontend framework\n- Install Stripe packages: @stripe/stripe-js and @stripe/react-stripe-js\n- Create Stripe provider component reading from NEXT_PUBLIC_ or equivalent env vars\n- Create payment form component template\n- Create example checkout page/component\n- Ensure all publishable keys read from environment\n\nPhase 5: Database Schema Setup\nGoal: Generate Supabase schemas for payment tracking\n\nActions:\n- Create migration for customers table:\n  - id (uuid, primary key)\n  - stripe_customer_id (text, unique)\n  - email (text)\n  - created_at (timestamp)\n- Create migration for subscriptions table:\n  - id (uuid, primary key)\n  - customer_id (uuid, foreign key)\n  - stripe_subscription_id (text, unique)\n  - status (text)\n  - price_id (text)\n  - current_period_end (timestamp)\n- Create migration for payments table:\n  - id (uuid, primary key)\n  - customer_id (uuid, foreign key)\n  - stripe_payment_intent_id (text, unique)\n  - amount (integer)\n  - currency (text)\n  - status (text)\n  - created_at (timestamp)\n- Add RLS policies for each table\n- Document schema in README\n\nPhase 6: Stripe CLI Setup\nGoal: Configure local webhook testing environment\n\nActions:\n- Check if Stripe CLI is installed: !{bash which stripe}\n- Provide installation instructions for macOS/Linux/Windows\n- Create webhook testing script (listen mode, forward to localhost, event logging)\n- Document webhook testing workflow\n- NEVER include actual webhook secrets\n\nPhase 7: Validation\nGoal: Verify setup completeness without requiring actual keys\n\nActions:\n- Check required files exist (env.example, gitignore, payment service, components, migrations)\n- Validate placeholder format in .env.example\n- Verify no hardcoded API keys in code\n- Check all API key references use environment variables\n- Run type checking if applicable: !{bash npm run typecheck || true}\n- Verify dependencies installed\n\nPhase 8: Documentation\nGoal: Create comprehensive setup guide\n\nActions:\n- Generate PAYMENT_SETUP.md with:\n  - How to get Stripe API keys (test and live)\n  - Environment variable configuration steps\n  - Database migration instructions\n  - Stripe CLI setup and usage\n  - Testing payment flow locally\n  - Webhook event handling guide\n  - Security best practices\n  - Common troubleshooting\n- Include links to Stripe documentation\n- Add example payment flow diagrams\n- Document test card numbers\n\nPhase 9: Summary\nGoal: Report setup completion and next steps\n\nActions:\n- Display comprehensive summary:\n  - All files created\n  - Environment variables that need configuration\n  - Database migrations ready to apply\n  - Stripe CLI commands for testing\n  - Link to PAYMENT_SETUP.md\n- Next steps:\n  1. Copy .env.example to .env\n  2. Get Stripe API keys from dashboard\n  3. Fill in actual keys in .env (never commit!)\n  4. Apply database migrations\n  5. Install Stripe CLI for webhook testing\n  6. Test payment flow with test cards\n- Security reminder: NEVER commit .env with actual keys"
              }
            ],
            "skills": [
              {
                "name": "checkout-components",
                "description": "Next.js checkout UI components with Stripe Elements and payment forms. Use when creating checkout pages, payment forms, subscription UIs, customer portals, or when user mentions Stripe Elements, payment UI, checkout flow, subscription management, or payment forms.",
                "path": "plugins/payments/skills/checkout-components/SKILL.md",
                "frontmatter": {
                  "name": "checkout-components",
                  "description": "Next.js checkout UI components with Stripe Elements and payment forms. Use when creating checkout pages, payment forms, subscription UIs, customer portals, or when user mentions Stripe Elements, payment UI, checkout flow, subscription management, or payment forms.",
                  "allowed-tools": "Bash, Read, Write, Edit, Grep, Glob"
                },
                "content": "# Checkout Components\n\n## Purpose\n\nProduction-ready Next.js UI components for Stripe payment integration with TypeScript, Tailwind CSS, and accessibility features. Provides complete checkout flows, subscription management, and payment form components.\n\n## Security Requirements\n\n**CRITICAL: API Key Handling**\n\nWhen using these components:\n\n- Never hardcode Stripe publishable keys in components\n- ALWAYS use environment variables: `NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY`\n- Use placeholders in all examples: `pk_test_your_stripe_publishable_key_here`\n- Never commit actual API keys to git\n- Always add `.env` files to `.gitignore`\n\n**Example .env.local:**\n```bash\n# NEVER COMMIT THIS FILE\nNEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=pk_test_your_stripe_publishable_key_here\nSTRIPE_SECRET_KEY=sk_test_your_stripe_secret_key_here\nSTRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n```\n\n## Activation Triggers\n\n- Creating checkout pages\n- Implementing payment forms\n- Building subscription management UI\n- Adding customer portal components\n- Integrating Stripe Elements\n- Creating payment history displays\n- Setting up pricing tables\n\n## Component Overview\n\n### Provider Components\n- **StripeProvider.tsx** - Wraps app with Stripe Elements context\n\n### Payment Components\n- **CheckoutForm.tsx** - Complete checkout form with card input\n- **PaymentMethodForm.tsx** - Standalone payment method input\n- **SubscriptionCard.tsx** - Subscription display and management\n- **PricingTable.tsx** - Pricing tier comparison table\n- **PaymentHistory.tsx** - Transaction history component\n\n## Quick Start\n\n### 1. Install Dependencies\n\n```bash\n# Install Stripe React libraries\nbash scripts/install-stripe-react.sh\n```\n\n**Installs:**\n- `@stripe/stripe-js` - Stripe.js loader\n- `@stripe/react-stripe-js` - React Elements components\n- Type definitions for TypeScript\n\n### 2. Setup Stripe Provider\n\n```bash\n# Create provider wrapper in your app\nbash scripts/setup-stripe-provider.sh\n```\n\n**Creates:**\n- `lib/stripe-client.ts` - Stripe client initialization\n- `components/providers/stripe-provider.tsx` - Elements provider wrapper\n- Environment variable configuration\n\n### 3. Generate Components\n\n```bash\n# Generate specific payment component\nbash scripts/generate-component.sh checkout-form\nbash scripts/generate-component.sh subscription-card\nbash scripts/generate-component.sh pricing-table\n```\n\n### 4. Validate Setup\n\n```bash\n# Validate component structure and configuration\nbash scripts/validate-components.sh\n```\n\n**Checks:**\n- Environment variables configured\n- Dependencies installed\n- Component structure valid\n- TypeScript types correct\n- Accessibility compliance\n\n## Component Templates\n\n### StripeProvider Template\n\nWraps your app with Stripe Elements context:\n\n```typescript\nimport { StripeProvider } from '@/components/providers/stripe-provider';\n\nexport default function RootLayout({ children }) {\n  return (\n    <html>\n      <body>\n        <StripeProvider>\n          {children}\n        </StripeProvider>\n      </body>\n    </html>\n  );\n}\n```\n\n**Features:**\n- Automatic Stripe.js loading\n- Error boundary handling\n- Loading states\n- Theme customization\n\n### CheckoutForm Template\n\nComplete payment form with card input:\n\n```typescript\nimport { CheckoutForm } from '@/components/payments/checkout-form';\n\nexport default function CheckoutPage() {\n  return (\n    <div className=\"max-w-md mx-auto p-6\">\n      <h1 className=\"text-2xl font-semibold mb-6\">Complete Purchase</h1>\n      <CheckoutForm\n        amount={4999}\n        onSuccess={() => router.push('/success')}\n        onError={(error) => console.error(error)}\n      />\n    </div>\n  );\n}\n```\n\n**Features:**\n- Card Element with validation\n- Real-time error display\n- Loading states\n- Success/error callbacks\n- Responsive design\n- ARIA labels\n\n### PaymentMethodForm Template\n\nStandalone payment method collection:\n\n```typescript\nimport { PaymentMethodForm } from '@/components/payments/payment-method-form';\n\nexport default function AddPaymentMethod() {\n  return (\n    <PaymentMethodForm\n      onComplete={(paymentMethodId) => {\n        console.log('Payment method created:', paymentMethodId);\n      }}\n      customerId=\"cus_xxx\"\n    />\n  );\n}\n```\n\n**Features:**\n- Save cards for future use\n- Update existing payment methods\n- Validation and error handling\n- Loading indicators\n\n### SubscriptionCard Template\n\nDisplay and manage subscriptions:\n\n```typescript\nimport { SubscriptionCard } from '@/components/payments/subscription-card';\n\nexport default function SubscriptionPage() {\n  return (\n    <SubscriptionCard\n      subscription={{\n        id: 'sub_xxx',\n        status: 'active',\n        currentPeriodEnd: new Date('2025-12-01'),\n        plan: { name: 'Pro Plan', amount: 2999 }\n      }}\n      onCancel={() => handleCancellation()}\n      onUpdate={() => handleUpdate()}\n    />\n  );\n}\n```\n\n**Features:**\n- Status badge display\n- Renewal date\n- Cancel/upgrade actions\n- Usage tracking\n- Billing history link\n\n### PricingTable Template\n\nCompare pricing tiers:\n\n```typescript\nimport { PricingTable } from '@/components/payments/pricing-table';\n\nconst plans = [\n  {\n    id: 'price_free',\n    name: 'Free',\n    price: 0,\n    features: ['10 API calls/month', 'Basic support']\n  },\n  {\n    id: 'price_pro',\n    name: 'Pro',\n    price: 2999,\n    features: ['1000 API calls/month', 'Priority support', 'Advanced features']\n  }\n];\n\nexport default function Pricing() {\n  return <PricingTable plans={plans} onSelectPlan={(planId) => handleCheckout(planId)} />;\n}\n```\n\n**Features:**\n- Responsive grid layout\n- Feature comparison\n- Call-to-action buttons\n- Highlight recommended plan\n- Annual/monthly toggle\n\n### PaymentHistory Template\n\nDisplay transaction history:\n\n```typescript\nimport { PaymentHistory } from '@/components/payments/payment-history';\n\nexport default function BillingPage() {\n  return (\n    <PaymentHistory\n      customerId=\"cus_xxx\"\n      limit={10}\n      onLoadMore={() => loadMorePayments()}\n    />\n  );\n}\n```\n\n**Features:**\n- Paginated transaction list\n- Invoice download links\n- Status indicators\n- Date formatting\n- Search and filter\n\n## Styling Customization\n\nAll components use Tailwind CSS and support customization:\n\n### Element Styling\n\nCustomize Stripe Elements appearance:\n\n```typescript\nconst appearance = {\n  theme: 'stripe',\n  variables: {\n    colorPrimary: '#0070f3',\n    colorBackground: '#ffffff',\n    colorText: '#000000',\n    colorDanger: '#df1b41',\n    fontFamily: 'system-ui, sans-serif',\n    spacingUnit: '4px',\n    borderRadius: '4px'\n  }\n};\n\n<Elements stripe={stripePromise} options={{ appearance }}>\n  <CheckoutForm />\n</Elements>\n```\n\n### Component Classes\n\nOverride default Tailwind classes:\n\n```typescript\n<CheckoutForm\n  className=\"custom-checkout\"\n  buttonClassName=\"bg-blue-600 hover:bg-blue-700\"\n  errorClassName=\"text-red-600 text-sm\"\n/>\n```\n\n## TypeScript Types\n\nAll components include full TypeScript support:\n\n```typescript\nimport type {\n  CheckoutFormProps,\n  PaymentMethodFormProps,\n  SubscriptionCardProps,\n  PricingTableProps,\n  PaymentHistoryProps\n} from '@/types/payments';\n```\n\n**Type Definitions:**\n- Props interfaces\n- Stripe object types\n- Event handlers\n- Error types\n- Response types\n\n## Error Handling\n\nComponents provide comprehensive error handling:\n\n```typescript\n<CheckoutForm\n  onError={(error) => {\n    switch (error.type) {\n      case 'card_error':\n        // Show user-friendly message\n        toast.error(error.message);\n        break;\n      case 'validation_error':\n        // Handle validation errors\n        break;\n      default:\n        // Generic error handling\n        console.error(error);\n    }\n  }}\n/>\n```\n\n**Error Types:**\n- Card validation errors\n- Payment processing errors\n- Network errors\n- Configuration errors\n\n## Accessibility Features\n\nAll components follow WCAG 2.1 AA standards:\n\n- Proper ARIA labels\n- Keyboard navigation support\n- Focus management\n- Screen reader friendly\n- Error announcements\n- Color contrast compliance\n\n**Example:**\n```typescript\n<button\n  type=\"submit\"\n  aria-label=\"Complete payment\"\n  aria-disabled={isProcessing}\n  className=\"focus:ring-2 focus:ring-blue-500\"\n>\n  {isProcessing ? 'Processing...' : 'Pay Now'}\n</button>\n```\n\n## Testing\n\n### Component Testing\n\n```bash\n# Validate all components\nbash scripts/validate-components.sh\n\n# Test specific component\nbash scripts/validate-components.sh CheckoutForm\n```\n\n### Integration Testing\n\nUse Stripe test cards:\n\n```\n4242 4242 4242 4242 - Success\n4000 0000 0000 0002 - Decline\n4000 0000 0000 9995 - Insufficient funds\n```\n\n## Examples\n\n### Example 1: Complete Checkout Page\n\nFull checkout implementation with cart summary:\n\n```typescript\n// See: examples/checkout-page-example.tsx\n// Complete page with:\n// - Order summary\n// - Checkout form\n// - Success/error handling\n// - Loading states\n// - Receipt generation\n```\n\n### Example 2: Subscription Management\n\nSubscription portal with upgrade/downgrade:\n\n```typescript\n// See: examples/subscription-management-example.tsx\n// Complete portal with:\n// - Current plan display\n// - Pricing comparison\n// - Upgrade/downgrade flow\n// - Cancellation handling\n// - Invoice history\n```\n\n### Example 3: Payment Form Integration\n\nIntegrate payment form in multi-step flow:\n\n```typescript\n// See: examples/payment-form-integration-example.tsx\n// Multi-step checkout with:\n// - Shipping information\n// - Payment details\n// - Order confirmation\n// - Progress indicator\n```\n\n## Environment Setup\n\nRequired environment variables:\n\n```bash\n# .env.local (NEVER commit this file)\nNEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=pk_test_your_key_here\nSTRIPE_SECRET_KEY=sk_test_your_key_here\nSTRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n\n# Optional\nNEXT_PUBLIC_STRIPE_CURRENCY=usd\nNEXT_PUBLIC_APP_URL=http://localhost:3000\n```\n\nAdd to `.gitignore`:\n```\n.env.local\n.env.*.local\n.env\n```\n\n## Requirements\n\n**Dependencies:**\n- Next.js 13+ with App Router\n- React 18+\n- TypeScript 5+\n- Tailwind CSS 3+\n- @stripe/stripe-js\n- @stripe/react-stripe-js\n\n**Stripe Setup:**\n- Stripe account (test mode works)\n- Publishable key\n- Secret key\n- Webhook endpoint configured\n\n## Security Best Practices\n\n1. **Never expose secret keys client-side**\n   - Only use `NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY` in components\n   - Secret key stays server-side only\n\n2. **Validate on server**\n   - Always verify payments server-side\n   - Use webhooks for payment confirmations\n   - Never trust client-side data\n\n3. **Use HTTPS in production**\n   - Required for Stripe integration\n   - Protects payment data in transit\n\n4. **Implement CSP headers**\n   - Allow Stripe.js domain\n   - Restrict other script sources\n\n5. **Handle PCI compliance**\n   - Never store card numbers\n   - Use Stripe Elements (SAQ A compliant)\n   - Let Stripe handle card data\n\n## Troubleshooting\n\n### Stripe.js Not Loading\n\nCheck environment variable:\n```bash\necho $NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY\n```\n\nVerify format starts with `pk_test_` or `pk_live_`\n\n### Payment Intent Creation Fails\n\nEnsure server-side API route exists:\n```typescript\n// app/api/create-payment-intent/route.ts\nimport { stripe } from '@/lib/stripe-server';\n\nexport async function POST(req: Request) {\n  const { amount } = await req.json();\n\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n  });\n\n  return Response.json({ clientSecret: paymentIntent.client_secret });\n}\n```\n\n### Component Not Rendering\n\nVerify StripeProvider wraps component:\n```typescript\n// Must be wrapped\n<StripeProvider>\n  <CheckoutForm />\n</StripeProvider>\n```\n\n## Resources\n\n**Scripts:**\n- `scripts/install-stripe-react.sh` - Install dependencies\n- `scripts/setup-stripe-provider.sh` - Configure provider\n- `scripts/generate-component.sh` - Generate new components\n- `scripts/validate-components.sh` - Validate setup\n\n**Templates:**\n- `templates/StripeProvider.tsx` - Provider wrapper\n- `templates/CheckoutForm.tsx` - Checkout form\n- `templates/PaymentMethodForm.tsx` - Payment method input\n- `templates/SubscriptionCard.tsx` - Subscription display\n- `templates/PricingTable.tsx` - Pricing comparison\n- `templates/PaymentHistory.tsx` - Transaction history\n\n**Examples:**\n- `examples/checkout-page-example.tsx` - Complete checkout\n- `examples/subscription-management-example.tsx` - Subscription UI\n- `examples/payment-form-integration-example.tsx` - Multi-step flow\n\n---\n\n**Plugin:** payments\n**Version:** 1.0.0\n**Category:** UI Components\n**Framework:** Next.js + Stripe Elements"
              },
              {
                "name": "stripe-patterns",
                "description": "Stripe integration templates with reusable code for Checkout, Payment Intents, and Subscriptions. Use when implementing Stripe payments, building checkout flows, handling subscriptions, or integrating payment processing.",
                "path": "plugins/payments/skills/stripe-patterns/SKILL.md",
                "frontmatter": {
                  "name": "stripe-patterns",
                  "description": "Stripe integration templates with reusable code for Checkout, Payment Intents, and Subscriptions. Use when implementing Stripe payments, building checkout flows, handling subscriptions, or integrating payment processing.",
                  "allowed-tools": "Read, Write, Edit, Bash, Grep, Glob"
                },
                "content": "# Stripe Integration Patterns\n\nComprehensive Stripe integration templates for FastAPI backends and Next.js frontends, including Checkout Sessions, Payment Intents, and Subscription billing.\n\n## Use When\n\n- Implementing Stripe payment processing in applications\n- Building checkout flows for one-time or recurring payments\n- Integrating subscription billing systems\n- Setting up webhook handlers for payment events\n- Creating secure payment forms with Stripe Elements\n- Validating Stripe API configuration\n\n## Security Requirements\n\n**CRITICAL: All templates follow strict security rules:**\n\n- All API keys use placeholders: `your_stripe_key_here`\n- Code reads from environment variables only\n- `.env.example` templates provided with placeholders\n- `.gitignore` protects secret files\n- Documentation explains how to obtain Stripe keys\n\n**NEVER hardcode actual Stripe API keys in any files!**\n\n## Integration Patterns\n\n### 1. Checkout Sessions (Redirect Flow)\n\n**Best for:** Quick integration, hosted payment pages, minimal frontend code\n\n**Flow:**\n1. Create Checkout Session on backend with line items and success URL\n2. Redirect customer to Stripe-hosted checkout page\n3. Handle `checkout.session.completed` webhook for fulfillment\n\n**Use:** `scripts/setup-stripe-checkout.sh` and `templates/checkout_session.py`\n\n### 2. Payment Intents (Custom UI)\n\n**Best for:** Custom payment forms, direct card collection, advanced UX control\n\n**Flow:**\n1. Create PaymentIntent on backend when checkout begins\n2. Pass client secret to frontend\n3. Collect payment details with Stripe Elements\n4. Confirm payment on client side\n5. Monitor webhooks for payment success/failure\n\n**Use:** `scripts/setup-payment-intents.sh` and `templates/payment_intent.py`\n\n### 3. Subscriptions (Recurring Billing)\n\n**Best for:** Recurring revenue, subscription services, membership access\n\n**Flow:**\n1. Create or retrieve Customer object\n2. Attach payment method to customer\n3. Create Subscription with pricing and billing cycle\n4. Handle subscription lifecycle events via webhooks\n\n**Use:** `scripts/setup-subscriptions.sh` and `templates/subscription.py`\n\n## Available Scripts\n\n### setup-stripe-checkout.sh\nCreates complete Checkout Session implementation with FastAPI endpoint, webhook handler, and success/cancel pages.\n\n```bash\nbash scripts/setup-stripe-checkout.sh\n```\n\n### setup-payment-intents.sh\nSets up Payment Intent workflow with client secret handling, Stripe Elements integration, and payment confirmation.\n\n```bash\nbash scripts/setup-payment-intents.sh\n```\n\n### setup-subscriptions.sh\nImplements subscription billing with customer management, subscription creation, and lifecycle webhook handlers.\n\n```bash\nbash scripts/setup-subscriptions.sh\n```\n\n### validate-stripe-config.sh\nValidates Stripe configuration including API keys, webhook secrets, environment setup, and .gitignore protection.\n\n```bash\nbash scripts/validate-stripe-config.sh\n```\n\n## Available Templates\n\n### Backend Templates (Python/FastAPI)\n\n**checkout_session.py** - Complete Checkout Session endpoint\n- Creates session with line items\n- Handles success/cancel redirects\n- Returns session ID for redirect\n\n**payment_intent.py** - Payment Intent workflow\n- Creates PaymentIntent with amount/currency\n- Returns client secret for frontend\n- Handles payment confirmation\n\n**subscription.py** - Subscription management\n- Customer creation and retrieval\n- Payment method attachment\n- Subscription creation with pricing\n\n### Frontend Templates (TypeScript/Next.js)\n\n**stripe_elements.tsx** - Stripe Elements component\n- Card input with validation\n- Payment Intent confirmation\n- Error handling and status updates\n\n**checkout_page.tsx** - Complete checkout page\n- Product display with pricing\n- Checkout Session redirect flow\n- Success/cancel page handling\n\n## Available Examples\n\n### fastapi-checkout-example.py\nComplete working example with:\n- FastAPI application setup\n- Checkout Session creation endpoint\n- Webhook handler for fulfillment\n- Environment configuration\n- Error handling\n\n### nextjs-payment-form-example.tsx\nFull payment form implementation:\n- Stripe Elements integration\n- Payment Intent confirmation\n- Loading states and error display\n- Success/failure handling\n\n### subscription-flow-example.py\nEnd-to-end subscription workflow:\n- Customer creation\n- Payment method collection\n- Subscription creation\n- Webhook event processing\n- Subscription lifecycle management\n\n## Setup Instructions\n\n### 1. Obtain Stripe Keys\n\n**Test Mode** (for development):\n1. Visit https://dashboard.stripe.com/test/apikeys\n2. Copy \"Publishable key\" and \"Secret key\"\n3. Use test card: `4242 4242 4242 4242`\n\n**Live Mode** (for production):\n1. Complete account verification\n2. Visit https://dashboard.stripe.com/apikeys\n3. Copy production keys\n4. **NEVER commit live keys to git!**\n\n### 2. Configure Environment\n\nCreate `.env` file (use `.env.example` as template):\n\n```bash\n# .env (NEVER commit this file)\nSTRIPE_SECRET_KEY=your_stripe_secret_key_here\nSTRIPE_PUBLISHABLE_KEY=your_stripe_publishable_key_here\nSTRIPE_WEBHOOK_SECRET=your_webhook_secret_here\n```\n\nAdd to `.gitignore`:\n\n```\n.env\n.env.local\n.env.development\n.env.production\n!.env.example\n```\n\n### 3. Install Dependencies\n\n**Backend (Python):**\n```bash\npip install stripe fastapi uvicorn python-dotenv\n```\n\n**Frontend (Next.js):**\n```bash\nnpm install @stripe/stripe-js @stripe/react-stripe-js\n```\n\n### 4. Set Up Webhooks\n\n**Local Development:**\n1. Install Stripe CLI: https://stripe.com/docs/stripe-cli\n2. Run `stripe login`\n3. Forward events: `stripe listen --forward-to localhost:8000/webhook`\n4. Copy webhook signing secret to `.env`\n\n**Production:**\n1. Add endpoint in Stripe Dashboard\n2. Select events to listen for\n3. Copy webhook signing secret\n\n## Best Practices\n\n### Security\n- Always use environment variables for API keys\n- Validate webhook signatures to prevent tampering\n- Use HTTPS in production for all endpoints\n- Never log sensitive payment information\n- Implement proper error handling without exposing keys\n\n### Payment Flow\n- Create PaymentIntent as early as possible (when amount is known)\n- Store PaymentIntent ID for retrieval on page refresh\n- Use idempotency keys for safe retries\n- Handle all possible payment statuses\n- Implement proper loading states in UI\n\n### Subscriptions\n- Use `default_incomplete` payment behavior\n- Collect payment method before creating subscription\n- Handle trial periods correctly\n- Monitor subscription status changes via webhooks\n- Implement proper cancellation and upgrade flows\n\n### Testing\n- Use Stripe test cards: https://stripe.com/docs/testing\n- Test webhook events with Stripe CLI\n- Verify error handling for declined cards\n- Test 3D Secure authentication flows\n- Validate success and failure paths\n\n## Common Use Cases\n\n### One-Time Payment\nUse **Checkout Sessions** for simplicity or **Payment Intents** for custom UI.\n\n### Recurring Billing\nUse **Subscriptions** with automatic invoice generation.\n\n### Free Trial\nCreate subscription with `trial_period_days` parameter.\n\n### Usage-Based Billing\nUse Subscriptions with metered billing and usage reports.\n\n### Multiple Payment Methods\nStore payment methods on Customer object and set default.\n\n## Troubleshooting\n\n### Payment Fails Silently\n- Check webhook endpoint is accessible\n- Verify webhook signature validation\n- Review Stripe Dashboard event logs\n\n### Checkout Session Expires\n- Sessions expire after 24 hours\n- Create new session for retry\n\n### Subscription Status Stuck\n- Check for failed payments in Dashboard\n- Verify payment method is valid\n- Review subscription payment settings\n\n### CORS Errors\n- Configure CORS middleware in FastAPI\n- Allow Stripe.js origins in production\n\n## References\n\n- Stripe Checkout: https://docs.stripe.com/payments/checkout\n- Payment Intents: https://docs.stripe.com/payments/payment-intents\n- Subscriptions: https://docs.stripe.com/billing/subscriptions\n- Webhooks: https://docs.stripe.com/webhooks\n- Testing: https://docs.stripe.com/testing"
              },
              {
                "name": "subscription-schemas",
                "description": "Production-ready Supabase database schemas for customers, subscriptions, payments, invoices, and webhook events with comprehensive Row Level Security policies. Use when setting up payment infrastructure, creating subscription tables, implementing secure payment data storage, or configuring RLS policies for multi-tenant payment systems.",
                "path": "plugins/payments/skills/subscription-schemas/SKILL.md",
                "frontmatter": {
                  "name": "subscription-schemas",
                  "description": "Production-ready Supabase database schemas for customers, subscriptions, payments, invoices, and webhook events with comprehensive Row Level Security policies. Use when setting up payment infrastructure, creating subscription tables, implementing secure payment data storage, or configuring RLS policies for multi-tenant payment systems.",
                  "allowed-tools": "Read, Write, Bash, mcp__plugin_nextjs-frontend_design-system, mcp__plugin_supabase_supabase"
                },
                "content": "# Subscription Schemas\n\nProduction-ready Supabase database schemas for subscription and payment management with comprehensive security policies.\n\n## Security Requirements\n\nThis skill follows strict security rules:\n\n- **NO hardcoded database credentials** - All connection strings use placeholders\n- **Environment variable references** - Code reads from `SUPABASE_URL` and `SUPABASE_KEY`\n- **Row Level Security (RLS)** - All tables protected with comprehensive policies\n- **Data encryption** - Sensitive payment data properly protected\n- **Audit logging** - Webhook events tracked for compliance\n\nAll examples use placeholder values like `your_supabase_url_here`.\n\n## Database Schema Overview\n\nThe subscription schema consists of five core tables:\n\n1. **customers** - Customer profiles linked to auth.users\n2. **subscriptions** - Active and historical subscriptions\n3. **payments** - Payment transaction records\n4. **invoices** - Invoice history and status\n5. **webhook_events** - Payment provider webhook logs\n\n## Table Relationships\n\n```\nauth.users (Supabase Auth)\n    ‚Üì\ncustomers (1:1 with users)\n    ‚Üì\nsubscriptions (1:many)\n    ‚Üì\npayments (1:many per subscription)\ninvoices (1:many per subscription)\n\nwebhook_events (independent audit log)\n```\n\n## Use When\n\n- Setting up a new subscription-based application\n- Implementing payment tracking for SaaS products\n- Migrating payment infrastructure to Supabase\n- Adding Row Level Security to payment tables\n- Configuring multi-tenant payment isolation\n- Creating invoice and payment history tracking\n- Implementing webhook event logging for Stripe/Paddle/LemonSqueezy\n\n## Instructions\n\n### Phase 1: Create Database Tables\n\n1. **Review table schemas**:\n   - Read `templates/customers_table.sql` for customer profiles\n   - Read `templates/subscriptions_table.sql` for subscription management\n   - Read `templates/payments_table.sql` for payment records\n   - Read `templates/invoices_table.sql` for invoice tracking\n   - Read `templates/webhook_events_table.sql` for webhook logging\n\n2. **Execute table creation**:\n   ```bash\n   bash scripts/create-payment-tables.sh\n   ```\n\n   This script will:\n   - Create all five tables with proper indexes\n   - Set up foreign key relationships\n   - Add check constraints for data validation\n   - Create updated_at triggers\n\n### Phase 2: Implement Row Level Security\n\n1. **Review RLS policies**:\n   - Read `templates/rls_policies.sql` for complete policy definitions\n   - Understand customer data isolation\n   - Review subscription access controls\n   - Check payment data protection rules\n\n2. **Enable RLS and create policies**:\n   ```bash\n   bash scripts/setup-rls-policies.sh\n   ```\n\n   This script will:\n   - Enable RLS on all payment tables\n   - Create SELECT policies for authenticated users\n   - Create INSERT policies with validation\n   - Create UPDATE policies with ownership checks\n   - Create DELETE policies (restricted)\n\n### Phase 3: Run Complete Migration\n\n1. **Use the complete schema migration**:\n   ```bash\n   bash scripts/migrate-schema.sh\n   ```\n\n   This orchestrates:\n   - Table creation in correct order\n   - Index creation for performance\n   - RLS policy setup\n   - Validation of schema structure\n\n2. **Verify migration success**:\n   ```bash\n   bash scripts/validate-schema.sh\n   ```\n\n   Validates:\n   - All tables exist\n   - Indexes are created\n   - RLS is enabled\n   - Policies are active\n   - Foreign keys are valid\n\n### Phase 4: Test RLS Policies\n\n1. **Review RLS testing examples**:\n   - Read `examples/rls-testing-examples.sql`\n   - Test customer data isolation\n   - Verify subscription access controls\n   - Confirm payment data protection\n\n2. **Run sample queries**:\n   - Read `examples/sample-queries.sql`\n   - Test common subscription queries\n   - Verify payment history retrieval\n   - Check invoice generation queries\n\n## Security Compliance\n\n### Row Level Security Policies\n\nAll tables implement RLS with these principles:\n\n1. **Customer Isolation**: Users only access their own customer record\n2. **Subscription Ownership**: Users only see their own subscriptions\n3. **Payment Privacy**: Payment records restricted to owners\n4. **Invoice Access**: Invoices accessible only to associated customers\n5. **Webhook Audit**: Webhook events visible to admins only\n\n### Data Protection\n\n- **Sensitive fields**: Payment methods, billing details encrypted\n- **PII protection**: Customer data isolated per user\n- **Audit trail**: All webhook events logged\n- **No direct access**: All queries filtered through RLS\n\n### Environment Variables\n\nConnection configuration reads from:\n\n```bash\nSUPABASE_URL=your_supabase_url_here\nSUPABASE_ANON_KEY=your_supabase_anon_key_here\nSUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here  # For migrations only\n```\n\n## Available Scripts\n\n### create-payment-tables.sh\nCreates all five payment tables with proper structure, indexes, and constraints.\n\n**Usage:**\n```bash\nbash scripts/create-payment-tables.sh\n```\n\n**Environment Required:**\n- `SUPABASE_URL`\n- `SUPABASE_SERVICE_ROLE_KEY`\n\n### setup-rls-policies.sh\nEnables RLS and creates comprehensive security policies for all tables.\n\n**Usage:**\n```bash\nbash scripts/setup-rls-policies.sh\n```\n\n**Environment Required:**\n- `SUPABASE_URL`\n- `SUPABASE_SERVICE_ROLE_KEY`\n\n### migrate-schema.sh\nOrchestrates complete schema setup including tables, indexes, and RLS.\n\n**Usage:**\n```bash\nbash scripts/migrate-schema.sh\n```\n\n**Environment Required:**\n- `SUPABASE_URL`\n- `SUPABASE_SERVICE_ROLE_KEY`\n\n### validate-schema.sh\nValidates that all tables, indexes, and policies are correctly configured.\n\n**Usage:**\n```bash\nbash scripts/validate-schema.sh\n```\n\n**Environment Required:**\n- `SUPABASE_URL`\n- `SUPABASE_ANON_KEY`\n\n## Available Templates\n\n### customers_table.sql\nCustomer profile table with Supabase Auth integration.\n\n**Fields:**\n- `id` (uuid, primary key)\n- `user_id` (uuid, foreign key to auth.users)\n- `email` (text)\n- `name` (text)\n- `billing_address` (jsonb)\n- `created_at`, `updated_at` (timestamptz)\n\n### subscriptions_table.sql\nSubscription tracking with status and billing cycles.\n\n**Fields:**\n- `id` (uuid, primary key)\n- `customer_id` (uuid, foreign key)\n- `plan_id` (text)\n- `status` (enum: active, canceled, past_due, trialing)\n- `current_period_start`, `current_period_end` (timestamptz)\n- `cancel_at_period_end` (boolean)\n- `created_at`, `updated_at` (timestamptz)\n\n### payments_table.sql\nPayment transaction records.\n\n**Fields:**\n- `id` (uuid, primary key)\n- `subscription_id` (uuid, foreign key)\n- `amount` (numeric)\n- `currency` (text)\n- `status` (enum: succeeded, pending, failed)\n- `payment_method` (text)\n- `provider_payment_id` (text)\n- `created_at` (timestamptz)\n\n### invoices_table.sql\nInvoice generation and tracking.\n\n**Fields:**\n- `id` (uuid, primary key)\n- `subscription_id` (uuid, foreign key)\n- `invoice_number` (text, unique)\n- `amount_due` (numeric)\n- `amount_paid` (numeric)\n- `status` (enum: draft, open, paid, void)\n- `due_date` (date)\n- `created_at`, `updated_at` (timestamptz)\n\n### webhook_events_table.sql\nWebhook event logging for audit and replay.\n\n**Fields:**\n- `id` (uuid, primary key)\n- `provider` (text, e.g., 'stripe', 'paddle')\n- `event_type` (text)\n- `payload` (jsonb)\n- `processed` (boolean)\n- `created_at` (timestamptz)\n\n### rls_policies.sql\nComplete RLS policy definitions for all tables with customer isolation.\n\n## Available Examples\n\n### complete-schema-migration.sql\nComplete migration script showing full schema setup in one file.\n\n### sample-queries.sql\nCommon query patterns:\n- Get active subscriptions for user\n- Retrieve payment history\n- Generate invoice summaries\n- Check subscription status\n\n### rls-testing-examples.sql\nTest cases for RLS policies:\n- Verify customer isolation\n- Test subscription access\n- Validate payment privacy\n- Confirm admin-only webhook access\n\n## Requirements\n\n- Supabase project with database access\n- Service role key for migrations (secure storage required)\n- Anon key for client-side queries\n- PostgreSQL extensions: `uuid-ossp`, `pgcrypto`\n\n## Migration Strategy\n\n### Initial Setup\n1. Run `create-payment-tables.sh` to create schema\n2. Run `setup-rls-policies.sh` to enable security\n3. Run `validate-schema.sh` to confirm setup\n\n### Schema Updates\n1. Create new migration file in `templates/`\n2. Test in development environment first\n3. Apply using `migrate-schema.sh`\n4. Always validate after migrations\n\n### Rollback Support\nEach template includes a rollback section:\n```sql\n-- Rollback\nDROP TABLE IF EXISTS table_name CASCADE;\n```\n\n## Performance Considerations\n\n### Indexes Created\n- `customers.user_id` - Fast auth lookups\n- `subscriptions.customer_id` - Customer subscription queries\n- `subscriptions.status` - Status filtering\n- `payments.subscription_id` - Payment history\n- `invoices.subscription_id` - Invoice retrieval\n- `webhook_events.provider, event_type` - Event filtering\n\n### Query Optimization\n- Use explicit WHERE clauses with RLS\n- Include `auth.uid()` checks in queries\n- Cache frequently accessed data\n- Use `EXPLAIN ANALYZE` for slow queries\n\n## Integration with Payment Providers\n\n### Stripe\n- Use `webhook_events` to log Stripe webhooks\n- Map `provider_payment_id` to Stripe payment intent IDs\n- Store Stripe customer ID in `customers.metadata`\n\n### Paddle\n- Log Paddle webhooks with `provider='paddle'`\n- Map subscription IDs to Paddle subscription IDs\n- Store Paddle customer ID in customer metadata\n\n### LemonSqueezy\n- Track LemonSqueezy events in webhook_events\n- Map variant IDs to plan_id in subscriptions\n- Store LemonSqueezy customer ID in metadata\n\n## Compliance Notes\n\n- **PCI DSS**: No credit card numbers stored (use payment provider tokens)\n- **GDPR**: Customer data can be deleted via user_id cascade\n- **Audit Trail**: All webhook events logged for compliance\n- **Data Retention**: Configure automated archival policies as needed"
              },
              {
                "name": "webhook-security",
                "description": "Webhook validation patterns with signature verification, event logging, and testing tools. Use when implementing webhooks, validating webhook signatures, securing payment webhooks, testing webhook endpoints, preventing replay attacks, or when user mentions webhook security, Stripe webhooks, signature verification, webhook testing, or event validation.",
                "path": "plugins/payments/skills/webhook-security/SKILL.md",
                "frontmatter": {
                  "name": "webhook-security",
                  "description": "Webhook validation patterns with signature verification, event logging, and testing tools. Use when implementing webhooks, validating webhook signatures, securing payment webhooks, testing webhook endpoints, preventing replay attacks, or when user mentions webhook security, Stripe webhooks, signature verification, webhook testing, or event validation.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# webhook-security\n\n## Instructions\n\nThis skill provides comprehensive webhook security patterns for payment integrations (Stripe, PayPal, and other providers). It covers signature verification, replay attack prevention, event logging, idempotency, and local testing workflows.\n\n### 1. Webhook Signature Verification\n\nImplement cryptographic signature verification to authenticate webhook requests:\n\n**Why Signature Verification Matters:**\n- Prevents attackers from forging webhook events\n- Ensures events actually come from the payment provider\n- Required for PCI compliance in production\n- Protects against man-in-the-middle attacks\n\n**Setup Process:**\n```bash\n# Generate and configure webhook endpoint with signature verification\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/setup-webhook-endpoint.sh stripe\n```\n\n**Verification Algorithm (Stripe):**\n1. Extract timestamp and signature from webhook headers\n2. Construct signed payload: `timestamp.raw_body`\n3. Compute HMAC-SHA256 hash using webhook secret\n4. Compare computed signature with received signature\n5. Verify timestamp is within tolerance (5 minutes default)\n\n### 2. Replay Attack Prevention\n\nProtect against replay attacks where attackers resend captured webhook events:\n\n**Defense Mechanisms:**\n- **Timestamp Validation:** Reject events older than 5 minutes\n- **Event ID Tracking:** Store processed event IDs to prevent duplicates\n- **Signature Verification:** Ensures event hasn't been tampered with\n- **Idempotency Keys:** Safe to process same event multiple times\n\n**Implementation:**\n```python\n# Use the signature verification script\npython /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/verify-signature.py\n```\n\n### 3. Event Logging and Auditing\n\nLog all webhook events for debugging, compliance, and dispute resolution:\n\n**What to Log:**\n- Raw webhook payload (for signature re-verification)\n- Event type and ID\n- Timestamp received\n- Processing status (success, failure, retry)\n- Error messages if processing failed\n- User/account associated with event\n\n**Template Usage:**\n```python\n# Use event logger template for database storage\ncat /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/templates/event_logger.py\n```\n\n### 4. Local Webhook Testing\n\nTest webhooks locally before deploying to production:\n\n**Using Stripe CLI:**\n```bash\n# Forward webhooks to local development server\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/test-webhook-locally.sh\n```\n\n**Testing Workflow:**\n1. Install Stripe CLI (`stripe` command)\n2. Login to your Stripe account\n3. Forward webhooks to localhost\n4. Trigger test events from Stripe Dashboard or CLI\n5. Verify signature verification works\n6. Check event logging and processing\n\n### 5. Webhook Secret Management\n\nSecurely manage webhook signing secrets:\n\n**CRITICAL SECURITY RULE:**\n```bash\n# ‚úÖ CORRECT - Never hardcode secrets\nSTRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n\n# ‚ùå WRONG - Never commit real secrets\nSTRIPE_WEBHOOK_SECRET=whsec_1234567890abcdef...  # DON'T DO THIS\n```\n\n**Generate Webhook Secret:**\n```bash\n# Get webhook secret for your endpoint\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/generate-webhook-secret.sh\n```\n\n**Secret Rotation:**\n- Rotate webhook secrets quarterly\n- Update all endpoints when rotating\n- Test new secret before deactivating old one\n- Store in environment variables, never in code\n\n### 6. Error Handling and Retries\n\nHandle webhook processing failures gracefully:\n\n**Retry Logic:**\n- Payment providers retry failed webhooks automatically\n- Return 200 OK only after successful processing\n- Return 500/503 for temporary failures (triggers retry)\n- Return 400 for permanent failures (stops retries)\n\n**Template Usage:**\n```python\n# Use retry handler template\ncat /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/templates/retry_handler.py\n```\n\n**Best Practices:**\n- Make webhook handlers idempotent (safe to retry)\n- Process events asynchronously (queue for background jobs)\n- Respond quickly (< 5 seconds) to avoid timeouts\n- Store events before processing (persist first, process later)\n\n## Examples\n\n### Example 1: Stripe Subscription Webhook with Full Security\n\nComplete implementation with signature verification, logging, and idempotency:\n\n```bash\n# 1. Set up webhook endpoint with security\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/setup-webhook-endpoint.sh stripe\n\n# 2. View the complete implementation\ncat /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/examples/complete-webhook-handler.py\n\n# 3. Test locally before deploying\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/test-webhook-locally.sh\n\n# 4. Deploy to production\n# Ensure STRIPE_WEBHOOK_SECRET environment variable is set\n# Configure webhook endpoint URL in Stripe Dashboard\n```\n\n**Result:** Production-ready webhook handler that:\n- Verifies Stripe signatures cryptographically\n- Prevents replay attacks via timestamp validation\n- Logs all events to database with full audit trail\n- Handles retries idempotently (safe to process twice)\n- Returns appropriate HTTP status codes\n\n### Example 2: Event Processing with Idempotency\n\nProcess payment events safely with duplicate protection:\n\n```python\n# Use the event processing example\ncat /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/examples/event-processing-example.py\n```\n\n**Implementation:**\n1. Check if event ID already processed (database lookup)\n2. If processed, return 200 OK immediately (idempotent)\n3. If new, store event to database with \"pending\" status\n4. Process the event (update subscription, send email, etc.)\n5. Update event status to \"processed\"\n6. Return 200 OK\n\n**Result:** Safe to receive duplicate events without side effects\n\n### Example 3: Multi-Provider Webhook Handler\n\nSupport multiple payment providers with unified security:\n\n```bash\n# Set up webhooks for Stripe, PayPal, and Square\nfor provider in stripe paypal square; do\n  bash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/scripts/setup-webhook-endpoint.sh $provider\ndone\n```\n\n**Features:**\n- Provider-specific signature verification (different algorithms)\n- Unified event logging across providers\n- Consistent retry handling\n- Single testing workflow for all providers\n\n### Example 4: Complete Testing Workflow\n\nEnd-to-end webhook testing before production deployment:\n\n```bash\n# Use the complete testing example\nbash /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/payments/skills/webhook-security/examples/webhook-testing-example.sh\n```\n\n**Test Scenarios:**\n1. Valid signature - Should process successfully\n2. Invalid signature - Should reject with 401\n3. Expired timestamp - Should reject (replay protection)\n4. Duplicate event ID - Should return 200 (idempotent)\n5. Malformed payload - Should return 400\n6. Processing failure - Should return 500 (triggers retry)\n\n**Result:** High confidence before production deployment\n\n## Requirements\n\n**Environment Variables:**\n```bash\n# Stripe Configuration\nSTRIPE_API_KEY=sk_test_your_stripe_key_here\nSTRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n\n# PayPal Configuration (if using PayPal)\nPAYPAL_CLIENT_ID=your_paypal_client_id_here\nPAYPAL_CLIENT_SECRET=your_paypal_client_secret_here\nPAYPAL_WEBHOOK_ID=your_webhook_id_here\n\n# Database Configuration\nDATABASE_URL=postgresql://user:pass@localhost:5432/dbname\n```\n\n**Dependencies:**\n\nPython (FastAPI):\n- `fastapi` - Web framework for webhook endpoints\n- `stripe` - Stripe Python SDK\n- `sqlalchemy` - Database ORM for event logging\n- `pydantic` - Data validation\n- `python-dotenv` - Environment variable management\n- `httpx` - HTTP client for webhook testing\n\nDevelopment Tools:\n- `stripe-cli` - Local webhook testing (download from Stripe)\n- `ngrok` or `localtunnel` - Expose localhost for testing\n- `pytest` - Testing framework\n- `requests` - HTTP client for manual testing\n\n**Database Setup:**\n\nRequired table for event logging:\n```sql\nCREATE TABLE webhook_events (\n    id SERIAL PRIMARY KEY,\n    event_id VARCHAR(255) UNIQUE NOT NULL,\n    event_type VARCHAR(100) NOT NULL,\n    provider VARCHAR(50) NOT NULL,\n    payload JSONB NOT NULL,\n    signature VARCHAR(255) NOT NULL,\n    status VARCHAR(50) DEFAULT 'pending',\n    error_message TEXT,\n    created_at TIMESTAMP DEFAULT NOW(),\n    processed_at TIMESTAMP\n);\n\nCREATE INDEX idx_event_id ON webhook_events(event_id);\nCREATE INDEX idx_status ON webhook_events(status);\n```\n\n**Payment Provider Setup:**\n\nStripe:\n1. Create Stripe account (test mode for development)\n2. Configure webhook endpoint in Stripe Dashboard\n3. Select events to listen to (e.g., `customer.subscription.updated`)\n4. Copy webhook signing secret\n5. Set `STRIPE_WEBHOOK_SECRET` environment variable\n\nPayPal:\n1. Create PayPal Developer account\n2. Create REST API app\n3. Configure webhook endpoint in PayPal Developer Dashboard\n4. Copy webhook ID\n5. Set `PAYPAL_WEBHOOK_ID` environment variable\n\n## Security Best Practices\n\n**CRITICAL: Never Hardcode Webhook Secrets**\n\n```bash\n# ‚úÖ CORRECT - Use environment variables\nexport STRIPE_WEBHOOK_SECRET=whsec_your_webhook_secret_here\n\n# ‚úÖ CORRECT - Read from environment in code\nimport os\nwebhook_secret = os.getenv(\"STRIPE_WEBHOOK_SECRET\")\nif not webhook_secret:\n    raise ValueError(\"STRIPE_WEBHOOK_SECRET not set\")\n\n# ‚ùå WRONG - Never hardcode secrets\nwebhook_secret = \"whsec_1234567890abcdef...\"  # DON'T DO THIS\n```\n\n**Always Verify Signatures:**\n- Never trust webhook data without verification\n- Verify before any database operations\n- Reject requests with invalid signatures immediately\n- Log signature verification failures\n\n**Prevent Replay Attacks:**\n- Check timestamp is within 5-minute window\n- Store processed event IDs to detect duplicates\n- Use database constraints (UNIQUE on event_id)\n- Never process events older than tolerance window\n\n**Protect Webhook Endpoints:**\n- Use HTTPS in production (required by most providers)\n- Don't expose endpoint URL publicly\n- Rate limit webhook endpoints\n- Monitor for suspicious activity\n\n**Event Logging:**\n- Log raw payloads for audit trail\n- Store signature for re-verification\n- Record processing status and errors\n- Retain logs for dispute resolution (90+ days)\n\n**Idempotency:**\n- Check event_id before processing\n- Make handlers safe to retry\n- Use database transactions\n- Return 200 OK for duplicate events\n\n**Error Handling:**\n- Return appropriate HTTP status codes\n- 200 - Successfully processed\n- 400 - Invalid payload (don't retry)\n- 401 - Invalid signature (don't retry)\n- 500 - Processing error (will retry)\n- Respond within 5 seconds to avoid timeout\n\n## Common Webhook Events\n\n**Stripe Subscription Events:**\n- `customer.subscription.created` - New subscription\n- `customer.subscription.updated` - Plan change, renewal\n- `customer.subscription.deleted` - Cancellation\n- `invoice.payment_succeeded` - Successful payment\n- `invoice.payment_failed` - Failed payment\n\n**Stripe Payment Events:**\n- `payment_intent.succeeded` - One-time payment success\n- `payment_intent.payment_failed` - Payment failure\n- `charge.refunded` - Refund processed\n- `charge.dispute.created` - Chargeback dispute\n\n**PayPal Events:**\n- `PAYMENT.SALE.COMPLETED` - Payment completed\n- `BILLING.SUBSCRIPTION.CREATED` - New subscription\n- `BILLING.SUBSCRIPTION.CANCELLED` - Subscription cancelled\n- `CUSTOMER.DISPUTE.CREATED` - Dispute opened\n\n## Troubleshooting\n\n**Signature Verification Failing:**\n1. Check webhook secret is correct\n2. Verify using raw request body (not parsed JSON)\n3. Check timestamp tolerance (default 5 minutes)\n4. Ensure secret matches the endpoint in provider dashboard\n5. Test with Stripe CLI to rule out code issues\n\n**Events Not Reaching Endpoint:**\n1. Check endpoint URL is publicly accessible\n2. Verify HTTPS is configured (required in production)\n3. Check firewall/security group rules\n4. Review webhook logs in provider dashboard\n5. Test with ngrok/localtunnel for local development\n\n**Duplicate Events Being Processed:**\n1. Ensure event_id is stored before processing\n2. Check database has UNIQUE constraint on event_id\n3. Verify idempotency logic is implemented\n4. Review transaction handling in event processing\n\n---\n\n**Plugin:** payments\n**Version:** 1.0.0\n**Category:** Security\n**Skill Type:** Webhook Security"
              }
            ]
          },
          {
            "name": "celery",
            "description": "Production-ready Celery distributed task queue with worker management, beat scheduling, monitoring (Flower), and framework integrations (Django, Flask, FastAPI)",
            "source": "./plugins/celery",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "builder@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install celery@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-beat",
                "description": "Configure Celery Beat periodic task scheduling with crontab, interval, and custom schedules",
                "path": "plugins/celery/commands/add-beat.md",
                "frontmatter": {
                  "description": "Configure Celery Beat periodic task scheduling with crontab, interval, and custom schedules",
                  "argument-hint": [
                    "schedule-description"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure Celery Beat periodic task scheduling for automated background jobs\n\nCore Principles:\n- Detect existing Celery configuration before adding Beat\n- Support multiple schedule types (crontab, interval, solar)\n- Configure scheduler backend appropriately\n- Integrate with project framework (Django/Flask/FastAPI)\n- Validate Beat configuration before deployment\n\nPhase 1: Discovery\nGoal: Understand project structure and existing Celery setup\n\nActions:\n- Parse $ARGUMENTS for schedule requirements and task names\n- Detect project framework and Celery configuration\n- Example: !{bash find . -name \"celery.py\" -o -name \"celeryconfig.py\" -o -name \"celery_app.py\" 2>/dev/null | head -5}\n- Check for existing Beat configuration\n- Example: !{bash grep -r \"beat_schedule\" --include=\"*.py\" . 2>/dev/null | head -3}\n- Identify tasks directory structure\n- Load Celery configuration files for context\n\nPhase 2: Analysis\nGoal: Understand current Celery setup and requirements\n\nActions:\n- Read identified Celery configuration files\n- Check if Beat is already configured\n- Determine scheduler backend (default, DatabaseScheduler, Redis)\n- Identify existing tasks that need scheduling\n- Verify broker configuration supports Beat\n\nPhase 3: Planning\nGoal: Design Beat configuration approach\n\nActions:\n- Determine schedule types needed from $ARGUMENTS:\n  - Crontab: for time-based schedules (daily, weekly, monthly)\n  - Interval: for fixed intervals (every N seconds/minutes/hours)\n  - Solar: for sunrise/sunset-based schedules\n  - Custom: for complex scheduling logic\n- Choose appropriate scheduler backend:\n  - Default: In-memory (development)\n  - DatabaseScheduler: Persistent (Django/Flask)\n  - Redis: Distributed (production)\n- Plan integration with existing tasks\n- Outline configuration structure\n\nPhase 4: Implementation\nGoal: Configure Celery Beat with scheduled tasks\n\nActions:\n\nTask(description=\"Configure Celery Beat scheduling\", subagent_type=\"celery:beat-scheduler-agent\", prompt=\"You are the beat-scheduler-agent. Configure Celery Beat periodic task scheduling for $ARGUMENTS.\n\nProject context: Framework and current Celery setup identified in discovery phase\n\nRequirements:\n- Configure beat_schedule in Celery configuration\n- Add schedule definitions for specified tasks\n- Set up appropriate scheduler backend\n- Configure Beat service for framework (Django/Flask/FastAPI)\n- Add timezone configuration if needed\n- Create example scheduled tasks\n- Document schedule format and options\n- Add Beat process to deployment configuration\n\nSchedule types to support:\n- Crontab: Time-based (minute, hour, day_of_week, day_of_month, month_of_year)\n- Interval: Fixed intervals (timedelta or seconds)\n- Solar: Sunrise/sunset-based (for applicable use cases)\n- Custom: User-defined schedule classes\n\nExpected output:\n- Updated Celery configuration with beat_schedule\n- Scheduler backend configuration\n- Example scheduled tasks\n- Beat startup command\n- Documentation for adding new schedules\")\n\nPhase 5: Verification\nGoal: Validate Beat configuration and test scheduled tasks\n\nActions:\n- Check Celery configuration syntax\n- Example: !{bash python -c \"from celery_app import app; print(app.conf.beat_schedule)\" 2>&1 | head -10}\n- Verify scheduled tasks are defined correctly\n- Test Beat scheduler can start\n- Example: !{bash timeout 5 celery -A celery_app beat --loglevel=info 2>&1 || echo \"Beat validation check\"}\n- Review timezone configuration\n- Confirm integration with existing tasks\n\nPhase 6: Summary\nGoal: Document Beat configuration and usage\n\nActions:\n- Summarize configured schedules:\n  - Task names and descriptions\n  - Schedule types and timing\n  - Scheduler backend used\n- Provide Beat startup commands:\n  - Development: celery -A app beat\n  - Production: supervisord/systemd configuration\n- Document how to add new scheduled tasks\n- Highlight important configuration options:\n  - Timezone settings\n  - Max interval for Beat\n  - Scheduler backend persistence\n- Suggest monitoring and testing steps"
              },
              {
                "name": "/add-broker",
                "description": "Configure message broker (Redis, RabbitMQ, SQS) for Celery",
                "path": "plugins/celery/commands/add-broker.md",
                "frontmatter": {
                  "description": "Configure message broker (Redis, RabbitMQ, SQS) for Celery",
                  "argument-hint": [
                    "broker-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, AskUserQuestion, Glob, Grep"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure message broker for Celery with connection settings and security\n\nCore Principles:\n- Ask for broker type if not specified\n- Detect existing configuration\n- Use environment variables for credentials\n- Never hardcode secrets\n\nPhase 1: Discovery\nGoal: Understand broker requirements and current setup\n\nActions:\n- Parse $ARGUMENTS for broker type (redis, rabbitmq, sqs)\n- If broker type not provided, use AskUserQuestion to determine:\n  - Which broker to use (Redis, RabbitMQ, AWS SQS)?\n  - What environment (development, production)?\n  - Any specific connection requirements?\n- Check for existing Celery configuration:\n  - !{bash find . -name \"celery*.py\" -o -name \"*celery*.py\" 2>/dev/null | head -5}\n  - !{bash test -f .env && echo \".env exists\" || echo \"No .env file\"}\n- Load existing configuration if found\n\nPhase 2: Preparation\nGoal: Prepare context for broker configuration\n\nActions:\n- Detect project type:\n  - !{bash ls -la | grep -E \"requirements.txt|pyproject.toml|setup.py\" | head -3}\n- Check for existing broker packages:\n  - !{bash grep -r \"redis\\|celery\\|kombu\\|amqp\\|boto3\" requirements.txt pyproject.toml setup.py 2>/dev/null | head -10}\n- Load relevant files for context:\n  - @requirements.txt (if exists)\n  - @pyproject.toml (if exists)\n  - @.env.example (if exists)\n\nPhase 3: Configuration\nGoal: Configure broker with specialist agent\n\nActions:\n\nTask(description=\"Configure message broker\", subagent_type=\"celery:broker-specialist\", prompt=\"You are the celery:broker-specialist agent. Configure $ARGUMENTS message broker for Celery.\n\nContext from discovery:\n- Existing configuration files found\n- Project structure analyzed\n- Dependencies identified\n\nRequirements:\n- Install required broker packages (redis, kombu, boto3)\n- Configure broker URL with environment variables\n- Set up connection pooling and retry logic\n- Configure SSL/TLS if production environment\n- Add health check endpoints\n- Update .env.example with broker configuration placeholders\n- Create broker initialization module if needed\n\nSecurity Requirements:\n- Use environment variables for credentials\n- Never hardcode broker URLs with credentials\n- Use placeholders: redis_your_password_here, rabbitmq_your_password_here\n- Add .env to .gitignore if not already present\n- Document where to obtain credentials\n\nExpected output:\n- Broker configuration files created/updated\n- Dependencies added to requirements\n- Environment variable templates\n- Connection verification code\n- Documentation for setup\")\n\nPhase 4: Verification\nGoal: Verify broker configuration\n\nActions:\n- Check that configuration files were created\n- Verify environment variables use placeholders only\n- Confirm .gitignore protects .env file:\n  - !{bash grep \"^\\.env$\" .gitignore 2>/dev/null || echo \".env not in .gitignore\"}\n- Validate broker connection code exists\n- Check for proper error handling\n\nPhase 5: Summary\nGoal: Document what was configured\n\nActions:\n- Summarize broker configuration:\n  - Broker type configured\n  - Files created/modified\n  - Dependencies added\n  - Environment variables needed\n- Provide next steps:\n  - Set actual broker credentials in .env\n  - Install dependencies: pip install -r requirements.txt\n  - Test broker connection\n  - Configure result backend if needed\n- Display connection string format (with placeholders)"
              },
              {
                "name": "/add-error-handling",
                "description": "Implement comprehensive error handling and retry strategies for Celery tasks",
                "path": "plugins/celery/commands/add-error-handling.md",
                "frontmatter": {
                  "description": "Implement comprehensive error handling and retry strategies for Celery tasks",
                  "argument-hint": [
                    "task-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add robust error handling, retry logic, and exception tracking to Celery tasks\n\nCore Principles:\n- Detect existing error handling patterns\n- Configure exponential backoff retries\n- Implement custom error handlers\n- Add logging and monitoring\n\nPhase 1: Discovery\nGoal: Understand current task implementation and error handling state\n\nActions:\n- Parse $ARGUMENTS for task name/pattern\n- Find existing Celery tasks:\n  - !{bash find . -type f -name \"*.py\" -path \"*/tasks/*\" 2>/dev/null | head -20}\n- Detect current error handling approaches\n- Load project configuration for Celery settings\n\nPhase 2: Analysis\nGoal: Identify error-prone operations and retry requirements\n\nActions:\n- Read identified task files to understand:\n  - Current retry decorators\n  - Exception handling blocks\n  - Logging patterns\n  - External API calls or database operations\n- Check for existing Celery configuration:\n  - !{bash grep -r \"CELERY_\" . --include=\"*.py\" --include=\"settings.py\" | head -10}\n\nPhase 3: Implementation\nGoal: Add comprehensive error handling via task-generator-agent\n\nActions:\n\nTask(description=\"Implement error handling and retries\", subagent_type=\"celery:task-generator-agent\", prompt=\"You are the task-generator-agent. Implement comprehensive error handling and retry strategies for $ARGUMENTS.\n\nContext:\n- Task files and patterns identified in discovery\n- Existing error handling approaches\n- Project Celery configuration\n\nRequirements:\n- Add retry decorators with exponential backoff\n- Implement custom exception handlers for specific errors\n- Add structured logging for debugging\n- Configure max_retries and retry_backoff parameters\n- Add error callbacks for final failure handling\n- Use autoretry_for for known transient errors\n- Implement custom on_failure handlers\n- Add error tracking integration (Sentry if available)\n\nError Handling Patterns:\n1. Network/API errors - Retry with backoff\n2. Database errors - Short retry with circuit breaker\n3. Business logic errors - No retry, log and alert\n4. Validation errors - No retry, immediate failure\n5. Resource exhaustion - Exponential backoff\n\nExpected Output:\n- Updated task files with retry decorators\n- Custom exception classes if needed\n- Error handler implementations\n- Celery configuration updates\n- Logging configuration\n- Documentation of error handling strategy\")\n\nPhase 4: Validation\nGoal: Verify error handling implementation\n\nActions:\n- Check updated task files for proper decorators\n- Verify retry configuration is present\n- Ensure exception handlers are implemented\n- Validate logging is configured\n- Run syntax check: !{bash python -m py_compile $(find . -name \"tasks.py\" 2>/dev/null) 2>&1 | head -20}\n\nPhase 5: Summary\nGoal: Document error handling implementation\n\nActions:\n- List tasks updated with error handling\n- Summarize retry strategies configured\n- Document exception types handled\n- Show error tracking integration\n- Provide testing recommendations"
              },
              {
                "name": "/add-monitoring",
                "description": "Set up Flower web monitoring interface for Celery task tracking",
                "path": "plugins/celery/commands/add-monitoring.md",
                "frontmatter": {
                  "description": "Set up Flower web monitoring interface for Celery task tracking",
                  "argument-hint": [
                    "optional-port"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Set up Flower web monitoring dashboard for real-time Celery task tracking and management\n\nCore Principles:\n- Detect existing Celery configuration before adding monitoring\n- Install Flower with appropriate dependencies\n- Configure secure access with authentication if needed\n- Provide clear instructions for accessing the dashboard\n\nPhase 1: Discovery\nGoal: Understand project structure and Celery setup\n\nActions:\n- Parse $ARGUMENTS for optional port configuration (default: 5555)\n- Detect project type and package manager\n- Example: !{bash ls package.json pyproject.toml requirements.txt setup.py 2>/dev/null | head -1}\n- Locate existing Celery configuration\n- Example: !{bash find . -name \"celery.py\" -o -name \"celery_app.py\" -o -name \"*celery*.py\" 2>/dev/null | grep -v __pycache__ | head -5}\n\nPhase 2: Validation\nGoal: Verify Celery is configured\n\nActions:\n- Check if Celery is installed and configured\n- Verify broker connection settings exist\n- Load Celery configuration file for context\n- Confirm project is ready for monitoring integration\n\nPhase 3: Implementation\nGoal: Install and configure Flower monitoring\n\nActions:\n\nTask(description=\"Set up Flower monitoring\", subagent_type=\"celery:monitoring-integrator\", prompt=\"You are the monitoring-integrator agent. Set up Flower web monitoring interface for this Celery project.\n\nPort configuration: $ARGUMENTS (use 5555 if not specified)\n\nRequirements:\n- Install Flower package with appropriate version\n- Create Flower configuration file with security settings\n- Configure authentication if needed (basic auth recommended)\n- Set up systemd service or Docker configuration for production\n- Create startup script for development use\n- Configure CORS if needed for external access\n\nDeliverables:\n- Flower installed and configured\n- Configuration file with security settings\n- Startup scripts for dev and production\n- Access instructions with URL and credentials\")\n\nPhase 4: Verification\nGoal: Verify Flower monitoring is accessible\n\nActions:\n- Check if Flower package is installed\n- Example: !{bash python -c \"import flower; print(flower.__version__)\" 2>/dev/null || pip list | grep flower}\n- Verify configuration files were created\n- Test Flower can start (without blocking)\n- Provide access URL and instructions\n\nPhase 5: Summary\nGoal: Document setup and next steps\n\nActions:\n- Display Flower dashboard URL (http://localhost:5555 or custom port)\n- Show authentication credentials if configured\n- Explain how to start Flower for monitoring\n- List key features available in dashboard\n- Provide troubleshooting tips\n- Suggest monitoring best practices"
              },
              {
                "name": "/add-result-backend",
                "description": "Configure result backend (Redis, Database, RPC)",
                "path": "plugins/celery/commands/add-result-backend.md",
                "frontmatter": {
                  "description": "Configure result backend (Redis, Database, RPC)",
                  "argument-hint": [
                    "backend-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, AskUserQuestion, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure Celery result backend for task result storage with Redis, Database, or RPC backend.\n\nCore Principles:\n- Ask about backend type and requirements upfront\n- Detect existing configuration before modifying\n- Use environment variables for sensitive values (NEVER hardcode credentials)\n- Validate configuration after setup\n\nPhase 1: Discovery\nGoal: Understand current setup and requirements\n\nActions:\n- Parse $ARGUMENTS for backend type if provided\n- Check for existing Celery configuration:\n  - !{bash find . -name \"celery*.py\" -o -name \"celeryconfig.py\" 2>/dev/null | head -5}\n  - @celeryconfig.py (if exists)\n  - @celery.py (if exists)\n- Check for existing result backend configuration\n- If backend type unclear from $ARGUMENTS, use AskUserQuestion to gather:\n  - Which result backend to use? (Redis, Database, RPC)\n  - Result persistence requirements (temporary/permanent)?\n  - Performance and availability needs?\n  - Existing infrastructure (Redis/Database already available)?\n\nPhase 2: Requirements Validation\nGoal: Confirm backend selection and requirements\n\nActions:\n- Validate backend type selection from $ARGUMENTS or user response\n- Check project dependencies:\n  - @requirements.txt (if Python)\n  - @pyproject.toml (if Poetry)\n  - @setup.py (if exists)\n- Verify required packages:\n  - Redis backend: needs `celery[redis]` or `redis>=4.5.0`\n  - Database backend: needs `celery[sqlalchemy]` or `sqlalchemy>=1.4.0`\n  - RPC backend: included with RabbitMQ broker\n- Confirm configuration approach with user if significant changes needed\n\nPhase 3: Implementation\nGoal: Configure result backend with specialist agent\n\nActions:\n\nTask(description=\"Configure result backend\", subagent_type=\"celery:backend-specialist\", prompt=\"You are the Celery backend-specialist agent. Configure $ARGUMENTS result backend for this project.\n\nContext:\n- Backend type: [from discovery phase]\n- Existing config: [summary from discovery]\n- Requirements: [from user responses]\n\nRequirements:\n- Configure result_backend URL with environment variables (NO hardcoded credentials)\n- Install required backend packages\n- Set up result serialization and expiration\n- Configure connection pooling and retry logic\n- Implement monitoring and health checks\n- Create .env.example with clear placeholders\n\nSecurity Critical:\n- NEVER hardcode Redis passwords or database credentials\n- ALWAYS use environment variables for sensitive values\n- ALWAYS use placeholders like 'your_redis_password_here' in examples\n\nExpected output:\n- Updated Celery configuration with result backend\n- Environment variable template (.env.example)\n- Verification steps and testing instructions\n- Documentation on backend usage\")\n\nPhase 4: Verification\nGoal: Validate result backend configuration\n\nActions:\n- Check configuration files were updated correctly\n- Verify environment variable placeholders used (not hardcoded secrets)\n- Test backend connectivity if possible:\n  - !{bash python -c \"from celery import Celery; app = Celery(); print('Config loaded')\" 2>&1 || echo \"Check configuration\"}\n- Verify .env.example created with placeholders\n- Check .gitignore protects .env files\n\nPhase 5: Summary\nGoal: Document configuration and next steps\n\nActions:\n- Summarize result backend configuration:\n  - Backend type configured\n  - Configuration files modified\n  - Required packages to install\n  - Environment variables to set\n- Highlight security reminders:\n  - Set actual credentials in .env (not committed to git)\n  - Never use hardcoded passwords\n  - Refer to .env.example for required variables\n- Suggest next steps:\n  - Install required packages\n  - Set environment variables with actual credentials\n  - Test result backend with simple task\n  - Run /celery:validate to verify configuration"
              },
              {
                "name": "/add-routing",
                "description": "Configure task routing to specific queues/workers",
                "path": "plugins/celery/commands/add-routing.md",
                "frontmatter": {
                  "description": "Configure task routing to specific queues/workers",
                  "argument-hint": [
                    "routing-config"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure Celery task routing to direct specific tasks to designated queues and workers for optimal workload distribution and resource utilization.\n\nCore Principles:\n- Detect existing Celery configuration before modifying\n- Route by task priority, resource requirements, or business logic\n- Follow Celery routing best practices\n- Validate routing configuration before applying\n\nPhase 1: Discovery\nGoal: Understand current Celery setup and routing requirements\n\nActions:\n- Parse $ARGUMENTS for routing specification\n- Detect project structure and framework\n- Example: !{bash ls -la | grep -E \"(manage.py|main.py|app.py|celery.py)\"}\n- Identify existing Celery configuration files\n- Example: !{bash find . -name \"celery.py\" -o -name \"celeryconfig.py\" 2>/dev/null}\n- Load current configuration to understand baseline\n- Check for existing queue definitions\n\nPhase 2: Analysis\nGoal: Understand current architecture and identify routing needs\n\nActions:\n- Read Celery configuration files\n- Example: @celery.py or @celeryconfig.py\n- Scan for existing task definitions\n- Example: !{bash grep -r \"@shared_task\\|@task\\|@app.task\" --include=\"*.py\" | head -20}\n- Identify current queue setup and worker configuration\n- Determine routing requirements from $ARGUMENTS\n\nPhase 3: Planning\nGoal: Design routing strategy\n\nActions:\n- Determine routing approach:\n  - Task name patterns (default.*, email.*, reports.*)\n  - Task priority levels (high, medium, low)\n  - Resource requirements (cpu-intensive, io-bound, memory-intensive)\n  - Business domain (billing, notifications, analytics)\n- Plan queue definitions needed\n- Design routing rules and exchange configuration\n- Outline worker startup commands for each queue\n\nPhase 4: Implementation\nGoal: Configure routing with worker-architect agent\n\nActions:\n\nTask(description=\"Configure task routing\", subagent_type=\"celery:worker-architect\", prompt=\"You are the worker-architect agent. Configure Celery task routing for $ARGUMENTS.\n\nContext: Setting up task routing to direct specific tasks to designated queues and workers.\n\nRequirements:\n- Define queues with appropriate names and priorities\n- Configure task_routes to map tasks to queues\n- Set up task_queue_max_priority if using priority queues\n- Configure worker_prefetch_multiplier for each queue type\n- Add task_default_queue and task_default_exchange settings\n- Include routing documentation in comments\n- Provide worker startup commands for each queue\n\nRouting Configuration Should Include:\n- Queue definitions with exchange and routing key\n- Task routing rules (glob patterns or explicit names)\n- Priority settings if applicable\n- Worker configuration for optimal performance\n- Example startup commands for production\n\nExpected Output:\n- Updated Celery configuration with routing rules\n- Queue definitions\n- Worker startup commands\n- Documentation on how to test routing\")\n\nPhase 5: Verification\nGoal: Validate routing configuration\n\nActions:\n- Check that configuration files are syntactically correct\n- Example: !{bash python -m py_compile celery.py 2>&1}\n- Verify queue definitions are complete\n- Ensure task routes are properly configured\n- Review worker startup commands for correctness\n- Test configuration if Celery is available\n- Example: !{bash celery -A myapp inspect active_queues 2>/dev/null || echo \"Start workers to verify\"}\n\nPhase 6: Summary\nGoal: Report routing configuration results\n\nActions:\n- Display configured queues and their purposes\n- Show task routing rules that were added\n- Present worker startup commands for each queue\n- Provide testing instructions:\n  - How to start workers for specific queues\n  - How to send tasks to specific queues\n  - How to verify routing is working\n- Suggest monitoring and optimization strategies"
              },
              {
                "name": "/add-task",
                "description": "Generate new Celery task with retries and validation",
                "path": "plugins/celery/commands/add-task.md",
                "frontmatter": {
                  "description": "Generate new Celery task with retries and validation",
                  "argument-hint": "task-name",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate a production-ready Celery task with proper error handling, retries, and validation\n\nCore Principles:\n- Parse task requirements from $ARGUMENTS\n- Delegate specialized task generation to task-generator-agent\n- Follow Celery best practices for task configuration\n- Ensure proper file structure and imports\n\nPhase 1: Discovery\nGoal: Understand task requirements and project structure\n\nActions:\n- Parse $ARGUMENTS to extract task name and any optional parameters\n- Detect if Celery is already configured in the project\n- Check for existing tasks directory structure\n- Example: !{bash find . -name \"celery.py\" -o -name \"tasks.py\" 2>/dev/null | head -5}\n- Load existing Celery configuration if present\n\nPhase 2: Validation\nGoal: Verify Celery setup and task name validity\n\nActions:\n- Confirm Celery is installed and configured\n- Validate task name follows Python naming conventions\n- Check if task with same name already exists\n- Verify tasks directory exists or can be created\n- Example: !{bash python -c \"import celery; print('Celery installed')\" 2>/dev/null}\n\nPhase 3: Context Loading\nGoal: Understand existing task patterns\n\nActions:\n- Find and load existing task files for pattern reference\n- Example: !{bash find . -name \"*tasks*.py\" -type f 2>/dev/null | head -3}\n- If existing tasks found, load one as reference: @tasks.py\n- Note existing retry strategies, error handling patterns\n- Identify project-specific task decorators or mixins\n\nPhase 4: Task Generation\nGoal: Create production-ready Celery task\n\nActions:\n\nTask(description=\"Generate Celery task\", subagent_type=\"celery:task-generator-agent\", prompt=\"You are the task-generator-agent. Generate a production-ready Celery task for $ARGUMENTS.\n\nContext:\n- Task name: [parsed from $ARGUMENTS]\n- Existing patterns: [identified from Phase 3]\n- Project structure: [identified from Phase 1]\n\nRequirements:\n- Include proper task decorator with name and configuration\n- Add retry logic with exponential backoff\n- Implement input validation\n- Add comprehensive error handling\n- Include docstring with usage examples\n- Follow existing project patterns\n- Add logging for debugging\n- Include type hints if project uses them\n\nDeliverable:\n- Complete task function with all imports\n- Ready to write to appropriate file location\n- Follows Celery best practices\")\n\nPhase 5: File Integration\nGoal: Write task to correct location\n\nActions:\n- Determine appropriate file location (existing tasks.py or new file)\n- Write generated task code to file\n- Update imports if needed\n- Format code using project formatter if available\n- Example: !{bash black tasks.py 2>/dev/null || echo \"Skipping formatting\"}\n\nPhase 6: Verification\nGoal: Validate the generated task\n\nActions:\n- Check Python syntax is valid\n- Example: !{bash python -m py_compile [task_file]}\n- Verify imports are correct\n- Confirm task is registered with Celery\n- Run quick smoke test if possible\n\nPhase 7: Summary\nGoal: Document what was created\n\nActions:\n- Display task file location\n- Show task name and key features\n- Explain retry configuration\n- Provide usage example\n- Suggest next steps:\n  - Test the task: celery -A [app] worker --loglevel=info\n  - Call the task: task_name.delay(args)\n  - Monitor with Flower if available"
              },
              {
                "name": "/add-workers",
                "description": null,
                "path": "plugins/celery/commands/add-workers.md",
                "frontmatter": null,
                "content": "---\ndescription: Configure worker pools and concurrency\nargument-hint: [pool-type] [options]\nallowed-tools: Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure Celery workers with optimal pool types and concurrency settings based on project requirements\n\nCore Principles:\n- Detect existing Celery configuration before modifying\n- Choose pool type based on workload characteristics\n- Set concurrency based on CPU cores and task types\n- Provide clear worker configuration documentation\n\nPhase 1: Discovery\nGoal: Understand project structure and current worker configuration\n\nActions:\n- Parse $ARGUMENTS for pool type preference (prefork, gevent, eventlet, solo, threads)\n- Detect project type and existing Celery setup\n- Example: !{bash ls celery.py celeryconfig.py worker.py tasks.py 2>/dev/null || echo \"No Celery files found\"}\n- Find existing worker configuration:\n  - @celery.py\n  - @celeryconfig.py\n  - @worker.py\n- Check for existing pool and concurrency settings\n\nPhase 2: Analysis\nGoal: Determine optimal worker configuration\n\nActions:\n- Analyze task types if tasks.py exists\n- Count CPU cores for concurrency defaults\n- Example: !{bash nproc || python3 -c \"import os; print(os.cpu_count())\"}\n- Review task characteristics:\n  - CPU-bound vs I/O-bound\n  - Long-running vs short tasks\n  - Memory requirements\n- Identify appropriate pool type:\n  - **prefork**: CPU-bound tasks (default)\n  - **gevent**: I/O-bound, many concurrent tasks\n  - **eventlet**: Similar to gevent, alternative greenlet implementation\n  - **threads**: Thread-based concurrency\n  - **solo**: Single worker, debugging\n\nPhase 3: Implementation\nGoal: Configure workers with architect agent\n\nActions:\n\nTask(description=\"Configure Celery workers\", subagent_type=\"celery:worker-architect\", prompt=\"You are the celery:worker-architect agent. Configure Celery workers for $ARGUMENTS.\n\nCurrent project context:\n- Detected pool type preference: [from $ARGUMENTS or auto-detect]\n- CPU cores available: [from nproc]\n- Existing configuration: [from celeryconfig.py or celery.py]\n\nRequirements:\n- Set appropriate pool type (prefork/gevent/eventlet/threads/solo)\n- Configure concurrency based on pool type and CPU cores\n- Set worker autoscaling if applicable\n- Configure worker prefetch settings\n- Set time limits (soft/hard)\n- Configure memory management (max-tasks-per-child)\n- Add worker monitoring hooks if needed\n- Create worker startup script\n\nDeliverable:\n- Updated worker configuration files\n- Worker startup script with proper arguments\n- Documentation of worker settings and rationale\")\n\nPhase 4: Verification\nGoal: Validate worker configuration\n\nActions:\n- Check generated configuration files exist\n- Verify pool type is set correctly\n- Confirm concurrency settings are appropriate\n- Review worker startup script\n- Example: !{bash cat worker.py 2>/dev/null | grep -E \"pool|concurrency|autoscale\"}\n- Test worker configuration syntax (dry run if possible)\n\nPhase 5: Summary\nGoal: Document worker configuration\n\nActions:\n- Display configured settings:\n  - Pool type and rationale\n  - Concurrency settings\n  - Autoscaling configuration\n  - Memory limits\n  - Time limits\n- Show worker startup command\n- Provide next steps:\n  - Test worker: `celery -A app worker --loglevel=info`\n  - Monitor workers: `celery -A app inspect stats`\n  - Scale workers: `celery -A app control pool_grow N`\n- Document performance tuning considerations\n"
              },
              {
                "name": "/add-workflow",
                "description": "Create task workflows (chains, groups, chords)",
                "path": "plugins/celery/commands/add-workflow.md",
                "frontmatter": {
                  "description": "Create task workflows (chains, groups, chords)",
                  "argument-hint": [
                    "workflow-type or description"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Design and implement Celery task workflows using Canvas primitives (chains, groups, chords) for orchestrating distributed task execution.\n\nCore Principles:\n- Understand workflow requirements before implementing\n- Use appropriate Canvas primitives (chain, group, chord)\n- Implement proper error handling and retry strategies\n- Follow Celery Canvas best practices\n- Ask clarifying questions when workflow requirements are unclear\n\nPhase 1: Discovery\nGoal: Understand project structure and workflow requirements\n\nActions:\n- Parse $ARGUMENTS for workflow type or description\n- Detect existing Celery tasks in project\n- Example: !{bash find . -name \"*tasks.py\" -o -name \"celery.py\" 2>/dev/null | head -10}\n- Identify task definitions that will be used in workflow\n\nIf $ARGUMENTS is unclear or insufficient, use AskUserQuestion to gather:\n- What is the workflow goal (ETL pipeline, map-reduce, batch processing)?\n- Are tasks sequential (chain), parallel (group), or map-reduce (chord)?\n- What tasks need to be orchestrated?\n- What error handling strategy is needed?\n- Should results be aggregated?\n\nPhase 2: Task Analysis\nGoal: Analyze existing tasks and determine workflow pattern\n\nActions:\n- Search for task definitions in project\n- Example: !{bash grep -r \"@.*task\" --include=\"*tasks.py\" . | head -20}\n- Load relevant task files for context\n- Understand task signatures and parameters\n- Determine workflow pattern based on requirements:\n  - Chain: Sequential tasks where output flows to next task\n  - Group: Independent parallel tasks\n  - Chord: Parallel tasks with result aggregation (map-reduce)\n  - Nested: Complex workflows combining multiple patterns\n\nPhase 3: Workflow Design\nGoal: Design workflow structure and Canvas composition\n\nActions:\n- Outline workflow structure and task flow\n- Identify data dependencies between tasks\n- Plan error handling strategy (link_error, retries, fallbacks)\n- Determine result backend requirements\n- Present design to user with explanation of Canvas primitives chosen\n- Confirm approach before implementation\n\nPhase 4: Implementation\nGoal: Generate workflow code using workflow-specialist agent\n\nActions:\n\nTask(description=\"Implement Celery workflow\", subagent_type=\"celery:workflow-specialist\", prompt=\"You are the workflow-specialist agent. Create Celery workflow for $ARGUMENTS.\n\nWorkflow Requirements:\n- Pattern: [chain/group/chord based on Phase 2 analysis]\n- Tasks to orchestrate: [list from task analysis]\n- Error handling: [strategy from user requirements]\n- Result aggregation: [if applicable for chords]\n\nImplementation Requirements:\n- Use appropriate Canvas primitives (chain, group, chord)\n- Implement immutable signatures\n- Add error handling with link_error or retries\n- Include result aggregation logic if chord pattern\n- Add workflow testing code\n- Follow Celery Canvas best practices\n- Include comments explaining workflow structure\n\nExpected Output:\n- Complete workflow implementation file\n- Error handling and retry logic\n- Test workflow function\n- Documentation of workflow behavior\")\n\nPhase 5: Verification\nGoal: Validate workflow implementation\n\nActions:\n- Review generated workflow code\n- Verify Canvas primitive usage is correct\n- Check error handling implementation\n- Validate workflow syntax\n- Example: !{bash python -m py_compile workflows.py 2>&1}\n- Run basic workflow tests if available\n- Example: !{bash python -m pytest tests/test_workflows.py -v 2>&1 || echo \"No tests found\"}\n\nPhase 6: Summary\nGoal: Document workflow creation and provide usage guidance\n\nActions:\n- Summarize workflow created:\n  - Workflow pattern used (chain/group/chord)\n  - Tasks orchestrated\n  - Error handling strategy\n  - Files created/modified\n- Provide workflow usage examples\n- Suggest next steps:\n  - Run workflow with sample data\n  - Add monitoring and observability\n  - Optimize task granularity if needed\n  - Set up result backend if using chords"
              },
              {
                "name": "/deploy",
                "description": "Production deployment configuration (Docker, K8s, systemd)",
                "path": "plugins/celery/commands/deploy.md",
                "frontmatter": {
                  "description": "Production deployment configuration (Docker, K8s, systemd)",
                  "argument-hint": [
                    "platform"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate production-ready deployment configurations for Celery workers, beat scheduler, and flower monitoring\n\nCore Principles:\n- Ask clarifying questions about deployment platform and requirements\n- Detect existing Celery configuration before generating deployment files\n- Use environment variables with placeholders (never hardcode secrets)\n- Follow platform-specific best practices for scaling and reliability\n\nPhase 1: Discovery\nGoal: Understand deployment requirements and existing setup\n\nActions:\n- Parse $ARGUMENTS for target platform (systemd/docker/kubernetes)\n- If platform unclear, use AskUserQuestion to gather:\n  - Which deployment platform? (systemd, Docker Compose, Kubernetes)\n  - How many worker processes needed?\n  - Will you deploy beat scheduler and flower monitoring?\n  - What are your scaling requirements?\n- Detect project structure and package manager\n- Example: !{bash ls requirements.txt pyproject.toml setup.py docker-compose.yml Dockerfile 2>/dev/null}\n- Locate existing Celery configuration\n- Example: !{bash find . -name \"celery.py\" -o -name \"celery_app.py\" 2>/dev/null | grep -v __pycache__ | head -5}\n\nPhase 2: Analysis\nGoal: Understand current Celery setup\n\nActions:\n- Read Celery configuration file for broker and backend settings\n- Check for existing deployment configurations\n- Identify task modules and queue configuration\n- Verify broker type (Redis/RabbitMQ/SQS)\n- Load application entry point\n\nPhase 3: Planning\nGoal: Design deployment architecture\n\nActions:\n- Outline deployment architecture based on platform:\n  - systemd: Service units for worker, beat, flower\n  - Docker: Multi-container compose with health checks\n  - Kubernetes: Deployments, ConfigMaps, Secrets, HPA\n- Plan scaling strategy (manual vs autoscaling)\n- Design health check approach\n- Determine resource limits\n- Present deployment plan to user\n\nPhase 4: Implementation\nGoal: Generate deployment configurations\n\nActions:\n\nTask(description=\"Generate deployment configs\", subagent_type=\"celery:deployment-architect\", prompt=\"You are the deployment-architect agent. Generate production deployment configurations for this Celery application.\n\nPlatform: $ARGUMENTS\nDetected configuration: [broker type, task modules, queues]\n\nRequirements:\n- Create platform-specific deployment files (systemd/Docker/K8s)\n- Configure celery worker with appropriate concurrency\n- Set up beat scheduler as singleton service\n- Include flower monitoring dashboard\n- Use environment variables with placeholders only\n- Add graceful shutdown handling (SIGTERM)\n- Configure health checks for all services\n- Set reasonable resource limits\n- Include security best practices\n\nDeliverables:\n- Complete deployment configuration files\n- .env.example with placeholder values only\n- Deployment documentation with setup steps\n- Scaling and troubleshooting guide\")\n\nPhase 5: Verification\nGoal: Validate generated configurations\n\nActions:\n- Verify all configuration files were created\n- Check for hardcoded secrets (should be none)\n- Confirm .env.example uses placeholders only\n- Validate syntax based on platform:\n  - Docker: !{bash docker-compose config 2>&1 || echo \"Validation pending\"}\n  - Kubernetes: !{bash kubectl apply --dry-run=client -f . 2>&1 || echo \"Validation pending\"}\n- Ensure documentation is complete\n\nPhase 6: Summary\nGoal: Document deployment setup\n\nActions:\n- List all generated configuration files\n- Highlight key configuration parameters\n- Explain deployment workflow\n- Provide platform-specific commands:\n  - systemd: sudo systemctl start celery-worker@1\n  - Docker: docker-compose up -d\n  - Kubernetes: kubectl apply -f .\n- Show monitoring access (Flower URL)\n- Note scaling procedures\n- List next steps for production readiness"
              },
              {
                "name": "/init",
                "description": "Initialize Celery in existing project with framework detection",
                "path": "plugins/celery/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Celery in existing project with framework detection",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Initialize Celery with automatic framework detection and broker configuration\n\nCore Principles:\n- Detect framework before configuring (Django, Flask, FastAPI, standalone)\n- Use environment variables for all broker credentials\n- Never hardcode API keys or passwords\n- Follow framework-specific best practices\n- Verify setup before completion\n\nPhase 1: Discovery\nGoal: Understand project structure and detect framework\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory)\n- Detect Python project files\n- Check for framework indicators:\n  - Django: manage.py, settings.py\n  - Flask: app.py with Flask imports\n  - FastAPI: main.py with FastAPI imports\n  - Standalone: generic Python project\n\n!{bash cd \"${ARGUMENTS:-.}\" && pwd}\n!{bash ls -la manage.py settings.py app.py main.py requirements.txt pyproject.toml 2>/dev/null | head -20}\n\nPhase 2: Framework Analysis\nGoal: Load project files to confirm framework type\n\nActions:\n- Read requirements or pyproject.toml to identify dependencies\n- Confirm framework from imports and structure\n- Identify project name and layout\n\nExample checks:\n- @requirements.txt (if exists)\n- @pyproject.toml (if exists)\n- !{bash find . -name \"settings.py\" -o -name \"app.py\" -o -name \"main.py\" | head -5}\n\nPhase 3: Setup Initialization\nGoal: Invoke celery-setup-agent with detected context\n\nActions:\n\nTask(description=\"Initialize Celery with framework detection\", subagent_type=\"celery:celery-setup-agent\", prompt=\"You are the celery-setup-agent. Initialize Celery for project at: $ARGUMENTS\n\nFramework Detection Results:\n- Project path: [detected path]\n- Framework type: [Django/Flask/FastAPI/Standalone]\n- Python version: [if detected]\n- Existing dependencies: [from requirements/pyproject.toml]\n\nRequirements:\n1. Load appropriate Celery documentation via WebFetch\n2. Ask user for broker choice (Redis/RabbitMQ/SQS)\n3. Ask if result backend is needed\n4. Install Celery and broker dependencies\n5. Create framework-appropriate configuration files\n6. Generate .env.example with placeholders (NO real credentials)\n7. Update .gitignore to protect .env files\n8. Create example task to verify setup\n9. Document worker startup commands\n\nSecurity Requirements:\n- NEVER hardcode broker passwords or credentials\n- ALWAYS use environment variables for broker URLs\n- CREATE .env.example with obvious placeholders\n- ENSURE .gitignore protects .env files\n\nExpected deliverable:\n- Complete Celery initialization\n- Configuration files with security best practices\n- Documentation of setup and next steps\n- Verification that Celery imports work\")\n\nPhase 4: Verification\nGoal: Confirm Celery is properly initialized\n\nActions:\n- Verify Celery installation: !{bash cd \"${ARGUMENTS:-.}\" && python -c \"import celery; print(f'Celery {celery.__version__} installed')\" 2>&1}\n- Check configuration files exist\n- Validate .env.example has placeholders only\n- Confirm .gitignore protects .env\n\nPhase 5: Summary\nGoal: Document what was accomplished and next steps\n\nActions:\n- Display initialization summary:\n  - Framework detected and configured\n  - Broker type selected\n  - Configuration files created\n  - Dependencies installed\n\n- Next steps:\n  - Copy .env.example to .env and add real broker credentials\n  - Create tasks using: /celery:task-generator\n  - Configure workers using: /celery:worker-setup\n  - Start worker: celery -A [app_name] worker --loglevel=info\n  - Set up monitoring: /celery:monitoring-setup\n\n- Security reminder:\n  - Never commit .env file with real credentials\n  - Keep .env.example with placeholders for reference\n  - Use environment-specific credentials for dev/staging/prod"
              },
              {
                "name": "/integrate-django",
                "description": "Full Celery integration with Django project",
                "path": "plugins/celery/commands/integrate-django.md",
                "frontmatter": {
                  "description": "Full Celery integration with Django project",
                  "argument-hint": [
                    "django-project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Celery with Django project, enabling task autodiscovery, transaction-safe dispatching, and result storage\n\nCore Principles:\n- Detect Django structure before integrating\n- Configure transaction-safe task patterns\n- Enable task autodiscovery from Django apps\n- Use environment variables for all credentials\n\nPhase 1: Discovery\nGoal: Understand Django project structure\n\nActions:\n- Parse $ARGUMENTS for Django project path\n- If path not provided, assume current directory\n- Detect Django project structure:\n  - !{bash find . -name \"manage.py\" -type f 2>/dev/null | head -1}\n  - !{bash find . -name \"settings.py\" -type f 2>/dev/null | head -1}\n- Identify Django project name from settings.py location\n- Check existing INSTALLED_APPS for Celery packages\n- Load Django settings for context: @*/settings.py\n\nPhase 2: Requirements Gathering\nGoal: Determine integration scope\n\nActions:\n- Ask user about integration requirements:\n  - Which Django apps will contain Celery tasks?\n  - Do you need periodic task scheduling (django-celery-beat)?\n  - Should results be stored in Django database (django-celery-results)?\n  - What broker will you use (Redis, RabbitMQ, other)?\n- Create todo list tracking integration phases:\n  - Core Celery setup\n  - Django configuration\n  - Task autodiscovery\n  - Result backend (if needed)\n  - Beat scheduler (if needed)\n  - Verification\n\nPhase 3: Django Integration\nGoal: Integrate Celery with Django project\n\nActions:\n\nTask(description=\"Integrate Celery with Django\", subagent_type=\"celery:django-integrator\", prompt=\"You are the django-integrator agent. Integrate Celery with the Django project at $ARGUMENTS.\n\nContext from discovery:\n- Django project name: [detected from settings.py]\n- INSTALLED_APPS: [current apps]\n- Database backend: [detected from settings]\n\nRequirements:\n- Create project_name/celery.py with Django settings autodiscovery\n- Update project_name/__init__.py to load Celery app on startup\n- Add CELERY_* settings to settings.py with environment variables\n- Add django-celery-results to INSTALLED_APPS if result storage needed\n- Add django-celery-beat to INSTALLED_APPS if periodic tasks needed\n- Create example tasks.py in one Django app showing @shared_task\n- Implement transaction-safe task dispatch with transaction.on_commit()\n- Create/update .env.example with broker and backend URL placeholders\n- Add .env to .gitignore if not present\n- Run migrations for celery tables (results/beat)\n- Document setup in README or docs/\n\nExpected output:\n- Fully integrated Django-Celery setup\n- celery.py module in project root\n- Updated settings.py with CELERY configuration\n- Sample tasks demonstrating transaction safety\n- Environment variable setup documented\n- Migrations applied\n- Verification commands provided\")\n\nWait for agent to complete integration.\n\nPhase 4: Verification\nGoal: Verify integration works correctly\n\nActions:\n- Check celery.py was created with correct Django integration\n- Verify __init__.py imports Celery app\n- Confirm settings.py contains CELERY_* configuration\n- Test task discovery: !{bash cd $ARGUMENTS && python manage.py shell -c \"from django import setup; setup(); from celery import current_app; print(current_app.tasks.keys())\"}\n- Verify migrations applied: !{bash cd $ARGUMENTS && python manage.py showmigrations | grep celery}\n- Check worker can start: !{bash cd $ARGUMENTS && timeout 5 python manage.py celery worker --loglevel=info 2>&1 | head -20}\n- Update todos marking verification complete\n\nPhase 5: Summary\nGoal: Document integration and provide next steps\n\nActions:\n- Mark all todos complete\n- Summarize what was configured:\n  - Celery app location and configuration\n  - Django settings added\n  - Result backend setup (if used)\n  - Beat scheduler setup (if used)\n  - Example tasks created\n  - Transaction-safe patterns implemented\n- Provide startup commands:\n  - Worker: `python manage.py celery worker -l info`\n  - Beat (if used): `python manage.py celery beat -l info`\n  - Combined: `python manage.py celery worker -B -l info`\n- Show how to test tasks from Django shell\n- Highlight transaction safety patterns to follow\n- Note environment variables that need real values"
              },
              {
                "name": "/integrate-fastapi",
                "description": "Integrate Celery with FastAPI with async support",
                "path": "plugins/celery/commands/integrate-fastapi.md",
                "frontmatter": {
                  "description": "Integrate Celery with FastAPI with async support",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Celery task queues with FastAPI applications for async background processing\n\nCore Principles:\n- Detect existing FastAPI and Celery setup before acting\n- Configure async-compatible task submission and tracking\n- Implement complete task lifecycle with status endpoints\n- Ensure production-ready error handling and monitoring\n\nPhase 1: Discovery\nGoal: Understand existing FastAPI and Celery configuration\n\nActions:\n- Parse $ARGUMENTS for integration options and requirements\n- Detect FastAPI application structure:\n  - !{bash find . -name \"main.py\" -o -name \"app.py\" | grep -v \"__pycache__\" | head -5}\n- Check for existing Celery configuration:\n  - !{bash find . -name \"celery*.py\" -o -name \"tasks.py\" | grep -v \"__pycache__\" | head -5}\n- Load package configuration to understand dependencies:\n  - @requirements.txt (if exists)\n  - @pyproject.toml (if exists)\n  - @package.json (if exists)\n- Check current FastAPI routes and structure\n\nPhase 2: Analysis\nGoal: Assess integration requirements and current state\n\nActions:\n- Analyze FastAPI application architecture:\n  - Current routing structure\n  - Middleware configuration\n  - Dependency injection patterns in use\n- Review existing Celery setup (if any):\n  - Broker configuration\n  - Task definitions\n  - Result backend setup\n- Identify integration points:\n  - Where to add task submission endpoints\n  - Status tracking endpoint design\n  - Health check integration\n- Check for async/await compatibility requirements\n\nPhase 3: Planning\nGoal: Design FastAPI-Celery integration approach\n\nActions:\n- Plan directory structure for integration:\n  - app/celery_app.py (Celery instance)\n  - app/tasks.py (task definitions)\n  - app/routers/tasks.py (FastAPI endpoints)\n  - app/dependencies.py (dependency injection)\n- Design endpoint architecture:\n  - POST /tasks/{task_name} (submit task)\n  - GET /tasks/{task_id}/status (check status)\n  - GET /tasks/{task_id}/result (retrieve result)\n  - GET /health/celery (health check)\n- Plan environment configuration:\n  - CELERY_BROKER_URL\n  - CELERY_RESULT_BACKEND\n  - Connection settings\n- Outline monitoring and error handling strategy\n\nPhase 4: Implementation\nGoal: Execute integration with fastapi-integrator agent\n\nActions:\n\nTask(description=\"Integrate Celery with FastAPI\", subagent_type=\"celery:fastapi-integrator\", prompt=\"You are the fastapi-integrator agent. Integrate Celery with FastAPI for $ARGUMENTS.\n\nContext:\n- FastAPI application structure detected\n- Existing Celery configuration reviewed\n- Integration points identified\n\nRequirements:\n- Create Celery app instance with broker configuration\n- Implement FastAPI dependency injection for Celery\n- Build task submission endpoints (POST /tasks/*)\n- Build status tracking endpoints (GET /tasks/{task_id}/status)\n- Build result retrieval endpoints (GET /tasks/{task_id}/result)\n- Add health check endpoint (GET /health/celery)\n- Configure environment variables with placeholders\n- Add startup/shutdown lifecycle events\n- Create example tasks with progress tracking\n- Implement error handling for broker failures\n- Generate OpenAPI documentation for endpoints\n- Add type hints throughout\n\nDeliverable:\n- Complete FastAPI-Celery integration\n- Task submission and tracking endpoints\n- Environment configuration with placeholders\n- Health check and monitoring endpoints\n- Example tasks demonstrating patterns\n- Type-safe, production-ready code\")\n\nPhase 5: Verification\nGoal: Verify integration works correctly\n\nActions:\n- Run type checking if mypy available:\n  - !{bash which mypy > /dev/null && mypy app/ || echo \"mypy not installed, skipping type check\"}\n- Check generated files exist:\n  - !{bash ls -la app/celery_app.py app/tasks.py app/routers/tasks.py 2>/dev/null || echo \"Checking file structure\"}\n- Verify environment configuration:\n  - @.env.example\n- Display FastAPI OpenAPI endpoint:\n  - FastAPI docs will be available at /docs endpoint\n  - Task endpoints will appear in OpenAPI schema\n\nPhase 6: Summary\nGoal: Document integration and next steps\n\nActions:\n- Summarize integration completed:\n  - Celery app instance with broker configuration\n  - FastAPI task submission and tracking endpoints\n  - Health check and monitoring setup\n  - Example tasks with progress tracking\n- Key files created: app/celery_app.py, app/tasks.py, app/routers/tasks.py, .env.example\n- Environment variables: CELERY_BROKER_URL and CELERY_RESULT_BACKEND (placeholders added)\n- Testing: Start Redis (docker run -d -p 6379:6379 redis), start worker (celery -A app.celery_app worker), start FastAPI (uvicorn app.main:app)\n- Next steps: Configure broker, add custom tasks, set up production workers, enable Flower monitoring"
              },
              {
                "name": "/integrate-flask",
                "description": "Integrate Celery with Flask application",
                "path": "plugins/celery/commands/integrate-flask.md",
                "frontmatter": {
                  "description": "Integrate Celery with Flask application",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Celery task queue with Flask web application, handling app context, factory patterns, and background task execution.\n\nCore Principles:\n- Detect Flask patterns (factory vs direct instantiation)\n- Preserve Flask application context in tasks\n- Share configuration between Flask and Celery\n- Follow Flask-Celery best practices\n\nPhase 1: Discovery\nGoal: Understand Flask application structure and requirements\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory)\n- Detect Flask application structure:\n  - !{bash find . -name \"app.py\" -o -name \"__init__.py\" -o -name \"wsgi.py\" 2>/dev/null | head -5}\n- Check for application factory pattern:\n  - !{bash grep -r \"def create_app\" . --include=\"*.py\" 2>/dev/null}\n- Identify configuration files:\n  - @config.py (if exists)\n  - @.env.example (if exists)\n- Check for existing Celery setup:\n  - !{bash grep -r \"from celery import\" . --include=\"*.py\" 2>/dev/null}\n- Determine Flask version and dependencies:\n  - @requirements.txt (if exists)\n\nPhase 2: Planning\nGoal: Design Flask-Celery integration approach\n\nActions:\n- Create todo list for integration steps using TodoWrite\n- Determine integration pattern based on discovery:\n  - Factory pattern if create_app() found\n  - Direct instantiation for simple apps\n- Identify broker and backend (Redis/RabbitMQ from config or ask)\n- Plan task module organization\n- Check database requirements (SQLAlchemy sessions in tasks)\n- Outline configuration sharing strategy\n\nPhase 3: Implementation\nGoal: Execute Flask-Celery integration with agent\n\nActions:\n\nTask(description=\"Integrate Celery with Flask\", subagent_type=\"celery:flask-integrator\", prompt=\"You are the flask-integrator agent. Integrate Celery with the Flask application at $ARGUMENTS.\n\nContext from Discovery:\n- Flask application structure detected\n- Application factory pattern usage identified\n- Configuration files analyzed\n- Database ORM requirements determined\n\nRequirements:\n- Create celery_app.py with Flask-aware Task class\n- Implement proper app context handling\n- Configure Celery to share Flask configuration\n- Create example tasks with context access\n- Handle database session management if SQLAlchemy used\n- Install required dependencies (celery[redis] or celery[amqp])\n- Create .env.example with placeholder broker URLs\n- Update .gitignore for environment files\n- Provide worker startup commands\n\nIntegration Patterns:\n- Use FlaskTask base class for app context preservation\n- Implement celery_init_app() for factory pattern\n- Configure broker_url and result_backend from Flask config\n- Register tasks properly for discovery\n- Handle proper cleanup and error handling\n\nExpected Output:\n- Complete Flask-Celery integration\n- Sample tasks demonstrating context usage\n- Configuration files with placeholders\n- Documentation for running workers\n- Verification commands for testing setup\")\n\nPhase 4: Verification\nGoal: Verify Flask-Celery integration works correctly\n\nActions:\n- Check created files exist:\n  - !{bash ls celery_app.py tasks.py .env.example 2>/dev/null}\n- Verify Flask app initializes:\n  - !{bash python -c \"from app import create_app; app=create_app(); print('Flask app created')\" 2>&1 || echo \"Check import paths\"}\n- Check Celery configuration:\n  - !{bash grep -E \"(broker_url|result_backend)\" celery_app.py config.py 2>/dev/null}\n- Verify no hardcoded credentials:\n  - !{bash grep -rE \"(redis://.*password|amqp://.*:.+@)\" . --include=\"*.py\" && echo \"WARNING: Found credentials\" || echo \"No credentials found\"}\n- Update TodoWrite with verification results\n\nPhase 5: Summary\nGoal: Document integration completion and next steps\n\nActions:\n- Mark all todos complete using TodoWrite\n- Display integration summary:\n  - Files created (celery_app.py, tasks.py, config updates)\n  - Configuration approach used\n  - Context handling strategy implemented\n  - Sample tasks provided\n- Show next steps:\n  - Start worker: celery -A app.celery_app worker --loglevel=info\n  - Start Flask app: flask run\n  - Test task execution from Flask shell or API\n  - Monitor tasks with flower (if monitoring added)\n- Highlight key files to review:\n  - celery_app.py: Celery initialization and Flask context\n  - tasks.py: Example background tasks\n  - .env.example: Configuration placeholders\n  - config.py: Shared Flask-Celery configuration"
              },
              {
                "name": "/test",
                "description": "Generate test suite for Celery tasks",
                "path": "plugins/celery/commands/test.md",
                "frontmatter": {
                  "description": "Generate test suite for Celery tasks",
                  "argument-hint": "task-name-or-path",
                  "allowed-tools": "Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Generate comprehensive pytest test suite for Celery tasks with mocking, fixtures, and async testing\n\nCore Principles:\n- Detect don't assume task structure\n- Generate tests following pytest best practices\n- Mock Celery internals for unit testing\n- Include tests for retries, failures, and edge cases\n\nPhase 1: Discovery\nGoal: Find tasks and understand project structure\n\nActions:\n- Parse $ARGUMENTS for specific task name or path\n- Detect project testing framework and structure\n- Example: !{bash ls pytest.ini setup.cfg pyproject.toml conftest.py 2>/dev/null}\n- Find existing Celery tasks to test\n- Example: !{bash find . -name \"*tasks*.py\" -type f 2>/dev/null | head -10}\n- Check for existing test directory structure\n- Example: !{bash find . -type d -name \"tests\" -o -name \"test\" 2>/dev/null}\n\nPhase 2: Task Analysis\nGoal: Load and understand task implementations\n\nActions:\n- If $ARGUMENTS specifies task file, load it: @[task_file]\n- Otherwise, find and load all task files\n- Parse task decorators and configurations\n- Identify task parameters, return types, and error handling\n- Note retry policies and exception handling\n- Example: !{bash grep -n \"@task\\|@shared_task\\|@app.task\" [task_files]}\n\nPhase 3: Test Structure Setup\nGoal: Prepare test directory and configuration\n\nActions:\n- Determine test file location following project conventions\n- Check if conftest.py exists for fixtures\n- Verify pytest and pytest-celery are installed\n- Example: !{bash pip list | grep pytest}\n- Create test directory if needed\n- Example: !{bash mkdir -p tests/tasks 2>/dev/null}\n\nPhase 4: Test Generation\nGoal: Generate comprehensive test cases\n\nActions:\n- For each task found, generate test suite including:\n  - Test successful execution\n  - Test with valid inputs\n  - Test with invalid inputs (validation)\n  - Test retry behavior on failure\n  - Test exception handling\n  - Test task state (PENDING, SUCCESS, FAILURE, RETRY)\n  - Mock external dependencies\n  - Test async behavior if applicable\n- Include pytest fixtures for Celery app and mocked dependencies\n- Add docstrings explaining what each test validates\n- Follow naming convention: test_[task_name]_[scenario]\n\nPhase 5: Test File Creation\nGoal: Write test files to appropriate locations\n\nActions:\n- Write test file for each task module\n- Create conftest.py with shared fixtures if not present\n- Add __init__.py to test directories if needed\n- Include imports for pytest, pytest-celery, and mocks\n- Follow pytest conventions with fixtures and parametrized tests\n- Include test for task success, failures, retries, and validation\n\nPhase 6: Validation\nGoal: Verify tests are syntactically correct\n\nActions:\n- Check Python syntax of generated test files\n- Example: !{bash python -m py_compile tests/test_*.py}\n- Verify pytest can discover tests\n- Example: !{bash pytest --collect-only tests/}\n- Check for missing imports or dependencies\n\nPhase 7: Summary\nGoal: Report what was created and how to run tests\n\nActions:\n- Display generated test files and locations\n- Show test count per task\n- Provide command to run tests:\n  - All tests: pytest tests/\n  - Specific task: pytest tests/test_[task_name].py\n  - With coverage: pytest --cov=tasks tests/\n  - Verbose: pytest -v tests/\n- List test coverage areas:\n  - Success cases\n  - Error handling\n  - Retry logic\n  - Input validation\n- Suggest next steps:\n  - Install pytest-celery if not present\n  - Configure Celery for testing (task_always_eager)\n  - Add CI integration\n  - Generate coverage report"
              }
            ],
            "skills": [
              {
                "name": "beat-scheduling",
                "description": "Periodic task scheduling patterns with Celery Beat (crontab, interval, solar). Use when configuring periodic tasks, setting up task schedules, implementing recurring jobs, configuring django-celery-beat, or creating dynamic schedules.",
                "path": "plugins/celery/skills/beat-scheduling/SKILL.md",
                "frontmatter": {
                  "name": "beat-scheduling",
                  "description": "Periodic task scheduling patterns with Celery Beat (crontab, interval, solar). Use when configuring periodic tasks, setting up task schedules, implementing recurring jobs, configuring django-celery-beat, or creating dynamic schedules.",
                  "allowed-tools": "Bash, Read, Write, Edit, Grep, Glob, WebFetch"
                },
                "content": "# Beat Scheduling Skill\n\nProvides comprehensive patterns and templates for implementing periodic task scheduling with Celery Beat, including crontab, interval, and solar schedules.\n\n## Use When\n\n- Configuring periodic/scheduled tasks in Celery applications\n- Setting up cron-like schedules for recurring jobs\n- Implementing interval-based task execution\n- Configuring solar-based scheduling (sunrise/sunset triggers)\n- Setting up django-celery-beat for database-backed schedules\n- Creating dynamic schedules that can be modified at runtime\n- Migrating from cron to Celery Beat\n- Implementing time-zone aware scheduling\n\n## Core Capabilities\n\n### 1. Crontab Schedule Configuration\n\nGenerate crontab-based schedules with precise timing control:\n\n```python\n# See templates/crontab-schedule.py for complete implementation\nfrom celery.schedules import crontab\n\napp.conf.beat_schedule = {\n    'daily-report': {\n        'task': 'tasks.generate_report',\n        'schedule': crontab(hour=0, minute=0),  # Midnight daily\n    },\n    'business-hours': {\n        'task': 'tasks.process_orders',\n        'schedule': crontab(hour='9-17', minute='*/15', day_of_week='mon-fri'),\n    }\n}\n```\n\n**Key patterns:**\n- Daily, weekly, monthly schedules\n- Business hours execution\n- Complex cron expressions with multiple constraints\n- Timezone-aware scheduling\n\n### 2. Interval Schedule Configuration\n\nSimple interval-based recurring tasks:\n\n```python\n# See templates/interval-schedule.py for complete implementation\nfrom celery.schedules import schedule\n\napp.conf.beat_schedule = {\n    'every-30-seconds': {\n        'task': 'tasks.check_status',\n        'schedule': 30.0,  # Execute every 30 seconds\n    },\n    'every-hour': {\n        'task': 'tasks.cleanup',\n        'schedule': timedelta(hours=1),\n    }\n}\n```\n\n**Key patterns:**\n- Fixed interval execution\n- Relative vs absolute timing\n- Preventing task overlap\n\n### 3. Solar Schedule Configuration\n\nEvent-based scheduling using solar calculations:\n\n```python\n# See templates/solar-schedule.py for complete implementation\nfrom celery.schedules import solar\n\napp.conf.beat_schedule = {\n    'morning-task': {\n        'task': 'tasks.sunrise_routine',\n        'schedule': solar('sunrise', 40.7128, -74.0060),  # NYC coordinates\n    }\n}\n```\n\n**Supported events:** sunrise, sunset, dawn_civil, dusk_astronomical, solar_noon\n\n### 4. Django Celery Beat Integration\n\nDatabase-backed dynamic schedules:\n\n```python\n# See templates/django-celery-beat.py for complete implementation\n# Schedules stored in Django database, editable via Django Admin\nINSTALLED_APPS += ['django_celery_beat']\nCELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'\n```\n\n**Benefits:**\n- Edit schedules without code changes\n- Django Admin interface for schedule management\n- Persistence across deployments\n- Multiple scheduler instances coordination\n\n### 5. Dynamic Schedule Updates\n\nProgrammatic schedule registration:\n\n```python\n# See templates/dynamic-schedules.py for complete implementation\nfrom celery import Celery\n\napp = Celery('tasks')\n\n@app.on_after_configure.connect\ndef setup_periodic_tasks(sender, **kwargs):\n    # Add tasks programmatically\n    sender.add_periodic_task(30.0, check_status.s(), name='status-check')\n    sender.add_periodic_task(\n        crontab(hour=7, minute=30),\n        morning_report.s(),\n        name='morning-report'\n    )\n```\n\n## Template Usage\n\n### Crontab Schedule Template\n**File:** `templates/crontab-schedule.py`\n\nUse for precise timing requirements:\n- Daily/weekly/monthly reports\n- Business hours processing\n- End-of-day batch jobs\n- Time-zone specific execution\n\n### Interval Schedule Template\n**File:** `templates/interval-schedule.py`\n\nUse for fixed interval tasks:\n- Health checks every N seconds\n- Regular cleanup jobs\n- Polling external services\n- Rate-limited API calls\n\n### Solar Schedule Template\n**File:** `templates/solar-schedule.py`\n\nUse for location-based timing:\n- Outdoor equipment control\n- Photography/lighting automation\n- Energy optimization based on daylight\n- Agricultural/environmental monitoring\n\n### Django Celery Beat Template\n**File:** `templates/django-celery-beat.py`\n\nUse for runtime schedule management:\n- Multi-tenant applications with per-tenant schedules\n- User-configurable recurring tasks\n- Dynamic schedule requirements\n- Administrative schedule control\n\n### Dynamic Schedules Template\n**File:** `templates/dynamic-schedules.py`\n\nUse for programmatic configuration:\n- Environment-based schedule setup\n- Plugin/module-based task registration\n- Conditional schedule activation\n- Testing and development schedules\n\n## Script Tools\n\n### validate-schedule.sh\nValidates schedule configuration syntax and structure.\n\n**Usage:**\n```bash\nbash scripts/validate-schedule.sh <config-file>\n```\n\n**Checks:**\n- Valid crontab expressions\n- Proper schedule type usage\n- Timezone configuration\n- Task name references\n- Schedule conflict detection\n\n### test-beat.sh\nTests Celery Beat configuration and execution.\n\n**Usage:**\n```bash\nbash scripts/test-beat.sh <celery-app>\n```\n\n**Tests:**\n- Beat scheduler startup\n- Schedule registration\n- Task execution timing\n- Timezone handling\n\n## Implementation Workflow\n\n### 1. Choose Schedule Type\nDetermine the appropriate scheduling pattern:\n- **Crontab:** Specific times (daily at 3am, weekdays at 9am)\n- **Interval:** Fixed frequency (every 30 seconds, hourly)\n- **Solar:** Sun-based events (sunrise, sunset)\n\n### 2. Select Template\nLoad the appropriate template for your schedule type:\n```bash\n# For crontab schedules\nRead: templates/crontab-schedule.py\n\n# For interval schedules\nRead: templates/interval-schedule.py\n\n# For Django integration\nRead: templates/django-celery-beat.py\n```\n\n### 3. Configure Schedule\nCustomize the template with your task details:\n- Task name and function reference\n- Schedule expression\n- Task arguments and options\n- Timezone settings\n\n### 4. Validate Configuration\nRun validation to catch errors:\n```bash\nbash scripts/validate-schedule.sh celeryconfig.py\n```\n\n### 5. Test Execution\nVerify schedule works as expected:\n```bash\nbash scripts/test-beat.sh myapp\n```\n\n### 6. Deploy Beat Scheduler\nStart Celery Beat in production:\n```bash\ncelery -A myapp beat --loglevel=info\n```\n\n## Best Practices\n\n### Schedule Design\n- Use crontab for time-of-day requirements\n- Use intervals for fixed frequency needs\n- Consider timezone implications for distributed systems\n- Avoid overlapping executions with proper task design\n\n### Production Deployment\n- Run beat scheduler as separate process (not embedded in worker)\n- Use persistent schedule storage (django-celery-beat) for production\n- Monitor beat scheduler health and uptime\n- Implement locking for tasks that shouldn't overlap\n\n### Testing\n- Test schedules with shorter intervals in development\n- Verify timezone handling across environments\n- Test task execution at scheduled times\n- Monitor task queue depth for schedule correctness\n\n### Performance\n- Limit number of scheduled tasks (beat scheduler overhead)\n- Use appropriate schedule precision (avoid unnecessary frequent checks)\n- Consider batch processing vs individual schedules\n- Monitor beat scheduler memory and CPU usage\n\n## Common Patterns\n\n### Daily Reports\n```python\n'daily-report': {\n    'task': 'reports.generate_daily',\n    'schedule': crontab(hour=0, minute=0),\n}\n```\n\n### Business Hours Processing\n```python\n'business-hours-sync': {\n    'task': 'sync.external_api',\n    'schedule': crontab(hour='9-17', minute='*/15', day_of_week='mon-fri'),\n}\n```\n\n### Health Checks\n```python\n'health-check': {\n    'task': 'monitoring.check_services',\n    'schedule': 30.0,  # Every 30 seconds\n}\n```\n\n### Weekend Maintenance\n```python\n'weekend-cleanup': {\n    'task': 'maintenance.cleanup',\n    'schedule': crontab(hour=2, minute=0, day_of_week='sat,sun'),\n}\n```\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n\n## Troubleshooting\n\n### Schedule Not Executing\n- Verify beat scheduler is running\n- Check schedule syntax with validation script\n- Review beat scheduler logs for errors\n- Confirm task name matches actual task\n\n### Timezone Issues\n- Set explicit timezone: `app.conf.timezone = 'UTC'`\n- Use timezone-aware datetime objects\n- Test schedule across timezone boundaries\n- Review Celery timezone documentation\n\n### Task Overlap\n- Implement task locking (Redis/database)\n- Use `expires` option to prevent old task execution\n- Monitor task execution duration\n- Adjust schedule frequency if needed\n\n## Examples\n\nSee `examples/` directory for detailed implementation examples:\n- `crontab-examples.md` - Comprehensive crontab schedule patterns\n- `interval-examples.md` - Interval schedule use cases\n- `django-celery-beat-setup.md` - Complete Django integration guide\n\n## References\n\n- **Celery Documentation:** https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html\n- **Django Celery Beat:** https://django-celery-beat.readthedocs.io/\n- **Crontab Reference:** https://crontab.guru/\n- **Solar Events:** https://docs.celeryq.dev/en/stable/reference/celery.schedules.html#celery.schedules.solar"
              },
              {
                "name": "broker-configurations",
                "description": "Message broker setup patterns (Redis, RabbitMQ, SQS) for Celery including connection strings, SSL configuration, high availability, and production best practices. Use when configuring message brokers, setting up Redis/RabbitMQ/SQS, troubleshooting broker connections, implementing HA/failover, securing broker communications with SSL, or when user mentions broker setup, connection issues, sentinel, quorum queues, or AWS SQS integration.",
                "path": "plugins/celery/skills/broker-configurations/SKILL.md",
                "frontmatter": {
                  "name": "broker-configurations",
                  "description": "Message broker setup patterns (Redis, RabbitMQ, SQS) for Celery including connection strings, SSL configuration, high availability, and production best practices. Use when configuring message brokers, setting up Redis/RabbitMQ/SQS, troubleshooting broker connections, implementing HA/failover, securing broker communications with SSL, or when user mentions broker setup, connection issues, sentinel, quorum queues, or AWS SQS integration.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# Broker Configurations\n\n**Purpose:** Comprehensive message broker configuration for Celery with production-ready patterns for Redis, RabbitMQ, and Amazon SQS.\n\n**Activation Triggers:**\n- Broker connection errors\n- Setting up new Celery project\n- Implementing high availability\n- SSL/TLS configuration needed\n- Performance tuning required\n- Multi-broker comparison\n- Cloud deployment (AWS/GCP/Azure)\n- Sentinel or cluster setup\n\n**Key Resources:**\n- `templates/redis-config.py` - Production Redis configuration\n- `templates/rabbitmq-config.py` - RabbitMQ with quorum queues\n- `templates/sqs-config.py` - AWS SQS with IAM roles\n- `templates/connection-strings.env` - Connection string formats\n- `templates/ssl-config.py` - SSL/TLS configuration\n- `scripts/test-broker-connection.sh` - Connection testing\n- `scripts/setup-redis.sh` - Redis installation and configuration\n- `scripts/setup-rabbitmq.sh` - RabbitMQ cluster setup\n- `examples/redis-sentinel.md` - High availability with Sentinel\n- `examples/rabbitmq-ha.md` - RabbitMQ clustering and quorum queues\n- `examples/sqs-setup.md` - Complete AWS SQS integration\n\n## Broker Selection Guide\n\n### Quick Comparison\n\n| Feature | Redis | RabbitMQ | SQS |\n|---------|-------|----------|-----|\n| **Performance** | Excellent (small msgs) | Very Good | Good |\n| **Reliability** | Good (with Sentinel) | Excellent (quorum) | Excellent |\n| **Monitoring** | Yes | Yes | Limited |\n| **Remote Control** | Yes | Yes | No |\n| **Management** | Manual/Cloud | Manual | Fully Managed |\n| **Cost** | Server/Cloud | Server/Cloud | Pay-per-use |\n| **Best For** | Speed, simple setup | Reliability, features | AWS, serverless |\n\n### Decision Matrix\n\n**Choose Redis when:**\n- Need maximum speed for small messages\n- Want simple setup and maintenance\n- Already using Redis for caching/results\n- Budget for managed Redis (Upstash, ElastiCache)\n- Can accept brief downtime during failover\n\n**Choose RabbitMQ when:**\n- Need guaranteed message delivery\n- Require complex routing patterns\n- Need worker remote control features\n- Want built-in monitoring and management UI\n- Have dedicated ops team for management\n\n**Choose SQS when:**\n- Running on AWS infrastructure\n- Want zero operational overhead\n- Need unlimited automatic scaling\n- Prefer pay-per-use pricing\n- Don't need worker remote control\n\n## Redis Broker Setup\n\n### 1. Basic Configuration\n\n```bash\n# Install dependencies\npip install \"celery[redis]\"\n\n# Use template\ncp templates/redis-config.py celeryconfig.py\n\n# Configure environment\ncp templates/connection-strings.env .env\n# Edit .env with actual values\n```\n\n**Key settings:**\n```python\n# templates/redis-config.py\nbroker_url = 'redis://:password@localhost:6379/0'\n\nbroker_transport_options = {\n    'visibility_timeout': 3600,\n    'retry_on_timeout': True,\n    'max_connections': 50,\n}\n```\n\n### 2. Production Settings\n\n**CRITICAL:** Set `maxmemory-policy noeviction` in Redis config:\n```bash\n# Run setup script\n./scripts/setup-redis.sh --install --configure\n\n# Or manually\nredis-cli CONFIG SET maxmemory-policy noeviction\n```\n\n**Why:** Prevents Redis from evicting task data, which would cause task loss.\n\n### 3. High Availability with Sentinel\n\n```bash\n# See comprehensive guide\ncat examples/redis-sentinel.md\n\n# Quick setup\ndocker-compose -f examples/docker-compose.sentinel.yml up -d\n\n# Configure Celery for Sentinel\nCELERY_BROKER_URL='sentinel://host1:26379;host2:26379;host3:26379/0'\n```\n\n**Connection string format:**\n```python\n# Sentinel URL\nbroker_url = 'sentinel://sentinel1:26379;sentinel2:26379/0'\n\nbroker_transport_options = {\n    'master_name': 'mymaster',\n    'sentinel_kwargs': {'password': 'sentinel_password'},\n}\n```\n\n## RabbitMQ Broker Setup\n\n### 1. Basic Configuration\n\n```bash\n# Install dependencies\npip install \"celery[amqp]\"\n\n# Use template\ncp templates/rabbitmq-config.py celeryconfig.py\n\n# Run setup script\n./scripts/setup-rabbitmq.sh --install --configure\n```\n\n**Key settings:**\n```python\n# templates/rabbitmq-config.py\nbroker_url = 'amqp://user:password@localhost:5672/vhost'\n\n# CRITICAL for quorum queues\nbroker_transport_options = {\n    'confirm_publish': True,  # Required!\n}\n```\n\n### 2. Quorum Queues (Recommended)\n\n**Provides:**\n- Automatic replication\n- Leader election on failure\n- No message loss\n\n```python\nfrom kombu import Queue\n\ntask_queues = (\n    Queue(\n        'default',\n        queue_arguments={\n            'x-queue-type': 'quorum',\n            'x-delivery-limit': 3,\n        }\n    ),\n)\n```\n\n### 3. High Availability Cluster\n\n```bash\n# See comprehensive guide\ncat examples/rabbitmq-ha.md\n\n# Docker cluster setup\ndocker-compose -f examples/docker-compose.rabbitmq-cluster.yml up -d\n\n# Verify cluster\ndocker exec rabbitmq-1 rabbitmqctl cluster_status\n```\n\n**HAProxy load balancing:**\n```yaml\n# Distribute connections across cluster\nbroker_url = 'amqp://user:password@haproxy:5670/vhost'\n```\n\n## AWS SQS Broker Setup\n\n### 1. IAM Configuration\n\n```bash\n# Create IAM policy and user\naws iam create-policy \\\n  --policy-name CelerySQSPolicy \\\n  --policy-document file://examples/celery-sqs-policy.json\n\n# Or use IAM role (recommended for EC2/ECS)\n# See examples/sqs-setup.md for complete guide\n```\n\n### 2. Basic Configuration\n\n```bash\n# Install dependencies\npip install \"celery[sqs]\"\n\n# Use template\ncp templates/sqs-config.py celeryconfig.py\n```\n\n**With IAM role (recommended):**\n```python\n# No credentials needed\nbroker_url = 'sqs://'\n\nbroker_transport_options = {\n    'region': 'us-east-1',\n    'visibility_timeout': 3600,\n    'polling_interval': 1,\n    'wait_time_seconds': 10,  # Long polling\n}\n```\n\n**With explicit credentials:**\n```python\nbroker_url = 'sqs://access_key:secret_key@'\n```\n\n### 3. FIFO Queues\n\n**For ordered task processing:**\n```python\ntask_queues = (\n    Queue(\n        'celery-default.fifo',\n        queue_arguments={\n            'FifoQueue': 'true',\n            'ContentBasedDeduplication': 'true',\n        }\n    ),\n)\n\n# Send with message group ID\ntask.apply_async(\n    args=[data],\n    properties={'MessageGroupId': 'user-123'}\n)\n```\n\n### 4. Result Backend\n\n**SQS doesn't support results - use S3 or DynamoDB:**\n```python\n# Option 1: S3\nresult_backend = 's3://my-bucket/celery-results/'\n\n# Option 2: DynamoDB\nresult_backend = 'dynamodb://'\nresult_backend_transport_options = {\n    'table_name': 'celery-results',\n}\n\n# Option 3: Redis (hybrid)\nresult_backend = 'redis://redis.example.com:6379/0'\n```\n\n## SSL/TLS Configuration\n\n### 1. Redis with SSL\n\n```python\n# Use templates/ssl-config.py\nfrom templates.ssl_config import app\n\n# Or manually\nbroker_url = 'rediss://password@host:6380/0'  # Note: rediss://\n\nbroker_use_ssl = {\n    'ssl_cert_reqs': ssl.CERT_REQUIRED,\n    'ssl_ca_certs': '/path/to/ca.pem',\n    'ssl_certfile': '/path/to/client-cert.pem',\n    'ssl_keyfile': '/path/to/client-key.pem',\n}\n```\n\n### 2. RabbitMQ with SSL\n\n```python\nbroker_url = 'amqps://user:password@host:5671/vhost'  # Note: amqps://\n\nbroker_use_ssl = {\n    'ssl_cert_reqs': ssl.CERT_REQUIRED,\n    'ssl_ca_certs': '/path/to/ca.pem',\n    'ssl_certfile': '/path/to/client-cert.pem',\n    'ssl_keyfile': '/path/to/client-key.pem',\n}\n```\n\n### 3. Environment Variables\n\n```bash\n# .env\nBROKER_SSL_ENABLED=true\nBROKER_SSL_CERT=/path/to/client-cert.pem\nBROKER_SSL_KEY=/path/to/client-key.pem\nBROKER_SSL_CA=/path/to/ca-cert.pem\nBROKER_SSL_VERIFY_MODE=CERT_REQUIRED\n```\n\n## Testing and Validation\n\n### 1. Test Connection\n\n```bash\n# Test any broker type\n./scripts/test-broker-connection.sh redis\n./scripts/test-broker-connection.sh rabbitmq\n./scripts/test-broker-connection.sh sqs\n\n# Check specific features\n./scripts/test-broker-connection.sh redis --ssl\n```\n\n**Script checks:**\n- Broker connectivity\n- Authentication\n- SSL/TLS validation\n- Configuration correctness\n- Performance baseline\n\n### 2. Python Test\n\n```python\nfrom celery import Celery\n\napp = Celery(broker='redis://localhost:6379/0')\n\n# Test connection\ntry:\n    with app.connection() as conn:\n        conn.ensure_connection(max_retries=3, timeout=5)\n        print(\"‚úÖ Connection successful\")\nexcept Exception as e:\n    print(f\"‚ùå Connection failed: {e}\")\n```\n\n## Common Issues and Solutions\n\n### Redis: \"maxmemory-policy not noeviction\"\n\n**Problem:** Redis evicting task data\n**Solution:**\n```bash\n./scripts/setup-redis.sh --configure\n# Or manually:\nredis-cli CONFIG SET maxmemory-policy noeviction\n```\n\n### RabbitMQ: \"Basic.publish: NOT_FOUND\"\n\n**Problem:** Queue doesn't exist or wrong vhost\n**Solution:**\n```bash\n# Check vhost\nrabbitmqctl list_vhosts\n\n# Check permissions\nrabbitmqctl list_permissions -p /celery_vhost\n```\n\n### SQS: \"Access Denied\"\n\n**Problem:** IAM permissions insufficient\n**Solution:**\n```bash\n# Verify IAM policy includes all required actions\n# See examples/sqs-setup.md for complete policy\n\n# Test credentials\naws sts get-caller-identity\naws sqs list-queues --queue-name-prefix celery-\n```\n\n### Connection Timeout\n\n**Problem:** Network/firewall blocking connection\n**Solution:**\n```bash\n# Test network connectivity\ntelnet broker-host 6379  # Redis\ntelnet broker-host 5672  # RabbitMQ\n\n# Check firewall rules\n# Verify security groups (AWS)\n# Check iptables rules\n```\n\n## Performance Tuning\n\n### Redis Optimization\n\n```python\nbroker_transport_options = {\n    'visibility_timeout': 3600,\n    'max_connections': 100,  # Increase for high concurrency\n    'socket_timeout': 5.0,\n    'socket_keepalive': True,\n    'health_check_interval': 30,\n}\n\n# Worker settings\nworker_prefetch_multiplier = 4  # Prefetch 4x concurrency\nworker_max_tasks_per_child = 1000\n```\n\n### RabbitMQ Optimization\n\n```python\nbroker_transport_options = {\n    'confirm_publish': True,\n    'max_retries': 3,\n}\n\n# Use quorum queues for reliability\n# Adjust prefetch for throughput\nworker_prefetch_multiplier = 4\n\n# Disable QoS for maximum speed (classic queues only)\n# Note: Not compatible with worker autoscaling\n```\n\n### SQS Optimization\n\n```python\n# Reduce API calls (costs)\nbroker_transport_options = {\n    'polling_interval': 5,  # Poll less frequently\n    'wait_time_seconds': 20,  # Max long polling\n}\n\n# CRITICAL: Prevent visibility timeout\nworker_prefetch_multiplier = 1  # Must be 1 for SQS\n```\n\n## Monitoring\n\n### Key Metrics\n\n**Redis:**\n- Connection count: `INFO clients`\n- Memory usage: `INFO memory`\n- Key count: `DBSIZE`\n- Commands/sec: `INFO stats`\n\n**RabbitMQ:**\n- Queue length: `rabbitmqctl list_queues`\n- Consumer count: Check management UI\n- Memory usage: `rabbitmqctl status`\n- Message rate: Check management UI\n\n**SQS:**\n- `ApproximateNumberOfMessagesVisible`: Queue backlog\n- `NumberOfMessagesSent`: Task creation rate\n- `NumberOfMessagesReceived`: Task completion rate\n- `NumberOfEmptyReceives`: Polling efficiency\n\n### Health Check Script\n\n```bash\n#!/bin/bash\n# health-check.sh\n\nBROKER_TYPE=\"${1:-redis}\"\n\ncase \"$BROKER_TYPE\" in\n  redis)\n    redis-cli PING || exit 1\n    ;;\n  rabbitmq)\n    rabbitmqctl status || exit 1\n    ;;\n  sqs)\n    aws sqs list-queues --queue-name-prefix celery- || exit 1\n    ;;\nesac\n\necho \"‚úÖ Broker healthy\"\n```\n\n## Security Best Practices\n\n### 1. Authentication\n\n- **Redis:** Always set `requirepass`\n- **RabbitMQ:** Use strong passwords, dedicated vhosts\n- **SQS:** Use IAM roles, not access keys\n\n### 2. Network Security\n\n- Use SSL/TLS for production\n- Configure firewalls/security groups\n- Use VPC/private networks when possible\n- Restrict broker access to worker nodes only\n\n### 3. Access Control\n\n- Principle of least privilege\n- Separate credentials per environment\n- Rotate passwords regularly\n- Use secret management (Vault, Doppler, AWS Secrets Manager)\n\n### 4. Monitoring\n\n- Enable audit logging\n- Monitor authentication failures\n- Alert on configuration changes\n- Track connection patterns\n\n## Resources\n\n**Templates:**\n- `redis-config.py` - Production Redis configuration\n- `rabbitmq-config.py` - RabbitMQ with quorum queues\n- `sqs-config.py` - AWS SQS with IAM\n- `connection-strings.env` - All connection formats\n- `ssl-config.py` - SSL/TLS configuration\n\n**Scripts:**\n- `test-broker-connection.sh` - Test any broker\n- `setup-redis.sh` - Redis installation and config\n- `setup-rabbitmq.sh` - RabbitMQ cluster setup\n\n**Examples:**\n- `redis-sentinel.md` - High availability setup\n- `rabbitmq-ha.md` - Clustering and quorum queues\n- `sqs-setup.md` - Complete AWS integration\n\n**Documentation:**\n- Redis: https://redis.io/docs/\n- RabbitMQ: https://www.rabbitmq.com/documentation.html\n- SQS: https://docs.aws.amazon.com/sqs/\n- Celery Brokers: https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/\n\n---\n\n**Version:** 1.0.0\n**Celery Compatibility:** 5.0+\n**Supported Brokers:** Redis 6+, RabbitMQ 3.8+, Amazon SQS\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "celery-config-patterns",
                "description": "Celery configuration templates for all frameworks (Django, Flask, FastAPI, standalone). Use when configuring Celery, setting up task queues, creating Celery apps, integrating with frameworks, or when user mentions Celery configuration, task queue setup, broker configuration, or framework integration.",
                "path": "plugins/celery/skills/celery-config-patterns/SKILL.md",
                "frontmatter": {
                  "name": "celery-config-patterns",
                  "description": "Celery configuration templates for all frameworks (Django, Flask, FastAPI, standalone). Use when configuring Celery, setting up task queues, creating Celery apps, integrating with frameworks, or when user mentions Celery configuration, task queue setup, broker configuration, or framework integration.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep"
                },
                "content": "# celery-config-patterns\n\nProvides production-ready Celery configuration templates for all major Python frameworks (Django, Flask, FastAPI, standalone) with complete broker setup (Redis, RabbitMQ), security best practices, and framework-specific integration patterns.\n\n## Use When\n\n- Configuring Celery for the first time in any Python project\n- Integrating Celery with Django, Flask, FastAPI, or standalone applications\n- Setting up Redis or RabbitMQ as message broker\n- Creating production-ready Celery configurations\n- Implementing task routing and queue configurations\n- Setting up monitoring and logging for Celery\n- Migrating Celery configurations between frameworks\n\n## Directory Structure\n\n```\ncelery-config-patterns/\n‚îú‚îÄ‚îÄ SKILL.md                          # This file\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ validate-config.sh            # Validate Celery configuration\n‚îÇ   ‚îú‚îÄ‚îÄ detect-framework.sh           # Auto-detect Python framework\n‚îÇ   ‚îú‚îÄ‚îÄ generate-config.sh            # Generate framework-specific config\n‚îÇ   ‚îî‚îÄ‚îÄ test-broker-connection.sh     # Test broker connectivity\n‚îú‚îÄ‚îÄ templates/\n‚îÇ   ‚îú‚îÄ‚îÄ celery-app-standalone.py      # Standalone Celery app\n‚îÇ   ‚îú‚îÄ‚îÄ celery-app-django.py          # Django Celery integration\n‚îÇ   ‚îú‚îÄ‚îÄ celery-app-flask.py           # Flask Celery integration\n‚îÇ   ‚îú‚îÄ‚îÄ celery-app-fastapi.py         # FastAPI Celery integration\n‚îÇ   ‚îú‚îÄ‚îÄ config-redis.py               # Redis broker configuration\n‚îÇ   ‚îú‚îÄ‚îÄ config-rabbitmq.py            # RabbitMQ broker configuration\n‚îÇ   ‚îú‚îÄ‚îÄ tasks-example.py              # Sample task definitions\n‚îÇ   ‚îî‚îÄ‚îÄ beat-schedule.py              # Celery Beat schedule config\n‚îî‚îÄ‚îÄ examples/\n    ‚îú‚îÄ‚îÄ django-setup.md               # Complete Django setup guide\n    ‚îú‚îÄ‚îÄ flask-setup.md                # Complete Flask setup guide\n    ‚îú‚îÄ‚îÄ fastapi-setup.md              # Complete FastAPI setup guide\n    ‚îî‚îÄ‚îÄ standalone-setup.md           # Standalone Python setup guide\n```\n\n## Instructions\n\n### Step 1: Detect Framework\n\nUse the detection script to identify the Python framework:\n\n```bash\nbash scripts/detect-framework.sh\n```\n\n**Detects:**\n- Django (looks for `manage.py`, `settings.py`)\n- Flask (looks for Flask imports, `app.py`)\n- FastAPI (looks for FastAPI imports, `main.py`)\n- Standalone (no framework detected)\n\n### Step 2: Select Template\n\nBased on framework detection, choose the appropriate template:\n\n| Framework | Template | Integration File |\n|-----------|----------|------------------|\n| Django | `celery-app-django.py` | `projectname/celery.py` |\n| Flask | `celery-app-flask.py` | `app/celery.py` or `celery.py` |\n| FastAPI | `celery-app-fastapi.py` | `app/celery.py` or `celery.py` |\n| Standalone | `celery-app-standalone.py` | `celery_app.py` |\n\n### Step 3: Configure Broker\n\nChoose broker configuration template:\n\n**Redis** (Recommended for simplicity):\n```python\n# Use templates/config-redis.py\nCELERY_BROKER_URL = 'redis://localhost:6379/0'\nCELERY_RESULT_BACKEND = 'redis://localhost:6379/0'\n```\n\n**RabbitMQ** (Recommended for production):\n```python\n# Use templates/config-rabbitmq.py\nCELERY_BROKER_URL = 'amqp://guest:guest@localhost:5672//'\nCELERY_RESULT_BACKEND = 'rpc://'\n```\n\n### Step 4: Generate Configuration\n\nUse the generation script:\n\n```bash\nbash scripts/generate-config.sh --framework=django --broker=redis\nbash scripts/generate-config.sh --framework=flask --broker=rabbitmq\nbash scripts/generate-config.sh --framework=fastapi --broker=redis\n```\n\n**Script will:**\n1. Copy appropriate framework template\n2. Apply broker configuration\n3. Create environment file with placeholders\n4. Add to .gitignore if needed\n5. Generate setup documentation\n\n### Step 5: Validate Configuration\n\nRun validation script before starting Celery:\n\n```bash\nbash scripts/validate-config.sh\n```\n\n**Validates:**\n- Celery app exists and is importable\n- Broker connection is valid\n- Configuration syntax is correct\n- Required environment variables are set\n- Task discovery paths are correct\n\n### Step 6: Test Broker Connection\n\nVerify broker connectivity:\n\n```bash\nbash scripts/test-broker-connection.sh\n```\n\n**Tests:**\n- Broker URL is reachable\n- Authentication credentials are valid\n- Connection pool can be established\n- Basic message routing works\n\n## Template Descriptions\n\n### celery-app-standalone.py\n\nStandalone Celery application without web framework integration.\n\n**Features:**\n- Basic Celery app configuration\n- Task autodiscovery from tasks.py\n- Configurable broker and backend\n- Logging setup\n- Beat schedule integration\n\n**Usage:**\n```python\n# celery_app.py\nfrom celery import Celery\n\napp = Celery('myapp')\napp.config_from_object('celeryconfig')\n\n@app.task\ndef add(x, y):\n    return x + y\n```\n\n### celery-app-django.py\n\nDjango-specific Celery configuration using Django settings.\n\n**Features:**\n- Reads configuration from Django settings\n- Auto-discovers tasks in all installed apps\n- Uses Django's database for result backend (optional)\n- Integrates with Django logging\n- Supports Django-celery-beat for periodic tasks\n\n**Usage:**\n```python\n# myproject/celery.py\nimport os\nfrom celery import Celery\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings')\napp = Celery('myproject')\napp.config_from_object('django.conf:settings', namespace='CELERY')\napp.autodiscover_tasks()\n```\n\n### celery-app-flask.py\n\nFlask-specific Celery configuration with Flask app context.\n\n**Features:**\n- Flask app factory pattern support\n- Celery app context integration\n- Uses Flask configuration\n- Compatible with Flask-Celery extensions\n- Request context handling for tasks\n\n**Usage:**\n```python\n# celery.py\nfrom celery import Celery\n\ndef make_celery(app):\n    celery = Celery(app.import_name)\n    celery.conf.update(app.config)\n    return celery\n\n# app.py\ncelery = make_celery(app)\n```\n\n### celery-app-fastapi.py\n\nFastAPI-specific Celery configuration with async support.\n\n**Features:**\n- FastAPI lifespan event integration\n- Async task support\n- Pydantic model validation in tasks\n- Background task coordination\n- API endpoint integration examples\n\n**Usage:**\n```python\n# celery.py\nfrom celery import Celery\n\ncelery_app = Celery('fastapi_app')\ncelery_app.config_from_object('celeryconfig')\n\n# main.py\nfrom fastapi import FastAPI\nfrom celery_app import celery_app\n```\n\n### config-redis.py\n\nRedis broker and result backend configuration.\n\n**Features:**\n- Connection pooling\n- SSL/TLS support\n- Sentinel configuration\n- Health checks\n- Connection retry logic\n\n**Configuration:**\n```python\nCELERY_BROKER_URL = 'redis://localhost:6379/0'\nCELERY_RESULT_BACKEND = 'redis://localhost:6379/0'\nCELERY_BROKER_CONNECTION_RETRY_ON_STARTUP = True\nCELERY_REDIS_MAX_CONNECTIONS = 50\n```\n\n### config-rabbitmq.py\n\nRabbitMQ broker configuration with advanced routing.\n\n**Features:**\n- Virtual host configuration\n- Exchange and queue declarations\n- Routing keys and bindings\n- Dead letter queues\n- Priority queues\n\n**Configuration:**\n```python\nCELERY_BROKER_URL = 'amqp://user:pass@localhost:5672//'\nCELERY_RESULT_BACKEND = 'rpc://'\nCELERY_TASK_ROUTES = {\n    'app.tasks.critical': {'queue': 'critical'},\n    'app.tasks.normal': {'queue': 'default'},\n}\n```\n\n### tasks-example.py\n\nExample task definitions with best practices.\n\n**Features:**\n- Basic task examples\n- Task with retry logic\n- Task with rate limiting\n- Task with time limits\n- Task with custom routing\n- Task with result expiration\n\n**Examples:**\n```python\n@app.task(bind=True, max_retries=3)\ndef process_data(self, data):\n    try:\n        # Process data\n        return result\n    except Exception as exc:\n        raise self.retry(exc=exc, countdown=60)\n\n@app.task(rate_limit='10/m')\ndef send_email(to, subject, body):\n    # Send email\n    pass\n```\n\n### beat-schedule.py\n\nCelery Beat periodic task scheduling.\n\n**Features:**\n- Crontab schedules\n- Interval schedules\n- Solar schedules\n- Task arguments and kwargs\n- Timezone support\n\n**Examples:**\n```python\nCELERY_BEAT_SCHEDULE = {\n    'cleanup-every-midnight': {\n        'task': 'app.tasks.cleanup',\n        'schedule': crontab(hour=0, minute=0),\n    },\n    'report-every-monday': {\n        'task': 'app.tasks.weekly_report',\n        'schedule': crontab(day_of_week=1, hour=9),\n    },\n}\n```\n\n## Script Usage\n\n### validate-config.sh\n\nValidates Celery configuration for errors.\n\n**Usage:**\n```bash\nbash scripts/validate-config.sh\nbash scripts/validate-config.sh --config=celeryconfig.py\nbash scripts/validate-config.sh --app=myapp.celery:app\n```\n\n**Checks:**\n- Python syntax is valid\n- Celery app is importable\n- Broker URL format is correct\n- Required settings are present\n- Task modules can be discovered\n- No conflicting configurations\n\n**Exit Codes:**\n- 0: Configuration is valid\n- 1: Validation errors found\n- 2: Import errors\n\n### detect-framework.sh\n\nDetects Python web framework in current project.\n\n**Usage:**\n```bash\nbash scripts/detect-framework.sh\nbash scripts/detect-framework.sh /path/to/project\n```\n\n**Output:**\n```json\n{\n  \"framework\": \"django\",\n  \"version\": \"4.2.0\",\n  \"celery_location\": \"myproject/celery.py\",\n  \"settings_location\": \"myproject/settings.py\"\n}\n```\n\n**Detection Logic:**\n1. Django: Check for `manage.py` and `settings.py`\n2. Flask: Check for Flask imports and `app.py`\n3. FastAPI: Check for FastAPI imports and `main.py`\n4. Standalone: No framework files detected\n\n### generate-config.sh\n\nGenerates framework-specific Celery configuration.\n\n**Usage:**\n```bash\nbash scripts/generate-config.sh --framework=django --broker=redis\nbash scripts/generate-config.sh --framework=flask --broker=rabbitmq --output=celery_config.py\n```\n\n**Options:**\n- `--framework`: django, flask, fastapi, standalone\n- `--broker`: redis, rabbitmq\n- `--output`: Custom output file path\n- `--with-beat`: Include Celery Beat configuration\n- `--with-monitoring`: Include monitoring configuration\n\n**Generated Files:**\n- Celery app configuration\n- `.env.example` with broker credentials\n- Setup documentation\n- Example tasks file\n\n### test-broker-connection.sh\n\nTests message broker connectivity.\n\n**Usage:**\n```bash\nbash scripts/test-broker-connection.sh\nbash scripts/test-broker-connection.sh redis://localhost:6379/0\nbash scripts/test-broker-connection.sh amqp://guest:guest@localhost:5672//\n```\n\n**Tests:**\n1. DNS resolution of broker host\n2. Port connectivity\n3. Authentication\n4. Basic message publish/consume\n5. Connection pooling\n\n**Output:**\n```\n‚úì Broker host is reachable\n‚úì Port 6379 is open\n‚úì Authentication successful\n‚úì Message publish successful\n‚úì Message consume successful\n‚úì Connection pool working\n\nAll tests passed!\n```\n\n## Examples\n\n### Django Setup Example\n\nSee `examples/django-setup.md` for complete walkthrough:\n\n1. Install dependencies\n2. Create Celery app in `myproject/celery.py`\n3. Configure in `settings.py`\n4. Create tasks in app directories\n5. Run worker and beat\n\n### Flask Setup Example\n\nSee `examples/flask-setup.md` for complete walkthrough:\n\n1. Install dependencies\n2. Create Celery factory function\n3. Initialize with Flask app\n4. Define tasks\n5. Run worker\n\n### FastAPI Setup Example\n\nSee `examples/fastapi-setup.md` for complete walkthrough:\n\n1. Install dependencies\n2. Create Celery app\n3. Integrate with FastAPI lifespan\n4. Create async tasks\n5. Run worker alongside FastAPI\n\n### Standalone Setup Example\n\nSee `examples/standalone-setup.md` for complete walkthrough:\n\n1. Install Celery and broker\n2. Create celery_app.py\n3. Create celeryconfig.py\n4. Define tasks\n5. Run worker\n\n## Security Requirements\n\nAll configuration templates and examples in this skill follow strict security rules:\n\n**Environment Variables:**\n- Broker credentials use placeholders: `your_redis_password_here`\n- Connection strings never include real passwords\n- All sensitive values read from environment\n\n**Example .env.example:**\n```bash\n# Redis Configuration\nCELERY_BROKER_URL=redis://:your_redis_password_here@localhost:6379/0\nCELERY_RESULT_BACKEND=redis://:your_redis_password_here@localhost:6379/0\n\n# RabbitMQ Configuration\nCELERY_BROKER_URL=amqp://your_rabbitmq_user:your_rabbitmq_password@localhost:5672//\n```\n\n**Code Examples:**\n```python\n# ALWAYS read from environment\nimport os\nCELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')\n```\n\n**Security Checklist:**\n- [ ] No hardcoded broker passwords\n- [ ] No hardcoded database credentials\n- [ ] All examples use environment variables\n- [ ] .env added to .gitignore\n- [ ] .env.example provided with placeholders\n- [ ] Documentation explains credential management\n\n## Configuration Best Practices\n\n**Production Settings:**\n```python\n# Task execution\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_ACCEPT_CONTENT = ['json']\nCELERY_TIMEZONE = 'UTC'\nCELERY_ENABLE_UTC = True\n\n# Performance\nCELERY_WORKER_PREFETCH_MULTIPLIER = 4\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 1000\nCELERY_BROKER_POOL_LIMIT = 10\n\n# Reliability\nCELERY_TASK_ACKS_LATE = True\nCELERY_TASK_REJECT_ON_WORKER_LOST = True\nCELERY_TASK_TIME_LIMIT = 300\nCELERY_TASK_SOFT_TIME_LIMIT = 240\n```\n\n**Development Settings:**\n```python\n# Easier debugging\nCELERY_TASK_ALWAYS_EAGER = True\nCELERY_TASK_EAGER_PROPAGATES = True\n```\n\n## Troubleshooting\n\n**Broker Connection Failed:**\n```bash\n# Test broker connectivity\nbash scripts/test-broker-connection.sh\n\n# Check broker is running\n# Redis: redis-cli ping\n# RabbitMQ: rabbitmqctl status\n```\n\n**Tasks Not Discovered:**\n```bash\n# Validate configuration\nbash scripts/validate-config.sh\n\n# Check task module paths\ncelery -A myapp inspect registered\n```\n\n**Import Errors:**\n```bash\n# Verify Python path includes project\nexport PYTHONPATH=\"${PYTHONPATH}:/path/to/project\"\n\n# Check Celery app is importable\npython -c \"from myapp.celery import app; print(app)\"\n```\n\n## Requirements\n\n**Python Packages:**\n- celery>=5.3.0\n- redis>=4.5.0 (for Redis broker)\n- kombu>=5.3.0 (for RabbitMQ broker)\n\n**Framework-Specific:**\n- Django: django-celery-beat, django-celery-results\n- Flask: Flask-Celery\n- FastAPI: No additional packages\n\n**Broker Requirements:**\n- Redis: redis-server running on port 6379\n- RabbitMQ: rabbitmq-server running on port 5672\n\n**System Tools:**\n- bash (for scripts)\n- python3 (3.8+)\n- Access to broker (Redis/RabbitMQ)\n\n---\n\n**Skill Version:** 1.0.0\n**Last Updated:** 2025-11-16"
              },
              {
                "name": "deployment-configs",
                "description": "Production deployment configurations for Celery workers and beat schedulers across Docker, Kubernetes, and systemd environments. Use when deploying Celery to production, containerizing workers, orchestrating with Kubernetes, setting up systemd services, configuring health checks, implementing graceful shutdowns, or when user mentions deployment, Docker, Kubernetes, systemd, production setup, or worker containerization.",
                "path": "plugins/celery/skills/deployment-configs/SKILL.md",
                "frontmatter": {
                  "name": "deployment-configs",
                  "description": "Production deployment configurations for Celery workers and beat schedulers across Docker, Kubernetes, and systemd environments. Use when deploying Celery to production, containerizing workers, orchestrating with Kubernetes, setting up systemd services, configuring health checks, implementing graceful shutdowns, or when user mentions deployment, Docker, Kubernetes, systemd, production setup, or worker containerization.",
                  "allowed-tools": "Read, Write, Edit, Bash, Glob, Grep"
                },
                "content": "# Deployment Configurations\n\n**Purpose:** Generate production-ready deployment configurations for Celery workers and beat schedulers across multiple deployment platforms.\n\n**Activation Triggers:**\n- Production deployment setup\n- Docker/containerization requirements\n- Kubernetes orchestration needs\n- Systemd service configuration\n- Health check implementation\n- Graceful shutdown requirements\n- Multi-worker scaling\n- Environment-specific configuration\n\n**Key Resources:**\n- `scripts/deploy.sh` - Complete deployment orchestration\n- `scripts/test-deployment.sh` - Validate deployment health\n- `scripts/health-check.sh` - Comprehensive health verification\n- `templates/docker-compose.yml` - Full Docker stack\n- `templates/Dockerfile.worker` - Optimized worker container\n- `templates/kubernetes/` - K8s manifests for workers and beat\n- `templates/systemd/` - Systemd service units\n- `templates/health-checks.py` - Python health check implementation\n- `examples/` - Complete deployment scenarios\n\n## Deployment Platforms\n\n### Docker Compose (Development/Staging)\n\n**Use Case:** Local development, staging environments, simple production setups\n\n**Configuration:** `templates/docker-compose.yml`\n\n**Services Included:**\n- Redis (broker)\n- PostgreSQL (result backend)\n- Celery worker(s)\n- Celery beat scheduler\n- Flower monitoring\n- Health check sidecar\n\n**Quick Start:**\n```bash\n# Generate Docker configuration\n./scripts/deploy.sh docker --env=staging\n\n# Start all services\ndocker-compose up -d\n\n# Scale workers\ndocker-compose up -d --scale celery-worker=4\n\n# View logs\ndocker-compose logs -f celery-worker\n```\n\n**Key Features:**\n- Multi-worker support with easy scaling\n- Volume mounts for code hot-reload\n- Environment-specific configurations\n- Health checks with restart policies\n- Networking between services\n- Persistent data volumes\n\n### Kubernetes (Production)\n\n**Use Case:** Production environments requiring orchestration, auto-scaling, and high availability\n\n**Manifests:** `templates/kubernetes/`\n\n**Resources:**\n- `celery-worker.yaml` - Worker Deployment with HPA\n- `celery-beat.yaml` - Beat StatefulSet (singleton)\n- `celery-configmap.yaml` - Environment configuration\n- `celery-secrets.yaml` - Sensitive credentials\n- `celery-service.yaml` - Internal service endpoints\n- `celery-hpa.yaml` - Horizontal Pod Autoscaler\n\n**Quick Start:**\n```bash\n# Generate K8s manifests\n./scripts/deploy.sh kubernetes --namespace=production\n\n# Apply configuration\nkubectl apply -f kubernetes/\n\n# Scale workers\nkubectl scale deployment celery-worker --replicas=10\n\n# Monitor status\nkubectl get pods -l app=celery-worker\nkubectl logs -f deployment/celery-worker\n```\n\n**Key Features:**\n- Horizontal Pod Autoscaling based on CPU/queue depth\n- ConfigMaps for environment variables\n- Secrets management for credentials\n- Rolling updates with zero downtime\n- Resource limits and requests\n- Liveness and readiness probes\n- Pod disruption budgets\n- Anti-affinity for worker distribution\n\n### Systemd (Bare Metal/VMs)\n\n**Use Case:** Traditional server deployments, VPS, dedicated servers\n\n**Service Units:** `templates/systemd/`\n\n**Services:**\n- `celery-worker.service` - Worker daemon\n- `celery-beat.service` - Beat scheduler daemon\n- `celery-flower.service` - Monitoring dashboard\n\n**Quick Start:**\n```bash\n# Generate systemd units\n./scripts/deploy.sh systemd --workers=4\n\n# Install services\nsudo cp systemd/*.service /etc/systemd/system/\nsudo systemctl daemon-reload\n\n# Enable and start\nsudo systemctl enable celery-worker@{1..4}.service celery-beat.service\nsudo systemctl start celery-worker@{1..4}.service celery-beat.service\n\n# Check status\nsudo systemctl status celery-worker@*.service\nsudo journalctl -u celery-worker@1.service -f\n```\n\n**Key Features:**\n- Multi-instance worker support (@instance syntax)\n- Automatic restart on failure\n- Resource limits (CPU, memory)\n- Graceful shutdown handling\n- Log management via journald\n- User/group isolation\n- Environment file support\n\n## Health Checks Implementation\n\n### Python Health Check Module\n\n**Location:** `templates/health-checks.py`\n\n**Capabilities:**\n- Broker connectivity verification\n- Result backend validation\n- Worker discovery and ping\n- Queue depth monitoring\n- Task execution test\n- Memory and CPU metrics\n\n**Usage:**\n```python\nfrom health_checks import CeleryHealthCheck\n\n# Initialize checker\nhealth = CeleryHealthCheck(app)\n\n# Run all checks\nstatus = health.run_all_checks()\n\n# Individual checks\nbroker_ok = health.check_broker()\nworkers_ok = health.check_workers()\nqueues_ok = health.check_queue_depth(threshold=1000)\n```\n\n**Integration:**\n```python\n# Flask endpoint\n@app.route('/health')\ndef health_check():\n    checker = CeleryHealthCheck(celery_app)\n    result = checker.run_all_checks()\n    return jsonify(result), 200 if result['healthy'] else 503\n\n# FastAPI endpoint\n@app.get(\"/health\")\nasync def health_check():\n    checker = CeleryHealthCheck(celery_app)\n    result = checker.run_all_checks()\n    return result if result['healthy'] else JSONResponse(\n        status_code=503, content=result\n    )\n```\n\n### Shell Script Health Checks\n\n**Location:** `scripts/health-check.sh`\n\n**Features:**\n- Standalone health verification\n- Exit code compatibility (0=healthy, 1=unhealthy)\n- JSON output for parsing\n- Configurable timeouts\n- Retry logic\n\n**Usage:**\n```bash\n# Basic health check\n./scripts/health-check.sh\n\n# With custom timeout\n./scripts/health-check.sh --timeout=30\n\n# JSON output\n./scripts/health-check.sh --json\n\n# Specific checks\n./scripts/health-check.sh --check=broker,workers\n```\n\n**Docker Integration:**\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=10s --retries=3 \\\n  CMD /app/scripts/health-check.sh || exit 1\n```\n\n**Kubernetes Integration:**\n```yaml\nlivenessProbe:\n  exec:\n    command: [\"/app/scripts/health-check.sh\"]\n  initialDelaySeconds: 30\n  periodSeconds: 10\nreadinessProbe:\n  exec:\n    command: [\"/app/scripts/health-check.sh\", \"--check=workers\"]\n  initialDelaySeconds: 10\n  periodSeconds: 5\n```\n\n## Deployment Scripts\n\n### Main Deployment Orchestrator\n\n**Script:** `scripts/deploy.sh`\n\n**Capabilities:**\n- Platform detection and configuration\n- Environment-specific settings\n- Secret management validation\n- Pre-deployment checks\n- Configuration generation\n- Deployment execution\n- Post-deployment verification\n\n**Usage:**\n```bash\n# Deploy to Docker\n./scripts/deploy.sh docker --env=production\n\n# Deploy to Kubernetes\n./scripts/deploy.sh kubernetes --namespace=prod --replicas=10\n\n# Deploy systemd services\n./scripts/deploy.sh systemd --workers=4 --user=celery\n\n# Dry run (generate configs only)\n./scripts/deploy.sh kubernetes --dry-run\n\n# With custom configuration\n./scripts/deploy.sh docker --config=custom-config.yml\n```\n\n**Pre-deployment Checks:**\n- Broker accessibility\n- Result backend connectivity\n- Required environment variables\n- Secret availability (no hardcoded keys)\n- Python dependencies\n- Celery app importability\n- Task discovery\n\n### Deployment Testing\n\n**Script:** `scripts/test-deployment.sh`\n\n**Test Coverage:**\n- Service availability\n- Health endpoint responses\n- Worker registration\n- Task execution end-to-end\n- Beat schedule validation\n- Monitoring dashboard access\n- Log output verification\n- Resource utilization\n\n**Usage:**\n```bash\n# Test Docker deployment\n./scripts/test-deployment.sh docker\n\n# Test Kubernetes deployment\n./scripts/test-deployment.sh kubernetes --namespace=prod\n\n# Test systemd services\n./scripts/test-deployment.sh systemd\n\n# Verbose output\n./scripts/test-deployment.sh docker --verbose\n\n# Continuous monitoring\n./scripts/test-deployment.sh docker --watch --interval=60\n```\n\n**Test Scenarios:**\n1. Submit test task to each queue\n2. Verify task completion\n3. Check worker logs for errors\n4. Validate beat schedule execution\n5. Test graceful shutdown\n6. Verify task retry behavior\n7. Check monitoring metrics\n\n## Configuration Templates\n\n### Docker Compose Template\n\n**File:** `templates/docker-compose.yml`\n\n**Highlights:**\n- Multi-stage build support\n- Environment variable templating\n- Volume management\n- Network configuration\n- Health check definitions\n- Resource limits\n- Logging configuration\n\n**Key Sections:**\n```yaml\nservices:\n  redis:\n    # Broker configuration with persistence\n\n  postgres:\n    # Result backend with backup volumes\n\n  celery-worker:\n    # Worker with auto-scaling support\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n\n  celery-beat:\n    # Singleton scheduler\n    deploy:\n      replicas: 1\n\n  flower:\n    # Monitoring dashboard\n```\n\n### Dockerfile Template\n\n**File:** `templates/Dockerfile.worker`\n\n**Optimization:**\n- Multi-stage build for smaller images\n- Layer caching for dependencies\n- Non-root user execution\n- Security scanning compatible\n- Build-time arguments for flexibility\n\n**Stages:**\n1. **Base:** Python runtime + system dependencies\n2. **Builder:** Install and compile Python packages\n3. **Runtime:** Copy built packages, add application code\n\n**Size Optimization:**\n- Minimal base image (python:3.11-slim)\n- Remove build tools in final stage\n- Use .dockerignore\n- Multi-arch support\n\n### Kubernetes Manifests\n\n**Worker Deployment:** `templates/kubernetes/celery-worker.yaml`\n\n**Features:**\n- Rolling update strategy\n- Resource requests and limits\n- Horizontal Pod Autoscaler integration\n- Anti-affinity rules\n- Graceful termination (SIGTERM handling)\n- ConfigMap and Secret mounting\n\n**Beat StatefulSet:** `templates/kubernetes/celery-beat.yaml`\n\n**Features:**\n- Singleton guarantee (replicas: 1)\n- Persistent volume for schedule state\n- Leader election (optional)\n- Ordered deployment\n\n**Autoscaling:** `templates/kubernetes/celery-hpa.yaml`\n\n**Metrics:**\n- CPU utilization\n- Memory utilization\n- Custom metrics (queue depth via Prometheus)\n\n**Scaling Behavior:**\n```yaml\nminReplicas: 2\nmaxReplicas: 50\nbehavior:\n  scaleUp:\n    stabilizationWindowSeconds: 60\n    policies:\n    - type: Percent\n      value: 50\n      periodSeconds: 60\n  scaleDown:\n    stabilizationWindowSeconds: 300\n    policies:\n    - type: Pods\n      value: 2\n      periodSeconds: 120\n```\n\n### Systemd Service Units\n\n**Worker Service:** `templates/systemd/celery-worker.service`\n\n**Configuration:**\n```ini\n[Unit]\nDescription=Celery Worker Instance %i\nAfter=network.target redis.service\n\n[Service]\nType=forking\nUser=celery\nGroup=celery\nEnvironmentFile=/etc/celery/celery.conf\nWorkingDirectory=/opt/celery\nExecStart=/opt/celery/venv/bin/celery -A myapp worker \\\n  --loglevel=info \\\n  --logfile=/var/log/celery/worker-%i.log \\\n  --pidfile=/var/run/celery/worker-%i.pid \\\n  --hostname=worker%i@%%h \\\n  --concurrency=4\n\nExecStop=/bin/kill -s TERM $MAINPID\nExecReload=/bin/kill -s HUP $MAINPID\n\nRestart=always\nRestartSec=10s\n\n# Resource limits\nCPUQuota=100%\nMemoryLimit=1G\n\n[Install]\nWantedBy=multi-user.target\n```\n\n**Beat Service:** `templates/systemd/celery-beat.service`\n\n**Singleton Management:**\n- Single instance enforcement\n- Schedule persistence\n- Lock file management\n\n## Security Best Practices\n\n### Secrets Management\n\n**CRITICAL: Never hardcode credentials in configurations!**\n\n**Docker Compose:**\n```yaml\nservices:\n  celery-worker:\n    environment:\n      CELERY_BROKER_URL: ${CELERY_BROKER_URL}\n      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}\n    env_file:\n      - .env  # Never commit this file!\n```\n\n**Kubernetes:**\n```yaml\n# Use Secrets, not ConfigMaps\nenv:\n  - name: CELERY_BROKER_URL\n    valueFrom:\n      secretKeyRef:\n        name: celery-secrets\n        key: broker-url\n```\n\n**Systemd:**\n```ini\nEnvironmentFile=/etc/celery/secrets.env  # Mode 0600, owned by celery user\n```\n\n### User Isolation\n\n**Docker:**\n```dockerfile\nRUN useradd -m -u 1000 celery\nUSER celery\n```\n\n**Kubernetes:**\n```yaml\nsecurityContext:\n  runAsUser: 1000\n  runAsNonRoot: true\n  readOnlyRootFilesystem: true\n```\n\n**Systemd:**\n```ini\nUser=celery\nGroup=celery\n```\n\n### Network Security\n\n- Use TLS for broker connections\n- Encrypt result backend connections\n- Isolate worker networks\n- Implement network policies (K8s)\n- Use firewall rules (systemd)\n\n## Monitoring Integration\n\n### Prometheus Metrics\n\n**Expose Metrics:**\n```python\nfrom prometheus_client import start_http_server, Counter, Gauge\n\ntask_counter = Counter('celery_task_total', 'Total tasks', ['name', 'state'])\nworker_gauge = Gauge('celery_workers_active', 'Active workers')\n\n# Start metrics server\nstart_http_server(8000)\n```\n\n**Kubernetes ServiceMonitor:**\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: celery-metrics\nspec:\n  selector:\n    matchLabels:\n      app: celery-worker\n  endpoints:\n  - port: metrics\n```\n\n### Flower Dashboard\n\n**Configuration:**\n```yaml\n# docker-compose.yml\nflower:\n  image: mher/flower:latest\n  command: celery flower --broker=redis://redis:6379/0\n  ports:\n    - \"5555:5555\"\n  environment:\n    FLOWER_BASIC_AUTH: user:your_password_here  # Use env var!\n```\n\n### Log Aggregation\n\n**Docker:**\n```yaml\nlogging:\n  driver: \"json-file\"\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n```\n\n**Kubernetes:**\n- Use sidecar containers (Fluent Bit, Fluentd)\n- Send logs to ELK stack or Loki\n\n**Systemd:**\n```bash\njournalctl -u celery-worker@1.service -f --output=json\n```\n\n## Graceful Shutdown\n\n### Signal Handling\n\n**Worker Shutdown:**\n```python\nfrom celery.signals import worker_shutdown\n\n@worker_shutdown.connect\ndef graceful_shutdown(sender, **kwargs):\n    logger.info(\"Worker shutting down gracefully...\")\n    # Finish current tasks\n    # Close connections\n    # Release resources\n```\n\n**Docker:**\n```dockerfile\nSTOPSIGNAL SIGTERM\n```\n\n**Kubernetes:**\n```yaml\nlifecycle:\n  preStop:\n    exec:\n      command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\nterminationGracePeriodSeconds: 60\n```\n\n**Systemd:**\n```ini\nKillSignal=SIGTERM\nTimeoutStopSec=60\n```\n\n### Task Acknowledgment\n\n**Late ACK Pattern:**\n```python\ntask_acks_late = True\ntask_reject_on_worker_lost = True\n```\n\nEnsures tasks are re-queued if worker dies during execution.\n\n## Examples\n\n### Complete Docker Deployment\n\n**File:** `examples/docker-deployment.md`\n\n**Scenario:** Deploy full Celery stack with Redis, PostgreSQL, 3 workers, beat, and Flower\n\n**Steps:**\n1. Generate Docker Compose configuration\n2. Configure environment variables\n3. Build worker image\n4. Start all services\n5. Verify health checks\n6. Submit test tasks\n7. Monitor via Flower\n8. Scale workers based on load\n\n### Kubernetes Production Setup\n\n**File:** `examples/kubernetes-deployment.md`\n\n**Scenario:** Production-grade K8s deployment with autoscaling, monitoring, and HA\n\n**Components:**\n- 3+ worker replicas (autoscaling enabled)\n- Single beat instance (StatefulSet)\n- Redis Sentinel (HA broker)\n- PostgreSQL cluster (result backend)\n- Prometheus monitoring\n- Grafana dashboards\n- Horizontal Pod Autoscaler\n\n**Advanced Features:**\n- Pod Disruption Budgets\n- Network Policies\n- Resource Quotas\n- RBAC permissions\n- Ingress for Flower\n\n### Systemd Enterprise Deployment\n\n**File:** `examples/systemd-setup.md`\n\n**Scenario:** Multi-server deployment with systemd management\n\n**Architecture:**\n- 3 worker servers (4 workers each)\n- 1 beat scheduler server\n- Shared Redis cluster\n- Shared PostgreSQL database\n- Centralized logging\n\n**Management:**\n```bash\n# Start all workers across servers\nansible celery-workers -a \"systemctl start celery-worker@{1..4}.service\"\n\n# Rolling restart\nfor i in {1..4}; do\n  systemctl restart celery-worker@$i.service\n  sleep 30\ndone\n\n# Health check all workers\nansible celery-workers -m shell -a \"/opt/celery/scripts/health-check.sh\"\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Workers not starting:**\n- Check broker connectivity\n- Verify Python dependencies\n- Review worker logs\n- Validate Celery app import\n\n**Tasks not executing:**\n- Confirm workers are registered\n- Check queue routing\n- Verify task signatures\n- Review task ACK settings\n\n**Beat not scheduling:**\n- Ensure single beat instance\n- Check schedule file permissions\n- Verify timezone configuration\n- Review beat logs for errors\n\n**High memory usage:**\n- Reduce worker concurrency\n- Enable max-tasks-per-child\n- Check for memory leaks in tasks\n- Monitor with profiling tools\n\n**Container restarts:**\n- Review health check configuration\n- Check resource limits\n- Analyze OOM events\n- Verify signal handling\n\n## Resources\n\n**Scripts:** All deployment and health check scripts in `scripts/` directory\n\n**Templates:** Production-ready configuration templates in `templates/` directory\n\n**Examples:** Complete deployment scenarios with step-by-step instructions in `examples/` directory\n\n**Documentation:**\n- Docker Compose reference\n- Kubernetes best practices\n- Systemd unit configuration\n- Celery production checklist\n\n---\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n- Secrets management best practices enforced\n\n**Version:** 1.0.0\n**Celery Compatibility:** 5.0+\n**Platforms:** Docker, Kubernetes, Systemd"
              },
              {
                "name": "framework-integrations",
                "description": "Django, Flask, FastAPI integration patterns for Celery. Use when integrating Celery with Django, Flask, or FastAPI, setting up framework-specific configurations, handling application contexts, managing database transactions with tasks, configuring async workers, or when user mentions Django Celery, Flask Celery, FastAPI background tasks, framework integration, or web framework task queues.",
                "path": "plugins/celery/skills/framework-integrations/SKILL.md",
                "frontmatter": {
                  "name": "framework-integrations",
                  "description": "Django, Flask, FastAPI integration patterns for Celery. Use when integrating Celery with Django, Flask, or FastAPI, setting up framework-specific configurations, handling application contexts, managing database transactions with tasks, configuring async workers, or when user mentions Django Celery, Flask Celery, FastAPI background tasks, framework integration, or web framework task queues.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep"
                },
                "content": "# Framework Integrations\n\n**Purpose:** Integrate Celery with Django, Flask, and FastAPI using framework-specific patterns and best practices.\n\n**Activation Triggers:**\n- Setting up Celery with Django/Flask/FastAPI\n- Application context issues in tasks\n- Database transaction handling\n- Framework-specific configuration\n- Async worker setup\n- Request context in background tasks\n\n**Key Resources:**\n- `templates/django-integration/` - Django app structure with Celery\n- `templates/flask-integration/` - Flask factory pattern with Celery\n- `templates/fastapi-integration/` - FastAPI async integration\n- `templates/transaction-safe-django.py` - Django transaction handling\n- `templates/fastapi-background.py` - FastAPI BackgroundTasks vs Celery\n- `templates/flask-context.py` - Flask app context in tasks\n- `scripts/test-integration.sh` - Test framework integration\n- `scripts/validate-framework.sh` - Validate framework setup\n- `examples/` - Complete integration examples\n\n## Integration Patterns\n\n### Django + Celery\n\n**Core Setup:**\n\n1. **Install packages:**\n```bash\npip install celery django-celery-beat django-celery-results\n```\n\n2. **Project structure:**\n```\nmyproject/\n‚îú‚îÄ‚îÄ myproject/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ celery.py      # Celery app configuration\n‚îÇ   ‚îú‚îÄ‚îÄ settings.py    # Django settings with Celery config\n‚îÇ   ‚îî‚îÄ‚îÄ urls.py\n‚îú‚îÄ‚îÄ myapp/\n‚îÇ   ‚îú‚îÄ‚îÄ tasks.py       # Task definitions\n‚îÇ   ‚îî‚îÄ‚îÄ views.py       # Views that call tasks\n‚îî‚îÄ‚îÄ manage.py\n```\n\n3. **Use template:**\n```bash\n# Copy Django integration template\ncp -r templates/django-integration/* /path/to/project/\n```\n\n**Key Patterns:**\n\n**Transaction-Safe Tasks:**\n- Template: `templates/transaction-safe-django.py`\n- Use `transaction.on_commit()` to delay task execution\n- Prevents tasks from running before database commits\n- Essential for tasks that depend on database state\n\n**Django ORM in Tasks:**\n- Always use ORM (don't pass model instances)\n- Pass primary keys, reload in task\n- Close connections explicitly if needed\n- Use `CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP = True`\n\n**Configuration:**\n- Set `CELERY_BROKER_URL` in settings.py\n- Configure `CELERY_RESULT_BACKEND` for task results\n- Use `django-celery-beat` for periodic tasks (admin UI)\n- Use `django-celery-results` to store results in Django DB\n\n### Flask + Celery\n\n**Core Setup:**\n\n1. **Install packages:**\n```bash\npip install celery flask\n```\n\n2. **Factory pattern structure:**\n```\nmyapp/\n‚îú‚îÄ‚îÄ __init__.py        # Flask app factory\n‚îú‚îÄ‚îÄ celery.py          # Celery app with Flask context\n‚îú‚îÄ‚îÄ tasks.py           # Task definitions\n‚îú‚îÄ‚îÄ views.py           # Routes\n‚îî‚îÄ‚îÄ config.py          # Configuration\n```\n\n3. **Use template:**\n```bash\n# Copy Flask integration template\ncp -r templates/flask-integration/* /path/to/project/\n```\n\n**Key Patterns:**\n\n**Application Context:**\n- Template: `templates/flask-context.py`\n- Use `app.app_context()` for database/config access\n- Required for Flask-SQLAlchemy, Flask-Mail, etc.\n- Push context in task decorator\n\n**Factory Pattern:**\n- Create Celery app that creates Flask app internally\n- Subclass Celery task to push app context\n- Allows tasks to access Flask extensions\n- Essential for modular Flask apps\n\n**Configuration:**\n- Use Flask config for broker/backend URLs\n- Set `CELERY_BROKER_URL` from `app.config`\n- Share config between Flask and Celery\n- Load different configs per environment\n\n### FastAPI + Celery\n\n**Core Setup:**\n\n1. **Install packages:**\n```bash\npip install celery fastapi uvicorn\n```\n\n2. **Async structure:**\n```\napp/\n‚îú‚îÄ‚îÄ main.py            # FastAPI app\n‚îú‚îÄ‚îÄ celery_app.py      # Celery configuration\n‚îú‚îÄ‚îÄ tasks.py           # Task definitions\n‚îú‚îÄ‚îÄ routers/\n‚îÇ   ‚îî‚îÄ‚îÄ api.py         # API routes\n‚îî‚îÄ‚îÄ config.py          # Settings\n```\n\n3. **Use template:**\n```bash\n# Copy FastAPI integration template\ncp -r templates/fastapi-integration/* /path/to/project/\n```\n\n**Key Patterns:**\n\n**Async Tasks:**\n- Celery tasks can be async or sync\n- FastAPI endpoints are async by default\n- Use `.delay()` or `.apply_async()` from sync context\n- Use `asyncio.create_task()` for FastAPI BackgroundTasks\n\n**BackgroundTasks vs Celery:**\n- Template: `templates/fastapi-background.py`\n- **FastAPI BackgroundTasks:** Short tasks (<30s), no retries, dies with request\n- **Celery:** Long tasks, retries, persistent, distributed workers\n- Use both: BackgroundTasks for logging, Celery for heavy work\n\n**Dependency Injection:**\n- Don't inject FastAPI dependencies into Celery tasks\n- Tasks run in separate worker processes\n- Pass data explicitly, not dependency objects\n- Reload database objects in task\n\n**Configuration:**\n- Use Pydantic Settings for both FastAPI and Celery\n- Share config class between apps\n- Set broker/backend from environment variables\n- Use `app.state` for shared resources (if needed)\n\n## Common Integration Issues\n\n### Database Connections\n\n**Problem:** \"Lost connection to MySQL/Postgres during task\"\n\n**Solution:**\n```python\n# Close connections before task runs (Django)\nfrom django.db import connection\nconnection.close()\n\n# Or configure connection pooling\nCELERY_BROKER_POOL_LIMIT = None  # Unlimited\nDATABASES = {\n    'default': {\n        'CONN_MAX_AGE': 0,  # Close after request\n    }\n}\n```\n\n### Application Context\n\n**Problem:** \"Working outside of application context\" (Flask)\n\n**Solution:**\n```python\n# Use context template\nfrom templates.flask_context import make_celery\ncelery = make_celery(app)\n\n# Or manually push context\n@celery.task\ndef my_task():\n    with app.app_context():\n        # Access db, config, etc.\n        pass\n```\n\n### Transaction Timing\n\n**Problem:** Task runs before database commit (Django)\n\n**Solution:**\n```python\n# Use on_commit\nfrom django.db import transaction\n\ndef my_view(request):\n    obj = MyModel.objects.create(name=\"test\")\n    transaction.on_commit(\n        lambda: process_object.delay(obj.id)\n    )\n```\n\n### Async/Sync Mixing\n\n**Problem:** \"Cannot call async function from sync context\" (FastAPI)\n\n**Solution:**\n```python\n# Celery task (sync or async)\n@celery.task\nasync def process_data(data):\n    # Can use async here\n    result = await some_async_function(data)\n    return result\n\n# FastAPI endpoint (async)\n@app.post(\"/process\")\nasync def process(data: dict):\n    # Call Celery from async context\n    task = process_data.delay(data)\n    return {\"task_id\": task.id}\n```\n\n## Validation & Testing\n\n### Validate Framework Setup\n\n```bash\n# Check integration is configured correctly\n./scripts/validate-framework.sh django\n./scripts/validate-framework.sh flask\n./scripts/validate-framework.sh fastapi\n```\n\n**Checks:**\n- Required packages installed\n- Celery app configured correctly\n- Framework config has broker URL\n- Task discovery working\n- App context handling (Flask)\n- Transaction handling (Django)\n\n### Test Integration\n\n```bash\n# Run integration tests\n./scripts/test-integration.sh django\n./scripts/test-integration.sh flask\n./scripts/test-integration.sh fastapi\n```\n\n**Tests:**\n- Task execution from framework\n- Database access in tasks\n- Context handling\n- Transaction safety\n- Async task execution (FastAPI)\n- Error propagation\n\n## Security Considerations\n\n**CRITICAL: Never hardcode credentials in integration code**\n\n‚úÖ **CORRECT:**\n```python\n# Django settings.py\nCELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://your_redis_url_here')\n\n# Flask config.py\nCELERY_BROKER_URL = os.environ.get('CELERY_BROKER_URL', 'redis://your_redis_url_here')\n\n# FastAPI config.py\nclass Settings(BaseSettings):\n    celery_broker_url: str = \"redis://your_redis_url_here\"\n\n    class Config:\n        env_file = \".env\"\n```\n\n‚ùå **WRONG:**\n```python\nCELERY_BROKER_URL = \"redis://actual-password@redis.example.com\"\nCELERY_BROKER_URL = \"amqp://user:real_password@rabbitmq\"\n```\n\n**Always:**\n- Use environment variables for broker URLs\n- Use placeholders in examples: `your_redis_url_here`\n- Add `.env` to `.gitignore`\n- Document where to obtain credentials\n- Never commit actual connection strings\n\n## Framework-Specific Resources\n\n### Django\n- **Template:** `templates/django-integration/`\n- **Transaction Safety:** `templates/transaction-safe-django.py`\n- **Example:** `examples/django-complete-setup.md`\n- Includes: settings.py, celery.py, tasks.py, views.py\n\n### Flask\n- **Template:** `templates/flask-integration/`\n- **Context Handling:** `templates/flask-context.py`\n- **Example:** `examples/flask-factory-pattern.md`\n- Includes: Factory pattern, app context, extensions\n\n### FastAPI\n- **Template:** `templates/fastapi-integration/`\n- **Background Tasks:** `templates/fastapi-background.py`\n- **Example:** `examples/fastapi-async.md`\n- Includes: Async tasks, dependency injection, BackgroundTasks vs Celery\n\n## Quick Reference\n\n| Framework | Context Needed | Transaction Safe | Async Support |\n|-----------|---------------|------------------|---------------|\n| Django    | Auto          | Use on_commit    | Via celery    |\n| Flask     | Manual push   | N/A              | Via celery    |\n| FastAPI   | Not needed    | N/A              | Native        |\n\n| Package | Purpose | Required For |\n|---------|---------|--------------|\n| `django-celery-beat` | Periodic tasks UI | Django admin scheduling |\n| `django-celery-results` | Store results in DB | Django result persistence |\n| `flask` | Web framework | Flask integration |\n| `fastapi[all]` | Async web framework | FastAPI integration |\n\n---\n\n**Supported Frameworks:** Django 4+, Flask 2+, FastAPI 0.100+\n**Celery Version:** 5.3+\n**Python:** 3.9+"
              },
              {
                "name": "monitoring-flower",
                "description": "Flower monitoring setup and configuration for Celery including real-time monitoring, authentication, custom dashboards, and Prometheus metrics integration. Use when setting up Celery monitoring, configuring Flower web UI, implementing authentication, creating custom dashboards, integrating with Prometheus, or when user mentions Flower, Celery monitoring, task monitoring, worker monitoring, or real-time metrics.",
                "path": "plugins/celery/skills/monitoring-flower/SKILL.md",
                "frontmatter": {
                  "name": "monitoring-flower",
                  "description": "Flower monitoring setup and configuration for Celery including real-time monitoring, authentication, custom dashboards, and Prometheus metrics integration. Use when setting up Celery monitoring, configuring Flower web UI, implementing authentication, creating custom dashboards, integrating with Prometheus, or when user mentions Flower, Celery monitoring, task monitoring, worker monitoring, or real-time metrics.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Flower Monitoring Skill\n\nThis skill provides comprehensive templates and configurations for setting up Flower, the real-time monitoring tool for Celery. Includes authentication, custom dashboards, Prometheus metrics integration, and production deployment patterns.\n\n## Overview\n\nFlower is a web-based monitoring and administration tool for Celery that provides:\n\n1. **Real-time Monitoring** - Worker status, task progress, event tracking\n2. **Task Management** - View, revoke, and retry tasks\n3. **Authentication** - Basic auth, OAuth, and custom authentication\n4. **Metrics Export** - Prometheus integration for external monitoring\n5. **Custom Dashboards** - Tailored views for specific workflows\n\nThis skill covers production-ready Flower deployments with security and scalability.\n\n## Available Scripts\n\n### 1. Start Flower Server\n\n**Script**: `scripts/start-flower.sh <broker-url> <port>`\n\n**Purpose**: Starts Flower monitoring server with proper configuration\n\n**Parameters**:\n- `broker-url` - Redis/RabbitMQ broker URL (default: redis://localhost:6379/0)\n- `port` - Port to run Flower on (default: 5555)\n\n**Usage**:\n```bash\n# Start with default settings\n./scripts/start-flower.sh\n\n# Start with custom Redis broker\n./scripts/start-flower.sh redis://redis:6379/0 5555\n\n# Start with RabbitMQ\n./scripts/start-flower.sh amqp://guest:guest@localhost:5672// 5555\n\n# Start with authentication\nFLOWER_BASIC_AUTH=\"user:password\" ./scripts/start-flower.sh\n```\n\n**Environment Variables**:\n- `FLOWER_BASIC_AUTH` - Basic auth credentials (user:password)\n- `FLOWER_OAUTH2_REDIRECT_URI` - OAuth2 redirect URI\n- `FLOWER_MAX_TASKS` - Maximum tasks to keep in memory (default: 10000)\n\n**Output**: Flower web UI available at http://localhost:5555\n\n### 2. Install Flower as Systemd Service\n\n**Script**: `scripts/flower-systemd.service`\n\n**Purpose**: Systemd service file for production Flower deployment\n\n**Usage**:\n```bash\n# Copy service file\nsudo cp scripts/flower-systemd.service /etc/systemd/system/flower.service\n\n# Edit service file with your paths\nsudo nano /etc/systemd/system/flower.service\n\n# Reload systemd\nsudo systemctl daemon-reload\n\n# Enable and start service\nsudo systemctl enable flower\nsudo systemctl start flower\n\n# Check status\nsudo systemctl status flower\n```\n\n**Configuration Points**:\n- `WorkingDirectory` - Your project directory\n- `User` - User to run service as\n- `Environment` - Broker URL and authentication\n- `ExecStart` - Flower command with options\n\n### 3. Test Flower Configuration\n\n**Script**: `scripts/test-flower.sh <flower-url>`\n\n**Purpose**: Validates Flower setup and connectivity\n\n**Checks**:\n- Flower web UI accessible\n- Worker nodes visible\n- Task history available\n- Metrics endpoint working\n- Authentication configured\n- No security warnings\n\n**Usage**:\n```bash\n# Test local Flower instance\n./scripts/test-flower.sh http://localhost:5555\n\n# Test with authentication\n./scripts/test-flower.sh http://user:password@localhost:5555\n\n# Test production instance\n./scripts/test-flower.sh https://flower.example.com\n```\n\n**Exit Codes**:\n- `0` - All checks passed\n- `1` - Flower not accessible\n- `2` - No workers detected\n- `3` - Authentication issues\n\n## Available Templates\n\n### 1. Flower Configuration\n\n**Template**: `templates/flower-config.py`\n\n**Purpose**: Complete Flower configuration file with all options\n\n**Features**:\n- Broker and backend URLs\n- Port and address binding\n- Task retention settings\n- URL prefix for reverse proxy\n- Database persistence\n- Max workers and tasks\n\n**Usage**:\n```python\n# Save as flowerconfig.py in your project\n# Flower will auto-detect this file\n\n# Or specify explicitly:\ncelery -A myapp flower --conf=flowerconfig.py\n```\n\n**Key Configuration Options**:\n- `broker_api` - Broker management API URL\n- `persistent` - Enable database persistence\n- `db` - SQLite database path\n- `max_tasks` - Task history limit\n- `url_prefix` - Prefix for reverse proxy\n\n### 2. Flower Authentication\n\n**Template**: `templates/flower-auth.py`\n\n**Purpose**: Authentication configurations including basic auth and OAuth\n\n**Authentication Methods**:\n\n**Basic Authentication**:\n```python\n# Username/password protection\nflower --basic_auth=user1:password1,user2:password2\n```\n\n**OAuth2 (Google)**:\n```python\n# Google OAuth integration\nflower \\\n  --auth=\".*@example\\.com\" \\\n  --oauth2_key=your_google_client_id_here \\\n  --oauth2_secret=your_google_client_secret_here \\\n  --oauth2_redirect_uri=http://localhost:5555/login\n```\n\n**Custom Authentication**:\n```python\n# Implement custom auth provider\nfrom flower.views.auth import Auth\n\nclass CustomAuth(Auth):\n    def authenticate(self, username, password):\n        # Your authentication logic\n        return username in allowed_users\n```\n\n**Security Notes**:\n- Never hardcode credentials in config files\n- Use environment variables for secrets\n- Enable HTTPS in production\n- Implement rate limiting\n- Use OAuth for team access\n\n### 3. Prometheus Metrics\n\n**Template**: `templates/prometheus-metrics.py`\n\n**Purpose**: Export Celery metrics to Prometheus\n\n**Metrics Exposed**:\n- `celery_tasks_total` - Total tasks by state\n- `celery_workers_online` - Active worker count\n- `celery_task_runtime_seconds` - Task execution time\n- `celery_queue_length` - Queue depth by queue name\n\n**Usage**:\n```python\n# Run metrics exporter alongside Flower\npython templates/prometheus-metrics.py\n\n# Metrics available at http://localhost:8000/metrics\n```\n\n**Prometheus Scrape Config**:\n```yaml\nscrape_configs:\n  - job_name: 'celery'\n    static_configs:\n      - targets: ['localhost:8000']\n```\n\n**Grafana Integration**:\n- Import Celery dashboard template\n- Connect to Prometheus data source\n- Visualize task rates, queue depths, worker health\n\n### 4. Custom Dashboard\n\n**Template**: `templates/custom-dashboard.py`\n\n**Purpose**: Create custom Flower views for specific workflows\n\n**Custom Views**:\n- Task filtering by type\n- Worker grouping by role\n- Custom metrics display\n- Workflow-specific dashboards\n\n**Implementation**:\n```python\nfrom flower.views import BaseHandler\n\nclass CustomDashboard(BaseHandler):\n    def get(self):\n        # Your custom dashboard logic\n        self.render(\"custom_dashboard.html\", data=data)\n```\n\n**Template Variables**:\n- `workers` - Active worker list\n- `tasks` - Recent task history\n- `queues` - Queue statistics\n- `custom_metrics` - Your computed metrics\n\n## Available Examples\n\n### 1. Complete Flower Setup\n\n**Example**: `examples/flower-setup.md`\n\n**Covers**:\n- Initial Flower installation\n- Configuration file setup\n- Authentication implementation\n- Systemd service creation\n- Reverse proxy configuration (Nginx)\n- SSL/TLS setup\n- Monitoring integration\n\n**Step-by-Step Guide**:\n1. Install Flower: `pip install flower`\n2. Create configuration file\n3. Configure authentication\n4. Test locally\n5. Deploy as systemd service\n6. Configure reverse proxy\n7. Enable SSL\n8. Connect monitoring tools\n\n**Production Checklist**:\n- [ ] Authentication enabled\n- [ ] HTTPS configured\n- [ ] Database persistence enabled\n- [ ] Task retention limits set\n- [ ] Resource limits configured\n- [ ] Monitoring integrated\n- [ ] Backup strategy defined\n\n### 2. Prometheus Integration\n\n**Example**: `examples/prometheus-integration.md`\n\n**Covers**:\n- Metrics exporter setup\n- Prometheus configuration\n- Grafana dashboard creation\n- Alerting rules\n- Performance optimization\n\n**Metrics Collection**:\n```python\n# Key metrics to monitor\n- Task success/failure rates\n- Average task duration\n- Queue depths\n- Worker availability\n- Task retries\n- Error rates by task type\n```\n\n**Alert Examples**:\n- High task failure rate\n- Queue depth exceeding threshold\n- Worker offline detection\n- Slow task execution\n- Memory usage alerts\n\n### 3. Custom Dashboards\n\n**Example**: `examples/custom-dashboards.md`\n\n**Covers**:\n- Creating custom views\n- Template customization\n- Adding custom metrics\n- Filtering and grouping\n- Real-time updates\n\n**Use Cases**:\n- ML training job monitoring\n- ETL pipeline tracking\n- Report generation status\n- Video processing workflows\n- Multi-tenant task views\n\n**Custom View Features**:\n- Task filtering by tags\n- Worker grouping by zone\n- Custom time ranges\n- Export capabilities\n- Email notifications\n\n## Security Compliance\n\n**CRITICAL:** This skill follows strict security rules:\n\n‚ùå **NEVER hardcode:**\n- Basic auth credentials\n- OAuth client secrets\n- API keys\n- Database passwords\n- Broker credentials\n\n‚úÖ **ALWAYS:**\n- Use environment variables for secrets\n- Generate `.env.example` with placeholders\n- Add `.env*` to `.gitignore`\n- Use HTTPS in production\n- Implement authentication\n- Enable rate limiting\n- Document credential requirements\n\n**Placeholder format:**\n```bash\n# .env.example\nFLOWER_BASIC_AUTH=username_your_password_here\nFLOWER_OAUTH2_KEY=your_google_client_id_here\nFLOWER_OAUTH2_SECRET=your_google_client_secret_here\nCELERY_BROKER_URL=redis_your_password_here@localhost:6379/0\n```\n\n## Progressive Disclosure\n\nThis skill provides immediate setup guidance with references to detailed documentation:\n\n- **Quick Start**: Use `start-flower.sh` for immediate local setup\n- **Production**: Reference `flower-setup.md` for complete deployment guide\n- **Metrics**: Use `prometheus-metrics.py` for monitoring integration\n- **Custom Views**: Reference `custom-dashboards.md` for advanced customization\n\nLoad additional files only when specific customization is needed.\n\n## Common Workflows\n\n### 1. Local Development Setup\n\n```bash\n# Install Flower\npip install flower\n\n# Start with basic auth\nFLOWER_BASIC_AUTH=\"dev:dev_password_here\" \\\n  ./scripts/start-flower.sh redis://localhost:6379/0 5555\n\n# Access at http://localhost:5555\n```\n\n### 2. Production Deployment\n\n```bash\n# 1. Configure authentication\ncp templates/flower-auth.py flowerconfig.py\n# Edit with environment-specific settings\n\n# 2. Install systemd service\nsudo cp scripts/flower-systemd.service /etc/systemd/system/flower.service\nsudo systemctl enable flower\nsudo systemctl start flower\n\n# 3. Configure Nginx reverse proxy\n# 4. Enable SSL with Let's Encrypt\n# 5. Test connectivity\n./scripts/test-flower.sh https://flower.example.com\n```\n\n### 3. Metrics Integration\n\n```bash\n# 1. Start Prometheus metrics exporter\npython templates/prometheus-metrics.py &\n\n# 2. Configure Prometheus scraping\n# 3. Import Grafana dashboard\n# 4. Set up alerting rules\n```\n\n## Troubleshooting\n\n### Flower Won't Start\n\n**Check**:\n- Broker URL is correct and accessible\n- Port is not already in use\n- Virtual environment is activated\n- Celery workers are running\n\n**Debug**:\n```bash\n# Test broker connectivity\ncelery -A myapp inspect ping\n\n# Check port availability\nlsof -i :5555\n\n# Run with verbose logging\ncelery -A myapp flower --logging=debug\n```\n\n### Workers Not Visible\n\n**Check**:\n- Workers are running and connected to same broker\n- Flower is monitoring correct broker\n- No firewall blocking connections\n- Worker events are enabled\n\n**Fix**:\n```bash\n# Enable events on workers\ncelery -A myapp control enable_events\n\n# Verify broker URL matches\necho $CELERY_BROKER_URL\n```\n\n### Authentication Issues\n\n**Check**:\n- Credentials are properly formatted\n- OAuth redirect URI is correct\n- No typos in username/password\n- Environment variables are set\n\n**Debug**:\n```bash\n# Test basic auth\ncurl -u username:password http://localhost:5555\n\n# Check OAuth configuration\ncurl http://localhost:5555/login\n```\n\n## Dependencies\n\n**Required**:\n- `flower>=2.0.0` - Flower monitoring tool\n- `celery>=5.3.0` - Celery task queue\n- `redis>=4.5.0` or `kombu>=5.3.0` - Broker client\n\n**Optional**:\n- `prometheus-client>=0.16.0` - For Prometheus metrics\n- `tornado>=6.0` - For async support\n- `SQLAlchemy>=2.0.0` - For persistent storage\n\n**Installation**:\n```bash\n# Basic installation\npip install flower\n\n# With Prometheus metrics\npip install flower prometheus-client\n\n# With persistent storage\npip install flower sqlalchemy\n```\n\n## Best Practices\n\n1. **Authentication**: Always enable authentication in production\n2. **Task Retention**: Set `max_tasks` to prevent memory issues\n3. **Database Persistence**: Use SQLite/PostgreSQL for task history\n4. **Reverse Proxy**: Run behind Nginx/Caddy for SSL and rate limiting\n5. **Monitoring**: Export metrics to Prometheus/Grafana\n6. **Resource Limits**: Configure systemd limits for production\n7. **Backup**: Regularly backup Flower database if using persistence\n\n## Additional Resources\n\n- **Flower Documentation**: https://flower.readthedocs.io/\n- **Celery Monitoring Guide**: https://docs.celeryq.dev/en/stable/userguide/monitoring.html\n- **Prometheus Integration**: https://prometheus.io/docs/instrumenting/exporters/\n- **Production Deployment**: Reference `examples/flower-setup.md`"
              },
              {
                "name": "result-backend-patterns",
                "description": "Result backend configuration patterns for Celery including Redis, Database, and RPC backends with serialization, expiration policies, and performance optimization. Use when configuring result storage, troubleshooting result persistence, implementing custom serializers, migrating between backends, optimizing result expiration, or when user mentions result backends, task results, Redis backend, PostgreSQL results, result serialization, or backend migration.",
                "path": "plugins/celery/skills/result-backend-patterns/SKILL.md",
                "frontmatter": {
                  "name": "result-backend-patterns",
                  "description": "Result backend configuration patterns for Celery including Redis, Database, and RPC backends with serialization, expiration policies, and performance optimization. Use when configuring result storage, troubleshooting result persistence, implementing custom serializers, migrating between backends, optimizing result expiration, or when user mentions result backends, task results, Redis backend, PostgreSQL results, result serialization, or backend migration.",
                  "allowed-tools": "Read, Write, Edit, Bash, Glob, Grep"
                },
                "content": "# Result Backend Patterns\n\n**Purpose:** Configure and optimize Celery result backends for reliable task result storage and retrieval.\n\n**Activation Triggers:**\n- Setting up result backend for first time\n- Migrating from one backend to another\n- Result retrieval failures or timeouts\n- Serialization errors with complex objects\n- Performance issues with result storage\n- Expired result cleanup problems\n- Custom serialization requirements\n\n**Key Resources:**\n- `templates/redis-backend.py` - Redis result backend configuration\n- `templates/db-backend.py` - Database (SQLAlchemy) backend setup\n- `templates/rpc-backend.py` - RPC (AMQP) backend configuration\n- `templates/result-expiration.py` - Expiration and cleanup policies\n- `templates/custom-serializers.py` - Custom serialization patterns\n- `scripts/test-backend.sh` - Backend connection and functionality testing\n- `scripts/migrate-backend.sh` - Safe backend migration with data preservation\n- `examples/` - Complete setup guides for each backend type\n\n## Backend Selection Guide\n\n### Redis Backend (Recommended for Most Cases)\n**Best for:**\n- High-performance applications\n- Frequent result access\n- Short to medium result retention (minutes to days)\n- Real-time status updates\n\n**Characteristics:**\n- Fast in-memory storage\n- Automatic expiration support\n- Connection pooling built-in\n- TTL-based cleanup\n\n**Use template:** `templates/redis-backend.py`\n\n### Database Backend (PostgreSQL/MySQL)\n**Best for:**\n- Long-term result storage (weeks to months)\n- Applications with existing database infrastructure\n- Complex result queries and reporting\n- Audit trail requirements\n\n**Characteristics:**\n- Persistent disk storage\n- SQL query capabilities\n- Automatic table creation\n- Transaction support\n\n**Use template:** `templates/db-backend.py`\n\n### RPC Backend (Message Broker)\n**Best for:**\n- Transient results consumed immediately\n- Microservice architectures\n- Results used only by initiating client\n- Minimal infrastructure requirements\n\n**Characteristics:**\n- No additional backend service needed\n- Results as AMQP messages\n- Single-retrieval pattern\n- Optional persistence mode\n\n**Use template:** `templates/rpc-backend.py`\n\n## Configuration Workflow\n\n### 1. Choose Backend Based on Requirements\n\n**Decision Matrix:**\n```\nPerformance Priority + Short Retention ‚Üí Redis\nLong-term Storage + Query Needs ‚Üí Database\nImmediate Consumption Only ‚Üí RPC\nExisting Redis Infrastructure ‚Üí Redis\nExisting Database Infrastructure ‚Üí Database\n```\n\n### 2. Apply Base Configuration Template\n\n```bash\n# Copy appropriate template to your celeryconfig.py or settings\ncp templates/redis-backend.py your_project/celeryconfig.py\n# OR\ncp templates/db-backend.py your_project/celeryconfig.py\n# OR\ncp templates/rpc-backend.py your_project/celeryconfig.py\n```\n\n### 3. Configure Connection Settings\n\n**Redis Example:**\n```python\n# Security: Use environment variables, never hardcode\nimport os\n\nresult_backend = f'redis://:{os.getenv(\"REDIS_PASSWORD\", \"\")}@' \\\n                 f'{os.getenv(\"REDIS_HOST\", \"localhost\")}:' \\\n                 f'{os.getenv(\"REDIS_PORT\", \"6379\")}/0'\n```\n\n**Database Example:**\n```python\n# Security: Use environment variables for credentials\nimport os\n\ndb_user = os.getenv(\"DB_USER\", \"celery\")\ndb_pass = os.getenv(\"DB_PASSWORD\", \"your_password_here\")\ndb_host = os.getenv(\"DB_HOST\", \"localhost\")\ndb_name = os.getenv(\"DB_NAME\", \"celery_results\")\n\nresult_backend = f'db+postgresql://{db_user}:{db_pass}@{db_host}/{db_name}'\n```\n\n### 4. Set Serialization Options\n\nReference `templates/custom-serializers.py` for advanced patterns:\n\n```python\n# JSON (default, secure, cross-language)\nresult_serializer = 'json'\nresult_accept_content = ['json']\n\n# Enable compression for large results\nresult_compression = 'gzip'\n\n# Store extended metadata (task name, args, retries)\nresult_extended = True\n```\n\n### 5. Configure Expiration Policy\n\nReference `templates/result-expiration.py`:\n\n```python\n# Expire after 24 hours (default: 1 day)\nresult_expires = 86400\n\n# Disable expiration for critical results\nresult_expires = None\n\n# Enable automatic cleanup (requires celery beat)\nbeat_schedule = {\n    'cleanup-results': {\n        'task': 'celery.backend_cleanup',\n        'schedule': crontab(hour=4, minute=0),\n    }\n}\n```\n\n### 6. Test Backend Connection\n\n```bash\n# Verify backend is reachable and functional\n./scripts/test-backend.sh redis\n# OR\n./scripts/test-backend.sh postgresql\n# OR\n./scripts/test-backend.sh rpc\n```\n\n## Backend-Specific Configurations\n\n### Redis Optimization\n\n**Connection Pooling:**\n```python\nredis_max_connections = 50  # Adjust based on worker count\nredis_socket_timeout = 120\nredis_socket_keepalive = True\nredis_retry_on_timeout = True\n```\n\n**Persistence vs Performance:**\n```python\n# For critical results, ensure Redis persistence\n# Configure in redis.conf:\n# save 900 1      # Save after 900s if 1 key changed\n# save 300 10     # Save after 300s if 10 keys changed\n# appendonly yes  # Enable AOF for durability\n```\n\n### Database Optimization\n\n**Connection Management:**\n```python\ndatabase_engine_options = {\n    'pool_size': 10,\n    'pool_recycle': 3600,\n    'pool_pre_ping': True,  # Verify connections before use\n}\n\n# Resolve stale connections\ndatabase_short_lived_sessions = True\n```\n\n**Table Customization:**\n```python\ndatabase_table_names = {\n    'task': 'celery_taskmeta',\n    'group': 'celery_groupmeta',\n}\n\n# Auto-create tables at startup (Celery 5.5+)\ndatabase_create_tables_at_setup = True\n```\n\n**MySQL Transaction Isolation:**\n```python\n# CRITICAL for MySQL\ndatabase_engine_options = {\n    'isolation_level': 'READ COMMITTED',\n}\n```\n\n### RPC Configuration\n\n**Persistent Messages:**\n```python\n# Make results survive broker restarts\nresult_persistent = True\n\n# Configure result exchange\nresult_exchange = 'celery_results'\nresult_exchange_type = 'direct'\n```\n\n## Serialization Patterns\n\n### JSON (Recommended Default)\n\n**Advantages:**\n- Human-readable\n- Cross-language compatible\n- Secure (no code execution)\n- Widely supported\n\n**Limitations:**\n- Cannot serialize complex Python objects\n- No datetime support (use ISO strings)\n- Limited binary data support\n\n**Example:** See `templates/custom-serializers.py`\n\n### Custom Serializers\n\n**When to Use:**\n- Complex domain objects\n- Binary data (images, files)\n- Custom data types\n- Performance optimization\n\n**Implementation:**\n```python\nfrom kombu.serialization import register\n\ndef custom_encoder(obj):\n    # Your encoding logic\n    return serialized_data\n\ndef custom_decoder(data):\n    # Your decoding logic\n    return deserialized_obj\n\nregister(\n    'myformat',\n    custom_encoder,\n    custom_decoder,\n    content_type='application/x-myformat',\n    content_encoding='utf-8'\n)\n\n# Use in config\nresult_serializer = 'myformat'\nresult_accept_content = ['myformat', 'json']\n```\n\n## Migration Between Backends\n\n### Safe Migration Process\n\n```bash\n# Use migration script for zero-downtime migration\n./scripts/migrate-backend.sh redis postgresql\n\n# Process:\n# 1. Configure new backend alongside old\n# 2. Dual-write to both backends\n# 3. Verify new backend functionality\n# 4. Switch reads to new backend\n# 5. Deprecate old backend\n```\n\n### Manual Migration Steps\n\n**1. Add new backend configuration:**\n```python\n# Keep old backend active\nresult_backend = 'redis://localhost:6379/0'\n\n# Add new backend (not active yet)\n# new_result_backend = 'db+postgresql://...'\n```\n\n**2. Deploy with dual-write capability:**\n```python\n# Custom backend that writes to both\nclass DualBackend:\n    def __init__(self):\n        self.old_backend = RedisBackend(...)\n        self.new_backend = DatabaseBackend(...)\n\n    def store_result(self, task_id, result, state):\n        # Write to both backends\n        self.old_backend.store_result(task_id, result, state)\n        self.new_backend.store_result(task_id, result, state)\n```\n\n**3. Verify and switch:**\n```bash\n# Test new backend\n./scripts/test-backend.sh postgresql\n\n# Update config to use new backend\nresult_backend = 'db+postgresql://...'\n```\n\n## Performance Optimization\n\n### Disable Results When Not Needed\n\n```python\n# Global setting\ntask_ignore_result = True\n\n# Per-task override\n@app.task(ignore_result=True)\ndef fire_and_forget_task():\n    # Results not stored\n    pass\n```\n\n### Connection Pooling\n\n**Redis:**\n```python\nredis_max_connections = None  # No limit (use with caution)\n# OR\nredis_max_connections = worker_concurrency * 2  # Rule of thumb\n```\n\n**Database:**\n```python\ndatabase_engine_options = {\n    'pool_size': 20,\n    'max_overflow': 10,\n}\n```\n\n### Result Compression\n\n```python\n# Compress large results\nresult_compression = 'gzip'  # or 'bzip2'\n\n# Only compress results over threshold\nresult_compression = 'gzip'\nresult_compression_level = 6  # 1-9, higher = more compression\n```\n\n### Batch Result Retrieval\n\n```python\n# Retrieve multiple results efficiently\nfrom celery.result import GroupResult\n\njob = group(task.s(i) for i in range(100))()\nresults = job.get(timeout=10, propagate=False)\n```\n\n## Troubleshooting\n\n### Results Not Persisting\n\n**Check:**\n1. Backend connection string format\n2. Backend service is running\n3. Credentials are correct\n4. `ignore_result` is not set globally\n5. Task completed without errors\n\n**Debug:**\n```bash\n./scripts/test-backend.sh <backend-type>\n```\n\n### Serialization Errors\n\n**Symptoms:**\n- `TypeError: Object of type X is not JSON serializable`\n- `pickle.PicklingError`\n\n**Solutions:**\n1. Use JSON-compatible types only\n2. Implement custom serializer (see `templates/custom-serializers.py`)\n3. Convert complex objects before returning\n4. Use `result_serializer = 'pickle'` (security risk!)\n\n### Performance Degradation\n\n**Redis:**\n- Increase `redis_max_connections`\n- Enable connection pooling\n- Monitor Redis memory usage\n- Implement aggressive expiration\n\n**Database:**\n- Add indexes on `task_id` column\n- Enable `database_short_lived_sessions`\n- Increase connection pool size\n- Archive old results periodically\n\n### Expired Results\n\n**Check expiration settings:**\n```python\n# View current setting\nprint(app.conf.result_expires)\n\n# Extend retention\nresult_expires = 7 * 86400  # 7 days\n```\n\n**Enable automatic cleanup:**\n```python\n# Requires celery beat\nbeat_schedule = {\n    'cleanup-results': {\n        'task': 'celery.backend_cleanup',\n        'schedule': crontab(hour=4, minute=0),\n    }\n}\n```\n\n## Security Best Practices\n\n**Connection Credentials:**\n- Store in environment variables, never hardcode\n- Use `.env.example` with placeholders\n- Add `.env` to `.gitignore`\n- Rotate credentials regularly\n\n**Network Security:**\n- Use TLS/SSL for Redis connections (`rediss://`)\n- Enable SSL for database connections\n- Restrict backend access by IP/firewall\n- Use authentication for all backends\n\n**Serialization:**\n- Avoid pickle serializer (code execution risk)\n- Use JSON for cross-language compatibility\n- Validate deserialized data\n- Implement content type whitelisting\n\n## Examples\n\nAll backend configurations have complete examples in `examples/`:\n\n- `redis-backend-setup.md` - Complete Redis setup with sentinel and cluster\n- `postgresql-backend.md` - PostgreSQL configuration with migrations\n- `result-expiration-policies.md` - Expiration strategies and cleanup patterns\n\n## Resources\n\n**Templates:** Complete configuration files in `templates/` directory\n**Scripts:** Testing and migration tools in `scripts/` directory\n**Examples:** Real-world setup guides in `examples/` directory\n\n---\n\n**Backend Support:** Redis, PostgreSQL, MySQL, SQLite, MongoDB, RPC (AMQP)\n**Celery Version:** 5.0+\n**Last Updated:** 2025-11-16"
              },
              {
                "name": "routing-strategies",
                "description": "Task routing and queue management patterns for Celery including priority queues, topic exchanges, worker-specific routing, and advanced queue configurations. Use when configuring task routing, managing queues, setting up priority queues, implementing worker routing, configuring topic exchanges, or when user mentions task routing, queue management, Celery routing, worker assignments, or message broker routing.",
                "path": "plugins/celery/skills/routing-strategies/SKILL.md",
                "frontmatter": {
                  "name": "routing-strategies",
                  "description": "Task routing and queue management patterns for Celery including priority queues, topic exchanges, worker-specific routing, and advanced queue configurations. Use when configuring task routing, managing queues, setting up priority queues, implementing worker routing, configuring topic exchanges, or when user mentions task routing, queue management, Celery routing, worker assignments, or message broker routing.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Celery Task Routing Strategies Skill\n\nThis skill provides comprehensive templates, scripts, and patterns for implementing advanced task routing and queue management in Celery applications, including priority queues, topic-based routing, and worker-specific queue assignments.\n\n## Overview\n\nEffective task routing is crucial for:\n\n1. **Performance Optimization** - Route compute-intensive tasks to dedicated workers\n2. **Priority Management** - High-priority tasks bypass slower queues\n3. **Resource Isolation** - Separate critical operations from background jobs\n4. **Scalability** - Independent scaling of different task types\n\nThis skill covers routing with RabbitMQ, Redis, and custom broker configurations.\n\n## Available Scripts\n\n### 1. Test Routing Configuration\n\n**Script**: `scripts/test-routing.sh <config-file>`\n\n**Purpose**: Validates routing configuration and tests queue connectivity\n\n**Checks**:\n- Broker connectivity (RabbitMQ/Redis)\n- Queue declarations\n- Exchange configurations\n- Routing key patterns\n- Worker queue bindings\n- Priority queue setup\n\n**Usage**:\n```bash\n# Test routing configuration\n./scripts/test-routing.sh ./celery_config.py\n\n# Test with custom broker URL\nBROKER_URL=amqp://user:password@localhost:5672// ./scripts/test-routing.sh ./celery_config.py\n\n# Verbose output\nVERBOSE=1 ./scripts/test-routing.sh ./celery_config.py\n```\n\n**Exit Codes**:\n- `0`: All routing tests passed\n- `1`: Configuration errors detected\n- `2`: Broker connection failed\n\n### 2. Validate Queue Configuration\n\n**Script**: `scripts/validate-queues.sh <project-dir>`\n\n**Purpose**: Validates queue setup across application code\n\n**Checks**:\n- Task decorators use valid queues\n- No hardcoded queue names (use config)\n- All queues defined in routing configuration\n- Priority settings are valid (0-255)\n- Exchange types match routing patterns\n- Worker configurations reference valid queues\n\n**Usage**:\n```bash\n# Validate current project\n./scripts/validate-queues.sh .\n\n# Validate specific directory\n./scripts/validate-queues.sh /path/to/celery-app\n\n# Generate detailed report\nREPORT=1 ./scripts/validate-queues.sh . > queue-validation-report.md\n```\n\n**Exit Codes**:\n- `0`: Validation passed\n- `1`: Validation failed (must fix issues)\n\n## Available Templates\n\n### 1. Basic Queue Configuration\n\n**Template**: `templates/queue-config.py`\n\n**Features**:\n- Default queue setup\n- Named queues for different task types\n- Queue-to-exchange bindings\n- Priority settings\n- Worker routing configuration\n\n**Usage**:\n```python\nfrom celery import Celery\nfrom templates.queue_config import CELERY_ROUTES, CELERY_QUEUES\n\napp = Celery('myapp')\napp.conf.task_routes = CELERY_ROUTES\napp.conf.task_queues = CELERY_QUEUES\n```\n\n**Configuration Example**:\n```python\nCELERY_QUEUES = (\n    Queue('default', Exchange('default'), routing_key='default'),\n    Queue('high_priority', Exchange('default'), routing_key='high'),\n    Queue('low_priority', Exchange('default'), routing_key='low'),\n    Queue('emails', Exchange('emails'), routing_key='email.*'),\n    Queue('reports', Exchange('reports'), routing_key='report.*'),\n)\n\nCELERY_ROUTES = {\n    'myapp.tasks.send_email': {'queue': 'emails', 'routing_key': 'email.send'},\n    'myapp.tasks.generate_report': {'queue': 'reports', 'routing_key': 'report.generate'},\n    'myapp.tasks.urgent_task': {'queue': 'high_priority', 'priority': 9},\n}\n```\n\n### 2. Dynamic Routing Rules\n\n**Template**: `templates/routing-rules.py`\n\n**Features**:\n- Pattern-based routing\n- Conditional routing logic\n- Dynamic queue selection\n- Routing by task name patterns\n- Routing by task arguments\n\n**Key Functions**:\n```python\ndef route_task(name, args, kwargs, options, task=None, **kw):\n    \"\"\"\n    Dynamic routing based on task name or arguments\n    \"\"\"\n    if name.startswith('urgent.'):\n        return {'queue': 'high_priority', 'priority': 9}\n\n    if 'priority' in kwargs and kwargs['priority'] == 'high':\n        return {'queue': 'high_priority'}\n\n    if name.startswith('email.'):\n        return {'queue': 'emails', 'exchange': 'emails'}\n\n    return {'queue': 'default'}\n\napp.conf.task_routes = (route_task,)\n```\n\n### 3. Priority Queue Setup\n\n**Template**: `templates/priority-queues.py`\n\n**Features**:\n- Multi-level priority queues (0-255)\n- Priority inheritance\n- Default priority configuration\n- Queue priority enforcement\n\n**Priority Levels**:\n```python\n# Priority queue configuration\nCELERY_QUEUES = (\n    Queue('critical', Exchange('tasks'), routing_key='critical',\n          queue_arguments={'x-max-priority': 10}),\n    Queue('high', Exchange('tasks'), routing_key='high',\n          queue_arguments={'x-max-priority': 10}),\n    Queue('normal', Exchange('tasks'), routing_key='normal',\n          queue_arguments={'x-max-priority': 10}),\n    Queue('low', Exchange('tasks'), routing_key='low',\n          queue_arguments={'x-max-priority': 10}),\n)\n\n# Task priority mapping\nPRIORITY_LEVELS = {\n    'critical': 10,  # Highest priority\n    'high': 7,\n    'normal': 5,\n    'low': 2,\n}\n\n# Apply priority to task\n@app.task(priority=PRIORITY_LEVELS['high'])\ndef urgent_processing():\n    pass\n```\n\n### 4. Topic Exchange Routing\n\n**Template**: `templates/topic-exchange.py`\n\n**Features**:\n- Topic-based routing patterns\n- Wildcard routing keys\n- Multi-queue routing\n- Pattern matching\n\n**Topic Patterns**:\n```python\nfrom kombu import Exchange, Queue\n\n# Topic exchange setup\ntask_exchange = Exchange('tasks', type='topic', durable=True)\n\nCELERY_QUEUES = (\n    # Match specific patterns\n    Queue('user.notifications', exchange=task_exchange,\n          routing_key='user.notification.*'),\n\n    # Match all email types\n    Queue('emails', exchange=task_exchange,\n          routing_key='email.#'),\n\n    # Match processing tasks\n    Queue('processing', exchange=task_exchange,\n          routing_key='*.processing.*'),\n\n    # Match all reports\n    Queue('reports', exchange=task_exchange,\n          routing_key='report.*'),\n)\n\n# Routing configuration\nCELERY_ROUTES = {\n    'myapp.tasks.send_welcome_email': {\n        'exchange': 'tasks',\n        'routing_key': 'email.welcome.send'\n    },\n    'myapp.tasks.notify_user': {\n        'exchange': 'tasks',\n        'routing_key': 'user.notification.send'\n    },\n}\n```\n\n### 5. Worker-Specific Routing\n\n**Template**: `templates/worker-routing.py`\n\n**Features**:\n- Dedicated worker pools\n- Worker-specific queues\n- CPU vs I/O task separation\n- Geographic routing\n- Resource-based routing\n\n**Worker Configuration**:\n```python\n# Worker pool definitions\nWORKER_POOLS = {\n    'cpu_intensive': {\n        'queues': ['ml_training', 'video_processing', 'data_analysis'],\n        'concurrency': 4,\n        'prefetch_multiplier': 1,\n    },\n    'io_intensive': {\n        'queues': ['api_calls', 'file_uploads', 'emails'],\n        'concurrency': 50,\n        'prefetch_multiplier': 10,\n    },\n    'general': {\n        'queues': ['default', 'background'],\n        'concurrency': 10,\n        'prefetch_multiplier': 4,\n    },\n}\n\n# Start workers\n# celery -A myapp worker --queues=ml_training,video_processing -c 4 -n cpu_worker@%h\n# celery -A myapp worker --queues=api_calls,file_uploads -c 50 -n io_worker@%h\n```\n\n## Available Examples\n\n### 1. Priority Queue Setup Guide\n\n**Example**: `examples/priority-queue-setup.md`\n\n**Demonstrates**:\n- Configuring RabbitMQ priority queues\n- Setting task priorities\n- Priority inheritance patterns\n- Testing priority routing\n- Monitoring priority queue performance\n\n**Key Concepts**:\n- Priority range: 0 (lowest) to 255 (highest)\n- RabbitMQ `x-max-priority` argument\n- Priority at task definition vs runtime\n- Queue argument configuration\n\n### 2. Topic-Based Routing Implementation\n\n**Example**: `examples/topic-routing.md`\n\n**Demonstrates**:\n- Topic exchange setup\n- Routing key patterns (* and # wildcards)\n- Multi-queue routing\n- Pattern matching strategies\n- Consumer binding patterns\n\n**Routing Key Patterns**:\n- `*` - Matches exactly one word\n- `#` - Matches zero or more words\n- Example: `email.*.send` matches `email.welcome.send`, `email.notification.send`\n- Example: `user.#` matches `user.create`, `user.update.profile`\n\n### 3. Worker Queue Assignment Strategy\n\n**Example**: `examples/worker-queue-assignment.md`\n\n**Demonstrates**:\n- CPU-bound vs I/O-bound task separation\n- Worker pool configuration\n- Queue-to-worker mapping\n- Scaling strategies per worker type\n- Resource allocation patterns\n\n**Worker Types**:\n```bash\n# CPU-intensive workers (low concurrency)\ncelery -A myapp worker -Q ml_training,video_processing -c 4 -n cpu@%h\n\n# I/O-intensive workers (high concurrency)\ncelery -A myapp worker -Q api_calls,emails -c 50 -n io@%h\n\n# General purpose workers\ncelery -A myapp worker -Q default,background -c 10 -n general@%h\n```\n\n## Routing Strategies Comparison\n\n### 1. Direct Exchange (Default)\n- **Use Case**: Simple queue-to-task mapping\n- **Pros**: Simple, predictable, fast\n- **Cons**: Limited flexibility\n- **Example**: Each task type goes to one specific queue\n\n### 2. Topic Exchange\n- **Use Case**: Pattern-based routing, hierarchical task categories\n- **Pros**: Flexible, supports wildcards, multi-queue routing\n- **Cons**: More complex configuration\n- **Example**: `email.*.send` routes all email types to email queue\n\n### 3. Fanout Exchange\n- **Use Case**: Broadcasting tasks to multiple queues\n- **Pros**: Simple broadcast mechanism\n- **Cons**: No routing logic, all queues receive all messages\n- **Example**: Notifications sent to multiple consumer types\n\n### 4. Headers Exchange\n- **Use Case**: Complex routing based on message headers\n- **Pros**: Very flexible, metadata-based routing\n- **Cons**: Performance overhead, complex configuration\n- **Example**: Route by `priority=high` and `region=us-east`\n\n## Performance Considerations\n\n### 1. Queue Configuration\n- **Durable queues**: Messages persist across broker restarts (use for critical tasks)\n- **Transient queues**: Faster but messages lost on restart (use for disposable tasks)\n- **Queue length limits**: Prevent memory issues with `x-max-length`\n\n### 2. Prefetch Settings\n- **CPU-bound tasks**: Low prefetch (1-2) to prevent blocking\n- **I/O-bound tasks**: High prefetch (10+) to keep workers busy\n- **Configure per worker**: `celery worker --prefetch-multiplier=4`\n\n### 3. Priority Queue Performance\n- **RabbitMQ**: Native priority support, efficient\n- **Redis**: Priority emulation via separate queues, less efficient\n- **Trade-off**: Priority checking adds overhead, use only when needed\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real broker credentials, passwords, or secrets\n- Environment variable references in all code (BROKER_URL, BACKEND_URL)\n- `.gitignore` protection documented\n- Broker URLs use placeholder format: `amqp://user:password@localhost:5672//`\n\n**Never hardcode**:\n```python\n# ‚ùå WRONG\nBROKER_URL = 'amqp://myuser:secretpass123@rabbitmq.example.com:5672//'\n\n# ‚úÖ CORRECT\nimport os\nBROKER_URL = os.environ.get('CELERY_BROKER_URL', 'amqp://guest:guest@localhost:5672//')\n```\n\n## Best Practices\n\n1. **Use Environment-Based Configuration** - Different queues for dev/staging/prod\n2. **Separate Critical Tasks** - High-priority queue for time-sensitive operations\n3. **Match Worker to Task Type** - CPU workers for compute, I/O workers for network/disk\n4. **Monitor Queue Lengths** - Alert on queue buildup indicating bottlenecks\n5. **Use Topic Exchanges for Hierarchical Tasks** - Cleaner routing than multiple direct exchanges\n6. **Test Routing Configuration** - Validate routes before deploying to production\n7. **Document Routing Logic** - Especially for complex pattern-based routing\n8. **Use Priority Sparingly** - Overuse defeats the purpose and adds overhead\n9. **Configure Prefetch Per Worker Type** - Optimize based on task characteristics\n10. **Plan for Scaling** - Design routing to allow independent queue scaling\n\n## Requirements\n\n- Celery 5.0+\n- Message broker (RabbitMQ 3.8+ or Redis 6.0+)\n- Python 3.8+\n- kombu library (included with Celery)\n- Environment variables:\n  - `CELERY_BROKER_URL` (required)\n  - `CELERY_RESULT_BACKEND` (optional)\n- For RabbitMQ priority queues: RabbitMQ 3.5+\n- For testing scripts: netcat/telnet for connectivity checks\n\n## Progressive Disclosure\n\nFor advanced routing patterns, see:\n- `examples/priority-queue-setup.md` - Priority queue implementation\n- `examples/topic-routing.md` - Topic exchange patterns\n- `examples/worker-queue-assignment.md` - Worker pool strategies\n\n## Troubleshooting\n\n### Queue Not Receiving Tasks\n1. Check routing configuration matches task name\n2. Verify queue declaration in CELERY_QUEUES\n3. Ensure workers are listening to correct queues\n4. Check broker connectivity with test script\n\n### Priority Not Working\n1. Verify `x-max-priority` set on queue (RabbitMQ only)\n2. Check tasks are setting priority correctly\n3. Confirm workers consuming from priority queue\n4. Redis: Implement separate high/low priority queues\n\n### Worker Not Processing Tasks\n1. Verify worker queue list matches routed queues\n2. Check prefetch_multiplier isn't too low\n3. Ensure no task failures blocking queue\n4. Monitor worker logs for errors\n\n---\n\n**Plugin**: celery\n**Version**: 1.0.0\n**Last Updated**: 2025-11-16"
              },
              {
                "name": "task-patterns",
                "description": "Production-ready Celery task templates with error handling, retries, rate limiting, time limits, and custom task classes. Use when creating Celery tasks, implementing retry logic, adding rate limiting, setting time limits, building custom task classes, validating task inputs with Pydantic, handling database operations, making API calls, or when user mentions task patterns, retry mechanisms, task templates, error handling, task best practices.",
                "path": "plugins/celery/skills/task-patterns/SKILL.md",
                "frontmatter": {
                  "name": "task-patterns",
                  "description": "Production-ready Celery task templates with error handling, retries, rate limiting, time limits, and custom task classes. Use when creating Celery tasks, implementing retry logic, adding rate limiting, setting time limits, building custom task classes, validating task inputs with Pydantic, handling database operations, making API calls, or when user mentions task patterns, retry mechanisms, task templates, error handling, task best practices.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Task Patterns Skill\n\nProduction-ready Celery task templates with comprehensive error handling, retry mechanisms, rate limiting, and custom behavior patterns.\n\n## Overview\n\nThis skill provides battle-tested templates and patterns for building robust Celery tasks. Each template demonstrates production-ready code with proper error handling, logging, retry logic, and security best practices.\n\n## Core Patterns\n\n### 1. Basic Task Template\n\n**Template**: `templates/basic-task.py`\n\nSimple task with standard error handling and logging.\n\n**Use when**:\n- Creating your first Celery task\n- Need straightforward task execution\n- No complex retry or limiting requirements\n\n**Features**:\n- Proper logging setup\n- Basic error handling\n- Synchronous and asynchronous execution examples\n- Clear docstrings\n\n### 2. Retry Task Template\n\n**Template**: `templates/retry-task.py`\n\nTasks with automatic retry mechanisms and exponential backoff.\n\n**Use when**:\n- Calling external APIs that may fail transiently\n- Database operations that could timeout\n- Network operations prone to connection issues\n\n**Features**:\n- Automatic retry with `autoretry_for`\n- Exponential backoff with jitter\n- Manual retry control\n- Fixed delay retries\n- Configurable max retries\n\n**Example**: See `examples/task-with-retries.md` for complete implementation guide\n\n### 3. Rate Limited Task Template\n\n**Template**: `templates/rate-limited-task.py`\n\nTasks with rate limiting to control execution speed.\n\n**Use when**:\n- Respecting external API rate limits\n- Protecting database from overload\n- Controlling resource consumption\n- Meeting SLA requirements\n\n**Features**:\n- Per-second, per-minute, per-hour limits\n- Batch processing with rate control\n- Large dataset handling\n- Dynamic rate limit adjustment\n\n**Example**: See `examples/rate-limiting.md` for complete guide\n\n### 4. Time Limited Task Template\n\n**Template**: `templates/time-limited-task.py`\n\nTasks with soft and hard time limits to prevent runaway execution.\n\n**Use when**:\n- Long-running operations that could hang\n- Operations with external dependencies\n- Tasks that must complete within timeframe\n- Preventing resource exhaustion\n\n**Features**:\n- Soft time limit (catchable)\n- Hard time limit (force kill)\n- Graceful timeout handling\n- Progress saving on timeout\n- Combined with retry logic\n\n### 5. Custom Task Class Template\n\n**Template**: `templates/custom-task-class.py`\n\nCustom task base classes with specialized behavior.\n\n**Use when**:\n- Need database connection pooling\n- Want to cache task results\n- Require metrics tracking\n- Need shared resources across tasks\n- Implementing lifecycle hooks\n\n**Features**:\n- Database connection pooling\n- Automatic result caching\n- Metrics and monitoring\n- Resource pool management\n- Lifecycle hooks (before_start, on_success, on_failure, on_retry, after_return)\n\n**Example**: See `examples/custom-task-classes.md` for comprehensive guide\n\n### 6. Pydantic Validation Template\n\n**Template**: `templates/pydantic-validation.py`\n\nType-safe tasks with Pydantic model validation.\n\n**Use when**:\n- Need strict input validation\n- Want type safety\n- Complex nested data structures\n- API contract enforcement\n\n**Features**:\n- Pydantic models for input validation\n- Enum types for constrained values\n- Custom validators\n- Standardized result format\n- Comprehensive error messages\n\n### 7. Database Task Template\n\n**Template**: `templates/database-task.py`\n\nBest practices for database operations in tasks.\n\n**Use when**:\n- Performing database queries\n- Bulk database operations\n- Transactional operations\n- Database migrations\n\n**Features**:\n- Connection pooling\n- Parameterized queries (SQL injection prevention)\n- Transaction management\n- Bulk insert operations\n- Pagination support\n- Aggregation queries\n\n### 8. API Task Template\n\n**Template**: `templates/api-task.py`\n\nBest practices for external API calls in tasks.\n\n**Use when**:\n- Calling external APIs\n- Webhook delivery\n- Paginated API fetching\n- Batch API calls\n- API authentication\n\n**Features**:\n- Automatic retry on connection errors\n- Rate limiting for API quotas\n- Timeout handling\n- Authentication patterns (Bearer, API key)\n- Pagination support\n- Batch processing\n\n## Scripts\n\n### generate-task.sh\n\n**Usage**: `./scripts/generate-task.sh <template_name> <output_file> [task_name]`\n\nGenerate new Celery task from template with customizations.\n\n**Available templates**:\n- basic-task\n- retry-task\n- rate-limited-task\n- time-limited-task\n- custom-task-class\n- pydantic-validation\n- database-task\n- api-task\n\n**Example**:\n```bash\n./scripts/generate-task.sh retry-task tasks/my_api_call.py fetch_data\n```\n\n### test-task.sh\n\n**Usage**: `./scripts/test-task.sh <task_file.py> [task_name]`\n\nTest Celery task by:\n1. Validating Python syntax\n2. Checking imports\n3. Verifying task structure\n4. Security scanning\n5. Best practices check\n6. Optionally running task\n\n**Example**:\n```bash\n./scripts/test-task.sh templates/retry-task.py fetch_api_data\n```\n\n### validate-task.sh\n\n**Usage**: `./scripts/validate-task.sh <task_file.py>`\n\nComprehensive validation including:\n- Required elements (Celery import, app initialization, decorators)\n- Best practices (docstrings, type hints, error handling, logging)\n- Retry configuration\n- Security checks (no hardcoded credentials, SQL injection)\n- Performance checks (rate limits, time limits)\n- Code quality (examples, configuration)\n- Pattern detection\n\n**Example**:\n```bash\n./scripts/validate-task.sh templates/api-task.py\n```\n\n## Usage Examples\n\n### Example 1: Create API Task with Retries\n\n```python\n# Use retry-task.py template\nfrom templates.retry_task import fetch_api_data\n\n# Queue task\nresult = fetch_api_data.delay('https://api.example.com/data')\n\n# Get result\ndata = result.get(timeout=60)\nprint(data)\n```\n\n### Example 2: Rate-Limited Batch Processing\n\n```python\n# Use rate-limited-task.py template\nfrom templates.rate_limited_task import api_call_rate_limited\n\n# Process 100 items at 10/minute rate\nfor i in range(100):\n    api_call_rate_limited.delay(f'/endpoint/{i}')\n\n# Celery automatically enforces rate limit\n```\n\n### Example 3: Database Operations with Connection Pooling\n\n```python\n# Use database-task.py template\nfrom templates.database_task import insert_record, bulk_insert\n\n# Single insert\nresult1 = insert_record.delay('users', {\n    'name': 'John Doe',\n    'email': 'john@example.com'\n})\n\n# Bulk insert\nrecords = [\n    {'name': 'Jane', 'email': 'jane@example.com'},\n    {'name': 'Bob', 'email': 'bob@example.com'},\n]\nresult2 = bulk_insert.delay('users', records)\n```\n\n### Example 4: Custom Task with Metrics\n\n```python\n# Use custom-task-class.py template\nfrom templates.custom_task_class import monitored_task\n\n# Automatic metrics tracking\nresult = monitored_task.delay(123)\n\n# Metrics logged automatically:\n# - Start time\n# - Duration\n# - Success/failure\n# - Arguments\n```\n\n### Example 5: Type-Safe Task with Pydantic\n\n```python\n# Use pydantic-validation.py template\nfrom templates.pydantic_validation import process_user\n\n# Valid data\nuser_data = {\n    'user_id': 123,\n    'email': 'user@example.com',\n    'username': 'john_doe',\n    'age': 30,\n    'tags': ['premium']\n}\nresult = process_user.delay(user_data)\n\n# Invalid data returns structured error\ninvalid_data = {\n    'user_id': -1,  # Invalid\n    'email': 'not-email',  # Invalid\n}\nresult = process_user.delay(invalid_data)\n# Returns: {'status': 'error', 'errors': [...]}\n```\n\n## Best Practices\n\n### 1. Always Use Placeholders for Credentials\n\n```python\n# ‚úÖ CORRECT\napi_key = os.getenv('API_KEY', 'your_api_key_here')\n\n# ‚ùå WRONG\napi_key = 'sk-abc123xyz456'\n```\n\n### 2. Set Appropriate Timeouts\n\n```python\n# ‚úÖ CORRECT\nresponse = requests.get(url, timeout=30)\n\n# ‚ùå WRONG\nresponse = requests.get(url)  # Can hang forever\n```\n\n### 3. Use Parameterized Queries\n\n```python\n# ‚úÖ CORRECT\ndb.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,))\n\n# ‚ùå WRONG\ndb.execute(f\"SELECT * FROM users WHERE id = {user_id}\")\n```\n\n### 4. Combine Retry with Rate Limiting\n\n```python\n@app.task(\n    bind=True,\n    autoretry_for=(RequestException,),\n    retry_backoff=True,\n    rate_limit='10/m'\n)\ndef robust_api_call(self, url: str):\n    \"\"\"Retries on failure, respects rate limits.\"\"\"\n    pass\n```\n\n### 5. Log All Important Events\n\n```python\n@app.task(bind=True)\ndef well_logged_task(self, item_id: int):\n    logger.info(f\"Starting task for item {item_id}\")\n    try:\n        result = process(item_id)\n        logger.info(f\"Successfully processed {item_id}\")\n        return result\n    except Exception as exc:\n        logger.error(f\"Failed to process {item_id}: {exc}\")\n        raise\n```\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All templates use placeholders for credentials\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- Parameterized queries to prevent SQL injection\n- Security validation in test scripts\n\n## Pattern Selection Guide\n\n**Choose basic-task.py when**:\n- Simple, straightforward operations\n- No external dependencies\n- Low failure risk\n\n**Choose retry-task.py when**:\n- External API calls\n- Network operations\n- Transient failures expected\n\n**Choose rate-limited-task.py when**:\n- API rate limits exist\n- Need to control execution speed\n- Protecting resources from overload\n\n**Choose time-limited-task.py when**:\n- Long-running operations\n- Risk of hanging\n- Need guaranteed completion time\n\n**Choose custom-task-class.py when**:\n- Need resource pooling\n- Want lifecycle hooks\n- Shared logic across tasks\n- Metrics tracking required\n\n**Choose pydantic-validation.py when**:\n- Complex input validation needed\n- Type safety important\n- API contract enforcement\n- Clear error messages required\n\n**Choose database-task.py when**:\n- Database operations\n- Need connection pooling\n- Transactional operations\n- Bulk processing\n\n**Choose api-task.py when**:\n- External API integration\n- Webhook delivery\n- Authentication required\n- Pagination needed\n\n## Testing Tasks\n\n### Test Individual Task\n\n```bash\n# Validate task structure\n./scripts/validate-task.sh templates/retry-task.py\n\n# Test task execution\n./scripts/test-task.sh templates/retry-task.py fetch_api_data\n```\n\n### Run with Celery Worker\n\n```bash\n# Start worker\ncelery -A tasks worker --loglevel=info\n\n# Execute tasks\npython3 templates/retry-task.py\n```\n\n## Common Patterns\n\n### Pattern: Retry with Backoff\n\n```python\n@app.task(\n    bind=True,\n    autoretry_for=(RequestException,),\n    retry_backoff=True,\n    retry_jitter=True,\n    max_retries=5\n)\n```\n\n### Pattern: Rate + Time Limits\n\n```python\n@app.task(\n    rate_limit='10/m',\n    soft_time_limit=60,\n    time_limit=120\n)\n```\n\n### Pattern: Validation + Retry\n\n```python\n@app.task(\n    bind=True,\n    autoretry_for=(ValidationError, RequestException),\n    retry_backoff=True,\n    max_retries=3\n)\ndef validated_api_task(self, request: dict):\n    # Validate with Pydantic\n    req = ApiRequest(**request)\n    # Make API call with retry\n    return make_call(req.url)\n```\n\n## Quick Reference\n\n### Task Decorator Options\n\n```python\n@app.task(\n    bind=True,                    # Access self (required for retry)\n    base=CustomTask,              # Custom task class\n    autoretry_for=(Exception,),   # Exceptions to retry\n    retry_backoff=True,           # Exponential backoff\n    retry_backoff_max=600,        # Max backoff seconds\n    retry_jitter=True,            # Add randomness\n    max_retries=5,                # Max attempts\n    default_retry_delay=10,       # Default delay\n    rate_limit='10/m',            # Rate limit\n    soft_time_limit=60,           # Soft timeout (catchable)\n    time_limit=120,               # Hard timeout (kills task)\n    ignore_result=False,          # Store result\n    priority=5                    # Task priority (0-9)\n)\n```\n\n### Retry Syntax\n\n```python\n# Automatic\nautoretry_for=(RequestException, Timeout)\n\n# Manual\nraise self.retry(exc=exc, countdown=60)\n```\n\n### Rate Limit Syntax\n\n```python\nrate_limit='10/s'   # 10 per second\nrate_limit='100/m'  # 100 per minute\nrate_limit='1000/h' # 1000 per hour\n```\n\n## References\n\n- [Celery Tasks Documentation](https://docs.celeryq.dev/en/stable/userguide/tasks.html)\n- `examples/task-with-retries.md` - Complete retry guide\n- `examples/rate-limiting.md` - Complete rate limiting guide\n- `examples/custom-task-classes.md` - Custom class guide\n\n## Validation\n\nAlways run validation before committing:\n\n```bash\n./scripts/validate-task.sh your-task.py\n```\n\nValidation checks:\n- ‚úÖ Required imports and setup\n- ‚úÖ Task decorators present\n- ‚úÖ Docstrings and type hints\n- ‚úÖ Error handling\n- ‚úÖ Security (no hardcoded credentials)\n- ‚úÖ Best practices compliance"
              },
              {
                "name": "workflow-canvas",
                "description": "Celery canvas patterns for workflow composition including chains, groups, chords, signatures, error handling, and nested workflows. Use when building complex task workflows, parallel execution patterns, task synchronization, callback handling, or when user mentions canvas primitives, workflow composition, task chains, parallel processing, or chord patterns.",
                "path": "plugins/celery/skills/workflow-canvas/SKILL.md",
                "frontmatter": {
                  "name": "workflow-canvas",
                  "description": "Celery canvas patterns for workflow composition including chains, groups, chords, signatures, error handling, and nested workflows. Use when building complex task workflows, parallel execution patterns, task synchronization, callback handling, or when user mentions canvas primitives, workflow composition, task chains, parallel processing, or chord patterns.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Workflow Canvas\n\n**Purpose:** Implement Celery canvas patterns for composing complex, distributed task workflows.\n\n**Activation Triggers:**\n- Sequential task execution needed (chains)\n- Parallel task processing required (groups)\n- Synchronization after parallel work (chords)\n- Complex workflow composition\n- Task result aggregation\n- Error handling in workflows\n- Nested or conditional workflows\n\n**Key Resources:**\n- `scripts/test-workflow.sh` - Test workflow execution and validate patterns\n- `scripts/validate-canvas.sh` - Validate canvas structure and dependencies\n- `scripts/generate-workflow.sh` - Generate workflows from templates\n- `templates/` - Complete workflow pattern implementations\n- `examples/` - Real-world workflow scenarios with explanations\n\n## Canvas Primitives\n\n### 1. Signatures\n\nFoundation of canvas system - encapsulate task invocation details:\n\n```python\nfrom celery import signature\n\n# Named signature\nsignature('tasks.add', args=(2, 2), countdown=10)\n\n# Using task method\nadd.signature((2, 2), countdown=10)\n\n# Shortcut syntax (most common)\nadd.s(2, 2)\n\n# Immutable signature (prevents result forwarding)\nadd.si(2, 2)\n```\n\n**Use templates/chain-workflow.py** for signature examples.\n\n### 2. Chains\n\nSequential task execution where each task's result becomes next task's first argument:\n\n```python\nfrom celery import chain\n\n# Explicit chain\nchain(add.s(2, 2), add.s(4), add.s(8))()\n\n# Pipe syntax (recommended)\n(add.s(2, 2) | add.s(4) | add.s(8))()\n\n# With immutable tasks (independent execution)\n(add.si(2, 2) | process.s() | notify.si('done'))()\n```\n\n**Key Pattern:** Use `.si()` when task should NOT receive previous result.\n\n**See templates/chain-workflow.py** for complete patterns.\n\n### 3. Groups\n\nExecute multiple tasks in parallel, returns GroupResult:\n\n```python\nfrom celery import group\n\n# Parallel execution\ngroup(add.s(i, i) for i in range(10))()\n\n# With result tracking\njob = group(process.s(item) for item in batch)\nresult = job.apply_async()\nresult.get()  # Wait for all tasks\n```\n\n**Requirements:**\n- Tasks must NOT ignore results\n- Result backend required\n\n**See templates/group-parallel.py** for implementation.\n\n### 4. Chords\n\nGroup header + callback that executes after all header tasks complete:\n\n```python\nfrom celery import chord\n\n# Basic chord\nchord(add.s(i, i) for i in range(100))(tsum.s()).get()\n\n# With error handling\nchord(\n    process.s(item) for item in batch\n)(aggregate_results.s()).on_error(handle_chord_error.s())\n```\n\n**Critical Requirements:**\n- Result backend MUST be enabled\n- Set `task_ignore_result=False` explicitly\n- Redis 2.2+ for proper operation\n\n**Error Handling:**\nFailed header tasks ‚Üí callback receives ChordError with task ID and exception.\n\n**See templates/chord-pattern.py** for complete implementation.\n\n### 5. Map & Starmap\n\nBuilt-in tasks for sequence processing (single message, sequential execution):\n\n```python\n# Map: one call per element\nadd.map([(2, 2), (4, 4), (8, 8)])\n\n# Starmap: unpacks tuples\nadd.starmap(zip(range(100), range(100)))\n```\n\n**Difference from groups:** Single task message vs multiple messages.\n\n### 6. Chunks\n\nDivide large iterables into sized batches:\n\n```python\n# Process 100 items in chunks of 10\nadd.chunks(zip(range(100), range(100)), 10)\n```\n\n**Returns:** Nested lists corresponding to chunk outputs.\n\n## Advanced Patterns\n\n### Partial Application\n\n```python\n# Incomplete signature\npartial = add.s(2)\n\n# Complete later (arguments prepended)\npartial.delay(4)  # Executes add(4, 2)\n\n# Kwargs merge with precedence\npartial = process.s(timeout=30)\npartial.apply_async(kwargs={'timeout': 60})  # Uses 60\n```\n\n### Nested Workflows\n\nCombine primitives for complex logic:\n\n```python\n# Chain of chords\nworkflow = chain(\n    chord([task1.s(), task2.s()])(callback1.s()),\n    chord([task3.s(), task4.s()])(callback2.s())\n)\n\n# Groups in chains\nworkflow = (\n    prepare.s() |\n    group([process.s(i) for i in range(10)]) |\n    finalize.s()\n)\n```\n\n**See templates/nested-workflows.py** for production patterns.\n\n### Error Callbacks\n\n```python\n# Task-level error handling\nadd.s(2, 2).on_error(log_error.s()).delay()\n\n# Chord error handling\nchord(\n    header_tasks\n)(callback.s()).on_error(handle_error.s())\n```\n\n**Errback signature:** `(request, exc, traceback)`\n\n**Execution:** Synchronous in worker.\n\n**See templates/error-handling-workflow.py** for comprehensive patterns.\n\n### Complex Workflow Example\n\n```python\nfrom celery import chain, group, chord\n\n# Data processing pipeline\nworkflow = chain(\n    # Stage 1: Fetch and validate\n    fetch_data.s(),\n    validate_data.s(),\n\n    # Stage 2: Parallel processing\n    group([\n        transform_batch.s(i) for i in range(num_batches)\n    ]),\n\n    # Stage 3: Aggregate results\n    chord([\n        aggregate_batch.s(i) for i in range(num_batches)\n    ])(finalize_report.s()),\n\n    # Stage 4: Notify\n    send_notification.si('pipeline_complete')\n)\n\n# Execute with error handling\nresult = workflow.apply_async(\n    link_error=handle_pipeline_error.s()\n)\n```\n\n**See templates/complex-workflow.py** for production-ready implementation.\n\n## Testing Workflows\n\n### Validate Canvas Structure\n\n```bash\n# Check workflow composition\n./scripts/validate-canvas.sh path/to/workflow.py\n\n# Validates:\n# - Result backend enabled\n# - Task ignore_result settings\n# - Proper signature usage\n# - Error callback patterns\n```\n\n### Test Workflow Execution\n\n```bash\n# Run workflow with test data\n./scripts/test-workflow.sh workflow_name\n\n# Options:\n# --dry-run    : Validate without execution\n# --verbose    : Show detailed task flow\n# --timeout 30 : Set execution timeout\n```\n\n## Best Practices\n\n**DO:**\n- Enable result backend for groups/chords\n- Use `.si()` for independent tasks in chains\n- Implement error callbacks for critical workflows\n- Set explicit timeouts for long-running workflows\n- Use chunks for large data processing\n- Add metadata with stamping API (5.3+)\n- Make error callbacks idempotent\n\n**DON'T:**\n- Have tasks wait synchronously for other tasks\n- Ignore results for tasks in groups\n- Forget to call `super()` in `after_return()` overrides\n- Use chords without Redis 2.2+\n- Create deeply nested workflows (>3 levels)\n- Mix synchronous and async task calls\n\n## Configuration Requirements\n\n**Celery Config:**\n```python\n# Required for canvas\nresult_backend = 'redis://localhost:6379/0'\nresult_extended = True  # Store task args/kwargs\n\n# Recommended\ntask_track_started = True\nresult_expires = 3600  # 1 hour\n\n# For large workflows\nworker_prefetch_multiplier = 1\ntask_acks_late = True\n```\n\n## Debugging\n\n**Visualize workflow:**\n```python\nfrom celery import DependencyGraph\n\ngraph = workflow.__graph__()\nwith open('workflow.dot', 'w') as f:\n    f.write(graph.to_dot())\n\n# Convert to image:\n# dot -Tpng workflow.dot -o workflow.png\n```\n\n**Monitor execution:**\n```python\nresult = workflow.apply_async()\nprint(f\"Task ID: {result.id}\")\nprint(f\"Status: {result.status}\")\nprint(f\"Children: {result.children}\")\n```\n\n## Resources\n\n**Templates:**\n- `chain-workflow.py` - Sequential task patterns\n- `group-parallel.py` - Parallel execution patterns\n- `chord-pattern.py` - Synchronization patterns\n- `complex-workflow.py` - Multi-stage pipelines\n- `error-handling-workflow.py` - Error callback patterns\n- `nested-workflows.py` - Advanced composition\n\n**Scripts:**\n- `test-workflow.sh` - Execute and validate workflows\n- `validate-canvas.sh` - Static analysis of canvas patterns\n- `generate-workflow.sh` - Generate workflows from templates\n\n**Examples:**\n- `examples/chain-example.md` - Real-world chain scenarios\n- `examples/group-example.md` - Parallel processing use cases\n- `examples/chord-example.md` - Synchronization patterns\n- `examples/complex-workflows.md` - Production workflow architectures\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n\n---\n\n**Version:** 1.0.0\n**Celery Compatibility:** 5.0+\n**Required Backend:** Redis 2.2+ or compatible result backend"
              }
            ]
          },
          {
            "name": "clerk",
            "description": "Comprehensive Clerk authentication plugin with OAuth (19 providers), organizations, billing, MFA, Next.js integration, and Supabase sync for building secure multi-tenant applications",
            "source": "./plugins/clerk",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install clerk@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-auth",
                "description": "Add authentication to pages/routes - configure sign-in/sign-up flows and protected routes",
                "path": "plugins/clerk/commands/add-auth.md",
                "frontmatter": {
                  "description": "Add authentication to pages/routes - configure sign-in/sign-up flows and protected routes",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, AskUserQuestion, Bash, Glob"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Add Clerk authentication flows and protected routes to the application\n\nCore Principles:\n- Detect framework before implementing\n- Ask about authentication requirements upfront\n- Use placeholders for all API keys\n- Implement complete flows with error handling\n\nPhase 1: Discovery\nGoal: Understand project structure and authentication needs\n\nActions:\n- Detect framework and project type\n- Check for existing authentication setup\n- Load package.json to understand dependencies\n- Example: !{bash test -f package.json && cat package.json | grep -E \"(next|react|remix)\" || echo \"No package.json found\"}\n\nPhase 2: Requirements Gathering\nGoal: Clarify authentication requirements\n\nActions:\n- If $ARGUMENTS does not specify requirements, use AskUserQuestion to gather:\n  - Which authentication methods are needed? (email/password, social OAuth, phone, passwordless)\n  - Which social providers? (Google, GitHub, Microsoft, etc.)\n  - Do you need multi-factor authentication (MFA)?\n  - Do you need organization/team features?\n  - Which routes should be protected vs public?\n- Summarize requirements and confirm with user\n\nPhase 3: Framework Detection\nGoal: Identify framework and structure\n\nActions:\n- Read package.json to determine framework\n- Check for Next.js App Router vs Pages Router\n- Identify existing middleware or provider files\n- Find where auth components should be placed\n- Example: !{bash test -d app && echo \"App Router\" || test -d pages && echo \"Pages Router\" || echo \"React\"}\n\nPhase 4: Implementation\nGoal: Implement authentication with clerk-auth-builder agent\n\nActions:\n\nTask(description=\"Implement Clerk authentication\", subagent_type=\"clerk:clerk-auth-builder\", prompt=\"You are the clerk-auth-builder agent. Implement Clerk authentication for this project.\n\nRequirements from user: $ARGUMENTS\n\nFramework detected: [framework from Phase 3]\n\nAuthentication methods needed:\n- [Methods from Phase 2]\n\nSocial providers:\n- [Providers from Phase 2]\n\nAdditional features:\n- MFA: [Yes/No from Phase 2]\n- Organizations: [Yes/No from Phase 2]\n\nProtected routes: [Routes from Phase 2]\n\nDeliverables:\n1. Install required Clerk packages\n2. Create/update environment files (.env.example with placeholders)\n3. Set up ClerkProvider wrapper\n4. Create authentication components (SignIn, SignUp, UserButton)\n5. Configure middleware for route protection\n6. Implement requested authentication methods\n7. Add error handling and loading states\n8. Generate TypeScript types if applicable\n9. Document Clerk dashboard configuration steps\")\n\nPhase 5: Verification\nGoal: Validate authentication implementation\n\nActions:\n- Check that environment files use placeholders only\n- Verify TypeScript compilation (if TypeScript project)\n- Example: !{bash test -f tsconfig.json && npx tsc --noEmit || echo \"Not a TypeScript project\"}\n- Confirm Clerk components were created\n- Verify middleware file exists for route protection\n- Check that .env.example exists with proper placeholders\n\nPhase 6: Summary\nGoal: Document what was implemented\n\nActions:\n- List files created or modified\n- Highlight authentication methods configured\n- Show which routes are now protected\n- Provide next steps:\n  - Sign up for Clerk account at https://clerk.com\n  - Create application in Clerk dashboard\n  - Configure authentication providers\n  - Copy publishable and secret keys\n  - Update .env.local with real keys\n  - Test authentication flows"
              },
              {
                "name": "/add-billing",
                "description": "Integrate Clerk Billing for subscriptions, pricing plans, and payment webhooks",
                "path": "plugins/clerk/commands/add-billing.md",
                "frontmatter": {
                  "description": "Integrate Clerk Billing for subscriptions, pricing plans, and payment webhooks",
                  "argument-hint": "none",
                  "allowed-tools": "Task, AskUserQuestion, Read"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Clerk Billing with subscription management, pricing plans, and payment webhooks\n\nCore Principles:\n- Ask about pricing model before implementation\n- Follow Clerk Billing best practices\n- Integrate with existing authentication setup\n- Ensure webhook security and validation\n\nPhase 1: Discovery\nGoal: Understand project structure and billing requirements\n\nActions:\n- Check for existing Clerk configuration\n- Identify framework (Next.js, React, etc.)\n- Load relevant configuration files\n- Example: @package.json\n\nPhase 2: Requirements Gathering\nGoal: Understand billing needs and pricing model\n\nActions:\n\nUse AskUserQuestion to gather:\n\nQuestion 1: \"What pricing model do you want to implement?\"\nOptions:\n- \"Subscription-based\" - Monthly/yearly recurring billing\n- \"Usage-based\" - Pay-per-use or metered billing\n- \"Hybrid\" - Combination of subscription + usage\n- \"One-time\" - Single payment purchases\n\nQuestion 2: \"Which payment provider are you using?\"\nOptions:\n- \"Stripe\" - Most common, full feature set\n- \"Other\" - Specify your provider\n\nQuestion 3: \"Do you need webhook handling?\"\nOptions:\n- \"Yes\" - Handle subscription events, payment updates\n- \"No\" - Just basic billing integration\n\nPhase 3: Implementation\nGoal: Integrate Clerk Billing with agent\n\nActions:\n\nTask(description=\"Integrate Clerk Billing\", subagent_type=\"clerk:clerk-billing-integrator\", prompt=\"You are the clerk-billing-integrator agent. Integrate Clerk Billing for this project.\n\nPricing Model: [Answer from Question 1]\nPayment Provider: [Answer from Question 2]\nWebhook Handling: [Answer from Question 3]\n\nRequirements:\n- Set up Clerk Billing configuration\n- Implement pricing plans and subscription logic\n- Configure payment webhooks if requested\n- Add billing UI components (pricing page, subscription management)\n- Implement subscription check middleware/guards\n- Set up webhook endpoint for payment events\n- Add proper error handling and validation\n- Follow security best practices for payment data\n- Integrate with existing Clerk authentication\n\nExpected output:\n- Billing configuration files\n- Pricing plan definitions\n- Subscription management components\n- Webhook handlers (if enabled)\n- Middleware for subscription checks\n- Documentation for setup and usage\")\n\nPhase 4: Summary\nGoal: Confirm integration and provide next steps\n\nActions:\n- Summarize what was integrated\n- List files created/modified\n- Provide setup instructions:\n  - Environment variables needed\n  - Clerk Dashboard configuration steps\n  - Payment provider setup (Stripe, etc.)\n- Highlight key features:\n  - Pricing plans configured\n  - Subscription management UI\n  - Webhook endpoints (if enabled)\n- Suggest next steps:\n  - Test subscription flows\n  - Configure production payment provider\n  - Set up webhook endpoints in Clerk Dashboard"
              },
              {
                "name": "/add-mfa",
                "description": "Configure multi-factor authentication with TOTP/SMS and backup codes",
                "path": "plugins/clerk/commands/add-mfa.md",
                "frontmatter": {
                  "description": "Configure multi-factor authentication with TOTP/SMS and backup codes",
                  "argument-hint": "none",
                  "allowed-tools": "Task, AskUserQuestion, Read"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure multi-factor authentication (MFA) with TOTP, SMS, and backup codes\n\nCore Principles:\n- Ask about MFA methods before implementation\n- Follow Clerk MFA best practices\n- Integrate with existing authentication setup\n- Ensure secure session handling\n\nPhase 1: Discovery\nGoal: Understand project structure and existing authentication\n\nActions:\n- Check for existing Clerk configuration\n- Identify framework (Next.js, React, etc.)\n- Load relevant configuration files\n- Example: @package.json\n- Verify Clerk packages are installed\n\nPhase 2: Requirements Gathering\nGoal: Understand MFA needs and preferences\n\nActions:\n\nUse AskUserQuestion to gather:\n\nQuestion 1: \"Which MFA methods do you want to support?\"\nOptions:\n- \"TOTP only\" - Time-based one-time passwords (authenticator apps)\n- \"SMS only\" - SMS-based verification codes\n- \"Both TOTP and SMS\" - Give users choice of methods\n- \"TOTP with SMS fallback\" - Preferred method with backup\n\nQuestion 2: \"Should MFA be optional or enforced?\"\nOptions:\n- \"Optional\" - Users can choose to enable MFA\n- \"Enforced for all users\" - Required for account security\n- \"Enforced for admin users only\" - Role-based requirement\n\nQuestion 3: \"Do you need backup codes for account recovery?\"\nOptions:\n- \"Yes\" - Generate recovery codes for MFA bypass\n- \"No\" - Skip backup codes feature\n\nQuestion 4: \"What UI framework are you using?\"\nOptions:\n- \"shadcn/ui\" - Use shadcn/ui components\n- \"Custom CSS\" - Build custom styled components\n- \"Tailwind CSS\" - Use Tailwind utility classes\n- \"Other\" - Specify your framework\n\nPhase 3: Implementation\nGoal: Configure MFA with specialist agent\n\nActions:\n\nTask(description=\"Configure MFA\", subagent_type=\"clerk:clerk-mfa-specialist\", prompt=\"You are the clerk-mfa-specialist agent. Configure multi-factor authentication for this project.\n\nMFA Methods: [Answer from Question 1]\nEnforcement Policy: [Answer from Question 2]\nBackup Codes: [Answer from Question 3]\nUI Framework: [Answer from Question 4]\n\nRequirements:\n- Set up TOTP authentication if requested (QR codes, manual entry)\n- Configure SMS authentication if requested (phone verification)\n- Implement backup code generation and storage if requested\n- Create MFA enrollment UI components\n- Build MFA verification flow components\n- Add MFA settings management page\n- Implement session security with MFA\n- Configure enforcement policies based on requirements\n- Add proper error handling and validation\n- Follow Clerk MFA best practices\n- Integrate with existing Clerk authentication\n\nExpected output:\n- MFA configuration files\n- TOTP/SMS setup components\n- MFA verification components\n- Settings management UI\n- Backup code generation and display\n- Documentation for setup and usage\")\n\nPhase 4: Summary\nGoal: Confirm integration and provide next steps\n\nActions:\n- Summarize what was configured\n- List files created/modified\n- Provide setup instructions:\n  - Environment variables needed (if any)\n  - Clerk Dashboard configuration steps\n  - MFA provider setup (SMS service, etc.)\n- Highlight key features:\n  - MFA methods enabled\n  - Enforcement policy configured\n  - UI components created\n  - Backup codes (if enabled)\n- Suggest next steps:\n  - Test MFA enrollment flow\n  - Test MFA verification flow\n  - Test backup code recovery (if enabled)\n  - Configure production SMS provider (if using SMS)\n  - Review security settings in Clerk Dashboard"
              },
              {
                "name": "/add-oauth",
                "description": "Configure OAuth providers - setup redirect URLs and test OAuth flows",
                "path": "plugins/clerk/commands/add-oauth.md",
                "frontmatter": {
                  "description": "Configure OAuth providers - setup redirect URLs and test OAuth flows",
                  "argument-hint": [
                    "provider-names"
                  ],
                  "allowed-tools": "Task, AskUserQuestion, Read, Write, Bash, Glob, Grep"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Configure OAuth authentication providers in Clerk with redirect URLs and test flows\n\nCore Principles:\n- Ask for provider selection if not specified\n- Detect existing Clerk configuration\n- Configure providers with proper redirect URLs\n- Test OAuth flows after setup\n\nPhase 1: Discovery\nGoal: Gather provider requirements and detect project setup\n\nActions:\n- Parse $ARGUMENTS for provider names (e.g., \"google github microsoft\")\n- If no providers specified, use AskUserQuestion to gather:\n  - Which OAuth providers to configure? (Google, GitHub, Microsoft, etc.)\n  - What environments? (development, production)\n  - Any custom redirect URLs needed?\n- Detect project type and framework:\n  - !{bash ls package.json next.config.js app/ pages/ 2>/dev/null}\n- Load existing Clerk configuration if present:\n  - @.env.local\n  - @middleware.ts or @middleware.js\n  - @app/layout.tsx or @pages/_app.tsx\n\nPhase 2: Context Analysis\nGoal: Understand current authentication setup\n\nActions:\n- Read Clerk environment configuration\n- Identify configured authentication methods\n- Check for existing OAuth provider setup\n- Detect application routes and redirect patterns\n- Example: !{bash grep -r \"ClerkProvider\\|SignIn\\|SignUp\" app/ pages/ src/ 2>/dev/null | head -20}\n\nPhase 3: Planning\nGoal: Design OAuth provider configuration approach\n\nActions:\n- List providers to configure based on $ARGUMENTS or user input\n- Determine redirect URLs based on detected framework:\n  - Next.js App Router: /api/auth/callback\n  - Next.js Pages Router: /api/auth/callback\n  - Standard React: /auth/callback\n- Plan environment variable updates\n- Identify Clerk Dashboard configuration steps\n- Present configuration plan to user\n\nPhase 4: Implementation\nGoal: Configure OAuth providers with Clerk specialist\n\nActions:\n\nTask(description=\"Configure OAuth providers\", subagent_type=\"clerk:clerk-oauth-specialist\", prompt=\"You are the clerk-oauth-specialist agent. Configure OAuth authentication providers for this application.\n\nProviders to configure: $ARGUMENTS (or from user input)\n\nContext from detection:\n- Project framework and structure\n- Existing Clerk configuration\n- Current authentication setup\n- Application routes\n\nRequirements:\n- Configure each provider in Clerk Dashboard (provide step-by-step)\n- Set up redirect URLs based on detected framework\n- Update environment variables with provider credentials\n- Configure OAuth scopes appropriately\n- Add provider buttons to sign-in/sign-up components\n- Test OAuth flow for each provider\n\nExpected output:\n- Environment variable updates (.env.local with placeholders)\n- Component code changes for provider buttons\n- Clerk Dashboard configuration steps\n- Redirect URL configuration\n- Testing instructions for each provider\n- Security best practices checklist\")\n\nPhase 5: Verification\nGoal: Validate OAuth provider configuration\n\nActions:\n- Review generated environment variables (ensure placeholders used)\n- Check that .env.local is in .gitignore\n- Verify component updates follow framework patterns\n- Confirm redirect URLs match detected framework\n- Example: !{bash grep \"CLERK_\" .env.local 2>/dev/null}\n\nPhase 6: Summary\nGoal: Document OAuth provider setup completion\n\nActions:\n- List configured providers\n- Display environment variables to set (with placeholders)\n- Show Clerk Dashboard configuration steps\n- Provide testing instructions\n- Next steps:\n  - Add actual OAuth credentials from provider consoles\n  - Test each OAuth flow in development\n  - Configure production redirect URLs\n  - Review OAuth scopes and permissions"
              },
              {
                "name": "/add-organizations",
                "description": "Enable multi-tenant organizations with RBAC and organization switching",
                "path": "plugins/clerk/commands/add-organizations.md",
                "frontmatter": {
                  "description": "Enable multi-tenant organizations with RBAC and organization switching",
                  "argument-hint": "none",
                  "allowed-tools": "Task, AskUserQuestion, Read, Bash, Glob"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Enable Clerk Organizations with role-based access control (RBAC) and organization switching for multi-tenant applications.\n\nCore Principles:\n- Ask about RBAC requirements before implementation\n- Detect existing project structure\n- Follow Clerk best practices for organizations\n- Provide complete organization management\n\nPhase 1: Discovery\nGoal: Understand project structure and organization requirements\n\nActions:\n- Detect project framework and structure\n- Check for existing Clerk setup\n- Example: !{bash ls -la app/ src/ pages/ 2>/dev/null | head -20}\n- Verify Clerk is already configured\n- Example: !{bash grep -r \"ClerkProvider\" . --include=\"*.tsx\" --include=\"*.ts\" 2>/dev/null | head -5}\n\nPhase 2: Requirements Gathering\nGoal: Understand RBAC and organization needs\n\nActions:\n\nUse AskUserQuestion to gather:\n\n1. **RBAC Requirements**:\n   - What roles do you need? (e.g., admin, member, billing)\n   - What permissions per role? (e.g., admin can invite, member read-only)\n   - Any custom permissions beyond standard roles?\n\n2. **Organization Features**:\n   - Enable organization switching UI?\n   - Enable member invitation flows?\n   - Enable role management UI?\n   - Enable organization settings page?\n\n3. **Integration Points**:\n   - Database backend? (Supabase, Prisma, Drizzle)\n   - API routes need organization context?\n   - Any organization-specific data models?\n\nPhase 3: Validation\nGoal: Verify Clerk setup exists\n\nActions:\n- Read Clerk configuration files\n- Verify environment variables are set\n- Example: !{bash grep \"NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY\" .env.local .env 2>/dev/null}\n- Check middleware configuration\n- Example: @middleware.ts\n\nPhase 4: Implementation\nGoal: Enable organizations with RBAC\n\nActions:\n\nTask(description=\"Enable Clerk organizations with RBAC\", subagent_type=\"clerk:clerk-organization-builder\", prompt=\"You are the clerk-organization-builder agent. Enable Clerk Organizations with RBAC for this project based on the requirements gathered.\n\nRequirements from user:\n- Roles: [roles specified]\n- Permissions: [permissions specified]\n- Features: [features requested]\n- Database: [database backend if specified]\n- API integration: [if needed]\n\nProject context:\n- Framework: [detected framework]\n- Existing Clerk setup: [configuration found]\n\nImplementation scope:\n1. Enable organizations in Clerk dashboard settings\n2. Update ClerkProvider with organization props\n3. Create organization UI components:\n   - Organization switcher\n   - Member management (if requested)\n   - Role assignment UI (if requested)\n   - Organization settings (if requested)\n4. Add organization middleware and guards\n5. Implement RBAC helpers and hooks\n6. Add API route protection with org context\n7. Database integration (if specified)\n8. Add example organization flows\n\nExpected output:\n- Complete organization implementation\n- RBAC utilities and hooks\n- UI components for org management\n- Documentation for organization features\n- Example usage in key areas\")\n\nPhase 5: Verification\nGoal: Verify organization setup\n\nActions:\n- Check that organization components exist\n- Example: !{bash ls -la components/organizations/ app/organizations/ 2>/dev/null}\n- Verify RBAC utilities are present\n- Example: !{bash grep -r \"useOrganization\\|useRole\" . --include=\"*.tsx\" --include=\"*.ts\" 2>/dev/null | head -5}\n- Run type checking if TypeScript\n- Example: !{bash npm run typecheck 2>/dev/null || npx tsc --noEmit 2>/dev/null || echo \"No TypeScript check available\"}\n\nPhase 6: Summary\nGoal: Document organization setup\n\nActions:\n- Summarize changes made:\n  - Organization components created\n  - RBAC implementation details\n  - Roles and permissions configured\n  - Database integration (if added)\n- Next steps:\n  - Configure organization settings in Clerk dashboard\n  - Test organization creation flow\n  - Test role assignment and permissions\n  - Set up production organization domains (if needed)\n- Documentation references:\n  - Clerk Organizations: https://clerk.com/docs/organizations/overview\n  - RBAC Guide: https://clerk.com/docs/organizations/roles-permissions"
              },
              {
                "name": "/customize-ui",
                "description": "Customize Clerk UI appearance and branding - themes, localization, email templates",
                "path": "plugins/clerk/commands/customize-ui.md",
                "frontmatter": {
                  "description": "Customize Clerk UI appearance and branding - themes, localization, email templates",
                  "argument-hint": "none",
                  "allowed-tools": "Task, AskUserQuestion, Read"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Customize Clerk UI components, themes, localization, and email templates\n\nCore Principles:\n- Understand customization goals before implementing\n- Ask when requirements are unclear\n- Follow Clerk's theming and localization best practices\n- Validate customizations with proper testing\n\nPhase 1: Discovery\nGoal: Understand customization requirements\n\nActions:\n- If $ARGUMENTS is unclear or empty, use AskUserQuestion to gather:\n  - What aspects need customization? (themes, localization, email templates, components)\n  - Are you using Clerk Components or custom components?\n  - What branding requirements do you have?\n  - Do you need dark mode support?\n  - Which languages need to be supported?\n- Load Clerk configuration files for context\n- Example: @src/app/layout.tsx or @app/ClerkProvider\n\nPhase 2: Analysis\nGoal: Understand current Clerk setup\n\nActions:\n- Read Clerk provider configuration to understand current setup\n- Check for existing theme customizations\n- Identify Clerk components in use (SignIn, SignUp, UserButton, etc.)\n- Look for existing localization configuration\n- Example: !{bash find src -name \"*clerk*\" -o -name \"*sign-in*\" -o -name \"*sign-up*\" | head -10}\n\nPhase 3: Planning\nGoal: Design customization approach\n\nActions:\n- Determine which customization areas to focus on:\n  - Theme variables (colors, fonts, borders, radii)\n  - Component appearance (buttons, inputs, layouts)\n  - Localization strings\n  - Email templates\n- Identify files that need to be created or modified\n- Confirm approach with user if significant changes required\n\nPhase 4: Implementation\nGoal: Apply customizations with clerk-ui-customizer agent\n\nActions:\n\nTask(description=\"Customize Clerk UI\", subagent_type=\"clerk:clerk-ui-customizer\", prompt=\"You are the clerk-ui-customizer agent. Customize Clerk UI for $ARGUMENTS.\n\nContext: User wants to customize Clerk's appearance and branding\n\nCustomization Requirements:\n- Theme customization (colors, fonts, spacing)\n- Component styling (buttons, inputs, cards)\n- Localization setup (if multi-language support needed)\n- Email template customization (if needed)\n- Dark mode support (if required)\n\nCurrent Setup:\n- Clerk provider configuration detected in project\n- Existing Clerk components identified\n\nExpected Output:\n- Appearance prop configuration in ClerkProvider\n- Theme variables defined\n- Localization configuration (if needed)\n- Custom CSS or styling (if needed)\n- Email template customizations (if applicable)\n- Dark mode implementation (if required)\n- Documentation of customization approach\")\n\nPhase 5: Verification\nGoal: Verify customizations work correctly\n\nActions:\n- Review implemented customizations\n- Check that theme variables are properly applied\n- Verify localization files are correctly configured\n- Ensure email templates follow Clerk's requirements\n- Validate dark mode toggle works (if implemented)\n- Example: !{bash npm run typecheck || echo \"No typecheck available\"}\n\nPhase 6: Summary\nGoal: Document customizations and next steps\n\nActions:\n- Summarize customizations applied:\n  - Theme variables configured\n  - Components styled\n  - Localization languages added\n  - Email templates customized\n- Highlight customization decisions:\n  - Color scheme choices\n  - Font selections\n  - Localization strategy\n- Suggest next steps:\n  - Test in different browsers\n  - Verify email templates in Clerk Dashboard\n  - Add additional language translations\n  - Test dark mode across all components"
              },
              {
                "name": "/init",
                "description": "Initialize Clerk authentication in existing project - install SDK, configure environment, setup provider",
                "path": "plugins/clerk/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Clerk authentication in existing project - install SDK, configure environment, setup provider",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, AskUserQuestion, Bash"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\n## Security: API Key Handling\n\n**CRITICAL:** Read comprehensive security rules:\n\n@docs/security/SECURITY-RULES.md\n\n**Never hardcode API keys, passwords, or secrets in any generated files.**\n\nWhen generating configuration or code:\n- ‚ùå NEVER use real API keys or credentials\n- ‚úÖ ALWAYS use placeholders: `your_clerk_key_here`\n- ‚úÖ Format: `clerk_{env}_your_key_here` for multi-environment\n- ‚úÖ Read from environment variables in code\n- ‚úÖ Add `.env*` to `.gitignore` (except `.env.example`)\n- ‚úÖ Document how to obtain real keys from Clerk Dashboard\n\nGoal: Initialize Clerk authentication in an existing project with complete SDK installation, environment configuration, and provider setup using placeholder credentials.\n\nCore Principles:\n- Detect framework before installation (Next.js, React, Node.js)\n- Ask user for provider preferences early\n- Delegate complete setup to specialized agent\n- Verify installation and environment security\n\nPhase 1: Framework Detection\nGoal: Identify project framework and existing configuration\n\nActions:\n- Check if package.json exists to determine project type\n- Example: !{bash test -f package.json && echo \"Node.js project detected\" || echo \"No package.json found\"}\n- Read package.json to detect framework:\n  - Next.js (has \"next\" dependency)\n  - React (has \"react\" without \"next\")\n  - Node.js backend (has \"express\" or neither)\n- Check if Clerk is already installed\n- Example: !{bash grep -q \"@clerk\" package.json 2>/dev/null && echo \"Clerk found\" || echo \"Clerk not installed\"}\n\nPhase 2: Gather Requirements\nGoal: Understand user's authentication needs and preferences\n\nActions:\n- If framework is unclear or unsupported, use AskUserQuestion to confirm:\n  - What framework/stack are you using?\n  - Is this Next.js App Router, Pages Router, React SPA, or Node.js?\n- Ask about authentication provider preferences:\n  - Which providers do you want? (Google, GitHub, Email, etc.)\n  - Do you need middleware for protected routes?\n  - Do you need custom sign-in/sign-up pages?\n- Confirm environment setup approach:\n  - Development mode (test keys) or production mode?\n  - Multi-environment setup needed?\n\nPhase 3: Setup Execution\nGoal: Initialize Clerk with complete SDK installation and configuration\n\nActions:\n\nTask(description=\"Initialize Clerk authentication\", subagent_type=\"clerk:clerk-setup-agent\", prompt=\"You are the clerk-setup-agent. Initialize Clerk authentication in this project.\n\nFramework detected: [Framework from Phase 1]\nUser preferences from Phase 2:\n- Authentication providers: [List from Phase 2]\n- Middleware needed: [Yes/No from Phase 2]\n- Custom auth pages: [Yes/No from Phase 2]\n- Environment mode: [Development/Production from Phase 2]\n\nRequirements:\n- Install correct Clerk package based on framework\n- Generate .env.local with placeholder keys (NEVER use real keys)\n- Generate .env.example with same structure\n- Configure ClerkProvider in root component\n- Add middleware if requested (Next.js only)\n- Update .gitignore to exclude .env files (except .env.example)\n- Document how to obtain real Clerk keys from dashboard\n\nExpected deliverables:\n1. Installed Clerk SDK package\n2. .env.local with placeholder values only\n3. .env.example with documentation\n4. ClerkProvider configuration\n5. Middleware setup (if applicable)\n6. Setup documentation explaining key acquisition\n7. Verification that no real keys are hardcoded\n\nSecurity requirement: ALL environment files must use placeholders in format 'clerk_{env}_your_key_here'. NEVER hardcode actual API keys.\")\n\nPhase 4: Verification\nGoal: Confirm successful installation and security compliance\n\nActions:\n- Verify Clerk package appears in package.json dependencies\n- Example: !{bash grep \"@clerk\" package.json}\n- Check .env.local exists with placeholder keys\n- Verify .env.example is created\n- Confirm .gitignore excludes .env files\n- Example: !{bash grep -q \".env*\" .gitignore && echo \"Protected\" || echo \"WARNING: .gitignore missing .env protection\"}\n- Validate no real API keys are present in any files\n- Run TypeScript check if applicable\n- Example: !{bash command -v tsc &>/dev/null && npx tsc --noEmit || echo \"TypeScript not available\"}\n\nPhase 5: Summary\nGoal: Document setup completion and next steps\n\nActions:\n- Summarize what was installed:\n  - Clerk package version\n  - Environment files created\n  - Provider configuration location\n  - Middleware setup (if added)\n- Highlight critical next steps:\n  - Get Clerk keys from https://dashboard.clerk.com\n  - Add real keys to .env.local (NEVER commit this file)\n  - Configure authentication providers in Clerk Dashboard\n  - Test sign-in/sign-up flows\n- Provide quick start commands:\n  - npm run dev (or appropriate dev command)\n  - Navigate to /sign-in to test authentication\n- Security reminder:\n  - NEVER commit .env.local to git\n  - ONLY commit .env.example with placeholders\n  - Keep Clerk secret keys secure and private"
              },
              {
                "name": "/integrate-api",
                "description": "Setup API authentication with JWT middleware and backend SDK",
                "path": "plugins/clerk/commands/integrate-api.md",
                "frontmatter": {
                  "description": "Setup API authentication with JWT middleware and backend SDK",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, AskUserQuestion, Bash, Glob"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Setup backend API authentication with Clerk JWT middleware and SDK integration\n\nCore Principles:\n- Ask about backend framework before proceeding\n- Detect existing backend setup when possible\n- Follow framework-specific patterns for JWT middleware\n- Provide clear integration instructions\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating configuration or integration code:\n\n‚ùå NEVER hardcode actual API keys or secrets\n‚ùå NEVER include real Clerk publishable or secret keys\n\n‚úÖ ALWAYS use placeholders: `clerk_your_publishable_key_here`, `clerk_your_secret_key_here`\n‚úÖ ALWAYS read from environment variables in code\n‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n‚úÖ ALWAYS document where to obtain real keys from Clerk Dashboard\n\nPhase 1: Discovery\nGoal: Understand backend framework and current setup\n\nActions:\n- Check if backend framework can be detected from project files\n- Look for existing backend configuration:\n  - !{bash ls -la package.json pyproject.toml requirements.txt Cargo.toml go.mod 2>/dev/null | head -5}\n- If detected, note framework type (Express, FastAPI, Django, etc.)\n\nPhase 2: Gather Requirements\nGoal: Understand specific integration needs\n\nActions:\n- Use AskUserQuestion to ask about:\n  - Which backend framework? (Express, FastAPI, Django, Flask, Next.js API routes, etc.)\n  - What endpoints need authentication? (All routes, specific endpoints, custom logic)\n  - Session or JWT validation? (stateless JWT recommended)\n  - Any existing auth middleware to replace?\n\nPhase 3: Implementation\nGoal: Setup Clerk authentication in backend\n\nActions:\n\nTask(description=\"Setup backend API authentication\", subagent_type=\"clerk:clerk-api-builder\", prompt=\"You are the clerk-api-builder agent. Setup backend API authentication with Clerk for this project.\n\nBackend Framework: [Framework from user answers]\nEndpoints to Protect: [From user answers]\nAuthentication Type: JWT validation (stateless)\n\nRequirements:\n- Install Clerk backend SDK for the framework\n- Configure JWT verification middleware\n- Add environment variables with PLACEHOLDER values (clerk_your_publishable_key_here, clerk_your_secret_key_here)\n- Protect specified routes/endpoints with auth middleware\n- Add user context to requests (req.auth, request.user, etc.)\n- Provide error handling for invalid/missing tokens\n- Create example protected endpoint showing user data access\n\nFramework-Specific Integration:\n- Express: Use @clerk/express or @clerk/clerk-sdk-node with middleware\n- FastAPI: Use clerk-backend-api with dependency injection\n- Django: Use clerk-backend-api with middleware and decorators\n- Flask: Use clerk-backend-api with decorators\n- Next.js API routes: Use @clerk/nextjs getAuth() helper\n\nSecurity Rules:\n- Use placeholder API keys only\n- Read keys from environment variables\n- Add .env to .gitignore\n- Include .env.example with placeholders\n\nExpected output:\n- Installed SDK packages\n- JWT middleware configured\n- Environment variables setup with placeholders\n- Protected routes/endpoints\n- Example usage showing authenticated requests\n- Documentation on obtaining real keys from Clerk Dashboard\")\n\nPhase 4: Verification\nGoal: Ensure integration is complete and documented\n\nActions:\n- Verify middleware setup is framework-appropriate\n- Check environment variables use placeholders\n- Confirm .gitignore protects .env files\n- Validate example protected endpoint exists\n- Review documentation for clarity\n\nPhase 5: Summary\nGoal: Provide clear next steps\n\nActions:\n- Summarize what was configured:\n  - Backend framework\n  - SDK installed\n  - Middleware setup\n  - Protected endpoints\n- Document how to obtain real API keys from Clerk Dashboard\n- Provide testing instructions for authenticated requests\n- Suggest next steps (frontend integration, custom claims, organizations)"
              },
              {
                "name": "/integrate-supabase",
                "description": "Integrate Clerk with Supabase - user sync, RLS policies, webhook handlers",
                "path": "plugins/clerk/commands/integrate-supabase.md",
                "frontmatter": {
                  "description": "Integrate Clerk with Supabase - user sync, RLS policies, webhook handlers",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, AskUserQuestion, Bash, Glob, Grep"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Clerk authentication with Supabase database for seamless user management, row-level security, and real-time synchronization.\n\nCore Principles:\n- Ask about sync strategy before implementing\n- Understand project context (Supabase setup, table structure)\n- Follow Clerk-Supabase integration best practices\n- Implement secure webhook handling\n- Configure proper RLS policies\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating configuration files or code:\n\n‚ùå NEVER hardcode actual API keys or secrets\n‚ùå NEVER include real credentials in examples\n‚ùå NEVER commit sensitive values to git\n\n‚úÖ ALWAYS use placeholders: `your_clerk_key_here`, `your_supabase_key_here`\n‚úÖ ALWAYS create `.env.example` with placeholders only\n‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n‚úÖ ALWAYS read from environment variables in code\n‚úÖ ALWAYS document where to obtain keys\n\n**Placeholder format:** `clerk_{env}_your_key_here`, `supabase_{env}_your_key_here`\n\nPhase 1: Discovery\nGoal: Understand current setup and integration requirements\n\nActions:\n- Parse $ARGUMENTS for any specific requirements\n- Detect if Supabase is already configured\n- Check for existing Clerk setup\n- Example: !{bash test -f .env && grep -q \"NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY\" .env && echo \"Clerk configured\" || echo \"Clerk not found\"}\n- Example: !{bash test -f .env && grep -q \"NEXT_PUBLIC_SUPABASE_URL\" .env && echo \"Supabase configured\" || echo \"Supabase not found\"}\n\nPhase 2: Requirements Gathering\nGoal: Ask user about sync strategy and integration preferences\n\nActions:\n\nUse AskUserQuestion to gather:\n- What sync strategy do you want?\n  - Real-time sync via webhooks (recommended)\n  - On-demand sync on login\n  - Manual sync\n- What user data should be synced?\n  - Basic profile (email, name)\n  - Full profile with metadata\n  - Custom user fields\n- Do you need RLS policies?\n  - Yes, secure user data per-user\n  - Yes, with team/org isolation\n  - No, public access\n\nPhase 3: Project Analysis\nGoal: Understand existing Supabase schema and tables\n\nActions:\n- List Supabase migration files to understand schema\n- Example: !{bash ls -la supabase/migrations/*.sql 2>/dev/null | head -10}\n- Read package.json to detect framework\n- Example: @package.json\n- Check if user table already exists\n- Identify which tables need RLS policies\n\nPhase 4: Implementation\nGoal: Integrate Clerk with Supabase via agent\n\nActions:\n\nTask(description=\"Integrate Clerk with Supabase\", subagent_type=\"clerk:clerk-supabase-integrator\", prompt=\"You are the clerk-supabase-integrator agent. Integrate Clerk authentication with Supabase for this project.\n\nContext:\n- Sync strategy: [From Phase 2 answers]\n- User data scope: [From Phase 2 answers]\n- RLS requirements: [From Phase 2 answers]\n- Existing setup: [From Phase 1 detection]\n\nRequirements:\n- Create/update user table in Supabase with Clerk user_id\n- Implement sync mechanism based on chosen strategy\n- Set up webhook handlers if real-time sync selected\n- Configure RLS policies if requested\n- Create helper functions for user operations\n- Update environment variables with placeholders\n- Generate comprehensive setup documentation\n\nExpected output:\n- SQL migration files for user table and RLS\n- Webhook handler implementation (if applicable)\n- Environment configuration with placeholders\n- Setup instructions and testing guide\")\n\nPhase 5: Verification\nGoal: Verify integration is properly configured\n\nActions:\n- Check that migration files were created\n- Example: !{bash ls -la supabase/migrations/*.sql | tail -5}\n- Verify environment variables are documented\n- Confirm RLS policies are in place\n- Check webhook endpoint is implemented (if applicable)\n\nPhase 6: Summary\nGoal: Document what was accomplished and next steps\n\nActions:\n- Summarize integration components created:\n  - User table schema and migrations\n  - Sync mechanism (webhooks/on-demand)\n  - RLS policies configured\n  - Helper functions and utilities\n- Display next steps:\n  - Apply Supabase migrations: `supabase db push`\n  - Configure webhook endpoint in Clerk Dashboard\n  - Test user sync flow\n  - Verify RLS policies work correctly\n- Highlight important files:\n  - Migration files location\n  - Webhook handler location\n  - Environment variables to configure\n  - Setup documentation"
              },
              {
                "name": "/integrate-vercel-ai",
                "description": "Setup authentication for Vercel AI SDK applications using Clerk",
                "path": "plugins/clerk/commands/integrate-vercel-ai.md",
                "frontmatter": {
                  "description": "Setup authentication for Vercel AI SDK applications using Clerk",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, AskUserQuestion, Bash, Glob, Grep"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Integrate Clerk authentication with Vercel AI SDK applications to protect AI routes and provide user context.\n\nCore Principles:\n- Detect existing setup before making changes\n- Protect all AI routes with authentication\n- Provide user context to AI interactions\n- Use placeholders for all API keys\n- Follow Next.js App Router patterns\n\nPhase 1: Discovery\nGoal: Understand the current project setup and requirements\n\nActions:\n- Check if project uses Vercel AI SDK and Clerk\n- Load package.json to verify dependencies: @package.json\n- Detect framework version: !{bash cat package.json | grep -E '\"next\"|\"@clerk/nextjs\"|\"ai\"' || echo \"Dependencies not found\"}\n- Check for existing middleware: !{bash test -f middleware.ts && echo \"Found\" || echo \"Not found\"}\n- Ask user about integration requirements if unclear:\n  - Which AI provider are you using (OpenAI, Anthropic, other)?\n  - Do you need role-based access for AI features?\n  - Should AI conversations be stored per user?\n\nPhase 2: Analysis\nGoal: Assess current authentication and AI SDK configuration\n\nActions:\n- Find all AI routes that need protection: !{bash find app/api -name \"route.ts\" 2>/dev/null | head -20 || echo \"No routes found\"}\n- Search for existing AI SDK usage\n- Check for Clerk provider setup\n- Identify authentication gaps in AI endpoints\n- Verify environment configuration\n\nPhase 3: Planning\nGoal: Design the authentication integration approach\n\nActions:\n- Outline required changes:\n  - Middleware configuration for AI route protection\n  - Authentication helpers for route handlers\n  - User context injection into AI calls\n  - Error handling for unauthenticated requests\n- Identify files to create or modify:\n  - middleware.ts (route protection)\n  - app/api/chat/route.ts (auth wrapper)\n  - lib/auth.ts (helper utilities)\n  - .env.example (Clerk key placeholders)\n- Confirm approach with user before proceeding\n\nPhase 4: Implementation\nGoal: Execute Vercel AI SDK + Clerk integration\n\nActions:\n\nTask(description=\"Integrate Clerk with Vercel AI SDK\", subagent_type=\"clerk:clerk-vercel-ai-integrator\", prompt=\"You are the clerk-vercel-ai-integrator agent. Integrate Clerk authentication with Vercel AI SDK for this project.\n\nContext: This project uses Vercel AI SDK and needs Clerk authentication integration.\n\nRequirements:\n- Protect all AI routes (app/api/chat, etc.) with Clerk middleware\n- Add auth() checks in AI route handlers\n- Pass authenticated user context to AI providers\n- Handle streaming responses with proper auth error handling\n- Implement getAuthenticatedUserId() helper function\n- Set up environment variables with placeholders only\n- Add .env.local to .gitignore if not already present\n- Create .env.example with clear Clerk key placeholders\n\nExpected output:\n- Updated middleware.ts protecting AI routes\n- AI route handlers with authentication checks\n- Helper utilities in lib/auth.ts\n- Environment configuration with placeholders\n- TypeScript compilation passing\n- Complete integration ready for testing\")\n\nPhase 5: Review\nGoal: Verify the integration works correctly\n\nActions:\n- Check TypeScript compilation: !{bash npx tsc --noEmit 2>&1 | head -50 || echo \"Type check complete\"}\n- Verify all AI routes are protected\n- Confirm environment variables use placeholders only\n- Check that .env.local is in .gitignore\n- Validate error handling covers edge cases\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize changes made:\n  - Files created or modified\n  - AI routes now protected\n  - User context integration points\n  - Environment setup completed\n- Highlight next steps:\n  - Add actual Clerk keys to .env.local\n  - Test authentication flow with real users\n  - Configure rate limiting if needed\n  - Set up conversation history storage\n- Provide setup instructions for Clerk Dashboard"
              },
              {
                "name": "/start-guide",
                "description": "Interactive framework selection and setup guide with tailored Clerk configuration",
                "path": "plugins/clerk/commands/start-guide.md",
                "frontmatter": {
                  "description": "Interactive framework selection and setup guide with tailored Clerk configuration",
                  "argument-hint": "none",
                  "allowed-tools": "Task, AskUserQuestion, Read, Bash, Write"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\nGoal: Guide users through complete Clerk setup with interactive questions about framework, features, and integrations, then orchestrate appropriate agents based on selections.\n\nCore Principles:\n- Ask clarifying questions early to understand user needs\n- Tailor setup based on framework and feature selections\n- Orchestrate specialized agents for implementation\n- Provide clear next steps after setup\n\nPhase 1: Framework Selection\nGoal: Determine which framework the user is building with\n\nActions:\n- Use AskUserQuestion to determine framework:\n\nWhich framework are you building with?\n\nOptions:\n1. Next.js App Router (recommended)\n2. Next.js Pages Router\n3. React (Vite/CRA)\n4. Remix\n5. Express.js\n6. Other/Not sure\n\n- Store framework selection for Phase 4\n\nPhase 2: Feature Requirements\nGoal: Understand which Clerk features are needed\n\nActions:\n- Use AskUserQuestion to determine features (allow multiple selections):\n\nWhich Clerk features do you need? (Select all that apply)\n\nOptions:\n1. OAuth Providers (Google, GitHub, etc.)\n2. Organizations (multi-tenant)\n3. Billing Integration\n4. Multi-factor Authentication (MFA)\n5. Custom User Fields\n6. Session Management\n7. Webhooks\n\n- Store feature selections for Phase 5\n\nPhase 3: Integration Requirements\nGoal: Identify third-party integrations needed\n\nActions:\n- Use AskUserQuestion to determine integrations:\n\nWhich integrations do you need with Clerk?\n\nOptions:\n1. Supabase (database + RLS)\n2. Vercel AI SDK (chat history, user context)\n3. Sentry (error tracking with user context)\n4. None\n\n- Store integration selections for Phase 6\n\nPhase 4: Framework Setup\nGoal: Set up Clerk for the selected framework\n\nActions:\n\nBased on framework selection from Phase 1, invoke the appropriate setup agent:\n\nTask(description=\"Set up Clerk for selected framework\", subagent_type=\"clerk-setup-agent\", prompt=\"You are the clerk-setup-agent. Set up Clerk authentication for the framework selected in Phase 1.\n\nFramework: [Insert framework from Phase 1]\n\nSetup requirements:\n- Install Clerk SDK packages\n- Configure environment variables (.env.example)\n- Set up middleware/providers based on framework\n- Create sign-in and sign-up pages\n- Add protected route examples\n\nDeliverable: Complete framework-specific Clerk setup with example pages\")\n\nWait for agent to complete setup.\n\nPhase 5: Feature Implementation\nGoal: Implement selected Clerk features\n\nActions:\n\nBased on feature selections from Phase 2, invoke feature-specific agents:\n\nFor OAuth Providers:\nTask(description=\"Configure OAuth providers\", subagent_type=\"clerk-oauth-specialist\", prompt=\"Configure OAuth providers (Google, GitHub, etc.) for Clerk. Create provider setup documentation and .env variables.\")\n\nFor Organizations:\nTask(description=\"Add organization support\", subagent_type=\"clerk-organization-builder\", prompt=\"Implement Clerk organizations with role-based access control, organization switching, and member management.\")\n\nFor Billing Integration:\nTask(description=\"Set up billing integration\", subagent_type=\"clerk-billing-integrator\", prompt=\"Integrate Clerk with billing provider (Stripe recommended). Add subscription checks and billing portal.\")\n\nFor MFA:\nTask(description=\"Configure MFA\", subagent_type=\"clerk-mfa-specialist\", prompt=\"Set up multi-factor authentication with SMS and authenticator app support.\")\n\nFor Custom User Fields:\nTask(description=\"Add custom user metadata\", subagent_type=\"clerk-setup-agent\", prompt=\"Configure custom user fields and metadata management in Clerk using user metadata API.\")\n\nFor Session Management:\nTask(description=\"Configure session management\", subagent_type=\"clerk-api-builder\", prompt=\"Set up advanced session management with custom JWT claims and API authentication patterns.\")\n\nFor Webhooks:\nTask(description=\"Set up Clerk webhooks\", subagent_type=\"clerk-api-builder\", prompt=\"Configure Clerk webhooks for user events (user.created, user.updated, etc.) with webhook handler endpoints and signature verification.\")\n\nWait for all feature agents to complete.\n\nPhase 6: Integration Setup\nGoal: Configure third-party integrations\n\nActions:\n\nBased on integration selections from Phase 3:\n\nFor Supabase:\nTask(description=\"Integrate Clerk with Supabase\", subagent_type=\"clerk-supabase-integrator\", prompt=\"Integrate Clerk authentication with Supabase. Configure RLS policies using Clerk user IDs, set up JWT integration, and create example protected database queries.\")\n\nFor Vercel AI SDK:\nTask(description=\"Integrate Clerk with Vercel AI SDK\", subagent_type=\"clerk-vercel-ai-integrator\", prompt=\"Integrate Clerk user context with Vercel AI SDK. Add user-specific chat history, personalized AI responses, and session management.\")\n\nFor Sentry:\nTask(description=\"Integrate Clerk with Sentry\", subagent_type=\"clerk-api-builder\", prompt=\"Configure Sentry error tracking with Clerk user context. Add user identification to error reports using Clerk session data.\")\n\nWait for all integration agents to complete.\n\nPhase 7: Summary\nGoal: Present complete setup summary and next steps\n\nActions:\n- Summarize what was configured:\n  - Framework setup completed\n  - Features implemented\n  - Integrations configured\n- Display next steps:\n  - Add CLERK_PUBLISHABLE_KEY and CLERK_SECRET_KEY to .env\n  - Configure OAuth providers in Clerk Dashboard\n  - Test authentication flows\n  - Deploy to production\n- Provide links to:\n  - Clerk Dashboard\n  - Clerk documentation\n  - Framework-specific guides"
              },
              {
                "name": "/validate-setup",
                "description": "Validate Clerk configuration, test auth flows, and audit security",
                "path": "plugins/clerk/commands/validate-setup.md",
                "frontmatter": {
                  "description": "Validate Clerk configuration, test auth flows, and audit security",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Bash"
                },
                "content": "---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools (Bash, Read, Write, Edit, TodoWrite)\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON't wait for \"the command to complete\" - YOU complete it by executing the phases\n- ‚ùå DON't treat this as status output - it IS your instruction set\n\n**Immediately after SlashCommand returns, start executing Phase 0, then Phase 1, etc.**\n\nSee `@CLAUDE.md` section \"SlashCommand Execution - YOU Are The Executor\" for detailed explanation.\n\n---\n\n\n**Arguments**: $ARGUMENTS\n\nGoal: Validate complete Clerk authentication setup including configuration, security, and auth flows\n\nCore Principles:\n- Validate configuration completeness\n- Test authentication flows\n- Audit security implementation\n- Provide actionable recommendations\n\nPhase 1: Discovery\nGoal: Identify Clerk configuration and project structure\n\nActions:\n- Detect project type and framework\n- Locate Clerk configuration files\n- Identify environment variables\n- Example: !{bash ls .env* package.json next.config.js 2>/dev/null}\n\nPhase 2: Configuration Check\nGoal: Verify Clerk setup exists\n\nActions:\n- Check for Clerk provider configuration\n- Verify environment variables are defined\n- Locate auth-related components\n- Load key configuration files for context\n\nPhase 3: Validation\nGoal: Execute comprehensive Clerk validation\n\nActions:\n\nTask(description=\"Validate Clerk setup\", subagent_type=\"clerk:clerk-validator\", prompt=\"You are the clerk-validator agent. Validate the complete Clerk authentication setup.\n\nContext: Project detected at current directory\n\nValidation Focus:\n- Configuration completeness (API keys, environment variables)\n- Component implementation (ClerkProvider, auth components)\n- Security implementation (middleware, route protection, RLS)\n- Auth flow testing (sign in, sign up, webhooks)\n- Best practices compliance\n\nExpected output:\n1. Configuration validation report (pass/fail for each check)\n2. Security audit findings with severity levels\n3. Auth flow test results\n4. List of issues with specific file locations and line numbers\n5. Actionable recommendations for fixes\n\nDeliverable: Comprehensive validation report with prioritized action items\")\n\nWait for validation to complete.\n\nPhase 4: Summary\nGoal: Present validation results\n\nActions:\n- Display validation report organized by:\n  - CRITICAL issues (security vulnerabilities, broken auth)\n  - WARNINGS (incomplete configuration, missing best practices)\n  - INFO (optimization suggestions, documentation gaps)\n- Highlight specific files and lines that need attention\n- Provide next steps for resolving issues\n- Suggest running specific fix commands if available"
              }
            ],
            "skills": [
              {
                "name": "api-authentication",
                "description": "Backend API authentication patterns with Clerk JWT middleware and route protection. Use when building REST APIs, GraphQL APIs, protecting backend routes, implementing JWT validation, setting up Express middleware, or when user mentions API authentication, backend security, JWT tokens, or protected endpoints.",
                "path": "plugins/clerk/skills/api-authentication/SKILL.md",
                "frontmatter": {
                  "name": "api-authentication",
                  "description": "Backend API authentication patterns with Clerk JWT middleware and route protection. Use when building REST APIs, GraphQL APIs, protecting backend routes, implementing JWT validation, setting up Express middleware, or when user mentions API authentication, backend security, JWT tokens, or protected endpoints.",
                  "allowed-tools": "Read, Write, Bash, Glob, Grep"
                },
                "content": "# api-authentication\n\nBackend API authentication skill for Clerk integration. Provides JWT middleware, route protection patterns, and API client generation for REST and GraphQL backends.\n\n## Instructions\n\n### Phase 1: Understand Requirements\n\n1. Identify backend framework (Express, Fastify, Next.js API routes, etc.)\n2. Determine authentication strategy (JWT validation, session tokens)\n3. Check for existing Clerk configuration\n4. Identify API endpoints to protect\n\n### Phase 2: Setup API Authentication\n\nRun the setup script to configure backend authentication:\n\n```bash\nbash scripts/setup-api-auth.sh <framework> <project-path>\n```\n\n**Supported Frameworks:**\n- `express` - Express.js middleware\n- `fastify` - Fastify decorators\n- `nextjs` - Next.js API route helpers\n- `fastapi` - FastAPI dependencies (Python)\n\n**What it does:**\n- Installs required Clerk SDK packages\n- Creates middleware files from templates\n- Configures environment variables\n- Sets up JWT verification utilities\n- Creates route protection helpers\n\n### Phase 3: Implement Route Protection\n\n**For Express/Node.js backends:**\n\nUse the `api-middleware.ts` template:\n\n```typescript\nimport { requireAuth } from './middleware/clerk-auth'\n\n// Protect individual routes\napp.get('/api/protected', requireAuth, (req, res) => {\n  const userId = req.auth.userId\n  res.json({ message: 'Protected data', userId })\n})\n\n// Protect route groups\napp.use('/api/admin', requireAuth, adminRouter)\n```\n\n**For Next.js API routes:**\n\nUse the `api-routes.ts` template:\n\n```typescript\nimport { withAuth } from '@/lib/clerk-middleware'\n\nexport default withAuth(async (req, res) => {\n  const { userId } = req.auth\n  // Protected route logic\n})\n```\n\n**For GraphQL:**\n\nUse the `graphql-clerk.ts` example:\n\n```typescript\nimport { ClerkExpressRequireAuth } from '@clerk/clerk-sdk-node'\n\nconst server = new ApolloServer({\n  context: ({ req }) => ({\n    userId: req.auth?.userId,\n    user: req.auth?.user\n  })\n})\n\napp.use('/graphql', ClerkExpressRequireAuth(), apolloMiddleware)\n```\n\n### Phase 4: Generate API Client\n\nCreate type-safe API clients with authentication headers:\n\n```bash\nbash scripts/generate-api-client.sh <api-type> <output-path>\n```\n\n**API Types:**\n- `rest` - REST API client with fetch\n- `graphql` - GraphQL client with Apollo\n- `axios` - Axios-based REST client\n- `trpc` - tRPC client with auth context\n\n**Generated Client Features:**\n- Automatic JWT token attachment\n- Token refresh handling\n- Type-safe request methods\n- Error handling for auth failures\n\n### Phase 5: Test Authentication\n\nRun comprehensive authentication tests:\n\n```bash\nbash scripts/test-api-auth.sh <project-path>\n```\n\n**Test Coverage:**\n- ‚úÖ Unauthenticated requests rejected (401)\n- ‚úÖ Valid JWT tokens accepted\n- ‚úÖ Expired tokens refreshed\n- ‚úÖ Invalid tokens rejected\n- ‚úÖ Protected routes accessible with auth\n- ‚úÖ User context available in handlers\n\n## Common Patterns\n\n### Pattern 1: Express Middleware\n\n```typescript\n// middleware/clerk-auth.ts\nimport { ClerkExpressRequireAuth } from '@clerk/clerk-sdk-node'\n\nexport const requireAuth = ClerkExpressRequireAuth({\n  onError: (error) => {\n    console.error('Auth error:', error)\n    return { status: 401, message: 'Unauthorized' }\n  }\n})\n\n// Optional auth (allows both authenticated and anonymous)\nexport const optionalAuth = ClerkExpressWithAuth()\n```\n\n### Pattern 2: Custom JWT Validation\n\n```typescript\n// lib/jwt-verify.ts\nimport { verifyToken } from '@clerk/backend'\n\nexport async function validateJWT(token: string) {\n  try {\n    const payload = await verifyToken(token, {\n      secretKey: process.env.CLERK_SECRET_KEY\n    })\n    return { valid: true, userId: payload.sub }\n  } catch (error) {\n    return { valid: false, error: error.message }\n  }\n}\n```\n\n### Pattern 3: Role-Based Access Control\n\n```typescript\n// middleware/rbac.ts\nexport function requireRole(role: string) {\n  return async (req, res, next) => {\n    const { userId } = req.auth\n    const user = await clerkClient.users.getUser(userId)\n\n    if (user.publicMetadata.role !== role) {\n      return res.status(403).json({ error: 'Forbidden' })\n    }\n    next()\n  }\n}\n\n// Usage\napp.get('/api/admin', requireAuth, requireRole('admin'), handler)\n```\n\n### Pattern 4: GraphQL Context Integration\n\n```typescript\n// graphql/context.ts\nimport { ClerkExpressRequireAuth } from '@clerk/clerk-sdk-node'\n\nexport const context = async ({ req }) => {\n  const userId = req.auth?.userId\n\n  if (!userId) {\n    throw new AuthenticationError('Must be authenticated')\n  }\n\n  const user = await clerkClient.users.getUser(userId)\n\n  return {\n    userId,\n    user,\n    isAdmin: user.publicMetadata.role === 'admin'\n  }\n}\n```\n\n## Environment Variables\n\nRequired environment variables (always use placeholders in committed files):\n\n```bash\n# .env.example\nCLERK_PUBLISHABLE_KEY=your_clerk_publishable_key_here\nCLERK_SECRET_KEY=your_clerk_secret_key_here\n\n# Optional: For webhook verification\nCLERK_WEBHOOK_SECRET=your_webhook_secret_here\n\n# Optional: For custom JWT configuration\nCLERK_JWT_KEY=your_jwt_key_here\n```\n\n## Security Best Practices\n\n1. **Always validate tokens server-side** - Never trust client-side validation alone\n2. **Use HTTPS in production** - JWT tokens must be transmitted securely\n3. **Implement rate limiting** - Prevent brute force attacks on protected endpoints\n4. **Sanitize user inputs** - Validate all data even from authenticated users\n5. **Log authentication events** - Track failed auth attempts and suspicious activity\n6. **Rotate secrets regularly** - Update webhook and JWT secrets periodically\n7. **Use environment variables** - Never hardcode API keys or secrets\n\n## Troubleshooting\n\n**Issue: \"Invalid token\" errors**\n- Verify `CLERK_SECRET_KEY` is correct\n- Check token expiration settings in Clerk dashboard\n- Ensure clock sync between client and server\n\n**Issue: CORS errors on API requests**\n- Configure CORS middleware before Clerk middleware\n- Whitelist your frontend domain in CORS config\n- Include credentials in fetch requests\n\n**Issue: \"Missing userId\" in request context**\n- Verify middleware is applied to route\n- Check that token is sent in Authorization header\n- Ensure middleware order is correct\n\n**Issue: GraphQL authentication not working**\n- Apply Clerk middleware before GraphQL middleware\n- Extract auth from request in context function\n- Check that Apollo Server receives request object\n\n## Requirements\n\n- Clerk account with secret key\n- Backend framework (Express, Fastify, Next.js, etc.)\n- Node.js 16+ or Python 3.8+ (for FastAPI)\n- Environment variables configured\n- HTTPS enabled in production\n\n## Templates Reference\n\n- `templates/api-middleware.ts` - Express/Node.js middleware\n- `templates/api-routes.ts` - Next.js API route helpers\n- `templates/backend-sdk-setup.ts` - Backend SDK initialization\n- `templates/fastapi-middleware.py` - FastAPI authentication dependencies\n\n## Examples Reference\n\n- `examples/rest-api.md` - Complete REST API with authentication\n- `examples/graphql-api.md` - GraphQL server with Clerk context\n- `examples/webhooks.md` - Webhook event handling and processing\n\n## Scripts Reference\n\n- `scripts/setup-api-auth.sh` - Configure backend authentication\n- `scripts/generate-api-client.sh` - Create authenticated API clients\n- `scripts/test-api-auth.sh` - Test authentication flows\n\n---\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n\n**Reference:** `@docs/security/SECURITY-RULES.md`"
              },
              {
                "name": "auth-components",
                "description": "Pre-built and custom Clerk authentication component templates with theming and customization patterns. Use when building authentication UI, creating sign-in/sign-up pages, customizing Clerk components, implementing user buttons, theming auth flows, or when user mentions Clerk components, SignIn, SignUp, UserButton, auth UI, appearance customization, or authentication theming.",
                "path": "plugins/clerk/skills/auth-components/SKILL.md",
                "frontmatter": {
                  "name": "auth-components",
                  "description": "Pre-built and custom Clerk authentication component templates with theming and customization patterns. Use when building authentication UI, creating sign-in/sign-up pages, customizing Clerk components, implementing user buttons, theming auth flows, or when user mentions Clerk components, SignIn, SignUp, UserButton, auth UI, appearance customization, or authentication theming.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Clerk Auth Components Skill\n\nThis skill provides comprehensive templates and patterns for implementing and customizing Clerk authentication components including pre-built components, Clerk Elements for custom flows, and appearance theming.\n\n## Overview\n\nClerk offers two approaches for authentication UI:\n\n1. **Pre-built Components** - Ready-to-use `<SignIn />`, `<SignUp />`, `<UserButton />` with minimal configuration\n2. **Clerk Elements** - Custom components with granular control for advanced use-cases\n\nThis skill covers both approaches with practical templates and customization patterns.\n\n## Available Scripts\n\n### 1. Generate Authentication UI Pages\n\n**Script**: `scripts/generate-auth-ui.sh <output-dir> <component-type>`\n\n**Purpose**: Generates complete authentication page templates\n\n**Component Types**:\n- `signin` - SignIn page with routing\n- `signup` - SignUp page with routing\n- `both` - Both SignIn and SignUp pages\n- `profile` - User profile page\n- `all` - Complete auth UI set\n\n**Usage**:\n```bash\n# Generate sign-in page\n./scripts/generate-auth-ui.sh ./app/sign-in signin\n\n# Generate both sign-in and sign-up\n./scripts/generate-auth-ui.sh ./app signup\n\n# Generate complete auth UI\n./scripts/generate-auth-ui.sh ./app all\n```\n\n**Generated Files**:\n- `app/sign-in/[[...sign-in]]/page.tsx`\n- `app/sign-up/[[...sign-up]]/page.tsx`\n- `app/profile/[[...profile]]/page.tsx`\n- `components/auth/protected-wrapper.tsx`\n\n### 2. Customize Appearance and Theming\n\n**Script**: `scripts/customize-appearance.sh <config-file> <theme-preset>`\n\n**Purpose**: Generates appearance configuration for Clerk components\n\n**Theme Presets**:\n- `default` - Clerk default theme\n- `dark` - Dark mode theme\n- `neobrutalist` - Neobrutalist theme\n- `shadesOfPurple` - Shades of Purple theme\n- `custom` - Custom theme template\n\n**Usage**:\n```bash\n# Generate dark theme config\n./scripts/customize-appearance.sh ./lib/clerk-config.ts dark\n\n# Generate custom theme template\n./scripts/customize-appearance.sh ./lib/clerk-config.ts custom\n\n# Generate theme with custom variables\nBRAND_COLOR=\"#6366f1\" ./scripts/customize-appearance.sh ./lib/clerk-config.ts custom\n```\n\n**Environment Variables**:\n- `BRAND_COLOR` - Primary brand color (hex)\n- `BACKGROUND` - Background color (hex)\n- `TEXT_COLOR` - Text color (hex)\n\n### 3. Validate Component Implementation\n\n**Script**: `scripts/validate-components.sh <project-dir>`\n\n**Purpose**: Validates Clerk component setup and configuration\n\n**Checks**:\n- Clerk dependencies installed (@clerk/nextjs)\n- Environment variables configured\n- ClerkProvider setup in layout\n- Authentication pages exist\n- Middleware configured\n- No hardcoded secrets\n\n**Usage**:\n```bash\n# Validate current project\n./scripts/validate-components.sh .\n\n# Validate specific directory\n./scripts/validate-components.sh /path/to/project\n```\n\n**Exit Codes**:\n- `0`: Validation passed\n- `1`: Validation failed (must fix issues)\n\n## Available Templates\n\n### 1. Sign-In Page Template\n\n**Template**: `templates/sign-in-page.tsx`\n\n**Features**:\n- Next.js App Router integration\n- Catch-all routing `[[...sign-in]]`\n- After sign-in redirect configuration\n- Centered layout with responsive design\n\n**Usage**:\n```typescript\n// app/sign-in/[[...sign-in]]/page.tsx\nimport { SignIn } from '@clerk/nextjs'\n\nexport default function SignInPage() {\n  return (\n    <div className=\"flex min-h-screen items-center justify-center\">\n      <SignIn\n        appearance={{\n          elements: {\n            rootBox: \"mx-auto\",\n            card: \"shadow-lg\"\n          }\n        }}\n        afterSignInUrl=\"/dashboard\"\n      />\n    </div>\n  )\n}\n```\n\n### 2. Sign-Up Page Template\n\n**Template**: `templates/sign-up-page.tsx`\n\n**Features**:\n- Next.js App Router integration\n- Catch-all routing `[[...sign-up]]`\n- After sign-up redirect configuration\n- Custom appearance configuration\n\n### 3. Custom User Button Template\n\n**Template**: `templates/user-button-custom.tsx`\n\n**Features**:\n- Custom menu items\n- Appearance customization\n- Avatar size control\n- Dropdown actions\n\n**Customization Example**:\n```typescript\n<UserButton\n  appearance={{\n    elements: {\n      userButtonAvatarBox: \"w-10 h-10\",\n      userButtonPopoverCard: \"shadow-xl\"\n    }\n  }}\n>\n  <UserButton.MenuItems>\n    <UserButton.Link\n      label=\"Dashboard\"\n      labelIcon={<LayoutDashboard size={16} />}\n      href=\"/dashboard\"\n    />\n    <UserButton.Action\n      label=\"Settings\"\n      labelIcon={<Settings size={16} />}\n      onClick={() => router.push('/settings')}\n    />\n  </UserButton.MenuItems>\n</UserButton>\n```\n\n### 4. Protected Route Wrapper\n\n**Template**: `templates/protected-wrapper.tsx`\n\n**Features**:\n- Authentication guard for routes\n- Loading states\n- Redirect configuration\n- Reusable HOC pattern\n\n**Usage**:\n```typescript\n// app/dashboard/page.tsx\nimport { ProtectedRoute } from '@/components/auth/protected-wrapper'\n\nexport default function DashboardPage() {\n  return (\n    <ProtectedRoute>\n      <div>Protected Dashboard Content</div>\n    </ProtectedRoute>\n  )\n}\n```\n\n## Available Examples\n\n### 1. Custom Sign-In with Clerk Elements\n\n**Example**: `examples/custom-sign-in-guide.md` (code: `examples/custom-sign-in.tsx`)\n\n**Demonstrates**:\n- Clerk Elements for custom sign-in flow\n- Step-based authentication\n- Strategy selection (password, OAuth)\n- Form validation\n- Error handling\n- Custom styling\n\n**Key Components**:\n```typescript\n<SignIn.Root>\n  <SignIn.Step name=\"start\">\n    {/* Email/username input */}\n    <SignIn.Strategy name=\"password\">\n      {/* Password input */}\n    </SignIn.Strategy>\n    <SignIn.Strategy name=\"email_code\">\n      {/* Email verification */}\n    </SignIn.Strategy>\n  </SignIn.Step>\n</SignIn.Root>\n```\n\n### 2. Social Authentication Buttons\n\n**Example**: `examples/social-authentication-guide.md` (code: `examples/social-buttons.tsx`)\n\n**Demonstrates**:\n- OAuth provider buttons\n- Custom social button styling\n- Loading states\n- Error handling\n- Multiple providers (Google, GitHub, Discord)\n\n**Supported Providers**:\n- Google\n- GitHub\n- Discord\n- Microsoft\n- Facebook\n- Apple\n\n### 3. Complete Theme Configuration\n\n**Example**: `examples/theming-guide.md` (code: `examples/theme-config.tsx`)\n\n**Demonstrates**:\n- Complete appearance configuration\n- CSS variables customization\n- Layout configuration\n- Element-specific styling\n- Dark mode support\n- Responsive design\n\n## Appearance Customization Guide\n\n### 1. Appearance Prop Structure\n\nThe `appearance` prop accepts:\n\n```typescript\nappearance={{\n  baseTheme: dark,           // Base theme\n  layout: {                  // Layout options\n    shimmer: true,\n    logoPlacement: 'inside'\n  },\n  variables: {               // CSS variables\n    colorPrimary: '#6366f1',\n    colorBackground: '#ffffff',\n    colorText: '#1f2937',\n    borderRadius: '0.5rem'\n  },\n  elements: {                // Element overrides\n    card: 'shadow-lg',\n    formButtonPrimary: 'bg-blue-500',\n    footerActionLink: 'text-blue-600'\n  }\n}}\n```\n\n### 2. Global vs Component-Level Theming\n\n**Global (ClerkProvider)**:\n```typescript\n<ClerkProvider appearance={{\n  baseTheme: dark,\n  variables: { colorPrimary: '#6366f1' }\n}}>\n  {children}\n</ClerkProvider>\n```\n\n**Component-Level**:\n```typescript\n<SignIn appearance={{\n  elements: {\n    card: 'shadow-xl',\n    rootBox: 'mx-auto'\n  }\n}} />\n```\n\n### 3. Tailwind CSS v4 Integration\n\nFor Tailwind CSS v4 support:\n\n```typescript\n<ClerkProvider\n  appearance={{\n    cssLayerName: 'clerk'  // Ensures Tailwind utilities override\n  }}\n>\n```\n\n### 4. Element Targeting\n\nCommon element selectors:\n\n- `rootBox` - Root container\n- `card` - Main card container\n- `headerTitle` - Header text\n- `formButtonPrimary` - Submit buttons\n- `formFieldInput` - Input fields\n- `footerActionLink` - Footer links\n- `userButtonAvatarBox` - User avatar\n- `userButtonPopoverCard` - Dropdown menu\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n- CLERK_PUBLISHABLE_KEY and CLERK_SECRET_KEY read from environment\n\n## Best Practices\n\n1. **Use Pre-built Components First** - Start with `<SignIn />`, `<SignUp />` before custom Elements\n2. **Apply Global Themes** - Configure appearance at `<ClerkProvider>` level for consistency\n3. **Leverage CSS Variables** - Use `variables` prop for brand colors, spacing, and typography\n4. **Element Overrides for Fine-Tuning** - Use `elements` prop for specific component styling\n5. **Protect Routes** - Always wrap protected content in authentication checks\n6. **Handle Loading States** - Show loading indicators during authentication state checks\n7. **Configure Redirects** - Set `afterSignInUrl` and `afterSignUpUrl` for better UX\n8. **Responsive Design** - Test auth components on mobile and desktop viewports\n\n## Requirements\n\n- Clerk account with API keys\n- Next.js 13+ with App Router (for examples)\n- React 18+\n- Environment variables:\n  - `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY`\n  - `CLERK_SECRET_KEY`\n- Optional: `@clerk/themes` for pre-built themes\n- Optional: Tailwind CSS for styling examples\n\n## Progressive Disclosure\n\nFor advanced customization patterns, see:\n- `examples/custom-sign-in.tsx` - Complete Clerk Elements implementation\n- `examples/social-buttons.tsx` - OAuth provider integration\n- `examples/theme-config.tsx` - Advanced theming patterns"
              },
              {
                "name": "billing-integration",
                "description": "Clerk Billing and Stripe subscription management setup. Use when implementing subscriptions, configuring pricing plans, setting up billing, adding payment flows, managing entitlements, or when user mentions Clerk Billing, Stripe integration, subscription management, pricing tables, payment processing, or monetization.",
                "path": "plugins/clerk/skills/billing-integration/SKILL.md",
                "frontmatter": {
                  "name": "billing-integration",
                  "description": "Clerk Billing and Stripe subscription management setup. Use when implementing subscriptions, configuring pricing plans, setting up billing, adding payment flows, managing entitlements, or when user mentions Clerk Billing, Stripe integration, subscription management, pricing tables, payment processing, or monetization.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# billing-integration\n\n## Instructions\n\nThis skill provides complete Clerk Billing integration with Stripe for subscription management, pricing plans, and payment processing. Clerk Billing eliminates the need for custom webhooks and payment UI by connecting directly to Stripe while handling user interface, entitlement logic, and session-aware billing flows.\n\n### 1. Enable Clerk Billing\n\nInitialize Clerk Billing in your Clerk Dashboard:\n\n```bash\n# Run automated setup script\nbash ./skills/billing-integration/scripts/setup-billing.sh\n```\n\n**What This Does:**\n- Guides you through Clerk Dashboard setup\n- Connects your Stripe account\n- Enables billing features\n- Configures production/sandbox modes\n\n**Manual Setup Steps:**\n1. Navigate to Clerk Dashboard > Configure > Billing\n2. Click \"Enable Billing\"\n3. Connect your Stripe account (or use sandbox mode for development)\n4. Configure billing settings and defaults\n\n### 2. Configure Pricing Plans\n\nCreate subscription plans and features:\n\n```bash\n# Run plan configuration script\nbash ./skills/billing-integration/scripts/configure-plans.sh\n```\n\n**Configuration Process:**\n1. Navigate to Dashboard > Configure > Subscription Plans\n2. Click \"Add User Plan\" (B2C) or \"Add Organization Plan\" (B2B)\n3. Enter plan details:\n   - Name (e.g., \"Pro Plan\")\n   - Slug (auto-generated, e.g., \"pro_plan\")\n   - Description\n   - Pricing (monthly/yearly)\n4. Add features to plan:\n   - Feature name (e.g., \"AI Assistant\")\n   - Feature slug (e.g., \"ai_assistant\")\n   - Feature description\n5. Set feature limits if usage-based\n\n**Plan Types:**\n- **User Plans**: B2C subscriptions tied to individual users\n- **Organization Plans**: B2B subscriptions for teams/companies\n- **Free Tier**: Optional free plan with limited features\n- **Trial Plans**: Time-limited free access to paid features\n\n### 3. Implement Pricing Table\n\nAdd the pricing display component:\n\n**Using Template:**\n```bash\n# Copy pricing page template\ncp ./skills/billing-integration/templates/pricing-page.tsx app/pricing/page.tsx\n```\n\n**Basic Implementation:**\n```typescript\nimport { PricingTable } from '@clerk/nextjs'\n\nexport default function PricingPage() {\n  return (\n    <div className=\"container mx-auto py-12\">\n      <h1 className=\"text-4xl font-bold text-center mb-8\">\n        Choose Your Plan\n      </h1>\n      <PricingTable />\n    </div>\n  )\n}\n```\n\n**What PricingTable Provides:**\n- Automatic plan rendering from Dashboard configuration\n- Built-in payment form with Stripe Elements\n- Subscription creation and management\n- Responsive design matching Clerk UI\n- Session-aware (shows current plan for logged-in users)\n\n### 4. Implement Checkout Flow\n\nCreate a complete checkout experience:\n\n```bash\n# Copy checkout flow template\ncp ./skills/billing-integration/templates/checkout-flow.tsx components/checkout-flow.tsx\n```\n\n**Checkout Features:**\n- Plan selection and comparison\n- Payment method collection\n- Subscription confirmation\n- Error handling\n- Success redirects\n- Email receipts (automatic via Stripe)\n\n**Customization Options:**\n```typescript\n<PricingTable\n  appearance={{\n    theme: 'dark',\n    variables: {\n      colorPrimary: '#3b82f6',\n      borderRadius: '0.5rem',\n    }\n  }}\n  onSuccess={(subscription) => {\n    // Custom success handler\n    router.push('/dashboard')\n  }}\n/>\n```\n\n### 5. Access Control & Entitlements\n\nProtect features based on subscription status:\n\n**Frontend Protection:**\n```typescript\nimport { useAuth } from '@clerk/nextjs'\n\nexport default function FeatureComponent() {\n  const { has } = useAuth()\n  const canUseFeature = has?.({ feature: 'ai_assistant' })\n\n  if (!canUseFeature) {\n    return <UpgradePrompt feature=\"AI Assistant\" />\n  }\n\n  return <AIAssistantInterface />\n}\n```\n\n**Backend Protection (API Routes):**\n```typescript\nimport { auth } from '@clerk/nextjs/server'\n\nexport async function POST(request: Request) {\n  const { has } = await auth()\n\n  if (!has({ feature: 'ai_assistant' })) {\n    return Response.json(\n      { error: 'Subscription required' },\n      { status: 403 }\n    )\n  }\n\n  // Process request\n}\n```\n\n**Organization-Level Access:**\n```typescript\nconst { has } = useAuth()\nconst orgHasFeature = has?.({\n  permission: 'org:billing:manage',\n  feature: 'team_workspace'\n})\n```\n\n### 6. Subscription Management\n\nEnable users to manage subscriptions:\n\n**Built-in Management:**\n```typescript\nimport { UserButton } from '@clerk/nextjs'\n\nexport default function Navigation() {\n  return (\n    <UserButton afterSignOutUrl=\"/\" />\n  )\n}\n```\n\n**What Users Can Access:**\n- View current plan and features\n- See invoice history\n- Update payment methods\n- Upgrade/downgrade plans\n- Cancel subscriptions\n- View upcoming renewals\n\n**Custom Management Interface:**\n```bash\n# Copy subscription management template\ncp ./skills/billing-integration/templates/subscription-management.tsx components/subscription-management.tsx\n```\n\n### 7. Webhook Handling (Optional)\n\nWhile Clerk handles most webhook logic automatically, you may need custom webhooks for:\n- Usage tracking\n- Custom notifications\n- Analytics integration\n- Third-party service integration\n\n```bash\n# Setup webhook handlers\nbash ./skills/billing-integration/scripts/setup-webhooks.sh\n```\n\n**Common Webhook Events:**\n- `subscription.created` - New subscription\n- `subscription.updated` - Plan change\n- `subscription.deleted` - Cancellation\n- `invoice.payment_succeeded` - Successful payment\n- `invoice.payment_failed` - Failed payment\n\n**Handler Templates:**\n```bash\n# Copy webhook handler templates\ncp ./skills/billing-integration/templates/webhook-handlers/subscription-created.ts app/api/webhooks/subscription-created/route.ts\ncp ./skills/billing-integration/templates/webhook-handlers/payment-succeeded.ts app/api/webhooks/payment-succeeded/route.ts\n```\n\n### 8. Testing & Development\n\n**Sandbox Mode:**\n- Use Clerk's sandbox mode for development\n- No Stripe account required initially\n- Test all billing flows without real payments\n- Switch to production when ready\n\n**Test Cards (Stripe):**\n```\nSuccess: 4242 4242 4242 4242\nDecline: 4000 0000 0000 0002\n3D Secure: 4000 0027 6000 3184\n```\n\n**Testing Checklist:**\n- [ ] Plan selection and display\n- [ ] Payment form functionality\n- [ ] Subscription creation\n- [ ] Feature access control\n- [ ] Plan upgrades/downgrades\n- [ ] Subscription cancellation\n- [ ] Invoice generation\n- [ ] Email notifications\n\n## Examples\n\n### Example 1: Complete SaaS Billing Flow\n\n```bash\n# 1. Enable billing in Dashboard\nbash ./skills/billing-integration/scripts/setup-billing.sh\n\n# 2. Configure pricing plans\nbash ./skills/billing-integration/scripts/configure-plans.sh\n\n# 3. Implement pricing page\ncp ./skills/billing-integration/templates/pricing-page.tsx app/pricing/page.tsx\n\n# 4. Add subscription management\ncp ./skills/billing-integration/templates/subscription-management.tsx components/subscription-management.tsx\n\n# 5. Copy complete example\ncp ./skills/billing-integration/examples/saas-billing-flow.tsx app/subscribe/page.tsx\n```\n\n**Result:** Full billing system with pricing display, checkout, and subscription management\n\n### Example 2: Freemium Model with Feature Gates\n\n**Setup:**\n1. Create \"Free\" plan with basic features\n2. Create \"Pro\" plan with premium features\n3. Implement feature gates throughout app\n\n**Implementation:**\n```typescript\n// Feature-gated component\nimport { useAuth } from '@clerk/nextjs'\nimport { PricingTable } from '@clerk/nextjs'\n\nexport default function PremiumFeature() {\n  const { has } = useAuth()\n\n  if (!has({ feature: 'premium_analytics' })) {\n    return (\n      <div className=\"border rounded-lg p-6\">\n        <h3>Premium Analytics</h3>\n        <p>Upgrade to Pro to unlock advanced analytics</p>\n        <PricingTable />\n      </div>\n    )\n  }\n\n  return <AdvancedAnalyticsDashboard />\n}\n```\n\n### Example 3: Organization Billing (B2B)\n\n**Setup:**\n1. Enable Organization Plans in Dashboard\n2. Configure team-based pricing\n3. Implement organization billing interface\n\n**Implementation:**\n```typescript\nimport { useOrganization } from '@clerk/nextjs'\n\nexport default function OrgBilling() {\n  const { organization } = useOrganization()\n\n  return (\n    <div>\n      <h2>Billing for {organization?.name}</h2>\n      <PricingTable mode=\"organization\" />\n    </div>\n  )\n}\n```\n\n### Example 4: Usage-Based Billing\n\n**Setup:**\n1. Configure usage-based features in Dashboard\n2. Set usage limits per plan\n3. Track usage in your application\n\n**Implementation:**\n```typescript\n// Track API usage\nimport { clerkClient } from '@clerk/nextjs/server'\n\nexport async function POST(request: Request) {\n  const { userId } = await auth()\n\n  // Check current usage\n  const user = await clerkClient.users.getUser(userId)\n  const currentUsage = user.publicMetadata.apiUsage || 0\n\n  // Check limit\n  const { has } = await auth()\n  const limit = has({ feature: 'api_calls_1000' }) ? 1000 : 100\n\n  if (currentUsage >= limit) {\n    return Response.json({ error: 'Usage limit reached' }, { status: 429 })\n  }\n\n  // Increment usage\n  await clerkClient.users.updateUserMetadata(userId, {\n    publicMetadata: {\n      apiUsage: currentUsage + 1\n    }\n  })\n\n  // Process request\n}\n```\n\n## Requirements\n\n**Clerk Setup:**\n- Clerk account with application configured\n- Clerk Billing enabled in Dashboard\n- Next.js 13.4+ with App Router\n- `@clerk/nextjs` version 5.0+\n\n**Stripe Requirements:**\n- Stripe account (production or test mode)\n- Stripe publishable and secret keys\n- Products and prices configured in Stripe (synced via Clerk)\n\n**Environment Variables:**\n```bash\nNEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_...\nCLERK_SECRET_KEY=sk_test_...\n```\n\n**Dependencies:**\n```json\n{\n  \"@clerk/nextjs\": \"^5.0.0\",\n  \"next\": \"^14.0.0\",\n  \"react\": \"^18.0.0\"\n}\n```\n\n**Project Structure:**\n```\napp/\n‚îú‚îÄ‚îÄ pricing/\n‚îÇ   ‚îî‚îÄ‚îÄ page.tsx          # Pricing page with PricingTable\n‚îú‚îÄ‚îÄ subscribe/\n‚îÇ   ‚îî‚îÄ‚îÄ page.tsx          # Checkout flow\n‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îî‚îÄ‚îÄ webhooks/         # Optional custom webhooks\ncomponents/\n‚îú‚îÄ‚îÄ subscription-management.tsx\n‚îî‚îÄ‚îÄ upgrade-prompt.tsx\n```\n\n## Security Best Practices\n\n**Never Hardcode Credentials:**\n```bash\n# ‚ùå WRONG\nSTRIPE_SECRET_KEY=sk_live_actual_key_here\n\n# ‚úÖ CORRECT\nSTRIPE_SECRET_KEY=your_stripe_secret_key_here\n```\n\n**Always Use Environment Variables:**\n```typescript\n// ‚úÖ Read from environment\nconst stripeKey = process.env.STRIPE_SECRET_KEY\nif (!stripeKey) {\n  throw new Error('STRIPE_SECRET_KEY not configured')\n}\n```\n\n**Protect API Routes:**\n- Always verify authentication with `auth()`\n- Check feature entitlements on server-side\n- Validate webhook signatures\n- Use HTTPS in production\n\n**Webhook Security:**\n```typescript\nimport { headers } from 'next/headers'\nimport { stripe } from '@/lib/stripe'\n\nexport async function POST(request: Request) {\n  const body = await request.text()\n  const signature = headers().get('stripe-signature')\n\n  try {\n    const event = stripe.webhooks.constructEvent(\n      body,\n      signature,\n      process.env.STRIPE_WEBHOOK_SECRET\n    )\n    // Process event\n  } catch (err) {\n    return Response.json({ error: 'Invalid signature' }, { status: 400 })\n  }\n}\n```\n\n## Configuration Files\n\n**Clerk Billing Settings (Dashboard):**\n- Subscription plans and features\n- Pricing tiers and intervals\n- Trial period configuration\n- Cancellation behavior\n- Invoice settings\n- Payment methods accepted\n\n**Stripe Configuration (Synced):**\n- Products ‚Üí Clerk Plans\n- Prices ‚Üí Clerk Pricing Tiers\n- Customers ‚Üí Clerk Users\n- Subscriptions ‚Üí User/Org Subscriptions\n- Payment Methods ‚Üí Stored automatically\n\n## Best Practices\n\n**Plan Design:**\n- Start with 2-3 clear tiers (Free, Pro, Enterprise)\n- Make feature differences obvious\n- Use clear, benefit-focused names\n- Set appropriate price points\n- Consider annual discounts\n\n**User Experience:**\n- Show current plan clearly\n- Make upgrading easy (one-click from app)\n- Provide grace period on cancellation\n- Send clear email notifications\n- Display usage limits proactively\n\n**Access Control:**\n- Always check entitlements server-side\n- Use frontend checks for UX only\n- Implement graceful degradation\n- Show upgrade prompts at relevant moments\n- Don't break existing functionality on downgrade\n\n**Pricing Strategy:**\n- Price based on value, not cost\n- Use psychological pricing ($29 vs $30)\n- Offer annual plans with discount\n- Consider usage-based for scalability\n- Test pricing with real users\n\n**Monetization Approach:**\n- Freemium: Free tier + paid upgrades\n- Trial: 7-14 day free trial, then charge\n- Tiered: Multiple plans with different features\n- Usage-based: Pay per API call/user/feature\n- Hybrid: Base fee + usage charges\n\n## Integration with Clerk Features\n\n**Organizations:**\n- Enable organization billing for B2B\n- Set per-seat pricing\n- Allow organization admins to manage billing\n- Track usage at organization level\n\n**User Management:**\n- Link subscriptions to user accounts\n- Store subscription status in user metadata\n- Use `publicMetadata` for client-accessible data\n- Use `privateMetadata` for sensitive billing data\n\n**Session Awareness:**\n- PricingTable shows current plan when logged in\n- Automatically handles upgrade flows\n- Redirects to login if unauthenticated\n- Preserves intended plan selection\n\n## Troubleshooting\n\n**Common Issues:**\n\n1. **PricingTable not displaying:**\n   - Verify Billing is enabled in Dashboard\n   - Check plans are configured\n   - Ensure Stripe is connected\n   - Verify component import\n\n2. **Payment failing:**\n   - Check Stripe API keys\n   - Verify webhook endpoint\n   - Test with Stripe test cards\n   - Review Stripe Dashboard logs\n\n3. **Access control not working:**\n   - Verify feature slugs match exactly\n   - Check plan includes the feature\n   - Ensure subscription is active\n   - Test both client and server checks\n\n4. **Webhooks not firing:**\n   - Verify webhook URL is correct\n   - Check webhook signing secret\n   - Enable webhook forwarding for local dev\n   - Review Clerk webhook logs\n\n**Debugging:**\n```typescript\n// Log subscription status\nconst { has, debug } = useAuth()\nconsole.log('Auth debug:', debug)\nconsole.log('Has feature:', has({ feature: 'ai_assistant' }))\n```\n\n---\n\n**Plugin:** clerk\n**Version:** 1.0.0\n**Category:** Billing & Monetization\n**Skill Type:** Integration & Setup"
              },
              {
                "name": "middleware-protection",
                "description": "Route protection and authorization patterns for Clerk middleware. Use when implementing route guards, protecting API routes, configuring middleware matchers, setting up role-based access control, creating auth boundaries, or when user mentions middleware, route protection, auth guards, protected routes, public routes, matcher patterns, or authorization middleware.",
                "path": "plugins/clerk/skills/middleware-protection/SKILL.md",
                "frontmatter": {
                  "name": "middleware-protection",
                  "description": "Route protection and authorization patterns for Clerk middleware. Use when implementing route guards, protecting API routes, configuring middleware matchers, setting up role-based access control, creating auth boundaries, or when user mentions middleware, route protection, auth guards, protected routes, public routes, matcher patterns, or authorization middleware.",
                  "allowed-tools": "Read, Write, Edit, Bash, Glob"
                },
                "content": "# Middleware Protection\n\nComprehensive route protection and authorization patterns for Clerk middleware in Next.js applications. Provides middleware configuration, route matchers, role-based access control, and authentication boundaries.\n\n## Core Concepts\n\n### Middleware Architecture\n- **Edge Runtime**: Clerk middleware runs on Cloudflare Workers/Vercel Edge\n- **Request Interception**: Middleware executes before route handlers\n- **Auth State**: Access to authentication state via `auth()` helper\n- **Matcher Patterns**: Configure which routes middleware applies to\n\n### Route Protection Levels\n1. **Public Routes**: Accessible without authentication (sign-in, sign-up, landing pages)\n2. **Protected Routes**: Require authentication (dashboards, user profiles)\n3. **Organization Routes**: Require organization membership\n4. **Role-Based Routes**: Require specific roles or permissions\n\n### Security Principles\n- **Deny by Default**: All routes protected unless explicitly made public\n- **Defense in Depth**: Middleware + server component checks + API route guards\n- **Session Validation**: Automatic token validation on every request\n- **CSRF Protection**: Built-in protection against cross-site request forgery\n\n## Instructions\n\n### Basic Middleware Setup\n\n1. **Create middleware.ts in project root**\n   - Import `clerkMiddleware` from `@clerk/nextjs/server`\n   - Export default middleware function\n   - Configure matcher for routes to protect\n\n2. **Configure Public Routes**\n   - Define routes accessible without authentication\n   - Use glob patterns for route matching\n   - Include sign-in/sign-up pages as public\n\n3. **Set Protected Routes**\n   - Specify which routes require authentication\n   - Use route groups for organization\n   - Apply different protection levels\n\n### Advanced Patterns\n\n1. **Role-Based Access Control**\n   - Check user roles in middleware\n   - Redirect based on permissions\n   - Implement organization-level permissions\n\n2. **Conditional Route Protection**\n   - Apply different rules based on route patterns\n   - Check custom metadata\n   - Implement feature flags\n\n3. **API Route Protection**\n   - Secure API endpoints with middleware\n   - Validate session tokens\n   - Check permissions before processing\n\n4. **Multi-Tenant Protection**\n   - Organization-scoped routes\n   - Tenant isolation\n   - Cross-organization access prevention\n\n### Testing Protection\n\n1. **Test Authentication Boundaries**\n   - Verify unauthenticated redirects\n   - Check protected route access\n   - Validate role requirements\n\n2. **Test Edge Cases**\n   - Token expiration handling\n   - Invalid session handling\n   - Missing organization membership\n\n## Templates\n\nUse these templates for middleware implementation:\n\n### Core Templates\n- `templates/middleware.ts` - Basic middleware configuration\n- `templates/route-matchers.ts` - Route matching patterns\n- `templates/role-based-middleware.ts` - Role-based access control\n\n### Configuration Templates\n- `templates/public-routes-config.ts` - Public route definitions\n- `templates/protected-routes-config.ts` - Protected route setup\n- `templates/api-middleware-config.ts` - API route protection\n\n### Advanced Templates\n- `templates/organization-middleware.ts` - Organization-scoped protection\n- `templates/conditional-middleware.ts` - Conditional route logic\n- `templates/custom-redirects.ts` - Custom redirect handling\n\n## Scripts\n\nUse these scripts for middleware setup and testing:\n\n- `scripts/generate-middleware.sh` - Generate middleware.ts with configuration\n- `scripts/configure-routes.sh` - Setup route protection patterns\n- `scripts/test-protection.sh` - Test authentication guards and boundaries\n- `scripts/validate-middleware.sh` - Validate middleware configuration\n\n## Examples\n\nSee complete examples in the `examples/` directory:\n\n### Basic Examples\n- `examples/basic-middleware.ts` - Simple middleware setup\n- `examples/public-private-routes.ts` - Public vs protected routes\n- `examples/api-middleware.ts` - API route protection\n\n### Advanced Examples\n- `examples/role-based-protection.ts` - Role-based access control\n- `examples/organization-routes.ts` - Organization-scoped routes\n- `examples/conditional-routing.ts` - Conditional protection logic\n- `examples/custom-auth-flow.ts` - Custom authentication flows\n\n### Testing Examples\n- `examples/middleware-tests.ts` - Middleware unit tests\n- `examples/integration-tests.ts` - Full protection integration tests\n\n## Security Best Practices\n\n### API Key Handling\n**CRITICAL**: When generating middleware configuration:\n\n- ‚ùå NEVER hardcode CLERK_SECRET_KEY or NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY\n- ‚ùå NEVER include real API keys in examples\n- ‚úÖ ALWAYS use placeholders: `your_clerk_secret_key_here`\n- ‚úÖ ALWAYS read from environment variables: `process.env.CLERK_SECRET_KEY`\n- ‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n- ‚úÖ ALWAYS document where to obtain keys from Clerk Dashboard\n\n### Middleware Security\n1. **Validate All Requests**: Don't skip middleware on any protected routes\n2. **Check Session Validity**: Always validate session tokens\n3. **Implement Rate Limiting**: Protect against brute force attacks\n4. **Log Security Events**: Track authentication failures and suspicious activity\n5. **Use HTTPS Only**: Never run authentication over HTTP in production\n\n## Requirements\n\n**Next.js Version:**\n- Next.js 13.4+ (App Router support)\n- Next.js 12+ (Pages Router support)\n\n**Clerk SDK:**\n- @clerk/nextjs 4.0+ (latest stable)\n- Node.js 16+\n\n**Configuration Files:**\n- `.env.local` with Clerk environment variables\n- `middleware.ts` in project root\n- `.gitignore` protecting secrets\n\n## Common Patterns\n\n### Pattern 1: Public Landing + Protected Dashboard\n```typescript\n// Public: /, /about, /pricing\n// Protected: /dashboard/*, /profile/*\n// Matcher: Protect everything except public routes\n```\n\n### Pattern 2: API Route Protection\n```typescript\n// Protect all /api/* except /api/webhooks/clerk\n// Validate session tokens on protected endpoints\n// Return 401 for unauthenticated requests\n```\n\n### Pattern 3: Organization-Scoped Routes\n```typescript\n// Require organization membership for /org/*\n// Check active organization in middleware\n// Redirect to organization selection if needed\n```\n\n### Pattern 4: Role-Based Access\n```typescript\n// Check user roles in middleware\n// Redirect based on permissions (admin vs user)\n// Implement feature-specific access control\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Middleware Not Running**\n   - Check matcher configuration\n   - Verify middleware.ts location (project root)\n   - Ensure Next.js version supports middleware\n\n2. **Infinite Redirect Loops**\n   - Sign-in page must be public\n   - Check redirect logic in middleware\n   - Verify afterSignInUrl configuration\n\n3. **Protected Routes Accessible**\n   - Verify matcher includes routes\n   - Check auth state validation\n   - Ensure middleware executes before route\n\n4. **Session Not Found**\n   - Check environment variables loaded\n   - Verify Clerk keys are correct\n   - Ensure cookies not blocked\n\n---\n\n**Purpose**: Provide comprehensive middleware protection patterns for Clerk authentication\n**Load when**: Implementing route guards, protecting routes, setting up middleware, configuring auth boundaries"
              },
              {
                "name": "nextjs-integration",
                "description": "Complete Next.js integration patterns for Clerk authentication with App Router and Pages Router. Use when setting up Clerk in Next.js, configuring authentication middleware, implementing protected routes, setting up server/client components with auth, or when user mentions Clerk Next.js setup, App Router auth, Pages Router auth, or Next.js authentication integration.",
                "path": "plugins/clerk/skills/nextjs-integration/SKILL.md",
                "frontmatter": {
                  "name": "nextjs-integration",
                  "description": "Complete Next.js integration patterns for Clerk authentication with App Router and Pages Router. Use when setting up Clerk in Next.js, configuring authentication middleware, implementing protected routes, setting up server/client components with auth, or when user mentions Clerk Next.js setup, App Router auth, Pages Router auth, or Next.js authentication integration.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# nextjs-integration\n\n## Instructions\n\nThis skill provides complete Clerk authentication integration for Next.js applications, supporting both App Router (Next.js 13+) and Pages Router patterns. It covers installation, middleware configuration, authentication helpers, and protected route patterns.\n\n### 1. Clerk Installation & Setup\n\nInstall Clerk SDK and configure environment variables:\n\n```bash\n# Run automated installation script\nbash ./skills/nextjs-integration/scripts/install-clerk.sh\n\n# Or manually install\nnpm install @clerk/nextjs\n```\n\n**Environment Variables:**\n```bash\n# Create .env.local with Clerk credentials\nNEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_your_key_here\nCLERK_SECRET_KEY=sk_test_your_key_here\n\n# Optional: Customize sign-in/sign-up URLs\nNEXT_PUBLIC_CLERK_SIGN_IN_URL=/sign-in\nNEXT_PUBLIC_CLERK_SIGN_UP_URL=/sign-up\nNEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=/dashboard\nNEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=/onboarding\n```\n\n**What This Does:**\n- Installs @clerk/nextjs package\n- Creates .env.local with placeholder keys\n- Configures redirect URLs for sign-in/sign-up flows\n- Sets up authentication endpoints\n\n### 2. App Router Integration\n\nConfigure Clerk for Next.js App Router (13.4+):\n\n```bash\n# Run App Router setup script\nbash ./skills/nextjs-integration/scripts/setup-app-router.sh\n```\n\n**Files Created:**\n- `middleware.ts` - Route protection at edge\n- `app/layout.tsx` - ClerkProvider wrapper\n- `app/sign-in/[[...sign-in]]/page.tsx` - Sign-in page\n- `app/sign-up/[[...sign-up]]/page.tsx` - Sign-up page\n\n**App Router Features:**\n- Server Components with `auth()` helper\n- Client Components with `useAuth()` hook\n- Edge middleware for route protection\n- Automatic session management\n- RSC-compatible authentication\n\n**Copy Template Files:**\n```bash\n# Middleware configuration\ncp ./skills/nextjs-integration/templates/app-router/middleware.ts ./middleware.ts\n\n# Root layout with ClerkProvider\ncp ./skills/nextjs-integration/templates/app-router/layout.tsx ./app/layout.tsx\n```\n\n### 3. Pages Router Integration\n\nConfigure Clerk for Next.js Pages Router (12.x and earlier):\n\n```bash\n# Run Pages Router setup script\nbash ./skills/nextjs-integration/scripts/setup-pages-router.sh\n```\n\n**Files Created:**\n- `pages/_app.tsx` - ClerkProvider wrapper\n- `pages/api/auth.ts` - API route for auth callbacks\n- `pages/sign-in/[[...index]].tsx` - Sign-in page\n- `pages/sign-up/[[...index]].tsx` - Sign-up page\n\n**Pages Router Features:**\n- getServerSideProps with auth\n- API routes with auth protection\n- Client-side authentication hooks\n- Custom sign-in/sign-up components\n\n**Copy Template Files:**\n```bash\n# _app.tsx with ClerkProvider\ncp ./skills/nextjs-integration/templates/pages-router/_app.tsx ./pages/_app.tsx\n\n# Auth API route\ncp ./skills/nextjs-integration/templates/pages-router/api/auth.ts ./pages/api/auth.ts\n```\n\n### 4. Authentication Middleware\n\nConfigure middleware for route protection:\n\n```bash\n# Setup auth middleware with route matching\nbash ./skills/nextjs-integration/scripts/configure-middleware.sh\n```\n\n**Middleware Patterns:**\n- Protect specific routes (e.g., `/dashboard/*`)\n- Public routes configuration\n- API route protection\n- Custom redirect logic\n- Matcher configuration for edge runtime\n\n**Middleware Configuration:**\n```typescript\n// middleware.ts - Protects routes at the edge\nimport { authMiddleware } from \"@clerk/nextjs\";\n\nexport default authMiddleware({\n  publicRoutes: [\"/\", \"/api/public\"],\n  ignoredRoutes: [\"/api/webhook\"],\n});\n\nexport const config = {\n  matcher: ['/((?!.+\\\\.[\\\\w]+$|_next).*)', '/', '/(api|trpc)(.*)'],\n};\n```\n\n### 5. Server Component Authentication (App Router)\n\nUse `auth()` helper in Server Components:\n\n```typescript\nimport { auth } from '@clerk/nextjs';\n\nexport default async function DashboardPage() {\n  const { userId } = auth();\n\n  if (!userId) {\n    redirect('/sign-in');\n  }\n\n  // Protected server component logic\n  const userData = await fetchUserData(userId);\n\n  return <div>Welcome {userData.name}</div>;\n}\n```\n\n**Server-Side Helpers:**\n- `auth()` - Get current user session\n- `currentUser()` - Get full user object\n- `redirectToSignIn()` - Redirect helper\n- `clerkClient` - Server-side Clerk API client\n\n### 6. Client Component Authentication\n\nUse React hooks in Client Components:\n\n```typescript\n'use client';\nimport { useAuth, useUser } from '@clerk/nextjs';\n\nexport function UserProfile() {\n  const { userId, isLoaded, isSignedIn } = useAuth();\n  const { user } = useUser();\n\n  if (!isLoaded) return <div>Loading...</div>;\n  if (!isSignedIn) return <div>Please sign in</div>;\n\n  return <div>Hello {user.firstName}</div>;\n}\n```\n\n**Client-Side Hooks:**\n- `useAuth()` - Authentication state\n- `useUser()` - Current user data\n- `useClerk()` - Clerk instance methods\n- `useSignIn()` - Sign-in flow control\n- `useSignUp()` - Sign-up flow control\n\n## Examples\n\n### Example 1: Complete App Router Setup\n\n```bash\n# 1. Install Clerk\nbash ./skills/nextjs-integration/scripts/install-clerk.sh\n\n# 2. Configure App Router\nbash ./skills/nextjs-integration/scripts/setup-app-router.sh\n\n# 3. Setup middleware\nbash ./skills/nextjs-integration/scripts/configure-middleware.sh\n\n# 4. Copy protected route example\ncp ./skills/nextjs-integration/examples/protected-route.tsx ./app/dashboard/page.tsx\n\n# 5. Copy server component auth example\ncp ./skills/nextjs-integration/examples/server-component-auth.tsx ./app/profile/page.tsx\n\n# 6. Start development server\nnpm run dev\n```\n\n**Result:** Fully configured Next.js App Router with Clerk authentication, protected routes, and sign-in/sign-up pages\n\n### Example 2: Pages Router with API Routes\n\n```bash\n# 1. Install Clerk\nbash ./skills/nextjs-integration/scripts/install-clerk.sh\n\n# 2. Configure Pages Router\nbash ./skills/nextjs-integration/scripts/setup-pages-router.sh\n\n# 3. Copy API route template\ncp ./skills/nextjs-integration/templates/pages-router/api/auth.ts ./pages/api/auth.ts\n\n# 4. Test authentication\nnpm run dev\n```\n\n**Result:** Pages Router setup with API route authentication and custom auth pages\n\n### Example 3: Multi-Tenant Application\n\nConfigure organization-based authentication:\n\n```typescript\n// Server Component with organization context\nimport { auth } from '@clerk/nextjs';\n\nexport default async function TeamDashboard() {\n  const { orgId, userId } = auth();\n\n  if (!orgId) {\n    return <div>Please select an organization</div>;\n  }\n\n  const teamData = await fetchTeamData(orgId);\n  return <TeamView data={teamData} />;\n}\n```\n\n**Organization Features:**\n- Multi-tenant support\n- Organization switching\n- Role-based access control\n- Team member management\n\n### Example 4: Protected API Routes\n\n```typescript\n// App Router API route with auth\nimport { auth } from '@clerk/nextjs';\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const { userId } = auth();\n\n  if (!userId) {\n    return new NextResponse('Unauthorized', { status: 401 });\n  }\n\n  const data = await fetchProtectedData(userId);\n  return NextResponse.json(data);\n}\n```\n\n## Requirements\n\n**Dependencies:**\n- `@clerk/nextjs` - Clerk Next.js SDK\n- Next.js 12.0+ (Pages Router) or 13.4+ (App Router)\n- React 18+\n- Node.js 18.17+ or 20+\n\n**Clerk Account:**\n- Active Clerk application (free tier available)\n- Publishable key and secret key from dashboard\n- Configured sign-in/sign-up flows in Clerk dashboard\n\n**Project Structure:**\n- Next.js project initialized with `create-next-app`\n- `app/` directory (App Router) or `pages/` directory (Pages Router)\n- TypeScript recommended but not required\n\n**Environment Variables:**\n- `.env.local` for local development\n- `.env.production` for production (never commit secrets)\n- Vercel/deployment platform environment variable configuration\n\n## Security Best Practices\n\n**Never Hardcode API Keys:**\n```bash\n# ‚úÖ CORRECT - Use environment variables\nCLERK_SECRET_KEY=your_clerk_secret_key_here\n\n# ‚ùå WRONG - Never commit secrets\nconst clerkSecret = \"sk_test_abc123...\" // DON'T DO THIS\n```\n\n**Protect Sensitive Routes:**\n- Use middleware for edge-level protection\n- Validate auth in Server Components\n- Check authentication in API routes\n- Never trust client-side auth alone\n\n**Secure API Routes:**\n```typescript\n// Always validate auth server-side\nconst { userId } = auth();\nif (!userId) {\n  return new NextResponse('Unauthorized', { status: 401 });\n}\n```\n\n**Environment Variable Management:**\n- Use `.env.local` for development (git-ignored)\n- Use `.env.example` with placeholders (safe to commit)\n- Store production keys in deployment platform\n- Rotate keys periodically\n\n## App Router vs Pages Router\n\n**Use App Router When:**\n- Building new Next.js 13.4+ applications\n- Need Server Components for better performance\n- Want edge middleware capabilities\n- Require streaming and suspense features\n\n**Use Pages Router When:**\n- Maintaining existing Next.js 12.x applications\n- Team familiar with traditional Next.js patterns\n- Need getServerSideProps/getStaticProps\n- Gradual migration from older Next.js versions\n\n**Migration Path:**\n- Can mix both routers in same application\n- Migrate routes incrementally\n- App Router is recommended for new features\n\n## Integration Patterns\n\n**With Supabase:**\n- Use Clerk for authentication\n- Pass Clerk user ID to Supabase RLS policies\n- Sync user data between Clerk and Supabase\n\n**With tRPC:**\n- Add Clerk user context to tRPC context\n- Protect procedures with auth middleware\n- Type-safe authentication in API layer\n\n**With Prisma:**\n- Store Clerk user ID as foreign key\n- Link user data to Clerk profiles\n- Use userId for data isolation\n\n**With Vercel:**\n- Automatic environment variable sync\n- Edge middleware deployment\n- Preview deployments with auth\n\n## Troubleshooting\n\n**Middleware Not Running:**\n- Check matcher configuration in middleware.ts\n- Ensure middleware.ts is at project root\n- Verify Edge Runtime compatibility\n\n**Sign-In Redirect Loop:**\n- Check `NEXT_PUBLIC_CLERK_SIGN_IN_URL` matches route\n- Verify publicRoutes includes sign-in page\n- Ensure middleware doesn't protect auth routes\n\n**Server Component Hydration:**\n- Don't use useAuth in Server Components\n- Use auth() helper instead\n- Ensure ClerkProvider wraps layout\n\n**Environment Variables Not Loading:**\n- Restart development server after .env changes\n- Use NEXT_PUBLIC_ prefix for client-side vars\n- Check .env.local exists and is git-ignored\n\n---\n\n**Plugin:** clerk\n**Version:** 1.0.0\n**Category:** Authentication\n**Skill Type:** Integration & Configuration"
              },
              {
                "name": "oauth-providers",
                "description": "Configure OAuth authentication providers for Clerk (Google, GitHub, Discord, Apple, Microsoft, Facebook, LinkedIn, Twitter, and 11+ more). Use when setting up social login, configuring OAuth providers, implementing authentication flows, generating redirect URLs, testing OAuth connections, or when user mentions Clerk OAuth, social authentication, provider setup, or multi-provider auth.",
                "path": "plugins/clerk/skills/oauth-providers/SKILL.md",
                "frontmatter": {
                  "name": "oauth-providers",
                  "description": "Configure OAuth authentication providers for Clerk (Google, GitHub, Discord, Apple, Microsoft, Facebook, LinkedIn, Twitter, and 11+ more). Use when setting up social login, configuring OAuth providers, implementing authentication flows, generating redirect URLs, testing OAuth connections, or when user mentions Clerk OAuth, social authentication, provider setup, or multi-provider auth.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# oauth-providers\n\n## Instructions\n\nThis skill provides complete OAuth provider configuration for Clerk-powered applications. It covers all 19+ supported OAuth providers with templates, setup scripts, testing utilities, and integration patterns for Next.js, React, and other frameworks.\n\n### Supported OAuth Providers\n\n**Tier 1 (Most Common):**\n- Google - Consumer apps, Google Workspace integration\n- GitHub - Developer tools, technical audiences\n- Discord - Gaming, community platforms\n- Microsoft - Enterprise applications, Microsoft 365 integration\n\n**Tier 2 (Social & Professional):**\n- Facebook - Social applications, consumer products\n- LinkedIn - Professional networks, B2B applications\n- Twitter/X - Social media integration\n- Apple - iOS applications, consumer products\n\n**Tier 3 (Specialized):**\n- GitLab - Developer platforms, CI/CD tools\n- Bitbucket - Atlassian ecosystem integration\n- Dropbox - File storage integration\n- Notion - Productivity app integration\n- Slack - Workspace collaboration\n- Linear - Project management tools\n- Coinbase - Crypto wallet authentication\n- TikTok - Short-form video platforms\n- Twitch - Live streaming platforms\n- HubSpot - CRM integration\n- X/Twitter - Social media (rebranded)\n\n### 1. Provider Setup Script\n\nConfigure any OAuth provider with automated setup:\n\n```bash\n# Set up single provider\nbash scripts/setup-provider.sh google\n\n# Set up multiple providers\nbash scripts/setup-provider.sh google github discord\n\n# Interactive setup with prompts\nbash scripts/setup-provider.sh --interactive\n```\n\n**What the Script Does:**\n1. Detects Clerk project configuration\n2. Generates provider-specific configuration\n3. Creates redirect URLs for all environments (dev, staging, production)\n4. Provides step-by-step setup instructions\n5. Generates environment variable templates\n6. Creates provider testing utilities\n\n**Output:**\n- Provider configuration JSON\n- Environment variable template\n- Setup instructions markdown\n- Test credentials configuration\n\n### 2. Generate Redirect URLs\n\nGenerate callback URLs for all environments:\n\n```bash\n# Generate for specific provider\nbash scripts/generate-redirect-urls.sh google\n\n# Generate for all configured providers\nbash scripts/generate-redirect-urls.sh --all\n\n# Export to environment file\nbash scripts/generate-redirect-urls.sh google --export > .env.oauth\n```\n\n**Redirect URL Patterns:**\n```\nDevelopment:\nhttp://localhost:3000/api/auth/callback/google\n\nProduction:\nhttps://yourdomain.com/api/auth/callback/google\n\nClerk Default:\nhttps://your-clerk-domain.clerk.accounts.dev/v1/oauth_callback\n```\n\n### 3. Test OAuth Flow\n\nValidate OAuth configuration end-to-end:\n\n```bash\n# Test single provider\nbash scripts/test-oauth-flow.sh google\n\n# Test all providers\nbash scripts/test-oauth-flow.sh --all\n\n# Generate test report\nbash scripts/test-oauth-flow.sh google --report\n```\n\n**Tests Performed:**\n- Provider configuration validation\n- Redirect URL accessibility\n- OAuth flow initiation\n- Callback handling\n- Token exchange validation\n- User profile retrieval\n- Error handling scenarios\n\n### 4. Provider Templates\n\nAccess pre-configured templates for each provider:\n\n**Google OAuth:**\n```typescript\n// templates/google/clerk-config.ts\nimport { google } from '@clerk/clerk-sdk-node';\n\nexport const googleConfig = {\n  clientId: process.env.GOOGLE_CLIENT_ID,\n  clientSecret: process.env.GOOGLE_CLIENT_SECRET,\n  redirectUri: process.env.GOOGLE_REDIRECT_URI,\n  scopes: ['profile', 'email'],\n  // Google-specific options\n  accessType: 'offline',\n  prompt: 'consent'\n};\n```\n\n**GitHub OAuth:**\n```typescript\n// templates/github/clerk-config.ts\nexport const githubConfig = {\n  clientId: process.env.GITHUB_CLIENT_ID,\n  clientSecret: process.env.GITHUB_CLIENT_SECRET,\n  redirectUri: process.env.GITHUB_REDIRECT_URI,\n  scopes: ['read:user', 'user:email'],\n  // GitHub-specific options\n  allowSignup: true\n};\n```\n\n**Discord OAuth:**\n```typescript\n// templates/discord/clerk-config.ts\nexport const discordConfig = {\n  clientId: process.env.DISCORD_CLIENT_ID,\n  clientSecret: process.env.DISCORD_CLIENT_SECRET,\n  redirectUri: process.env.DISCORD_REDIRECT_URI,\n  scopes: ['identify', 'email'],\n  // Discord-specific options\n  permissions: '0'\n};\n```\n\n### 5. Multi-Provider Integration\n\n**React/Next.js Components:**\n```typescript\n// templates/oauth-shared/AuthButtons.tsx\nimport { SignIn } from '@clerk/nextjs';\n\nexport function AuthButtons() {\n  return (\n    <div className=\"auth-providers\">\n      <SignIn.Root>\n        <SignIn.Step name=\"start\">\n          <div className=\"provider-buttons\">\n            <SignIn.Strategy name=\"oauth_google\">\n              <button>Continue with Google</button>\n            </SignIn.Strategy>\n            <SignIn.Strategy name=\"oauth_github\">\n              <button>Continue with GitHub</button>\n            </SignIn.Strategy>\n            <SignIn.Strategy name=\"oauth_discord\">\n              <button>Continue with Discord</button>\n            </SignIn.Strategy>\n          </div>\n        </SignIn.Step>\n      </SignIn.Root>\n    </div>\n  );\n}\n```\n\n**Clerk Dashboard Configuration:**\n```typescript\n// templates/oauth-shared/clerk-dashboard-config.ts\nexport const oauthProviders = [\n  {\n    provider: 'google',\n    enabled: true,\n    clientId: 'GOOGLE_CLIENT_ID',\n    clientSecret: 'GOOGLE_CLIENT_SECRET',\n    scopes: ['profile', 'email']\n  },\n  {\n    provider: 'github',\n    enabled: true,\n    clientId: 'GITHUB_CLIENT_ID',\n    clientSecret: 'GITHUB_CLIENT_SECRET',\n    scopes: ['read:user', 'user:email']\n  },\n  {\n    provider: 'discord',\n    enabled: true,\n    clientId: 'DISCORD_CLIENT_ID',\n    clientSecret: 'DISCORD_CLIENT_SECRET',\n    scopes: ['identify', 'email']\n  }\n];\n```\n\n## Examples\n\n### Example 1: Google OAuth for SaaS Application\n\n```bash\n# 1. Set up Google OAuth\nbash scripts/setup-provider.sh google\n\n# Follow prompts:\n# - Create OAuth app in Google Cloud Console\n# - Configure authorized redirect URIs\n# - Copy Client ID and Client Secret\n# - Add to Clerk Dashboard\n\n# 2. Generate redirect URLs\nbash scripts/generate-redirect-urls.sh google --export > .env.google\n\n# 3. Test OAuth flow\nbash scripts/test-oauth-flow.sh google\n\n# 4. Add to React app\ncp templates/google/clerk-config.ts ./lib/auth/google.ts\ncp templates/oauth-shared/AuthButtons.tsx ./components/auth/\n```\n\n**Result:** Fully configured Google OAuth with sign-in button and tested flow\n\n### Example 2: Multi-Provider Authentication (Google + GitHub + Discord)\n\n```bash\n# Set up all providers\nfor provider in google github discord; do\n  bash scripts/setup-provider.sh $provider\ndone\n\n# Generate all redirect URLs\nbash scripts/generate-redirect-urls.sh --all --export > .env.oauth\n\n# Test all providers\nbash scripts/test-oauth-flow.sh --all --report\n\n# Deploy multi-provider UI\ncp templates/oauth-shared/AuthButtons.tsx ./components/auth/\n```\n\n**Result:** Users can sign in with Google, GitHub, or Discord\n\n### Example 3: Enterprise Application with Microsoft + LinkedIn\n\n```bash\n# Set up enterprise providers\nbash scripts/setup-provider.sh microsoft linkedin\n\n# Configure enterprise-specific scopes\n# Edit templates/microsoft/clerk-config.ts to add Azure AD scopes\n# Edit templates/linkedin/clerk-config.ts for LinkedIn API v2\n\n# Test enterprise flows\nbash scripts/test-oauth-flow.sh microsoft --report\nbash scripts/test-oauth-flow.sh linkedin --report\n```\n\n**Result:** Enterprise authentication with Microsoft 365 and LinkedIn integration\n\n### Example 4: Gaming Platform with Discord + Twitch\n\n```bash\n# Set up gaming providers\nbash scripts/setup-provider.sh discord twitch\n\n# Configure gaming-specific permissions\n# Discord: guild membership, voice state\n# Twitch: user:read:email, channel subscriptions\n\n# Test gaming provider flows\nbash scripts/test-oauth-flow.sh discord twitch\n```\n\n**Result:** Gaming platform authentication with Discord and Twitch integration\n\n## Requirements\n\n**Environment Variables:**\n- `CLERK_PUBLISHABLE_KEY` - Clerk public key\n- `CLERK_SECRET_KEY` - Clerk secret key\n- Provider-specific credentials (e.g., `GOOGLE_CLIENT_ID`, `GOOGLE_CLIENT_SECRET`)\n\n**Dependencies:**\n- `@clerk/clerk-sdk-node` - Clerk Node.js SDK\n- `@clerk/nextjs` - Clerk Next.js integration (for Next.js apps)\n- `@clerk/clerk-react` - Clerk React components (for React apps)\n- Node.js 18+ or compatible runtime\n- jq (for JSON processing in scripts)\n\n**Clerk Project Setup:**\n- Active Clerk account (free tier available)\n- Clerk application configured\n- Development and production instances (optional)\n\n**For Each OAuth Provider:**\n- Developer account on provider platform\n- OAuth application created\n- Client credentials obtained\n- Redirect URIs configured\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n- NEVER hardcode actual API keys or secrets\n- NEVER include real credentials in examples\n- NEVER commit sensitive values to git\n\n- ALWAYS use placeholders: `your_service_key_here`\n- ALWAYS create `.env.example` with placeholders only\n- ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n- ALWAYS read from environment variables in code\n- ALWAYS document where to obtain keys\n\n**Placeholder format:** `{provider}_{env}_your_key_here`\n\nExample:\n```bash\n# .env.example (safe to commit)\nGOOGLE_CLIENT_ID=google_dev_your_client_id_here\nGOOGLE_CLIENT_SECRET=google_dev_your_client_secret_here\n\n# .env (NEVER commit)\nGOOGLE_CLIENT_ID=actual_client_id_from_google_cloud\nGOOGLE_CLIENT_SECRET=actual_secret_from_google_cloud\n```\n\n## Provider-Specific Setup Guides\n\n### Google OAuth Setup\n\n**Documentation:** templates/google/SETUP.md\n\n1. Create project in Google Cloud Console\n2. Enable Google+ API\n3. Configure OAuth consent screen\n4. Create OAuth 2.0 credentials\n5. Add authorized redirect URIs\n6. Copy Client ID and Client Secret\n7. Add to Clerk Dashboard\n\n**Required Scopes:**\n- `profile` - User profile information\n- `email` - Email address\n- `openid` - OpenID Connect\n\n### GitHub OAuth Setup\n\n**Documentation:** templates/github/SETUP.md\n\n1. Navigate to GitHub Settings > Developer Settings\n2. Create new OAuth App\n3. Configure application name and homepage URL\n4. Add authorization callback URL\n5. Generate client secret\n6. Add to Clerk Dashboard\n\n**Required Scopes:**\n- `read:user` - Read user profile\n- `user:email` - Access user email addresses\n\n### Discord OAuth Setup\n\n**Documentation:** templates/discord/SETUP.md\n\n1. Create application in Discord Developer Portal\n2. Navigate to OAuth2 section\n3. Add redirect URIs\n4. Copy Client ID and Client Secret\n5. Configure bot permissions (if needed)\n6. Add to Clerk Dashboard\n\n**Required Scopes:**\n- `identify` - User identity\n- `email` - Email address\n- `guilds` - Server list (optional)\n\n### Microsoft OAuth Setup\n\n**Documentation:** templates/microsoft/SETUP.md\n\n1. Register application in Azure AD Portal\n2. Configure platform settings (Web)\n3. Add redirect URIs\n4. Generate client secret\n5. Configure API permissions\n6. Add to Clerk Dashboard\n\n**Required Scopes:**\n- `openid` - OpenID Connect\n- `profile` - User profile\n- `email` - Email address\n- `User.Read` - Microsoft Graph API\n\n### Apple OAuth Setup\n\n**Documentation:** templates/apple/SETUP.md\n\n1. Create App ID in Apple Developer Portal\n2. Enable Sign in with Apple capability\n3. Create Service ID\n4. Configure domains and redirect URLs\n5. Create private key for authentication\n6. Add to Clerk Dashboard\n\n**Required Scopes:**\n- `name` - User name\n- `email` - Email address\n\n## Best Practices\n\n**Multi-Provider Strategy:**\n- Offer 2-3 primary providers (Google, GitHub, Discord)\n- Add enterprise providers for B2B apps (Microsoft, LinkedIn)\n- Include email/password as fallback option\n- Test all providers regularly\n\n**Redirect URL Management:**\n- Use environment-specific URLs\n- Whitelist exact URLs (no wildcards)\n- Use HTTPS in production\n- Document all redirect URLs\n\n**Scope Configuration:**\n- Request minimum necessary scopes\n- Document why each scope is needed\n- Handle scope changes gracefully\n- Test with restricted scopes\n\n**Error Handling:**\n- Handle provider-specific errors\n- Provide clear user feedback\n- Log authentication failures\n- Implement retry logic\n\n**Testing:**\n- Test all providers before launch\n- Verify redirect URLs in all environments\n- Test with fresh user accounts\n- Validate token refresh flows\n\n**Security:**\n- Store credentials in environment variables\n- Use HTTPS for all OAuth flows\n- Implement CSRF protection\n- Validate state parameter\n- Rotate secrets periodically\n\n## Troubleshooting\n\n**Redirect URI Mismatch:**\n- Verify exact URL match in provider console\n- Check for trailing slashes\n- Validate protocol (http vs https)\n- Confirm environment configuration\n\n**Invalid Client Credentials:**\n- Verify client ID and secret are correct\n- Check for whitespace in credentials\n- Ensure credentials match environment\n- Regenerate if compromised\n\n**Scope Authorization Failed:**\n- Verify scopes are supported by provider\n- Check provider API version\n- Validate scope syntax\n- Request app review if needed (some providers)\n\n**Token Exchange Error:**\n- Verify authorization code is valid\n- Check token endpoint URL\n- Validate code verifier (PKCE)\n- Ensure timely token exchange\n\n**User Profile Retrieval Failed:**\n- Verify access token is valid\n- Check profile endpoint permissions\n- Validate scope for profile access\n- Handle rate limits\n\n---\n\n**Plugin:** clerk\n**Version:** 1.0.0\n**Category:** Authentication\n**Skill Type:** Configuration\n**Providers Supported:** 19+"
              },
              {
                "name": "organization-management",
                "description": "Implement Clerk multi-tenant organization features with RBAC, role-based access control, organization switching, member management, and tenant isolation. Use when building multi-tenant SaaS applications, implementing organization hierarchies, configuring custom roles and permissions, setting up organization-scoped data isolation, or when user mentions organizations, RBAC, multi-tenancy, roles, permissions, organization switcher, member management, or tenant isolation.",
                "path": "plugins/clerk/skills/organization-management/SKILL.md",
                "frontmatter": {
                  "name": "organization-management",
                  "description": "Implement Clerk multi-tenant organization features with RBAC, role-based access control, organization switching, member management, and tenant isolation. Use when building multi-tenant SaaS applications, implementing organization hierarchies, configuring custom roles and permissions, setting up organization-scoped data isolation, or when user mentions organizations, RBAC, multi-tenancy, roles, permissions, organization switcher, member management, or tenant isolation.",
                  "allowed-tools": "Read, Write, Edit, Grep, Glob, Bash"
                },
                "content": "# Organization Management\n\n**Purpose:** Autonomously implement and configure Clerk organization features for multi-tenant applications with RBAC.\n\n**Activation Triggers:**\n- Multi-tenant application requirements\n- Organization creation/management needs\n- Role-based access control (RBAC) implementation\n- Organization switcher/profile components\n- Member invitation and management\n- Organization-scoped data isolation\n- Custom role and permission setup\n- Tenant-specific features\n\n**Key Resources:**\n- `scripts/setup-organizations.sh` - Enable and configure organizations\n- `scripts/configure-roles.sh` - Setup RBAC with custom roles\n- `scripts/test-org-isolation.sh` - Test tenant data isolation\n- `templates/organization-schema.md` - Multi-tenant database schema patterns\n- `templates/rbac-policies.ts` - Role and permission definitions\n- `templates/organization-switcher.tsx` - Organization switcher component\n- `examples/multi-tenant-app.tsx` - Complete multi-tenant application\n- `examples/org-admin-dashboard.tsx` - Organization admin interface\n\n## Implementation Workflow\n\n### 1. Enable Organizations in Clerk Dashboard\n\nBefore implementing organization features, enable them in your Clerk Dashboard:\n\n```bash\n# Run setup script to guide through Clerk Dashboard configuration\n./scripts/setup-organizations.sh\n```\n\n**Manual steps (documented in script):**\n1. Go to Clerk Dashboard ‚Üí Organization Settings\n2. Enable \"Organizations\" feature\n3. Configure organization creation settings\n4. Set up default roles (admin, member)\n5. Configure organization metadata fields (if needed)\n\n### 2. Configure RBAC (Role-Based Access Control)\n\n```bash\n# Setup custom roles and permissions\n./scripts/configure-roles.sh [basic|advanced|custom]\n\n# Examples:\n./scripts/configure-roles.sh basic       # Admin + Member only\n./scripts/configure-roles.sh advanced    # Admin + Manager + Member + Viewer\n./scripts/configure-roles.sh custom      # Interactive custom role creation\n```\n\n**Outputs:**\n- RBAC policy TypeScript file with role definitions\n- Permission checking middleware\n- Role-based route protection examples\n- Clerk Dashboard configuration guide\n\n### 3. Implement Organization Components\n\nUse templates to create organization UI:\n\n**Organization Switcher (for multi-org users):**\n```bash\n# Copy and customize organization switcher\ncp templates/organization-switcher.tsx src/components/OrganizationSwitcher.tsx\n```\n\n**Organization Schema (for database integration):**\n```bash\n# View multi-tenant database schema patterns\ncat templates/organization-schema.md\n```\n\n**RBAC Policies:**\n```bash\n# Copy RBAC policy definitions\ncp templates/rbac-policies.ts src/lib/rbac.ts\n```\n\n### 4. Test Organization Isolation\n\n```bash\n# Test tenant data isolation (requires project running)\n./scripts/test-org-isolation.sh\n\n# Validates:\n# - Data scoped to organization_id\n# - Cross-tenant data leakage prevention\n# - Role-based access enforcement\n# - Organization switching works correctly\n```\n\n## Organization Architecture Patterns\n\n### Multi-Tenant Data Isolation\n\n**Row-Level Security (RLS) Pattern:**\n```sql\n-- Every table includes organization_id\nCREATE TABLE projects (\n  id UUID PRIMARY KEY,\n  organization_id TEXT NOT NULL,\n  name TEXT NOT NULL,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- RLS Policy (Supabase/Postgres)\nCREATE POLICY \"Users can only access their org's projects\"\nON projects FOR ALL\nUSING (organization_id = current_setting('app.current_organization_id'));\n```\n\n**Application-Level Scoping:**\n```typescript\n// Always filter by organization_id in queries\nconst projects = await db.projects.findMany({\n  where: {\n    organizationId: user.organizationId\n  }\n});\n```\n\n### RBAC Implementation Levels\n\n**Level 1: Basic (Admin/Member)**\n- Admin: Full organization control\n- Member: Limited access to org resources\n\n**Level 2: Advanced (4+ roles)**\n- Admin: Full control\n- Manager: Team management, no billing\n- Member: Regular access\n- Viewer: Read-only access\n\n**Level 3: Custom (Granular permissions)**\n- Define specific permissions: `project:create`, `billing:manage`, `members:invite`\n- Assign permissions to roles\n- Check permissions at route/component level\n\n## Component Examples\n\n### Organization Switcher Component\n\nSee `templates/organization-switcher.tsx` for complete component with:\n- Organization selection dropdown\n- Create new organization option\n- Organization settings link\n- Active organization indicator\n- Keyboard navigation support\n\n### Organization Admin Dashboard\n\nSee `examples/org-admin-dashboard.tsx` for:\n- Member list with role management\n- Pending invitations display\n- Role assignment interface\n- Organization settings\n- Billing integration (if using Clerk Billing)\n\n### Multi-Tenant Application Structure\n\nSee `examples/multi-tenant-app.tsx` for:\n- Organization context provider\n- Organization-scoped routing\n- Data isolation patterns\n- Permission-based UI rendering\n- Organization switching flow\n\n## Common Use Cases\n\n### Use Case 1: SaaS Application with Teams\n```bash\n# Enable organizations + advanced RBAC\n./scripts/setup-organizations.sh\n./scripts/configure-roles.sh advanced\n\n# Implement organization switcher\ncp templates/organization-switcher.tsx src/components/\n```\n\n### Use Case 2: Enterprise Multi-Tenant Platform\n```bash\n# Enable organizations + custom RBAC with granular permissions\n./scripts/setup-organizations.sh\n./scripts/configure-roles.sh custom\n\n# Setup database isolation\n# Follow templates/organization-schema.md for RLS setup\n```\n\n### Use Case 3: Organization-Scoped Data Isolation\n```bash\n# Test isolation after implementing schema\n./scripts/test-org-isolation.sh\n\n# Validates:\n# - No cross-tenant data access\n# - RLS policies working correctly\n# - Organization switching updates context\n```\n\n## RBAC Permission Checking\n\n**Middleware Protection:**\n```typescript\n// Check role in middleware\nimport { auth } from '@clerk/nextjs/server';\n\nexport default async function middleware(req: Request) {\n  const { orgRole } = await auth();\n\n  if (orgRole !== 'org:admin') {\n    return new Response('Forbidden', { status: 403 });\n  }\n}\n```\n\n**Component-Level Checks:**\n```typescript\n// Hide UI based on role\nimport { useOrganization } from '@clerk/nextjs';\n\nfunction AdminOnlyButton() {\n  const { membership } = useOrganization();\n\n  if (membership?.role !== 'org:admin') return null;\n\n  return <button>Admin Action</button>;\n}\n```\n\n**Custom Permission Checks:**\n```typescript\n// Check specific permission\nimport { checkPermission } from '@/lib/rbac';\n\nif (await checkPermission(user, 'billing:manage')) {\n  // Allow billing access\n}\n```\n\n## Database Integration\n\n### Supabase Integration (with RLS)\n\n1. **Add organization_id to all tables**\n2. **Enable RLS on tables**\n3. **Create RLS policies scoped to organization_id**\n4. **Set organization context in Supabase client**\n\nSee `templates/organization-schema.md` for complete schema patterns.\n\n### Prisma Integration\n\n```typescript\n// Organization-scoped queries\nconst projects = await prisma.project.findMany({\n  where: {\n    organizationId: user.organizationId\n  }\n});\n\n// Middleware to auto-inject organization_id\nprisma.$use(async (params, next) => {\n  if (params.action === 'create') {\n    params.args.data.organizationId = getCurrentOrgId();\n  }\n  return next(params);\n});\n```\n\n## Environment Variables\n\n```bash\n# .env.example (no real keys!)\nNEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_your_clerk_key_here\nCLERK_SECRET_KEY=sk_test_your_clerk_secret_here\n\n# Organization feature flags (optional)\nNEXT_PUBLIC_ENABLE_ORG_CREATION=true\nNEXT_PUBLIC_MAX_ORGS_PER_USER=5\n```\n\n## Troubleshooting\n\n**Organizations not appearing:**\n- Verify organizations enabled in Clerk Dashboard\n- Check environment variables are set correctly\n- Ensure user has created/joined an organization\n\n**RBAC not working:**\n- Verify custom roles defined in Clerk Dashboard\n- Check role assignment in organization settings\n- Confirm middleware/permission checks use correct role names\n\n**Data isolation failing:**\n- Verify all tables include organization_id column\n- Check RLS policies are enabled and correct\n- Test with multiple organizations to confirm isolation\n\n## Resources\n\n**Scripts:** All scripts in `scripts/` directory are executable and include detailed usage instructions\n\n**Templates:** `templates/` contains production-ready components and schema patterns\n\n**Examples:** `examples/` contains complete application examples with organization features\n\n---\n\n**Clerk Dashboard Configuration Required:** Organizations must be enabled in your Clerk Dashboard before using this skill\n\n**Framework Support:** Next.js (App Router & Pages Router), React, Remix, Gatsby\n\n**Version:** 1.0.0\n**Clerk SDK Compatibility:** @clerk/nextjs 5+, @clerk/clerk-react 5+"
              },
              {
                "name": "session-management",
                "description": "Clerk session handling, JWT verification, token management, and multi-session workflows. Use when implementing session validation, JWT claims customization, token refresh patterns, session lifecycle management, or when user mentions session errors, authentication tokens, JWT verification, multi-device sessions, or session security.",
                "path": "plugins/clerk/skills/session-management/SKILL.md",
                "frontmatter": {
                  "name": "session-management",
                  "description": "Clerk session handling, JWT verification, token management, and multi-session workflows. Use when implementing session validation, JWT claims customization, token refresh patterns, session lifecycle management, or when user mentions session errors, authentication tokens, JWT verification, multi-device sessions, or session security.",
                  "allowed-tools": "Read, Grep, Glob, Bash"
                },
                "content": "# Session Management\n\n**Purpose:** Autonomously configure, validate, and troubleshoot Clerk session handling, JWT verification, and token management.\n\n**Activation Triggers:**\n- Session validation failures\n- JWT verification errors\n- Token expiration issues\n- Multi-session conflicts\n- Custom claims configuration\n- Session refresh problems\n- Authentication middleware setup\n- Session security audits\n\n**Key Resources:**\n- `scripts/configure-sessions.sh` - Session configuration helper\n- `scripts/setup-jwt.sh` - JWT template setup and validation\n- `scripts/test-sessions.sh` - Session testing and verification\n- `templates/session-config.ts` - Session configuration patterns\n- `templates/jwt-verification.ts` - JWT verification middleware\n- `templates/custom-claims.ts` - Custom JWT claims setup\n- `templates/session-types.ts` - TypeScript type definitions\n- `examples/multi-session.tsx` - Multi-session management\n- `examples/session-refresh.ts` - Session refresh patterns\n- `examples/session-debugging.ts` - Debugging utilities\n\n## Session Configuration Workflow\n\n### 1. Configure Session Settings\n\n```bash\n# Interactive session configuration\n./scripts/configure-sessions.sh\n\n# Options configured:\n# - Session lifetime (default, maximum)\n# - Multi-session mode (single, multi-device)\n# - Refresh token strategy\n# - Session activity tracking\n# - Secure cookie settings\n```\n\n**What it configures:**\n- ‚úÖ Session duration and expiration\n- ‚úÖ Multi-session behavior (allow/restrict)\n- ‚úÖ Token refresh intervals\n- ‚úÖ Activity-based session extension\n- ‚úÖ Cookie security attributes (SameSite, Secure, HttpOnly)\n\n### 2. Setup JWT Templates\n\n```bash\n# Create/update JWT templates for custom claims\n./scripts/setup-jwt.sh <template-name>\n\n# Examples:\n./scripts/setup-jwt.sh default      # Standard user claims\n./scripts/setup-jwt.sh hasura       # Hasura integration claims\n./scripts/setup-jwt.sh supabase     # Supabase integration claims\n./scripts/setup-jwt.sh custom       # Custom business logic claims\n```\n\n**Configures:**\n- Session ID and user ID claims\n- Organization membership\n- Role and permission claims\n- Custom metadata fields\n- Database integration claims (Hasura, Supabase)\n\n### 3. Test Session Validation\n\n```bash\n# Test session validation and JWT verification\n./scripts/test-sessions.sh <test-type>\n\n# Test types:\n# - basic           ‚Üí Verify session creation and validation\n# - jwt-verify      ‚Üí Test JWT signature verification\n# - custom-claims   ‚Üí Validate custom claims presence\n# - multi-session   ‚Üí Test multi-device session handling\n# - refresh         ‚Üí Test token refresh flow\n# - expiration      ‚Üí Test session expiration handling\n```\n\n## Session Management Patterns\n\n### Backend Session Verification\n\n**Next.js App Router:**\n```typescript\n// Use auth() for session access\nimport { auth } from '@clerk/nextjs/server';\n\nexport async function GET() {\n  const { userId, sessionId, sessionClaims } = await auth();\n\n  if (!userId) {\n    return Response.json({ error: 'Unauthorized' }, { status: 401 });\n  }\n\n  // Access custom claims\n  const userRole = sessionClaims?.role;\n  const orgId = sessionClaims?.org_id;\n\n  return Response.json({ userId, role: userRole });\n}\n```\n\n**Middleware Pattern:**\n```typescript\n// See templates/jwt-verification.ts for complete implementation\nimport { clerkMiddleware, createRouteMatcher } from '@clerk/nextjs/server';\n\nconst isProtectedRoute = createRouteMatcher(['/dashboard(.*)']);\n\nexport default clerkMiddleware((auth, req) => {\n  if (isProtectedRoute(req)) {\n    auth().protect();\n  }\n});\n```\n\n### Frontend Session Access\n\n**React/Next.js:**\n```typescript\nimport { useAuth, useSession } from '@clerk/nextjs';\n\nfunction Component() {\n  const { userId, sessionId } = useAuth();\n  const { session } = useSession();\n\n  // Session properties\n  const lastActiveAt = session?.lastActiveAt;\n  const expireAt = session?.expireAt;\n\n  // Session management\n  const handleRefresh = () => session?.touch(); // Extend session\n\n  return (\n    <div>\n      <p>Session ID: {sessionId}</p>\n      <p>Expires: {expireAt?.toLocaleString()}</p>\n    </div>\n  );\n}\n```\n\n### Multi-Session Handling\n\n**Enable multi-session mode:**\n```typescript\n// See examples/multi-session.tsx for complete implementation\nimport { useClerk } from '@clerk/nextjs';\n\nfunction SessionSwitcher() {\n  const { client } = useClerk();\n  const sessions = client?.sessions || [];\n\n  // Switch between sessions\n  const switchSession = async (sessionId: string) => {\n    await client?.setActiveSession(sessionId);\n  };\n\n  // Sign out of specific session\n  const signOutSession = async (sessionId: string) => {\n    const session = client?.sessions.find(s => s.id === sessionId);\n    await session?.remove();\n  };\n\n  return (/* session switcher UI */);\n}\n```\n\n### JWT Verification (Backend)\n\n**Manual verification:**\n```typescript\n// See templates/jwt-verification.ts\nimport { verifyToken } from '@clerk/backend';\n\nasync function verifySessionToken(token: string) {\n  try {\n    const payload = await verifyToken(token, {\n      secretKey: process.env.CLERK_SECRET_KEY!,\n      // Optional: custom verification options\n      authorizedParties: ['https://app.example.com'],\n    });\n\n    return {\n      valid: true,\n      userId: payload.sub,\n      sessionId: payload.sid,\n      claims: payload,\n    };\n  } catch (error) {\n    return { valid: false, error: error.message };\n  }\n}\n```\n\n### Custom Claims Configuration\n\n**Dashboard setup:**\n1. Navigate to Clerk Dashboard ‚Üí JWT Templates\n2. Create/edit template\n3. Add custom claims in JSON format:\n\n```json\n{\n  \"metadata\": \"{{user.public_metadata}}\",\n  \"role\": \"{{user.public_metadata.role}}\",\n  \"org_id\": \"{{org.id}}\",\n  \"org_role\": \"{{org_membership.role}}\",\n  \"permissions\": \"{{org_membership.permissions}}\"\n}\n```\n\n**Access in code:**\n```typescript\n// See templates/custom-claims.ts\nimport { auth } from '@clerk/nextjs/server';\n\nconst { sessionClaims } = await auth();\n\nconst role = sessionClaims?.role as string;\nconst orgId = sessionClaims?.org_id as string;\nconst permissions = sessionClaims?.permissions as string[];\n```\n\n## Session Refresh Patterns\n\n### Automatic Refresh\n\n**Client-side auto-refresh:**\n```typescript\n// See examples/session-refresh.ts\nimport { useSession } from '@clerk/nextjs';\nimport { useEffect } from 'react';\n\nfunction useSessionRefresh() {\n  const { session } = useSession();\n\n  useEffect(() => {\n    if (!session) return;\n\n    // Refresh session before expiration\n    const expiresAt = session.expireAt?.getTime() || 0;\n    const refreshAt = expiresAt - (5 * 60 * 1000); // 5 min before expiry\n    const now = Date.now();\n\n    if (refreshAt > now) {\n      const timeout = setTimeout(() => {\n        session.touch(); // Extends session\n      }, refreshAt - now);\n\n      return () => clearTimeout(timeout);\n    }\n  }, [session]);\n}\n```\n\n### Manual Session Extension\n\n```typescript\nimport { useSession } from '@clerk/nextjs';\n\nfunction Component() {\n  const { session } = useSession();\n\n  const extendSession = async () => {\n    // Touch session to extend lifetime\n    await session?.touch();\n  };\n\n  return <button onClick={extendSession}>Stay Logged In</button>;\n}\n```\n\n## Security Best Practices\n\n### Session Configuration\n\n**Recommended settings:**\n- Session lifetime: 7 days (default), 30 days (maximum)\n- Refresh window: Last 10% of session lifetime\n- Multi-session: Enabled for consumer apps, restricted for enterprise\n- Secure cookies: Always enable in production\n- SameSite: 'lax' (most apps), 'strict' (high security)\n\n### JWT Security\n\n**Verification checklist:**\n- ‚úÖ Always verify JWT signature\n- ‚úÖ Validate expiration (`exp` claim)\n- ‚úÖ Check issuer (`iss` claim matches Clerk)\n- ‚úÖ Verify audience (`aud` if using multiple apps)\n- ‚úÖ Validate authorized parties for multi-domain\n- ‚úÖ Never trust client-provided tokens without verification\n\n### Session Storage\n\n**Frontend:**\n- Clerk automatically manages session tokens\n- Never store session tokens in localStorage\n- Cookies are HttpOnly and Secure in production\n\n**Backend:**\n- Validate session on every protected request\n- Cache validation results with short TTL (< 1 min)\n- Invalidate cache on user metadata changes\n\n## Common Issues & Fixes\n\n### Session Not Persisting\n\n**Problem:** User logged out on page refresh\n\n**Solutions:**\n```bash\n# Check cookie configuration\n./scripts/test-sessions.sh basic\n\n# Verify:\n# - Domain settings match deployment URL\n# - SameSite attribute compatible with architecture\n# - Secure flag enabled in production only\n```\n\n### JWT Verification Failure\n\n**Problem:** `verifyToken` throws error\n\n**Diagnosis:**\n```bash\n./scripts/test-sessions.sh jwt-verify\n\n# Common causes:\n# - Wrong CLERK_SECRET_KEY (check .env)\n# - Token expired (check exp claim)\n# - Issuer mismatch (check iss claim)\n# - Invalid signature (token tampered)\n```\n\n### Custom Claims Not Available\n\n**Problem:** Custom claims undefined in sessionClaims\n\n**Fix:**\n```bash\n# Verify JWT template configuration\n./scripts/setup-jwt.sh custom\n\n# Steps:\n# 1. Ensure template is set as default in Dashboard\n# 2. User must sign out and sign in again\n# 3. Check claim paths match metadata structure\n```\n\n### Multi-Session Conflicts\n\n**Problem:** Wrong session active after sign-in\n\n**Solutions:**\n```typescript\n// See examples/multi-session.tsx\n\n// Force specific session active\nawait clerk.setActiveSession(sessionId);\n\n// Or restrict to single session in Dashboard:\n// Settings ‚Üí Sessions ‚Üí Multi-session handling ‚Üí Single session\n```\n\n## Resources\n\n**Scripts:** All scripts in `scripts/` directory handle:\n- Session configuration validation\n- JWT template setup and testing\n- Session flow verification\n- Error diagnosis and fixes\n\n**Templates:** `templates/` contains production-ready code for:\n- Session configuration objects\n- JWT verification middleware\n- Custom claims type definitions\n- Session refresh utilities\n\n**Examples:** `examples/` demonstrates:\n- Multi-session UI components\n- Session refresh strategies\n- Protected route patterns\n- Session debugging helpers\n\n## Security Compliance\n\n**CRITICAL:** This skill follows strict security rules:\n- All code examples use placeholder API keys only\n- No real secrets or credentials in templates\n- Environment variable references throughout\n- `.gitignore` protection documented in all setup scripts\n\n---\n\n**Supported Frameworks:** Next.js (App Router, Pages Router), React, Express, Fastify, Remix\n**Clerk SDK Version:** @clerk/nextjs 5+, @clerk/backend 1+\n**Version:** 1.0.0"
              },
              {
                "name": "supabase-clerk-sync",
                "description": "Clerk and Supabase integration patterns for user sync, JWT authentication, and RLS policies. Use when integrating Clerk authentication with Supabase, syncing user data between platforms, configuring RLS with Clerk JWT tokens, setting up webhooks for user events, implementing secure database access with Clerk identity, or when user mentions Clerk Supabase sync, user synchronization, JWT RLS, authentication webhooks, or database user management.",
                "path": "plugins/clerk/skills/supabase-clerk-sync/SKILL.md",
                "frontmatter": {
                  "name": "supabase-clerk-sync",
                  "description": "Clerk and Supabase integration patterns for user sync, JWT authentication, and RLS policies. Use when integrating Clerk authentication with Supabase, syncing user data between platforms, configuring RLS with Clerk JWT tokens, setting up webhooks for user events, implementing secure database access with Clerk identity, or when user mentions Clerk Supabase sync, user synchronization, JWT RLS, authentication webhooks, or database user management.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Supabase-Clerk Sync\n\nComprehensive integration patterns for syncing Clerk authentication with Supabase databases, including JWT configuration, RLS policies, webhook setup, and user data synchronization.\n\n## Instructions\n\n### When Integrating Clerk with Supabase\n\n1. **JWT Configuration Strategy**\n   - Configure Supabase to validate Clerk JWT tokens\n   - Extract Clerk JWT template from dashboard\n   - Configure Supabase auth settings with Clerk JWKS endpoint\n   - Map Clerk user claims to Supabase RLS policies\n   - Handle JWT expiration and refresh flows\n\n2. **User Sync Architecture**\n   - Choose sync strategy: Webhook-based (recommended) or Client-side\n   - Set up Clerk webhooks for user lifecycle events\n   - Create Supabase tables for user profile data\n   - Implement idempotent sync operations\n   - Handle race conditions and eventual consistency\n\n3. **RLS Policy Design**\n   - Use Clerk `sub` claim as user identifier in policies\n   - Extract user metadata from JWT for policy decisions\n   - Implement role-based access with Clerk organizations\n   - Create policies for user-owned resources\n   - Test policies with different JWT claims\n\n4. **Webhook Implementation**\n   - Verify Clerk webhook signatures (svix library)\n   - Handle user.created, user.updated, user.deleted events\n   - Implement retry logic for failed syncs\n   - Log sync operations for debugging\n   - Use Supabase service role key for webhook operations\n\n### Integration Workflow\n\n**Phase 1: JWT Setup**\n1. Get Clerk JWKS URL from dashboard\n2. Configure Supabase auth.jwt_secret\n3. Update Supabase JWT settings with Clerk issuer\n4. Test JWT validation with sample tokens\n\n**Phase 2: Database Schema**\n1. Create users table with Clerk ID mapping\n2. Add user metadata columns\n3. Set up RLS policies using JWT claims\n4. Create indexes for performance\n\n**Phase 3: Webhook Configuration**\n1. Deploy webhook endpoint (Supabase Edge Function or external)\n2. Register webhook URL in Clerk dashboard\n3. Select user events to sync\n4. Implement signature verification\n\n**Phase 4: Client Integration**\n1. Configure Supabase client with Clerk session token\n2. Implement token refresh in client\n3. Test authenticated requests\n4. Handle authentication errors\n\n### Security Best Practices\n\n**JWT Validation:**\n- Always verify JWT signature with Clerk JWKS\n- Validate issuer matches Clerk instance\n- Check token expiration\n- Reject tokens with missing required claims\n\n**RLS Policies:**\n- Use `auth.uid()` to extract Clerk user ID from JWT\n- Never trust client-provided user IDs\n- Test policies with different user roles\n- Implement least-privilege access\n\n**Webhook Security:**\n- Verify Svix signatures on all webhook requests\n- Use HTTPS for webhook endpoints\n- Store Clerk webhook secret securely\n- Implement rate limiting on webhook endpoints\n\n**API Keys:**\n- Never hardcode API keys in client code\n- Use environment variables for secrets\n- Rotate keys periodically\n- Use Supabase anon key for client, service role for webhooks\n\n### Sync Strategies\n\n**Webhook-Based Sync (Recommended):**\n- Real-time synchronization\n- Server-side security\n- Centralized logic\n- Better error handling\n- Recommended for production\n\n**Client-Side Sync:**\n- User initiates sync on login\n- Requires RLS policies for user writes\n- Simpler setup for prototypes\n- Race conditions possible\n\n### Common Integration Patterns\n\n**Pattern 1: Public Profile with Private Data**\n```\nusers_public (readable by all, RLS enforced)\n  - clerk_id\n  - username\n  - avatar_url\n\nusers_private (readable only by owner)\n  - clerk_id\n  - email\n  - metadata\n```\n\n**Pattern 2: Organization-Based Access**\n```\nRLS Policy:\n  auth.jwt()->>'org_id' = organizations.clerk_org_id\n```\n\n**Pattern 3: Role-Based Permissions**\n```\nRLS Policy:\n  auth.jwt()->>'role' IN ('admin', 'editor')\n```\n\n### Troubleshooting\n\n**JWT Validation Fails:**\n- Verify JWKS URL is correct\n- Check Supabase JWT secret configuration\n- Ensure token hasn't expired\n- Validate issuer claim matches\n\n**User Sync Delays:**\n- Check webhook delivery in Clerk dashboard\n- Verify webhook endpoint is accessible\n- Review webhook logs for errors\n- Confirm Supabase connection\n\n**RLS Policy Denies Access:**\n- Inspect JWT claims in request\n- Test policy SQL in Supabase SQL editor\n- Verify user ID extraction from JWT\n- Check table permissions\n\n## Scripts\n\nAutomated setup and configuration scripts:\n\n- `scripts/setup-sync.sh` - Configure Supabase for Clerk JWT validation\n- `scripts/configure-rls.sh` - Generate RLS policies for Clerk authentication\n- `scripts/create-webhooks.sh` - Deploy webhook infrastructure\n- `scripts/test-jwt.sh` - Test JWT validation and claim extraction\n- `scripts/sync-users.sh` - Manually trigger user synchronization\n\n## Templates\n\nIntegration code templates for different scenarios:\n\n### TypeScript Templates\n- `templates/supabase-client-clerk.ts` - Supabase client with Clerk session token\n- `templates/webhook-sync.ts` - Clerk webhook handler for user sync\n- `templates/edge-function-webhook.ts` - Supabase Edge Function webhook\n- `templates/middleware-auth.ts` - Next.js middleware with Clerk + Supabase\n\n### SQL Templates\n- `templates/rls-policies-clerk.sql` - Comprehensive RLS policies\n- `templates/user-schema.sql` - User tables schema\n- `templates/triggers.sql` - Database triggers for audit logging\n\n### Configuration Templates\n- `templates/env.example` - Environment variables template\n- `templates/clerk-jwt-template.json` - JWT template configuration\n\n## Examples\n\nComplete working examples:\n\n- `examples/complete-integration.tsx` - Full Next.js app with Clerk + Supabase\n- `examples/webhook-handler.ts` - Production webhook implementation\n- `examples/protected-route.tsx` - Protected page with RLS\n- `examples/organization-access.tsx` - Multi-tenant with organizations\n\n## Migration Guide\n\n### From Supabase Auth to Clerk\n\n1. **Export Users:**\n   - Extract user data from Supabase auth.users\n   - Prepare user import CSV for Clerk\n\n2. **Update Database:**\n   - Add clerk_id column to user tables\n   - Migrate RLS policies to use JWT claims\n   - Update foreign key references\n\n3. **Deploy Webhooks:**\n   - Set up webhook infrastructure\n   - Enable user sync\n\n4. **Update Client:**\n   - Replace Supabase auth with Clerk\n   - Update Supabase client initialization\n   - Test authentication flows\n\n5. **Cutover:**\n   - Enable Clerk in production\n   - Disable Supabase Auth\n   - Monitor for issues\n\n## Performance Optimization\n\n**Database Indexes:**\n```sql\nCREATE INDEX idx_users_clerk_id ON users(clerk_id);\nCREATE INDEX idx_orgs_clerk_org_id ON organizations(clerk_org_id);\n```\n\n**Webhook Performance:**\n- Use database connection pooling\n- Batch user updates when possible\n- Implement async processing for large syncs\n- Cache frequently accessed user data\n\n**Client Performance:**\n- Cache Supabase client instance\n- Refresh tokens proactively\n- Use Supabase realtime for live updates\n- Implement optimistic updates\n\n## Reference Documentation\n\n**Clerk:**\n- JWT Templates: Configure custom claims for Supabase\n- Webhooks: Event types and payload structure\n- Organizations: Multi-tenant patterns\n\n**Supabase:**\n- JWT Authentication: Custom JWT provider setup\n- RLS Policies: Policy syntax and testing\n- Edge Functions: Serverless webhook handlers\n- PostgreSQL Functions: Custom JWT claim extraction\n\n## Security Checklist\n\nBefore going to production:\n\n- [ ] JWT signature validation configured\n- [ ] Webhook signatures verified\n- [ ] RLS policies tested with different users\n- [ ] Service role key stored securely\n- [ ] Webhook endpoint uses HTTPS\n- [ ] Rate limiting implemented\n- [ ] Error logging configured\n- [ ] User data encrypted at rest\n- [ ] API keys rotated\n- [ ] Audit logging enabled\n\n## Use When\n\n- Integrating Clerk authentication with Supabase database\n- Syncing user profiles from Clerk to Supabase\n- Implementing Row Level Security with Clerk JWT tokens\n- Setting up webhooks for real-time user synchronization\n- Building multi-tenant applications with Clerk organizations\n- Migrating from Supabase Auth to Clerk\n- Configuring secure database access with external auth provider\n- Implementing organization-based data access patterns"
              },
              {
                "name": "testing-validation",
                "description": "Comprehensive testing and validation tools for Clerk authentication integrations. Includes E2E auth flow testing, security audits, configuration validation, unit testing patterns for sign-in/sign-up flows. Use when implementing Clerk tests, validating authentication setup, testing auth flows, running security audits, creating E2E tests for Clerk, or when user mentions Clerk testing, auth validation, E2E authentication tests, security audit, or test coverage.",
                "path": "plugins/clerk/skills/testing-validation/SKILL.md",
                "frontmatter": {
                  "name": "testing-validation",
                  "description": "Comprehensive testing and validation tools for Clerk authentication integrations. Includes E2E auth flow testing, security audits, configuration validation, unit testing patterns for sign-in/sign-up flows. Use when implementing Clerk tests, validating authentication setup, testing auth flows, running security audits, creating E2E tests for Clerk, or when user mentions Clerk testing, auth validation, E2E authentication tests, security audit, or test coverage.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Clerk Testing & Validation\n\nComprehensive testing and validation toolkit for Clerk authentication integrations. Provides test templates, validation scripts, security audit tools, and E2E testing patterns for sign-in, sign-up, session management, and multi-factor authentication flows.\n\n## Instructions\n\n### When Validating Clerk Setup\n\n1. **Run Configuration Validation**\n   - Execute `scripts/validate-setup.sh` to verify:\n     - Environment variables (CLERK_PUBLISHABLE_KEY, CLERK_SECRET_KEY)\n     - Middleware configuration\n     - Protected routes setup\n     - Provider configuration (Google, GitHub, etc.)\n   - Check output for missing configurations or security warnings\n   - Review generated validation report\n\n2. **What Gets Validated**\n   - Environment variable presence and format\n   - API key validity (publishable vs secret key patterns)\n   - ClerkProvider wrapper in app structure\n   - Middleware configuration in middleware.ts/js\n   - Protected route patterns in route configuration\n   - CORS and domain settings for production\n\n### When Testing Authentication Flows\n\n1. **Run E2E Authentication Tests**\n   - Execute `scripts/test-auth-flows.sh` to test:\n     - Sign-up flow (email/password, OAuth providers)\n     - Sign-in flow (all configured providers)\n     - Session persistence across page reloads\n     - Sign-out functionality\n     - Protected route access control\n   - Supports both Playwright and Cypress\n   - Generates test coverage reports\n\n2. **Authentication Flow Coverage**\n   - Email/password registration and login\n   - OAuth provider authentication (Google, GitHub, Microsoft)\n   - Magic link authentication\n   - Multi-factor authentication (2FA/MFA)\n   - Session management and token refresh\n   - User profile updates\n   - Password reset flows\n\n### When Running Security Audits\n\n1. **Execute Security Checks**\n   - Run `scripts/check-security.sh` to audit:\n     - Environment variable exposure (no keys in client bundles)\n     - Public vs secret key usage\n     - Protected route coverage\n     - Session security configuration\n     - CSRF protection implementation\n     - XSS prevention patterns\n   - Review security findings report\n   - Address high-priority vulnerabilities immediately\n\n2. **Security Checklist Items**\n   - No secret keys exposed to client\n   - All admin routes properly protected\n   - Session tokens stored securely (httpOnly cookies)\n   - Rate limiting on auth endpoints\n   - Input sanitization for user data\n   - HTTPS enforcement in production\n   - Proper CORS configuration\n\n### When Creating Unit Tests\n\n1. **Use Provided Test Templates**\n   - For **React components**: `templates/test-suites/clerk-react.test.tsx`\n   - For **Next.js pages**: `templates/test-suites/clerk-nextjs.test.tsx`\n   - For **API routes**: `templates/test-suites/clerk-api.test.ts`\n   - Templates include mocking patterns for Clerk hooks\n\n2. **Unit Test Coverage**\n   - Mock `useAuth()`, `useUser()`, `useSession()` hooks\n   - Test component behavior for authenticated/unauthenticated states\n   - Verify loading states during auth\n   - Test error handling for auth failures\n   - Validate conditional rendering based on auth status\n\n### When Creating E2E Tests\n\n1. **Use Playwright Templates**\n   - Base template: `templates/e2e-tests/clerk-auth-flows.spec.ts`\n   - OAuth template: `templates/e2e-tests/clerk-oauth.spec.ts`\n   - Protected routes: `templates/e2e-tests/clerk-protected-routes.spec.ts`\n   - Templates include Clerk test helpers and fixtures\n\n2. **E2E Test Patterns**\n   - Use Clerk test users (configured in .env.test)\n   - Test complete user journeys (sign-up ‚Üí profile ‚Üí sign-out)\n   - Verify redirect flows after authentication\n   - Test session persistence across browser tabs\n   - Validate error messages and UI feedback\n\n## Templates\n\n### Test Suite Templates\n\n**React Component Tests:**\n- `templates/test-suites/clerk-react.test.tsx` - Jest/Vitest tests with React Testing Library\n- `templates/test-suites/clerk-hooks.test.ts` - Unit tests for Clerk hook integrations\n- `templates/test-suites/clerk-components.test.tsx` - Tests for SignIn, SignUp, UserButton components\n\n**Next.js Tests:**\n- `templates/test-suites/clerk-nextjs.test.tsx` - App Router component tests\n- `templates/test-suites/clerk-middleware.test.ts` - Middleware function tests\n- `templates/test-suites/clerk-api.test.ts` - API route authentication tests\n\n**Backend Tests:**\n- `templates/test-suites/clerk-backend.test.ts` - Server-side auth validation\n- `templates/test-suites/clerk-webhooks.test.ts` - Webhook handler tests\n\n### E2E Test Templates\n\n**Playwright Tests:**\n- `templates/e2e-tests/clerk-auth-flows.spec.ts` - Complete auth flow testing\n- `templates/e2e-tests/clerk-oauth.spec.ts` - OAuth provider testing\n- `templates/e2e-tests/clerk-protected-routes.spec.ts` - Route protection tests\n- `templates/e2e-tests/clerk-session.spec.ts` - Session management tests\n- `templates/e2e-tests/clerk-mfa.spec.ts` - Multi-factor authentication tests\n\n**Cypress Tests:**\n- `templates/e2e-tests/cypress/clerk-signup.cy.ts` - Sign-up flow\n- `templates/e2e-tests/cypress/clerk-signin.cy.ts` - Sign-in flow\n- `templates/e2e-tests/cypress/clerk-profile.cy.ts` - User profile tests\n\n### Validation Resources\n\n- `templates/validation-checklist.md` - Comprehensive validation checklist\n- `templates/security-audit-report.md` - Security audit report template\n- `templates/test-coverage-report.md` - Test coverage analysis template\n\n## Scripts\n\n### Validation Scripts\n\n**`scripts/validate-setup.sh`**\n- Validates Clerk environment configuration\n- Checks API key format and presence\n- Verifies middleware and provider setup\n- Outputs detailed validation report\n- Exit code 0 for success, 1 for failures\n\n**Usage:**\n```bash\nbash scripts/validate-setup.sh [--fix]\n```\n\n### Testing Scripts\n\n**`scripts/test-auth-flows.sh`**\n- Runs E2E authentication flow tests\n- Supports Playwright and Cypress\n- Generates coverage reports\n- Can run in CI/CD environments\n\n**Usage:**\n```bash\nbash scripts/test-auth-flows.sh [--playwright|--cypress] [--headed]\n```\n\n**`scripts/run-unit-tests.sh`**\n- Executes Jest/Vitest unit tests\n- Focuses on Clerk component and hook tests\n- Generates coverage reports\n\n**Usage:**\n```bash\nbash scripts/run-unit-tests.sh [--watch] [--coverage]\n```\n\n### Security Scripts\n\n**`scripts/check-security.sh`**\n- Performs security audit of Clerk integration\n- Checks for exposed secrets\n- Validates authentication patterns\n- Outputs security findings report\n\n**Usage:**\n```bash\nbash scripts/check-security.sh [--detailed]\n```\n\n## Examples\n\n### Complete Test Examples\n\n**`examples/auth-flow-tests.spec.ts`**\n- Full Playwright test suite for authentication flows\n- Tests sign-up, sign-in, sign-out\n- Validates session persistence\n- Tests OAuth providers\n- Includes setup and teardown\n\n**`examples/security-audit.ts`**\n- Automated security audit script\n- Scans codebase for security issues\n- Checks environment variable usage\n- Validates route protection patterns\n- Generates detailed audit report\n\n**`examples/clerk-unit-tests.test.tsx`**\n- Comprehensive unit test examples\n- React component testing with Clerk hooks\n- Mocking patterns for useAuth, useUser\n- Testing authenticated/unauthenticated states\n\n**`examples/webhook-testing.test.ts`**\n- Clerk webhook handler tests\n- Validates signature verification\n- Tests event processing\n- Error handling patterns\n\n## Security: API Key Handling\n\n**CRITICAL:** This skill enforces security best practices:\n\n- **Validation scripts** check for exposed API keys in client code\n- **Security audit** scans for hardcoded credentials\n- **Test templates** use environment variables only\n- **Examples** demonstrate proper secret management\n\nAll generated tests use placeholders:\n```typescript\n// .env.test\nCLERK_PUBLISHABLE_KEY=pk_test_your_key_here\nCLERK_SECRET_KEY=sk_test_your_key_here\nTEST_USER_EMAIL=test_user@example.com\nTEST_USER_PASSWORD=test_password_here\n```\n\nNever commit real API keys or test credentials to version control.\n\n## Requirements\n\n**Testing Frameworks:**\n- Jest 29.x or Vitest 1.x (for unit tests)\n- Playwright 1.40+ or Cypress 13+ (for E2E tests)\n- React Testing Library 14+ (for component tests)\n\n**Clerk SDKs:**\n- @clerk/nextjs 4.x or 5.x\n- @clerk/clerk-react (for React apps)\n- @clerk/clerk-js (for vanilla JS)\n\n**Node.js:**\n- Node.js 18+ (LTS recommended)\n- npm 9+ or pnpm 8+\n\n**Environment:**\n- Test Clerk application (separate from production)\n- Test user accounts configured\n- .env.test file with test credentials\n\n## Best Practices\n\n1. **Separate Test Environments** - Use dedicated Clerk test application, never test against production\n2. **Mock External Services** - Mock OAuth providers in unit tests, use real providers only in E2E\n3. **Test User Isolation** - Create/delete test users for each test suite to avoid conflicts\n4. **Security First** - Always run security audit before deployment\n5. **Comprehensive Coverage** - Test both happy paths and error scenarios\n6. **CI/CD Integration** - Run validation and tests in CI pipeline\n7. **Regular Security Audits** - Schedule weekly security checks\n8. **Keep Tests Updated** - Update tests when Clerk SDK versions change\n\n## Validation Workflow\n\n**Recommended Testing Pipeline:**\n\n1. **Setup Validation** ‚Üí Run `validate-setup.sh` to ensure proper configuration\n2. **Unit Tests** ‚Üí Run component and hook tests with coverage\n3. **E2E Tests** ‚Üí Execute authentication flow tests\n4. **Security Audit** ‚Üí Run security checks before deployment\n5. **Review Reports** ‚Üí Analyze coverage and security findings\n6. **Fix Issues** ‚Üí Address any failures or warnings\n7. **Repeat** ‚Üí Run full suite in CI/CD pipeline\n\n---\n\n**Purpose**: Standardize Clerk authentication testing and security validation\n**Load when**: Testing Clerk integrations, validating auth setup, running security audits\n**Security Level**: High - Enforces environment variable usage, scans for exposed secrets"
              }
            ]
          },
          {
            "name": "redis",
            "description": "Production-ready Redis integration for caching, sessions, rate limiting, pub/sub, and AI embedding cache with multi-framework support",
            "source": "./plugins/redis",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "builder@example.com"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install redis@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-cache",
                "description": "Add caching layer with strategy selection",
                "path": "plugins/redis/commands/add-cache.md",
                "frontmatter": {
                  "description": "Add caching layer with strategy selection",
                  "argument-hint": [
                    "cache-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add caching layer with chosen strategy (cache-aside, write-through, write-behind).\n\n**Phase 1**: Detect framework and existing Redis setup\n**Phase 2**: Ask user for caching strategy and use case\n**Phase 3**: Task(subagent_type=\"redis:cache-architect\") to implement\n**Phase 4**: Verify caching works, measure hit rates\n**Phase 5**: Display summary and monitoring guidance"
              },
              {
                "name": "/add-cluster",
                "description": "Configure Redis Cluster",
                "path": "plugins/redis/commands/add-cluster.md",
                "frontmatter": {
                  "description": "Configure Redis Cluster",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Configure Redis Cluster for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/add-connection-pool",
                "description": "Configure connection pooling",
                "path": "plugins/redis/commands/add-connection-pool.md",
                "frontmatter": {
                  "description": "Configure connection pooling",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Configure connection pooling for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/add-monitoring",
                "description": "Add monitoring and metrics",
                "path": "plugins/redis/commands/add-monitoring.md",
                "frontmatter": {
                  "description": "Add monitoring and metrics",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add monitoring and metrics for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/add-pub-sub",
                "description": "Add pub/sub messaging for real-time features",
                "path": "plugins/redis/commands/add-pub-sub.md",
                "frontmatter": {
                  "description": "Add pub/sub messaging for real-time features",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add Redis pub/sub for real-time messaging.\n\n**Phase 1**: Detect framework and WebSocket support\n**Phase 2**: Ask for messaging use case (chat, notifications, events)\n**Phase 3**: Task(subagent_type=\"redis:pub-sub-specialist\") to implement\n**Phase 4**: Test pub/sub messaging\n**Phase 5**: Summary with channel configuration"
              },
              {
                "name": "/add-queue",
                "description": "Add simple queue implementation",
                "path": "plugins/redis/commands/add-queue.md",
                "frontmatter": {
                  "description": "Add simple queue implementation",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add simple queue implementation for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/add-rag",
                "description": "Add RAG (Retrieval-Augmented Generation) pipeline with Google Gemini File API and Redis caching",
                "path": "plugins/redis/commands/add-rag.md",
                "frontmatter": {
                  "description": "Add RAG (Retrieval-Augmented Generation) pipeline with Google Gemini File API and Redis caching",
                  "argument-hint": [
                    "feature-description"
                  ],
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Implement a complete RAG pipeline using Google Gemini File API as the primary RAG solution with Redis as an intelligent caching layer for Gemini responses, embeddings, and file processing results.\n\nCore Principles:\n- Google Gemini File API is the PRIMARY RAG solution (handles embeddings, file uploads, retrieval)\n- Redis provides CACHING layer (semantic cache for responses, embedding cache, file processing cache)\n- Optimize for cost/speed by caching expensive Gemini API calls\n- Support optional LangChain/LlamaIndex integration for flexibility\n- Never hardcode API keys - use placeholders and environment variables\n- Detect existing project structure before implementing\n\nPhase 1: Discovery\nGoal: Understand project context and RAG requirements\n\nActions:\n- Parse $ARGUMENTS for feature description or use case\n- If unclear, use AskUserQuestion to gather:\n  - What documents/data will be processed? (PDFs, text files, web pages, etc.)\n  - What queries will users ask? (search, Q&A, summarization, etc.)\n  - Caching strategy preference? (aggressive for cost savings, conservative for freshness)\n  - Integration preference? (Gemini only, or also LangChain/LlamaIndex)\n- Detect project type and framework\n- Example: !{bash ls package.json requirements.txt pyproject.toml 2>/dev/null}\n- Check for existing Redis configuration\n- Example: @.env.example\n\nPhase 2: Architecture Planning\nGoal: Design RAG pipeline architecture\n\nActions:\n- Load existing project configuration files for context\n- Determine architecture based on requirements:\n  - **Gemini File API Layer**: File uploads, vector embeddings, semantic retrieval\n  - **Redis Caching Layer**: Semantic cache, embedding cache, file processing results\n  - **Optional Framework Integration**: LangChain or LlamaIndex abstractions\n- Identify required components:\n  - File upload/processing pipeline\n  - Embedding generation (via Gemini)\n  - Vector similarity search (via Gemini)\n  - Redis semantic caching configuration\n  - Query processing and response generation\n- Present architecture plan to user for confirmation\n\nPhase 3: Implementation\nGoal: Build RAG pipeline with Gemini + Redis\n\nActions:\n\nTask(description=\"Implement RAG pipeline with Google Gemini File API and Redis caching\", subagent_type=\"redis:rag-specialist\", prompt=\"You are the redis:rag-specialist agent. Implement a complete RAG pipeline with Google Gemini File API and Redis caching for $ARGUMENTS.\n\nContext from Discovery:\n- Feature description: $ARGUMENTS\n- Project type detected\n- Existing Redis configuration (if any)\n- User requirements gathered\n\nArchitecture Requirements:\n- **PRIMARY**: Google Gemini File API for RAG\n  - File upload and processing (PDF, text, web)\n  - Vector embedding generation\n  - Semantic search and retrieval\n  - Document chunking strategies\n- **CACHING**: Redis for performance optimization\n  - Semantic cache for Gemini API responses\n  - Embedding cache to avoid recomputation\n  - File processing results cache\n  - Query result cache with TTL\n- **OPTIONAL**: LangChain/LlamaIndex integration\n  - Only if user requested framework integration\n  - Use as abstraction layer over Gemini + Redis\n\nSecurity Requirements:\n- Use environment variables for API keys (GEMINI_API_KEY, REDIS_URL)\n- Create .env.example with placeholders only\n- Never hardcode credentials\n- Document key acquisition in setup guide\n\nImplementation Deliverables:\n1. Gemini File API integration code\n   - File upload handlers\n   - Embedding generation functions\n   - Semantic search/retrieval logic\n2. Redis caching layer\n   - Semantic cache implementation\n   - Embedding cache with TTL\n   - File processing cache\n3. RAG pipeline orchestration\n   - Query processing flow\n   - Cache-first strategy\n   - Fallback to Gemini API\n4. Configuration files\n   - .env.example with placeholders\n   - Redis cache configuration\n   - Gemini API settings\n5. Documentation\n   - Setup guide with API key instructions\n   - Usage examples\n   - Caching strategy explanation\n   - Cost optimization tips\n6. Example code\n   - Sample RAG queries\n   - File upload examples\n   - Cache monitoring\n\nExpected Output: Complete RAG implementation with Gemini as primary engine and Redis as intelligent cache\")\n\nPhase 4: Validation\nGoal: Verify RAG pipeline functionality\n\nActions:\n- Check that all required files were created\n- Verify Redis caching configuration\n- Validate Gemini API integration setup\n- Run example queries if test data provided\n- Example: !{bash python -c \"import redis; redis.Redis().ping()\" 2>/dev/null && echo \"Redis OK\" || echo \"Redis not running\"}\n\nPhase 5: Summary\nGoal: Document implementation and next steps\n\nActions:\n- Summarize RAG pipeline architecture:\n  - Gemini File API components implemented\n  - Redis caching layers configured\n  - Framework integrations (if any)\n- List files created/modified\n- Highlight key configuration:\n  - Cache TTL settings\n  - Embedding dimensions\n  - Chunk sizes and strategies\n- Provide next steps:\n  - Set up Gemini API key (link to console)\n  - Configure Redis connection\n  - Upload first documents\n  - Test sample queries\n- Show cost optimization tips:\n  - Cache hit rate monitoring\n  - Embedding reuse strategies\n  - Query result caching"
              },
              {
                "name": "/add-rate-limiting",
                "description": "Add rate limiting for APIs",
                "path": "plugins/redis/commands/add-rate-limiting.md",
                "frontmatter": {
                  "description": "Add rate limiting for APIs",
                  "argument-hint": [
                    "requests-per-minute"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add rate limiting to protect APIs.\n\n**Phase 1**: Detect API framework\n**Phase 2**: Ask for rate limit strategy (per-user, per-IP, per-key)\n**Phase 3**: Task(subagent_type=\"redis:rate-limiter-specialist\") to implement\n**Phase 4**: Test rate limiting, verify 429 responses\n**Phase 5**: Summary with rate limit configuration"
              },
              {
                "name": "/add-redisvl",
                "description": "Add RedisVL (Redis Vector Library) integration for vector similarity search with HNSW",
                "path": "plugins/redis/commands/add-redisvl.md",
                "frontmatter": {
                  "description": "Add RedisVL (Redis Vector Library) integration for vector similarity search with HNSW",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Integrate RedisVL for vector similarity search with HNSW indexing\n\nCore Principles:\n- Detect project language and framework before setup\n- Use environment variables for Redis configuration\n- Follow RedisVL best practices for vector search\n- Provide clear setup instructions and examples\n\nPhase 1: Discovery\nGoal: Understand project structure and requirements\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory)\n- Detect project language (Python, Node.js, etc.)\n- Check if Redis is already configured\n- Load package manager files: @package.json or @pyproject.toml or @requirements.txt\n- Detect existing vector search implementations\n\nPhase 2: Requirements Gathering\nGoal: Clarify vector search configuration needs\n\nActions:\n- If requirements are unclear, use AskUserQuestion to gather:\n  - What data will be indexed? (documents, images, embeddings)\n  - What embedding model? (OpenAI, Cohere, local model)\n  - What distance metric? (cosine, euclidean, dot product)\n  - Index size expectations? (1K, 100K, 1M+ vectors)\n  - HNSW parameters? (M, ef_construction, ef_runtime)\n- Document user requirements for agent context\n\nPhase 3: Pre-flight Validation\nGoal: Verify environment readiness\n\nActions:\n- Check Redis availability: !{bash redis-cli ping 2>/dev/null || echo \"Redis not running\"}\n- Verify Redis version supports vector search (7.2+)\n- Check if RedisVL is already installed\n- Detect conflicts with existing vector search libraries\n\nPhase 4: Implementation\nGoal: Execute RedisVL integration\n\nActions:\n\nTask(description=\"Add RedisVL integration\", subagent_type=\"redis:redisvl-integrator\", prompt=\"You are the redisvl-integrator agent. Add RedisVL (Redis Vector Library) integration for $ARGUMENTS.\n\nProject Context:\n- Language: [Detected from Phase 1]\n- Framework: [Detected from Phase 1]\n- Existing Redis config: [From Phase 1]\n\nUser Requirements:\n- Data type: [From Phase 2]\n- Embedding model: [From Phase 2]\n- Distance metric: [From Phase 2]\n- Index size: [From Phase 2]\n- HNSW parameters: [From Phase 2]\n\nImplementation Tasks:\n1. Install RedisVL package (pip install redisvl OR npm install redisvl)\n2. Configure Redis connection with environment variables\n3. Create vector index schema with HNSW algorithm\n4. Generate example code for:\n   - Creating vector index\n   - Storing embeddings\n   - Performing similarity search\n   - Updating/deleting vectors\n5. Add configuration file (.env.example with placeholders)\n6. Create setup documentation\n\nSecurity Requirements:\n- Use REDIS_URL environment variable\n- Never hardcode credentials\n- Add .env to .gitignore\n- Document where to get Redis credentials\n\nExpected Deliverables:\n- RedisVL package installed\n- Vector index schema file\n- Example usage code\n- Environment variable template\n- Setup documentation with next steps\")\n\nPhase 5: Verification\nGoal: Validate RedisVL integration\n\nActions:\n- Verify RedisVL package is installed\n- Check environment variable template exists\n- Validate example code syntax\n- Test Redis connection if available\n- Run type checking if TypeScript: !{bash npm run typecheck 2>/dev/null || true}\n- Run linting if configured: !{bash npm run lint 2>/dev/null || pylint . 2>/dev/null || true}\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize changes made:\n  - Package installed\n  - Files created/modified\n  - Vector index configuration\n  - HNSW parameters used\n- Provide next steps:\n  - Set Redis URL in environment\n  - Configure embedding model\n  - Run example code to test\n  - Customize HNSW parameters for production\n- Share relevant file paths (absolute paths)\n- Display RedisVL documentation links"
              },
              {
                "name": "/add-semantic-cache",
                "description": "Add AI query result caching with semantic similarity",
                "path": "plugins/redis/commands/add-semantic-cache.md",
                "frontmatter": {
                  "description": "Add AI query result caching with semantic similarity",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add semantic caching for AI query results.\n\n**Phase 1**: Detect AI usage in project\n**Phase 2**: Ask for similarity threshold (0.85-0.95)\n**Phase 3**: Task(subagent_type=\"redis:semantic-cache-specialist\") to implement\n**Phase 4**: Tune similarity threshold, measure savings\n**Phase 5**: Summary with cost reduction metrics"
              },
              {
                "name": "/add-sentinel",
                "description": "Configure Redis Sentinel for HA",
                "path": "plugins/redis/commands/add-sentinel.md",
                "frontmatter": {
                  "description": "Configure Redis Sentinel for HA",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Configure Redis Sentinel for HA for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/add-session-store",
                "description": "Add session management with Redis",
                "path": "plugins/redis/commands/add-session-store.md",
                "frontmatter": {
                  "description": "Add session management with Redis",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add secure session storage with Redis.\n\n**Phase 1**: Detect framework (FastAPI, Next.js, Express, Django)\n**Phase 2**: Ask about auth requirements (OAuth, JWT, custom)\n**Phase 3**: Task(subagent_type=\"redis:session-manager\") to implement\n**Phase 4**: Test session create/read/delete, verify security\n**Phase 5**: Summary with session configuration details"
              },
              {
                "name": "/add-vector-cache",
                "description": "Add AI embedding cache for cost optimization",
                "path": "plugins/redis/commands/add-vector-cache.md",
                "frontmatter": {
                  "description": "Add AI embedding cache for cost optimization",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Add embedding cache to reduce AI API costs by 50%+.\n\n**Phase 1**: Detect AI framework (LangChain, LlamaIndex, custom)\n**Phase 2**: Ask for AI provider (OpenAI, Anthropic, Cohere)\n**Phase 3**: Task(subagent_type=\"redis:vector-cache-specialist\") to implement\n**Phase 4**: Measure cache hit rates and cost savings\n**Phase 5**: Summary with optimization tips"
              },
              {
                "name": "/add-vector-search",
                "description": "Add vector search capabilities with KNN, range queries, and metadata filtering",
                "path": "plugins/redis/commands/add-vector-search.md",
                "frontmatter": {
                  "description": "Add vector search capabilities with KNN, range queries, and metadata filtering",
                  "argument-hint": "<feature-description>",
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Implement comprehensive vector search capabilities in Redis with KNN search, range queries, metadata filtering, and hybrid search patterns.\n\nCore Principles:\n- Detect existing Redis configuration before assuming structure\n- Ask clarifying questions about vector dimensions and use cases\n- Follow Redis vector search best practices and indexing strategies\n- Provide runtime query configuration examples\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n- Never hardcode actual API keys or secrets\n- Never include real credentials in examples\n- Always use placeholders: `your_redis_key_here`\n- Always read from environment variables in code\n- Always add `.env*` to `.gitignore` (except `.env.example`)\n- Always document where to obtain keys\n\n**Placeholder format:** `redis_{env}_your_key_here`\n\nPhase 1: Discovery\nGoal: Understand project context and vector search requirements\n\nActions:\n- Parse $ARGUMENTS for feature description and requirements\n- Check if Redis is already configured: @.env, @package.json, @requirements.txt\n- Detect project language/framework:\n  - !{bash ls package.json pyproject.toml go.mod 2>/dev/null}\n- Load existing Redis configuration if present\n\nPhase 2: Requirements Gathering\nGoal: Clarify vector search specifications\n\nActions:\n- If $ARGUMENTS lacks detail, use AskUserQuestion to gather:\n  - Vector dimension (e.g., 1536 for OpenAI embeddings)\n  - Use case (semantic search, recommendations, image similarity)\n  - Metadata fields for filtering (user_id, category, timestamp)\n  - Distance metric (cosine, euclidean, inner product)\n- Summarize requirements and confirm with user\n\nPhase 3: Analysis\nGoal: Understand existing codebase patterns\n\nActions:\n- Search for existing Redis usage:\n  - !{bash find . -type f \\( -name \"*.ts\" -o -name \"*.js\" -o -name \"*.py\" \\) -exec grep -l \"redis\\|Redis\" {} \\; 2>/dev/null | head -10}\n- Check for existing vector/embedding code:\n  - !{bash find . -type f \\( -name \"*.ts\" -o -name \"*.js\" -o -name \"*.py\" \\) -exec grep -l \"embedding\\|vector\" {} \\; 2>/dev/null | head -10}\n- Read relevant files to understand current architecture\n- Identify integration points for vector search\n\nPhase 4: Planning\nGoal: Design the vector search implementation\n\nActions:\n- Outline approach: index schema, query patterns, integration points\n- Present plan: index config, code structure, query examples, performance notes\n- Get user approval before implementing\n\nPhase 5: Implementation\nGoal: Build vector search capabilities with specialist agent\n\nDO NOT START WITHOUT USER APPROVAL\n\nActions:\n\nTask(description=\"Implement Redis vector search\", subagent_type=\"redis:vector-search-specialist\", prompt=\"You are the redis:vector-search-specialist agent. Implement comprehensive vector search capabilities for $ARGUMENTS.\n\nRequirements from discovery:\n- Vector dimension: [dimension from Phase 2]\n- Use case: [use case from Phase 2]\n- Metadata fields: [fields from Phase 2]\n- Distance metric: [metric from Phase 2]\n- Project language: [detected language]\n\nImplementation Requirements:\n- Create vector index with proper schema configuration\n- Implement KNN (k-nearest neighbor) search queries\n- Implement vector range queries for similarity thresholds\n- Implement metadata filtering combined with vector search\n- Implement hybrid search (vector + keyword/tag filtering)\n- Provide runtime query configuration examples\n- Include error handling and validation\n- Add comprehensive code comments\n\nSearch Patterns to Implement:\n1. Pure KNN: Find top-k most similar vectors\n2. Range Query: Find all vectors within distance threshold\n3. Metadata Filter: KNN with user_id/category/tag filters\n4. Hybrid Search: Combine vector similarity with keyword matching\n5. Multi-vector: Search across multiple vector fields\n\nExpected Deliverables:\n- Index creation code with schema definition\n- Query functions for each search pattern\n- Example usage code demonstrating all patterns\n- Configuration file with tunable parameters\n- Documentation of query capabilities and performance tips\n\nUse existing project patterns and follow codebase conventions.\")\n\nPhase 6: Verification\nGoal: Validate vector search implementation\n\nActions:\n- Check that all required files were created\n- Verify index schema includes all specified fields\n- Verify all search patterns are implemented:\n  - KNN search\n  - Range queries\n  - Metadata filtering\n  - Hybrid search\n- Review code for proper error handling\n- Check for placeholder API keys (no hardcoded secrets)\n- Run type checking if applicable:\n  - !{bash npm run typecheck 2>/dev/null || python -m mypy . 2>/dev/null || echo \"No type checking available\"}\n\nPhase 7: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize implementation:\n  - Files created and their purposes\n  - Index schema configuration\n  - Available search patterns\n  - Example queries for each pattern\n  - Configuration parameters\n- Highlight key features:\n  - Vector dimension and distance metric\n  - Metadata filtering capabilities\n  - Hybrid search options\n  - Performance considerations\n- Suggest next steps:\n  - Test with sample embeddings\n  - Tune index parameters for dataset size\n  - Add monitoring/logging for query performance\n  - Consider adding batch indexing for large datasets"
              },
              {
                "name": "/create-vector-index",
                "description": "Create vector index with FLAT or HNSW algorithm for similarity search",
                "path": "plugins/redis/commands/create-vector-index.md",
                "frontmatter": {
                  "description": "Create vector index with FLAT or HNSW algorithm for similarity search",
                  "argument-hint": "<index-name> [--type=FLAT|HNSW] [--metric=COSINE|L2|IP] [--dimensions=N]",
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Create a Redis vector index configured for similarity search with support for FLAT or HNSW algorithms, custom distance metrics, and metadata fields.\n\nCore Principles:\n- Detect existing Redis configuration before creating index\n- Validate schema and parameters before execution\n- Support both FLAT (exact search) and HNSW (approximate) algorithms\n- Configure appropriate distance metrics (COSINE, L2, IP)\n- Add metadata fields for filtering and hybrid search\n\nPhase 1: Discovery\nGoal: Understand project context and requirements\n\nActions:\n- Parse $ARGUMENTS to extract:\n  - Index name (required)\n  - Index type: --type=FLAT or --type=HNSW (default: HNSW)\n  - Distance metric: --metric=COSINE|L2|IP (default: COSINE)\n  - Vector dimensions: --dimensions=N (required)\n  - Additional metadata fields (optional)\n- Detect project structure and Redis configuration:\n  - Check for existing Redis connection files\n  - Look for environment variables or config files\n  - Example: !{bash find . -name \"*.env*\" -o -name \"*redis*config*\" 2>/dev/null | head -5}\n- If any required parameters are missing, use AskUserQuestion:\n  - What is the vector dimension? (e.g., 1536 for OpenAI embeddings)\n  - Which index type? (FLAT for exact search, HNSW for approximate)\n  - Which distance metric? (COSINE for normalized vectors, L2/IP for others)\n  - What metadata fields to include? (e.g., user_id, category, timestamp)\n\nPhase 2: Validation\nGoal: Verify inputs and environment are ready\n\nActions:\n- Validate index parameters:\n  - Index name is valid (alphanumeric, underscores)\n  - Dimensions is positive integer\n  - Type is either FLAT or HNSW\n  - Metric is COSINE, L2, or IP\n- Check Redis connection configuration:\n  - Verify REDIS_URL or connection settings exist\n  - Example: @.env (if exists)\n- Confirm prerequisites:\n  - Redis Stack or RedisJSON module available\n  - Vector search capability enabled\n\nPhase 3: Implementation\nGoal: Create vector index using specialized agent\n\nActions:\n\nTask(description=\"Create vector index\", subagent_type=\"vector-index-architect\", prompt=\"You are the vector-index-architect agent. Create a Redis vector index for $ARGUMENTS.\n\nContext:\n- Project directory detected in Phase 1\n- Index parameters validated in Phase 2\n- Redis configuration available\n\nRequirements:\n- Create FT.CREATE command with proper schema\n- Configure index type (FLAT or HNSW) based on parameters\n- Set up vector field with correct dimensions and distance metric\n- Add metadata fields for filtering (if specified)\n- Include schema validation\n- Generate example queries for the index\n- Provide connection code snippet\n\nIndex Configuration:\n- Name: [extracted from $ARGUMENTS]\n- Type: [FLAT or HNSW]\n- Metric: [COSINE, L2, or IP]\n- Dimensions: [extracted from $ARGUMENTS]\n- Metadata: [extracted from $ARGUMENTS or defaults]\n\nExpected output:\n1. Redis FT.CREATE command for index creation\n2. Schema validation script\n3. Example search queries (KNN, range, hybrid)\n4. Connection code snippet for application integration\n5. Performance tuning recommendations\")\n\nPhase 4: Verification\nGoal: Validate the created index configuration\n\nActions:\n- Review agent's output for completeness\n- Check that FT.CREATE command includes:\n  - Correct index type (FLAT or HNSW)\n  - Vector field with proper VECTOR parameters\n  - Distance metric configuration\n  - All metadata fields\n- Verify example queries are syntactically correct\n- Confirm connection code matches project language/framework\n\nPhase 5: Summary\nGoal: Report index creation and next steps\n\nActions:\n- Display index configuration summary:\n  - Index name and type\n  - Vector dimensions and metric\n  - Metadata fields included\n  - Algorithm parameters (if HNSW: M, EF_CONSTRUCTION, EF_RUNTIME)\n- Show FT.CREATE command to execute\n- Provide example search queries\n- Suggest next steps:\n  - Execute FT.CREATE command on Redis instance\n  - Insert sample vectors for testing\n  - Integrate search queries into application\n  - Monitor index performance and adjust parameters"
              },
              {
                "name": "/deploy",
                "description": "Production deployment",
                "path": "plugins/redis/commands/deploy.md",
                "frontmatter": {
                  "description": "Production deployment",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Production deployment for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/init",
                "description": "Initialize Redis in project with framework detection",
                "path": "plugins/redis/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Redis in project with framework detection",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite, AskUserQuestion"
                },
                "content": "üö® **EXECUTION NOTICE FOR CLAUDE**\n[Standard execution notice - see CLAUDE.md]\n\n## Security Requirements\n@~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Arguments**: $ARGUMENTS\n\n**Goal**: Initialize Redis in a project with automatic framework detection and production-ready configuration.\n\n**Core Principles**:\n- Auto-detect framework before configuration\n- Environment variables for all credentials\n- Never hardcode passwords or connection strings\n- Verify setup before completion\n\n**Phase 0: Create Todo List**\nTodoWrite: Track all initialization phases\n\n**Phase 1: Discovery**\nGoal: Detect framework and project structure\n\nActions:\n- Parse $ARGUMENTS for project path (default: current directory)\n- Detect framework:\n  - Read package.json ‚Üí Node.js (Next.js, Express)\n  - Read requirements.txt/pyproject.toml ‚Üí Python (FastAPI, Django, Flask)\n- Check for existing Redis configuration\n- Ask about deployment target (local Docker, Redis Cloud, self-hosted)\n\n**Phase 2: Load Documentation**\nGoal: Fetch framework-specific Redis docs\n\nActions:\n- WebFetch: https://redis.io/docs/latest/develop/connect/clients/\n- Based on framework:\n  - If Python: WebFetch https://redis.io/docs/latest/develop/clients/redis-py/\n  - If Node.js: WebFetch https://redis.io/docs/latest/develop/clients/node-redis/\n\n**Phase 3: Implementation**\nGoal: Set up Redis client and configuration\n\nActions:\nTask(description=\"Initialize Redis\", subagent_type=\"redis:redis-setup-agent\", prompt=\"Initialize Redis for detected framework.\n\nFramework: [detected framework]\nDeployment target: [user selected]\n\nDeliverables:\n- Install Redis client library\n- Create .env.example with placeholders\n- Create .env.development for local Docker\n- Configure Redis client with connection pooling\n- Add to framework (lifespan events, singleton, middleware)\n- Create docker-compose.yml if needed\n- Add .gitignore rules for .env files\n- Test connection with ping\")\n\n**Phase 4: Verification**\nGoal: Test Redis setup\n\nActions:\n- Run connection test\n- Verify .env files created correctly\n- Check .gitignore protects secrets\n- Validate no hardcoded credentials\n\n**Phase 5: Summary**\nGoal: Display setup results\n\nActions:\n- Show what was configured\n- Display .env.example content\n- Next steps (add caching, sessions, etc.)\n- Security reminders"
              },
              {
                "name": "/integrate-celery",
                "description": "Configure as Celery broker/backend",
                "path": "plugins/redis/commands/integrate-celery.md",
                "frontmatter": {
                  "description": "Configure as Celery broker/backend",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Configure as Celery broker/backend for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/integrate-express",
                "description": "Integrate with Express",
                "path": "plugins/redis/commands/integrate-express.md",
                "frontmatter": {
                  "description": "Integrate with Express",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Integrate with Express for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/integrate-fastapi",
                "description": "Integrate with FastAPI",
                "path": "plugins/redis/commands/integrate-fastapi.md",
                "frontmatter": {
                  "description": "Integrate with FastAPI",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Integrate with FastAPI for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/integrate-langchain",
                "description": "Integrate LangChain vector store and memory with Redis",
                "path": "plugins/redis/commands/integrate-langchain.md",
                "frontmatter": {
                  "description": "Integrate LangChain vector store and memory with Redis",
                  "argument-hint": "none",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\nGoal: Integrate LangChain vector store and memory capabilities with Redis for AI applications\n\nCore Principles:\n- Detect existing LangChain usage before adding\n- Ask user which features they need (vector store, memory, chat history, semantic cache)\n- Use RedisVectorStore for embeddings and semantic search\n- Implement appropriate memory types based on use case\n- Follow LangChain best practices and patterns\n\nPhase 1: Discovery\nGoal: Understand project context and LangChain usage\n\nActions:\n- Detect if LangChain is already installed\n- Check for existing vector store or memory implementations\n- Identify AI/LLM usage patterns in codebase\n- Example: !{bash grep -r \"langchain\" . --include=\"*.py\" --include=\"*.js\" --include=\"*.ts\" 2>/dev/null | head -5}\n\nPhase 2: Requirements Gathering\nGoal: Determine which LangChain + Redis features to integrate\n\nActions:\n- Use AskUserQuestion to determine needs:\n  - Which features? (vector store, conversation memory, entity memory, summary memory, chat history, semantic cache)\n  - What embedding model? (OpenAI, Anthropic, local model)\n  - Memory retention policy? (session-based, persistent, TTL)\n  - Semantic similarity threshold? (0.85-0.95 recommended)\n- Confirm project type (Python, JavaScript/TypeScript)\n- Validate Redis connection details available\n\nPhase 3: Planning\nGoal: Design the LangChain + Redis integration approach\n\nActions:\n- Based on requirements, plan integration:\n  - Vector store: RedisVectorStore with HNSW indexes\n  - Memory types: ConversationBufferMemory, EntityMemory, ConversationSummaryMemory\n  - Chat history: RedisChatMessageHistory for persistence\n  - Semantic cache: Integrated with LangChain cache layer\n- Identify files to create/modify\n- Present plan to user for approval\n\nPhase 4: Implementation\nGoal: Integrate LangChain features with Redis\n\nActions:\n\nTask(description=\"Integrate LangChain with Redis\", subagent_type=\"redis:langchain-integrator\", prompt=\"You are the redis:langchain-integrator agent. Integrate LangChain vector store and memory with Redis for $ARGUMENTS.\n\nContext: User selected the following features to integrate\nFeatures requested: [List from Phase 2]\nEmbedding model: [From Phase 2]\nMemory policy: [From Phase 2]\nSimilarity threshold: [From Phase 2]\n\nRequirements:\n- Install langchain-redis package and dependencies\n- Configure RedisVectorStore with HNSW indexes for vector similarity\n- Implement selected memory types (conversation, entity, summary)\n- Set up RedisChatMessageHistory for chat persistence\n- Integrate semantic cache if requested\n- Follow LangChain patterns for retrieval chains\n- Add environment variable configuration (REDIS_URL, embedding API keys)\n- Create example usage code showing integration\n- Add error handling and connection validation\n- Include cost optimization recommendations\n\nExpected output:\n- LangChain + Redis integration code\n- Configuration files with placeholder API keys\n- Example usage demonstrating features\n- Documentation on memory patterns and retention policies\")\n\nPhase 5: Verification\nGoal: Validate LangChain + Redis integration works\n\nActions:\n- Verify all dependencies installed\n- Check configuration files use placeholders (no hardcoded keys)\n- Test vector store connection and indexing\n- Test memory persistence and retrieval\n- Validate chat history storage\n- Example: !{bash python -c \"from langchain_redis import RedisVectorStore; print('Import successful')\" 2>&1}\n\nPhase 6: Summary\nGoal: Document integration and provide guidance\n\nActions:\n- Summarize what was integrated:\n  - Vector store capabilities (embedding search, similarity)\n  - Memory types implemented (conversation, entity, summary)\n  - Chat history persistence\n  - Semantic cache if enabled\n- Highlight key files created/modified\n- Provide cost optimization tips:\n  - Semantic cache hit rate monitoring\n  - Memory TTL tuning\n  - Embedding reuse strategies\n- Suggest next steps:\n  - Test with real queries\n  - Tune similarity thresholds\n  - Monitor memory usage\n  - Set up chat history cleanup policies"
              },
              {
                "name": "/integrate-llamaindex",
                "description": "Integrate LlamaIndex VectorStoreIndex and query engines with Redis",
                "path": "plugins/redis/commands/integrate-llamaindex.md",
                "frontmatter": {
                  "description": "Integrate LlamaIndex VectorStoreIndex and query engines with Redis",
                  "argument-hint": [
                    "feature-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Set up complete LlamaIndex integration with Redis vector store for semantic search, RAG applications, and AI-powered data retrieval\n\nCore Principles:\n- Detect project structure and framework\n- Use RedisVectorStore for optimal performance\n- Configure appropriate query and chat engines\n- Follow LlamaIndex best practices\n- Provide working examples\n\nPhase 1: Discovery\nGoal: Understand project context and requirements\n\nActions:\n- Parse $ARGUMENTS for integration type (basic, rag, chat, custom)\n- Detect project structure and dependencies\n- Check for existing LlamaIndex or Redis setup\n- Example: !{bash test -f package.json && echo \"node\" || test -f pyproject.toml && echo \"python\" || test -f requirements.txt && echo \"python\"}\n- Load existing configuration files if present\n- Example: @.env\n\nPhase 2: Requirements Gathering\nGoal: Clarify integration specifications\n\nActions:\n- If $ARGUMENTS is unclear or minimal, use AskUserQuestion to gather:\n  - Which LlamaIndex features? (VectorStoreIndex, query engine, chat engine, agents)\n  - Data source types? (documents, text, structured data)\n  - Vector embedding model? (OpenAI, HuggingFace, custom)\n  - Index strategy? (FLAT, HNSW)\n  - Memory requirements? (chat history, conversation memory)\n- Summarize requirements and confirm approach\n\nPhase 3: Environment Analysis\nGoal: Assess current setup and identify gaps\n\nActions:\n- Check for required dependencies\n- Python: !{bash pip list | grep -E \"llama-index|redis\" || echo \"Not found\"}\n- Node.js: !{bash npm list llamaindex redis 2>/dev/null || echo \"Not found\"}\n- Verify Redis connection details\n- Check for existing vector indices\n- Identify integration points in codebase\n\nPhase 4: Integration Planning\nGoal: Design the implementation approach\n\nActions:\n- Outline integration strategy based on findings\n- Determine file structure (index setup, query handlers, utilities)\n- Plan configuration approach (environment variables, config files)\n- Identify example use cases to implement\n- Present plan to user for confirmation\n\nPhase 5: Implementation\nGoal: Execute LlamaIndex Redis integration\n\nActions:\n\nTask(description=\"Integrate LlamaIndex with Redis\", subagent_type=\"redis:llamaindex-integrator\", prompt=\"You are the llamaindex-integrator agent. Implement complete LlamaIndex integration with Redis for $ARGUMENTS.\n\nProject Context: [Context from Phase 3]\n\nRequirements:\n- Set up RedisVectorStore with appropriate configuration\n- Implement VectorStoreIndex for semantic search\n- Configure query engines (vector, keyword, hybrid)\n- Add chat engines with conversation memory if requested\n- Create working examples and documentation\n- Follow security best practices (no hardcoded API keys)\n- Use environment variables for all credentials\n\nIntegration Type: [From Phase 2]\nEmbedding Model: [From Phase 2]\nIndex Strategy: [From Phase 2]\nAdditional Features: [From Phase 2]\n\nExpected Output:\n- Complete integration code files\n- Configuration templates (.env.example)\n- Usage examples and documentation\n- Test cases or validation scripts\")\n\nPhase 6: Verification\nGoal: Validate the integration works correctly\n\nActions:\n- Review generated integration code\n- Check all configuration files use placeholders\n- Verify .env.example exists with proper placeholders\n- Ensure .gitignore protects secrets\n- Run validation if applicable\n- Python: !{bash python -m pytest tests/ -v 2>/dev/null || echo \"No tests found\"}\n- Node.js: !{bash npm test 2>/dev/null || echo \"No tests configured\"}\n\nPhase 7: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize integration components created:\n  - Vector store configuration\n  - Index setup code\n  - Query engine implementations\n  - Chat engine setup (if applicable)\n  - Example usage files\n  - Documentation\n- Highlight key configuration options\n- Provide next steps:\n  - How to obtain API keys (OpenAI, etc.)\n  - How to populate vector index with data\n  - How to run examples\n  - How to customize for specific use cases\n- Show file locations with absolute paths"
              },
              {
                "name": "/integrate-nextjs",
                "description": "Integrate with Next.js",
                "path": "plugins/redis/commands/integrate-nextjs.md",
                "frontmatter": {
                  "description": "Integrate with Next.js",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Integrate with Next.js for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              },
              {
                "name": "/test",
                "description": "Generate test suite",
                "path": "plugins/redis/commands/test.md",
                "frontmatter": {
                  "description": "Generate test suite",
                  "argument-hint": [
                    "options"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n**Security**: @~/.claude/plugins/marketplaces/dev-lifecycle-marketplace/docs/security/SECURITY-RULES.md\n\n**Goal**: Generate test suite for Redis plugin.\n\n**Phase 1**: Detect framework and existing setup\n**Phase 2**: Ask user for configuration options\n**Phase 3**: Invoke appropriate agent to implement\n**Phase 4**: Validate implementation\n**Phase 5**: Display summary and next steps"
              }
            ],
            "skills": [
              {
                "name": "ai-cache-patterns",
                "description": "Embedding/vector caching for AI cost optimization",
                "path": "plugins/redis/skills/ai-cache-patterns/SKILL.md",
                "frontmatter": {
                  "name": "ai-cache-patterns",
                  "description": "Embedding/vector caching for AI cost optimization",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# ai-cache-patterns\n\n**Purpose**: Embedding/vector caching for AI cost optimization for Redis implementations.\n\n**Activation Triggers**:\n- When implementing ai cache patterns\n- When user mentions Redis Embedding/vector caching for AI cost optimization\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "cache-strategies",
                "description": "Caching patterns, TTL management, eviction policies for Redis implementations. Use when implementing cache-aside, write-through, write-back patterns, TTL management, or cache invalidation strategies.",
                "path": "plugins/redis/skills/cache-strategies/SKILL.md",
                "frontmatter": {
                  "name": "cache-strategies",
                  "description": "Caching patterns, TTL management, eviction policies for Redis implementations. Use when implementing cache-aside, write-through, write-back patterns, TTL management, or cache invalidation strategies.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# cache-strategies\n\n**Purpose**: Caching patterns, TTL management, eviction policies for Redis implementations.\n\n**Activation Triggers**:\n- When implementing cache strategies\n- When user mentions Redis Caching patterns, TTL management, eviction policies\n- When designing Redis architecture\n\n## Quick Reference\n\nThis skill provides templates, scripts, and examples for Redis caching patterns.\n\n## Scripts\n\n- `scripts/setup-redis.sh` - Initial Redis setup and configuration\n- `scripts/validate-config.sh` - Validate Redis configuration\n- `scripts/test-connection.sh` - Test Redis connectivity\n\n## Templates\n\n- `templates/basic-config.ts.template` - TypeScript Redis configuration\n- `templates/basic-config.py.template` - Python Redis configuration\n- `templates/redis-config.env.template` - Environment variables template\n- `templates/docker-compose.yml.template` - Docker Compose setup\n\n## Examples\n\n- `examples/basic-usage.md` - Basic caching patterns and usage\n- `examples/fastapi-example.md` - FastAPI integration example\n- `examples/nextjs-example.md` - Next.js integration example\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "connection-management",
                "description": "Redis connection pooling, client configuration, and reconnection strategies. Use when setting up Redis clients, managing connection pools, handling reconnection logic, or configuring sentinel/cluster modes.",
                "path": "plugins/redis/skills/connection-management/SKILL.md",
                "frontmatter": {
                  "name": "connection-management",
                  "description": "Redis connection pooling, client configuration, and reconnection strategies. Use when setting up Redis clients, managing connection pools, handling reconnection logic, or configuring sentinel/cluster modes.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# connection-management\n\n**Purpose**: Redis connection pooling, client configuration, and reconnection strategies.\n\n**Activation Triggers**:\n- When setting up Redis connections\n- When user mentions connection pooling, client configuration\n- When implementing high-availability Redis setups\n\n## Quick Reference\n\nThis skill provides templates, scripts, and examples for Redis connection management.\n\n## Scripts\n\n- `scripts/setup-redis.sh` - Initial Redis setup and configuration\n- `scripts/validate-config.sh` - Validate Redis configuration\n- `scripts/test-connection.sh` - Test Redis connectivity\n\n## Templates\n\n- `templates/basic-config.ts.template` - TypeScript Redis connection config\n- `templates/basic-config.py.template` - Python Redis connection config\n- `templates/redis-config.env.template` - Environment variables template\n- `templates/docker-compose.yml.template` - Docker Compose setup\n\n## Examples\n\n- `examples/basic-usage.md` - Basic connection patterns\n- `examples/fastapi-example.md` - FastAPI integration example\n- `examples/nextjs-example.md` - Next.js integration example\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "deployment-configs",
                "description": "Docker, K8s, systemd configurations",
                "path": "plugins/redis/skills/deployment-configs/SKILL.md",
                "frontmatter": {
                  "name": "deployment-configs",
                  "description": "Docker, K8s, systemd configurations",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# deployment-configs\n\n**Purpose**: Docker, K8s, systemd configurations for Redis implementations.\n\n**Activation Triggers**:\n- When implementing deployment configs\n- When user mentions Redis Docker, K8s, systemd configurations\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "framework-integrations",
                "description": "FastAPI/Next.js/Express integration patterns",
                "path": "plugins/redis/skills/framework-integrations/SKILL.md",
                "frontmatter": {
                  "name": "framework-integrations",
                  "description": "FastAPI/Next.js/Express integration patterns",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# framework-integrations\n\n**Purpose**: FastAPI/Next.js/Express integration patterns for Redis implementations.\n\n**Activation Triggers**:\n- When implementing framework integrations\n- When user mentions Redis FastAPI/Next.js/Express integration patterns\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "monitoring-patterns",
                "description": "Metrics, health checks, alerting",
                "path": "plugins/redis/skills/monitoring-patterns/SKILL.md",
                "frontmatter": {
                  "name": "monitoring-patterns",
                  "description": "Metrics, health checks, alerting",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# monitoring-patterns\n\n**Purpose**: Metrics, health checks, alerting for Redis implementations.\n\n**Activation Triggers**:\n- When implementing monitoring patterns\n- When user mentions Redis Metrics, health checks, alerting\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "pub-sub-patterns",
                "description": "Redis pub/sub messaging patterns and implementations. Use when implementing real-time messaging, event-driven architectures, notification systems, or message broadcasting with Redis pub/sub.",
                "path": "plugins/redis/skills/pub-sub-patterns/SKILL.md",
                "frontmatter": {
                  "name": "pub-sub-patterns",
                  "description": "Redis pub/sub messaging patterns and implementations. Use when implementing real-time messaging, event-driven architectures, notification systems, or message broadcasting with Redis pub/sub.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# pub-sub-patterns\n\n**Purpose**: Redis pub/sub messaging patterns and implementations.\n\n**Activation Triggers**:\n- When implementing pub/sub messaging\n- When user mentions real-time notifications, event broadcasting\n- When designing event-driven architectures\n\n## Quick Reference\n\nThis skill provides templates, scripts, and examples for Redis pub/sub patterns.\n\n## Scripts\n\n- `scripts/setup-redis.sh` - Initial Redis setup and configuration\n- `scripts/validate-config.sh` - Validate Redis configuration\n- `scripts/test-connection.sh` - Test Redis connectivity\n\n## Templates\n\n- `templates/basic-config.ts.template` - TypeScript Redis pub/sub config\n- `templates/basic-config.py.template` - Python Redis pub/sub config\n- `templates/redis-config.env.template` - Environment variables template\n- `templates/docker-compose.yml.template` - Docker Compose setup\n\n## Examples\n\n- `examples/basic-usage.md` - Basic pub/sub patterns\n- `examples/fastapi-example.md` - FastAPI integration example\n- `examples/nextjs-example.md` - Next.js integration example\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "rate-limiting-patterns",
                "description": "Redis-based rate limiting implementations with token bucket, leaky bucket, and sliding window algorithms. Use when implementing API rate limiting, throttling, or request quota management.",
                "path": "plugins/redis/skills/rate-limiting-patterns/SKILL.md",
                "frontmatter": {
                  "name": "rate-limiting-patterns",
                  "description": "Redis-based rate limiting implementations with token bucket, leaky bucket, and sliding window algorithms. Use when implementing API rate limiting, throttling, or request quota management.",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# rate-limiting-patterns\n\n**Purpose**: Redis-based rate limiting implementations with various algorithms.\n\n**Activation Triggers**:\n- When implementing rate limiting\n- When user mentions API throttling, request quotas\n- When designing rate-limited endpoints\n\n## Quick Reference\n\nThis skill provides templates, scripts, and examples for Redis rate limiting.\n\n## Scripts\n\n- `scripts/setup-redis.sh` - Initial Redis setup and configuration\n- `scripts/validate-config.sh` - Validate Redis configuration\n- `scripts/test-connection.sh` - Test Redis connectivity\n\n## Templates\n\n- `templates/basic-config.ts.template` - TypeScript rate limiting config\n- `templates/basic-config.py.template` - Python rate limiting config\n- `templates/redis-config.env.template` - Environment variables template\n- `templates/docker-compose.yml.template` - Docker Compose setup\n\n## Examples\n\n- `examples/basic-usage.md` - Basic rate limiting patterns\n- `examples/fastapi-example.md` - FastAPI integration example\n- `examples/nextjs-example.md` - Next.js integration example\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "sentinel-configurations",
                "description": "High availability templates and configs",
                "path": "plugins/redis/skills/sentinel-configurations/SKILL.md",
                "frontmatter": {
                  "name": "sentinel-configurations",
                  "description": "High availability templates and configs",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# sentinel-configurations\n\n**Purpose**: High availability templates and configs for Redis implementations.\n\n**Activation Triggers**:\n- When implementing sentinel configurations\n- When user mentions Redis High availability templates and configs\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              },
              {
                "name": "session-management",
                "description": "Session store implementations across frameworks",
                "path": "plugins/redis/skills/session-management/SKILL.md",
                "frontmatter": {
                  "name": "session-management",
                  "description": "Session store implementations across frameworks",
                  "allowed-tools": "Read, Bash, Grep, Glob"
                },
                "content": "# session-management\n\n**Purpose**: Session store implementations across frameworks for Redis implementations.\n\n**Activation Triggers**:\n- When implementing session management\n- When user mentions Redis Session store implementations across frameworks\n- When designing Redis architecture\n\n## Quick Reference\n\nSee templates/, scripts/, and examples/ for implementation patterns.\n\n## Templates\n\n- `templates/` - Configuration file templates\n- All templates use placeholders (no hardcoded credentials)\n\n## Scripts\n\n- `scripts/` - Automation and testing scripts\n\n## Examples\n\n- `examples/` - Implementation examples for common use cases\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              }
            ]
          },
          {
            "name": "resend",
            "description": "Complete Resend email API integration for transactional emails, React Email templates, contacts, broadcasts, webhooks, and domain management",
            "source": "./plugins/resend",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "AI Dev Marketplace",
              "email": "noreply@ai-dev-marketplace.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install resend@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-broadcasts",
                "description": "Add broadcast/campaign functionality for sending to audiences",
                "path": "plugins/resend/commands/add-broadcasts.md",
                "frontmatter": {
                  "description": "Add broadcast/campaign functionality for sending to audiences",
                  "argument-hint": [
                    "broadcast-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive broadcast and campaign functionality to Resend plugin with audience targeting, scheduling, and analytics.\n\nCore Principles:\n- Build broadcast management service following existing patterns\n- Support flexible audience targeting via segments\n- Integrate scheduling capabilities\n- Include tracking and analytics hooks for measurement\n\nPhase 1: Discovery\nGoal: Understand existing Resend plugin structure and patterns\n\nActions:\n- Examine current service architecture\n- Example: !{bash find /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend -name \"*.ts\" -o -name \"*.md\" | head -20}\n- Load existing services to understand patterns: @/home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend/src/services\n\nPhase 2: Analysis\nGoal: Identify integration points and required components\n\nActions:\n- Review existing Resend service implementations\n- Identify audience/segment management patterns\n- Check for scheduling mechanisms\n- Look for analytics hooks in email services\n- Create todo list for broadcast implementation:\n\nTodoWrite:\n- status: in_progress\n- content: Discover broadcast patterns and integration points\n- activeForm: Discovering broadcast patterns\n- status: pending\n- content: Implement broadcast management service\n- activeForm: Implementing broadcast management service\n- status: pending\n- content: Add audience targeting and segmentation\n- activeForm: Adding audience targeting and segmentation\n- status: pending\n- content: Integrate scheduling capabilities\n- activeForm: Integrating scheduling capabilities\n- status: pending\n- content: Add tracking and analytics hooks\n- activeForm: Adding tracking and analytics hooks\n- status: pending\n- content: Validate broadcast implementation\n- activeForm: Validating broadcast implementation\n\nPhase 3: Implementation\nGoal: Build broadcast functionality with agent guidance\n\nActions:\n\nTask(description=\"Build broadcast functionality\", subagent_type=\"resend-broadcasts-agent\", prompt=\"You are the resend-broadcasts-agent. Add comprehensive broadcast and campaign functionality to the Resend plugin for $ARGUMENTS.\n\nRequirements:\n- Create broadcast management service with CRUD operations\n- Support audience targeting via customer segments\n- Implement campaign scheduling (immediate, delayed, recurring)\n- Add delivery and engagement tracking hooks\n- Include template management for campaigns\n- Support A/B testing capabilities\n- Integrate with existing Resend patterns\n\nDeliverable: Complete broadcast service implementation with types, repository patterns, and integration ready for use.\")\n\nPhase 4: Review and Summary\nGoal: Validate and document broadcast implementation\n\nActions:\n- Review generated broadcast service code\n- Verify integration with existing Resend patterns\n- Check type safety and error handling\n- Confirm scheduling and analytics hooks\n- Summarize: Broadcast functionality added with audience targeting, scheduling, and analytics tracking"
              },
              {
                "name": "/add-contacts",
                "description": null,
                "path": "plugins/resend/commands/add-contacts.md",
                "frontmatter": null,
                "content": "---\ndescription: Add contact management with segments, topics, and custom properties\nargument-hint: [--with-segments] [--with-topics] [--bulk-import]\nallowed-tools: Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion\n---\n\n**Arguments**: $ARGUMENTS\n\nGoal: Implement comprehensive contact management system for Resend email campaigns with segments, topics, and custom properties\n\nCore Principles:\n- Understand contact data requirements before building\n- Leverage Resend API documentation for patterns\n- Include bulk import/export from the start\n- Enable segment-based targeting and personalization\n\nPhase 1: Discovery\nGoal: Understand contact management requirements and current setup\n\nActions:\n- Check package.json for framework and dependencies\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - What contact fields do you need to track?\n  - How will contacts be imported (API, CSV, real-time)?\n  - Do you need audience segmentation?\n  - Will you use topic preferences for email subscriptions?\n  - What custom properties will you manage?\n- Load existing Resend configuration if available\n\nPhase 2: Analysis\nGoal: Determine scope and feature requirements\n\nActions:\n- Parse $ARGUMENTS for flags (--with-segments, --with-topics, --bulk-import)\n- Check for existing contact infrastructure\n- Identify contact data sources and volumes\n- Determine segment complexity and update frequency\n- Understand existing contact properties or schema\n- Example: !{bash find src -name \"*contact*\" -o -name \"*segment*\" 2>/dev/null | head -10}\n\nPhase 3: Implementation\nGoal: Build contact management system with resend-contacts-agent\n\nActions:\n\nTask(description=\"Implement contact management system\", subagent_type=\"resend-contacts-agent\", prompt=\"You are the resend-contacts-agent. Implement a comprehensive contact management system for Resend for $ARGUMENTS.\n\nRequirements from user:\n- Include contact CRUD operations (create, read, update, delete)\n- Add bulk import/export functionality for CSV and JSON\n- Implement segment management if --with-segments flag is present\n- Implement topic preferences if --with-topics flag is present\n- Support custom contact properties and fields\n- Ensure API key handling via environment variables\n\nDeliverables:\n1. Contact service with full CRUD operations\n2. Bulk import/export handlers\n3. Segment management (if requested)\n4. Topic management (if requested)\n5. Custom property definitions and validation\n6. Error handling and logging\n7. Rate limiting awareness for batch operations\n8. .env.example with placeholder credentials\n9. TypeScript types (if applicable)\")\n\nPhase 4: Verification\nGoal: Ensure contact management system works correctly\n\nActions:\n- Check for contact service file creation\n- Verify contact CRUD operations implemented\n- Confirm bulk import/export functionality\n- Validate segment operations if included\n- Check topic management if included\n- Verify environment variable handling\n- Example: !{bash find . -name \"*.env.example\" | grep -q . && echo \"Config OK\" || echo \"Missing .env.example\"}\n\nPhase 5: Summary\nGoal: Document what was implemented\n\nActions:\n- Summarize contact management features added\n- Highlight segments and topics (if included)\n- Show next steps for integration\n- Provide usage examples for contact operations\n"
              },
              {
                "name": "/add-domains",
                "description": "Configure sending domains with DNS verification (SPF, DKIM, DMARC)",
                "path": "plugins/resend/commands/add-domains.md",
                "frontmatter": {
                  "description": "Configure sending domains with DNS verification (SPF, DKIM, DMARC)",
                  "argument-hint": "<domain-name>",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add and configure a sending domain in Resend with complete DNS verification setup\n\nCore Principles:\n- Understand the domain before configuration\n- Generate clear DNS records with verification steps\n- Provide actionable guidance for DNS setup\n- Validate domain status after configuration\n\nPhase 1: Discovery\nGoal: Understand the domain configuration request\n\nActions:\n- Parse $ARGUMENTS to extract domain name\n- If domain unclear, prompt user for clarification\n- Load Resend API documentation for domain endpoints\n- Verify domain format is valid (basic DNS validation)\n\nPhase 2: Analysis\nGoal: Understand current setup and Resend capabilities\n\nActions:\n- Read @plugins/resend/agents/resend-domains-webhooks-agent.md to understand agent capabilities\n- Identify required DNS records (SPF, DKIM, DMARC)\n- Plan verification workflow:\n  - Add domain to Resend\n  - Generate DNS records\n  - Provide setup instructions\n  - Check verification status\n\nPhase 3: Configuration\nGoal: Configure domain in Resend with complete DNS setup\n\nActions:\n\nTask(description=\"Configure sending domain\", subagent_type=\"resend-domains-webhooks-agent\", prompt=\"You are the resend-domains-webhooks-agent. Configure the sending domain $ARGUMENTS in Resend with complete DNS verification setup.\n\nActions to perform:\n1. Add domain to Resend using the Resend API\n2. Generate DNS records needed for verification:\n   - SPF record (Sender Policy Framework)\n   - DKIM record (DomainKeys Identified Mail)\n   - DMARC record (Domain-based Message Authentication)\n3. Provide clear step-by-step instructions for:\n   - Where to find DNS settings in domain registrar\n   - Exact DNS records to add\n   - TTL recommendations\n   - Verification timeline expectations\n4. Check domain verification status\n5. Return domain configuration summary with:\n   - Domain details (name, created date)\n   - DNS records to add (with exact values)\n   - Verification status\n   - Next steps if verification needed\n\nRequired output format:\n- Domain Configuration Summary\n- DNS Records to Add (3 records minimum)\n- Setup Instructions\n- Verification Status\n- Troubleshooting tips\")\n\nPhase 4: Summary\nGoal: Document the domain configuration process\n\nActions:\n- Summarize domain name and configuration status\n- List all DNS records that need to be added\n- Provide next steps for verification completion\n- Suggest timeline for email delivery readiness"
              },
              {
                "name": "/add-react-email",
                "description": "Integrate React Email for building beautiful email templates with components",
                "path": "plugins/resend/commands/add-react-email.md",
                "frontmatter": {
                  "description": "Integrate React Email for building beautiful email templates with components",
                  "argument-hint": [
                    "template-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Set up React Email integration with Resend for building and managing professional email templates with reusable React components.\n\nCore Principles:\n- Automate dependency installation and directory setup\n- Provide example templates demonstrating best practices\n- Enable preview server for template development\n- Integrate seamlessly with Resend API for sending\n\nPhase 1: Discovery\nGoal: Understand project context and requirements\n\nActions:\n- Check if $ARGUMENTS contains template name or is empty\n- Detect if project is Node.js: !{bash test -f package.json && echo \"Node.js project\" || echo \"Not found\"}\n- Load package.json to understand existing dependencies: @package.json\n\nPhase 2: Planning\nGoal: Design the React Email setup approach\n\nActions:\n- Determine if project has existing email setup: !{bash ls -la src/emails/ emails/ 2>/dev/null || echo \"No email directory found\"}\n- Verify Node.js version supports React Email: !{bash node -v}\n- Plan directory structure: emails/ for components, emails/templates/ for template exports\n- Identify integration points with Resend API\n\nPhase 3: Setup\nGoal: Execute React Email integration setup\n\nActions:\n\nTask(description=\"Setup React Email integration\", subagent_type=\"resend-templates-agent\", prompt=\"You are the resend-templates-agent. Set up React Email integration for $ARGUMENTS.\n\nContext: This is a fresh React Email setup for building professional email templates.\n\nRequirements:\n- Install @react-email/components and related dependencies\n- Create emails/ directory structure with subdirectories (components, templates, previews)\n- Generate 3 example templates: welcome email, password reset, notification\n- Create preview server configuration for email development\n- Add npm scripts for preview server and template building\n- Document React Email component usage and Resend integration\n\nEach template should demonstrate:\n- React Email component structure\n- Variable placeholders for dynamic content\n- Responsive design patterns\n- Email client compatibility\n\nExpected output: Complete React Email setup with working examples and documentation\")\n\nPhase 4: Verification\nGoal: Confirm setup completed successfully\n\nActions:\n- Check emails directory was created: !{bash test -d emails && echo \"Success: emails directory created\" || echo \"Error: directory not found\"}\n- List generated template files: !{bash ls -la emails/templates/ 2>/dev/null || echo \"Templates not found\"}\n- Verify dependencies installed: !{bash npm list @react-email/components 2>/dev/null | head -3}\n\nPhase 5: Summary\nGoal: Document what was accomplished\n\nActions:\n- List all created files and directories\n- Display preview server startup instructions\n- Show example template locations and usage\n- Document next steps for creating additional templates\n- Suggest integration with Resend send API"
              },
              {
                "name": "/add-templates",
                "description": "Add email template management with React Email integration",
                "path": "plugins/resend/commands/add-templates.md",
                "frontmatter": {
                  "description": "Add email template management with React Email integration",
                  "argument-hint": [
                    "--react-email"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive email template management to Resend plugin with React Email component-based templates, versioning support, and publish/duplicate functionality.\n\nCore Principles:\n- Build template management service following existing patterns\n- Integrate React Email for component-based template development\n- Support template versioning and publish workflows\n- Include template duplication and metadata management\n\nPhase 1: Discovery\nGoal: Understand existing Resend plugin structure and template patterns\n\nActions:\n- Examine current service architecture\n- Example: !{bash find /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend -name \"*.ts\" -o -name \"*.md\" | head -20}\n- Load existing services to understand patterns: @/home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend/src/services\n\nPhase 2: Analysis\nGoal: Identify template integration points and React Email requirements\n\nActions:\n- Review existing Resend service implementations\n- Check for template patterns in email services\n- Identify where React Email components fit\n- Look for versioning mechanisms\n- Create todo list for template implementation:\n\nTodoWrite:\n- status: in_progress\n- content: Discover template patterns and integration points\n- activeForm: Discovering template patterns\n- status: pending\n- content: Design React Email component structure\n- activeForm: Designing React Email component structure\n- status: pending\n- content: Implement template management service\n- activeForm: Implementing template management service\n- status: pending\n- content: Add versioning support\n- activeForm: Adding versioning support\n- status: pending\n- content: Implement publish and duplicate functionality\n- activeForm: Implementing publish and duplicate functionality\n- status: pending\n- content: Validate template implementation\n- activeForm: Validating template implementation\n\nPhase 3: Implementation\nGoal: Build template functionality with agent guidance\n\nActions:\n\nTask(description=\"Build template management\", subagent_type=\"resend-templates-agent\", prompt=\"You are the resend-templates-agent. Add comprehensive email template management to the Resend plugin for $ARGUMENTS.\n\nRequirements:\n- Create template management service with CRUD operations\n- Integrate React Email for component-based templates\n- Implement template versioning with version history\n- Support template publishing workflows\n- Add template duplication with metadata management\n- Include template preview capabilities\n- Support dynamic variable substitution in templates\n- Integrate with existing Resend patterns\n\nDeliverable: Complete template service implementation with types, repository patterns, React Email integration, and integration ready for use.\")\n\nPhase 4: Review and Summary\nGoal: Validate and document template implementation\n\nActions:\n- Review generated template service code\n- Verify React Email component integration\n- Check versioning and publish workflows\n- Confirm duplication and metadata handling\n- Summarize: Template management added with React Email integration, versioning, and publish/duplicate functionality"
              },
              {
                "name": "/add-webhooks",
                "description": "Set up webhook handlers for email events (sent, delivered, bounced, opened, clicked)",
                "path": "plugins/resend/commands/add-webhooks.md",
                "frontmatter": {
                  "description": "Set up webhook handlers for email events (sent, delivered, bounced, opened, clicked)",
                  "argument-hint": "<webhook-url>",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Set up comprehensive webhook handlers for Resend email events with signature verification, event logging, and processing.\n\nCore Principles:\n- Secure by default: Always verify webhook signatures\n- Event coverage: Support all Resend email event types\n- Clear logging: Log all events for audit and debugging\n- Error resilient: Handle webhook delivery failures gracefully\n\nPhase 1: Requirements Gathering\nGoal: Understand webhook needs and environment\n\nActions:\n- Parse $ARGUMENTS to extract webhook URL\n- If not provided clearly, ask for clarification:\n  - Webhook URL for event delivery\n  - Which events to monitor (sent, delivered, bounced, opened, clicked, complained)\n  - Log destination (file, database, external service)\n  - Any custom event filtering needs\n\nPhase 2: Discovery\nGoal: Understand current project structure and setup\n\nActions:\n- Check if project has existing webhook handler: !{bash find . -type f -name \"*webhook*\" 2>/dev/null | head -5}\n- Look for environment configuration: @.env.example\n- Identify project type and framework: !{bash test -f package.json && cat package.json | grep -E '\"(type|scripts)\"|\"(next|fastify|express)\"' || echo \"No Node.js project\"}\n- Load any existing API endpoint structure\n\nPhase 3: Implementation\nGoal: Set up webhook infrastructure with agent\n\nActions:\n\nTask(description=\"Set up webhook handlers\", subagent_type=\"resend-domains-webhooks-agent\", prompt=\"You are the resend-domains-webhooks-agent. Set up comprehensive webhook handlers for Resend email events.\n\nWebhook URL: $ARGUMENTS\n\nYour tasks:\n1. Create webhook endpoint handler supporting all event types (sent, delivered, bounced, opened, clicked, complained)\n2. Implement cryptographic signature verification using Resend's webhook signing secret\n3. Add event logging and processing infrastructure\n4. Create error handling for failed webhook deliveries\n5. Generate configuration for environment variables\n6. Provide testing instructions\n\nDeliverables:\n- Complete webhook handler code\n- Signature verification middleware\n- Event type definitions and processors\n- Error handling and retry logic\n- .env configuration template\n- Documentation for testing webhooks\")\n\nPhase 4: Verification\nGoal: Ensure webhook setup is production-ready\n\nActions:\n- Verify webhook handler exists: !{bash test -f webhook.ts || test -f webhook.js && echo \"Handler created\" || echo \"Handler not found\"}\n- Check for signature verification code: !{bash grep -r \"signature\\|SIGNING\" . --include=\"*.ts\" --include=\"*.js\" 2>/dev/null | head -3}\n- Validate environment configuration: @.env.example\n- Confirm all event types are handled: !{bash grep -E \"sent|delivered|bounced|opened|clicked|complained\" webhook.* 2>/dev/null | wc -l}\n\nPhase 5: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize webhook setup:\n  - Handler implementation created\n  - All 6 event types supported (sent, delivered, bounced, opened, clicked, complained)\n  - Signature verification implemented\n  - Event logging configured\n- List generated files and their purposes\n- Provide next steps:\n  - Deploy webhook endpoint\n  - Register webhook URL in Resend dashboard\n  - Test event delivery with sample events\n  - Monitor logs for production issues\n- Note any configuration that needs manual updates"
              },
              {
                "name": "/build-email-system",
                "description": null,
                "path": "plugins/resend/commands/build-email-system.md",
                "frontmatter": null,
                "content": "---\ndescription: Build complete email system with Resend - SDK setup, API routes, React Email templates, webhooks, and contact management\nargument-hint: [--frontend-only] [--backend-only] [--with-contacts] [--with-broadcasts]\nallowed-tools: Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite, AskUserQuestion, WebFetch\n---\n\n---\nüö® **EXECUTION NOTICE FOR CLAUDE**\n\nWhen you invoke this command via SlashCommand, the system returns THESE INSTRUCTIONS below.\n\n**YOU are the executor. This is NOT an autonomous subprocess.**\n\n- ‚úÖ The phases below are YOUR execution checklist\n- ‚úÖ YOU must run each phase immediately using tools\n- ‚úÖ Complete ALL phases before considering this command done\n- ‚ùå DON'T wait for \"the command to complete\" - YOU complete it by executing the phases\n\n---\n\n## Security Requirements\n\n**CRITICAL:** All generated files must follow security rules:\n- Never hardcode API keys - use `RESEND_API_KEY=your_resend_api_key_here`\n- Protect `.env` files with `.gitignore`\n- Create `.env.example` with placeholders only\n\n**Arguments**: $ARGUMENTS\n\n## Goal\n\nBuild a complete, production-ready email system using Resend API that integrates with your existing project. This orchestrator coordinates multiple specialized agents in parallel to build:\n\n1. **Backend**: API routes for email operations (Next.js API routes or FastAPI endpoints)\n2. **Frontend**: React Email templates, email composer UI components\n3. **Infrastructure**: Webhooks, domain configuration, contact management\n4. **Integration**: Connects with existing auth, database, and UI systems\n\n---\n\n## Phase 1: Discovery & Project Analysis\n\n**Goal**: Understand project structure and determine what needs to be built\n\n**Actions**:\n\n1. Detect project type and existing infrastructure:\n\n!{bash ls package.json pyproject.toml next.config.* tsconfig.json requirements.txt 2>/dev/null}\n\n!{bash test -f package.json && cat package.json | grep -E '\"(next|react|express|fastify)\"' | head -5}\n\n2. Check for existing Resend setup:\n\n!{bash test -f .env.example && grep -i resend .env.example || echo \"No Resend config found\"}\n\n!{bash find . -name \"*.ts\" -o -name \"*.tsx\" -o -name \"*.py\" 2>/dev/null | xargs grep -l \"resend\" 2>/dev/null | head -5}\n\n3. Detect existing integrations:\n- Supabase: !{bash grep -r \"supabase\" package.json .env.example 2>/dev/null | head -3}\n- Clerk/Auth: !{bash grep -r \"clerk\\|nextauth\\|auth\" package.json 2>/dev/null | head -3}\n- Database: !{bash test -d prisma && echo \"Prisma detected\" || echo \"No Prisma\"}\n\n4. Parse $ARGUMENTS for flags:\n- `--frontend-only`: Skip backend API routes\n- `--backend-only`: Skip React Email templates and UI\n- `--with-contacts`: Include contact management system\n- `--with-broadcasts`: Include broadcast/campaign functionality\n\n---\n\n## Phase 2: Requirements Gathering\n\n**Goal**: Clarify what email features the user needs\n\n**Actions**:\n\nUse AskUserQuestion to gather requirements:\n\n```\nAskUserQuestion([\n  {\n    question: \"What email features do you need?\",\n    header: \"Features\",\n    options: [\n      { label: \"Transactional only\", description: \"Password reset, welcome, confirmations\" },\n      { label: \"Marketing + Transactional\", description: \"Newsletters, broadcasts + transactional\" },\n      { label: \"Full suite\", description: \"All features including contacts, segments, analytics\" }\n    ],\n    multiSelect: false\n  },\n  {\n    question: \"What's your project stack?\",\n    header: \"Stack\",\n    options: [\n      { label: \"Next.js Full-stack\", description: \"Next.js with API routes\" },\n      { label: \"Next.js + FastAPI\", description: \"Next.js frontend, FastAPI backend\" },\n      { label: \"React + Express\", description: \"React SPA with Express backend\" },\n      { label: \"Python only\", description: \"FastAPI or Flask backend\" }\n    ],\n    multiSelect: false\n  },\n  {\n    question: \"Do you want React Email for templates?\",\n    header: \"Templates\",\n    options: [\n      { label: \"Yes - React Email\", description: \"Component-based email templates\" },\n      { label: \"No - HTML templates\", description: \"Simple HTML email templates\" }\n    ],\n    multiSelect: false\n  }\n])\n```\n\nStore responses for agent coordination.\n\n---\n\n## Phase 3: Create Implementation Plan\n\n**Goal**: Design the build plan and create tracking todos\n\n**Actions**:\n\nBased on discovery and requirements, create TodoWrite entries:\n\n```\nTodoWrite([\n  { content: \"Initialize Resend SDK and environment\", status: \"pending\", activeForm: \"Initializing Resend SDK\" },\n  { content: \"Build backend API routes for email operations\", status: \"pending\", activeForm: \"Building backend API routes\" },\n  { content: \"Create React Email templates\", status: \"pending\", activeForm: \"Creating React Email templates\" },\n  { content: \"Build frontend email UI components\", status: \"pending\", activeForm: \"Building frontend UI components\" },\n  { content: \"Setup webhook handlers for email events\", status: \"pending\", activeForm: \"Setting up webhook handlers\" },\n  { content: \"Configure domain and DNS (optional)\", status: \"pending\", activeForm: \"Configuring domain\" },\n  { content: \"Add contact management (if requested)\", status: \"pending\", activeForm: \"Adding contact management\" },\n  { content: \"Integration testing and verification\", status: \"pending\", activeForm: \"Running integration tests\" }\n])\n```\n\n---\n\n## Phase 4: Parallel Agent Execution - Core Setup\n\n**Goal**: Run foundational agents in parallel\n\n**CRITICAL: Launch ALL these Task() calls in a SINGLE message for parallel execution!**\n\n**Task 1: Resend SDK Setup**\n```\nTask(\n  description=\"Initialize Resend SDK\",\n  subagent_type=\"resend:resend-setup-agent\",\n  prompt=\"Initialize Resend SDK in this project.\n\nContext:\n- Project type: [detected from Phase 1]\n- Framework: [Next.js/FastAPI/Express]\n\nRequirements:\n1. Install resend SDK (npm or pip)\n2. Create .env.example with RESEND_API_KEY=your_resend_api_key_here\n3. Add .env to .gitignore\n4. Create lib/resend.ts or utils/resend.py client initialization\n5. Add TypeScript types for email payloads\n\nDeliverable: Working Resend client setup with proper environment configuration.\"\n)\n```\n\n**Task 2: Backend API Routes** (if Next.js or needs API)\n```\nTask(\n  description=\"Build email API routes\",\n  subagent_type=\"nextjs-frontend:api-route-generator-agent\",\n  prompt=\"Create Next.js API routes for Resend email operations.\n\nCreate these API routes in app/api/emails/:\n1. POST /api/emails/send - Send single transactional email\n2. POST /api/emails/batch - Send batch emails (up to 100)\n3. GET /api/emails/[id] - Get email status\n4. POST /api/emails/[id]/cancel - Cancel scheduled email\n\nEach route should:\n- Validate request body with zod\n- Use Resend SDK from lib/resend.ts\n- Handle errors properly (rate limits, validation)\n- Return proper status codes\n- Include TypeScript types\n\nSecurity: Read RESEND_API_KEY from environment, never expose in responses.\"\n)\n```\n\n**Task 3: FastAPI Endpoints** (if Python backend detected)\n```\nTask(\n  description=\"Build FastAPI email endpoints\",\n  subagent_type=\"fastapi-backend:endpoint-generator-agent\",\n  prompt=\"Create FastAPI endpoints for Resend email operations.\n\nCreate these endpoints in app/routers/emails.py:\n1. POST /api/emails/send - Send single email\n2. POST /api/emails/batch - Send batch emails\n3. GET /api/emails/{email_id} - Get email status\n4. POST /api/emails/{email_id}/cancel - Cancel scheduled email\n\nRequirements:\n- Pydantic models for request/response validation\n- Async Resend client usage\n- Proper error handling with HTTPException\n- Rate limit handling with retry logic\n- OpenAPI documentation\n\nSecurity: Read RESEND_API_KEY from environment using python-dotenv.\"\n)\n```\n\n**Wait for all Task() calls to complete before proceeding to Phase 5.**\n\n---\n\n## Phase 5: Parallel Agent Execution - Templates & UI\n\n**Goal**: Build email templates and UI components in parallel\n\n**CRITICAL: Launch ALL these Task() calls in a SINGLE message!**\n\n**Task 4: React Email Templates**\n```\nTask(\n  description=\"Create React Email templates\",\n  subagent_type=\"resend:resend-templates-agent\",\n  prompt=\"Create React Email templates for the project.\n\nInstall and setup:\n1. Install @react-email/components and react-email\n2. Create emails/ directory structure\n\nCreate these templates:\n1. emails/welcome.tsx - Welcome email with branding\n2. emails/password-reset.tsx - Password reset with secure link\n3. emails/notification.tsx - Generic notification template\n4. emails/receipt.tsx - Order/payment receipt (if e-commerce)\n\nEach template should:\n- Use @react-email/components (Html, Head, Body, Container, Text, Button, etc.)\n- Be responsive (mobile-friendly)\n- Include proper styling with Tailwind classes\n- Export props interface for type safety\n- Include preview text\n\nAdd npm script: 'email:dev' for preview server.\"\n)\n```\n\n**Task 5: Frontend UI Components**\n```\nTask(\n  description=\"Build email UI components\",\n  subagent_type=\"nextjs-frontend:component-builder-agent\",\n  prompt=\"Create React components for email functionality.\n\nCreate these components in components/email/:\n1. EmailComposer.tsx - Rich text email composer with:\n   - To, CC, BCC fields with email validation\n   - Subject line\n   - Rich text editor (use existing editor or textarea)\n   - Template selection dropdown\n   - Attachment upload\n   - Schedule send option\n   - Send button with loading state\n\n2. EmailStatus.tsx - Email delivery status display:\n   - Show sent, delivered, opened, clicked, bounced\n   - Timeline view of events\n   - Resend option for failed emails\n\n3. EmailList.tsx - List of sent emails:\n   - Sortable table with recipient, subject, status, date\n   - Pagination\n   - Search/filter\n   - Click to view details\n\nUse shadcn/ui components (Button, Input, Card, Table, Badge, etc.).\nFollow existing design system patterns in the project.\"\n)\n```\n\n**Task 6: Webhook Handlers**\n```\nTask(\n  description=\"Setup webhook handlers\",\n  subagent_type=\"resend:resend-domains-webhooks-agent\",\n  prompt=\"Create webhook handlers for Resend email events.\n\nCreate webhook endpoint at app/api/webhooks/resend/route.ts:\n\n1. Verify webhook signature using Resend signing secret\n2. Handle all event types:\n   - email.sent\n   - email.delivered\n   - email.delivery_delayed\n   - email.complained\n   - email.bounced\n   - email.opened\n   - email.clicked\n\n3. For each event:\n   - Log to database (if Prisma/Supabase detected)\n   - Update email status in your system\n   - Trigger notifications if needed (bounces, complaints)\n\n4. Create .env entry: RESEND_WEBHOOK_SECRET=your_webhook_secret_here\n\nSecurity: Always verify signatures before processing events.\"\n)\n```\n\n**Wait for all Task() calls to complete before proceeding to Phase 6.**\n\n---\n\n## Phase 6: Optional Features (Based on Flags)\n\n**Goal**: Add optional features based on user request\n\n**If --with-contacts flag or \"Full suite\" selected:**\n\n```\nTask(\n  description=\"Add contact management\",\n  subagent_type=\"resend:resend-contacts-agent\",\n  prompt=\"Add contact management system for Resend.\n\nCreate:\n1. lib/contacts.ts - Contact management utilities:\n   - createContact(email, name, properties)\n   - updateContact(id, data)\n   - deleteContact(id)\n   - addToSegment(contactId, segmentId)\n   - updateTopicPreferences(contactId, topics)\n\n2. API routes in app/api/contacts/:\n   - POST /api/contacts - Create contact\n   - GET /api/contacts - List contacts with pagination\n   - PATCH /api/contacts/[id] - Update contact\n   - DELETE /api/contacts/[id] - Delete contact\n   - POST /api/contacts/[id]/segments - Add to segment\n\n3. UI components:\n   - ContactList.tsx - Searchable contact table\n   - ContactForm.tsx - Add/edit contact form\n   - SegmentManager.tsx - Manage segments\n\nIntegrate with existing user/customer tables if detected.\"\n)\n```\n\n**If --with-broadcasts flag or \"Marketing + Transactional\" selected:**\n\n```\nTask(\n  description=\"Add broadcast functionality\",\n  subagent_type=\"resend:resend-broadcasts-agent\",\n  prompt=\"Add broadcast/campaign functionality for Resend.\n\nCreate:\n1. lib/broadcasts.ts - Broadcast management:\n   - createBroadcast(name, subject, template, audienceId)\n   - sendBroadcast(broadcastId)\n   - scheduleBroadcast(broadcastId, sendAt)\n   - getBroadcastStats(broadcastId)\n\n2. API routes in app/api/broadcasts/:\n   - POST /api/broadcasts - Create broadcast\n   - GET /api/broadcasts - List broadcasts\n   - POST /api/broadcasts/[id]/send - Send broadcast\n   - GET /api/broadcasts/[id]/stats - Get analytics\n\n3. UI components:\n   - BroadcastComposer.tsx - Campaign builder\n   - AudienceSelector.tsx - Select segments/audiences\n   - CampaignAnalytics.tsx - Open rates, click rates, etc.\n\nInclude A/B testing support for subject lines.\"\n)\n```\n\n---\n\n## Phase 7: Integration & Wiring\n\n**Goal**: Connect all pieces together\n\n**Actions**:\n\n1. Create central email service that ties everything together:\n\n!{Write app/services/email-service.ts with:\n- Unified interface for all email operations\n- Template rendering with React Email\n- Contact lookup integration\n- Event logging\n- Error handling with retries\n}\n\n2. Add email hooks/utilities:\n\n!{Write lib/hooks/useEmailStatus.ts - React hook for real-time email status}\n!{Write lib/hooks/useContacts.ts - React hook for contact management}\n\n3. Update existing pages to include email functionality:\n- Add email buttons to user profiles\n- Add notification preferences to settings\n- Add email history to admin dashboard\n\n---\n\n## Phase 8: Verification & Testing\n\n**Goal**: Ensure everything works\n\n**Actions**:\n\n1. Verify all files were created:\n!{bash find . -path ./node_modules -prune -o -name \"*.ts\" -o -name \"*.tsx\" | xargs grep -l \"resend\" 2>/dev/null | head -20}\n\n2. Check TypeScript compilation:\n!{bash npx tsc --noEmit 2>&1 | head -20}\n\n3. Verify environment setup:\n!{bash test -f .env.example && grep -E \"RESEND\" .env.example}\n\n4. List all created files:\n!{bash git status --short | grep -E \"^\\?\\?\" | head -30}\n\n---\n\n## Phase 9: Summary & Next Steps\n\n**Goal**: Provide completion summary and guidance\n\n**Display**:\n\n```\n## ‚úÖ Email System Built Successfully\n\n### Files Created:\n\n**Backend (API Routes)**:\n- app/api/emails/send/route.ts\n- app/api/emails/batch/route.ts\n- app/api/emails/[id]/route.ts\n- app/api/webhooks/resend/route.ts\n\n**Frontend (Templates & UI)**:\n- emails/welcome.tsx\n- emails/password-reset.tsx\n- emails/notification.tsx\n- components/email/EmailComposer.tsx\n- components/email/EmailStatus.tsx\n- components/email/EmailList.tsx\n\n**Infrastructure**:\n- lib/resend.ts (SDK client)\n- lib/email-service.ts (unified service)\n- .env.example (with RESEND_API_KEY placeholder)\n\n### Setup Required:\n\n1. **Get API Key**: https://resend.com/api-keys\n2. **Add to .env**:\n   ```\n   RESEND_API_KEY=re_xxxxxxxxx\n   RESEND_WEBHOOK_SECRET=whsec_xxxxxxxxx\n   ```\n\n3. **Configure Domain** (for production):\n   Run: /resend:add-domains yourdomain.com\n\n4. **Setup Webhooks** (for tracking):\n   - Go to https://resend.com/webhooks\n   - Add endpoint: https://yourdomain.com/api/webhooks/resend\n\n### Quick Test:\n\n```typescript\n// Test sending an email\nconst response = await fetch('/api/emails/send', {\n  method: 'POST',\n  body: JSON.stringify({\n    to: 'test@example.com',\n    subject: 'Hello from Resend!',\n    template: 'welcome'\n  })\n});\n```\n\n### Related Commands:\n- /resend:add-templates - Add more email templates\n- /resend:add-contacts - Enhance contact management\n- /resend:add-broadcasts - Add campaign features\n```\n\nMark all todos as completed.\n"
              },
              {
                "name": "/init",
                "description": "Initialize Resend SDK in project with environment setup and framework detection",
                "path": "plugins/resend/commands/init.md",
                "frontmatter": {
                  "description": "Initialize Resend SDK in project with environment setup and framework detection",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Initialize Resend SDK in a TypeScript/JavaScript or Python project with automatic framework detection, dependency installation, environment configuration, and type-safe setup files.\n\nCore Principles:\n- Detect project type before making assumptions\n- Create security-hardened setup with placeholder API keys\n- Install framework-appropriate SDK variants\n- Generate production-ready configuration\n\nPhase 1: Discovery\nGoal: Understand the project and validate inputs\n\nActions:\n- Verify project path exists: !{bash test -d \"$ARGUMENTS\" && echo \"Project found\" || echo \"Project not found\"}\n- Check for package.json (Node.js/TypeScript): !{bash test -f \"$ARGUMENTS/package.json\" && echo \"Node.js project\" || echo \"No Node.js project\"}\n- Check for requirements.txt or pyproject.toml (Python): !{bash test -f \"$ARGUMENTS/requirements.txt\" -o -f \"$ARGUMENTS/pyproject.toml\" && echo \"Python project\" || echo \"No Python project\"}\n- List framework indicators: !{bash cd \"$ARGUMENTS\" && ls -la | grep -E \"(next|express|fastapi|flask|django)\" 2>/dev/null || echo \"Analyzing...\"}\n\nPhase 2: Framework Context\nGoal: Gather information about project structure and preferences\n\nActions:\n- If package.json exists, read it to understand dependencies\n- If Python, check if async framework (FastAPI/Starlette) vs sync\n- Identify existing framework (Next.js, Express, FastAPI, etc.)\n- Load project structure overview\n\nPhase 3: Agent Invocation\nGoal: Execute Resend SDK setup via specialized agent\n\nActions:\n\nTask(description=\"Initialize Resend SDK\", subagent_type=\"resend-setup-agent\", prompt=\"You are the resend-setup-agent. Initialize Resend SDK in the project at $ARGUMENTS with proper configuration, environment setup, and framework integration.\n\nRequirements:\n- Detect project type (TypeScript/JavaScript/Python)\n- Identify framework (Next.js, Express, FastAPI, Starlette, etc.)\n- Install appropriate SDK: 'resend' for Node.js or 'resend-python' for Python\n- Create .env.example with RESEND_API_KEY=your_resend_api_key_here (placeholder only)\n- Add .env* to .gitignore (except .env.example)\n- Generate client initialization file (lib/resend.ts or lib/resend.py)\n- Create framework-specific example endpoint\n- Generate TypeScript types or Python type hints\n- Validate setup with type checking or compilation\n\nDeliverable: Report initialization completion with:\n- Files created\n- Dependencies installed\n- Framework integration details\n- Setup verification results\n- Security check (confirm no hardcoded API keys)\")\n\nPhase 4: Summary\nGoal: Confirm successful setup\n\nActions:\n- Verify .env.example exists with placeholder only: !{bash grep -q \"your_resend_api_key_here\" \"$ARGUMENTS/.env.example\" && echo \"‚úì Placeholder found\" || echo \"‚ö† Verify placeholder\"}\n- Verify .gitignore protects .env: !{bash grep -q \".env\" \"$ARGUMENTS/.gitignore\" && echo \"‚úì .env protected\" || echo \"‚ö† Check .gitignore\"}\n- List generated files: !{bash find \"$ARGUMENTS\" -type f \\( -name \"*resend*\" -o -name \".env.example\" \\) 2>/dev/null}\n- Display setup completion message with next steps"
              },
              {
                "name": "/send-email",
                "description": null,
                "path": "plugins/resend/commands/send-email.md",
                "frontmatter": null,
                "content": "---\ndescription: Add email sending functionality with templates and attachments support\nargument-hint: [--batch] [--scheduled]\nallowed-tools: Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite\n---\n\n**Arguments**: $ARGUMENTS\n\nGoal: Implement complete email sending functionality with template support, batch processing, attachments, and scheduling capabilities\n\nCore Principles:\n- Support single and batch email sending\n- Provide flexible template system\n- Handle attachments securely\n- Include scheduling capabilities\n- Robust error handling and logging\n\nPhase 1: Discovery\nGoal: Gather context and requirements\n\nActions:\n- Parse $ARGUMENTS for flags (--batch, --scheduled)\n- Load existing Resend plugin structure to understand current implementation\n- Example: !{bash find /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend -type f -name \"*.md\" -o -name \"*.ts\" -o -name \"*.js\" | head -20}\n- Check if services directory exists\n- Load any existing email utilities or configurations\n\nPhase 2: Analysis\nGoal: Understand existing patterns and integration points\n\nActions:\n- Read plugin README to understand scope\n- Check existing agents and skills for patterns\n- Identify where email service should integrate\n- Determine TypeScript/JavaScript target\n- Example: !{bash ls -la /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend/}\n\nPhase 3: Planning\nGoal: Design the email sending implementation\n\nActions:\n- Outline structure needed:\n  - Email service with send(), sendBatch(), sendScheduled() methods\n  - Template engine with variable substitution\n  - Attachment handler with validation\n  - Error handling and retry logic\n  - Scheduling system integration\n- Identify configuration requirements\n- Plan integration with existing plugin structure\n\nPhase 4: Implementation\nGoal: Generate email functionality with agent\n\nActions:\n\nTask(description=\"Create email sending service\", subagent_type=\"resend-email-agent\", prompt=\"You are the resend-email-agent. Create comprehensive email sending functionality for the Resend plugin.\n\nRequirements:\n- Create email.service.ts with:\n  - send(to, from, subject, html, text?) method for single emails\n  - sendBatch(emails[]) for batch operations with concurrency control\n  - sendScheduled(email, scheduledFor) for scheduled sending\n  - Proper Resend API integration with error handling\n\n- Create templates.ts with:\n  - Template interface: {id, subject, html, variables}\n  - renderTemplate(template, variables) function\n  - template registry/loader\n\n- Create attachments.ts with:\n  - Attachment validation (size, type)\n  - File attachment handler\n  - Buffer attachment support\n\n- Create scheduler.ts with:\n  - Schedule management\n  - Retry logic with exponential backoff\n  - Job persistence\n\n- Add error handling:\n  - RateLimitError, ValidationError, SendError\n  - Retry mechanism with backoff\n  - Detailed error logging\n\n- Export clean API from index.ts\n- Include TypeScript types for all interfaces\n- Add JSDoc comments for public methods\n\nGenerated files should be in proper directory structure ready for integration.\n\nExpected output: Complete, production-ready email service module with all files\")\n\nPhase 5: Review\nGoal: Verify the generated implementation\n\nActions:\n- Check generated files exist and are properly structured\n- Verify TypeScript compilation succeeds\n- Example: !{bash cd /home/gotime2022/.claude/plugins/marketplaces/ai-dev-marketplace/plugins/resend && npx tsc --noEmit 2>&1 | head -20}\n- Validate templates and attachments handling\n- Confirm error handling patterns\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize features implemented:\n  - Single email sending with Resend API\n  - Batch processing with concurrency control\n  - Scheduling support with retry logic\n  - Template system with variable substitution\n  - Attachment handling with validation\n  - Comprehensive error handling\n- Highlight key files created\n- Note any configuration needed in project setup\n- Suggest next steps (testing, integration examples, documentation)\n"
              }
            ],
            "skills": [
              {
                "name": "api-patterns",
                "description": "Resend API integration patterns, authentication, error handling, and rate limiting. Use when implementing API clients, handling authentication, managing rate limits, implementing retry strategies, or building resilient email service integrations.",
                "path": "plugins/resend/skills/api-patterns/SKILL.md",
                "frontmatter": {
                  "name": "api-patterns",
                  "description": "Resend API integration patterns, authentication, error handling, and rate limiting. Use when implementing API clients, handling authentication, managing rate limits, implementing retry strategies, or building resilient email service integrations.",
                  "allowed-tools": "Read, Write, Bash, Grep"
                },
                "content": "# API Patterns Skill\n\nComprehensive patterns and best practices for integrating with the Resend API, covering authentication, rate limiting, error handling, and resilient retry strategies.\n\n## Use When\n\n- Implementing Resend API clients in TypeScript/JavaScript\n- Building Python-based email service integrations\n- Handling API authentication and Bearer tokens\n- Managing rate limits (2 requests/second default)\n- Implementing exponential backoff retry logic\n- Processing API error responses (4xx/5xx codes)\n- Building resilient API integrations with error recovery\n- Validating API responses and handling edge cases\n\n## Core Patterns\n\n### 1. API Authentication\n\n**Bearer token authentication** for all Resend API requests:\n\n#### TypeScript/JavaScript\n\n```typescript\nimport { Resend } from 'resend';\n\n// Initialize with API key from environment\nconst resend = new Resend(process.env.RESEND_API_KEY);\n\n// Or with explicit initialization\nconst apiKey = process.env.RESEND_API_KEY;\nif (!apiKey) {\n  throw new Error('RESEND_API_KEY environment variable is not set');\n}\n\nconst resend = new Resend(apiKey);\n\n// For custom HTTP requests (if needed)\nasync function makeAuthenticatedRequest(endpoint: string, body: any) {\n  const response = await fetch(`https://api.resend.com/${endpoint}`, {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${process.env.RESEND_API_KEY}`,\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify(body),\n  });\n\n  return response.json();\n}\n```\n\n#### Python\n\n```python\nimport os\nfrom resend import Resend\n\n# Initialize with API key from environment\napi_key = os.environ.get(\"RESEND_API_KEY\")\nif not api_key:\n    raise ValueError(\"RESEND_API_KEY environment variable is not set\")\n\nclient = Resend(api_key=api_key)\n\n# For custom HTTP requests (if needed)\nimport httpx\n\nasync def make_authenticated_request(endpoint: str, body: dict):\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            f\"https://api.resend.com/{endpoint}\",\n            headers={\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\",\n            },\n            json=body,\n        )\n        return response.json()\n```\n\n### 2. Rate Limiting Handling\n\n**Resend API rate limits**: 2 requests per second per account\n\n#### TypeScript Implementation\n\n```typescript\nclass ResendAPIClient {\n  private requestQueue: Array<() => Promise<any>> = [];\n  private isProcessing = false;\n  private readonly requestsPerSecond = 2;\n  private lastRequestTime = 0;\n\n  async executeWithRateLimit<T>(fn: () => Promise<T>): Promise<T> {\n    return new Promise((resolve, reject) => {\n      this.requestQueue.push(async () => {\n        try {\n          const result = await fn();\n          resolve(result);\n        } catch (error) {\n          reject(error);\n        }\n      });\n\n      this.processQueue();\n    });\n  }\n\n  private async processQueue() {\n    if (this.isProcessing || this.requestQueue.length === 0) {\n      return;\n    }\n\n    this.isProcessing = true;\n\n    while (this.requestQueue.length > 0) {\n      const now = Date.now();\n      const timeSinceLastRequest = now - this.lastRequestTime;\n      const delayNeeded = (1000 / this.requestsPerSecond) - timeSinceLastRequest;\n\n      if (delayNeeded > 0) {\n        await new Promise(resolve => setTimeout(resolve, delayNeeded));\n      }\n\n      const request = this.requestQueue.shift();\n      if (request) {\n        await request();\n        this.lastRequestTime = Date.now();\n      }\n    }\n\n    this.isProcessing = false;\n  }\n\n  async sendEmail(payload: any) {\n    return this.executeWithRateLimit(() =>\n      resend.emails.send(payload)\n    );\n  }\n\n  async sendBatch(emails: any[]) {\n    return this.executeWithRateLimit(() =>\n      resend.batch.send(emails)\n    );\n  }\n}\n```\n\n#### Python Implementation\n\n```python\nimport asyncio\nimport time\nfrom typing import TypeVar, Callable, Coroutine, Any\n\nT = TypeVar('T')\n\nclass ResendAPIClient:\n    def __init__(self, api_key: str, requests_per_second: int = 2):\n        self.client = Resend(api_key=api_key)\n        self.request_queue = asyncio.Queue()\n        self.is_processing = False\n        self.requests_per_second = requests_per_second\n        self.last_request_time = 0\n\n    async def execute_with_rate_limit(\n        self,\n        fn: Callable[[], Coroutine[Any, Any, T]]\n    ) -> T:\n        await self.request_queue.put(fn)\n        asyncio.create_task(self.process_queue())\n\n        # Wait for result (simplified - in production use proper queuing)\n        return await fn()\n\n    async def process_queue(self):\n        if self.is_processing or self.request_queue.empty():\n            return\n\n        self.is_processing = True\n\n        while not self.request_queue.empty():\n            now = time.time()\n            time_since_last = now - self.last_request_time\n            delay_needed = (1.0 / self.requests_per_second) - time_since_last\n\n            if delay_needed > 0:\n                await asyncio.sleep(delay_needed)\n\n            try:\n                fn = self.request_queue.get_nowait()\n                await fn()\n                self.last_request_time = time.time()\n            except asyncio.QueueEmpty:\n                break\n\n        self.is_processing = False\n\n    async def send_email(self, payload: dict):\n        async def send():\n            return self.client.emails.send(payload)\n\n        return await self.execute_with_rate_limit(send)\n```\n\n### 3. Error Response Codes\n\n**HTTP status codes and error handling**:\n\n| Code | Error | Handling Strategy |\n|------|-------|-------------------|\n| 200 | Success | Process response normally |\n| 400 | Bad Request | Validate request payload, check required fields |\n| 401 | Unauthorized | Verify API key is correct and valid |\n| 403 | Forbidden | Check API key has required permissions |\n| 404 | Not Found | Verify resource ID/email address exists |\n| 409 | Conflict | Handle duplicate resource creation attempts |\n| 429 | Rate Limited | Implement exponential backoff retry |\n| 500 | Server Error | Retry with exponential backoff |\n| 502 | Bad Gateway | Retry with exponential backoff |\n| 503 | Service Unavailable | Retry with exponential backoff |\n\n#### TypeScript Error Handler\n\n```typescript\ninterface APIError {\n  code: number;\n  message: string;\n  statusText: string;\n}\n\nasync function handleAPIError(error: any): Promise<void> {\n  if (error.response) {\n    const status = error.response.status;\n    const data = error.response.data;\n\n    switch (status) {\n      case 400:\n        console.error('Bad Request:', data.message);\n        throw new Error(`Invalid request: ${data.message}`);\n\n      case 401:\n        console.error('Unauthorized: Check API key');\n        throw new Error('Invalid API key. Set RESEND_API_KEY environment variable.');\n\n      case 403:\n        console.error('Forbidden: Insufficient permissions');\n        throw new Error('API key lacks required permissions.');\n\n      case 404:\n        console.error('Not Found:', data.message);\n        throw new Error(`Resource not found: ${data.message}`);\n\n      case 409:\n        console.error('Conflict: Resource already exists');\n        throw new Error(`Duplicate resource: ${data.message}`);\n\n      case 429:\n        console.warn('Rate limited: Implement retry');\n        throw new Error('Rate limit exceeded. Retry after delay.');\n\n      case 500:\n      case 502:\n      case 503:\n        console.error(`Server error (${status}): Retry recommended`);\n        throw new Error(`Server error (${status}). Retry in progress...`);\n\n      default:\n        console.error(`Unknown error (${status}):`, data);\n        throw new Error(`API error: ${data.message || 'Unknown error'}`);\n    }\n  }\n\n  throw error;\n}\n```\n\n#### Python Error Handler\n\n```python\nimport logging\nfrom typing import Optional, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nclass ResendAPIError(Exception):\n    \"\"\"Base exception for Resend API errors\"\"\"\n    pass\n\nclass AuthenticationError(ResendAPIError):\n    \"\"\"API authentication failed\"\"\"\n    pass\n\nclass RateLimitError(ResendAPIError):\n    \"\"\"Rate limit exceeded\"\"\"\n    pass\n\nclass ServerError(ResendAPIError):\n    \"\"\"Server-side error\"\"\"\n    pass\n\ndef handle_api_error(error: Exception, status_code: Optional[int] = None) -> None:\n    \"\"\"Handle Resend API errors based on status code\"\"\"\n\n    if status_code == 400:\n        logger.error(f\"Bad Request: {str(error)}\")\n        raise ResendAPIError(f\"Invalid request: {str(error)}\")\n\n    elif status_code == 401:\n        logger.error(\"Unauthorized: Check API key\")\n        raise AuthenticationError(\n            \"Invalid API key. Set RESEND_API_KEY environment variable.\"\n        )\n\n    elif status_code == 403:\n        logger.error(\"Forbidden: Insufficient permissions\")\n        raise AuthenticationError(\"API key lacks required permissions.\")\n\n    elif status_code == 404:\n        logger.error(f\"Not Found: {str(error)}\")\n        raise ResendAPIError(f\"Resource not found: {str(error)}\")\n\n    elif status_code == 409:\n        logger.error(\"Conflict: Resource already exists\")\n        raise ResendAPIError(f\"Duplicate resource: {str(error)}\")\n\n    elif status_code == 429:\n        logger.warning(\"Rate limited: Implement retry\")\n        raise RateLimitError(\"Rate limit exceeded. Retry after delay.\")\n\n    elif status_code in [500, 502, 503]:\n        logger.error(f\"Server error ({status_code}): Retry recommended\")\n        raise ServerError(f\"Server error ({status_code}). Retry in progress...\")\n\n    else:\n        logger.error(f\"Unknown error: {str(error)}\")\n        raise ResendAPIError(f\"API error: {str(error)}\")\n```\n\n### 4. Exponential Backoff Retry Strategy\n\n**Resilient retry logic** with exponential backoff for transient failures:\n\n#### TypeScript Implementation\n\n```typescript\ninterface RetryOptions {\n  maxRetries?: number;\n  initialDelayMs?: number;\n  maxDelayMs?: number;\n  backoffMultiplier?: number;\n  retryableStatusCodes?: number[];\n}\n\nconst DEFAULT_RETRY_OPTIONS: Required<RetryOptions> = {\n  maxRetries: 5,\n  initialDelayMs: 100,\n  maxDelayMs: 30000,\n  backoffMultiplier: 2,\n  retryableStatusCodes: [408, 429, 500, 502, 503, 504],\n};\n\nasync function withExponentialBackoff<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions = {}\n): Promise<T> {\n  const config = { ...DEFAULT_RETRY_OPTIONS, ...options };\n  let lastError: Error | null = null;\n  let delayMs = config.initialDelayMs;\n\n  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error: any) {\n      lastError = error;\n\n      // Check if error is retryable\n      const isRetryable =\n        (error.response?.status &&\n         config.retryableStatusCodes.includes(error.response.status)) ||\n        error.code === 'ECONNREFUSED' ||\n        error.code === 'ETIMEDOUT';\n\n      if (!isRetryable || attempt === config.maxRetries) {\n        throw error;\n      }\n\n      // Calculate delay with jitter\n      const jitter = Math.random() * 0.1 * delayMs;\n      const actualDelay = Math.min(delayMs + jitter, config.maxDelayMs);\n\n      console.warn(\n        `Attempt ${attempt + 1} failed. Retrying in ${actualDelay.toFixed(0)}ms...`,\n        error.message\n      );\n\n      await new Promise(resolve => setTimeout(resolve, actualDelay));\n      delayMs = Math.min(delayMs * config.backoffMultiplier, config.maxDelayMs);\n    }\n  }\n\n  throw lastError || new Error('Retry loop exhausted');\n}\n\n// Usage\nasync function sendEmailWithRetry(payload: any) {\n  return withExponentialBackoff(\n    () => resend.emails.send(payload),\n    {\n      maxRetries: 5,\n      initialDelayMs: 100,\n      maxDelayMs: 10000,\n      backoffMultiplier: 2,\n    }\n  );\n}\n```\n\n#### Python Implementation\n\n```python\nimport asyncio\nimport random\nimport logging\nfrom typing import TypeVar, Callable, Coroutine, Optional, List\nfrom enum import Enum\n\nlogger = logging.getLogger(__name__)\nT = TypeVar('T')\n\nclass RetryStrategy(Enum):\n    EXPONENTIAL = \"exponential\"\n    LINEAR = \"linear\"\n\nasync def with_exponential_backoff(\n    fn: Callable[[], Coroutine],\n    max_retries: int = 5,\n    initial_delay_ms: float = 100,\n    max_delay_ms: float = 30000,\n    backoff_multiplier: float = 2,\n    retryable_status_codes: Optional[List[int]] = None,\n) -> T:\n    \"\"\"\n    Execute async function with exponential backoff retry\n\n    Args:\n        fn: Async function to execute\n        max_retries: Maximum retry attempts\n        initial_delay_ms: Initial delay in milliseconds\n        max_delay_ms: Maximum delay in milliseconds\n        backoff_multiplier: Multiplier for each retry\n        retryable_status_codes: HTTP status codes to retry on\n\n    Returns:\n        Result from function call\n\n    Raises:\n        Exception: If all retries exhausted\n    \"\"\"\n    if retryable_status_codes is None:\n        retryable_status_codes = [408, 429, 500, 502, 503, 504]\n\n    last_error = None\n    delay_ms = initial_delay_ms\n\n    for attempt in range(max_retries + 1):\n        try:\n            return await fn()\n        except Exception as error:\n            last_error = error\n\n            # Check if error is retryable\n            is_retryable = (\n                (hasattr(error, 'status_code') and\n                 error.status_code in retryable_status_codes) or\n                'timeout' in str(error).lower() or\n                'connection' in str(error).lower()\n            )\n\n            if not is_retryable or attempt == max_retries:\n                raise error\n\n            # Calculate delay with jitter\n            jitter = random.random() * 0.1 * delay_ms\n            actual_delay = min(delay_ms + jitter, max_delay_ms)\n\n            logger.warning(\n                f\"Attempt {attempt + 1} failed. \"\n                f\"Retrying in {actual_delay:.0f}ms... Error: {str(error)}\"\n            )\n\n            await asyncio.sleep(actual_delay / 1000)\n            delay_ms = min(delay_ms * backoff_multiplier, max_delay_ms)\n\n    if last_error:\n        raise last_error\n    raise Exception(\"Retry loop exhausted\")\n```\n\n### 5. Request Validation\n\n**Validate payloads before sending** to catch errors early:\n\n#### TypeScript Validator\n\n```typescript\ninterface EmailPayload {\n  from: string;\n  to: string | string[];\n  subject: string;\n  html?: string;\n  text?: string;\n  cc?: string[];\n  bcc?: string[];\n  reply_to?: string;\n  attachments?: Array<{ filename: string; content: Buffer | string }>;\n  scheduled_at?: string;\n  tags?: Array<{ name: string; value: string }>;\n}\n\nfunction validateEmailPayload(payload: any): { valid: boolean; errors: string[] } {\n  const errors: string[] = [];\n\n  // Check required fields\n  if (!payload.from) {\n    errors.push(\"'from' field is required\");\n  }\n  if (!payload.to) {\n    errors.push(\"'to' field is required\");\n  }\n  if (!payload.subject) {\n    errors.push(\"'subject' field is required\");\n  }\n\n  // Validate email addresses\n  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n\n  if (payload.from && !emailRegex.test(payload.from)) {\n    errors.push(`Invalid 'from' email: ${payload.from}`);\n  }\n\n  const recipients = Array.isArray(payload.to) ? payload.to : [payload.to];\n  recipients.forEach((email: string) => {\n    if (!emailRegex.test(email)) {\n      errors.push(`Invalid recipient email: ${email}`);\n    }\n  });\n\n  // Validate optional email fields\n  if (payload.reply_to && !emailRegex.test(payload.reply_to)) {\n    errors.push(`Invalid 'reply_to' email: ${payload.reply_to}`);\n  }\n\n  if (payload.cc) {\n    payload.cc.forEach((email: string) => {\n      if (!emailRegex.test(email)) {\n        errors.push(`Invalid 'cc' email: ${email}`);\n      }\n    });\n  }\n\n  if (payload.bcc) {\n    payload.bcc.forEach((email: string) => {\n      if (!emailRegex.test(email)) {\n        errors.push(`Invalid 'bcc' email: ${email}`);\n      }\n    });\n  }\n\n  // Validate attachment content\n  if (payload.attachments) {\n    payload.attachments.forEach((att: any, index: number) => {\n      if (!att.filename) {\n        errors.push(`Attachment ${index} missing 'filename'`);\n      }\n      if (!att.content) {\n        errors.push(`Attachment ${index} missing 'content'`);\n      }\n    });\n  }\n\n  // Validate scheduled_at format if present\n  if (payload.scheduled_at) {\n    try {\n      new Date(payload.scheduled_at);\n    } catch {\n      errors.push(`Invalid 'scheduled_at' format: ${payload.scheduled_at}`);\n    }\n  }\n\n  return {\n    valid: errors.length === 0,\n    errors,\n  };\n}\n\n// Usage\nasync function sendEmailSafely(payload: EmailPayload) {\n  const validation = validateEmailPayload(payload);\n\n  if (!validation.valid) {\n    console.error('Validation errors:', validation.errors);\n    throw new Error(`Payload validation failed: ${validation.errors.join(', ')}`);\n  }\n\n  return resend.emails.send(payload);\n}\n```\n\n#### Python Validator\n\n```python\nimport re\nfrom typing import Dict, List, Any, Tuple\nfrom datetime import datetime\n\nclass EmailPayloadValidator:\n    EMAIL_REGEX = r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$'\n\n    @staticmethod\n    def validate_email(email: str) -> bool:\n        \"\"\"Validate email address format\"\"\"\n        return re.match(EmailPayloadValidator.EMAIL_REGEX, email) is not None\n\n    @staticmethod\n    def validate_payload(payload: Dict[str, Any]) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Validate email payload before sending\n\n        Returns:\n            Tuple of (is_valid, error_list)\n        \"\"\"\n        errors = []\n\n        # Check required fields\n        if not payload.get('from'):\n            errors.append(\"'from' field is required\")\n        if not payload.get('to'):\n            errors.append(\"'to' field is required\")\n        if not payload.get('subject'):\n            errors.append(\"'subject' field is required\")\n\n        # Validate email addresses\n        if payload.get('from'):\n            if not EmailPayloadValidator.validate_email(payload['from']):\n                errors.append(f\"Invalid 'from' email: {payload['from']}\")\n\n        # Validate recipients\n        recipients = payload.get('to', [])\n        if isinstance(recipients, str):\n            recipients = [recipients]\n\n        for email in recipients:\n            if not EmailPayloadValidator.validate_email(email):\n                errors.append(f\"Invalid recipient email: {email}\")\n\n        # Validate optional email fields\n        if payload.get('reply_to'):\n            if not EmailPayloadValidator.validate_email(payload['reply_to']):\n                errors.append(f\"Invalid 'reply_to' email: {payload['reply_to']}\")\n\n        for cc_email in payload.get('cc', []):\n            if not EmailPayloadValidator.validate_email(cc_email):\n                errors.append(f\"Invalid 'cc' email: {cc_email}\")\n\n        for bcc_email in payload.get('bcc', []):\n            if not EmailPayloadValidator.validate_email(bcc_email):\n                errors.append(f\"Invalid 'bcc' email: {bcc_email}\")\n\n        # Validate attachments\n        for i, attachment in enumerate(payload.get('attachments', [])):\n            if not attachment.get('filename'):\n                errors.append(f\"Attachment {i} missing 'filename'\")\n            if not attachment.get('content'):\n                errors.append(f\"Attachment {i} missing 'content'\")\n\n        # Validate scheduled_at format\n        if payload.get('scheduled_at'):\n            try:\n                datetime.fromisoformat(\n                    payload['scheduled_at'].replace('Z', '+00:00')\n                )\n            except ValueError:\n                errors.append(f\"Invalid 'scheduled_at' format: {payload['scheduled_at']}\")\n\n        return len(errors) == 0, errors\n\ndef send_email_safely(client, payload: Dict[str, Any]):\n    \"\"\"Send email with validation\"\"\"\n    is_valid, errors = EmailPayloadValidator.validate_payload(payload)\n\n    if not is_valid:\n        raise ValueError(f\"Payload validation failed: {', '.join(errors)}\")\n\n    return client.emails.send(payload)\n```\n\n## Environment Setup\n\n### Required Environment Variables\n\n```bash\n# .env\nRESEND_API_KEY=your_resend_api_key_here\n```\n\n### .env.example (Safe to Commit)\n\n```bash\n# Resend API Configuration\nRESEND_API_KEY=your_resend_api_key_here\n\n# Optional: Custom request timeout (ms)\nRESEND_REQUEST_TIMEOUT=30000\n\n# Optional: Max retries for transient failures\nRESEND_MAX_RETRIES=5\n\n# Optional: Rate limit requests per second\nRESEND_RATE_LIMIT=2\n```\n\n### Installation\n\n#### TypeScript/JavaScript\n\n```bash\nnpm install resend\n# or\nyarn add resend\n# or\npnpm add resend\n```\n\n#### Python\n\n```bash\npip install resend\n```\n\n## Best Practices\n\n### 1. Always Use Environment Variables\n\n```typescript\n// CORRECT\nconst apiKey = process.env.RESEND_API_KEY;\n\n// WRONG - Never hardcode\nconst apiKey = 're_abc123xyz...';\n```\n\n### 2. Implement Graceful Degradation\n\n```typescript\nasync function sendEmailWithFallback(payload: EmailPayload) {\n  try {\n    return await resend.emails.send(payload);\n  } catch (error) {\n    // Log error for monitoring\n    console.error('Email send failed:', error);\n\n    // Implement fallback behavior\n    // - Queue for retry\n    // - Alert administrator\n    // - Use alternative service\n\n    throw error;\n  }\n}\n```\n\n### 3. Monitor API Usage\n\n```typescript\ninterface APIMetrics {\n  successCount: number;\n  failureCount: number;\n  rateLimitHits: number;\n  averageResponseTime: number;\n}\n\nclass APIMonitor {\n  private metrics: APIMetrics = {\n    successCount: 0,\n    failureCount: 0,\n    rateLimitHits: 0,\n    averageResponseTime: 0,\n  };\n\n  recordSuccess(duration: number) {\n    this.metrics.successCount++;\n    this.updateAverageResponseTime(duration);\n  }\n\n  recordFailure(error: any) {\n    this.metrics.failureCount++;\n    if (error?.response?.status === 429) {\n      this.metrics.rateLimitHits++;\n    }\n  }\n\n  private updateAverageResponseTime(duration: number) {\n    const totalRequests = this.metrics.successCount + this.metrics.failureCount;\n    const currentAvg = this.metrics.averageResponseTime;\n    this.metrics.averageResponseTime =\n      (currentAvg * (totalRequests - 1) + duration) / totalRequests;\n  }\n\n  getMetrics(): APIMetrics {\n    return { ...this.metrics };\n  }\n}\n```\n\n### 4. Implement Circuit Breaker Pattern\n\n```typescript\nenum CircuitState {\n  CLOSED = 'CLOSED',\n  OPEN = 'OPEN',\n  HALF_OPEN = 'HALF_OPEN',\n}\n\nclass CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount = 0;\n  private failureThreshold = 5;\n  private resetTimeout = 60000; // 1 minute\n  private lastFailureTime = 0;\n\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\n    if (this.state === CircuitState.OPEN) {\n      if (Date.now() - this.lastFailureTime > this.resetTimeout) {\n        this.state = CircuitState.HALF_OPEN;\n      } else {\n        throw new Error('Circuit breaker is OPEN');\n      }\n    }\n\n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      throw error;\n    }\n  }\n\n  private onSuccess() {\n    this.failureCount = 0;\n    this.state = CircuitState.CLOSED;\n  }\n\n  private onFailure() {\n    this.failureCount++;\n    this.lastFailureTime = Date.now();\n    if (this.failureCount >= this.failureThreshold) {\n      this.state = CircuitState.OPEN;\n    }\n  }\n}\n```\n\n## Examples Directory Structure\n\n- `typescript-client/` - Complete TypeScript client setup with authentication\n- `python-client/` - Complete Python client setup with authentication\n- `error-handling/` - Comprehensive error handling and recovery patterns\n\nSee individual example README files for complete code and usage patterns.\n\n## Related Skills\n\n- **email-delivery** - Email sending patterns (single, batch, scheduled)\n- **email-templates** - HTML template management and rendering\n- **email-webhooks** - Delivery event tracking and status handling\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented\n- Clear guidance on key acquisition\n\n## Resources\n\n- [Resend API Documentation](https://resend.com/docs)\n- [Resend API Reference](https://resend.com/docs/api-reference)\n- [Authentication Best Practices](https://resend.com/docs/knowledge-base/authentication)\n- [Rate Limiting Guide](https://resend.com/docs/knowledge-base/rate-limiting)\n- [HTTP Status Codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)"
              },
              {
                "name": "email-delivery",
                "description": "Email delivery patterns including single, batch, scheduled emails and attachment handling. Use when building transactional email systems, batch communication workflows, scheduled delivery, or implementing file/URL attachments with reply-to and CC/BCC functionality.",
                "path": "plugins/resend/skills/email-delivery/SKILL.md",
                "frontmatter": {
                  "name": "email-delivery",
                  "description": "Email delivery patterns including single, batch, scheduled emails and attachment handling. Use when building transactional email systems, batch communication workflows, scheduled delivery, or implementing file/URL attachments with reply-to and CC/BCC functionality.",
                  "allowed-tools": "Read, Write, Bash, Grep"
                },
                "content": "# Email Delivery Skill\n\nComprehensive patterns and templates for implementing robust email delivery with Resend, covering single emails, batch operations, scheduled delivery, and attachment handling.\n\n## Use When\n\n- Building transactional email systems (confirmations, notifications, alerts)\n- Implementing batch email campaigns (up to 100 recipients per request)\n- Setting up scheduled/delayed email delivery\n- Handling file attachments, buffers, or URL-based attachments\n- Adding reply-to, CC, and BCC functionality\n- Creating email templates with variables and dynamic content\n- Implementing retry logic and delivery error handling\n\n## Core Patterns\n\n### 1. Single Email Sending\n\n**Transactional emails** for immediate delivery with minimal latency:\n\n```typescript\nimport { Resend } from 'resend';\n\nconst resend = new Resend('your_resend_key_here');\n\nasync function sendTransactionalEmail() {\n  const { data, error } = await resend.emails.send({\n    from: 'notifications@example.com',\n    to: 'user@example.com',\n    subject: 'Welcome to Example',\n    html: '<h1>Welcome!</h1><p>Thank you for signing up.</p>',\n  });\n\n  if (error) {\n    console.error('Failed to send email:', error);\n    return null;\n  }\n\n  return data;\n}\n```\n\n### 2. Batch Email Sending\n\n**Bulk operations** for sending up to 100 emails in a single request:\n\n```typescript\nasync function sendBatchEmails(recipients: Array<{email: string; name: string}>) {\n  const emails = recipients.map(recipient => ({\n    from: 'newsletter@example.com',\n    to: recipient.email,\n    subject: `Hello ${recipient.name}!`,\n    html: `<p>Welcome ${recipient.name}</p>`,\n  }));\n\n  const { data, error } = await resend.batch.send(emails);\n\n  if (error) {\n    console.error('Batch send failed:', error);\n    return null;\n  }\n\n  return data;\n}\n```\n\n### 3. Scheduled Email Delivery\n\n**Time-based delivery** for emails sent at specific times:\n\n```typescript\nasync function scheduleEmail(scheduledAt: Date) {\n  const { data, error } = await resend.emails.send({\n    from: 'marketing@example.com',\n    to: 'user@example.com',\n    subject: 'Scheduled Message',\n    html: '<p>This was scheduled!</p>',\n    scheduled_at: scheduledAt.toISOString(),\n  });\n\n  if (error) {\n    console.error('Failed to schedule email:', error);\n    return null;\n  }\n\n  return data;\n}\n```\n\n### 4. Attachment Handling\n\n**File attachments** from files, buffers, or URLs:\n\n#### File-based Attachment\n\n```typescript\nimport fs from 'fs';\nimport path from 'path';\n\nasync function sendWithFileAttachment(filePath: string) {\n  const fileContent = fs.readFileSync(filePath);\n  const fileName = path.basename(filePath);\n\n  const { data, error } = await resend.emails.send({\n    from: 'documents@example.com',\n    to: 'recipient@example.com',\n    subject: 'Your Document',\n    html: '<p>Please find attached your document.</p>',\n    attachments: [\n      {\n        filename: fileName,\n        content: fileContent,\n      },\n    ],\n  });\n\n  return { data, error };\n}\n```\n\n#### Buffer-based Attachment\n\n```typescript\nasync function sendWithBufferAttachment(buffer: Buffer, filename: string) {\n  const { data, error } = await resend.emails.send({\n    from: 'reports@example.com',\n    to: 'user@example.com',\n    subject: 'Monthly Report',\n    html: '<p>Your monthly report is attached.</p>',\n    attachments: [\n      {\n        filename: filename,\n        content: buffer,\n      },\n    ],\n  });\n\n  return { data, error };\n}\n```\n\n#### URL-based Attachment\n\n```typescript\nasync function sendWithUrlAttachment(fileUrl: string) {\n  const response = await fetch(fileUrl);\n  const buffer = await response.arrayBuffer();\n\n  const { data, error } = await resend.emails.send({\n    from: 'notifications@example.com',\n    to: 'user@example.com',\n    subject: 'Download Your File',\n    html: '<p>Your file is ready.</p>',\n    attachments: [\n      {\n        filename: 'document.pdf',\n        content: Buffer.from(buffer),\n      },\n    ],\n  });\n\n  return { data, error };\n}\n```\n\n### 5. Reply-To and CC/BCC\n\n**Message routing** with multiple recipients and reply addresses:\n\n```typescript\nasync function sendWithRouting(mainRecipient: string) {\n  const { data, error } = await resend.emails.send({\n    from: 'support@example.com',\n    to: mainRecipient,\n    reply_to: 'support-team@example.com',\n    cc: ['manager@example.com'],\n    bcc: ['archive@example.com'],\n    subject: 'Support Ticket #12345',\n    html: '<p>We received your support request.</p>',\n  });\n\n  return { data, error };\n}\n```\n\n## Python Patterns\n\n### Single Email (Python)\n\n```python\nimport os\nfrom resend import Resend\n\nclient = Resend(api_key=os.environ.get(\"RESEND_API_KEY\"))\n\ndef send_email():\n    email = {\n        \"from\": \"notifications@example.com\",\n        \"to\": \"user@example.com\",\n        \"subject\": \"Welcome\",\n        \"html\": \"<h1>Welcome!</h1>\",\n    }\n\n    response = client.emails.send(email)\n    return response\n```\n\n### Batch Email (Python)\n\n```python\ndef send_batch_emails(recipients):\n    emails = [\n        {\n            \"from\": \"newsletter@example.com\",\n            \"to\": recipient[\"email\"],\n            \"subject\": f\"Hello {recipient['name']}\",\n            \"html\": f\"<p>Welcome {recipient['name']}</p>\",\n        }\n        for recipient in recipients\n    ]\n\n    response = client.batch.send(emails)\n    return response\n```\n\n### File Attachment (Python)\n\n```python\ndef send_with_attachment(file_path):\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    email = {\n        \"from\": \"documents@example.com\",\n        \"to\": \"recipient@example.com\",\n        \"subject\": \"Your Document\",\n        \"html\": \"<p>Document attached.</p>\",\n        \"attachments\": [\n            {\n                \"filename\": \"document.pdf\",\n                \"content\": file_content,\n            }\n        ],\n    }\n\n    response = client.emails.send(email)\n    return response\n```\n\n## Template Variables\n\n### Environment Variables Required\n\n```bash\nRESEND_API_KEY=your_resend_key_here\nRESEND_FROM_EMAIL=your-verified-email@example.com\n```\n\n### Email Template Variables\n\n```typescript\ninterface EmailPayload {\n  from: string;              // Verified sender email\n  to: string | string[];     // Recipient(s)\n  cc?: string[];            // Carbon copy recipients\n  bcc?: string[];           // Blind carbon copy\n  reply_to?: string;        // Reply-to address\n  subject: string;          // Email subject\n  html?: string;            // HTML content\n  text?: string;            // Plain text fallback\n  attachments?: Array<{\n    filename: string;\n    content: Buffer | string;\n  }>;\n  scheduled_at?: string;    // ISO 8601 datetime for scheduling\n  tags?: Array<{\n    name: string;\n    value: string;\n  }>;\n}\n```\n\n## Best Practices\n\n### Error Handling\n\nAlways implement retry logic for transient failures:\n\n```typescript\nasync function sendWithRetry(emailPayload, maxRetries = 3) {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    const { data, error } = await resend.emails.send(emailPayload);\n\n    if (!error) return { data, success: true };\n\n    if (error.message?.includes('rate_limit') && attempt < maxRetries) {\n      const delay = Math.pow(2, attempt) * 1000;\n      await new Promise(resolve => setTimeout(resolve, delay));\n      continue;\n    }\n\n    return { error, success: false };\n  }\n}\n```\n\n### Rate Limiting\n\nResend has rate limits. For batch operations over 100 emails:\n\n```typescript\nasync function sendLargeBatch(emails: EmailPayload[]) {\n  const batchSize = 100;\n  const results = [];\n\n  for (let i = 0; i < emails.length; i += batchSize) {\n    const batch = emails.slice(i, i + batchSize);\n    const { data, error } = await resend.batch.send(batch);\n\n    if (error) {\n      console.error(`Batch ${Math.floor(i / batchSize) + 1} failed:`, error);\n      results.push({ success: false, error });\n    } else {\n      results.push({ success: true, data });\n    }\n\n    // Rate limit handling - wait between batches\n    if (i + batchSize < emails.length) {\n      await new Promise(resolve => setTimeout(resolve, 1000));\n    }\n  }\n\n  return results;\n}\n```\n\n### Template Best Practices\n\n- Use responsive HTML templates\n- Always provide text fallback\n- Test across email clients\n- Include unsubscribe links for marketing emails\n- Avoid large attachments (keep under 25MB)\n\n## Examples Directory Structure\n\n- `single-email/` - Basic transactional email patterns\n- `batch-emails/` - Bulk sending with 50-100 recipients\n- `attachments/` - File, buffer, and URL attachment handling\n- `scheduled/` - Time-based delivery scheduling\n\nSee individual example README files for complete code and usage patterns.\n\n## Related Skills\n\n- **email-templates** - HTML template management and rendering\n- **email-validation** - Recipient address validation\n- **email-webhooks** - Delivery event tracking and bounce handling\n\n## Resources\n\n- [Resend API Documentation](https://resend.com/docs)\n- [Email Authentication (SPF, DKIM, DMARC)](https://resend.com/docs/knowledge-base/authentication)\n- [Batch Email API](https://resend.com/docs/api-reference/emails/batch)\n- [Scheduled Emails](https://resend.com/docs/api-reference/emails/send#scheduled_at)\n\n## Security Notes\n\n- API keys should be stored in environment variables (never hardcoded)\n- Use `RESEND_API_KEY` from secure secret management\n- Verify sender emails before using in production\n- Implement authentication for email submission endpoints\n- Log email events for compliance and debugging"
              },
              {
                "name": "react-email-templates",
                "description": "React Email component patterns for building responsive email templates using JSX, component composition, and preview servers. Use when creating reusable email components, building responsive templates, setting up email preview environments, or integrating React Email with Resend for dynamic email content.",
                "path": "plugins/resend/skills/react-email-templates/SKILL.md",
                "frontmatter": {
                  "name": "react-email-templates",
                  "description": "React Email component patterns for building responsive email templates using JSX, component composition, and preview servers. Use when creating reusable email components, building responsive templates, setting up email preview environments, or integrating React Email with Resend for dynamic email content.",
                  "allowed-tools": "Read, Write, Bash, Grep"
                },
                "content": "# React Email Templates Skill\n\nComprehensive patterns and templates for building modern, responsive email templates using React Email components and JSX, with preview server setup and Resend integration.\n\n## Use When\n\n- Building reusable email components with React JSX\n- Creating responsive email templates that work across clients\n- Setting up React Email preview servers for development\n- Integrating React Email with Resend for dynamic content\n- Implementing welcome, transactional, and marketing emails\n- Styling emails with Tailwind CSS via @react-email/components\n- Testing email layouts before sending\n- Creating component libraries for email templates\n\n## Core Concepts\n\n### What is React Email\n\nReact Email is a library for building responsive, maintainable emails using React components and JSX. It provides:\n\n- **JSX Templates**: Write email templates as React components\n- **Built-in Components**: Email-safe components (Container, Row, Column, Text, Image, etc.)\n- **Styling**: Tailwind CSS support with safe subset for email clients\n- **Preview Server**: Built-in development server to test email rendering\n- **Type Safety**: Full TypeScript support for email props\n- **Framework Agnostic**: Send emails with Resend, SendGrid, Nodemailer, etc.\n\n### Installation\n\n```bash\nnpm install react-email @react-email/components\n# or\nyarn add react-email @react-email/components\n```\n\n### Package Structure\n\n```typescript\n// Core React Email\nimport { render } from 'react-email';\n\n// Components\nimport {\n  Body,\n  Button,\n  Column,\n  Container,\n  Head,\n  Hr,\n  Html,\n  Img,\n  Link,\n  Preview,\n  Row,\n  Section,\n  Text,\n  Font,\n  Head as EmailHead,\n} from '@react-email/components';\n```\n\n## Core Patterns\n\n### 1. Basic Email Component\n\n**Simple, responsive email template structure:**\n\n```typescript\nimport { Body, Container, Head, Html, Preview, Section, Text } from '@react-email/components';\n\ninterface BasicEmailProps {\n  userName: string;\n  message: string;\n}\n\nexport const BasicEmail: React.FC<BasicEmailProps> = ({ userName, message }) => {\n  return (\n    <Html>\n      <Head />\n      <Preview>Welcome to our service, {userName}!</Preview>\n      <Body style={main}>\n        <Container style={container}>\n          <Section>\n            <Text style={heading}>Welcome, {userName}!</Text>\n            <Text style={paragraph}>{message}</Text>\n          </Section>\n        </Container>\n      </Body>\n    </Html>\n  );\n};\n\nconst main = {\n  backgroundColor: '#f6f9fc',\n  fontFamily: '-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Ubuntu,sans-serif',\n};\n\nconst container = {\n  backgroundColor: '#ffffff',\n  margin: '0 auto',\n  padding: '20px 0 48px',\n  marginBottom: '64px',\n};\n\nconst heading = {\n  fontSize: '32px',\n  lineHeight: '1.3',\n  fontWeight: '700',\n  color: '#1f2937',\n};\n\nconst paragraph = {\n  fontSize: '16px',\n  lineHeight: '26px',\n  color: '#525252',\n};\n```\n\n### 2. Welcome Email Template\n\n**Complete welcome email with branding, CTA button, and footer:**\n\n```typescript\nimport {\n  Body,\n  Button,\n  Column,\n  Container,\n  Head,\n  Hr,\n  Html,\n  Img,\n  Link,\n  Preview,\n  Row,\n  Section,\n  Text,\n} from '@react-email/components';\n\ninterface WelcomeEmailProps {\n  userName: string;\n  userEmail: string;\n  activationUrl: string;\n  companyName?: string;\n}\n\nexport const WelcomeEmail: React.FC<WelcomeEmailProps> = ({\n  userName,\n  userEmail,\n  activationUrl,\n  companyName = 'Our Company',\n}) => {\n  return (\n    <Html>\n      <Head>\n        <Font\n          fontFamily=\"Geist\"\n          fallbackFontFamily=\"Verdana\"\n          webFont={{\n            url: 'https://cdn.jsdelivr.net/npm/geist-font@1.0.0/dist/geist-mono.woff2',\n            format: 'woff2',\n          }}\n        />\n      </Head>\n      <Preview>Welcome to {companyName}, {userName}!</Preview>\n      <Body style={main}>\n        <Container style={container}>\n          {/* Header with Logo */}\n          <Section style={header}>\n            <Row>\n              <Column>\n                <Img src={`https://${process.env.VERCEL_URL}/logo.png`} width=\"40\" height=\"40\" alt={companyName} />\n              </Column>\n              <Column style={{ paddingLeft: '8px' }}>\n                <Text style={headerText}>{companyName}</Text>\n              </Column>\n            </Row>\n          </Section>\n\n          {/* Main Content */}\n          <Section style={content}>\n            <Text style={heading}>Welcome to {companyName}!</Text>\n            <Text style={paragraph}>\n              Hi {userName},\n            </Text>\n            <Text style={paragraph}>\n              Thank you for signing up. We're excited to have you on board. To get started, please verify your email address by clicking the button below.\n            </Text>\n\n            {/* CTA Button */}\n            <Section style={buttonContainer}>\n              <Button style={button} href={activationUrl}>\n                Verify Email Address\n              </Button>\n            </Section>\n\n            <Text style={paragraph}>\n              This link expires in 24 hours. If you didn't create this account, you can ignore this email.\n            </Text>\n          </Section>\n\n          <Hr style={hr} />\n\n          {/* Footer */}\n          <Section style={footer}>\n            <Row>\n              <Column>\n                <Text style={footerText}>\n                  {companyName} Inc. | {userEmail}\n                </Text>\n                <Text style={footerText}>\n                  <Link href=\"https://example.com/unsubscribe\" style={link}>\n                    Unsubscribe\n                  </Link>\n                  {' | '}\n                  <Link href=\"https://example.com/preferences\" style={link}>\n                    Preferences\n                  </Link>\n                </Text>\n              </Column>\n            </Row>\n          </Section>\n        </Container>\n      </Body>\n    </Html>\n  );\n};\n\n// Styles\nconst main = {\n  backgroundColor: '#f6f9fc',\n  fontFamily: 'Geist, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif',\n};\n\nconst container = {\n  backgroundColor: '#ffffff',\n  margin: '0 auto',\n  padding: '20px 0 48px',\n  marginBottom: '64px',\n};\n\nconst header = {\n  padding: '20px 0',\n  borderBottom: '1px solid #e5e7eb',\n};\n\nconst headerText = {\n  fontSize: '18px',\n  fontWeight: '700',\n  color: '#1f2937',\n};\n\nconst content = {\n  padding: '32px 0',\n};\n\nconst heading = {\n  fontSize: '28px',\n  lineHeight: '1.3',\n  fontWeight: '700',\n  color: '#1f2937',\n  marginBottom: '16px',\n};\n\nconst paragraph = {\n  fontSize: '15px',\n  lineHeight: '1.6',\n  color: '#525252',\n  marginBottom: '12px',\n};\n\nconst buttonContainer = {\n  paddingTop: '16px',\n  paddingBottom: '16px',\n};\n\nconst button = {\n  backgroundColor: '#2563eb',\n  borderRadius: '4px',\n  color: '#ffffff',\n  fontSize: '15px',\n  fontWeight: '600',\n  padding: '12px 20px',\n  textDecoration: 'none',\n  textAlign: 'center' as const,\n};\n\nconst hr = {\n  borderColor: '#e5e7eb',\n  margin: '0',\n};\n\nconst footer = {\n  paddingTop: '24px',\n  paddingBottom: '24px',\n  borderTop: '1px solid #e5e7eb',\n};\n\nconst footerText = {\n  fontSize: '12px',\n  color: '#6b7280',\n  margin: '0 0 4px 0',\n};\n\nconst link = {\n  color: '#2563eb',\n  textDecoration: 'underline',\n};\n```\n\n### 3. Transactional Email - Order Confirmation\n\n**Order confirmation email with itemized details and status tracking:**\n\n```typescript\nimport {\n  Body,\n  Button,\n  Column,\n  Container,\n  Head,\n  Html,\n  Img,\n  Link,\n  Preview,\n  Row,\n  Section,\n  Text,\n} from '@react-email/components';\n\ninterface OrderConfirmationProps {\n  orderNumber: string;\n  customerName: string;\n  orderDate: string;\n  items: Array<{\n    name: string;\n    quantity: number;\n    price: number;\n  }>;\n  subtotal: number;\n  tax: number;\n  shipping: number;\n  total: number;\n  trackingUrl: string;\n}\n\nexport const OrderConfirmation: React.FC<OrderConfirmationProps> = ({\n  orderNumber,\n  customerName,\n  orderDate,\n  items,\n  subtotal,\n  tax,\n  shipping,\n  total,\n  trackingUrl,\n}) => {\n  return (\n    <Html>\n      <Head />\n      <Preview>Order {orderNumber} confirmed</Preview>\n      <Body style={main}>\n        <Container style={container}>\n          {/* Header */}\n          <Section style={header}>\n            <Text style={heading}>Order Confirmation</Text>\n            <Text style={subHeading}>Order #{orderNumber}</Text>\n          </Section>\n\n          {/* Customer Info */}\n          <Section style={section}>\n            <Text style={sectionHeading}>Thank you, {customerName}!</Text>\n            <Text style={paragraph}>\n              Your order has been received and will be processed shortly.\n            </Text>\n            <Text style={paragraph}>\n              <strong>Order Date:</strong> {orderDate}\n            </Text>\n          </Section>\n\n          {/* Order Items */}\n          <Section style={section}>\n            <Text style={sectionHeading}>Order Items</Text>\n            <table style={table}>\n              <tbody>\n                {items.map((item, idx) => (\n                  <tr key={idx}>\n                    <td style={tableCell}>{item.name}</td>\n                    <td style={{ ...tableCell, textAlign: 'center' }}>{item.quantity}</td>\n                    <td style={{ ...tableCell, textAlign: 'right' }}>\n                      ${(item.price * item.quantity).toFixed(2)}\n                    </td>\n                  </tr>\n                ))}\n                <tr>\n                  <td colSpan={2} style={tableCellRight}>\n                    <strong>Subtotal:</strong>\n                  </td>\n                  <td style={tableCellRight}>${subtotal.toFixed(2)}</td>\n                </tr>\n                <tr>\n                  <td colSpan={2} style={tableCellRight}>\n                    <strong>Tax:</strong>\n                  </td>\n                  <td style={tableCellRight}>${tax.toFixed(2)}</td>\n                </tr>\n                <tr>\n                  <td colSpan={2} style={tableCellRight}>\n                    <strong>Shipping:</strong>\n                  </td>\n                  <td style={tableCellRight}>${shipping.toFixed(2)}</td>\n                </tr>\n                <tr style={totalRow}>\n                  <td colSpan={2} style={{ ...tableCellRight, color: '#ffffff' }}>\n                    <strong>Total:</strong>\n                  </td>\n                  <td style={{ ...tableCellRight, color: '#ffffff' }}>\n                    <strong>${total.toFixed(2)}</strong>\n                  </td>\n                </tr>\n              </tbody>\n            </table>\n          </Section>\n\n          {/* Tracking */}\n          <Section style={section}>\n            <Button style={button} href={trackingUrl}>\n              Track Your Order\n            </Button>\n          </Section>\n\n          {/* Footer */}\n          <Section style={footer}>\n            <Text style={footerText}>\n              Questions? Contact us at support@example.com\n            </Text>\n          </Section>\n        </Container>\n      </Body>\n    </Html>\n  );\n};\n\n// Styles\nconst main = {\n  backgroundColor: '#f6f9fc',\n  fontFamily: '-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif',\n};\n\nconst container = {\n  backgroundColor: '#ffffff',\n  margin: '0 auto',\n  padding: '20px 0 48px',\n  marginBottom: '64px',\n};\n\nconst header = {\n  padding: '20px 24px',\n  backgroundColor: '#f3f4f6',\n  borderBottom: '1px solid #e5e7eb',\n};\n\nconst heading = {\n  fontSize: '24px',\n  fontWeight: '700',\n  color: '#1f2937',\n  margin: '0 0 4px 0',\n};\n\nconst subHeading = {\n  fontSize: '14px',\n  color: '#6b7280',\n  margin: '0',\n};\n\nconst section = {\n  padding: '24px',\n  borderBottom: '1px solid #e5e7eb',\n};\n\nconst sectionHeading = {\n  fontSize: '16px',\n  fontWeight: '600',\n  color: '#1f2937',\n  marginBottom: '12px',\n};\n\nconst paragraph = {\n  fontSize: '14px',\n  lineHeight: '1.5',\n  color: '#525252',\n  margin: '0 0 8px 0',\n};\n\nconst table = {\n  width: '100%',\n  borderCollapse: 'collapse' as const,\n};\n\nconst tableCell = {\n  padding: '12px',\n  borderBottom: '1px solid #e5e7eb',\n  fontSize: '14px',\n  color: '#525252',\n};\n\nconst tableCellRight = {\n  ...tableCell,\n  textAlign: 'right' as const,\n};\n\nconst totalRow = {\n  backgroundColor: '#2563eb',\n};\n\nconst button = {\n  backgroundColor: '#2563eb',\n  borderRadius: '4px',\n  color: '#ffffff',\n  fontSize: '15px',\n  fontWeight: '600',\n  padding: '12px 20px',\n  textDecoration: 'none',\n  textAlign: 'center' as const,\n  display: 'block' as const,\n  width: 'fit-content',\n};\n\nconst footer = {\n  padding: '24px',\n  textAlign: 'center' as const,\n};\n\nconst footerText = {\n  fontSize: '12px',\n  color: '#6b7280',\n};\n```\n\n### 4. Password Reset Email\n\n**Secure password reset flow with time-limited link:**\n\n```typescript\nimport { Body, Button, Container, Head, Html, Preview, Section, Text } from '@react-email/components';\n\ninterface PasswordResetProps {\n  userName: string;\n  resetUrl: string;\n  expiresIn: string; // e.g., \"24 hours\"\n}\n\nexport const PasswordReset: React.FC<PasswordResetProps> = ({\n  userName,\n  resetUrl,\n  expiresIn,\n}) => {\n  return (\n    <Html>\n      <Head />\n      <Preview>Reset your password</Preview>\n      <Body style={main}>\n        <Container style={container}>\n          <Section style={content}>\n            <Text style={heading}>Password Reset Request</Text>\n            <Text style={paragraph}>\n              Hi {userName},\n            </Text>\n            <Text style={paragraph}>\n              We received a request to reset the password associated with your account. Click the button below to reset it.\n            </Text>\n\n            <Section style={buttonContainer}>\n              <Button style={button} href={resetUrl}>\n                Reset Password\n              </Button>\n            </Section>\n\n            <Text style={warningText}>\n              This link expires in {expiresIn}. If you didn't request this, you can ignore this email.\n            </Text>\n\n            <Text style={paragraph}>\n              For security reasons, never share this link with anyone.\n            </Text>\n          </Section>\n        </Container>\n      </Body>\n    </Html>\n  );\n};\n\nconst main = {\n  backgroundColor: '#f6f9fc',\n  fontFamily: '-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif',\n};\n\nconst container = {\n  backgroundColor: '#ffffff',\n  margin: '0 auto',\n  padding: '40px 20px',\n};\n\nconst content = {\n  padding: '20px',\n};\n\nconst heading = {\n  fontSize: '24px',\n  fontWeight: '700',\n  color: '#1f2937',\n  marginBottom: '20px',\n};\n\nconst paragraph = {\n  fontSize: '15px',\n  lineHeight: '1.6',\n  color: '#525252',\n  marginBottom: '16px',\n};\n\nconst buttonContainer = {\n  paddingTop: '20px',\n  paddingBottom: '20px',\n};\n\nconst button = {\n  backgroundColor: '#dc2626',\n  borderRadius: '4px',\n  color: '#ffffff',\n  fontSize: '15px',\n  fontWeight: '600',\n  padding: '12px 24px',\n  textDecoration: 'none',\n};\n\nconst warningText = {\n  fontSize: '13px',\n  color: '#d97706',\n  fontStyle: 'italic',\n  marginBottom: '16px',\n};\n```\n\n### 5. Marketing Email - Newsletter\n\n**Newsletter template with featured content, sections, and unsubscribe:**\n\n```typescript\nimport {\n  Body,\n  Column,\n  Container,\n  Head,\n  Html,\n  Img,\n  Link,\n  Preview,\n  Row,\n  Section,\n  Text,\n} from '@react-email/components';\n\ninterface NewsletterProps {\n  recipientName: string;\n  featuredTitle: string;\n  featuredImage: string;\n  featuredUrl: string;\n  articles: Array<{\n    title: string;\n    excerpt: string;\n    url: string;\n    image?: string;\n  }>;\n  unsubscribeUrl: string;\n}\n\nexport const Newsletter: React.FC<NewsletterProps> = ({\n  recipientName,\n  featuredTitle,\n  featuredImage,\n  featuredUrl,\n  articles,\n  unsubscribeUrl,\n}) => {\n  return (\n    <Html>\n      <Head />\n      <Preview>Latest from our community</Preview>\n      <Body style={main}>\n        <Container style={container}>\n          {/* Header */}\n          <Section style={header}>\n            <Text style={headerTitle}>Our Weekly Newsletter</Text>\n          </Section>\n\n          {/* Greeting */}\n          <Section style={content}>\n            <Text style={greeting}>Hi {recipientName},</Text>\n            <Text style={paragraph}>\n              Here's what's happening in our community this week.\n            </Text>\n          </Section>\n\n          {/* Featured Article */}\n          <Section style={featured}>\n            <Img src={featuredImage} width=\"100%\" height=\"auto\" alt={featuredTitle} />\n            <Section style={featuredContent}>\n              <Text style={featuredTitle}>{featuredTitle}</Text>\n              <Link href={featuredUrl} style={link}>\n                Read More ‚Üí\n              </Link>\n            </Section>\n          </Section>\n\n          {/* Articles Grid */}\n          <Section style={articlesSection}>\n            {articles.map((article, idx) => (\n              <Row key={idx}>\n                <Column style={articleColumn}>\n                  {article.image && (\n                    <Img src={article.image} width=\"100%\" height=\"auto\" alt={article.title} />\n                  )}\n                  <Text style={articleTitle}>{article.title}</Text>\n                  <Text style={articleExcerpt}>{article.excerpt}</Text>\n                  <Link href={article.url} style={link}>\n                    Read More ‚Üí\n                  </Link>\n                </Column>\n              </Row>\n            ))}\n          </Section>\n\n          {/* Footer */}\n          <Section style={footer}>\n            <Text style={footerText}>\n              You're receiving this because you're subscribed to our newsletter.\n            </Text>\n            <Text style={footerText}>\n              <Link href={unsubscribeUrl} style={link}>\n                Unsubscribe\n              </Link>\n            </Text>\n          </Section>\n        </Container>\n      </Body>\n    </Html>\n  );\n};\n\nconst main = {\n  backgroundColor: '#f9fafb',\n  fontFamily: '-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif',\n};\n\nconst container = {\n  backgroundColor: '#ffffff',\n  margin: '0 auto',\n  padding: '20px 0',\n  marginBottom: '64px',\n};\n\nconst header = {\n  backgroundColor: '#1f2937',\n  color: '#ffffff',\n  padding: '32px 20px',\n  textAlign: 'center' as const,\n};\n\nconst headerTitle = {\n  fontSize: '28px',\n  fontWeight: '700',\n  margin: '0',\n  color: '#ffffff',\n};\n\nconst content = {\n  padding: '20px',\n};\n\nconst greeting = {\n  fontSize: '18px',\n  fontWeight: '600',\n  color: '#1f2937',\n  marginBottom: '12px',\n};\n\nconst paragraph = {\n  fontSize: '15px',\n  lineHeight: '1.6',\n  color: '#525252',\n};\n\nconst featured = {\n  margin: '20px 0',\n};\n\nconst featuredContent = {\n  padding: '20px',\n  backgroundColor: '#f3f4f6',\n};\n\nconst featuredTitle = {\n  fontSize: '20px',\n  fontWeight: '700',\n  color: '#1f2937',\n  marginBottom: '12px',\n};\n\nconst articlesSection = {\n  padding: '20px',\n};\n\nconst articleColumn = {\n  padding: '12px',\n  borderRight: '1px solid #e5e7eb',\n};\n\nconst articleTitle = {\n  fontSize: '16px',\n  fontWeight: '600',\n  color: '#1f2937',\n  marginTop: '12px',\n  marginBottom: '8px',\n};\n\nconst articleExcerpt = {\n  fontSize: '14px',\n  lineHeight: '1.5',\n  color: '#6b7280',\n  marginBottom: '12px',\n};\n\nconst link = {\n  color: '#2563eb',\n  textDecoration: 'none',\n  fontWeight: '600',\n};\n\nconst footer = {\n  backgroundColor: '#f9fafb',\n  padding: '24px 20px',\n  textAlign: 'center' as const,\n  borderTop: '1px solid #e5e7eb',\n};\n\nconst footerText = {\n  fontSize: '12px',\n  color: '#6b7280',\n  margin: '0 0 8px 0',\n};\n```\n\n## Preview Server Setup\n\n### Development Server\n\nCreate a preview server for testing email templates:\n\n```typescript\n// emails/preview.tsx\nimport { render } from 'react-email';\nimport { WelcomeEmail } from './welcome';\n\nasync function main() {\n  const emailHtml = render(\n    <WelcomeEmail\n      userName=\"John Doe\"\n      userEmail=\"john@example.com\"\n      activationUrl=\"https://example.com/activate/abc123\"\n      companyName=\"Example Co\"\n    />,\n  );\n\n  console.log(emailHtml);\n}\n\nmain().catch(console.error);\n```\n\n### Resend Integration\n\n```typescript\n// Send email with React Email and Resend\nimport { render } from 'react-email';\nimport { Resend } from 'resend';\nimport { WelcomeEmail } from './emails/welcome';\n\nconst resend = new Resend('your_resend_key_here');\n\nasync function sendWelcomeEmail(userName: string, userEmail: string) {\n  const html = render(\n    <WelcomeEmail\n      userName={userName}\n      userEmail={userEmail}\n      activationUrl={`https://app.example.com/activate/${generateToken()}`}\n      companyName=\"Your Company\"\n    />,\n  );\n\n  const { data, error } = await resend.emails.send({\n    from: 'onboarding@example.com',\n    to: userEmail,\n    subject: `Welcome to Your Company, ${userName}!`,\n    html: html,\n  });\n\n  if (error) {\n    console.error('Failed to send email:', error);\n    return null;\n  }\n\n  return data;\n}\n```\n\n## Responsive Design Patterns\n\n### Mobile-First Approach\n\n```typescript\nimport { Container, Section, Column, Row } from '@react-email/components';\n\n// Responsive container\nconst ResponsiveSection = () => (\n  <Section>\n    <Row>\n      <Column style={{ width: '100%', maxWidth: '600px' }}>\n        {/* Content scales on mobile */}\n      </Column>\n    </Row>\n  </Section>\n);\n\n// Key CSS properties for email-safe styling\nconst responsiveStyles = {\n  // Use pixel values or percentages\n  width: '100%',\n  maxWidth: '600px',\n\n  // Safe font stacks\n  fontFamily: '-apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif',\n\n  // Avoid absolute positioning\n  margin: '0 auto',\n  padding: '20px',\n\n  // Safe colors (avoid rgba for older clients)\n  backgroundColor: '#ffffff',\n  color: '#1f2937',\n};\n```\n\n## Best Practices\n\n### Component Organization\n\n```typescript\n// emails/components/Header.tsx\nexport const Header: React.FC<{ title: string }> = ({ title }) => (\n  <Section style={headerStyle}>\n    <Text style={heading}>{title}</Text>\n  </Section>\n);\n\n// emails/components/Footer.tsx\nexport const Footer: React.FC = () => (\n  <Section style={footerStyle}>\n    <Text style={footerText}>¬© 2024 Company. All rights reserved.</Text>\n  </Section>\n);\n\n// emails/welcome.tsx\nimport { Header } from './components/Header';\nimport { Footer } from './components/Footer';\n\nexport const WelcomeEmail = () => (\n  <Html>\n    <Body>\n      <Container>\n        <Header title=\"Welcome!\" />\n        {/* Content */}\n        <Footer />\n      </Container>\n    </Body>\n  </Html>\n);\n```\n\n### Email Testing\n\n- Test across major email clients: Gmail, Outlook, Apple Mail\n- Use Resend's preview feature to verify rendering\n- Always provide text fallback for images\n- Test on mobile and desktop viewports\n- Validate links and button functionality\n- Check for accessibility (alt text, contrast ratios)\n\n### Performance Tips\n\n- Minimize inline CSS\n- Use safe color values (#hex)\n- Optimize images (JPEG for photos, PNG for graphics)\n- Keep emails under 25MB total\n- Use base64 only for small images\n- Avoid JavaScript (emails don't support it)\n\n### Security Best Practices\n\n- Never embed secrets in email HTML\n- Validate all dynamic content\n- Use HTTPS for all links\n- Implement authentication for preview URLs\n- Sanitize user-generated content\n- Use environment variables for API keys\n\n## Examples Directory Structure\n\n- `welcome-email/` - Complete welcome sequence templates\n- `transactional/` - Order confirmation, password reset, invoices\n- `marketing/` - Newsletters, promotional campaigns, digests\n\nSee individual example README files for complete implementation details.\n\n## Related Skills\n\n- **email-delivery** - Sending emails with Resend API\n- **email-validation** - Email address and template validation\n- **email-webhooks** - Delivery events and bounce handling\n\n## Resources\n\n- [React Email Documentation](https://react.email/)\n- [React Email Components](https://react.email/docs/components/intro)\n- [Resend React Email Integration](https://resend.com/docs/integrations/react-email)\n- [Email Client CSS Support](https://www.campaignmonitor.com/css/)\n- [MJML Email Framework](https://mjml.io/) - Alternative template approach\n\n## Security Notes\n\n- API keys must be stored in environment variables (never hardcoded)\n- Use `RESEND_API_KEY` from secure secret management\n- Sanitize all user input before rendering in emails\n- Validate preview URLs to prevent unauthorized access\n- Log email rendering errors for debugging\n- Test HTML sanitization for email content"
              },
              {
                "name": "webhook-handlers",
                "description": "Webhook event handling patterns for email tracking (sent, delivered, bounced, opened, clicked). Use when implementing email event webhooks, signature verification, processing delivery events, logging email analytics, or building real-time email status tracking.",
                "path": "plugins/resend/skills/webhook-handlers/SKILL.md",
                "frontmatter": {
                  "name": "webhook-handlers",
                  "description": "Webhook event handling patterns for email tracking (sent, delivered, bounced, opened, clicked). Use when implementing email event webhooks, signature verification, processing delivery events, logging email analytics, or building real-time email status tracking.",
                  "allowed-tools": "Read, Write, Bash, Grep"
                },
                "content": "# Webhook Handlers Skill\n\nComprehensive patterns and templates for implementing secure webhook handlers with Resend, covering event types, signature verification, and event processing strategies.\n\n## Use When\n\n- Implementing webhook endpoints for email events (sent, delivered, bounced, opened, clicked)\n- Setting up signature verification for webhook authenticity\n- Building email tracking and analytics systems\n- Processing bounce and complaint events for list management\n- Creating real-time email status dashboards\n- Logging delivery events to database\n- Implementing retry logic for webhook processing\n- Handling multiple webhook events in parallel\n\n## Webhook Event Types\n\n### Resend Webhook Events\n\nResend sends webhooks for the following email events:\n\n#### 1. Email Sent\n\nTriggered when email is accepted by Resend.\n\n```json\n{\n  \"type\": \"email.sent\",\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"recipient@example.com\",\n    \"subject\": \"Welcome to Example\",\n    \"created_at\": \"2024-01-15T10:30:00Z\"\n  }\n}\n```\n\n#### 2. Email Delivered\n\nTriggered when email reaches recipient's mail server.\n\n```json\n{\n  \"type\": \"email.delivered\",\n  \"created_at\": \"2024-01-15T10:35:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"recipient@example.com\",\n    \"created_at\": \"2024-01-15T10:35:00Z\"\n  }\n}\n```\n\n#### 3. Email Bounced\n\nTriggered when email cannot be delivered (hard bounce).\n\n```json\n{\n  \"type\": \"email.bounced\",\n  \"created_at\": \"2024-01-15T10:40:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"invalid@example.com\",\n    \"reason\": \"Mailbox does not exist\",\n    \"created_at\": \"2024-01-15T10:40:00Z\"\n  }\n}\n```\n\n#### 4. Email Opened\n\nTriggered when recipient opens the email (requires pixel tracking).\n\n```json\n{\n  \"type\": \"email.opened\",\n  \"created_at\": \"2024-01-15T11:00:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"recipient@example.com\",\n    \"user_agent\": \"Mozilla/5.0...\",\n    \"ip_address\": \"192.168.1.1\",\n    \"created_at\": \"2024-01-15T11:00:00Z\"\n  }\n}\n```\n\n#### 5. Email Clicked\n\nTriggered when recipient clicks a link in the email.\n\n```json\n{\n  \"type\": \"email.clicked\",\n  \"created_at\": \"2024-01-15T11:05:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"recipient@example.com\",\n    \"link\": \"https://example.com/promo\",\n    \"user_agent\": \"Mozilla/5.0...\",\n    \"ip_address\": \"192.168.1.1\",\n    \"created_at\": \"2024-01-15T11:05:00Z\"\n  }\n}\n```\n\n#### 6. Email Complained\n\nTriggered when recipient marks email as spam.\n\n```json\n{\n  \"type\": \"email.complained\",\n  \"created_at\": \"2024-01-15T11:10:00Z\",\n  \"data\": {\n    \"email_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"from\": \"notifications@example.com\",\n    \"to\": \"recipient@example.com\",\n    \"created_at\": \"2024-01-15T11:10:00Z\"\n  }\n}\n```\n\n## Signature Verification Patterns\n\n### TypeScript Signature Verification\n\nVerify webhook authenticity using HMAC-SHA256:\n\n```typescript\nimport crypto from 'crypto';\n\ninterface WebhookEvent {\n  type: string;\n  created_at: string;\n  data: Record<string, any>;\n}\n\nfunction verifyWebhookSignature(\n  payload: string,\n  signature: string,\n  signingSecret: string\n): boolean {\n  const expectedSignature = crypto\n    .createHmac('sha256', signingSecret)\n    .update(payload)\n    .digest('hex');\n\n  return crypto.timingSafeEqual(\n    Buffer.from(signature),\n    Buffer.from(expectedSignature)\n  );\n}\n\n// Usage in Express middleware\nimport express from 'express';\n\nconst webhookRouter = express.Router();\n\nwebhookRouter.post('/webhooks/resend', express.raw({ type: 'application/json' }), (req, res) => {\n  const signature = req.headers['x-resend-signature'] as string;\n  const payload = req.body.toString();\n\n  if (!verifyWebhookSignature(payload, signature, process.env.RESEND_WEBHOOK_SECRET!)) {\n    return res.status(401).json({ error: 'Invalid signature' });\n  }\n\n  const event: WebhookEvent = JSON.parse(payload);\n  handleWebhookEvent(event);\n\n  res.json({ success: true });\n});\n\nexport default webhookRouter;\n```\n\n### Python Signature Verification\n\n```python\nimport hmac\nimport hashlib\nimport json\nfrom typing import Tuple\n\ndef verify_webhook_signature(\n    payload: str,\n    signature: str,\n    signing_secret: str\n) -> bool:\n    \"\"\"Verify Resend webhook signature using HMAC-SHA256.\"\"\"\n    expected_signature = hmac.new(\n        signing_secret.encode(),\n        payload.encode(),\n        hashlib.sha256\n    ).hexdigest()\n\n    return hmac.compare_digest(signature, expected_signature)\n\n\ndef get_signature_from_headers(headers: dict) -> str:\n    \"\"\"Extract signature from request headers.\"\"\"\n    return headers.get('x-resend-signature', '')\n```\n\n## Event Processing Strategies\n\n### TypeScript Event Handler\n\n```typescript\ninterface EmailEvent {\n  type: 'sent' | 'delivered' | 'bounced' | 'opened' | 'clicked' | 'complained';\n  created_at: string;\n  data: {\n    email_id: string;\n    from: string;\n    to: string;\n    [key: string]: any;\n  };\n}\n\nasync function handleWebhookEvent(event: EmailEvent): Promise<void> {\n  try {\n    // Log the event\n    console.log(`Processing webhook: ${event.type}`, {\n      email_id: event.data.email_id,\n      timestamp: event.created_at,\n    });\n\n    // Route to specific handler\n    switch (event.type) {\n      case 'email.sent':\n        await handleEmailSent(event.data);\n        break;\n\n      case 'email.delivered':\n        await handleEmailDelivered(event.data);\n        break;\n\n      case 'email.bounced':\n        await handleEmailBounced(event.data);\n        break;\n\n      case 'email.opened':\n        await handleEmailOpened(event.data);\n        break;\n\n      case 'email.clicked':\n        await handleEmailClicked(event.data);\n        break;\n\n      case 'email.complained':\n        await handleEmailComplained(event.data);\n        break;\n\n      default:\n        console.warn(`Unknown event type: ${event.type}`);\n    }\n  } catch (error) {\n    console.error('Error handling webhook event:', error);\n    throw error;\n  }\n}\n\n// Individual event handlers\nasync function handleEmailSent(data: any): Promise<void> {\n  // Update email status in database\n  await db.emails.update(\n    { id: data.email_id },\n    {\n      status: 'sent',\n      sent_at: new Date(data.created_at),\n      updated_at: new Date(),\n    }\n  );\n}\n\nasync function handleEmailDelivered(data: any): Promise<void> {\n  await db.emails.update(\n    { id: data.email_id },\n    {\n      status: 'delivered',\n      delivered_at: new Date(data.created_at),\n      updated_at: new Date(),\n    }\n  );\n}\n\nasync function handleEmailBounced(data: any): Promise<void> {\n  // Update status and mark recipient as invalid\n  await db.emails.update(\n    { id: data.email_id },\n    {\n      status: 'bounced',\n      bounce_reason: data.reason,\n      bounced_at: new Date(data.created_at),\n      updated_at: new Date(),\n    }\n  );\n\n  // Add to bounce list\n  await db.bounced_emails.create({\n    email: data.to,\n    reason: data.reason,\n    bounced_at: new Date(data.created_at),\n  });\n}\n\nasync function handleEmailOpened(data: any): Promise<void> {\n  await db.email_events.create({\n    email_id: data.email_id,\n    event_type: 'opened',\n    user_agent: data.user_agent,\n    ip_address: data.ip_address,\n    created_at: new Date(data.created_at),\n  });\n}\n\nasync function handleEmailClicked(data: any): Promise<void> {\n  await db.email_events.create({\n    email_id: data.email_id,\n    event_type: 'clicked',\n    link: data.link,\n    user_agent: data.user_agent,\n    ip_address: data.ip_address,\n    created_at: new Date(data.created_at),\n  });\n}\n\nasync function handleEmailComplained(data: any): Promise<void> {\n  await db.emails.update(\n    { id: data.email_id },\n    {\n      status: 'complained',\n      complained_at: new Date(data.created_at),\n      updated_at: new Date(),\n    }\n  );\n\n  // Add to suppression list\n  await db.suppressed_emails.create({\n    email: data.to,\n    reason: 'complaint',\n    created_at: new Date(data.created_at),\n  });\n}\n```\n\n### Python Event Handler\n\n```python\nfrom enum import Enum\nfrom datetime import datetime\nfrom typing import Any, Dict\n\nclass EventType(Enum):\n    SENT = \"email.sent\"\n    DELIVERED = \"email.delivered\"\n    BOUNCED = \"email.bounced\"\n    OPENED = \"email.opened\"\n    CLICKED = \"email.clicked\"\n    COMPLAINED = \"email.complained\"\n\n\nasync def handle_webhook_event(event: Dict[str, Any]) -> None:\n    \"\"\"Route webhook events to appropriate handlers.\"\"\"\n    event_type = event.get('type')\n    event_data = event.get('data', {})\n\n    handlers = {\n        EventType.SENT.value: handle_email_sent,\n        EventType.DELIVERED.value: handle_email_delivered,\n        EventType.BOUNCED.value: handle_email_bounced,\n        EventType.OPENED.value: handle_email_opened,\n        EventType.CLICKED.value: handle_email_clicked,\n        EventType.COMPLAINED.value: handle_email_complained,\n    }\n\n    handler = handlers.get(event_type)\n    if handler:\n        await handler(event_data)\n    else:\n        print(f\"Unknown event type: {event_type}\")\n\n\nasync def handle_email_sent(data: Dict[str, Any]) -> None:\n    \"\"\"Update email status to sent.\"\"\"\n    await db.emails.update(\n        {\"id\": data[\"email_id\"]},\n        {\n            \"status\": \"sent\",\n            \"sent_at\": datetime.fromisoformat(data[\"created_at\"]),\n        }\n    )\n\n\nasync def handle_email_delivered(data: Dict[str, Any]) -> None:\n    \"\"\"Update email status to delivered.\"\"\"\n    await db.emails.update(\n        {\"id\": data[\"email_id\"]},\n        {\n            \"status\": \"delivered\",\n            \"delivered_at\": datetime.fromisoformat(data[\"created_at\"]),\n        }\n    )\n\n\nasync def handle_email_bounced(data: Dict[str, Any]) -> None:\n    \"\"\"Handle bounce event and add to suppression list.\"\"\"\n    await db.emails.update(\n        {\"id\": data[\"email_id\"]},\n        {\n            \"status\": \"bounced\",\n            \"bounce_reason\": data.get(\"reason\"),\n        }\n    )\n\n    await db.bounced_emails.create({\n        \"email\": data[\"to\"],\n        \"reason\": data.get(\"reason\"),\n        \"bounced_at\": datetime.fromisoformat(data[\"created_at\"]),\n    })\n\n\nasync def handle_email_opened(data: Dict[str, Any]) -> None:\n    \"\"\"Log email open event.\"\"\"\n    await db.email_events.create({\n        \"email_id\": data[\"email_id\"],\n        \"event_type\": \"opened\",\n        \"user_agent\": data.get(\"user_agent\"),\n        \"ip_address\": data.get(\"ip_address\"),\n        \"created_at\": datetime.fromisoformat(data[\"created_at\"]),\n    })\n\n\nasync def handle_email_clicked(data: Dict[str, Any]) -> None:\n    \"\"\"Log email click event.\"\"\"\n    await db.email_events.create({\n        \"email_id\": data[\"email_id\"],\n        \"event_type\": \"clicked\",\n        \"link\": data.get(\"link\"),\n        \"user_agent\": data.get(\"user_agent\"),\n        \"ip_address\": data.get(\"ip_address\"),\n        \"created_at\": datetime.fromisoformat(data[\"created_at\"]),\n    })\n\n\nasync def handle_email_complained(data: Dict[str, Any]) -> None:\n    \"\"\"Handle complaint event and add to suppression list.\"\"\"\n    await db.emails.update(\n        {\"id\": data[\"email_id\"]},\n        {\n            \"status\": \"complained\",\n        }\n    )\n\n    await db.suppressed_emails.create({\n        \"email\": data[\"to\"],\n        \"reason\": \"complaint\",\n        \"created_at\": datetime.fromisoformat(data[\"created_at\"]),\n    })\n```\n\n## Database Logging Patterns\n\n### TypeScript Database Schema\n\n```typescript\n// Prisma schema example\n\nmodel Email {\n  id                String    @id @default(uuid())\n  resend_id         String    @unique\n  from              String\n  to                String\n  subject           String\n  status            String    @default(\"sent\") // sent, delivered, bounced, opened, complained\n  sent_at           DateTime?\n  delivered_at      DateTime?\n  bounced_at        DateTime?\n  complained_at     DateTime?\n  bounce_reason     String?\n  created_at        DateTime  @default(now())\n  updated_at        DateTime  @updatedAt\n  events            EmailEvent[]\n\n  @@index([status])\n  @@index([to])\n  @@index([created_at])\n}\n\nmodel EmailEvent {\n  id          String    @id @default(uuid())\n  email_id    String\n  email       Email     @relation(fields: [email_id], references: [id], onDelete: Cascade)\n  event_type  String    // opened, clicked\n  link        String?\n  user_agent  String?\n  ip_address  String?\n  created_at  DateTime  @default(now())\n\n  @@index([email_id])\n  @@index([event_type])\n  @@index([created_at])\n}\n\nmodel BouncedEmail {\n  id          String    @id @default(uuid())\n  email       String    @unique\n  reason      String\n  bounced_at  DateTime\n  created_at  DateTime  @default(now())\n\n  @@index([email])\n}\n\nmodel SuppressedEmail {\n  id          String    @id @default(uuid())\n  email       String    @unique\n  reason      String    // complaint, bounce, unsubscribe\n  created_at  DateTime  @default(now())\n\n  @@index([email])\n}\n```\n\n### PostgreSQL Schema\n\n```sql\nCREATE TABLE emails (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  resend_id VARCHAR UNIQUE NOT NULL,\n  from_address VARCHAR NOT NULL,\n  to_address VARCHAR NOT NULL,\n  subject TEXT NOT NULL,\n  status VARCHAR DEFAULT 'sent',\n  sent_at TIMESTAMP,\n  delivered_at TIMESTAMP,\n  bounced_at TIMESTAMP,\n  complained_at TIMESTAMP,\n  bounce_reason TEXT,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE email_events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email_id UUID NOT NULL REFERENCES emails(id) ON DELETE CASCADE,\n  event_type VARCHAR NOT NULL,\n  link TEXT,\n  user_agent TEXT,\n  ip_address INET,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE bounced_emails (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR UNIQUE NOT NULL,\n  reason TEXT,\n  bounced_at TIMESTAMP NOT NULL,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE suppressed_emails (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR UNIQUE NOT NULL,\n  reason VARCHAR NOT NULL,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_emails_status ON emails(status);\nCREATE INDEX idx_emails_to ON emails(to_address);\nCREATE INDEX idx_emails_created ON emails(created_at);\nCREATE INDEX idx_email_events_email_id ON email_events(email_id);\nCREATE INDEX idx_email_events_type ON email_events(event_type);\nCREATE INDEX idx_bounced_emails_email ON bounced_emails(email);\nCREATE INDEX idx_suppressed_emails_email ON suppressed_emails(email);\n```\n\n## Webhook Setup\n\n### Setting Webhook URL in Resend\n\n```typescript\nimport { Resend } from 'resend';\n\nconst resend = new Resend(process.env.RESEND_API_KEY);\n\nasync function setupWebhook() {\n  const response = await resend.webhooks.create({\n    events: [\n      'email.sent',\n      'email.delivered',\n      'email.bounced',\n      'email.opened',\n      'email.clicked',\n      'email.complained',\n    ],\n    url: 'https://your-domain.com/api/webhooks/resend',\n  });\n\n  console.log('Webhook created:', response.data);\n  // Save the webhook ID and signing secret securely\n}\n```\n\n### Retrieving Webhook Details\n\n```typescript\nasync function getWebhookDetails(webhookId: string) {\n  const response = await resend.webhooks.get(webhookId);\n  return response.data;\n}\n\nasync function listWebhooks() {\n  const response = await resend.webhooks.list();\n  return response.data;\n}\n```\n\n## Environment Variables Required\n\n```bash\nRESEND_API_KEY=your_resend_key_here\nRESEND_WEBHOOK_SECRET=your_webhook_signing_secret_here\nDATABASE_URL=your_database_connection_string\n```\n\n## Error Handling and Retries\n\n### Webhook Retry Pattern\n\n```typescript\ninterface WebhookTask {\n  id: string;\n  event: WebhookEvent;\n  retries: number;\n  max_retries: number;\n  next_retry_at: Date;\n}\n\nasync function processWebhookWithRetry(\n  event: WebhookEvent,\n  maxRetries: number = 3\n): Promise<void> {\n  let lastError: Error | null = null;\n\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      await handleWebhookEvent(event);\n      console.log(`Webhook processed successfully: ${event.data.email_id}`);\n      return;\n    } catch (error) {\n      lastError = error as Error;\n      console.error(`Attempt ${attempt} failed:`, error);\n\n      if (attempt < maxRetries) {\n        // Exponential backoff: 5s, 25s, 125s\n        const delay = Math.pow(5, attempt) * 1000;\n        await new Promise(resolve => setTimeout(resolve, delay));\n      }\n    }\n  }\n\n  // Store failed event for manual review\n  await db.failed_webhooks.create({\n    event_id: event.data.email_id,\n    event_type: event.type,\n    payload: JSON.stringify(event),\n    error: lastError?.message,\n    created_at: new Date(),\n  });\n\n  throw lastError;\n}\n```\n\n## Best Practices\n\n- Implement idempotency: Track processed webhook IDs to prevent duplicate processing\n- Use database transactions for atomic updates\n- Log all webhook events for auditing and debugging\n- Implement proper error handling and retry logic\n- Verify webhook signatures on every request\n- Use environment variables for sensitive data (never hardcode)\n- Monitor webhook processing latency\n- Set up alerts for failed webhooks\n- Archive old webhook events for compliance\n\n## Examples Directory Structure\n\n- `nextjs-webhook/` - Next.js API route webhook handler\n- `fastapi-webhook/` - FastAPI webhook handler with FastAPI patterns\n- `event-processing/` - Database logging and event analytics\n\nSee individual example README files for complete code and usage patterns.\n\n## Related Skills\n\n- **email-delivery** - Email sending patterns and batch operations\n- **email-templates** - HTML template management and rendering\n- **email-validation** - Recipient address validation\n\n## Resources\n\n- [Resend Webhooks Documentation](https://resend.com/docs/webhooks)\n- [Webhook Event Reference](https://resend.com/docs/webhooks/events)\n- [Webhook Signature Verification](https://resend.com/docs/webhooks/signature-verification)\n- [Resend API Reference](https://resend.com/docs/api-reference)\n\n## Security Notes\n\n- Webhook signing secrets must be stored in environment variables only\n- Always verify webhook signatures before processing\n- Use HTTPS for webhook endpoints (never HTTP)\n- Implement rate limiting on webhook endpoints\n- Store webhook payloads securely (may contain PII)\n- Use database transactions for atomic operations\n- Never hardcode API keys or secrets\n- Implement request timeouts to prevent hanging\n- Log security-relevant events for compliance"
              }
            ]
          },
          {
            "name": "a2a-protocol",
            "description": "Complete A2A Protocol integration for multi-agent systems",
            "source": "./plugins/a2a-protocol",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Plugin Developer",
              "email": "noreply@a2a-protocol.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install a2a-protocol@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-agent",
                "description": "Add A2A agent to project with agent card and executor",
                "path": "plugins/a2a-protocol/commands/add-agent.md",
                "frontmatter": {
                  "description": "Add A2A agent to project with agent card and executor",
                  "argument-hint": "<agent-name> [description]",
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add a new A2A-compliant agent to the project with proper agent card, executor implementation, and registration.\n\nCore Principles:\n- Understand project structure before acting\n- Follow A2A Protocol specification exactly\n- Generate compliant agent cards with all required fields\n- Create executable agent implementations\n- Register agents properly in project configuration\n\nPhase 1: Discovery\nGoal: Gather requirements and understand project context\n\nActions:\n- Parse $ARGUMENTS to extract agent name and optional description\n- If description is unclear, use AskUserQuestion to gather:\n  - What is the agent's primary purpose?\n  - What tasks should it perform?\n  - What tools or capabilities does it need?\n  - Any specific A2A protocol version requirements?\n- Load project configuration to understand existing structure\n- Example: @package.json or @pyproject.toml\n\nPhase 2: Analysis\nGoal: Understand existing agents and project patterns\n\nActions:\n- Search for existing agent cards: !{bash find . -name \"*agent-card.json\" -o -name \"agents/*.json\" 2>/dev/null | head -5}\n- Read existing agent implementations to understand patterns\n- Identify agent registry or configuration files\n- Check for A2A protocol version in use\n\nPhase 3: Planning\nGoal: Design the agent implementation approach\n\nActions:\n- Confirm agent card structure based on A2A spec version\n- Determine where agent files should be created\n- Identify required fields for agent card\n- Plan executor implementation approach\n- Present plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Create agent with a2a-agent-builder\n\nActions:\n\nTask(description=\"Build A2A agent\", subagent_type=\"a2a-agent-builder\", prompt=\"You are the a2a-agent-builder agent. Create a complete A2A-compliant agent for $ARGUMENTS.\n\nContext:\n- Project structure and existing patterns identified in Phase 2\n- Agent requirements and capabilities from Phase 1\n- A2A protocol version and compliance requirements\n\nRequirements:\n- Generate valid agent card JSON with all required fields\n- Create executor implementation following project patterns\n- Include proper capability declarations\n- Add communication interface definitions\n- Implement error handling and logging\n- Follow A2A Protocol specification exactly\n\nExpected output:\n- Agent card JSON file\n- Executor implementation file\n- Registration in project configuration\n- Usage documentation\")\n\nPhase 5: Review\nGoal: Verify agent implementation\n\nActions:\n- Check that agent card is valid JSON\n- Example: !{bash cat agents/*/agent-card.json | python3 -m json.tool > /dev/null 2>&1 && echo \"Valid JSON\" || echo \"Invalid JSON\"}\n- Verify all required A2A fields are present\n- Confirm executor implementation is complete\n- Run project validation if available\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize changes made:\n  - Agent card location and key capabilities\n  - Executor implementation details\n  - Registration status\n- Provide usage instructions:\n  - How to invoke the agent\n  - Configuration options\n  - Testing recommendations\n- Suggest next steps:\n  - Test agent with sample tasks\n  - Integrate with existing agents\n  - Add to agent directory or catalog"
              },
              {
                "name": "/add-client",
                "description": "Add A2A client to communicate with agents",
                "path": "plugins/a2a-protocol/commands/add-client.md",
                "frontmatter": {
                  "description": "Add A2A client to communicate with agents",
                  "argument-hint": "<client-name> [target-agent-url]",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Configure and add an A2A protocol client to enable communication with remote agents\n\nCore Principles:\n- Detect project structure before generating client code\n- Ask for missing configuration details\n- Follow A2A protocol specification standards\n- Generate type-safe client implementations\n\nPhase 1: Discovery\nGoal: Understand project context and gather requirements\n\nActions:\n- Parse $ARGUMENTS to extract client name and optional target agent URL\n- Detect project type and language\n- Example: !{bash ls package.json pyproject.toml go.mod 2>/dev/null | head -1}\n- Load existing A2A configuration if present\n- Example: @a2a-config.json or @.a2a/config.json\n\nPhase 2: Requirements Gathering\nGoal: Collect necessary configuration details\n\nActions:\n- If target agent URL not provided in $ARGUMENTS, use AskUserQuestion to gather:\n  - What is the target agent's URL or endpoint?\n  - What authentication method should be used? (bearer token, API key, OAuth)\n  - Which A2A protocol version? (default: latest)\n  - Any specific capabilities required?\n- Validate URL format if provided\n- Confirm configuration with user before proceeding\n\nPhase 3: Planning\nGoal: Design the client implementation approach\n\nActions:\n- Determine language-specific client structure\n- Identify configuration files that need creation/update\n- Plan authentication setup\n- Outline integration points in existing codebase\n\nPhase 4: Implementation\nGoal: Generate A2A client code and configuration\n\nActions:\n\nTask(description=\"Build A2A protocol client\", subagent_type=\"a2a-client-builder\", prompt=\"You are the a2a-client-builder agent. Create an A2A protocol client for $ARGUMENTS.\n\nContext: Building client to communicate with remote agents following A2A protocol specification\n\nRequirements:\n- Generate type-safe client code for detected language\n- Implement proper authentication handling\n- Include error handling and retry logic\n- Add configuration file for client settings\n- Follow A2A protocol standards for message formatting\n- Include usage examples and documentation\n\nConfiguration Details:\n- Client name: [from $ARGUMENTS]\n- Target agent URL: [from Phase 2]\n- Authentication: [from Phase 2]\n- Protocol version: [from Phase 2]\n- Project type: [from Phase 1]\n\nExpected output: Complete client implementation with config files and usage documentation\")\n\nPhase 5: Verification\nGoal: Validate the generated client\n\nActions:\n- Check that all client files were created\n- Verify configuration file syntax\n- Run type checking if applicable\n- Example: !{bash npm run typecheck 2>/dev/null || python -m mypy . 2>/dev/null || go vet ./... 2>/dev/null}\n- Test client connection if possible\n\nPhase 6: Summary\nGoal: Report implementation results and next steps\n\nActions:\n- Summarize files created:\n  - Client implementation file(s)\n  - Configuration file\n  - Type definitions (if applicable)\n  - Documentation/examples\n- Show sample usage code\n- Provide next steps:\n  - How to configure authentication credentials\n  - How to test the client connection\n  - Where to find usage examples\n  - Link to A2A protocol documentation"
              },
              {
                "name": "/add-discovery",
                "description": "Add agent discovery mechanisms to project",
                "path": "plugins/a2a-protocol/commands/add-discovery.md",
                "frontmatter": {
                  "description": "Add agent discovery mechanisms to project",
                  "argument-hint": [
                    "discovery-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add A2A Protocol agent discovery mechanisms to enable agents to find and connect with each other in multi-agent systems.\n\nCore Principles:\n- Understand project architecture before implementing discovery\n- Support multiple discovery patterns (static, dynamic, hybrid)\n- Follow A2A Protocol discovery specifications\n- Provide clear examples and documentation\n\nPhase 1: Discovery\nGoal: Understand project context and discovery requirements\n\nActions:\n- Parse $ARGUMENTS to determine discovery type preference\n- If $ARGUMENTS is empty or unclear, use AskUserQuestion to gather:\n  - Which discovery pattern? (static registry, dynamic DNS-SD, hybrid)\n  - What agent metadata should be discoverable? (capabilities, protocols, endpoints)\n  - Any existing service discovery infrastructure?\n- Detect project type and framework\n- Example: @package.json or @pyproject.toml\n- Load existing A2A Protocol configuration if present\n- Example: @a2a-config.json or @.a2a/config.yaml\n\nPhase 2: Analysis\nGoal: Analyze project structure and identify integration points\n\nActions:\n- Search for existing agent implementations\n- Example: !{bash find . -name \"*agent*\" -type f 2>/dev/null | head -10}\n- Identify service registration locations\n- Check for existing discovery mechanisms\n- Understand deployment environment (local, cloud, containerized)\n\nPhase 3: Planning\nGoal: Design discovery implementation approach\n\nActions:\n- Determine discovery pattern based on requirements:\n  - Static Registry: Simple JSON/YAML configuration files\n  - Dynamic DNS-SD: mDNS/Bonjour for local network discovery\n  - Hybrid: Combination of static config + dynamic announcements\n- Identify files to create or modify\n- Plan agent metadata schema\n- Consider security and authentication requirements\n\nPhase 4: Implementation\nGoal: Implement discovery mechanism via specialized agent\n\nActions:\n\nTask(description=\"Implement A2A discovery mechanism\", subagent_type=\"a2a-discovery\", prompt=\"You are the a2a-discovery agent. Implement A2A Protocol agent discovery for $ARGUMENTS.\n\nContext from analysis:\n- Project type and framework detected\n- Existing agent implementations identified\n- Discovery pattern selected based on requirements\n\nRequirements:\n- Implement chosen discovery pattern (static/dynamic/hybrid)\n- Create agent registry with metadata schema\n- Add discovery client for finding other agents\n- Include announcement mechanism for agent availability\n- Provide clear documentation and examples\n- Follow A2A Protocol discovery specifications\n- Ensure backward compatibility with existing code\n\nExpected output:\n- Discovery implementation files (registry, client, announcements)\n- Agent metadata schema definition\n- Configuration files\n- Integration with existing agent code\n- Documentation with usage examples\n- Test cases for discovery functionality\")\n\nPhase 5: Review\nGoal: Verify discovery implementation\n\nActions:\n- Check that discovery files were created\n- Verify agent metadata schema is complete\n- Review discovery client implementation\n- Test discovery mechanism if applicable\n- Example: !{bash npm run test:discovery 2>/dev/null || echo \"No discovery tests configured\"}\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize discovery mechanism implemented:\n  - Discovery pattern used (static/dynamic/hybrid)\n  - Files created (registry, client, config)\n  - Agent metadata schema defined\n  - Integration points updated\n- Highlight key implementation decisions\n- Show example usage of discovery API\n- Suggest next steps:\n  - Test discovery with multiple agents\n  - Add authentication/authorization if needed\n  - Configure discovery for production deployment\n  - Set up monitoring for agent registry"
              },
              {
                "name": "/add-production",
                "description": "Add enterprise features and production configurations for A2A Protocol",
                "path": "plugins/a2a-protocol/commands/add-production.md",
                "frontmatter": {
                  "description": "Add enterprise features and production configurations for A2A Protocol",
                  "argument-hint": [
                    "feature-type"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add production-ready enterprise features to A2A Protocol implementation including rate limiting, circuit breakers, monitoring, observability, and security hardening.\n\nCore Principles:\n- Detect existing project structure before making changes\n- Ask for clarification when feature requirements are ambiguous\n- Follow A2A Protocol specifications and best practices\n- Implement enterprise patterns for reliability and observability\n\nPhase 1: Discovery\nGoal: Understand project context and production feature requirements\n\nActions:\n- Parse $ARGUMENTS to determine if specific feature type requested\n- Detect project type and existing A2A implementation\n- Example: @package.json or @pyproject.toml or @go.mod\n- Load existing A2A configuration files\n- Example: !{bash find . -name \"a2a-config.*\" -o -name \"a2a.config.*\" 2>/dev/null | head -5}\n\nPhase 2: Requirements Gathering\nGoal: Clarify which production features to implement\n\nActions:\n- If $ARGUMENTS specifies feature type (rate-limiting, circuit-breaker, monitoring, etc.), use that\n- Otherwise, use AskUserQuestion to gather:\n  - Which production features are needed? (rate limiting, circuit breakers, monitoring, telemetry, security)\n  - What monitoring/observability tools are being used? (Prometheus, Datadog, Sentry, etc.)\n  - What scale/throughput requirements exist?\n  - Any compliance or security requirements?\n- Summarize requirements and confirm with user\n\nPhase 3: Analysis\nGoal: Understand current A2A implementation and identify integration points\n\nActions:\n- Read existing A2A agent communication code\n- Identify message routing and handler patterns\n- Check for existing middleware or interceptor patterns\n- Analyze current error handling approach\n- Example: !{bash grep -r \"A2AMessage\\|MessageHandler\\|AgentCommunication\" src lib --include=\"*.ts\" --include=\"*.js\" --include=\"*.py\" --include=\"*.go\" | head -20}\n\nPhase 4: Implementation Planning\nGoal: Design the production feature integration approach\n\nActions:\n- Plan integration approach based on existing architecture\n- Identify files to modify and new files to create\n- Design how production features integrate with A2A message flow\n- Present plan to user for approval\n\nPhase 5: Implementation\nGoal: Add production features to A2A Protocol implementation\n\nActions:\n\nTask(description=\"Add production features to A2A Protocol\", subagent_type=\"a2a-production\", prompt=\"You are the a2a-production agent. Add enterprise production features to the A2A Protocol implementation for $ARGUMENTS.\n\nContext from Discovery:\n- Project type and structure identified in Phase 1\n- Feature requirements gathered in Phase 2\n- Current A2A implementation analyzed in Phase 3\n- Integration plan designed in Phase 4\n\nProduction Features to Implement:\n- Rate Limiting: Token bucket or sliding window for message throughput control\n- Circuit Breakers: Fail-fast patterns for agent communication resilience\n- Monitoring: Metrics collection for message volumes, latencies, errors\n- Observability: Distributed tracing for multi-agent message flows\n- Security: Message authentication, encryption, and authorization\n\nRequirements:\n- Follow A2A Protocol specification (RFC format)\n- Integrate seamlessly with existing message handlers\n- Add minimal performance overhead\n- Include comprehensive logging and metrics\n- Provide configuration options for tuning\n- Add health check endpoints\n- Include graceful degradation patterns\n\nExpected output:\n- Production feature implementations integrated with A2A message flow\n- Configuration files with sensible defaults\n- Documentation explaining each feature and how to configure it\n- Example usage showing how to enable/disable features\n- Health check and monitoring endpoints\")\n\nPhase 6: Verification\nGoal: Ensure production features work correctly\n\nActions:\n- Verify configuration files are valid\n- Check that production features integrate properly\n- Run type checking if applicable\n- Example: !{bash npm run typecheck 2>/dev/null || python -m mypy . 2>/dev/null || go vet ./... 2>/dev/null || echo \"No type checking available\"}\n- Test that health endpoints respond correctly\n\nPhase 7: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize production features added\n- List configuration options available\n- Explain how to monitor and tune each feature\n- Provide next steps for production deployment\n- Highlight any security or performance considerations"
              },
              {
                "name": "/add-streaming",
                "description": "Add streaming and async operations support to A2A Protocol implementation",
                "path": "plugins/a2a-protocol/commands/add-streaming.md",
                "frontmatter": {
                  "description": "Add streaming and async operations support to A2A Protocol implementation",
                  "argument-hint": [
                    "feature-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Implement streaming capabilities and async operations for A2A Protocol, enabling real-time message streaming, async task handling, and progressive response delivery.\n\nCore Principles:\n- Detect existing A2A Protocol implementation before modifying\n- Follow A2A Protocol specification for streaming operations\n- Implement both Server-Sent Events (SSE) and WebSocket streaming\n- Ensure backward compatibility with non-streaming operations\n- Provide comprehensive examples and testing utilities\n\nPhase 1: Discovery\nGoal: Understand the current A2A Protocol implementation and streaming requirements\n\nActions:\n- Parse $ARGUMENTS to identify which streaming features to add\n- If unclear, use AskUserQuestion to gather:\n  - What streaming features are needed? (SSE, WebSocket, or both)\n  - Are there existing async operations to enhance?\n  - What use cases require streaming? (chat, progress updates, file transfers)\n  - Should we add async task queue support?\n- Detect project type and framework\n- Example: !{bash ls package.json pyproject.toml go.mod 2>/dev/null}\n- Load existing A2A Protocol configuration if present\n- Example: @a2a-protocol.config.json\n\nPhase 2: Analysis\nGoal: Understand current codebase structure and identify integration points\n\nActions:\n- Find existing A2A Protocol implementation files\n- Example: !{bash find . -name \"*a2a*\" -o -name \"*agent*\" 2>/dev/null | grep -v node_modules | head -20}\n- Identify framework-specific patterns (Express, FastAPI, Next.js, etc.)\n- Read relevant configuration and implementation files\n- Understand current message handling architecture\n- Check for existing streaming or async infrastructure\n\nPhase 3: Planning\nGoal: Design the streaming implementation approach\n\nActions:\n- Outline implementation strategy based on detected framework\n- Identify files that need creation or modification\n- Plan streaming architecture:\n  - SSE endpoint configuration\n  - WebSocket connection handling\n  - Async task queue setup\n  - Message buffering and flow control\n- Determine testing approach\n- Present plan to user and confirm before proceeding\n\nPhase 4: Implementation\nGoal: Add streaming and async capabilities via specialized agent\n\nActions:\n\nTask(description=\"Implement A2A Protocol streaming and async operations\", subagent_type=\"a2a-streaming\", prompt=\"You are the a2a-streaming agent. Implement streaming and async operations support for A2A Protocol based on $ARGUMENTS.\n\nContext: A2A Protocol requires streaming capabilities for real-time agent communication, progress updates, and async task handling.\n\nRequirements:\n- Implement Server-Sent Events (SSE) for unidirectional streaming\n- Add WebSocket support for bidirectional real-time communication\n- Create async task queue for long-running operations\n- Add message chunking and buffering mechanisms\n- Implement flow control and backpressure handling\n- Ensure A2A Protocol compliance for streaming messages\n- Add comprehensive error handling for stream failures\n- Create TypeScript/Python types for streaming operations\n- Follow framework-specific best practices detected in Phase 2\n\nDeliverables:\n- Streaming endpoint implementations (SSE and/or WebSocket)\n- Async task queue infrastructure\n- Message serialization for streaming\n- Client-side streaming utilities\n- Example implementations showing streaming usage\n- Tests validating streaming operations\n- Documentation with integration examples\n\nExpected output: Complete streaming implementation with working examples and tests\")\n\nPhase 5: Verification\nGoal: Validate streaming implementation works correctly\n\nActions:\n- Check that streaming endpoints are properly configured\n- Verify async operations integrate with existing A2A Protocol code\n- Run tests if available\n- Example: !{bash npm test 2>/dev/null || python -m pytest 2>/dev/null || go test ./... 2>/dev/null}\n- Verify framework-specific integration (Next.js API routes, FastAPI endpoints, etc.)\n- Check for proper error handling in stream scenarios\n- Validate A2A Protocol message format compliance\n\nPhase 6: Summary\nGoal: Document what was accomplished and provide next steps\n\nActions:\n- Summarize streaming features added:\n  - SSE endpoint locations and usage\n  - WebSocket configuration and connection handling\n  - Async task queue implementation details\n  - Message buffering and flow control mechanisms\n- List files created or modified\n- Highlight key implementation decisions:\n  - Framework-specific patterns used\n  - Streaming protocol choices (SSE vs WebSocket)\n  - Error handling strategies\n  - Performance optimizations applied\n- Provide usage examples for developers\n- Suggest next steps:\n  - Testing streaming with real agents\n  - Performance tuning and load testing\n  - Adding monitoring and observability\n  - Implementing stream reconnection logic\n  - Scaling considerations for production"
              },
              {
                "name": "/build-full",
                "description": "Intelligent A2A Protocol setup - analyzes your project (project.json, features.json, specs/) and builds complete A2A integration tailored to your stack",
                "path": "plugins/a2a-protocol/commands/build-full.md",
                "frontmatter": {
                  "name": "build-full",
                  "description": "Intelligent A2A Protocol setup - analyzes your project (project.json, features.json, specs/) and builds complete A2A integration tailored to your stack",
                  "allowed-tools": "Task, Read, Write, Bash, Grep, Glob, TodoWrite"
                },
                "content": "# Build Complete A2A Protocol Stack (Intelligent)\n\nThis command intelligently analyzes your entire project and builds the appropriate A2A Protocol integration.\n\n**Execution Flow:**\n1. **Analyze Project** - Read all project files to understand what you're building\n2. **Plan Integration** - Determine which A2A features your project needs\n3. **Build Stack** - Delegate to appropriate commands to implement everything\n\n## Phase 1: Project Analysis\n\n### Step 1: Create Analysis Todo List\n\n```\nTodoWrite([\n  { content: \"Analyze project structure and configuration\", status: \"in_progress\", activeForm: \"Analyzing project structure and configuration\" },\n  { content: \"Plan A2A integration strategy\", status: \"pending\", activeForm: \"Planning A2A integration strategy\" },\n  { content: \"Execute A2A Protocol setup\", status: \"pending\", activeForm: \"Executing A2A Protocol setup\" },\n  { content: \"Validate and summarize\", status: \"pending\", activeForm: \"Validating and summarizing\" }\n])\n```\n\n### Step 2: Read All Project Configuration Files\n\nRead the following files to understand the project:\n\n**Core Configuration:**\n```\nRead(.claude/project.json)\nRead(features.json)  # If exists\nRead(.claude/features.json)  # Alternative location\nRead(package.json)  # If exists\nRead(requirements.txt)  # If exists\nRead(pyproject.toml)  # If exists\nRead(pom.xml)  # If exists\nRead(go.mod)  # If exists\n```\n\n**Architecture Documentation:**\n```\nGlob(pattern=\"docs/architecture/**/*.md\")\nRead all architecture docs found\n\nGlob(pattern=\"docs/specs/**/*.md\")\nRead all spec files found\n\nGlob(pattern=\"specs/**/*.md\")\nRead all spec files (alternative location)\n```\n\n**Feature Specifications:**\n```\nGlob(pattern=\"specs/features/*/spec.md\")\nRead all feature specs\n\nGlob(pattern=\"specs/features/*/tasks.md\")\nRead all feature task lists\n```\n\n**Application Design:**\n```\nRead(.claude/application-design.json)  # If exists\nRead(docs/application-design.json)  # If exists\n```\n\n### Step 3: Analyze Project with Agent\n\n**CRITICAL:** Use the Task tool to delegate analysis to an exploration agent:\n\n```\nTask(\n  description=\"Analyze project for A2A integration\",\n  subagent_type=\"Explore\",\n  prompt=\"Analyze this entire project to determine A2A Protocol integration needs.\n\n**Read and analyze ALL of the following:**\n\n1. **Project Configuration:**\n   - .claude/project.json\n   - features.json or .claude/features.json\n   - package.json / requirements.txt / pyproject.toml / pom.xml / go.mod\n\n2. **Architecture Documentation:**\n   - All files in docs/architecture/\n   - All files in docs/specs/\n   - All files in specs/\n\n3. **Feature Specifications:**\n   - All spec.md files in specs/features/\n   - All tasks.md files in specs/features/\n\n4. **Application Design:**\n   - .claude/application-design.json\n   - docs/application-design.json\n\n**Provide comprehensive analysis including:**\n\n- **Project Type:** (web app, API, microservices, CLI tool, etc.)\n- **Tech Stack:**\n  - Primary language(s)\n  - Frameworks (FastAPI, Next.js, Spring Boot, etc.)\n  - Database(s)\n  - Frontend framework (if applicable)\n  - Backend framework (if applicable)\n\n- **Existing Features:**\n  - List all features from features.json\n  - Summarize what the project does\n\n- **A2A Integration Opportunities:**\n  - Would this project benefit from A2A agents? (yes/no and why)\n  - Would this project benefit from A2A clients? (yes/no and why)\n  - Does this project need agent discovery? (yes/no and why)\n  - Does this project need streaming? (yes/no and why)\n  - Does this project need production features? (yes/no and why)\n\n- **Recommended A2A Features:**\n  - Prioritized list of A2A features to add\n  - Justification for each recommendation\n\n- **Implementation Strategy:**\n  - Which files would be modified\n  - Where A2A code should be added\n  - Integration points with existing code\n\nReturn this analysis in structured format.\"\n)\n```\n\nWait for analysis agent to complete and return results.\n\n### Step 4: Mark Analysis Complete\n\n```\nTodoWrite(mark first todo completed, second todo in_progress)\n```\n\n## Phase 2: Planning Integration\n\nBased on the analysis from the agent, create an integration plan.\n\n### Step 1: Extract Key Findings\n\nFrom the agent's analysis, extract:\n- Primary language\n- Recommended A2A features\n- Integration strategy\n\n### Step 2: Create Implementation Plan\n\nDetermine which commands to run in sequence:\n\n**Example decision logic:**\n```\nIf analysis recommends \"agents\":\n  ‚Üí Queue /a2a-protocol:add-agent\n\nIf analysis recommends \"clients\":\n  ‚Üí Queue /a2a-protocol:add-client\n\nIf analysis recommends \"discovery\":\n  ‚Üí Queue /a2a-protocol:add-discovery\n\nIf analysis recommends \"streaming\":\n  ‚Üí Queue /a2a-protocol:add-streaming\n\nIf analysis recommends \"production\":\n  ‚Üí Queue /a2a-protocol:add-production\n\nAlways queue: /a2a-protocol:test (at the end)\n```\n\n### Step 3: Display Plan to User\n\nBefore executing, show the user what will be done:\n\n```markdown\n# A2A Protocol Integration Plan\n\nBased on analysis of your project, here's what will be added:\n\n## Project Summary\n- **Type:** [detected project type]\n- **Language:** [primary language]\n- **Stack:** [key technologies]\n\n## Recommended A2A Features\n‚úÖ [Feature 1] - [Reason why]\n‚úÖ [Feature 2] - [Reason why]\n‚úÖ [Feature 3] - [Reason why]\n\n## Commands That Will Run\n1. /a2a-protocol:init\n2. /a2a-protocol:add-agent (because: [reason])\n3. /a2a-protocol:add-client (because: [reason])\n4. /a2a-protocol:add-streaming (because: [reason])\n5. /a2a-protocol:test\n\n## Integration Points\n- [Where A2A code will be added]\n- [Which files will be modified]\n- [New files that will be created]\n\nProceeding with implementation...\n```\n\n### Step 4: Mark Planning Complete\n\n```\nTodoWrite(mark second todo completed, third todo in_progress)\n```\n\n## Phase 3: Execute Integration\n\n### Step 1: Initialize A2A Protocol\n\n**ALWAYS run init first:**\n\n```\nSlashCommand(/a2a-protocol:init)\n```\n\n**CRITICAL:** Wait for init to complete before proceeding.\n\n### Step 2: Run Recommended Commands Sequentially\n\nBased on the plan, run each command **ONE AT A TIME**:\n\n```\nIf plan includes \"add-agent\":\n  SlashCommand(/a2a-protocol:add-agent)\n  Wait for completion\n\nIf plan includes \"add-client\":\n  SlashCommand(/a2a-protocol:add-client)\n  Wait for completion\n\nIf plan includes \"add-discovery\":\n  SlashCommand(/a2a-protocol:add-discovery)\n  Wait for completion\n\nIf plan includes \"add-streaming\":\n  SlashCommand(/a2a-protocol:add-streaming)\n  Wait for completion\n\nIf plan includes \"add-production\":\n  SlashCommand(/a2a-protocol:add-production)\n  Wait for completion\n```\n\n**CRITICAL RULES:**\n- Run commands ONE AT A TIME\n- WAIT for each to complete before starting next\n- DO NOT run in parallel\n- DO NOT skip waiting\n\n### Step 3: Run Tests\n\nAfter all features are added:\n\n```\nSlashCommand(/a2a-protocol:test)\n```\n\nWait for tests to complete.\n\n### Step 4: Mark Execution Complete\n\n```\nTodoWrite(mark third todo completed, fourth todo in_progress)\n```\n\n## Phase 4: Validation & Summary\n\n### Step 1: Validate Integration\n\nCheck that everything was created correctly:\n\n```\nRead integration files to verify they exist\nCheck for common issues\nValidate configuration\n```\n\n### Step 2: Display Comprehensive Summary\n\n```markdown\n# A2A Protocol Integration - Complete ‚úÖ\n\n## Project Analysis Results\n- **Project Type:** [type]\n- **Primary Language:** [language]\n- **Tech Stack:** [stack]\n- **Features Analyzed:** [count] features from features.json\n\n## What Was Added\n\n### A2A Features Installed\n‚úÖ A2A Protocol initialized\n[‚úÖ] Agent implementation (added because: [reason])\n[‚úÖ] Client implementation (added because: [reason])\n[‚úÖ] Agent discovery (added because: [reason])\n[‚úÖ] Streaming support (added because: [reason])\n[‚úÖ] Production features (added because: [reason])\n\n### Files Created\n- [List of new files]\n- [Configuration files]\n- [Implementation files]\n\n### Files Modified\n- [List of modified files]\n- [Integration points]\n\n## Integration Points\nYour A2A Protocol integration is connected to:\n- [Integration point 1]\n- [Integration point 2]\n- [Integration point 3]\n\n## Next Steps\n\n1. **Review generated code:**\n   - Check files in [directories]\n   - Review integration points\n\n2. **Configure environment:**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your actual API keys\n   ```\n\n3. **Start your application:**\n   - [Language-specific start command]\n\n4. **Test A2A features:**\n   ```bash\n   /a2a-protocol:test\n   ```\n\n5. **Explore documentation:**\n   - See docs/a2a/ for detailed usage\n   - Review generated examples\n\n## Available Commands\nRun these to modify your A2A setup:\n- `/a2a-protocol:add-agent` - Add more agents\n- `/a2a-protocol:add-client` - Add more clients\n- `/a2a-protocol:add-discovery` - Update discovery\n- `/a2a-protocol:add-streaming` - Modify streaming\n- `/a2a-protocol:test` - Run tests again\n\nYour intelligent A2A Protocol integration is complete! üöÄ\n```\n\n### Step 3: Mark Final Todo Complete\n\n```\nTodoWrite(mark fourth todo completed)\n```\n\n## Error Handling\n\nIf any step fails:\n\n1. **Stop execution immediately**\n2. **Display error details:**\n   ```\n   ‚ùå A2A Protocol Integration Failed\n\n   Failed at: [which command/step]\n   Error: [error message]\n\n   What to do:\n   - [Specific guidance for this error]\n   - [How to fix]\n   - [How to retry]\n   ```\n3. **Provide recovery options:**\n   - Manual command to retry\n   - How to continue from where it failed\n   - How to rollback if needed\n\n## Key Principles\n\n**Intelligence First:**\n- Analyze before acting\n- Understand the project before building\n- Tailor integration to actual needs\n\n**Project-Driven:**\n- Read project.json to understand stack\n- Read features.json to understand features\n- Read specs/ to understand requirements\n- Read architecture docs to understand design\n\n**Context-Aware:**\n- Different integration for FastAPI vs Next.js\n- Different features for API vs full-stack app\n- Different approach for microservices vs monolith\n\n**Sequential Execution:**\n- One command at a time\n- Wait for completion\n- Handle errors gracefully\n- Track progress clearly\n\nThis intelligent orchestrator adapts to YOUR project and builds exactly what YOU need."
              },
              {
                "name": "/init",
                "description": "Initialize A2A Protocol project with SDK setup and configuration",
                "path": "plugins/a2a-protocol/commands/init.md",
                "frontmatter": {
                  "description": "Initialize A2A Protocol project with SDK setup and configuration",
                  "argument-hint": [
                    "project-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Initialize A2A Protocol project with SDK installation, configuration files, and basic setup for agent-to-agent communication.\n\nCore Principles:\n- Detect existing project structure before assuming\n- Ask clarifying questions when requirements are unclear\n- Use a2a-setup agent for autonomous implementation\n- Validate setup after completion\n\nPhase 1: Discovery\nGoal: Understand project context and requirements\n\nActions:\n- Parse $ARGUMENTS for project name if provided\n- Detect if this is an existing project or new project\n- Check for package.json to identify Node.js/TypeScript setup\n- Example: !{bash ls package.json tsconfig.json 2>/dev/null}\n- Load existing configuration if present\n- Example: @package.json\n\nPhase 2: Requirements Gathering\nGoal: Clarify setup preferences\n\nActions:\n- If project details are unclear, use AskUserQuestion to gather:\n  - Is this a new project or adding to existing?\n  - Which programming language? (TypeScript/JavaScript/Python)\n  - What type of agents will be built? (simple/complex/both)\n  - Any specific A2A Protocol features needed? (authentication/encryption/discovery)\n  - Deployment target? (local/cloud/both)\n- Summarize understanding and confirm approach\n\nPhase 3: Project Analysis\nGoal: Understand current codebase structure\n\nActions:\n- If existing project, analyze current structure\n- Find relevant configuration files\n- Example: !{bash find . -maxdepth 3 -name \"*.json\" -o -name \"*.yaml\" 2>/dev/null | head -10}\n- Identify where A2A Protocol components should be added\n- Check for conflicting dependencies\n\nPhase 4: Implementation\nGoal: Execute setup with a2a-setup agent\n\nActions:\n\nTask(description=\"Initialize A2A Protocol SDK\", subagent_type=\"a2a-setup\", prompt=\"You are the a2a-setup agent. Initialize A2A Protocol project for $ARGUMENTS.\n\nProject Context: Based on discovery in Phases 1-3\n\nRequirements:\n- Install A2A Protocol SDK and dependencies\n- Create configuration files (.env.example, a2a.config.json)\n- Set up project structure (agents/, protocols/, utils/)\n- Generate example agent implementations\n- Configure authentication and security settings\n- Create README with setup instructions\n\nSecurity Compliance:\n- Use placeholders for all API keys and secrets\n- Create .env.example with placeholder values only\n- Add .env to .gitignore\n- Document where to obtain real credentials\n\nExpected output: Complete A2A Protocol project initialization with all configuration files, example code, and documentation.\")\n\nPhase 5: Validation\nGoal: Verify setup is correct\n\nActions:\n- Check that all expected files were created\n- Example: !{bash ls -la a2a.config.json .env.example package.json 2>/dev/null}\n- Verify dependencies are installed\n- Example: !{bash npm list @a2a-protocol/sdk 2>/dev/null || echo \"Not installed\"}\n- Run type checking if TypeScript project\n- Example: !{bash npx tsc --noEmit 2>/dev/null || echo \"No TypeScript\"}\n\nPhase 6: Summary\nGoal: Report what was accomplished\n\nActions:\n- Summarize changes made:\n  - Files created\n  - Dependencies installed\n  - Configuration generated\n- Display next steps:\n  - Fill in real API keys in .env\n  - Review a2a.config.json settings\n  - Explore example agents\n  - Run first agent with npm start\n- Show helpful commands for getting started"
              },
              {
                "name": "/test",
                "description": "Test and validate A2A implementations",
                "path": "plugins/a2a-protocol/commands/test.md",
                "frontmatter": {
                  "description": "Test and validate A2A implementations",
                  "argument-hint": [
                    "test-target"
                  ],
                  "allowed-tools": "Task, Read, Write, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Validate A2A protocol implementations through comprehensive testing\n\nCore Principles:\n- Verify protocol compliance before deployment\n- Detect integration issues early\n- Provide actionable feedback\n- Validate against A2A specification\n\nPhase 1: Discovery\nGoal: Understand what needs to be tested\n\nActions:\n- Parse $ARGUMENTS for test target (file, directory, or full project)\n- If unclear, use AskUserQuestion to gather:\n  - What should be tested? (specific files, features, or entire implementation)\n  - What protocol aspects to focus on? (messaging, discovery, authentication)\n  - Any specific test scenarios or edge cases?\n- Detect project type and A2A implementation location\n- Example: !{bash ls -la src/ 2>/dev/null | grep -i \"a2a\\|agent\"}\n\nPhase 2: Analysis\nGoal: Understand existing A2A implementation\n\nActions:\n- Locate A2A-related files\n- Example: !{bash find . -type f \\( -name \"*a2a*\" -o -name \"*agent*\" \\) 2>/dev/null | head -20}\n- Identify implementation patterns\n- Check for test files\n- Load relevant configuration files\n- Example: @package.json\n\nPhase 3: Planning\nGoal: Design test approach\n\nActions:\n- Determine test scope based on implementation\n- Identify critical protocol components to validate\n- Plan test scenarios (message handling, discovery, error cases)\n- Confirm approach with user if significant gaps found\n\nPhase 4: Implementation\nGoal: Execute validation with agent\n\nActions:\n\nTask(description=\"Validate A2A implementation\", subagent_type=\"a2a-verifier\", prompt=\"You are the a2a-verifier agent. Test and validate A2A protocol implementation for $ARGUMENTS.\n\nContext: Testing A2A implementation to ensure protocol compliance\n\nRequirements:\n- Verify message format compliance\n- Test discovery mechanisms\n- Validate authentication flows\n- Check error handling\n- Test interoperability with reference implementations\n\nExpected output: Comprehensive test report with pass/fail status, identified issues, and recommendations\")\n\nPhase 5: Review\nGoal: Verify test results\n\nActions:\n- Review agent's test report\n- Check if any critical issues found\n- Run additional validation if needed\n- Example: !{bash npm test 2>/dev/null || echo \"No test script available\"}\n\nPhase 6: Summary\nGoal: Document test results\n\nActions:\n- Summarize test outcomes\n- Highlight any protocol violations or issues\n- List recommendations for fixes\n- Suggest next steps for addressing failures"
              }
            ],
            "skills": [
              {
                "name": "a2a-executor-patterns",
                "description": "Agent-to-Agent (A2A) executor implementation patterns for task handling, execution management, and agent coordination. Use when building A2A executors, implementing task handlers, creating agent execution flows, or when user mentions A2A protocol, task execution, agent executors, task handlers, or agent coordination.",
                "path": "plugins/a2a-protocol/skills/a2a-executor-patterns/SKILL.md",
                "frontmatter": {
                  "name": "a2a-executor-patterns",
                  "description": "Agent-to-Agent (A2A) executor implementation patterns for task handling, execution management, and agent coordination. Use when building A2A executors, implementing task handlers, creating agent execution flows, or when user mentions A2A protocol, task execution, agent executors, task handlers, or agent coordination.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# A2A Executor Patterns\n\n**Purpose:** Provide production-ready executor patterns for implementing Agent-to-Agent (A2A) protocol task handlers with proper error handling, retry logic, and execution flows.\n\n**Activation Triggers:**\n- Implementing A2A protocol executors\n- Building task handler functions\n- Creating agent execution flows\n- Managing task lifecycle and state\n- Implementing retry and error recovery\n- Building executor middleware\n- Task validation and sanitization\n\n**Key Resources:**\n- `templates/basic-executor.ts` - Simple synchronous executor\n- `templates/basic-executor.py` - Python synchronous executor\n- `templates/async-executor.ts` - Asynchronous task executor\n- `templates/async-executor.py` - Python async executor\n- `templates/streaming-executor.ts` - Streaming result executor\n- `templates/streaming-executor.py` - Python streaming executor\n- `templates/batch-executor.ts` - Batch task processing\n- `scripts/validate-executor.sh` - Validate executor implementation\n- `scripts/test-executor.sh` - Test executor against A2A spec\n- `examples/` - Production executor implementations\n\n## Core Executor Patterns\n\n### 1. Basic Executor (Synchronous)\n\n**When to use:** Simple, fast tasks with immediate results\n\n**Template:** `templates/basic-executor.ts` or `templates/basic-executor.py`\n\n**Pattern:**\n```typescript\nasync function executeTask(task: A2ATask): Promise<A2AResult> {\n  // 1. Validate input\n  validateTask(task)\n\n  // 2. Execute task\n  const result = await processTask(task)\n\n  // 3. Return result\n  return {\n    status: 'completed',\n    result,\n    taskId: task.id\n  }\n}\n```\n\n**Best for:** Quick operations, validation tasks, simple transformations\n\n### 2. Async Executor (Long-Running)\n\n**When to use:** Tasks that take time and need status updates\n\n**Template:** `templates/async-executor.ts` or `templates/async-executor.py`\n\n**Pattern:**\n- Accept task and return task ID immediately\n- Process task asynchronously\n- Provide status endpoint\n- Send completion callback\n\n**Best for:** LLM inference, file processing, data analysis\n\n### 3. Streaming Executor\n\n**When to use:** Results should be delivered incrementally\n\n**Template:** `templates/streaming-executor.ts` or `templates/streaming-executor.py`\n\n**Pattern:**\n- Open stream connection\n- Send partial results as available\n- Close stream on completion\n- Handle backpressure\n\n**Best for:** Text generation, real-time data, progressive results\n\n### 4. Batch Executor\n\n**When to use:** Processing multiple related tasks efficiently\n\n**Template:** `templates/batch-executor.ts`\n\n**Pattern:**\n- Accept multiple tasks\n- Group by similarity\n- Process in parallel batches\n- Return aggregated results\n\n**Best for:** Bulk operations, parallel processing, resource optimization\n\n## Execution Flow Components\n\n### 1. Task Validation\n\n```typescript\nfunction validateTask(task: A2ATask): void {\n  // Validate required fields\n  if (!task.id) throw new ValidationError('Task ID required')\n  if (!task.type) throw new ValidationError('Task type required')\n\n  // Validate task parameters\n  validateParameters(task.parameters)\n\n  // Check executor capabilities\n  if (!supportsTaskType(task.type)) {\n    throw new UnsupportedTaskError(task.type)\n  }\n}\n```\n\n**Purpose:** Catch errors early, provide clear feedback\n\n### 2. Error Handling\n\n```typescript\nasync function executeWithErrorHandling(task: A2ATask) {\n  try {\n    return await executeTask(task)\n  } catch (error) {\n    if (error instanceof ValidationError) {\n      return { status: 'failed', error: error.message }\n    }\n    if (error instanceof RetryableError) {\n      return scheduleRetry(task)\n    }\n    // Log and return generic error\n    logger.error('Task execution failed', { taskId: task.id, error })\n    return { status: 'failed', error: 'Internal error' }\n  }\n}\n```\n\n**Error Types:**\n- `ValidationError` - Invalid input, don't retry\n- `RetryableError` - Temporary failure, safe to retry\n- `FatalError` - Permanent failure, abort\n\n### 3. Retry Logic\n\n```typescript\nconst retryConfig = {\n  maxAttempts: 3,\n  backoff: 'exponential', // or 'linear', 'fixed'\n  initialDelay: 1000, // ms\n  maxDelay: 30000\n}\n\nasync function executeWithRetry(\n  task: A2ATask,\n  attempt: number = 1\n): Promise<A2AResult> {\n  try {\n    return await executeTask(task)\n  } catch (error) {\n    if (attempt >= retryConfig.maxAttempts) {\n      throw new MaxRetriesExceededError(task.id)\n    }\n\n    if (error instanceof RetryableError) {\n      const delay = calculateBackoff(attempt)\n      await sleep(delay)\n      return executeWithRetry(task, attempt + 1)\n    }\n\n    throw error\n  }\n}\n```\n\n**Retry Strategies:**\n- Exponential backoff: `delay = initialDelay * (2 ^ attempt)`\n- Linear backoff: `delay = initialDelay * attempt`\n- Fixed delay: `delay = initialDelay`\n\n### 4. Task State Management\n\n```typescript\ninterface TaskState {\n  id: string\n  status: 'pending' | 'running' | 'completed' | 'failed'\n  result?: any\n  error?: string\n  startTime: Date\n  endTime?: Date\n  attempts: number\n}\n\nclass TaskStore {\n  private tasks = new Map<string, TaskState>()\n\n  createTask(id: string): TaskState {\n    const state: TaskState = {\n      id,\n      status: 'pending',\n      startTime: new Date(),\n      attempts: 0\n    }\n    this.tasks.set(id, state)\n    return state\n  }\n\n  updateTask(id: string, update: Partial<TaskState>): void {\n    const state = this.tasks.get(id)\n    if (state) {\n      Object.assign(state, update)\n    }\n  }\n\n  getTask(id: string): TaskState | undefined {\n    return this.tasks.get(id)\n  }\n}\n```\n\n## Executor Middleware\n\n### 1. Logging Middleware\n\n```typescript\nfunction loggingMiddleware(\n  executor: Executor\n): Executor {\n  return async (task) => {\n    logger.info('Task started', { taskId: task.id })\n    const start = Date.now()\n\n    try {\n      const result = await executor(task)\n      const duration = Date.now() - start\n      logger.info('Task completed', { taskId: task.id, duration })\n      return result\n    } catch (error) {\n      const duration = Date.now() - start\n      logger.error('Task failed', { taskId: task.id, duration, error })\n      throw error\n    }\n  }\n}\n```\n\n### 2. Metrics Middleware\n\n```typescript\nfunction metricsMiddleware(\n  executor: Executor\n): Executor {\n  return async (task) => {\n    metrics.increment('tasks.started', { type: task.type })\n    const start = Date.now()\n\n    try {\n      const result = await executor(task)\n      const duration = Date.now() - start\n      metrics.timing('tasks.duration', duration, { type: task.type })\n      metrics.increment('tasks.completed', { type: task.type })\n      return result\n    } catch (error) {\n      metrics.increment('tasks.failed', { type: task.type })\n      throw error\n    }\n  }\n}\n```\n\n### 3. Rate Limiting Middleware\n\n```typescript\nfunction rateLimitMiddleware(\n  executor: Executor,\n  limit: { requests: number, window: number }\n): Executor {\n  const limiter = new RateLimiter(limit.requests, limit.window)\n\n  return async (task) => {\n    await limiter.acquire()\n    try {\n      return await executor(task)\n    } finally {\n      limiter.release()\n    }\n  }\n}\n```\n\n## Production Best Practices\n\n### 1. Timeouts\n\n```typescript\nasync function executeWithTimeout(\n  task: A2ATask,\n  timeoutMs: number\n): Promise<A2AResult> {\n  return Promise.race([\n    executeTask(task),\n    new Promise((_, reject) =>\n      setTimeout(() => reject(new TimeoutError()), timeoutMs)\n    )\n  ])\n}\n```\n\n### 2. Resource Cleanup\n\n```typescript\nasync function executeWithCleanup(task: A2ATask) {\n  const resources = []\n\n  try {\n    const resource = await allocateResource()\n    resources.push(resource)\n\n    return await executeTask(task, resource)\n  } finally {\n    // Always cleanup, even on error\n    await Promise.all(\n      resources.map(r => r.cleanup())\n    )\n  }\n}\n```\n\n### 3. Graceful Shutdown\n\n```typescript\nclass GracefulExecutor {\n  private activeTasks = new Set<string>()\n  private shuttingDown = false\n\n  async execute(task: A2ATask): Promise<A2AResult> {\n    if (this.shuttingDown) {\n      throw new Error('Executor is shutting down')\n    }\n\n    this.activeTasks.add(task.id)\n\n    try {\n      return await executeTask(task)\n    } finally {\n      this.activeTasks.delete(task.id)\n    }\n  }\n\n  async shutdown(): Promise<void> {\n    this.shuttingDown = true\n\n    // Wait for active tasks to complete\n    while (this.activeTasks.size > 0) {\n      await sleep(100)\n    }\n  }\n}\n```\n\n## Common Executor Types\n\n### 1. LLM Executor\n\n**Example:** `examples/llm-executor.ts`\n\nExecutes LLM inference tasks with streaming\n\n### 2. Function Executor\n\n**Example:** `examples/function-executor.ts`\n\nCalls functions/tools and returns results\n\n### 3. Workflow Executor\n\n**Example:** `examples/workflow-executor.ts`\n\nOrchestrates multi-step workflows\n\n### 4. Validation Executor\n\n**Example:** `examples/validation-executor.ts`\n\nValidates data and returns compliance results\n\n## Validation and Testing\n\n**Scripts:**\n- `scripts/validate-executor.sh` - Validate executor structure\n- `scripts/test-executor.sh` - Test against A2A spec\n\n**Run validation:**\n```bash\nbash scripts/validate-executor.sh your-executor.ts\n```\n\n**Run tests:**\n```bash\nbash scripts/test-executor.sh your-executor.ts\n```\n\n## Resources\n\n**TypeScript Templates:**\n- `basic-executor.ts` - Simple sync executor\n- `async-executor.ts` - Async with status tracking\n- `streaming-executor.ts` - Streaming results\n- `batch-executor.ts` - Batch processing\n\n**Python Templates:**\n- `basic-executor.py` - Simple sync executor\n- `async-executor.py` - Async with status tracking\n- `streaming-executor.py` - Streaming results\n\n**Scripts:**\n- `validate-executor.sh` - Structure validation\n- `test-executor.sh` - A2A spec compliance\n\n**Examples:**\n- `llm-executor.ts` - LLM inference executor\n- `function-executor.ts` - Function calling executor\n- `workflow-executor.ts` - Multi-step workflows\n- `validation-executor.ts` - Data validation\n\n---\n\n**Protocol Version:** A2A Protocol v1.0\n**Runtime:** Node.js 18+, Python 3.9+\n\n**Best Practice:** Start with basic executor, add complexity (async, streaming, batching) only as needed"
              },
              {
                "name": "a2a-mcp-integration",
                "description": "Integration patterns for combining Agent-to-Agent (A2A) Protocol with Model Context Protocol (MCP) for hybrid agent communication. Use when building systems that need both agent-to-agent communication and agent-to-tool integration, implementing composite architectures, or when user mentions A2A+MCP integration, hybrid protocols, or multi-agent tool access.",
                "path": "plugins/a2a-protocol/skills/a2a-mcp-integration/SKILL.md",
                "frontmatter": {
                  "name": "a2a-mcp-integration",
                  "description": "Integration patterns for combining Agent-to-Agent (A2A) Protocol with Model Context Protocol (MCP) for hybrid agent communication. Use when building systems that need both agent-to-agent communication and agent-to-tool integration, implementing composite architectures, or when user mentions A2A+MCP integration, hybrid protocols, or multi-agent tool access.",
                  "allowed-tools": "Read, Bash, Write, Edit, Grep, Glob"
                },
                "content": "# A2A and MCP Integration Patterns\n\n**Purpose:** Provide integration patterns, configuration examples, and best practices for combining Agent-to-Agent Protocol with Model Context Protocol in multi-agent systems.\n\n**Activation Triggers:**\n- A2A + MCP integration setup\n- Hybrid protocol architecture\n- Agent-to-agent + agent-to-tool communication\n- Composite multi-agent systems\n- Protocol compatibility questions\n- Cross-protocol authentication\n- Combined SDK usage\n\n**Protocol Roles:**\n- **A2A Protocol:** Agent-to-agent communication and task delegation\n- **MCP:** Agent-to-tool communication and resource access\n- **Combined:** Agents communicate via A2A while accessing tools via MCP\n\n## Architecture Overview\n\n### Why Combine A2A and MCP?\n\nA2A and MCP are complementary protocols:\n\n- **MCP** standardizes how agents connect to tools, APIs, and data sources\n- **A2A** standardizes how agents communicate and coordinate with each other\n- **Together** they enable agents to collaborate while accessing shared resources\n\n### Integration Patterns\n\n1. **Hierarchical Agent Systems**\n   - Coordinator agent uses A2A to delegate tasks\n   - Worker agents use MCP to access tools\n   - Results flow back through A2A\n\n2. **Federated Tool Access**\n   - Multiple agents communicate via A2A\n   - Each agent has MCP tool access\n   - Agents share tool results through A2A messages\n\n3. **Resource-Sharing Networks**\n   - Agents discover each other via A2A\n   - Agents expose MCP servers to each other\n   - Dynamic tool delegation through agent network\n\n## Quick Start\n\n### Python Integration\n\n```bash\n# Install both SDKs\n./scripts/install-python-integration.sh\n\n# Validate installation\n./scripts/validate-python-integration.sh\n\n# Run example\npython examples/python-hybrid-agent.py\n```\n\n### TypeScript Integration\n\n```bash\n# Install both SDKs\n./scripts/install-typescript-integration.sh\n\n# Validate installation\n./scripts/validate-typescript-integration.sh\n\n# Run example\nts-node examples/typescript-hybrid-agent.ts\n```\n\n## Configuration\n\n### Environment Setup\n\nBoth protocols require environment configuration:\n\n```bash\n# A2A Configuration\nA2A_API_KEY=your_a2a_key_here\nA2A_BASE_URL=https://a2a.example.com\n\n# MCP Configuration\nMCP_SERVER_URL=http://localhost:3000\nMCP_TRANSPORT=stdio\n\n# Integration Settings\nHYBRID_AGENT_ID=agent-001\nHYBRID_AGENT_NAME=hybrid-coordinator\nENABLE_A2A=true\nENABLE_MCP=true\n```\n\nSee `templates/env-integration-template.txt` for complete configuration.\n\n### Authentication\n\nHandle authentication for both protocols:\n\n1. **Separate Credentials:** Each protocol uses its own auth\n2. **Shared Identity:** Agent identity spans both protocols\n3. **Token Forwarding:** Pass credentials through A2A messages (when appropriate)\n\nSee `templates/auth-hybrid-template.txt` for patterns.\n\n## Integration Patterns\n\n### Pattern 1: Coordinator-Worker with Shared Tools\n\n**Architecture:**\n- Coordinator agent receives tasks\n- Delegates via A2A to worker agents\n- Workers use MCP tools to complete tasks\n- Results return via A2A\n\n**Template:** `templates/coordinator-worker-pattern.py`\n\n**Use Case:** Complex workflows requiring specialized agents with tool access\n\n### Pattern 2: Peer-to-Peer with Tool Sharing\n\n**Architecture:**\n- Agents communicate as peers via A2A\n- Each agent exposes MCP server\n- Agents can request tools from each other\n- Distributed tool access\n\n**Template:** `templates/peer-tool-sharing-pattern.ts`\n\n**Use Case:** Decentralized systems where agents have different capabilities\n\n### Pattern 3: Agent Mesh with Centralized Tools\n\n**Architecture:**\n- Agents form A2A communication mesh\n- Centralized MCP server provides tools\n- All agents access same tool set\n- Coordination via A2A, execution via MCP\n\n**Template:** `templates/mesh-centralized-tools-pattern.py`\n\n**Use Case:** Teams of agents working with shared infrastructure\n\n### Pattern 4: Layered Protocol Stack\n\n**Architecture:**\n- MCP at base layer for tool access\n- A2A at orchestration layer for coordination\n- Application logic at top layer\n- Clean separation of concerns\n\n**Template:** `templates/layered-stack-pattern.ts`\n\n**Use Case:** Enterprise systems requiring protocol isolation\n\n## Scripts\n\n### Installation Scripts\n\n- `install-python-integration.sh` - Install Python A2A + MCP SDKs\n- `install-typescript-integration.sh` - Install TypeScript A2A + MCP SDKs\n- `install-java-integration.sh` - Install Java A2A + MCP SDKs\n\n### Validation Scripts\n\n- `validate-python-integration.sh` - Verify Python integration setup\n- `validate-typescript-integration.sh` - Verify TypeScript integration setup\n- `validate-protocol-compatibility.sh` - Check protocol version compatibility\n\n### Setup Scripts\n\n- `setup-hybrid-agent.sh` - Initialize hybrid agent environment\n- `setup-mcp-server.sh` - Configure MCP server for A2A agents\n- `setup-agent-discovery.sh` - Configure A2A agent discovery with MCP tools\n\n## Templates\n\n### Configuration Templates\n\n- `env-integration-template.txt` - Environment variables for both protocols\n- `auth-hybrid-template.txt` - Authentication configuration\n- `agent-config-hybrid.json` - Agent configuration with A2A+MCP\n\n### Code Templates\n\n**Python:**\n- `coordinator-worker-pattern.py` - Coordinator-worker implementation\n- `mesh-centralized-tools-pattern.py` - Agent mesh with central MCP\n- `python-hybrid-agent.py` - Basic hybrid agent\n\n**TypeScript:**\n- `peer-tool-sharing-pattern.ts` - Peer-to-peer tool sharing\n- `layered-stack-pattern.ts` - Layered protocol architecture\n- `typescript-hybrid-agent.ts` - Basic hybrid agent\n\n**Java:**\n- `java-hybrid-agent.java` - Basic Java integration\n- `java-coordinator-pattern.java` - Coordinator pattern in Java\n\n**Configuration:**\n- `mcp-server-config.json` - MCP server configuration for A2A agents\n- `a2a-agent-card.json` - Agent card with MCP tool references\n\n## Common Integration Scenarios\n\n### Scenario 1: Multi-Agent Data Pipeline\n\n**Problem:** Multiple agents process data through different tools\n\n**Solution:**\n1. Coordinator receives request via A2A\n2. Delegates to specialized agents (data-fetcher, data-processor, data-storage)\n3. Each agent uses MCP tools for its domain\n4. Results aggregate via A2A back to coordinator\n\n**Example:** `examples/data-pipeline-integration.py`\n\n### Scenario 2: Distributed Research Assistant\n\n**Problem:** Research task requires web search, document analysis, and synthesis\n\n**Solution:**\n1. Agents communicate via A2A to coordinate\n2. Search agent uses MCP web search tools\n3. Analysis agent uses MCP document processing tools\n4. Synthesis agent combines results using MCP output tools\n\n**Example:** `examples/research-assistant-integration.ts`\n\n### Scenario 3: Microservice-Style Agent Architecture\n\n**Problem:** Need modular, scalable agent system\n\n**Solution:**\n1. Each agent is a microservice with A2A interface\n2. Agents use MCP to access shared databases, APIs\n3. Service discovery via A2A agent cards\n4. Load balancing across agent instances\n\n**Example:** `examples/microservice-agents.py`\n\n## Error Handling\n\n### Protocol-Specific Errors\n\nHandle errors from both protocols:\n\n```python\nfrom a2a import A2AError\nfrom mcp import MCPError\n\ntry:\n    # A2A communication\n    response = await a2a_client.send_task(task)\n\n    # MCP tool execution\n    result = await mcp_client.call_tool(\"search\", params)\n\nexcept A2AError as e:\n    # Handle A2A communication errors\n    logger.error(f\"A2A error: {e}\")\n\nexcept MCPError as e:\n    # Handle MCP tool errors\n    logger.error(f\"MCP error: {e}\")\n```\n\nSee `examples/error-handling-integration.py` for complete patterns.\n\n### Connection Failures\n\nBoth protocols may fail independently:\n\n1. **A2A failure, MCP working:** Agent can execute local tools\n2. **MCP failure, A2A working:** Agent can delegate to others\n3. **Both failing:** Implement fallback logic\n\nSee `templates/failover-pattern.py` for resilience patterns.\n\n## Security Considerations\n\n### Authentication Boundaries\n\n**Separate Auth Per Protocol:**\n- A2A credentials for agent communication\n- MCP credentials for tool access\n- Never share credentials across protocols\n\n### Message Security\n\n**A2A Messages:**\n- End-to-end encryption between agents\n- Signature verification for agent identity\n- Do not include MCP credentials in A2A messages\n\n**MCP Communication:**\n- Secure tool access with proper authentication\n- Validate tool responses before sharing via A2A\n- Sandbox tool execution\n\n### Network Security\n\n**Hybrid Deployment:**\n- A2A may be internet-facing for agent discovery\n- MCP should be internal for tool security\n- Use VPN/private networks for MCP traffic\n- Implement network segmentation\n\nSee `examples/security-best-practices.md` for detailed guidance.\n\n## Performance Optimization\n\n### Protocol Selection\n\nChoose the right protocol for each interaction:\n\n**Use A2A when:**\n- Delegating tasks to other agents\n- Coordinating multi-agent workflows\n- Sharing results between agents\n\n**Use MCP when:**\n- Accessing tools and APIs\n- Reading/writing data sources\n- Executing specialized functions\n\n### Connection Pooling\n\nBoth protocols benefit from connection pooling:\n\n```python\n# A2A connection pool\na2a_pool = A2AConnectionPool(\n    max_connections=10,\n    timeout=30\n)\n\n# MCP connection pool\nmcp_pool = MCPConnectionPool(\n    max_connections=5,\n    timeout=15\n)\n```\n\nSee `templates/connection-pooling.py` for implementation.\n\n### Caching Strategies\n\n**Agent Discovery Caching:**\n- Cache A2A agent cards (refresh periodically)\n- Cache MCP tool schemas\n- Invalidate on protocol updates\n\n**Result Caching:**\n- Cache expensive MCP tool results\n- Share cache across A2A agent network\n- Implement cache coherence protocol\n\n## Testing\n\n### Integration Tests\n\nTest both protocols together:\n\n```bash\n# Run integration test suite\n./scripts/test-integration.sh\n\n# Test specific pattern\n./scripts/test-pattern.sh coordinator-worker\n\n# Test protocol compatibility\n./scripts/test-protocol-versions.sh\n```\n\n### Mock Servers\n\nUse mock servers for development:\n\n```bash\n# Start mock A2A server\n./scripts/start-mock-a2a.sh\n\n# Start mock MCP server\n./scripts/start-mock-mcp.sh\n\n# Run tests against mocks\n./scripts/test-with-mocks.sh\n```\n\n## Examples\n\nComplete working examples:\n\n**Python:**\n- `python-hybrid-agent.py` - Basic hybrid agent\n- `data-pipeline-integration.py` - Multi-agent data pipeline\n- `microservice-agents.py` - Microservice architecture\n- `error-handling-integration.py` - Error handling patterns\n\n**TypeScript:**\n- `typescript-hybrid-agent.ts` - Basic hybrid agent\n- `research-assistant-integration.ts` - Distributed research\n- `peer-coordination.ts` - Peer-to-peer coordination\n\n**Configuration:**\n- `docker-compose-integration.yml` - Docker setup for hybrid system\n- `kubernetes-hybrid-agents.yaml` - Kubernetes deployment\n\n**Documentation:**\n- `security-best-practices.md` - Security guidelines\n- `troubleshooting-integration.md` - Common issues and solutions\n- `architecture-patterns.md` - Detailed architecture patterns\n\n## Troubleshooting\n\n### Common Issues\n\n**Protocol Version Mismatch:**\n```bash\n# Check versions\n./scripts/validate-protocol-compatibility.sh\n\n# Upgrade if needed\npip install --upgrade a2a-protocol mcp-sdk\n```\n\n**Authentication Errors:**\n```bash\n# Verify both protocol credentials\necho $A2A_API_KEY\necho $MCP_SERVER_URL\n\n# Test separately\npython -c \"from a2a import Client; Client().ping()\"\npython -c \"from mcp import Client; Client().ping()\"\n```\n\n**Connection Issues:**\n- Check A2A agent is reachable\n- Verify MCP server is running\n- Test network connectivity separately\n- Review firewall rules\n\nSee `examples/troubleshooting-integration.md` for detailed solutions.\n\n## Resources\n\n**Official Documentation:**\n- A2A Protocol: https://a2a-protocol.org\n- MCP: https://modelcontextprotocol.io\n- Integration Guide: https://docs.a2a-protocol.org/mcp-integration\n\n**GitHub Repositories:**\n- A2A+MCP Examples: https://github.com/a2a/mcp-integration-examples\n- Python Integration: https://github.com/a2a/python-mcp-integration\n- TypeScript Integration: https://github.com/a2a/typescript-mcp-integration\n\n**Community:**\n- A2A Discord: https://discord.gg/a2a-protocol\n- MCP Discussion: https://github.com/modelcontextprotocol/specification/discussions\n- Integration Patterns: https://community.a2a-protocol.org/integrations\n\n---\n\n**Version:** 1.0.0\n**A2A Protocol Compatibility:** 1.0+\n**MCP Compatibility:** 1.0+\n**Last Updated:** 2025-12-20"
              },
              {
                "name": "a2a-sdk-patterns",
                "description": "SDK installation and setup patterns for Agent-to-Agent Protocol across Python, TypeScript, Java, C#, and Go. Use when implementing A2A protocol, setting up SDKs, configuring authentication, or when user mentions SDK installation, language-specific setup, or A2A integration.",
                "path": "plugins/a2a-protocol/skills/a2a-sdk-patterns/SKILL.md",
                "frontmatter": {
                  "name": "a2a-sdk-patterns",
                  "description": "SDK installation and setup patterns for Agent-to-Agent Protocol across Python, TypeScript, Java, C#, and Go. Use when implementing A2A protocol, setting up SDKs, configuring authentication, or when user mentions SDK installation, language-specific setup, or A2A integration.",
                  "allowed-tools": "Read, Bash, Write, Edit, Grep, Glob"
                },
                "content": "# Agent-to-Agent Protocol SDK Patterns\n\n**Purpose:** Provide installation, configuration, and usage patterns for A2A Protocol SDKs across multiple programming languages.\n\n**Activation Triggers:**\n- SDK installation requests\n- Language-specific A2A setup\n- Authentication configuration\n- Package dependency issues\n- SDK version compatibility\n- Import/setup errors\n\n**Supported Languages:**\n- Python (3.8+)\n- TypeScript/JavaScript (Node 18+)\n- Java (11+)\n- C# (.NET 6+)\n- Go (1.20+)\n\n## Quick Start by Language\n\n### Python\n\n```bash\n# Install SDK\n./scripts/install-python.sh\n\n# Verify installation\n./scripts/validate-python.sh\n```\n\n### TypeScript\n\n```bash\n# Install SDK\n./scripts/install-typescript.sh\n\n# Verify installation\n./scripts/validate-typescript.sh\n```\n\n### Java\n\n```bash\n# Install SDK\n./scripts/install-java.sh\n\n# Verify installation\n./scripts/validate-java.sh\n```\n\n### C#\n\n```bash\n# Install SDK\n./scripts/install-csharp.sh\n\n# Verify installation\n./scripts/validate-csharp.sh\n```\n\n### Go\n\n```bash\n# Install SDK\n./scripts/install-go.sh\n\n# Verify installation\n./scripts/validate-go.sh\n```\n\n## Installation Scripts\n\nAll installation scripts are in `scripts/` directory:\n\n- `install-python.sh` - Install Python SDK via pip\n- `install-typescript.sh` - Install TypeScript SDK via npm/yarn\n- `install-java.sh` - Install Java SDK via Maven/Gradle\n- `install-csharp.sh` - Install C# SDK via NuGet\n- `install-go.sh` - Install Go SDK via go get\n\nValidation scripts verify installation and dependencies:\n\n- `validate-python.sh` - Check Python SDK installation\n- `validate-typescript.sh` - Check TypeScript SDK installation\n- `validate-java.sh` - Check Java SDK installation\n- `validate-csharp.sh` - Check C# SDK installation\n- `validate-go.sh` - Check Go SDK installation\n\n## Configuration Templates\n\nTemplates are in `templates/` directory:\n\n**Environment Setup:**\n- `env-template.txt` - Environment variable template (all languages)\n- `python-config.py` - Python configuration example\n- `typescript-config.ts` - TypeScript configuration example\n- `java-config.xml` - Java Maven configuration\n- `csharp-config.csproj` - C# project configuration\n- `go-mod.txt` - Go module configuration\n\n**Authentication:**\n- `auth-api-key-template.txt` - API key authentication\n- `auth-oauth-template.txt` - OAuth authentication\n- `auth-jwt-template.txt` - JWT authentication\n\n## Common Setup Patterns\n\n### Environment Variables\n\nAll SDKs use environment variables for configuration:\n\n```bash\n# Required\nA2A_API_KEY=your_api_key_here\nA2A_BASE_URL=https://api.a2a.example.com\n\n# Optional\nA2A_TIMEOUT=30\nA2A_RETRY_ATTEMPTS=3\nA2A_LOG_LEVEL=info\n```\n\n**CRITICAL:** Always use placeholders in committed files. Create `.env.example` with placeholder values only.\n\n### Authentication Setup\n\nAll SDKs support three authentication methods:\n\n1. **API Key** - Simplest, for server-to-server\n2. **OAuth 2.0** - For user-delegated access\n3. **JWT** - For service-to-service with custom claims\n\nSee `templates/auth-*-template.txt` for implementation patterns.\n\n### Error Handling\n\nAll SDKs provide consistent error handling:\n\n- `A2AConnectionError` - Network/connectivity issues\n- `A2AAuthenticationError` - Invalid credentials\n- `A2ARateLimitError` - Rate limit exceeded\n- `A2AValidationError` - Invalid request data\n\nSee `examples/error-handling-*.md` for language-specific patterns.\n\n## Language-Specific Considerations\n\n### Python\n\n- Requires Python 3.8+\n- Install via pip: `pip install a2a-protocol`\n- Async support via asyncio\n- Type hints available\n- See `examples/python-basic.py`\n\n### TypeScript\n\n- Requires Node 18+\n- Install via npm: `npm install @a2a/protocol`\n- Full TypeScript definitions included\n- Promise-based async/await\n- See `examples/typescript-basic.ts`\n\n### Java\n\n- Requires Java 11+\n- Maven: Add to pom.xml\n- Gradle: Add to build.gradle\n- Thread-safe client\n- See `examples/java-basic.java`\n\n### C#\n\n- Requires .NET 6+\n- NuGet: `dotnet add package A2A.Protocol`\n- Async/await support\n- Dependency injection ready\n- See `examples/csharp-basic.cs`\n\n### Go\n\n- Requires Go 1.20+\n- Install: `go get github.com/a2a/protocol-go`\n- Context-aware operations\n- Goroutine-safe\n- See `examples/go-basic.go`\n\n## Troubleshooting\n\n### Installation Issues\n\n**Package not found:**\n```bash\n# Python\npip install --upgrade pip\npip install a2a-protocol\n\n# TypeScript\nnpm cache clean --force\nnpm install @a2a/protocol\n\n# Java\nmvn clean install -U\n\n# C#\ndotnet restore --force\n\n# Go\ngo clean -modcache\ngo get -u github.com/a2a/protocol-go\n```\n\n**Version conflicts:**\nRun the appropriate validation script to check dependencies:\n```bash\n./scripts/validate-<language>.sh\n```\n\n### Authentication Errors\n\n1. Check environment variables are set\n2. Verify API key format (no extra spaces/newlines)\n3. Ensure base URL is correct\n4. Check API key permissions\n\n### Connection Issues\n\n1. Verify network connectivity\n2. Check firewall/proxy settings\n3. Validate base URL is accessible\n4. Review timeout settings\n\n## Security Best Practices\n\n**Environment Variables:**\n- NEVER commit actual API keys\n- Use `.env` files (add to `.gitignore`)\n- Create `.env.example` with placeholders\n- Use secret management in production (Vault, AWS Secrets Manager, etc.)\n\n**API Keys:**\n- Rotate keys regularly\n- Use different keys for dev/staging/prod\n- Implement key expiration\n- Monitor key usage\n\n**Network Security:**\n- Always use HTTPS\n- Validate SSL certificates\n- Implement request signing for sensitive operations\n- Use VPN/private networks for production\n\n## Examples\n\nComplete examples for each language:\n\n- `examples/python-basic.py` - Basic Python usage\n- `examples/python-async.py` - Async Python usage\n- `examples/typescript-basic.ts` - Basic TypeScript usage\n- `examples/java-basic.java` - Basic Java usage\n- `examples/csharp-basic.cs` - Basic C# usage\n- `examples/go-basic.go` - Basic Go usage\n- `examples/error-handling-python.md` - Python error handling\n- `examples/error-handling-typescript.md` - TypeScript error handling\n- `examples/error-handling-java.md` - Java error handling\n\n## Resources\n\n**Official Documentation:**\n- Python SDK: https://docs.a2a-protocol.org/python\n- TypeScript SDK: https://docs.a2a-protocol.org/typescript\n- Java SDK: https://docs.a2a-protocol.org/java\n- C# SDK: https://docs.a2a-protocol.org/csharp\n- Go SDK: https://docs.a2a-protocol.org/go\n\n**GitHub Repositories:**\n- Python: https://github.com/a2a/protocol-python\n- TypeScript: https://github.com/a2a/protocol-ts\n- Java: https://github.com/a2a/protocol-java\n- C#: https://github.com/a2a/protocol-dotnet\n- Go: https://github.com/a2a/protocol-go\n\n---\n\n**Version:** 1.0.0\n**Protocol Compatibility:** A2A Protocol 1.0+"
              },
              {
                "name": "a2a-server-config",
                "description": "Agent-to-Agent (A2A) server configuration patterns for HTTP, STDIO, SSE, and WebSocket transports. Use when building A2A servers, configuring MCP transports, setting up server endpoints, or when user mentions A2A configuration, server transport, MCP server setup, or agent communication protocols.",
                "path": "plugins/a2a-protocol/skills/a2a-server-config/SKILL.md",
                "frontmatter": {
                  "name": "a2a-server-config",
                  "description": "Agent-to-Agent (A2A) server configuration patterns for HTTP, STDIO, SSE, and WebSocket transports. Use when building A2A servers, configuring MCP transports, setting up server endpoints, or when user mentions A2A configuration, server transport, MCP server setup, or agent communication protocols.",
                  "allowed-tools": "Bash, Read, Write, Edit, Grep, Glob"
                },
                "content": "# A2A Server Configuration\n\nProvides complete patterns and templates for configuring Agent-to-Agent (A2A) servers with different transport mechanisms (HTTP, STDIO, SSE, WebSocket) following MCP (Model Context Protocol) standards.\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n- NEVER hardcode actual API keys or secrets\n- NEVER include real credentials in examples\n- NEVER commit sensitive values to git\n\n- ALWAYS use placeholders: `your_service_key_here`\n- ALWAYS create `.env.example` with placeholders only\n- ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n- ALWAYS read from environment variables in code\n- ALWAYS document where to obtain keys\n\n**Placeholder format:** `{service}_{env}_your_key_here`\n\n## Instructions\n\n### Phase 1: Analyze Requirements\n\nDetermine server configuration needs:\n\n1. **Transport Type**\n   - HTTP: Remote access, REST-like communication, CORS support\n   - STDIO: Local process communication, pipe-based I/O\n   - SSE (Server-Sent Events): Real-time streaming, one-way server push\n   - WebSocket: Bidirectional real-time communication\n\n2. **Framework Detection**\n   - Python: FastAPI, Flask, Starlette\n   - TypeScript: Express, Fastify, Node.js native http\n   - Detect from package.json or requirements.txt\n\n3. **Configuration Needs**\n   - Port and host settings\n   - CORS configuration\n   - Authentication requirements\n   - Environment variables\n\n### Phase 2: Select and Load Templates\n\nBased on requirements, use templates from `templates/`:\n\n**Python Templates:**\n- `templates/python-http-server.py` - FastAPI HTTP server\n- `templates/python-stdio-server.py` - STDIO transport\n- `templates/python-sse-server.py` - SSE streaming\n- `templates/python-websocket-server.py` - WebSocket bidirectional\n\n**TypeScript Templates:**\n- `templates/typescript-http-server.ts` - Express HTTP server\n- `templates/typescript-stdio-server.ts` - STDIO transport\n- `templates/typescript-sse-server.ts` - SSE streaming\n- `templates/typescript-websocket-server.ts` - WebSocket bidirectional\n\n### Phase 3: Configure Transport\n\nApply configuration based on transport type:\n\n**HTTP Configuration:**\n```python\n# Python (FastAPI)\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=8000,\n        reload=True\n    )\n```\n\n```typescript\n// TypeScript (Express)\nconst PORT = process.env.PORT || 8000;\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n```\n\n**STDIO Configuration:**\n```python\n# Python\nmcp.run(transport=\"stdio\")\n```\n\n```typescript\n// TypeScript\nserver.connect(new StdioServerTransport());\n```\n\n**SSE Configuration:**\n```python\n# Python\n@app.get(\"/events\")\nasync def events():\n    return EventSourceResponse(event_generator())\n```\n\n**WebSocket Configuration:**\n```python\n# Python\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n```\n\n### Phase 4: Add CORS and Security\n\nFor HTTP/SSE/WebSocket servers:\n\n```python\n# Python\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configure appropriately\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\n```typescript\n// TypeScript\nimport cors from 'cors';\napp.use(cors({\n  origin: process.env.ALLOWED_ORIGINS?.split(',') || '*',\n  credentials: true\n}));\n```\n\n### Phase 5: Environment Configuration\n\nCreate `.env.example` with placeholders:\n\n```bash\n# Server Configuration\nPORT=8000\nHOST=0.0.0.0\nALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173\n\n# API Keys (NEVER commit real values)\nANTHROPIC_API_KEY=your_anthropic_key_here\nOPENAI_API_KEY=your_openai_key_here\n\n# Transport Settings\nTRANSPORT_TYPE=http\nENABLE_CORS=true\n```\n\n### Phase 6: Validation\n\nRun validation script:\n```bash\nbash scripts/validate-config.sh <server-file>\n```\n\nChecks:\n- No hardcoded API keys\n- Environment variable usage\n- CORS configuration\n- Transport setup validity\n- .gitignore includes .env files\n\n## Scripts\n\n- `scripts/validate-config.sh` - Validate server configuration\n- `scripts/generate-server.sh` - Generate server from template\n- `scripts/test-transport.sh` - Test transport connectivity\n\n## Templates\n\n**Python:**\n- `templates/python-http-server.py` - HTTP server with FastAPI\n- `templates/python-stdio-server.py` - STDIO transport\n- `templates/python-sse-server.py` - SSE streaming server\n- `templates/python-websocket-server.py` - WebSocket server\n\n**TypeScript:**\n- `templates/typescript-http-server.ts` - HTTP server with Express\n- `templates/typescript-stdio-server.ts` - STDIO transport\n- `templates/typescript-sse-server.ts` - SSE streaming server\n- `templates/typescript-websocket-server.ts` - WebSocket server\n\n## Examples\n\n- `examples/http-fastapi-example.md` - Complete HTTP server with FastAPI\n- `examples/stdio-simple-example.md` - Basic STDIO server\n- `examples/sse-streaming-example.md` - SSE streaming configuration\n- `examples/websocket-bidirectional-example.md` - WebSocket bidirectional communication\n\n## Requirements\n\n- Framework-specific dependencies (FastAPI/Express/etc.)\n- CORS middleware for HTTP/SSE/WebSocket\n- Environment variable management (python-dotenv/dotenv)\n- No hardcoded API keys or secrets\n- .gitignore protection for sensitive files\n\n## Use Cases\n\n1. **Setting up HTTP server for remote A2A communication**\n   - Load http template\n   - Configure CORS\n   - Set environment variables\n   - Validate configuration\n\n2. **Configuring STDIO for local agent communication**\n   - Load stdio template\n   - Configure process pipes\n   - Test connectivity\n\n3. **Implementing SSE for real-time agent updates**\n   - Load sse template\n   - Configure event streams\n   - Set up CORS\n   - Test streaming\n\n4. **Setting up WebSocket for bidirectional agent chat**\n   - Load websocket template\n   - Configure connection handling\n   - Set up authentication\n   - Test bidirectional flow"
              },
              {
                "name": "agent-card-templates",
                "description": "A2A agent card JSON templates with schema validation and examples for different agent types. Use when creating agent cards, implementing A2A protocol discovery, setting up agent metadata, configuring authentication schemes, defining agent capabilities, or when user mentions agent card, agent discovery, A2A metadata, service endpoint configuration, or agent authentication setup.",
                "path": "plugins/a2a-protocol/skills/agent-card-templates/SKILL.md",
                "frontmatter": {
                  "name": "agent-card-templates",
                  "description": "A2A agent card JSON templates with schema validation and examples for different agent types. Use when creating agent cards, implementing A2A protocol discovery, setting up agent metadata, configuring authentication schemes, defining agent capabilities, or when user mentions agent card, agent discovery, A2A metadata, service endpoint configuration, or agent authentication setup.",
                  "allowed-tools": "Read, Write, Bash, Grep, Glob"
                },
                "content": "# Agent Card Templates\n\n**Purpose:** Provide reusable JSON templates for creating A2A (Agent-to-Agent) protocol agent cards following the official specification.\n\n**Activation Triggers:**\n- Creating new A2A agent cards\n- Implementing agent discovery endpoints\n- Configuring authentication schemes\n- Defining agent capabilities and skills\n- Setting up service endpoints\n- Validating agent card JSON structure\n\n**Key Resources:**\n- `templates/schema.json` - Complete JSON schema for validation\n- `templates/basic-agent-card.json` - Simple agent card template\n- `templates/multi-capability-agent-card.json` - Agent with multiple skills\n- `templates/authenticated-agent-card.json` - Agent with auth requirements\n- `templates/streaming-agent-card.json` - Agent with streaming support\n- `examples/` - Real-world agent card examples\n- `scripts/validate-agent-card.sh` - Schema validation script\n- `scripts/generate-agent-card.sh` - Interactive card generator\n- `scripts/test-agent-card.sh` - Format and structure testing\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n‚ùå NEVER hardcode actual API keys or secrets\n‚ùå NEVER include real credentials in examples\n‚ùå NEVER commit sensitive values to git\n\n‚úÖ ALWAYS use placeholders: `your_service_key_here`\n‚úÖ ALWAYS create `.env.example` with placeholders only\n‚úÖ ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n‚úÖ ALWAYS read from environment variables in code\n‚úÖ ALWAYS document where to obtain keys\n\n**Placeholder format:** `{service}_{env}_your_key_here`\n\n## Agent Card Structure\n\n### Required Fields\n\n**Basic Identity:**\n- `id` - Unique identifier (URI or UUID)\n- `name` - Human-readable display name\n- `protocolVersion` - A2A protocol version (e.g., \"0.3\")\n- `serviceEndpoint` - Base URL for agent's A2A service\n- `provider` - Organization information (name, contactEmail, url)\n\n**Security:**\n- `securitySchemes` - Authentication scheme definitions\n- `security` - Required auth combinations\n\n**Capabilities:**\n- `capabilities` - Feature flags (streaming, pushNotifications)\n\n### Optional Fields\n\n- `description` - Detailed agent purpose explanation\n- `logo` - URL to logo image\n- `version` - Agent implementation version\n- `skills` - Array of agent capabilities with schemas\n- `extensions` - Extension support declarations\n- `metadata` - Custom key-value pairs\n\n## Template Categories\n\n### 1. Basic Agent Card\n**Use for:** Simple agents with minimal configuration\n**Template:** `templates/basic-agent-card.json`\n**Features:**\n- Required fields only\n- Simple API key authentication\n- No streaming or advanced capabilities\n- Single skill definition\n\n### 2. Multi-Capability Agent Card\n**Use for:** Agents with multiple skills and capabilities\n**Template:** `templates/multi-capability-agent-card.json`\n**Features:**\n- Multiple skill definitions\n- Input/output schemas for each skill\n- Capability flags enabled\n- Comprehensive metadata\n\n### 3. Authenticated Agent Card\n**Use for:** Agents with complex authentication requirements\n**Template:** `templates/authenticated-agent-card.json`\n**Features:**\n- Multiple authentication schemes (API Key, Bearer, OAuth2)\n- Alternative auth combinations\n- Security scopes and permissions\n- OpenID Connect support\n\n### 4. Streaming Agent Card\n**Use for:** Agents supporting real-time updates\n**Template:** `templates/streaming-agent-card.json`\n**Features:**\n- Streaming capability enabled\n- Push notification support\n- WebHook configuration\n- SSE (Server-Sent Events) ready\n\n## Authentication Schemes\n\n### Supported Types\n\n**API Key:**\n```json\n{\n  \"type\": \"apiKey\",\n  \"name\": \"X-API-Key\",\n  \"in\": \"header\"\n}\n```\n\n**Bearer Token:**\n```json\n{\n  \"type\": \"http\",\n  \"scheme\": \"bearer\"\n}\n```\n\n**OAuth 2.0:**\n```json\n{\n  \"type\": \"oauth2\",\n  \"flows\": {\n    \"authorizationCode\": {\n      \"authorizationUrl\": \"https://provider.example/oauth/authorize\",\n      \"tokenUrl\": \"https://provider.example/oauth/token\",\n      \"scopes\": {\n        \"read\": \"Read access\",\n        \"write\": \"Write access\"\n      }\n    }\n  }\n}\n```\n\n**Basic Auth:**\n```json\n{\n  \"type\": \"http\",\n  \"scheme\": \"basic\"\n}\n```\n\n## Usage Workflow\n\n### 1. Select Template\n\nChoose template based on agent requirements:\n```bash\n# List available templates\nls templates/\n\n# View template content\ncat templates/basic-agent-card.json\n```\n\n### 2. Generate Agent Card\n\nUse interactive generator:\n```bash\n./scripts/generate-agent-card.sh\n\n# Or specify template directly\n./scripts/generate-agent-card.sh --template basic\n```\n\n**Generator prompts for:**\n- Agent name and description\n- Service endpoint URL\n- Provider information\n- Authentication scheme\n- Capabilities and skills\n\n### 3. Validate Agent Card\n\nValidate against schema:\n```bash\n# Validate structure and required fields\n./scripts/validate-agent-card.sh agent-card.json\n\n# Test format and completeness\n./scripts/test-agent-card.sh agent-card.json\n```\n\n**Validation checks:**\n- Required fields present\n- Valid JSON syntax\n- Schema compliance\n- URL format validation\n- Authentication scheme correctness\n\n### 4. Deploy Agent Card\n\nHost at standard location:\n```\nhttps://<base_url>/.well-known/agent.json\n```\n\nOr alternative:\n```\nhttps://<base_url>/.well-known/agent-card.json\n```\n\n## Skill Definition\n\nEach skill in the agent card includes:\n\n```json\n{\n  \"name\": \"skill-identifier\",\n  \"description\": \"What the skill does\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"param1\": {\"type\": \"string\"}\n    },\n    \"required\": [\"param1\"]\n  },\n  \"outputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"result\": {\"type\": \"string\"}\n    }\n  },\n  \"inputModes\": [\"text/plain\", \"application/json\"],\n  \"outputModes\": [\"application/json\"],\n  \"tags\": [\"category\", \"feature\"],\n  \"examples\": [\n    {\n      \"input\": {\"param1\": \"example\"},\n      \"output\": {\"result\": \"example output\"}\n    }\n  ]\n}\n```\n\n## Scripts Reference\n\n### validate-agent-card.sh\n**Purpose:** Validate agent card against JSON schema\n**Usage:**\n```bash\n./scripts/validate-agent-card.sh <agent-card.json>\n```\n**Checks:**\n- JSON syntax validity\n- Required fields presence\n- Schema compliance\n- URL format validation\n\n### generate-agent-card.sh\n**Purpose:** Interactive agent card generator\n**Usage:**\n```bash\n./scripts/generate-agent-card.sh [--template TEMPLATE]\n```\n**Options:**\n- `--template basic|multi|authenticated|streaming`\n- `--output FILE` - Output file path\n- `--interactive` - Prompt for all values\n\n### test-agent-card.sh\n**Purpose:** Test agent card format and structure\n**Usage:**\n```bash\n./scripts/test-agent-card.sh <agent-card.json>\n```\n**Tests:**\n- Well-formed JSON\n- Required fields present\n- Valid authentication schemes\n- Capability flags consistency\n- Service endpoint accessibility\n\n## Examples Reference\n\n**`examples/calculator-agent.md`** - Simple math calculation agent\n**`examples/translation-agent.md`** - Multi-language translation service\n**`examples/data-analysis-agent.md`** - Complex data processing agent\n\nEach example includes:\n- Complete agent card JSON\n- Authentication configuration\n- Skill definitions\n- Usage scenarios\n- Deployment instructions\n\n## Best Practices\n\n‚úì **Use standard well-known URI** - `/.well-known/agent.json`\n‚úì **Include comprehensive descriptions** - Help with discovery\n‚úì **Define clear input/output schemas** - Enable validation\n‚úì **Specify authentication requirements** - Security first\n‚úì **Version your agent cards** - Track changes\n‚úì **Test card accessibility** - Ensure HTTP GET works\n‚úì **Document all skills** - Include examples\n‚úì **Use placeholder credentials** - Never hardcode secrets\n\n## Common Issues\n\n**Issue:** Agent card not discovered\n**Fix:** Verify `/.well-known/agent.json` is accessible via HTTP GET\n\n**Issue:** Authentication failures\n**Fix:** Check `securitySchemes` and `security` match implementation\n\n**Issue:** Schema validation errors\n**Fix:** Run `validate-agent-card.sh` to identify missing/invalid fields\n\n**Issue:** Skills not recognized\n**Fix:** Ensure input/output schemas are valid JSON Schema format\n\n## Resources\n\n**A2A Protocol Specification:** https://a2a-protocol.org/latest/specification/\n**Agent Card Concepts:** https://agent2agent.info/docs/concepts/agentcard/\n**JSON Schema Reference:** https://json-schema.org/\n\n---\n\n**Supported A2A Protocol Version:** 0.3+\n**Template Version:** 1.0.0\n**Last Updated:** 2025-12-20"
              }
            ]
          },
          {
            "name": "google-adk",
            "description": "Build AI agents with Google's Agent Development Kit (ADK) - supports Python, TypeScript, Go, and Java",
            "source": "./plugins/google-adk",
            "category": "development",
            "version": "1.0.0",
            "author": {
              "name": "Plugin Developer",
              "email": "noreply@google-adk.dev"
            },
            "install_commands": [
              "/plugin marketplace add vanman2024/ai-dev-marketplace",
              "/plugin install google-adk@ai-dev-marketplace"
            ],
            "signals": {
              "stars": 0,
              "forks": 0,
              "pushed_at": "2025-12-29T02:15:49Z",
              "created_at": "2025-10-25T05:00:07Z",
              "license": null
            },
            "commands": [
              {
                "name": "/add-a2a",
                "description": "Set up Agent-to-Agent (A2A) protocol for multi-agent systems using Google ADK",
                "path": "plugins/google-adk/commands/add-a2a.md",
                "frontmatter": {
                  "description": "Set up Agent-to-Agent (A2A) protocol for multi-agent systems using Google ADK",
                  "argument-hint": [
                    "project-path"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Configure Agent-to-Agent communication protocol to enable multi-agent coordination, task delegation, and collaborative problem-solving using Google Agent Development Kit.\n\nCore Principles:\n- Detect project structure before assuming implementation\n- Use Google ADK best practices for A2A protocol setup\n- Follow framework-specific patterns (FastAPI, Flask, Django, etc.)\n- Validate configuration and provide clear setup verification\n\nPhase 1: Discovery\nGoal: Understand project context and current state\n\nActions:\n- Parse $ARGUMENTS for project path (default to current directory if not provided)\n- Detect project type and framework using file patterns\n- Example: !{bash ls -la package.json pyproject.toml setup.py requirements.txt 2>/dev/null}\n- Load existing configuration files to understand current setup\n- Check if Google ADK is already installed/configured\n\nPhase 2: Validation\nGoal: Verify prerequisites and environment readiness\n\nActions:\n- Check if project directory exists and is valid\n- Verify Google ADK installation or requirements\n- Identify any existing agent implementations\n- Detect conflicts with existing A2A configurations\n- Load relevant framework files for context\n\nPhase 3: Planning\nGoal: Design A2A protocol implementation approach\n\nActions:\n- Determine A2A protocol architecture based on project type\n- Identify which agents need A2A communication\n- Plan message schema, routing, and coordination patterns\n- Consider security, authentication, and error handling\n- Present implementation plan with key decisions\n\nPhase 4: Implementation\nGoal: Set up A2A protocol with Google ADK\n\nActions:\n\nTask(description=\"Configure A2A protocol\", subagent_type=\"google-adk-a2a-specialist\", prompt=\"You are the google-adk-a2a-specialist agent. Set up Agent-to-Agent (A2A) protocol for $ARGUMENTS.\n\nContext: Project has been analyzed and prerequisites validated.\n\nYour responsibilities:\n1. Configure A2A communication infrastructure\n2. Implement agent discovery and registration\n3. Set up message routing and protocol handlers\n4. Configure task delegation and coordination patterns\n5. Implement error handling and retry logic\n6. Add monitoring and observability for agent interactions\n7. Create example agents demonstrating A2A communication\n8. Generate documentation for A2A usage\n\nRequirements:\n- Follow Google ADK A2A protocol specifications\n- Use framework-specific best practices\n- Implement secure agent-to-agent communication\n- Include comprehensive error handling\n- Add logging and debugging capabilities\n- Provide clear examples and documentation\n\nExpected output:\n- A2A protocol configuration files\n- Agent communication infrastructure\n- Example multi-agent implementations\n- Setup verification tests\n- Documentation and usage guides\")\n\nPhase 5: Verification\nGoal: Validate A2A protocol setup and functionality\n\nActions:\n- Verify all A2A configuration files are created\n- Check agent communication infrastructure is working\n- Run example agents to test A2A protocol\n- Example: !{bash python -m pytest tests/test_a2a_protocol.py 2>/dev/null || echo \"No tests found\"}\n- Validate message routing and task delegation\n- Test error handling and recovery mechanisms\n\nPhase 6: Summary\nGoal: Document what was accomplished and next steps\n\nActions:\n- Summarize A2A protocol setup completed\n- List all files created or modified\n- Highlight key configuration decisions made\n- Provide examples of using A2A protocol\n- Suggest next steps:\n  - Create additional agents using A2A\n  - Implement specific multi-agent workflows\n  - Configure advanced A2A features (security, monitoring)\n  - Test agent coordination scenarios"
              },
              {
                "name": "/add-agent",
                "description": "Add LLM, custom, or workflow agents to Google ADK project",
                "path": "plugins/google-adk/commands/add-agent.md",
                "frontmatter": {
                  "description": "Add LLM, custom, or workflow agents to Google ADK project",
                  "argument-hint": "<agent-type> <agent-name>",
                  "allowed-tools": "Task, Read, Write, Edit, Bash(*), Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add new agents (LLM-based, custom tool, or workflow) to a Google ADK project with proper configuration and integration.\n\nCore Principles:\n- Detect project structure before assuming configuration\n- Support all three agent types (LLM, custom, workflow)\n- Follow Google ADK best practices and patterns\n- Validate agent configuration before finalizing\n\nPhase 1: Discovery\nGoal: Understand the target project and agent requirements\n\nActions:\n- Parse $ARGUMENTS for agent type and name\n- If $ARGUMENTS is unclear or missing, use AskUserQuestion to gather:\n  - What type of agent? (llm/custom/workflow)\n  - What is the agent name?\n  - What should the agent do?\n  - Any specific tools or capabilities needed?\n- Verify this is a Google ADK project:\n  - Check for agent configuration patterns\n  - Example: !{bash ls -la agents/ app/ 2>/dev/null || echo \"Not found\"}\n- Load existing agent configurations for context\n- Example: @agents/\n\nPhase 2: Analysis\nGoal: Understand existing agent patterns and project structure\n\nActions:\n- Find existing agent implementations:\n  - Example: !{bash find agents/ -name \"*.py\" -o -name \"*.ts\" 2>/dev/null | head -5}\n- Read sample agent files to understand patterns\n- Identify agent registry location\n- Determine required dependencies and imports\n- Check for agent configuration files\n\nPhase 3: Planning\nGoal: Design the agent implementation approach\n\nActions:\n- Based on agent type (llm/custom/workflow), plan:\n  - Required files and structure\n  - Dependencies and imports\n  - Configuration format\n  - Integration points with existing agents\n- Present plan to user:\n  - Agent structure to be created\n  - Files that will be modified\n  - Configuration approach\n  - Any trade-offs or considerations\n\nPhase 4: Implementation\nGoal: Create the agent with proper configuration\n\nActions:\n\nTask(description=\"Add Google ADK agent\", subagent_type=\"google-adk-agent-builder\", prompt=\"You are the google-adk-agent-builder agent. Add a new agent to this Google ADK project for $ARGUMENTS.\n\nContext from discovery:\n- Agent type and name parsed from arguments\n- Existing project structure analyzed\n- Agent patterns identified\n\nRequirements:\n- Create agent implementation file\n- Configure agent according to type (LLM/custom/workflow)\n- Add proper imports and dependencies\n- Register agent in agent registry\n- Follow Google ADK best practices\n- Include appropriate error handling\n- Add documentation and comments\n\nAgent Types:\n- LLM: Uses language model for reasoning and responses\n- Custom: Uses custom tools and functions\n- Workflow: Orchestrates multiple steps or sub-agents\n\nExpected output:\n- Agent implementation file created\n- Agent registered and configured\n- Documentation included\n- Summary of capabilities and usage\")\n\nPhase 5: Verification\nGoal: Ensure the agent is properly configured\n\nActions:\n- Verify agent file was created\n- Check agent registration\n- Validate configuration syntax if applicable\n- Example: !{bash python -m py_compile agents/*.py 2>&1 || true}\n- Test agent can be imported/loaded\n- Confirm no syntax errors\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize agent creation:\n  - Agent type and name\n  - Files created or modified\n  - Key capabilities added\n  - How to use the agent\n- Provide next steps:\n  - Testing the agent\n  - Configuring additional tools if needed\n  - Integration with other agents\n  - Deployment considerations"
              },
              {
                "name": "/add-evaluation",
                "description": "Add evaluation criteria and testing workflows",
                "path": "plugins/google-adk/commands/add-evaluation.md",
                "frontmatter": {
                  "description": "Add evaluation criteria and testing workflows",
                  "argument-hint": "<evaluation-type>",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive evaluation criteria and testing workflows to measure agent performance and quality\n\nCore Principles:\n- Understand evaluation requirements before implementing\n- Detect existing testing infrastructure\n- Follow Google ADK best practices for evaluation\n- Provide actionable metrics and reporting\n\nPhase 1: Discovery\nGoal: Gather context and requirements\n\nActions:\n- Parse $ARGUMENTS to identify evaluation type (unit tests, integration tests, benchmarks, quality metrics)\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - What aspects need evaluation? (accuracy, latency, cost, safety)\n  - What are success criteria?\n  - What testing infrastructure exists?\n  - Any specific compliance requirements?\n- Detect project type and framework:\n\n!{bash ls -la 2>/dev/null | grep -E \"package.json|pyproject.toml|requirements.txt\"}\n\n- Load relevant configuration files for context:\n\n!{bash find . -maxdepth 3 -name \"*.config.*\" -o -name \"pytest.ini\" -o -name \"jest.config.*\" 2>/dev/null | head -5}\n\nPhase 2: Analysis\nGoal: Understand existing codebase and evaluation patterns\n\nActions:\n- Search for existing test files and evaluation code:\n\n!{bash find . -type f \\( -name \"*test*.py\" -o -name \"*test*.ts\" -o -name \"*test*.js\" -o -name \"*eval*.py\" \\) 2>/dev/null | head -10}\n\n- Identify agent implementations that need evaluation\n- Check for existing evaluation frameworks or tools\n- Understand current quality metrics and reporting\n\nPhase 3: Planning\nGoal: Design the evaluation approach\n\nActions:\n- Determine evaluation strategy based on requirements:\n  - Unit tests for individual components\n  - Integration tests for agent workflows\n  - Benchmark tests for performance metrics\n  - Quality scoring for output evaluation\n- Identify what metrics to track (accuracy, latency, cost, safety)\n- Plan reporting and visualization approach\n- Present evaluation plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Execute evaluation setup with specialized agent\n\nActions:\n\nTask(description=\"Add evaluation criteria and testing workflows\", subagent_type=\"google-adk-evaluation-specialist\", prompt=\"You are the google-adk-evaluation-specialist agent. Add evaluation criteria and testing workflows for $ARGUMENTS.\n\nContext from Discovery:\n- Evaluation type: [from Phase 1]\n- Project framework: [detected framework]\n- Existing tests: [found test files]\n- Success criteria: [from user requirements]\n\nRequirements:\n- Implement comprehensive test suites (unit, integration, benchmark)\n- Add quality metrics and scoring systems\n- Create evaluation reports and visualizations\n- Follow Google ADK evaluation best practices\n- Ensure tests are maintainable and extensible\n- Add CI/CD integration for automated testing\n\nExpected deliverables:\n- Test files with comprehensive coverage\n- Evaluation scripts and configuration\n- Metrics tracking and reporting setup\n- Documentation for running evaluations\n- CI/CD workflow files if needed\")\n\nPhase 5: Verification\nGoal: Verify evaluation setup works correctly\n\nActions:\n- Check that test files were created and are valid\n- Verify evaluation scripts can run successfully:\n\n!{bash if [ -f \"pytest.ini\" ]; then python -m pytest --collect-only 2>&1 | head -20; elif [ -f \"package.json\" ]; then npm test -- --listTests 2>&1 | head -20; fi}\n\n- Confirm metrics are being tracked correctly\n- Validate reporting output format\n- Test CI/CD integration if applicable\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize evaluation setup:\n  - Test files created and their purpose\n  - Metrics being tracked\n  - How to run evaluations locally\n  - CI/CD integration status\n- Highlight key evaluation criteria and success thresholds\n- Provide next steps:\n  - Run initial evaluation baseline\n  - Customize metrics for specific use cases\n  - Set up monitoring and alerting\n  - Schedule regular evaluation runs"
              },
              {
                "name": "/add-observability",
                "description": "Add logging, tracing, and analytics to Google ADK agents",
                "path": "plugins/google-adk/commands/add-observability.md",
                "frontmatter": {
                  "description": "Add logging, tracing, and analytics to Google ADK agents",
                  "argument-hint": [
                    "logging|tracing|analytics|all"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add comprehensive observability capabilities (logging, tracing, analytics) to Google ADK agents for monitoring and debugging.\n\nCore Principles:\n- Detect existing agent configurations before modifying\n- Understand observability requirements and scope\n- Follow Google ADK best practices for instrumentation\n- Integrate seamlessly with existing agent workflows\n\nPhase 1: Discovery\nGoal: Understand project context and observability requirements\n\nActions:\n- Parse $ARGUMENTS to determine observability type (logging, tracing, analytics, or all)\n- Detect Google ADK agents in the project\n- Example: !{bash find . -name \"*.py\" -type f | xargs grep -l \"from google.adk\"}\n- Check for existing observability configurations\n- Load package.json or pyproject.toml to understand dependencies\n\nPhase 2: Analysis\nGoal: Understand existing agent structure and identify integration points\n\nActions:\n- Read Google ADK agent files to understand current implementation\n- Identify where observability hooks should be added\n- Check for existing logging, tracing, or analytics libraries\n- Determine which observability tools to use based on project setup\n\nPhase 3: Requirements Clarification\nGoal: Gather specific observability needs from user\n\nActions:\n- If $ARGUMENTS is unclear or incomplete, use AskUserQuestion to gather:\n  - Which observability features are needed? (logging, tracing, analytics, or all)\n  - What logging level should be configured? (DEBUG, INFO, WARNING, ERROR)\n  - Should tracing include LLM call metadata?\n  - What analytics platform to integrate with? (Google Cloud Logging, OpenTelemetry, custom)\n  - Do you need real-time monitoring or batch analytics?\n\nPhase 4: Implementation\nGoal: Integrate observability using specialized agent\n\nActions:\n\nTask(description=\"Add observability to Google ADK agents\", subagent_type=\"google-adk-observability-integrator\", prompt=\"You are the google-adk-observability-integrator agent. Add observability capabilities to Google ADK agents based on $ARGUMENTS.\n\nContext from Discovery:\n- Observability type requested: $ARGUMENTS\n- Detected Google ADK agents and configurations\n- Existing project dependencies and structure\n\nRequirements:\n- Add appropriate logging instrumentation to agent lifecycle events\n- Configure tracing for LLM calls and tool executions if requested\n- Integrate analytics for performance metrics and usage tracking if requested\n- Follow Google ADK patterns for middleware and hooks\n- Ensure backward compatibility with existing agent code\n- Add necessary dependencies to project configuration\n- Create or update observability configuration files\n- Document observability setup and usage\n\nExpected output:\n- Modified agent files with observability hooks\n- Configuration files for logging/tracing/analytics\n- Updated dependency files (package.json, pyproject.toml)\n- Documentation on how to use and customize observability features\")\n\nPhase 5: Verification\nGoal: Ensure observability integration works correctly\n\nActions:\n- Check that all required dependencies are installed\n- Verify configuration files are properly formatted\n- Example: !{bash python -m py_compile $(find . -name \"*.py\" -type f) 2>&1 | head -20}\n- Review agent code for proper instrumentation\n- Confirm logging/tracing/analytics outputs are being generated\n\nPhase 6: Summary\nGoal: Report what was accomplished and next steps\n\nActions:\n- Summarize observability features added\n- List files modified and created\n- Provide examples of how to view logs, traces, or analytics\n- Suggest next steps:\n  - Test observability in development environment\n  - Configure production observability settings\n  - Set up dashboards for monitoring\n  - Review and adjust logging levels as needed"
              },
              {
                "name": "/add-streaming",
                "description": "Add bidirectional streaming and real-time features to Google ADK agents",
                "path": "plugins/google-adk/commands/add-streaming.md",
                "frontmatter": {
                  "description": "Add bidirectional streaming and real-time features to Google ADK agents",
                  "argument-hint": "agent-name or feature-description",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Add bidirectional streaming, real-time updates, and streaming capabilities to Google ADK agents for interactive, low-latency applications.\n\nCore Principles:\n- Understand existing agent structure before adding streaming\n- Follow Google ADK streaming patterns and best practices\n- Implement proper error handling and connection management\n- Provide real-time feedback during implementation\n\nPhase 1: Discovery\nGoal: Understand the target agent and streaming requirements\n\nActions:\n- Parse $ARGUMENTS to identify target agent or feature\n- If $ARGUMENTS is unclear, use AskUserQuestion to gather:\n  - Which agent needs streaming capabilities?\n  - What type of streaming is needed (bidi, server-to-client, client-to-server)?\n  - What real-time features are required?\n  - Any specific performance or latency requirements?\n- Detect project structure and Google ADK version\n- Example: !{bash find . -name \"package.json\" -o -name \"pyproject.toml\" | head -5}\n\nPhase 2: Analysis\nGoal: Understand existing agent implementation and identify integration points\n\nActions:\n- Locate target agent files using Glob\n- Example: !{bash find . -type f -name \"*agent*.py\" -o -name \"*agent*.ts\" | head -10}\n- Read agent configuration and implementation files\n- Identify current communication patterns\n- Check for existing streaming infrastructure\n- Understand dependencies and requirements\n\nPhase 3: Planning\nGoal: Design the streaming implementation approach\n\nActions:\n- Determine streaming architecture based on use case:\n  - Bidirectional streaming for interactive conversations\n  - Server-to-client for real-time updates\n  - Client-to-server for continuous input processing\n- Identify required dependencies (grpc, websockets, etc.)\n- Plan error handling and reconnection logic\n- Present implementation plan to user for confirmation\n\nPhase 4: Implementation\nGoal: Add streaming capabilities to the agent\n\nActions:\n\nTask(description=\"Add streaming capabilities\", subagent_type=\"google-adk-streaming-specialist\", prompt=\"You are the google-adk-streaming-specialist agent. Add bidirectional streaming and real-time features to $ARGUMENTS.\n\nContext: Google ADK agent requiring streaming capabilities\n\nRequirements:\n- Implement bidirectional streaming using Google ADK patterns\n- Add proper connection lifecycle management (connect, disconnect, reconnect)\n- Implement error handling and retry logic\n- Add real-time event handling and callbacks\n- Follow Google ADK best practices for streaming\n- Include code comments explaining streaming flow\n- Handle backpressure and flow control\n- Implement graceful degradation on connection issues\n\nStreaming Patterns to Consider:\n- gRPC bidirectional streaming for agent-to-agent communication\n- WebSocket connections for browser-based real-time updates\n- Server-Sent Events (SSE) for one-way server-to-client streaming\n- Streaming response handling with chunked data processing\n\nExpected output:\n- Updated agent files with streaming capabilities\n- Connection management implementation\n- Error handling and recovery logic\n- Example usage showing streaming in action\n- Documentation of streaming API\")\n\nPhase 5: Verification\nGoal: Verify streaming implementation works correctly\n\nActions:\n- Check that streaming code follows Google ADK patterns\n- Verify error handling and connection management\n- Review example usage and documentation\n- If tests exist, run them: !{bash npm test || python -m pytest || echo \"No tests configured\"}\n- Validate code quality: !{bash npm run lint || python -m pylint . || echo \"No linter configured\"}\n\nPhase 6: Summary\nGoal: Document what was accomplished\n\nActions:\n- Summarize streaming features added:\n  - Type of streaming implemented\n  - Connection management approach\n  - Error handling strategy\n  - Key files modified\n- Highlight streaming API usage\n- Suggest next steps:\n  - Test with real-time data\n  - Monitor connection stability\n  - Optimize for latency\n  - Add metrics and monitoring"
              },
              {
                "name": "/add-tools",
                "description": "Add Gemini API, Google Cloud, or custom tools to agents",
                "path": "plugins/google-adk/commands/add-tools.md",
                "frontmatter": {
                  "description": "Add Gemini API, Google Cloud, or custom tools to agents",
                  "argument-hint": "<tool-type> [tool-name]",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Integrate tools (Gemini API, Google Cloud services, or custom functions) into Google ADK agents for enhanced capabilities\n\nCore Principles:\n- Detect project context before suggesting tools\n- Validate Google Cloud credentials and API access\n- Follow Google ADK best practices for tool configuration\n- Test tool integration after implementation\n\nPhase 1: Discovery\nGoal: Understand project context and tool requirements\n\nActions:\n- Parse $ARGUMENTS to identify:\n  - Tool type (gemini-api, google-cloud, custom)\n  - Specific tool name if provided\n  - Target agent or application\n- Detect project structure and framework\n- Example: @pyproject.toml or @package.json\n- Check for existing Google ADK configuration\n- Example: !{bash find . -name \"agent.py\" -o -name \"*.ts\" | grep -E \"(agent|adk)\" | head -5}\n\nPhase 2: Validation\nGoal: Verify prerequisites and access\n\nActions:\n- Check if Google Cloud SDK is installed\n- Example: !{bash gcloud --version 2>/dev/null || echo \"Not installed\"}\n- Verify API credentials are configured\n- Check for required environment variables (GOOGLE_API_KEY, PROJECT_ID)\n- Validate tool compatibility with current ADK version\n- Load existing agent configurations to understand current tools\n- Example: @src/agent.py\n\nPhase 3: Planning\nGoal: Design tool integration approach\n\nActions:\n- Identify which tools to add based on requirements\n- Determine configuration approach (API keys, service accounts, OAuth)\n- Plan code changes needed in agent definitions\n- Outline testing strategy for new tools\n- If tool type is unclear, present options to user:\n  - Gemini API tools (code generation, multimodal processing)\n  - Google Cloud tools (Cloud Storage, BigQuery, Vertex AI)\n  - Custom tools (user-defined functions)\n\nPhase 4: Implementation\nGoal: Integrate tools using google-adk-tools-integrator agent\n\nActions:\n\nTask(description=\"Integrate tools into Google ADK agents\", subagent_type=\"google-adk-tools-integrator\", prompt=\"You are the google-adk-tools-integrator agent. Add tools to Google ADK agents for $ARGUMENTS.\n\nProject Context:\n- Framework detected in Phase 1\n- Existing agent configuration from Phase 2\n- Tool requirements from Phase 3\n\nRequirements:\n- Add tool definitions to agent configuration\n- Configure authentication and credentials\n- Implement error handling for tool failures\n- Follow Google ADK tool integration patterns\n- Create environment variable templates\n- Add usage examples and documentation\n\nDeliverable: Complete tool integration with configuration files, updated agent code, and usage examples\")\n\nPhase 5: Verification\nGoal: Validate tool integration works correctly\n\nActions:\n- Check that all configuration files are created\n- Verify environment variable templates exist\n- Example: @.env.example\n- Run syntax validation if applicable\n- Example: !{bash python -m py_compile src/agent.py 2>&1 || npm run typecheck 2>&1}\n- Test tool invocation if test suite exists\n- Confirm no hardcoded API keys in committed files\n\nPhase 6: Summary\nGoal: Document integration and next steps\n\nActions:\n- List tools that were integrated\n- Show configuration files created/modified\n- Highlight environment variables that need to be set\n- Provide next steps:\n  - Set up real API keys in .env file\n  - Test tool functionality with example queries\n  - Review security best practices for API key management\n- Display relevant file paths and code snippets"
              },
              {
                "name": "/build-full-stack",
                "description": "Complete Google ADK project setup - initializes project, adds agents/tools/streaming/A2A/observability/evaluation, validates, and deploys everything in one command",
                "path": "plugins/google-adk/commands/build-full-stack.md",
                "frontmatter": {
                  "description": "Complete Google ADK project setup - initializes project, adds agents/tools/streaming/A2A/observability/evaluation, validates, and deploys everything in one command",
                  "argument-hint": [
                    "project-name"
                  ],
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Complete end-to-end Google ADK project setup from initialization through deployment\n\nCore Principles:\n- Analyze project requirements FIRST before building\n- Execute sequentially - wait for each command to complete before proceeding\n- Track progress with TodoWrite\n- Validate at checkpoints\n- Each phase builds on previous output\n\nPhase 0: Project Analysis and Requirements Gathering\nGoal: Analyze existing project files to understand what needs to be built\n\nActions:\n\n**Read Project Configuration Files:**\n\n1. Read `.claude/project.json`:\n   !{Read .claude/project.json}\n   Extract:\n   - Tech stack (languages, frameworks)\n   - Infrastructure components\n   - Deployment targets\n   - Project name and description\n\n2. Read `features.json`:\n   !{Read features.json}\n   Extract:\n   - Feature list with IDs (F001, F002, etc.)\n   - Feature descriptions and requirements\n   - Dependencies between features\n   - Implementation status\n\n3. Read `execution.json` (if exists):\n   !{bash test -f execution.json && cat execution.json}\n   Extract:\n   - What's already been executed\n   - What's pending\n   - What failed and needs retry\n\n4. Scan `specs/` directory:\n   !{bash find specs/ -name \"*.md\" -type f 2>/dev/null | head -20}\n   Extract:\n   - Feature specifications\n   - Technical requirements\n   - API contracts\n   - Architecture decisions\n\n**Analyze Requirements:**\n\nParse project.json to determine:\n- Language: Python/TypeScript/Go/Java\n- Agent types needed: LLM/Custom/Workflow\n- Tools required: Gemini API, Google Cloud, Custom\n- Streaming needed: Yes/No (check for voice/video features)\n- A2A needed: Yes/No (check for multi-agent features)\n- Observability: Development/Production\n- Deployment target: Vertex AI/Cloud Run/GKE/Local\n\nParse features.json to determine:\n- Which features require agents\n- Which features need streaming\n- Which features need A2A protocol\n- Which features are already implemented\n\n**Create Customized Build Plan:**\n\nBased on analysis, determine execution order:\n- Skip init if SDK already configured\n- Only add components required by specs\n- Prioritize features with dependencies first\n- Use language-specific configurations\n- Target deployment platform from project.json\n\nStore analysis results:\n- $LANGUAGE (from project.json)\n- $AGENT_TYPES (from features.json)\n- $TOOLS_NEEDED (from specs)\n- $FEATURES_TO_BUILD (from features.json + execution.json)\n- $DEPLOYMENT_TARGET (from project.json)\n\nPhase 1: Initialize Progress Tracking\nGoal: Create todo list for all workflow phases\n\nActions:\n\nTodoWrite with todos:\n1. \"Initialize Google ADK project\" (pending)\n2. \"Add agents (LLM/custom/workflow)\" (pending)\n3. \"Integrate Gemini API and Google Cloud tools\" (pending)\n4. \"Configure bidirectional streaming\" (pending)\n5. \"Set up A2A protocol\" (pending)\n6. \"Add observability (logging/tracing/analytics)\" (pending)\n7. \"Configure evaluation and testing\" (pending)\n8. \"Deploy to cloud (Vertex AI/Cloud Run/GKE)\" (pending)\n\nPhase 2: Sequential Command Execution\nGoal: Execute all setup commands in order\n\nActions:\n\nMark todo #1 as in_progress.\nRun /google-adk:init $ARGUMENTS\nWAIT for completion.\n!{bash test -f pyproject.toml || test -f package.json}\nMark todo #1 as completed.\n\nMark todo #2 as in_progress.\nRun /google-adk:add-agent\nWAIT for completion.\nMark todo #2 as completed.\n\nMark todo #3 as in_progress.\nRun /google-adk:add-tools\nWAIT for completion.\nMark todo #3 as completed.\n\nMark todo #4 as in_progress.\nRun /google-adk:add-streaming\nWAIT for completion.\nMark todo #4 as completed.\n\nMark todo #5 as in_progress.\nRun /google-adk:add-a2a\nWAIT for completion.\nMark todo #5 as completed.\n\nMark todo #6 as in_progress.\nRun /google-adk:add-observability\nWAIT for completion.\nMark todo #6 as completed.\n\nMark todo #7 as in_progress.\nRun /google-adk:add-evaluation\nWAIT for completion.\nMark todo #7 as completed.\n\nMark todo #8 as in_progress.\nRun /google-adk:deploy\nWAIT for completion.\nMark todo #8 as completed.\n\nPhase 3: Summary\nGoal: Display comprehensive setup summary\n\nActions:\n\nDisplay summary:\n\n**Google ADK Full Stack Setup Complete**\n\nComponents Configured:\n- Project initialized with Google ADK SDK\n- Agents: LLM agents, custom agents, workflow agents\n- Tools: Gemini API, Google Cloud tools integrated\n- Streaming: Bidirectional streaming configured\n- A2A Protocol: Agent-to-Agent communication enabled\n- Observability: Logging, tracing, analytics active\n- Evaluation: Testing and metrics configured\n- Deployment: Live on Vertex AI/Cloud Run/GKE\n\nNext Steps:\n1. Test agent interactions\n2. Monitor observability dashboards\n3. Run evaluation test suite\n4. Access deployment endpoints\n\nMark all todos as completed."
              },
              {
                "name": "/deploy",
                "description": "Deploy Google ADK agents to Vertex AI, Cloud Run, or GKE",
                "path": "plugins/google-adk/commands/deploy.md",
                "frontmatter": {
                  "description": "Deploy Google ADK agents to Vertex AI, Cloud Run, or GKE",
                  "argument-hint": "deployment-target",
                  "allowed-tools": "Task, Read, Write, Edit, Bash, Glob, Grep, TodoWrite"
                },
                "content": "**Arguments**: $ARGUMENTS\n\nGoal: Deploy Google ADK agents to Google Cloud platforms (Vertex AI, Cloud Run, or GKE) with proper configuration and validation\n\nCore Principles:\n- Detect project structure and deployment requirements\n- Validate credentials and prerequisites before deployment\n- Support multiple deployment targets\n- Provide clear deployment status and URLs\n\nPhase 1: Discovery\nGoal: Understand project structure and deployment target\n\nActions:\n- Parse $ARGUMENTS for deployment target (vertex-ai, cloud-run, gke, or auto-detect)\n- Check for Google ADK project files\n- Example: !{bash ls -la adk.yaml package.json pyproject.toml go.mod pom.xml 2>/dev/null}\n- Detect project language and framework\n- Load deployment configuration if exists\n- Example: @adk.yaml\n\nPhase 2: Validation\nGoal: Verify prerequisites and credentials\n\nActions:\n- Check if gcloud CLI is installed and authenticated\n- Example: !{bash gcloud auth list --filter=status:ACTIVE --format=\"value(account)\" 2>/dev/null}\n- Verify Google Cloud project is set\n- Example: !{bash gcloud config get-value project 2>/dev/null}\n- Check for required deployment files (Dockerfile, k8s manifests, etc.)\n- Validate ADK configuration is complete\n- Load project context for agent\n\nPhase 3: Planning\nGoal: Determine deployment strategy\n\nActions:\n- Based on $ARGUMENTS and detected configuration:\n  - If \"vertex-ai\": Deploy as Vertex AI agent\n  - If \"cloud-run\": Deploy as Cloud Run service\n  - If \"gke\": Deploy to Google Kubernetes Engine\n  - If no target specified: Recommend based on project structure\n- Identify required Google Cloud APIs to enable\n- Check if deployment artifacts exist (container image, etc.)\n- Present deployment plan to user\n\nPhase 4: Deployment\nGoal: Execute deployment with specialist agent\n\nActions:\n\nTask(description=\"Deploy Google ADK agent\", subagent_type=\"google-adk-deployment-specialist\", prompt=\"You are the google-adk-deployment-specialist agent. Deploy the Google ADK agent for $ARGUMENTS.\n\nProject Context:\n- Project structure detected in Phase 1\n- Deployment target: [vertex-ai/cloud-run/gke]\n- Google Cloud project: [from gcloud config]\n- Language/framework: [detected language]\n\nRequirements:\n- Build container image if needed\n- Push to Google Container Registry or Artifact Registry\n- Configure deployment settings (scaling, resources, environment variables)\n- Deploy to target platform\n- Set up monitoring and logging\n- Configure IAM permissions if needed\n\nDeliverable:\n- Deployment status (success/failure)\n- Service URL or endpoint\n- Deployment configuration summary\n- Next steps for testing and monitoring\")\n\nPhase 5: Verification\nGoal: Confirm deployment succeeded\n\nActions:\n- Check deployment status\n- Verify service is running\n- Example: !{bash gcloud run services list --filter=\"metadata.name:adk-agent\" --format=\"value(status.url)\" 2>/dev/null}\n- Test health endpoint if available\n- Display service logs for verification\n\nPhase 6: Summary\nGoal: Report deployment results and next steps\n\nActions:\n- Display deployment summary:\n  - Platform deployed to\n  - Service URL/endpoint\n  - Configuration applied\n  - Resource allocation\n- Show monitoring and logging commands\n- Suggest next steps:\n  - Test the deployed agent\n  - Set up custom domain\n  - Configure CI/CD pipeline\n  - Enable additional monitoring"
              },
              {
                "name": "/init",
                "description": null,
                "path": "plugins/google-adk/commands/init.md",
                "frontmatter": null,
                "content": "---\ndescription: Initialize Google ADK project with Python/TypeScript/Go/Java support\nargument-hint: [language] [project-name]\nallowed-tools: Task, Read, Write, Edit, Bash, Glob, Grep, AskUserQuestion\n---\n\n**Arguments**: $ARGUMENTS\n\nGoal: Initialize a Google ADK (Agent Development Kit) project with the specified language runtime (Python, TypeScript, Go, or Java) and project structure.\n\nCore Principles:\n- Detect language from arguments or ask user\n- Validate environment and prerequisites\n- Follow Google ADK best practices for project structure\n- Provide clear setup instructions\n\nPhase 1: Discovery\nGoal: Parse arguments and determine project requirements\n\nActions:\n- Parse $ARGUMENTS to extract language and project name\n- Check if running in existing directory or need to create new one\n- Example: !{bash pwd}\n- If language or project name unclear, use AskUserQuestion to gather:\n  - Which language runtime? (Python, TypeScript, Go, or Java)\n  - What is the project name?\n  - Any specific ADK features needed? (LangChain, LangGraph, streaming)\n\nPhase 2: Environment Check\nGoal: Verify prerequisites for chosen language\n\nActions:\n- Check if required tools are installed based on language:\n  - Python: !{bash python3 --version 2>/dev/null || echo \"Not installed\"}\n  - TypeScript: !{bash node --version 2>/dev/null || echo \"Not installed\"}\n  - Go: !{bash go version 2>/dev/null || echo \"Not installed\"}\n  - Java: !{bash java -version 2>/dev/null || echo \"Not installed\"}\n- Check for existing Google ADK configuration\n- Verify if directory is empty or has existing files: !{bash ls -la}\n\nPhase 3: Project Setup\nGoal: Initialize ADK project with proper structure\n\nActions:\n\nTask(description=\"Initialize Google ADK project\", subagent_type=\"google-adk-setup-agent\", prompt=\"You are the google-adk-setup-agent. Initialize a Google ADK project for $ARGUMENTS.\n\nLanguage: [Extracted from arguments]\nProject Name: [Extracted from arguments]\n\nRequirements:\n- Create proper directory structure for the language runtime\n- Set up ADK configuration files (.mcp.json, package.json/pyproject.toml/go.mod/pom.xml)\n- Configure environment variables template (.env.example)\n- Add sample agent implementation\n- Include README with setup and usage instructions\n- Follow Google ADK documentation standards\n\n**Security**: Use placeholders for all API keys (GOOGLE_ADK_API_KEY=your_google_adk_key_here)\n\nDeliverable: Complete project structure with all configuration files\")\n\nPhase 4: Post-Setup Verification\nGoal: Verify initialization and provide next steps\n\nActions:\n- Check created files exist: !{bash ls -la}\n- Verify configuration files are valid\n- Display project structure: !{bash tree -L 2 2>/dev/null || find . -maxdepth 2 -type f}\n\nPhase 5: Summary\nGoal: Report what was created and next steps\n\nActions:\n- Summarize initialized project:\n  - Language runtime configured\n  - Files created\n  - Configuration completed\n- Provide clear next steps:\n  - How to install dependencies\n  - How to configure API keys\n  - How to run the first agent\n  - Link to Google ADK documentation\n"
              }
            ],
            "skills": [
              {
                "name": "a2a-patterns",
                "description": "Agent-to-Agent (A2A) protocol implementation patterns for Google ADK - exposing agents via A2A, consuming external agents, multi-agent communication, and protocol configuration. Use when building multi-agent systems, implementing A2A protocol, exposing agents as services, consuming remote agents, configuring agent cards, or when user mentions A2A, agent-to-agent, multi-agent collaboration, remote agents, or agent orchestration.",
                "path": "plugins/google-adk/skills/a2a-patterns/SKILL.md",
                "frontmatter": {
                  "name": "a2a-patterns",
                  "description": "Agent-to-Agent (A2A) protocol implementation patterns for Google ADK - exposing agents via A2A, consuming external agents, multi-agent communication, and protocol configuration. Use when building multi-agent systems, implementing A2A protocol, exposing agents as services, consuming remote agents, configuring agent cards, or when user mentions A2A, agent-to-agent, multi-agent collaboration, remote agents, or agent orchestration.",
                  "allowed-tools": "Bash, Read, Write, Edit, WebFetch"
                },
                "content": "# A2A Protocol Implementation Patterns\n\n## Instructions\n\nThis skill provides comprehensive patterns for implementing the Agent2Agent (A2A) protocol in Google's Agent Development Kit (ADK). The A2A protocol standardizes communication between AI agents, enabling multi-agent collaboration across different platforms and frameworks.\n\n## What is A2A?\n\nThe Agent2Agent (A2A) protocol enables AI agents to:\n- Discover each other's capabilities through Agent Cards\n- Communicate securely using standardized JSON-RPC messages\n- Collaborate across different frameworks (CrewAI, LangGraph, ADK)\n- Work across deployment platforms (Cloud Run, Agent Engine, GKE)\n\n**Key Concept:** A2A focuses on agent-to-agent collaboration in natural modalities, complementing MCP (Model Context Protocol) which handles tool/data connections.\n\n## Core Patterns\n\n### 1. Exposing Agents via A2A (Server-Side)\n\n**When to use:** Make your ADK agent available for other agents to consume\n\n**Template:** `templates/a2a-server.py`\n\n**Key Components:**\n- `AgentCard` at `/.well-known/agent.json` - Advertises capabilities\n- `AgentExecutor` - Handles incoming requests\n- `DefaultRequestHandler` - Processes JSON-RPC messages\n- `A2AStarletteApplication` - HTTP server implementation\n\n**Script:** `scripts/expose-agent.sh`\n\n### 2. Consuming External Agents (Client-Side)\n\n**When to use:** Integrate remote A2A agents as sub-agents\n\n**Template:** `templates/a2a-client.py`\n\n**Key Components:**\n- `A2ACardResolver` - Discovers remote agent capabilities\n- `send_task` tool - Sends messages to remote agents\n- Session tracking - Maintains context across interactions\n\n**Script:** `scripts/consume-agent.sh`\n\n### 3. Multi-Agent Communication\n\n**When to use:** Orchestrate multiple specialized agents collaborating on complex tasks\n\n**Template:** `templates/multi-agent-orchestration.py`\n\n**Pattern:**\n- Coordinator agent routes tasks\n- Specialist agents handle specific domains\n- Agent-to-agent messaging via A2A protocol\n- Result aggregation and synthesis\n\n**Example:** `examples/purchasing-concierge/`\n\n### 4. Agent Card Configuration\n\n**When to use:** Define agent capabilities for discovery\n\n**Template:** `templates/agent-card.json`\n\n**Contents:**\n- Agent metadata (name, description, version)\n- Capabilities and skills\n- Supported modalities (text, audio, video)\n- Endpoint URLs and protocol version\n- Streaming support indicators\n\n**Script:** `scripts/generate-agent-card.sh`\n\n## Implementation Patterns\n\n### Server-Side: Exposing an Agent\n\n```python\n# templates/a2a-server.py structure\nfrom adk import Agent\nfrom a2a import AgentExecutor, DefaultRequestHandler, AgentCard\n\nclass MyAgentExecutor(AgentExecutor):\n    \"\"\"Handle incoming A2A requests\"\"\"\n    async def execute(self, request):\n        # Process request using your agent\n        result = await self.agent.run(request.message)\n        return result\n\n# Configure Agent Card\nagent_card = AgentCard(\n    name=\"my-agent\",\n    description=\"Agent description\",\n    capabilities=[\"skill1\", \"skill2\"],\n    endpoint=\"https://my-agent.example.com\"\n)\n\n# Expose via HTTP\nfrom a2a import A2AStarletteApplication\n\napp = A2AStarletteApplication(\n    executor=MyAgentExecutor(),\n    card=agent_card\n)\n```\n\n**Deployment:**\n```bash\n# Deploy to Cloud Run\nbash scripts/expose-agent.sh --platform cloud-run\n\n# Deploy to Agent Engine\nbash scripts/expose-agent.sh --platform agent-engine\n\n# Deploy to GKE\nbash scripts/expose-agent.sh --platform gke\n```\n\n### Client-Side: Consuming an Agent\n\n```python\n# templates/a2a-client.py structure\nfrom adk import Agent\nfrom a2a import A2ACardResolver, send_task\n\n# Discover remote agent\nresolver = A2ACardResolver()\nagent_card = await resolver.resolve(\"https://remote-agent.example.com\")\n\n# Create tool to communicate with remote agent\nsend_task_tool = send_task(\n    agent_url=agent_card.endpoint,\n    session_id=\"unique-session-id\"\n)\n\n# Use in your agent\nmy_agent = Agent(\n    tools=[send_task_tool],\n    # ... other config\n)\n\n# Agent can now invoke remote agent\nresult = await my_agent.run(\"Ask the remote agent to do something\")\n```\n\n### Multi-Agent Orchestration\n\n```python\n# templates/multi-agent-orchestration.py structure\nfrom adk import Agent\nfrom a2a import A2ACardResolver, send_task\n\n# Discover specialist agents\nresolver = A2ACardResolver()\nresearch_agent = await resolver.resolve(\"https://research-agent.example.com\")\nanalysis_agent = await resolver.resolve(\"https://analysis-agent.example.com\")\nwriting_agent = await resolver.resolve(\"https://writing-agent.example.com\")\n\n# Coordinator agent\ncoordinator = Agent(\n    name=\"coordinator\",\n    tools=[\n        send_task(agent_url=research_agent.endpoint),\n        send_task(agent_url=analysis_agent.endpoint),\n        send_task(agent_url=writing_agent.endpoint)\n    ],\n    instructions=\"\"\"\n    You coordinate multiple specialist agents:\n    1. Use research agent to gather information\n    2. Use analysis agent to process findings\n    3. Use writing agent to synthesize results\n    \"\"\"\n)\n\n# Execute multi-agent workflow\nresult = await coordinator.run(\"Research and write a report on AI agents\")\n```\n\n## Agent Card Structure\n\n```json\n{\n  \"id\": \"my-agent\",\n  \"name\": \"My Agent\",\n  \"description\": \"Description of agent capabilities\",\n  \"version\": \"1.0.0\",\n  \"url\": \"https://my-agent.example.com\",\n  \"capabilities\": {\n    \"skills\": [\n      {\n        \"name\": \"skill1\",\n        \"description\": \"First skill description\"\n      },\n      {\n        \"name\": \"skill2\",\n        \"description\": \"Second skill description\"\n      }\n    ],\n    \"modalities\": [\"text\", \"image\"],\n    \"streaming\": true\n  },\n  \"protocol\": {\n    \"version\": \"0.3\",\n    \"transport\": \"grpc\"\n  }\n}\n```\n\n**Generation:**\n```bash\nbash scripts/generate-agent-card.sh \\\n  --name \"my-agent\" \\\n  --description \"Agent description\" \\\n  --skills \"skill1,skill2\" \\\n  --modalities \"text,image\" \\\n  --url \"https://my-agent.example.com\"\n```\n\n## Protocol Configuration\n\n### gRPC Transport (A2A v0.3+)\n\n```python\n# templates/grpc-config.py\nfrom a2a import A2AStarletteApplication, GrpcTransport\n\napp = A2AStarletteApplication(\n    executor=MyAgentExecutor(),\n    transport=GrpcTransport(\n        host=\"0.0.0.0\",\n        port=50051,\n        secure=True,\n        cert_file=\"/path/to/cert.pem\",\n        key_file=\"/path/to/key.pem\"\n    )\n)\n```\n\n### Security Cards (A2A v0.3+)\n\n```python\n# templates/security-card.py\nfrom a2a import SecurityCard, sign_card\n\n# Create security card\nsecurity_card = SecurityCard(\n    issuer=\"my-organization\",\n    audience=[\"trusted-agent-1\", \"trusted-agent-2\"],\n    permissions=[\"read\", \"write\"]\n)\n\n# Sign the card\nsigned_card = sign_card(\n    card=security_card,\n    private_key=\"/path/to/private-key.pem\"\n)\n```\n\n### JSON-RPC Message Format\n\n**Request:**\n```json\n{\n  \"id\": \"request-uuid\",\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"message/send\",\n  \"params\": {\n    \"message\": \"Task description\",\n    \"session_id\": \"session-uuid\",\n    \"context\": {}\n  }\n}\n```\n\n**Response:**\n```json\n{\n  \"id\": \"request-uuid\",\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"message\": \"Agent response\",\n    \"artifacts\": [],\n    \"status\": \"completed\"\n  }\n}\n```\n\n## Scripts\n\n### 1. Expose Agent via A2A\n\n```bash\nbash scripts/expose-agent.sh --platform cloud-run --region us-central1\n```\n\n**What it does:**\n- Generates Agent Card at `/.well-known/agent.json`\n- Creates Dockerfile with A2A server\n- Deploys to specified platform\n- Configures networking and security\n- Returns agent endpoint URL\n\n### 2. Consume Remote Agent\n\n```bash\nbash scripts/consume-agent.sh --url https://remote-agent.example.com\n```\n\n**What it does:**\n- Resolves Agent Card from remote URL\n- Validates capabilities\n- Generates client code\n- Creates `send_task` tool wrapper\n- Provides integration example\n\n### 3. Generate Agent Card\n\n```bash\nbash scripts/generate-agent-card.sh \\\n  --name \"my-agent\" \\\n  --description \"Agent description\" \\\n  --skills \"research,analysis,writing\"\n```\n\n**What it does:**\n- Creates JSON Agent Card\n- Validates against A2A schema\n- Generates `/.well-known/agent.json`\n- Provides endpoint configuration\n\n### 4. Validate A2A Configuration\n\n```bash\nbash scripts/validate-a2a.sh --config agent-card.json\n```\n\n**What it does:**\n- Checks Agent Card schema\n- Validates endpoint accessibility\n- Tests JSON-RPC message format\n- Verifies security configuration\n\n## Templates\n\n### Python Templates\n\n- `templates/a2a-server.py` - Server-side agent implementation\n- `templates/a2a-client.py` - Client-side agent consumption\n- `templates/multi-agent-orchestration.py` - Multi-agent coordination\n- `templates/grpc-config.py` - gRPC transport configuration\n- `templates/security-card.py` - Security card implementation\n\n### Go Templates\n\n- `templates/go/a2a-server.go` - Go server implementation\n- `templates/go/a2a-client.go` - Go client implementation\n\n### Configuration Templates\n\n- `templates/agent-card.json` - Agent Card JSON structure\n- `templates/deployment-config.yaml` - Cloud Run/GKE deployment\n- `templates/security-policy.json` - Security configuration\n\n## Examples\n\n### Example 1: Research Agent Cluster\n\n**Location:** `examples/research-cluster/`\n\n**Architecture:**\n- Coordinator agent orchestrates research workflow\n- Search agent gathers information\n- Analysis agent processes findings\n- Writing agent synthesizes results\n\n**Communication:** All agents communicate via A2A protocol\n\n### Example 2: E-Commerce Assistant\n\n**Location:** `examples/ecommerce-assistant/`\n\n**Architecture:**\n- Customer-facing agent handles inquiries\n- Inventory agent checks product availability\n- Pricing agent calculates costs\n- Payment agent processes transactions\n\n**Pattern:** Hierarchical agent structure with A2A messaging\n\n### Example 3: Code Review System\n\n**Location:** `examples/code-review/`\n\n**Architecture:**\n- Manager agent coordinates review process\n- Style agent checks code formatting\n- Security agent scans for vulnerabilities\n- Performance agent analyzes efficiency\n\n**Integration:** Each specialist agent is independent A2A service\n\n### Example 4: Data Pipeline\n\n**Location:** `examples/data-pipeline/`\n\n**Architecture:**\n- Ingestion agent collects data\n- Transformation agent processes data\n- Validation agent checks quality\n- Storage agent persists results\n\n**Workflow:** Sequential A2A agent execution\n\n## Production Best Practices\n\n### 1. Agent Discovery\n\n- Host Agent Cards at `/.well-known/agent.json`\n- Use semantic versioning for capabilities\n- Document all available skills clearly\n- Update cards when capabilities change\n\n### 2. Error Handling\n\n```python\ntry:\n    result = await send_task(remote_agent, task)\nexcept A2AConnectionError:\n    # Handle network failures\n    result = fallback_handler(task)\nexcept A2AAuthenticationError:\n    # Handle auth failures\n    log_security_event()\nexcept A2ATimeoutError:\n    # Handle timeouts\n    retry_with_backoff()\n```\n\n### 3. Security\n\n- Sign Agent Cards with private keys\n- Validate incoming requests\n- Use HTTPS/gRPC with TLS\n- Implement authentication and authorization\n- Rate limit agent-to-agent calls\n\n### 4. Monitoring\n\n```python\n# Track A2A metrics\nmetrics.record('a2a.request.count', {'agent': 'remote-agent'})\nmetrics.record('a2a.latency', latency_ms)\nmetrics.record('a2a.error.rate', error_count)\n```\n\n### 5. Testing\n\n```bash\n# Test agent exposure\nbash scripts/validate-a2a.sh --config agent-card.json\n\n# Test agent consumption\nbash scripts/test-a2a-client.sh --url https://remote-agent.example.com\n\n# Test multi-agent workflow\nbash scripts/test-orchestration.sh --config multi-agent-config.yaml\n```\n\n## Deployment Platforms\n\n### Cloud Run\n\n```bash\n# Deploy A2A agent to Cloud Run\ngcloud run deploy my-agent \\\n  --source . \\\n  --region us-central1 \\\n  --allow-unauthenticated\n```\n\n**Agent Card URL:** `https://my-agent-[hash]-uc.a.run.app/.well-known/agent.json`\n\n### Agent Engine\n\n```bash\n# Deploy to Agent Engine\nadk deploy --platform agent-engine --agent my-agent\n```\n\n**Features:** Managed scaling, built-in monitoring, A2A native\n\n### GKE (Google Kubernetes Engine)\n\n```bash\n# Deploy to GKE\nkubectl apply -f templates/deployment-config.yaml\n```\n\n**Benefits:** Full control, custom scaling, multi-region\n\n## Framework Interoperability\n\nA2A agents can use different frameworks:\n\n**ADK Agent:**\n```python\nfrom adk import Agent\nagent = Agent(name=\"adk-agent\", ...)\n```\n\n**CrewAI Agent:**\n```python\nfrom crewai import Agent\nagent = Agent(role=\"crew-agent\", ...)\n```\n\n**LangGraph Agent:**\n```python\nfrom langgraph import StateGraph\nagent = StateGraph(...)\n```\n\n**All communicate via A2A protocol** - Framework is transparent to clients\n\n## Integration with Gemini API\n\n```python\n# templates/gemini-integration.py\nfrom adk import Agent\nfrom vertexai.preview.generative_models import GenerativeModel\n\n# ADK agent using Gemini\nagent = Agent(\n    name=\"gemini-agent\",\n    model=GenerativeModel(\"gemini-2.0-flash-exp\"),\n    tools=[send_task_tool]\n)\n\n# Expose via A2A\nfrom a2a import A2AStarletteApplication\napp = A2AStarletteApplication(\n    executor=GeminiAgentExecutor(agent)\n)\n```\n\n## A2A + MCP Integration\n\n```python\n# templates/a2a-mcp-integration.py\nfrom adk import Agent\nfrom a2a import send_task\nfrom mcp import use_mcp_server\n\n# Agent with both A2A (agents) and MCP (tools)\nagent = Agent(\n    name=\"hybrid-agent\",\n    # A2A: Communicate with other agents\n    tools=[\n        send_task(agent_url=\"https://research-agent.example.com\")\n    ],\n    # MCP: Connect to data sources\n    mcps=[\n        use_mcp_server(\"filesystem\"),\n        use_mcp_server(\"database\")\n    ]\n)\n```\n\n**Use Case:** Agent uses MCP for data access, A2A for agent collaboration\n\n## Requirements\n\n**Environment Variables:**\n- `GOOGLE_CLOUD_PROJECT` - GCP project ID (for Gemini/Vertex AI)\n- `GOOGLE_APPLICATION_CREDENTIALS` - Service account key path\n- `A2A_AGENT_URL` - Your agent's public URL (for card generation)\n\n**Dependencies:**\n```bash\npip install google-adk[a2a]\npip install google-cloud-aiplatform\npip install grpcio  # For gRPC transport\n```\n\n**Infrastructure:**\n- GCP project with Vertex AI API enabled\n- Cloud Run or Agent Engine (for deployment)\n- Domain with HTTPS (for production Agent Cards)\n\n## Security: API Key Handling\n\n**CRITICAL:** When generating any configuration files or code:\n\n- NEVER hardcode actual API keys or secrets\n- NEVER include real credentials in examples\n- NEVER commit sensitive values to git\n\n- ALWAYS use placeholders: `your_service_key_here`\n- ALWAYS create `.env.example` with placeholders only\n- ALWAYS add `.env*` to `.gitignore` (except `.env.example`)\n- ALWAYS read from environment variables in code\n- ALWAYS document where to obtain keys\n\n**Placeholder format:** `{service}_{env}_your_key_here`\n\nExample:\n```bash\n# .env.example (safe to commit)\nGOOGLE_CLOUD_PROJECT=your_project_id_here\nGOOGLE_APPLICATION_CREDENTIALS=/path/to/your_service_account_key.json\nA2A_AGENT_URL=https://your_agent_url_here\n\n# .env (NEVER commit)\nGOOGLE_CLOUD_PROJECT=actual-project-id\nGOOGLE_APPLICATION_CREDENTIALS=/actual/path/to/key.json\nA2A_AGENT_URL=https://my-agent-xyz.run.app\n```\n\n## Troubleshooting\n\n**Agent Card Not Found:**\n- Verify `/.well-known/agent.json` is accessible\n- Check CORS configuration\n- Validate JSON schema\n\n**Connection Refused:**\n- Confirm agent is running\n- Check firewall rules\n- Verify endpoint URL\n\n**Authentication Failed:**\n- Validate security card signature\n- Check permissions\n- Review audience list\n\n**Message Format Error:**\n- Verify JSON-RPC 2.0 format\n- Check message structure\n- Validate parameter types\n\n## Resources\n\n**Official Documentation:**\n- A2A Protocol: https://a2a-protocol.org/\n- ADK A2A Guide: https://google.github.io/adk-docs/a2a/\n- Google Cloud Blog: https://cloud.google.com/blog/products/ai-machine-learning/agent2agent-protocol-is-getting-an-upgrade\n\n**Code Examples:**\n- Python A2A Samples: https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents\n- Purchasing Concierge Codelab: https://codelabs.developers.google.com/intro-a2a-purchasing-concierge\n\n**Community:**\n- A2A GitHub: https://github.com/a2aproject\n- ADK Python: https://github.com/google/adk-python\n\n---\n\n**Plugin:** google-adk\n**Version:** 1.0.0\n**Protocol Version:** A2A v0.3+\n**Language Support:** Python (stable), Go (stable)\n**Deployment Platforms:** Cloud Run, Agent Engine, GKE"
              },
              {
                "name": "observability-patterns",
                "description": "Comprehensive observability setup patterns for Google ADK agents including logging configuration, Cloud Trace integration, BigQuery Agent Analytics, and third-party observability tools (AgentOps, Phoenix, Weave). Use when implementing monitoring, debugging agent behavior, analyzing agent performance, setting up tracing, or when user mentions observability, logging, tracing, BigQuery analytics, AgentOps, Phoenix, Arize, or Weave.",
                "path": "plugins/google-adk/skills/observability-patterns/SKILL.md",
                "frontmatter": {
                  "name": "observability-patterns",
                  "description": "Comprehensive observability setup patterns for Google ADK agents including logging configuration, Cloud Trace integration, BigQuery Agent Analytics, and third-party observability tools (AgentOps, Phoenix, Weave). Use when implementing monitoring, debugging agent behavior, analyzing agent performance, setting up tracing, or when user mentions observability, logging, tracing, BigQuery analytics, AgentOps, Phoenix, Arize, or Weave.",
                  "allowed-tools": "Bash, Read, Write, Edit"
                },
                "content": "# Observability Patterns Skill\n\nThis skill provides comprehensive templates and configurations for implementing observability in Google ADK agents. Includes logging, tracing, BigQuery analytics, Cloud Trace integration, and third-party observability platforms.\n\n## Overview\n\nGoogle ADK supports multiple observability approaches for monitoring, debugging, and analyzing agent behavior:\n\n1. **Cloud Trace** - Google Cloud native tracing with OpenTelemetry\n2. **BigQuery Agent Analytics** - Comprehensive event logging and analysis\n3. **AgentOps** - Session replays and unified tracing analytics\n4. **Phoenix (Arize)** - Open-source observability with self-hosted control\n5. **Weave (W&B)** - Weights & Biases platform for tracking and visualization\n\nThis skill covers production-ready observability implementations with security and scalability.\n\n## Available Scripts\n\n### 1. Setup Cloud Trace\n\n**Script**: `scripts/setup-cloud-trace.sh <project-id>`\n\n**Purpose**: Configures Cloud Trace integration for ADK agents\n\n**Parameters**:\n- `project-id` - Google Cloud project ID (required)\n\n**Usage**:\n```bash\n# Setup Cloud Trace for local development\n./scripts/setup-cloud-trace.sh my-project-id\n\n# Setup with ADK CLI deployment\nadk deploy agent_engine --project=my-project-id --trace_to_cloud ./agent\n```\n\n**Environment Variables**:\n- `GOOGLE_CLOUD_PROJECT` - Project ID for Cloud Trace\n- `GOOGLE_APPLICATION_CREDENTIALS` - Path to service account key\n\n**Output**: Cloud Trace enabled, traces visible in console.cloud.google.com\n\n### 2. Setup BigQuery Agent Analytics\n\n**Script**: `scripts/setup-bigquery-analytics.sh <project-id> <dataset-id> [bucket-name]`\n\n**Purpose**: Configures BigQuery Agent Analytics plugin for comprehensive event logging\n\n**Parameters**:\n- `project-id` - Google Cloud project ID (required)\n- `dataset-id` - BigQuery dataset name (required)\n- `bucket-name` - GCS bucket for multimodal content (optional)\n\n**Usage**:\n```bash\n# Setup basic BigQuery analytics\n./scripts/setup-bigquery-analytics.sh my-project agent-analytics\n\n# Setup with GCS for multimodal content\n./scripts/setup-bigquery-analytics.sh my-project agent-analytics my-content-bucket\n\n# Create dataset and table\nbq mk --dataset my-project:agent-analytics\nbq mk --table agent-analytics.agent_events_v2 templates/bigquery-schema.json\n```\n\n**IAM Requirements**:\n- `roles/bigquery.jobUser` - Required for BigQuery operations\n- `roles/bigquery.dataEditor` - Required for writing data\n- `roles/storage.objectCreator` - Required if using GCS offloading\n\n**Output**: BigQuery table created, events streaming to dataset\n\n### 3. Setup AgentOps\n\n**Script**: `scripts/setup-agentops.sh`\n\n**Purpose**: Configures AgentOps integration for session replays and metrics\n\n**Usage**:\n```bash\n# Install AgentOps\npip install -U agentops\n\n# Setup with API key\nAGENTOPS_API_KEY=your_api_key_here ./scripts/setup-agentops.sh\n\n# Verify setup\npython -c \"import agentops; agentops.init(); print('AgentOps ready')\"\n```\n\n**Environment Variables**:\n- `AGENTOPS_API_KEY` - AgentOps API key from app.agentops.ai/settings/projects\n\n**Output**: AgentOps initialized, sessions visible in dashboard\n\n### 4. Setup Phoenix\n\n**Script**: `scripts/setup-phoenix.sh`\n\n**Purpose**: Configures Phoenix (Arize) integration for open-source observability\n\n**Usage**:\n```bash\n# Install Phoenix packages\npip install openinference-instrumentation-google-adk arize-phoenix-otel\n\n# Setup Phoenix with API key\nPHOENIX_API_KEY=your_key_here \\\nPHOENIX_COLLECTOR_ENDPOINT=https://app.phoenix.arize.com/s/your-space \\\n./scripts/setup-phoenix.sh\n\n# Verify Phoenix connection\npython scripts/verify-phoenix.py\n```\n\n**Environment Variables**:\n- `PHOENIX_API_KEY` - Phoenix API key from phoenix.arize.com\n- `PHOENIX_COLLECTOR_ENDPOINT` - Phoenix collector endpoint URL\n\n**Output**: Phoenix tracer initialized, traces visible in Phoenix dashboard\n\n### 5. Setup Weave\n\n**Script**: `scripts/setup-weave.sh <entity> <project>`\n\n**Purpose**: Configures Weave (W&B) integration for observability\n\n**Parameters**:\n- `entity` - W&B entity name (visible in Teams sidebar)\n- `project` - W&B project name\n\n**Usage**:\n```bash\n# Install Weave dependencies\npip install opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n\n# Setup Weave with API key\nWANDB_API_KEY=your_wandb_key_here ./scripts/setup-weave.sh my-team my-project\n\n# Verify Weave connection\npython scripts/verify-weave.py\n```\n\n**Environment Variables**:\n- `WANDB_API_KEY` - W&B API key from wandb.ai/authorize\n\n**Output**: Weave tracer initialized, traces visible in Weave dashboard\n\n### 6. Validate Observability Setup\n\n**Script**: `scripts/validate-observability.sh`\n\n**Purpose**: Validates observability configuration and connectivity\n\n**Checks**:\n- Cloud Trace connectivity\n- BigQuery dataset and table existence\n- AgentOps initialization\n- Phoenix endpoint reachability\n- Weave endpoint reachability\n- IAM permissions\n- Environment variables set\n\n**Usage**:\n```bash\n# Validate all observability configurations\n./scripts/validate-observability.sh\n\n# Validate specific tool\n./scripts/validate-observability.sh --tool=bigquery\n./scripts/validate-observability.sh --tool=cloud-trace\n./scripts/validate-observability.sh --tool=agentops\n```\n\n**Exit Codes**:\n- `0` - All checks passed\n- `1` - Configuration missing\n- `2` - Connectivity failed\n- `3` - Permission issues\n\n## Available Templates\n\n### 1. Cloud Trace Configuration\n\n**Template**: `templates/cloud-trace-config.py`\n\n**Purpose**: Cloud Trace integration for ADK agents\n\n**Features**:\n- OpenTelemetry configuration\n- Automatic span creation for agent runs\n- LLM and tool call tracing\n- Error and latency tracking\n\n**Usage**:\n```python\n# Enable Cloud Trace via ADK CLI\nadk deploy agent_engine --project=$GOOGLE_CLOUD_PROJECT --trace_to_cloud ./agent\n\n# Or via Python SDK\nfrom google.adk.app import AdkApp\n\napp = AdkApp(\n    agent=my_agent,\n    enable_tracing=True\n)\n```\n\n**Span Labels**:\n- `invocation` - Top-level agent invocation\n- `agent_run` - Individual agent execution\n- `call_llm` - LLM API calls\n- `execute_tool` - Tool executions\n\n### 2. BigQuery Analytics Configuration\n\n**Template**: `templates/bigquery-analytics-config.py`\n\n**Purpose**: Complete BigQuery Agent Analytics plugin configuration\n\n**Features**:\n- Asynchronous event logging\n- Multimodal content with GCS offloading\n- OpenTelemetry-style tracing (trace_id, span_id)\n- Event filtering and batching\n- Custom content formatting\n\n**Usage**:\n```python\nfrom google.adk.plugins.bigquery_agent_analytics_plugin import (\n    BigQueryAgentAnalyticsPlugin, BigQueryLoggerConfig\n)\n\nbq_config = BigQueryLoggerConfig(\n    enabled=True,\n    gcs_bucket_name=\"your-bucket-name\",\n    max_content_length=500 * 1024,  # 500KB inline limit\n    batch_size=1,  # Low latency\n    event_allowlist=[\"LLM_RESPONSE\", \"TOOL_COMPLETED\"]\n)\n\nplugin = BigQueryAgentAnalyticsPlugin(\n    project_id=\"your-project-id\",\n    dataset_id=\"your-dataset-id\",\n    config=bq_config\n)\n\napp = App(root_agent=agent, plugins=[plugin])\n```\n\n**Configuration Options**:\n- `enabled` - Toggle logging on/off\n- `gcs_bucket_name` - GCS bucket for large content\n- `max_content_length` - Inline text limit (default 500KB)\n- `batch_size` - Events per write (default 1)\n- `event_allowlist` - Whitelist specific event types\n- `event_denylist` - Blacklist specific event types\n- `content_formatter` - Custom formatting function\n\n### 3. BigQuery Schema\n\n**Template**: `templates/bigquery-schema.json`\n\n**Purpose**: BigQuery table schema for agent_events_v2\n\n**Schema Fields**:\n- `timestamp` - Event recording time\n- `event_type` - Event category (LLM_REQUEST, TOOL_STARTING, etc.)\n- `content` - Event-specific JSON payload\n- `content_parts` - Structured multimodal data\n- `trace_id` - OpenTelemetry trace ID\n- `span_id` - OpenTelemetry span ID\n- `agent` - Agent name\n- `user_id` - User identifier\n\n**Partitioning**: By DATE(timestamp) for cost optimization\n\n**Clustering**: By event_type, agent, user_id for query performance\n\n### 4. AgentOps Configuration\n\n**Template**: `templates/agentops-config.py`\n\n**Purpose**: AgentOps integration for session replays\n\n**Features**:\n- Minimal two-line integration\n- Hierarchical span visualization\n- LLM call tracking with prompts and completions\n- Token count and latency metrics\n- Cost tracking\n\n**Usage**:\n```python\nimport agentops\n\n# Initialize AgentOps (before ADK imports)\nagentops.init()\n\n# Your ADK agent code\nfrom google.adk.app import App\napp = App(root_agent=my_agent)\n```\n\n**Span Hierarchy**:\n- Agent spans: Named `adk.agent.{AgentName}`\n- LLM spans: Capture prompts, completions, tokens\n- Tool spans: Record parameters and results\n\n### 5. Phoenix Configuration\n\n**Template**: `templates/phoenix-config.py`\n\n**Purpose**: Phoenix (Arize) integration for open-source observability\n\n**Features**:\n- Self-hosted data control\n- OpenInference instrumentation\n- Trace evaluation\n- Performance debugging\n- Custom evaluators\n\n**Usage**:\n```python\nimport os\nfrom phoenix.otel import register\n\n# Set Phoenix credentials\nos.environ[\"PHOENIX_API_KEY\"] = \"your_api_key_here\"\nos.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/s/your-space\"\n\n# Register Phoenix tracer\ntracer_provider = register(\n    project_name=\"my-adk-agent\",\n    auto_instrument=True\n)\n\n# Your ADK agent code (Phoenix auto-captures traces)\nfrom google.adk.app import App\napp = App(root_agent=my_agent)\n```\n\n**Auto-Instrumentation**: Phoenix automatically traces all ADK operations\n\n### 6. Weave Configuration\n\n**Template**: `templates/weave-config.py`\n\n**Purpose**: Weave (W&B) integration for observability\n\n**Features**:\n- Timeline of agent calls\n- Tool invocation tracking\n- Reasoning process analysis\n- Span hierarchy visualization\n- Dashboard integration\n\n**Usage**:\n```python\nimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nimport base64\n\n# Setup Weave exporter\nwandb_api_key = os.environ[\"WANDB_API_KEY\"]\nentity = \"your-entity\"\nproject = \"your-project\"\n\nauth_string = f\"api:{wandb_api_key}\"\nencoded_auth = base64.b64encode(auth_string.encode()).decode()\n\nexporter = OTLPSpanExporter(\n    endpoint=\"https://trace.wandb.ai/otel/v1/traces\",\n    headers={\n        \"Authorization\": f\"Basic {encoded_auth}\",\n        \"project_id\": f\"{entity}/{project}\"\n    }\n)\n\n# Configure tracer provider (BEFORE ADK imports)\nprovider = TracerProvider()\nprovider.add_span_processor(SimpleSpanProcessor(exporter))\ntrace.set_tracer_provider(provider)\n\n# Your ADK agent code\nfrom google.adk.app import App\napp = App(root_agent=my_agent)\n```\n\n**Critical**: Set tracer provider before importing ADK components\n\n## Available Examples\n\n### 1. Complete Observability Setup\n\n**Example**: `examples/complete-observability.md`\n\n**Covers**:\n- Multi-tool observability setup\n- Cloud Trace + BigQuery combination\n- Third-party tool integration\n- Production deployment patterns\n- Cost optimization strategies\n\n**Step-by-Step Guide**:\n1. Enable Cloud Trace for distributed tracing\n2. Configure BigQuery for event logging\n3. Add AgentOps for session replays\n4. Optional: Phoenix or Weave for additional insights\n5. Validate all configurations\n6. Deploy to production\n\n**Production Checklist**:\n- [ ] Cloud Trace enabled in production\n- [ ] BigQuery dataset created with proper IAM\n- [ ] GCS bucket configured for multimodal content\n- [ ] Event filtering configured to control costs\n- [ ] Alert rules defined for error rates\n- [ ] Dashboard created for key metrics\n- [ ] Retention policies set for cost control\n\n### 2. BigQuery Analytics Queries\n\n**Example**: `examples/bigquery-queries.md`\n\n**Covers**:\n- Conversation trace retrieval\n- Token usage analysis\n- Error rate tracking\n- Tool usage statistics\n- Performance metrics\n- Cost analysis\n\n**Query Examples**:\n```sql\n-- Retrieve conversation traces\nSELECT timestamp, event_type, JSON_VALUE(content, '$.response')\nFROM agent_events_v2\nWHERE trace_id = 'your-trace-id'\nORDER BY timestamp ASC;\n\n-- Token usage by agent\nSELECT\n  agent,\n  AVG(CAST(JSON_VALUE(content, '$.usage.total') AS INT64)) as avg_tokens,\n  SUM(CAST(JSON_VALUE(content, '$.usage.total') AS INT64)) as total_tokens\nFROM agent_events_v2\nWHERE event_type = 'LLM_RESPONSE'\nGROUP BY agent;\n\n-- Error rate by event type\nSELECT\n  event_type,\n  COUNT(*) as error_count,\n  DATE(timestamp) as day\nFROM agent_events_v2\nWHERE event_type LIKE '%ERROR%'\nGROUP BY event_type, day\nORDER BY day DESC, error_count DESC;\n\n-- Tool usage frequency\nSELECT\n  JSON_VALUE(content, '$.tool_name') as tool,\n  COUNT(*) as usage_count\nFROM agent_events_v2\nWHERE event_type = 'TOOL_COMPLETED'\nGROUP BY tool\nORDER BY usage_count DESC;\n\n-- Access multimodal content from GCS\nSELECT\n  part.mime_type,\n  part.object_ref.uri as gcs_uri\nFROM agent_events_v2,\nUNNEST(content_parts) AS part\nWHERE part.storage_mode = 'GCS_REFERENCE';\n```\n\n### 3. Multi-Tool Integration\n\n**Example**: `examples/multi-tool-integration.md`\n\n**Covers**:\n- Using multiple observability tools together\n- Cloud Trace + BigQuery + AgentOps\n- Data correlation across platforms\n- Tool selection criteria\n- Cost vs. insight tradeoffs\n\n**Integration Patterns**:\n\n**Pattern 1: Google Cloud Native**\n- Cloud Trace for distributed tracing\n- BigQuery for detailed event analysis\n- Best for: GCP-centric deployments\n\n**Pattern 2: Comprehensive Monitoring**\n- Cloud Trace for infrastructure tracing\n- AgentOps for session replays\n- BigQuery for analytics\n- Best for: Production monitoring with detailed debugging\n\n**Pattern 3: Open Source**\n- Phoenix for self-hosted observability\n- BigQuery for long-term storage\n- Best for: Data sovereignty requirements\n\n**Pattern 4: ML-Focused**\n- Weave for experiment tracking\n- BigQuery for analytics\n- Best for: Research and experimentation\n\n### 4. Production Deployment\n\n**Example**: `examples/production-deployment.md`\n\n**Covers**:\n- Production-ready observability configuration\n- IAM role setup\n- Cost optimization\n- Alert configuration\n- Dashboard creation\n- Incident response\n\n**Production Setup**:\n1. **IAM Configuration**:\n   - Service account with minimal permissions\n   - Separate dev/staging/prod credentials\n   - Workload Identity for GKE deployments\n\n2. **Cost Controls**:\n   - Event filtering to reduce BigQuery writes\n   - GCS lifecycle policies for multimodal content\n   - Table partitioning and clustering\n   - Retention policies (30-90 days)\n\n3. **Monitoring**:\n   - Cloud Monitoring alerts for error rates\n   - BigQuery query dashboard in Looker Studio\n   - AgentOps session replay for debugging\n   - Trace analysis for performance issues\n\n4. **Security**:\n   - No credentials in code (environment variables only)\n   - VPC Service Controls for data protection\n   - Customer-managed encryption keys (CMEK)\n   - Audit logging for compliance\n\n## Security Compliance\n\n**CRITICAL:** This skill follows strict security rules:\n\n‚ùå **NEVER hardcode:**\n- API keys (AgentOps, Phoenix, Weave, W&B)\n- Google Cloud credentials\n- Service account keys\n- OAuth tokens\n- BigQuery connection strings\n\n‚úÖ **ALWAYS:**\n- Use environment variables for secrets\n- Generate `.env.example` with placeholders\n- Add `.env*` to `.gitignore`\n- Use Google Application Default Credentials\n- Document credential acquisition process\n- Use IAM roles instead of service account keys when possible\n\n**Placeholder format:**\n```bash\n# .env.example\nGOOGLE_CLOUD_PROJECT=your-project-id\nAGENTOPS_API_KEY=your_agentops_key_here\nPHOENIX_API_KEY=your_phoenix_key_here\nPHOENIX_COLLECTOR_ENDPOINT=https://app.phoenix.arize.com/s/your-space\nWANDB_API_KEY=your_wandb_key_here\n```\n\n## Progressive Disclosure\n\nThis skill provides immediate setup guidance with references to detailed documentation:\n\n- **Quick Start**: Use setup scripts for immediate configuration\n- **Production**: Reference `production-deployment.md` for complete guide\n- **Analytics**: Use `bigquery-queries.md` for query templates\n- **Integration**: Reference `multi-tool-integration.md` for advanced patterns\n\nLoad additional files only when specific customization is needed.\n\n## Common Workflows\n\n### 1. Local Development Setup\n\n```bash\n# Enable Cloud Trace for local debugging\nexport GOOGLE_CLOUD_PROJECT=your-project-id\n./scripts/setup-cloud-trace.sh your-project-id\n\n# Start agent with tracing\npython my_agent.py\n# View traces at console.cloud.google.com/traces\n```\n\n### 2. Production Deployment with BigQuery\n\n```bash\n# 1. Create BigQuery dataset\nbq mk --dataset my-project:agent-analytics\n\n# 2. Create events table\nbq mk --table agent-analytics.agent_events_v2 templates/bigquery-schema.json\n\n# 3. Create GCS bucket for multimodal content\ngsutil mb gs://my-agent-content/\n\n# 4. Setup BigQuery analytics\n./scripts/setup-bigquery-analytics.sh my-project agent-analytics my-agent-content\n\n# 5. Deploy agent\nadk deploy agent_engine --project=my-project ./agent\n\n# 6. Validate setup\n./scripts/validate-observability.sh --tool=bigquery\n```\n\n### 3. Multi-Tool Integration\n\n```bash\n# 1. Setup Cloud Trace\nexport GOOGLE_CLOUD_PROJECT=your-project-id\n./scripts/setup-cloud-trace.sh your-project-id\n\n# 2. Setup BigQuery Analytics\n./scripts/setup-bigquery-analytics.sh your-project agent-analytics my-bucket\n\n# 3. Setup AgentOps\nexport AGENTOPS_API_KEY=your_key_here\n./scripts/setup-agentops.sh\n\n# 4. Validate all configurations\n./scripts/validate-observability.sh\n```\n\n## Troubleshooting\n\n### Cloud Trace Not Showing Traces\n\n**Check**:\n- `GOOGLE_CLOUD_PROJECT` environment variable is set\n- Cloud Trace API is enabled\n- Service account has `roles/cloudtrace.agent`\n- Tracer initialized before ADK imports\n\n**Debug**:\n```bash\n# Check Cloud Trace API status\ngcloud services list --enabled | grep cloudtrace\n\n# Enable Cloud Trace API\ngcloud services enable cloudtrace.googleapis.com\n\n# Test trace export\npython scripts/test-cloud-trace.py\n```\n\n### BigQuery Events Not Appearing\n\n**Check**:\n- Dataset and table exist\n- Service account has correct IAM roles\n- BigQuery API is enabled\n- Plugin configuration is correct\n- No event filtering blocking events\n\n**Debug**:\n```bash\n# Check dataset exists\nbq ls my-project:\n\n# Check table schema\nbq show --schema agent-analytics.agent_events_v2\n\n# Check IAM permissions\ngcloud projects get-iam-policy my-project \\\n  --flatten=\"bindings[].members\" \\\n  --filter=\"bindings.members:serviceAccount:YOUR_SA_EMAIL\"\n\n# Test plugin manually\npython scripts/test-bigquery-plugin.py\n```\n\n### AgentOps Not Capturing Traces\n\n**Check**:\n- AgentOps initialized before ADK imports\n- API key is valid\n- Network connectivity to app.agentops.ai\n- AgentOps package version is latest\n\n**Fix**:\n```bash\n# Update AgentOps\npip install -U agentops\n\n# Test initialization\npython -c \"import agentops; agentops.init(); print('Success')\"\n\n# Check for conflicts with other tracers\n# Ensure AgentOps is initialized first\n```\n\n### Phoenix Connection Failed\n\n**Check**:\n- Phoenix API key is valid\n- Collector endpoint URL is correct\n- Network access to Phoenix endpoint\n- Required packages installed\n\n**Debug**:\n```bash\n# Test Phoenix endpoint\ncurl -H \"Authorization: Bearer YOUR_KEY\" \\\n  https://app.phoenix.arize.com/s/YOUR_SPACE\n\n# Verify package versions\npip list | grep -E \"(openinference|phoenix)\"\n\n# Run verification script\npython scripts/verify-phoenix.py\n```\n\n### Weave Traces Not Appearing\n\n**Check**:\n- Tracer provider set BEFORE ADK imports\n- W&B API key is valid\n- Entity and project names are correct\n- OTEL exporter configured properly\n\n**Fix**:\n```python\n# Verify initialization order\n# 1. Import OTEL packages\n# 2. Configure and set tracer provider\n# 3. THEN import ADK\n\n# Correct order:\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\ntrace.set_tracer_provider(TracerProvider())  # FIRST\n\nfrom google.adk.app import App  # THEN\n```\n\n## Dependencies\n\n**Required**:\n- `google-adk>=1.21.0` - ADK framework (version 1.21.0+ for full BigQuery features)\n- `google-cloud-trace>=1.13.0` - Cloud Trace client (optional)\n- `google-cloud-bigquery>=3.0.0` - BigQuery client (optional)\n\n**Optional (Third-party tools)**:\n- `agentops>=0.3.0` - AgentOps integration\n- `openinference-instrumentation-google-adk>=0.1.0` - Phoenix instrumentation\n- `arize-phoenix-otel>=0.1.0` - Phoenix OTEL exporter\n- `opentelemetry-sdk>=1.20.0` - OpenTelemetry SDK for Weave\n- `opentelemetry-exporter-otlp-proto-http>=1.20.0` - OTLP exporter for Weave\n\n**Installation**:\n```bash\n# Core ADK with Cloud Trace\npip install google-adk google-cloud-trace\n\n# With BigQuery Analytics\npip install google-adk google-cloud-bigquery\n\n# With AgentOps\npip install google-adk agentops\n\n# With Phoenix\npip install google-adk openinference-instrumentation-google-adk arize-phoenix-otel\n\n# With Weave\npip install google-adk opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n\n# All observability tools\npip install google-adk google-cloud-trace google-cloud-bigquery agentops \\\n  openinference-instrumentation-google-adk arize-phoenix-otel \\\n  opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n```\n\n## Best Practices\n\n1. **Multi-Layer Observability**: Use Cloud Trace for infrastructure, BigQuery for analytics, and AgentOps for debugging\n2. **Cost Control**: Implement event filtering and retention policies to manage BigQuery costs\n3. **Security**: Never hardcode credentials; use environment variables and IAM roles\n4. **Progressive Rollout**: Start with Cloud Trace, add BigQuery when analytics needed\n5. **Tool Selection**: Choose tools based on requirements (open-source vs. managed, cost vs. features)\n6. **Data Correlation**: Use trace_id across all tools for unified debugging\n7. **Alert Configuration**: Set up alerts for error rates, latency spikes, and cost anomalies\n8. **Dashboard Creation**: Build custom dashboards in Looker Studio, Grafana, or tool-native UIs\n\n## Additional Resources\n\n- **Cloud Trace**: https://cloud.google.com/trace/docs\n- **BigQuery Agent Analytics**: https://google.github.io/adk-docs/observability/bigquery-agent-analytics/\n- **AgentOps**: https://app.agentops.ai/\n- **Phoenix (Arize)**: https://arize.com/docs/phoenix/\n- **Weave (W&B)**: https://docs.wandb.ai/weave/\n- **ADK Observability Guide**: https://google.github.io/adk-docs/observability/\n- **OpenTelemetry**: https://opentelemetry.io/docs/\n\n## Tool Comparison\n\n| Feature | Cloud Trace | BigQuery | AgentOps | Phoenix | Weave |\n|---------|------------|----------|----------|---------|-------|\n| **Hosting** | Google Cloud | Google Cloud | SaaS | SaaS/Self-hosted | SaaS |\n| **Cost** | Free tier + usage | Storage + queries | Free tier + paid | Free tier + paid | Free tier + paid |\n| **Setup Complexity** | Low | Medium | Very Low | Low | Medium |\n| **Data Control** | Google Cloud | Google Cloud | Third-party | Self-host option | Third-party |\n| **Query Flexibility** | Low | Very High | Medium | High | Medium |\n| **Real-time** | Yes | Near real-time | Yes | Yes | Yes |\n| **Custom Dashboards** | Limited | Full (Looker) | Built-in | Built-in | Built-in |\n| **Best For** | Infrastructure tracing | Deep analytics | Quick debugging | Open-source, control | ML experiments |"
              },
              {
                "name": "streaming-patterns",
                "description": "Configure ADK bidi-streaming for real-time multimodal interactions. Use when building live voice/video agents, implementing real-time streaming, configuring LiveRequestQueue, setting up audio/video processing, or when user mentions bidi-streaming, real-time agents, streaming tools, multimodal streaming, or Gemini Live API.",
                "path": "plugins/google-adk/skills/streaming-patterns/SKILL.md",
                "frontmatter": {
                  "name": "streaming-patterns",
                  "description": "Configure ADK bidi-streaming for real-time multimodal interactions. Use when building live voice/video agents, implementing real-time streaming, configuring LiveRequestQueue, setting up audio/video processing, or when user mentions bidi-streaming, real-time agents, streaming tools, multimodal streaming, or Gemini Live API.",
                  "allowed-tools": "Read, Write, Bash"
                },
                "content": "# ADK Streaming Patterns\n\nComprehensive patterns for configuring Google ADK bidi-streaming (bidirectional streaming) to build real-time, multimodal AI agents with voice, video, and streaming tool capabilities.\n\n## Core Concepts\n\nADK bidi-streaming enables low-latency, bidirectional communication between users and AI agents with:\n\n- **Real-time interaction**: Process and respond while user is still providing input\n- **Natural interruption**: User can interrupt agent mid-response\n- **Multimodal support**: Text, audio, and video inputs/outputs\n- **Streaming tools**: Tools that yield intermediate results over time\n- **Session persistence**: Maintain context across ~10-minute connection timeouts\n\n## Quick Start Patterns\n\n### 1. Basic Bidi-Streaming Setup\n\n```python\nfrom google.adk.agents import Agent\nfrom google.adk.agents.run_config import RunConfig, StreamingMode\nfrom google.genai import types\n\n# Configure for audio streaming\nrun_config = RunConfig(\n    response_modalities=[\"AUDIO\"],\n    streaming_mode=StreamingMode.BIDI\n)\n\n# Run agent with bidi-streaming\nasync for event in agent.run_live(request_queue, run_config=run_config):\n    if event.server_content:\n        # Handle streaming response\n        handle_response(event)\n```\n\n### 2. LiveRequestQueue Pattern\n\n```python\nfrom google.adk.agents import LiveRequestQueue\n\n# Create queue for multimodal inputs\nrequest_queue = LiveRequestQueue()\n\n# Enqueue text\nawait request_queue.put(\"What's the weather?\")\n\n# Enqueue audio chunks\nawait request_queue.put(audio_bytes)\n\n# Signal activity boundaries\nawait request_queue.put(types.LiveClientRealtimeInput(\n    media_chunks=[types.LiveClientRealtimeInputMediaChunk(\n        data=audio_chunk\n    )]\n))\n```\n\n## Configuration Patterns\n\n### Response Modalities\n\n**CRITICAL**: Only ONE response modality per session. Cannot switch mid-session.\n\n```python\n# Audio output (voice agent)\nRunConfig(response_modalities=[\"AUDIO\"])\n\n# Text output (chat agent)\nRunConfig(response_modalities=[\"TEXT\"])\n```\n\n### Session Management\n\n**Session Resumption** (automatic reconnection):\n\n```python\nRunConfig(\n    session_resumption=types.SessionResumptionConfig()\n)\n```\n\n**Context Window Compression** (unlimited sessions):\n\n```python\nRunConfig(\n    context_window_compression=types.ContextWindowCompressionConfig(\n        trigger_tokens=100000,\n        sliding_window=types.SlidingWindow(target_tokens=80000)\n    )\n)\n```\n\n### Audio Configuration\n\nSee templates/audio-config.py for speech and transcription settings.\n\n### Platform Selection\n\nUse environment variable (no code changes needed):\n\n```bash\n# Google AI Studio (Gemini Live API)\nexport GOOGLE_GENAI_USE_VERTEXAI=FALSE\n\n# Vertex AI (Live API)\nexport GOOGLE_GENAI_USE_VERTEXAI=TRUE\n```\n\n## Streaming Tools Pattern\n\nDefine tools as async generators for continuous results:\n\n```python\n@streaming_tool\nasync def monitor_stock(symbol: str):\n    \"\"\"Stream real-time stock price updates.\"\"\"\n    while True:\n        price = await fetch_current_price(symbol)\n        yield f\"Current price: ${price}\"\n        await asyncio.sleep(1)\n```\n\nSee templates/streaming-tool-template.py for complete pattern.\n\n## Event Handling\n\nProcess events from run_live():\n\n```python\nasync for event in agent.run_live(request_queue, run_config=run_config):\n    # Server content (agent responses)\n    if event.server_content:\n        if event.server_content.model_turn:\n            # Text/audio from model\n            process_model_response(event.server_content.model_turn)\n\n        if event.server_content.turn_complete:\n            # Agent finished speaking\n            handle_turn_complete()\n\n    # Tool calls\n    if event.tool_call:\n        # ADK executes tools automatically\n        log_tool_execution(event.tool_call)\n\n    # Interruptions\n    if event.interrupted:\n        handle_interruption()\n```\n\n## Multi-Agent Streaming\n\nTransfer stateful sessions between agents:\n\n```python\n# Agent 1 creates session\nsession = await agent1.run_live(request_queue, run_config=config)\n\n# Transfer to Agent 2 (seamless handoff)\nawait agent2.run_live(\n    request_queue,\n    run_config=config,\n    session=session  # Maintains conversation context\n)\n```\n\n## Workflows\n\n### Complete Bidi-Streaming Agent Workflow\n\n1. **Configure RunConfig**\n   - Choose response modality (AUDIO or TEXT)\n   - Enable session resumption\n   - Configure context window compression (optional)\n   - Set audio/speech configs (for audio modality)\n\n2. **Create LiveRequestQueue**\n   - Initialize queue for multimodal inputs\n   - Enqueue messages as they arrive\n   - Use activity markers for segmentation\n\n3. **Implement Event Handling**\n   - Process server_content for agent responses\n   - Handle tool_call events\n   - Manage interruption events\n   - Track turn_complete signals\n\n4. **Define Streaming Tools** (optional)\n   - Use async generators for continuous output\n   - Yield intermediate results over time\n   - Support real-time monitoring/analysis\n\n5. **Test and Deploy**\n   - Validate audio/video processing\n   - Test interruption handling\n   - Verify session resumption\n   - Monitor quota usage\n\n### Audio/Video Workflow\n\nSee examples/audio-video-agent.py for complete multimodal setup including:\n- Audio input processing\n- Video frame handling\n- Speech configuration\n- Transcription settings\n\n## Templates\n\nAll templates use placeholders only (no hardcoded API keys):\n\n- **templates/bidi-streaming-config.py**: Complete RunConfig patterns\n- **templates/streaming-tool-template.py**: Async generator tool pattern\n- **templates/audio-config.py**: Speech and transcription setup\n- **templates/video-config.py**: Video frame processing\n- **templates/liverequest-queue.py**: Queue management patterns\n- **templates/event-handler.py**: Event processing patterns\n\n## Scripts\n\nUtility scripts for validation and setup:\n\n- **scripts/validate-streaming-config.py**: Validate RunConfig settings\n- **scripts/test-liverequest-queue.py**: Test queue functionality\n- **scripts/check-modality-support.py**: Verify modality compatibility\n\n## Examples\n\nReal-world streaming agent implementations:\n\n- **examples/voice-agent.py**: Complete audio streaming agent\n- **examples/video-agent.py**: Multimodal video processing agent\n- **examples/streaming-tool-agent.py**: Agent with streaming tools\n- **examples/multi-agent-handoff.py**: Session transfer between agents\n\n## Best Practices\n\n1. **Choose Modality Carefully**: Cannot switch response modality mid-session\n2. **Use Session Resumption**: Prevent disconnection issues\n3. **Enable Context Compression**: For extended conversations\n4. **Implement Streaming Tools**: For real-time monitoring/analysis\n5. **Handle Interruptions**: Natural conversation requires interruption support\n6. **Segment Context**: Use activity markers for logical event boundaries\n7. **Test Platform Switch**: Verify behavior on both AI Studio and Vertex AI\n\n## Common Patterns\n\n### Pattern 1: Voice Agent with Interruption\n\nSee examples/voice-agent.py\n\n### Pattern 2: Streaming Analysis Tool\n\nSee examples/streaming-tool-agent.py\n\n### Pattern 3: Multi-Agent Coordination\n\nSee examples/multi-agent-handoff.py\n\n## References\n\n- [ADK Bidi-streaming Docs](https://google.github.io/adk-docs/streaming/)\n- [RunConfig Guide (Part 4)](https://google.github.io/adk-docs/streaming/dev-guide/part4/)\n- [Audio/Video Guide (Part 5)](https://google.github.io/adk-docs/streaming/dev-guide/part5/)\n- [Real-Time Multi-Agent Architecture](https://developers.googleblog.com/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system/)\n\n## Security Compliance\n\nThis skill follows strict security rules:\n- All code examples use placeholder values only\n- No real API keys, passwords, or secrets\n- Environment variable references in all code\n- `.gitignore` protection documented"
              }
            ]
          }
        ]
      }
    }
  ]
}